{"buggy_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"irq.h\"\n#include \"mmu.h\"\n#include \"cpuid.h\"\n#include \"lapic.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/sched.h>\n#include <linux/moduleparam.h>\n#include <linux/mod_devicetable.h>\n#include <linux/trace_events.h>\n#include <linux/slab.h>\n#include <linux/tboot.h>\n#include <linux/hrtimer.h>\n#include <linux/frame.h>\n#include <linux/nospec.h>\n#include \"kvm_cache_regs.h\"\n#include \"x86.h\"\n\n#include <asm/cpu.h>\n#include <asm/io.h>\n#include <asm/desc.h>\n#include <asm/vmx.h>\n#include <asm/virtext.h>\n#include <asm/mce.h>\n#include <asm/fpu/internal.h>\n#include <asm/perf_event.h>\n#include <asm/debugreg.h>\n#include <asm/kexec.h>\n#include <asm/apic.h>\n#include <asm/irq_remapping.h>\n#include <asm/mmu_context.h>\n#include <asm/nospec-branch.h>\n#include <asm/mshyperv.h>\n\n#include \"trace.h\"\n#include \"pmu.h\"\n#include \"vmx_evmcs.h\"\n\n#define __ex(x) __kvm_handle_fault_on_reboot(x)\n#define __ex_clear(x, reg) \\\n\t____kvm_handle_fault_on_reboot(x, \"xor \" reg \" , \" reg)\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\nstatic const struct x86_cpu_id vmx_cpu_id[] = {\n\tX86_FEATURE_MATCH(X86_FEATURE_VMX),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);\n\nstatic bool __read_mostly enable_vpid = 1;\nmodule_param_named(vpid, enable_vpid, bool, 0444);\n\nstatic bool __read_mostly enable_vnmi = 1;\nmodule_param_named(vnmi, enable_vnmi, bool, S_IRUGO);\n\nstatic bool __read_mostly flexpriority_enabled = 1;\nmodule_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_ept = 1;\nmodule_param_named(ept, enable_ept, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_unrestricted_guest = 1;\nmodule_param_named(unrestricted_guest,\n\t\t\tenable_unrestricted_guest, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_ept_ad_bits = 1;\nmodule_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);\n\nstatic bool __read_mostly emulate_invalid_guest_state = true;\nmodule_param(emulate_invalid_guest_state, bool, S_IRUGO);\n\nstatic bool __read_mostly fasteoi = 1;\nmodule_param(fasteoi, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_apicv = 1;\nmodule_param(enable_apicv, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_shadow_vmcs = 1;\nmodule_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);\n/*\n * If nested=1, nested virtualization is supported, i.e., guests may use\n * VMX and be a hypervisor for its own guests. If nested=0, guests may not\n * use VMX instructions.\n */\nstatic bool __read_mostly nested = 0;\nmodule_param(nested, bool, S_IRUGO);\n\nstatic u64 __read_mostly host_xss;\n\nstatic bool __read_mostly enable_pml = 1;\nmodule_param_named(pml, enable_pml, bool, S_IRUGO);\n\n#define MSR_TYPE_R\t1\n#define MSR_TYPE_W\t2\n#define MSR_TYPE_RW\t3\n\n#define MSR_BITMAP_MODE_X2APIC\t\t1\n#define MSR_BITMAP_MODE_X2APIC_APICV\t2\n#define MSR_BITMAP_MODE_LM\t\t4\n\n#define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL\n\n/* Guest_tsc -> host_tsc conversion requires 64-bit division.  */\nstatic int __read_mostly cpu_preemption_timer_multi;\nstatic bool __read_mostly enable_preemption_timer = 1;\n#ifdef CONFIG_X86_64\nmodule_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);\n#endif\n\n#define KVM_GUEST_CR0_MASK (X86_CR0_NW | X86_CR0_CD)\n#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR0_NE\n#define KVM_VM_CR0_ALWAYS_ON\t\t\t\t\\\n\t(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | \t\\\n\t X86_CR0_WP | X86_CR0_PG | X86_CR0_PE)\n#define KVM_CR4_GUEST_OWNED_BITS\t\t\t\t      \\\n\t(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \\\n\t | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)\n\n#define KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR4_VMXE\n#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)\n#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)\n\n#define RMODE_GUEST_OWNED_EFLAGS_BITS (~(X86_EFLAGS_IOPL | X86_EFLAGS_VM))\n\n#define VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE 5\n\n/*\n * Hyper-V requires all of these, so mark them as supported even though\n * they are just treated the same as all-context.\n */\n#define VMX_VPID_EXTENT_SUPPORTED_MASK\t\t\\\n\t(VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_NON_GLOBAL_BIT)\n\n/*\n * These 2 parameters are used to config the controls for Pause-Loop Exiting:\n * ple_gap:    upper bound on the amount of time between two successive\n *             executions of PAUSE in a loop. Also indicate if ple enabled.\n *             According to test, this time is usually smaller than 128 cycles.\n * ple_window: upper bound on the amount of time a guest is allowed to execute\n *             in a PAUSE loop. Tests indicate that most spinlocks are held for\n *             less than 2^12 cycles\n * Time is measured based on a counter that runs at the same rate as the TSC,\n * refer SDM volume 3b section 21.6.13 & 22.1.3.\n */\nstatic unsigned int ple_gap = KVM_DEFAULT_PLE_GAP;\n\nstatic unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;\nmodule_param(ple_window, uint, 0444);\n\n/* Default doubles per-vcpu window every exit. */\nstatic unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;\nmodule_param(ple_window_grow, uint, 0444);\n\n/* Default resets per-vcpu window every exit to ple_window. */\nstatic unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;\nmodule_param(ple_window_shrink, uint, 0444);\n\n/* Default is to compute the maximum so we can never overflow. */\nstatic unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;\nmodule_param(ple_window_max, uint, 0444);\n\nextern const ulong vmx_return;\n\nstruct kvm_vmx {\n\tstruct kvm kvm;\n\n\tunsigned int tss_addr;\n\tbool ept_identity_pagetable_done;\n\tgpa_t ept_identity_map_addr;\n};\n\n#define NR_AUTOLOAD_MSRS 8\n\nstruct vmcs {\n\tu32 revision_id;\n\tu32 abort;\n\tchar data[0];\n};\n\n/*\n * Track a VMCS that may be loaded on a certain CPU. If it is (cpu!=-1), also\n * remember whether it was VMLAUNCHed, and maintain a linked list of all VMCSs\n * loaded on this CPU (so we can clear them if the CPU goes down).\n */\nstruct loaded_vmcs {\n\tstruct vmcs *vmcs;\n\tstruct vmcs *shadow_vmcs;\n\tint cpu;\n\tbool launched;\n\tbool nmi_known_unmasked;\n\tunsigned long vmcs_host_cr3;\t/* May not match real cr3 */\n\tunsigned long vmcs_host_cr4;\t/* May not match real cr4 */\n\t/* Support for vnmi-less CPUs */\n\tint soft_vnmi_blocked;\n\tktime_t entry_time;\n\ts64 vnmi_blocked_time;\n\tunsigned long *msr_bitmap;\n\tstruct list_head loaded_vmcss_on_cpu_link;\n};\n\nstruct shared_msr_entry {\n\tunsigned index;\n\tu64 data;\n\tu64 mask;\n};\n\n/*\n * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a\n * single nested guest (L2), hence the name vmcs12. Any VMX implementation has\n * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is\n * stored in guest memory specified by VMPTRLD, but is opaque to the guest,\n * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.\n * More than one of these structures may exist, if L1 runs multiple L2 guests.\n * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the\n * underlying hardware which will be used to run L2.\n * This structure is packed to ensure that its layout is identical across\n * machines (necessary for live migration).\n *\n * IMPORTANT: Changing the layout of existing fields in this structure\n * will break save/restore compatibility with older kvm releases. When\n * adding new fields, either use space in the reserved padding* arrays\n * or add the new fields to the end of the structure.\n */\ntypedef u64 natural_width;\nstruct __packed vmcs12 {\n\t/* According to the Intel spec, a VMCS region must start with the\n\t * following two fields. Then follow implementation-specific data.\n\t */\n\tu32 revision_id;\n\tu32 abort;\n\n\tu32 launch_state; /* set to 0 by VMCLEAR, to 1 by VMLAUNCH */\n\tu32 padding[7]; /* room for future expansion */\n\n\tu64 io_bitmap_a;\n\tu64 io_bitmap_b;\n\tu64 msr_bitmap;\n\tu64 vm_exit_msr_store_addr;\n\tu64 vm_exit_msr_load_addr;\n\tu64 vm_entry_msr_load_addr;\n\tu64 tsc_offset;\n\tu64 virtual_apic_page_addr;\n\tu64 apic_access_addr;\n\tu64 posted_intr_desc_addr;\n\tu64 ept_pointer;\n\tu64 eoi_exit_bitmap0;\n\tu64 eoi_exit_bitmap1;\n\tu64 eoi_exit_bitmap2;\n\tu64 eoi_exit_bitmap3;\n\tu64 xss_exit_bitmap;\n\tu64 guest_physical_address;\n\tu64 vmcs_link_pointer;\n\tu64 guest_ia32_debugctl;\n\tu64 guest_ia32_pat;\n\tu64 guest_ia32_efer;\n\tu64 guest_ia32_perf_global_ctrl;\n\tu64 guest_pdptr0;\n\tu64 guest_pdptr1;\n\tu64 guest_pdptr2;\n\tu64 guest_pdptr3;\n\tu64 guest_bndcfgs;\n\tu64 host_ia32_pat;\n\tu64 host_ia32_efer;\n\tu64 host_ia32_perf_global_ctrl;\n\tu64 vmread_bitmap;\n\tu64 vmwrite_bitmap;\n\tu64 vm_function_control;\n\tu64 eptp_list_address;\n\tu64 pml_address;\n\tu64 padding64[3]; /* room for future expansion */\n\t/*\n\t * To allow migration of L1 (complete with its L2 guests) between\n\t * machines of different natural widths (32 or 64 bit), we cannot have\n\t * unsigned long fields with no explict size. We use u64 (aliased\n\t * natural_width) instead. Luckily, x86 is little-endian.\n\t */\n\tnatural_width cr0_guest_host_mask;\n\tnatural_width cr4_guest_host_mask;\n\tnatural_width cr0_read_shadow;\n\tnatural_width cr4_read_shadow;\n\tnatural_width cr3_target_value0;\n\tnatural_width cr3_target_value1;\n\tnatural_width cr3_target_value2;\n\tnatural_width cr3_target_value3;\n\tnatural_width exit_qualification;\n\tnatural_width guest_linear_address;\n\tnatural_width guest_cr0;\n\tnatural_width guest_cr3;\n\tnatural_width guest_cr4;\n\tnatural_width guest_es_base;\n\tnatural_width guest_cs_base;\n\tnatural_width guest_ss_base;\n\tnatural_width guest_ds_base;\n\tnatural_width guest_fs_base;\n\tnatural_width guest_gs_base;\n\tnatural_width guest_ldtr_base;\n\tnatural_width guest_tr_base;\n\tnatural_width guest_gdtr_base;\n\tnatural_width guest_idtr_base;\n\tnatural_width guest_dr7;\n\tnatural_width guest_rsp;\n\tnatural_width guest_rip;\n\tnatural_width guest_rflags;\n\tnatural_width guest_pending_dbg_exceptions;\n\tnatural_width guest_sysenter_esp;\n\tnatural_width guest_sysenter_eip;\n\tnatural_width host_cr0;\n\tnatural_width host_cr3;\n\tnatural_width host_cr4;\n\tnatural_width host_fs_base;\n\tnatural_width host_gs_base;\n\tnatural_width host_tr_base;\n\tnatural_width host_gdtr_base;\n\tnatural_width host_idtr_base;\n\tnatural_width host_ia32_sysenter_esp;\n\tnatural_width host_ia32_sysenter_eip;\n\tnatural_width host_rsp;\n\tnatural_width host_rip;\n\tnatural_width paddingl[8]; /* room for future expansion */\n\tu32 pin_based_vm_exec_control;\n\tu32 cpu_based_vm_exec_control;\n\tu32 exception_bitmap;\n\tu32 page_fault_error_code_mask;\n\tu32 page_fault_error_code_match;\n\tu32 cr3_target_count;\n\tu32 vm_exit_controls;\n\tu32 vm_exit_msr_store_count;\n\tu32 vm_exit_msr_load_count;\n\tu32 vm_entry_controls;\n\tu32 vm_entry_msr_load_count;\n\tu32 vm_entry_intr_info_field;\n\tu32 vm_entry_exception_error_code;\n\tu32 vm_entry_instruction_len;\n\tu32 tpr_threshold;\n\tu32 secondary_vm_exec_control;\n\tu32 vm_instruction_error;\n\tu32 vm_exit_reason;\n\tu32 vm_exit_intr_info;\n\tu32 vm_exit_intr_error_code;\n\tu32 idt_vectoring_info_field;\n\tu32 idt_vectoring_error_code;\n\tu32 vm_exit_instruction_len;\n\tu32 vmx_instruction_info;\n\tu32 guest_es_limit;\n\tu32 guest_cs_limit;\n\tu32 guest_ss_limit;\n\tu32 guest_ds_limit;\n\tu32 guest_fs_limit;\n\tu32 guest_gs_limit;\n\tu32 guest_ldtr_limit;\n\tu32 guest_tr_limit;\n\tu32 guest_gdtr_limit;\n\tu32 guest_idtr_limit;\n\tu32 guest_es_ar_bytes;\n\tu32 guest_cs_ar_bytes;\n\tu32 guest_ss_ar_bytes;\n\tu32 guest_ds_ar_bytes;\n\tu32 guest_fs_ar_bytes;\n\tu32 guest_gs_ar_bytes;\n\tu32 guest_ldtr_ar_bytes;\n\tu32 guest_tr_ar_bytes;\n\tu32 guest_interruptibility_info;\n\tu32 guest_activity_state;\n\tu32 guest_sysenter_cs;\n\tu32 host_ia32_sysenter_cs;\n\tu32 vmx_preemption_timer_value;\n\tu32 padding32[7]; /* room for future expansion */\n\tu16 virtual_processor_id;\n\tu16 posted_intr_nv;\n\tu16 guest_es_selector;\n\tu16 guest_cs_selector;\n\tu16 guest_ss_selector;\n\tu16 guest_ds_selector;\n\tu16 guest_fs_selector;\n\tu16 guest_gs_selector;\n\tu16 guest_ldtr_selector;\n\tu16 guest_tr_selector;\n\tu16 guest_intr_status;\n\tu16 host_es_selector;\n\tu16 host_cs_selector;\n\tu16 host_ss_selector;\n\tu16 host_ds_selector;\n\tu16 host_fs_selector;\n\tu16 host_gs_selector;\n\tu16 host_tr_selector;\n\tu16 guest_pml_index;\n};\n\n/*\n * For save/restore compatibility, the vmcs12 field offsets must not change.\n */\n#define CHECK_OFFSET(field, loc)\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(offsetof(struct vmcs12, field) != (loc),\t\\\n\t\t\"Offset of \" #field \" in struct vmcs12 has changed.\")\n\nstatic inline void vmx_check_vmcs12_offsets(void) {\n\tCHECK_OFFSET(revision_id, 0);\n\tCHECK_OFFSET(abort, 4);\n\tCHECK_OFFSET(launch_state, 8);\n\tCHECK_OFFSET(io_bitmap_a, 40);\n\tCHECK_OFFSET(io_bitmap_b, 48);\n\tCHECK_OFFSET(msr_bitmap, 56);\n\tCHECK_OFFSET(vm_exit_msr_store_addr, 64);\n\tCHECK_OFFSET(vm_exit_msr_load_addr, 72);\n\tCHECK_OFFSET(vm_entry_msr_load_addr, 80);\n\tCHECK_OFFSET(tsc_offset, 88);\n\tCHECK_OFFSET(virtual_apic_page_addr, 96);\n\tCHECK_OFFSET(apic_access_addr, 104);\n\tCHECK_OFFSET(posted_intr_desc_addr, 112);\n\tCHECK_OFFSET(ept_pointer, 120);\n\tCHECK_OFFSET(eoi_exit_bitmap0, 128);\n\tCHECK_OFFSET(eoi_exit_bitmap1, 136);\n\tCHECK_OFFSET(eoi_exit_bitmap2, 144);\n\tCHECK_OFFSET(eoi_exit_bitmap3, 152);\n\tCHECK_OFFSET(xss_exit_bitmap, 160);\n\tCHECK_OFFSET(guest_physical_address, 168);\n\tCHECK_OFFSET(vmcs_link_pointer, 176);\n\tCHECK_OFFSET(guest_ia32_debugctl, 184);\n\tCHECK_OFFSET(guest_ia32_pat, 192);\n\tCHECK_OFFSET(guest_ia32_efer, 200);\n\tCHECK_OFFSET(guest_ia32_perf_global_ctrl, 208);\n\tCHECK_OFFSET(guest_pdptr0, 216);\n\tCHECK_OFFSET(guest_pdptr1, 224);\n\tCHECK_OFFSET(guest_pdptr2, 232);\n\tCHECK_OFFSET(guest_pdptr3, 240);\n\tCHECK_OFFSET(guest_bndcfgs, 248);\n\tCHECK_OFFSET(host_ia32_pat, 256);\n\tCHECK_OFFSET(host_ia32_efer, 264);\n\tCHECK_OFFSET(host_ia32_perf_global_ctrl, 272);\n\tCHECK_OFFSET(vmread_bitmap, 280);\n\tCHECK_OFFSET(vmwrite_bitmap, 288);\n\tCHECK_OFFSET(vm_function_control, 296);\n\tCHECK_OFFSET(eptp_list_address, 304);\n\tCHECK_OFFSET(pml_address, 312);\n\tCHECK_OFFSET(cr0_guest_host_mask, 344);\n\tCHECK_OFFSET(cr4_guest_host_mask, 352);\n\tCHECK_OFFSET(cr0_read_shadow, 360);\n\tCHECK_OFFSET(cr4_read_shadow, 368);\n\tCHECK_OFFSET(cr3_target_value0, 376);\n\tCHECK_OFFSET(cr3_target_value1, 384);\n\tCHECK_OFFSET(cr3_target_value2, 392);\n\tCHECK_OFFSET(cr3_target_value3, 400);\n\tCHECK_OFFSET(exit_qualification, 408);\n\tCHECK_OFFSET(guest_linear_address, 416);\n\tCHECK_OFFSET(guest_cr0, 424);\n\tCHECK_OFFSET(guest_cr3, 432);\n\tCHECK_OFFSET(guest_cr4, 440);\n\tCHECK_OFFSET(guest_es_base, 448);\n\tCHECK_OFFSET(guest_cs_base, 456);\n\tCHECK_OFFSET(guest_ss_base, 464);\n\tCHECK_OFFSET(guest_ds_base, 472);\n\tCHECK_OFFSET(guest_fs_base, 480);\n\tCHECK_OFFSET(guest_gs_base, 488);\n\tCHECK_OFFSET(guest_ldtr_base, 496);\n\tCHECK_OFFSET(guest_tr_base, 504);\n\tCHECK_OFFSET(guest_gdtr_base, 512);\n\tCHECK_OFFSET(guest_idtr_base, 520);\n\tCHECK_OFFSET(guest_dr7, 528);\n\tCHECK_OFFSET(guest_rsp, 536);\n\tCHECK_OFFSET(guest_rip, 544);\n\tCHECK_OFFSET(guest_rflags, 552);\n\tCHECK_OFFSET(guest_pending_dbg_exceptions, 560);\n\tCHECK_OFFSET(guest_sysenter_esp, 568);\n\tCHECK_OFFSET(guest_sysenter_eip, 576);\n\tCHECK_OFFSET(host_cr0, 584);\n\tCHECK_OFFSET(host_cr3, 592);\n\tCHECK_OFFSET(host_cr4, 600);\n\tCHECK_OFFSET(host_fs_base, 608);\n\tCHECK_OFFSET(host_gs_base, 616);\n\tCHECK_OFFSET(host_tr_base, 624);\n\tCHECK_OFFSET(host_gdtr_base, 632);\n\tCHECK_OFFSET(host_idtr_base, 640);\n\tCHECK_OFFSET(host_ia32_sysenter_esp, 648);\n\tCHECK_OFFSET(host_ia32_sysenter_eip, 656);\n\tCHECK_OFFSET(host_rsp, 664);\n\tCHECK_OFFSET(host_rip, 672);\n\tCHECK_OFFSET(pin_based_vm_exec_control, 744);\n\tCHECK_OFFSET(cpu_based_vm_exec_control, 748);\n\tCHECK_OFFSET(exception_bitmap, 752);\n\tCHECK_OFFSET(page_fault_error_code_mask, 756);\n\tCHECK_OFFSET(page_fault_error_code_match, 760);\n\tCHECK_OFFSET(cr3_target_count, 764);\n\tCHECK_OFFSET(vm_exit_controls, 768);\n\tCHECK_OFFSET(vm_exit_msr_store_count, 772);\n\tCHECK_OFFSET(vm_exit_msr_load_count, 776);\n\tCHECK_OFFSET(vm_entry_controls, 780);\n\tCHECK_OFFSET(vm_entry_msr_load_count, 784);\n\tCHECK_OFFSET(vm_entry_intr_info_field, 788);\n\tCHECK_OFFSET(vm_entry_exception_error_code, 792);\n\tCHECK_OFFSET(vm_entry_instruction_len, 796);\n\tCHECK_OFFSET(tpr_threshold, 800);\n\tCHECK_OFFSET(secondary_vm_exec_control, 804);\n\tCHECK_OFFSET(vm_instruction_error, 808);\n\tCHECK_OFFSET(vm_exit_reason, 812);\n\tCHECK_OFFSET(vm_exit_intr_info, 816);\n\tCHECK_OFFSET(vm_exit_intr_error_code, 820);\n\tCHECK_OFFSET(idt_vectoring_info_field, 824);\n\tCHECK_OFFSET(idt_vectoring_error_code, 828);\n\tCHECK_OFFSET(vm_exit_instruction_len, 832);\n\tCHECK_OFFSET(vmx_instruction_info, 836);\n\tCHECK_OFFSET(guest_es_limit, 840);\n\tCHECK_OFFSET(guest_cs_limit, 844);\n\tCHECK_OFFSET(guest_ss_limit, 848);\n\tCHECK_OFFSET(guest_ds_limit, 852);\n\tCHECK_OFFSET(guest_fs_limit, 856);\n\tCHECK_OFFSET(guest_gs_limit, 860);\n\tCHECK_OFFSET(guest_ldtr_limit, 864);\n\tCHECK_OFFSET(guest_tr_limit, 868);\n\tCHECK_OFFSET(guest_gdtr_limit, 872);\n\tCHECK_OFFSET(guest_idtr_limit, 876);\n\tCHECK_OFFSET(guest_es_ar_bytes, 880);\n\tCHECK_OFFSET(guest_cs_ar_bytes, 884);\n\tCHECK_OFFSET(guest_ss_ar_bytes, 888);\n\tCHECK_OFFSET(guest_ds_ar_bytes, 892);\n\tCHECK_OFFSET(guest_fs_ar_bytes, 896);\n\tCHECK_OFFSET(guest_gs_ar_bytes, 900);\n\tCHECK_OFFSET(guest_ldtr_ar_bytes, 904);\n\tCHECK_OFFSET(guest_tr_ar_bytes, 908);\n\tCHECK_OFFSET(guest_interruptibility_info, 912);\n\tCHECK_OFFSET(guest_activity_state, 916);\n\tCHECK_OFFSET(guest_sysenter_cs, 920);\n\tCHECK_OFFSET(host_ia32_sysenter_cs, 924);\n\tCHECK_OFFSET(vmx_preemption_timer_value, 928);\n\tCHECK_OFFSET(virtual_processor_id, 960);\n\tCHECK_OFFSET(posted_intr_nv, 962);\n\tCHECK_OFFSET(guest_es_selector, 964);\n\tCHECK_OFFSET(guest_cs_selector, 966);\n\tCHECK_OFFSET(guest_ss_selector, 968);\n\tCHECK_OFFSET(guest_ds_selector, 970);\n\tCHECK_OFFSET(guest_fs_selector, 972);\n\tCHECK_OFFSET(guest_gs_selector, 974);\n\tCHECK_OFFSET(guest_ldtr_selector, 976);\n\tCHECK_OFFSET(guest_tr_selector, 978);\n\tCHECK_OFFSET(guest_intr_status, 980);\n\tCHECK_OFFSET(host_es_selector, 982);\n\tCHECK_OFFSET(host_cs_selector, 984);\n\tCHECK_OFFSET(host_ss_selector, 986);\n\tCHECK_OFFSET(host_ds_selector, 988);\n\tCHECK_OFFSET(host_fs_selector, 990);\n\tCHECK_OFFSET(host_gs_selector, 992);\n\tCHECK_OFFSET(host_tr_selector, 994);\n\tCHECK_OFFSET(guest_pml_index, 996);\n}\n\n/*\n * VMCS12_REVISION is an arbitrary id that should be changed if the content or\n * layout of struct vmcs12 is changed. MSR_IA32_VMX_BASIC returns this id, and\n * VMPTRLD verifies that the VMCS region that L1 is loading contains this id.\n *\n * IMPORTANT: Changing this value will break save/restore compatibility with\n * older kvm releases.\n */\n#define VMCS12_REVISION 0x11e57ed0\n\n/*\n * VMCS12_SIZE is the number of bytes L1 should allocate for the VMXON region\n * and any VMCS region. Although only sizeof(struct vmcs12) are used by the\n * current implementation, 4K are reserved to avoid future complications.\n */\n#define VMCS12_SIZE 0x1000\n\n/*\n * VMCS12_MAX_FIELD_INDEX is the highest index value used in any\n * supported VMCS12 field encoding.\n */\n#define VMCS12_MAX_FIELD_INDEX 0x17\n\nstruct nested_vmx_msrs {\n\t/*\n\t * We only store the \"true\" versions of the VMX capability MSRs. We\n\t * generate the \"non-true\" versions by setting the must-be-1 bits\n\t * according to the SDM.\n\t */\n\tu32 procbased_ctls_low;\n\tu32 procbased_ctls_high;\n\tu32 secondary_ctls_low;\n\tu32 secondary_ctls_high;\n\tu32 pinbased_ctls_low;\n\tu32 pinbased_ctls_high;\n\tu32 exit_ctls_low;\n\tu32 exit_ctls_high;\n\tu32 entry_ctls_low;\n\tu32 entry_ctls_high;\n\tu32 misc_low;\n\tu32 misc_high;\n\tu32 ept_caps;\n\tu32 vpid_caps;\n\tu64 basic;\n\tu64 cr0_fixed0;\n\tu64 cr0_fixed1;\n\tu64 cr4_fixed0;\n\tu64 cr4_fixed1;\n\tu64 vmcs_enum;\n\tu64 vmfunc_controls;\n};\n\n/*\n * The nested_vmx structure is part of vcpu_vmx, and holds information we need\n * for correct emulation of VMX (i.e., nested VMX) on this vcpu.\n */\nstruct nested_vmx {\n\t/* Has the level1 guest done vmxon? */\n\tbool vmxon;\n\tgpa_t vmxon_ptr;\n\tbool pml_full;\n\n\t/* The guest-physical address of the current VMCS L1 keeps for L2 */\n\tgpa_t current_vmptr;\n\t/*\n\t * Cache of the guest's VMCS, existing outside of guest memory.\n\t * Loaded from guest memory during VMPTRLD. Flushed to guest\n\t * memory during VMCLEAR and VMPTRLD.\n\t */\n\tstruct vmcs12 *cached_vmcs12;\n\t/*\n\t * Indicates if the shadow vmcs must be updated with the\n\t * data hold by vmcs12\n\t */\n\tbool sync_shadow_vmcs;\n\tbool dirty_vmcs12;\n\n\tbool change_vmcs01_virtual_apic_mode;\n\n\t/* L2 must run next, and mustn't decide to exit to L1. */\n\tbool nested_run_pending;\n\n\tstruct loaded_vmcs vmcs02;\n\n\t/*\n\t * Guest pages referred to in the vmcs02 with host-physical\n\t * pointers, so we must keep them pinned while L2 runs.\n\t */\n\tstruct page *apic_access_page;\n\tstruct page *virtual_apic_page;\n\tstruct page *pi_desc_page;\n\tstruct pi_desc *pi_desc;\n\tbool pi_pending;\n\tu16 posted_intr_nv;\n\n\tstruct hrtimer preemption_timer;\n\tbool preemption_timer_expired;\n\n\t/* to migrate it to L2 if VM_ENTRY_LOAD_DEBUG_CONTROLS is off */\n\tu64 vmcs01_debugctl;\n\n\tu16 vpid02;\n\tu16 last_vpid;\n\n\tstruct nested_vmx_msrs msrs;\n\n\t/* SMM related state */\n\tstruct {\n\t\t/* in VMX operation on SMM entry? */\n\t\tbool vmxon;\n\t\t/* in guest mode on SMM entry? */\n\t\tbool guest_mode;\n\t} smm;\n};\n\n#define POSTED_INTR_ON  0\n#define POSTED_INTR_SN  1\n\n/* Posted-Interrupt Descriptor */\nstruct pi_desc {\n\tu32 pir[8];     /* Posted interrupt requested */\n\tunion {\n\t\tstruct {\n\t\t\t\t/* bit 256 - Outstanding Notification */\n\t\t\tu16\ton\t: 1,\n\t\t\t\t/* bit 257 - Suppress Notification */\n\t\t\t\tsn\t: 1,\n\t\t\t\t/* bit 271:258 - Reserved */\n\t\t\t\trsvd_1\t: 14;\n\t\t\t\t/* bit 279:272 - Notification Vector */\n\t\t\tu8\tnv;\n\t\t\t\t/* bit 287:280 - Reserved */\n\t\t\tu8\trsvd_2;\n\t\t\t\t/* bit 319:288 - Notification Destination */\n\t\t\tu32\tndst;\n\t\t};\n\t\tu64 control;\n\t};\n\tu32 rsvd[6];\n} __aligned(64);\n\nstatic bool pi_test_and_set_on(struct pi_desc *pi_desc)\n{\n\treturn test_and_set_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic bool pi_test_and_clear_on(struct pi_desc *pi_desc)\n{\n\treturn test_and_clear_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)\n{\n\treturn test_and_set_bit(vector, (unsigned long *)pi_desc->pir);\n}\n\nstatic inline void pi_clear_sn(struct pi_desc *pi_desc)\n{\n\treturn clear_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline void pi_set_sn(struct pi_desc *pi_desc)\n{\n\treturn set_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline void pi_clear_on(struct pi_desc *pi_desc)\n{\n\tclear_bit(POSTED_INTR_ON,\n  \t\t  (unsigned long *)&pi_desc->control);\n}\n\nstatic inline int pi_test_on(struct pi_desc *pi_desc)\n{\n\treturn test_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline int pi_test_sn(struct pi_desc *pi_desc)\n{\n\treturn test_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstruct vcpu_vmx {\n\tstruct kvm_vcpu       vcpu;\n\tunsigned long         host_rsp;\n\tu8                    fail;\n\tu8\t\t      msr_bitmap_mode;\n\tu32                   exit_intr_info;\n\tu32                   idt_vectoring_info;\n\tulong                 rflags;\n\tstruct shared_msr_entry *guest_msrs;\n\tint                   nmsrs;\n\tint                   save_nmsrs;\n\tunsigned long\t      host_idt_base;\n#ifdef CONFIG_X86_64\n\tu64 \t\t      msr_host_kernel_gs_base;\n\tu64 \t\t      msr_guest_kernel_gs_base;\n#endif\n\n\tu64 \t\t      arch_capabilities;\n\tu64 \t\t      spec_ctrl;\n\n\tu32 vm_entry_controls_shadow;\n\tu32 vm_exit_controls_shadow;\n\tu32 secondary_exec_control;\n\n\t/*\n\t * loaded_vmcs points to the VMCS currently used in this vcpu. For a\n\t * non-nested (L1) guest, it always points to vmcs01. For a nested\n\t * guest (L2), it points to a different VMCS.\n\t */\n\tstruct loaded_vmcs    vmcs01;\n\tstruct loaded_vmcs   *loaded_vmcs;\n\tbool                  __launched; /* temporary, used in vmx_vcpu_run */\n\tstruct msr_autoload {\n\t\tunsigned nr;\n\t\tstruct vmx_msr_entry guest[NR_AUTOLOAD_MSRS];\n\t\tstruct vmx_msr_entry host[NR_AUTOLOAD_MSRS];\n\t} msr_autoload;\n\tstruct {\n\t\tint           loaded;\n\t\tu16           fs_sel, gs_sel, ldt_sel;\n#ifdef CONFIG_X86_64\n\t\tu16           ds_sel, es_sel;\n#endif\n\t\tint           gs_ldt_reload_needed;\n\t\tint           fs_reload_needed;\n\t\tu64           msr_host_bndcfgs;\n\t} host_state;\n\tstruct {\n\t\tint vm86_active;\n\t\tulong save_rflags;\n\t\tstruct kvm_segment segs[8];\n\t} rmode;\n\tstruct {\n\t\tu32 bitmask; /* 4 bits per segment (1 bit per field) */\n\t\tstruct kvm_save_segment {\n\t\t\tu16 selector;\n\t\t\tunsigned long base;\n\t\t\tu32 limit;\n\t\t\tu32 ar;\n\t\t} seg[8];\n\t} segment_cache;\n\tint vpid;\n\tbool emulation_required;\n\n\tu32 exit_reason;\n\n\t/* Posted interrupt descriptor */\n\tstruct pi_desc pi_desc;\n\n\t/* Support for a guest hypervisor (nested VMX) */\n\tstruct nested_vmx nested;\n\n\t/* Dynamic PLE window. */\n\tint ple_window;\n\tbool ple_window_dirty;\n\n\t/* Support for PML */\n#define PML_ENTITY_NUM\t\t512\n\tstruct page *pml_pg;\n\n\t/* apic deadline value in host tsc */\n\tu64 hv_deadline_tsc;\n\n\tu64 current_tsc_ratio;\n\n\tu32 host_pkru;\n\n\tunsigned long host_debugctlmsr;\n\n\t/*\n\t * Only bits masked by msr_ia32_feature_control_valid_bits can be set in\n\t * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included\n\t * in msr_ia32_feature_control_valid_bits.\n\t */\n\tu64 msr_ia32_feature_control;\n\tu64 msr_ia32_feature_control_valid_bits;\n};\n\nenum segment_cache_field {\n\tSEG_FIELD_SEL = 0,\n\tSEG_FIELD_BASE = 1,\n\tSEG_FIELD_LIMIT = 2,\n\tSEG_FIELD_AR = 3,\n\n\tSEG_FIELD_NR = 4\n};\n\nstatic inline struct kvm_vmx *to_kvm_vmx(struct kvm *kvm)\n{\n\treturn container_of(kvm, struct kvm_vmx, kvm);\n}\n\nstatic inline struct vcpu_vmx *to_vmx(struct kvm_vcpu *vcpu)\n{\n\treturn container_of(vcpu, struct vcpu_vmx, vcpu);\n}\n\nstatic struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)\n{\n\treturn &(to_vmx(vcpu)->pi_desc);\n}\n\n#define ROL16(val, n) ((u16)(((u16)(val) << (n)) | ((u16)(val) >> (16 - (n)))))\n#define VMCS12_OFFSET(x) offsetof(struct vmcs12, x)\n#define FIELD(number, name)\t[ROL16(number, 6)] = VMCS12_OFFSET(name)\n#define FIELD64(number, name)\t\t\t\t\t\t\\\n\tFIELD(number, name),\t\t\t\t\t\t\\\n\t[ROL16(number##_HIGH, 6)] = VMCS12_OFFSET(name) + sizeof(u32)\n\n\nstatic u16 shadow_read_only_fields[] = {\n#define SHADOW_FIELD_RO(x) x,\n#include \"vmx_shadow_fields.h\"\n};\nstatic int max_shadow_read_only_fields =\n\tARRAY_SIZE(shadow_read_only_fields);\n\nstatic u16 shadow_read_write_fields[] = {\n#define SHADOW_FIELD_RW(x) x,\n#include \"vmx_shadow_fields.h\"\n};\nstatic int max_shadow_read_write_fields =\n\tARRAY_SIZE(shadow_read_write_fields);\n\nstatic const unsigned short vmcs_field_to_offset_table[] = {\n\tFIELD(VIRTUAL_PROCESSOR_ID, virtual_processor_id),\n\tFIELD(POSTED_INTR_NV, posted_intr_nv),\n\tFIELD(GUEST_ES_SELECTOR, guest_es_selector),\n\tFIELD(GUEST_CS_SELECTOR, guest_cs_selector),\n\tFIELD(GUEST_SS_SELECTOR, guest_ss_selector),\n\tFIELD(GUEST_DS_SELECTOR, guest_ds_selector),\n\tFIELD(GUEST_FS_SELECTOR, guest_fs_selector),\n\tFIELD(GUEST_GS_SELECTOR, guest_gs_selector),\n\tFIELD(GUEST_LDTR_SELECTOR, guest_ldtr_selector),\n\tFIELD(GUEST_TR_SELECTOR, guest_tr_selector),\n\tFIELD(GUEST_INTR_STATUS, guest_intr_status),\n\tFIELD(GUEST_PML_INDEX, guest_pml_index),\n\tFIELD(HOST_ES_SELECTOR, host_es_selector),\n\tFIELD(HOST_CS_SELECTOR, host_cs_selector),\n\tFIELD(HOST_SS_SELECTOR, host_ss_selector),\n\tFIELD(HOST_DS_SELECTOR, host_ds_selector),\n\tFIELD(HOST_FS_SELECTOR, host_fs_selector),\n\tFIELD(HOST_GS_SELECTOR, host_gs_selector),\n\tFIELD(HOST_TR_SELECTOR, host_tr_selector),\n\tFIELD64(IO_BITMAP_A, io_bitmap_a),\n\tFIELD64(IO_BITMAP_B, io_bitmap_b),\n\tFIELD64(MSR_BITMAP, msr_bitmap),\n\tFIELD64(VM_EXIT_MSR_STORE_ADDR, vm_exit_msr_store_addr),\n\tFIELD64(VM_EXIT_MSR_LOAD_ADDR, vm_exit_msr_load_addr),\n\tFIELD64(VM_ENTRY_MSR_LOAD_ADDR, vm_entry_msr_load_addr),\n\tFIELD64(PML_ADDRESS, pml_address),\n\tFIELD64(TSC_OFFSET, tsc_offset),\n\tFIELD64(VIRTUAL_APIC_PAGE_ADDR, virtual_apic_page_addr),\n\tFIELD64(APIC_ACCESS_ADDR, apic_access_addr),\n\tFIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),\n\tFIELD64(VM_FUNCTION_CONTROL, vm_function_control),\n\tFIELD64(EPT_POINTER, ept_pointer),\n\tFIELD64(EOI_EXIT_BITMAP0, eoi_exit_bitmap0),\n\tFIELD64(EOI_EXIT_BITMAP1, eoi_exit_bitmap1),\n\tFIELD64(EOI_EXIT_BITMAP2, eoi_exit_bitmap2),\n\tFIELD64(EOI_EXIT_BITMAP3, eoi_exit_bitmap3),\n\tFIELD64(EPTP_LIST_ADDRESS, eptp_list_address),\n\tFIELD64(VMREAD_BITMAP, vmread_bitmap),\n\tFIELD64(VMWRITE_BITMAP, vmwrite_bitmap),\n\tFIELD64(XSS_EXIT_BITMAP, xss_exit_bitmap),\n\tFIELD64(GUEST_PHYSICAL_ADDRESS, guest_physical_address),\n\tFIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),\n\tFIELD64(GUEST_IA32_DEBUGCTL, guest_ia32_debugctl),\n\tFIELD64(GUEST_IA32_PAT, guest_ia32_pat),\n\tFIELD64(GUEST_IA32_EFER, guest_ia32_efer),\n\tFIELD64(GUEST_IA32_PERF_GLOBAL_CTRL, guest_ia32_perf_global_ctrl),\n\tFIELD64(GUEST_PDPTR0, guest_pdptr0),\n\tFIELD64(GUEST_PDPTR1, guest_pdptr1),\n\tFIELD64(GUEST_PDPTR2, guest_pdptr2),\n\tFIELD64(GUEST_PDPTR3, guest_pdptr3),\n\tFIELD64(GUEST_BNDCFGS, guest_bndcfgs),\n\tFIELD64(HOST_IA32_PAT, host_ia32_pat),\n\tFIELD64(HOST_IA32_EFER, host_ia32_efer),\n\tFIELD64(HOST_IA32_PERF_GLOBAL_CTRL, host_ia32_perf_global_ctrl),\n\tFIELD(PIN_BASED_VM_EXEC_CONTROL, pin_based_vm_exec_control),\n\tFIELD(CPU_BASED_VM_EXEC_CONTROL, cpu_based_vm_exec_control),\n\tFIELD(EXCEPTION_BITMAP, exception_bitmap),\n\tFIELD(PAGE_FAULT_ERROR_CODE_MASK, page_fault_error_code_mask),\n\tFIELD(PAGE_FAULT_ERROR_CODE_MATCH, page_fault_error_code_match),\n\tFIELD(CR3_TARGET_COUNT, cr3_target_count),\n\tFIELD(VM_EXIT_CONTROLS, vm_exit_controls),\n\tFIELD(VM_EXIT_MSR_STORE_COUNT, vm_exit_msr_store_count),\n\tFIELD(VM_EXIT_MSR_LOAD_COUNT, vm_exit_msr_load_count),\n\tFIELD(VM_ENTRY_CONTROLS, vm_entry_controls),\n\tFIELD(VM_ENTRY_MSR_LOAD_COUNT, vm_entry_msr_load_count),\n\tFIELD(VM_ENTRY_INTR_INFO_FIELD, vm_entry_intr_info_field),\n\tFIELD(VM_ENTRY_EXCEPTION_ERROR_CODE, vm_entry_exception_error_code),\n\tFIELD(VM_ENTRY_INSTRUCTION_LEN, vm_entry_instruction_len),\n\tFIELD(TPR_THRESHOLD, tpr_threshold),\n\tFIELD(SECONDARY_VM_EXEC_CONTROL, secondary_vm_exec_control),\n\tFIELD(VM_INSTRUCTION_ERROR, vm_instruction_error),\n\tFIELD(VM_EXIT_REASON, vm_exit_reason),\n\tFIELD(VM_EXIT_INTR_INFO, vm_exit_intr_info),\n\tFIELD(VM_EXIT_INTR_ERROR_CODE, vm_exit_intr_error_code),\n\tFIELD(IDT_VECTORING_INFO_FIELD, idt_vectoring_info_field),\n\tFIELD(IDT_VECTORING_ERROR_CODE, idt_vectoring_error_code),\n\tFIELD(VM_EXIT_INSTRUCTION_LEN, vm_exit_instruction_len),\n\tFIELD(VMX_INSTRUCTION_INFO, vmx_instruction_info),\n\tFIELD(GUEST_ES_LIMIT, guest_es_limit),\n\tFIELD(GUEST_CS_LIMIT, guest_cs_limit),\n\tFIELD(GUEST_SS_LIMIT, guest_ss_limit),\n\tFIELD(GUEST_DS_LIMIT, guest_ds_limit),\n\tFIELD(GUEST_FS_LIMIT, guest_fs_limit),\n\tFIELD(GUEST_GS_LIMIT, guest_gs_limit),\n\tFIELD(GUEST_LDTR_LIMIT, guest_ldtr_limit),\n\tFIELD(GUEST_TR_LIMIT, guest_tr_limit),\n\tFIELD(GUEST_GDTR_LIMIT, guest_gdtr_limit),\n\tFIELD(GUEST_IDTR_LIMIT, guest_idtr_limit),\n\tFIELD(GUEST_ES_AR_BYTES, guest_es_ar_bytes),\n\tFIELD(GUEST_CS_AR_BYTES, guest_cs_ar_bytes),\n\tFIELD(GUEST_SS_AR_BYTES, guest_ss_ar_bytes),\n\tFIELD(GUEST_DS_AR_BYTES, guest_ds_ar_bytes),\n\tFIELD(GUEST_FS_AR_BYTES, guest_fs_ar_bytes),\n\tFIELD(GUEST_GS_AR_BYTES, guest_gs_ar_bytes),\n\tFIELD(GUEST_LDTR_AR_BYTES, guest_ldtr_ar_bytes),\n\tFIELD(GUEST_TR_AR_BYTES, guest_tr_ar_bytes),\n\tFIELD(GUEST_INTERRUPTIBILITY_INFO, guest_interruptibility_info),\n\tFIELD(GUEST_ACTIVITY_STATE, guest_activity_state),\n\tFIELD(GUEST_SYSENTER_CS, guest_sysenter_cs),\n\tFIELD(HOST_IA32_SYSENTER_CS, host_ia32_sysenter_cs),\n\tFIELD(VMX_PREEMPTION_TIMER_VALUE, vmx_preemption_timer_value),\n\tFIELD(CR0_GUEST_HOST_MASK, cr0_guest_host_mask),\n\tFIELD(CR4_GUEST_HOST_MASK, cr4_guest_host_mask),\n\tFIELD(CR0_READ_SHADOW, cr0_read_shadow),\n\tFIELD(CR4_READ_SHADOW, cr4_read_shadow),\n\tFIELD(CR3_TARGET_VALUE0, cr3_target_value0),\n\tFIELD(CR3_TARGET_VALUE1, cr3_target_value1),\n\tFIELD(CR3_TARGET_VALUE2, cr3_target_value2),\n\tFIELD(CR3_TARGET_VALUE3, cr3_target_value3),\n\tFIELD(EXIT_QUALIFICATION, exit_qualification),\n\tFIELD(GUEST_LINEAR_ADDRESS, guest_linear_address),\n\tFIELD(GUEST_CR0, guest_cr0),\n\tFIELD(GUEST_CR3, guest_cr3),\n\tFIELD(GUEST_CR4, guest_cr4),\n\tFIELD(GUEST_ES_BASE, guest_es_base),\n\tFIELD(GUEST_CS_BASE, guest_cs_base),\n\tFIELD(GUEST_SS_BASE, guest_ss_base),\n\tFIELD(GUEST_DS_BASE, guest_ds_base),\n\tFIELD(GUEST_FS_BASE, guest_fs_base),\n\tFIELD(GUEST_GS_BASE, guest_gs_base),\n\tFIELD(GUEST_LDTR_BASE, guest_ldtr_base),\n\tFIELD(GUEST_TR_BASE, guest_tr_base),\n\tFIELD(GUEST_GDTR_BASE, guest_gdtr_base),\n\tFIELD(GUEST_IDTR_BASE, guest_idtr_base),\n\tFIELD(GUEST_DR7, guest_dr7),\n\tFIELD(GUEST_RSP, guest_rsp),\n\tFIELD(GUEST_RIP, guest_rip),\n\tFIELD(GUEST_RFLAGS, guest_rflags),\n\tFIELD(GUEST_PENDING_DBG_EXCEPTIONS, guest_pending_dbg_exceptions),\n\tFIELD(GUEST_SYSENTER_ESP, guest_sysenter_esp),\n\tFIELD(GUEST_SYSENTER_EIP, guest_sysenter_eip),\n\tFIELD(HOST_CR0, host_cr0),\n\tFIELD(HOST_CR3, host_cr3),\n\tFIELD(HOST_CR4, host_cr4),\n\tFIELD(HOST_FS_BASE, host_fs_base),\n\tFIELD(HOST_GS_BASE, host_gs_base),\n\tFIELD(HOST_TR_BASE, host_tr_base),\n\tFIELD(HOST_GDTR_BASE, host_gdtr_base),\n\tFIELD(HOST_IDTR_BASE, host_idtr_base),\n\tFIELD(HOST_IA32_SYSENTER_ESP, host_ia32_sysenter_esp),\n\tFIELD(HOST_IA32_SYSENTER_EIP, host_ia32_sysenter_eip),\n\tFIELD(HOST_RSP, host_rsp),\n\tFIELD(HOST_RIP, host_rip),\n};\n\nstatic inline short vmcs_field_to_offset(unsigned long field)\n{\n\tconst size_t size = ARRAY_SIZE(vmcs_field_to_offset_table);\n\tunsigned short offset;\n\tunsigned index;\n\n\tif (field >> 15)\n\t\treturn -ENOENT;\n\n\tindex = ROL16(field, 6);\n\tif (index >= size)\n\t\treturn -ENOENT;\n\n\tindex = array_index_nospec(index, size);\n\toffset = vmcs_field_to_offset_table[index];\n\tif (offset == 0)\n\t\treturn -ENOENT;\n\treturn offset;\n}\n\nstatic inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)\n{\n\treturn to_vmx(vcpu)->nested.cached_vmcs12;\n}\n\nstatic bool nested_ept_ad_enabled(struct kvm_vcpu *vcpu);\nstatic unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu);\nstatic u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);\nstatic bool vmx_xsaves_supported(void);\nstatic void vmx_set_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\nstatic void vmx_get_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\nstatic bool guest_state_valid(struct kvm_vcpu *vcpu);\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var);\nstatic void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx);\nstatic bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);\nstatic void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);\nstatic bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,\n\t\t\t\t\t    u16 error_code);\nstatic void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu);\nstatic void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t  u32 msr, int type);\n\nstatic DEFINE_PER_CPU(struct vmcs *, vmxarea);\nstatic DEFINE_PER_CPU(struct vmcs *, current_vmcs);\n/*\n * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed\n * when a CPU is brought down, and we need to VMCLEAR all VMCSs loaded on it.\n */\nstatic DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);\n\n/*\n * We maintian a per-CPU linked-list of vCPU, so in wakeup_handler() we\n * can find which vCPU should be waken up.\n */\nstatic DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);\nstatic DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);\n\nenum {\n\tVMX_VMREAD_BITMAP,\n\tVMX_VMWRITE_BITMAP,\n\tVMX_BITMAP_NR\n};\n\nstatic unsigned long *vmx_bitmap[VMX_BITMAP_NR];\n\n#define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])\n#define vmx_vmwrite_bitmap                   (vmx_bitmap[VMX_VMWRITE_BITMAP])\n\nstatic bool cpu_has_load_ia32_efer;\nstatic bool cpu_has_load_perf_global_ctrl;\n\nstatic DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);\nstatic DEFINE_SPINLOCK(vmx_vpid_lock);\n\nstatic struct vmcs_config {\n\tint size;\n\tint order;\n\tu32 basic_cap;\n\tu32 revision_id;\n\tu32 pin_based_exec_ctrl;\n\tu32 cpu_based_exec_ctrl;\n\tu32 cpu_based_2nd_exec_ctrl;\n\tu32 vmexit_ctrl;\n\tu32 vmentry_ctrl;\n\tstruct nested_vmx_msrs nested;\n} vmcs_config;\n\nstatic struct vmx_capability {\n\tu32 ept;\n\tu32 vpid;\n} vmx_capability;\n\n#define VMX_SEGMENT_FIELD(seg)\t\t\t\t\t\\\n\t[VCPU_SREG_##seg] = {                                   \\\n\t\t.selector = GUEST_##seg##_SELECTOR,\t\t\\\n\t\t.base = GUEST_##seg##_BASE,\t\t   \t\\\n\t\t.limit = GUEST_##seg##_LIMIT,\t\t   \t\\\n\t\t.ar_bytes = GUEST_##seg##_AR_BYTES,\t   \t\\\n\t}\n\nstatic const struct kvm_vmx_segment_field {\n\tunsigned selector;\n\tunsigned base;\n\tunsigned limit;\n\tunsigned ar_bytes;\n} kvm_vmx_segment_fields[] = {\n\tVMX_SEGMENT_FIELD(CS),\n\tVMX_SEGMENT_FIELD(DS),\n\tVMX_SEGMENT_FIELD(ES),\n\tVMX_SEGMENT_FIELD(FS),\n\tVMX_SEGMENT_FIELD(GS),\n\tVMX_SEGMENT_FIELD(SS),\n\tVMX_SEGMENT_FIELD(TR),\n\tVMX_SEGMENT_FIELD(LDTR),\n};\n\nstatic u64 host_efer;\n\nstatic void ept_save_pdptrs(struct kvm_vcpu *vcpu);\n\n/*\n * Keep MSR_STAR at the end, as setup_msrs() will try to optimize it\n * away by decrementing the array size.\n */\nstatic const u32 vmx_msr_index[] = {\n#ifdef CONFIG_X86_64\n\tMSR_SYSCALL_MASK, MSR_LSTAR, MSR_CSTAR,\n#endif\n\tMSR_EFER, MSR_TSC_AUX, MSR_STAR,\n};\n\nDEFINE_STATIC_KEY_FALSE(enable_evmcs);\n\n#define current_evmcs ((struct hv_enlightened_vmcs *)this_cpu_read(current_vmcs))\n\n#define KVM_EVMCS_VERSION 1\n\n#if IS_ENABLED(CONFIG_HYPERV)\nstatic bool __read_mostly enlightened_vmcs = true;\nmodule_param(enlightened_vmcs, bool, 0444);\n\nstatic inline void evmcs_write64(unsigned long field, u64 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u64 *)((char *)current_evmcs + offset) = value;\n\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline void evmcs_write32(unsigned long field, u32 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u32 *)((char *)current_evmcs + offset) = value;\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline void evmcs_write16(unsigned long field, u16 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u16 *)((char *)current_evmcs + offset) = value;\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline u64 evmcs_read64(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u64 *)((char *)current_evmcs + offset);\n}\n\nstatic inline u32 evmcs_read32(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u32 *)((char *)current_evmcs + offset);\n}\n\nstatic inline u16 evmcs_read16(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u16 *)((char *)current_evmcs + offset);\n}\n\nstatic inline void evmcs_touch_msr_bitmap(void)\n{\n\tif (unlikely(!current_evmcs))\n\t\treturn;\n\n\tif (current_evmcs->hv_enlightenments_control.msr_bitmap)\n\t\tcurrent_evmcs->hv_clean_fields &=\n\t\t\t~HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP;\n}\n\nstatic void evmcs_load(u64 phys_addr)\n{\n\tstruct hv_vp_assist_page *vp_ap =\n\t\thv_get_vp_assist_page(smp_processor_id());\n\n\tvp_ap->current_nested_vmcs = phys_addr;\n\tvp_ap->enlighten_vmentry = 1;\n}\n\nstatic void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)\n{\n\t/*\n\t * Enlightened VMCSv1 doesn't support these:\n\t *\n\t *\tPOSTED_INTR_NV                  = 0x00000002,\n\t *\tGUEST_INTR_STATUS               = 0x00000810,\n\t *\tAPIC_ACCESS_ADDR\t\t= 0x00002014,\n\t *\tPOSTED_INTR_DESC_ADDR           = 0x00002016,\n\t *\tEOI_EXIT_BITMAP0                = 0x0000201c,\n\t *\tEOI_EXIT_BITMAP1                = 0x0000201e,\n\t *\tEOI_EXIT_BITMAP2                = 0x00002020,\n\t *\tEOI_EXIT_BITMAP3                = 0x00002022,\n\t */\n\tvmcs_conf->pin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_APIC_REGISTER_VIRT;\n\n\t/*\n\t *\tGUEST_PML_INDEX\t\t\t= 0x00000812,\n\t *\tPML_ADDRESS\t\t\t= 0x0000200e,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\t/*\tVM_FUNCTION_CONTROL             = 0x00002018, */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_ENABLE_VMFUNC;\n\n\t/*\n\t *\tEPTP_LIST_ADDRESS               = 0x00002024,\n\t *\tVMREAD_BITMAP                   = 0x00002026,\n\t *\tVMWRITE_BITMAP                  = 0x00002028,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\t/*\n\t *\tTSC_MULTIPLIER                  = 0x00002032,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_TSC_SCALING;\n\n\t/*\n\t *\tPLE_GAP                         = 0x00004020,\n\t *\tPLE_WINDOW                      = 0x00004022,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\n\t/*\n\t *\tVMX_PREEMPTION_TIMER_VALUE      = 0x0000482E,\n\t */\n\tvmcs_conf->pin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/*\n\t *      GUEST_IA32_PERF_GLOBAL_CTRL     = 0x00002808,\n\t *      HOST_IA32_PERF_GLOBAL_CTRL      = 0x00002c04,\n\t */\n\tvmcs_conf->vmexit_ctrl &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;\n\tvmcs_conf->vmentry_ctrl &= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;\n\n\t/*\n\t * Currently unsupported in KVM:\n\t *\tGUEST_IA32_RTIT_CTL\t\t= 0x00002814,\n\t */\n}\n#else /* !IS_ENABLED(CONFIG_HYPERV) */\nstatic inline void evmcs_write64(unsigned long field, u64 value) {}\nstatic inline void evmcs_write32(unsigned long field, u32 value) {}\nstatic inline void evmcs_write16(unsigned long field, u16 value) {}\nstatic inline u64 evmcs_read64(unsigned long field) { return 0; }\nstatic inline u32 evmcs_read32(unsigned long field) { return 0; }\nstatic inline u16 evmcs_read16(unsigned long field) { return 0; }\nstatic inline void evmcs_load(u64 phys_addr) {}\nstatic inline void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf) {}\nstatic inline void evmcs_touch_msr_bitmap(void) {}\n#endif /* IS_ENABLED(CONFIG_HYPERV) */\n\nstatic inline bool is_exception_n(u32 intr_info, u8 vector)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |\n\t\t\t     INTR_INFO_VALID_MASK)) ==\n\t\t(INTR_TYPE_HARD_EXCEPTION | vector | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool is_debug(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, DB_VECTOR);\n}\n\nstatic inline bool is_breakpoint(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, BP_VECTOR);\n}\n\nstatic inline bool is_page_fault(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, PF_VECTOR);\n}\n\nstatic inline bool is_no_device(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, NM_VECTOR);\n}\n\nstatic inline bool is_invalid_opcode(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, UD_VECTOR);\n}\n\nstatic inline bool is_gp_fault(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, GP_VECTOR);\n}\n\nstatic inline bool is_external_interrupt(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_EXT_INTR | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool is_machine_check(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |\n\t\t\t     INTR_INFO_VALID_MASK)) ==\n\t\t(INTR_TYPE_HARD_EXCEPTION | MC_VECTOR | INTR_INFO_VALID_MASK);\n}\n\n/* Undocumented: icebp/int1 */\nstatic inline bool is_icebp(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_PRIV_SW_EXCEPTION | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool cpu_has_vmx_msr_bitmap(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS;\n}\n\nstatic inline bool cpu_has_vmx_tpr_shadow(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW;\n}\n\nstatic inline bool cpu_need_tpr_shadow(struct kvm_vcpu *vcpu)\n{\n\treturn cpu_has_vmx_tpr_shadow() && lapic_in_kernel(vcpu);\n}\n\nstatic inline bool cpu_has_secondary_exec_ctrls(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl &\n\t\tCPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n}\n\nstatic inline bool cpu_has_vmx_virtualize_apic_accesses(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n}\n\nstatic inline bool cpu_has_vmx_virtualize_x2apic_mode(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n}\n\nstatic inline bool cpu_has_vmx_apic_register_virt(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT;\n}\n\nstatic inline bool cpu_has_vmx_virtual_intr_delivery(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;\n}\n\n/*\n * Comment's format: document - errata name - stepping - processor name.\n * Refer from\n * https://www.virtualbox.org/svn/vbox/trunk/src/VBox/VMM/VMMR0/HMR0.cpp\n */\nstatic u32 vmx_preemption_cpu_tfms[] = {\n/* 323344.pdf - BA86   - D0 - Xeon 7500 Series */\n0x000206E6,\n/* 323056.pdf - AAX65  - C2 - Xeon L3406 */\n/* 322814.pdf - AAT59  - C2 - i7-600, i5-500, i5-400 and i3-300 Mobile */\n/* 322911.pdf - AAU65  - C2 - i5-600, i3-500 Desktop and Pentium G6950 */\n0x00020652,\n/* 322911.pdf - AAU65  - K0 - i5-600, i3-500 Desktop and Pentium G6950 */\n0x00020655,\n/* 322373.pdf - AAO95  - B1 - Xeon 3400 Series */\n/* 322166.pdf - AAN92  - B1 - i7-800 and i5-700 Desktop */\n/*\n * 320767.pdf - AAP86  - B1 -\n * i7-900 Mobile Extreme, i7-800 and i7-700 Mobile\n */\n0x000106E5,\n/* 321333.pdf - AAM126 - C0 - Xeon 3500 */\n0x000106A0,\n/* 321333.pdf - AAM126 - C1 - Xeon 3500 */\n0x000106A1,\n/* 320836.pdf - AAJ124 - C0 - i7-900 Desktop Extreme and i7-900 Desktop */\n0x000106A4,\n /* 321333.pdf - AAM126 - D0 - Xeon 3500 */\n /* 321324.pdf - AAK139 - D0 - Xeon 5500 */\n /* 320836.pdf - AAJ124 - D0 - i7-900 Extreme and i7-900 Desktop */\n0x000106A5,\n};\n\nstatic inline bool cpu_has_broken_vmx_preemption_timer(void)\n{\n\tu32 eax = cpuid_eax(0x00000001), i;\n\n\t/* Clear the reserved bits */\n\teax &= ~(0x3U << 14 | 0xfU << 28);\n\tfor (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)\n\t\tif (eax == vmx_preemption_cpu_tfms[i])\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool cpu_has_vmx_preemption_timer(void)\n{\n\treturn vmcs_config.pin_based_exec_ctrl &\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n}\n\nstatic inline bool cpu_has_vmx_posted_intr(void)\n{\n\treturn IS_ENABLED(CONFIG_X86_LOCAL_APIC) &&\n\t\tvmcs_config.pin_based_exec_ctrl & PIN_BASED_POSTED_INTR;\n}\n\nstatic inline bool cpu_has_vmx_apicv(void)\n{\n\treturn cpu_has_vmx_apic_register_virt() &&\n\t\tcpu_has_vmx_virtual_intr_delivery() &&\n\t\tcpu_has_vmx_posted_intr();\n}\n\nstatic inline bool cpu_has_vmx_flexpriority(void)\n{\n\treturn cpu_has_vmx_tpr_shadow() &&\n\t\tcpu_has_vmx_virtualize_apic_accesses();\n}\n\nstatic inline bool cpu_has_vmx_ept_execute_only(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXECUTE_ONLY_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_2m_page(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_2MB_PAGE_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_1g_page(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_1GB_PAGE_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_4levels(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_4_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_mt_wb(void)\n{\n\treturn vmx_capability.ept & VMX_EPTP_WB_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_5levels(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_5_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_ad_bits(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_AD_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invept_context(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invept_global(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_GLOBAL_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_individual_addr(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_single(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_SINGLE_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_global(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_INVVPID_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_EPT;\n}\n\nstatic inline bool cpu_has_vmx_unrestricted_guest(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST;\n}\n\nstatic inline bool cpu_has_vmx_ple(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING;\n}\n\nstatic inline bool cpu_has_vmx_basic_inout(void)\n{\n\treturn\t(((u64)vmcs_config.basic_cap << 32) & VMX_BASIC_INOUT);\n}\n\nstatic inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)\n{\n\treturn flexpriority_enabled && lapic_in_kernel(vcpu);\n}\n\nstatic inline bool cpu_has_vmx_vpid(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VPID;\n}\n\nstatic inline bool cpu_has_vmx_rdtscp(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDTSCP;\n}\n\nstatic inline bool cpu_has_vmx_invpcid(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_INVPCID;\n}\n\nstatic inline bool cpu_has_virtual_nmis(void)\n{\n\treturn vmcs_config.pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS;\n}\n\nstatic inline bool cpu_has_vmx_wbinvd_exit(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_WBINVD_EXITING;\n}\n\nstatic inline bool cpu_has_vmx_shadow_vmcs(void)\n{\n\tu64 vmx_msr;\n\trdmsrl(MSR_IA32_VMX_MISC, vmx_msr);\n\t/* check if the cpu supports writing r/o exit information fields */\n\tif (!(vmx_msr & MSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS))\n\t\treturn false;\n\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_SHADOW_VMCS;\n}\n\nstatic inline bool cpu_has_vmx_pml(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_ENABLE_PML;\n}\n\nstatic inline bool cpu_has_vmx_tsc_scaling(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_TSC_SCALING;\n}\n\nstatic inline bool cpu_has_vmx_vmfunc(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n}\n\nstatic bool vmx_umip_emulated(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_DESC;\n}\n\nstatic inline bool report_flexpriority(void)\n{\n\treturn flexpriority_enabled;\n}\n\nstatic inline unsigned nested_cpu_vmx_misc_cr3_count(struct kvm_vcpu *vcpu)\n{\n\treturn vmx_misc_cr3_count(to_vmx(vcpu)->nested.msrs.misc_low);\n}\n\n/*\n * Do the virtual VMX capability MSRs specify that L1 can use VMWRITE\n * to modify any valid field of the VMCS, or are the VM-exit\n * information fields read-only?\n */\nstatic inline bool nested_cpu_has_vmwrite_any_field(struct kvm_vcpu *vcpu)\n{\n\treturn to_vmx(vcpu)->nested.msrs.misc_low &\n\t\tMSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS;\n}\n\nstatic inline bool nested_cpu_has(struct vmcs12 *vmcs12, u32 bit)\n{\n\treturn vmcs12->cpu_based_vm_exec_control & bit;\n}\n\nstatic inline bool nested_cpu_has2(struct vmcs12 *vmcs12, u32 bit)\n{\n\treturn (vmcs12->cpu_based_vm_exec_control &\n\t\t\tCPU_BASED_ACTIVATE_SECONDARY_CONTROLS) &&\n\t\t(vmcs12->secondary_vm_exec_control & bit);\n}\n\nstatic inline bool nested_cpu_has_preemption_timer(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control &\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n}\n\nstatic inline bool nested_cpu_has_nmi_exiting(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_NMI_EXITING;\n}\n\nstatic inline bool nested_cpu_has_virtual_nmis(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_VIRTUAL_NMIS;\n}\n\nstatic inline int nested_cpu_has_ept(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);\n}\n\nstatic inline bool nested_cpu_has_xsaves(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n}\n\nstatic inline bool nested_cpu_has_pml(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML);\n}\n\nstatic inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);\n}\n\nstatic inline bool nested_cpu_has_vpid(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VPID);\n}\n\nstatic inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);\n}\n\nstatic inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n}\n\nstatic inline bool nested_cpu_has_posted_intr(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_POSTED_INTR;\n}\n\nstatic inline bool nested_cpu_has_vmfunc(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VMFUNC);\n}\n\nstatic inline bool nested_cpu_has_eptp_switching(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has_vmfunc(vmcs12) &&\n\t\t(vmcs12->vm_function_control &\n\t\t VMX_VMFUNC_EPTP_SWITCHING);\n}\n\nstatic inline bool is_nmi(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK);\n}\n\nstatic void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,\n\t\t\t      u32 exit_intr_info,\n\t\t\t      unsigned long exit_qualification);\nstatic void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification);\n\nstatic int __find_msr_index(struct vcpu_vmx *vmx, u32 msr)\n{\n\tint i;\n\n\tfor (i = 0; i < vmx->nmsrs; ++i)\n\t\tif (vmx_msr_index[vmx->guest_msrs[i].index] == msr)\n\t\t\treturn i;\n\treturn -1;\n}\n\nstatic inline void __invvpid(int ext, u16 vpid, gva_t gva)\n{\n    struct {\n\tu64 vpid : 16;\n\tu64 rsvd : 48;\n\tu64 gva;\n    } operand = { vpid, 0, gva };\n\n    asm volatile (__ex(ASM_VMX_INVVPID)\n\t\t  /* CF==1 or ZF==1 --> rc = -1 */\n\t\t  \"; ja 1f ; ud2 ; 1:\"\n\t\t  : : \"a\"(&operand), \"c\"(ext) : \"cc\", \"memory\");\n}\n\nstatic inline void __invept(int ext, u64 eptp, gpa_t gpa)\n{\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand = {eptp, gpa};\n\n\tasm volatile (__ex(ASM_VMX_INVEPT)\n\t\t\t/* CF==1 or ZF==1 --> rc = -1 */\n\t\t\t\"; ja 1f ; ud2 ; 1:\\n\"\n\t\t\t: : \"a\" (&operand), \"c\" (ext) : \"cc\", \"memory\");\n}\n\nstatic struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr)\n{\n\tint i;\n\n\ti = __find_msr_index(vmx, msr);\n\tif (i >= 0)\n\t\treturn &vmx->guest_msrs[i];\n\treturn NULL;\n}\n\nstatic void vmcs_clear(struct vmcs *vmcs)\n{\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMCLEAR_RAX) \"; setna %0\"\n\t\t      : \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t      : \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmclear fail: %p/%llx\\n\",\n\t\t       vmcs, phys_addr);\n}\n\nstatic inline void loaded_vmcs_init(struct loaded_vmcs *loaded_vmcs)\n{\n\tvmcs_clear(loaded_vmcs->vmcs);\n\tif (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)\n\t\tvmcs_clear(loaded_vmcs->shadow_vmcs);\n\tloaded_vmcs->cpu = -1;\n\tloaded_vmcs->launched = 0;\n}\n\nstatic void vmcs_load(struct vmcs *vmcs)\n{\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_load(phys_addr);\n\n\tasm volatile (__ex(ASM_VMX_VMPTRLD_RAX) \"; setna %0\"\n\t\t\t: \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t\t: \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmptrld %p/%llx failed\\n\",\n\t\t       vmcs, phys_addr);\n}\n\n#ifdef CONFIG_KEXEC_CORE\n/*\n * This bitmap is used to indicate whether the vmclear\n * operation is enabled on all cpus. All disabled by\n * default.\n */\nstatic cpumask_t crash_vmclear_enabled_bitmap = CPU_MASK_NONE;\n\nstatic inline void crash_enable_local_vmclear(int cpu)\n{\n\tcpumask_set_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic inline void crash_disable_local_vmclear(int cpu)\n{\n\tcpumask_clear_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic inline int crash_local_vmclear_enabled(int cpu)\n{\n\treturn cpumask_test_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic void crash_vmclear_local_loaded_vmcss(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v;\n\n\tif (!crash_local_vmclear_enabled(cpu))\n\t\treturn;\n\n\tlist_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t    loaded_vmcss_on_cpu_link)\n\t\tvmcs_clear(v->vmcs);\n}\n#else\nstatic inline void crash_enable_local_vmclear(int cpu) { }\nstatic inline void crash_disable_local_vmclear(int cpu) { }\n#endif /* CONFIG_KEXEC_CORE */\n\nstatic void __loaded_vmcs_clear(void *arg)\n{\n\tstruct loaded_vmcs *loaded_vmcs = arg;\n\tint cpu = raw_smp_processor_id();\n\n\tif (loaded_vmcs->cpu != cpu)\n\t\treturn; /* vcpu migration can race with cpu offline */\n\tif (per_cpu(current_vmcs, cpu) == loaded_vmcs->vmcs)\n\t\tper_cpu(current_vmcs, cpu) = NULL;\n\tcrash_disable_local_vmclear(cpu);\n\tlist_del(&loaded_vmcs->loaded_vmcss_on_cpu_link);\n\n\t/*\n\t * we should ensure updating loaded_vmcs->loaded_vmcss_on_cpu_link\n\t * is before setting loaded_vmcs->vcpu to -1 which is done in\n\t * loaded_vmcs_init. Otherwise, other cpu can see vcpu = -1 fist\n\t * then adds the vmcs into percpu list before it is deleted.\n\t */\n\tsmp_wmb();\n\n\tloaded_vmcs_init(loaded_vmcs);\n\tcrash_enable_local_vmclear(cpu);\n}\n\nstatic void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)\n{\n\tint cpu = loaded_vmcs->cpu;\n\n\tif (cpu != -1)\n\t\tsmp_call_function_single(cpu,\n\t\t\t __loaded_vmcs_clear, loaded_vmcs, 1);\n}\n\nstatic inline void vpid_sync_vcpu_single(int vpid)\n{\n\tif (vpid == 0)\n\t\treturn;\n\n\tif (cpu_has_vmx_invvpid_single())\n\t\t__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);\n}\n\nstatic inline void vpid_sync_vcpu_global(void)\n{\n\tif (cpu_has_vmx_invvpid_global())\n\t\t__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);\n}\n\nstatic inline void vpid_sync_context(int vpid)\n{\n\tif (cpu_has_vmx_invvpid_single())\n\t\tvpid_sync_vcpu_single(vpid);\n\telse\n\t\tvpid_sync_vcpu_global();\n}\n\nstatic inline void ept_sync_global(void)\n{\n\t__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);\n}\n\nstatic inline void ept_sync_context(u64 eptp)\n{\n\tif (cpu_has_vmx_invept_context())\n\t\t__invept(VMX_EPT_EXTENT_CONTEXT, eptp, 0);\n\telse\n\t\tept_sync_global();\n}\n\nstatic __always_inline void vmcs_check16(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,\n\t\t\t \"16-bit accessor invalid for 64-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"16-bit accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"16-bit accessor invalid for 32-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"16-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_check32(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"32-bit accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"32-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_check64(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"64-bit accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"64-bit accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"64-bit accessor invalid for 32-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"64-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_checkl(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"Natural width accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,\n\t\t\t \"Natural width accessor invalid for 64-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"Natural width accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"Natural width accessor invalid for 32-bit field\");\n}\n\nstatic __always_inline unsigned long __vmcs_readl(unsigned long field)\n{\n\tunsigned long value;\n\n\tasm volatile (__ex_clear(ASM_VMX_VMREAD_RDX_RAX, \"%0\")\n\t\t      : \"=a\"(value) : \"d\"(field) : \"cc\");\n\treturn value;\n}\n\nstatic __always_inline u16 vmcs_read16(unsigned long field)\n{\n\tvmcs_check16(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read16(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic __always_inline u32 vmcs_read32(unsigned long field)\n{\n\tvmcs_check32(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read32(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic __always_inline u64 vmcs_read64(unsigned long field)\n{\n\tvmcs_check64(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read64(field);\n#ifdef CONFIG_X86_64\n\treturn __vmcs_readl(field);\n#else\n\treturn __vmcs_readl(field) | ((u64)__vmcs_readl(field+1) << 32);\n#endif\n}\n\nstatic __always_inline unsigned long vmcs_readl(unsigned long field)\n{\n\tvmcs_checkl(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read64(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic noinline void vmwrite_error(unsigned long field, unsigned long value)\n{\n\tprintk(KERN_ERR \"vmwrite error: reg %lx value %lx (err %d)\\n\",\n\t       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));\n\tdump_stack();\n}\n\nstatic __always_inline void __vmcs_writel(unsigned long field, unsigned long value)\n{\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMWRITE_RAX_RDX) \"; setna %0\"\n\t\t       : \"=q\"(error) : \"a\"(value), \"d\"(field) : \"cc\");\n\tif (unlikely(error))\n\t\tvmwrite_error(field, value);\n}\n\nstatic __always_inline void vmcs_write16(unsigned long field, u16 value)\n{\n\tvmcs_check16(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write16(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_write32(unsigned long field, u32 value)\n{\n\tvmcs_check32(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_write64(unsigned long field, u64 value)\n{\n\tvmcs_check64(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write64(field, value);\n\n\t__vmcs_writel(field, value);\n#ifndef CONFIG_X86_64\n\tasm volatile (\"\");\n\t__vmcs_writel(field+1, value >> 32);\n#endif\n}\n\nstatic __always_inline void vmcs_writel(unsigned long field, unsigned long value)\n{\n\tvmcs_checkl(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write64(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_clear_bits(unsigned long field, u32 mask)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,\n\t\t\t \"vmcs_clear_bits does not support 64-bit fields\");\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, evmcs_read32(field) & ~mask);\n\n\t__vmcs_writel(field, __vmcs_readl(field) & ~mask);\n}\n\nstatic __always_inline void vmcs_set_bits(unsigned long field, u32 mask)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,\n\t\t\t \"vmcs_set_bits does not support 64-bit fields\");\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, evmcs_read32(field) | mask);\n\n\t__vmcs_writel(field, __vmcs_readl(field) | mask);\n}\n\nstatic inline void vm_entry_controls_reset_shadow(struct vcpu_vmx *vmx)\n{\n\tvmx->vm_entry_controls_shadow = vmcs_read32(VM_ENTRY_CONTROLS);\n}\n\nstatic inline void vm_entry_controls_init(struct vcpu_vmx *vmx, u32 val)\n{\n\tvmcs_write32(VM_ENTRY_CONTROLS, val);\n\tvmx->vm_entry_controls_shadow = val;\n}\n\nstatic inline void vm_entry_controls_set(struct vcpu_vmx *vmx, u32 val)\n{\n\tif (vmx->vm_entry_controls_shadow != val)\n\t\tvm_entry_controls_init(vmx, val);\n}\n\nstatic inline u32 vm_entry_controls_get(struct vcpu_vmx *vmx)\n{\n\treturn vmx->vm_entry_controls_shadow;\n}\n\n\nstatic inline void vm_entry_controls_setbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_entry_controls_set(vmx, vm_entry_controls_get(vmx) | val);\n}\n\nstatic inline void vm_entry_controls_clearbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_entry_controls_set(vmx, vm_entry_controls_get(vmx) & ~val);\n}\n\nstatic inline void vm_exit_controls_reset_shadow(struct vcpu_vmx *vmx)\n{\n\tvmx->vm_exit_controls_shadow = vmcs_read32(VM_EXIT_CONTROLS);\n}\n\nstatic inline void vm_exit_controls_init(struct vcpu_vmx *vmx, u32 val)\n{\n\tvmcs_write32(VM_EXIT_CONTROLS, val);\n\tvmx->vm_exit_controls_shadow = val;\n}\n\nstatic inline void vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val)\n{\n\tif (vmx->vm_exit_controls_shadow != val)\n\t\tvm_exit_controls_init(vmx, val);\n}\n\nstatic inline u32 vm_exit_controls_get(struct vcpu_vmx *vmx)\n{\n\treturn vmx->vm_exit_controls_shadow;\n}\n\n\nstatic inline void vm_exit_controls_setbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_exit_controls_set(vmx, vm_exit_controls_get(vmx) | val);\n}\n\nstatic inline void vm_exit_controls_clearbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_exit_controls_set(vmx, vm_exit_controls_get(vmx) & ~val);\n}\n\nstatic void vmx_segment_cache_clear(struct vcpu_vmx *vmx)\n{\n\tvmx->segment_cache.bitmask = 0;\n}\n\nstatic bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,\n\t\t\t\t       unsigned field)\n{\n\tbool ret;\n\tu32 mask = 1 << (seg * SEG_FIELD_NR + field);\n\n\tif (!(vmx->vcpu.arch.regs_avail & (1 << VCPU_EXREG_SEGMENTS))) {\n\t\tvmx->vcpu.arch.regs_avail |= (1 << VCPU_EXREG_SEGMENTS);\n\t\tvmx->segment_cache.bitmask = 0;\n\t}\n\tret = vmx->segment_cache.bitmask & mask;\n\tvmx->segment_cache.bitmask |= mask;\n\treturn ret;\n}\n\nstatic u16 vmx_read_guest_seg_selector(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu16 *p = &vmx->segment_cache.seg[seg].selector;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_SEL))\n\t\t*p = vmcs_read16(kvm_vmx_segment_fields[seg].selector);\n\treturn *p;\n}\n\nstatic ulong vmx_read_guest_seg_base(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tulong *p = &vmx->segment_cache.seg[seg].base;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_BASE))\n\t\t*p = vmcs_readl(kvm_vmx_segment_fields[seg].base);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_limit(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].limit;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_LIMIT))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].limit);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].ar;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_AR))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].ar_bytes);\n\treturn *p;\n}\n\nstatic void update_exception_bitmap(struct kvm_vcpu *vcpu)\n{\n\tu32 eb;\n\n\teb = (1u << PF_VECTOR) | (1u << UD_VECTOR) | (1u << MC_VECTOR) |\n\t     (1u << DB_VECTOR) | (1u << AC_VECTOR);\n\t/*\n\t * Guest access to VMware backdoor ports could legitimately\n\t * trigger #GP because of TSS I/O permission bitmap.\n\t * We intercept those #GP and allow access to them anyway\n\t * as VMware does.\n\t */\n\tif (enable_vmware_backdoor)\n\t\teb |= (1u << GP_VECTOR);\n\tif ((vcpu->guest_debug &\n\t     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==\n\t    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))\n\t\teb |= 1u << BP_VECTOR;\n\tif (to_vmx(vcpu)->rmode.vm86_active)\n\t\teb = ~0;\n\tif (enable_ept)\n\t\teb &= ~(1u << PF_VECTOR); /* bypass_guest_pf = 0 */\n\n\t/* When we are running a nested L2 guest and L1 specified for it a\n\t * certain exception bitmap, we must trap the same exceptions and pass\n\t * them to L1. When running L2, we will only handle the exceptions\n\t * specified above if L1 did not want them.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\teb |= get_vmcs12(vcpu)->exception_bitmap;\n\n\tvmcs_write32(EXCEPTION_BITMAP, eb);\n}\n\n/*\n * Check if MSR is intercepted for currently loaded MSR bitmap.\n */\nstatic bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tunsigned long *msr_bitmap;\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn true;\n\n\tmsr_bitmap = to_vmx(vcpu)->loaded_vmcs->msr_bitmap;\n\n\tif (msr <= 0x1fff) {\n\t\treturn !!test_bit(msr, msr_bitmap + 0x800 / f);\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\treturn !!test_bit(msr, msr_bitmap + 0xc00 / f);\n\t}\n\n\treturn true;\n}\n\n/*\n * Check if MSR is intercepted for L01 MSR bitmap.\n */\nstatic bool msr_write_intercepted_l01(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tunsigned long *msr_bitmap;\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn true;\n\n\tmsr_bitmap = to_vmx(vcpu)->vmcs01.msr_bitmap;\n\n\tif (msr <= 0x1fff) {\n\t\treturn !!test_bit(msr, msr_bitmap + 0x800 / f);\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\treturn !!test_bit(msr, msr_bitmap + 0xc00 / f);\n\t}\n\n\treturn true;\n}\n\nstatic void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit)\n{\n\tvm_entry_controls_clearbit(vmx, entry);\n\tvm_exit_controls_clearbit(vmx, exit);\n}\n\nstatic void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)\n{\n\tunsigned i;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\t}\n\n\tfor (i = 0; i < m->nr; ++i)\n\t\tif (m->guest[i].index == msr)\n\t\t\tbreak;\n\n\tif (i == m->nr)\n\t\treturn;\n\t--m->nr;\n\tm->guest[i] = m->guest[m->nr];\n\tm->host[i] = m->host[m->nr];\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);\n}\n\nstatic void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit,\n\t\tunsigned long guest_val_vmcs, unsigned long host_val_vmcs,\n\t\tu64 guest_val, u64 host_val)\n{\n\tvmcs_write64(guest_val_vmcs, guest_val);\n\tvmcs_write64(host_val_vmcs, host_val);\n\tvm_entry_controls_setbit(vmx, entry);\n\tvm_exit_controls_setbit(vmx, exit);\n}\n\nstatic void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,\n\t\t\t\t  u64 guest_val, u64 host_val)\n{\n\tunsigned i;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER,\n\t\t\t\t\tGUEST_IA32_EFER,\n\t\t\t\t\tHOST_IA32_EFER,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tGUEST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tHOST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\t/* PEBS needs a quiescent period after being disabled (to write\n\t\t * a record).  Disabling PEBS through VMX MSR swapping doesn't\n\t\t * provide that period, so a CPU could write host's record into\n\t\t * guest's memory.\n\t\t */\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n\t}\n\n\tfor (i = 0; i < m->nr; ++i)\n\t\tif (m->guest[i].index == msr)\n\t\t\tbreak;\n\n\tif (i == NR_AUTOLOAD_MSRS) {\n\t\tprintk_once(KERN_WARNING \"Not enough msr switch entries. \"\n\t\t\t\t\"Can't add msr %x\\n\", msr);\n\t\treturn;\n\t} else if (i == m->nr) {\n\t\t++m->nr;\n\t\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);\n\t\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);\n\t}\n\n\tm->guest[i].index = msr;\n\tm->guest[i].value = guest_val;\n\tm->host[i].index = msr;\n\tm->host[i].value = host_val;\n}\n\nstatic bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)\n{\n\tu64 guest_efer = vmx->vcpu.arch.efer;\n\tu64 ignore_bits = 0;\n\n\tif (!enable_ept) {\n\t\t/*\n\t\t * NX is needed to handle CR0.WP=1, CR4.SMEP=1.  Testing\n\t\t * host CPUID is more efficient than testing guest CPUID\n\t\t * or CR4.  Host SMEP is anyway a requirement for guest SMEP.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_SMEP))\n\t\t\tguest_efer |= EFER_NX;\n\t\telse if (!(guest_efer & EFER_NX))\n\t\t\tignore_bits |= EFER_NX;\n\t}\n\n\t/*\n\t * LMA and LME handled by hardware; SCE meaningless outside long mode.\n\t */\n\tignore_bits |= EFER_SCE;\n#ifdef CONFIG_X86_64\n\tignore_bits |= EFER_LMA | EFER_LME;\n\t/* SCE is meaningful only in long mode on Intel */\n\tif (guest_efer & EFER_LMA)\n\t\tignore_bits &= ~(u64)EFER_SCE;\n#endif\n\n\tclear_atomic_switch_msr(vmx, MSR_EFER);\n\n\t/*\n\t * On EPT, we can't emulate NX, so we must switch EFER atomically.\n\t * On CPUs that support \"load IA32_EFER\", always switch EFER\n\t * atomically, since it's faster than switching it manually.\n\t */\n\tif (cpu_has_load_ia32_efer ||\n\t    (enable_ept && ((vmx->vcpu.arch.efer ^ host_efer) & EFER_NX))) {\n\t\tif (!(guest_efer & EFER_LMA))\n\t\t\tguest_efer &= ~EFER_LME;\n\t\tif (guest_efer != host_efer)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_EFER,\n\t\t\t\t\t      guest_efer, host_efer);\n\t\treturn false;\n\t} else {\n\t\tguest_efer &= ~ignore_bits;\n\t\tguest_efer |= host_efer & ignore_bits;\n\n\t\tvmx->guest_msrs[efer_offset].data = guest_efer;\n\t\tvmx->guest_msrs[efer_offset].mask = ~ignore_bits;\n\n\t\treturn true;\n\t}\n}\n\n#ifdef CONFIG_X86_32\n/*\n * On 32-bit kernels, VM exits still load the FS and GS bases from the\n * VMCS rather than the segment table.  KVM uses this helper to figure\n * out the current bases to poke them into the VMCS before entry.\n */\nstatic unsigned long segment_base(u16 selector)\n{\n\tstruct desc_struct *table;\n\tunsigned long v;\n\n\tif (!(selector & ~SEGMENT_RPL_MASK))\n\t\treturn 0;\n\n\ttable = get_current_gdt_ro();\n\n\tif ((selector & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tu16 ldt_selector = kvm_read_ldt();\n\n\t\tif (!(ldt_selector & ~SEGMENT_RPL_MASK))\n\t\t\treturn 0;\n\n\t\ttable = (struct desc_struct *)segment_base(ldt_selector);\n\t}\n\tv = get_desc_base(&table[selector >> 3]);\n\treturn v;\n}\n#endif\n\nstatic void vmx_save_host_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n#ifdef CONFIG_X86_64\n\tint cpu = raw_smp_processor_id();\n#endif\n\tint i;\n\n\tif (vmx->host_state.loaded)\n\t\treturn;\n\n\tvmx->host_state.loaded = 1;\n\t/*\n\t * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not\n\t * allow segment selectors with cpl > 0 or ti == 1.\n\t */\n\tvmx->host_state.ldt_sel = kvm_read_ldt();\n\tvmx->host_state.gs_ldt_reload_needed = vmx->host_state.ldt_sel;\n\n#ifdef CONFIG_X86_64\n\tsave_fsgs_for_kvm();\n\tvmx->host_state.fs_sel = current->thread.fsindex;\n\tvmx->host_state.gs_sel = current->thread.gsindex;\n#else\n\tsavesegment(fs, vmx->host_state.fs_sel);\n\tsavesegment(gs, vmx->host_state.gs_sel);\n#endif\n\tif (!(vmx->host_state.fs_sel & 7)) {\n\t\tvmcs_write16(HOST_FS_SELECTOR, vmx->host_state.fs_sel);\n\t\tvmx->host_state.fs_reload_needed = 0;\n\t} else {\n\t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n\t\tvmx->host_state.fs_reload_needed = 1;\n\t}\n\tif (!(vmx->host_state.gs_sel & 7))\n\t\tvmcs_write16(HOST_GS_SELECTOR, vmx->host_state.gs_sel);\n\telse {\n\t\tvmcs_write16(HOST_GS_SELECTOR, 0);\n\t\tvmx->host_state.gs_ldt_reload_needed = 1;\n\t}\n\n#ifdef CONFIG_X86_64\n\tsavesegment(ds, vmx->host_state.ds_sel);\n\tsavesegment(es, vmx->host_state.es_sel);\n\n\tvmcs_writel(HOST_FS_BASE, current->thread.fsbase);\n\tvmcs_writel(HOST_GS_BASE, cpu_kernelmode_gs_base(cpu));\n\n\tvmx->msr_host_kernel_gs_base = current->thread.gsbase;\n\tif (is_long_mode(&vmx->vcpu))\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#else\n\tvmcs_writel(HOST_FS_BASE, segment_base(vmx->host_state.fs_sel));\n\tvmcs_writel(HOST_GS_BASE, segment_base(vmx->host_state.gs_sel));\n#endif\n\tif (boot_cpu_has(X86_FEATURE_MPX))\n\t\trdmsrl(MSR_IA32_BNDCFGS, vmx->host_state.msr_host_bndcfgs);\n\tfor (i = 0; i < vmx->save_nmsrs; ++i)\n\t\tkvm_set_shared_msr(vmx->guest_msrs[i].index,\n\t\t\t\t   vmx->guest_msrs[i].data,\n\t\t\t\t   vmx->guest_msrs[i].mask);\n}\n\nstatic void __vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->host_state.loaded)\n\t\treturn;\n\n\t++vmx->vcpu.stat.host_state_reload;\n\tvmx->host_state.loaded = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu))\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#endif\n\tif (vmx->host_state.gs_ldt_reload_needed) {\n\t\tkvm_load_ldt(vmx->host_state.ldt_sel);\n#ifdef CONFIG_X86_64\n\t\tload_gs_index(vmx->host_state.gs_sel);\n#else\n\t\tloadsegment(gs, vmx->host_state.gs_sel);\n#endif\n\t}\n\tif (vmx->host_state.fs_reload_needed)\n\t\tloadsegment(fs, vmx->host_state.fs_sel);\n#ifdef CONFIG_X86_64\n\tif (unlikely(vmx->host_state.ds_sel | vmx->host_state.es_sel)) {\n\t\tloadsegment(ds, vmx->host_state.ds_sel);\n\t\tloadsegment(es, vmx->host_state.es_sel);\n\t}\n#endif\n\tinvalidate_tss_limit();\n#ifdef CONFIG_X86_64\n\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n#endif\n\tif (vmx->host_state.msr_host_bndcfgs)\n\t\twrmsrl(MSR_IA32_BNDCFGS, vmx->host_state.msr_host_bndcfgs);\n\tload_fixmap_gdt(raw_smp_processor_id());\n}\n\nstatic void vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tpreempt_disable();\n\t__vmx_load_host_state(vmx);\n\tpreempt_enable();\n}\n\nstatic void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\tstruct pi_desc old, new;\n\tunsigned int dest;\n\n\t/*\n\t * In case of hot-plug or hot-unplug, we may have to undo\n\t * vmx_vcpu_pi_put even if there is no assigned device.  And we\n\t * always keep PI.NDST up to date for simplicity: it makes the\n\t * code easier, and CPU migration is not a fast path.\n\t */\n\tif (!pi_test_sn(pi_desc) && vcpu->cpu == cpu)\n\t\treturn;\n\n\t/*\n\t * First handle the simple case where no cmpxchg is necessary; just\n\t * allow posting non-urgent interrupts.\n\t *\n\t * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change\n\t * PI.NDST: pi_post_block will do it for us and the wakeup_handler\n\t * expects the VCPU to be on the blocked_vcpu_list that matches\n\t * PI.NDST.\n\t */\n\tif (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR ||\n\t    vcpu->cpu == cpu) {\n\t\tpi_clear_sn(pi_desc);\n\t\treturn;\n\t}\n\n\t/* The full case.  */\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\tdest = cpu_physical_id(cpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\tnew.sn = 0;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n}\n\nstatic void decache_tsc_multiplier(struct vcpu_vmx *vmx)\n{\n\tvmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;\n\tvmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put(), but assumes\n * vcpu mutex is already taken.\n */\nstatic void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool already_loaded = vmx->loaded_vmcs->cpu == cpu;\n\n\tif (!already_loaded) {\n\t\tloaded_vmcs_clear(vmx->loaded_vmcs);\n\t\tlocal_irq_disable();\n\t\tcrash_disable_local_vmclear(cpu);\n\n\t\t/*\n\t\t * Read loaded_vmcs->cpu should be before fetching\n\t\t * loaded_vmcs->loaded_vmcss_on_cpu_link.\n\t\t * See the comments in __loaded_vmcs_clear().\n\t\t */\n\t\tsmp_rmb();\n\n\t\tlist_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,\n\t\t\t &per_cpu(loaded_vmcss_on_cpu, cpu));\n\t\tcrash_enable_local_vmclear(cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\tif (per_cpu(current_vmcs, cpu) != vmx->loaded_vmcs->vmcs) {\n\t\tper_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;\n\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (!already_loaded) {\n\t\tvoid *gdt = get_current_gdt_ro();\n\t\tunsigned long sysenter_esp;\n\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\n\t\t/*\n\t\t * Linux uses per-cpu TSS and GDT, so set these when switching\n\t\t * processors.  See 22.2.4.\n\t\t */\n\t\tvmcs_writel(HOST_TR_BASE,\n\t\t\t    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);\n\t\tvmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */\n\n\t\t/*\n\t\t * VM exits change the host TR limit to 0x67 after a VM\n\t\t * exit.  This is okay, since 0x67 covers everything except\n\t\t * the IO bitmap and have have code to handle the IO bitmap\n\t\t * being lost after a VM exit.\n\t\t */\n\t\tBUILD_BUG_ON(IO_BITMAP_OFFSET - 1 != 0x67);\n\n\t\trdmsrl(MSR_IA32_SYSENTER_ESP, sysenter_esp);\n\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP, sysenter_esp); /* 22.2.3 */\n\n\t\tvmx->loaded_vmcs->cpu = cpu;\n\t}\n\n\t/* Setup TSC multiplier */\n\tif (kvm_has_tsc_control &&\n\t    vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tvmx_vcpu_pi_load(vcpu, cpu);\n\tvmx->host_pkru = read_pkru();\n\tvmx->host_debugctlmsr = get_debugctlmsr();\n}\n\nstatic void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\t/* Set SN when the vCPU is preempted */\n\tif (vcpu->preempted)\n\t\tpi_set_sn(pi_desc);\n}\n\nstatic void vmx_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tvmx_vcpu_pi_put(vcpu);\n\n\t__vmx_load_host_state(to_vmx(vcpu));\n}\n\nstatic bool emulation_required(struct kvm_vcpu *vcpu)\n{\n\treturn emulate_invalid_guest_state && !guest_state_valid(vcpu);\n}\n\nstatic void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);\n\n/*\n * Return the cr0 value that a nested guest would read. This is a combination\n * of the real cr0 used to run the guest (guest_cr0), and the bits shadowed by\n * its hypervisor (cr0_read_shadow).\n */\nstatic inline unsigned long nested_read_cr0(struct vmcs12 *fields)\n{\n\treturn (fields->guest_cr0 & ~fields->cr0_guest_host_mask) |\n\t\t(fields->cr0_read_shadow & fields->cr0_guest_host_mask);\n}\nstatic inline unsigned long nested_read_cr4(struct vmcs12 *fields)\n{\n\treturn (fields->guest_cr4 & ~fields->cr4_guest_host_mask) |\n\t\t(fields->cr4_read_shadow & fields->cr4_guest_host_mask);\n}\n\nstatic unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags, save_rflags;\n\n\tif (!test_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail)) {\n\t\t__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);\n\t\trflags = vmcs_readl(GUEST_RFLAGS);\n\t\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\t\trflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t\tsave_rflags = to_vmx(vcpu)->rmode.save_rflags;\n\t\t\trflags |= save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t}\n\t\tto_vmx(vcpu)->rflags = rflags;\n\t}\n\treturn to_vmx(vcpu)->rflags;\n}\n\nstatic void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tunsigned long old_rflags = vmx_get_rflags(vcpu);\n\n\t__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);\n\tto_vmx(vcpu)->rflags = rflags;\n\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\tto_vmx(vcpu)->rmode.save_rflags = rflags;\n\t\trflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\t}\n\tvmcs_writel(GUEST_RFLAGS, rflags);\n\n\tif ((old_rflags ^ to_vmx(vcpu)->rflags) & X86_EFLAGS_VM)\n\t\tto_vmx(vcpu)->emulation_required = emulation_required(vcpu);\n}\n\nstatic u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)\n{\n\tu32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tint ret = 0;\n\n\tif (interruptibility & GUEST_INTR_STATE_STI)\n\t\tret |= KVM_X86_SHADOW_INT_STI;\n\tif (interruptibility & GUEST_INTR_STATE_MOV_SS)\n\t\tret |= KVM_X86_SHADOW_INT_MOV_SS;\n\n\treturn ret;\n}\n\nstatic void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\n{\n\tu32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tu32 interruptibility = interruptibility_old;\n\n\tinterruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);\n\n\tif (mask & KVM_X86_SHADOW_INT_MOV_SS)\n\t\tinterruptibility |= GUEST_INTR_STATE_MOV_SS;\n\telse if (mask & KVM_X86_SHADOW_INT_STI)\n\t\tinterruptibility |= GUEST_INTR_STATE_STI;\n\n\tif ((interruptibility != interruptibility_old))\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);\n}\n\nstatic void skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rip;\n\n\trip = kvm_rip_read(vcpu);\n\trip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tkvm_rip_write(vcpu, rip);\n\n\t/* skipping an emulated instruction also counts */\n\tvmx_set_interrupt_shadow(vcpu, 0);\n}\n\nstatic void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       unsigned long exit_qual)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned int nr = vcpu->arch.exception.nr;\n\tu32 intr_info = nr | INTR_INFO_VALID_MASK;\n\n\tif (vcpu->arch.exception.has_error_code) {\n\t\tvmcs12->vm_exit_intr_error_code = vcpu->arch.exception.error_code;\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (kvm_exception_is_soft(nr))\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\telse\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tif (!(vmcs12->idt_vectoring_info_field & VECTORING_INFO_VALID_MASK) &&\n\t    vmx_get_nmi_mask(vcpu))\n\t\tintr_info |= INTR_INFO_UNBLOCK_NMI;\n\n\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);\n}\n\n/*\n * KVM wants to inject page-faults which it got to the guest. This function\n * checks whether in a nested guest, we need to inject them to L1 or L2.\n */\nstatic int nested_vmx_check_exception(struct kvm_vcpu *vcpu, unsigned long *exit_qual)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned int nr = vcpu->arch.exception.nr;\n\n\tif (nr == PF_VECTOR) {\n\t\tif (vcpu->arch.exception.nested_apf) {\n\t\t\t*exit_qual = vcpu->arch.apf.nested_apf_token;\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * FIXME: we must not write CR2 when L1 intercepts an L2 #PF exception.\n\t\t * The fix is to add the ancillary datum (CR2 or DR6) to structs\n\t\t * kvm_queued_exception and kvm_vcpu_events, so that CR2 and DR6\n\t\t * can be written only when inject_pending_event runs.  This should be\n\t\t * conditional on a new capability---if the capability is disabled,\n\t\t * kvm_multiple_exception would write the ancillary information to\n\t\t * CR2 or DR6, for backwards ABI-compatibility.\n\t\t */\n\t\tif (nested_vmx_is_page_fault_vmexit(vmcs12,\n\t\t\t\t\t\t    vcpu->arch.exception.error_code)) {\n\t\t\t*exit_qual = vcpu->arch.cr2;\n\t\t\treturn 1;\n\t\t}\n\t} else {\n\t\tif (vmcs12->exception_bitmap & (1u << nr)) {\n\t\t\tif (nr == DB_VECTOR)\n\t\t\t\t*exit_qual = vcpu->arch.dr6;\n\t\t\telse\n\t\t\t\t*exit_qual = 0;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_clear_hlt(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * Ensure that we clear the HLT state in the VMCS.  We don't need to\n\t * explicitly skip the instruction because if the HLT state is set,\n\t * then the instruction is already executing and RIP has already been\n\t * advanced.\n\t */\n\tif (kvm_hlt_in_guest(vcpu->kvm) &&\n\t\t\tvmcs_read32(GUEST_ACTIVITY_STATE) == GUEST_ACTIVITY_HLT)\n\t\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n}\n\nstatic void vmx_queue_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned nr = vcpu->arch.exception.nr;\n\tbool has_error_code = vcpu->arch.exception.has_error_code;\n\tu32 error_code = vcpu->arch.exception.error_code;\n\tu32 intr_info = nr | INTR_INFO_VALID_MASK;\n\n\tif (has_error_code) {\n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (kvm_exception_is_soft(nr))\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tif (kvm_inject_realmode_interrupt(vcpu, nr, inc_eip) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(vmx->emulation_required);\n\n\tif (kvm_exception_is_soft(nr)) {\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\t} else\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic bool vmx_rdtscp_supported(void)\n{\n\treturn cpu_has_vmx_rdtscp();\n}\n\nstatic bool vmx_invpcid_supported(void)\n{\n\treturn cpu_has_vmx_invpcid() && enable_ept;\n}\n\n/*\n * Swap MSR entry in host/guest MSR entry array.\n */\nstatic void move_msr_up(struct vcpu_vmx *vmx, int from, int to)\n{\n\tstruct shared_msr_entry tmp;\n\n\ttmp = vmx->guest_msrs[to];\n\tvmx->guest_msrs[to] = vmx->guest_msrs[from];\n\tvmx->guest_msrs[from] = tmp;\n}\n\n/*\n * Set up the vmcs to automatically save and restore system\n * msrs.  Don't touch the 64-bit msrs if the guest is in legacy\n * mode, as fiddling with msrs is very expensive.\n */\nstatic void setup_msrs(struct vcpu_vmx *vmx)\n{\n\tint save_nmsrs, index;\n\n\tsave_nmsrs = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\tindex = __find_msr_index(vmx, MSR_SYSCALL_MASK);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_LSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_CSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_TSC_AUX);\n\t\tif (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\t/*\n\t\t * MSR_STAR is only needed on long mode guests, and only\n\t\t * if efer.sce is enabled.\n\t\t */\n\t\tindex = __find_msr_index(vmx, MSR_STAR);\n\t\tif ((index >= 0) && (vmx->vcpu.arch.efer & EFER_SCE))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t}\n#endif\n\tindex = __find_msr_index(vmx, MSR_EFER);\n\tif (index >= 0 && update_transition_efer(vmx, index))\n\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\n\tvmx->save_nmsrs = save_nmsrs;\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(&vmx->vcpu);\n}\n\nstatic u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t    (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING))\n\t\treturn vcpu->arch.tsc_offset - vmcs12->tsc_offset;\n\n\treturn vcpu->arch.tsc_offset;\n}\n\n/*\n * writes 'offset' into guest's timestamp counter offset register\n */\nstatic void vmx_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * We're here if L1 chose not to trap WRMSR to TSC. According\n\t\t * to the spec, this should set L1's TSC; The offset that L1\n\t\t * set for L2 remains unchanged, and still needs to be added\n\t\t * to the newly set TSC to get L2's TSC.\n\t\t */\n\t\tstruct vmcs12 *vmcs12;\n\t\t/* recalculate vmcs02.TSC_OFFSET: */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tvmcs_write64(TSC_OFFSET, offset +\n\t\t\t(nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETING) ?\n\t\t\t vmcs12->tsc_offset : 0));\n\t} else {\n\t\ttrace_kvm_write_tsc_offset(vcpu->vcpu_id,\n\t\t\t\t\t   vmcs_read64(TSC_OFFSET), offset);\n\t\tvmcs_write64(TSC_OFFSET, offset);\n\t}\n}\n\n/*\n * nested_vmx_allowed() checks whether a guest should be allowed to use VMX\n * instructions and MSRs (i.e., nested VMX). Nested VMX is disabled for\n * all guests if the \"nested\" module option is off, and can also be disabled\n * for a single guest by disabling its VMX cpuid bit.\n */\nstatic inline bool nested_vmx_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn nested && guest_cpuid_has(vcpu, X86_FEATURE_VMX);\n}\n\n/*\n * nested_vmx_setup_ctls_msrs() sets up variables containing the values to be\n * returned for the various VMX controls MSRs when nested VMX is enabled.\n * The same values should also be used to verify that vmcs12 control fields are\n * valid during nested entry from L1 to L2.\n * Each of these control msrs has a low and high 32-bit half: A low bit is on\n * if the corresponding bit in the (32-bit) control field *must* be on, and a\n * bit in the high half is on if the corresponding bit in the control field\n * may be on. See also vmx_control_verify().\n */\nstatic void nested_vmx_setup_ctls_msrs(struct nested_vmx_msrs *msrs, bool apicv)\n{\n\tif (!nested) {\n\t\tmemset(msrs, 0, sizeof(*msrs));\n\t\treturn;\n\t}\n\n\t/*\n\t * Note that as a general rule, the high half of the MSRs (bits in\n\t * the control fields which may be 1) should be initialized by the\n\t * intersection of the underlying hardware's MSR (i.e., features which\n\t * can be supported) and the list of features we want to expose -\n\t * because they are known to be properly supported in our code.\n\t * Also, usually, the low half of the MSRs (bits which must be 1) can\n\t * be set to 0, meaning that L1 may turn off any of these bits. The\n\t * reason is that if one of these bits is necessary, it will appear\n\t * in vmcs01 and prepare_vmcs02, when it bitwise-or's the control\n\t * fields of vmcs01 and vmcs02, will turn these bits off - and\n\t * nested_vmx_exit_reflected() will not pass related exits to L1.\n\t * These rules have exceptions below.\n\t */\n\n\t/* pin-based controls */\n\trdmsr(MSR_IA32_VMX_PINBASED_CTLS,\n\t\tmsrs->pinbased_ctls_low,\n\t\tmsrs->pinbased_ctls_high);\n\tmsrs->pinbased_ctls_low |=\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->pinbased_ctls_high &=\n\t\tPIN_BASED_EXT_INTR_MASK |\n\t\tPIN_BASED_NMI_EXITING |\n\t\tPIN_BASED_VIRTUAL_NMIS |\n\t\t(apicv ? PIN_BASED_POSTED_INTR : 0);\n\tmsrs->pinbased_ctls_high |=\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/* exit controls */\n\trdmsr(MSR_IA32_VMX_EXIT_CTLS,\n\t\tmsrs->exit_ctls_low,\n\t\tmsrs->exit_ctls_high);\n\tmsrs->exit_ctls_low =\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->exit_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_EXIT_HOST_ADDR_SPACE_SIZE |\n#endif\n\t\tVM_EXIT_LOAD_IA32_PAT | VM_EXIT_SAVE_IA32_PAT;\n\tmsrs->exit_ctls_high |=\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tVM_EXIT_LOAD_IA32_EFER | VM_EXIT_SAVE_IA32_EFER |\n\t\tVM_EXIT_SAVE_VMX_PREEMPTION_TIMER | VM_EXIT_ACK_INTR_ON_EXIT;\n\n\tif (kvm_mpx_supported())\n\t\tmsrs->exit_ctls_high |= VM_EXIT_CLEAR_BNDCFGS;\n\n\t/* We support free control of debug control saving. */\n\tmsrs->exit_ctls_low &= ~VM_EXIT_SAVE_DEBUG_CONTROLS;\n\n\t/* entry controls */\n\trdmsr(MSR_IA32_VMX_ENTRY_CTLS,\n\t\tmsrs->entry_ctls_low,\n\t\tmsrs->entry_ctls_high);\n\tmsrs->entry_ctls_low =\n\t\tVM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->entry_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_ENTRY_IA32E_MODE |\n#endif\n\t\tVM_ENTRY_LOAD_IA32_PAT;\n\tmsrs->entry_ctls_high |=\n\t\t(VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR | VM_ENTRY_LOAD_IA32_EFER);\n\tif (kvm_mpx_supported())\n\t\tmsrs->entry_ctls_high |= VM_ENTRY_LOAD_BNDCFGS;\n\n\t/* We support free control of debug control loading. */\n\tmsrs->entry_ctls_low &= ~VM_ENTRY_LOAD_DEBUG_CONTROLS;\n\n\t/* cpu-based controls */\n\trdmsr(MSR_IA32_VMX_PROCBASED_CTLS,\n\t\tmsrs->procbased_ctls_low,\n\t\tmsrs->procbased_ctls_high);\n\tmsrs->procbased_ctls_low =\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->procbased_ctls_high &=\n\t\tCPU_BASED_VIRTUAL_INTR_PENDING |\n\t\tCPU_BASED_VIRTUAL_NMI_PENDING | CPU_BASED_USE_TSC_OFFSETING |\n\t\tCPU_BASED_HLT_EXITING | CPU_BASED_INVLPG_EXITING |\n\t\tCPU_BASED_MWAIT_EXITING | CPU_BASED_CR3_LOAD_EXITING |\n\t\tCPU_BASED_CR3_STORE_EXITING |\n#ifdef CONFIG_X86_64\n\t\tCPU_BASED_CR8_LOAD_EXITING | CPU_BASED_CR8_STORE_EXITING |\n#endif\n\t\tCPU_BASED_MOV_DR_EXITING | CPU_BASED_UNCOND_IO_EXITING |\n\t\tCPU_BASED_USE_IO_BITMAPS | CPU_BASED_MONITOR_TRAP_FLAG |\n\t\tCPU_BASED_MONITOR_EXITING | CPU_BASED_RDPMC_EXITING |\n\t\tCPU_BASED_RDTSC_EXITING | CPU_BASED_PAUSE_EXITING |\n\t\tCPU_BASED_TPR_SHADOW | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n\t/*\n\t * We can allow some features even when not supported by the\n\t * hardware. For example, L1 can specify an MSR bitmap - and we\n\t * can use it to avoid exits to L1 - even when L0 runs L2\n\t * without MSR bitmaps.\n\t */\n\tmsrs->procbased_ctls_high |=\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tCPU_BASED_USE_MSR_BITMAPS;\n\n\t/* We support free control of CR3 access interception. */\n\tmsrs->procbased_ctls_low &=\n\t\t~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);\n\n\t/*\n\t * secondary cpu-based controls.  Do not include those that\n\t * depend on CPUID bits, they are added later by vmx_cpuid_update.\n\t */\n\trdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,\n\t\tmsrs->secondary_ctls_low,\n\t\tmsrs->secondary_ctls_high);\n\tmsrs->secondary_ctls_low = 0;\n\tmsrs->secondary_ctls_high &=\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\tSECONDARY_EXEC_DESC |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\tSECONDARY_EXEC_WBINVD_EXITING;\n\n\tif (enable_ept) {\n\t\t/* nested EPT: emulate EPT also to L1 */\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_EPT;\n\t\tmsrs->ept_caps = VMX_EPT_PAGE_WALK_4_BIT |\n\t\t\t VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT;\n\t\tif (cpu_has_vmx_ept_execute_only())\n\t\t\tmsrs->ept_caps |=\n\t\t\t\tVMX_EPT_EXECUTE_ONLY_BIT;\n\t\tmsrs->ept_caps &= vmx_capability.ept;\n\t\tmsrs->ept_caps |= VMX_EPT_EXTENT_GLOBAL_BIT |\n\t\t\tVMX_EPT_EXTENT_CONTEXT_BIT | VMX_EPT_2MB_PAGE_BIT |\n\t\t\tVMX_EPT_1GB_PAGE_BIT;\n\t\tif (enable_ept_ad_bits) {\n\t\t\tmsrs->secondary_ctls_high |=\n\t\t\t\tSECONDARY_EXEC_ENABLE_PML;\n\t\t\tmsrs->ept_caps |= VMX_EPT_AD_BIT;\n\t\t}\n\t}\n\n\tif (cpu_has_vmx_vmfunc()) {\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n\t\t/*\n\t\t * Advertise EPTP switching unconditionally\n\t\t * since we emulate it\n\t\t */\n\t\tif (enable_ept)\n\t\t\tmsrs->vmfunc_controls =\n\t\t\t\tVMX_VMFUNC_EPTP_SWITCHING;\n\t}\n\n\t/*\n\t * Old versions of KVM use the single-context version without\n\t * checking for support, so declare that it is supported even\n\t * though it is treated as global context.  The alternative is\n\t * not failing the single-context invvpid, and it is worse.\n\t */\n\tif (enable_vpid) {\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_VPID;\n\t\tmsrs->vpid_caps = VMX_VPID_INVVPID_BIT |\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK;\n\t}\n\n\tif (enable_unrestricted_guest)\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST;\n\n\t/* miscellaneous data */\n\trdmsr(MSR_IA32_VMX_MISC,\n\t\tmsrs->misc_low,\n\t\tmsrs->misc_high);\n\tmsrs->misc_low &= VMX_MISC_SAVE_EFER_LMA;\n\tmsrs->misc_low |=\n\t\tMSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS |\n\t\tVMX_MISC_EMULATED_PREEMPTION_TIMER_RATE |\n\t\tVMX_MISC_ACTIVITY_HLT;\n\tmsrs->misc_high = 0;\n\n\t/*\n\t * This MSR reports some information about VMX support. We\n\t * should return information about the VMX we emulate for the\n\t * guest, and the VMCS structure we give it - not about the\n\t * VMX support of the underlying hardware.\n\t */\n\tmsrs->basic =\n\t\tVMCS12_REVISION |\n\t\tVMX_BASIC_TRUE_CTLS |\n\t\t((u64)VMCS12_SIZE << VMX_BASIC_VMCS_SIZE_SHIFT) |\n\t\t(VMX_BASIC_MEM_TYPE_WB << VMX_BASIC_MEM_TYPE_SHIFT);\n\n\tif (cpu_has_vmx_basic_inout())\n\t\tmsrs->basic |= VMX_BASIC_INOUT;\n\n\t/*\n\t * These MSRs specify bits which the guest must keep fixed on\n\t * while L1 is in VMXON mode (in L1's root mode, or running an L2).\n\t * We picked the standard core2 setting.\n\t */\n#define VMXON_CR0_ALWAYSON     (X86_CR0_PE | X86_CR0_PG | X86_CR0_NE)\n#define VMXON_CR4_ALWAYSON     X86_CR4_VMXE\n\tmsrs->cr0_fixed0 = VMXON_CR0_ALWAYSON;\n\tmsrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;\n\n\t/* These MSRs specify bits which the guest must keep fixed off. */\n\trdmsrl(MSR_IA32_VMX_CR0_FIXED1, msrs->cr0_fixed1);\n\trdmsrl(MSR_IA32_VMX_CR4_FIXED1, msrs->cr4_fixed1);\n\n\t/* highest index: VMX_PREEMPTION_TIMER_VALUE */\n\tmsrs->vmcs_enum = VMCS12_MAX_FIELD_INDEX << 1;\n}\n\n/*\n * if fixed0[i] == 1: val[i] must be 1\n * if fixed1[i] == 0: val[i] must be 0\n */\nstatic inline bool fixed_bits_valid(u64 val, u64 fixed0, u64 fixed1)\n{\n\treturn ((val & fixed1) | fixed0) == val;\n}\n\nstatic inline bool vmx_control_verify(u32 control, u32 low, u32 high)\n{\n\treturn fixed_bits_valid(control, low, high);\n}\n\nstatic inline u64 vmx_control_msr(u32 low, u32 high)\n{\n\treturn low | ((u64)high << 32);\n}\n\nstatic bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)\n{\n\tsuperset &= mask;\n\tsubset &= mask;\n\n\treturn (superset | subset) == superset;\n}\n\nstatic int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved =\n\t\t/* feature (except bit 48; see below) */\n\t\tBIT_ULL(49) | BIT_ULL(54) | BIT_ULL(55) |\n\t\t/* reserved */\n\t\tBIT_ULL(31) | GENMASK_ULL(47, 45) | GENMASK_ULL(63, 56);\n\tu64 vmx_basic = vmx->nested.msrs.basic;\n\n\tif (!is_bitwise_subset(vmx_basic, data, feature_and_reserved))\n\t\treturn -EINVAL;\n\n\t/*\n\t * KVM does not emulate a version of VMX that constrains physical\n\t * addresses of VMX structures (e.g. VMCS) to 32-bits.\n\t */\n\tif (data & BIT_ULL(48))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_revision_id(vmx_basic) !=\n\t    vmx_basic_vmcs_revision_id(data))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_size(vmx_basic) > vmx_basic_vmcs_size(data))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.basic = data;\n\treturn 0;\n}\n\nstatic int\nvmx_restore_control_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tu64 supported;\n\tu32 *lowp, *highp;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\t\tlowp = &vmx->nested.msrs.pinbased_ctls_low;\n\t\thighp = &vmx->nested.msrs.pinbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\t\tlowp = &vmx->nested.msrs.procbased_ctls_low;\n\t\thighp = &vmx->nested.msrs.procbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\t\tlowp = &vmx->nested.msrs.exit_ctls_low;\n\t\thighp = &vmx->nested.msrs.exit_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\t\tlowp = &vmx->nested.msrs.entry_ctls_low;\n\t\thighp = &vmx->nested.msrs.entry_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\tlowp = &vmx->nested.msrs.secondary_ctls_low;\n\t\thighp = &vmx->nested.msrs.secondary_ctls_high;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsupported = vmx_control_msr(*lowp, *highp);\n\n\t/* Check must-be-1 bits are still 1. */\n\tif (!is_bitwise_subset(data, supported, GENMASK_ULL(31, 0)))\n\t\treturn -EINVAL;\n\n\t/* Check must-be-0 bits are still 0. */\n\tif (!is_bitwise_subset(supported, data, GENMASK_ULL(63, 32)))\n\t\treturn -EINVAL;\n\n\t*lowp = data;\n\t*highp = data >> 32;\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_misc(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved_bits =\n\t\t/* feature */\n\t\tBIT_ULL(5) | GENMASK_ULL(8, 6) | BIT_ULL(14) | BIT_ULL(15) |\n\t\tBIT_ULL(28) | BIT_ULL(29) | BIT_ULL(30) |\n\t\t/* reserved */\n\t\tGENMASK_ULL(13, 9) | BIT_ULL(31);\n\tu64 vmx_misc;\n\n\tvmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,\n\t\t\t\t   vmx->nested.msrs.misc_high);\n\n\tif (!is_bitwise_subset(vmx_misc, data, feature_and_reserved_bits))\n\t\treturn -EINVAL;\n\n\tif ((vmx->nested.msrs.pinbased_ctls_high &\n\t     PIN_BASED_VMX_PREEMPTION_TIMER) &&\n\t    vmx_misc_preemption_timer_rate(data) !=\n\t    vmx_misc_preemption_timer_rate(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_cr3_count(data) > vmx_misc_cr3_count(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_max_msr(data) > vmx_misc_max_msr(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_mseg_revid(data) != vmx_misc_mseg_revid(vmx_misc))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.misc_low = data;\n\tvmx->nested.msrs.misc_high = data >> 32;\n\n\t/*\n\t * If L1 has read-only VM-exit information fields, use the\n\t * less permissive vmx_vmwrite_bitmap to specify write\n\t * permissions for the shadow VMCS.\n\t */\n\tif (enable_shadow_vmcs && !nested_cpu_has_vmwrite_any_field(&vmx->vcpu))\n\t\tvmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));\n\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_ept_vpid_cap(struct vcpu_vmx *vmx, u64 data)\n{\n\tu64 vmx_ept_vpid_cap;\n\n\tvmx_ept_vpid_cap = vmx_control_msr(vmx->nested.msrs.ept_caps,\n\t\t\t\t\t   vmx->nested.msrs.vpid_caps);\n\n\t/* Every bit is either reserved or a feature bit. */\n\tif (!is_bitwise_subset(vmx_ept_vpid_cap, data, -1ULL))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.ept_caps = data;\n\tvmx->nested.msrs.vpid_caps = data >> 32;\n\treturn 0;\n}\n\nstatic int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tu64 *msr;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\tmsr = &vmx->nested.msrs.cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\tmsr = &vmx->nested.msrs.cr4_fixed0;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\t/*\n\t * 1 bits (which indicates bits which \"must-be-1\" during VMX operation)\n\t * must be 1 in the restored value.\n\t */\n\tif (!is_bitwise_subset(data, *msr, -1ULL))\n\t\treturn -EINVAL;\n\n\t*msr = data;\n\treturn 0;\n}\n\n/*\n * Called when userspace is restoring VMX MSRs.\n *\n * Returns 0 on success, non-0 otherwise.\n */\nstatic int vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t/*\n\t * Don't allow changes to the VMX capability MSRs while the vCPU\n\t * is in VMX operation.\n\t */\n\tif (vmx->nested.vmxon)\n\t\treturn -EBUSY;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\treturn vmx_restore_vmx_basic(vmx, data);\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t/*\n\t\t * The \"non-true\" VMX capability MSRs are generated from the\n\t\t * \"true\" MSRs, so we do not support restoring them directly.\n\t\t *\n\t\t * If userspace wants to emulate VMX_BASIC[55]=0, userspace\n\t\t * should restore the \"true\" MSRs with the must-be-1 bits\n\t\t * set according to the SDM Vol 3. A.2 \"RESERVED CONTROLS AND\n\t\t * DEFAULT SETTINGS\".\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\treturn vmx_restore_control_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_MISC:\n\t\treturn vmx_restore_vmx_misc(vmx, data);\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn vmx_restore_fixed0_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t/*\n\t\t * These MSRs are generated based on the vCPU's CPUID, so we\n\t\t * do not support restoring them directly.\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\treturn vmx_restore_vmx_ept_vpid_cap(vmx, data);\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\tvmx->nested.msrs.vmcs_enum = data;\n\t\treturn 0;\n\tdefault:\n\t\t/*\n\t\t * The rest of the VMX capability MSRs do not support restore.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Returns 0 on success, non-0 otherwise. */\nstatic int vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)\n{\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\t*pdata = msrs->basic;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->pinbased_ctls_low,\n\t\t\tmsrs->pinbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PINBASED_CTLS)\n\t\t\t*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->procbased_ctls_low,\n\t\t\tmsrs->procbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)\n\t\t\t*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->exit_ctls_low,\n\t\t\tmsrs->exit_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_EXIT_CTLS)\n\t\t\t*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->entry_ctls_low,\n\t\t\tmsrs->entry_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_ENTRY_CTLS)\n\t\t\t*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_MISC:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->misc_low,\n\t\t\tmsrs->misc_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\t*pdata = msrs->cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\t\t*pdata = msrs->cr0_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\t*pdata = msrs->cr4_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t*pdata = msrs->cr4_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\t*pdata = msrs->vmcs_enum;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->secondary_ctls_low,\n\t\t\tmsrs->secondary_ctls_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\t*pdata = msrs->ept_caps |\n\t\t\t((u64)msrs->vpid_caps << 32);\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\t*pdata = msrs->vmfunc_controls;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t uint64_t val)\n{\n\tuint64_t valid_bits = to_vmx(vcpu)->msr_ia32_feature_control_valid_bits;\n\n\treturn !(val & ~valid_bits);\n}\n\nstatic int vmx_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!nested)\n\t\t\treturn 1;\n\t\treturn vmx_get_vmx_msr(&vmcs_config.nested, msr->index, &msr->data);\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Reads an msr value (of 'msr_index') into 'pdata'.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\n\tswitch (msr_info->index) {\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_FS_BASE);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_GS_BASE);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tmsr_info->data = vmx->msr_guest_kernel_gs_base;\n\t\tbreak;\n#endif\n\tcase MSR_EFER:\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = to_vmx(vcpu)->spec_ctrl;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))\n\t\t\treturn 1;\n\t\tmsr_info->data = to_vmx(vcpu)->arch_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tmsr_info->data = vmcs_read32(GUEST_SYSENTER_CS);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmcs_read64(GUEST_BNDCFGS);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !(vmx->msr_ia32_feature_control &\n\t\t      FEATURE_CONTROL_LMCE))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.mcg_ext_ctl;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tmsr_info->data = vmx->msr_ia32_feature_control;\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,\n\t\t\t\t       &msr_info->data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.ia32_xss;\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_info->index);\n\t\tif (msr) {\n\t\t\tmsr_info->data = msr->data;\n\t\t\tbreak;\n\t\t}\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_leave_nested(struct kvm_vcpu *vcpu);\n\n/*\n * Writes msr value into into the appropriate \"register\".\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\tint ret = 0;\n\tu32 msr_index = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr_index) {\n\tcase MSR_EFER:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_FS_BASE, data);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_GS_BASE, data);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tvmx->msr_guest_kernel_gs_base = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tvmcs_write32(GUEST_SYSENTER_CS, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, data);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data & PAGE_MASK, vcpu) ||\n\t\t    (data & MSR_IA32_BNDCFGS_RSVD))\n\t\t\treturn 1;\n\t\tvmcs_write64(GUEST_BNDCFGS, data);\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\t/* The STIBP bit doesn't fault even if it's not advertised */\n\t\tif (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP))\n\t\t\treturn 1;\n\n\t\tvmx->spec_ctrl = data;\n\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * For non-nested:\n\t\t * When it's written (to non-zero) for the first time, pass\n\t\t * it through.\n\t\t *\n\t\t * For nested:\n\t\t * The handling of the MSR bitmap for L2 guests is done in\n\t\t * nested_vmx_merge_msr_bitmap. We should not touch the\n\t\t * vmcs02.msr_bitmap here since it gets completely overwritten\n\t\t * in the merging. We update the vmcs01 here for L1 as well\n\t\t * since it will end up touching the MSR anyway now.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap,\n\t\t\t\t\t      MSR_IA32_SPEC_CTRL,\n\t\t\t\t\t      MSR_TYPE_RW);\n\t\tbreak;\n\tcase MSR_IA32_PRED_CMD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBPB) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\tif (data & ~PRED_CMD_IBPB)\n\t\t\treturn 1;\n\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\twrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);\n\n\t\t/*\n\t\t * For non-nested:\n\t\t * When it's written (to non-zero) for the first time, pass\n\t\t * it through.\n\t\t *\n\t\t * For nested:\n\t\t * The handling of the MSR bitmap for L2 guests is done in\n\t\t * nested_vmx_merge_msr_bitmap. We should not touch the\n\t\t * vmcs02.msr_bitmap here since it gets completely overwritten\n\t\t * in the merging.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap, MSR_IA32_PRED_CMD,\n\t\t\t\t\t      MSR_TYPE_W);\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvmx->arch_capabilities = data;\n\t\tbreak;\n\tcase MSR_IA32_CR_PAT:\n\t\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\t\tif (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))\n\t\t\t\treturn 1;\n\t\t\tvmcs_write64(GUEST_IA32_PAT, data);\n\t\t\tvcpu->arch.pat = data;\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif ((!msr_info->host_initiated &&\n\t\t     !(to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t       FEATURE_CONTROL_LMCE)) ||\n\t\t    (data & ~MCG_EXT_CTL_LMCE_EN))\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ext_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tif (!vmx_feature_control_msr_valid(vcpu, data) ||\n\t\t    (to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tvmx->msr_ia32_feature_control = data;\n\t\tif (msr_info->host_initiated && data == 0)\n\t\t\tvmx_leave_nested(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1; /* they are read-only */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_set_vmx_msr(vcpu, msr_index, data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\t/*\n\t\t * The only supported bit as of Skylake is bit 8, but\n\t\t * it is not supported on KVM.\n\t\t */\n\t\tif (data != 0)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tif (vcpu->arch.ia32_xss != host_xss)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_IA32_XSS,\n\t\t\t\tvcpu->arch.ia32_xss, host_xss);\n\t\telse\n\t\t\tclear_atomic_switch_msr(vmx, MSR_IA32_XSS);\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Check reserved bit, higher 32 bits should be zero */\n\t\tif ((data >> 32) != 0)\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_index);\n\t\tif (msr) {\n\t\t\tu64 old_msr_data = msr->data;\n\t\t\tmsr->data = data;\n\t\t\tif (msr - vmx->guest_msrs < vmx->save_nmsrs) {\n\t\t\t\tpreempt_disable();\n\t\t\t\tret = kvm_set_shared_msr(msr->index, msr->data,\n\t\t\t\t\t\t\t msr->mask);\n\t\t\t\tpreempt_enable();\n\t\t\t\tif (ret)\n\t\t\t\t\tmsr->data = old_msr_data;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t}\n\n\treturn ret;\n}\n\nstatic void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)\n{\n\t__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n\tswitch (reg) {\n\tcase VCPU_REGS_RSP:\n\t\tvcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);\n\t\tbreak;\n\tcase VCPU_REGS_RIP:\n\t\tvcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);\n\t\tbreak;\n\tcase VCPU_EXREG_PDPTR:\n\t\tif (enable_ept)\n\t\t\tept_save_pdptrs(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic __init int cpu_has_kvm_support(void)\n{\n\treturn cpu_has_vmx();\n}\n\nstatic __init int vmx_disabled_by_bios(void)\n{\n\tu64 msr;\n\n\trdmsrl(MSR_IA32_FEATURE_CONTROL, msr);\n\tif (msr & FEATURE_CONTROL_LOCKED) {\n\t\t/* launched w/ TXT and VMX disabled */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)\n\t\t\t&& tboot_enabled())\n\t\t\treturn 1;\n\t\t/* launched w/o TXT and VMX only enabled w/ TXT */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)\n\t\t\t&& (msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)\n\t\t\t&& !tboot_enabled()) {\n\t\t\tprintk(KERN_WARNING \"kvm: disable TXT in the BIOS or \"\n\t\t\t\t\"activate TXT before enabling KVM\\n\");\n\t\t\treturn 1;\n\t\t}\n\t\t/* launched w/o TXT and VMX disabled */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)\n\t\t\t&& !tboot_enabled())\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void kvm_cpu_vmxon(u64 addr)\n{\n\tcr4_set_bits(X86_CR4_VMXE);\n\tintel_pt_handle_vmx(1);\n\n\tasm volatile (ASM_VMX_VMXON_RAX\n\t\t\t: : \"a\"(&addr), \"m\"(addr)\n\t\t\t: \"memory\", \"cc\");\n}\n\nstatic int hardware_enable(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tu64 phys_addr = __pa(per_cpu(vmxarea, cpu));\n\tu64 old, test_bits;\n\n\tif (cr4_read_shadow() & X86_CR4_VMXE)\n\t\treturn -EBUSY;\n\n\t/*\n\t * This can happen if we hot-added a CPU but failed to allocate\n\t * VP assist page for it.\n\t */\n\tif (static_branch_unlikely(&enable_evmcs) &&\n\t    !hv_get_vp_assist_page(cpu))\n\t\treturn -EFAULT;\n\n\tINIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));\n\tINIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));\n\tspin_lock_init(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n\n\t/*\n\t * Now we can enable the vmclear operation in kdump\n\t * since the loaded_vmcss_on_cpu list on this cpu\n\t * has been initialized.\n\t *\n\t * Though the cpu is not in VMX operation now, there\n\t * is no problem to enable the vmclear operation\n\t * for the loaded_vmcss_on_cpu list is empty!\n\t */\n\tcrash_enable_local_vmclear(cpu);\n\n\trdmsrl(MSR_IA32_FEATURE_CONTROL, old);\n\n\ttest_bits = FEATURE_CONTROL_LOCKED;\n\ttest_bits |= FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\tif (tboot_enabled())\n\t\ttest_bits |= FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX;\n\n\tif ((old & test_bits) != test_bits) {\n\t\t/* enable and lock */\n\t\twrmsrl(MSR_IA32_FEATURE_CONTROL, old | test_bits);\n\t}\n\tkvm_cpu_vmxon(phys_addr);\n\tif (enable_ept)\n\t\tept_sync_global();\n\n\treturn 0;\n}\n\nstatic void vmclear_local_loaded_vmcss(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v, *n;\n\n\tlist_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t\t loaded_vmcss_on_cpu_link)\n\t\t__loaded_vmcs_clear(v);\n}\n\n\n/* Just like cpu_vmxoff(), but with the __kvm_handle_fault_on_reboot()\n * tricks.\n */\nstatic void kvm_cpu_vmxoff(void)\n{\n\tasm volatile (__ex(ASM_VMX_VMXOFF) : : : \"cc\");\n\n\tintel_pt_handle_vmx(0);\n\tcr4_clear_bits(X86_CR4_VMXE);\n}\n\nstatic void hardware_disable(void)\n{\n\tvmclear_local_loaded_vmcss();\n\tkvm_cpu_vmxoff();\n}\n\nstatic __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,\n\t\t\t\t      u32 msr, u32 *result)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 ctl = ctl_min | ctl_opt;\n\n\trdmsr(msr, vmx_msr_low, vmx_msr_high);\n\n\tctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */\n\tctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */\n\n\t/* Ensure minimum (required) set of control bits are supported. */\n\tif (ctl_min & ~ctl)\n\t\treturn -EIO;\n\n\t*result = ctl;\n\treturn 0;\n}\n\nstatic __init bool allow_1_setting(u32 msr, u32 ctl)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\n\trdmsr(msr, vmx_msr_low, vmx_msr_high);\n\treturn vmx_msr_high & ctl;\n}\n\nstatic __init int setup_vmcs_config(struct vmcs_config *vmcs_conf)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 min, opt, min2, opt2;\n\tu32 _pin_based_exec_control = 0;\n\tu32 _cpu_based_exec_control = 0;\n\tu32 _cpu_based_2nd_exec_control = 0;\n\tu32 _vmexit_control = 0;\n\tu32 _vmentry_control = 0;\n\n\tmemset(vmcs_conf, 0, sizeof(*vmcs_conf));\n\tmin = CPU_BASED_HLT_EXITING |\n#ifdef CONFIG_X86_64\n\t      CPU_BASED_CR8_LOAD_EXITING |\n\t      CPU_BASED_CR8_STORE_EXITING |\n#endif\n\t      CPU_BASED_CR3_LOAD_EXITING |\n\t      CPU_BASED_CR3_STORE_EXITING |\n\t      CPU_BASED_UNCOND_IO_EXITING |\n\t      CPU_BASED_MOV_DR_EXITING |\n\t      CPU_BASED_USE_TSC_OFFSETING |\n\t      CPU_BASED_MWAIT_EXITING |\n\t      CPU_BASED_MONITOR_EXITING |\n\t      CPU_BASED_INVLPG_EXITING |\n\t      CPU_BASED_RDPMC_EXITING;\n\n\topt = CPU_BASED_TPR_SHADOW |\n\t      CPU_BASED_USE_MSR_BITMAPS |\n\t      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,\n\t\t\t\t&_cpu_based_exec_control) < 0)\n\t\treturn -EIO;\n#ifdef CONFIG_X86_64\n\tif ((_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))\n\t\t_cpu_based_exec_control &= ~CPU_BASED_CR8_LOAD_EXITING &\n\t\t\t\t\t   ~CPU_BASED_CR8_STORE_EXITING;\n#endif\n\tif (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {\n\t\tmin2 = 0;\n\t\topt2 = SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\tSECONDARY_EXEC_WBINVD_EXITING |\n\t\t\tSECONDARY_EXEC_ENABLE_VPID |\n\t\t\tSECONDARY_EXEC_ENABLE_EPT |\n\t\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST |\n\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING |\n\t\t\tSECONDARY_EXEC_DESC |\n\t\t\tSECONDARY_EXEC_RDTSCP |\n\t\t\tSECONDARY_EXEC_ENABLE_INVPCID |\n\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\t\tSECONDARY_EXEC_SHADOW_VMCS |\n\t\t\tSECONDARY_EXEC_XSAVES |\n\t\t\tSECONDARY_EXEC_RDSEED_EXITING |\n\t\t\tSECONDARY_EXEC_RDRAND_EXITING |\n\t\t\tSECONDARY_EXEC_ENABLE_PML |\n\t\t\tSECONDARY_EXEC_TSC_SCALING |\n\t\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n\t\tif (adjust_vmx_controls(min2, opt2,\n\t\t\t\t\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\t\t\t\t\t&_cpu_based_2nd_exec_control) < 0)\n\t\t\treturn -EIO;\n\t}\n#ifndef CONFIG_X86_64\n\tif (!(_cpu_based_2nd_exec_control &\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\t_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;\n#endif\n\n\tif (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))\n\t\t_cpu_based_2nd_exec_control &= ~(\n\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\n\trdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,\n\t\t&vmx_capability.ept, &vmx_capability.vpid);\n\n\tif (_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) {\n\t\t/* CR3 accesses and invlpg don't need to cause VM Exits when EPT\n\t\t   enabled */\n\t\t_cpu_based_exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t\t\t     CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\t\t     CPU_BASED_INVLPG_EXITING);\n\t} else if (vmx_capability.ept) {\n\t\tvmx_capability.ept = 0;\n\t\tpr_warn_once(\"EPT CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable EPT VM-execution control\\n\");\n\t}\n\tif (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&\n\t\tvmx_capability.vpid) {\n\t\tvmx_capability.vpid = 0;\n\t\tpr_warn_once(\"VPID CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable VPID VM-execution control\\n\");\n\t}\n\n\tmin = VM_EXIT_SAVE_DEBUG_CONTROLS | VM_EXIT_ACK_INTR_ON_EXIT;\n#ifdef CONFIG_X86_64\n\tmin |= VM_EXIT_HOST_ADDR_SPACE_SIZE;\n#endif\n\topt = VM_EXIT_SAVE_IA32_PAT | VM_EXIT_LOAD_IA32_PAT |\n\t\tVM_EXIT_CLEAR_BNDCFGS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t&_vmexit_control) < 0)\n\t\treturn -EIO;\n\n\tmin = PIN_BASED_EXT_INTR_MASK | PIN_BASED_NMI_EXITING;\n\topt = PIN_BASED_VIRTUAL_NMIS | PIN_BASED_POSTED_INTR |\n\t\t PIN_BASED_VMX_PREEMPTION_TIMER;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,\n\t\t\t\t&_pin_based_exec_control) < 0)\n\t\treturn -EIO;\n\n\tif (cpu_has_broken_vmx_preemption_timer())\n\t\t_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\tif (!(_cpu_based_2nd_exec_control &\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))\n\t\t_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;\n\n\tmin = VM_ENTRY_LOAD_DEBUG_CONTROLS;\n\topt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\t&_vmentry_control) < 0)\n\t\treturn -EIO;\n\n\trdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);\n\n\t/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */\n\tif ((vmx_msr_high & 0x1fff) > PAGE_SIZE)\n\t\treturn -EIO;\n\n#ifdef CONFIG_X86_64\n\t/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */\n\tif (vmx_msr_high & (1u<<16))\n\t\treturn -EIO;\n#endif\n\n\t/* Require Write-Back (WB) memory type for VMCS accesses. */\n\tif (((vmx_msr_high >> 18) & 15) != 6)\n\t\treturn -EIO;\n\n\tvmcs_conf->size = vmx_msr_high & 0x1fff;\n\tvmcs_conf->order = get_order(vmcs_conf->size);\n\tvmcs_conf->basic_cap = vmx_msr_high & ~0x1fff;\n\n\t/* KVM supports Enlightened VMCS v1 only */\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tvmcs_conf->revision_id = KVM_EVMCS_VERSION;\n\telse\n\t\tvmcs_conf->revision_id = vmx_msr_low;\n\n\tvmcs_conf->pin_based_exec_ctrl = _pin_based_exec_control;\n\tvmcs_conf->cpu_based_exec_ctrl = _cpu_based_exec_control;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;\n\tvmcs_conf->vmexit_ctrl         = _vmexit_control;\n\tvmcs_conf->vmentry_ctrl        = _vmentry_control;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_sanitize_exec_ctrls(vmcs_conf);\n\n\tcpu_has_load_ia32_efer =\n\t\tallow_1_setting(MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\tVM_ENTRY_LOAD_IA32_EFER)\n\t\t&& allow_1_setting(MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t   VM_EXIT_LOAD_IA32_EFER);\n\n\tcpu_has_load_perf_global_ctrl =\n\t\tallow_1_setting(MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\t&& allow_1_setting(MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t   VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);\n\n\t/*\n\t * Some cpus support VM_ENTRY_(LOAD|SAVE)_IA32_PERF_GLOBAL_CTRL\n\t * but due to errata below it can't be used. Workaround is to use\n\t * msr load mechanism to switch IA32_PERF_GLOBAL_CTRL.\n\t *\n\t * VM Exit May Incorrectly Clear IA32_PERF_GLOBAL_CTRL [34:32]\n\t *\n\t * AAK155             (model 26)\n\t * AAP115             (model 30)\n\t * AAT100             (model 37)\n\t * BC86,AAY89,BD102   (model 44)\n\t * BA97               (model 46)\n\t *\n\t */\n\tif (cpu_has_load_perf_global_ctrl && boot_cpu_data.x86 == 0x6) {\n\t\tswitch (boot_cpu_data.x86_model) {\n\t\tcase 26:\n\t\tcase 30:\n\t\tcase 37:\n\t\tcase 44:\n\t\tcase 46:\n\t\t\tcpu_has_load_perf_global_ctrl = false;\n\t\t\tprintk_once(KERN_WARNING\"kvm: VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL \"\n\t\t\t\t\t\"does not work properly. Using workaround\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\trdmsrl(MSR_IA32_XSS, host_xss);\n\n\treturn 0;\n}\n\nstatic struct vmcs *alloc_vmcs_cpu(int cpu)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct page *pages;\n\tstruct vmcs *vmcs;\n\n\tpages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);\n\tif (!pages)\n\t\treturn NULL;\n\tvmcs = page_address(pages);\n\tmemset(vmcs, 0, vmcs_config.size);\n\tvmcs->revision_id = vmcs_config.revision_id; /* vmcs revision id */\n\treturn vmcs;\n}\n\nstatic void free_vmcs(struct vmcs *vmcs)\n{\n\tfree_pages((unsigned long)vmcs, vmcs_config.order);\n}\n\n/*\n * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded\n */\nstatic void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tif (!loaded_vmcs->vmcs)\n\t\treturn;\n\tloaded_vmcs_clear(loaded_vmcs);\n\tfree_vmcs(loaded_vmcs->vmcs);\n\tloaded_vmcs->vmcs = NULL;\n\tif (loaded_vmcs->msr_bitmap)\n\t\tfree_page((unsigned long)loaded_vmcs->msr_bitmap);\n\tWARN_ON(loaded_vmcs->shadow_vmcs != NULL);\n}\n\nstatic struct vmcs *alloc_vmcs(void)\n{\n\treturn alloc_vmcs_cpu(raw_smp_processor_id());\n}\n\nstatic int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tloaded_vmcs->vmcs = alloc_vmcs();\n\tif (!loaded_vmcs->vmcs)\n\t\treturn -ENOMEM;\n\n\tloaded_vmcs->shadow_vmcs = NULL;\n\tloaded_vmcs_init(loaded_vmcs);\n\n\tif (cpu_has_vmx_msr_bitmap()) {\n\t\tloaded_vmcs->msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!loaded_vmcs->msr_bitmap)\n\t\t\tgoto out_vmcs;\n\t\tmemset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);\n\n\t\tif (static_branch_unlikely(&enable_evmcs) &&\n\t\t    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {\n\t\t\tstruct hv_enlightened_vmcs *evmcs =\n\t\t\t\t(struct hv_enlightened_vmcs *)loaded_vmcs->vmcs;\n\n\t\t\tevmcs->hv_enlightenments_control.msr_bitmap = 1;\n\t\t}\n\t}\n\treturn 0;\n\nout_vmcs:\n\tfree_loaded_vmcs(loaded_vmcs);\n\treturn -ENOMEM;\n}\n\nstatic void free_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfree_vmcs(per_cpu(vmxarea, cpu));\n\t\tper_cpu(vmxarea, cpu) = NULL;\n\t}\n}\n\nenum vmcs_field_width {\n\tVMCS_FIELD_WIDTH_U16 = 0,\n\tVMCS_FIELD_WIDTH_U64 = 1,\n\tVMCS_FIELD_WIDTH_U32 = 2,\n\tVMCS_FIELD_WIDTH_NATURAL_WIDTH = 3\n};\n\nstatic inline int vmcs_field_width(unsigned long field)\n{\n\tif (0x1 & field)\t/* the *_HIGH fields are all 32 bit */\n\t\treturn VMCS_FIELD_WIDTH_U32;\n\treturn (field >> 13) & 0x3 ;\n}\n\nstatic inline int vmcs_field_readonly(unsigned long field)\n{\n\treturn (((field >> 10) & 0x3) == 1);\n}\n\nstatic void init_vmcs_shadow_fields(void)\n{\n\tint i, j;\n\n\tfor (i = j = 0; i < max_shadow_read_only_fields; i++) {\n\t\tu16 field = shadow_read_only_fields[i];\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_only_fields ||\n\t\t     shadow_read_only_fields[i + 1] != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_only_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\tclear_bit(field, vmx_vmread_bitmap);\n#ifdef CONFIG_X86_64\n\t\tif (field & 1)\n\t\t\tcontinue;\n#endif\n\t\tif (j < i)\n\t\t\tshadow_read_only_fields[j] = field;\n\t\tj++;\n\t}\n\tmax_shadow_read_only_fields = j;\n\n\tfor (i = j = 0; i < max_shadow_read_write_fields; i++) {\n\t\tu16 field = shadow_read_write_fields[i];\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_write_fields ||\n\t\t     shadow_read_write_fields[i + 1] != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_write_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\t/*\n\t\t * PML and the preemption timer can be emulated, but the\n\t\t * processor cannot vmwrite to fields that don't exist\n\t\t * on bare metal.\n\t\t */\n\t\tswitch (field) {\n\t\tcase GUEST_PML_INDEX:\n\t\t\tif (!cpu_has_vmx_pml())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase VMX_PREEMPTION_TIMER_VALUE:\n\t\t\tif (!cpu_has_vmx_preemption_timer())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase GUEST_INTR_STATUS:\n\t\t\tif (!cpu_has_vmx_apicv())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tclear_bit(field, vmx_vmwrite_bitmap);\n\t\tclear_bit(field, vmx_vmread_bitmap);\n#ifdef CONFIG_X86_64\n\t\tif (field & 1)\n\t\t\tcontinue;\n#endif\n\t\tif (j < i)\n\t\t\tshadow_read_write_fields[j] = field;\n\t\tj++;\n\t}\n\tmax_shadow_read_write_fields = j;\n}\n\nstatic __init int alloc_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct vmcs *vmcs;\n\n\t\tvmcs = alloc_vmcs_cpu(cpu);\n\t\tif (!vmcs) {\n\t\t\tfree_kvm_area();\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tper_cpu(vmxarea, cpu) = vmcs;\n\t}\n\treturn 0;\n}\n\nstatic void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,\n\t\tstruct kvm_segment *save)\n{\n\tif (!emulate_invalid_guest_state) {\n\t\t/*\n\t\t * CS and SS RPL should be equal during guest entry according\n\t\t * to VMX spec, but in reality it is not always so. Since vcpu\n\t\t * is in the middle of the transition from real mode to\n\t\t * protected mode it is safe to assume that RPL 0 is a good\n\t\t * default value.\n\t\t */\n\t\tif (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)\n\t\t\tsave->selector &= ~SEGMENT_RPL_MASK;\n\t\tsave->dpl = save->selector & SEGMENT_RPL_MASK;\n\t\tsave->s = 1;\n\t}\n\tvmx_set_segment(vcpu, save, seg);\n}\n\nstatic void enter_pmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t/*\n\t * Update real mode segment cache. It may be not up-to-date if sement\n\t * register was written while vcpu was in a guest mode.\n\t */\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 0;\n\n\tvmx_segment_cache_clear(vmx);\n\n\tvmx_set_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tflags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\n\tvmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |\n\t\t\t(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));\n\n\tupdate_exception_bitmap(vcpu);\n\n\tfix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n}\n\nstatic void fix_rmode_seg(int seg, struct kvm_segment *save)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tstruct kvm_segment var = *save;\n\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\n\tif (!emulate_invalid_guest_state) {\n\t\tvar.selector = var.base >> 4;\n\t\tvar.base = var.base & 0xffff0;\n\t\tvar.limit = 0xffff;\n\t\tvar.g = 0;\n\t\tvar.db = 0;\n\t\tvar.present = 1;\n\t\tvar.s = 1;\n\t\tvar.l = 0;\n\t\tvar.unusable = 0;\n\t\tvar.type = 0x3;\n\t\tvar.avl = 0;\n\t\tif (save->base & 0xf)\n\t\t\tprintk_once(KERN_WARNING \"kvm: segment base is not \"\n\t\t\t\t\t\"paragraph aligned when entering \"\n\t\t\t\t\t\"protected mode (seg=%d)\", seg);\n\t}\n\n\tvmcs_write16(sf->selector, var.selector);\n\tvmcs_writel(sf->base, var.base);\n\tvmcs_write32(sf->limit, var.limit);\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));\n}\n\nstatic void enter_rmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);\n\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 1;\n\n\t/*\n\t * Very old userspace does not call KVM_SET_TSS_ADDR before entering\n\t * vcpu. Warn the user that an update is overdue.\n\t */\n\tif (!kvm_vmx->tss_addr)\n\t\tprintk_once(KERN_WARNING \"kvm: KVM_SET_TSS_ADDR need to be \"\n\t\t\t     \"called before entering vcpu\\n\");\n\n\tvmx_segment_cache_clear(vmx);\n\n\tvmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);\n\tvmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tvmx->rmode.save_rflags = flags;\n\n\tflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\tvmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);\n\tupdate_exception_bitmap(vcpu);\n\n\tfix_rmode_seg(VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_rmode_seg(VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_rmode_seg(VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_rmode_seg(VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_rmode_seg(VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n\tfix_rmode_seg(VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n\n\tkvm_mmu_reset_context(vcpu);\n}\n\nstatic void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr = find_msr_entry(vmx, MSR_EFER);\n\n\tif (!msr)\n\t\treturn;\n\n\t/*\n\t * Force kernel_gs_base reloading before EFER changes, as control\n\t * of this msr depends on is_long_mode().\n\t */\n\tvmx_load_host_state(to_vmx(vcpu));\n\tvcpu->arch.efer = efer;\n\tif (efer & EFER_LMA) {\n\t\tvm_entry_controls_setbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\t\tmsr->data = efer;\n\t} else {\n\t\tvm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\n\t\tmsr->data = efer & ~EFER_LME;\n\t}\n\tsetup_msrs(vmx);\n}\n\n#ifdef CONFIG_X86_64\n\nstatic void enter_lmode(struct kvm_vcpu *vcpu)\n{\n\tu32 guest_tr_ar;\n\n\tvmx_segment_cache_clear(to_vmx(vcpu));\n\n\tguest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);\n\tif ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {\n\t\tpr_debug_ratelimited(\"%s: tss fixup for long mode. \\n\",\n\t\t\t\t     __func__);\n\t\tvmcs_write32(GUEST_TR_AR_BYTES,\n\t\t\t     (guest_tr_ar & ~VMX_AR_TYPE_MASK)\n\t\t\t     | VMX_AR_TYPE_BUSY_64_TSS);\n\t}\n\tvmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);\n}\n\nstatic void exit_lmode(struct kvm_vcpu *vcpu)\n{\n\tvm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\tvmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);\n}\n\n#endif\n\nstatic inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,\n\t\t\t\tbool invalidate_gpa)\n{\n\tif (enable_ept && (invalidate_gpa || !enable_vpid)) {\n\t\tif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\n\t\t\treturn;\n\t\tept_sync_context(construct_eptp(vcpu, vcpu->arch.mmu.root_hpa));\n\t} else {\n\t\tvpid_sync_context(vpid);\n\t}\n}\n\nstatic void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)\n{\n\t__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);\n}\n\nstatic void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)\n{\n\tulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;\n\n\tvcpu->arch.cr0 &= ~cr0_guest_owned_bits;\n\tvcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;\n}\n\nstatic void vmx_decache_cr3(struct kvm_vcpu *vcpu)\n{\n\tif (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))\n\t\tvcpu->arch.cr3 = vmcs_readl(GUEST_CR3);\n\t__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);\n}\n\nstatic void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)\n{\n\tulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;\n\n\tvcpu->arch.cr4 &= ~cr4_guest_owned_bits;\n\tvcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & cr4_guest_owned_bits;\n}\n\nstatic void ept_load_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (!test_bit(VCPU_EXREG_PDPTR,\n\t\t      (unsigned long *)&vcpu->arch.regs_dirty))\n\t\treturn;\n\n\tif (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {\n\t\tvmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);\n\t\tvmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);\n\t\tvmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);\n\t\tvmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);\n\t}\n}\n\nstatic void ept_save_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {\n\t\tmmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);\n\t\tmmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);\n\t\tmmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);\n\t\tmmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\t__set_bit(VCPU_EXREG_PDPTR,\n\t\t  (unsigned long *)&vcpu->arch.regs_avail);\n\t__set_bit(VCPU_EXREG_PDPTR,\n\t\t  (unsigned long *)&vcpu->arch.regs_dirty);\n}\n\nstatic bool nested_guest_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (to_vmx(vcpu)->nested.msrs.secondary_ctls_high &\n\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST &&\n\t    nested_cpu_has2(vmcs12, SECONDARY_EXEC_UNRESTRICTED_GUEST))\n\t\tfixed0 &= ~(X86_CR0_PE | X86_CR0_PG);\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\nstatic bool nested_host_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\nstatic bool nested_cr4_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr4_fixed1;\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\n/* No difference in the restrictions on guest and host CR4 in VMX operation. */\n#define nested_guest_cr4_valid\tnested_cr4_valid\n#define nested_host_cr4_valid\tnested_cr4_valid\n\nstatic int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);\n\nstatic void ept_update_paging_mode_cr0(unsigned long *hw_cr0,\n\t\t\t\t\tunsigned long cr0,\n\t\t\t\t\tstruct kvm_vcpu *vcpu)\n{\n\tif (!test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))\n\t\tvmx_decache_cr3(vcpu);\n\tif (!(cr0 & X86_CR0_PG)) {\n\t\t/* From paging/starting to nonpaging */\n\t\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) |\n\t\t\t     (CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t      CPU_BASED_CR3_STORE_EXITING));\n\t\tvcpu->arch.cr0 = cr0;\n\t\tvmx_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\t} else if (!is_paging(vcpu)) {\n\t\t/* From nonpaging to paging */\n\t\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) &\n\t\t\t     ~(CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t       CPU_BASED_CR3_STORE_EXITING));\n\t\tvcpu->arch.cr0 = cr0;\n\t\tvmx_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\t}\n\n\tif (!(cr0 & X86_CR0_WP))\n\t\t*hw_cr0 &= ~X86_CR0_WP;\n}\n\nstatic void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long hw_cr0;\n\n\thw_cr0 = (cr0 & ~KVM_GUEST_CR0_MASK);\n\tif (enable_unrestricted_guest)\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse {\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON;\n\n\t\tif (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))\n\t\t\tenter_pmode(vcpu);\n\n\t\tif (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))\n\t\t\tenter_rmode(vcpu);\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (vcpu->arch.efer & EFER_LME) {\n\t\tif (!is_paging(vcpu) && (cr0 & X86_CR0_PG))\n\t\t\tenter_lmode(vcpu);\n\t\tif (is_paging(vcpu) && !(cr0 & X86_CR0_PG))\n\t\t\texit_lmode(vcpu);\n\t}\n#endif\n\n\tif (enable_ept && !enable_unrestricted_guest)\n\t\tept_update_paging_mode_cr0(&hw_cr0, cr0, vcpu);\n\n\tvmcs_writel(CR0_READ_SHADOW, cr0);\n\tvmcs_writel(GUEST_CR0, hw_cr0);\n\tvcpu->arch.cr0 = cr0;\n\n\t/* depends on vcpu->arch.cr0 to be set to a new value */\n\tvmx->emulation_required = emulation_required(vcpu);\n}\n\nstatic int get_ept_level(struct kvm_vcpu *vcpu)\n{\n\tif (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))\n\t\treturn 5;\n\treturn 4;\n}\n\nstatic u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)\n{\n\tu64 eptp = VMX_EPTP_MT_WB;\n\n\teptp |= (get_ept_level(vcpu) == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;\n\n\tif (enable_ept_ad_bits &&\n\t    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))\n\t\teptp |= VMX_EPTP_AD_ENABLE_BIT;\n\teptp |= (root_hpa & PAGE_MASK);\n\n\treturn eptp;\n}\n\nstatic void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tunsigned long guest_cr3;\n\tu64 eptp;\n\n\tguest_cr3 = cr3;\n\tif (enable_ept) {\n\t\teptp = construct_eptp(vcpu, cr3);\n\t\tvmcs_write64(EPT_POINTER, eptp);\n\t\tif (enable_unrestricted_guest || is_paging(vcpu) ||\n\t\t    is_guest_mode(vcpu))\n\t\t\tguest_cr3 = kvm_read_cr3(vcpu);\n\t\telse\n\t\t\tguest_cr3 = to_kvm_vmx(vcpu->kvm)->ept_identity_map_addr;\n\t\tept_load_pdptrs(vcpu);\n\t}\n\n\tvmx_flush_tlb(vcpu, true);\n\tvmcs_writel(GUEST_CR3, guest_cr3);\n}\n\nstatic int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\t/*\n\t * Pass through host's Machine Check Enable value to hw_cr4, which\n\t * is in force while we are in guest mode.  Do not let guests control\n\t * this bit, even if host CR4.MCE == 0.\n\t */\n\tunsigned long hw_cr4;\n\n\thw_cr4 = (cr4_read_shadow() & X86_CR4_MCE) | (cr4 & ~X86_CR4_MCE);\n\tif (enable_unrestricted_guest)\n\t\thw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse if (to_vmx(vcpu)->rmode.vm86_active)\n\t\thw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;\n\telse\n\t\thw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;\n\n\tif (!boot_cpu_has(X86_FEATURE_UMIP) && vmx_umip_emulated()) {\n\t\tif (cr4 & X86_CR4_UMIP) {\n\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\tSECONDARY_EXEC_DESC);\n\t\t\thw_cr4 &= ~X86_CR4_UMIP;\n\t\t} else if (!is_guest_mode(vcpu) ||\n\t\t\t!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC))\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_DESC);\n\t}\n\n\tif (cr4 & X86_CR4_VMXE) {\n\t\t/*\n\t\t * To use VMXON (and later other VMX instructions), a guest\n\t\t * must first be able to turn on cr4.VMXE (see handle_vmon()).\n\t\t * So basically the check on whether to allow nested VMX\n\t\t * is here.\n\t\t */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tif (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))\n\t\treturn 1;\n\n\tvcpu->arch.cr4 = cr4;\n\n\tif (!enable_unrestricted_guest) {\n\t\tif (enable_ept) {\n\t\t\tif (!is_paging(vcpu)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t\thw_cr4 |= X86_CR4_PSE;\n\t\t\t} else if (!(cr4 & X86_CR4_PAE)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in\n\t\t * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs\n\t\t * to be manually disabled when guest switches to non-paging\n\t\t * mode.\n\t\t *\n\t\t * If !enable_unrestricted_guest, the CPU is always running\n\t\t * with CR0.PG=1 and CR4 needs to be modified.\n\t\t * If enable_unrestricted_guest, the CPU automatically\n\t\t * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.\n\t\t */\n\t\tif (!is_paging(vcpu))\n\t\t\thw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);\n\t}\n\n\tvmcs_writel(CR4_READ_SHADOW, cr4);\n\tvmcs_writel(GUEST_CR4, hw_cr4);\n\treturn 0;\n}\n\nstatic void vmx_get_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 ar;\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\t*var = vmx->rmode.segs[seg];\n\t\tif (seg == VCPU_SREG_TR\n\t\t    || var->selector == vmx_read_guest_seg_selector(vmx, seg))\n\t\t\treturn;\n\t\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\t\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\t\treturn;\n\t}\n\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\tvar->limit = vmx_read_guest_seg_limit(vmx, seg);\n\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\tar = vmx_read_guest_seg_ar(vmx, seg);\n\tvar->unusable = (ar >> 16) & 1;\n\tvar->type = ar & 15;\n\tvar->s = (ar >> 4) & 1;\n\tvar->dpl = (ar >> 5) & 3;\n\t/*\n\t * Some userspaces do not preserve unusable property. Since usable\n\t * segment has to be present according to VMX spec we can use present\n\t * property to amend userspace bug by making unusable segment always\n\t * nonpresent. vmx_segment_access_rights() already marks nonpresent\n\t * segment as unusable.\n\t */\n\tvar->present = !var->unusable;\n\tvar->avl = (ar >> 12) & 1;\n\tvar->l = (ar >> 13) & 1;\n\tvar->db = (ar >> 14) & 1;\n\tvar->g = (ar >> 15) & 1;\n}\n\nstatic u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment s;\n\n\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\tvmx_get_segment(vcpu, &s, seg);\n\t\treturn s.base;\n\t}\n\treturn vmx_read_guest_seg_base(to_vmx(vcpu), seg);\n}\n\nstatic int vmx_get_cpl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (unlikely(vmx->rmode.vm86_active))\n\t\treturn 0;\n\telse {\n\t\tint ar = vmx_read_guest_seg_ar(vmx, VCPU_SREG_SS);\n\t\treturn VMX_AR_DPL(ar);\n\t}\n}\n\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var)\n{\n\tu32 ar;\n\n\tif (var->unusable || !var->present)\n\t\tar = 1 << 16;\n\telse {\n\t\tar = var->type & 15;\n\t\tar |= (var->s & 1) << 4;\n\t\tar |= (var->dpl & 3) << 5;\n\t\tar |= (var->present & 1) << 7;\n\t\tar |= (var->avl & 1) << 12;\n\t\tar |= (var->l & 1) << 13;\n\t\tar |= (var->db & 1) << 14;\n\t\tar |= (var->g & 1) << 15;\n\t}\n\n\treturn ar;\n}\n\nstatic void vmx_set_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\n\tvmx_segment_cache_clear(vmx);\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\tvmx->rmode.segs[seg] = *var;\n\t\tif (seg == VCPU_SREG_TR)\n\t\t\tvmcs_write16(sf->selector, var->selector);\n\t\telse if (var->s)\n\t\t\tfix_rmode_seg(seg, &vmx->rmode.segs[seg]);\n\t\tgoto out;\n\t}\n\n\tvmcs_writel(sf->base, var->base);\n\tvmcs_write32(sf->limit, var->limit);\n\tvmcs_write16(sf->selector, var->selector);\n\n\t/*\n\t *   Fix the \"Accessed\" bit in AR field of segment registers for older\n\t * qemu binaries.\n\t *   IA32 arch specifies that at the time of processor reset the\n\t * \"Accessed\" bit in the AR field of segment registers is 1. And qemu\n\t * is setting it to 0 in the userland code. This causes invalid guest\n\t * state vmexit when \"unrestricted guest\" mode is turned on.\n\t *    Fix for this setup issue in cpu_reset is being pushed in the qemu\n\t * tree. Newer qemu binaries with that qemu fix would not need this\n\t * kvm hack.\n\t */\n\tif (enable_unrestricted_guest && (seg != VCPU_SREG_LDTR))\n\t\tvar->type |= 0x1; /* Accessed */\n\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(var));\n\nout:\n\tvmx->emulation_required = emulation_required(vcpu);\n}\n\nstatic void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)\n{\n\tu32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);\n\n\t*db = (ar >> 14) & 1;\n\t*l = (ar >> 13) & 1;\n}\n\nstatic void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_IDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_IDTR_BASE);\n}\n\nstatic void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_IDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_IDTR_BASE, dt->address);\n}\n\nstatic void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_GDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_GDTR_BASE);\n}\n\nstatic void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_GDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_GDTR_BASE, dt->address);\n}\n\nstatic bool rmode_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tu32 ar;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\tar = vmx_segment_access_rights(&var);\n\n\tif (var.base != (var.selector << 4))\n\t\treturn false;\n\tif (var.limit != 0xffff)\n\t\treturn false;\n\tif (ar != 0xf3)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool code_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs;\n\tunsigned int cs_rpl;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs_rpl = cs.selector & SEGMENT_RPL_MASK;\n\n\tif (cs.unusable)\n\t\treturn false;\n\tif (~cs.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_ACCESSES_MASK))\n\t\treturn false;\n\tif (!cs.s)\n\t\treturn false;\n\tif (cs.type & VMX_AR_TYPE_WRITEABLE_MASK) {\n\t\tif (cs.dpl > cs_rpl)\n\t\t\treturn false;\n\t} else {\n\t\tif (cs.dpl != cs_rpl)\n\t\t\treturn false;\n\t}\n\tif (!cs.present)\n\t\treturn false;\n\n\t/* TODO: Add Reserved field check, this'll require a new member in the kvm_segment_field structure */\n\treturn true;\n}\n\nstatic bool stack_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ss;\n\tunsigned int ss_rpl;\n\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\tss_rpl = ss.selector & SEGMENT_RPL_MASK;\n\n\tif (ss.unusable)\n\t\treturn true;\n\tif (ss.type != 3 && ss.type != 7)\n\t\treturn false;\n\tif (!ss.s)\n\t\treturn false;\n\tif (ss.dpl != ss_rpl) /* DPL != RPL */\n\t\treturn false;\n\tif (!ss.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool data_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tunsigned int rpl;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\trpl = var.selector & SEGMENT_RPL_MASK;\n\n\tif (var.unusable)\n\t\treturn true;\n\tif (!var.s)\n\t\treturn false;\n\tif (!var.present)\n\t\treturn false;\n\tif (~var.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_WRITEABLE_MASK)) {\n\t\tif (var.dpl < rpl) /* DPL < RPL */\n\t\t\treturn false;\n\t}\n\n\t/* TODO: Add other members to kvm_segment_field to allow checking for other access\n\t * rights flags\n\t */\n\treturn true;\n}\n\nstatic bool tr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment tr;\n\n\tvmx_get_segment(vcpu, &tr, VCPU_SREG_TR);\n\n\tif (tr.unusable)\n\t\treturn false;\n\tif (tr.selector & SEGMENT_TI_MASK)\t/* TI = 1 */\n\t\treturn false;\n\tif (tr.type != 3 && tr.type != 11) /* TODO: Check if guest is in IA32e mode */\n\t\treturn false;\n\tif (!tr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool ldtr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ldtr;\n\n\tvmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);\n\n\tif (ldtr.unusable)\n\t\treturn true;\n\tif (ldtr.selector & SEGMENT_TI_MASK)\t/* TI = 1 */\n\t\treturn false;\n\tif (ldtr.type != 2)\n\t\treturn false;\n\tif (!ldtr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool cs_ss_rpl_check(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs, ss;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\n\treturn ((cs.selector & SEGMENT_RPL_MASK) ==\n\t\t (ss.selector & SEGMENT_RPL_MASK));\n}\n\n/*\n * Check if guest state is valid. Returns true if valid, false if\n * not.\n * We assume that registers are always usable\n */\nstatic bool guest_state_valid(struct kvm_vcpu *vcpu)\n{\n\tif (enable_unrestricted_guest)\n\t\treturn true;\n\n\t/* real mode guest state checks */\n\tif (!is_protmode(vcpu) || (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_CS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_SS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t} else {\n\t/* protected mode guest state checks */\n\t\tif (!cs_ss_rpl_check(vcpu))\n\t\t\treturn false;\n\t\tif (!code_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!stack_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t\tif (!tr_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!ldtr_valid(vcpu))\n\t\t\treturn false;\n\t}\n\t/* TODO:\n\t * - Add checks on RIP\n\t * - Add checks on RFLAGS\n\t */\n\n\treturn true;\n}\n\nstatic bool page_address_valid(struct kvm_vcpu *vcpu, gpa_t gpa)\n{\n\treturn PAGE_ALIGNED(gpa) && !(gpa >> cpuid_maxphyaddr(vcpu));\n}\n\nstatic int init_rmode_tss(struct kvm *kvm)\n{\n\tgfn_t fn;\n\tu16 data = 0;\n\tint idx, r;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tfn = to_kvm_vmx(kvm)->tss_addr >> PAGE_SHIFT;\n\tr = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tdata = TSS_BASE_SIZE + TSS_REDIRECTION_SIZE;\n\tr = kvm_write_guest_page(kvm, fn++, &data,\n\t\t\tTSS_IOPB_BASE_OFFSET, sizeof(u16));\n\tif (r < 0)\n\t\tgoto out;\n\tr = kvm_clear_guest_page(kvm, fn++, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tr = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tdata = ~0;\n\tr = kvm_write_guest_page(kvm, fn, &data,\n\t\t\t\t RMODE_TSS_SIZE - 2 * PAGE_SIZE - 1,\n\t\t\t\t sizeof(u8));\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\treturn r;\n}\n\nstatic int init_rmode_identity_map(struct kvm *kvm)\n{\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\tint i, idx, r = 0;\n\tkvm_pfn_t identity_map_pfn;\n\tu32 tmp;\n\n\t/* Protect kvm_vmx->ept_identity_pagetable_done. */\n\tmutex_lock(&kvm->slots_lock);\n\n\tif (likely(kvm_vmx->ept_identity_pagetable_done))\n\t\tgoto out2;\n\n\tif (!kvm_vmx->ept_identity_map_addr)\n\t\tkvm_vmx->ept_identity_map_addr = VMX_EPT_IDENTITY_PAGETABLE_ADDR;\n\tidentity_map_pfn = kvm_vmx->ept_identity_map_addr >> PAGE_SHIFT;\n\n\tr = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t    kvm_vmx->ept_identity_map_addr, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out2;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tr = kvm_clear_guest_page(kvm, identity_map_pfn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\t/* Set up identity-mapping pagetable for EPT in real mode */\n\tfor (i = 0; i < PT32_ENT_PER_PAGE; i++) {\n\t\ttmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |\n\t\t\t_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);\n\t\tr = kvm_write_guest_page(kvm, identity_map_pfn,\n\t\t\t\t&tmp, i * sizeof(tmp), sizeof(tmp));\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t}\n\tkvm_vmx->ept_identity_pagetable_done = true;\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\nout2:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic void seg_setup(int seg)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tunsigned int ar;\n\n\tvmcs_write16(sf->selector, 0);\n\tvmcs_writel(sf->base, 0);\n\tvmcs_write32(sf->limit, 0xffff);\n\tar = 0x93;\n\tif (seg == VCPU_SREG_CS)\n\t\tar |= 0x08; /* code segment */\n\n\tvmcs_write32(sf->ar_bytes, ar);\n}\n\nstatic int alloc_apic_access_page(struct kvm *kvm)\n{\n\tstruct page *page;\n\tint r = 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tif (kvm->arch.apic_access_page_done)\n\t\tgoto out;\n\tr = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t    APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);\n\tif (r)\n\t\tgoto out;\n\n\tpage = gfn_to_page(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (is_error_page(page)) {\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Do not pin the page in memory, so that memory hot-unplug\n\t * is able to migrate it.\n\t */\n\tput_page(page);\n\tkvm->arch.apic_access_page_done = true;\nout:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic int allocate_vpid(void)\n{\n\tint vpid;\n\n\tif (!enable_vpid)\n\t\treturn 0;\n\tspin_lock(&vmx_vpid_lock);\n\tvpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);\n\tif (vpid < VMX_NR_VPIDS)\n\t\t__set_bit(vpid, vmx_vpid_bitmap);\n\telse\n\t\tvpid = 0;\n\tspin_unlock(&vmx_vpid_lock);\n\treturn vpid;\n}\n\nstatic void free_vpid(int vpid)\n{\n\tif (!enable_vpid || vpid == 0)\n\t\treturn;\n\tspin_lock(&vmx_vpid_lock);\n\t__clear_bit(vpid, vmx_vpid_bitmap);\n\tspin_unlock(&vmx_vpid_lock);\n}\n\nstatic void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t  u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_touch_msr_bitmap();\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-low */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-low */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-high */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-high */\n\t\t\t__clear_bit(msr, msr_bitmap + 0xc00 / f);\n\n\t}\n}\n\nstatic void __always_inline vmx_enable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_touch_msr_bitmap();\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-low */\n\t\t\t__set_bit(msr, msr_bitmap + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-low */\n\t\t\t__set_bit(msr, msr_bitmap + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-high */\n\t\t\t__set_bit(msr, msr_bitmap + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-high */\n\t\t\t__set_bit(msr, msr_bitmap + 0xc00 / f);\n\n\t}\n}\n\nstatic void __always_inline vmx_set_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t     \t\t\t      u32 msr, int type, bool value)\n{\n\tif (value)\n\t\tvmx_enable_intercept_for_msr(msr_bitmap, msr, type);\n\telse\n\t\tvmx_disable_intercept_for_msr(msr_bitmap, msr, type);\n}\n\n/*\n * If a msr is allowed by L0, we should check whether it is allowed by L1.\n * The corresponding bit will be cleared unless both of L0 and L1 allow it.\n */\nstatic void nested_vmx_disable_intercept_for_msr(unsigned long *msr_bitmap_l1,\n\t\t\t\t\t       unsigned long *msr_bitmap_nested,\n\t\t\t\t\t       u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x000 / f))\n\t\t\t/* read-low */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x800 / f))\n\t\t\t/* write-low */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x400 / f))\n\t\t\t/* read-high */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0xc00 / f))\n\t\t\t/* write-high */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0xc00 / f);\n\n\t}\n}\n\nstatic u8 vmx_msr_bitmap_mode(struct kvm_vcpu *vcpu)\n{\n\tu8 mode = 0;\n\n\tif (cpu_has_secondary_exec_ctrls() &&\n\t    (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &\n\t     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {\n\t\tmode |= MSR_BITMAP_MODE_X2APIC;\n\t\tif (enable_apicv && kvm_vcpu_apicv_active(vcpu))\n\t\t\tmode |= MSR_BITMAP_MODE_X2APIC_APICV;\n\t}\n\n\tif (is_long_mode(vcpu))\n\t\tmode |= MSR_BITMAP_MODE_LM;\n\n\treturn mode;\n}\n\n#define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))\n\nstatic void vmx_update_msr_bitmap_x2apic(unsigned long *msr_bitmap,\n\t\t\t\t\t u8 mode)\n{\n\tint msr;\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\tunsigned word = msr / BITS_PER_LONG;\n\t\tmsr_bitmap[word] = (mode & MSR_BITMAP_MODE_X2APIC_APICV) ? 0 : ~0;\n\t\tmsr_bitmap[word + (0x800 / sizeof(long))] = ~0;\n\t}\n\n\tif (mode & MSR_BITMAP_MODE_X2APIC) {\n\t\t/*\n\t\t * TPR reads and writes can be virtualized even if virtual interrupt\n\t\t * delivery is not in use.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW);\n\t\tif (mode & MSR_BITMAP_MODE_X2APIC_APICV) {\n\t\t\tvmx_enable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_R);\n\t\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);\n\t\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);\n\t\t}\n\t}\n}\n\nstatic void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;\n\tu8 mode = vmx_msr_bitmap_mode(vcpu);\n\tu8 changed = mode ^ vmx->msr_bitmap_mode;\n\n\tif (!changed)\n\t\treturn;\n\n\tvmx_set_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW,\n\t\t\t\t  !(mode & MSR_BITMAP_MODE_LM));\n\n\tif (changed & (MSR_BITMAP_MODE_X2APIC | MSR_BITMAP_MODE_X2APIC_APICV))\n\t\tvmx_update_msr_bitmap_x2apic(msr_bitmap, mode);\n\n\tvmx->msr_bitmap_mode = mode;\n}\n\nstatic bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)\n{\n\treturn enable_apicv;\n}\n\nstatic void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tgfn_t gfn;\n\n\t/*\n\t * Don't need to mark the APIC access page dirty; it is never\n\t * written to by the CPU during APIC virtualization.\n\t */\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tgfn = vmcs12->virtual_apic_page_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tgfn = vmcs12->posted_intr_desc_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n}\n\n\nstatic void vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tvoid *vapic_page;\n\tu16 status;\n\n\tif (!vmx->nested.pi_desc || !vmx->nested.pi_pending)\n\t\treturn;\n\n\tvmx->nested.pi_pending = false;\n\tif (!pi_test_and_clear_on(vmx->nested.pi_desc))\n\t\treturn;\n\n\tmax_irr = find_last_bit((unsigned long *)vmx->nested.pi_desc->pir, 256);\n\tif (max_irr != 256) {\n\t\tvapic_page = kmap(vmx->nested.virtual_apic_page);\n\t\t__kvm_apic_update_irr(vmx->nested.pi_desc->pir,\n\t\t\tvapic_page, &max_irr);\n\t\tkunmap(vmx->nested.virtual_apic_page);\n\n\t\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\t\tif ((u8)max_irr > ((u8)status & 0xff)) {\n\t\t\tstatus &= ~0xff;\n\t\t\tstatus |= (u8)max_irr;\n\t\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t\t}\n\t}\n\n\tnested_mark_vmcs12_pages_dirty(vcpu);\n}\n\nstatic inline bool kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     bool nested)\n{\n#ifdef CONFIG_SMP\n\tint pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;\n\n\tif (vcpu->mode == IN_GUEST_MODE) {\n\t\t/*\n\t\t * The vector of interrupt to be delivered to vcpu had\n\t\t * been set in PIR before this function.\n\t\t *\n\t\t * Following cases will be reached in this block, and\n\t\t * we always send a notification event in all cases as\n\t\t * explained below.\n\t\t *\n\t\t * Case 1: vcpu keeps in non-root mode. Sending a\n\t\t * notification event posts the interrupt to vcpu.\n\t\t *\n\t\t * Case 2: vcpu exits to root mode and is still\n\t\t * runnable. PIR will be synced to vIRR before the\n\t\t * next vcpu entry. Sending a notification event in\n\t\t * this case has no effect, as vcpu is not in root\n\t\t * mode.\n\t\t *\n\t\t * Case 3: vcpu exits to root mode and is blocked.\n\t\t * vcpu_block() has already synced PIR to vIRR and\n\t\t * never blocks vcpu if vIRR is not cleared. Therefore,\n\t\t * a blocked vcpu here does not wait for any requested\n\t\t * interrupts in PIR, and sending a notification event\n\t\t * which has no effect is safe here.\n\t\t */\n\n\t\tapic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\nstatic int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tint vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t    vector == vmx->nested.posted_intr_nv) {\n\t\t/*\n\t\t * If a posted intr is not recognized by hardware,\n\t\t * we will accomplish it in the next vmentry.\n\t\t */\n\t\tvmx->nested.pi_pending = true;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t/* the PIR and ON have been set by L1. */\n\t\tif (!kvm_vcpu_trigger_posted_interrupt(vcpu, true))\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\treturn 0;\n\t}\n\treturn -1;\n}\n/*\n * Send interrupt to vcpu via posted interrupt way.\n * 1. If target vcpu is running(non-root mode), send posted interrupt\n * notification to vcpu and hardware will sync PIR to vIRR atomically.\n * 2. If target vcpu isn't running(root mode), kick it to pick up the\n * interrupt from PIR in next vmentry.\n */\nstatic void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint r;\n\n\tr = vmx_deliver_nested_posted_interrupt(vcpu, vector);\n\tif (!r)\n\t\treturn;\n\n\tif (pi_test_and_set_pir(vector, &vmx->pi_desc))\n\t\treturn;\n\n\t/* If a previous notification has sent the IPI, nothing to do.  */\n\tif (pi_test_and_set_on(&vmx->pi_desc))\n\t\treturn;\n\n\tif (!kvm_vcpu_trigger_posted_interrupt(vcpu, false))\n\t\tkvm_vcpu_kick(vcpu);\n}\n\n/*\n * Set up the vmcs's constant host-state fields, i.e., host-state fields that\n * will not change in the lifetime of the guest.\n * Note that host-state that does change is set elsewhere. E.g., host-state\n * that is set differently for each CPU is set in vmx_vcpu_load(), not here.\n */\nstatic void vmx_set_constant_host_state(struct vcpu_vmx *vmx)\n{\n\tu32 low32, high32;\n\tunsigned long tmpl;\n\tstruct desc_ptr dt;\n\tunsigned long cr0, cr3, cr4;\n\n\tcr0 = read_cr0();\n\tWARN_ON(cr0 & X86_CR0_TS);\n\tvmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */\n\n\t/*\n\t * Save the most likely value for this task's CR3 in the VMCS.\n\t * We can't use __get_current_cr3_fast() because we're not atomic.\n\t */\n\tcr3 = __read_cr3();\n\tvmcs_writel(HOST_CR3, cr3);\t\t/* 22.2.3  FIXME: shadow tables */\n\tvmx->loaded_vmcs->vmcs_host_cr3 = cr3;\n\n\t/* Save the most likely value for this task's CR4 in the VMCS. */\n\tcr4 = cr4_read_shadow();\n\tvmcs_writel(HOST_CR4, cr4);\t\t\t/* 22.2.3, 22.2.5 */\n\tvmx->loaded_vmcs->vmcs_host_cr4 = cr4;\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\t/*\n\t * Load null selectors, so we can avoid reloading them in\n\t * __vmx_load_host_state(), in case userspace uses the null selectors\n\t * too (the expected case).\n\t */\n\tvmcs_write16(HOST_DS_SELECTOR, 0);\n\tvmcs_write16(HOST_ES_SELECTOR, 0);\n#else\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#endif\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tstore_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\tvmx->host_idt_base = dt.address;\n\n\tvmcs_writel(HOST_RIP, vmx_return); /* 22.2.5 */\n\n\trdmsr(MSR_IA32_SYSENTER_CS, low32, high32);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, low32);\n\trdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, low32, high32);\n\t\tvmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));\n\t}\n}\n\nstatic void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)\n{\n\tvmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;\n\tif (enable_ept)\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;\n\tif (is_guest_mode(&vmx->vcpu))\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits &=\n\t\t\t~get_vmcs12(&vmx->vcpu)->cr4_guest_host_mask;\n\tvmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);\n}\n\nstatic u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)\n{\n\tu32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;\n\n\tif (!kvm_vcpu_apicv_active(&vmx->vcpu))\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;\n\n\tif (!enable_vnmi)\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;\n\n\t/* Enable the preemption timer dynamically */\n\tpin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\treturn pin_based_exec_ctrl;\n}\n\nstatic void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tif (kvm_vcpu_apicv_active(vcpu))\n\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t      SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t\telse\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t}\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(vcpu);\n}\n\nstatic u32 vmx_exec_control(struct vcpu_vmx *vmx)\n{\n\tu32 exec_control = vmcs_config.cpu_based_exec_ctrl;\n\n\tif (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)\n\t\texec_control &= ~CPU_BASED_MOV_DR_EXITING;\n\n\tif (!cpu_need_tpr_shadow(&vmx->vcpu)) {\n\t\texec_control &= ~CPU_BASED_TPR_SHADOW;\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR8_LOAD_EXITING;\n#endif\n\t}\n\tif (!enable_ept)\n\t\texec_control |= CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR3_LOAD_EXITING  |\n\t\t\t\tCPU_BASED_INVLPG_EXITING;\n\tif (kvm_mwait_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~(CPU_BASED_MWAIT_EXITING |\n\t\t\t\tCPU_BASED_MONITOR_EXITING);\n\tif (kvm_hlt_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~CPU_BASED_HLT_EXITING;\n\treturn exec_control;\n}\n\nstatic bool vmx_rdrand_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDRAND_EXITING;\n}\n\nstatic bool vmx_rdseed_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDSEED_EXITING;\n}\n\nstatic void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)\n{\n\tstruct kvm_vcpu *vcpu = &vmx->vcpu;\n\n\tu32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;\n\n\tif (!cpu_need_virtualize_apic_accesses(vcpu))\n\t\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\tif (vmx->vpid == 0)\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_VPID;\n\tif (!enable_ept) {\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_EPT;\n\t\tenable_unrestricted_guest = 0;\n\t\t/* Enable INVPCID for non-ept guests may cause performance regression. */\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;\n\t}\n\tif (!enable_unrestricted_guest)\n\t\texec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\tif (kvm_pause_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\texec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\n\t/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,\n\t * in vmx_set_cr4.  */\n\texec_control &= ~SECONDARY_EXEC_DESC;\n\n\t/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD\n\t   (handle_vmptrld).\n\t   We can NOT enable shadow_vmcs here because we don't have yet\n\t   a current VMCS12\n\t*/\n\texec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\tif (!enable_pml)\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\tif (vmx_xsaves_supported()) {\n\t\t/* Exposing XSAVES only when XSAVE is exposed */\n\t\tbool xsaves_enabled =\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_XSAVES);\n\n\t\tif (!xsaves_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_XSAVES;\n\n\t\tif (nested) {\n\t\t\tif (xsaves_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_XSAVES;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_XSAVES;\n\t\t}\n\t}\n\n\tif (vmx_rdtscp_supported()) {\n\t\tbool rdtscp_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP);\n\t\tif (!rdtscp_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDTSCP;\n\n\t\tif (nested) {\n\t\t\tif (rdtscp_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDTSCP;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDTSCP;\n\t\t}\n\t}\n\n\tif (vmx_invpcid_supported()) {\n\t\t/* Exposing INVPCID only when PCID is exposed */\n\t\tbool invpcid_enabled =\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_INVPCID) &&\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_PCID);\n\n\t\tif (!invpcid_enabled) {\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;\n\t\t\tguest_cpuid_clear(vcpu, X86_FEATURE_INVPCID);\n\t\t}\n\n\t\tif (nested) {\n\t\t\tif (invpcid_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_ENABLE_INVPCID;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_ENABLE_INVPCID;\n\t\t}\n\t}\n\n\tif (vmx_rdrand_supported()) {\n\t\tbool rdrand_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDRAND);\n\t\tif (rdrand_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDRAND_EXITING;\n\n\t\tif (nested) {\n\t\t\tif (rdrand_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDRAND_EXITING;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDRAND_EXITING;\n\t\t}\n\t}\n\n\tif (vmx_rdseed_supported()) {\n\t\tbool rdseed_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDSEED);\n\t\tif (rdseed_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDSEED_EXITING;\n\n\t\tif (nested) {\n\t\t\tif (rdseed_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDSEED_EXITING;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDSEED_EXITING;\n\t\t}\n\t}\n\n\tvmx->secondary_exec_control = exec_control;\n}\n\nstatic void ept_set_mmio_spte_mask(void)\n{\n\t/*\n\t * EPT Misconfigurations can be generated if the value of bits 2:0\n\t * of an EPT paging-structure entry is 110b (write/execute).\n\t */\n\tkvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,\n\t\t\t\t   VMX_EPT_MISCONFIG_WX_VALUE);\n}\n\n#define VMX_XSS_EXIT_BITMAP 0\n/*\n * Sets up the vmcs for emulated real mode.\n */\nstatic void vmx_vcpu_setup(struct vcpu_vmx *vmx)\n{\n#ifdef CONFIG_X86_64\n\tunsigned long a;\n#endif\n\tint i;\n\n\tif (enable_shadow_vmcs) {\n\t\t/*\n\t\t * At vCPU creation, \"VMWRITE to any supported field\n\t\t * in the VMCS\" is supported, so use the more\n\t\t * permissive vmx_vmread_bitmap to specify both read\n\t\t * and write permissions for the shadow VMCS.\n\t\t */\n\t\tvmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));\n\t\tvmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmread_bitmap));\n\t}\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));\n\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */\n\n\t/* Control */\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n\tvmx->hv_deadline_tsc = -1;\n\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, vmx_exec_control(vmx));\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tvmx_compute_secondary_exec_control(vmx);\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t     vmx->secondary_exec_control);\n\t}\n\n\tif (kvm_vcpu_apicv_active(&vmx->vcpu)) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, 0);\n\n\t\tvmcs_write16(GUEST_INTR_STATUS, 0);\n\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));\n\t}\n\n\tif (!kvm_pause_in_guest(vmx->vcpu.kvm)) {\n\t\tvmcs_write32(PLE_GAP, ple_gap);\n\t\tvmx->ple_window = ple_window;\n\t\tvmx->ple_window_dirty = true;\n\t}\n\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);\n\tvmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */\n\n\tvmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */\n\tvmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */\n\tvmx_set_constant_host_state(vmx);\n#ifdef CONFIG_X86_64\n\trdmsrl(MSR_FS_BASE, a);\n\tvmcs_writel(HOST_FS_BASE, a); /* 22.2.4 */\n\trdmsrl(MSR_GS_BASE, a);\n\tvmcs_writel(HOST_GS_BASE, a); /* 22.2.4 */\n#else\n\tvmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */\n\tvmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */\n#endif\n\n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i) {\n\t\tu32 index = vmx_msr_index[i];\n\t\tu32 data_low, data_high;\n\t\tint j = vmx->nmsrs;\n\n\t\tif (rdmsr_safe(index, &data_low, &data_high) < 0)\n\t\t\tcontinue;\n\t\tif (wrmsr_safe(index, data_low, data_high) < 0)\n\t\t\tcontinue;\n\t\tvmx->guest_msrs[j].index = i;\n\t\tvmx->guest_msrs[j].data = 0;\n\t\tvmx->guest_msrs[j].mask = -1ull;\n\t\t++vmx->nmsrs;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))\n\t\trdmsrl(MSR_IA32_ARCH_CAPABILITIES, vmx->arch_capabilities);\n\n\tvm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl);\n\n\t/* 22.2.1, 20.8.1 */\n\tvm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);\n\n\tvmx->vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tif (vmx_xsaves_supported())\n\t\tvmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);\n\n\tif (enable_pml) {\n\t\tASSERT(vmx->pml_pg);\n\t\tvmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));\n\t\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n\t}\n}\n\nstatic void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct msr_data apic_base_msr;\n\tu64 cr0;\n\n\tvmx->rmode.vm86_active = 0;\n\tvmx->spec_ctrl = 0;\n\n\tvcpu->arch.microcode_version = 0x100000000ULL;\n\tvmx->vcpu.arch.regs[VCPU_REGS_RDX] = get_rdx_init_val();\n\tkvm_set_cr8(vcpu, 0);\n\n\tif (!init_event) {\n\t\tapic_base_msr.data = APIC_DEFAULT_PHYS_BASE |\n\t\t\t\t     MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tapic_base_msr.data |= MSR_IA32_APICBASE_BSP;\n\t\tapic_base_msr.host_initiated = true;\n\t\tkvm_set_apic_base(vcpu, &apic_base_msr);\n\t}\n\n\tvmx_segment_cache_clear(vmx);\n\n\tseg_setup(VCPU_SREG_CS);\n\tvmcs_write16(GUEST_CS_SELECTOR, 0xf000);\n\tvmcs_writel(GUEST_CS_BASE, 0xffff0000ul);\n\n\tseg_setup(VCPU_SREG_DS);\n\tseg_setup(VCPU_SREG_ES);\n\tseg_setup(VCPU_SREG_FS);\n\tseg_setup(VCPU_SREG_GS);\n\tseg_setup(VCPU_SREG_SS);\n\n\tvmcs_write16(GUEST_TR_SELECTOR, 0);\n\tvmcs_writel(GUEST_TR_BASE, 0);\n\tvmcs_write32(GUEST_TR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tvmcs_write16(GUEST_LDTR_SELECTOR, 0);\n\tvmcs_writel(GUEST_LDTR_BASE, 0);\n\tvmcs_write32(GUEST_LDTR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);\n\n\tif (!init_event) {\n\t\tvmcs_write32(GUEST_SYSENTER_CS, 0);\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, 0);\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, 0);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\t}\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0xfff0);\n\n\tvmcs_writel(GUEST_GDTR_BASE, 0);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xffff);\n\n\tvmcs_writel(GUEST_IDTR_BASE, 0);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xffff);\n\n\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);\n\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);\n\tif (kvm_mpx_supported())\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tsetup_msrs(vmx);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */\n\n\tif (cpu_has_vmx_tpr_shadow() && !init_event) {\n\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);\n\t\tif (cpu_need_tpr_shadow(vcpu))\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR,\n\t\t\t\t     __pa(vcpu->arch.apic->regs));\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t}\n\n\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\tif (vmx->vpid != 0)\n\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\n\tcr0 = X86_CR0_NW | X86_CR0_CD | X86_CR0_ET;\n\tvmx->vcpu.arch.cr0 = cr0;\n\tvmx_set_cr0(vcpu, cr0); /* enter rmode */\n\tvmx_set_cr4(vcpu, 0);\n\tvmx_set_efer(vcpu, 0);\n\n\tupdate_exception_bitmap(vcpu);\n\n\tvpid_sync_context(vmx->vpid);\n\tif (init_event)\n\t\tvmx_clear_hlt(vcpu);\n}\n\n/*\n * In nested virtualization, check if L1 asked to exit on external interrupts.\n * For most existing hypervisors, this will always return true.\n */\nstatic bool nested_exit_on_intr(struct kvm_vcpu *vcpu)\n{\n\treturn get_vmcs12(vcpu)->pin_based_vm_exec_control &\n\t\tPIN_BASED_EXT_INTR_MASK;\n}\n\n/*\n * In nested virtualization, check if L1 has set\n * VM_EXIT_ACK_INTR_ON_EXIT\n */\nstatic bool nested_exit_intr_ack_set(struct kvm_vcpu *vcpu)\n{\n\treturn get_vmcs12(vcpu)->vm_exit_controls &\n\t\tVM_EXIT_ACK_INTR_ON_EXIT;\n}\n\nstatic bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)\n{\n\treturn nested_cpu_has_nmi_exiting(get_vmcs12(vcpu));\n}\n\nstatic void enable_irq_window(struct kvm_vcpu *vcpu)\n{\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_INTR_PENDING);\n}\n\nstatic void enable_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tif (!enable_vnmi ||\n\t    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {\n\t\tenable_irq_window(vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_NMI_PENDING);\n}\n\nstatic void vmx_inject_irq(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tuint32_t intr;\n\tint irq = vcpu->arch.interrupt.nr;\n\n\ttrace_kvm_inj_virq(irq);\n\n\t++vcpu->stat.irq_injections;\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (vcpu->arch.interrupt.soft)\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tif (kvm_inject_realmode_interrupt(vcpu, irq, inc_eip) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\tintr = irq | INTR_INFO_VALID_MASK;\n\tif (vcpu->arch.interrupt.soft) {\n\t\tintr |= INTR_TYPE_SOFT_INTR;\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t} else\n\t\tintr |= INTR_TYPE_EXT_INTR;\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic void vmx_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\t/*\n\t\t * Tracking the NMI-blocked state in software is built upon\n\t\t * finding the next open IRQ window. This, in turn, depends on\n\t\t * well-behaving guests: They have to keep IRQs disabled at\n\t\t * least as long as the NMI handler runs. Otherwise we may\n\t\t * cause NMI nesting, maybe breaking the guest. But as this is\n\t\t * highly unlikely, we can live with the residual risk.\n\t\t */\n\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 1;\n\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t}\n\n\t++vcpu->stat.nmi_injections;\n\tvmx->loaded_vmcs->nmi_known_unmasked = false;\n\n\tif (vmx->rmode.vm86_active) {\n\t\tif (kvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool masked;\n\n\tif (!enable_vnmi)\n\t\treturn vmx->loaded_vmcs->soft_vnmi_blocked;\n\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\treturn false;\n\tmasked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;\n\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\treturn masked;\n}\n\nstatic void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = masked;\n\t\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t\t}\n\t} else {\n\t\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\t\tif (masked)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t\tGUEST_INTR_STATE_NMI);\n\t}\n}\n\nstatic int vmx_nmi_allowed(struct kvm_vcpu *vcpu)\n{\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn 0;\n\n\tif (!enable_vnmi &&\n\t    to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)\n\t\treturn 0;\n\n\treturn\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t  (GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI\n\t\t   | GUEST_INTR_STATE_NMI));\n}\n\nstatic int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn (!to_vmx(vcpu)->nested.nested_run_pending &&\n\t\tvmcs_readl(GUEST_RFLAGS) & X86_EFLAGS_IF) &&\n\t\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t\t(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));\n}\n\nstatic int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)\n{\n\tint ret;\n\n\tif (enable_unrestricted_guest)\n\t\treturn 0;\n\n\tret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,\n\t\t\t\t    PAGE_SIZE * 3);\n\tif (ret)\n\t\treturn ret;\n\tto_kvm_vmx(kvm)->tss_addr = addr;\n\treturn init_rmode_tss(kvm);\n}\n\nstatic int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)\n{\n\tto_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;\n\treturn 0;\n}\n\nstatic bool rmode_exception(struct kvm_vcpu *vcpu, int vec)\n{\n\tswitch (vec) {\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject the exception\n\t\t * from user space while in guest debugging mode.\n\t\t */\n\t\tto_vmx(vcpu)->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\t/* fall through */\n\tcase DB_VECTOR:\n\t\tif (vcpu->guest_debug &\n\t\t\t(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\t/* fall through */\n\tcase DE_VECTOR:\n\tcase OF_VECTOR:\n\tcase BR_VECTOR:\n\tcase UD_VECTOR:\n\tcase DF_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\tcase MF_VECTOR:\n\t\treturn true;\n\tbreak;\n\t}\n\treturn false;\n}\n\nstatic int handle_rmode_exception(struct kvm_vcpu *vcpu,\n\t\t\t\t  int vec, u32 err_code)\n{\n\t/*\n\t * Instruction with address size override prefix opcode 0x67\n\t * Cause the #SS fault with 0 error code in VM86 mode.\n\t */\n\tif (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {\n\t\tif (emulate_instruction(vcpu, 0) == EMULATE_DONE) {\n\t\t\tif (vcpu->arch.halt_request) {\n\t\t\t\tvcpu->arch.halt_request = 0;\n\t\t\t\treturn kvm_vcpu_halt(vcpu);\n\t\t\t}\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Forward all other exceptions that are valid in real mode.\n\t * FIXME: Breaks guest debugging in real mode, needs to be fixed with\n\t *        the required debugging infrastructure rework.\n\t */\n\tkvm_queue_exception(vcpu, vec);\n\treturn 1;\n}\n\n/*\n * Trigger machine check on the host. We assume all the MSRs are already set up\n * by the CPU and that we still run on the same CPU as the MCE occurred on.\n * We pass a fake environment to the machine check handler because we want\n * the guest to be always treated like user space, no matter what context\n * it used internally.\n */\nstatic void kvm_machine_check(void)\n{\n#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_64)\n\tstruct pt_regs regs = {\n\t\t.cs = 3, /* Fake ring 3 no matter what the guest ran on */\n\t\t.flags = X86_EFLAGS_IF,\n\t};\n\n\tdo_machine_check(&regs, 0);\n#endif\n}\n\nstatic int handle_machine_check(struct kvm_vcpu *vcpu)\n{\n\t/* already handled by vcpu_run */\n\treturn 1;\n}\n\nstatic int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif (is_nmi(intr_info))\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_invalid_opcode(intr_info))\n\t\treturn handle_ud(vcpu);\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\tif (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {\n\t\tWARN_ON_ONCE(!enable_vmware_backdoor);\n\t\ter = emulate_instruction(vcpu,\n\t\t\tEMULTYPE_VMWARE | EMULTYPE_NO_UD_ON_FAIL);\n\t\tif (er == EMULATE_USER_EXIT)\n\t\t\treturn 0;\n\t\telse if (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, error_code);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\t/* EPT won't cause page fault directly */\n\t\tWARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);\n\t\treturn kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (is_icebp(intr_info))\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int handle_external_interrupt(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.irq_exits;\n\treturn 1;\n}\n\nstatic int handle_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\tvcpu->mmio_needed = 0;\n\treturn 0;\n}\n\nstatic int handle_io(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint size, in, string;\n\tunsigned port;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tstring = (exit_qualification & 16) != 0;\n\n\t++vcpu->stat.io_exits;\n\n\tif (string)\n\t\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\tin = (exit_qualification & 8) != 0;\n\n\treturn kvm_fast_pio(vcpu, size, port, in);\n}\n\nstatic void\nvmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)\n{\n\t/*\n\t * Patch in the VMCALL instruction:\n\t */\n\thypercall[0] = 0x0f;\n\thypercall[1] = 0x01;\n\thypercall[2] = 0xc1;\n}\n\n/* called to set cr0 as appropriate for a mov-to-cr0 exit. */\nstatic int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t/*\n\t\t * We get here when L2 changed cr0 in a way that did not change\n\t\t * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),\n\t\t * but did change L0 shadowed bits. So we first calculate the\n\t\t * effective cr0 value that L1 would like to write into the\n\t\t * hardware. It consists of the L2-owned bits from the new\n\t\t * value combined with the L1-owned bits from L1's guest_cr0.\n\t\t */\n\t\tval = (val & ~vmcs12->cr0_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);\n\n\t\tif (!nested_guest_cr0_valid(vcpu, val))\n\t\t\treturn 1;\n\n\t\tif (kvm_set_cr0(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR0_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else {\n\t\tif (to_vmx(vcpu)->nested.vmxon &&\n\t\t    !nested_host_cr0_valid(vcpu, val))\n\t\t\treturn 1;\n\n\t\treturn kvm_set_cr0(vcpu, val);\n\t}\n}\n\nstatic int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t/* analogously to handle_set_cr0 */\n\t\tval = (val & ~vmcs12->cr4_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);\n\t\tif (kvm_set_cr4(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR4_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else\n\t\treturn kvm_set_cr4(vcpu, val);\n}\n\nstatic int handle_desc(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_cr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification, val;\n\tint cr;\n\tint reg;\n\tint err;\n\tint ret;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tcr = exit_qualification & 15;\n\treg = (exit_qualification >> 8) & 15;\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\terr = handle_set_cr0(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 4:\n\t\t\terr = handle_set_cr4(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 8: {\n\t\t\t\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t\t\t\tu8 cr8 = (u8)val;\n\t\t\t\terr = kvm_set_cr8(vcpu, cr8);\n\t\t\t\tret = kvm_complete_insn_gp(vcpu, err);\n\t\t\t\tif (lapic_in_kernel(vcpu))\n\t\t\t\t\treturn ret;\n\t\t\t\tif (cr8_prev <= cr8)\n\t\t\t\t\treturn ret;\n\t\t\t\t/*\n\t\t\t\t * TODO: we might be squashing a\n\t\t\t\t * KVM_GUESTDBG_SINGLESTEP-triggered\n\t\t\t\t * KVM_EXIT_DEBUG here.\n\t\t\t\t */\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tWARN_ONCE(1, \"Guest should always own CR0.TS\");\n\t\tvmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));\n\t\ttrace_kvm_cr_write(0, kvm_read_cr0(vcpu));\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tcase 1: /*mov from cr*/\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\ttrace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);\n\t\tkvm_lmsw(vcpu, val);\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\tvcpu->run->exit_reason = 0;\n\tvcpu_unimpl(vcpu, \"unhandled control register: op %d cr %d\\n\",\n\t       (int)(exit_qualification >> 4) & 3, cr);\n\treturn 0;\n}\n\nstatic int handle_dr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint dr, dr7, reg;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tdr = exit_qualification & DEBUG_REG_ACCESS_NUM;\n\n\t/* First, if DR does not exist, trigger UD */\n\tif (!kvm_require_dr(vcpu, dr))\n\t\treturn 1;\n\n\t/* Do not handle if the CPL > 0, will trigger GP on re-entry */\n\tif (!kvm_require_cpl(vcpu, 0))\n\t\treturn 1;\n\tdr7 = vmcs_readl(GUEST_DR7);\n\tif (dr7 & DR7_GD) {\n\t\t/*\n\t\t * As the vm-exit takes precedence over the debug trap, we\n\t\t * need to emulate the latter, either for the host or the\n\t\t * guest debugging itself.\n\t\t */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\t\tvcpu->run->debug.arch.dr6 = vcpu->arch.dr6;\n\t\t\tvcpu->run->debug.arch.dr7 = dr7;\n\t\t\tvcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\t\tvcpu->run->debug.arch.exception = DB_VECTOR;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= DR6_BD | DR6_RTM;\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (vcpu->guest_debug == 0) {\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_MOV_DR_EXITING);\n\n\t\t/*\n\t\t * No more DR vmexits; force a reload of the debug registers\n\t\t * and reenter on this instruction.  The next vmexit will\n\t\t * retrieve the full state of the debug registers.\n\t\t */\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\treg = DEBUG_REG_ACCESS_REG(exit_qualification);\n\tif (exit_qualification & TYPE_MOV_FROM_DR) {\n\t\tunsigned long val;\n\n\t\tif (kvm_get_dr(vcpu, dr, &val))\n\t\t\treturn 1;\n\t\tkvm_register_write(vcpu, reg, val);\n\t} else\n\t\tif (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))\n\t\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic u64 vmx_get_dr6(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.dr6;\n}\n\nstatic void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)\n{\n}\n\nstatic void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)\n{\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\tget_debugreg(vcpu->arch.dr6, 6);\n\tvcpu->arch.dr7 = vmcs_readl(GUEST_DR7);\n\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);\n}\n\nstatic void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tvmcs_writel(GUEST_DR7, val);\n}\n\nstatic int handle_cpuid(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_cpuid(vcpu);\n}\n\nstatic int handle_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];\n\tstruct msr_data msr_info;\n\n\tmsr_info.index = ecx;\n\tmsr_info.host_initiated = false;\n\tif (vmx_get_msr(vcpu, &msr_info)) {\n\t\ttrace_kvm_msr_read_ex(ecx);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_read(ecx, msr_info.data);\n\n\t/* FIXME: handling of bits 32:63 of rax, rdx */\n\tvcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;\n\tvcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_wrmsr(struct kvm_vcpu *vcpu)\n{\n\tstruct msr_data msr;\n\tu32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];\n\tu64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)\n\t\t| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);\n\n\tmsr.data = data;\n\tmsr.index = ecx;\n\tmsr.host_initiated = false;\n\tif (kvm_set_msr(vcpu, &msr) != 0) {\n\t\ttrace_kvm_msr_write_ex(ecx, data);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_write(ecx, data);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)\n{\n\tkvm_apic_update_ppr(vcpu);\n\treturn 1;\n}\n\nstatic int handle_interrupt_window(struct kvm_vcpu *vcpu)\n{\n\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\tCPU_BASED_VIRTUAL_INTR_PENDING);\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t++vcpu->stat.irq_window_exits;\n\treturn 1;\n}\n\nstatic int handle_halt(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_halt(vcpu);\n}\n\nstatic int handle_vmcall(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_hypercall(vcpu);\n}\n\nstatic int handle_invd(struct kvm_vcpu *vcpu)\n{\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_invlpg(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\tkvm_mmu_invlpg(vcpu, exit_qualification);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_rdpmc(struct kvm_vcpu *vcpu)\n{\n\tint err;\n\n\terr = kvm_rdpmc(vcpu);\n\treturn kvm_complete_insn_gp(vcpu, err);\n}\n\nstatic int handle_wbinvd(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_wbinvd(vcpu);\n}\n\nstatic int handle_xsetbv(struct kvm_vcpu *vcpu)\n{\n\tu64 new_bv = kvm_read_edx_eax(vcpu);\n\tu32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);\n\n\tif (kvm_set_xcr(vcpu, index, new_bv) == 0)\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\treturn 1;\n}\n\nstatic int handle_xsaves(struct kvm_vcpu *vcpu)\n{\n\tkvm_skip_emulated_instruction(vcpu);\n\tWARN(1, \"this should never happen\\n\");\n\treturn 1;\n}\n\nstatic int handle_xrstors(struct kvm_vcpu *vcpu)\n{\n\tkvm_skip_emulated_instruction(vcpu);\n\tWARN(1, \"this should never happen\\n\");\n\treturn 1;\n}\n\nstatic int handle_apic_access(struct kvm_vcpu *vcpu)\n{\n\tif (likely(fasteoi)) {\n\t\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\t\tint access_type, offset;\n\n\t\taccess_type = exit_qualification & APIC_ACCESS_TYPE;\n\t\toffset = exit_qualification & APIC_ACCESS_OFFSET;\n\t\t/*\n\t\t * Sane guest uses MOV to write EOI, with written value\n\t\t * not cared. So make a short-circuit here by avoiding\n\t\t * heavy instruction emulation.\n\t\t */\n\t\tif ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&\n\t\t    (offset == APIC_EOI)) {\n\t\t\tkvm_lapic_set_eoi(vcpu);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t}\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tint vector = exit_qualification & 0xff;\n\n\t/* EOI-induced VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_set_eoi_accelerated(vcpu, vector);\n\treturn 1;\n}\n\nstatic int handle_apic_write(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 offset = exit_qualification & 0xfff;\n\n\t/* APIC-write VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}\n\nstatic int handle_task_switch(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\tu16 tss_selector;\n\tint reason, type, idt_v, idt_index;\n\n\tidt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);\n\tidt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);\n\ttype = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\treason = (u32)exit_qualification >> 30;\n\tif (reason == TASK_SWITCH_GATE && idt_v) {\n\t\tswitch (type) {\n\t\tcase INTR_TYPE_NMI_INTR:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tvmx_set_nmi_mask(vcpu, true);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_EXT_INTR:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\t\tif (vmx->idt_vectoring_info &\n\t\t\t    VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\tvmcs_read32(IDT_VECTORING_ERROR_CODE);\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\ttss_selector = exit_qualification;\n\n\tif (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&\n\t\t       type != INTR_TYPE_EXT_INTR &&\n\t\t       type != INTR_TYPE_NMI_INTR))\n\t\tskip_emulated_instruction(vcpu);\n\n\tif (kvm_task_switch(vcpu, tss_selector,\n\t\t\t    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,\n\t\t\t    has_error_code, error_code) == EMULATE_FAIL) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: What about debug traps on tss switch?\n\t *       Are we supposed to inject them and update dr6?\n\t */\n\n\treturn 1;\n}\n\nstatic int handle_ept_violation(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tgpa_t gpa;\n\tu64 error_code;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\t/*\n\t * EPT violation happened while executing iret from NMI,\n\t * \"blocked by NMI\" bit has to be set before next VM entry.\n\t * There are errata that may cause this bit to not be set:\n\t * AAK134, BY25.\n\t */\n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);\n\n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\ttrace_kvm_page_fault(gpa, exit_qualification);\n\n\t/* Is it a read fault? */\n\terror_code = (exit_qualification & EPT_VIOLATION_ACC_READ)\n\t\t     ? PFERR_USER_MASK : 0;\n\t/* Is it a write fault? */\n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)\n\t\t      ? PFERR_WRITE_MASK : 0;\n\t/* Is it a fetch fault? */\n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)\n\t\t      ? PFERR_FETCH_MASK : 0;\n\t/* ept page table entry is present? */\n\terror_code |= (exit_qualification &\n\t\t       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |\n\t\t\tEPT_VIOLATION_EXECUTABLE))\n\t\t      ? PFERR_PRESENT_MASK : 0;\n\n\terror_code |= (exit_qualification & 0x100) != 0 ?\n\t       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;\n\n\tvcpu->arch.exit_qualification = exit_qualification;\n\treturn kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);\n}\n\nstatic int handle_ept_misconfig(struct kvm_vcpu *vcpu)\n{\n\tgpa_t gpa;\n\n\t/*\n\t * A nested guest cannot optimize MMIO vmexits, because we have an\n\t * nGPA here instead of the required GPA.\n\t */\n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\tif (!is_guest_mode(vcpu) &&\n\t    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {\n\t\ttrace_kvm_fast_mmio(gpa);\n\t\t/*\n\t\t * Doing kvm_skip_emulated_instruction() depends on undefined\n\t\t * behavior: Intel's manual doesn't mandate\n\t\t * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG\n\t\t * occurs and while on real hardware it was observed to be set,\n\t\t * other hypervisors (namely Hyper-V) don't set it, we end up\n\t\t * advancing IP with some random value. Disable fast mmio when\n\t\t * running nested and keep it for real hardware in hope that\n\t\t * VM_EXIT_INSTRUCTION_LEN will always be set correctly.\n\t\t */\n\t\tif (!static_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\telse\n\t\t\treturn x86_emulate_instruction(vcpu, gpa, EMULTYPE_SKIP,\n\t\t\t\t\t\t       NULL, 0) == EMULATE_DONE;\n\t}\n\n\treturn kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);\n}\n\nstatic int handle_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON_ONCE(!enable_vnmi);\n\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\tCPU_BASED_VIRTUAL_NMI_PENDING);\n\t++vcpu->stat.nmi_window_exits;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 1;\n}\n\nstatic int handle_invalid_guest_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tenum emulation_result err = EMULATE_DONE;\n\tint ret = 1;\n\tu32 cpu_exec_ctrl;\n\tbool intr_window_requested;\n\tunsigned count = 130;\n\n\t/*\n\t * We should never reach the point where we are emulating L2\n\t * due to invalid guest state as that means we incorrectly\n\t * allowed a nested VMEntry with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);\n\n\tcpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);\n\tintr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;\n\n\twhile (vmx->emulation_required && count-- != 0) {\n\t\tif (intr_window_requested && vmx_interrupt_allowed(vcpu))\n\t\t\treturn handle_interrupt_window(&vmx->vcpu);\n\n\t\tif (kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\t\treturn 1;\n\n\t\terr = emulate_instruction(vcpu, 0);\n\n\t\tif (err == EMULATE_USER_EXIT) {\n\t\t\t++vcpu->stat.mmio_exits;\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (err != EMULATE_DONE)\n\t\t\tgoto emulation_error;\n\n\t\tif (vmx->emulation_required && !vmx->rmode.vm86_active &&\n\t\t    vcpu->arch.exception.pending)\n\t\t\tgoto emulation_error;\n\n\t\tif (vcpu->arch.halt_request) {\n\t\t\tvcpu->arch.halt_request = 0;\n\t\t\tret = kvm_vcpu_halt(vcpu);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tif (need_resched())\n\t\t\tschedule();\n\t}\n\nout:\n\treturn ret;\n\nemulation_error:\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\tvcpu->run->internal.ndata = 0;\n\treturn 0;\n}\n\nstatic void grow_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint old = vmx->ple_window;\n\n\tvmx->ple_window = __grow_ple_window(old, ple_window,\n\t\t\t\t\t    ple_window_grow,\n\t\t\t\t\t    ple_window_max);\n\n\tif (vmx->ple_window != old)\n\t\tvmx->ple_window_dirty = true;\n\n\ttrace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);\n}\n\nstatic void shrink_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint old = vmx->ple_window;\n\n\tvmx->ple_window = __shrink_ple_window(old, ple_window,\n\t\t\t\t\t      ple_window_shrink,\n\t\t\t\t\t      ple_window);\n\n\tif (vmx->ple_window != old)\n\t\tvmx->ple_window_dirty = true;\n\n\ttrace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);\n}\n\n/*\n * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.\n */\nstatic void wakeup_handler(void)\n{\n\tstruct kvm_vcpu *vcpu;\n\tint cpu = smp_processor_id();\n\n\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n\tlist_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),\n\t\t\tblocked_vcpu_list) {\n\t\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\t\tif (pi_test_on(pi_desc) == 1)\n\t\t\tkvm_vcpu_kick(vcpu);\n\t}\n\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n}\n\nstatic void vmx_enable_tdp(void)\n{\n\tkvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,\n\t\tenable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\tenable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t0ull, VMX_EPT_EXECUTABLE_MASK,\n\t\tcpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK,\n\t\tVMX_EPT_RWX_MASK, 0ull);\n\n\tept_set_mmio_spte_mask();\n\tkvm_enable_tdp();\n}\n\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tfor (i = 0; i < VMX_BITMAP_NR; i++) {\n\t\tvmx_bitmap[i] = (unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_bitmap[i])\n\t\t\tgoto out;\n\t}\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid() || !cpu_has_vmx_invvpid() ||\n\t\t!(cpu_has_vmx_invvpid_single() || cpu_has_vmx_invvpid_global()))\n\t\tenable_vpid = 0;\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels() ||\n\t    !cpu_has_vmx_ept_mt_wb() ||\n\t    !cpu_has_vmx_invept_global())\n\t\tenable_ept = 0;\n\n\tif (!cpu_has_vmx_ept_ad_bits() || !enable_ept)\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest() || !enable_ept)\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\tif (!cpu_has_virtual_nmis())\n\t\tenable_vnmi = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple()) {\n\t\tple_gap = 0;\n\t\tple_window = 0;\n\t\tple_window_grow = 0;\n\t\tple_window_max = 0;\n\t\tple_window_shrink = 0;\n\t}\n\n\tif (!cpu_has_vmx_apicv()) {\n\t\tenable_apicv = 0;\n\t\tkvm_x86_ops->sync_pir_to_irr = NULL;\n\t}\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_ept)\n\t\tvmx_enable_tdp();\n\telse\n\t\tkvm_disable_tdp();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tif (cpu_has_vmx_preemption_timer() && enable_preemption_timer) {\n\t\tu64 vmx_msr;\n\n\t\trdmsrl(MSR_IA32_VMX_MISC, vmx_msr);\n\t\tcpu_preemption_timer_multi =\n\t\t\t vmx_msr & VMX_MISC_PREEMPTION_TIMER_RATE_MASK;\n\t} else {\n\t\tkvm_x86_ops->set_hv_timer = NULL;\n\t\tkvm_x86_ops->cancel_hv_timer = NULL;\n\t}\n\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\tnested_vmx_setup_ctls_msrs(&vmcs_config.nested, enable_apicv);\n\n\tkvm_mce_cap_supported |= MCG_LMCE_P;\n\n\treturn alloc_kvm_area();\n\nout:\n\tfor (i = 0; i < VMX_BITMAP_NR; i++)\n\t\tfree_page((unsigned long)vmx_bitmap[i]);\n\n    return r;\n}\n\nstatic __exit void hardware_unsetup(void)\n{\n\tint i;\n\n\tfor (i = 0; i < VMX_BITMAP_NR; i++)\n\t\tfree_page((unsigned long)vmx_bitmap[i]);\n\n\tfree_kvm_area();\n}\n\n/*\n * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE\n * exiting, so only get here on cpu with PAUSE-Loop-Exiting.\n */\nstatic int handle_pause(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tgrow_ple_window(vcpu);\n\n\t/*\n\t * Intel sdm vol3 ch-25.1.3 says: The \"PAUSE-loop exiting\"\n\t * VM-execution control is ignored if CPL > 0. OTOH, KVM\n\t * never set PAUSE_EXITING and just set PLE if supported,\n\t * so the vcpu must be CPL=0 if it gets a PAUSE exit.\n\t */\n\tkvm_vcpu_on_spin(vcpu, true);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_nop(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_mwait(struct kvm_vcpu *vcpu)\n{\n\tprintk_once(KERN_WARNING \"kvm: MWAIT instruction emulated as NOP!\\n\");\n\treturn handle_nop(vcpu);\n}\n\nstatic int handle_invalid_op(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\n\nstatic int handle_monitor_trap(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int handle_monitor(struct kvm_vcpu *vcpu)\n{\n\tprintk_once(KERN_WARNING \"kvm: MONITOR instruction emulated as NOP!\\n\");\n\treturn handle_nop(vcpu);\n}\n\n/*\n * The following 3 functions, nested_vmx_succeed()/failValid()/failInvalid(),\n * set the success or error code of an emulated VMX instruction, as specified\n * by Vol 2B, VMX Instruction Reference, \"Conventions\".\n */\nstatic void nested_vmx_succeed(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_OF));\n}\n\nstatic void nested_vmx_failInvalid(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_PF | X86_EFLAGS_AF | X86_EFLAGS_ZF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_CF);\n}\n\nstatic void nested_vmx_failValid(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu32 vm_instruction_error)\n{\n\tif (to_vmx(vcpu)->nested.current_vmptr == -1ull) {\n\t\t/*\n\t\t * failValid writes the error number to the current VMCS, which\n\t\t * can't be done there isn't a current VMCS.\n\t\t */\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn;\n\t}\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_ZF);\n\tget_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;\n\t/*\n\t * We don't need to force a shadow sync because\n\t * VM_INSTRUCTION_ERROR is not shadowed\n\t */\n}\n\nstatic void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)\n{\n\t/* TODO: not to reset guest simply here. */\n\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\tpr_debug_ratelimited(\"kvm: nested vmx abort, indicator %d\\n\", indicator);\n}\n\nstatic enum hrtimer_restart vmx_preemption_timer_fn(struct hrtimer *timer)\n{\n\tstruct vcpu_vmx *vmx =\n\t\tcontainer_of(timer, struct vcpu_vmx, nested.preemption_timer);\n\n\tvmx->nested.preemption_timer_expired = true;\n\tkvm_make_request(KVM_REQ_EVENT, &vmx->vcpu);\n\tkvm_vcpu_kick(&vmx->vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\n/*\n * Decode the memory-address operand of a vmx instruction, as recorded on an\n * exit caused by such an instruction (run by a guest hypervisor).\n * On success, returns 0. When the operand is invalid, returns 1 and throws\n * #UD or #GP.\n */\nstatic int get_vmx_mem_address(struct kvm_vcpu *vcpu,\n\t\t\t\t unsigned long exit_qualification,\n\t\t\t\t u32 vmx_instruction_info, bool wr, gva_t *ret)\n{\n\tgva_t off;\n\tbool exn;\n\tstruct kvm_segment s;\n\n\t/*\n\t * According to Vol. 3B, \"Information for VM Exits Due to Instruction\n\t * Execution\", on an exit, vmx_instruction_info holds most of the\n\t * addressing components of the operand. Only the displacement part\n\t * is put in exit_qualification (see 3B, \"Basic VM-Exit Information\").\n\t * For how an actual address is calculated from all these components,\n\t * refer to Vol. 1, \"Operand Addressing\".\n\t */\n\tint  scaling = vmx_instruction_info & 3;\n\tint  addr_size = (vmx_instruction_info >> 7) & 7;\n\tbool is_reg = vmx_instruction_info & (1u << 10);\n\tint  seg_reg = (vmx_instruction_info >> 15) & 7;\n\tint  index_reg = (vmx_instruction_info >> 18) & 0xf;\n\tbool index_is_valid = !(vmx_instruction_info & (1u << 22));\n\tint  base_reg       = (vmx_instruction_info >> 23) & 0xf;\n\tbool base_is_valid  = !(vmx_instruction_info & (1u << 27));\n\n\tif (is_reg) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t/* Addr = segment_base + offset */\n\t/* offset = base + [index * scale] + displacement */\n\toff = exit_qualification; /* holds the displacement */\n\tif (base_is_valid)\n\t\toff += kvm_register_read(vcpu, base_reg);\n\tif (index_is_valid)\n\t\toff += kvm_register_read(vcpu, index_reg)<<scaling;\n\tvmx_get_segment(vcpu, &s, seg_reg);\n\t*ret = s.base + off;\n\n\tif (addr_size == 1) /* 32 bit */\n\t\t*ret &= 0xffffffff;\n\n\t/* Checks for #GP/#SS exceptions. */\n\texn = false;\n\tif (is_long_mode(vcpu)) {\n\t\t/* Long mode: #GP(0)/#SS(0) if the memory address is in a\n\t\t * non-canonical form. This is the only check on the memory\n\t\t * destination for long mode!\n\t\t */\n\t\texn = is_noncanonical_address(*ret, vcpu);\n\t} else if (is_protmode(vcpu)) {\n\t\t/* Protected mode: apply checks for segment validity in the\n\t\t * following order:\n\t\t * - segment type check (#GP(0) may be thrown)\n\t\t * - usability check (#GP(0)/#SS(0))\n\t\t * - limit check (#GP(0)/#SS(0))\n\t\t */\n\t\tif (wr)\n\t\t\t/* #GP(0) if the destination operand is located in a\n\t\t\t * read-only data segment or any code segment.\n\t\t\t */\n\t\t\texn = ((s.type & 0xa) == 0 || (s.type & 8));\n\t\telse\n\t\t\t/* #GP(0) if the source operand is located in an\n\t\t\t * execute-only code segment\n\t\t\t */\n\t\t\texn = ((s.type & 0xa) == 8);\n\t\tif (exn) {\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\t\treturn 1;\n\t\t}\n\t\t/* Protected mode: #GP(0)/#SS(0) if the segment is unusable.\n\t\t */\n\t\texn = (s.unusable != 0);\n\t\t/* Protected mode: #GP(0)/#SS(0) if the memory\n\t\t * operand is outside the segment limit.\n\t\t */\n\t\texn = exn || (off + sizeof(u64) > s.limit);\n\t}\n\tif (exn) {\n\t\tkvm_queue_exception_e(vcpu,\n\t\t\t\t      seg_reg == VCPU_SREG_SS ?\n\t\t\t\t\t\tSS_VECTOR : GP_VECTOR,\n\t\t\t\t      0);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer)\n{\n\tgva_t gva;\n\tstruct x86_exception e;\n\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmcs_read32(VMX_INSTRUCTION_INFO), false, &gva))\n\t\treturn 1;\n\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, vmpointer,\n\t\t\t\tsizeof(*vmpointer), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int enter_vmx_operation(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs *shadow_vmcs;\n\tint r;\n\n\tr = alloc_loaded_vmcs(&vmx->nested.vmcs02);\n\tif (r < 0)\n\t\tgoto out_vmcs02;\n\n\tvmx->nested.cached_vmcs12 = kmalloc(VMCS12_SIZE, GFP_KERNEL);\n\tif (!vmx->nested.cached_vmcs12)\n\t\tgoto out_cached_vmcs12;\n\n\tif (enable_shadow_vmcs) {\n\t\tshadow_vmcs = alloc_vmcs();\n\t\tif (!shadow_vmcs)\n\t\t\tgoto out_shadow_vmcs;\n\t\t/* mark vmcs as shadow */\n\t\tshadow_vmcs->revision_id |= (1u << 31);\n\t\t/* init shadow vmcs */\n\t\tvmcs_clear(shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = shadow_vmcs;\n\t}\n\n\thrtimer_init(&vmx->nested.preemption_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL_PINNED);\n\tvmx->nested.preemption_timer.function = vmx_preemption_timer_fn;\n\n\tvmx->nested.vmxon = true;\n\treturn 0;\n\nout_shadow_vmcs:\n\tkfree(vmx->nested.cached_vmcs12);\n\nout_cached_vmcs12:\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n\nout_vmcs02:\n\treturn -ENOMEM;\n}\n\n/*\n * Emulate the VMXON instruction.\n * Currently, we just remember that VMX is active, and do not save or even\n * inspect the argument to VMXON (the so-called \"VMXON pointer\") because we\n * do not currently need to store anything in that guest-allocated memory\n * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their\n * argument is different from the VMXON pointer (which the spec says they do).\n */\nstatic int handle_vmon(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\tgpa_t vmptr;\n\tstruct page *page;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst u64 VMXON_NEEDED_FEATURES = FEATURE_CONTROL_LOCKED\n\t\t| FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\n\t/*\n\t * The Intel VMX Instruction Reference lists a bunch of bits that are\n\t * prerequisite to running VMXON, most notably cr4.VMXE must be set to\n\t * 1 (see vmx_set_cr4() for when we allow the guest to set this).\n\t * Otherwise, we should fail with #UD.  But most faulting conditions\n\t * have already been checked by hardware, prior to the VM-exit for\n\t * VMXON.  We do test guest cr4.VMXE because processor CR4 always has\n\t * that bit set to 1 in non-root mode.\n\t */\n\tif (!kvm_read_cr4_bits(vcpu, X86_CR4_VMXE)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (vmx->nested.vmxon) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif ((vmx->msr_ia32_feature_control & VMXON_NEEDED_FEATURES)\n\t\t\t!= VMXON_NEEDED_FEATURES) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\t/*\n\t * SDM 3: 24.11.5\n\t * The first 4 bytes of VMXON region contain the supported\n\t * VMCS revision identifier\n\t *\n\t * Note - IA32_VMX_BASIC[48] will never be 1 for the nested case;\n\t * which replaces physical address width with 32\n\t */\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmptr);\n\tif (is_error_page(page)) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\tif (*(u32 *)kmap(page) != VMCS12_REVISION) {\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\tvmx->nested.vmxon_ptr = vmptr;\n\tret = enter_vmx_operation(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/*\n * Intel's VMX Instruction Reference specifies a common set of prerequisites\n * for running VMX instructions (except VMXON, whose prerequisites are\n * slightly different). It also specifies what exception to inject otherwise.\n * Note that many of these exceptions have priority over VM exits, so they\n * don't have to be checked again here.\n */\nstatic int nested_vmx_check_permission(struct kvm_vcpu *vcpu)\n{\n\tif (!to_vmx(vcpu)->nested.vmxon) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)\n{\n\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL, SECONDARY_EXEC_SHADOW_VMCS);\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull);\n}\n\nstatic inline void nested_release_vmcs12(struct vcpu_vmx *vmx)\n{\n\tif (vmx->nested.current_vmptr == -1ull)\n\t\treturn;\n\n\tif (enable_shadow_vmcs) {\n\t\t/* copy to memory all shadowed fields in case\n\t\t   they were modified */\n\t\tcopy_shadow_to_vmcs12(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t}\n\tvmx->nested.posted_intr_nv = -1;\n\n\t/* Flush VMCS12 to guest memory */\n\tkvm_vcpu_write_guest_page(&vmx->vcpu,\n\t\t\t\t  vmx->nested.current_vmptr >> PAGE_SHIFT,\n\t\t\t\t  vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);\n\n\tvmx->nested.current_vmptr = -1ull;\n}\n\n/*\n * Free whatever needs to be freed from vmx->nested when L1 goes down, or\n * just stops using VMX.\n */\nstatic void free_nested(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n\t\treturn;\n\n\tvmx->nested.vmxon = false;\n\tvmx->nested.smm.vmxon = false;\n\tfree_vpid(vmx->nested.vpid02);\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\tif (enable_shadow_vmcs) {\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\tfree_vmcs(vmx->vmcs01.shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = NULL;\n\t}\n\tkfree(vmx->nested.cached_vmcs12);\n\t/* Unpin physical memory we referred to in the vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n}\n\n/* Emulate the VMXOFF instruction */\nstatic int handle_vmoff(struct kvm_vcpu *vcpu)\n{\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\tfree_nested(to_vmx(vcpu));\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the VMCLEAR instruction */\nstatic int handle_vmclear(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 zero = 0;\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.current_vmptr)\n\t\tnested_release_vmcs12(vmx);\n\n\tkvm_vcpu_write_guest(vcpu,\n\t\t\tvmptr + offsetof(struct vmcs12, launch_state),\n\t\t\t&zero, sizeof(zero));\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);\n\n/* Emulate the VMLAUNCH instruction */\nstatic int handle_vmlaunch(struct kvm_vcpu *vcpu)\n{\n\treturn nested_vmx_run(vcpu, true);\n}\n\n/* Emulate the VMRESUME instruction */\nstatic int handle_vmresume(struct kvm_vcpu *vcpu)\n{\n\n\treturn nested_vmx_run(vcpu, false);\n}\n\n/*\n * Read a vmcs12 field. Since these can have varying lengths and we return\n * one type, we chose the biggest type (u64) and zero-extend the return value\n * to that size. Note that the caller, handle_vmread, might need to use only\n * some of the bits we return here (e.g., on 32-bit guests, only 32 bits of\n * 64-bit fields are to be returned).\n */\nstatic inline int vmcs12_read_any(struct kvm_vcpu *vcpu,\n\t\t\t\t  unsigned long field, u64 *ret)\n{\n\tshort offset = vmcs_field_to_offset(field);\n\tchar *p;\n\n\tif (offset < 0)\n\t\treturn offset;\n\n\tp = ((char *)(get_vmcs12(vcpu))) + offset;\n\n\tswitch (vmcs_field_width(field)) {\n\tcase VMCS_FIELD_WIDTH_NATURAL_WIDTH:\n\t\t*ret = *((natural_width *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U16:\n\t\t*ret = *((u16 *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U32:\n\t\t*ret = *((u32 *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U64:\n\t\t*ret = *((u64 *)p);\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOENT;\n\t}\n}\n\n\nstatic inline int vmcs12_write_any(struct kvm_vcpu *vcpu,\n\t\t\t\t   unsigned long field, u64 field_value){\n\tshort offset = vmcs_field_to_offset(field);\n\tchar *p = ((char *) get_vmcs12(vcpu)) + offset;\n\tif (offset < 0)\n\t\treturn offset;\n\n\tswitch (vmcs_field_width(field)) {\n\tcase VMCS_FIELD_WIDTH_U16:\n\t\t*(u16 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U32:\n\t\t*(u32 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U64:\n\t\t*(u64 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_NATURAL_WIDTH:\n\t\t*(natural_width *)p = field_value;\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOENT;\n\t}\n\n}\n\n/*\n * Copy the writable VMCS shadow fields back to the VMCS12, in case\n * they have been modified by the L1 guest. Note that the \"read-only\"\n * VM-exit information fields are actually writable if the vCPU is\n * configured to support \"VMWRITE to any supported field in the VMCS.\"\n */\nstatic void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)\n{\n\tconst u16 *fields[] = {\n\t\tshadow_read_write_fields,\n\t\tshadow_read_only_fields\n\t};\n\tconst int max_fields[] = {\n\t\tmax_shadow_read_write_fields,\n\t\tmax_shadow_read_only_fields\n\t};\n\tint i, q;\n\tunsigned long field;\n\tu64 field_value;\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\n\tpreempt_disable();\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (q = 0; q < ARRAY_SIZE(fields); q++) {\n\t\tfor (i = 0; i < max_fields[q]; i++) {\n\t\t\tfield = fields[q][i];\n\t\t\tfield_value = __vmcs_readl(field);\n\t\t\tvmcs12_write_any(&vmx->vcpu, field, field_value);\n\t\t}\n\t\t/*\n\t\t * Skip the VM-exit information fields if they are read-only.\n\t\t */\n\t\tif (!nested_cpu_has_vmwrite_any_field(&vmx->vcpu))\n\t\t\tbreak;\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\tpreempt_enable();\n}\n\nstatic void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)\n{\n\tconst u16 *fields[] = {\n\t\tshadow_read_write_fields,\n\t\tshadow_read_only_fields\n\t};\n\tconst int max_fields[] = {\n\t\tmax_shadow_read_write_fields,\n\t\tmax_shadow_read_only_fields\n\t};\n\tint i, q;\n\tunsigned long field;\n\tu64 field_value = 0;\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (q = 0; q < ARRAY_SIZE(fields); q++) {\n\t\tfor (i = 0; i < max_fields[q]; i++) {\n\t\t\tfield = fields[q][i];\n\t\t\tvmcs12_read_any(&vmx->vcpu, field, &field_value);\n\t\t\t__vmcs_writel(field, field_value);\n\t\t}\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n}\n\n/*\n * VMX instructions which assume a current vmcs12 (i.e., that VMPTRLD was\n * used before) all generate the same failure when it is missing.\n */\nstatic int nested_vmx_check_vmcs12(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tif (vmx->nested.current_vmptr == -1ull) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int handle_vmread(struct kvm_vcpu *vcpu)\n{\n\tunsigned long field;\n\tu64 field_value;\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t gva = 0;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\t/* Decode instruction info and find the field to read */\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/* Read the field, zero-extended to a u64 field_value */\n\tif (vmcs12_read_any(vcpu, field, &field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\t/*\n\t * Now copy part of this value to register or memory, as requested.\n\t * Note that the number of bits actually copied is 32 or 64 depending\n\t * on the guest's mode (32 or 64 bit), not on the given field's length.\n\t */\n\tif (vmx_instruction_info & (1u << 10)) {\n\t\tkvm_register_writel(vcpu, (((vmx_instruction_info) >> 3) & 0xf),\n\t\t\tfield_value);\n\t} else {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, true, &gva))\n\t\t\treturn 1;\n\t\t/* _system ok, as hardware has verified cpl=0 */\n\t\tkvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n\nstatic int handle_vmwrite(struct kvm_vcpu *vcpu)\n{\n\tunsigned long field;\n\tgva_t gva;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\n\t/* The value to write might be 32 or 64 bits, depending on L1's long\n\t * mode, and eventually we need to write that into a field of several\n\t * possible lengths. The code below first zero-extends the value to 64\n\t * bit (field_value), and then copies only the appropriate number of\n\t * bits into the vmcs12 field.\n\t */\n\tu64 field_value = 0;\n\tstruct x86_exception e;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tif (vmx_instruction_info & (1u << 10))\n\t\tfield_value = kvm_register_readl(vcpu,\n\t\t\t(((vmx_instruction_info) >> 3) & 0xf));\n\telse {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, false, &gva))\n\t\t\treturn 1;\n\t\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t   &field_value, (is_64_bit_mode(vcpu) ? 8 : 4), &e)) {\n\t\t\tkvm_inject_page_fault(vcpu, &e);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/*\n\t * If the vCPU supports \"VMWRITE to any supported field in the\n\t * VMCS,\" then the \"read-only\" fields are actually read/write.\n\t */\n\tif (vmcs_field_readonly(field) &&\n\t    !nested_cpu_has_vmwrite_any_field(vcpu)) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmcs12_write_any(vcpu, field, field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tswitch (field) {\n#define SHADOW_FIELD_RW(x) case x:\n#include \"vmx_shadow_fields.h\"\n\t\t/*\n\t\t * The fields that can be updated by L1 without a vmexit are\n\t\t * always updated in the vmcs02, the others go down the slow\n\t\t * path of prepare_vmcs02.\n\t\t */\n\t\tbreak;\n\tdefault:\n\t\tvmx->nested.dirty_vmcs12 = true;\n\t\tbreak;\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)\n{\n\tvmx->nested.current_vmptr = vmptr;\n\tif (enable_shadow_vmcs) {\n\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t      SECONDARY_EXEC_SHADOW_VMCS);\n\t\tvmcs_write64(VMCS_LINK_POINTER,\n\t\t\t     __pa(vmx->vmcs01.shadow_vmcs));\n\t\tvmx->nested.sync_shadow_vmcs = true;\n\t}\n\tvmx->nested.dirty_vmcs12 = true;\n}\n\n/* Emulate the VMPTRLD instruction */\nstatic int handle_vmptrld(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmx->nested.current_vmptr != vmptr) {\n\t\tstruct vmcs12 *new_vmcs12;\n\t\tstruct page *page;\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmptr);\n\t\tif (is_error_page(page)) {\n\t\t\tnested_vmx_failInvalid(vcpu);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tnew_vmcs12 = kmap(page);\n\t\tif (new_vmcs12->revision_id != VMCS12_REVISION) {\n\t\t\tkunmap(page);\n\t\t\tkvm_release_page_clean(page);\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\n\t\tnested_release_vmcs12(vmx);\n\t\t/*\n\t\t * Load VMCS12 from guest memory since it is not already\n\t\t * cached.\n\t\t */\n\t\tmemcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\n\t\tset_current_vmptr(vmx, vmptr);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the VMPTRST instruction */\nstatic int handle_vmptrst(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t vmcs_gva;\n\tstruct x86_exception e;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\tvmx_instruction_info, true, &vmcs_gva))\n\t\treturn 1;\n\t/* ok to use *_system, as hardware has verified cpl=0 */\n\tif (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,\n\t\t\t\t (void *)&to_vmx(vcpu)->nested.current_vmptr,\n\t\t\t\t sizeof(u64), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the INVEPT instruction */\nstatic int handle_invept(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info, types;\n\tunsigned long type;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_EPT) ||\n\t    !(vmx->nested.msrs.ept_caps & VMX_EPT_INVEPT_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (vmx->nested.msrs.ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;\n\n\tif (type >= 32 || !(types & (1 << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\t/* According to the Intel VMX instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, false, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\tswitch (type) {\n\tcase VMX_EPT_EXTENT_GLOBAL:\n\t/*\n\t * TODO: track mappings and invalidate\n\t * single context requests appropriately\n\t */\n\tcase VMX_EPT_EXTENT_CONTEXT:\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\t\tnested_vmx_succeed(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_invvpid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info;\n\tunsigned long type, types;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 vpid;\n\t\tu64 gla;\n\t} operand;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_VPID) ||\n\t\t\t!(vmx->nested.msrs.vpid_caps & VMX_VPID_INVVPID_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (vmx->nested.msrs.vpid_caps &\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;\n\n\tif (type >= 32 || !(types & (1 << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\t/* according to the intel vmx instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, false, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\tif (operand.vpid >> 16) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tswitch (type) {\n\tcase VMX_VPID_EXTENT_INDIVIDUAL_ADDR:\n\t\tif (!operand.vpid ||\n\t\t    is_noncanonical_address(operand.gla, vcpu)) {\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tif (cpu_has_vmx_invvpid_individual_addr() &&\n\t\t    vmx->nested.vpid02) {\n\t\t\t__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR,\n\t\t\t\tvmx->nested.vpid02, operand.gla);\n\t\t} else\n\t\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_SINGLE_CONTEXT:\n\tcase VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:\n\t\tif (!operand.vpid) {\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_ALL_CONTEXT:\n\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_pml_full(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\n\ttrace_kvm_pml_full(vcpu->vcpu_id);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\t/*\n\t * PML buffer FULL happened while executing iret from NMI,\n\t * \"blocked by NMI\" bit has to be set before next VM entry.\n\t */\n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\tGUEST_INTR_STATE_NMI);\n\n\t/*\n\t * PML buffer already flushed at beginning of VMEXIT. Nothing to do\n\t * here.., and there's no userspace involvement needed for PML.\n\t */\n\treturn 1;\n}\n\nstatic int handle_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\tkvm_lapic_expired_hv_timer(vcpu);\n\treturn 1;\n}\n\nstatic bool valid_ept_address(struct kvm_vcpu *vcpu, u64 address)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint maxphyaddr = cpuid_maxphyaddr(vcpu);\n\n\t/* Check for memory type validity */\n\tswitch (address & VMX_EPTP_MT_MASK) {\n\tcase VMX_EPTP_MT_UC:\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT))\n\t\t\treturn false;\n\t\tbreak;\n\tcase VMX_EPTP_MT_WB:\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t/* only 4 levels page-walk length are valid */\n\tif ((address & VMX_EPTP_PWL_MASK) != VMX_EPTP_PWL_4)\n\t\treturn false;\n\n\t/* Reserved bits should not be set */\n\tif (address >> maxphyaddr || ((address >> 7) & 0x1f))\n\t\treturn false;\n\n\t/* AD, if set, should be supported */\n\tif (address & VMX_EPTP_AD_ENABLE_BIT) {\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmcs12 *vmcs12)\n{\n\tu32 index = vcpu->arch.regs[VCPU_REGS_RCX];\n\tu64 address;\n\tbool accessed_dirty;\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (!nested_cpu_has_eptp_switching(vmcs12) ||\n\t    !nested_cpu_has_ept(vmcs12))\n\t\treturn 1;\n\n\tif (index >= VMFUNC_EPTP_ENTRIES)\n\t\treturn 1;\n\n\n\tif (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,\n\t\t\t\t     &address, index * 8, 8))\n\t\treturn 1;\n\n\taccessed_dirty = !!(address & VMX_EPTP_AD_ENABLE_BIT);\n\n\t/*\n\t * If the (L2) guest does a vmfunc to the currently\n\t * active ept pointer, we don't have to do anything else\n\t */\n\tif (vmcs12->ept_pointer != address) {\n\t\tif (!valid_ept_address(vcpu, address))\n\t\t\treturn 1;\n\n\t\tkvm_mmu_unload(vcpu);\n\t\tmmu->ept_ad = accessed_dirty;\n\t\tmmu->base_role.ad_disabled = !accessed_dirty;\n\t\tvmcs12->ept_pointer = address;\n\t\t/*\n\t\t * TODO: Check what's the correct approach in case\n\t\t * mmu reload fails. Currently, we just let the next\n\t\t * reload potentially fail\n\t\t */\n\t\tkvm_mmu_reload(vcpu);\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_vmfunc(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12;\n\tu32 function = vcpu->arch.regs[VCPU_REGS_RAX];\n\n\t/*\n\t * VMFUNC is only supported for nested guests, but we always enable the\n\t * secondary control for simplicity; for non-nested mode, fake that we\n\t * didn't by injecting #UD.\n\t */\n\tif (!is_guest_mode(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcs12 = get_vmcs12(vcpu);\n\tif ((vmcs12->vm_function_control & (1 << function)) == 0)\n\t\tgoto fail;\n\n\tswitch (function) {\n\tcase 0:\n\t\tif (nested_vmx_eptp_switching(vcpu, vmcs12))\n\t\t\tgoto fail;\n\t\tbreak;\n\tdefault:\n\t\tgoto fail;\n\t}\n\treturn kvm_skip_emulated_instruction(vcpu);\n\nfail:\n\tnested_vmx_vmexit(vcpu, vmx->exit_reason,\n\t\t\t  vmcs_read32(VM_EXIT_INTR_INFO),\n\t\t\t  vmcs_readl(EXIT_QUALIFICATION));\n\treturn 1;\n}\n\n/*\n * The exit handlers return 1 if the exit was handled fully and guest execution\n * may resume.  Otherwise they set the kvm_run parameter to indicate what needs\n * to be done to userspace and return 0.\n */\nstatic int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {\n\t[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,\n\t[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,\n\t[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,\n\t[EXIT_REASON_NMI_WINDOW]\t      = handle_nmi_window,\n\t[EXIT_REASON_IO_INSTRUCTION]          = handle_io,\n\t[EXIT_REASON_CR_ACCESS]               = handle_cr,\n\t[EXIT_REASON_DR_ACCESS]               = handle_dr,\n\t[EXIT_REASON_CPUID]                   = handle_cpuid,\n\t[EXIT_REASON_MSR_READ]                = handle_rdmsr,\n\t[EXIT_REASON_MSR_WRITE]               = handle_wrmsr,\n\t[EXIT_REASON_PENDING_INTERRUPT]       = handle_interrupt_window,\n\t[EXIT_REASON_HLT]                     = handle_halt,\n\t[EXIT_REASON_INVD]\t\t      = handle_invd,\n\t[EXIT_REASON_INVLPG]\t\t      = handle_invlpg,\n\t[EXIT_REASON_RDPMC]                   = handle_rdpmc,\n\t[EXIT_REASON_VMCALL]                  = handle_vmcall,\n\t[EXIT_REASON_VMCLEAR]\t              = handle_vmclear,\n\t[EXIT_REASON_VMLAUNCH]                = handle_vmlaunch,\n\t[EXIT_REASON_VMPTRLD]                 = handle_vmptrld,\n\t[EXIT_REASON_VMPTRST]                 = handle_vmptrst,\n\t[EXIT_REASON_VMREAD]                  = handle_vmread,\n\t[EXIT_REASON_VMRESUME]                = handle_vmresume,\n\t[EXIT_REASON_VMWRITE]                 = handle_vmwrite,\n\t[EXIT_REASON_VMOFF]                   = handle_vmoff,\n\t[EXIT_REASON_VMON]                    = handle_vmon,\n\t[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,\n\t[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,\n\t[EXIT_REASON_APIC_WRITE]              = handle_apic_write,\n\t[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,\n\t[EXIT_REASON_WBINVD]                  = handle_wbinvd,\n\t[EXIT_REASON_XSETBV]                  = handle_xsetbv,\n\t[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,\n\t[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,\n\t[EXIT_REASON_GDTR_IDTR]\t\t      = handle_desc,\n\t[EXIT_REASON_LDTR_TR]\t\t      = handle_desc,\n\t[EXIT_REASON_EPT_VIOLATION]\t      = handle_ept_violation,\n\t[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,\n\t[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,\n\t[EXIT_REASON_MWAIT_INSTRUCTION]\t      = handle_mwait,\n\t[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,\n\t[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,\n\t[EXIT_REASON_INVEPT]                  = handle_invept,\n\t[EXIT_REASON_INVVPID]                 = handle_invvpid,\n\t[EXIT_REASON_RDRAND]                  = handle_invalid_op,\n\t[EXIT_REASON_RDSEED]                  = handle_invalid_op,\n\t[EXIT_REASON_XSAVES]                  = handle_xsaves,\n\t[EXIT_REASON_XRSTORS]                 = handle_xrstors,\n\t[EXIT_REASON_PML_FULL]\t\t      = handle_pml_full,\n\t[EXIT_REASON_VMFUNC]                  = handle_vmfunc,\n\t[EXIT_REASON_PREEMPTION_TIMER]\t      = handle_preemption_timer,\n};\n\nstatic const int kvm_vmx_max_exit_handlers =\n\tARRAY_SIZE(kvm_vmx_exit_handlers);\n\nstatic bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification;\n\tgpa_t bitmap, last_bitmap;\n\tunsigned int port;\n\tint size;\n\tu8 b;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_UNCOND_IO_EXITING);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\n\tlast_bitmap = (gpa_t)-1;\n\tb = -1;\n\n\twhile (size > 0) {\n\t\tif (port < 0x8000)\n\t\t\tbitmap = vmcs12->io_bitmap_a;\n\t\telse if (port < 0x10000)\n\t\t\tbitmap = vmcs12->io_bitmap_b;\n\t\telse\n\t\t\treturn true;\n\t\tbitmap += (port & 0x7fff) / 8;\n\n\t\tif (last_bitmap != bitmap)\n\t\t\tif (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))\n\t\t\t\treturn true;\n\t\tif (b & (1 << (port & 7)))\n\t\t\treturn true;\n\n\t\tport++;\n\t\tsize--;\n\t\tlast_bitmap = bitmap;\n\t}\n\n\treturn false;\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle an MSR access access,\n * rather than handle it ourselves in L0. I.e., check whether L1 expressed\n * disinterest in the current event (read or write a specific MSR) by using an\n * MSR bitmap. This may be the case even when L0 doesn't use MSR bitmaps.\n */\nstatic bool nested_vmx_exit_handled_msr(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12, u32 exit_reason)\n{\n\tu32 msr_index = vcpu->arch.regs[VCPU_REGS_RCX];\n\tgpa_t bitmap;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn true;\n\n\t/*\n\t * The MSR_BITMAP page is divided into four 1024-byte bitmaps,\n\t * for the four combinations of read/write and low/high MSR numbers.\n\t * First we need to figure out which of the four to use:\n\t */\n\tbitmap = vmcs12->msr_bitmap;\n\tif (exit_reason == EXIT_REASON_MSR_WRITE)\n\t\tbitmap += 2048;\n\tif (msr_index >= 0xc0000000) {\n\t\tmsr_index -= 0xc0000000;\n\t\tbitmap += 1024;\n\t}\n\n\t/* Then read the msr_index'th bit from this bitmap: */\n\tif (msr_index < 1024*8) {\n\t\tunsigned char b;\n\t\tif (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))\n\t\t\treturn true;\n\t\treturn 1 & (b >> (msr_index & 7));\n\t} else\n\t\treturn true; /* let L1 handle the wrong parameter */\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle a CR access exit,\n * rather than handle it ourselves in L0. I.e., check if L1 wanted to\n * intercept (via guest_host_mask etc.) the current event.\n */\nstatic bool nested_vmx_exit_handled_cr(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tint cr = exit_qualification & 15;\n\tint reg;\n\tunsigned long val;\n\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\treg = (exit_qualification >> 8) & 15;\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\tif (vmcs12->cr0_guest_host_mask &\n\t\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif ((vmcs12->cr3_target_count >= 1 &&\n\t\t\t\t\tvmcs12->cr3_target_value0 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 2 &&\n\t\t\t\t\tvmcs12->cr3_target_value1 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 3 &&\n\t\t\t\t\tvmcs12->cr3_target_value2 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 4 &&\n\t\t\t\t\tvmcs12->cr3_target_value3 == val))\n\t\t\t\treturn false;\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR3_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tif (vmcs12->cr4_guest_host_mask &\n\t\t\t    (vmcs12->cr4_read_shadow ^ val))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tif ((vmcs12->cr0_guest_host_mask & X86_CR0_TS) &&\n\t\t    (vmcs12->cr0_read_shadow & X86_CR0_TS))\n\t\t\treturn true;\n\t\tbreak;\n\tcase 1: /* mov from cr */\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR3_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR8_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\t/*\n\t\t * lmsw can change bits 1..3 of cr0, and only set bit 0 of\n\t\t * cr0. Other attempted changes are ignored, with no exit.\n\t\t */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\tif (vmcs12->cr0_guest_host_mask & 0xe &\n\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\treturn true;\n\t\tif ((vmcs12->cr0_guest_host_mask & 0x1) &&\n\t\t    !(vmcs12->cr0_read_shadow & 0x1) &&\n\t\t    (val & 0x1))\n\t\t\treturn true;\n\t\tbreak;\n\t}\n\treturn false;\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle an exit, or 0 if we\n * should handle it ourselves in L0 (and then continue L2). Only call this\n * when in is_guest_mode (L2).\n */\nstatic bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn false;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn true;\n\t}\n\n\t/*\n\t * The host physical addresses of some pages of guest memory\n\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t * Page). The CPU may write to these pages via their host\n\t * physical address while L2 is running, bypassing any\n\t * address-translation-based dirty tracking (e.g. EPT write\n\t * protection).\n\t *\n\t * Mark them dirty on every exit from L2 to prevent them from\n\t * getting out of sync with dirty tracking.\n\t */\n\tnested_mark_vmcs12_pages_dirty(vcpu);\n\n\ttrace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,\n\t\t\t\tvmcs_readl(EXIT_QUALIFICATION),\n\t\t\t\tvmx->idt_vectoring_info,\n\t\t\t\tintr_info,\n\t\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t\t\t\tKVM_ISA_VMX);\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (is_nmi(intr_info))\n\t\t\treturn false;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn !vmx->vcpu.arch.apf.host_apf_reason && enable_ept;\n\t\telse if (is_no_device(intr_info) &&\n\t\t\t !(vmcs12->guest_cr0 & X86_CR0_TS))\n\t\t\treturn false;\n\t\telse if (is_debug(intr_info) &&\n\t\t\t vcpu->guest_debug &\n\t\t\t (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\telse if (is_breakpoint(intr_info) &&\n\t\t\t vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn false;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn true;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn true;\n\tcase EXIT_REASON_CPUID:\n\t\treturn true;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn true;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDRAND:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDRAND_EXITING);\n\tcase EXIT_REASON_RDSEED:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDSEED_EXITING);\n\tcase EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn true;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_TRAP_FLAG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn false;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);\n\tcase EXIT_REASON_APIC_ACCESS:\n\tcase EXIT_REASON_APIC_WRITE:\n\tcase EXIT_REASON_EOI_INDUCED:\n\t\t/*\n\t\t * The controls for \"virtualize APIC accesses,\" \"APIC-\n\t\t * register virtualization,\" and \"virtual-interrupt\n\t\t * delivery\" only come from vmcs12.\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_EPT_VIOLATION:\n\t\t/*\n\t\t * L0 always deals with the EPT violation. If nested EPT is\n\t\t * used, and the nested mmu code discovers that the address is\n\t\t * missing in the guest EPT table (EPT12), the EPT violation\n\t\t * will be injected with nested_ept_inject_page_fault()\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\t/*\n\t\t * L2 never uses directly L1's EPT, but rather L0's own EPT\n\t\t * table (shadow on EPT) or a merged EPT table that L0 built\n\t\t * (EPT on EPT). So any problems with the structure of the\n\t\t * table is L0's fault.\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_INVPCID:\n\t\treturn\n\t\t\tnested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_INVPCID) &&\n\t\t\tnested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn true;\n\tcase EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:\n\t\t/*\n\t\t * This should never happen, since it is not possible to\n\t\t * set XSS to a non-zero value---neither in L1 nor in L2.\n\t\t * If if it were, XSS would have to be checked against\n\t\t * the XSS exit bitmap in vmcs12.\n\t\t */\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn false;\n\tcase EXIT_REASON_PML_FULL:\n\t\t/* We emulate PML support to L1. */\n\t\treturn false;\n\tcase EXIT_REASON_VMFUNC:\n\t\t/* VM functions are emulated through L2->L0 vmexits. */\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)\n{\n\tu32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\n\t/*\n\t * At this point, the exit interruption info in exit_intr_info\n\t * is only valid for EXCEPTION_NMI exits.  For EXTERNAL_INTERRUPT\n\t * we need to query the in-kernel LAPIC.\n\t */\n\tWARN_ON(exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT);\n\tif ((exit_intr_info &\n\t     (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) ==\n\t    (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tvmcs12->vm_exit_intr_error_code =\n\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\t}\n\n\tnested_vmx_vmexit(vcpu, exit_reason, exit_intr_info,\n\t\t\t  vmcs_readl(EXIT_QUALIFICATION));\n\treturn 1;\n}\n\nstatic void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)\n{\n\t*info1 = vmcs_readl(EXIT_QUALIFICATION);\n\t*info2 = vmcs_read32(VM_EXIT_INTR_INFO);\n}\n\nstatic void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)\n{\n\tif (vmx->pml_pg) {\n\t\t__free_page(vmx->pml_pg);\n\t\tvmx->pml_pg = NULL;\n\t}\n}\n\nstatic void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 *pml_buf;\n\tu16 pml_idx;\n\n\tpml_idx = vmcs_read16(GUEST_PML_INDEX);\n\n\t/* Do nothing if PML buffer is empty */\n\tif (pml_idx == (PML_ENTITY_NUM - 1))\n\t\treturn;\n\n\t/* PML index always points to next available PML buffer entity */\n\tif (pml_idx >= PML_ENTITY_NUM)\n\t\tpml_idx = 0;\n\telse\n\t\tpml_idx++;\n\n\tpml_buf = page_address(vmx->pml_pg);\n\tfor (; pml_idx < PML_ENTITY_NUM; pml_idx++) {\n\t\tu64 gpa;\n\n\t\tgpa = pml_buf[pml_idx];\n\t\tWARN_ON(gpa & (PAGE_SIZE - 1));\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);\n\t}\n\n\t/* reset PML index */\n\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n}\n\n/*\n * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.\n * Called before reporting dirty_bitmap to userspace.\n */\nstatic void kvm_flush_pml_buffers(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_vcpu *vcpu;\n\t/*\n\t * We only need to kick vcpu out of guest mode here, as PML buffer\n\t * is flushed at beginning of all VMEXITs, and it's obvious that only\n\t * vcpus running in guest are possible to have unflushed GPAs in PML\n\t * buffer.\n\t */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nstatic void vmx_dump_sel(char *name, uint32_t sel)\n{\n\tpr_err(\"%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read16(sel),\n\t       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),\n\t       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),\n\t       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));\n}\n\nstatic void vmx_dump_dtsel(char *name, uint32_t limit)\n{\n\tpr_err(\"%s                           limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read32(limit),\n\t       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));\n}\n\nstatic void dump_vmcs(void)\n{\n\tu32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);\n\tu32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);\n\tu32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);\n\tu32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);\n\tu32 secondary_exec_control = 0;\n\tunsigned long cr4 = vmcs_readl(GUEST_CR4);\n\tu64 efer = vmcs_read64(GUEST_IA32_EFER);\n\tint i, n;\n\n\tif (cpu_has_secondary_exec_ctrls())\n\t\tsecondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\n\tpr_err(\"*** Guest State ***\\n\");\n\tpr_err(\"CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),\n\t       vmcs_readl(CR0_GUEST_HOST_MASK));\n\tpr_err(\"CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));\n\tpr_err(\"CR3 = 0x%016lx\\n\", vmcs_readl(GUEST_CR3));\n\tif ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&\n\t    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))\n\t{\n\t\tpr_err(\"PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));\n\t\tpr_err(\"PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));\n\t}\n\tpr_err(\"RSP = 0x%016lx  RIP = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));\n\tpr_err(\"RFLAGS=0x%08lx         DR7 = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(GUEST_SYSENTER_ESP),\n\t       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));\n\tvmx_dump_sel(\"CS:  \", GUEST_CS_SELECTOR);\n\tvmx_dump_sel(\"DS:  \", GUEST_DS_SELECTOR);\n\tvmx_dump_sel(\"SS:  \", GUEST_SS_SELECTOR);\n\tvmx_dump_sel(\"ES:  \", GUEST_ES_SELECTOR);\n\tvmx_dump_sel(\"FS:  \", GUEST_FS_SELECTOR);\n\tvmx_dump_sel(\"GS:  \", GUEST_GS_SELECTOR);\n\tvmx_dump_dtsel(\"GDTR:\", GUEST_GDTR_LIMIT);\n\tvmx_dump_sel(\"LDTR:\", GUEST_LDTR_SELECTOR);\n\tvmx_dump_dtsel(\"IDTR:\", GUEST_IDTR_LIMIT);\n\tvmx_dump_sel(\"TR:  \", GUEST_TR_SELECTOR);\n\tif ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||\n\t    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))\n\t\tpr_err(\"EFER =     0x%016llx  PAT = 0x%016llx\\n\",\n\t\t       efer, vmcs_read64(GUEST_IA32_PAT));\n\tpr_err(\"DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\\n\",\n\t       vmcs_read64(GUEST_IA32_DEBUGCTL),\n\t       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));\n\tif (cpu_has_load_perf_global_ctrl &&\n\t    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));\n\tif (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)\n\t\tpr_err(\"BndCfgS = 0x%016llx\\n\", vmcs_read64(GUEST_BNDCFGS));\n\tpr_err(\"Interruptibility = %08x  ActivityState = %08x\\n\",\n\t       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),\n\t       vmcs_read32(GUEST_ACTIVITY_STATE));\n\tif (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\tpr_err(\"InterruptStatus = %04x\\n\",\n\t\t       vmcs_read16(GUEST_INTR_STATUS));\n\n\tpr_err(\"*** Host State ***\\n\");\n\tpr_err(\"RIP = 0x%016lx  RSP = 0x%016lx\\n\",\n\t       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));\n\tpr_err(\"CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\\n\",\n\t       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),\n\t       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),\n\t       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),\n\t       vmcs_read16(HOST_TR_SELECTOR));\n\tpr_err(\"FSBase=%016lx GSBase=%016lx TRBase=%016lx\\n\",\n\t       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),\n\t       vmcs_readl(HOST_TR_BASE));\n\tpr_err(\"GDTBase=%016lx IDTBase=%016lx\\n\",\n\t       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));\n\tpr_err(\"CR0=%016lx CR3=%016lx CR4=%016lx\\n\",\n\t       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),\n\t       vmcs_readl(HOST_CR4));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(HOST_IA32_SYSENTER_ESP),\n\t       vmcs_read32(HOST_IA32_SYSENTER_CS),\n\t       vmcs_readl(HOST_IA32_SYSENTER_EIP));\n\tif (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))\n\t\tpr_err(\"EFER = 0x%016llx  PAT = 0x%016llx\\n\",\n\t\t       vmcs_read64(HOST_IA32_EFER),\n\t\t       vmcs_read64(HOST_IA32_PAT));\n\tif (cpu_has_load_perf_global_ctrl &&\n\t    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));\n\n\tpr_err(\"*** Control State ***\\n\");\n\tpr_err(\"PinBased=%08x CPUBased=%08x SecondaryExec=%08x\\n\",\n\t       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);\n\tpr_err(\"EntryControls=%08x ExitControls=%08x\\n\", vmentry_ctl, vmexit_ctl);\n\tpr_err(\"ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\\n\",\n\t       vmcs_read32(EXCEPTION_BITMAP),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));\n\tpr_err(\"VMEntry: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),\n\t       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));\n\tpr_err(\"VMExit: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_EXIT_INTR_INFO),\n\t       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));\n\tpr_err(\"        reason=%08x qualification=%016lx\\n\",\n\t       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));\n\tpr_err(\"IDTVectoring: info=%08x errcode=%08x\\n\",\n\t       vmcs_read32(IDT_VECTORING_INFO_FIELD),\n\t       vmcs_read32(IDT_VECTORING_ERROR_CODE));\n\tpr_err(\"TSC Offset = 0x%016llx\\n\", vmcs_read64(TSC_OFFSET));\n\tif (secondary_exec_control & SECONDARY_EXEC_TSC_SCALING)\n\t\tpr_err(\"TSC Multiplier = 0x%016llx\\n\",\n\t\t       vmcs_read64(TSC_MULTIPLIER));\n\tif (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)\n\t\tpr_err(\"TPR Threshold = 0x%02x\\n\", vmcs_read32(TPR_THRESHOLD));\n\tif (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)\n\t\tpr_err(\"PostedIntrVec = 0x%02x\\n\", vmcs_read16(POSTED_INTR_NV));\n\tif ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))\n\t\tpr_err(\"EPT pointer = 0x%016llx\\n\", vmcs_read64(EPT_POINTER));\n\tn = vmcs_read32(CR3_TARGET_COUNT);\n\tfor (i = 0; i + 1 < n; i += 4)\n\t\tpr_err(\"CR3 target%u=%016lx target%u=%016lx\\n\",\n\t\t       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),\n\t\t       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));\n\tif (i < n)\n\t\tpr_err(\"CR3 target%u=%016lx\\n\",\n\t\t       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));\n\tif (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)\n\t\tpr_err(\"PLE Gap=%08x Window=%08x\\n\",\n\t\t       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));\n\tif (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)\n\t\tpr_err(\"Virtual processor ID = 0x%04x\\n\",\n\t\t       vmcs_read16(VIRTUAL_PROCESSOR_ID));\n}\n\n/*\n * The guest has exited.  See if we can fix it or if we need userspace\n * assistance.\n */\nstatic int vmx_handle_exit(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\n\ttrace_kvm_exit(exit_reason, vcpu, KVM_ISA_VMX);\n\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.\n\t */\n\tif (enable_pml)\n\t\tvmx_flush_pml_buffer(vcpu);\n\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\n\tif (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason))\n\t\treturn nested_vmx_reflect_vmexit(vcpu, exit_reason);\n\n\tif (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(vmx->fail)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\t(exit_reason != EXIT_REASON_EXCEPTION_NMI &&\n\t\t\texit_reason != EXIT_REASON_EPT_VIOLATION &&\n\t\t\texit_reason != EXIT_REASON_PML_FULL &&\n\t\t\texit_reason != EXIT_REASON_TASK_SWITCH)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.ndata++;\n\t\t\tvcpu->run->internal.data[3] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (vmx_interrupt_allowed(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}\n\n\tif (exit_reason < kvm_vmx_max_exit_handlers\n\t    && kvm_vmx_exit_handlers[exit_reason])\n\t\treturn kvm_vmx_exit_handlers[exit_reason](vcpu);\n\telse {\n\t\tvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\n\t\t\t\texit_reason);\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n}\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t\tnested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn;\n\n\tif (irr == -1 || tpr < irr) {\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t\treturn;\n\t}\n\n\tvmcs_write32(TPR_THRESHOLD, irr);\n}\n\nstatic void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)\n{\n\tu32 sec_exec_control;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\t/* Postpone execution until vmcs01 is the current VMCS. */\n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;\n\t\treturn;\n\t}\n\n\tif (!cpu_need_tpr_shadow(vcpu))\n\t\treturn;\n\n\tsec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\tsec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);\n\n\tswitch (kvm_get_apic_mode(vcpu)) {\n\tcase LAPIC_MODE_INVALID:\n\t\tWARN_ONCE(true, \"Invalid local APIC state\");\n\tcase LAPIC_MODE_DISABLED:\n\t\tbreak;\n\tcase LAPIC_MODE_XAPIC:\n\t\tif (flexpriority_enabled) {\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\t\t\tvmx_flush_tlb(vcpu, true);\n\t\t}\n\t\tbreak;\n\tcase LAPIC_MODE_X2APIC:\n\t\tif (cpu_has_vmx_virtualize_x2apic_mode())\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\t\tbreak;\n\t}\n\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);\n\n\tvmx_update_msr_bitmap(vcpu);\n}\n\nstatic void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)\n{\n\tif (!is_guest_mode(vcpu)) {\n\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n}\n\nstatic void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (max_isr == -1)\n\t\tmax_isr = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = status >> 8;\n\tif (max_isr != old) {\n\t\tstatus &= 0xff;\n\t\tstatus |= max_isr << 8;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_set_rvi(int vector)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (vector == -1)\n\t\tvector = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = (u8)status & 0xff;\n\tif ((u8)vector != old) {\n\t\tstatus &= ~0xff;\n\t\tstatus |= (u8)vector;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)\n{\n\t/*\n\t * When running L2, updating RVI is only relevant when\n\t * vmcs12 virtual-interrupt-delivery enabled.\n\t * However, it can be enabled only when L1 also\n\t * intercepts external-interrupts and in that case\n\t * we should not update vmcs02 RVI but instead intercept\n\t * interrupt. Therefore, do nothing when running L2.\n\t */\n\tif (!is_guest_mode(vcpu))\n\t\tvmx_set_rvi(max_irr);\n}\n\nstatic int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tbool max_irr_updated;\n\n\tWARN_ON(!vcpu->arch.apicv_active);\n\tif (pi_test_on(&vmx->pi_desc)) {\n\t\tpi_clear_on(&vmx->pi_desc);\n\t\t/*\n\t\t * IOMMU can write to PIR.ON, so the barrier matters even on UP.\n\t\t * But on x86 this is just a compiler barrier anyway.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tmax_irr_updated =\n\t\t\tkvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);\n\n\t\t/*\n\t\t * If we are running L2 and L1 has a new pending interrupt\n\t\t * which can be injected, we should re-evaluate\n\t\t * what should be done with this new L1 interrupt.\n\t\t * If L1 intercepts external-interrupts, we should\n\t\t * exit from L2 to L1. Otherwise, interrupt should be\n\t\t * delivered directly to L2.\n\t\t */\n\t\tif (is_guest_mode(vcpu) && max_irr_updated) {\n\t\t\tif (nested_exit_on_intr(vcpu))\n\t\t\t\tkvm_vcpu_exiting_guest_mode(vcpu);\n\t\t\telse\n\t\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t}\n\t} else {\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\t}\n\tvmx_hwapic_irr_update(vcpu, max_irr);\n\treturn max_irr;\n}\n\nstatic void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)\n{\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\tvmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);\n\tvmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);\n\tvmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);\n\tvmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);\n}\n\nstatic void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tpi_clear_on(&vmx->pi_desc);\n\tmemset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));\n}\n\nstatic void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info = 0;\n\tu16 basic_exit_reason = (u16)vmx->exit_reason;\n\n\tif (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tif (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tvmx->exit_intr_info = exit_intr_info;\n\n\t/* if exit due to PF check for async PF */\n\tif (is_page_fault(exit_intr_info))\n\t\tvmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||\n\t    is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif (is_nmi(exit_intr_info)) {\n\t\tkvm_before_interrupt(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_interrupt(&vmx->vcpu);\n\t}\n}\n\nstatic void vmx_handle_external_intr(struct kvm_vcpu *vcpu)\n{\n\tu32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\n\tif ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))\n\t\t\t== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {\n\t\tunsigned int vector;\n\t\tunsigned long entry;\n\t\tgate_desc *desc;\n\t\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n#ifdef CONFIG_X86_64\n\t\tunsigned long tmp;\n#endif\n\n\t\tvector =  exit_intr_info & INTR_INFO_VECTOR_MASK;\n\t\tdesc = (gate_desc *)vmx->host_idt_base + vector;\n\t\tentry = gate_offset(desc);\n\t\tasm volatile(\n#ifdef CONFIG_X86_64\n\t\t\t\"mov %%\" _ASM_SP \", %[sp]\\n\\t\"\n\t\t\t\"and $0xfffffffffffffff0, %%\" _ASM_SP \"\\n\\t\"\n\t\t\t\"push $%c[ss]\\n\\t\"\n\t\t\t\"push %[sp]\\n\\t\"\n#endif\n\t\t\t\"pushf\\n\\t\"\n\t\t\t__ASM_SIZE(push) \" $%c[cs]\\n\\t\"\n\t\t\tCALL_NOSPEC\n\t\t\t:\n#ifdef CONFIG_X86_64\n\t\t\t[sp]\"=&r\"(tmp),\n#endif\n\t\t\tASM_CALL_CONSTRAINT\n\t\t\t:\n\t\t\tTHUNK_TARGET(entry),\n\t\t\t[ss]\"i\"(__KERNEL_DS),\n\t\t\t[cs]\"i\"(__KERNEL_CS)\n\t\t\t);\n\t}\n}\nSTACK_FRAME_NON_STANDARD(vmx_handle_external_intr);\n\nstatic bool vmx_has_high_real_mode_segbase(void)\n{\n\treturn enable_unrestricted_guest || emulate_invalid_guest_state;\n}\n\nstatic bool vmx_mpx_supported(void)\n{\n\treturn (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&\n\t\t(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);\n}\n\nstatic bool vmx_xsaves_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_XSAVES;\n}\n\nstatic void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info;\n\tbool unblock_nmi;\n\tu8 vector;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tif (enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\t\treturn;\n\t\t/*\n\t\t * Can't use vmx->exit_intr_info since we're not sure what\n\t\t * the exit reason is.\n\t\t */\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\t\tunblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;\n\t\tvector = exit_intr_info & INTR_INFO_VECTOR_MASK;\n\t\t/*\n\t\t * SDM 3: 27.7.1.2 (September 2008)\n\t\t * Re-set bit \"block by NMI\" before VM entry if vmexit caused by\n\t\t * a guest IRET fault.\n\t\t * SDM 3: 23.2.2 (September 2008)\n\t\t * Bit 12 is undefined in any of the following cases:\n\t\t *  If the VM exit sets the valid bit in the IDT-vectoring\n\t\t *   information field.\n\t\t *  If the VM exit is due to a double fault.\n\t\t */\n\t\tif ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&\n\t\t    vector != DF_VECTOR && !idtv_info_valid)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)\n\t\t\t\t  & GUEST_INTR_STATE_NMI);\n\t} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->vnmi_blocked_time +=\n\t\t\tktime_to_ns(ktime_sub(ktime_get(),\n\t\t\t\t\t      vmx->loaded_vmcs->entry_time));\n}\n\nstatic void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,\n\t\t\t\t      u32 idt_vectoring_info,\n\t\t\t\t      int instr_len_field,\n\t\t\t\t      int error_code_field)\n{\n\tu8 vector;\n\tint type;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tif (!idtv_info_valid)\n\t\treturn;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;\n\ttype = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;\n\n\tswitch (type) {\n\tcase INTR_TYPE_NMI_INTR:\n\t\tvcpu->arch.nmi_injected = true;\n\t\t/*\n\t\t * SDM 3: 27.7.1.2 (September 2008)\n\t\t * Clear bit \"block by NMI\" before VM entry if a NMI\n\t\t * delivery faulted.\n\t\t */\n\t\tvmx_set_nmi_mask(vcpu, false);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\t/* fall through */\n\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\tif (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\tu32 err = vmcs_read32(error_code_field);\n\t\t\tkvm_requeue_exception_e(vcpu, vector, err);\n\t\t} else\n\t\t\tkvm_requeue_exception(vcpu, vector);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_INTR:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\t/* fall through */\n\tcase INTR_TYPE_EXT_INTR:\n\t\tkvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void vmx_complete_interrupts(struct vcpu_vmx *vmx)\n{\n\t__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,\n\t\t\t\t  VM_EXIT_INSTRUCTION_LEN,\n\t\t\t\t  IDT_VECTORING_ERROR_CODE);\n}\n\nstatic void vmx_cancel_injection(struct kvm_vcpu *vcpu)\n{\n\t__vmx_complete_interrupts(vcpu,\n\t\t\t\t  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t\t\t\t  VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t\t  VM_ENTRY_EXCEPTION_ERROR_CODE);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n}\n\nstatic void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)\n{\n\tint i, nr_msrs;\n\tstruct perf_guest_switch_msr *msrs;\n\n\tmsrs = perf_guest_get_msrs(&nr_msrs);\n\n\tif (!msrs)\n\t\treturn;\n\n\tfor (i = 0; i < nr_msrs; i++)\n\t\tif (msrs[i].host == msrs[i].guest)\n\t\t\tclear_atomic_switch_msr(vmx, msrs[i].msr);\n\t\telse\n\t\t\tadd_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,\n\t\t\t\t\tmsrs[i].host);\n}\n\nstatic void vmx_arm_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 tscl;\n\tu32 delta_tsc;\n\n\tif (vmx->hv_deadline_tsc == -1)\n\t\treturn;\n\n\ttscl = rdtsc();\n\tif (vmx->hv_deadline_tsc > tscl)\n\t\t/* sure to be 32 bit only because checked on set_hv_timer */\n\t\tdelta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>\n\t\t\tcpu_preemption_timer_multi);\n\telse\n\t\tdelta_tsc = 0;\n\n\tvmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);\n}\n\nstatic void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long cr3, cr4, evmcs_rsp;\n\n\t/* Record the guest's net vcpu time for enforced NMI injections. */\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->entry_time = ktime_get();\n\n\t/* Don't enter VMX if guest state is invalid, let the exit handler\n\t   start emulation until we arrive back to a valid state */\n\tif (vmx->emulation_required)\n\t\treturn;\n\n\tif (vmx->ple_window_dirty) {\n\t\tvmx->ple_window_dirty = false;\n\t\tvmcs_write32(PLE_WINDOW, vmx->ple_window);\n\t}\n\n\tif (vmx->nested.sync_shadow_vmcs) {\n\t\tcopy_vmcs12_to_shadow(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t}\n\n\tif (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n\tif (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n\n\tcr3 = __get_current_cr3_fast();\n\tif (unlikely(cr3 != vmx->loaded_vmcs->vmcs_host_cr3)) {\n\t\tvmcs_writel(HOST_CR3, cr3);\n\t\tvmx->loaded_vmcs->vmcs_host_cr3 = cr3;\n\t}\n\n\tcr4 = cr4_read_shadow();\n\tif (unlikely(cr4 != vmx->loaded_vmcs->vmcs_host_cr4)) {\n\t\tvmcs_writel(HOST_CR4, cr4);\n\t\tvmx->loaded_vmcs->vmcs_host_cr4 = cr4;\n\t}\n\n\t/* When single-stepping over STI and MOV SS, we must clear the\n\t * corresponding interruptibility bits in the guest state. Otherwise\n\t * vmentry fails as it then expects bit 14 (BS) in pending debug\n\t * exceptions being set, but that's not correct for the guest debugging\n\t * case. */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&\n\t    vcpu->arch.pkru != vmx->host_pkru)\n\t\t__write_pkru(vcpu->arch.pkru);\n\n\tatomic_switch_perf_msrs(vmx);\n\n\tvmx_arm_hv_timer(vcpu);\n\n\t/*\n\t * If this vCPU has touched SPEC_CTRL, restore the guest's value if\n\t * it's non-zero. Since vmentry is serialising on affected CPUs, there\n\t * is no need to worry about the conditional branch over the wrmsr\n\t * being speculatively taken.\n\t */\n\tif (vmx->spec_ctrl)\n\t\tnative_wrmsrl(MSR_IA32_SPEC_CTRL, vmx->spec_ctrl);\n\n\tvmx->__launched = vmx->loaded_vmcs->launched;\n\n\tevmcs_rsp = static_branch_unlikely(&enable_evmcs) ?\n\t\t(unsigned long)&current_evmcs->host_rsp : 0;\n\n\tasm(\n\t\t/* Store host registers */\n\t\t\"push %%\" _ASM_DX \"; push %%\" _ASM_BP \";\"\n\t\t\"push %%\" _ASM_CX \" \\n\\t\" /* placeholder for guest rcx */\n\t\t\"push %%\" _ASM_CX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t\"je 1f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t/* Avoid VMWRITE when Enlightened VMCS is in use */\n\t\t\"test %%\" _ASM_SI \", %%\" _ASM_SI \" \\n\\t\"\n\t\t\"jz 2f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", (%%\" _ASM_SI \") \\n\\t\"\n\t\t\"jmp 1f \\n\\t\"\n\t\t\"2: \\n\\t\"\n\t\t__ex(ASM_VMX_VMWRITE_RSP_RDX) \"\\n\\t\"\n\t\t\"1: \\n\\t\"\n\t\t/* Reload cr2 if changed */\n\t\t\"mov %c[cr2](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %%cr2, %%\" _ASM_DX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_AX \", %%\" _ASM_DX \" \\n\\t\"\n\t\t\"je 3f \\n\\t\"\n\t\t\"mov %%\" _ASM_AX\", %%cr2 \\n\\t\"\n\t\t\"3: \\n\\t\"\n\t\t/* Check if vmlaunch of vmresume is needed */\n\t\t\"cmpl $0, %c[launched](%0) \\n\\t\"\n\t\t/* Load guest registers.  Don't clobber flags. */\n\t\t\"mov %c[rax](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %c[rbx](%0), %%\" _ASM_BX \" \\n\\t\"\n\t\t\"mov %c[rdx](%0), %%\" _ASM_DX \" \\n\\t\"\n\t\t\"mov %c[rsi](%0), %%\" _ASM_SI \" \\n\\t\"\n\t\t\"mov %c[rdi](%0), %%\" _ASM_DI \" \\n\\t\"\n\t\t\"mov %c[rbp](%0), %%\" _ASM_BP \" \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%0),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%0),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%0), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%0), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%0), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%0), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%0), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%0), %%r15 \\n\\t\"\n#endif\n\t\t\"mov %c[rcx](%0), %%\" _ASM_CX \" \\n\\t\" /* kills %0 (ecx) */\n\n\t\t/* Enter guest mode */\n\t\t\"jne 1f \\n\\t\"\n\t\t__ex(ASM_VMX_VMLAUNCH) \"\\n\\t\"\n\t\t\"jmp 2f \\n\\t\"\n\t\t\"1: \" __ex(ASM_VMX_VMRESUME) \"\\n\\t\"\n\t\t\"2: \"\n\t\t/* Save guest registers, load host registers, keep flags */\n\t\t\"mov %0, %c[wordsize](%%\" _ASM_SP \") \\n\\t\"\n\t\t\"pop %0 \\n\\t\"\n\t\t\"setbe %c[fail](%0)\\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[rax](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BX \", %c[rbx](%0) \\n\\t\"\n\t\t__ASM_SIZE(pop) \" %c[rcx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DX \", %c[rdx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_SI \", %c[rsi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DI \", %c[rdi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BP \", %c[rbp](%0) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%0) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%0) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%0) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%0) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%0) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%0) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%0) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%0) \\n\\t\"\n\t\t\"xor %%r8d,  %%r8d \\n\\t\"\n\t\t\"xor %%r9d,  %%r9d \\n\\t\"\n\t\t\"xor %%r10d, %%r10d \\n\\t\"\n\t\t\"xor %%r11d, %%r11d \\n\\t\"\n\t\t\"xor %%r12d, %%r12d \\n\\t\"\n\t\t\"xor %%r13d, %%r13d \\n\\t\"\n\t\t\"xor %%r14d, %%r14d \\n\\t\"\n\t\t\"xor %%r15d, %%r15d \\n\\t\"\n#endif\n\t\t\"mov %%cr2, %%\" _ASM_AX \"   \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[cr2](%0) \\n\\t\"\n\n\t\t\"xor %%eax, %%eax \\n\\t\"\n\t\t\"xor %%ebx, %%ebx \\n\\t\"\n\t\t\"xor %%esi, %%esi \\n\\t\"\n\t\t\"xor %%edi, %%edi \\n\\t\"\n\t\t\"pop  %%\" _ASM_BP \"; pop  %%\" _ASM_DX \" \\n\\t\"\n\t\t\".pushsection .rodata \\n\\t\"\n\t\t\".global vmx_return \\n\\t\"\n\t\t\"vmx_return: \" _ASM_PTR \" 2b \\n\\t\"\n\t\t\".popsection\"\n\t      : : \"c\"(vmx), \"d\"((unsigned long)HOST_RSP), \"S\"(evmcs_rsp),\n\t\t[launched]\"i\"(offsetof(struct vcpu_vmx, __launched)),\n\t\t[fail]\"i\"(offsetof(struct vcpu_vmx, fail)),\n\t\t[host_rsp]\"i\"(offsetof(struct vcpu_vmx, host_rsp)),\n\t\t[rax]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),\n\t\t[rbx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t[rcx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t[rdx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t[rsi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t[rdi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t[rbp]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),\n#ifdef CONFIG_X86_64\n\t\t[r8]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t[r9]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t[r10]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t[r11]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t[r12]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t[r13]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t[r14]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t[r15]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),\n#endif\n\t\t[cr2]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),\n\t\t[wordsize]\"i\"(sizeof(ulong))\n\t      : \"cc\", \"memory\"\n#ifdef CONFIG_X86_64\n\t\t, \"rax\", \"rbx\", \"rdi\"\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\", \"r12\", \"r13\", \"r14\", \"r15\"\n#else\n\t\t, \"eax\", \"ebx\", \"edi\"\n#endif\n\t      );\n\n\t/*\n\t * We do not use IBRS in the kernel. If this vCPU has used the\n\t * SPEC_CTRL MSR it may have left it on; save the value and\n\t * turn it off. This is much more efficient than blindly adding\n\t * it to the atomic save/restore list. Especially as the former\n\t * (Saving guest MSRs on vmexit) doesn't even exist in KVM.\n\t *\n\t * For non-nested case:\n\t * If the L01 MSR bitmap does not intercept the MSR, then we need to\n\t * save it.\n\t *\n\t * For nested case:\n\t * If the L02 MSR bitmap does not intercept the MSR, then we need to\n\t * save it.\n\t */\n\tif (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))\n\t\tvmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);\n\n\tif (vmx->spec_ctrl)\n\t\tnative_wrmsrl(MSR_IA32_SPEC_CTRL, 0);\n\n\t/* Eliminate branch target predictions from guest mode */\n\tvmexit_fill_RSB();\n\n\t/* All fields are clean at this point */\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tcurrent_evmcs->hv_clean_fields |=\n\t\t\tHV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;\n\n\t/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */\n\tif (vmx->host_debugctlmsr)\n\t\tupdate_debugctlmsr(vmx->host_debugctlmsr);\n\n#ifndef CONFIG_X86_64\n\t/*\n\t * The sysexit path does not restore ds/es, so we must set them to\n\t * a reasonable value ourselves.\n\t *\n\t * We can't defer this to vmx_load_host_state() since that function\n\t * may be executed in interrupt context, which saves and restore segments\n\t * around it, nullifying its effect.\n\t */\n\tloadsegment(ds, __USER_DS);\n\tloadsegment(es, __USER_DS);\n#endif\n\n\tvcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)\n\t\t\t\t  | (1 << VCPU_EXREG_RFLAGS)\n\t\t\t\t  | (1 << VCPU_EXREG_PDPTR)\n\t\t\t\t  | (1 << VCPU_EXREG_SEGMENTS)\n\t\t\t\t  | (1 << VCPU_EXREG_CR3));\n\tvcpu->arch.regs_dirty = 0;\n\n\t/*\n\t * eager fpu is enabled if PKEY is supported and CR4 is switched\n\t * back on host, so it is safe to read guest PKRU from current\n\t * XSAVE.\n\t */\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {\n\t\tvcpu->arch.pkru = __read_pkru();\n\t\tif (vcpu->arch.pkru != vmx->host_pkru)\n\t\t\t__write_pkru(vmx->host_pkru);\n\t}\n\n\tvmx->nested.nested_run_pending = 0;\n\tvmx->idt_vectoring_info = 0;\n\n\tvmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);\n\tif (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\treturn;\n\n\tvmx->loaded_vmcs->launched = 1;\n\tvmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);\n\n\tvmx_complete_atomic_exit(vmx);\n\tvmx_recover_nmi_blocking(vmx);\n\tvmx_complete_interrupts(vmx);\n}\nSTACK_FRAME_NON_STANDARD(vmx_vcpu_run);\n\nstatic struct kvm *vmx_vm_alloc(void)\n{\n\tstruct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));\n\treturn &kvm_vmx->kvm;\n}\n\nstatic void vmx_vm_free(struct kvm *kvm)\n{\n\tvfree(to_kvm_vmx(kvm));\n}\n\nstatic void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint cpu;\n\n\tif (vmx->loaded_vmcs == vmcs)\n\t\treturn;\n\n\tcpu = get_cpu();\n\tvmx->loaded_vmcs = vmcs;\n\tvmx_vcpu_put(vcpu);\n\tvmx_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\n/*\n * Ensure that the current vmcs of the logical processor is the\n * vmcs01 of the vcpu before calling free_nested().\n */\nstatic void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)\n{\n       struct vcpu_vmx *vmx = to_vmx(vcpu);\n\n       vcpu_load(vcpu);\n       vmx_switch_vmcs(vcpu, &vmx->vmcs01);\n       free_nested(vmx);\n       vcpu_put(vcpu);\n}\n\nstatic void vmx_free_vcpu(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (enable_pml)\n\t\tvmx_destroy_pml_buffer(vmx);\n\tfree_vpid(vmx->vpid);\n\tleave_guest_mode(vcpu);\n\tvmx_free_vcpu_nested(vcpu);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\n\tkfree(vmx->guest_msrs);\n\tkvm_vcpu_uninit(vcpu);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n}\n\nstatic struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)\n{\n\tint err;\n\tstruct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tunsigned long *msr_bitmap;\n\tint cpu;\n\n\tif (!vmx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvmx->vpid = allocate_vpid();\n\n\terr = kvm_vcpu_init(&vmx->vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = -ENOMEM;\n\n\t/*\n\t * If PML is turned on, failure on enabling PML just results in failure\n\t * of creating the vcpu, therefore we can simplify PML logic (by\n\t * avoiding dealing with cases, such as enabling PML partially on vcpus\n\t * for the guest, etc.\n\t */\n\tif (enable_pml) {\n\t\tvmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\t\tif (!vmx->pml_pg)\n\t\t\tgoto uninit_vcpu;\n\t}\n\n\tvmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tBUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])\n\t\t     > PAGE_SIZE);\n\n\tif (!vmx->guest_msrs)\n\t\tgoto free_pml;\n\n\terr = alloc_loaded_vmcs(&vmx->vmcs01);\n\tif (err < 0)\n\t\tgoto free_msrs;\n\n\tmsr_bitmap = vmx->vmcs01.msr_bitmap;\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_FS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_GS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);\n\tvmx->msr_bitmap_mode = 0;\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\tcpu = get_cpu();\n\tvmx_vcpu_load(&vmx->vcpu, cpu);\n\tvmx->vcpu.cpu = cpu;\n\tvmx_vcpu_setup(vmx);\n\tvmx_vcpu_put(&vmx->vcpu);\n\tput_cpu();\n\tif (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {\n\t\terr = alloc_apic_access_page(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (enable_ept && !enable_unrestricted_guest) {\n\t\terr = init_rmode_identity_map(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (nested) {\n\t\tnested_vmx_setup_ctls_msrs(&vmx->nested.msrs,\n\t\t\t\t\t   kvm_vcpu_apicv_active(&vmx->vcpu));\n\t\tvmx->nested.vpid02 = allocate_vpid();\n\t}\n\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\n\tvmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;\n\n\t/*\n\t * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR\n\t * or POSTED_INTR_WAKEUP_VECTOR.\n\t */\n\tvmx->pi_desc.nv = POSTED_INTR_VECTOR;\n\tvmx->pi_desc.sn = 1;\n\n\treturn &vmx->vcpu;\n\nfree_vmcs:\n\tfree_vpid(vmx->nested.vpid02);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\nfree_msrs:\n\tkfree(vmx->guest_msrs);\nfree_pml:\n\tvmx_destroy_pml_buffer(vmx);\nuninit_vcpu:\n\tkvm_vcpu_uninit(&vmx->vcpu);\nfree_vcpu:\n\tfree_vpid(vmx->vpid);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n\treturn ERR_PTR(err);\n}\n\nstatic int vmx_vm_init(struct kvm *kvm)\n{\n\tif (!ple_gap)\n\t\tkvm->arch.pause_in_guest = true;\n\treturn 0;\n}\n\nstatic void __init vmx_check_processor_compat(void *rtn)\n{\n\tstruct vmcs_config vmcs_conf;\n\n\t*(int *)rtn = 0;\n\tif (setup_vmcs_config(&vmcs_conf) < 0)\n\t\t*(int *)rtn = -EIO;\n\tnested_vmx_setup_ctls_msrs(&vmcs_conf.nested, enable_apicv);\n\tif (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config)) != 0) {\n\t\tprintk(KERN_ERR \"kvm: CPU %d feature inconsistency!\\n\",\n\t\t\t\tsmp_processor_id());\n\t\t*(int *)rtn = -EIO;\n\t}\n}\n\nstatic u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)\n{\n\tu8 cache;\n\tu64 ipat = 0;\n\n\t/* For VT-d and EPT combination\n\t * 1. MMIO: always map as UC\n\t * 2. EPT with VT-d:\n\t *   a. VT-d without snooping control feature: can't guarantee the\n\t *\tresult, try to trust guest.\n\t *   b. VT-d with snooping control feature: snooping control feature of\n\t *\tVT-d engine can guarantee the cache correctness. Just set it\n\t *\tto WB to keep consistent with host. So the same as item 3.\n\t * 3. EPT without VT-d: always map as WB and set IPAT=1 to keep\n\t *    consistent with host MTRR\n\t */\n\tif (is_mmio) {\n\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tif (!kvm_arch_has_noncoherent_dma(vcpu->kvm)) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tcache = MTRR_TYPE_WRBACK;\n\t\tgoto exit;\n\t}\n\n\tif (kvm_read_cr0(vcpu) & X86_CR0_CD) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\t\tcache = MTRR_TYPE_WRBACK;\n\t\telse\n\t\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tcache = kvm_mtrr_get_guest_memory_type(vcpu, gfn);\n\nexit:\n\treturn (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;\n}\n\nstatic int vmx_get_lpage_level(void)\n{\n\tif (enable_ept && !cpu_has_vmx_ept_1g_page())\n\t\treturn PT_DIRECTORY_LEVEL;\n\telse\n\t\t/* For shadow and EPT supported 1GB page */\n\t\treturn PT_PDPE_LEVEL;\n}\n\nstatic void vmcs_set_secondary_exec_control(u32 new_ctl)\n{\n\t/*\n\t * These bits in the secondary execution controls field\n\t * are dynamic, the others are mostly based on the hypervisor\n\t * architecture and the guest's CPUID.  Do not touch the\n\t * dynamic bits.\n\t */\n\tu32 mask =\n\t\tSECONDARY_EXEC_SHADOW_VMCS |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\tSECONDARY_EXEC_DESC;\n\n\tu32 cur_ctl = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\n\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL,\n\t\t     (new_ctl & ~mask) | (cur_ctl & mask));\n}\n\n/*\n * Generate MSR_IA32_VMX_CR{0,4}_FIXED1 according to CPUID. Only set bits\n * (indicating \"allowed-1\") if they are supported in the guest's CPUID.\n */\nstatic void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_cpuid_entry2 *entry;\n\n\tvmx->nested.msrs.cr0_fixed1 = 0xffffffff;\n\tvmx->nested.msrs.cr4_fixed1 = X86_CR4_PCE;\n\n#define cr4_fixed1_update(_cr4_mask, _reg, _cpuid_mask) do {\t\t\\\n\tif (entry && (entry->_reg & (_cpuid_mask)))\t\t\t\\\n\t\tvmx->nested.msrs.cr4_fixed1 |= (_cr4_mask);\t\\\n} while (0)\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0x1, 0);\n\tcr4_fixed1_update(X86_CR4_VME,        edx, bit(X86_FEATURE_VME));\n\tcr4_fixed1_update(X86_CR4_PVI,        edx, bit(X86_FEATURE_VME));\n\tcr4_fixed1_update(X86_CR4_TSD,        edx, bit(X86_FEATURE_TSC));\n\tcr4_fixed1_update(X86_CR4_DE,         edx, bit(X86_FEATURE_DE));\n\tcr4_fixed1_update(X86_CR4_PSE,        edx, bit(X86_FEATURE_PSE));\n\tcr4_fixed1_update(X86_CR4_PAE,        edx, bit(X86_FEATURE_PAE));\n\tcr4_fixed1_update(X86_CR4_MCE,        edx, bit(X86_FEATURE_MCE));\n\tcr4_fixed1_update(X86_CR4_PGE,        edx, bit(X86_FEATURE_PGE));\n\tcr4_fixed1_update(X86_CR4_OSFXSR,     edx, bit(X86_FEATURE_FXSR));\n\tcr4_fixed1_update(X86_CR4_OSXMMEXCPT, edx, bit(X86_FEATURE_XMM));\n\tcr4_fixed1_update(X86_CR4_VMXE,       ecx, bit(X86_FEATURE_VMX));\n\tcr4_fixed1_update(X86_CR4_SMXE,       ecx, bit(X86_FEATURE_SMX));\n\tcr4_fixed1_update(X86_CR4_PCIDE,      ecx, bit(X86_FEATURE_PCID));\n\tcr4_fixed1_update(X86_CR4_OSXSAVE,    ecx, bit(X86_FEATURE_XSAVE));\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0x7, 0);\n\tcr4_fixed1_update(X86_CR4_FSGSBASE,   ebx, bit(X86_FEATURE_FSGSBASE));\n\tcr4_fixed1_update(X86_CR4_SMEP,       ebx, bit(X86_FEATURE_SMEP));\n\tcr4_fixed1_update(X86_CR4_SMAP,       ebx, bit(X86_FEATURE_SMAP));\n\tcr4_fixed1_update(X86_CR4_PKE,        ecx, bit(X86_FEATURE_PKU));\n\tcr4_fixed1_update(X86_CR4_UMIP,       ecx, bit(X86_FEATURE_UMIP));\n\n#undef cr4_fixed1_update\n}\n\nstatic void vmx_cpuid_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tvmx_compute_secondary_exec_control(vmx);\n\t\tvmcs_set_secondary_exec_control(vmx->secondary_exec_control);\n\t}\n\n\tif (nested_vmx_allowed(vcpu))\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\telse\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\n\tif (nested_vmx_allowed(vcpu))\n\t\tnested_vmx_cr_fixed1_bits_update(vcpu);\n}\n\nstatic void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)\n{\n\tif (func == 1 && nested)\n\t\tentry->ecx |= bit(X86_FEATURE_VMX);\n}\n\nstatic void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason;\n\tunsigned long exit_qualification = vcpu->arch.exit_qualification;\n\n\tif (vmx->nested.pml_full) {\n\t\texit_reason = EXIT_REASON_PML_FULL;\n\t\tvmx->nested.pml_full = false;\n\t\texit_qualification &= INTR_INFO_UNBLOCK_NMI;\n\t} else if (fault->error_code & PFERR_RSVD_MASK)\n\t\texit_reason = EXIT_REASON_EPT_MISCONFIG;\n\telse\n\t\texit_reason = EXIT_REASON_EPT_VIOLATION;\n\n\tnested_vmx_vmexit(vcpu, exit_reason, 0, exit_qualification);\n\tvmcs12->guest_physical_address = fault->address;\n}\n\nstatic bool nested_ept_ad_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn nested_ept_get_cr3(vcpu) & VMX_EPTP_AD_ENABLE_BIT;\n}\n\n/* Callbacks for nested_ept_init_mmu_context: */\n\nstatic unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu)\n{\n\t/* return the page table to be shadowed - in our case, EPT12 */\n\treturn get_vmcs12(vcpu)->ept_pointer;\n}\n\nstatic int nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON(mmu_is_nested(vcpu));\n\tif (!valid_ept_address(vcpu, nested_ept_get_cr3(vcpu)))\n\t\treturn 1;\n\n\tkvm_mmu_unload(vcpu);\n\tkvm_init_shadow_ept_mmu(vcpu,\n\t\t\tto_vmx(vcpu)->nested.msrs.ept_caps &\n\t\t\tVMX_EPT_EXECUTE_ONLY_BIT,\n\t\t\tnested_ept_ad_enabled(vcpu));\n\tvcpu->arch.mmu.set_cr3           = vmx_set_cr3;\n\tvcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;\n\tvcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;\n\n\tvcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;\n\treturn 0;\n}\n\nstatic void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.walk_mmu = &vcpu->arch.mmu;\n}\n\nstatic bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,\n\t\t\t\t\t    u16 error_code)\n{\n\tbool inequality, bit;\n\n\tbit = (vmcs12->exception_bitmap & (1u << PF_VECTOR)) != 0;\n\tinequality =\n\t\t(error_code & vmcs12->page_fault_error_code_mask) !=\n\t\t vmcs12->page_fault_error_code_match;\n\treturn inequality ^ bit;\n}\n\nstatic void vmx_inject_page_fault_nested(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tWARN_ON(!is_guest_mode(vcpu));\n\n\tif (nested_vmx_is_page_fault_vmexit(vmcs12, fault->error_code) &&\n\t\t!to_vmx(vcpu)->nested.nested_run_pending) {\n\t\tvmcs12->vm_exit_intr_error_code = fault->error_code;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  PF_VECTOR | INTR_TYPE_HARD_EXCEPTION |\n\t\t\t\t  INTR_INFO_DELIVER_CODE_MASK | INTR_INFO_VALID_MASK,\n\t\t\t\t  fault->address);\n\t} else {\n\t\tkvm_inject_page_fault(vcpu, fault);\n\t}\n}\n\nstatic inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12);\n\nstatic void nested_get_vmcs12_pages(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct page *page;\n\tu64 hpa;\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\t/*\n\t\t * Translate L1 physical address to host physical\n\t\t * address for vmcs02. Keep the page pinned, so this\n\t\t * physical address remains valid. We keep a reference\n\t\t * to it so we can release it later.\n\t\t */\n\t\tif (vmx->nested.apic_access_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\t\tvmx->nested.apic_access_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->apic_access_addr);\n\t\t/*\n\t\t * If translation failed, no matter: This feature asks\n\t\t * to exit when accessing the given address, and if it\n\t\t * can never be accessed, this feature won't do\n\t\t * anything anyway.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.apic_access_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.apic_access_page);\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\t} else {\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\t\t}\n\t}\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tif (vmx->nested.virtual_apic_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\t\tvmx->nested.virtual_apic_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->virtual_apic_page_addr);\n\n\t\t/*\n\t\t * If translation failed, VM entry will fail because\n\t\t * prepare_vmcs02 set VIRTUAL_APIC_PAGE_ADDR to -1ull.\n\t\t * Failing the vm entry is _not_ what the processor\n\t\t * does but it's basically the only possibility we\n\t\t * have.  We could still enter the guest if CR8 load\n\t\t * exits are enabled, CR8 store exits are enabled, and\n\t\t * virtualize APIC access is disabled; in this case\n\t\t * the processor would never use the TPR shadow and we\n\t\t * could simply clear the bit from the execution\n\t\t * control.  But such a configuration is useless, so\n\t\t * let's keep the code simple.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.virtual_apic_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.virtual_apic_page);\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, hpa);\n\t\t}\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tif (vmx->nested.pi_desc_page) { /* shouldn't happen */\n\t\t\tkunmap(vmx->nested.pi_desc_page);\n\t\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\t\tvmx->nested.pi_desc_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->posted_intr_desc_addr);\n\t\tif (is_error_page(page))\n\t\t\treturn;\n\t\tvmx->nested.pi_desc_page = page;\n\t\tvmx->nested.pi_desc = kmap(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc =\n\t\t\t(struct pi_desc *)((void *)vmx->nested.pi_desc +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR,\n\t\t\tpage_to_phys(vmx->nested.pi_desc_page) +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t}\n\tif (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))\n\t\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t      CPU_BASED_USE_MSR_BITMAPS);\n\telse\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_USE_MSR_BITMAPS);\n}\n\nstatic void vmx_start_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\tu64 preemption_timeout = get_vmcs12(vcpu)->vmx_preemption_timer_value;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vcpu->arch.virtual_tsc_khz == 0)\n\t\treturn;\n\n\t/* Make sure short timeouts reliably trigger an immediate vmexit.\n\t * hrtimer_start does not guarantee this. */\n\tif (preemption_timeout <= 1) {\n\t\tvmx_preemption_timer_fn(&vmx->nested.preemption_timer);\n\t\treturn;\n\t}\n\n\tpreemption_timeout <<= VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\tpreemption_timeout *= 1000000;\n\tdo_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);\n\thrtimer_start(&vmx->nested.preemption_timer,\n\t\t      ns_to_ktime(preemption_timeout), HRTIMER_MODE_REL);\n}\n\nstatic int nested_vmx_check_io_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->io_bitmap_a) ||\n\t    !page_address_valid(vcpu, vmcs12->io_bitmap_b))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->msr_bitmap))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->virtual_apic_page_addr))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Merge L0's and L1's MSR bitmap, return false to indicate that\n * we do not use the hardware.\n */\nstatic inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tint msr;\n\tstruct page *page;\n\tunsigned long *msr_bitmap_l1;\n\tunsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.vmcs02.msr_bitmap;\n\t/*\n\t * pred_cmd & spec_ctrl are trying to verify two things:\n\t *\n\t * 1. L0 gave a permission to L1 to actually passthrough the MSR. This\n\t *    ensures that we do not accidentally generate an L02 MSR bitmap\n\t *    from the L12 MSR bitmap that is too permissive.\n\t * 2. That L1 or L2s have actually used the MSR. This avoids\n\t *    unnecessarily merging of the bitmap if the MSR is unused. This\n\t *    works properly because we only update the L01 MSR bitmap lazily.\n\t *    So even if L0 should pass L1 these MSRs, the L01 bitmap is only\n\t *    updated to reflect this when L1 (or its L2s) actually write to\n\t *    the MSR.\n\t */\n\tbool pred_cmd = !msr_write_intercepted_l01(vcpu, MSR_IA32_PRED_CMD);\n\tbool spec_ctrl = !msr_write_intercepted_l01(vcpu, MSR_IA32_SPEC_CTRL);\n\n\t/* Nothing to do if the MSR bitmap is not in use.  */\n\tif (!cpu_has_vmx_msr_bitmap() ||\n\t    !nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn false;\n\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    !pred_cmd && !spec_ctrl)\n\t\treturn false;\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->msr_bitmap);\n\tif (is_error_page(page))\n\t\treturn false;\n\n\tmsr_bitmap_l1 = (unsigned long *)kmap(page);\n\tif (nested_cpu_has_apic_reg_virt(vmcs12)) {\n\t\t/*\n\t\t * L0 need not intercept reads for MSRs between 0x800 and 0x8ff, it\n\t\t * just lets the processor take the value from the virtual-APIC page;\n\t\t * take those 256 bits directly from the L1 bitmap.\n\t\t */\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\t\tunsigned word = msr / BITS_PER_LONG;\n\t\t\tmsr_bitmap_l0[word] = msr_bitmap_l1[word];\n\t\t\tmsr_bitmap_l0[word + (0x800 / sizeof(long))] = ~0;\n\t\t}\n\t} else {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\t\tunsigned word = msr / BITS_PER_LONG;\n\t\t\tmsr_bitmap_l0[word] = ~0;\n\t\t\tmsr_bitmap_l0[word + (0x800 / sizeof(long))] = ~0;\n\t\t}\n\t}\n\n\tnested_vmx_disable_intercept_for_msr(\n\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\tX2APIC_MSR(APIC_TASKPRI),\n\t\tMSR_TYPE_W);\n\n\tif (nested_cpu_has_vid(vmcs12)) {\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\tX2APIC_MSR(APIC_EOI),\n\t\t\tMSR_TYPE_W);\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\tX2APIC_MSR(APIC_SELF_IPI),\n\t\t\tMSR_TYPE_W);\n\t}\n\n\tif (spec_ctrl)\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tMSR_IA32_SPEC_CTRL,\n\t\t\t\t\tMSR_TYPE_R | MSR_TYPE_W);\n\n\tif (pred_cmd)\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tMSR_IA32_PRED_CMD,\n\t\t\t\t\tMSR_TYPE_W);\n\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\treturn true;\n}\n\nstatic int nested_vmx_check_apic_access_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES) &&\n\t    !page_address_valid(vcpu, vmcs12->apic_access_addr))\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\n\nstatic int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    !nested_cpu_has_apic_reg_virt(vmcs12) &&\n\t    !nested_cpu_has_vid(vmcs12) &&\n\t    !nested_cpu_has_posted_intr(vmcs12))\n\t\treturn 0;\n\n\t/*\n\t * If virtualize x2apic mode is enabled,\n\t * virtualize apic access must be disabled.\n\t */\n\tif (nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If virtual interrupt delivery is enabled,\n\t * we must exit on external interrupts.\n\t */\n\tif (nested_cpu_has_vid(vmcs12) &&\n\t   !nested_exit_on_intr(vcpu))\n\t\treturn -EINVAL;\n\n\t/*\n\t * bits 15:8 should be zero in posted_intr_nv,\n\t * the descriptor address has been already checked\n\t * in nested_get_vmcs12_pages.\n\t */\n\tif (nested_cpu_has_posted_intr(vmcs12) &&\n\t   (!nested_cpu_has_vid(vmcs12) ||\n\t    !nested_exit_intr_ack_set(vcpu) ||\n\t    vmcs12->posted_intr_nv & 0xff00))\n\t\treturn -EINVAL;\n\n\t/* tpr shadow is needed by all apicv features. */\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_switch(struct kvm_vcpu *vcpu,\n\t\t\t\t       unsigned long count_field,\n\t\t\t\t       unsigned long addr_field)\n{\n\tint maxphyaddr;\n\tu64 count, addr;\n\n\tif (vmcs12_read_any(vcpu, count_field, &count) ||\n\t    vmcs12_read_any(vcpu, addr_field, &addr)) {\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\tif (count == 0)\n\t\treturn 0;\n\tmaxphyaddr = cpuid_maxphyaddr(vcpu);\n\tif (!IS_ALIGNED(addr, 16) || addr >> maxphyaddr ||\n\t    (addr + count * sizeof(struct vmx_msr_entry) - 1) >> maxphyaddr) {\n\t\tpr_debug_ratelimited(\n\t\t\t\"nVMX: invalid MSR switch (0x%lx, %d, %llu, 0x%08llx)\",\n\t\t\taddr_field, maxphyaddr, count, addr);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_switch_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (vmcs12->vm_exit_msr_load_count == 0 &&\n\t    vmcs12->vm_exit_msr_store_count == 0 &&\n\t    vmcs12->vm_entry_msr_load_count == 0)\n\t\treturn 0; /* Fast path */\n\tif (nested_vmx_check_msr_switch(vcpu, VM_EXIT_MSR_LOAD_COUNT,\n\t\t\t\t\tVM_EXIT_MSR_LOAD_ADDR) ||\n\t    nested_vmx_check_msr_switch(vcpu, VM_EXIT_MSR_STORE_COUNT,\n\t\t\t\t\tVM_EXIT_MSR_STORE_ADDR) ||\n\t    nested_vmx_check_msr_switch(vcpu, VM_ENTRY_MSR_LOAD_COUNT,\n\t\t\t\t\tVM_ENTRY_MSR_LOAD_ADDR))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_check_pml_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tu64 address = vmcs12->pml_address;\n\tint maxphyaddr = cpuid_maxphyaddr(vcpu);\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML)) {\n\t\tif (!nested_cpu_has_ept(vmcs12) ||\n\t\t    !IS_ALIGNED(address, 4096)  ||\n\t\t    address >> maxphyaddr)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmx_msr_entry *e)\n{\n\t/* x2APIC MSR accesses are not allowed */\n\tif (vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8)\n\t\treturn -EINVAL;\n\tif (e->index == MSR_IA32_UCODE_WRITE || /* SDM Table 35-2 */\n\t    e->index == MSR_IA32_UCODE_REV)\n\t\treturn -EINVAL;\n\tif (e->reserved != 0)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_load_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmx_msr_entry *e)\n{\n\tif (e->index == MSR_FS_BASE ||\n\t    e->index == MSR_GS_BASE ||\n\t    e->index == MSR_IA32_SMM_MONITOR_CTL || /* SMM is not supported */\n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_store_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmx_msr_entry *e)\n{\n\tif (e->index == MSR_IA32_SMBASE || /* SMM is not supported */\n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n/*\n * Load guest's/host's msr at nested entry/exit.\n * return 0 for success, entry index for failure.\n */\nstatic u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\tstruct msr_data msr;\n\n\tmsr.host_initiated = false;\n\tfor (i = 0; i < count; i++) {\n\t\tif (kvm_vcpu_read_guest(vcpu, gpa + i * sizeof(e),\n\t\t\t\t\t&e, sizeof(e))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa + i * sizeof(e));\n\t\t\tgoto fail;\n\t\t}\n\t\tif (nested_vmx_load_msr_check(vcpu, &e)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index, e.reserved);\n\t\t\tgoto fail;\n\t\t}\n\t\tmsr.index = e.index;\n\t\tmsr.data = e.value;\n\t\tif (kvm_set_msr(vcpu, &msr)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, e.value);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\nfail:\n\treturn i + 1;\n}\n\nstatic int nested_vmx_store_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct msr_data msr_info;\n\t\tif (kvm_vcpu_read_guest(vcpu,\n\t\t\t\t\tgpa + i * sizeof(e),\n\t\t\t\t\t&e, 2 * sizeof(u32))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa + i * sizeof(e));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (nested_vmx_store_msr_check(vcpu, &e)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index, e.reserved);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsr_info.host_initiated = false;\n\t\tmsr_info.index = e.index;\n\t\tif (kvm_get_msr(vcpu, &msr_info)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR (%u, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (kvm_vcpu_write_guest(vcpu,\n\t\t\t\t\t gpa + i * sizeof(e) +\n\t\t\t\t\t     offsetof(struct vmx_msr_entry, value),\n\t\t\t\t\t &msr_info.data, sizeof(msr_info.data))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, msr_info.data);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool nested_cr3_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tunsigned long invalid_mask;\n\n\tinvalid_mask = (~0ULL) << cpuid_maxphyaddr(vcpu);\n\treturn (val & invalid_mask) == 0;\n}\n\n/*\n * Load guest's/host's cr3 at nested entry/exit. nested_ept is true if we are\n * emulating VM entry into a guest with EPT enabled.\n * Returns 0 on success, 1 on failure. Invalid state exit qualification code\n * is assigned to entry_failure_code on failure.\n */\nstatic int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3, bool nested_ept,\n\t\t\t       u32 *entry_failure_code)\n{\n\tif (cr3 != kvm_read_cr3(vcpu) || (!nested_ept && pdptrs_changed(vcpu))) {\n\t\tif (!nested_cr3_valid(vcpu, cr3)) {\n\t\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * If PAE paging and EPT are both on, CR3 is not used by the CPU and\n\t\t * must not be dereferenced.\n\t\t */\n\t\tif (!is_long_mode(vcpu) && is_pae(vcpu) && is_paging(vcpu) &&\n\t\t    !nested_ept) {\n\t\t\tif (!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3)) {\n\t\t\t\t*entry_failure_code = ENTRY_FAIL_PDPTE;\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\n\t\tvcpu->arch.cr3 = cr3;\n\t\t__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);\n\t}\n\n\tkvm_mmu_reset_context(vcpu);\n\treturn 0;\n}\n\nstatic void prepare_vmcs02_full(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write16(GUEST_ES_SELECTOR, vmcs12->guest_es_selector);\n\tvmcs_write16(GUEST_SS_SELECTOR, vmcs12->guest_ss_selector);\n\tvmcs_write16(GUEST_DS_SELECTOR, vmcs12->guest_ds_selector);\n\tvmcs_write16(GUEST_FS_SELECTOR, vmcs12->guest_fs_selector);\n\tvmcs_write16(GUEST_GS_SELECTOR, vmcs12->guest_gs_selector);\n\tvmcs_write16(GUEST_LDTR_SELECTOR, vmcs12->guest_ldtr_selector);\n\tvmcs_write16(GUEST_TR_SELECTOR, vmcs12->guest_tr_selector);\n\tvmcs_write32(GUEST_ES_LIMIT, vmcs12->guest_es_limit);\n\tvmcs_write32(GUEST_SS_LIMIT, vmcs12->guest_ss_limit);\n\tvmcs_write32(GUEST_DS_LIMIT, vmcs12->guest_ds_limit);\n\tvmcs_write32(GUEST_FS_LIMIT, vmcs12->guest_fs_limit);\n\tvmcs_write32(GUEST_GS_LIMIT, vmcs12->guest_gs_limit);\n\tvmcs_write32(GUEST_LDTR_LIMIT, vmcs12->guest_ldtr_limit);\n\tvmcs_write32(GUEST_TR_LIMIT, vmcs12->guest_tr_limit);\n\tvmcs_write32(GUEST_GDTR_LIMIT, vmcs12->guest_gdtr_limit);\n\tvmcs_write32(GUEST_IDTR_LIMIT, vmcs12->guest_idtr_limit);\n\tvmcs_write32(GUEST_ES_AR_BYTES, vmcs12->guest_es_ar_bytes);\n\tvmcs_write32(GUEST_SS_AR_BYTES, vmcs12->guest_ss_ar_bytes);\n\tvmcs_write32(GUEST_DS_AR_BYTES, vmcs12->guest_ds_ar_bytes);\n\tvmcs_write32(GUEST_FS_AR_BYTES, vmcs12->guest_fs_ar_bytes);\n\tvmcs_write32(GUEST_GS_AR_BYTES, vmcs12->guest_gs_ar_bytes);\n\tvmcs_write32(GUEST_LDTR_AR_BYTES, vmcs12->guest_ldtr_ar_bytes);\n\tvmcs_write32(GUEST_TR_AR_BYTES, vmcs12->guest_tr_ar_bytes);\n\tvmcs_writel(GUEST_SS_BASE, vmcs12->guest_ss_base);\n\tvmcs_writel(GUEST_DS_BASE, vmcs12->guest_ds_base);\n\tvmcs_writel(GUEST_FS_BASE, vmcs12->guest_fs_base);\n\tvmcs_writel(GUEST_GS_BASE, vmcs12->guest_gs_base);\n\tvmcs_writel(GUEST_LDTR_BASE, vmcs12->guest_ldtr_base);\n\tvmcs_writel(GUEST_TR_BASE, vmcs12->guest_tr_base);\n\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->guest_gdtr_base);\n\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->guest_idtr_base);\n\n\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->guest_sysenter_cs);\n\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS,\n\t\tvmcs12->guest_pending_dbg_exceptions);\n\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->guest_sysenter_esp);\n\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->guest_sysenter_eip);\n\n\tif (nested_cpu_has_xsaves(vmcs12))\n\t\tvmcs_write64(XSS_EXIT_BITMAP, vmcs12->xss_exit_bitmap);\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull);\n\n\tif (cpu_has_vmx_posted_intr())\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_NESTED_VECTOR);\n\n\t/*\n\t * Whether page-faults are trapped is determined by a combination of\n\t * 3 settings: PFEC_MASK, PFEC_MATCH and EXCEPTION_BITMAP.PF.\n\t * If enable_ept, L0 doesn't care about page faults and we should\n\t * set all of these to L1's desires. However, if !enable_ept, L0 does\n\t * care about (at least some) page faults, and because it is not easy\n\t * (if at all possible?) to merge L0 and L1's desires, we simply ask\n\t * to exit on each and every L2 page fault. This is done by setting\n\t * MASK=MATCH=0 and (see below) EB.PF=1.\n\t * Note that below we don't need special code to set EB.PF beyond the\n\t * \"or\"ing of the EB of vmcs01 and vmcs12, because when enable_ept,\n\t * vmcs01's EB.PF is 0 so the \"or\" will take vmcs12's value, and when\n\t * !enable_ept, EB.PF is 1, so the \"or\" will always be 1.\n\t */\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK,\n\t\tenable_ept ? vmcs12->page_fault_error_code_mask : 0);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH,\n\t\tenable_ept ? vmcs12->page_fault_error_code_match : 0);\n\n\t/* All VMFUNCs are currently emulated through L0 vmexits.  */\n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tif (cpu_has_vmx_apicv()) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, vmcs12->eoi_exit_bitmap2);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, vmcs12->eoi_exit_bitmap3);\n\t}\n\n\t/*\n\t * Set host-state according to L0's settings (vmcs12 is irrelevant here)\n\t * Some constant fields are set here by vmx_set_constant_host_state().\n\t * Other fields are different per CPU, and will be set later when\n\t * vmx_vcpu_load() is called, and when vmx_save_host_state() is called.\n\t */\n\tvmx_set_constant_host_state(vmx);\n\n\t/*\n\t * Set the MSR load/store lists to match L0's settings.\n\t */\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tif (vmx_mpx_supported())\n\t\tvmcs_write64(GUEST_BNDCFGS, vmcs12->guest_bndcfgs);\n\n\tif (enable_vpid) {\n\t\tif (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02)\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->nested.vpid02);\n\t\telse\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\t}\n\n\t/*\n\t * L1 may access the L2's PDPTR, so save them to construct vmcs12\n\t */\n\tif (enable_ept) {\n\t\tvmcs_write64(GUEST_PDPTR0, vmcs12->guest_pdptr0);\n\t\tvmcs_write64(GUEST_PDPTR1, vmcs12->guest_pdptr1);\n\t\tvmcs_write64(GUEST_PDPTR2, vmcs12->guest_pdptr2);\n\t\tvmcs_write64(GUEST_PDPTR3, vmcs12->guest_pdptr3);\n\t}\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));\n}\n\n/*\n * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested\n * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function \"merges\" it\n * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2\n * guest in a way that will both be appropriate to L1's requests, and our\n * needs. In addition to modifying the active vmcs (which is vmcs02), this\n * function also has additional necessary side-effects, like setting various\n * vcpu->arch fields.\n * Returns 0 on success, 1 on failure. Invalid state exit qualification code\n * is assigned to entry_failure_code on failure.\n */\nstatic int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t  u32 *entry_failure_code)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exec_control, vmcs12_exec_ctrl;\n\n\tif (vmx->nested.dirty_vmcs12) {\n\t\tprepare_vmcs02_full(vcpu, vmcs12);\n\t\tvmx->nested.dirty_vmcs12 = false;\n\t}\n\n\t/*\n\t * First, the fields that are shadowed.  This must be kept in sync\n\t * with vmx_shadow_fields.h.\n\t */\n\n\tvmcs_write16(GUEST_CS_SELECTOR, vmcs12->guest_cs_selector);\n\tvmcs_write32(GUEST_CS_LIMIT, vmcs12->guest_cs_limit);\n\tvmcs_write32(GUEST_CS_AR_BYTES, vmcs12->guest_cs_ar_bytes);\n\tvmcs_writel(GUEST_ES_BASE, vmcs12->guest_es_base);\n\tvmcs_writel(GUEST_CS_BASE, vmcs12->guest_cs_base);\n\n\t/*\n\t * Not in vmcs02: GUEST_PML_INDEX, HOST_FS_SELECTOR, HOST_GS_SELECTOR,\n\t * HOST_FS_BASE, HOST_GS_BASE.\n\t */\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS)) {\n\t\tkvm_set_dr(vcpu, 7, vmcs12->guest_dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmcs12->guest_ia32_debugctl);\n\t} else {\n\t\tkvm_set_dr(vcpu, 7, vcpu->arch.dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmx->nested.vmcs01_debugctl);\n\t}\n\tif (vmx->nested.nested_run_pending) {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\t     vmcs12->vm_entry_intr_info_field);\n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE,\n\t\t\t     vmcs12->vm_entry_exception_error_code);\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmcs12->vm_entry_instruction_len);\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t     vmcs12->guest_interruptibility_info);\n\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t!(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);\n\t} else {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n\t}\n\tvmx_set_rflags(vcpu, vmcs12->guest_rflags);\n\n\texec_control = vmcs12->pin_based_vm_exec_control;\n\n\t/* Preemption timer setting is only taken from vmcs01.  */\n\texec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\texec_control |= vmcs_config.pin_based_exec_ctrl;\n\tif (vmx->hv_deadline_tsc == -1)\n\t\texec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/* Posted interrupts setting is only taken from vmcs12.  */\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tvmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;\n\t\tvmx->nested.pi_pending = false;\n\t} else {\n\t\texec_control &= ~PIN_BASED_POSTED_INTR;\n\t}\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, exec_control);\n\n\tvmx->nested.preemption_timer_expired = false;\n\tif (nested_cpu_has_preemption_timer(vmcs12))\n\t\tvmx_start_preemption_timer(vcpu);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\texec_control = vmx->secondary_exec_control;\n\n\t\t/* Take the following fields only from vmcs12 */\n\t\texec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_INVPCID |\n\t\t\t\t  SECONDARY_EXEC_RDTSCP |\n\t\t\t\t  SECONDARY_EXEC_XSAVES |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\t\t\t  SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_VMFUNC);\n\t\tif (nested_cpu_has(vmcs12,\n\t\t\t\t   CPU_BASED_ACTIVATE_SECONDARY_CONTROLS)) {\n\t\t\tvmcs12_exec_ctrl = vmcs12->secondary_vm_exec_control &\n\t\t\t\t~SECONDARY_EXEC_ENABLE_PML;\n\t\t\texec_control |= vmcs12_exec_ctrl;\n\t\t}\n\n\t\tif (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\t\tvmcs_write16(GUEST_INTR_STATUS,\n\t\t\t\tvmcs12->guest_intr_status);\n\n\t\t/*\n\t\t * Write an illegal value to APIC_ACCESS_ADDR. Later,\n\t\t * nested_get_vmcs12_pages will either fix it up or\n\t\t * remove the VM execution control.\n\t\t */\n\t\tif (exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, -1ull);\n\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);\n\t}\n\n\t/*\n\t * HOST_RSP is normally set correctly in vmx_vcpu_run() just before\n\t * entry, but only if the current (host) sp changed from the value\n\t * we wrote last (vmx->host_rsp). This cache is no longer relevant\n\t * if we switch vmcs, and rather than hold a separate cache per vmcs,\n\t * here we just force the write to happen on entry.\n\t */\n\tvmx->host_rsp = 0;\n\n\texec_control = vmx_exec_control(vmx); /* L0's desires */\n\texec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n\texec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n\texec_control &= ~CPU_BASED_TPR_SHADOW;\n\texec_control |= vmcs12->cpu_based_vm_exec_control;\n\n\t/*\n\t * Write an illegal value to VIRTUAL_APIC_PAGE_ADDR. Later, if\n\t * nested_get_vmcs12_pages can't fix it up, the illegal value\n\t * will result in a VM entry failure.\n\t */\n\tif (exec_control & CPU_BASED_TPR_SHADOW) {\n\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, -1ull);\n\t\tvmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);\n\t} else {\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_LOAD_EXITING |\n\t\t\t\tCPU_BASED_CR8_STORE_EXITING;\n#endif\n\t}\n\n\t/*\n\t * A vmexit (to either L1 hypervisor or L0 userspace) is always needed\n\t * for I/O port accesses.\n\t */\n\texec_control &= ~CPU_BASED_USE_IO_BITMAPS;\n\texec_control |= CPU_BASED_UNCOND_IO_EXITING;\n\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, exec_control);\n\n\t/* EXCEPTION_BITMAP and CR0_GUEST_HOST_MASK should basically be the\n\t * bitwise-or of what L1 wants to trap for L2, and what we want to\n\t * trap. Note that CR0.TS also needs updating - we do this later.\n\t */\n\tupdate_exception_bitmap(vcpu);\n\tvcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);\n\n\t/* L2->L1 exit controls are emulated - the hardware exit is to L0 so\n\t * we should use its exit controls. Note that VM_EXIT_LOAD_IA32_EFER\n\t * bits are further modified by vmx_set_efer() below.\n\t */\n\tvmcs_write32(VM_EXIT_CONTROLS, vmcs_config.vmexit_ctrl);\n\n\t/* vmcs12's VM_ENTRY_LOAD_IA32_EFER and VM_ENTRY_IA32E_MODE are\n\t * emulated by vmx_set_efer(), below.\n\t */\n\tvm_entry_controls_init(vmx, \n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_LOAD_IA32_EFER &\n\t\t\t~VM_ENTRY_IA32E_MODE) |\n\t\t(vmcs_config.vmentry_ctrl & ~VM_ENTRY_IA32E_MODE));\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT)) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->guest_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->guest_ia32_pat;\n\t} else if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\t}\n\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\n\tif (kvm_has_tsc_control)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tif (enable_vpid) {\n\t\t/*\n\t\t * There is no direct mapping between vpid02 and vpid12, the\n\t\t * vpid02 is per-vCPU for L0 and reused while the value of\n\t\t * vpid12 is changed w/ one invvpid during nested vmentry.\n\t\t * The vpid12 is allocated by L1 for L2, so it will not\n\t\t * influence global bitmap(for vpid01 and vpid02 allocation)\n\t\t * even if spawn a lot of nested vCPUs.\n\t\t */\n\t\tif (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02) {\n\t\t\tif (vmcs12->virtual_processor_id != vmx->nested.last_vpid) {\n\t\t\t\tvmx->nested.last_vpid = vmcs12->virtual_processor_id;\n\t\t\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\t\t}\n\t\t} else {\n\t\t\tvmx_flush_tlb(vcpu, true);\n\t\t}\n\t}\n\n\tif (enable_pml) {\n\t\t/*\n\t\t * Conceptually we want to copy the PML address and index from\n\t\t * vmcs01 here, and then back to vmcs01 on nested vmexit. But,\n\t\t * since we always flush the log on each vmexit, this happens\n\t\t * to be equivalent to simply resetting the fields in vmcs02.\n\t\t */\n\t\tASSERT(vmx->pml_pg);\n\t\tvmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));\n\t\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n\t}\n\n\tif (nested_cpu_has_ept(vmcs12)) {\n\t\tif (nested_ept_init_mmu_context(vcpu)) {\n\t\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\t\treturn 1;\n\t\t}\n\t} else if (nested_cpu_has2(vmcs12,\n\t\t\t\t   SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\t/*\n\t * This sets GUEST_CR0 to vmcs12->guest_cr0, possibly modifying those\n\t * bits which we consider mandatory enabled.\n\t * The CR0_READ_SHADOW is what L2 should have expected to read given\n\t * the specifications by L1; It's not enough to take\n\t * vmcs12->cr0_read_shadow because on our cr0_guest_host_mask we we\n\t * have more bits than L1 expected.\n\t */\n\tvmx_set_cr0(vcpu, vmcs12->guest_cr0);\n\tvmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));\n\n\tvmx_set_cr4(vcpu, vmcs12->guest_cr4);\n\tvmcs_writel(CR4_READ_SHADOW, nested_read_cr4(vmcs12));\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER))\n\t\tvcpu->arch.efer = vmcs12->guest_ia32_efer;\n\telse if (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE)\n\t\tvcpu->arch.efer |= (EFER_LMA | EFER_LME);\n\telse\n\t\tvcpu->arch.efer &= ~(EFER_LMA | EFER_LME);\n\t/* Note: modifies VM_ENTRY/EXIT_CONTROLS and GUEST/HOST_IA32_EFER */\n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\t/*\n\t * Guest state is invalid and unrestricted guest is disabled,\n\t * which means L1 attempted VMEntry to L2 with invalid state.\n\t * Fail the VMEntry.\n\t */\n\tif (vmx->emulation_required) {\n\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\treturn 1;\n\t}\n\n\t/* Shadow page tables on either EPT or shadow page tables. */\n\tif (nested_vmx_load_cr3(vcpu, vmcs12->guest_cr3, nested_cpu_has_ept(vmcs12),\n\t\t\t\tentry_failure_code))\n\t\treturn 1;\n\n\tif (!enable_ept)\n\t\tvcpu->arch.walk_mmu->inject_page_fault = vmx_inject_page_fault_nested;\n\n\tkvm_register_write(vcpu, VCPU_REGS_RSP, vmcs12->guest_rsp);\n\tkvm_register_write(vcpu, VCPU_REGS_RIP, vmcs12->guest_rip);\n\treturn 0;\n}\n\nstatic int nested_vmx_check_nmi_controls(struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_nmi_exiting(vmcs12) &&\n\t    nested_cpu_has_virtual_nmis(vmcs12))\n\t\treturn -EINVAL;\n\n\tif (!nested_cpu_has_virtual_nmis(vmcs12) &&\n\t    nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int check_vmentry_prereqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vmcs12->guest_activity_state != GUEST_ACTIVITY_ACTIVE &&\n\t    vmcs12->guest_activity_state != GUEST_ACTIVITY_HLT)\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_io_bitmap_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_msr_bitmap_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_apic_access_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_tpr_shadow_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_apicv_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_msr_switch_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_pml_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (!vmx_control_verify(vmcs12->cpu_based_vm_exec_control,\n\t\t\t\tvmx->nested.msrs.procbased_ctls_low,\n\t\t\t\tvmx->nested.msrs.procbased_ctls_high) ||\n\t    (nested_cpu_has(vmcs12, CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) &&\n\t     !vmx_control_verify(vmcs12->secondary_vm_exec_control,\n\t\t\t\t vmx->nested.msrs.secondary_ctls_low,\n\t\t\t\t vmx->nested.msrs.secondary_ctls_high)) ||\n\t    !vmx_control_verify(vmcs12->pin_based_vm_exec_control,\n\t\t\t\tvmx->nested.msrs.pinbased_ctls_low,\n\t\t\t\tvmx->nested.msrs.pinbased_ctls_high) ||\n\t    !vmx_control_verify(vmcs12->vm_exit_controls,\n\t\t\t\tvmx->nested.msrs.exit_ctls_low,\n\t\t\t\tvmx->nested.msrs.exit_ctls_high) ||\n\t    !vmx_control_verify(vmcs12->vm_entry_controls,\n\t\t\t\tvmx->nested.msrs.entry_ctls_low,\n\t\t\t\tvmx->nested.msrs.entry_ctls_high))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_nmi_controls(vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_cpu_has_vmfunc(vmcs12)) {\n\t\tif (vmcs12->vm_function_control &\n\t\t    ~vmx->nested.msrs.vmfunc_controls)\n\t\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\t\tif (nested_cpu_has_eptp_switching(vmcs12)) {\n\t\t\tif (!nested_cpu_has_ept(vmcs12) ||\n\t\t\t    !page_address_valid(vcpu, vmcs12->eptp_list_address))\n\t\t\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\t\t}\n\t}\n\n\tif (vmcs12->cr3_target_count > nested_cpu_vmx_misc_cr3_count(vcpu))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (!nested_host_cr0_valid(vcpu, vmcs12->host_cr0) ||\n\t    !nested_host_cr4_valid(vcpu, vmcs12->host_cr4) ||\n\t    !nested_cr3_valid(vcpu, vmcs12->host_cr3))\n\t\treturn VMXERR_ENTRY_INVALID_HOST_STATE_FIELD;\n\n\treturn 0;\n}\n\nstatic int check_vmentry_postreqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t\t  u32 *exit_qual)\n{\n\tbool ia32e;\n\n\t*exit_qual = ENTRY_FAIL_DEFAULT;\n\n\tif (!nested_guest_cr0_valid(vcpu, vmcs12->guest_cr0) ||\n\t    !nested_guest_cr4_valid(vcpu, vmcs12->guest_cr4))\n\t\treturn 1;\n\n\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_SHADOW_VMCS) &&\n\t    vmcs12->vmcs_link_pointer != -1ull) {\n\t\t*exit_qual = ENTRY_FAIL_VMCS_LINK_PTR;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-entry control is 1, the following checks\n\t * are performed on the field for the IA32_EFER MSR:\n\t * - Bits reserved in the IA32_EFER MSR must be 0.\n\t * - Bit 10 (corresponding to IA32_EFER.LMA) must equal the value of\n\t *   the IA-32e mode guest VM-exit control. It must also be identical\n\t *   to bit 8 (LME) if bit 31 in the CR0 field (corresponding to\n\t *   CR0.PG) is 1.\n\t */\n\tif (to_vmx(vcpu)->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER)) {\n\t\tia32e = (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->guest_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->guest_ia32_efer & EFER_LMA) ||\n\t\t    ((vmcs12->guest_cr0 & X86_CR0_PG) &&\n\t\t     ia32e != !!(vmcs12->guest_ia32_efer & EFER_LME)))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-exit control is 1, bits reserved in the\n\t * IA32_EFER MSR must be 0 in the field for that register. In addition,\n\t * the values of the LMA and LME bits in the field must each be that of\n\t * the host address-space size VM-exit control.\n\t */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER) {\n\t\tia32e = (vmcs12->vm_exit_controls &\n\t\t\t VM_EXIT_HOST_ADDR_SPACE_SIZE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->host_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LMA) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS) &&\n\t\t(is_noncanonical_address(vmcs12->guest_bndcfgs & PAGE_MASK, vcpu) ||\n\t\t(vmcs12->guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD)))\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 msr_entry_idx;\n\tu32 exit_qual;\n\tint r;\n\n\tenter_guest_mode(vcpu);\n\n\tif (!(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS))\n\t\tvmx->nested.vmcs01_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\n\tvmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);\n\tvmx_segment_cache_clear(vmx);\n\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset += vmcs12->tsc_offset;\n\n\tr = EXIT_REASON_INVALID_STATE;\n\tif (prepare_vmcs02(vcpu, vmcs12, &exit_qual))\n\t\tgoto fail;\n\n\tnested_get_vmcs12_pages(vcpu, vmcs12);\n\n\tr = EXIT_REASON_MSR_LOAD_FAIL;\n\tmsr_entry_idx = nested_vmx_load_msr(vcpu,\n\t\t\t\t\t    vmcs12->vm_entry_msr_load_addr,\n\t\t\t\t\t    vmcs12->vm_entry_msr_load_count);\n\tif (msr_entry_idx)\n\t\tgoto fail;\n\n\t/*\n\t * Note no nested_vmx_succeed or nested_vmx_fail here. At this point\n\t * we are no longer running L1, and VMLAUNCH/VMRESUME has not yet\n\t * returned as far as L1 is concerned. It will only return (and set\n\t * the success flag) when L2 exits (see nested_vmx_vmexit()).\n\t */\n\treturn 0;\n\nfail:\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset -= vmcs12->tsc_offset;\n\tleave_guest_mode(vcpu);\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\tnested_vmx_entry_failure(vcpu, vmcs12, r, exit_qual);\n\treturn 1;\n}\n\n/*\n * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1\n * for running an L2 nested guest.\n */\nstatic int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)\n{\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 interrupt_shadow = vmx_get_interrupt_shadow(vcpu);\n\tu32 exit_qual;\n\tint ret;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\tgoto out;\n\n\tvmcs12 = get_vmcs12(vcpu);\n\n\tif (enable_shadow_vmcs)\n\t\tcopy_shadow_to_vmcs12(vmx);\n\n\t/*\n\t * The nested entry process starts with enforcing various prerequisites\n\t * on vmcs12 as required by the Intel SDM, and act appropriately when\n\t * they fail: As the SDM explains, some conditions should cause the\n\t * instruction to fail, while others will cause the instruction to seem\n\t * to succeed, but return an EXIT_REASON_INVALID_STATE.\n\t * To speed up the normal (success) code path, we should avoid checking\n\t * for misconfigurations which will anyway be caught by the processor\n\t * when using the merged vmcs02.\n\t */\n\tif (interrupt_shadow & KVM_X86_SHADOW_INT_MOV_SS) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\t     VMXERR_ENTRY_EVENTS_BLOCKED_BY_MOV_SS);\n\t\tgoto out;\n\t}\n\n\tif (vmcs12->launch_state == launch) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tlaunch ? VMXERR_VMLAUNCH_NONCLEAR_VMCS\n\t\t\t       : VMXERR_VMRESUME_NONLAUNCHED_VMCS);\n\t\tgoto out;\n\t}\n\n\tret = check_vmentry_prereqs(vcpu, vmcs12);\n\tif (ret) {\n\t\tnested_vmx_failValid(vcpu, ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * After this point, the trap flag no longer triggers a singlestep trap\n\t * on the vm entry instructions; don't call kvm_skip_emulated_instruction.\n\t * This is not 100% correct; for performance reasons, we delegate most\n\t * of the checks on host state to the processor.  If those fail,\n\t * the singlestep trap is missed.\n\t */\n\tskip_emulated_instruction(vcpu);\n\n\tret = check_vmentry_postreqs(vcpu, vmcs12, &exit_qual);\n\tif (ret) {\n\t\tnested_vmx_entry_failure(vcpu, vmcs12,\n\t\t\t\t\t EXIT_REASON_INVALID_STATE, exit_qual);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * We're finally done with prerequisite checking, and can start with\n\t * the nested entry.\n\t */\n\n\tvmx->nested.nested_run_pending = 1;\n\tret = enter_vmx_non_root_mode(vcpu);\n\tif (ret) {\n\t\tvmx->nested.nested_run_pending = 0;\n\t\treturn ret;\n\t}\n\n\t/*\n\t * If we're entering a halted L2 vcpu and the L2 vcpu won't be woken\n\t * by event injection, halt vcpu.\n\t */\n\tif ((vmcs12->guest_activity_state == GUEST_ACTIVITY_HLT) &&\n\t    !(vmcs12->vm_entry_intr_info_field & INTR_INFO_VALID_MASK)) {\n\t\tvmx->nested.nested_run_pending = 0;\n\t\treturn kvm_vcpu_halt(vcpu);\n\t}\n\treturn 1;\n\nout:\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/*\n * On a nested exit from L2 to L1, vmcs12.guest_cr0 might not be up-to-date\n * because L2 may have changed some cr0 bits directly (CRO_GUEST_HOST_MASK).\n * This function returns the new value we should put in vmcs12.guest_cr0.\n * It's not enough to just return the vmcs02 GUEST_CR0. Rather,\n *  1. Bits that neither L0 nor L1 trapped, were set directly by L2 and are now\n *     available in vmcs02 GUEST_CR0. (Note: It's enough to check that L0\n *     didn't trap the bit, because if L1 did, so would L0).\n *  2. Bits that L1 asked to trap (and therefore L0 also did) could not have\n *     been modified by L2, and L1 knows it. So just leave the old value of\n *     the bit from vmcs12.guest_cr0. Note that the bit from vmcs02 GUEST_CR0\n *     isn't relevant, because if L0 traps this bit it can set it to anything.\n *  3. Bits that L1 didn't trap, but L0 did. L1 believes the guest could have\n *     changed these bits, and therefore they need to be updated, but L0\n *     didn't necessarily allow them to be changed in GUEST_CR0 - and rather\n *     put them in vmcs02 CR0_READ_SHADOW. So take these bits from there.\n */\nstatic inline unsigned long\nvmcs12_guest_cr0(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t/*1*/\t(vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |\n\t/*2*/\t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask) |\n\t/*3*/\t(vmcs_readl(CR0_READ_SHADOW) & ~(vmcs12->cr0_guest_host_mask |\n\t\t\tvcpu->arch.cr0_guest_owned_bits));\n}\n\nstatic inline unsigned long\nvmcs12_guest_cr4(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t/*1*/\t(vmcs_readl(GUEST_CR4) & vcpu->arch.cr4_guest_owned_bits) |\n\t/*2*/\t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask) |\n\t/*3*/\t(vmcs_readl(CR4_READ_SHADOW) & ~(vmcs12->cr4_guest_host_mask |\n\t\t\tvcpu->arch.cr4_guest_owned_bits));\n}\n\nstatic void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tu32 idt_vectoring;\n\tunsigned int nr;\n\n\tif (vcpu->arch.exception.injected) {\n\t\tnr = vcpu->arch.exception.nr;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (kvm_exception_is_soft(nr)) {\n\t\t\tvmcs12->vm_exit_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_EXCEPTION;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_HARD_EXCEPTION;\n\n\t\tif (vcpu->arch.exception.has_error_code) {\n\t\t\tidt_vectoring |= VECTORING_INFO_DELIVER_CODE_MASK;\n\t\t\tvmcs12->idt_vectoring_error_code =\n\t\t\t\tvcpu->arch.exception.error_code;\n\t\t}\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t} else if (vcpu->arch.nmi_injected) {\n\t\tvmcs12->idt_vectoring_info_field =\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR;\n\t} else if (vcpu->arch.interrupt.injected) {\n\t\tnr = vcpu->arch.interrupt.nr;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (vcpu->arch.interrupt.soft) {\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_INTR;\n\t\t\tvmcs12->vm_entry_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_EXT_INTR;\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t}\n}\n\nstatic int vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qual;\n\tbool block_nested_events =\n\t    vmx->nested.nested_run_pending || kvm_event_needs_reinjection(vcpu);\n\n\tif (vcpu->arch.exception.pending &&\n\t\tnested_vmx_check_exception(vcpu, &exit_qual)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_inject_exception_vmexit(vcpu, exit_qual);\n\t\treturn 0;\n\t}\n\n\tif (nested_cpu_has_preemption_timer(get_vmcs12(vcpu)) &&\n\t    vmx->nested.preemption_timer_expired) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && nested_exit_on_nmi(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  NMI_VECTOR | INTR_TYPE_NMI_INTR |\n\t\t\t\t  INTR_INFO_VALID_MASK, 0);\n\t\t/*\n\t\t * The NMI-triggered VM exit counts as injection:\n\t\t * clear this one and block further NMIs.\n\t\t */\n\t\tvcpu->arch.nmi_pending = 0;\n\t\tvmx_set_nmi_mask(vcpu, true);\n\t\treturn 0;\n\t}\n\n\tif ((kvm_cpu_has_interrupt(vcpu) || external_intr) &&\n\t    nested_exit_on_intr(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);\n\t\treturn 0;\n\t}\n\n\tvmx_complete_nested_posted_interrupt(vcpu);\n\treturn 0;\n}\n\nstatic u32 vmx_get_preemption_timer_value(struct kvm_vcpu *vcpu)\n{\n\tktime_t remaining =\n\t\thrtimer_get_remaining(&to_vmx(vcpu)->nested.preemption_timer);\n\tu64 value;\n\n\tif (ktime_to_ns(remaining) <= 0)\n\t\treturn 0;\n\n\tvalue = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;\n\tdo_div(value, 1000000);\n\treturn value >> VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n}\n\n/*\n * Update the guest state fields of vmcs12 to reflect changes that\n * occurred while L2 was running. (The \"IA-32e mode guest\" bit of the\n * VM-entry controls is also updated, since this is really a guest\n * state bit.)\n */\nstatic void sync_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tvmcs12->guest_cr0 = vmcs12_guest_cr0(vcpu, vmcs12);\n\tvmcs12->guest_cr4 = vmcs12_guest_cr4(vcpu, vmcs12);\n\n\tvmcs12->guest_rsp = kvm_register_read(vcpu, VCPU_REGS_RSP);\n\tvmcs12->guest_rip = kvm_register_read(vcpu, VCPU_REGS_RIP);\n\tvmcs12->guest_rflags = vmcs_readl(GUEST_RFLAGS);\n\n\tvmcs12->guest_es_selector = vmcs_read16(GUEST_ES_SELECTOR);\n\tvmcs12->guest_cs_selector = vmcs_read16(GUEST_CS_SELECTOR);\n\tvmcs12->guest_ss_selector = vmcs_read16(GUEST_SS_SELECTOR);\n\tvmcs12->guest_ds_selector = vmcs_read16(GUEST_DS_SELECTOR);\n\tvmcs12->guest_fs_selector = vmcs_read16(GUEST_FS_SELECTOR);\n\tvmcs12->guest_gs_selector = vmcs_read16(GUEST_GS_SELECTOR);\n\tvmcs12->guest_ldtr_selector = vmcs_read16(GUEST_LDTR_SELECTOR);\n\tvmcs12->guest_tr_selector = vmcs_read16(GUEST_TR_SELECTOR);\n\tvmcs12->guest_es_limit = vmcs_read32(GUEST_ES_LIMIT);\n\tvmcs12->guest_cs_limit = vmcs_read32(GUEST_CS_LIMIT);\n\tvmcs12->guest_ss_limit = vmcs_read32(GUEST_SS_LIMIT);\n\tvmcs12->guest_ds_limit = vmcs_read32(GUEST_DS_LIMIT);\n\tvmcs12->guest_fs_limit = vmcs_read32(GUEST_FS_LIMIT);\n\tvmcs12->guest_gs_limit = vmcs_read32(GUEST_GS_LIMIT);\n\tvmcs12->guest_ldtr_limit = vmcs_read32(GUEST_LDTR_LIMIT);\n\tvmcs12->guest_tr_limit = vmcs_read32(GUEST_TR_LIMIT);\n\tvmcs12->guest_gdtr_limit = vmcs_read32(GUEST_GDTR_LIMIT);\n\tvmcs12->guest_idtr_limit = vmcs_read32(GUEST_IDTR_LIMIT);\n\tvmcs12->guest_es_ar_bytes = vmcs_read32(GUEST_ES_AR_BYTES);\n\tvmcs12->guest_cs_ar_bytes = vmcs_read32(GUEST_CS_AR_BYTES);\n\tvmcs12->guest_ss_ar_bytes = vmcs_read32(GUEST_SS_AR_BYTES);\n\tvmcs12->guest_ds_ar_bytes = vmcs_read32(GUEST_DS_AR_BYTES);\n\tvmcs12->guest_fs_ar_bytes = vmcs_read32(GUEST_FS_AR_BYTES);\n\tvmcs12->guest_gs_ar_bytes = vmcs_read32(GUEST_GS_AR_BYTES);\n\tvmcs12->guest_ldtr_ar_bytes = vmcs_read32(GUEST_LDTR_AR_BYTES);\n\tvmcs12->guest_tr_ar_bytes = vmcs_read32(GUEST_TR_AR_BYTES);\n\tvmcs12->guest_es_base = vmcs_readl(GUEST_ES_BASE);\n\tvmcs12->guest_cs_base = vmcs_readl(GUEST_CS_BASE);\n\tvmcs12->guest_ss_base = vmcs_readl(GUEST_SS_BASE);\n\tvmcs12->guest_ds_base = vmcs_readl(GUEST_DS_BASE);\n\tvmcs12->guest_fs_base = vmcs_readl(GUEST_FS_BASE);\n\tvmcs12->guest_gs_base = vmcs_readl(GUEST_GS_BASE);\n\tvmcs12->guest_ldtr_base = vmcs_readl(GUEST_LDTR_BASE);\n\tvmcs12->guest_tr_base = vmcs_readl(GUEST_TR_BASE);\n\tvmcs12->guest_gdtr_base = vmcs_readl(GUEST_GDTR_BASE);\n\tvmcs12->guest_idtr_base = vmcs_readl(GUEST_IDTR_BASE);\n\n\tvmcs12->guest_interruptibility_info =\n\t\tvmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tvmcs12->guest_pending_dbg_exceptions =\n\t\tvmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_HLT;\n\telse\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_ACTIVE;\n\n\tif (nested_cpu_has_preemption_timer(vmcs12)) {\n\t\tif (vmcs12->vm_exit_controls &\n\t\t    VM_EXIT_SAVE_VMX_PREEMPTION_TIMER)\n\t\t\tvmcs12->vmx_preemption_timer_value =\n\t\t\t\tvmx_get_preemption_timer_value(vcpu);\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\t}\n\n\t/*\n\t * In some cases (usually, nested EPT), L2 is allowed to change its\n\t * own CR3 without exiting. If it has changed it, we must keep it.\n\t * Of course, if L0 is using shadow page tables, GUEST_CR3 was defined\n\t * by L0, not L1 or L2, so we mustn't unconditionally copy it to vmcs12.\n\t *\n\t * Additionally, restore L2's PDPTR to vmcs12.\n\t */\n\tif (enable_ept) {\n\t\tvmcs12->guest_cr3 = vmcs_readl(GUEST_CR3);\n\t\tvmcs12->guest_pdptr0 = vmcs_read64(GUEST_PDPTR0);\n\t\tvmcs12->guest_pdptr1 = vmcs_read64(GUEST_PDPTR1);\n\t\tvmcs12->guest_pdptr2 = vmcs_read64(GUEST_PDPTR2);\n\t\tvmcs12->guest_pdptr3 = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\tvmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);\n\n\tif (nested_cpu_has_vid(vmcs12))\n\t\tvmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);\n\n\tvmcs12->vm_entry_controls =\n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_IA32E_MODE) |\n\t\t(vm_entry_controls_get(to_vmx(vcpu)) & VM_ENTRY_IA32E_MODE);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_DEBUG_CONTROLS) {\n\t\tkvm_get_dr(vcpu, 7, (unsigned long *)&vmcs12->guest_dr7);\n\t\tvmcs12->guest_ia32_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\t}\n\n\t/* TODO: These cannot have changed unless we have MSR bitmaps and\n\t * the relevant bit asks not to trap the change */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)\n\t\tvmcs12->guest_ia32_pat = vmcs_read64(GUEST_IA32_PAT);\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_EFER)\n\t\tvmcs12->guest_ia32_efer = vcpu->arch.efer;\n\tvmcs12->guest_sysenter_cs = vmcs_read32(GUEST_SYSENTER_CS);\n\tvmcs12->guest_sysenter_esp = vmcs_readl(GUEST_SYSENTER_ESP);\n\tvmcs12->guest_sysenter_eip = vmcs_readl(GUEST_SYSENTER_EIP);\n\tif (kvm_mpx_supported())\n\t\tvmcs12->guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);\n}\n\n/*\n * prepare_vmcs12 is part of what we need to do when the nested L2 guest exits\n * and we want to prepare to run its L1 parent. L1 keeps a vmcs for L2 (vmcs12),\n * and this function updates it to reflect the changes to the guest state while\n * L2 was running (and perhaps made some exits which were handled directly by L0\n * without going back to L1), and to reflect the exit reason.\n * Note that we do not have to copy here all VMCS fields, just those that\n * could have changed by the L2 guest or the exit - i.e., the guest-state and\n * exit-information fields only. Other fields are modified by L1 with VMWRITE,\n * which already writes to vmcs12 directly.\n */\nstatic void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t   u32 exit_reason, u32 exit_intr_info,\n\t\t\t   unsigned long exit_qualification)\n{\n\t/* update guest state fields: */\n\tsync_vmcs12(vcpu, vmcs12);\n\n\t/* update exit information fields: */\n\n\tvmcs12->vm_exit_reason = exit_reason;\n\tvmcs12->exit_qualification = exit_qualification;\n\tvmcs12->vm_exit_intr_info = exit_intr_info;\n\n\tvmcs12->idt_vectoring_info_field = 0;\n\tvmcs12->vm_exit_instruction_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tvmcs12->vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\n\tif (!(vmcs12->vm_exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY)) {\n\t\tvmcs12->launch_state = 1;\n\n\t\t/* vm_entry_intr_info_field is cleared on exit. Emulate this\n\t\t * instead of reading the real value. */\n\t\tvmcs12->vm_entry_intr_info_field &= ~INTR_INFO_VALID_MASK;\n\n\t\t/*\n\t\t * Transfer the event that L0 or L1 may wanted to inject into\n\t\t * L2 to IDT_VECTORING_INFO_FIELD.\n\t\t */\n\t\tvmcs12_save_pending_event(vcpu, vmcs12);\n\t}\n\n\t/*\n\t * Drop what we picked up for L2 via vmx_complete_interrupts. It is\n\t * preserved above and would only end up incorrectly in L1.\n\t */\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n}\n\nstatic void load_vmcs12_mmu_host_state(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12)\n{\n\tu32 entry_failure_code;\n\n\tnested_ept_uninit_mmu_context(vcpu);\n\n\t/*\n\t * Only PDPTE load can fail as the value of cr3 was checked on entry and\n\t * couldn't have changed.\n\t */\n\tif (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, &entry_failure_code))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);\n\n\tif (!enable_ept)\n\t\tvcpu->arch.walk_mmu->inject_page_fault = kvm_inject_page_fault;\n}\n\n/*\n * A part of what we need to when the nested L2 guest exits and we want to\n * run its L1 parent, is to reset L1's guest state to the host state specified\n * in vmcs12.\n * This function is to be called not only on normal nested exit, but also on\n * a nested entry failure, as explained in Intel's spec, 3B.23.7 (\"VM-Entry\n * Failures During or After Loading Guest State\").\n * This function should be called when the active VMCS is L1's (vmcs01).\n */\nstatic void load_vmcs12_host_state(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tstruct kvm_segment seg;\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER)\n\t\tvcpu->arch.efer = vmcs12->host_ia32_efer;\n\telse if (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tvcpu->arch.efer |= (EFER_LMA | EFER_LME);\n\telse\n\t\tvcpu->arch.efer &= ~(EFER_LMA | EFER_LME);\n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\tkvm_register_write(vcpu, VCPU_REGS_RSP, vmcs12->host_rsp);\n\tkvm_register_write(vcpu, VCPU_REGS_RIP, vmcs12->host_rip);\n\tvmx_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\t/*\n\t * Note that calling vmx_set_cr0 is important, even if cr0 hasn't\n\t * actually changed, because vmx_set_cr0 refers to efer set above.\n\t *\n\t * CR0_GUEST_HOST_MASK is already set in the original vmcs01\n\t * (KVM doesn't change it);\n\t */\n\tvcpu->arch.cr0_guest_owned_bits = X86_CR0_TS;\n\tvmx_set_cr0(vcpu, vmcs12->host_cr0);\n\n\t/* Same as above - no reason to call set_cr4_guest_host_mask().  */\n\tvcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);\n\tvmx_set_cr4(vcpu, vmcs12->host_cr4);\n\n\tload_vmcs12_mmu_host_state(vcpu, vmcs12);\n\n\t/*\n\t * If vmcs01 don't use VPID, CPU flushes TLB on every\n\t * VMEntry/VMExit. Thus, no need to flush TLB.\n\t *\n\t * If vmcs12 uses VPID, TLB entries populated by L2 are\n\t * tagged with vmx->nested.vpid02 while L1 entries are tagged\n\t * with vmx->vpid. Thus, no need to flush TLB.\n\t *\n\t * Therefore, flush TLB only in case vmcs01 uses VPID and\n\t * vmcs12 don't use VPID as in this case L1 & L2 TLB entries\n\t * are both tagged with vmx->vpid.\n\t */\n\tif (enable_vpid &&\n\t    !(nested_cpu_has_vpid(vmcs12) && to_vmx(vcpu)->nested.vpid02)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->host_ia32_sysenter_cs);\n\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->host_ia32_sysenter_esp);\n\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->host_ia32_sysenter_eip);\n\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->host_idtr_base);\n\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->host_gdtr_base);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xFFFF);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xFFFF);\n\n\t/* If not VM_EXIT_CLEAR_BNDCFGS, the L2 value propagates to L1.  */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_CLEAR_BNDCFGS)\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->host_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->host_ia32_pat;\n\t}\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tvmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL,\n\t\t\tvmcs12->host_ia32_perf_global_ctrl);\n\n\t/* Set L1 segment info according to Intel SDM\n\t    27.5.2 Loading Host Segment and Descriptor-Table Registers */\n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.selector = vmcs12->host_cs_selector,\n\t\t.type = 11,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.g = 1\n\t};\n\tif (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tseg.l = 1;\n\telse\n\t\tseg.db = 1;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_CS);\n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.type = 3,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.db = 1,\n\t\t.g = 1\n\t};\n\tseg.selector = vmcs12->host_ds_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_DS);\n\tseg.selector = vmcs12->host_es_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_ES);\n\tseg.selector = vmcs12->host_ss_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_SS);\n\tseg.selector = vmcs12->host_fs_selector;\n\tseg.base = vmcs12->host_fs_base;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_FS);\n\tseg.selector = vmcs12->host_gs_selector;\n\tseg.base = vmcs12->host_gs_base;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_GS);\n\tseg = (struct kvm_segment) {\n\t\t.base = vmcs12->host_tr_base,\n\t\t.limit = 0x67,\n\t\t.selector = vmcs12->host_tr_selector,\n\t\t.type = 11,\n\t\t.present = 1\n\t};\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_TR);\n\n\tkvm_set_dr(vcpu, 7, 0x400);\n\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(vcpu);\n\n\tif (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,\n\t\t\t\tvmcs12->vm_exit_msr_load_count))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);\n}\n\n/*\n * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1\n * and modify vmcs12 to make it see what it would expect to see there if\n * L2 was its real guest. Must only be called when in L2 (is_guest_mode())\n */\nstatic void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,\n\t\t\t      u32 exit_intr_info,\n\t\t\t      unsigned long exit_qualification)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t/* trying to cancel vmlaunch/vmresume is a bug */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\t/*\n\t * The only expected VM-instruction error is \"VM entry with\n\t * invalid control field(s).\" Anything else indicates a\n\t * problem with L0.\n\t */\n\tWARN_ON_ONCE(vmx->fail && (vmcs_read32(VM_INSTRUCTION_ERROR) !=\n\t\t\t\t   VMXERR_ENTRY_INVALID_CONTROL_FIELD));\n\n\tleave_guest_mode(vcpu);\n\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset -= vmcs12->tsc_offset;\n\n\tif (likely(!vmx->fail)) {\n\t\tif (exit_reason == -1)\n\t\t\tsync_vmcs12(vcpu, vmcs12);\n\t\telse\n\t\t\tprepare_vmcs12(vcpu, vmcs12, exit_reason, exit_intr_info,\n\t\t\t\t       exit_qualification);\n\n\t\tif (nested_vmx_store_msr(vcpu, vmcs12->vm_exit_msr_store_addr,\n\t\t\t\t\t vmcs12->vm_exit_msr_store_count))\n\t\t\tnested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);\n\t}\n\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\tvm_entry_controls_reset_shadow(vmx);\n\tvm_exit_controls_reset_shadow(vmx);\n\tvmx_segment_cache_clear(vmx);\n\n\t/* Update any VMCS fields that might have changed while L2 ran */\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (vmx->hv_deadline_tsc == -1)\n\t\tvmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n\telse\n\t\tvmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t      PIN_BASED_VMX_PREEMPTION_TIMER);\n\tif (kvm_has_tsc_control)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tif (vmx->nested.change_vmcs01_virtual_apic_mode) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = false;\n\t\tvmx_set_virtual_apic_mode(vcpu);\n\t} else if (!nested_cpu_has_ept(vmcs12) &&\n\t\t   nested_cpu_has2(vmcs12,\n\t\t\t\t   SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\t/* This is needed for same reason as it was needed in prepare_vmcs02 */\n\tvmx->host_rsp = 0;\n\n\t/* Unpin physical memory we referred to in vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\t/*\n\t * We are now running in L2, mmu_notifier will force to reload the\n\t * page's hpa for L2 vmcs. Need to reload it for L1 before entering L1.\n\t */\n\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\tif (enable_shadow_vmcs && exit_reason != -1)\n\t\tvmx->nested.sync_shadow_vmcs = true;\n\n\t/* in case we halted in L2 */\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tif (likely(!vmx->fail)) {\n\t\t/*\n\t\t * TODO: SDM says that with acknowledge interrupt on\n\t\t * exit, bit 31 of the VM-exit interrupt information\n\t\t * (valid interrupt) is always set to 1 on\n\t\t * EXIT_REASON_EXTERNAL_INTERRUPT, so we shouldn't\n\t\t * need kvm_cpu_has_interrupt().  See the commit\n\t\t * message for details.\n\t\t */\n\t\tif (nested_exit_intr_ack_set(vcpu) &&\n\t\t    exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&\n\t\t    kvm_cpu_has_interrupt(vcpu)) {\n\t\t\tint irq = kvm_cpu_get_interrupt(vcpu);\n\t\t\tWARN_ON(irq < 0);\n\t\t\tvmcs12->vm_exit_intr_info = irq |\n\t\t\t\tINTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;\n\t\t}\n\n\t\tif (exit_reason != -1)\n\t\t\ttrace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,\n\t\t\t\t\t\t       vmcs12->exit_qualification,\n\t\t\t\t\t\t       vmcs12->idt_vectoring_info_field,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_info,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_error_code,\n\t\t\t\t\t\t       KVM_ISA_VMX);\n\n\t\tload_vmcs12_host_state(vcpu, vmcs12);\n\n\t\treturn;\n\t}\n\t\n\t/*\n\t * After an early L2 VM-entry failure, we're now back\n\t * in L1 which thinks it just finished a VMLAUNCH or\n\t * VMRESUME instruction, so we need to set the failure\n\t * flag and the VM-instruction error field of the VMCS\n\t * accordingly.\n\t */\n\tnested_vmx_failValid(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\tload_vmcs12_mmu_host_state(vcpu, vmcs12);\n\n\t/*\n\t * The emulated instruction was already skipped in\n\t * nested_vmx_run, but the updated RIP was never\n\t * written back to the vmcs01.\n\t */\n\tskip_emulated_instruction(vcpu);\n\tvmx->fail = 0;\n}\n\n/*\n * Forcibly leave nested mode in order to be able to reset the VCPU later on.\n */\nstatic void vmx_leave_nested(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.nested_run_pending = 0;\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\t}\n\tfree_nested(to_vmx(vcpu));\n}\n\n/*\n * L1's failure to enter L2 is a subset of a normal exit, as explained in\n * 23.7 \"VM-entry failures during or after loading guest state\" (this also\n * lists the acceptable exit-reason and exit-qualification parameters).\n * It should only be called before L2 actually succeeded to run, and when\n * vmcs01 is current (it doesn't leave_guest_mode() or switch vmcss).\n */\nstatic void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification)\n{\n\tload_vmcs12_host_state(vcpu, vmcs12);\n\tvmcs12->vm_exit_reason = reason | VMX_EXIT_REASONS_FAILED_VMENTRY;\n\tvmcs12->exit_qualification = qualification;\n\tnested_vmx_succeed(vcpu);\n\tif (enable_shadow_vmcs)\n\t\tto_vmx(vcpu)->nested.sync_shadow_vmcs = true;\n}\n\nstatic int vmx_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct x86_emulate_ctxt *ctxt = &vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * RDPID causes #UD if disabled through secondary execution controls.\n\t * Because it is marked as EmulateOnUD, we need to intercept it here.\n\t */\n\tif (info->intercept == x86_intercept_rdtscp &&\n\t    !nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDTSCP)) {\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->exception.error_code_valid = false;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\t/* TODO: check more intercepts... */\n\treturn X86EMUL_CONTINUE;\n}\n\n#ifdef CONFIG_X86_64\n/* (a << shift) / divisor, return 1 if overflow otherwise 0 */\nstatic inline int u64_shl_div_u64(u64 a, unsigned int shift,\n\t\t\t\t  u64 divisor, u64 *result)\n{\n\tu64 low = a << shift, high = a >> (64 - shift);\n\n\t/* To avoid the overflow on divq */\n\tif (high >= divisor)\n\t\treturn 1;\n\n\t/* Low hold the result, high hold rem which is discarded */\n\tasm(\"divq %2\\n\\t\" : \"=a\" (low), \"=d\" (high) :\n\t    \"rm\" (divisor), \"0\" (low), \"1\" (high));\n\t*result = low;\n\n\treturn 0;\n}\n\nstatic int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)\n{\n\tstruct vcpu_vmx *vmx;\n\tu64 tscl, guest_tscl, delta_tsc, lapic_timer_advance_cycles;\n\n\tif (kvm_mwait_in_guest(vcpu->kvm))\n\t\treturn -EOPNOTSUPP;\n\n\tvmx = to_vmx(vcpu);\n\ttscl = rdtsc();\n\tguest_tscl = kvm_read_l1_tsc(vcpu, tscl);\n\tdelta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;\n\tlapic_timer_advance_cycles = nsec_to_cycles(vcpu, lapic_timer_advance_ns);\n\n\tif (delta_tsc > lapic_timer_advance_cycles)\n\t\tdelta_tsc -= lapic_timer_advance_cycles;\n\telse\n\t\tdelta_tsc = 0;\n\n\t/* Convert to host delta tsc if tsc scaling is enabled */\n\tif (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&\n\t\t\tu64_shl_div_u64(delta_tsc,\n\t\t\t\tkvm_tsc_scaling_ratio_frac_bits,\n\t\t\t\tvcpu->arch.tsc_scaling_ratio,\n\t\t\t\t&delta_tsc))\n\t\treturn -ERANGE;\n\n\t/*\n\t * If the delta tsc can't fit in the 32 bit after the multi shift,\n\t * we can't use the preemption timer.\n\t * It's possible that it fits on later vmentries, but checking\n\t * on every vmentry is costly so we just use an hrtimer.\n\t */\n\tif (delta_tsc >> (cpu_preemption_timer_multi + 32))\n\t\treturn -ERANGE;\n\n\tvmx->hv_deadline_tsc = tscl + delta_tsc;\n\tvmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n\n\treturn delta_tsc == 0;\n}\n\nstatic void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tvmx->hv_deadline_tsc = -1;\n\tvmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n}\n#endif\n\nstatic void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tshrink_ple_window(vcpu);\n}\n\nstatic void vmx_slot_enable_log_dirty(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *slot)\n{\n\tkvm_mmu_slot_leaf_clear_dirty(kvm, slot);\n\tkvm_mmu_slot_largepage_remove_write_access(kvm, slot);\n}\n\nstatic void vmx_slot_disable_log_dirty(struct kvm *kvm,\n\t\t\t\t       struct kvm_memory_slot *slot)\n{\n\tkvm_mmu_slot_set_dirty(kvm, slot);\n}\n\nstatic void vmx_flush_log_dirty(struct kvm *kvm)\n{\n\tkvm_flush_pml_buffers(kvm);\n}\n\nstatic int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t gpa;\n\tstruct page *page = NULL;\n\tu64 *pml_address;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tWARN_ON_ONCE(vmx->nested.pml_full);\n\n\t\t/*\n\t\t * Check if PML is enabled for the nested guest.\n\t\t * Whether eptp bit 6 is set is already checked\n\t\t * as part of A/D emulation.\n\t\t */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tif (!nested_cpu_has_pml(vmcs12))\n\t\t\treturn 0;\n\n\t\tif (vmcs12->guest_pml_index >= PML_ENTITY_NUM) {\n\t\t\tvmx->nested.pml_full = true;\n\t\t\treturn 1;\n\t\t}\n\n\t\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS) & ~0xFFFull;\n\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->pml_address);\n\t\tif (is_error_page(page))\n\t\t\treturn 0;\n\n\t\tpml_address = kmap(page);\n\t\tpml_address[vmcs12->guest_pml_index--] = gpa;\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_enable_log_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t\t   gfn_t offset, unsigned long mask)\n{\n\tkvm_mmu_clear_dirty_pt_masked(kvm, memslot, offset, mask);\n}\n\nstatic void __pi_post_block(struct kvm_vcpu *vcpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\tstruct pi_desc old, new;\n\tunsigned int dest;\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\t\tWARN(old.nv != POSTED_INTR_WAKEUP_VECTOR,\n\t\t     \"Wakeup handler not enabled while the VCPU is blocked\\n\");\n\n\t\tdest = cpu_physical_id(vcpu->cpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'notification vector' */\n\t\tnew.nv = POSTED_INTR_VECTOR;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n\n\tif (!WARN_ON_ONCE(vcpu->pre_pcpu == -1)) {\n\t\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tlist_del(&vcpu->blocked_vcpu_list);\n\t\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tvcpu->pre_pcpu = -1;\n\t}\n}\n\n/*\n * This routine does the following things for vCPU which is going\n * to be blocked if VT-d PI is enabled.\n * - Store the vCPU to the wakeup list, so when interrupts happen\n *   we can find the right vCPU to wake up.\n * - Change the Posted-interrupt descriptor as below:\n *      'NDST' <-- vcpu->pre_pcpu\n *      'NV' <-- POSTED_INTR_WAKEUP_VECTOR\n * - If 'ON' is set during this process, which means at least one\n *   interrupt is posted for this vCPU, we cannot block it, in\n *   this case, return 1, otherwise, return 0.\n *\n */\nstatic int pi_pre_block(struct kvm_vcpu *vcpu)\n{\n\tunsigned int dest;\n\tstruct pi_desc old, new;\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn 0;\n\n\tWARN_ON(irqs_disabled());\n\tlocal_irq_disable();\n\tif (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {\n\t\tvcpu->pre_pcpu = vcpu->cpu;\n\t\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tlist_add_tail(&vcpu->blocked_vcpu_list,\n\t\t\t      &per_cpu(blocked_vcpu_on_cpu,\n\t\t\t\t       vcpu->pre_pcpu));\n\t\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t}\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\tWARN((pi_desc->sn == 1),\n\t\t     \"Warning: SN field of posted-interrupts \"\n\t\t     \"is set before blocking\\n\");\n\n\t\t/*\n\t\t * Since vCPU can be preempted during this process,\n\t\t * vcpu->cpu could be different with pre_pcpu, we\n\t\t * need to set pre_pcpu as the destination of wakeup\n\t\t * notification event, then we can find the right vCPU\n\t\t * to wakeup in wakeup handler if interrupts happen\n\t\t * when the vCPU is in blocked state.\n\t\t */\n\t\tdest = cpu_physical_id(vcpu->pre_pcpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'wakeup vector' */\n\t\tnew.nv = POSTED_INTR_WAKEUP_VECTOR;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n\n\t/* We should not block the vCPU if an interrupt is posted for it.  */\n\tif (pi_test_on(pi_desc) == 1)\n\t\t__pi_post_block(vcpu);\n\n\tlocal_irq_enable();\n\treturn (vcpu->pre_pcpu == -1);\n}\n\nstatic int vmx_pre_block(struct kvm_vcpu *vcpu)\n{\n\tif (pi_pre_block(vcpu))\n\t\treturn 1;\n\n\tif (kvm_lapic_hv_timer_in_use(vcpu))\n\t\tkvm_lapic_switch_to_sw_timer(vcpu);\n\n\treturn 0;\n}\n\nstatic void pi_post_block(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->pre_pcpu == -1)\n\t\treturn;\n\n\tWARN_ON(irqs_disabled());\n\tlocal_irq_disable();\n\t__pi_post_block(vcpu);\n\tlocal_irq_enable();\n}\n\nstatic void vmx_post_block(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_x86_ops->set_hv_timer)\n\t\tkvm_lapic_switch_to_hv_timer(vcpu);\n\n\tpi_post_block(vcpu);\n}\n\n/*\n * vmx_update_pi_irte - set IRTE for Posted-Interrupts\n *\n * @kvm: kvm\n * @host_irq: host irq of the interrupt\n * @guest_irq: gsi of the interrupt\n * @set: set or unset PI\n * returns 0 on success, < 0 on failure\n */\nstatic int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\t      uint32_t guest_irq, bool set)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_irq_routing_table *irq_rt;\n\tstruct kvm_lapic_irq irq;\n\tstruct kvm_vcpu *vcpu;\n\tstruct vcpu_data vcpu_info;\n\tint idx, ret = 0;\n\n\tif (!kvm_arch_has_assigned_device(kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP) ||\n\t\t!kvm_vcpu_apicv_active(kvm->vcpus[0]))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n\tif (guest_irq >= irq_rt->nr_rt_entries ||\n\t    hlist_empty(&irq_rt->map[guest_irq])) {\n\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",\n\t\t\t     guest_irq, irq_rt->nr_rt_entries);\n\t\tgoto out;\n\t}\n\n\thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n\t\tif (e->type != KVM_IRQ_ROUTING_MSI)\n\t\t\tcontinue;\n\t\t/*\n\t\t * VT-d PI cannot support posting multicast/broadcast\n\t\t * interrupts to a vCPU, we still use interrupt remapping\n\t\t * for these kind of interrupts.\n\t\t *\n\t\t * For lowest-priority interrupts, we only support\n\t\t * those with single CPU as the destination, e.g. user\n\t\t * configures the interrupts via /proc/irq or uses\n\t\t * irqbalance to make the interrupts single-CPU.\n\t\t *\n\t\t * We will support full lowest-priority interrupt later.\n\t\t */\n\n\t\tkvm_set_msi_irq(kvm, e, &irq);\n\t\tif (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu)) {\n\t\t\t/*\n\t\t\t * Make sure the IRTE is in remapped mode if\n\t\t\t * we don't handle it in posted mode.\n\t\t\t */\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\tprintk(KERN_INFO\n\t\t\t\t   \"failed to back to remapped mode, irq: %u\\n\",\n\t\t\t\t   host_irq);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tvcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));\n\t\tvcpu_info.vector = irq.vector;\n\n\t\ttrace_kvm_pi_irte_update(host_irq, vcpu->vcpu_id, e->gsi,\n\t\t\t\tvcpu_info.vector, vcpu_info.pi_desc_addr, set);\n\n\t\tif (set)\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &vcpu_info);\n\t\telse\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\n\t\tif (ret < 0) {\n\t\t\tprintk(KERN_INFO \"%s: failed to update PI IRTE\\n\",\n\t\t\t\t\t__func__);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\treturn ret;\n}\n\nstatic void vmx_setup_mce(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.mcg_cap & MCG_LMCE_P)\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEATURE_CONTROL_LMCE;\n\telse\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEATURE_CONTROL_LMCE;\n}\n\nstatic int vmx_smi_allowed(struct kvm_vcpu *vcpu)\n{\n\t/* we need a nested vmexit to enter SMM, postpone if run is pending */\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmx->nested.smm.guest_mode = is_guest_mode(vcpu);\n\tif (vmx->nested.smm.guest_mode)\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\n\tvmx->nested.smm.vmxon = vmx->nested.vmxon;\n\tvmx->nested.vmxon = false;\n\tvmx_clear_hlt(vcpu);\n\treturn 0;\n}\n\nstatic int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint ret;\n\n\tif (vmx->nested.smm.vmxon) {\n\t\tvmx->nested.vmxon = true;\n\t\tvmx->nested.smm.vmxon = false;\n\t}\n\n\tif (vmx->nested.smm.guest_mode) {\n\t\tvcpu->arch.hflags &= ~HF_SMM_MASK;\n\t\tret = enter_vmx_non_root_mode(vcpu);\n\t\tvcpu->arch.hflags |= HF_SMM_MASK;\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvmx->nested.smm.guest_mode = false;\n\t}\n\treturn 0;\n}\n\nstatic int enable_smi_window(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nstatic struct kvm_x86_ops vmx_x86_ops __ro_after_init = {\n\t.cpu_has_kvm_support = cpu_has_kvm_support,\n\t.disabled_by_bios = vmx_disabled_by_bios,\n\t.hardware_setup = hardware_setup,\n\t.hardware_unsetup = hardware_unsetup,\n\t.check_processor_compatibility = vmx_check_processor_compat,\n\t.hardware_enable = hardware_enable,\n\t.hardware_disable = hardware_disable,\n\t.cpu_has_accelerated_tpr = report_flexpriority,\n\t.cpu_has_high_real_mode_segbase = vmx_has_high_real_mode_segbase,\n\n\t.vm_init = vmx_vm_init,\n\t.vm_alloc = vmx_vm_alloc,\n\t.vm_free = vmx_vm_free,\n\n\t.vcpu_create = vmx_create_vcpu,\n\t.vcpu_free = vmx_free_vcpu,\n\t.vcpu_reset = vmx_vcpu_reset,\n\n\t.prepare_guest_switch = vmx_save_host_state,\n\t.vcpu_load = vmx_vcpu_load,\n\t.vcpu_put = vmx_vcpu_put,\n\n\t.update_bp_intercept = update_exception_bitmap,\n\t.get_msr_feature = vmx_get_msr_feature,\n\t.get_msr = vmx_get_msr,\n\t.set_msr = vmx_set_msr,\n\t.get_segment_base = vmx_get_segment_base,\n\t.get_segment = vmx_get_segment,\n\t.set_segment = vmx_set_segment,\n\t.get_cpl = vmx_get_cpl,\n\t.get_cs_db_l_bits = vmx_get_cs_db_l_bits,\n\t.decache_cr0_guest_bits = vmx_decache_cr0_guest_bits,\n\t.decache_cr3 = vmx_decache_cr3,\n\t.decache_cr4_guest_bits = vmx_decache_cr4_guest_bits,\n\t.set_cr0 = vmx_set_cr0,\n\t.set_cr3 = vmx_set_cr3,\n\t.set_cr4 = vmx_set_cr4,\n\t.set_efer = vmx_set_efer,\n\t.get_idt = vmx_get_idt,\n\t.set_idt = vmx_set_idt,\n\t.get_gdt = vmx_get_gdt,\n\t.set_gdt = vmx_set_gdt,\n\t.get_dr6 = vmx_get_dr6,\n\t.set_dr6 = vmx_set_dr6,\n\t.set_dr7 = vmx_set_dr7,\n\t.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,\n\t.cache_reg = vmx_cache_reg,\n\t.get_rflags = vmx_get_rflags,\n\t.set_rflags = vmx_set_rflags,\n\n\t.tlb_flush = vmx_flush_tlb,\n\n\t.run = vmx_vcpu_run,\n\t.handle_exit = vmx_handle_exit,\n\t.skip_emulated_instruction = skip_emulated_instruction,\n\t.set_interrupt_shadow = vmx_set_interrupt_shadow,\n\t.get_interrupt_shadow = vmx_get_interrupt_shadow,\n\t.patch_hypercall = vmx_patch_hypercall,\n\t.set_irq = vmx_inject_irq,\n\t.set_nmi = vmx_inject_nmi,\n\t.queue_exception = vmx_queue_exception,\n\t.cancel_injection = vmx_cancel_injection,\n\t.interrupt_allowed = vmx_interrupt_allowed,\n\t.nmi_allowed = vmx_nmi_allowed,\n\t.get_nmi_mask = vmx_get_nmi_mask,\n\t.set_nmi_mask = vmx_set_nmi_mask,\n\t.enable_nmi_window = enable_nmi_window,\n\t.enable_irq_window = enable_irq_window,\n\t.update_cr8_intercept = update_cr8_intercept,\n\t.set_virtual_apic_mode = vmx_set_virtual_apic_mode,\n\t.set_apic_access_page_addr = vmx_set_apic_access_page_addr,\n\t.get_enable_apicv = vmx_get_enable_apicv,\n\t.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,\n\t.load_eoi_exitmap = vmx_load_eoi_exitmap,\n\t.apicv_post_state_restore = vmx_apicv_post_state_restore,\n\t.hwapic_irr_update = vmx_hwapic_irr_update,\n\t.hwapic_isr_update = vmx_hwapic_isr_update,\n\t.sync_pir_to_irr = vmx_sync_pir_to_irr,\n\t.deliver_posted_interrupt = vmx_deliver_posted_interrupt,\n\n\t.set_tss_addr = vmx_set_tss_addr,\n\t.set_identity_map_addr = vmx_set_identity_map_addr,\n\t.get_tdp_level = get_ept_level,\n\t.get_mt_mask = vmx_get_mt_mask,\n\n\t.get_exit_info = vmx_get_exit_info,\n\n\t.get_lpage_level = vmx_get_lpage_level,\n\n\t.cpuid_update = vmx_cpuid_update,\n\n\t.rdtscp_supported = vmx_rdtscp_supported,\n\t.invpcid_supported = vmx_invpcid_supported,\n\n\t.set_supported_cpuid = vmx_set_supported_cpuid,\n\n\t.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,\n\n\t.read_l1_tsc_offset = vmx_read_l1_tsc_offset,\n\t.write_tsc_offset = vmx_write_tsc_offset,\n\n\t.set_tdp_cr3 = vmx_set_cr3,\n\n\t.check_intercept = vmx_check_intercept,\n\t.handle_external_intr = vmx_handle_external_intr,\n\t.mpx_supported = vmx_mpx_supported,\n\t.xsaves_supported = vmx_xsaves_supported,\n\t.umip_emulated = vmx_umip_emulated,\n\n\t.check_nested_events = vmx_check_nested_events,\n\n\t.sched_in = vmx_sched_in,\n\n\t.slot_enable_log_dirty = vmx_slot_enable_log_dirty,\n\t.slot_disable_log_dirty = vmx_slot_disable_log_dirty,\n\t.flush_log_dirty = vmx_flush_log_dirty,\n\t.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked,\n\t.write_log_dirty = vmx_write_pml_buffer,\n\n\t.pre_block = vmx_pre_block,\n\t.post_block = vmx_post_block,\n\n\t.pmu_ops = &intel_pmu_ops,\n\n\t.update_pi_irte = vmx_update_pi_irte,\n\n#ifdef CONFIG_X86_64\n\t.set_hv_timer = vmx_set_hv_timer,\n\t.cancel_hv_timer = vmx_cancel_hv_timer,\n#endif\n\n\t.setup_mce = vmx_setup_mce,\n\n\t.smi_allowed = vmx_smi_allowed,\n\t.pre_enter_smm = vmx_pre_enter_smm,\n\t.pre_leave_smm = vmx_pre_leave_smm,\n\t.enable_smi_window = enable_smi_window,\n};\n\nstatic int __init vmx_init(void)\n{\n\tint r;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\t/*\n\t * Enlightened VMCS usage should be recommended and the host needs\n\t * to support eVMCS v1 or above. We can also disable eVMCS support\n\t * with module parameter.\n\t */\n\tif (enlightened_vmcs &&\n\t    ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&\n\t    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=\n\t    KVM_EVMCS_VERSION) {\n\t\tint cpu;\n\n\t\t/* Check that we have assist pages on all online CPUs */\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tif (!hv_get_vp_assist_page(cpu)) {\n\t\t\t\tenlightened_vmcs = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (enlightened_vmcs) {\n\t\t\tpr_info(\"KVM: vmx: using Hyper-V Enlightened VMCS\\n\");\n\t\t\tstatic_branch_enable(&enable_evmcs);\n\t\t}\n\t} else {\n\t\tenlightened_vmcs = false;\n\t}\n#endif\n\n\tr = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),\n                     __alignof__(struct vcpu_vmx), THIS_MODULE);\n\tif (r)\n\t\treturn r;\n\n#ifdef CONFIG_KEXEC_CORE\n\trcu_assign_pointer(crash_vmclear_loaded_vmcss,\n\t\t\t   crash_vmclear_local_loaded_vmcss);\n#endif\n\tvmx_check_vmcs12_offsets();\n\n\treturn 0;\n}\n\nstatic void __exit vmx_exit(void)\n{\n#ifdef CONFIG_KEXEC_CORE\n\tRCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);\n\tsynchronize_rcu();\n#endif\n\n\tkvm_exit();\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tif (static_branch_unlikely(&enable_evmcs)) {\n\t\tint cpu;\n\t\tstruct hv_vp_assist_page *vp_ap;\n\t\t/*\n\t\t * Reset everything to support using non-enlightened VMCS\n\t\t * access later (e.g. when we reload the module with\n\t\t * enlightened_vmcs=0)\n\t\t */\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tvp_ap =\thv_get_vp_assist_page(cpu);\n\n\t\t\tif (!vp_ap)\n\t\t\t\tcontinue;\n\n\t\t\tvp_ap->current_nested_vmcs = 0;\n\t\t\tvp_ap->enlighten_vmentry = 0;\n\t\t}\n\n\t\tstatic_branch_disable(&enable_evmcs);\n\t}\n#endif\n}\n\nmodule_init(vmx_init)\nmodule_exit(vmx_exit)\n"], "fixing_code": ["/*\n * Kernel-based Virtual Machine driver for Linux\n *\n * This module enables machines with Intel VT-x extensions to run virtual\n * machines without emulation or binary translation.\n *\n * Copyright (C) 2006 Qumranet, Inc.\n * Copyright 2010 Red Hat, Inc. and/or its affiliates.\n *\n * Authors:\n *   Avi Kivity   <avi@qumranet.com>\n *   Yaniv Kamay  <yaniv@qumranet.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"irq.h\"\n#include \"mmu.h\"\n#include \"cpuid.h\"\n#include \"lapic.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/sched.h>\n#include <linux/moduleparam.h>\n#include <linux/mod_devicetable.h>\n#include <linux/trace_events.h>\n#include <linux/slab.h>\n#include <linux/tboot.h>\n#include <linux/hrtimer.h>\n#include <linux/frame.h>\n#include <linux/nospec.h>\n#include \"kvm_cache_regs.h\"\n#include \"x86.h\"\n\n#include <asm/cpu.h>\n#include <asm/io.h>\n#include <asm/desc.h>\n#include <asm/vmx.h>\n#include <asm/virtext.h>\n#include <asm/mce.h>\n#include <asm/fpu/internal.h>\n#include <asm/perf_event.h>\n#include <asm/debugreg.h>\n#include <asm/kexec.h>\n#include <asm/apic.h>\n#include <asm/irq_remapping.h>\n#include <asm/mmu_context.h>\n#include <asm/nospec-branch.h>\n#include <asm/mshyperv.h>\n\n#include \"trace.h\"\n#include \"pmu.h\"\n#include \"vmx_evmcs.h\"\n\n#define __ex(x) __kvm_handle_fault_on_reboot(x)\n#define __ex_clear(x, reg) \\\n\t____kvm_handle_fault_on_reboot(x, \"xor \" reg \" , \" reg)\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\nstatic const struct x86_cpu_id vmx_cpu_id[] = {\n\tX86_FEATURE_MATCH(X86_FEATURE_VMX),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);\n\nstatic bool __read_mostly enable_vpid = 1;\nmodule_param_named(vpid, enable_vpid, bool, 0444);\n\nstatic bool __read_mostly enable_vnmi = 1;\nmodule_param_named(vnmi, enable_vnmi, bool, S_IRUGO);\n\nstatic bool __read_mostly flexpriority_enabled = 1;\nmodule_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_ept = 1;\nmodule_param_named(ept, enable_ept, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_unrestricted_guest = 1;\nmodule_param_named(unrestricted_guest,\n\t\t\tenable_unrestricted_guest, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_ept_ad_bits = 1;\nmodule_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);\n\nstatic bool __read_mostly emulate_invalid_guest_state = true;\nmodule_param(emulate_invalid_guest_state, bool, S_IRUGO);\n\nstatic bool __read_mostly fasteoi = 1;\nmodule_param(fasteoi, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_apicv = 1;\nmodule_param(enable_apicv, bool, S_IRUGO);\n\nstatic bool __read_mostly enable_shadow_vmcs = 1;\nmodule_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);\n/*\n * If nested=1, nested virtualization is supported, i.e., guests may use\n * VMX and be a hypervisor for its own guests. If nested=0, guests may not\n * use VMX instructions.\n */\nstatic bool __read_mostly nested = 0;\nmodule_param(nested, bool, S_IRUGO);\n\nstatic u64 __read_mostly host_xss;\n\nstatic bool __read_mostly enable_pml = 1;\nmodule_param_named(pml, enable_pml, bool, S_IRUGO);\n\n#define MSR_TYPE_R\t1\n#define MSR_TYPE_W\t2\n#define MSR_TYPE_RW\t3\n\n#define MSR_BITMAP_MODE_X2APIC\t\t1\n#define MSR_BITMAP_MODE_X2APIC_APICV\t2\n#define MSR_BITMAP_MODE_LM\t\t4\n\n#define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL\n\n/* Guest_tsc -> host_tsc conversion requires 64-bit division.  */\nstatic int __read_mostly cpu_preemption_timer_multi;\nstatic bool __read_mostly enable_preemption_timer = 1;\n#ifdef CONFIG_X86_64\nmodule_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);\n#endif\n\n#define KVM_GUEST_CR0_MASK (X86_CR0_NW | X86_CR0_CD)\n#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR0_NE\n#define KVM_VM_CR0_ALWAYS_ON\t\t\t\t\\\n\t(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | \t\\\n\t X86_CR0_WP | X86_CR0_PG | X86_CR0_PE)\n#define KVM_CR4_GUEST_OWNED_BITS\t\t\t\t      \\\n\t(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \\\n\t | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)\n\n#define KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR4_VMXE\n#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)\n#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)\n\n#define RMODE_GUEST_OWNED_EFLAGS_BITS (~(X86_EFLAGS_IOPL | X86_EFLAGS_VM))\n\n#define VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE 5\n\n/*\n * Hyper-V requires all of these, so mark them as supported even though\n * they are just treated the same as all-context.\n */\n#define VMX_VPID_EXTENT_SUPPORTED_MASK\t\t\\\n\t(VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_NON_GLOBAL_BIT)\n\n/*\n * These 2 parameters are used to config the controls for Pause-Loop Exiting:\n * ple_gap:    upper bound on the amount of time between two successive\n *             executions of PAUSE in a loop. Also indicate if ple enabled.\n *             According to test, this time is usually smaller than 128 cycles.\n * ple_window: upper bound on the amount of time a guest is allowed to execute\n *             in a PAUSE loop. Tests indicate that most spinlocks are held for\n *             less than 2^12 cycles\n * Time is measured based on a counter that runs at the same rate as the TSC,\n * refer SDM volume 3b section 21.6.13 & 22.1.3.\n */\nstatic unsigned int ple_gap = KVM_DEFAULT_PLE_GAP;\n\nstatic unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;\nmodule_param(ple_window, uint, 0444);\n\n/* Default doubles per-vcpu window every exit. */\nstatic unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;\nmodule_param(ple_window_grow, uint, 0444);\n\n/* Default resets per-vcpu window every exit to ple_window. */\nstatic unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;\nmodule_param(ple_window_shrink, uint, 0444);\n\n/* Default is to compute the maximum so we can never overflow. */\nstatic unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;\nmodule_param(ple_window_max, uint, 0444);\n\nextern const ulong vmx_return;\n\nstruct kvm_vmx {\n\tstruct kvm kvm;\n\n\tunsigned int tss_addr;\n\tbool ept_identity_pagetable_done;\n\tgpa_t ept_identity_map_addr;\n};\n\n#define NR_AUTOLOAD_MSRS 8\n\nstruct vmcs {\n\tu32 revision_id;\n\tu32 abort;\n\tchar data[0];\n};\n\n/*\n * Track a VMCS that may be loaded on a certain CPU. If it is (cpu!=-1), also\n * remember whether it was VMLAUNCHed, and maintain a linked list of all VMCSs\n * loaded on this CPU (so we can clear them if the CPU goes down).\n */\nstruct loaded_vmcs {\n\tstruct vmcs *vmcs;\n\tstruct vmcs *shadow_vmcs;\n\tint cpu;\n\tbool launched;\n\tbool nmi_known_unmasked;\n\tunsigned long vmcs_host_cr3;\t/* May not match real cr3 */\n\tunsigned long vmcs_host_cr4;\t/* May not match real cr4 */\n\t/* Support for vnmi-less CPUs */\n\tint soft_vnmi_blocked;\n\tktime_t entry_time;\n\ts64 vnmi_blocked_time;\n\tunsigned long *msr_bitmap;\n\tstruct list_head loaded_vmcss_on_cpu_link;\n};\n\nstruct shared_msr_entry {\n\tunsigned index;\n\tu64 data;\n\tu64 mask;\n};\n\n/*\n * struct vmcs12 describes the state that our guest hypervisor (L1) keeps for a\n * single nested guest (L2), hence the name vmcs12. Any VMX implementation has\n * a VMCS structure, and vmcs12 is our emulated VMX's VMCS. This structure is\n * stored in guest memory specified by VMPTRLD, but is opaque to the guest,\n * which must access it using VMREAD/VMWRITE/VMCLEAR instructions.\n * More than one of these structures may exist, if L1 runs multiple L2 guests.\n * nested_vmx_run() will use the data here to build the vmcs02: a VMCS for the\n * underlying hardware which will be used to run L2.\n * This structure is packed to ensure that its layout is identical across\n * machines (necessary for live migration).\n *\n * IMPORTANT: Changing the layout of existing fields in this structure\n * will break save/restore compatibility with older kvm releases. When\n * adding new fields, either use space in the reserved padding* arrays\n * or add the new fields to the end of the structure.\n */\ntypedef u64 natural_width;\nstruct __packed vmcs12 {\n\t/* According to the Intel spec, a VMCS region must start with the\n\t * following two fields. Then follow implementation-specific data.\n\t */\n\tu32 revision_id;\n\tu32 abort;\n\n\tu32 launch_state; /* set to 0 by VMCLEAR, to 1 by VMLAUNCH */\n\tu32 padding[7]; /* room for future expansion */\n\n\tu64 io_bitmap_a;\n\tu64 io_bitmap_b;\n\tu64 msr_bitmap;\n\tu64 vm_exit_msr_store_addr;\n\tu64 vm_exit_msr_load_addr;\n\tu64 vm_entry_msr_load_addr;\n\tu64 tsc_offset;\n\tu64 virtual_apic_page_addr;\n\tu64 apic_access_addr;\n\tu64 posted_intr_desc_addr;\n\tu64 ept_pointer;\n\tu64 eoi_exit_bitmap0;\n\tu64 eoi_exit_bitmap1;\n\tu64 eoi_exit_bitmap2;\n\tu64 eoi_exit_bitmap3;\n\tu64 xss_exit_bitmap;\n\tu64 guest_physical_address;\n\tu64 vmcs_link_pointer;\n\tu64 guest_ia32_debugctl;\n\tu64 guest_ia32_pat;\n\tu64 guest_ia32_efer;\n\tu64 guest_ia32_perf_global_ctrl;\n\tu64 guest_pdptr0;\n\tu64 guest_pdptr1;\n\tu64 guest_pdptr2;\n\tu64 guest_pdptr3;\n\tu64 guest_bndcfgs;\n\tu64 host_ia32_pat;\n\tu64 host_ia32_efer;\n\tu64 host_ia32_perf_global_ctrl;\n\tu64 vmread_bitmap;\n\tu64 vmwrite_bitmap;\n\tu64 vm_function_control;\n\tu64 eptp_list_address;\n\tu64 pml_address;\n\tu64 padding64[3]; /* room for future expansion */\n\t/*\n\t * To allow migration of L1 (complete with its L2 guests) between\n\t * machines of different natural widths (32 or 64 bit), we cannot have\n\t * unsigned long fields with no explict size. We use u64 (aliased\n\t * natural_width) instead. Luckily, x86 is little-endian.\n\t */\n\tnatural_width cr0_guest_host_mask;\n\tnatural_width cr4_guest_host_mask;\n\tnatural_width cr0_read_shadow;\n\tnatural_width cr4_read_shadow;\n\tnatural_width cr3_target_value0;\n\tnatural_width cr3_target_value1;\n\tnatural_width cr3_target_value2;\n\tnatural_width cr3_target_value3;\n\tnatural_width exit_qualification;\n\tnatural_width guest_linear_address;\n\tnatural_width guest_cr0;\n\tnatural_width guest_cr3;\n\tnatural_width guest_cr4;\n\tnatural_width guest_es_base;\n\tnatural_width guest_cs_base;\n\tnatural_width guest_ss_base;\n\tnatural_width guest_ds_base;\n\tnatural_width guest_fs_base;\n\tnatural_width guest_gs_base;\n\tnatural_width guest_ldtr_base;\n\tnatural_width guest_tr_base;\n\tnatural_width guest_gdtr_base;\n\tnatural_width guest_idtr_base;\n\tnatural_width guest_dr7;\n\tnatural_width guest_rsp;\n\tnatural_width guest_rip;\n\tnatural_width guest_rflags;\n\tnatural_width guest_pending_dbg_exceptions;\n\tnatural_width guest_sysenter_esp;\n\tnatural_width guest_sysenter_eip;\n\tnatural_width host_cr0;\n\tnatural_width host_cr3;\n\tnatural_width host_cr4;\n\tnatural_width host_fs_base;\n\tnatural_width host_gs_base;\n\tnatural_width host_tr_base;\n\tnatural_width host_gdtr_base;\n\tnatural_width host_idtr_base;\n\tnatural_width host_ia32_sysenter_esp;\n\tnatural_width host_ia32_sysenter_eip;\n\tnatural_width host_rsp;\n\tnatural_width host_rip;\n\tnatural_width paddingl[8]; /* room for future expansion */\n\tu32 pin_based_vm_exec_control;\n\tu32 cpu_based_vm_exec_control;\n\tu32 exception_bitmap;\n\tu32 page_fault_error_code_mask;\n\tu32 page_fault_error_code_match;\n\tu32 cr3_target_count;\n\tu32 vm_exit_controls;\n\tu32 vm_exit_msr_store_count;\n\tu32 vm_exit_msr_load_count;\n\tu32 vm_entry_controls;\n\tu32 vm_entry_msr_load_count;\n\tu32 vm_entry_intr_info_field;\n\tu32 vm_entry_exception_error_code;\n\tu32 vm_entry_instruction_len;\n\tu32 tpr_threshold;\n\tu32 secondary_vm_exec_control;\n\tu32 vm_instruction_error;\n\tu32 vm_exit_reason;\n\tu32 vm_exit_intr_info;\n\tu32 vm_exit_intr_error_code;\n\tu32 idt_vectoring_info_field;\n\tu32 idt_vectoring_error_code;\n\tu32 vm_exit_instruction_len;\n\tu32 vmx_instruction_info;\n\tu32 guest_es_limit;\n\tu32 guest_cs_limit;\n\tu32 guest_ss_limit;\n\tu32 guest_ds_limit;\n\tu32 guest_fs_limit;\n\tu32 guest_gs_limit;\n\tu32 guest_ldtr_limit;\n\tu32 guest_tr_limit;\n\tu32 guest_gdtr_limit;\n\tu32 guest_idtr_limit;\n\tu32 guest_es_ar_bytes;\n\tu32 guest_cs_ar_bytes;\n\tu32 guest_ss_ar_bytes;\n\tu32 guest_ds_ar_bytes;\n\tu32 guest_fs_ar_bytes;\n\tu32 guest_gs_ar_bytes;\n\tu32 guest_ldtr_ar_bytes;\n\tu32 guest_tr_ar_bytes;\n\tu32 guest_interruptibility_info;\n\tu32 guest_activity_state;\n\tu32 guest_sysenter_cs;\n\tu32 host_ia32_sysenter_cs;\n\tu32 vmx_preemption_timer_value;\n\tu32 padding32[7]; /* room for future expansion */\n\tu16 virtual_processor_id;\n\tu16 posted_intr_nv;\n\tu16 guest_es_selector;\n\tu16 guest_cs_selector;\n\tu16 guest_ss_selector;\n\tu16 guest_ds_selector;\n\tu16 guest_fs_selector;\n\tu16 guest_gs_selector;\n\tu16 guest_ldtr_selector;\n\tu16 guest_tr_selector;\n\tu16 guest_intr_status;\n\tu16 host_es_selector;\n\tu16 host_cs_selector;\n\tu16 host_ss_selector;\n\tu16 host_ds_selector;\n\tu16 host_fs_selector;\n\tu16 host_gs_selector;\n\tu16 host_tr_selector;\n\tu16 guest_pml_index;\n};\n\n/*\n * For save/restore compatibility, the vmcs12 field offsets must not change.\n */\n#define CHECK_OFFSET(field, loc)\t\t\t\t\\\n\tBUILD_BUG_ON_MSG(offsetof(struct vmcs12, field) != (loc),\t\\\n\t\t\"Offset of \" #field \" in struct vmcs12 has changed.\")\n\nstatic inline void vmx_check_vmcs12_offsets(void) {\n\tCHECK_OFFSET(revision_id, 0);\n\tCHECK_OFFSET(abort, 4);\n\tCHECK_OFFSET(launch_state, 8);\n\tCHECK_OFFSET(io_bitmap_a, 40);\n\tCHECK_OFFSET(io_bitmap_b, 48);\n\tCHECK_OFFSET(msr_bitmap, 56);\n\tCHECK_OFFSET(vm_exit_msr_store_addr, 64);\n\tCHECK_OFFSET(vm_exit_msr_load_addr, 72);\n\tCHECK_OFFSET(vm_entry_msr_load_addr, 80);\n\tCHECK_OFFSET(tsc_offset, 88);\n\tCHECK_OFFSET(virtual_apic_page_addr, 96);\n\tCHECK_OFFSET(apic_access_addr, 104);\n\tCHECK_OFFSET(posted_intr_desc_addr, 112);\n\tCHECK_OFFSET(ept_pointer, 120);\n\tCHECK_OFFSET(eoi_exit_bitmap0, 128);\n\tCHECK_OFFSET(eoi_exit_bitmap1, 136);\n\tCHECK_OFFSET(eoi_exit_bitmap2, 144);\n\tCHECK_OFFSET(eoi_exit_bitmap3, 152);\n\tCHECK_OFFSET(xss_exit_bitmap, 160);\n\tCHECK_OFFSET(guest_physical_address, 168);\n\tCHECK_OFFSET(vmcs_link_pointer, 176);\n\tCHECK_OFFSET(guest_ia32_debugctl, 184);\n\tCHECK_OFFSET(guest_ia32_pat, 192);\n\tCHECK_OFFSET(guest_ia32_efer, 200);\n\tCHECK_OFFSET(guest_ia32_perf_global_ctrl, 208);\n\tCHECK_OFFSET(guest_pdptr0, 216);\n\tCHECK_OFFSET(guest_pdptr1, 224);\n\tCHECK_OFFSET(guest_pdptr2, 232);\n\tCHECK_OFFSET(guest_pdptr3, 240);\n\tCHECK_OFFSET(guest_bndcfgs, 248);\n\tCHECK_OFFSET(host_ia32_pat, 256);\n\tCHECK_OFFSET(host_ia32_efer, 264);\n\tCHECK_OFFSET(host_ia32_perf_global_ctrl, 272);\n\tCHECK_OFFSET(vmread_bitmap, 280);\n\tCHECK_OFFSET(vmwrite_bitmap, 288);\n\tCHECK_OFFSET(vm_function_control, 296);\n\tCHECK_OFFSET(eptp_list_address, 304);\n\tCHECK_OFFSET(pml_address, 312);\n\tCHECK_OFFSET(cr0_guest_host_mask, 344);\n\tCHECK_OFFSET(cr4_guest_host_mask, 352);\n\tCHECK_OFFSET(cr0_read_shadow, 360);\n\tCHECK_OFFSET(cr4_read_shadow, 368);\n\tCHECK_OFFSET(cr3_target_value0, 376);\n\tCHECK_OFFSET(cr3_target_value1, 384);\n\tCHECK_OFFSET(cr3_target_value2, 392);\n\tCHECK_OFFSET(cr3_target_value3, 400);\n\tCHECK_OFFSET(exit_qualification, 408);\n\tCHECK_OFFSET(guest_linear_address, 416);\n\tCHECK_OFFSET(guest_cr0, 424);\n\tCHECK_OFFSET(guest_cr3, 432);\n\tCHECK_OFFSET(guest_cr4, 440);\n\tCHECK_OFFSET(guest_es_base, 448);\n\tCHECK_OFFSET(guest_cs_base, 456);\n\tCHECK_OFFSET(guest_ss_base, 464);\n\tCHECK_OFFSET(guest_ds_base, 472);\n\tCHECK_OFFSET(guest_fs_base, 480);\n\tCHECK_OFFSET(guest_gs_base, 488);\n\tCHECK_OFFSET(guest_ldtr_base, 496);\n\tCHECK_OFFSET(guest_tr_base, 504);\n\tCHECK_OFFSET(guest_gdtr_base, 512);\n\tCHECK_OFFSET(guest_idtr_base, 520);\n\tCHECK_OFFSET(guest_dr7, 528);\n\tCHECK_OFFSET(guest_rsp, 536);\n\tCHECK_OFFSET(guest_rip, 544);\n\tCHECK_OFFSET(guest_rflags, 552);\n\tCHECK_OFFSET(guest_pending_dbg_exceptions, 560);\n\tCHECK_OFFSET(guest_sysenter_esp, 568);\n\tCHECK_OFFSET(guest_sysenter_eip, 576);\n\tCHECK_OFFSET(host_cr0, 584);\n\tCHECK_OFFSET(host_cr3, 592);\n\tCHECK_OFFSET(host_cr4, 600);\n\tCHECK_OFFSET(host_fs_base, 608);\n\tCHECK_OFFSET(host_gs_base, 616);\n\tCHECK_OFFSET(host_tr_base, 624);\n\tCHECK_OFFSET(host_gdtr_base, 632);\n\tCHECK_OFFSET(host_idtr_base, 640);\n\tCHECK_OFFSET(host_ia32_sysenter_esp, 648);\n\tCHECK_OFFSET(host_ia32_sysenter_eip, 656);\n\tCHECK_OFFSET(host_rsp, 664);\n\tCHECK_OFFSET(host_rip, 672);\n\tCHECK_OFFSET(pin_based_vm_exec_control, 744);\n\tCHECK_OFFSET(cpu_based_vm_exec_control, 748);\n\tCHECK_OFFSET(exception_bitmap, 752);\n\tCHECK_OFFSET(page_fault_error_code_mask, 756);\n\tCHECK_OFFSET(page_fault_error_code_match, 760);\n\tCHECK_OFFSET(cr3_target_count, 764);\n\tCHECK_OFFSET(vm_exit_controls, 768);\n\tCHECK_OFFSET(vm_exit_msr_store_count, 772);\n\tCHECK_OFFSET(vm_exit_msr_load_count, 776);\n\tCHECK_OFFSET(vm_entry_controls, 780);\n\tCHECK_OFFSET(vm_entry_msr_load_count, 784);\n\tCHECK_OFFSET(vm_entry_intr_info_field, 788);\n\tCHECK_OFFSET(vm_entry_exception_error_code, 792);\n\tCHECK_OFFSET(vm_entry_instruction_len, 796);\n\tCHECK_OFFSET(tpr_threshold, 800);\n\tCHECK_OFFSET(secondary_vm_exec_control, 804);\n\tCHECK_OFFSET(vm_instruction_error, 808);\n\tCHECK_OFFSET(vm_exit_reason, 812);\n\tCHECK_OFFSET(vm_exit_intr_info, 816);\n\tCHECK_OFFSET(vm_exit_intr_error_code, 820);\n\tCHECK_OFFSET(idt_vectoring_info_field, 824);\n\tCHECK_OFFSET(idt_vectoring_error_code, 828);\n\tCHECK_OFFSET(vm_exit_instruction_len, 832);\n\tCHECK_OFFSET(vmx_instruction_info, 836);\n\tCHECK_OFFSET(guest_es_limit, 840);\n\tCHECK_OFFSET(guest_cs_limit, 844);\n\tCHECK_OFFSET(guest_ss_limit, 848);\n\tCHECK_OFFSET(guest_ds_limit, 852);\n\tCHECK_OFFSET(guest_fs_limit, 856);\n\tCHECK_OFFSET(guest_gs_limit, 860);\n\tCHECK_OFFSET(guest_ldtr_limit, 864);\n\tCHECK_OFFSET(guest_tr_limit, 868);\n\tCHECK_OFFSET(guest_gdtr_limit, 872);\n\tCHECK_OFFSET(guest_idtr_limit, 876);\n\tCHECK_OFFSET(guest_es_ar_bytes, 880);\n\tCHECK_OFFSET(guest_cs_ar_bytes, 884);\n\tCHECK_OFFSET(guest_ss_ar_bytes, 888);\n\tCHECK_OFFSET(guest_ds_ar_bytes, 892);\n\tCHECK_OFFSET(guest_fs_ar_bytes, 896);\n\tCHECK_OFFSET(guest_gs_ar_bytes, 900);\n\tCHECK_OFFSET(guest_ldtr_ar_bytes, 904);\n\tCHECK_OFFSET(guest_tr_ar_bytes, 908);\n\tCHECK_OFFSET(guest_interruptibility_info, 912);\n\tCHECK_OFFSET(guest_activity_state, 916);\n\tCHECK_OFFSET(guest_sysenter_cs, 920);\n\tCHECK_OFFSET(host_ia32_sysenter_cs, 924);\n\tCHECK_OFFSET(vmx_preemption_timer_value, 928);\n\tCHECK_OFFSET(virtual_processor_id, 960);\n\tCHECK_OFFSET(posted_intr_nv, 962);\n\tCHECK_OFFSET(guest_es_selector, 964);\n\tCHECK_OFFSET(guest_cs_selector, 966);\n\tCHECK_OFFSET(guest_ss_selector, 968);\n\tCHECK_OFFSET(guest_ds_selector, 970);\n\tCHECK_OFFSET(guest_fs_selector, 972);\n\tCHECK_OFFSET(guest_gs_selector, 974);\n\tCHECK_OFFSET(guest_ldtr_selector, 976);\n\tCHECK_OFFSET(guest_tr_selector, 978);\n\tCHECK_OFFSET(guest_intr_status, 980);\n\tCHECK_OFFSET(host_es_selector, 982);\n\tCHECK_OFFSET(host_cs_selector, 984);\n\tCHECK_OFFSET(host_ss_selector, 986);\n\tCHECK_OFFSET(host_ds_selector, 988);\n\tCHECK_OFFSET(host_fs_selector, 990);\n\tCHECK_OFFSET(host_gs_selector, 992);\n\tCHECK_OFFSET(host_tr_selector, 994);\n\tCHECK_OFFSET(guest_pml_index, 996);\n}\n\n/*\n * VMCS12_REVISION is an arbitrary id that should be changed if the content or\n * layout of struct vmcs12 is changed. MSR_IA32_VMX_BASIC returns this id, and\n * VMPTRLD verifies that the VMCS region that L1 is loading contains this id.\n *\n * IMPORTANT: Changing this value will break save/restore compatibility with\n * older kvm releases.\n */\n#define VMCS12_REVISION 0x11e57ed0\n\n/*\n * VMCS12_SIZE is the number of bytes L1 should allocate for the VMXON region\n * and any VMCS region. Although only sizeof(struct vmcs12) are used by the\n * current implementation, 4K are reserved to avoid future complications.\n */\n#define VMCS12_SIZE 0x1000\n\n/*\n * VMCS12_MAX_FIELD_INDEX is the highest index value used in any\n * supported VMCS12 field encoding.\n */\n#define VMCS12_MAX_FIELD_INDEX 0x17\n\nstruct nested_vmx_msrs {\n\t/*\n\t * We only store the \"true\" versions of the VMX capability MSRs. We\n\t * generate the \"non-true\" versions by setting the must-be-1 bits\n\t * according to the SDM.\n\t */\n\tu32 procbased_ctls_low;\n\tu32 procbased_ctls_high;\n\tu32 secondary_ctls_low;\n\tu32 secondary_ctls_high;\n\tu32 pinbased_ctls_low;\n\tu32 pinbased_ctls_high;\n\tu32 exit_ctls_low;\n\tu32 exit_ctls_high;\n\tu32 entry_ctls_low;\n\tu32 entry_ctls_high;\n\tu32 misc_low;\n\tu32 misc_high;\n\tu32 ept_caps;\n\tu32 vpid_caps;\n\tu64 basic;\n\tu64 cr0_fixed0;\n\tu64 cr0_fixed1;\n\tu64 cr4_fixed0;\n\tu64 cr4_fixed1;\n\tu64 vmcs_enum;\n\tu64 vmfunc_controls;\n};\n\n/*\n * The nested_vmx structure is part of vcpu_vmx, and holds information we need\n * for correct emulation of VMX (i.e., nested VMX) on this vcpu.\n */\nstruct nested_vmx {\n\t/* Has the level1 guest done vmxon? */\n\tbool vmxon;\n\tgpa_t vmxon_ptr;\n\tbool pml_full;\n\n\t/* The guest-physical address of the current VMCS L1 keeps for L2 */\n\tgpa_t current_vmptr;\n\t/*\n\t * Cache of the guest's VMCS, existing outside of guest memory.\n\t * Loaded from guest memory during VMPTRLD. Flushed to guest\n\t * memory during VMCLEAR and VMPTRLD.\n\t */\n\tstruct vmcs12 *cached_vmcs12;\n\t/*\n\t * Indicates if the shadow vmcs must be updated with the\n\t * data hold by vmcs12\n\t */\n\tbool sync_shadow_vmcs;\n\tbool dirty_vmcs12;\n\n\tbool change_vmcs01_virtual_apic_mode;\n\n\t/* L2 must run next, and mustn't decide to exit to L1. */\n\tbool nested_run_pending;\n\n\tstruct loaded_vmcs vmcs02;\n\n\t/*\n\t * Guest pages referred to in the vmcs02 with host-physical\n\t * pointers, so we must keep them pinned while L2 runs.\n\t */\n\tstruct page *apic_access_page;\n\tstruct page *virtual_apic_page;\n\tstruct page *pi_desc_page;\n\tstruct pi_desc *pi_desc;\n\tbool pi_pending;\n\tu16 posted_intr_nv;\n\n\tstruct hrtimer preemption_timer;\n\tbool preemption_timer_expired;\n\n\t/* to migrate it to L2 if VM_ENTRY_LOAD_DEBUG_CONTROLS is off */\n\tu64 vmcs01_debugctl;\n\n\tu16 vpid02;\n\tu16 last_vpid;\n\n\tstruct nested_vmx_msrs msrs;\n\n\t/* SMM related state */\n\tstruct {\n\t\t/* in VMX operation on SMM entry? */\n\t\tbool vmxon;\n\t\t/* in guest mode on SMM entry? */\n\t\tbool guest_mode;\n\t} smm;\n};\n\n#define POSTED_INTR_ON  0\n#define POSTED_INTR_SN  1\n\n/* Posted-Interrupt Descriptor */\nstruct pi_desc {\n\tu32 pir[8];     /* Posted interrupt requested */\n\tunion {\n\t\tstruct {\n\t\t\t\t/* bit 256 - Outstanding Notification */\n\t\t\tu16\ton\t: 1,\n\t\t\t\t/* bit 257 - Suppress Notification */\n\t\t\t\tsn\t: 1,\n\t\t\t\t/* bit 271:258 - Reserved */\n\t\t\t\trsvd_1\t: 14;\n\t\t\t\t/* bit 279:272 - Notification Vector */\n\t\t\tu8\tnv;\n\t\t\t\t/* bit 287:280 - Reserved */\n\t\t\tu8\trsvd_2;\n\t\t\t\t/* bit 319:288 - Notification Destination */\n\t\t\tu32\tndst;\n\t\t};\n\t\tu64 control;\n\t};\n\tu32 rsvd[6];\n} __aligned(64);\n\nstatic bool pi_test_and_set_on(struct pi_desc *pi_desc)\n{\n\treturn test_and_set_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic bool pi_test_and_clear_on(struct pi_desc *pi_desc)\n{\n\treturn test_and_clear_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic int pi_test_and_set_pir(int vector, struct pi_desc *pi_desc)\n{\n\treturn test_and_set_bit(vector, (unsigned long *)pi_desc->pir);\n}\n\nstatic inline void pi_clear_sn(struct pi_desc *pi_desc)\n{\n\treturn clear_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline void pi_set_sn(struct pi_desc *pi_desc)\n{\n\treturn set_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline void pi_clear_on(struct pi_desc *pi_desc)\n{\n\tclear_bit(POSTED_INTR_ON,\n  \t\t  (unsigned long *)&pi_desc->control);\n}\n\nstatic inline int pi_test_on(struct pi_desc *pi_desc)\n{\n\treturn test_bit(POSTED_INTR_ON,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstatic inline int pi_test_sn(struct pi_desc *pi_desc)\n{\n\treturn test_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}\n\nstruct vcpu_vmx {\n\tstruct kvm_vcpu       vcpu;\n\tunsigned long         host_rsp;\n\tu8                    fail;\n\tu8\t\t      msr_bitmap_mode;\n\tu32                   exit_intr_info;\n\tu32                   idt_vectoring_info;\n\tulong                 rflags;\n\tstruct shared_msr_entry *guest_msrs;\n\tint                   nmsrs;\n\tint                   save_nmsrs;\n\tunsigned long\t      host_idt_base;\n#ifdef CONFIG_X86_64\n\tu64 \t\t      msr_host_kernel_gs_base;\n\tu64 \t\t      msr_guest_kernel_gs_base;\n#endif\n\n\tu64 \t\t      arch_capabilities;\n\tu64 \t\t      spec_ctrl;\n\n\tu32 vm_entry_controls_shadow;\n\tu32 vm_exit_controls_shadow;\n\tu32 secondary_exec_control;\n\n\t/*\n\t * loaded_vmcs points to the VMCS currently used in this vcpu. For a\n\t * non-nested (L1) guest, it always points to vmcs01. For a nested\n\t * guest (L2), it points to a different VMCS.\n\t */\n\tstruct loaded_vmcs    vmcs01;\n\tstruct loaded_vmcs   *loaded_vmcs;\n\tbool                  __launched; /* temporary, used in vmx_vcpu_run */\n\tstruct msr_autoload {\n\t\tunsigned nr;\n\t\tstruct vmx_msr_entry guest[NR_AUTOLOAD_MSRS];\n\t\tstruct vmx_msr_entry host[NR_AUTOLOAD_MSRS];\n\t} msr_autoload;\n\tstruct {\n\t\tint           loaded;\n\t\tu16           fs_sel, gs_sel, ldt_sel;\n#ifdef CONFIG_X86_64\n\t\tu16           ds_sel, es_sel;\n#endif\n\t\tint           gs_ldt_reload_needed;\n\t\tint           fs_reload_needed;\n\t\tu64           msr_host_bndcfgs;\n\t} host_state;\n\tstruct {\n\t\tint vm86_active;\n\t\tulong save_rflags;\n\t\tstruct kvm_segment segs[8];\n\t} rmode;\n\tstruct {\n\t\tu32 bitmask; /* 4 bits per segment (1 bit per field) */\n\t\tstruct kvm_save_segment {\n\t\t\tu16 selector;\n\t\t\tunsigned long base;\n\t\t\tu32 limit;\n\t\t\tu32 ar;\n\t\t} seg[8];\n\t} segment_cache;\n\tint vpid;\n\tbool emulation_required;\n\n\tu32 exit_reason;\n\n\t/* Posted interrupt descriptor */\n\tstruct pi_desc pi_desc;\n\n\t/* Support for a guest hypervisor (nested VMX) */\n\tstruct nested_vmx nested;\n\n\t/* Dynamic PLE window. */\n\tint ple_window;\n\tbool ple_window_dirty;\n\n\t/* Support for PML */\n#define PML_ENTITY_NUM\t\t512\n\tstruct page *pml_pg;\n\n\t/* apic deadline value in host tsc */\n\tu64 hv_deadline_tsc;\n\n\tu64 current_tsc_ratio;\n\n\tu32 host_pkru;\n\n\tunsigned long host_debugctlmsr;\n\n\t/*\n\t * Only bits masked by msr_ia32_feature_control_valid_bits can be set in\n\t * msr_ia32_feature_control. FEATURE_CONTROL_LOCKED is always included\n\t * in msr_ia32_feature_control_valid_bits.\n\t */\n\tu64 msr_ia32_feature_control;\n\tu64 msr_ia32_feature_control_valid_bits;\n};\n\nenum segment_cache_field {\n\tSEG_FIELD_SEL = 0,\n\tSEG_FIELD_BASE = 1,\n\tSEG_FIELD_LIMIT = 2,\n\tSEG_FIELD_AR = 3,\n\n\tSEG_FIELD_NR = 4\n};\n\nstatic inline struct kvm_vmx *to_kvm_vmx(struct kvm *kvm)\n{\n\treturn container_of(kvm, struct kvm_vmx, kvm);\n}\n\nstatic inline struct vcpu_vmx *to_vmx(struct kvm_vcpu *vcpu)\n{\n\treturn container_of(vcpu, struct vcpu_vmx, vcpu);\n}\n\nstatic struct pi_desc *vcpu_to_pi_desc(struct kvm_vcpu *vcpu)\n{\n\treturn &(to_vmx(vcpu)->pi_desc);\n}\n\n#define ROL16(val, n) ((u16)(((u16)(val) << (n)) | ((u16)(val) >> (16 - (n)))))\n#define VMCS12_OFFSET(x) offsetof(struct vmcs12, x)\n#define FIELD(number, name)\t[ROL16(number, 6)] = VMCS12_OFFSET(name)\n#define FIELD64(number, name)\t\t\t\t\t\t\\\n\tFIELD(number, name),\t\t\t\t\t\t\\\n\t[ROL16(number##_HIGH, 6)] = VMCS12_OFFSET(name) + sizeof(u32)\n\n\nstatic u16 shadow_read_only_fields[] = {\n#define SHADOW_FIELD_RO(x) x,\n#include \"vmx_shadow_fields.h\"\n};\nstatic int max_shadow_read_only_fields =\n\tARRAY_SIZE(shadow_read_only_fields);\n\nstatic u16 shadow_read_write_fields[] = {\n#define SHADOW_FIELD_RW(x) x,\n#include \"vmx_shadow_fields.h\"\n};\nstatic int max_shadow_read_write_fields =\n\tARRAY_SIZE(shadow_read_write_fields);\n\nstatic const unsigned short vmcs_field_to_offset_table[] = {\n\tFIELD(VIRTUAL_PROCESSOR_ID, virtual_processor_id),\n\tFIELD(POSTED_INTR_NV, posted_intr_nv),\n\tFIELD(GUEST_ES_SELECTOR, guest_es_selector),\n\tFIELD(GUEST_CS_SELECTOR, guest_cs_selector),\n\tFIELD(GUEST_SS_SELECTOR, guest_ss_selector),\n\tFIELD(GUEST_DS_SELECTOR, guest_ds_selector),\n\tFIELD(GUEST_FS_SELECTOR, guest_fs_selector),\n\tFIELD(GUEST_GS_SELECTOR, guest_gs_selector),\n\tFIELD(GUEST_LDTR_SELECTOR, guest_ldtr_selector),\n\tFIELD(GUEST_TR_SELECTOR, guest_tr_selector),\n\tFIELD(GUEST_INTR_STATUS, guest_intr_status),\n\tFIELD(GUEST_PML_INDEX, guest_pml_index),\n\tFIELD(HOST_ES_SELECTOR, host_es_selector),\n\tFIELD(HOST_CS_SELECTOR, host_cs_selector),\n\tFIELD(HOST_SS_SELECTOR, host_ss_selector),\n\tFIELD(HOST_DS_SELECTOR, host_ds_selector),\n\tFIELD(HOST_FS_SELECTOR, host_fs_selector),\n\tFIELD(HOST_GS_SELECTOR, host_gs_selector),\n\tFIELD(HOST_TR_SELECTOR, host_tr_selector),\n\tFIELD64(IO_BITMAP_A, io_bitmap_a),\n\tFIELD64(IO_BITMAP_B, io_bitmap_b),\n\tFIELD64(MSR_BITMAP, msr_bitmap),\n\tFIELD64(VM_EXIT_MSR_STORE_ADDR, vm_exit_msr_store_addr),\n\tFIELD64(VM_EXIT_MSR_LOAD_ADDR, vm_exit_msr_load_addr),\n\tFIELD64(VM_ENTRY_MSR_LOAD_ADDR, vm_entry_msr_load_addr),\n\tFIELD64(PML_ADDRESS, pml_address),\n\tFIELD64(TSC_OFFSET, tsc_offset),\n\tFIELD64(VIRTUAL_APIC_PAGE_ADDR, virtual_apic_page_addr),\n\tFIELD64(APIC_ACCESS_ADDR, apic_access_addr),\n\tFIELD64(POSTED_INTR_DESC_ADDR, posted_intr_desc_addr),\n\tFIELD64(VM_FUNCTION_CONTROL, vm_function_control),\n\tFIELD64(EPT_POINTER, ept_pointer),\n\tFIELD64(EOI_EXIT_BITMAP0, eoi_exit_bitmap0),\n\tFIELD64(EOI_EXIT_BITMAP1, eoi_exit_bitmap1),\n\tFIELD64(EOI_EXIT_BITMAP2, eoi_exit_bitmap2),\n\tFIELD64(EOI_EXIT_BITMAP3, eoi_exit_bitmap3),\n\tFIELD64(EPTP_LIST_ADDRESS, eptp_list_address),\n\tFIELD64(VMREAD_BITMAP, vmread_bitmap),\n\tFIELD64(VMWRITE_BITMAP, vmwrite_bitmap),\n\tFIELD64(XSS_EXIT_BITMAP, xss_exit_bitmap),\n\tFIELD64(GUEST_PHYSICAL_ADDRESS, guest_physical_address),\n\tFIELD64(VMCS_LINK_POINTER, vmcs_link_pointer),\n\tFIELD64(GUEST_IA32_DEBUGCTL, guest_ia32_debugctl),\n\tFIELD64(GUEST_IA32_PAT, guest_ia32_pat),\n\tFIELD64(GUEST_IA32_EFER, guest_ia32_efer),\n\tFIELD64(GUEST_IA32_PERF_GLOBAL_CTRL, guest_ia32_perf_global_ctrl),\n\tFIELD64(GUEST_PDPTR0, guest_pdptr0),\n\tFIELD64(GUEST_PDPTR1, guest_pdptr1),\n\tFIELD64(GUEST_PDPTR2, guest_pdptr2),\n\tFIELD64(GUEST_PDPTR3, guest_pdptr3),\n\tFIELD64(GUEST_BNDCFGS, guest_bndcfgs),\n\tFIELD64(HOST_IA32_PAT, host_ia32_pat),\n\tFIELD64(HOST_IA32_EFER, host_ia32_efer),\n\tFIELD64(HOST_IA32_PERF_GLOBAL_CTRL, host_ia32_perf_global_ctrl),\n\tFIELD(PIN_BASED_VM_EXEC_CONTROL, pin_based_vm_exec_control),\n\tFIELD(CPU_BASED_VM_EXEC_CONTROL, cpu_based_vm_exec_control),\n\tFIELD(EXCEPTION_BITMAP, exception_bitmap),\n\tFIELD(PAGE_FAULT_ERROR_CODE_MASK, page_fault_error_code_mask),\n\tFIELD(PAGE_FAULT_ERROR_CODE_MATCH, page_fault_error_code_match),\n\tFIELD(CR3_TARGET_COUNT, cr3_target_count),\n\tFIELD(VM_EXIT_CONTROLS, vm_exit_controls),\n\tFIELD(VM_EXIT_MSR_STORE_COUNT, vm_exit_msr_store_count),\n\tFIELD(VM_EXIT_MSR_LOAD_COUNT, vm_exit_msr_load_count),\n\tFIELD(VM_ENTRY_CONTROLS, vm_entry_controls),\n\tFIELD(VM_ENTRY_MSR_LOAD_COUNT, vm_entry_msr_load_count),\n\tFIELD(VM_ENTRY_INTR_INFO_FIELD, vm_entry_intr_info_field),\n\tFIELD(VM_ENTRY_EXCEPTION_ERROR_CODE, vm_entry_exception_error_code),\n\tFIELD(VM_ENTRY_INSTRUCTION_LEN, vm_entry_instruction_len),\n\tFIELD(TPR_THRESHOLD, tpr_threshold),\n\tFIELD(SECONDARY_VM_EXEC_CONTROL, secondary_vm_exec_control),\n\tFIELD(VM_INSTRUCTION_ERROR, vm_instruction_error),\n\tFIELD(VM_EXIT_REASON, vm_exit_reason),\n\tFIELD(VM_EXIT_INTR_INFO, vm_exit_intr_info),\n\tFIELD(VM_EXIT_INTR_ERROR_CODE, vm_exit_intr_error_code),\n\tFIELD(IDT_VECTORING_INFO_FIELD, idt_vectoring_info_field),\n\tFIELD(IDT_VECTORING_ERROR_CODE, idt_vectoring_error_code),\n\tFIELD(VM_EXIT_INSTRUCTION_LEN, vm_exit_instruction_len),\n\tFIELD(VMX_INSTRUCTION_INFO, vmx_instruction_info),\n\tFIELD(GUEST_ES_LIMIT, guest_es_limit),\n\tFIELD(GUEST_CS_LIMIT, guest_cs_limit),\n\tFIELD(GUEST_SS_LIMIT, guest_ss_limit),\n\tFIELD(GUEST_DS_LIMIT, guest_ds_limit),\n\tFIELD(GUEST_FS_LIMIT, guest_fs_limit),\n\tFIELD(GUEST_GS_LIMIT, guest_gs_limit),\n\tFIELD(GUEST_LDTR_LIMIT, guest_ldtr_limit),\n\tFIELD(GUEST_TR_LIMIT, guest_tr_limit),\n\tFIELD(GUEST_GDTR_LIMIT, guest_gdtr_limit),\n\tFIELD(GUEST_IDTR_LIMIT, guest_idtr_limit),\n\tFIELD(GUEST_ES_AR_BYTES, guest_es_ar_bytes),\n\tFIELD(GUEST_CS_AR_BYTES, guest_cs_ar_bytes),\n\tFIELD(GUEST_SS_AR_BYTES, guest_ss_ar_bytes),\n\tFIELD(GUEST_DS_AR_BYTES, guest_ds_ar_bytes),\n\tFIELD(GUEST_FS_AR_BYTES, guest_fs_ar_bytes),\n\tFIELD(GUEST_GS_AR_BYTES, guest_gs_ar_bytes),\n\tFIELD(GUEST_LDTR_AR_BYTES, guest_ldtr_ar_bytes),\n\tFIELD(GUEST_TR_AR_BYTES, guest_tr_ar_bytes),\n\tFIELD(GUEST_INTERRUPTIBILITY_INFO, guest_interruptibility_info),\n\tFIELD(GUEST_ACTIVITY_STATE, guest_activity_state),\n\tFIELD(GUEST_SYSENTER_CS, guest_sysenter_cs),\n\tFIELD(HOST_IA32_SYSENTER_CS, host_ia32_sysenter_cs),\n\tFIELD(VMX_PREEMPTION_TIMER_VALUE, vmx_preemption_timer_value),\n\tFIELD(CR0_GUEST_HOST_MASK, cr0_guest_host_mask),\n\tFIELD(CR4_GUEST_HOST_MASK, cr4_guest_host_mask),\n\tFIELD(CR0_READ_SHADOW, cr0_read_shadow),\n\tFIELD(CR4_READ_SHADOW, cr4_read_shadow),\n\tFIELD(CR3_TARGET_VALUE0, cr3_target_value0),\n\tFIELD(CR3_TARGET_VALUE1, cr3_target_value1),\n\tFIELD(CR3_TARGET_VALUE2, cr3_target_value2),\n\tFIELD(CR3_TARGET_VALUE3, cr3_target_value3),\n\tFIELD(EXIT_QUALIFICATION, exit_qualification),\n\tFIELD(GUEST_LINEAR_ADDRESS, guest_linear_address),\n\tFIELD(GUEST_CR0, guest_cr0),\n\tFIELD(GUEST_CR3, guest_cr3),\n\tFIELD(GUEST_CR4, guest_cr4),\n\tFIELD(GUEST_ES_BASE, guest_es_base),\n\tFIELD(GUEST_CS_BASE, guest_cs_base),\n\tFIELD(GUEST_SS_BASE, guest_ss_base),\n\tFIELD(GUEST_DS_BASE, guest_ds_base),\n\tFIELD(GUEST_FS_BASE, guest_fs_base),\n\tFIELD(GUEST_GS_BASE, guest_gs_base),\n\tFIELD(GUEST_LDTR_BASE, guest_ldtr_base),\n\tFIELD(GUEST_TR_BASE, guest_tr_base),\n\tFIELD(GUEST_GDTR_BASE, guest_gdtr_base),\n\tFIELD(GUEST_IDTR_BASE, guest_idtr_base),\n\tFIELD(GUEST_DR7, guest_dr7),\n\tFIELD(GUEST_RSP, guest_rsp),\n\tFIELD(GUEST_RIP, guest_rip),\n\tFIELD(GUEST_RFLAGS, guest_rflags),\n\tFIELD(GUEST_PENDING_DBG_EXCEPTIONS, guest_pending_dbg_exceptions),\n\tFIELD(GUEST_SYSENTER_ESP, guest_sysenter_esp),\n\tFIELD(GUEST_SYSENTER_EIP, guest_sysenter_eip),\n\tFIELD(HOST_CR0, host_cr0),\n\tFIELD(HOST_CR3, host_cr3),\n\tFIELD(HOST_CR4, host_cr4),\n\tFIELD(HOST_FS_BASE, host_fs_base),\n\tFIELD(HOST_GS_BASE, host_gs_base),\n\tFIELD(HOST_TR_BASE, host_tr_base),\n\tFIELD(HOST_GDTR_BASE, host_gdtr_base),\n\tFIELD(HOST_IDTR_BASE, host_idtr_base),\n\tFIELD(HOST_IA32_SYSENTER_ESP, host_ia32_sysenter_esp),\n\tFIELD(HOST_IA32_SYSENTER_EIP, host_ia32_sysenter_eip),\n\tFIELD(HOST_RSP, host_rsp),\n\tFIELD(HOST_RIP, host_rip),\n};\n\nstatic inline short vmcs_field_to_offset(unsigned long field)\n{\n\tconst size_t size = ARRAY_SIZE(vmcs_field_to_offset_table);\n\tunsigned short offset;\n\tunsigned index;\n\n\tif (field >> 15)\n\t\treturn -ENOENT;\n\n\tindex = ROL16(field, 6);\n\tif (index >= size)\n\t\treturn -ENOENT;\n\n\tindex = array_index_nospec(index, size);\n\toffset = vmcs_field_to_offset_table[index];\n\tif (offset == 0)\n\t\treturn -ENOENT;\n\treturn offset;\n}\n\nstatic inline struct vmcs12 *get_vmcs12(struct kvm_vcpu *vcpu)\n{\n\treturn to_vmx(vcpu)->nested.cached_vmcs12;\n}\n\nstatic bool nested_ept_ad_enabled(struct kvm_vcpu *vcpu);\nstatic unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu);\nstatic u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa);\nstatic bool vmx_xsaves_supported(void);\nstatic void vmx_set_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\nstatic void vmx_get_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\nstatic bool guest_state_valid(struct kvm_vcpu *vcpu);\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var);\nstatic void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx);\nstatic bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu);\nstatic void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked);\nstatic bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,\n\t\t\t\t\t    u16 error_code);\nstatic void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu);\nstatic void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t  u32 msr, int type);\n\nstatic DEFINE_PER_CPU(struct vmcs *, vmxarea);\nstatic DEFINE_PER_CPU(struct vmcs *, current_vmcs);\n/*\n * We maintain a per-CPU linked-list of VMCS loaded on that CPU. This is needed\n * when a CPU is brought down, and we need to VMCLEAR all VMCSs loaded on it.\n */\nstatic DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);\n\n/*\n * We maintian a per-CPU linked-list of vCPU, so in wakeup_handler() we\n * can find which vCPU should be waken up.\n */\nstatic DEFINE_PER_CPU(struct list_head, blocked_vcpu_on_cpu);\nstatic DEFINE_PER_CPU(spinlock_t, blocked_vcpu_on_cpu_lock);\n\nenum {\n\tVMX_VMREAD_BITMAP,\n\tVMX_VMWRITE_BITMAP,\n\tVMX_BITMAP_NR\n};\n\nstatic unsigned long *vmx_bitmap[VMX_BITMAP_NR];\n\n#define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])\n#define vmx_vmwrite_bitmap                   (vmx_bitmap[VMX_VMWRITE_BITMAP])\n\nstatic bool cpu_has_load_ia32_efer;\nstatic bool cpu_has_load_perf_global_ctrl;\n\nstatic DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);\nstatic DEFINE_SPINLOCK(vmx_vpid_lock);\n\nstatic struct vmcs_config {\n\tint size;\n\tint order;\n\tu32 basic_cap;\n\tu32 revision_id;\n\tu32 pin_based_exec_ctrl;\n\tu32 cpu_based_exec_ctrl;\n\tu32 cpu_based_2nd_exec_ctrl;\n\tu32 vmexit_ctrl;\n\tu32 vmentry_ctrl;\n\tstruct nested_vmx_msrs nested;\n} vmcs_config;\n\nstatic struct vmx_capability {\n\tu32 ept;\n\tu32 vpid;\n} vmx_capability;\n\n#define VMX_SEGMENT_FIELD(seg)\t\t\t\t\t\\\n\t[VCPU_SREG_##seg] = {                                   \\\n\t\t.selector = GUEST_##seg##_SELECTOR,\t\t\\\n\t\t.base = GUEST_##seg##_BASE,\t\t   \t\\\n\t\t.limit = GUEST_##seg##_LIMIT,\t\t   \t\\\n\t\t.ar_bytes = GUEST_##seg##_AR_BYTES,\t   \t\\\n\t}\n\nstatic const struct kvm_vmx_segment_field {\n\tunsigned selector;\n\tunsigned base;\n\tunsigned limit;\n\tunsigned ar_bytes;\n} kvm_vmx_segment_fields[] = {\n\tVMX_SEGMENT_FIELD(CS),\n\tVMX_SEGMENT_FIELD(DS),\n\tVMX_SEGMENT_FIELD(ES),\n\tVMX_SEGMENT_FIELD(FS),\n\tVMX_SEGMENT_FIELD(GS),\n\tVMX_SEGMENT_FIELD(SS),\n\tVMX_SEGMENT_FIELD(TR),\n\tVMX_SEGMENT_FIELD(LDTR),\n};\n\nstatic u64 host_efer;\n\nstatic void ept_save_pdptrs(struct kvm_vcpu *vcpu);\n\n/*\n * Keep MSR_STAR at the end, as setup_msrs() will try to optimize it\n * away by decrementing the array size.\n */\nstatic const u32 vmx_msr_index[] = {\n#ifdef CONFIG_X86_64\n\tMSR_SYSCALL_MASK, MSR_LSTAR, MSR_CSTAR,\n#endif\n\tMSR_EFER, MSR_TSC_AUX, MSR_STAR,\n};\n\nDEFINE_STATIC_KEY_FALSE(enable_evmcs);\n\n#define current_evmcs ((struct hv_enlightened_vmcs *)this_cpu_read(current_vmcs))\n\n#define KVM_EVMCS_VERSION 1\n\n#if IS_ENABLED(CONFIG_HYPERV)\nstatic bool __read_mostly enlightened_vmcs = true;\nmodule_param(enlightened_vmcs, bool, 0444);\n\nstatic inline void evmcs_write64(unsigned long field, u64 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u64 *)((char *)current_evmcs + offset) = value;\n\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline void evmcs_write32(unsigned long field, u32 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u32 *)((char *)current_evmcs + offset) = value;\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline void evmcs_write16(unsigned long field, u16 value)\n{\n\tu16 clean_field;\n\tint offset = get_evmcs_offset(field, &clean_field);\n\n\tif (offset < 0)\n\t\treturn;\n\n\t*(u16 *)((char *)current_evmcs + offset) = value;\n\tcurrent_evmcs->hv_clean_fields &= ~clean_field;\n}\n\nstatic inline u64 evmcs_read64(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u64 *)((char *)current_evmcs + offset);\n}\n\nstatic inline u32 evmcs_read32(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u32 *)((char *)current_evmcs + offset);\n}\n\nstatic inline u16 evmcs_read16(unsigned long field)\n{\n\tint offset = get_evmcs_offset(field, NULL);\n\n\tif (offset < 0)\n\t\treturn 0;\n\n\treturn *(u16 *)((char *)current_evmcs + offset);\n}\n\nstatic inline void evmcs_touch_msr_bitmap(void)\n{\n\tif (unlikely(!current_evmcs))\n\t\treturn;\n\n\tif (current_evmcs->hv_enlightenments_control.msr_bitmap)\n\t\tcurrent_evmcs->hv_clean_fields &=\n\t\t\t~HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP;\n}\n\nstatic void evmcs_load(u64 phys_addr)\n{\n\tstruct hv_vp_assist_page *vp_ap =\n\t\thv_get_vp_assist_page(smp_processor_id());\n\n\tvp_ap->current_nested_vmcs = phys_addr;\n\tvp_ap->enlighten_vmentry = 1;\n}\n\nstatic void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf)\n{\n\t/*\n\t * Enlightened VMCSv1 doesn't support these:\n\t *\n\t *\tPOSTED_INTR_NV                  = 0x00000002,\n\t *\tGUEST_INTR_STATUS               = 0x00000810,\n\t *\tAPIC_ACCESS_ADDR\t\t= 0x00002014,\n\t *\tPOSTED_INTR_DESC_ADDR           = 0x00002016,\n\t *\tEOI_EXIT_BITMAP0                = 0x0000201c,\n\t *\tEOI_EXIT_BITMAP1                = 0x0000201e,\n\t *\tEOI_EXIT_BITMAP2                = 0x00002020,\n\t *\tEOI_EXIT_BITMAP3                = 0x00002022,\n\t */\n\tvmcs_conf->pin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &=\n\t\t~SECONDARY_EXEC_APIC_REGISTER_VIRT;\n\n\t/*\n\t *\tGUEST_PML_INDEX\t\t\t= 0x00000812,\n\t *\tPML_ADDRESS\t\t\t= 0x0000200e,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\t/*\tVM_FUNCTION_CONTROL             = 0x00002018, */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_ENABLE_VMFUNC;\n\n\t/*\n\t *\tEPTP_LIST_ADDRESS               = 0x00002024,\n\t *\tVMREAD_BITMAP                   = 0x00002026,\n\t *\tVMWRITE_BITMAP                  = 0x00002028,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\t/*\n\t *\tTSC_MULTIPLIER                  = 0x00002032,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_TSC_SCALING;\n\n\t/*\n\t *\tPLE_GAP                         = 0x00004020,\n\t *\tPLE_WINDOW                      = 0x00004022,\n\t */\n\tvmcs_conf->cpu_based_2nd_exec_ctrl &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\n\t/*\n\t *\tVMX_PREEMPTION_TIMER_VALUE      = 0x0000482E,\n\t */\n\tvmcs_conf->pin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/*\n\t *      GUEST_IA32_PERF_GLOBAL_CTRL     = 0x00002808,\n\t *      HOST_IA32_PERF_GLOBAL_CTRL      = 0x00002c04,\n\t */\n\tvmcs_conf->vmexit_ctrl &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;\n\tvmcs_conf->vmentry_ctrl &= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;\n\n\t/*\n\t * Currently unsupported in KVM:\n\t *\tGUEST_IA32_RTIT_CTL\t\t= 0x00002814,\n\t */\n}\n#else /* !IS_ENABLED(CONFIG_HYPERV) */\nstatic inline void evmcs_write64(unsigned long field, u64 value) {}\nstatic inline void evmcs_write32(unsigned long field, u32 value) {}\nstatic inline void evmcs_write16(unsigned long field, u16 value) {}\nstatic inline u64 evmcs_read64(unsigned long field) { return 0; }\nstatic inline u32 evmcs_read32(unsigned long field) { return 0; }\nstatic inline u16 evmcs_read16(unsigned long field) { return 0; }\nstatic inline void evmcs_load(u64 phys_addr) {}\nstatic inline void evmcs_sanitize_exec_ctrls(struct vmcs_config *vmcs_conf) {}\nstatic inline void evmcs_touch_msr_bitmap(void) {}\n#endif /* IS_ENABLED(CONFIG_HYPERV) */\n\nstatic inline bool is_exception_n(u32 intr_info, u8 vector)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |\n\t\t\t     INTR_INFO_VALID_MASK)) ==\n\t\t(INTR_TYPE_HARD_EXCEPTION | vector | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool is_debug(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, DB_VECTOR);\n}\n\nstatic inline bool is_breakpoint(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, BP_VECTOR);\n}\n\nstatic inline bool is_page_fault(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, PF_VECTOR);\n}\n\nstatic inline bool is_no_device(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, NM_VECTOR);\n}\n\nstatic inline bool is_invalid_opcode(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, UD_VECTOR);\n}\n\nstatic inline bool is_gp_fault(u32 intr_info)\n{\n\treturn is_exception_n(intr_info, GP_VECTOR);\n}\n\nstatic inline bool is_external_interrupt(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_EXT_INTR | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool is_machine_check(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VECTOR_MASK |\n\t\t\t     INTR_INFO_VALID_MASK)) ==\n\t\t(INTR_TYPE_HARD_EXCEPTION | MC_VECTOR | INTR_INFO_VALID_MASK);\n}\n\n/* Undocumented: icebp/int1 */\nstatic inline bool is_icebp(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_PRIV_SW_EXCEPTION | INTR_INFO_VALID_MASK);\n}\n\nstatic inline bool cpu_has_vmx_msr_bitmap(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS;\n}\n\nstatic inline bool cpu_has_vmx_tpr_shadow(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW;\n}\n\nstatic inline bool cpu_need_tpr_shadow(struct kvm_vcpu *vcpu)\n{\n\treturn cpu_has_vmx_tpr_shadow() && lapic_in_kernel(vcpu);\n}\n\nstatic inline bool cpu_has_secondary_exec_ctrls(void)\n{\n\treturn vmcs_config.cpu_based_exec_ctrl &\n\t\tCPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n}\n\nstatic inline bool cpu_has_vmx_virtualize_apic_accesses(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n}\n\nstatic inline bool cpu_has_vmx_virtualize_x2apic_mode(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n}\n\nstatic inline bool cpu_has_vmx_apic_register_virt(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT;\n}\n\nstatic inline bool cpu_has_vmx_virtual_intr_delivery(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY;\n}\n\n/*\n * Comment's format: document - errata name - stepping - processor name.\n * Refer from\n * https://www.virtualbox.org/svn/vbox/trunk/src/VBox/VMM/VMMR0/HMR0.cpp\n */\nstatic u32 vmx_preemption_cpu_tfms[] = {\n/* 323344.pdf - BA86   - D0 - Xeon 7500 Series */\n0x000206E6,\n/* 323056.pdf - AAX65  - C2 - Xeon L3406 */\n/* 322814.pdf - AAT59  - C2 - i7-600, i5-500, i5-400 and i3-300 Mobile */\n/* 322911.pdf - AAU65  - C2 - i5-600, i3-500 Desktop and Pentium G6950 */\n0x00020652,\n/* 322911.pdf - AAU65  - K0 - i5-600, i3-500 Desktop and Pentium G6950 */\n0x00020655,\n/* 322373.pdf - AAO95  - B1 - Xeon 3400 Series */\n/* 322166.pdf - AAN92  - B1 - i7-800 and i5-700 Desktop */\n/*\n * 320767.pdf - AAP86  - B1 -\n * i7-900 Mobile Extreme, i7-800 and i7-700 Mobile\n */\n0x000106E5,\n/* 321333.pdf - AAM126 - C0 - Xeon 3500 */\n0x000106A0,\n/* 321333.pdf - AAM126 - C1 - Xeon 3500 */\n0x000106A1,\n/* 320836.pdf - AAJ124 - C0 - i7-900 Desktop Extreme and i7-900 Desktop */\n0x000106A4,\n /* 321333.pdf - AAM126 - D0 - Xeon 3500 */\n /* 321324.pdf - AAK139 - D0 - Xeon 5500 */\n /* 320836.pdf - AAJ124 - D0 - i7-900 Extreme and i7-900 Desktop */\n0x000106A5,\n};\n\nstatic inline bool cpu_has_broken_vmx_preemption_timer(void)\n{\n\tu32 eax = cpuid_eax(0x00000001), i;\n\n\t/* Clear the reserved bits */\n\teax &= ~(0x3U << 14 | 0xfU << 28);\n\tfor (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)\n\t\tif (eax == vmx_preemption_cpu_tfms[i])\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool cpu_has_vmx_preemption_timer(void)\n{\n\treturn vmcs_config.pin_based_exec_ctrl &\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n}\n\nstatic inline bool cpu_has_vmx_posted_intr(void)\n{\n\treturn IS_ENABLED(CONFIG_X86_LOCAL_APIC) &&\n\t\tvmcs_config.pin_based_exec_ctrl & PIN_BASED_POSTED_INTR;\n}\n\nstatic inline bool cpu_has_vmx_apicv(void)\n{\n\treturn cpu_has_vmx_apic_register_virt() &&\n\t\tcpu_has_vmx_virtual_intr_delivery() &&\n\t\tcpu_has_vmx_posted_intr();\n}\n\nstatic inline bool cpu_has_vmx_flexpriority(void)\n{\n\treturn cpu_has_vmx_tpr_shadow() &&\n\t\tcpu_has_vmx_virtualize_apic_accesses();\n}\n\nstatic inline bool cpu_has_vmx_ept_execute_only(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXECUTE_ONLY_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_2m_page(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_2MB_PAGE_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_1g_page(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_1GB_PAGE_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_4levels(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_4_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_mt_wb(void)\n{\n\treturn vmx_capability.ept & VMX_EPTP_WB_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_5levels(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_5_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept_ad_bits(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_AD_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invept_context(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invept_global(void)\n{\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_GLOBAL_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_individual_addr(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_single(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_SINGLE_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid_global(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT;\n}\n\nstatic inline bool cpu_has_vmx_invvpid(void)\n{\n\treturn vmx_capability.vpid & VMX_VPID_INVVPID_BIT;\n}\n\nstatic inline bool cpu_has_vmx_ept(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_EPT;\n}\n\nstatic inline bool cpu_has_vmx_unrestricted_guest(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST;\n}\n\nstatic inline bool cpu_has_vmx_ple(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING;\n}\n\nstatic inline bool cpu_has_vmx_basic_inout(void)\n{\n\treturn\t(((u64)vmcs_config.basic_cap << 32) & VMX_BASIC_INOUT);\n}\n\nstatic inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)\n{\n\treturn flexpriority_enabled && lapic_in_kernel(vcpu);\n}\n\nstatic inline bool cpu_has_vmx_vpid(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VPID;\n}\n\nstatic inline bool cpu_has_vmx_rdtscp(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDTSCP;\n}\n\nstatic inline bool cpu_has_vmx_invpcid(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_INVPCID;\n}\n\nstatic inline bool cpu_has_virtual_nmis(void)\n{\n\treturn vmcs_config.pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS;\n}\n\nstatic inline bool cpu_has_vmx_wbinvd_exit(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_WBINVD_EXITING;\n}\n\nstatic inline bool cpu_has_vmx_shadow_vmcs(void)\n{\n\tu64 vmx_msr;\n\trdmsrl(MSR_IA32_VMX_MISC, vmx_msr);\n\t/* check if the cpu supports writing r/o exit information fields */\n\tif (!(vmx_msr & MSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS))\n\t\treturn false;\n\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_SHADOW_VMCS;\n}\n\nstatic inline bool cpu_has_vmx_pml(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_ENABLE_PML;\n}\n\nstatic inline bool cpu_has_vmx_tsc_scaling(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_TSC_SCALING;\n}\n\nstatic inline bool cpu_has_vmx_vmfunc(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n}\n\nstatic bool vmx_umip_emulated(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_DESC;\n}\n\nstatic inline bool report_flexpriority(void)\n{\n\treturn flexpriority_enabled;\n}\n\nstatic inline unsigned nested_cpu_vmx_misc_cr3_count(struct kvm_vcpu *vcpu)\n{\n\treturn vmx_misc_cr3_count(to_vmx(vcpu)->nested.msrs.misc_low);\n}\n\n/*\n * Do the virtual VMX capability MSRs specify that L1 can use VMWRITE\n * to modify any valid field of the VMCS, or are the VM-exit\n * information fields read-only?\n */\nstatic inline bool nested_cpu_has_vmwrite_any_field(struct kvm_vcpu *vcpu)\n{\n\treturn to_vmx(vcpu)->nested.msrs.misc_low &\n\t\tMSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS;\n}\n\nstatic inline bool nested_cpu_has(struct vmcs12 *vmcs12, u32 bit)\n{\n\treturn vmcs12->cpu_based_vm_exec_control & bit;\n}\n\nstatic inline bool nested_cpu_has2(struct vmcs12 *vmcs12, u32 bit)\n{\n\treturn (vmcs12->cpu_based_vm_exec_control &\n\t\t\tCPU_BASED_ACTIVATE_SECONDARY_CONTROLS) &&\n\t\t(vmcs12->secondary_vm_exec_control & bit);\n}\n\nstatic inline bool nested_cpu_has_preemption_timer(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control &\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n}\n\nstatic inline bool nested_cpu_has_nmi_exiting(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_NMI_EXITING;\n}\n\nstatic inline bool nested_cpu_has_virtual_nmis(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_VIRTUAL_NMIS;\n}\n\nstatic inline int nested_cpu_has_ept(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);\n}\n\nstatic inline bool nested_cpu_has_xsaves(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n}\n\nstatic inline bool nested_cpu_has_pml(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML);\n}\n\nstatic inline bool nested_cpu_has_virt_x2apic_mode(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);\n}\n\nstatic inline bool nested_cpu_has_vpid(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VPID);\n}\n\nstatic inline bool nested_cpu_has_apic_reg_virt(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_APIC_REGISTER_VIRT);\n}\n\nstatic inline bool nested_cpu_has_vid(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n}\n\nstatic inline bool nested_cpu_has_posted_intr(struct vmcs12 *vmcs12)\n{\n\treturn vmcs12->pin_based_vm_exec_control & PIN_BASED_POSTED_INTR;\n}\n\nstatic inline bool nested_cpu_has_vmfunc(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VMFUNC);\n}\n\nstatic inline bool nested_cpu_has_eptp_switching(struct vmcs12 *vmcs12)\n{\n\treturn nested_cpu_has_vmfunc(vmcs12) &&\n\t\t(vmcs12->vm_function_control &\n\t\t VMX_VMFUNC_EPTP_SWITCHING);\n}\n\nstatic inline bool is_nmi(u32 intr_info)\n{\n\treturn (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))\n\t\t== (INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK);\n}\n\nstatic void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,\n\t\t\t      u32 exit_intr_info,\n\t\t\t      unsigned long exit_qualification);\nstatic void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification);\n\nstatic int __find_msr_index(struct vcpu_vmx *vmx, u32 msr)\n{\n\tint i;\n\n\tfor (i = 0; i < vmx->nmsrs; ++i)\n\t\tif (vmx_msr_index[vmx->guest_msrs[i].index] == msr)\n\t\t\treturn i;\n\treturn -1;\n}\n\nstatic inline void __invvpid(int ext, u16 vpid, gva_t gva)\n{\n    struct {\n\tu64 vpid : 16;\n\tu64 rsvd : 48;\n\tu64 gva;\n    } operand = { vpid, 0, gva };\n\n    asm volatile (__ex(ASM_VMX_INVVPID)\n\t\t  /* CF==1 or ZF==1 --> rc = -1 */\n\t\t  \"; ja 1f ; ud2 ; 1:\"\n\t\t  : : \"a\"(&operand), \"c\"(ext) : \"cc\", \"memory\");\n}\n\nstatic inline void __invept(int ext, u64 eptp, gpa_t gpa)\n{\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand = {eptp, gpa};\n\n\tasm volatile (__ex(ASM_VMX_INVEPT)\n\t\t\t/* CF==1 or ZF==1 --> rc = -1 */\n\t\t\t\"; ja 1f ; ud2 ; 1:\\n\"\n\t\t\t: : \"a\" (&operand), \"c\" (ext) : \"cc\", \"memory\");\n}\n\nstatic struct shared_msr_entry *find_msr_entry(struct vcpu_vmx *vmx, u32 msr)\n{\n\tint i;\n\n\ti = __find_msr_index(vmx, msr);\n\tif (i >= 0)\n\t\treturn &vmx->guest_msrs[i];\n\treturn NULL;\n}\n\nstatic void vmcs_clear(struct vmcs *vmcs)\n{\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMCLEAR_RAX) \"; setna %0\"\n\t\t      : \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t      : \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmclear fail: %p/%llx\\n\",\n\t\t       vmcs, phys_addr);\n}\n\nstatic inline void loaded_vmcs_init(struct loaded_vmcs *loaded_vmcs)\n{\n\tvmcs_clear(loaded_vmcs->vmcs);\n\tif (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)\n\t\tvmcs_clear(loaded_vmcs->shadow_vmcs);\n\tloaded_vmcs->cpu = -1;\n\tloaded_vmcs->launched = 0;\n}\n\nstatic void vmcs_load(struct vmcs *vmcs)\n{\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_load(phys_addr);\n\n\tasm volatile (__ex(ASM_VMX_VMPTRLD_RAX) \"; setna %0\"\n\t\t\t: \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t\t: \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmptrld %p/%llx failed\\n\",\n\t\t       vmcs, phys_addr);\n}\n\n#ifdef CONFIG_KEXEC_CORE\n/*\n * This bitmap is used to indicate whether the vmclear\n * operation is enabled on all cpus. All disabled by\n * default.\n */\nstatic cpumask_t crash_vmclear_enabled_bitmap = CPU_MASK_NONE;\n\nstatic inline void crash_enable_local_vmclear(int cpu)\n{\n\tcpumask_set_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic inline void crash_disable_local_vmclear(int cpu)\n{\n\tcpumask_clear_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic inline int crash_local_vmclear_enabled(int cpu)\n{\n\treturn cpumask_test_cpu(cpu, &crash_vmclear_enabled_bitmap);\n}\n\nstatic void crash_vmclear_local_loaded_vmcss(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v;\n\n\tif (!crash_local_vmclear_enabled(cpu))\n\t\treturn;\n\n\tlist_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t    loaded_vmcss_on_cpu_link)\n\t\tvmcs_clear(v->vmcs);\n}\n#else\nstatic inline void crash_enable_local_vmclear(int cpu) { }\nstatic inline void crash_disable_local_vmclear(int cpu) { }\n#endif /* CONFIG_KEXEC_CORE */\n\nstatic void __loaded_vmcs_clear(void *arg)\n{\n\tstruct loaded_vmcs *loaded_vmcs = arg;\n\tint cpu = raw_smp_processor_id();\n\n\tif (loaded_vmcs->cpu != cpu)\n\t\treturn; /* vcpu migration can race with cpu offline */\n\tif (per_cpu(current_vmcs, cpu) == loaded_vmcs->vmcs)\n\t\tper_cpu(current_vmcs, cpu) = NULL;\n\tcrash_disable_local_vmclear(cpu);\n\tlist_del(&loaded_vmcs->loaded_vmcss_on_cpu_link);\n\n\t/*\n\t * we should ensure updating loaded_vmcs->loaded_vmcss_on_cpu_link\n\t * is before setting loaded_vmcs->vcpu to -1 which is done in\n\t * loaded_vmcs_init. Otherwise, other cpu can see vcpu = -1 fist\n\t * then adds the vmcs into percpu list before it is deleted.\n\t */\n\tsmp_wmb();\n\n\tloaded_vmcs_init(loaded_vmcs);\n\tcrash_enable_local_vmclear(cpu);\n}\n\nstatic void loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)\n{\n\tint cpu = loaded_vmcs->cpu;\n\n\tif (cpu != -1)\n\t\tsmp_call_function_single(cpu,\n\t\t\t __loaded_vmcs_clear, loaded_vmcs, 1);\n}\n\nstatic inline void vpid_sync_vcpu_single(int vpid)\n{\n\tif (vpid == 0)\n\t\treturn;\n\n\tif (cpu_has_vmx_invvpid_single())\n\t\t__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);\n}\n\nstatic inline void vpid_sync_vcpu_global(void)\n{\n\tif (cpu_has_vmx_invvpid_global())\n\t\t__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);\n}\n\nstatic inline void vpid_sync_context(int vpid)\n{\n\tif (cpu_has_vmx_invvpid_single())\n\t\tvpid_sync_vcpu_single(vpid);\n\telse\n\t\tvpid_sync_vcpu_global();\n}\n\nstatic inline void ept_sync_global(void)\n{\n\t__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);\n}\n\nstatic inline void ept_sync_context(u64 eptp)\n{\n\tif (cpu_has_vmx_invept_context())\n\t\t__invept(VMX_EPT_EXTENT_CONTEXT, eptp, 0);\n\telse\n\t\tept_sync_global();\n}\n\nstatic __always_inline void vmcs_check16(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,\n\t\t\t \"16-bit accessor invalid for 64-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"16-bit accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"16-bit accessor invalid for 32-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"16-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_check32(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"32-bit accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"32-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_check64(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"64-bit accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"64-bit accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"64-bit accessor invalid for 32-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,\n\t\t\t \"64-bit accessor invalid for natural width field\");\n}\n\nstatic __always_inline void vmcs_checkl(unsigned long field)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,\n\t\t\t \"Natural width accessor invalid for 16-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,\n\t\t\t \"Natural width accessor invalid for 64-bit field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,\n\t\t\t \"Natural width accessor invalid for 64-bit high field\");\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,\n\t\t\t \"Natural width accessor invalid for 32-bit field\");\n}\n\nstatic __always_inline unsigned long __vmcs_readl(unsigned long field)\n{\n\tunsigned long value;\n\n\tasm volatile (__ex_clear(ASM_VMX_VMREAD_RDX_RAX, \"%0\")\n\t\t      : \"=a\"(value) : \"d\"(field) : \"cc\");\n\treturn value;\n}\n\nstatic __always_inline u16 vmcs_read16(unsigned long field)\n{\n\tvmcs_check16(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read16(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic __always_inline u32 vmcs_read32(unsigned long field)\n{\n\tvmcs_check32(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read32(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic __always_inline u64 vmcs_read64(unsigned long field)\n{\n\tvmcs_check64(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read64(field);\n#ifdef CONFIG_X86_64\n\treturn __vmcs_readl(field);\n#else\n\treturn __vmcs_readl(field) | ((u64)__vmcs_readl(field+1) << 32);\n#endif\n}\n\nstatic __always_inline unsigned long vmcs_readl(unsigned long field)\n{\n\tvmcs_checkl(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_read64(field);\n\treturn __vmcs_readl(field);\n}\n\nstatic noinline void vmwrite_error(unsigned long field, unsigned long value)\n{\n\tprintk(KERN_ERR \"vmwrite error: reg %lx value %lx (err %d)\\n\",\n\t       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));\n\tdump_stack();\n}\n\nstatic __always_inline void __vmcs_writel(unsigned long field, unsigned long value)\n{\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMWRITE_RAX_RDX) \"; setna %0\"\n\t\t       : \"=q\"(error) : \"a\"(value), \"d\"(field) : \"cc\");\n\tif (unlikely(error))\n\t\tvmwrite_error(field, value);\n}\n\nstatic __always_inline void vmcs_write16(unsigned long field, u16 value)\n{\n\tvmcs_check16(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write16(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_write32(unsigned long field, u32 value)\n{\n\tvmcs_check32(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_write64(unsigned long field, u64 value)\n{\n\tvmcs_check64(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write64(field, value);\n\n\t__vmcs_writel(field, value);\n#ifndef CONFIG_X86_64\n\tasm volatile (\"\");\n\t__vmcs_writel(field+1, value >> 32);\n#endif\n}\n\nstatic __always_inline void vmcs_writel(unsigned long field, unsigned long value)\n{\n\tvmcs_checkl(field);\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write64(field, value);\n\n\t__vmcs_writel(field, value);\n}\n\nstatic __always_inline void vmcs_clear_bits(unsigned long field, u32 mask)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,\n\t\t\t \"vmcs_clear_bits does not support 64-bit fields\");\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, evmcs_read32(field) & ~mask);\n\n\t__vmcs_writel(field, __vmcs_readl(field) & ~mask);\n}\n\nstatic __always_inline void vmcs_set_bits(unsigned long field, u32 mask)\n{\n        BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x2000,\n\t\t\t \"vmcs_set_bits does not support 64-bit fields\");\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\treturn evmcs_write32(field, evmcs_read32(field) | mask);\n\n\t__vmcs_writel(field, __vmcs_readl(field) | mask);\n}\n\nstatic inline void vm_entry_controls_reset_shadow(struct vcpu_vmx *vmx)\n{\n\tvmx->vm_entry_controls_shadow = vmcs_read32(VM_ENTRY_CONTROLS);\n}\n\nstatic inline void vm_entry_controls_init(struct vcpu_vmx *vmx, u32 val)\n{\n\tvmcs_write32(VM_ENTRY_CONTROLS, val);\n\tvmx->vm_entry_controls_shadow = val;\n}\n\nstatic inline void vm_entry_controls_set(struct vcpu_vmx *vmx, u32 val)\n{\n\tif (vmx->vm_entry_controls_shadow != val)\n\t\tvm_entry_controls_init(vmx, val);\n}\n\nstatic inline u32 vm_entry_controls_get(struct vcpu_vmx *vmx)\n{\n\treturn vmx->vm_entry_controls_shadow;\n}\n\n\nstatic inline void vm_entry_controls_setbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_entry_controls_set(vmx, vm_entry_controls_get(vmx) | val);\n}\n\nstatic inline void vm_entry_controls_clearbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_entry_controls_set(vmx, vm_entry_controls_get(vmx) & ~val);\n}\n\nstatic inline void vm_exit_controls_reset_shadow(struct vcpu_vmx *vmx)\n{\n\tvmx->vm_exit_controls_shadow = vmcs_read32(VM_EXIT_CONTROLS);\n}\n\nstatic inline void vm_exit_controls_init(struct vcpu_vmx *vmx, u32 val)\n{\n\tvmcs_write32(VM_EXIT_CONTROLS, val);\n\tvmx->vm_exit_controls_shadow = val;\n}\n\nstatic inline void vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val)\n{\n\tif (vmx->vm_exit_controls_shadow != val)\n\t\tvm_exit_controls_init(vmx, val);\n}\n\nstatic inline u32 vm_exit_controls_get(struct vcpu_vmx *vmx)\n{\n\treturn vmx->vm_exit_controls_shadow;\n}\n\n\nstatic inline void vm_exit_controls_setbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_exit_controls_set(vmx, vm_exit_controls_get(vmx) | val);\n}\n\nstatic inline void vm_exit_controls_clearbit(struct vcpu_vmx *vmx, u32 val)\n{\n\tvm_exit_controls_set(vmx, vm_exit_controls_get(vmx) & ~val);\n}\n\nstatic void vmx_segment_cache_clear(struct vcpu_vmx *vmx)\n{\n\tvmx->segment_cache.bitmask = 0;\n}\n\nstatic bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,\n\t\t\t\t       unsigned field)\n{\n\tbool ret;\n\tu32 mask = 1 << (seg * SEG_FIELD_NR + field);\n\n\tif (!(vmx->vcpu.arch.regs_avail & (1 << VCPU_EXREG_SEGMENTS))) {\n\t\tvmx->vcpu.arch.regs_avail |= (1 << VCPU_EXREG_SEGMENTS);\n\t\tvmx->segment_cache.bitmask = 0;\n\t}\n\tret = vmx->segment_cache.bitmask & mask;\n\tvmx->segment_cache.bitmask |= mask;\n\treturn ret;\n}\n\nstatic u16 vmx_read_guest_seg_selector(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu16 *p = &vmx->segment_cache.seg[seg].selector;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_SEL))\n\t\t*p = vmcs_read16(kvm_vmx_segment_fields[seg].selector);\n\treturn *p;\n}\n\nstatic ulong vmx_read_guest_seg_base(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tulong *p = &vmx->segment_cache.seg[seg].base;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_BASE))\n\t\t*p = vmcs_readl(kvm_vmx_segment_fields[seg].base);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_limit(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].limit;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_LIMIT))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].limit);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].ar;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_AR))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].ar_bytes);\n\treturn *p;\n}\n\nstatic void update_exception_bitmap(struct kvm_vcpu *vcpu)\n{\n\tu32 eb;\n\n\teb = (1u << PF_VECTOR) | (1u << UD_VECTOR) | (1u << MC_VECTOR) |\n\t     (1u << DB_VECTOR) | (1u << AC_VECTOR);\n\t/*\n\t * Guest access to VMware backdoor ports could legitimately\n\t * trigger #GP because of TSS I/O permission bitmap.\n\t * We intercept those #GP and allow access to them anyway\n\t * as VMware does.\n\t */\n\tif (enable_vmware_backdoor)\n\t\teb |= (1u << GP_VECTOR);\n\tif ((vcpu->guest_debug &\n\t     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==\n\t    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))\n\t\teb |= 1u << BP_VECTOR;\n\tif (to_vmx(vcpu)->rmode.vm86_active)\n\t\teb = ~0;\n\tif (enable_ept)\n\t\teb &= ~(1u << PF_VECTOR); /* bypass_guest_pf = 0 */\n\n\t/* When we are running a nested L2 guest and L1 specified for it a\n\t * certain exception bitmap, we must trap the same exceptions and pass\n\t * them to L1. When running L2, we will only handle the exceptions\n\t * specified above if L1 did not want them.\n\t */\n\tif (is_guest_mode(vcpu))\n\t\teb |= get_vmcs12(vcpu)->exception_bitmap;\n\n\tvmcs_write32(EXCEPTION_BITMAP, eb);\n}\n\n/*\n * Check if MSR is intercepted for currently loaded MSR bitmap.\n */\nstatic bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tunsigned long *msr_bitmap;\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn true;\n\n\tmsr_bitmap = to_vmx(vcpu)->loaded_vmcs->msr_bitmap;\n\n\tif (msr <= 0x1fff) {\n\t\treturn !!test_bit(msr, msr_bitmap + 0x800 / f);\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\treturn !!test_bit(msr, msr_bitmap + 0xc00 / f);\n\t}\n\n\treturn true;\n}\n\n/*\n * Check if MSR is intercepted for L01 MSR bitmap.\n */\nstatic bool msr_write_intercepted_l01(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tunsigned long *msr_bitmap;\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn true;\n\n\tmsr_bitmap = to_vmx(vcpu)->vmcs01.msr_bitmap;\n\n\tif (msr <= 0x1fff) {\n\t\treturn !!test_bit(msr, msr_bitmap + 0x800 / f);\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\treturn !!test_bit(msr, msr_bitmap + 0xc00 / f);\n\t}\n\n\treturn true;\n}\n\nstatic void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit)\n{\n\tvm_entry_controls_clearbit(vmx, entry);\n\tvm_exit_controls_clearbit(vmx, exit);\n}\n\nstatic void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)\n{\n\tunsigned i;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\t}\n\n\tfor (i = 0; i < m->nr; ++i)\n\t\tif (m->guest[i].index == msr)\n\t\t\tbreak;\n\n\tif (i == m->nr)\n\t\treturn;\n\t--m->nr;\n\tm->guest[i] = m->guest[m->nr];\n\tm->host[i] = m->host[m->nr];\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);\n}\n\nstatic void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit,\n\t\tunsigned long guest_val_vmcs, unsigned long host_val_vmcs,\n\t\tu64 guest_val, u64 host_val)\n{\n\tvmcs_write64(guest_val_vmcs, guest_val);\n\tvmcs_write64(host_val_vmcs, host_val);\n\tvm_entry_controls_setbit(vmx, entry);\n\tvm_exit_controls_setbit(vmx, exit);\n}\n\nstatic void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,\n\t\t\t\t  u64 guest_val, u64 host_val)\n{\n\tunsigned i;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER,\n\t\t\t\t\tGUEST_IA32_EFER,\n\t\t\t\t\tHOST_IA32_EFER,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tGUEST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tHOST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\t/* PEBS needs a quiescent period after being disabled (to write\n\t\t * a record).  Disabling PEBS through VMX MSR swapping doesn't\n\t\t * provide that period, so a CPU could write host's record into\n\t\t * guest's memory.\n\t\t */\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n\t}\n\n\tfor (i = 0; i < m->nr; ++i)\n\t\tif (m->guest[i].index == msr)\n\t\t\tbreak;\n\n\tif (i == NR_AUTOLOAD_MSRS) {\n\t\tprintk_once(KERN_WARNING \"Not enough msr switch entries. \"\n\t\t\t\t\"Can't add msr %x\\n\", msr);\n\t\treturn;\n\t} else if (i == m->nr) {\n\t\t++m->nr;\n\t\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->nr);\n\t\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->nr);\n\t}\n\n\tm->guest[i].index = msr;\n\tm->guest[i].value = guest_val;\n\tm->host[i].index = msr;\n\tm->host[i].value = host_val;\n}\n\nstatic bool update_transition_efer(struct vcpu_vmx *vmx, int efer_offset)\n{\n\tu64 guest_efer = vmx->vcpu.arch.efer;\n\tu64 ignore_bits = 0;\n\n\tif (!enable_ept) {\n\t\t/*\n\t\t * NX is needed to handle CR0.WP=1, CR4.SMEP=1.  Testing\n\t\t * host CPUID is more efficient than testing guest CPUID\n\t\t * or CR4.  Host SMEP is anyway a requirement for guest SMEP.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_SMEP))\n\t\t\tguest_efer |= EFER_NX;\n\t\telse if (!(guest_efer & EFER_NX))\n\t\t\tignore_bits |= EFER_NX;\n\t}\n\n\t/*\n\t * LMA and LME handled by hardware; SCE meaningless outside long mode.\n\t */\n\tignore_bits |= EFER_SCE;\n#ifdef CONFIG_X86_64\n\tignore_bits |= EFER_LMA | EFER_LME;\n\t/* SCE is meaningful only in long mode on Intel */\n\tif (guest_efer & EFER_LMA)\n\t\tignore_bits &= ~(u64)EFER_SCE;\n#endif\n\n\tclear_atomic_switch_msr(vmx, MSR_EFER);\n\n\t/*\n\t * On EPT, we can't emulate NX, so we must switch EFER atomically.\n\t * On CPUs that support \"load IA32_EFER\", always switch EFER\n\t * atomically, since it's faster than switching it manually.\n\t */\n\tif (cpu_has_load_ia32_efer ||\n\t    (enable_ept && ((vmx->vcpu.arch.efer ^ host_efer) & EFER_NX))) {\n\t\tif (!(guest_efer & EFER_LMA))\n\t\t\tguest_efer &= ~EFER_LME;\n\t\tif (guest_efer != host_efer)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_EFER,\n\t\t\t\t\t      guest_efer, host_efer);\n\t\treturn false;\n\t} else {\n\t\tguest_efer &= ~ignore_bits;\n\t\tguest_efer |= host_efer & ignore_bits;\n\n\t\tvmx->guest_msrs[efer_offset].data = guest_efer;\n\t\tvmx->guest_msrs[efer_offset].mask = ~ignore_bits;\n\n\t\treturn true;\n\t}\n}\n\n#ifdef CONFIG_X86_32\n/*\n * On 32-bit kernels, VM exits still load the FS and GS bases from the\n * VMCS rather than the segment table.  KVM uses this helper to figure\n * out the current bases to poke them into the VMCS before entry.\n */\nstatic unsigned long segment_base(u16 selector)\n{\n\tstruct desc_struct *table;\n\tunsigned long v;\n\n\tif (!(selector & ~SEGMENT_RPL_MASK))\n\t\treturn 0;\n\n\ttable = get_current_gdt_ro();\n\n\tif ((selector & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tu16 ldt_selector = kvm_read_ldt();\n\n\t\tif (!(ldt_selector & ~SEGMENT_RPL_MASK))\n\t\t\treturn 0;\n\n\t\ttable = (struct desc_struct *)segment_base(ldt_selector);\n\t}\n\tv = get_desc_base(&table[selector >> 3]);\n\treturn v;\n}\n#endif\n\nstatic void vmx_save_host_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n#ifdef CONFIG_X86_64\n\tint cpu = raw_smp_processor_id();\n#endif\n\tint i;\n\n\tif (vmx->host_state.loaded)\n\t\treturn;\n\n\tvmx->host_state.loaded = 1;\n\t/*\n\t * Set host fs and gs selectors.  Unfortunately, 22.2.3 does not\n\t * allow segment selectors with cpl > 0 or ti == 1.\n\t */\n\tvmx->host_state.ldt_sel = kvm_read_ldt();\n\tvmx->host_state.gs_ldt_reload_needed = vmx->host_state.ldt_sel;\n\n#ifdef CONFIG_X86_64\n\tsave_fsgs_for_kvm();\n\tvmx->host_state.fs_sel = current->thread.fsindex;\n\tvmx->host_state.gs_sel = current->thread.gsindex;\n#else\n\tsavesegment(fs, vmx->host_state.fs_sel);\n\tsavesegment(gs, vmx->host_state.gs_sel);\n#endif\n\tif (!(vmx->host_state.fs_sel & 7)) {\n\t\tvmcs_write16(HOST_FS_SELECTOR, vmx->host_state.fs_sel);\n\t\tvmx->host_state.fs_reload_needed = 0;\n\t} else {\n\t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n\t\tvmx->host_state.fs_reload_needed = 1;\n\t}\n\tif (!(vmx->host_state.gs_sel & 7))\n\t\tvmcs_write16(HOST_GS_SELECTOR, vmx->host_state.gs_sel);\n\telse {\n\t\tvmcs_write16(HOST_GS_SELECTOR, 0);\n\t\tvmx->host_state.gs_ldt_reload_needed = 1;\n\t}\n\n#ifdef CONFIG_X86_64\n\tsavesegment(ds, vmx->host_state.ds_sel);\n\tsavesegment(es, vmx->host_state.es_sel);\n\n\tvmcs_writel(HOST_FS_BASE, current->thread.fsbase);\n\tvmcs_writel(HOST_GS_BASE, cpu_kernelmode_gs_base(cpu));\n\n\tvmx->msr_host_kernel_gs_base = current->thread.gsbase;\n\tif (is_long_mode(&vmx->vcpu))\n\t\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#else\n\tvmcs_writel(HOST_FS_BASE, segment_base(vmx->host_state.fs_sel));\n\tvmcs_writel(HOST_GS_BASE, segment_base(vmx->host_state.gs_sel));\n#endif\n\tif (boot_cpu_has(X86_FEATURE_MPX))\n\t\trdmsrl(MSR_IA32_BNDCFGS, vmx->host_state.msr_host_bndcfgs);\n\tfor (i = 0; i < vmx->save_nmsrs; ++i)\n\t\tkvm_set_shared_msr(vmx->guest_msrs[i].index,\n\t\t\t\t   vmx->guest_msrs[i].data,\n\t\t\t\t   vmx->guest_msrs[i].mask);\n}\n\nstatic void __vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->host_state.loaded)\n\t\treturn;\n\n\t++vmx->vcpu.stat.host_state_reload;\n\tvmx->host_state.loaded = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu))\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#endif\n\tif (vmx->host_state.gs_ldt_reload_needed) {\n\t\tkvm_load_ldt(vmx->host_state.ldt_sel);\n#ifdef CONFIG_X86_64\n\t\tload_gs_index(vmx->host_state.gs_sel);\n#else\n\t\tloadsegment(gs, vmx->host_state.gs_sel);\n#endif\n\t}\n\tif (vmx->host_state.fs_reload_needed)\n\t\tloadsegment(fs, vmx->host_state.fs_sel);\n#ifdef CONFIG_X86_64\n\tif (unlikely(vmx->host_state.ds_sel | vmx->host_state.es_sel)) {\n\t\tloadsegment(ds, vmx->host_state.ds_sel);\n\t\tloadsegment(es, vmx->host_state.es_sel);\n\t}\n#endif\n\tinvalidate_tss_limit();\n#ifdef CONFIG_X86_64\n\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n#endif\n\tif (vmx->host_state.msr_host_bndcfgs)\n\t\twrmsrl(MSR_IA32_BNDCFGS, vmx->host_state.msr_host_bndcfgs);\n\tload_fixmap_gdt(raw_smp_processor_id());\n}\n\nstatic void vmx_load_host_state(struct vcpu_vmx *vmx)\n{\n\tpreempt_disable();\n\t__vmx_load_host_state(vmx);\n\tpreempt_enable();\n}\n\nstatic void vmx_vcpu_pi_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\tstruct pi_desc old, new;\n\tunsigned int dest;\n\n\t/*\n\t * In case of hot-plug or hot-unplug, we may have to undo\n\t * vmx_vcpu_pi_put even if there is no assigned device.  And we\n\t * always keep PI.NDST up to date for simplicity: it makes the\n\t * code easier, and CPU migration is not a fast path.\n\t */\n\tif (!pi_test_sn(pi_desc) && vcpu->cpu == cpu)\n\t\treturn;\n\n\t/*\n\t * First handle the simple case where no cmpxchg is necessary; just\n\t * allow posting non-urgent interrupts.\n\t *\n\t * If the 'nv' field is POSTED_INTR_WAKEUP_VECTOR, do not change\n\t * PI.NDST: pi_post_block will do it for us and the wakeup_handler\n\t * expects the VCPU to be on the blocked_vcpu_list that matches\n\t * PI.NDST.\n\t */\n\tif (pi_desc->nv == POSTED_INTR_WAKEUP_VECTOR ||\n\t    vcpu->cpu == cpu) {\n\t\tpi_clear_sn(pi_desc);\n\t\treturn;\n\t}\n\n\t/* The full case.  */\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\tdest = cpu_physical_id(cpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\tnew.sn = 0;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n}\n\nstatic void decache_tsc_multiplier(struct vcpu_vmx *vmx)\n{\n\tvmx->current_tsc_ratio = vmx->vcpu.arch.tsc_scaling_ratio;\n\tvmcs_write64(TSC_MULTIPLIER, vmx->current_tsc_ratio);\n}\n\n/*\n * Switches to specified vcpu, until a matching vcpu_put(), but assumes\n * vcpu mutex is already taken.\n */\nstatic void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool already_loaded = vmx->loaded_vmcs->cpu == cpu;\n\n\tif (!already_loaded) {\n\t\tloaded_vmcs_clear(vmx->loaded_vmcs);\n\t\tlocal_irq_disable();\n\t\tcrash_disable_local_vmclear(cpu);\n\n\t\t/*\n\t\t * Read loaded_vmcs->cpu should be before fetching\n\t\t * loaded_vmcs->loaded_vmcss_on_cpu_link.\n\t\t * See the comments in __loaded_vmcs_clear().\n\t\t */\n\t\tsmp_rmb();\n\n\t\tlist_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,\n\t\t\t &per_cpu(loaded_vmcss_on_cpu, cpu));\n\t\tcrash_enable_local_vmclear(cpu);\n\t\tlocal_irq_enable();\n\t}\n\n\tif (per_cpu(current_vmcs, cpu) != vmx->loaded_vmcs->vmcs) {\n\t\tper_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;\n\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (!already_loaded) {\n\t\tvoid *gdt = get_current_gdt_ro();\n\t\tunsigned long sysenter_esp;\n\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\n\t\t/*\n\t\t * Linux uses per-cpu TSS and GDT, so set these when switching\n\t\t * processors.  See 22.2.4.\n\t\t */\n\t\tvmcs_writel(HOST_TR_BASE,\n\t\t\t    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);\n\t\tvmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);   /* 22.2.4 */\n\n\t\t/*\n\t\t * VM exits change the host TR limit to 0x67 after a VM\n\t\t * exit.  This is okay, since 0x67 covers everything except\n\t\t * the IO bitmap and have have code to handle the IO bitmap\n\t\t * being lost after a VM exit.\n\t\t */\n\t\tBUILD_BUG_ON(IO_BITMAP_OFFSET - 1 != 0x67);\n\n\t\trdmsrl(MSR_IA32_SYSENTER_ESP, sysenter_esp);\n\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP, sysenter_esp); /* 22.2.3 */\n\n\t\tvmx->loaded_vmcs->cpu = cpu;\n\t}\n\n\t/* Setup TSC multiplier */\n\tif (kvm_has_tsc_control &&\n\t    vmx->current_tsc_ratio != vcpu->arch.tsc_scaling_ratio)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tvmx_vcpu_pi_load(vcpu, cpu);\n\tvmx->host_pkru = read_pkru();\n\tvmx->host_debugctlmsr = get_debugctlmsr();\n}\n\nstatic void vmx_vcpu_pi_put(struct kvm_vcpu *vcpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\t/* Set SN when the vCPU is preempted */\n\tif (vcpu->preempted)\n\t\tpi_set_sn(pi_desc);\n}\n\nstatic void vmx_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tvmx_vcpu_pi_put(vcpu);\n\n\t__vmx_load_host_state(to_vmx(vcpu));\n}\n\nstatic bool emulation_required(struct kvm_vcpu *vcpu)\n{\n\treturn emulate_invalid_guest_state && !guest_state_valid(vcpu);\n}\n\nstatic void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu);\n\n/*\n * Return the cr0 value that a nested guest would read. This is a combination\n * of the real cr0 used to run the guest (guest_cr0), and the bits shadowed by\n * its hypervisor (cr0_read_shadow).\n */\nstatic inline unsigned long nested_read_cr0(struct vmcs12 *fields)\n{\n\treturn (fields->guest_cr0 & ~fields->cr0_guest_host_mask) |\n\t\t(fields->cr0_read_shadow & fields->cr0_guest_host_mask);\n}\nstatic inline unsigned long nested_read_cr4(struct vmcs12 *fields)\n{\n\treturn (fields->guest_cr4 & ~fields->cr4_guest_host_mask) |\n\t\t(fields->cr4_read_shadow & fields->cr4_guest_host_mask);\n}\n\nstatic unsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rflags, save_rflags;\n\n\tif (!test_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail)) {\n\t\t__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);\n\t\trflags = vmcs_readl(GUEST_RFLAGS);\n\t\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\t\trflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t\tsave_rflags = to_vmx(vcpu)->rmode.save_rflags;\n\t\t\trflags |= save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t}\n\t\tto_vmx(vcpu)->rflags = rflags;\n\t}\n\treturn to_vmx(vcpu)->rflags;\n}\n\nstatic void vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tunsigned long old_rflags = vmx_get_rflags(vcpu);\n\n\t__set_bit(VCPU_EXREG_RFLAGS, (ulong *)&vcpu->arch.regs_avail);\n\tto_vmx(vcpu)->rflags = rflags;\n\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\tto_vmx(vcpu)->rmode.save_rflags = rflags;\n\t\trflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\t}\n\tvmcs_writel(GUEST_RFLAGS, rflags);\n\n\tif ((old_rflags ^ to_vmx(vcpu)->rflags) & X86_EFLAGS_VM)\n\t\tto_vmx(vcpu)->emulation_required = emulation_required(vcpu);\n}\n\nstatic u32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)\n{\n\tu32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tint ret = 0;\n\n\tif (interruptibility & GUEST_INTR_STATE_STI)\n\t\tret |= KVM_X86_SHADOW_INT_STI;\n\tif (interruptibility & GUEST_INTR_STATE_MOV_SS)\n\t\tret |= KVM_X86_SHADOW_INT_MOV_SS;\n\n\treturn ret;\n}\n\nstatic void vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\n{\n\tu32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tu32 interruptibility = interruptibility_old;\n\n\tinterruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);\n\n\tif (mask & KVM_X86_SHADOW_INT_MOV_SS)\n\t\tinterruptibility |= GUEST_INTR_STATE_MOV_SS;\n\telse if (mask & KVM_X86_SHADOW_INT_STI)\n\t\tinterruptibility |= GUEST_INTR_STATE_STI;\n\n\tif ((interruptibility != interruptibility_old))\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);\n}\n\nstatic void skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rip;\n\n\trip = kvm_rip_read(vcpu);\n\trip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tkvm_rip_write(vcpu, rip);\n\n\t/* skipping an emulated instruction also counts */\n\tvmx_set_interrupt_shadow(vcpu, 0);\n}\n\nstatic void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       unsigned long exit_qual)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned int nr = vcpu->arch.exception.nr;\n\tu32 intr_info = nr | INTR_INFO_VALID_MASK;\n\n\tif (vcpu->arch.exception.has_error_code) {\n\t\tvmcs12->vm_exit_intr_error_code = vcpu->arch.exception.error_code;\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (kvm_exception_is_soft(nr))\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\telse\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tif (!(vmcs12->idt_vectoring_info_field & VECTORING_INFO_VALID_MASK) &&\n\t    vmx_get_nmi_mask(vcpu))\n\t\tintr_info |= INTR_INFO_UNBLOCK_NMI;\n\n\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);\n}\n\n/*\n * KVM wants to inject page-faults which it got to the guest. This function\n * checks whether in a nested guest, we need to inject them to L1 or L2.\n */\nstatic int nested_vmx_check_exception(struct kvm_vcpu *vcpu, unsigned long *exit_qual)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned int nr = vcpu->arch.exception.nr;\n\n\tif (nr == PF_VECTOR) {\n\t\tif (vcpu->arch.exception.nested_apf) {\n\t\t\t*exit_qual = vcpu->arch.apf.nested_apf_token;\n\t\t\treturn 1;\n\t\t}\n\t\t/*\n\t\t * FIXME: we must not write CR2 when L1 intercepts an L2 #PF exception.\n\t\t * The fix is to add the ancillary datum (CR2 or DR6) to structs\n\t\t * kvm_queued_exception and kvm_vcpu_events, so that CR2 and DR6\n\t\t * can be written only when inject_pending_event runs.  This should be\n\t\t * conditional on a new capability---if the capability is disabled,\n\t\t * kvm_multiple_exception would write the ancillary information to\n\t\t * CR2 or DR6, for backwards ABI-compatibility.\n\t\t */\n\t\tif (nested_vmx_is_page_fault_vmexit(vmcs12,\n\t\t\t\t\t\t    vcpu->arch.exception.error_code)) {\n\t\t\t*exit_qual = vcpu->arch.cr2;\n\t\t\treturn 1;\n\t\t}\n\t} else {\n\t\tif (vmcs12->exception_bitmap & (1u << nr)) {\n\t\t\tif (nr == DB_VECTOR)\n\t\t\t\t*exit_qual = vcpu->arch.dr6;\n\t\t\telse\n\t\t\t\t*exit_qual = 0;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_clear_hlt(struct kvm_vcpu *vcpu)\n{\n\t/*\n\t * Ensure that we clear the HLT state in the VMCS.  We don't need to\n\t * explicitly skip the instruction because if the HLT state is set,\n\t * then the instruction is already executing and RIP has already been\n\t * advanced.\n\t */\n\tif (kvm_hlt_in_guest(vcpu->kvm) &&\n\t\t\tvmcs_read32(GUEST_ACTIVITY_STATE) == GUEST_ACTIVITY_HLT)\n\t\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n}\n\nstatic void vmx_queue_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned nr = vcpu->arch.exception.nr;\n\tbool has_error_code = vcpu->arch.exception.has_error_code;\n\tu32 error_code = vcpu->arch.exception.error_code;\n\tu32 intr_info = nr | INTR_INFO_VALID_MASK;\n\n\tif (has_error_code) {\n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (kvm_exception_is_soft(nr))\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tif (kvm_inject_realmode_interrupt(vcpu, nr, inc_eip) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(vmx->emulation_required);\n\n\tif (kvm_exception_is_soft(nr)) {\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\t} else\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic bool vmx_rdtscp_supported(void)\n{\n\treturn cpu_has_vmx_rdtscp();\n}\n\nstatic bool vmx_invpcid_supported(void)\n{\n\treturn cpu_has_vmx_invpcid() && enable_ept;\n}\n\n/*\n * Swap MSR entry in host/guest MSR entry array.\n */\nstatic void move_msr_up(struct vcpu_vmx *vmx, int from, int to)\n{\n\tstruct shared_msr_entry tmp;\n\n\ttmp = vmx->guest_msrs[to];\n\tvmx->guest_msrs[to] = vmx->guest_msrs[from];\n\tvmx->guest_msrs[from] = tmp;\n}\n\n/*\n * Set up the vmcs to automatically save and restore system\n * msrs.  Don't touch the 64-bit msrs if the guest is in legacy\n * mode, as fiddling with msrs is very expensive.\n */\nstatic void setup_msrs(struct vcpu_vmx *vmx)\n{\n\tint save_nmsrs, index;\n\n\tsave_nmsrs = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\tindex = __find_msr_index(vmx, MSR_SYSCALL_MASK);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_LSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_CSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_TSC_AUX);\n\t\tif (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\t/*\n\t\t * MSR_STAR is only needed on long mode guests, and only\n\t\t * if efer.sce is enabled.\n\t\t */\n\t\tindex = __find_msr_index(vmx, MSR_STAR);\n\t\tif ((index >= 0) && (vmx->vcpu.arch.efer & EFER_SCE))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t}\n#endif\n\tindex = __find_msr_index(vmx, MSR_EFER);\n\tif (index >= 0 && update_transition_efer(vmx, index))\n\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\n\tvmx->save_nmsrs = save_nmsrs;\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(&vmx->vcpu);\n}\n\nstatic u64 vmx_read_l1_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t    (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING))\n\t\treturn vcpu->arch.tsc_offset - vmcs12->tsc_offset;\n\n\treturn vcpu->arch.tsc_offset;\n}\n\n/*\n * writes 'offset' into guest's timestamp counter offset register\n */\nstatic void vmx_write_tsc_offset(struct kvm_vcpu *vcpu, u64 offset)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * We're here if L1 chose not to trap WRMSR to TSC. According\n\t\t * to the spec, this should set L1's TSC; The offset that L1\n\t\t * set for L2 remains unchanged, and still needs to be added\n\t\t * to the newly set TSC to get L2's TSC.\n\t\t */\n\t\tstruct vmcs12 *vmcs12;\n\t\t/* recalculate vmcs02.TSC_OFFSET: */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tvmcs_write64(TSC_OFFSET, offset +\n\t\t\t(nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETING) ?\n\t\t\t vmcs12->tsc_offset : 0));\n\t} else {\n\t\ttrace_kvm_write_tsc_offset(vcpu->vcpu_id,\n\t\t\t\t\t   vmcs_read64(TSC_OFFSET), offset);\n\t\tvmcs_write64(TSC_OFFSET, offset);\n\t}\n}\n\n/*\n * nested_vmx_allowed() checks whether a guest should be allowed to use VMX\n * instructions and MSRs (i.e., nested VMX). Nested VMX is disabled for\n * all guests if the \"nested\" module option is off, and can also be disabled\n * for a single guest by disabling its VMX cpuid bit.\n */\nstatic inline bool nested_vmx_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn nested && guest_cpuid_has(vcpu, X86_FEATURE_VMX);\n}\n\n/*\n * nested_vmx_setup_ctls_msrs() sets up variables containing the values to be\n * returned for the various VMX controls MSRs when nested VMX is enabled.\n * The same values should also be used to verify that vmcs12 control fields are\n * valid during nested entry from L1 to L2.\n * Each of these control msrs has a low and high 32-bit half: A low bit is on\n * if the corresponding bit in the (32-bit) control field *must* be on, and a\n * bit in the high half is on if the corresponding bit in the control field\n * may be on. See also vmx_control_verify().\n */\nstatic void nested_vmx_setup_ctls_msrs(struct nested_vmx_msrs *msrs, bool apicv)\n{\n\tif (!nested) {\n\t\tmemset(msrs, 0, sizeof(*msrs));\n\t\treturn;\n\t}\n\n\t/*\n\t * Note that as a general rule, the high half of the MSRs (bits in\n\t * the control fields which may be 1) should be initialized by the\n\t * intersection of the underlying hardware's MSR (i.e., features which\n\t * can be supported) and the list of features we want to expose -\n\t * because they are known to be properly supported in our code.\n\t * Also, usually, the low half of the MSRs (bits which must be 1) can\n\t * be set to 0, meaning that L1 may turn off any of these bits. The\n\t * reason is that if one of these bits is necessary, it will appear\n\t * in vmcs01 and prepare_vmcs02, when it bitwise-or's the control\n\t * fields of vmcs01 and vmcs02, will turn these bits off - and\n\t * nested_vmx_exit_reflected() will not pass related exits to L1.\n\t * These rules have exceptions below.\n\t */\n\n\t/* pin-based controls */\n\trdmsr(MSR_IA32_VMX_PINBASED_CTLS,\n\t\tmsrs->pinbased_ctls_low,\n\t\tmsrs->pinbased_ctls_high);\n\tmsrs->pinbased_ctls_low |=\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->pinbased_ctls_high &=\n\t\tPIN_BASED_EXT_INTR_MASK |\n\t\tPIN_BASED_NMI_EXITING |\n\t\tPIN_BASED_VIRTUAL_NMIS |\n\t\t(apicv ? PIN_BASED_POSTED_INTR : 0);\n\tmsrs->pinbased_ctls_high |=\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/* exit controls */\n\trdmsr(MSR_IA32_VMX_EXIT_CTLS,\n\t\tmsrs->exit_ctls_low,\n\t\tmsrs->exit_ctls_high);\n\tmsrs->exit_ctls_low =\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->exit_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_EXIT_HOST_ADDR_SPACE_SIZE |\n#endif\n\t\tVM_EXIT_LOAD_IA32_PAT | VM_EXIT_SAVE_IA32_PAT;\n\tmsrs->exit_ctls_high |=\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tVM_EXIT_LOAD_IA32_EFER | VM_EXIT_SAVE_IA32_EFER |\n\t\tVM_EXIT_SAVE_VMX_PREEMPTION_TIMER | VM_EXIT_ACK_INTR_ON_EXIT;\n\n\tif (kvm_mpx_supported())\n\t\tmsrs->exit_ctls_high |= VM_EXIT_CLEAR_BNDCFGS;\n\n\t/* We support free control of debug control saving. */\n\tmsrs->exit_ctls_low &= ~VM_EXIT_SAVE_DEBUG_CONTROLS;\n\n\t/* entry controls */\n\trdmsr(MSR_IA32_VMX_ENTRY_CTLS,\n\t\tmsrs->entry_ctls_low,\n\t\tmsrs->entry_ctls_high);\n\tmsrs->entry_ctls_low =\n\t\tVM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->entry_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_ENTRY_IA32E_MODE |\n#endif\n\t\tVM_ENTRY_LOAD_IA32_PAT;\n\tmsrs->entry_ctls_high |=\n\t\t(VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR | VM_ENTRY_LOAD_IA32_EFER);\n\tif (kvm_mpx_supported())\n\t\tmsrs->entry_ctls_high |= VM_ENTRY_LOAD_BNDCFGS;\n\n\t/* We support free control of debug control loading. */\n\tmsrs->entry_ctls_low &= ~VM_ENTRY_LOAD_DEBUG_CONTROLS;\n\n\t/* cpu-based controls */\n\trdmsr(MSR_IA32_VMX_PROCBASED_CTLS,\n\t\tmsrs->procbased_ctls_low,\n\t\tmsrs->procbased_ctls_high);\n\tmsrs->procbased_ctls_low =\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\tmsrs->procbased_ctls_high &=\n\t\tCPU_BASED_VIRTUAL_INTR_PENDING |\n\t\tCPU_BASED_VIRTUAL_NMI_PENDING | CPU_BASED_USE_TSC_OFFSETING |\n\t\tCPU_BASED_HLT_EXITING | CPU_BASED_INVLPG_EXITING |\n\t\tCPU_BASED_MWAIT_EXITING | CPU_BASED_CR3_LOAD_EXITING |\n\t\tCPU_BASED_CR3_STORE_EXITING |\n#ifdef CONFIG_X86_64\n\t\tCPU_BASED_CR8_LOAD_EXITING | CPU_BASED_CR8_STORE_EXITING |\n#endif\n\t\tCPU_BASED_MOV_DR_EXITING | CPU_BASED_UNCOND_IO_EXITING |\n\t\tCPU_BASED_USE_IO_BITMAPS | CPU_BASED_MONITOR_TRAP_FLAG |\n\t\tCPU_BASED_MONITOR_EXITING | CPU_BASED_RDPMC_EXITING |\n\t\tCPU_BASED_RDTSC_EXITING | CPU_BASED_PAUSE_EXITING |\n\t\tCPU_BASED_TPR_SHADOW | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n\t/*\n\t * We can allow some features even when not supported by the\n\t * hardware. For example, L1 can specify an MSR bitmap - and we\n\t * can use it to avoid exits to L1 - even when L0 runs L2\n\t * without MSR bitmaps.\n\t */\n\tmsrs->procbased_ctls_high |=\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tCPU_BASED_USE_MSR_BITMAPS;\n\n\t/* We support free control of CR3 access interception. */\n\tmsrs->procbased_ctls_low &=\n\t\t~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);\n\n\t/*\n\t * secondary cpu-based controls.  Do not include those that\n\t * depend on CPUID bits, they are added later by vmx_cpuid_update.\n\t */\n\trdmsr(MSR_IA32_VMX_PROCBASED_CTLS2,\n\t\tmsrs->secondary_ctls_low,\n\t\tmsrs->secondary_ctls_high);\n\tmsrs->secondary_ctls_low = 0;\n\tmsrs->secondary_ctls_high &=\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\tSECONDARY_EXEC_DESC |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\tSECONDARY_EXEC_WBINVD_EXITING;\n\n\tif (enable_ept) {\n\t\t/* nested EPT: emulate EPT also to L1 */\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_EPT;\n\t\tmsrs->ept_caps = VMX_EPT_PAGE_WALK_4_BIT |\n\t\t\t VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT;\n\t\tif (cpu_has_vmx_ept_execute_only())\n\t\t\tmsrs->ept_caps |=\n\t\t\t\tVMX_EPT_EXECUTE_ONLY_BIT;\n\t\tmsrs->ept_caps &= vmx_capability.ept;\n\t\tmsrs->ept_caps |= VMX_EPT_EXTENT_GLOBAL_BIT |\n\t\t\tVMX_EPT_EXTENT_CONTEXT_BIT | VMX_EPT_2MB_PAGE_BIT |\n\t\t\tVMX_EPT_1GB_PAGE_BIT;\n\t\tif (enable_ept_ad_bits) {\n\t\t\tmsrs->secondary_ctls_high |=\n\t\t\t\tSECONDARY_EXEC_ENABLE_PML;\n\t\t\tmsrs->ept_caps |= VMX_EPT_AD_BIT;\n\t\t}\n\t}\n\n\tif (cpu_has_vmx_vmfunc()) {\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n\t\t/*\n\t\t * Advertise EPTP switching unconditionally\n\t\t * since we emulate it\n\t\t */\n\t\tif (enable_ept)\n\t\t\tmsrs->vmfunc_controls =\n\t\t\t\tVMX_VMFUNC_EPTP_SWITCHING;\n\t}\n\n\t/*\n\t * Old versions of KVM use the single-context version without\n\t * checking for support, so declare that it is supported even\n\t * though it is treated as global context.  The alternative is\n\t * not failing the single-context invvpid, and it is worse.\n\t */\n\tif (enable_vpid) {\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_VPID;\n\t\tmsrs->vpid_caps = VMX_VPID_INVVPID_BIT |\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK;\n\t}\n\n\tif (enable_unrestricted_guest)\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST;\n\n\t/* miscellaneous data */\n\trdmsr(MSR_IA32_VMX_MISC,\n\t\tmsrs->misc_low,\n\t\tmsrs->misc_high);\n\tmsrs->misc_low &= VMX_MISC_SAVE_EFER_LMA;\n\tmsrs->misc_low |=\n\t\tMSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS |\n\t\tVMX_MISC_EMULATED_PREEMPTION_TIMER_RATE |\n\t\tVMX_MISC_ACTIVITY_HLT;\n\tmsrs->misc_high = 0;\n\n\t/*\n\t * This MSR reports some information about VMX support. We\n\t * should return information about the VMX we emulate for the\n\t * guest, and the VMCS structure we give it - not about the\n\t * VMX support of the underlying hardware.\n\t */\n\tmsrs->basic =\n\t\tVMCS12_REVISION |\n\t\tVMX_BASIC_TRUE_CTLS |\n\t\t((u64)VMCS12_SIZE << VMX_BASIC_VMCS_SIZE_SHIFT) |\n\t\t(VMX_BASIC_MEM_TYPE_WB << VMX_BASIC_MEM_TYPE_SHIFT);\n\n\tif (cpu_has_vmx_basic_inout())\n\t\tmsrs->basic |= VMX_BASIC_INOUT;\n\n\t/*\n\t * These MSRs specify bits which the guest must keep fixed on\n\t * while L1 is in VMXON mode (in L1's root mode, or running an L2).\n\t * We picked the standard core2 setting.\n\t */\n#define VMXON_CR0_ALWAYSON     (X86_CR0_PE | X86_CR0_PG | X86_CR0_NE)\n#define VMXON_CR4_ALWAYSON     X86_CR4_VMXE\n\tmsrs->cr0_fixed0 = VMXON_CR0_ALWAYSON;\n\tmsrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;\n\n\t/* These MSRs specify bits which the guest must keep fixed off. */\n\trdmsrl(MSR_IA32_VMX_CR0_FIXED1, msrs->cr0_fixed1);\n\trdmsrl(MSR_IA32_VMX_CR4_FIXED1, msrs->cr4_fixed1);\n\n\t/* highest index: VMX_PREEMPTION_TIMER_VALUE */\n\tmsrs->vmcs_enum = VMCS12_MAX_FIELD_INDEX << 1;\n}\n\n/*\n * if fixed0[i] == 1: val[i] must be 1\n * if fixed1[i] == 0: val[i] must be 0\n */\nstatic inline bool fixed_bits_valid(u64 val, u64 fixed0, u64 fixed1)\n{\n\treturn ((val & fixed1) | fixed0) == val;\n}\n\nstatic inline bool vmx_control_verify(u32 control, u32 low, u32 high)\n{\n\treturn fixed_bits_valid(control, low, high);\n}\n\nstatic inline u64 vmx_control_msr(u32 low, u32 high)\n{\n\treturn low | ((u64)high << 32);\n}\n\nstatic bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)\n{\n\tsuperset &= mask;\n\tsubset &= mask;\n\n\treturn (superset | subset) == superset;\n}\n\nstatic int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved =\n\t\t/* feature (except bit 48; see below) */\n\t\tBIT_ULL(49) | BIT_ULL(54) | BIT_ULL(55) |\n\t\t/* reserved */\n\t\tBIT_ULL(31) | GENMASK_ULL(47, 45) | GENMASK_ULL(63, 56);\n\tu64 vmx_basic = vmx->nested.msrs.basic;\n\n\tif (!is_bitwise_subset(vmx_basic, data, feature_and_reserved))\n\t\treturn -EINVAL;\n\n\t/*\n\t * KVM does not emulate a version of VMX that constrains physical\n\t * addresses of VMX structures (e.g. VMCS) to 32-bits.\n\t */\n\tif (data & BIT_ULL(48))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_revision_id(vmx_basic) !=\n\t    vmx_basic_vmcs_revision_id(data))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_size(vmx_basic) > vmx_basic_vmcs_size(data))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.basic = data;\n\treturn 0;\n}\n\nstatic int\nvmx_restore_control_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tu64 supported;\n\tu32 *lowp, *highp;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\t\tlowp = &vmx->nested.msrs.pinbased_ctls_low;\n\t\thighp = &vmx->nested.msrs.pinbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\t\tlowp = &vmx->nested.msrs.procbased_ctls_low;\n\t\thighp = &vmx->nested.msrs.procbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\t\tlowp = &vmx->nested.msrs.exit_ctls_low;\n\t\thighp = &vmx->nested.msrs.exit_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\t\tlowp = &vmx->nested.msrs.entry_ctls_low;\n\t\thighp = &vmx->nested.msrs.entry_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\tlowp = &vmx->nested.msrs.secondary_ctls_low;\n\t\thighp = &vmx->nested.msrs.secondary_ctls_high;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsupported = vmx_control_msr(*lowp, *highp);\n\n\t/* Check must-be-1 bits are still 1. */\n\tif (!is_bitwise_subset(data, supported, GENMASK_ULL(31, 0)))\n\t\treturn -EINVAL;\n\n\t/* Check must-be-0 bits are still 0. */\n\tif (!is_bitwise_subset(supported, data, GENMASK_ULL(63, 32)))\n\t\treturn -EINVAL;\n\n\t*lowp = data;\n\t*highp = data >> 32;\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_misc(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved_bits =\n\t\t/* feature */\n\t\tBIT_ULL(5) | GENMASK_ULL(8, 6) | BIT_ULL(14) | BIT_ULL(15) |\n\t\tBIT_ULL(28) | BIT_ULL(29) | BIT_ULL(30) |\n\t\t/* reserved */\n\t\tGENMASK_ULL(13, 9) | BIT_ULL(31);\n\tu64 vmx_misc;\n\n\tvmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,\n\t\t\t\t   vmx->nested.msrs.misc_high);\n\n\tif (!is_bitwise_subset(vmx_misc, data, feature_and_reserved_bits))\n\t\treturn -EINVAL;\n\n\tif ((vmx->nested.msrs.pinbased_ctls_high &\n\t     PIN_BASED_VMX_PREEMPTION_TIMER) &&\n\t    vmx_misc_preemption_timer_rate(data) !=\n\t    vmx_misc_preemption_timer_rate(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_cr3_count(data) > vmx_misc_cr3_count(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_max_msr(data) > vmx_misc_max_msr(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_mseg_revid(data) != vmx_misc_mseg_revid(vmx_misc))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.misc_low = data;\n\tvmx->nested.msrs.misc_high = data >> 32;\n\n\t/*\n\t * If L1 has read-only VM-exit information fields, use the\n\t * less permissive vmx_vmwrite_bitmap to specify write\n\t * permissions for the shadow VMCS.\n\t */\n\tif (enable_shadow_vmcs && !nested_cpu_has_vmwrite_any_field(&vmx->vcpu))\n\t\tvmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));\n\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_ept_vpid_cap(struct vcpu_vmx *vmx, u64 data)\n{\n\tu64 vmx_ept_vpid_cap;\n\n\tvmx_ept_vpid_cap = vmx_control_msr(vmx->nested.msrs.ept_caps,\n\t\t\t\t\t   vmx->nested.msrs.vpid_caps);\n\n\t/* Every bit is either reserved or a feature bit. */\n\tif (!is_bitwise_subset(vmx_ept_vpid_cap, data, -1ULL))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.ept_caps = data;\n\tvmx->nested.msrs.vpid_caps = data >> 32;\n\treturn 0;\n}\n\nstatic int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tu64 *msr;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\tmsr = &vmx->nested.msrs.cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\tmsr = &vmx->nested.msrs.cr4_fixed0;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\t/*\n\t * 1 bits (which indicates bits which \"must-be-1\" during VMX operation)\n\t * must be 1 in the restored value.\n\t */\n\tif (!is_bitwise_subset(data, *msr, -1ULL))\n\t\treturn -EINVAL;\n\n\t*msr = data;\n\treturn 0;\n}\n\n/*\n * Called when userspace is restoring VMX MSRs.\n *\n * Returns 0 on success, non-0 otherwise.\n */\nstatic int vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t/*\n\t * Don't allow changes to the VMX capability MSRs while the vCPU\n\t * is in VMX operation.\n\t */\n\tif (vmx->nested.vmxon)\n\t\treturn -EBUSY;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\treturn vmx_restore_vmx_basic(vmx, data);\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t/*\n\t\t * The \"non-true\" VMX capability MSRs are generated from the\n\t\t * \"true\" MSRs, so we do not support restoring them directly.\n\t\t *\n\t\t * If userspace wants to emulate VMX_BASIC[55]=0, userspace\n\t\t * should restore the \"true\" MSRs with the must-be-1 bits\n\t\t * set according to the SDM Vol 3. A.2 \"RESERVED CONTROLS AND\n\t\t * DEFAULT SETTINGS\".\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\treturn vmx_restore_control_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_MISC:\n\t\treturn vmx_restore_vmx_misc(vmx, data);\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn vmx_restore_fixed0_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t/*\n\t\t * These MSRs are generated based on the vCPU's CPUID, so we\n\t\t * do not support restoring them directly.\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\treturn vmx_restore_vmx_ept_vpid_cap(vmx, data);\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\tvmx->nested.msrs.vmcs_enum = data;\n\t\treturn 0;\n\tdefault:\n\t\t/*\n\t\t * The rest of the VMX capability MSRs do not support restore.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Returns 0 on success, non-0 otherwise. */\nstatic int vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)\n{\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\t*pdata = msrs->basic;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->pinbased_ctls_low,\n\t\t\tmsrs->pinbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PINBASED_CTLS)\n\t\t\t*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->procbased_ctls_low,\n\t\t\tmsrs->procbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)\n\t\t\t*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->exit_ctls_low,\n\t\t\tmsrs->exit_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_EXIT_CTLS)\n\t\t\t*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->entry_ctls_low,\n\t\t\tmsrs->entry_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_ENTRY_CTLS)\n\t\t\t*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_MISC:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->misc_low,\n\t\t\tmsrs->misc_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\t*pdata = msrs->cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\t\t*pdata = msrs->cr0_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\t*pdata = msrs->cr4_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t*pdata = msrs->cr4_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\t*pdata = msrs->vmcs_enum;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->secondary_ctls_low,\n\t\t\tmsrs->secondary_ctls_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\t*pdata = msrs->ept_caps |\n\t\t\t((u64)msrs->vpid_caps << 32);\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\t*pdata = msrs->vmfunc_controls;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool vmx_feature_control_msr_valid(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t uint64_t val)\n{\n\tuint64_t valid_bits = to_vmx(vcpu)->msr_ia32_feature_control_valid_bits;\n\n\treturn !(val & ~valid_bits);\n}\n\nstatic int vmx_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!nested)\n\t\t\treturn 1;\n\t\treturn vmx_get_vmx_msr(&vmcs_config.nested, msr->index, &msr->data);\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Reads an msr value (of 'msr_index') into 'pdata'.\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\n\tswitch (msr_info->index) {\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_FS_BASE);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_GS_BASE);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tmsr_info->data = vmx->msr_guest_kernel_gs_base;\n\t\tbreak;\n#endif\n\tcase MSR_EFER:\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = to_vmx(vcpu)->spec_ctrl;\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_ARCH_CAPABILITIES))\n\t\t\treturn 1;\n\t\tmsr_info->data = to_vmx(vcpu)->arch_capabilities;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tmsr_info->data = vmcs_read32(GUEST_SYSENTER_CS);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmcs_read64(GUEST_BNDCFGS);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !(vmx->msr_ia32_feature_control &\n\t\t      FEATURE_CONTROL_LMCE))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.mcg_ext_ctl;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tmsr_info->data = vmx->msr_ia32_feature_control;\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,\n\t\t\t\t       &msr_info->data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.ia32_xss;\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_info->index);\n\t\tif (msr) {\n\t\t\tmsr_info->data = msr->data;\n\t\t\tbreak;\n\t\t}\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_leave_nested(struct kvm_vcpu *vcpu);\n\n/*\n * Writes msr value into into the appropriate \"register\".\n * Returns 0 on success, non-0 otherwise.\n * Assumes vcpu_load() was already called.\n */\nstatic int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\tint ret = 0;\n\tu32 msr_index = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr_index) {\n\tcase MSR_EFER:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_FS_BASE, data);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_GS_BASE, data);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tvmx->msr_guest_kernel_gs_base = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tvmcs_write32(GUEST_SYSENTER_CS, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, data);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data & PAGE_MASK, vcpu) ||\n\t\t    (data & MSR_IA32_BNDCFGS_RSVD))\n\t\t\treturn 1;\n\t\tvmcs_write64(GUEST_BNDCFGS, data);\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBRS) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\t/* The STIBP bit doesn't fault even if it's not advertised */\n\t\tif (data & ~(SPEC_CTRL_IBRS | SPEC_CTRL_STIBP))\n\t\t\treturn 1;\n\n\t\tvmx->spec_ctrl = data;\n\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * For non-nested:\n\t\t * When it's written (to non-zero) for the first time, pass\n\t\t * it through.\n\t\t *\n\t\t * For nested:\n\t\t * The handling of the MSR bitmap for L2 guests is done in\n\t\t * nested_vmx_merge_msr_bitmap. We should not touch the\n\t\t * vmcs02.msr_bitmap here since it gets completely overwritten\n\t\t * in the merging. We update the vmcs01 here for L1 as well\n\t\t * since it will end up touching the MSR anyway now.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap,\n\t\t\t\t\t      MSR_IA32_SPEC_CTRL,\n\t\t\t\t\t      MSR_TYPE_RW);\n\t\tbreak;\n\tcase MSR_IA32_PRED_CMD:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_IBPB) &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\t\treturn 1;\n\n\t\tif (data & ~PRED_CMD_IBPB)\n\t\t\treturn 1;\n\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\twrmsrl(MSR_IA32_PRED_CMD, PRED_CMD_IBPB);\n\n\t\t/*\n\t\t * For non-nested:\n\t\t * When it's written (to non-zero) for the first time, pass\n\t\t * it through.\n\t\t *\n\t\t * For nested:\n\t\t * The handling of the MSR bitmap for L2 guests is done in\n\t\t * nested_vmx_merge_msr_bitmap. We should not touch the\n\t\t * vmcs02.msr_bitmap here since it gets completely overwritten\n\t\t * in the merging.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(vmx->vmcs01.msr_bitmap, MSR_IA32_PRED_CMD,\n\t\t\t\t\t      MSR_TYPE_W);\n\t\tbreak;\n\tcase MSR_IA32_ARCH_CAPABILITIES:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;\n\t\tvmx->arch_capabilities = data;\n\t\tbreak;\n\tcase MSR_IA32_CR_PAT:\n\t\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\t\tif (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))\n\t\t\t\treturn 1;\n\t\t\tvmcs_write64(GUEST_IA32_PAT, data);\n\t\t\tvcpu->arch.pat = data;\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif ((!msr_info->host_initiated &&\n\t\t     !(to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t       FEATURE_CONTROL_LMCE)) ||\n\t\t    (data & ~MCG_EXT_CTL_LMCE_EN))\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ext_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tif (!vmx_feature_control_msr_valid(vcpu, data) ||\n\t\t    (to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tvmx->msr_ia32_feature_control = data;\n\t\tif (msr_info->host_initiated && data == 0)\n\t\t\tvmx_leave_nested(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1; /* they are read-only */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_set_vmx_msr(vcpu, msr_index, data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\t/*\n\t\t * The only supported bit as of Skylake is bit 8, but\n\t\t * it is not supported on KVM.\n\t\t */\n\t\tif (data != 0)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tif (vcpu->arch.ia32_xss != host_xss)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_IA32_XSS,\n\t\t\t\tvcpu->arch.ia32_xss, host_xss);\n\t\telse\n\t\t\tclear_atomic_switch_msr(vmx, MSR_IA32_XSS);\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Check reserved bit, higher 32 bits should be zero */\n\t\tif ((data >> 32) != 0)\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_index);\n\t\tif (msr) {\n\t\t\tu64 old_msr_data = msr->data;\n\t\t\tmsr->data = data;\n\t\t\tif (msr - vmx->guest_msrs < vmx->save_nmsrs) {\n\t\t\t\tpreempt_disable();\n\t\t\t\tret = kvm_set_shared_msr(msr->index, msr->data,\n\t\t\t\t\t\t\t msr->mask);\n\t\t\t\tpreempt_enable();\n\t\t\t\tif (ret)\n\t\t\t\t\tmsr->data = old_msr_data;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t}\n\n\treturn ret;\n}\n\nstatic void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)\n{\n\t__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n\tswitch (reg) {\n\tcase VCPU_REGS_RSP:\n\t\tvcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);\n\t\tbreak;\n\tcase VCPU_REGS_RIP:\n\t\tvcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);\n\t\tbreak;\n\tcase VCPU_EXREG_PDPTR:\n\t\tif (enable_ept)\n\t\t\tept_save_pdptrs(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic __init int cpu_has_kvm_support(void)\n{\n\treturn cpu_has_vmx();\n}\n\nstatic __init int vmx_disabled_by_bios(void)\n{\n\tu64 msr;\n\n\trdmsrl(MSR_IA32_FEATURE_CONTROL, msr);\n\tif (msr & FEATURE_CONTROL_LOCKED) {\n\t\t/* launched w/ TXT and VMX disabled */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)\n\t\t\t&& tboot_enabled())\n\t\t\treturn 1;\n\t\t/* launched w/o TXT and VMX only enabled w/ TXT */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)\n\t\t\t&& (msr & FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX)\n\t\t\t&& !tboot_enabled()) {\n\t\t\tprintk(KERN_WARNING \"kvm: disable TXT in the BIOS or \"\n\t\t\t\t\"activate TXT before enabling KVM\\n\");\n\t\t\treturn 1;\n\t\t}\n\t\t/* launched w/o TXT and VMX disabled */\n\t\tif (!(msr & FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX)\n\t\t\t&& !tboot_enabled())\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void kvm_cpu_vmxon(u64 addr)\n{\n\tcr4_set_bits(X86_CR4_VMXE);\n\tintel_pt_handle_vmx(1);\n\n\tasm volatile (ASM_VMX_VMXON_RAX\n\t\t\t: : \"a\"(&addr), \"m\"(addr)\n\t\t\t: \"memory\", \"cc\");\n}\n\nstatic int hardware_enable(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tu64 phys_addr = __pa(per_cpu(vmxarea, cpu));\n\tu64 old, test_bits;\n\n\tif (cr4_read_shadow() & X86_CR4_VMXE)\n\t\treturn -EBUSY;\n\n\t/*\n\t * This can happen if we hot-added a CPU but failed to allocate\n\t * VP assist page for it.\n\t */\n\tif (static_branch_unlikely(&enable_evmcs) &&\n\t    !hv_get_vp_assist_page(cpu))\n\t\treturn -EFAULT;\n\n\tINIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));\n\tINIT_LIST_HEAD(&per_cpu(blocked_vcpu_on_cpu, cpu));\n\tspin_lock_init(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n\n\t/*\n\t * Now we can enable the vmclear operation in kdump\n\t * since the loaded_vmcss_on_cpu list on this cpu\n\t * has been initialized.\n\t *\n\t * Though the cpu is not in VMX operation now, there\n\t * is no problem to enable the vmclear operation\n\t * for the loaded_vmcss_on_cpu list is empty!\n\t */\n\tcrash_enable_local_vmclear(cpu);\n\n\trdmsrl(MSR_IA32_FEATURE_CONTROL, old);\n\n\ttest_bits = FEATURE_CONTROL_LOCKED;\n\ttest_bits |= FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\tif (tboot_enabled())\n\t\ttest_bits |= FEATURE_CONTROL_VMXON_ENABLED_INSIDE_SMX;\n\n\tif ((old & test_bits) != test_bits) {\n\t\t/* enable and lock */\n\t\twrmsrl(MSR_IA32_FEATURE_CONTROL, old | test_bits);\n\t}\n\tkvm_cpu_vmxon(phys_addr);\n\tif (enable_ept)\n\t\tept_sync_global();\n\n\treturn 0;\n}\n\nstatic void vmclear_local_loaded_vmcss(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v, *n;\n\n\tlist_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t\t loaded_vmcss_on_cpu_link)\n\t\t__loaded_vmcs_clear(v);\n}\n\n\n/* Just like cpu_vmxoff(), but with the __kvm_handle_fault_on_reboot()\n * tricks.\n */\nstatic void kvm_cpu_vmxoff(void)\n{\n\tasm volatile (__ex(ASM_VMX_VMXOFF) : : : \"cc\");\n\n\tintel_pt_handle_vmx(0);\n\tcr4_clear_bits(X86_CR4_VMXE);\n}\n\nstatic void hardware_disable(void)\n{\n\tvmclear_local_loaded_vmcss();\n\tkvm_cpu_vmxoff();\n}\n\nstatic __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,\n\t\t\t\t      u32 msr, u32 *result)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 ctl = ctl_min | ctl_opt;\n\n\trdmsr(msr, vmx_msr_low, vmx_msr_high);\n\n\tctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */\n\tctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */\n\n\t/* Ensure minimum (required) set of control bits are supported. */\n\tif (ctl_min & ~ctl)\n\t\treturn -EIO;\n\n\t*result = ctl;\n\treturn 0;\n}\n\nstatic __init bool allow_1_setting(u32 msr, u32 ctl)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\n\trdmsr(msr, vmx_msr_low, vmx_msr_high);\n\treturn vmx_msr_high & ctl;\n}\n\nstatic __init int setup_vmcs_config(struct vmcs_config *vmcs_conf)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 min, opt, min2, opt2;\n\tu32 _pin_based_exec_control = 0;\n\tu32 _cpu_based_exec_control = 0;\n\tu32 _cpu_based_2nd_exec_control = 0;\n\tu32 _vmexit_control = 0;\n\tu32 _vmentry_control = 0;\n\n\tmemset(vmcs_conf, 0, sizeof(*vmcs_conf));\n\tmin = CPU_BASED_HLT_EXITING |\n#ifdef CONFIG_X86_64\n\t      CPU_BASED_CR8_LOAD_EXITING |\n\t      CPU_BASED_CR8_STORE_EXITING |\n#endif\n\t      CPU_BASED_CR3_LOAD_EXITING |\n\t      CPU_BASED_CR3_STORE_EXITING |\n\t      CPU_BASED_UNCOND_IO_EXITING |\n\t      CPU_BASED_MOV_DR_EXITING |\n\t      CPU_BASED_USE_TSC_OFFSETING |\n\t      CPU_BASED_MWAIT_EXITING |\n\t      CPU_BASED_MONITOR_EXITING |\n\t      CPU_BASED_INVLPG_EXITING |\n\t      CPU_BASED_RDPMC_EXITING;\n\n\topt = CPU_BASED_TPR_SHADOW |\n\t      CPU_BASED_USE_MSR_BITMAPS |\n\t      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,\n\t\t\t\t&_cpu_based_exec_control) < 0)\n\t\treturn -EIO;\n#ifdef CONFIG_X86_64\n\tif ((_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))\n\t\t_cpu_based_exec_control &= ~CPU_BASED_CR8_LOAD_EXITING &\n\t\t\t\t\t   ~CPU_BASED_CR8_STORE_EXITING;\n#endif\n\tif (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {\n\t\tmin2 = 0;\n\t\topt2 = SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\tSECONDARY_EXEC_WBINVD_EXITING |\n\t\t\tSECONDARY_EXEC_ENABLE_VPID |\n\t\t\tSECONDARY_EXEC_ENABLE_EPT |\n\t\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST |\n\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING |\n\t\t\tSECONDARY_EXEC_DESC |\n\t\t\tSECONDARY_EXEC_RDTSCP |\n\t\t\tSECONDARY_EXEC_ENABLE_INVPCID |\n\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\t\tSECONDARY_EXEC_SHADOW_VMCS |\n\t\t\tSECONDARY_EXEC_XSAVES |\n\t\t\tSECONDARY_EXEC_RDSEED_EXITING |\n\t\t\tSECONDARY_EXEC_RDRAND_EXITING |\n\t\t\tSECONDARY_EXEC_ENABLE_PML |\n\t\t\tSECONDARY_EXEC_TSC_SCALING |\n\t\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n\t\tif (adjust_vmx_controls(min2, opt2,\n\t\t\t\t\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\t\t\t\t\t&_cpu_based_2nd_exec_control) < 0)\n\t\t\treturn -EIO;\n\t}\n#ifndef CONFIG_X86_64\n\tif (!(_cpu_based_2nd_exec_control &\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\t_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;\n#endif\n\n\tif (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))\n\t\t_cpu_based_2nd_exec_control &= ~(\n\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\n\trdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,\n\t\t&vmx_capability.ept, &vmx_capability.vpid);\n\n\tif (_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) {\n\t\t/* CR3 accesses and invlpg don't need to cause VM Exits when EPT\n\t\t   enabled */\n\t\t_cpu_based_exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t\t\t     CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\t\t     CPU_BASED_INVLPG_EXITING);\n\t} else if (vmx_capability.ept) {\n\t\tvmx_capability.ept = 0;\n\t\tpr_warn_once(\"EPT CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable EPT VM-execution control\\n\");\n\t}\n\tif (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&\n\t\tvmx_capability.vpid) {\n\t\tvmx_capability.vpid = 0;\n\t\tpr_warn_once(\"VPID CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable VPID VM-execution control\\n\");\n\t}\n\n\tmin = VM_EXIT_SAVE_DEBUG_CONTROLS | VM_EXIT_ACK_INTR_ON_EXIT;\n#ifdef CONFIG_X86_64\n\tmin |= VM_EXIT_HOST_ADDR_SPACE_SIZE;\n#endif\n\topt = VM_EXIT_SAVE_IA32_PAT | VM_EXIT_LOAD_IA32_PAT |\n\t\tVM_EXIT_CLEAR_BNDCFGS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t&_vmexit_control) < 0)\n\t\treturn -EIO;\n\n\tmin = PIN_BASED_EXT_INTR_MASK | PIN_BASED_NMI_EXITING;\n\topt = PIN_BASED_VIRTUAL_NMIS | PIN_BASED_POSTED_INTR |\n\t\t PIN_BASED_VMX_PREEMPTION_TIMER;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,\n\t\t\t\t&_pin_based_exec_control) < 0)\n\t\treturn -EIO;\n\n\tif (cpu_has_broken_vmx_preemption_timer())\n\t\t_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\tif (!(_cpu_based_2nd_exec_control &\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))\n\t\t_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;\n\n\tmin = VM_ENTRY_LOAD_DEBUG_CONTROLS;\n\topt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS;\n\tif (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\t&_vmentry_control) < 0)\n\t\treturn -EIO;\n\n\trdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);\n\n\t/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */\n\tif ((vmx_msr_high & 0x1fff) > PAGE_SIZE)\n\t\treturn -EIO;\n\n#ifdef CONFIG_X86_64\n\t/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */\n\tif (vmx_msr_high & (1u<<16))\n\t\treturn -EIO;\n#endif\n\n\t/* Require Write-Back (WB) memory type for VMCS accesses. */\n\tif (((vmx_msr_high >> 18) & 15) != 6)\n\t\treturn -EIO;\n\n\tvmcs_conf->size = vmx_msr_high & 0x1fff;\n\tvmcs_conf->order = get_order(vmcs_conf->size);\n\tvmcs_conf->basic_cap = vmx_msr_high & ~0x1fff;\n\n\t/* KVM supports Enlightened VMCS v1 only */\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tvmcs_conf->revision_id = KVM_EVMCS_VERSION;\n\telse\n\t\tvmcs_conf->revision_id = vmx_msr_low;\n\n\tvmcs_conf->pin_based_exec_ctrl = _pin_based_exec_control;\n\tvmcs_conf->cpu_based_exec_ctrl = _cpu_based_exec_control;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;\n\tvmcs_conf->vmexit_ctrl         = _vmexit_control;\n\tvmcs_conf->vmentry_ctrl        = _vmentry_control;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_sanitize_exec_ctrls(vmcs_conf);\n\n\tcpu_has_load_ia32_efer =\n\t\tallow_1_setting(MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\tVM_ENTRY_LOAD_IA32_EFER)\n\t\t&& allow_1_setting(MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t   VM_EXIT_LOAD_IA32_EFER);\n\n\tcpu_has_load_perf_global_ctrl =\n\t\tallow_1_setting(MSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\t&& allow_1_setting(MSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t   VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);\n\n\t/*\n\t * Some cpus support VM_ENTRY_(LOAD|SAVE)_IA32_PERF_GLOBAL_CTRL\n\t * but due to errata below it can't be used. Workaround is to use\n\t * msr load mechanism to switch IA32_PERF_GLOBAL_CTRL.\n\t *\n\t * VM Exit May Incorrectly Clear IA32_PERF_GLOBAL_CTRL [34:32]\n\t *\n\t * AAK155             (model 26)\n\t * AAP115             (model 30)\n\t * AAT100             (model 37)\n\t * BC86,AAY89,BD102   (model 44)\n\t * BA97               (model 46)\n\t *\n\t */\n\tif (cpu_has_load_perf_global_ctrl && boot_cpu_data.x86 == 0x6) {\n\t\tswitch (boot_cpu_data.x86_model) {\n\t\tcase 26:\n\t\tcase 30:\n\t\tcase 37:\n\t\tcase 44:\n\t\tcase 46:\n\t\t\tcpu_has_load_perf_global_ctrl = false;\n\t\t\tprintk_once(KERN_WARNING\"kvm: VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL \"\n\t\t\t\t\t\"does not work properly. Using workaround\\n\");\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_XSAVES))\n\t\trdmsrl(MSR_IA32_XSS, host_xss);\n\n\treturn 0;\n}\n\nstatic struct vmcs *alloc_vmcs_cpu(int cpu)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct page *pages;\n\tstruct vmcs *vmcs;\n\n\tpages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);\n\tif (!pages)\n\t\treturn NULL;\n\tvmcs = page_address(pages);\n\tmemset(vmcs, 0, vmcs_config.size);\n\tvmcs->revision_id = vmcs_config.revision_id; /* vmcs revision id */\n\treturn vmcs;\n}\n\nstatic void free_vmcs(struct vmcs *vmcs)\n{\n\tfree_pages((unsigned long)vmcs, vmcs_config.order);\n}\n\n/*\n * Free a VMCS, but before that VMCLEAR it on the CPU where it was last loaded\n */\nstatic void free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tif (!loaded_vmcs->vmcs)\n\t\treturn;\n\tloaded_vmcs_clear(loaded_vmcs);\n\tfree_vmcs(loaded_vmcs->vmcs);\n\tloaded_vmcs->vmcs = NULL;\n\tif (loaded_vmcs->msr_bitmap)\n\t\tfree_page((unsigned long)loaded_vmcs->msr_bitmap);\n\tWARN_ON(loaded_vmcs->shadow_vmcs != NULL);\n}\n\nstatic struct vmcs *alloc_vmcs(void)\n{\n\treturn alloc_vmcs_cpu(raw_smp_processor_id());\n}\n\nstatic int alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tloaded_vmcs->vmcs = alloc_vmcs();\n\tif (!loaded_vmcs->vmcs)\n\t\treturn -ENOMEM;\n\n\tloaded_vmcs->shadow_vmcs = NULL;\n\tloaded_vmcs_init(loaded_vmcs);\n\n\tif (cpu_has_vmx_msr_bitmap()) {\n\t\tloaded_vmcs->msr_bitmap = (unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!loaded_vmcs->msr_bitmap)\n\t\t\tgoto out_vmcs;\n\t\tmemset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);\n\n\t\tif (static_branch_unlikely(&enable_evmcs) &&\n\t\t    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {\n\t\t\tstruct hv_enlightened_vmcs *evmcs =\n\t\t\t\t(struct hv_enlightened_vmcs *)loaded_vmcs->vmcs;\n\n\t\t\tevmcs->hv_enlightenments_control.msr_bitmap = 1;\n\t\t}\n\t}\n\treturn 0;\n\nout_vmcs:\n\tfree_loaded_vmcs(loaded_vmcs);\n\treturn -ENOMEM;\n}\n\nstatic void free_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfree_vmcs(per_cpu(vmxarea, cpu));\n\t\tper_cpu(vmxarea, cpu) = NULL;\n\t}\n}\n\nenum vmcs_field_width {\n\tVMCS_FIELD_WIDTH_U16 = 0,\n\tVMCS_FIELD_WIDTH_U64 = 1,\n\tVMCS_FIELD_WIDTH_U32 = 2,\n\tVMCS_FIELD_WIDTH_NATURAL_WIDTH = 3\n};\n\nstatic inline int vmcs_field_width(unsigned long field)\n{\n\tif (0x1 & field)\t/* the *_HIGH fields are all 32 bit */\n\t\treturn VMCS_FIELD_WIDTH_U32;\n\treturn (field >> 13) & 0x3 ;\n}\n\nstatic inline int vmcs_field_readonly(unsigned long field)\n{\n\treturn (((field >> 10) & 0x3) == 1);\n}\n\nstatic void init_vmcs_shadow_fields(void)\n{\n\tint i, j;\n\n\tfor (i = j = 0; i < max_shadow_read_only_fields; i++) {\n\t\tu16 field = shadow_read_only_fields[i];\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_only_fields ||\n\t\t     shadow_read_only_fields[i + 1] != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_only_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\tclear_bit(field, vmx_vmread_bitmap);\n#ifdef CONFIG_X86_64\n\t\tif (field & 1)\n\t\t\tcontinue;\n#endif\n\t\tif (j < i)\n\t\t\tshadow_read_only_fields[j] = field;\n\t\tj++;\n\t}\n\tmax_shadow_read_only_fields = j;\n\n\tfor (i = j = 0; i < max_shadow_read_write_fields; i++) {\n\t\tu16 field = shadow_read_write_fields[i];\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_write_fields ||\n\t\t     shadow_read_write_fields[i + 1] != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_write_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\t/*\n\t\t * PML and the preemption timer can be emulated, but the\n\t\t * processor cannot vmwrite to fields that don't exist\n\t\t * on bare metal.\n\t\t */\n\t\tswitch (field) {\n\t\tcase GUEST_PML_INDEX:\n\t\t\tif (!cpu_has_vmx_pml())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase VMX_PREEMPTION_TIMER_VALUE:\n\t\t\tif (!cpu_has_vmx_preemption_timer())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase GUEST_INTR_STATUS:\n\t\t\tif (!cpu_has_vmx_apicv())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tclear_bit(field, vmx_vmwrite_bitmap);\n\t\tclear_bit(field, vmx_vmread_bitmap);\n#ifdef CONFIG_X86_64\n\t\tif (field & 1)\n\t\t\tcontinue;\n#endif\n\t\tif (j < i)\n\t\t\tshadow_read_write_fields[j] = field;\n\t\tj++;\n\t}\n\tmax_shadow_read_write_fields = j;\n}\n\nstatic __init int alloc_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct vmcs *vmcs;\n\n\t\tvmcs = alloc_vmcs_cpu(cpu);\n\t\tif (!vmcs) {\n\t\t\tfree_kvm_area();\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tper_cpu(vmxarea, cpu) = vmcs;\n\t}\n\treturn 0;\n}\n\nstatic void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,\n\t\tstruct kvm_segment *save)\n{\n\tif (!emulate_invalid_guest_state) {\n\t\t/*\n\t\t * CS and SS RPL should be equal during guest entry according\n\t\t * to VMX spec, but in reality it is not always so. Since vcpu\n\t\t * is in the middle of the transition from real mode to\n\t\t * protected mode it is safe to assume that RPL 0 is a good\n\t\t * default value.\n\t\t */\n\t\tif (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)\n\t\t\tsave->selector &= ~SEGMENT_RPL_MASK;\n\t\tsave->dpl = save->selector & SEGMENT_RPL_MASK;\n\t\tsave->s = 1;\n\t}\n\tvmx_set_segment(vcpu, save, seg);\n}\n\nstatic void enter_pmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t/*\n\t * Update real mode segment cache. It may be not up-to-date if sement\n\t * register was written while vcpu was in a guest mode.\n\t */\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 0;\n\n\tvmx_segment_cache_clear(vmx);\n\n\tvmx_set_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tflags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\n\tvmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |\n\t\t\t(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));\n\n\tupdate_exception_bitmap(vcpu);\n\n\tfix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n}\n\nstatic void fix_rmode_seg(int seg, struct kvm_segment *save)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tstruct kvm_segment var = *save;\n\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\n\tif (!emulate_invalid_guest_state) {\n\t\tvar.selector = var.base >> 4;\n\t\tvar.base = var.base & 0xffff0;\n\t\tvar.limit = 0xffff;\n\t\tvar.g = 0;\n\t\tvar.db = 0;\n\t\tvar.present = 1;\n\t\tvar.s = 1;\n\t\tvar.l = 0;\n\t\tvar.unusable = 0;\n\t\tvar.type = 0x3;\n\t\tvar.avl = 0;\n\t\tif (save->base & 0xf)\n\t\t\tprintk_once(KERN_WARNING \"kvm: segment base is not \"\n\t\t\t\t\t\"paragraph aligned when entering \"\n\t\t\t\t\t\"protected mode (seg=%d)\", seg);\n\t}\n\n\tvmcs_write16(sf->selector, var.selector);\n\tvmcs_writel(sf->base, var.base);\n\tvmcs_write32(sf->limit, var.limit);\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));\n}\n\nstatic void enter_rmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);\n\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 1;\n\n\t/*\n\t * Very old userspace does not call KVM_SET_TSS_ADDR before entering\n\t * vcpu. Warn the user that an update is overdue.\n\t */\n\tif (!kvm_vmx->tss_addr)\n\t\tprintk_once(KERN_WARNING \"kvm: KVM_SET_TSS_ADDR need to be \"\n\t\t\t     \"called before entering vcpu\\n\");\n\n\tvmx_segment_cache_clear(vmx);\n\n\tvmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);\n\tvmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tvmx->rmode.save_rflags = flags;\n\n\tflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\tvmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);\n\tupdate_exception_bitmap(vcpu);\n\n\tfix_rmode_seg(VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_rmode_seg(VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_rmode_seg(VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_rmode_seg(VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_rmode_seg(VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n\tfix_rmode_seg(VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n\n\tkvm_mmu_reset_context(vcpu);\n}\n\nstatic void vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr = find_msr_entry(vmx, MSR_EFER);\n\n\tif (!msr)\n\t\treturn;\n\n\t/*\n\t * Force kernel_gs_base reloading before EFER changes, as control\n\t * of this msr depends on is_long_mode().\n\t */\n\tvmx_load_host_state(to_vmx(vcpu));\n\tvcpu->arch.efer = efer;\n\tif (efer & EFER_LMA) {\n\t\tvm_entry_controls_setbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\t\tmsr->data = efer;\n\t} else {\n\t\tvm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\n\t\tmsr->data = efer & ~EFER_LME;\n\t}\n\tsetup_msrs(vmx);\n}\n\n#ifdef CONFIG_X86_64\n\nstatic void enter_lmode(struct kvm_vcpu *vcpu)\n{\n\tu32 guest_tr_ar;\n\n\tvmx_segment_cache_clear(to_vmx(vcpu));\n\n\tguest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);\n\tif ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {\n\t\tpr_debug_ratelimited(\"%s: tss fixup for long mode. \\n\",\n\t\t\t\t     __func__);\n\t\tvmcs_write32(GUEST_TR_AR_BYTES,\n\t\t\t     (guest_tr_ar & ~VMX_AR_TYPE_MASK)\n\t\t\t     | VMX_AR_TYPE_BUSY_64_TSS);\n\t}\n\tvmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);\n}\n\nstatic void exit_lmode(struct kvm_vcpu *vcpu)\n{\n\tvm_entry_controls_clearbit(to_vmx(vcpu), VM_ENTRY_IA32E_MODE);\n\tvmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);\n}\n\n#endif\n\nstatic inline void __vmx_flush_tlb(struct kvm_vcpu *vcpu, int vpid,\n\t\t\t\tbool invalidate_gpa)\n{\n\tif (enable_ept && (invalidate_gpa || !enable_vpid)) {\n\t\tif (!VALID_PAGE(vcpu->arch.mmu.root_hpa))\n\t\t\treturn;\n\t\tept_sync_context(construct_eptp(vcpu, vcpu->arch.mmu.root_hpa));\n\t} else {\n\t\tvpid_sync_context(vpid);\n\t}\n}\n\nstatic void vmx_flush_tlb(struct kvm_vcpu *vcpu, bool invalidate_gpa)\n{\n\t__vmx_flush_tlb(vcpu, to_vmx(vcpu)->vpid, invalidate_gpa);\n}\n\nstatic void vmx_decache_cr0_guest_bits(struct kvm_vcpu *vcpu)\n{\n\tulong cr0_guest_owned_bits = vcpu->arch.cr0_guest_owned_bits;\n\n\tvcpu->arch.cr0 &= ~cr0_guest_owned_bits;\n\tvcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & cr0_guest_owned_bits;\n}\n\nstatic void vmx_decache_cr3(struct kvm_vcpu *vcpu)\n{\n\tif (enable_unrestricted_guest || (enable_ept && is_paging(vcpu)))\n\t\tvcpu->arch.cr3 = vmcs_readl(GUEST_CR3);\n\t__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);\n}\n\nstatic void vmx_decache_cr4_guest_bits(struct kvm_vcpu *vcpu)\n{\n\tulong cr4_guest_owned_bits = vcpu->arch.cr4_guest_owned_bits;\n\n\tvcpu->arch.cr4 &= ~cr4_guest_owned_bits;\n\tvcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & cr4_guest_owned_bits;\n}\n\nstatic void ept_load_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (!test_bit(VCPU_EXREG_PDPTR,\n\t\t      (unsigned long *)&vcpu->arch.regs_dirty))\n\t\treturn;\n\n\tif (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {\n\t\tvmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);\n\t\tvmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);\n\t\tvmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);\n\t\tvmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);\n\t}\n}\n\nstatic void ept_save_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (is_paging(vcpu) && is_pae(vcpu) && !is_long_mode(vcpu)) {\n\t\tmmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);\n\t\tmmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);\n\t\tmmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);\n\t\tmmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\t__set_bit(VCPU_EXREG_PDPTR,\n\t\t  (unsigned long *)&vcpu->arch.regs_avail);\n\t__set_bit(VCPU_EXREG_PDPTR,\n\t\t  (unsigned long *)&vcpu->arch.regs_dirty);\n}\n\nstatic bool nested_guest_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (to_vmx(vcpu)->nested.msrs.secondary_ctls_high &\n\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST &&\n\t    nested_cpu_has2(vmcs12, SECONDARY_EXEC_UNRESTRICTED_GUEST))\n\t\tfixed0 &= ~(X86_CR0_PE | X86_CR0_PG);\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\nstatic bool nested_host_cr0_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr0_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr0_fixed1;\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\nstatic bool nested_cr4_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tu64 fixed0 = to_vmx(vcpu)->nested.msrs.cr4_fixed0;\n\tu64 fixed1 = to_vmx(vcpu)->nested.msrs.cr4_fixed1;\n\n\treturn fixed_bits_valid(val, fixed0, fixed1);\n}\n\n/* No difference in the restrictions on guest and host CR4 in VMX operation. */\n#define nested_guest_cr4_valid\tnested_cr4_valid\n#define nested_host_cr4_valid\tnested_cr4_valid\n\nstatic int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);\n\nstatic void ept_update_paging_mode_cr0(unsigned long *hw_cr0,\n\t\t\t\t\tunsigned long cr0,\n\t\t\t\t\tstruct kvm_vcpu *vcpu)\n{\n\tif (!test_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail))\n\t\tvmx_decache_cr3(vcpu);\n\tif (!(cr0 & X86_CR0_PG)) {\n\t\t/* From paging/starting to nonpaging */\n\t\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) |\n\t\t\t     (CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t      CPU_BASED_CR3_STORE_EXITING));\n\t\tvcpu->arch.cr0 = cr0;\n\t\tvmx_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\t} else if (!is_paging(vcpu)) {\n\t\t/* From nonpaging to paging */\n\t\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t     vmcs_read32(CPU_BASED_VM_EXEC_CONTROL) &\n\t\t\t     ~(CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t       CPU_BASED_CR3_STORE_EXITING));\n\t\tvcpu->arch.cr0 = cr0;\n\t\tvmx_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\t}\n\n\tif (!(cr0 & X86_CR0_WP))\n\t\t*hw_cr0 &= ~X86_CR0_WP;\n}\n\nstatic void vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long hw_cr0;\n\n\thw_cr0 = (cr0 & ~KVM_GUEST_CR0_MASK);\n\tif (enable_unrestricted_guest)\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse {\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON;\n\n\t\tif (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))\n\t\t\tenter_pmode(vcpu);\n\n\t\tif (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))\n\t\t\tenter_rmode(vcpu);\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (vcpu->arch.efer & EFER_LME) {\n\t\tif (!is_paging(vcpu) && (cr0 & X86_CR0_PG))\n\t\t\tenter_lmode(vcpu);\n\t\tif (is_paging(vcpu) && !(cr0 & X86_CR0_PG))\n\t\t\texit_lmode(vcpu);\n\t}\n#endif\n\n\tif (enable_ept && !enable_unrestricted_guest)\n\t\tept_update_paging_mode_cr0(&hw_cr0, cr0, vcpu);\n\n\tvmcs_writel(CR0_READ_SHADOW, cr0);\n\tvmcs_writel(GUEST_CR0, hw_cr0);\n\tvcpu->arch.cr0 = cr0;\n\n\t/* depends on vcpu->arch.cr0 to be set to a new value */\n\tvmx->emulation_required = emulation_required(vcpu);\n}\n\nstatic int get_ept_level(struct kvm_vcpu *vcpu)\n{\n\tif (cpu_has_vmx_ept_5levels() && (cpuid_maxphyaddr(vcpu) > 48))\n\t\treturn 5;\n\treturn 4;\n}\n\nstatic u64 construct_eptp(struct kvm_vcpu *vcpu, unsigned long root_hpa)\n{\n\tu64 eptp = VMX_EPTP_MT_WB;\n\n\teptp |= (get_ept_level(vcpu) == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;\n\n\tif (enable_ept_ad_bits &&\n\t    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))\n\t\teptp |= VMX_EPTP_AD_ENABLE_BIT;\n\teptp |= (root_hpa & PAGE_MASK);\n\n\treturn eptp;\n}\n\nstatic void vmx_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tunsigned long guest_cr3;\n\tu64 eptp;\n\n\tguest_cr3 = cr3;\n\tif (enable_ept) {\n\t\teptp = construct_eptp(vcpu, cr3);\n\t\tvmcs_write64(EPT_POINTER, eptp);\n\t\tif (enable_unrestricted_guest || is_paging(vcpu) ||\n\t\t    is_guest_mode(vcpu))\n\t\t\tguest_cr3 = kvm_read_cr3(vcpu);\n\t\telse\n\t\t\tguest_cr3 = to_kvm_vmx(vcpu->kvm)->ept_identity_map_addr;\n\t\tept_load_pdptrs(vcpu);\n\t}\n\n\tvmx_flush_tlb(vcpu, true);\n\tvmcs_writel(GUEST_CR3, guest_cr3);\n}\n\nstatic int vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\t/*\n\t * Pass through host's Machine Check Enable value to hw_cr4, which\n\t * is in force while we are in guest mode.  Do not let guests control\n\t * this bit, even if host CR4.MCE == 0.\n\t */\n\tunsigned long hw_cr4;\n\n\thw_cr4 = (cr4_read_shadow() & X86_CR4_MCE) | (cr4 & ~X86_CR4_MCE);\n\tif (enable_unrestricted_guest)\n\t\thw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse if (to_vmx(vcpu)->rmode.vm86_active)\n\t\thw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;\n\telse\n\t\thw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;\n\n\tif (!boot_cpu_has(X86_FEATURE_UMIP) && vmx_umip_emulated()) {\n\t\tif (cr4 & X86_CR4_UMIP) {\n\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\tSECONDARY_EXEC_DESC);\n\t\t\thw_cr4 &= ~X86_CR4_UMIP;\n\t\t} else if (!is_guest_mode(vcpu) ||\n\t\t\t!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC))\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_DESC);\n\t}\n\n\tif (cr4 & X86_CR4_VMXE) {\n\t\t/*\n\t\t * To use VMXON (and later other VMX instructions), a guest\n\t\t * must first be able to turn on cr4.VMXE (see handle_vmon()).\n\t\t * So basically the check on whether to allow nested VMX\n\t\t * is here.\n\t\t */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tif (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))\n\t\treturn 1;\n\n\tvcpu->arch.cr4 = cr4;\n\n\tif (!enable_unrestricted_guest) {\n\t\tif (enable_ept) {\n\t\t\tif (!is_paging(vcpu)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t\thw_cr4 |= X86_CR4_PSE;\n\t\t\t} else if (!(cr4 & X86_CR4_PAE)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * SMEP/SMAP/PKU is disabled if CPU is in non-paging mode in\n\t\t * hardware.  To emulate this behavior, SMEP/SMAP/PKU needs\n\t\t * to be manually disabled when guest switches to non-paging\n\t\t * mode.\n\t\t *\n\t\t * If !enable_unrestricted_guest, the CPU is always running\n\t\t * with CR0.PG=1 and CR4 needs to be modified.\n\t\t * If enable_unrestricted_guest, the CPU automatically\n\t\t * disables SMEP/SMAP/PKU when the guest sets CR0.PG=0.\n\t\t */\n\t\tif (!is_paging(vcpu))\n\t\t\thw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);\n\t}\n\n\tvmcs_writel(CR4_READ_SHADOW, cr4);\n\tvmcs_writel(GUEST_CR4, hw_cr4);\n\treturn 0;\n}\n\nstatic void vmx_get_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 ar;\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\t*var = vmx->rmode.segs[seg];\n\t\tif (seg == VCPU_SREG_TR\n\t\t    || var->selector == vmx_read_guest_seg_selector(vmx, seg))\n\t\t\treturn;\n\t\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\t\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\t\treturn;\n\t}\n\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\tvar->limit = vmx_read_guest_seg_limit(vmx, seg);\n\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\tar = vmx_read_guest_seg_ar(vmx, seg);\n\tvar->unusable = (ar >> 16) & 1;\n\tvar->type = ar & 15;\n\tvar->s = (ar >> 4) & 1;\n\tvar->dpl = (ar >> 5) & 3;\n\t/*\n\t * Some userspaces do not preserve unusable property. Since usable\n\t * segment has to be present according to VMX spec we can use present\n\t * property to amend userspace bug by making unusable segment always\n\t * nonpresent. vmx_segment_access_rights() already marks nonpresent\n\t * segment as unusable.\n\t */\n\tvar->present = !var->unusable;\n\tvar->avl = (ar >> 12) & 1;\n\tvar->l = (ar >> 13) & 1;\n\tvar->db = (ar >> 14) & 1;\n\tvar->g = (ar >> 15) & 1;\n}\n\nstatic u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment s;\n\n\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\tvmx_get_segment(vcpu, &s, seg);\n\t\treturn s.base;\n\t}\n\treturn vmx_read_guest_seg_base(to_vmx(vcpu), seg);\n}\n\nstatic int vmx_get_cpl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (unlikely(vmx->rmode.vm86_active))\n\t\treturn 0;\n\telse {\n\t\tint ar = vmx_read_guest_seg_ar(vmx, VCPU_SREG_SS);\n\t\treturn VMX_AR_DPL(ar);\n\t}\n}\n\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var)\n{\n\tu32 ar;\n\n\tif (var->unusable || !var->present)\n\t\tar = 1 << 16;\n\telse {\n\t\tar = var->type & 15;\n\t\tar |= (var->s & 1) << 4;\n\t\tar |= (var->dpl & 3) << 5;\n\t\tar |= (var->present & 1) << 7;\n\t\tar |= (var->avl & 1) << 12;\n\t\tar |= (var->l & 1) << 13;\n\t\tar |= (var->db & 1) << 14;\n\t\tar |= (var->g & 1) << 15;\n\t}\n\n\treturn ar;\n}\n\nstatic void vmx_set_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\n\tvmx_segment_cache_clear(vmx);\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\tvmx->rmode.segs[seg] = *var;\n\t\tif (seg == VCPU_SREG_TR)\n\t\t\tvmcs_write16(sf->selector, var->selector);\n\t\telse if (var->s)\n\t\t\tfix_rmode_seg(seg, &vmx->rmode.segs[seg]);\n\t\tgoto out;\n\t}\n\n\tvmcs_writel(sf->base, var->base);\n\tvmcs_write32(sf->limit, var->limit);\n\tvmcs_write16(sf->selector, var->selector);\n\n\t/*\n\t *   Fix the \"Accessed\" bit in AR field of segment registers for older\n\t * qemu binaries.\n\t *   IA32 arch specifies that at the time of processor reset the\n\t * \"Accessed\" bit in the AR field of segment registers is 1. And qemu\n\t * is setting it to 0 in the userland code. This causes invalid guest\n\t * state vmexit when \"unrestricted guest\" mode is turned on.\n\t *    Fix for this setup issue in cpu_reset is being pushed in the qemu\n\t * tree. Newer qemu binaries with that qemu fix would not need this\n\t * kvm hack.\n\t */\n\tif (enable_unrestricted_guest && (seg != VCPU_SREG_LDTR))\n\t\tvar->type |= 0x1; /* Accessed */\n\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(var));\n\nout:\n\tvmx->emulation_required = emulation_required(vcpu);\n}\n\nstatic void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)\n{\n\tu32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);\n\n\t*db = (ar >> 14) & 1;\n\t*l = (ar >> 13) & 1;\n}\n\nstatic void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_IDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_IDTR_BASE);\n}\n\nstatic void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_IDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_IDTR_BASE, dt->address);\n}\n\nstatic void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_GDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_GDTR_BASE);\n}\n\nstatic void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_GDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_GDTR_BASE, dt->address);\n}\n\nstatic bool rmode_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tu32 ar;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\tar = vmx_segment_access_rights(&var);\n\n\tif (var.base != (var.selector << 4))\n\t\treturn false;\n\tif (var.limit != 0xffff)\n\t\treturn false;\n\tif (ar != 0xf3)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool code_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs;\n\tunsigned int cs_rpl;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs_rpl = cs.selector & SEGMENT_RPL_MASK;\n\n\tif (cs.unusable)\n\t\treturn false;\n\tif (~cs.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_ACCESSES_MASK))\n\t\treturn false;\n\tif (!cs.s)\n\t\treturn false;\n\tif (cs.type & VMX_AR_TYPE_WRITEABLE_MASK) {\n\t\tif (cs.dpl > cs_rpl)\n\t\t\treturn false;\n\t} else {\n\t\tif (cs.dpl != cs_rpl)\n\t\t\treturn false;\n\t}\n\tif (!cs.present)\n\t\treturn false;\n\n\t/* TODO: Add Reserved field check, this'll require a new member in the kvm_segment_field structure */\n\treturn true;\n}\n\nstatic bool stack_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ss;\n\tunsigned int ss_rpl;\n\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\tss_rpl = ss.selector & SEGMENT_RPL_MASK;\n\n\tif (ss.unusable)\n\t\treturn true;\n\tif (ss.type != 3 && ss.type != 7)\n\t\treturn false;\n\tif (!ss.s)\n\t\treturn false;\n\tif (ss.dpl != ss_rpl) /* DPL != RPL */\n\t\treturn false;\n\tif (!ss.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool data_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tunsigned int rpl;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\trpl = var.selector & SEGMENT_RPL_MASK;\n\n\tif (var.unusable)\n\t\treturn true;\n\tif (!var.s)\n\t\treturn false;\n\tif (!var.present)\n\t\treturn false;\n\tif (~var.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_WRITEABLE_MASK)) {\n\t\tif (var.dpl < rpl) /* DPL < RPL */\n\t\t\treturn false;\n\t}\n\n\t/* TODO: Add other members to kvm_segment_field to allow checking for other access\n\t * rights flags\n\t */\n\treturn true;\n}\n\nstatic bool tr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment tr;\n\n\tvmx_get_segment(vcpu, &tr, VCPU_SREG_TR);\n\n\tif (tr.unusable)\n\t\treturn false;\n\tif (tr.selector & SEGMENT_TI_MASK)\t/* TI = 1 */\n\t\treturn false;\n\tif (tr.type != 3 && tr.type != 11) /* TODO: Check if guest is in IA32e mode */\n\t\treturn false;\n\tif (!tr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool ldtr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ldtr;\n\n\tvmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);\n\n\tif (ldtr.unusable)\n\t\treturn true;\n\tif (ldtr.selector & SEGMENT_TI_MASK)\t/* TI = 1 */\n\t\treturn false;\n\tif (ldtr.type != 2)\n\t\treturn false;\n\tif (!ldtr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool cs_ss_rpl_check(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs, ss;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\n\treturn ((cs.selector & SEGMENT_RPL_MASK) ==\n\t\t (ss.selector & SEGMENT_RPL_MASK));\n}\n\n/*\n * Check if guest state is valid. Returns true if valid, false if\n * not.\n * We assume that registers are always usable\n */\nstatic bool guest_state_valid(struct kvm_vcpu *vcpu)\n{\n\tif (enable_unrestricted_guest)\n\t\treturn true;\n\n\t/* real mode guest state checks */\n\tif (!is_protmode(vcpu) || (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_CS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_SS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t} else {\n\t/* protected mode guest state checks */\n\t\tif (!cs_ss_rpl_check(vcpu))\n\t\t\treturn false;\n\t\tif (!code_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!stack_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t\tif (!tr_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!ldtr_valid(vcpu))\n\t\t\treturn false;\n\t}\n\t/* TODO:\n\t * - Add checks on RIP\n\t * - Add checks on RFLAGS\n\t */\n\n\treturn true;\n}\n\nstatic bool page_address_valid(struct kvm_vcpu *vcpu, gpa_t gpa)\n{\n\treturn PAGE_ALIGNED(gpa) && !(gpa >> cpuid_maxphyaddr(vcpu));\n}\n\nstatic int init_rmode_tss(struct kvm *kvm)\n{\n\tgfn_t fn;\n\tu16 data = 0;\n\tint idx, r;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tfn = to_kvm_vmx(kvm)->tss_addr >> PAGE_SHIFT;\n\tr = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tdata = TSS_BASE_SIZE + TSS_REDIRECTION_SIZE;\n\tr = kvm_write_guest_page(kvm, fn++, &data,\n\t\t\tTSS_IOPB_BASE_OFFSET, sizeof(u16));\n\tif (r < 0)\n\t\tgoto out;\n\tr = kvm_clear_guest_page(kvm, fn++, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tr = kvm_clear_guest_page(kvm, fn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\tdata = ~0;\n\tr = kvm_write_guest_page(kvm, fn, &data,\n\t\t\t\t RMODE_TSS_SIZE - 2 * PAGE_SIZE - 1,\n\t\t\t\t sizeof(u8));\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\treturn r;\n}\n\nstatic int init_rmode_identity_map(struct kvm *kvm)\n{\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\tint i, idx, r = 0;\n\tkvm_pfn_t identity_map_pfn;\n\tu32 tmp;\n\n\t/* Protect kvm_vmx->ept_identity_pagetable_done. */\n\tmutex_lock(&kvm->slots_lock);\n\n\tif (likely(kvm_vmx->ept_identity_pagetable_done))\n\t\tgoto out2;\n\n\tif (!kvm_vmx->ept_identity_map_addr)\n\t\tkvm_vmx->ept_identity_map_addr = VMX_EPT_IDENTITY_PAGETABLE_ADDR;\n\tidentity_map_pfn = kvm_vmx->ept_identity_map_addr >> PAGE_SHIFT;\n\n\tr = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t    kvm_vmx->ept_identity_map_addr, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out2;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tr = kvm_clear_guest_page(kvm, identity_map_pfn, 0, PAGE_SIZE);\n\tif (r < 0)\n\t\tgoto out;\n\t/* Set up identity-mapping pagetable for EPT in real mode */\n\tfor (i = 0; i < PT32_ENT_PER_PAGE; i++) {\n\t\ttmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |\n\t\t\t_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);\n\t\tr = kvm_write_guest_page(kvm, identity_map_pfn,\n\t\t\t\t&tmp, i * sizeof(tmp), sizeof(tmp));\n\t\tif (r < 0)\n\t\t\tgoto out;\n\t}\n\tkvm_vmx->ept_identity_pagetable_done = true;\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\nout2:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic void seg_setup(int seg)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tunsigned int ar;\n\n\tvmcs_write16(sf->selector, 0);\n\tvmcs_writel(sf->base, 0);\n\tvmcs_write32(sf->limit, 0xffff);\n\tar = 0x93;\n\tif (seg == VCPU_SREG_CS)\n\t\tar |= 0x08; /* code segment */\n\n\tvmcs_write32(sf->ar_bytes, ar);\n}\n\nstatic int alloc_apic_access_page(struct kvm *kvm)\n{\n\tstruct page *page;\n\tint r = 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tif (kvm->arch.apic_access_page_done)\n\t\tgoto out;\n\tr = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t    APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);\n\tif (r)\n\t\tgoto out;\n\n\tpage = gfn_to_page(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (is_error_page(page)) {\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Do not pin the page in memory, so that memory hot-unplug\n\t * is able to migrate it.\n\t */\n\tput_page(page);\n\tkvm->arch.apic_access_page_done = true;\nout:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic int allocate_vpid(void)\n{\n\tint vpid;\n\n\tif (!enable_vpid)\n\t\treturn 0;\n\tspin_lock(&vmx_vpid_lock);\n\tvpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);\n\tif (vpid < VMX_NR_VPIDS)\n\t\t__set_bit(vpid, vmx_vpid_bitmap);\n\telse\n\t\tvpid = 0;\n\tspin_unlock(&vmx_vpid_lock);\n\treturn vpid;\n}\n\nstatic void free_vpid(int vpid)\n{\n\tif (!enable_vpid || vpid == 0)\n\t\treturn;\n\tspin_lock(&vmx_vpid_lock);\n\t__clear_bit(vpid, vmx_vpid_bitmap);\n\tspin_unlock(&vmx_vpid_lock);\n}\n\nstatic void __always_inline vmx_disable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t  u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_touch_msr_bitmap();\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-low */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-low */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-high */\n\t\t\t__clear_bit(msr, msr_bitmap + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-high */\n\t\t\t__clear_bit(msr, msr_bitmap + 0xc00 / f);\n\n\t}\n}\n\nstatic void __always_inline vmx_enable_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t\t\t\t\t u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tevmcs_touch_msr_bitmap();\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-low */\n\t\t\t__set_bit(msr, msr_bitmap + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-low */\n\t\t\t__set_bit(msr, msr_bitmap + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R)\n\t\t\t/* read-high */\n\t\t\t__set_bit(msr, msr_bitmap + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W)\n\t\t\t/* write-high */\n\t\t\t__set_bit(msr, msr_bitmap + 0xc00 / f);\n\n\t}\n}\n\nstatic void __always_inline vmx_set_intercept_for_msr(unsigned long *msr_bitmap,\n\t\t\t     \t\t\t      u32 msr, int type, bool value)\n{\n\tif (value)\n\t\tvmx_enable_intercept_for_msr(msr_bitmap, msr, type);\n\telse\n\t\tvmx_disable_intercept_for_msr(msr_bitmap, msr, type);\n}\n\n/*\n * If a msr is allowed by L0, we should check whether it is allowed by L1.\n * The corresponding bit will be cleared unless both of L0 and L1 allow it.\n */\nstatic void nested_vmx_disable_intercept_for_msr(unsigned long *msr_bitmap_l1,\n\t\t\t\t\t       unsigned long *msr_bitmap_nested,\n\t\t\t\t\t       u32 msr, int type)\n{\n\tint f = sizeof(unsigned long);\n\n\t/*\n\t * See Intel PRM Vol. 3, 20.6.9 (MSR-Bitmap Address). Early manuals\n\t * have the write-low and read-high bitmap offsets the wrong way round.\n\t * We can control MSRs 0x00000000-0x00001fff and 0xc0000000-0xc0001fff.\n\t */\n\tif (msr <= 0x1fff) {\n\t\tif (type & MSR_TYPE_R &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x000 / f))\n\t\t\t/* read-low */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x000 / f);\n\n\t\tif (type & MSR_TYPE_W &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x800 / f))\n\t\t\t/* write-low */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x800 / f);\n\n\t} else if ((msr >= 0xc0000000) && (msr <= 0xc0001fff)) {\n\t\tmsr &= 0x1fff;\n\t\tif (type & MSR_TYPE_R &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0x400 / f))\n\t\t\t/* read-high */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0x400 / f);\n\n\t\tif (type & MSR_TYPE_W &&\n\t\t   !test_bit(msr, msr_bitmap_l1 + 0xc00 / f))\n\t\t\t/* write-high */\n\t\t\t__clear_bit(msr, msr_bitmap_nested + 0xc00 / f);\n\n\t}\n}\n\nstatic u8 vmx_msr_bitmap_mode(struct kvm_vcpu *vcpu)\n{\n\tu8 mode = 0;\n\n\tif (cpu_has_secondary_exec_ctrls() &&\n\t    (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &\n\t     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {\n\t\tmode |= MSR_BITMAP_MODE_X2APIC;\n\t\tif (enable_apicv && kvm_vcpu_apicv_active(vcpu))\n\t\t\tmode |= MSR_BITMAP_MODE_X2APIC_APICV;\n\t}\n\n\tif (is_long_mode(vcpu))\n\t\tmode |= MSR_BITMAP_MODE_LM;\n\n\treturn mode;\n}\n\n#define X2APIC_MSR(r) (APIC_BASE_MSR + ((r) >> 4))\n\nstatic void vmx_update_msr_bitmap_x2apic(unsigned long *msr_bitmap,\n\t\t\t\t\t u8 mode)\n{\n\tint msr;\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\tunsigned word = msr / BITS_PER_LONG;\n\t\tmsr_bitmap[word] = (mode & MSR_BITMAP_MODE_X2APIC_APICV) ? 0 : ~0;\n\t\tmsr_bitmap[word + (0x800 / sizeof(long))] = ~0;\n\t}\n\n\tif (mode & MSR_BITMAP_MODE_X2APIC) {\n\t\t/*\n\t\t * TPR reads and writes can be virtualized even if virtual interrupt\n\t\t * delivery is not in use.\n\t\t */\n\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW);\n\t\tif (mode & MSR_BITMAP_MODE_X2APIC_APICV) {\n\t\t\tvmx_enable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_R);\n\t\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);\n\t\t\tvmx_disable_intercept_for_msr(msr_bitmap, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);\n\t\t}\n\t}\n}\n\nstatic void vmx_update_msr_bitmap(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;\n\tu8 mode = vmx_msr_bitmap_mode(vcpu);\n\tu8 changed = mode ^ vmx->msr_bitmap_mode;\n\n\tif (!changed)\n\t\treturn;\n\n\tvmx_set_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW,\n\t\t\t\t  !(mode & MSR_BITMAP_MODE_LM));\n\n\tif (changed & (MSR_BITMAP_MODE_X2APIC | MSR_BITMAP_MODE_X2APIC_APICV))\n\t\tvmx_update_msr_bitmap_x2apic(msr_bitmap, mode);\n\n\tvmx->msr_bitmap_mode = mode;\n}\n\nstatic bool vmx_get_enable_apicv(struct kvm_vcpu *vcpu)\n{\n\treturn enable_apicv;\n}\n\nstatic void nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tgfn_t gfn;\n\n\t/*\n\t * Don't need to mark the APIC access page dirty; it is never\n\t * written to by the CPU during APIC virtualization.\n\t */\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tgfn = vmcs12->virtual_apic_page_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tgfn = vmcs12->posted_intr_desc_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n}\n\n\nstatic void vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tvoid *vapic_page;\n\tu16 status;\n\n\tif (!vmx->nested.pi_desc || !vmx->nested.pi_pending)\n\t\treturn;\n\n\tvmx->nested.pi_pending = false;\n\tif (!pi_test_and_clear_on(vmx->nested.pi_desc))\n\t\treturn;\n\n\tmax_irr = find_last_bit((unsigned long *)vmx->nested.pi_desc->pir, 256);\n\tif (max_irr != 256) {\n\t\tvapic_page = kmap(vmx->nested.virtual_apic_page);\n\t\t__kvm_apic_update_irr(vmx->nested.pi_desc->pir,\n\t\t\tvapic_page, &max_irr);\n\t\tkunmap(vmx->nested.virtual_apic_page);\n\n\t\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\t\tif ((u8)max_irr > ((u8)status & 0xff)) {\n\t\t\tstatus &= ~0xff;\n\t\t\tstatus |= (u8)max_irr;\n\t\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t\t}\n\t}\n\n\tnested_mark_vmcs12_pages_dirty(vcpu);\n}\n\nstatic inline bool kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     bool nested)\n{\n#ifdef CONFIG_SMP\n\tint pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;\n\n\tif (vcpu->mode == IN_GUEST_MODE) {\n\t\t/*\n\t\t * The vector of interrupt to be delivered to vcpu had\n\t\t * been set in PIR before this function.\n\t\t *\n\t\t * Following cases will be reached in this block, and\n\t\t * we always send a notification event in all cases as\n\t\t * explained below.\n\t\t *\n\t\t * Case 1: vcpu keeps in non-root mode. Sending a\n\t\t * notification event posts the interrupt to vcpu.\n\t\t *\n\t\t * Case 2: vcpu exits to root mode and is still\n\t\t * runnable. PIR will be synced to vIRR before the\n\t\t * next vcpu entry. Sending a notification event in\n\t\t * this case has no effect, as vcpu is not in root\n\t\t * mode.\n\t\t *\n\t\t * Case 3: vcpu exits to root mode and is blocked.\n\t\t * vcpu_block() has already synced PIR to vIRR and\n\t\t * never blocks vcpu if vIRR is not cleared. Therefore,\n\t\t * a blocked vcpu here does not wait for any requested\n\t\t * interrupts in PIR, and sending a notification event\n\t\t * which has no effect is safe here.\n\t\t */\n\n\t\tapic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\nstatic int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tint vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t    vector == vmx->nested.posted_intr_nv) {\n\t\t/*\n\t\t * If a posted intr is not recognized by hardware,\n\t\t * we will accomplish it in the next vmentry.\n\t\t */\n\t\tvmx->nested.pi_pending = true;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t/* the PIR and ON have been set by L1. */\n\t\tif (!kvm_vcpu_trigger_posted_interrupt(vcpu, true))\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\treturn 0;\n\t}\n\treturn -1;\n}\n/*\n * Send interrupt to vcpu via posted interrupt way.\n * 1. If target vcpu is running(non-root mode), send posted interrupt\n * notification to vcpu and hardware will sync PIR to vIRR atomically.\n * 2. If target vcpu isn't running(root mode), kick it to pick up the\n * interrupt from PIR in next vmentry.\n */\nstatic void vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint r;\n\n\tr = vmx_deliver_nested_posted_interrupt(vcpu, vector);\n\tif (!r)\n\t\treturn;\n\n\tif (pi_test_and_set_pir(vector, &vmx->pi_desc))\n\t\treturn;\n\n\t/* If a previous notification has sent the IPI, nothing to do.  */\n\tif (pi_test_and_set_on(&vmx->pi_desc))\n\t\treturn;\n\n\tif (!kvm_vcpu_trigger_posted_interrupt(vcpu, false))\n\t\tkvm_vcpu_kick(vcpu);\n}\n\n/*\n * Set up the vmcs's constant host-state fields, i.e., host-state fields that\n * will not change in the lifetime of the guest.\n * Note that host-state that does change is set elsewhere. E.g., host-state\n * that is set differently for each CPU is set in vmx_vcpu_load(), not here.\n */\nstatic void vmx_set_constant_host_state(struct vcpu_vmx *vmx)\n{\n\tu32 low32, high32;\n\tunsigned long tmpl;\n\tstruct desc_ptr dt;\n\tunsigned long cr0, cr3, cr4;\n\n\tcr0 = read_cr0();\n\tWARN_ON(cr0 & X86_CR0_TS);\n\tvmcs_writel(HOST_CR0, cr0);  /* 22.2.3 */\n\n\t/*\n\t * Save the most likely value for this task's CR3 in the VMCS.\n\t * We can't use __get_current_cr3_fast() because we're not atomic.\n\t */\n\tcr3 = __read_cr3();\n\tvmcs_writel(HOST_CR3, cr3);\t\t/* 22.2.3  FIXME: shadow tables */\n\tvmx->loaded_vmcs->vmcs_host_cr3 = cr3;\n\n\t/* Save the most likely value for this task's CR4 in the VMCS. */\n\tcr4 = cr4_read_shadow();\n\tvmcs_writel(HOST_CR4, cr4);\t\t\t/* 22.2.3, 22.2.5 */\n\tvmx->loaded_vmcs->vmcs_host_cr4 = cr4;\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */\n#ifdef CONFIG_X86_64\n\t/*\n\t * Load null selectors, so we can avoid reloading them in\n\t * __vmx_load_host_state(), in case userspace uses the null selectors\n\t * too (the expected case).\n\t */\n\tvmcs_write16(HOST_DS_SELECTOR, 0);\n\tvmcs_write16(HOST_ES_SELECTOR, 0);\n#else\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n#endif\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */\n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */\n\n\tstore_idt(&dt);\n\tvmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */\n\tvmx->host_idt_base = dt.address;\n\n\tvmcs_writel(HOST_RIP, vmx_return); /* 22.2.5 */\n\n\trdmsr(MSR_IA32_SYSENTER_CS, low32, high32);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, low32);\n\trdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */\n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, low32, high32);\n\t\tvmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));\n\t}\n}\n\nstatic void set_cr4_guest_host_mask(struct vcpu_vmx *vmx)\n{\n\tvmx->vcpu.arch.cr4_guest_owned_bits = KVM_CR4_GUEST_OWNED_BITS;\n\tif (enable_ept)\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits |= X86_CR4_PGE;\n\tif (is_guest_mode(&vmx->vcpu))\n\t\tvmx->vcpu.arch.cr4_guest_owned_bits &=\n\t\t\t~get_vmcs12(&vmx->vcpu)->cr4_guest_host_mask;\n\tvmcs_writel(CR4_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr4_guest_owned_bits);\n}\n\nstatic u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)\n{\n\tu32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;\n\n\tif (!kvm_vcpu_apicv_active(&vmx->vcpu))\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;\n\n\tif (!enable_vnmi)\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;\n\n\t/* Enable the preemption timer dynamically */\n\tpin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\treturn pin_based_exec_ctrl;\n}\n\nstatic void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tif (kvm_vcpu_apicv_active(vcpu))\n\t\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t      SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t\telse\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t}\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(vcpu);\n}\n\nstatic u32 vmx_exec_control(struct vcpu_vmx *vmx)\n{\n\tu32 exec_control = vmcs_config.cpu_based_exec_ctrl;\n\n\tif (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)\n\t\texec_control &= ~CPU_BASED_MOV_DR_EXITING;\n\n\tif (!cpu_need_tpr_shadow(&vmx->vcpu)) {\n\t\texec_control &= ~CPU_BASED_TPR_SHADOW;\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR8_LOAD_EXITING;\n#endif\n\t}\n\tif (!enable_ept)\n\t\texec_control |= CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR3_LOAD_EXITING  |\n\t\t\t\tCPU_BASED_INVLPG_EXITING;\n\tif (kvm_mwait_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~(CPU_BASED_MWAIT_EXITING |\n\t\t\t\tCPU_BASED_MONITOR_EXITING);\n\tif (kvm_hlt_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~CPU_BASED_HLT_EXITING;\n\treturn exec_control;\n}\n\nstatic bool vmx_rdrand_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDRAND_EXITING;\n}\n\nstatic bool vmx_rdseed_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_RDSEED_EXITING;\n}\n\nstatic void vmx_compute_secondary_exec_control(struct vcpu_vmx *vmx)\n{\n\tstruct kvm_vcpu *vcpu = &vmx->vcpu;\n\n\tu32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;\n\n\tif (!cpu_need_virtualize_apic_accesses(vcpu))\n\t\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\tif (vmx->vpid == 0)\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_VPID;\n\tif (!enable_ept) {\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_EPT;\n\t\tenable_unrestricted_guest = 0;\n\t\t/* Enable INVPCID for non-ept guests may cause performance regression. */\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;\n\t}\n\tif (!enable_unrestricted_guest)\n\t\texec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\tif (kvm_pause_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\texec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\n\t/* SECONDARY_EXEC_DESC is enabled/disabled on writes to CR4.UMIP,\n\t * in vmx_set_cr4.  */\n\texec_control &= ~SECONDARY_EXEC_DESC;\n\n\t/* SECONDARY_EXEC_SHADOW_VMCS is enabled when L1 executes VMPTRLD\n\t   (handle_vmptrld).\n\t   We can NOT enable shadow_vmcs here because we don't have yet\n\t   a current VMCS12\n\t*/\n\texec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\tif (!enable_pml)\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\tif (vmx_xsaves_supported()) {\n\t\t/* Exposing XSAVES only when XSAVE is exposed */\n\t\tbool xsaves_enabled =\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_XSAVE) &&\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_XSAVES);\n\n\t\tif (!xsaves_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_XSAVES;\n\n\t\tif (nested) {\n\t\t\tif (xsaves_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_XSAVES;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_XSAVES;\n\t\t}\n\t}\n\n\tif (vmx_rdtscp_supported()) {\n\t\tbool rdtscp_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP);\n\t\tif (!rdtscp_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDTSCP;\n\n\t\tif (nested) {\n\t\t\tif (rdtscp_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDTSCP;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDTSCP;\n\t\t}\n\t}\n\n\tif (vmx_invpcid_supported()) {\n\t\t/* Exposing INVPCID only when PCID is exposed */\n\t\tbool invpcid_enabled =\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_INVPCID) &&\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_PCID);\n\n\t\tif (!invpcid_enabled) {\n\t\t\texec_control &= ~SECONDARY_EXEC_ENABLE_INVPCID;\n\t\t\tguest_cpuid_clear(vcpu, X86_FEATURE_INVPCID);\n\t\t}\n\n\t\tif (nested) {\n\t\t\tif (invpcid_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_ENABLE_INVPCID;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_ENABLE_INVPCID;\n\t\t}\n\t}\n\n\tif (vmx_rdrand_supported()) {\n\t\tbool rdrand_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDRAND);\n\t\tif (rdrand_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDRAND_EXITING;\n\n\t\tif (nested) {\n\t\t\tif (rdrand_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDRAND_EXITING;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDRAND_EXITING;\n\t\t}\n\t}\n\n\tif (vmx_rdseed_supported()) {\n\t\tbool rdseed_enabled = guest_cpuid_has(vcpu, X86_FEATURE_RDSEED);\n\t\tif (rdseed_enabled)\n\t\t\texec_control &= ~SECONDARY_EXEC_RDSEED_EXITING;\n\n\t\tif (nested) {\n\t\t\tif (rdseed_enabled)\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high |=\n\t\t\t\t\tSECONDARY_EXEC_RDSEED_EXITING;\n\t\t\telse\n\t\t\t\tvmx->nested.msrs.secondary_ctls_high &=\n\t\t\t\t\t~SECONDARY_EXEC_RDSEED_EXITING;\n\t\t}\n\t}\n\n\tvmx->secondary_exec_control = exec_control;\n}\n\nstatic void ept_set_mmio_spte_mask(void)\n{\n\t/*\n\t * EPT Misconfigurations can be generated if the value of bits 2:0\n\t * of an EPT paging-structure entry is 110b (write/execute).\n\t */\n\tkvm_mmu_set_mmio_spte_mask(VMX_EPT_RWX_MASK,\n\t\t\t\t   VMX_EPT_MISCONFIG_WX_VALUE);\n}\n\n#define VMX_XSS_EXIT_BITMAP 0\n/*\n * Sets up the vmcs for emulated real mode.\n */\nstatic void vmx_vcpu_setup(struct vcpu_vmx *vmx)\n{\n#ifdef CONFIG_X86_64\n\tunsigned long a;\n#endif\n\tint i;\n\n\tif (enable_shadow_vmcs) {\n\t\t/*\n\t\t * At vCPU creation, \"VMWRITE to any supported field\n\t\t * in the VMCS\" is supported, so use the more\n\t\t * permissive vmx_vmread_bitmap to specify both read\n\t\t * and write permissions for the shadow VMCS.\n\t\t */\n\t\tvmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));\n\t\tvmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmread_bitmap));\n\t}\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));\n\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */\n\n\t/* Control */\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));\n\tvmx->hv_deadline_tsc = -1;\n\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, vmx_exec_control(vmx));\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tvmx_compute_secondary_exec_control(vmx);\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t     vmx->secondary_exec_control);\n\t}\n\n\tif (kvm_vcpu_apicv_active(&vmx->vcpu)) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, 0);\n\n\t\tvmcs_write16(GUEST_INTR_STATUS, 0);\n\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));\n\t}\n\n\tif (!kvm_pause_in_guest(vmx->vcpu.kvm)) {\n\t\tvmcs_write32(PLE_GAP, ple_gap);\n\t\tvmx->ple_window = ple_window;\n\t\tvmx->ple_window_dirty = true;\n\t}\n\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);\n\tvmcs_write32(CR3_TARGET_COUNT, 0);           /* 22.2.1 */\n\n\tvmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */\n\tvmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */\n\tvmx_set_constant_host_state(vmx);\n#ifdef CONFIG_X86_64\n\trdmsrl(MSR_FS_BASE, a);\n\tvmcs_writel(HOST_FS_BASE, a); /* 22.2.4 */\n\trdmsrl(MSR_GS_BASE, a);\n\tvmcs_writel(HOST_GS_BASE, a); /* 22.2.4 */\n#else\n\tvmcs_writel(HOST_FS_BASE, 0); /* 22.2.4 */\n\tvmcs_writel(HOST_GS_BASE, 0); /* 22.2.4 */\n#endif\n\n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i) {\n\t\tu32 index = vmx_msr_index[i];\n\t\tu32 data_low, data_high;\n\t\tint j = vmx->nmsrs;\n\n\t\tif (rdmsr_safe(index, &data_low, &data_high) < 0)\n\t\t\tcontinue;\n\t\tif (wrmsr_safe(index, data_low, data_high) < 0)\n\t\t\tcontinue;\n\t\tvmx->guest_msrs[j].index = i;\n\t\tvmx->guest_msrs[j].data = 0;\n\t\tvmx->guest_msrs[j].mask = -1ull;\n\t\t++vmx->nmsrs;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_CAPABILITIES))\n\t\trdmsrl(MSR_IA32_ARCH_CAPABILITIES, vmx->arch_capabilities);\n\n\tvm_exit_controls_init(vmx, vmcs_config.vmexit_ctrl);\n\n\t/* 22.2.1, 20.8.1 */\n\tvm_entry_controls_init(vmx, vmcs_config.vmentry_ctrl);\n\n\tvmx->vcpu.arch.cr0_guest_owned_bits = X86_CR0_TS;\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tif (vmx_xsaves_supported())\n\t\tvmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);\n\n\tif (enable_pml) {\n\t\tASSERT(vmx->pml_pg);\n\t\tvmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));\n\t\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n\t}\n}\n\nstatic void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct msr_data apic_base_msr;\n\tu64 cr0;\n\n\tvmx->rmode.vm86_active = 0;\n\tvmx->spec_ctrl = 0;\n\n\tvcpu->arch.microcode_version = 0x100000000ULL;\n\tvmx->vcpu.arch.regs[VCPU_REGS_RDX] = get_rdx_init_val();\n\tkvm_set_cr8(vcpu, 0);\n\n\tif (!init_event) {\n\t\tapic_base_msr.data = APIC_DEFAULT_PHYS_BASE |\n\t\t\t\t     MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tapic_base_msr.data |= MSR_IA32_APICBASE_BSP;\n\t\tapic_base_msr.host_initiated = true;\n\t\tkvm_set_apic_base(vcpu, &apic_base_msr);\n\t}\n\n\tvmx_segment_cache_clear(vmx);\n\n\tseg_setup(VCPU_SREG_CS);\n\tvmcs_write16(GUEST_CS_SELECTOR, 0xf000);\n\tvmcs_writel(GUEST_CS_BASE, 0xffff0000ul);\n\n\tseg_setup(VCPU_SREG_DS);\n\tseg_setup(VCPU_SREG_ES);\n\tseg_setup(VCPU_SREG_FS);\n\tseg_setup(VCPU_SREG_GS);\n\tseg_setup(VCPU_SREG_SS);\n\n\tvmcs_write16(GUEST_TR_SELECTOR, 0);\n\tvmcs_writel(GUEST_TR_BASE, 0);\n\tvmcs_write32(GUEST_TR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tvmcs_write16(GUEST_LDTR_SELECTOR, 0);\n\tvmcs_writel(GUEST_LDTR_BASE, 0);\n\tvmcs_write32(GUEST_LDTR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);\n\n\tif (!init_event) {\n\t\tvmcs_write32(GUEST_SYSENTER_CS, 0);\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, 0);\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, 0);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\t}\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0xfff0);\n\n\tvmcs_writel(GUEST_GDTR_BASE, 0);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xffff);\n\n\tvmcs_writel(GUEST_IDTR_BASE, 0);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xffff);\n\n\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);\n\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);\n\tif (kvm_mpx_supported())\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tsetup_msrs(vmx);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);  /* 22.2.1 */\n\n\tif (cpu_has_vmx_tpr_shadow() && !init_event) {\n\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);\n\t\tif (cpu_need_tpr_shadow(vcpu))\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR,\n\t\t\t\t     __pa(vcpu->arch.apic->regs));\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t}\n\n\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\tif (vmx->vpid != 0)\n\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\n\tcr0 = X86_CR0_NW | X86_CR0_CD | X86_CR0_ET;\n\tvmx->vcpu.arch.cr0 = cr0;\n\tvmx_set_cr0(vcpu, cr0); /* enter rmode */\n\tvmx_set_cr4(vcpu, 0);\n\tvmx_set_efer(vcpu, 0);\n\n\tupdate_exception_bitmap(vcpu);\n\n\tvpid_sync_context(vmx->vpid);\n\tif (init_event)\n\t\tvmx_clear_hlt(vcpu);\n}\n\n/*\n * In nested virtualization, check if L1 asked to exit on external interrupts.\n * For most existing hypervisors, this will always return true.\n */\nstatic bool nested_exit_on_intr(struct kvm_vcpu *vcpu)\n{\n\treturn get_vmcs12(vcpu)->pin_based_vm_exec_control &\n\t\tPIN_BASED_EXT_INTR_MASK;\n}\n\n/*\n * In nested virtualization, check if L1 has set\n * VM_EXIT_ACK_INTR_ON_EXIT\n */\nstatic bool nested_exit_intr_ack_set(struct kvm_vcpu *vcpu)\n{\n\treturn get_vmcs12(vcpu)->vm_exit_controls &\n\t\tVM_EXIT_ACK_INTR_ON_EXIT;\n}\n\nstatic bool nested_exit_on_nmi(struct kvm_vcpu *vcpu)\n{\n\treturn nested_cpu_has_nmi_exiting(get_vmcs12(vcpu));\n}\n\nstatic void enable_irq_window(struct kvm_vcpu *vcpu)\n{\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_INTR_PENDING);\n}\n\nstatic void enable_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tif (!enable_vnmi ||\n\t    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {\n\t\tenable_irq_window(vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_NMI_PENDING);\n}\n\nstatic void vmx_inject_irq(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tuint32_t intr;\n\tint irq = vcpu->arch.interrupt.nr;\n\n\ttrace_kvm_inj_virq(irq);\n\n\t++vcpu->stat.irq_injections;\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (vcpu->arch.interrupt.soft)\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tif (kvm_inject_realmode_interrupt(vcpu, irq, inc_eip) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\tintr = irq | INTR_INFO_VALID_MASK;\n\tif (vcpu->arch.interrupt.soft) {\n\t\tintr |= INTR_TYPE_SOFT_INTR;\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t} else\n\t\tintr |= INTR_TYPE_EXT_INTR;\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic void vmx_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\t/*\n\t\t * Tracking the NMI-blocked state in software is built upon\n\t\t * finding the next open IRQ window. This, in turn, depends on\n\t\t * well-behaving guests: They have to keep IRQs disabled at\n\t\t * least as long as the NMI handler runs. Otherwise we may\n\t\t * cause NMI nesting, maybe breaking the guest. But as this is\n\t\t * highly unlikely, we can live with the residual risk.\n\t\t */\n\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 1;\n\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t}\n\n\t++vcpu->stat.nmi_injections;\n\tvmx->loaded_vmcs->nmi_known_unmasked = false;\n\n\tif (vmx->rmode.vm86_active) {\n\t\tif (kvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0) != EMULATE_DONE)\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic bool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool masked;\n\n\tif (!enable_vnmi)\n\t\treturn vmx->loaded_vmcs->soft_vnmi_blocked;\n\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\treturn false;\n\tmasked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;\n\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\treturn masked;\n}\n\nstatic void vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = masked;\n\t\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t\t}\n\t} else {\n\t\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\t\tif (masked)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t\tGUEST_INTR_STATE_NMI);\n\t}\n}\n\nstatic int vmx_nmi_allowed(struct kvm_vcpu *vcpu)\n{\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn 0;\n\n\tif (!enable_vnmi &&\n\t    to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)\n\t\treturn 0;\n\n\treturn\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t  (GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI\n\t\t   | GUEST_INTR_STATE_NMI));\n}\n\nstatic int vmx_interrupt_allowed(struct kvm_vcpu *vcpu)\n{\n\treturn (!to_vmx(vcpu)->nested.nested_run_pending &&\n\t\tvmcs_readl(GUEST_RFLAGS) & X86_EFLAGS_IF) &&\n\t\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t\t(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));\n}\n\nstatic int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)\n{\n\tint ret;\n\n\tif (enable_unrestricted_guest)\n\t\treturn 0;\n\n\tret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,\n\t\t\t\t    PAGE_SIZE * 3);\n\tif (ret)\n\t\treturn ret;\n\tto_kvm_vmx(kvm)->tss_addr = addr;\n\treturn init_rmode_tss(kvm);\n}\n\nstatic int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)\n{\n\tto_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;\n\treturn 0;\n}\n\nstatic bool rmode_exception(struct kvm_vcpu *vcpu, int vec)\n{\n\tswitch (vec) {\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject the exception\n\t\t * from user space while in guest debugging mode.\n\t\t */\n\t\tto_vmx(vcpu)->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\t/* fall through */\n\tcase DB_VECTOR:\n\t\tif (vcpu->guest_debug &\n\t\t\t(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\t/* fall through */\n\tcase DE_VECTOR:\n\tcase OF_VECTOR:\n\tcase BR_VECTOR:\n\tcase UD_VECTOR:\n\tcase DF_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\tcase MF_VECTOR:\n\t\treturn true;\n\tbreak;\n\t}\n\treturn false;\n}\n\nstatic int handle_rmode_exception(struct kvm_vcpu *vcpu,\n\t\t\t\t  int vec, u32 err_code)\n{\n\t/*\n\t * Instruction with address size override prefix opcode 0x67\n\t * Cause the #SS fault with 0 error code in VM86 mode.\n\t */\n\tif (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {\n\t\tif (emulate_instruction(vcpu, 0) == EMULATE_DONE) {\n\t\t\tif (vcpu->arch.halt_request) {\n\t\t\t\tvcpu->arch.halt_request = 0;\n\t\t\t\treturn kvm_vcpu_halt(vcpu);\n\t\t\t}\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Forward all other exceptions that are valid in real mode.\n\t * FIXME: Breaks guest debugging in real mode, needs to be fixed with\n\t *        the required debugging infrastructure rework.\n\t */\n\tkvm_queue_exception(vcpu, vec);\n\treturn 1;\n}\n\n/*\n * Trigger machine check on the host. We assume all the MSRs are already set up\n * by the CPU and that we still run on the same CPU as the MCE occurred on.\n * We pass a fake environment to the machine check handler because we want\n * the guest to be always treated like user space, no matter what context\n * it used internally.\n */\nstatic void kvm_machine_check(void)\n{\n#if defined(CONFIG_X86_MCE) && defined(CONFIG_X86_64)\n\tstruct pt_regs regs = {\n\t\t.cs = 3, /* Fake ring 3 no matter what the guest ran on */\n\t\t.flags = X86_EFLAGS_IF,\n\t};\n\n\tdo_machine_check(&regs, 0);\n#endif\n}\n\nstatic int handle_machine_check(struct kvm_vcpu *vcpu)\n{\n\t/* already handled by vcpu_run */\n\treturn 1;\n}\n\nstatic int handle_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, rip, dr6;\n\tu32 vect_info;\n\tenum emulation_result er;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx->exit_intr_info;\n\n\tif (is_machine_check(intr_info))\n\t\treturn handle_machine_check(vcpu);\n\n\tif (is_nmi(intr_info))\n\t\treturn 1;  /* already handled by vmx_vcpu_run() */\n\n\tif (is_invalid_opcode(intr_info))\n\t\treturn handle_ud(vcpu);\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\tif (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {\n\t\tWARN_ON_ONCE(!enable_vmware_backdoor);\n\t\ter = emulate_instruction(vcpu,\n\t\t\tEMULTYPE_VMWARE | EMULTYPE_NO_UD_ON_FAIL);\n\t\tif (er == EMULATE_USER_EXIT)\n\t\t\treturn 0;\n\t\telse if (er != EMULATE_DONE)\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, error_code);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * The #PF with PFEC.RSVD = 1 indicates the guest is accessing\n\t * MMIO, it is better to report an internal error.\n\t * See the comments in vmx_handle_exit.\n\t */\n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\tcr2 = vmcs_readl(EXIT_QUALIFICATION);\n\t\t/* EPT won't cause page fault directly */\n\t\tWARN_ON_ONCE(!vcpu->arch.apf.host_apf_reason && enable_ept);\n\t\treturn kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase AC_VECTOR:\n\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\treturn 1;\n\tcase DB_VECTOR:\n\t\tdr6 = vmcs_readl(EXIT_QUALIFICATION);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= dr6 | DR6_RTM;\n\t\t\tif (is_icebp(intr_info))\n\t\t\t\tskip_emulated_instruction(vcpu);\n\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_FIXED_1;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\t/* fall through */\n\tcase BP_VECTOR:\n\t\t/*\n\t\t * Update instruction length as we may reinject #BP from\n\t\t * user space while in guest debugging mode. Reading it for\n\t\t * #DB as well causes no harm, it is not used in that case.\n\t\t */\n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\trip = kvm_rip_read(vcpu);\n\t\tkvm_run->debug.arch.pc = vmcs_readl(GUEST_CS_BASE) + rip;\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int handle_external_interrupt(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.irq_exits;\n\treturn 1;\n}\n\nstatic int handle_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\tvcpu->mmio_needed = 0;\n\treturn 0;\n}\n\nstatic int handle_io(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint size, in, string;\n\tunsigned port;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tstring = (exit_qualification & 16) != 0;\n\n\t++vcpu->stat.io_exits;\n\n\tif (string)\n\t\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\tin = (exit_qualification & 8) != 0;\n\n\treturn kvm_fast_pio(vcpu, size, port, in);\n}\n\nstatic void\nvmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)\n{\n\t/*\n\t * Patch in the VMCALL instruction:\n\t */\n\thypercall[0] = 0x0f;\n\thypercall[1] = 0x01;\n\thypercall[2] = 0xc1;\n}\n\n/* called to set cr0 as appropriate for a mov-to-cr0 exit. */\nstatic int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t/*\n\t\t * We get here when L2 changed cr0 in a way that did not change\n\t\t * any of L1's shadowed bits (see nested_vmx_exit_handled_cr),\n\t\t * but did change L0 shadowed bits. So we first calculate the\n\t\t * effective cr0 value that L1 would like to write into the\n\t\t * hardware. It consists of the L2-owned bits from the new\n\t\t * value combined with the L1-owned bits from L1's guest_cr0.\n\t\t */\n\t\tval = (val & ~vmcs12->cr0_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);\n\n\t\tif (!nested_guest_cr0_valid(vcpu, val))\n\t\t\treturn 1;\n\n\t\tif (kvm_set_cr0(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR0_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else {\n\t\tif (to_vmx(vcpu)->nested.vmxon &&\n\t\t    !nested_host_cr0_valid(vcpu, val))\n\t\t\treturn 1;\n\n\t\treturn kvm_set_cr0(vcpu, val);\n\t}\n}\n\nstatic int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t/* analogously to handle_set_cr0 */\n\t\tval = (val & ~vmcs12->cr4_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);\n\t\tif (kvm_set_cr4(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR4_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else\n\t\treturn kvm_set_cr4(vcpu, val);\n}\n\nstatic int handle_desc(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON(!(vcpu->arch.cr4 & X86_CR4_UMIP));\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_cr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification, val;\n\tint cr;\n\tint reg;\n\tint err;\n\tint ret;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tcr = exit_qualification & 15;\n\treg = (exit_qualification >> 8) & 15;\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\terr = handle_set_cr0(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 4:\n\t\t\terr = handle_set_cr4(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 8: {\n\t\t\t\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t\t\t\tu8 cr8 = (u8)val;\n\t\t\t\terr = kvm_set_cr8(vcpu, cr8);\n\t\t\t\tret = kvm_complete_insn_gp(vcpu, err);\n\t\t\t\tif (lapic_in_kernel(vcpu))\n\t\t\t\t\treturn ret;\n\t\t\t\tif (cr8_prev <= cr8)\n\t\t\t\t\treturn ret;\n\t\t\t\t/*\n\t\t\t\t * TODO: we might be squashing a\n\t\t\t\t * KVM_GUESTDBG_SINGLESTEP-triggered\n\t\t\t\t * KVM_EXIT_DEBUG here.\n\t\t\t\t */\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tWARN_ONCE(1, \"Guest should always own CR0.TS\");\n\t\tvmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));\n\t\ttrace_kvm_cr_write(0, kvm_read_cr0(vcpu));\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tcase 1: /*mov from cr*/\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\ttrace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);\n\t\tkvm_lmsw(vcpu, val);\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\tvcpu->run->exit_reason = 0;\n\tvcpu_unimpl(vcpu, \"unhandled control register: op %d cr %d\\n\",\n\t       (int)(exit_qualification >> 4) & 3, cr);\n\treturn 0;\n}\n\nstatic int handle_dr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint dr, dr7, reg;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tdr = exit_qualification & DEBUG_REG_ACCESS_NUM;\n\n\t/* First, if DR does not exist, trigger UD */\n\tif (!kvm_require_dr(vcpu, dr))\n\t\treturn 1;\n\n\t/* Do not handle if the CPL > 0, will trigger GP on re-entry */\n\tif (!kvm_require_cpl(vcpu, 0))\n\t\treturn 1;\n\tdr7 = vmcs_readl(GUEST_DR7);\n\tif (dr7 & DR7_GD) {\n\t\t/*\n\t\t * As the vm-exit takes precedence over the debug trap, we\n\t\t * need to emulate the latter, either for the host or the\n\t\t * guest debugging itself.\n\t\t */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\t\tvcpu->run->debug.arch.dr6 = vcpu->arch.dr6;\n\t\t\tvcpu->run->debug.arch.dr7 = dr7;\n\t\t\tvcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\t\tvcpu->run->debug.arch.exception = DB_VECTOR;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= DR6_BD | DR6_RTM;\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (vcpu->guest_debug == 0) {\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_MOV_DR_EXITING);\n\n\t\t/*\n\t\t * No more DR vmexits; force a reload of the debug registers\n\t\t * and reenter on this instruction.  The next vmexit will\n\t\t * retrieve the full state of the debug registers.\n\t\t */\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\treg = DEBUG_REG_ACCESS_REG(exit_qualification);\n\tif (exit_qualification & TYPE_MOV_FROM_DR) {\n\t\tunsigned long val;\n\n\t\tif (kvm_get_dr(vcpu, dr, &val))\n\t\t\treturn 1;\n\t\tkvm_register_write(vcpu, reg, val);\n\t} else\n\t\tif (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))\n\t\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic u64 vmx_get_dr6(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.dr6;\n}\n\nstatic void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)\n{\n}\n\nstatic void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)\n{\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\tget_debugreg(vcpu->arch.dr6, 6);\n\tvcpu->arch.dr7 = vmcs_readl(GUEST_DR7);\n\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);\n}\n\nstatic void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tvmcs_writel(GUEST_DR7, val);\n}\n\nstatic int handle_cpuid(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_cpuid(vcpu);\n}\n\nstatic int handle_rdmsr(struct kvm_vcpu *vcpu)\n{\n\tu32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];\n\tstruct msr_data msr_info;\n\n\tmsr_info.index = ecx;\n\tmsr_info.host_initiated = false;\n\tif (vmx_get_msr(vcpu, &msr_info)) {\n\t\ttrace_kvm_msr_read_ex(ecx);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_read(ecx, msr_info.data);\n\n\t/* FIXME: handling of bits 32:63 of rax, rdx */\n\tvcpu->arch.regs[VCPU_REGS_RAX] = msr_info.data & -1u;\n\tvcpu->arch.regs[VCPU_REGS_RDX] = (msr_info.data >> 32) & -1u;\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_wrmsr(struct kvm_vcpu *vcpu)\n{\n\tstruct msr_data msr;\n\tu32 ecx = vcpu->arch.regs[VCPU_REGS_RCX];\n\tu64 data = (vcpu->arch.regs[VCPU_REGS_RAX] & -1u)\n\t\t| ((u64)(vcpu->arch.regs[VCPU_REGS_RDX] & -1u) << 32);\n\n\tmsr.data = data;\n\tmsr.index = ecx;\n\tmsr.host_initiated = false;\n\tif (kvm_set_msr(vcpu, &msr) != 0) {\n\t\ttrace_kvm_msr_write_ex(ecx, data);\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\ttrace_kvm_msr_write(ecx, data);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)\n{\n\tkvm_apic_update_ppr(vcpu);\n\treturn 1;\n}\n\nstatic int handle_interrupt_window(struct kvm_vcpu *vcpu)\n{\n\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\tCPU_BASED_VIRTUAL_INTR_PENDING);\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t++vcpu->stat.irq_window_exits;\n\treturn 1;\n}\n\nstatic int handle_halt(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_halt(vcpu);\n}\n\nstatic int handle_vmcall(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_hypercall(vcpu);\n}\n\nstatic int handle_invd(struct kvm_vcpu *vcpu)\n{\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_invlpg(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\tkvm_mmu_invlpg(vcpu, exit_qualification);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_rdpmc(struct kvm_vcpu *vcpu)\n{\n\tint err;\n\n\terr = kvm_rdpmc(vcpu);\n\treturn kvm_complete_insn_gp(vcpu, err);\n}\n\nstatic int handle_wbinvd(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_wbinvd(vcpu);\n}\n\nstatic int handle_xsetbv(struct kvm_vcpu *vcpu)\n{\n\tu64 new_bv = kvm_read_edx_eax(vcpu);\n\tu32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);\n\n\tif (kvm_set_xcr(vcpu, index, new_bv) == 0)\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\treturn 1;\n}\n\nstatic int handle_xsaves(struct kvm_vcpu *vcpu)\n{\n\tkvm_skip_emulated_instruction(vcpu);\n\tWARN(1, \"this should never happen\\n\");\n\treturn 1;\n}\n\nstatic int handle_xrstors(struct kvm_vcpu *vcpu)\n{\n\tkvm_skip_emulated_instruction(vcpu);\n\tWARN(1, \"this should never happen\\n\");\n\treturn 1;\n}\n\nstatic int handle_apic_access(struct kvm_vcpu *vcpu)\n{\n\tif (likely(fasteoi)) {\n\t\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\t\tint access_type, offset;\n\n\t\taccess_type = exit_qualification & APIC_ACCESS_TYPE;\n\t\toffset = exit_qualification & APIC_ACCESS_OFFSET;\n\t\t/*\n\t\t * Sane guest uses MOV to write EOI, with written value\n\t\t * not cared. So make a short-circuit here by avoiding\n\t\t * heavy instruction emulation.\n\t\t */\n\t\tif ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&\n\t\t    (offset == APIC_EOI)) {\n\t\t\tkvm_lapic_set_eoi(vcpu);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t}\n\treturn emulate_instruction(vcpu, 0) == EMULATE_DONE;\n}\n\nstatic int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tint vector = exit_qualification & 0xff;\n\n\t/* EOI-induced VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_set_eoi_accelerated(vcpu, vector);\n\treturn 1;\n}\n\nstatic int handle_apic_write(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 offset = exit_qualification & 0xfff;\n\n\t/* APIC-write VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}\n\nstatic int handle_task_switch(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\tu16 tss_selector;\n\tint reason, type, idt_v, idt_index;\n\n\tidt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);\n\tidt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);\n\ttype = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\treason = (u32)exit_qualification >> 30;\n\tif (reason == TASK_SWITCH_GATE && idt_v) {\n\t\tswitch (type) {\n\t\tcase INTR_TYPE_NMI_INTR:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tvmx_set_nmi_mask(vcpu, true);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_EXT_INTR:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\t\tif (vmx->idt_vectoring_info &\n\t\t\t    VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\tvmcs_read32(IDT_VECTORING_ERROR_CODE);\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\ttss_selector = exit_qualification;\n\n\tif (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&\n\t\t       type != INTR_TYPE_EXT_INTR &&\n\t\t       type != INTR_TYPE_NMI_INTR))\n\t\tskip_emulated_instruction(vcpu);\n\n\tif (kvm_task_switch(vcpu, tss_selector,\n\t\t\t    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,\n\t\t\t    has_error_code, error_code) == EMULATE_FAIL) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: What about debug traps on tss switch?\n\t *       Are we supposed to inject them and update dr6?\n\t */\n\n\treturn 1;\n}\n\nstatic int handle_ept_violation(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tgpa_t gpa;\n\tu64 error_code;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\t/*\n\t * EPT violation happened while executing iret from NMI,\n\t * \"blocked by NMI\" bit has to be set before next VM entry.\n\t * There are errata that may cause this bit to not be set:\n\t * AAK134, BY25.\n\t */\n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);\n\n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\ttrace_kvm_page_fault(gpa, exit_qualification);\n\n\t/* Is it a read fault? */\n\terror_code = (exit_qualification & EPT_VIOLATION_ACC_READ)\n\t\t     ? PFERR_USER_MASK : 0;\n\t/* Is it a write fault? */\n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)\n\t\t      ? PFERR_WRITE_MASK : 0;\n\t/* Is it a fetch fault? */\n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)\n\t\t      ? PFERR_FETCH_MASK : 0;\n\t/* ept page table entry is present? */\n\terror_code |= (exit_qualification &\n\t\t       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |\n\t\t\tEPT_VIOLATION_EXECUTABLE))\n\t\t      ? PFERR_PRESENT_MASK : 0;\n\n\terror_code |= (exit_qualification & 0x100) != 0 ?\n\t       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;\n\n\tvcpu->arch.exit_qualification = exit_qualification;\n\treturn kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);\n}\n\nstatic int handle_ept_misconfig(struct kvm_vcpu *vcpu)\n{\n\tgpa_t gpa;\n\n\t/*\n\t * A nested guest cannot optimize MMIO vmexits, because we have an\n\t * nGPA here instead of the required GPA.\n\t */\n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\tif (!is_guest_mode(vcpu) &&\n\t    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {\n\t\ttrace_kvm_fast_mmio(gpa);\n\t\t/*\n\t\t * Doing kvm_skip_emulated_instruction() depends on undefined\n\t\t * behavior: Intel's manual doesn't mandate\n\t\t * VM_EXIT_INSTRUCTION_LEN to be set in VMCS when EPT MISCONFIG\n\t\t * occurs and while on real hardware it was observed to be set,\n\t\t * other hypervisors (namely Hyper-V) don't set it, we end up\n\t\t * advancing IP with some random value. Disable fast mmio when\n\t\t * running nested and keep it for real hardware in hope that\n\t\t * VM_EXIT_INSTRUCTION_LEN will always be set correctly.\n\t\t */\n\t\tif (!static_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\telse\n\t\t\treturn x86_emulate_instruction(vcpu, gpa, EMULTYPE_SKIP,\n\t\t\t\t\t\t       NULL, 0) == EMULATE_DONE;\n\t}\n\n\treturn kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);\n}\n\nstatic int handle_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON_ONCE(!enable_vnmi);\n\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\tCPU_BASED_VIRTUAL_NMI_PENDING);\n\t++vcpu->stat.nmi_window_exits;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 1;\n}\n\nstatic int handle_invalid_guest_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tenum emulation_result err = EMULATE_DONE;\n\tint ret = 1;\n\tu32 cpu_exec_ctrl;\n\tbool intr_window_requested;\n\tunsigned count = 130;\n\n\t/*\n\t * We should never reach the point where we are emulating L2\n\t * due to invalid guest state as that means we incorrectly\n\t * allowed a nested VMEntry with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->emulation_required && vmx->nested.nested_run_pending);\n\n\tcpu_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);\n\tintr_window_requested = cpu_exec_ctrl & CPU_BASED_VIRTUAL_INTR_PENDING;\n\n\twhile (vmx->emulation_required && count-- != 0) {\n\t\tif (intr_window_requested && vmx_interrupt_allowed(vcpu))\n\t\t\treturn handle_interrupt_window(&vmx->vcpu);\n\n\t\tif (kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\t\treturn 1;\n\n\t\terr = emulate_instruction(vcpu, 0);\n\n\t\tif (err == EMULATE_USER_EXIT) {\n\t\t\t++vcpu->stat.mmio_exits;\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (err != EMULATE_DONE)\n\t\t\tgoto emulation_error;\n\n\t\tif (vmx->emulation_required && !vmx->rmode.vm86_active &&\n\t\t    vcpu->arch.exception.pending)\n\t\t\tgoto emulation_error;\n\n\t\tif (vcpu->arch.halt_request) {\n\t\t\tvcpu->arch.halt_request = 0;\n\t\t\tret = kvm_vcpu_halt(vcpu);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tif (need_resched())\n\t\t\tschedule();\n\t}\n\nout:\n\treturn ret;\n\nemulation_error:\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\tvcpu->run->internal.ndata = 0;\n\treturn 0;\n}\n\nstatic void grow_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint old = vmx->ple_window;\n\n\tvmx->ple_window = __grow_ple_window(old, ple_window,\n\t\t\t\t\t    ple_window_grow,\n\t\t\t\t\t    ple_window_max);\n\n\tif (vmx->ple_window != old)\n\t\tvmx->ple_window_dirty = true;\n\n\ttrace_kvm_ple_window_grow(vcpu->vcpu_id, vmx->ple_window, old);\n}\n\nstatic void shrink_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint old = vmx->ple_window;\n\n\tvmx->ple_window = __shrink_ple_window(old, ple_window,\n\t\t\t\t\t      ple_window_shrink,\n\t\t\t\t\t      ple_window);\n\n\tif (vmx->ple_window != old)\n\t\tvmx->ple_window_dirty = true;\n\n\ttrace_kvm_ple_window_shrink(vcpu->vcpu_id, vmx->ple_window, old);\n}\n\n/*\n * Handler for POSTED_INTERRUPT_WAKEUP_VECTOR.\n */\nstatic void wakeup_handler(void)\n{\n\tstruct kvm_vcpu *vcpu;\n\tint cpu = smp_processor_id();\n\n\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n\tlist_for_each_entry(vcpu, &per_cpu(blocked_vcpu_on_cpu, cpu),\n\t\t\tblocked_vcpu_list) {\n\t\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\t\tif (pi_test_on(pi_desc) == 1)\n\t\t\tkvm_vcpu_kick(vcpu);\n\t}\n\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, cpu));\n}\n\nstatic void vmx_enable_tdp(void)\n{\n\tkvm_mmu_set_mask_ptes(VMX_EPT_READABLE_MASK,\n\t\tenable_ept_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull,\n\t\tenable_ept_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull,\n\t\t0ull, VMX_EPT_EXECUTABLE_MASK,\n\t\tcpu_has_vmx_ept_execute_only() ? 0ull : VMX_EPT_READABLE_MASK,\n\t\tVMX_EPT_RWX_MASK, 0ull);\n\n\tept_set_mmio_spte_mask();\n\tkvm_enable_tdp();\n}\n\nstatic __init int hardware_setup(void)\n{\n\tint r = -ENOMEM, i;\n\n\trdmsrl_safe(MSR_EFER, &host_efer);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_msr_index); ++i)\n\t\tkvm_define_shared_msr(i, vmx_msr_index[i]);\n\n\tfor (i = 0; i < VMX_BITMAP_NR; i++) {\n\t\tvmx_bitmap[i] = (unsigned long *)__get_free_page(GFP_KERNEL);\n\t\tif (!vmx_bitmap[i])\n\t\t\tgoto out;\n\t}\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\tif (setup_vmcs_config(&vmcs_config) < 0) {\n\t\tr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (!cpu_has_vmx_vpid() || !cpu_has_vmx_invvpid() ||\n\t\t!(cpu_has_vmx_invvpid_single() || cpu_has_vmx_invvpid_global()))\n\t\tenable_vpid = 0;\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels() ||\n\t    !cpu_has_vmx_ept_mt_wb() ||\n\t    !cpu_has_vmx_invept_global())\n\t\tenable_ept = 0;\n\n\tif (!cpu_has_vmx_ept_ad_bits() || !enable_ept)\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest() || !enable_ept)\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\tif (!cpu_has_virtual_nmis())\n\t\tenable_vnmi = 0;\n\n\t/*\n\t * set_apic_access_page_addr() is used to reload apic access\n\t * page upon invalidation.  No need to do anything if not\n\t * using the APIC_ACCESS_ADDR VMCS field.\n\t */\n\tif (!flexpriority_enabled)\n\t\tkvm_x86_ops->set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tkvm_x86_ops->update_cr8_intercept = NULL;\n\n\tif (enable_ept && !cpu_has_vmx_ept_2m_page())\n\t\tkvm_disable_largepages();\n\n\tif (!cpu_has_vmx_ple()) {\n\t\tple_gap = 0;\n\t\tple_window = 0;\n\t\tple_window_grow = 0;\n\t\tple_window_max = 0;\n\t\tple_window_shrink = 0;\n\t}\n\n\tif (!cpu_has_vmx_apicv()) {\n\t\tenable_apicv = 0;\n\t\tkvm_x86_ops->sync_pir_to_irr = NULL;\n\t}\n\n\tif (cpu_has_vmx_tsc_scaling()) {\n\t\tkvm_has_tsc_control = true;\n\t\tkvm_max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\t\tkvm_tsc_scaling_ratio_frac_bits = 48;\n\t}\n\n\tset_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */\n\n\tif (enable_ept)\n\t\tvmx_enable_tdp();\n\telse\n\t\tkvm_disable_tdp();\n\n\t/*\n\t * Only enable PML when hardware supports PML feature, and both EPT\n\t * and EPT A/D bit features are enabled -- PML depends on them to work.\n\t */\n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml) {\n\t\tkvm_x86_ops->slot_enable_log_dirty = NULL;\n\t\tkvm_x86_ops->slot_disable_log_dirty = NULL;\n\t\tkvm_x86_ops->flush_log_dirty = NULL;\n\t\tkvm_x86_ops->enable_log_dirty_pt_masked = NULL;\n\t}\n\n\tif (cpu_has_vmx_preemption_timer() && enable_preemption_timer) {\n\t\tu64 vmx_msr;\n\n\t\trdmsrl(MSR_IA32_VMX_MISC, vmx_msr);\n\t\tcpu_preemption_timer_multi =\n\t\t\t vmx_msr & VMX_MISC_PREEMPTION_TIMER_RATE_MASK;\n\t} else {\n\t\tkvm_x86_ops->set_hv_timer = NULL;\n\t\tkvm_x86_ops->cancel_hv_timer = NULL;\n\t}\n\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs)\n\t\tinit_vmcs_shadow_fields();\n\n\tkvm_set_posted_intr_wakeup_handler(wakeup_handler);\n\tnested_vmx_setup_ctls_msrs(&vmcs_config.nested, enable_apicv);\n\n\tkvm_mce_cap_supported |= MCG_LMCE_P;\n\n\treturn alloc_kvm_area();\n\nout:\n\tfor (i = 0; i < VMX_BITMAP_NR; i++)\n\t\tfree_page((unsigned long)vmx_bitmap[i]);\n\n    return r;\n}\n\nstatic __exit void hardware_unsetup(void)\n{\n\tint i;\n\n\tfor (i = 0; i < VMX_BITMAP_NR; i++)\n\t\tfree_page((unsigned long)vmx_bitmap[i]);\n\n\tfree_kvm_area();\n}\n\n/*\n * Indicate a busy-waiting vcpu in spinlock. We do not enable the PAUSE\n * exiting, so only get here on cpu with PAUSE-Loop-Exiting.\n */\nstatic int handle_pause(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tgrow_ple_window(vcpu);\n\n\t/*\n\t * Intel sdm vol3 ch-25.1.3 says: The \"PAUSE-loop exiting\"\n\t * VM-execution control is ignored if CPL > 0. OTOH, KVM\n\t * never set PAUSE_EXITING and just set PLE if supported,\n\t * so the vcpu must be CPL=0 if it gets a PAUSE exit.\n\t */\n\tkvm_vcpu_on_spin(vcpu, true);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_nop(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_mwait(struct kvm_vcpu *vcpu)\n{\n\tprintk_once(KERN_WARNING \"kvm: MWAIT instruction emulated as NOP!\\n\");\n\treturn handle_nop(vcpu);\n}\n\nstatic int handle_invalid_op(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\n\nstatic int handle_monitor_trap(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int handle_monitor(struct kvm_vcpu *vcpu)\n{\n\tprintk_once(KERN_WARNING \"kvm: MONITOR instruction emulated as NOP!\\n\");\n\treturn handle_nop(vcpu);\n}\n\n/*\n * The following 3 functions, nested_vmx_succeed()/failValid()/failInvalid(),\n * set the success or error code of an emulated VMX instruction, as specified\n * by Vol 2B, VMX Instruction Reference, \"Conventions\".\n */\nstatic void nested_vmx_succeed(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_OF));\n}\n\nstatic void nested_vmx_failInvalid(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_PF | X86_EFLAGS_AF | X86_EFLAGS_ZF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_CF);\n}\n\nstatic void nested_vmx_failValid(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu32 vm_instruction_error)\n{\n\tif (to_vmx(vcpu)->nested.current_vmptr == -1ull) {\n\t\t/*\n\t\t * failValid writes the error number to the current VMCS, which\n\t\t * can't be done there isn't a current VMCS.\n\t\t */\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn;\n\t}\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_ZF);\n\tget_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;\n\t/*\n\t * We don't need to force a shadow sync because\n\t * VM_INSTRUCTION_ERROR is not shadowed\n\t */\n}\n\nstatic void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)\n{\n\t/* TODO: not to reset guest simply here. */\n\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\tpr_debug_ratelimited(\"kvm: nested vmx abort, indicator %d\\n\", indicator);\n}\n\nstatic enum hrtimer_restart vmx_preemption_timer_fn(struct hrtimer *timer)\n{\n\tstruct vcpu_vmx *vmx =\n\t\tcontainer_of(timer, struct vcpu_vmx, nested.preemption_timer);\n\n\tvmx->nested.preemption_timer_expired = true;\n\tkvm_make_request(KVM_REQ_EVENT, &vmx->vcpu);\n\tkvm_vcpu_kick(&vmx->vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\n/*\n * Decode the memory-address operand of a vmx instruction, as recorded on an\n * exit caused by such an instruction (run by a guest hypervisor).\n * On success, returns 0. When the operand is invalid, returns 1 and throws\n * #UD or #GP.\n */\nstatic int get_vmx_mem_address(struct kvm_vcpu *vcpu,\n\t\t\t\t unsigned long exit_qualification,\n\t\t\t\t u32 vmx_instruction_info, bool wr, gva_t *ret)\n{\n\tgva_t off;\n\tbool exn;\n\tstruct kvm_segment s;\n\n\t/*\n\t * According to Vol. 3B, \"Information for VM Exits Due to Instruction\n\t * Execution\", on an exit, vmx_instruction_info holds most of the\n\t * addressing components of the operand. Only the displacement part\n\t * is put in exit_qualification (see 3B, \"Basic VM-Exit Information\").\n\t * For how an actual address is calculated from all these components,\n\t * refer to Vol. 1, \"Operand Addressing\".\n\t */\n\tint  scaling = vmx_instruction_info & 3;\n\tint  addr_size = (vmx_instruction_info >> 7) & 7;\n\tbool is_reg = vmx_instruction_info & (1u << 10);\n\tint  seg_reg = (vmx_instruction_info >> 15) & 7;\n\tint  index_reg = (vmx_instruction_info >> 18) & 0xf;\n\tbool index_is_valid = !(vmx_instruction_info & (1u << 22));\n\tint  base_reg       = (vmx_instruction_info >> 23) & 0xf;\n\tbool base_is_valid  = !(vmx_instruction_info & (1u << 27));\n\n\tif (is_reg) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t/* Addr = segment_base + offset */\n\t/* offset = base + [index * scale] + displacement */\n\toff = exit_qualification; /* holds the displacement */\n\tif (base_is_valid)\n\t\toff += kvm_register_read(vcpu, base_reg);\n\tif (index_is_valid)\n\t\toff += kvm_register_read(vcpu, index_reg)<<scaling;\n\tvmx_get_segment(vcpu, &s, seg_reg);\n\t*ret = s.base + off;\n\n\tif (addr_size == 1) /* 32 bit */\n\t\t*ret &= 0xffffffff;\n\n\t/* Checks for #GP/#SS exceptions. */\n\texn = false;\n\tif (is_long_mode(vcpu)) {\n\t\t/* Long mode: #GP(0)/#SS(0) if the memory address is in a\n\t\t * non-canonical form. This is the only check on the memory\n\t\t * destination for long mode!\n\t\t */\n\t\texn = is_noncanonical_address(*ret, vcpu);\n\t} else if (is_protmode(vcpu)) {\n\t\t/* Protected mode: apply checks for segment validity in the\n\t\t * following order:\n\t\t * - segment type check (#GP(0) may be thrown)\n\t\t * - usability check (#GP(0)/#SS(0))\n\t\t * - limit check (#GP(0)/#SS(0))\n\t\t */\n\t\tif (wr)\n\t\t\t/* #GP(0) if the destination operand is located in a\n\t\t\t * read-only data segment or any code segment.\n\t\t\t */\n\t\t\texn = ((s.type & 0xa) == 0 || (s.type & 8));\n\t\telse\n\t\t\t/* #GP(0) if the source operand is located in an\n\t\t\t * execute-only code segment\n\t\t\t */\n\t\t\texn = ((s.type & 0xa) == 8);\n\t\tif (exn) {\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\t\treturn 1;\n\t\t}\n\t\t/* Protected mode: #GP(0)/#SS(0) if the segment is unusable.\n\t\t */\n\t\texn = (s.unusable != 0);\n\t\t/* Protected mode: #GP(0)/#SS(0) if the memory\n\t\t * operand is outside the segment limit.\n\t\t */\n\t\texn = exn || (off + sizeof(u64) > s.limit);\n\t}\n\tif (exn) {\n\t\tkvm_queue_exception_e(vcpu,\n\t\t\t\t      seg_reg == VCPU_SREG_SS ?\n\t\t\t\t\t\tSS_VECTOR : GP_VECTOR,\n\t\t\t\t      0);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer)\n{\n\tgva_t gva;\n\tstruct x86_exception e;\n\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmcs_read32(VMX_INSTRUCTION_INFO), false, &gva))\n\t\treturn 1;\n\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, vmpointer,\n\t\t\t\tsizeof(*vmpointer), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int enter_vmx_operation(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs *shadow_vmcs;\n\tint r;\n\n\tr = alloc_loaded_vmcs(&vmx->nested.vmcs02);\n\tif (r < 0)\n\t\tgoto out_vmcs02;\n\n\tvmx->nested.cached_vmcs12 = kmalloc(VMCS12_SIZE, GFP_KERNEL);\n\tif (!vmx->nested.cached_vmcs12)\n\t\tgoto out_cached_vmcs12;\n\n\tif (enable_shadow_vmcs) {\n\t\tshadow_vmcs = alloc_vmcs();\n\t\tif (!shadow_vmcs)\n\t\t\tgoto out_shadow_vmcs;\n\t\t/* mark vmcs as shadow */\n\t\tshadow_vmcs->revision_id |= (1u << 31);\n\t\t/* init shadow vmcs */\n\t\tvmcs_clear(shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = shadow_vmcs;\n\t}\n\n\thrtimer_init(&vmx->nested.preemption_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL_PINNED);\n\tvmx->nested.preemption_timer.function = vmx_preemption_timer_fn;\n\n\tvmx->nested.vmxon = true;\n\treturn 0;\n\nout_shadow_vmcs:\n\tkfree(vmx->nested.cached_vmcs12);\n\nout_cached_vmcs12:\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n\nout_vmcs02:\n\treturn -ENOMEM;\n}\n\n/*\n * Emulate the VMXON instruction.\n * Currently, we just remember that VMX is active, and do not save or even\n * inspect the argument to VMXON (the so-called \"VMXON pointer\") because we\n * do not currently need to store anything in that guest-allocated memory\n * region. Consequently, VMCLEAR and VMPTRLD also do not verify that the their\n * argument is different from the VMXON pointer (which the spec says they do).\n */\nstatic int handle_vmon(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\tgpa_t vmptr;\n\tstruct page *page;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst u64 VMXON_NEEDED_FEATURES = FEATURE_CONTROL_LOCKED\n\t\t| FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\n\t/*\n\t * The Intel VMX Instruction Reference lists a bunch of bits that are\n\t * prerequisite to running VMXON, most notably cr4.VMXE must be set to\n\t * 1 (see vmx_set_cr4() for when we allow the guest to set this).\n\t * Otherwise, we should fail with #UD.  But most faulting conditions\n\t * have already been checked by hardware, prior to the VM-exit for\n\t * VMXON.  We do test guest cr4.VMXE because processor CR4 always has\n\t * that bit set to 1 in non-root mode.\n\t */\n\tif (!kvm_read_cr4_bits(vcpu, X86_CR4_VMXE)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t/* CPL=0 must be checked manually. */\n\tif (vmx_get_cpl(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (vmx->nested.vmxon) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif ((vmx->msr_ia32_feature_control & VMXON_NEEDED_FEATURES)\n\t\t\t!= VMXON_NEEDED_FEATURES) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\t/*\n\t * SDM 3: 24.11.5\n\t * The first 4 bytes of VMXON region contain the supported\n\t * VMCS revision identifier\n\t *\n\t * Note - IA32_VMX_BASIC[48] will never be 1 for the nested case;\n\t * which replaces physical address width with 32\n\t */\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmptr);\n\tif (is_error_page(page)) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\tif (*(u32 *)kmap(page) != VMCS12_REVISION) {\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\tvmx->nested.vmxon_ptr = vmptr;\n\tret = enter_vmx_operation(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/*\n * Intel's VMX Instruction Reference specifies a common set of prerequisites\n * for running VMX instructions (except VMXON, whose prerequisites are\n * slightly different). It also specifies what exception to inject otherwise.\n * Note that many of these exceptions have priority over VM exits, so they\n * don't have to be checked again here.\n */\nstatic int nested_vmx_check_permission(struct kvm_vcpu *vcpu)\n{\n\tif (vmx_get_cpl(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 0;\n\t}\n\n\tif (!to_vmx(vcpu)->nested.vmxon) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)\n{\n\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL, SECONDARY_EXEC_SHADOW_VMCS);\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull);\n}\n\nstatic inline void nested_release_vmcs12(struct vcpu_vmx *vmx)\n{\n\tif (vmx->nested.current_vmptr == -1ull)\n\t\treturn;\n\n\tif (enable_shadow_vmcs) {\n\t\t/* copy to memory all shadowed fields in case\n\t\t   they were modified */\n\t\tcopy_shadow_to_vmcs12(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t}\n\tvmx->nested.posted_intr_nv = -1;\n\n\t/* Flush VMCS12 to guest memory */\n\tkvm_vcpu_write_guest_page(&vmx->vcpu,\n\t\t\t\t  vmx->nested.current_vmptr >> PAGE_SHIFT,\n\t\t\t\t  vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);\n\n\tvmx->nested.current_vmptr = -1ull;\n}\n\n/*\n * Free whatever needs to be freed from vmx->nested when L1 goes down, or\n * just stops using VMX.\n */\nstatic void free_nested(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n\t\treturn;\n\n\tvmx->nested.vmxon = false;\n\tvmx->nested.smm.vmxon = false;\n\tfree_vpid(vmx->nested.vpid02);\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\tif (enable_shadow_vmcs) {\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\tfree_vmcs(vmx->vmcs01.shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = NULL;\n\t}\n\tkfree(vmx->nested.cached_vmcs12);\n\t/* Unpin physical memory we referred to in the vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n}\n\n/* Emulate the VMXOFF instruction */\nstatic int handle_vmoff(struct kvm_vcpu *vcpu)\n{\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\tfree_nested(to_vmx(vcpu));\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the VMCLEAR instruction */\nstatic int handle_vmclear(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 zero = 0;\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.current_vmptr)\n\t\tnested_release_vmcs12(vmx);\n\n\tkvm_vcpu_write_guest(vcpu,\n\t\t\tvmptr + offsetof(struct vmcs12, launch_state),\n\t\t\t&zero, sizeof(zero));\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);\n\n/* Emulate the VMLAUNCH instruction */\nstatic int handle_vmlaunch(struct kvm_vcpu *vcpu)\n{\n\treturn nested_vmx_run(vcpu, true);\n}\n\n/* Emulate the VMRESUME instruction */\nstatic int handle_vmresume(struct kvm_vcpu *vcpu)\n{\n\n\treturn nested_vmx_run(vcpu, false);\n}\n\n/*\n * Read a vmcs12 field. Since these can have varying lengths and we return\n * one type, we chose the biggest type (u64) and zero-extend the return value\n * to that size. Note that the caller, handle_vmread, might need to use only\n * some of the bits we return here (e.g., on 32-bit guests, only 32 bits of\n * 64-bit fields are to be returned).\n */\nstatic inline int vmcs12_read_any(struct kvm_vcpu *vcpu,\n\t\t\t\t  unsigned long field, u64 *ret)\n{\n\tshort offset = vmcs_field_to_offset(field);\n\tchar *p;\n\n\tif (offset < 0)\n\t\treturn offset;\n\n\tp = ((char *)(get_vmcs12(vcpu))) + offset;\n\n\tswitch (vmcs_field_width(field)) {\n\tcase VMCS_FIELD_WIDTH_NATURAL_WIDTH:\n\t\t*ret = *((natural_width *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U16:\n\t\t*ret = *((u16 *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U32:\n\t\t*ret = *((u32 *)p);\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U64:\n\t\t*ret = *((u64 *)p);\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOENT;\n\t}\n}\n\n\nstatic inline int vmcs12_write_any(struct kvm_vcpu *vcpu,\n\t\t\t\t   unsigned long field, u64 field_value){\n\tshort offset = vmcs_field_to_offset(field);\n\tchar *p = ((char *) get_vmcs12(vcpu)) + offset;\n\tif (offset < 0)\n\t\treturn offset;\n\n\tswitch (vmcs_field_width(field)) {\n\tcase VMCS_FIELD_WIDTH_U16:\n\t\t*(u16 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U32:\n\t\t*(u32 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_U64:\n\t\t*(u64 *)p = field_value;\n\t\treturn 0;\n\tcase VMCS_FIELD_WIDTH_NATURAL_WIDTH:\n\t\t*(natural_width *)p = field_value;\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOENT;\n\t}\n\n}\n\n/*\n * Copy the writable VMCS shadow fields back to the VMCS12, in case\n * they have been modified by the L1 guest. Note that the \"read-only\"\n * VM-exit information fields are actually writable if the vCPU is\n * configured to support \"VMWRITE to any supported field in the VMCS.\"\n */\nstatic void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)\n{\n\tconst u16 *fields[] = {\n\t\tshadow_read_write_fields,\n\t\tshadow_read_only_fields\n\t};\n\tconst int max_fields[] = {\n\t\tmax_shadow_read_write_fields,\n\t\tmax_shadow_read_only_fields\n\t};\n\tint i, q;\n\tunsigned long field;\n\tu64 field_value;\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\n\tpreempt_disable();\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (q = 0; q < ARRAY_SIZE(fields); q++) {\n\t\tfor (i = 0; i < max_fields[q]; i++) {\n\t\t\tfield = fields[q][i];\n\t\t\tfield_value = __vmcs_readl(field);\n\t\t\tvmcs12_write_any(&vmx->vcpu, field, field_value);\n\t\t}\n\t\t/*\n\t\t * Skip the VM-exit information fields if they are read-only.\n\t\t */\n\t\tif (!nested_cpu_has_vmwrite_any_field(&vmx->vcpu))\n\t\t\tbreak;\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\tpreempt_enable();\n}\n\nstatic void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)\n{\n\tconst u16 *fields[] = {\n\t\tshadow_read_write_fields,\n\t\tshadow_read_only_fields\n\t};\n\tconst int max_fields[] = {\n\t\tmax_shadow_read_write_fields,\n\t\tmax_shadow_read_only_fields\n\t};\n\tint i, q;\n\tunsigned long field;\n\tu64 field_value = 0;\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (q = 0; q < ARRAY_SIZE(fields); q++) {\n\t\tfor (i = 0; i < max_fields[q]; i++) {\n\t\t\tfield = fields[q][i];\n\t\t\tvmcs12_read_any(&vmx->vcpu, field, &field_value);\n\t\t\t__vmcs_writel(field, field_value);\n\t\t}\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n}\n\n/*\n * VMX instructions which assume a current vmcs12 (i.e., that VMPTRLD was\n * used before) all generate the same failure when it is missing.\n */\nstatic int nested_vmx_check_vmcs12(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tif (vmx->nested.current_vmptr == -1ull) {\n\t\tnested_vmx_failInvalid(vcpu);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int handle_vmread(struct kvm_vcpu *vcpu)\n{\n\tunsigned long field;\n\tu64 field_value;\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t gva = 0;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\t/* Decode instruction info and find the field to read */\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/* Read the field, zero-extended to a u64 field_value */\n\tif (vmcs12_read_any(vcpu, field, &field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\t/*\n\t * Now copy part of this value to register or memory, as requested.\n\t * Note that the number of bits actually copied is 32 or 64 depending\n\t * on the guest's mode (32 or 64 bit), not on the given field's length.\n\t */\n\tif (vmx_instruction_info & (1u << 10)) {\n\t\tkvm_register_writel(vcpu, (((vmx_instruction_info) >> 3) & 0xf),\n\t\t\tfield_value);\n\t} else {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, true, &gva))\n\t\t\treturn 1;\n\t\t/* _system ok, nested_vmx_check_permission has verified cpl=0 */\n\t\tkvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n\nstatic int handle_vmwrite(struct kvm_vcpu *vcpu)\n{\n\tunsigned long field;\n\tgva_t gva;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\n\t/* The value to write might be 32 or 64 bits, depending on L1's long\n\t * mode, and eventually we need to write that into a field of several\n\t * possible lengths. The code below first zero-extends the value to 64\n\t * bit (field_value), and then copies only the appropriate number of\n\t * bits into the vmcs12 field.\n\t */\n\tu64 field_value = 0;\n\tstruct x86_exception e;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\tif (vmx_instruction_info & (1u << 10))\n\t\tfield_value = kvm_register_readl(vcpu,\n\t\t\t(((vmx_instruction_info) >> 3) & 0xf));\n\telse {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, false, &gva))\n\t\t\treturn 1;\n\t\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t   &field_value, (is_64_bit_mode(vcpu) ? 8 : 4), &e)) {\n\t\t\tkvm_inject_page_fault(vcpu, &e);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/*\n\t * If the vCPU supports \"VMWRITE to any supported field in the\n\t * VMCS,\" then the \"read-only\" fields are actually read/write.\n\t */\n\tif (vmcs_field_readonly(field) &&\n\t    !nested_cpu_has_vmwrite_any_field(vcpu)) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmcs12_write_any(vcpu, field, field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tswitch (field) {\n#define SHADOW_FIELD_RW(x) case x:\n#include \"vmx_shadow_fields.h\"\n\t\t/*\n\t\t * The fields that can be updated by L1 without a vmexit are\n\t\t * always updated in the vmcs02, the others go down the slow\n\t\t * path of prepare_vmcs02.\n\t\t */\n\t\tbreak;\n\tdefault:\n\t\tvmx->nested.dirty_vmcs12 = true;\n\t\tbreak;\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)\n{\n\tvmx->nested.current_vmptr = vmptr;\n\tif (enable_shadow_vmcs) {\n\t\tvmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t      SECONDARY_EXEC_SHADOW_VMCS);\n\t\tvmcs_write64(VMCS_LINK_POINTER,\n\t\t\t     __pa(vmx->vmcs01.shadow_vmcs));\n\t\tvmx->nested.sync_shadow_vmcs = true;\n\t}\n\tvmx->nested.dirty_vmcs12 = true;\n}\n\n/* Emulate the VMPTRLD instruction */\nstatic int handle_vmptrld(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmx->nested.current_vmptr != vmptr) {\n\t\tstruct vmcs12 *new_vmcs12;\n\t\tstruct page *page;\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmptr);\n\t\tif (is_error_page(page)) {\n\t\t\tnested_vmx_failInvalid(vcpu);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tnew_vmcs12 = kmap(page);\n\t\tif (new_vmcs12->revision_id != VMCS12_REVISION) {\n\t\t\tkunmap(page);\n\t\t\tkvm_release_page_clean(page);\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\n\t\tnested_release_vmcs12(vmx);\n\t\t/*\n\t\t * Load VMCS12 from guest memory since it is not already\n\t\t * cached.\n\t\t */\n\t\tmemcpy(vmx->nested.cached_vmcs12, new_vmcs12, VMCS12_SIZE);\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\n\t\tset_current_vmptr(vmx, vmptr);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the VMPTRST instruction */\nstatic int handle_vmptrst(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t vmcs_gva;\n\tstruct x86_exception e;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\tvmx_instruction_info, true, &vmcs_gva))\n\t\treturn 1;\n\t/* *_system ok, nested_vmx_check_permission has verified cpl=0 */\n\tif (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,\n\t\t\t\t (void *)&to_vmx(vcpu)->nested.current_vmptr,\n\t\t\t\t sizeof(u64), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/* Emulate the INVEPT instruction */\nstatic int handle_invept(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info, types;\n\tunsigned long type;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_EPT) ||\n\t    !(vmx->nested.msrs.ept_caps & VMX_EPT_INVEPT_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (vmx->nested.msrs.ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;\n\n\tif (type >= 32 || !(types & (1 << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\t/* According to the Intel VMX instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, false, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\n\tswitch (type) {\n\tcase VMX_EPT_EXTENT_GLOBAL:\n\t/*\n\t * TODO: track mappings and invalidate\n\t * single context requests appropriately\n\t */\n\tcase VMX_EPT_EXTENT_CONTEXT:\n\t\tkvm_mmu_sync_roots(vcpu);\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\t\tnested_vmx_succeed(vcpu);\n\t\tbreak;\n\tdefault:\n\t\tBUG_ON(1);\n\t\tbreak;\n\t}\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_invvpid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info;\n\tunsigned long type, types;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 vpid;\n\t\tu64 gla;\n\t} operand;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_VPID) ||\n\t\t\t!(vmx->nested.msrs.vpid_caps & VMX_VPID_INVVPID_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\ttype = kvm_register_readl(vcpu, (vmx_instruction_info >> 28) & 0xf);\n\n\ttypes = (vmx->nested.msrs.vpid_caps &\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;\n\n\tif (type >= 32 || !(types & (1 << type))) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\t/* according to the intel vmx instruction reference, the memory\n\t * operand is read even if it isn't needed (e.g., for type==global)\n\t */\n\tif (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),\n\t\t\tvmx_instruction_info, false, &gva))\n\t\treturn 1;\n\tif (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,\n\t\t\t\tsizeof(operand), &e)) {\n\t\tkvm_inject_page_fault(vcpu, &e);\n\t\treturn 1;\n\t}\n\tif (operand.vpid >> 16) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tswitch (type) {\n\tcase VMX_VPID_EXTENT_INDIVIDUAL_ADDR:\n\t\tif (!operand.vpid ||\n\t\t    is_noncanonical_address(operand.gla, vcpu)) {\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tif (cpu_has_vmx_invvpid_individual_addr() &&\n\t\t    vmx->nested.vpid02) {\n\t\t\t__invvpid(VMX_VPID_EXTENT_INDIVIDUAL_ADDR,\n\t\t\t\tvmx->nested.vpid02, operand.gla);\n\t\t} else\n\t\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_SINGLE_CONTEXT:\n\tcase VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:\n\t\tif (!operand.vpid) {\n\t\t\tnested_vmx_failValid(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_ALL_CONTEXT:\n\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_pml_full(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\n\ttrace_kvm_pml_full(vcpu->vcpu_id);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\t/*\n\t * PML buffer FULL happened while executing iret from NMI,\n\t * \"blocked by NMI\" bit has to be set before next VM entry.\n\t */\n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\tGUEST_INTR_STATE_NMI);\n\n\t/*\n\t * PML buffer already flushed at beginning of VMEXIT. Nothing to do\n\t * here.., and there's no userspace involvement needed for PML.\n\t */\n\treturn 1;\n}\n\nstatic int handle_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\tkvm_lapic_expired_hv_timer(vcpu);\n\treturn 1;\n}\n\nstatic bool valid_ept_address(struct kvm_vcpu *vcpu, u64 address)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint maxphyaddr = cpuid_maxphyaddr(vcpu);\n\n\t/* Check for memory type validity */\n\tswitch (address & VMX_EPTP_MT_MASK) {\n\tcase VMX_EPTP_MT_UC:\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT))\n\t\t\treturn false;\n\t\tbreak;\n\tcase VMX_EPTP_MT_WB:\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t/* only 4 levels page-walk length are valid */\n\tif ((address & VMX_EPTP_PWL_MASK) != VMX_EPTP_PWL_4)\n\t\treturn false;\n\n\t/* Reserved bits should not be set */\n\tif (address >> maxphyaddr || ((address >> 7) & 0x1f))\n\t\treturn false;\n\n\t/* AD, if set, should be supported */\n\tif (address & VMX_EPTP_AD_ENABLE_BIT) {\n\t\tif (!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmcs12 *vmcs12)\n{\n\tu32 index = vcpu->arch.regs[VCPU_REGS_RCX];\n\tu64 address;\n\tbool accessed_dirty;\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (!nested_cpu_has_eptp_switching(vmcs12) ||\n\t    !nested_cpu_has_ept(vmcs12))\n\t\treturn 1;\n\n\tif (index >= VMFUNC_EPTP_ENTRIES)\n\t\treturn 1;\n\n\n\tif (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,\n\t\t\t\t     &address, index * 8, 8))\n\t\treturn 1;\n\n\taccessed_dirty = !!(address & VMX_EPTP_AD_ENABLE_BIT);\n\n\t/*\n\t * If the (L2) guest does a vmfunc to the currently\n\t * active ept pointer, we don't have to do anything else\n\t */\n\tif (vmcs12->ept_pointer != address) {\n\t\tif (!valid_ept_address(vcpu, address))\n\t\t\treturn 1;\n\n\t\tkvm_mmu_unload(vcpu);\n\t\tmmu->ept_ad = accessed_dirty;\n\t\tmmu->base_role.ad_disabled = !accessed_dirty;\n\t\tvmcs12->ept_pointer = address;\n\t\t/*\n\t\t * TODO: Check what's the correct approach in case\n\t\t * mmu reload fails. Currently, we just let the next\n\t\t * reload potentially fail\n\t\t */\n\t\tkvm_mmu_reload(vcpu);\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_vmfunc(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12;\n\tu32 function = vcpu->arch.regs[VCPU_REGS_RAX];\n\n\t/*\n\t * VMFUNC is only supported for nested guests, but we always enable the\n\t * secondary control for simplicity; for non-nested mode, fake that we\n\t * didn't by injecting #UD.\n\t */\n\tif (!is_guest_mode(vcpu)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcs12 = get_vmcs12(vcpu);\n\tif ((vmcs12->vm_function_control & (1 << function)) == 0)\n\t\tgoto fail;\n\n\tswitch (function) {\n\tcase 0:\n\t\tif (nested_vmx_eptp_switching(vcpu, vmcs12))\n\t\t\tgoto fail;\n\t\tbreak;\n\tdefault:\n\t\tgoto fail;\n\t}\n\treturn kvm_skip_emulated_instruction(vcpu);\n\nfail:\n\tnested_vmx_vmexit(vcpu, vmx->exit_reason,\n\t\t\t  vmcs_read32(VM_EXIT_INTR_INFO),\n\t\t\t  vmcs_readl(EXIT_QUALIFICATION));\n\treturn 1;\n}\n\n/*\n * The exit handlers return 1 if the exit was handled fully and guest execution\n * may resume.  Otherwise they set the kvm_run parameter to indicate what needs\n * to be done to userspace and return 0.\n */\nstatic int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {\n\t[EXIT_REASON_EXCEPTION_NMI]           = handle_exception,\n\t[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,\n\t[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,\n\t[EXIT_REASON_NMI_WINDOW]\t      = handle_nmi_window,\n\t[EXIT_REASON_IO_INSTRUCTION]          = handle_io,\n\t[EXIT_REASON_CR_ACCESS]               = handle_cr,\n\t[EXIT_REASON_DR_ACCESS]               = handle_dr,\n\t[EXIT_REASON_CPUID]                   = handle_cpuid,\n\t[EXIT_REASON_MSR_READ]                = handle_rdmsr,\n\t[EXIT_REASON_MSR_WRITE]               = handle_wrmsr,\n\t[EXIT_REASON_PENDING_INTERRUPT]       = handle_interrupt_window,\n\t[EXIT_REASON_HLT]                     = handle_halt,\n\t[EXIT_REASON_INVD]\t\t      = handle_invd,\n\t[EXIT_REASON_INVLPG]\t\t      = handle_invlpg,\n\t[EXIT_REASON_RDPMC]                   = handle_rdpmc,\n\t[EXIT_REASON_VMCALL]                  = handle_vmcall,\n\t[EXIT_REASON_VMCLEAR]\t              = handle_vmclear,\n\t[EXIT_REASON_VMLAUNCH]                = handle_vmlaunch,\n\t[EXIT_REASON_VMPTRLD]                 = handle_vmptrld,\n\t[EXIT_REASON_VMPTRST]                 = handle_vmptrst,\n\t[EXIT_REASON_VMREAD]                  = handle_vmread,\n\t[EXIT_REASON_VMRESUME]                = handle_vmresume,\n\t[EXIT_REASON_VMWRITE]                 = handle_vmwrite,\n\t[EXIT_REASON_VMOFF]                   = handle_vmoff,\n\t[EXIT_REASON_VMON]                    = handle_vmon,\n\t[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,\n\t[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,\n\t[EXIT_REASON_APIC_WRITE]              = handle_apic_write,\n\t[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,\n\t[EXIT_REASON_WBINVD]                  = handle_wbinvd,\n\t[EXIT_REASON_XSETBV]                  = handle_xsetbv,\n\t[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,\n\t[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,\n\t[EXIT_REASON_GDTR_IDTR]\t\t      = handle_desc,\n\t[EXIT_REASON_LDTR_TR]\t\t      = handle_desc,\n\t[EXIT_REASON_EPT_VIOLATION]\t      = handle_ept_violation,\n\t[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,\n\t[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,\n\t[EXIT_REASON_MWAIT_INSTRUCTION]\t      = handle_mwait,\n\t[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,\n\t[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,\n\t[EXIT_REASON_INVEPT]                  = handle_invept,\n\t[EXIT_REASON_INVVPID]                 = handle_invvpid,\n\t[EXIT_REASON_RDRAND]                  = handle_invalid_op,\n\t[EXIT_REASON_RDSEED]                  = handle_invalid_op,\n\t[EXIT_REASON_XSAVES]                  = handle_xsaves,\n\t[EXIT_REASON_XRSTORS]                 = handle_xrstors,\n\t[EXIT_REASON_PML_FULL]\t\t      = handle_pml_full,\n\t[EXIT_REASON_VMFUNC]                  = handle_vmfunc,\n\t[EXIT_REASON_PREEMPTION_TIMER]\t      = handle_preemption_timer,\n};\n\nstatic const int kvm_vmx_max_exit_handlers =\n\tARRAY_SIZE(kvm_vmx_exit_handlers);\n\nstatic bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification;\n\tgpa_t bitmap, last_bitmap;\n\tunsigned int port;\n\tint size;\n\tu8 b;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_UNCOND_IO_EXITING);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\n\tlast_bitmap = (gpa_t)-1;\n\tb = -1;\n\n\twhile (size > 0) {\n\t\tif (port < 0x8000)\n\t\t\tbitmap = vmcs12->io_bitmap_a;\n\t\telse if (port < 0x10000)\n\t\t\tbitmap = vmcs12->io_bitmap_b;\n\t\telse\n\t\t\treturn true;\n\t\tbitmap += (port & 0x7fff) / 8;\n\n\t\tif (last_bitmap != bitmap)\n\t\t\tif (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))\n\t\t\t\treturn true;\n\t\tif (b & (1 << (port & 7)))\n\t\t\treturn true;\n\n\t\tport++;\n\t\tsize--;\n\t\tlast_bitmap = bitmap;\n\t}\n\n\treturn false;\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle an MSR access access,\n * rather than handle it ourselves in L0. I.e., check whether L1 expressed\n * disinterest in the current event (read or write a specific MSR) by using an\n * MSR bitmap. This may be the case even when L0 doesn't use MSR bitmaps.\n */\nstatic bool nested_vmx_exit_handled_msr(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12, u32 exit_reason)\n{\n\tu32 msr_index = vcpu->arch.regs[VCPU_REGS_RCX];\n\tgpa_t bitmap;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn true;\n\n\t/*\n\t * The MSR_BITMAP page is divided into four 1024-byte bitmaps,\n\t * for the four combinations of read/write and low/high MSR numbers.\n\t * First we need to figure out which of the four to use:\n\t */\n\tbitmap = vmcs12->msr_bitmap;\n\tif (exit_reason == EXIT_REASON_MSR_WRITE)\n\t\tbitmap += 2048;\n\tif (msr_index >= 0xc0000000) {\n\t\tmsr_index -= 0xc0000000;\n\t\tbitmap += 1024;\n\t}\n\n\t/* Then read the msr_index'th bit from this bitmap: */\n\tif (msr_index < 1024*8) {\n\t\tunsigned char b;\n\t\tif (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))\n\t\t\treturn true;\n\t\treturn 1 & (b >> (msr_index & 7));\n\t} else\n\t\treturn true; /* let L1 handle the wrong parameter */\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle a CR access exit,\n * rather than handle it ourselves in L0. I.e., check if L1 wanted to\n * intercept (via guest_host_mask etc.) the current event.\n */\nstatic bool nested_vmx_exit_handled_cr(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tint cr = exit_qualification & 15;\n\tint reg;\n\tunsigned long val;\n\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\treg = (exit_qualification >> 8) & 15;\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\tif (vmcs12->cr0_guest_host_mask &\n\t\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif ((vmcs12->cr3_target_count >= 1 &&\n\t\t\t\t\tvmcs12->cr3_target_value0 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 2 &&\n\t\t\t\t\tvmcs12->cr3_target_value1 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 3 &&\n\t\t\t\t\tvmcs12->cr3_target_value2 == val) ||\n\t\t\t\t(vmcs12->cr3_target_count >= 4 &&\n\t\t\t\t\tvmcs12->cr3_target_value3 == val))\n\t\t\t\treturn false;\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR3_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tif (vmcs12->cr4_guest_host_mask &\n\t\t\t    (vmcs12->cr4_read_shadow ^ val))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tif ((vmcs12->cr0_guest_host_mask & X86_CR0_TS) &&\n\t\t    (vmcs12->cr0_read_shadow & X86_CR0_TS))\n\t\t\treturn true;\n\t\tbreak;\n\tcase 1: /* mov from cr */\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR3_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR8_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\t/*\n\t\t * lmsw can change bits 1..3 of cr0, and only set bit 0 of\n\t\t * cr0. Other attempted changes are ignored, with no exit.\n\t\t */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\tif (vmcs12->cr0_guest_host_mask & 0xe &\n\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\treturn true;\n\t\tif ((vmcs12->cr0_guest_host_mask & 0x1) &&\n\t\t    !(vmcs12->cr0_read_shadow & 0x1) &&\n\t\t    (val & 0x1))\n\t\t\treturn true;\n\t\tbreak;\n\t}\n\treturn false;\n}\n\n/*\n * Return 1 if we should exit from L2 to L1 to handle an exit, or 0 if we\n * should handle it ourselves in L0 (and then continue L2). Only call this\n * when in is_guest_mode (L2).\n */\nstatic bool nested_vmx_exit_reflected(struct kvm_vcpu *vcpu, u32 exit_reason)\n{\n\tu32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (vmx->nested.nested_run_pending)\n\t\treturn false;\n\n\tif (unlikely(vmx->fail)) {\n\t\tpr_info_ratelimited(\"%s failed vm entry %x\\n\", __func__,\n\t\t\t\t    vmcs_read32(VM_INSTRUCTION_ERROR));\n\t\treturn true;\n\t}\n\n\t/*\n\t * The host physical addresses of some pages of guest memory\n\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t * Page). The CPU may write to these pages via their host\n\t * physical address while L2 is running, bypassing any\n\t * address-translation-based dirty tracking (e.g. EPT write\n\t * protection).\n\t *\n\t * Mark them dirty on every exit from L2 to prevent them from\n\t * getting out of sync with dirty tracking.\n\t */\n\tnested_mark_vmcs12_pages_dirty(vcpu);\n\n\ttrace_kvm_nested_vmexit(kvm_rip_read(vcpu), exit_reason,\n\t\t\t\tvmcs_readl(EXIT_QUALIFICATION),\n\t\t\t\tvmx->idt_vectoring_info,\n\t\t\t\tintr_info,\n\t\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t\t\t\tKVM_ISA_VMX);\n\n\tswitch (exit_reason) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tif (is_nmi(intr_info))\n\t\t\treturn false;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn !vmx->vcpu.arch.apf.host_apf_reason && enable_ept;\n\t\telse if (is_no_device(intr_info) &&\n\t\t\t !(vmcs12->guest_cr0 & X86_CR0_TS))\n\t\t\treturn false;\n\t\telse if (is_debug(intr_info) &&\n\t\t\t vcpu->guest_debug &\n\t\t\t (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn false;\n\t\telse if (is_breakpoint(intr_info) &&\n\t\t\t vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn false;\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn true;\n\tcase EXIT_REASON_PENDING_INTERRUPT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_INTR_PENDING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn true;\n\tcase EXIT_REASON_CPUID:\n\t\treturn true;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn true;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDRAND:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDRAND_EXITING);\n\tcase EXIT_REASON_RDSEED:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDSEED_EXITING);\n\tcase EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:\n\tcase EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:\n\t\t/*\n\t\t * VMX instructions trap unconditionally. This allows L1 to\n\t\t * emulate them for its L2 guest, i.e., allows 3-level nesting!\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn true;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_TRAP_FLAG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_TRAP_FLAG);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn false;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);\n\tcase EXIT_REASON_APIC_ACCESS:\n\tcase EXIT_REASON_APIC_WRITE:\n\tcase EXIT_REASON_EOI_INDUCED:\n\t\t/*\n\t\t * The controls for \"virtualize APIC accesses,\" \"APIC-\n\t\t * register virtualization,\" and \"virtual-interrupt\n\t\t * delivery\" only come from vmcs12.\n\t\t */\n\t\treturn true;\n\tcase EXIT_REASON_EPT_VIOLATION:\n\t\t/*\n\t\t * L0 always deals with the EPT violation. If nested EPT is\n\t\t * used, and the nested mmu code discovers that the address is\n\t\t * missing in the guest EPT table (EPT12), the EPT violation\n\t\t * will be injected with nested_ept_inject_page_fault()\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\t/*\n\t\t * L2 never uses directly L1's EPT, but rather L0's own EPT\n\t\t * table (shadow on EPT) or a merged EPT table that L0 built\n\t\t * (EPT on EPT). So any problems with the structure of the\n\t\t * table is L0's fault.\n\t\t */\n\t\treturn false;\n\tcase EXIT_REASON_INVPCID:\n\t\treturn\n\t\t\tnested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_INVPCID) &&\n\t\t\tnested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn true;\n\tcase EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:\n\t\t/*\n\t\t * This should never happen, since it is not possible to\n\t\t * set XSS to a non-zero value---neither in L1 nor in L2.\n\t\t * If if it were, XSS would have to be checked against\n\t\t * the XSS exit bitmap in vmcs12.\n\t\t */\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_XSAVES);\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn false;\n\tcase EXIT_REASON_PML_FULL:\n\t\t/* We emulate PML support to L1. */\n\t\treturn false;\n\tcase EXIT_REASON_VMFUNC:\n\t\t/* VM functions are emulated through L2->L0 vmexits. */\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic int nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason)\n{\n\tu32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\n\t/*\n\t * At this point, the exit interruption info in exit_intr_info\n\t * is only valid for EXCEPTION_NMI exits.  For EXTERNAL_INTERRUPT\n\t * we need to query the in-kernel LAPIC.\n\t */\n\tWARN_ON(exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT);\n\tif ((exit_intr_info &\n\t     (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) ==\n\t    (INTR_INFO_VALID_MASK | INTR_INFO_DELIVER_CODE_MASK)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tvmcs12->vm_exit_intr_error_code =\n\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\t}\n\n\tnested_vmx_vmexit(vcpu, exit_reason, exit_intr_info,\n\t\t\t  vmcs_readl(EXIT_QUALIFICATION));\n\treturn 1;\n}\n\nstatic void vmx_get_exit_info(struct kvm_vcpu *vcpu, u64 *info1, u64 *info2)\n{\n\t*info1 = vmcs_readl(EXIT_QUALIFICATION);\n\t*info2 = vmcs_read32(VM_EXIT_INTR_INFO);\n}\n\nstatic void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)\n{\n\tif (vmx->pml_pg) {\n\t\t__free_page(vmx->pml_pg);\n\t\tvmx->pml_pg = NULL;\n\t}\n}\n\nstatic void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 *pml_buf;\n\tu16 pml_idx;\n\n\tpml_idx = vmcs_read16(GUEST_PML_INDEX);\n\n\t/* Do nothing if PML buffer is empty */\n\tif (pml_idx == (PML_ENTITY_NUM - 1))\n\t\treturn;\n\n\t/* PML index always points to next available PML buffer entity */\n\tif (pml_idx >= PML_ENTITY_NUM)\n\t\tpml_idx = 0;\n\telse\n\t\tpml_idx++;\n\n\tpml_buf = page_address(vmx->pml_pg);\n\tfor (; pml_idx < PML_ENTITY_NUM; pml_idx++) {\n\t\tu64 gpa;\n\n\t\tgpa = pml_buf[pml_idx];\n\t\tWARN_ON(gpa & (PAGE_SIZE - 1));\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);\n\t}\n\n\t/* reset PML index */\n\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n}\n\n/*\n * Flush all vcpus' PML buffer and update logged GPAs to dirty_bitmap.\n * Called before reporting dirty_bitmap to userspace.\n */\nstatic void kvm_flush_pml_buffers(struct kvm *kvm)\n{\n\tint i;\n\tstruct kvm_vcpu *vcpu;\n\t/*\n\t * We only need to kick vcpu out of guest mode here, as PML buffer\n\t * is flushed at beginning of all VMEXITs, and it's obvious that only\n\t * vcpus running in guest are possible to have unflushed GPAs in PML\n\t * buffer.\n\t */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nstatic void vmx_dump_sel(char *name, uint32_t sel)\n{\n\tpr_err(\"%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read16(sel),\n\t       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),\n\t       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),\n\t       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));\n}\n\nstatic void vmx_dump_dtsel(char *name, uint32_t limit)\n{\n\tpr_err(\"%s                           limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read32(limit),\n\t       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));\n}\n\nstatic void dump_vmcs(void)\n{\n\tu32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);\n\tu32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);\n\tu32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);\n\tu32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);\n\tu32 secondary_exec_control = 0;\n\tunsigned long cr4 = vmcs_readl(GUEST_CR4);\n\tu64 efer = vmcs_read64(GUEST_IA32_EFER);\n\tint i, n;\n\n\tif (cpu_has_secondary_exec_ctrls())\n\t\tsecondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\n\tpr_err(\"*** Guest State ***\\n\");\n\tpr_err(\"CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),\n\t       vmcs_readl(CR0_GUEST_HOST_MASK));\n\tpr_err(\"CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));\n\tpr_err(\"CR3 = 0x%016lx\\n\", vmcs_readl(GUEST_CR3));\n\tif ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&\n\t    (cr4 & X86_CR4_PAE) && !(efer & EFER_LMA))\n\t{\n\t\tpr_err(\"PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));\n\t\tpr_err(\"PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));\n\t}\n\tpr_err(\"RSP = 0x%016lx  RIP = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));\n\tpr_err(\"RFLAGS=0x%08lx         DR7 = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(GUEST_SYSENTER_ESP),\n\t       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));\n\tvmx_dump_sel(\"CS:  \", GUEST_CS_SELECTOR);\n\tvmx_dump_sel(\"DS:  \", GUEST_DS_SELECTOR);\n\tvmx_dump_sel(\"SS:  \", GUEST_SS_SELECTOR);\n\tvmx_dump_sel(\"ES:  \", GUEST_ES_SELECTOR);\n\tvmx_dump_sel(\"FS:  \", GUEST_FS_SELECTOR);\n\tvmx_dump_sel(\"GS:  \", GUEST_GS_SELECTOR);\n\tvmx_dump_dtsel(\"GDTR:\", GUEST_GDTR_LIMIT);\n\tvmx_dump_sel(\"LDTR:\", GUEST_LDTR_SELECTOR);\n\tvmx_dump_dtsel(\"IDTR:\", GUEST_IDTR_LIMIT);\n\tvmx_dump_sel(\"TR:  \", GUEST_TR_SELECTOR);\n\tif ((vmexit_ctl & (VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER)) ||\n\t    (vmentry_ctl & (VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_IA32_EFER)))\n\t\tpr_err(\"EFER =     0x%016llx  PAT = 0x%016llx\\n\",\n\t\t       efer, vmcs_read64(GUEST_IA32_PAT));\n\tpr_err(\"DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\\n\",\n\t       vmcs_read64(GUEST_IA32_DEBUGCTL),\n\t       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));\n\tif (cpu_has_load_perf_global_ctrl &&\n\t    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));\n\tif (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)\n\t\tpr_err(\"BndCfgS = 0x%016llx\\n\", vmcs_read64(GUEST_BNDCFGS));\n\tpr_err(\"Interruptibility = %08x  ActivityState = %08x\\n\",\n\t       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),\n\t       vmcs_read32(GUEST_ACTIVITY_STATE));\n\tif (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\tpr_err(\"InterruptStatus = %04x\\n\",\n\t\t       vmcs_read16(GUEST_INTR_STATUS));\n\n\tpr_err(\"*** Host State ***\\n\");\n\tpr_err(\"RIP = 0x%016lx  RSP = 0x%016lx\\n\",\n\t       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));\n\tpr_err(\"CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\\n\",\n\t       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),\n\t       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),\n\t       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),\n\t       vmcs_read16(HOST_TR_SELECTOR));\n\tpr_err(\"FSBase=%016lx GSBase=%016lx TRBase=%016lx\\n\",\n\t       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),\n\t       vmcs_readl(HOST_TR_BASE));\n\tpr_err(\"GDTBase=%016lx IDTBase=%016lx\\n\",\n\t       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));\n\tpr_err(\"CR0=%016lx CR3=%016lx CR4=%016lx\\n\",\n\t       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),\n\t       vmcs_readl(HOST_CR4));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(HOST_IA32_SYSENTER_ESP),\n\t       vmcs_read32(HOST_IA32_SYSENTER_CS),\n\t       vmcs_readl(HOST_IA32_SYSENTER_EIP));\n\tif (vmexit_ctl & (VM_EXIT_LOAD_IA32_PAT | VM_EXIT_LOAD_IA32_EFER))\n\t\tpr_err(\"EFER = 0x%016llx  PAT = 0x%016llx\\n\",\n\t\t       vmcs_read64(HOST_IA32_EFER),\n\t\t       vmcs_read64(HOST_IA32_PAT));\n\tif (cpu_has_load_perf_global_ctrl &&\n\t    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));\n\n\tpr_err(\"*** Control State ***\\n\");\n\tpr_err(\"PinBased=%08x CPUBased=%08x SecondaryExec=%08x\\n\",\n\t       pin_based_exec_ctrl, cpu_based_exec_ctrl, secondary_exec_control);\n\tpr_err(\"EntryControls=%08x ExitControls=%08x\\n\", vmentry_ctl, vmexit_ctl);\n\tpr_err(\"ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\\n\",\n\t       vmcs_read32(EXCEPTION_BITMAP),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));\n\tpr_err(\"VMEntry: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),\n\t       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));\n\tpr_err(\"VMExit: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_EXIT_INTR_INFO),\n\t       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));\n\tpr_err(\"        reason=%08x qualification=%016lx\\n\",\n\t       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));\n\tpr_err(\"IDTVectoring: info=%08x errcode=%08x\\n\",\n\t       vmcs_read32(IDT_VECTORING_INFO_FIELD),\n\t       vmcs_read32(IDT_VECTORING_ERROR_CODE));\n\tpr_err(\"TSC Offset = 0x%016llx\\n\", vmcs_read64(TSC_OFFSET));\n\tif (secondary_exec_control & SECONDARY_EXEC_TSC_SCALING)\n\t\tpr_err(\"TSC Multiplier = 0x%016llx\\n\",\n\t\t       vmcs_read64(TSC_MULTIPLIER));\n\tif (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW)\n\t\tpr_err(\"TPR Threshold = 0x%02x\\n\", vmcs_read32(TPR_THRESHOLD));\n\tif (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)\n\t\tpr_err(\"PostedIntrVec = 0x%02x\\n\", vmcs_read16(POSTED_INTR_NV));\n\tif ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))\n\t\tpr_err(\"EPT pointer = 0x%016llx\\n\", vmcs_read64(EPT_POINTER));\n\tn = vmcs_read32(CR3_TARGET_COUNT);\n\tfor (i = 0; i + 1 < n; i += 4)\n\t\tpr_err(\"CR3 target%u=%016lx target%u=%016lx\\n\",\n\t\t       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2),\n\t\t       i + 1, vmcs_readl(CR3_TARGET_VALUE0 + i * 2 + 2));\n\tif (i < n)\n\t\tpr_err(\"CR3 target%u=%016lx\\n\",\n\t\t       i, vmcs_readl(CR3_TARGET_VALUE0 + i * 2));\n\tif (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)\n\t\tpr_err(\"PLE Gap=%08x Window=%08x\\n\",\n\t\t       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));\n\tif (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)\n\t\tpr_err(\"Virtual processor ID = 0x%04x\\n\",\n\t\t       vmcs_read16(VIRTUAL_PROCESSOR_ID));\n}\n\n/*\n * The guest has exited.  See if we can fix it or if we need userspace\n * assistance.\n */\nstatic int vmx_handle_exit(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\n\ttrace_kvm_exit(exit_reason, vcpu, KVM_ISA_VMX);\n\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.\n\t */\n\tif (enable_pml)\n\t\tvmx_flush_pml_buffer(vcpu);\n\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\n\tif (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason))\n\t\treturn nested_vmx_reflect_vmexit(vcpu, exit_reason);\n\n\tif (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(vmx->fail)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\t(exit_reason != EXIT_REASON_EXCEPTION_NMI &&\n\t\t\texit_reason != EXIT_REASON_EPT_VIOLATION &&\n\t\t\texit_reason != EXIT_REASON_PML_FULL &&\n\t\t\texit_reason != EXIT_REASON_TASK_SWITCH)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.ndata++;\n\t\t\tvcpu->run->internal.data[3] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (vmx_interrupt_allowed(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}\n\n\tif (exit_reason < kvm_vmx_max_exit_handlers\n\t    && kvm_vmx_exit_handlers[exit_reason])\n\t\treturn kvm_vmx_exit_handlers[exit_reason](vcpu);\n\telse {\n\t\tvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\n\t\t\t\texit_reason);\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n}\n\nstatic void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t\tnested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn;\n\n\tif (irr == -1 || tpr < irr) {\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t\treturn;\n\t}\n\n\tvmcs_write32(TPR_THRESHOLD, irr);\n}\n\nstatic void vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)\n{\n\tu32 sec_exec_control;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\t/* Postpone execution until vmcs01 is the current VMCS. */\n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.change_vmcs01_virtual_apic_mode = true;\n\t\treturn;\n\t}\n\n\tif (!cpu_need_tpr_shadow(vcpu))\n\t\treturn;\n\n\tsec_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\tsec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);\n\n\tswitch (kvm_get_apic_mode(vcpu)) {\n\tcase LAPIC_MODE_INVALID:\n\t\tWARN_ONCE(true, \"Invalid local APIC state\");\n\tcase LAPIC_MODE_DISABLED:\n\t\tbreak;\n\tcase LAPIC_MODE_XAPIC:\n\t\tif (flexpriority_enabled) {\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\t\t\tvmx_flush_tlb(vcpu, true);\n\t\t}\n\t\tbreak;\n\tcase LAPIC_MODE_X2APIC:\n\t\tif (cpu_has_vmx_virtualize_x2apic_mode())\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\t\tbreak;\n\t}\n\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, sec_exec_control);\n\n\tvmx_update_msr_bitmap(vcpu);\n}\n\nstatic void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu, hpa_t hpa)\n{\n\tif (!is_guest_mode(vcpu)) {\n\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n}\n\nstatic void vmx_hwapic_isr_update(struct kvm_vcpu *vcpu, int max_isr)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (max_isr == -1)\n\t\tmax_isr = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = status >> 8;\n\tif (max_isr != old) {\n\t\tstatus &= 0xff;\n\t\tstatus |= max_isr << 8;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_set_rvi(int vector)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (vector == -1)\n\t\tvector = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = (u8)status & 0xff;\n\tif ((u8)vector != old) {\n\t\tstatus &= ~0xff;\n\t\tstatus |= (u8)vector;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)\n{\n\t/*\n\t * When running L2, updating RVI is only relevant when\n\t * vmcs12 virtual-interrupt-delivery enabled.\n\t * However, it can be enabled only when L1 also\n\t * intercepts external-interrupts and in that case\n\t * we should not update vmcs02 RVI but instead intercept\n\t * interrupt. Therefore, do nothing when running L2.\n\t */\n\tif (!is_guest_mode(vcpu))\n\t\tvmx_set_rvi(max_irr);\n}\n\nstatic int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tbool max_irr_updated;\n\n\tWARN_ON(!vcpu->arch.apicv_active);\n\tif (pi_test_on(&vmx->pi_desc)) {\n\t\tpi_clear_on(&vmx->pi_desc);\n\t\t/*\n\t\t * IOMMU can write to PIR.ON, so the barrier matters even on UP.\n\t\t * But on x86 this is just a compiler barrier anyway.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tmax_irr_updated =\n\t\t\tkvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);\n\n\t\t/*\n\t\t * If we are running L2 and L1 has a new pending interrupt\n\t\t * which can be injected, we should re-evaluate\n\t\t * what should be done with this new L1 interrupt.\n\t\t * If L1 intercepts external-interrupts, we should\n\t\t * exit from L2 to L1. Otherwise, interrupt should be\n\t\t * delivered directly to L2.\n\t\t */\n\t\tif (is_guest_mode(vcpu) && max_irr_updated) {\n\t\t\tif (nested_exit_on_intr(vcpu))\n\t\t\t\tkvm_vcpu_exiting_guest_mode(vcpu);\n\t\t\telse\n\t\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t}\n\t} else {\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\t}\n\tvmx_hwapic_irr_update(vcpu, max_irr);\n\treturn max_irr;\n}\n\nstatic void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)\n{\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\tvmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);\n\tvmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);\n\tvmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);\n\tvmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);\n}\n\nstatic void vmx_apicv_post_state_restore(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tpi_clear_on(&vmx->pi_desc);\n\tmemset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));\n}\n\nstatic void vmx_complete_atomic_exit(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info = 0;\n\tu16 basic_exit_reason = (u16)vmx->exit_reason;\n\n\tif (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tif (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tvmx->exit_intr_info = exit_intr_info;\n\n\t/* if exit due to PF check for async PF */\n\tif (is_page_fault(exit_intr_info))\n\t\tvmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||\n\t    is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif (is_nmi(exit_intr_info)) {\n\t\tkvm_before_interrupt(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_interrupt(&vmx->vcpu);\n\t}\n}\n\nstatic void vmx_handle_external_intr(struct kvm_vcpu *vcpu)\n{\n\tu32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\n\tif ((exit_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK))\n\t\t\t== (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) {\n\t\tunsigned int vector;\n\t\tunsigned long entry;\n\t\tgate_desc *desc;\n\t\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n#ifdef CONFIG_X86_64\n\t\tunsigned long tmp;\n#endif\n\n\t\tvector =  exit_intr_info & INTR_INFO_VECTOR_MASK;\n\t\tdesc = (gate_desc *)vmx->host_idt_base + vector;\n\t\tentry = gate_offset(desc);\n\t\tasm volatile(\n#ifdef CONFIG_X86_64\n\t\t\t\"mov %%\" _ASM_SP \", %[sp]\\n\\t\"\n\t\t\t\"and $0xfffffffffffffff0, %%\" _ASM_SP \"\\n\\t\"\n\t\t\t\"push $%c[ss]\\n\\t\"\n\t\t\t\"push %[sp]\\n\\t\"\n#endif\n\t\t\t\"pushf\\n\\t\"\n\t\t\t__ASM_SIZE(push) \" $%c[cs]\\n\\t\"\n\t\t\tCALL_NOSPEC\n\t\t\t:\n#ifdef CONFIG_X86_64\n\t\t\t[sp]\"=&r\"(tmp),\n#endif\n\t\t\tASM_CALL_CONSTRAINT\n\t\t\t:\n\t\t\tTHUNK_TARGET(entry),\n\t\t\t[ss]\"i\"(__KERNEL_DS),\n\t\t\t[cs]\"i\"(__KERNEL_CS)\n\t\t\t);\n\t}\n}\nSTACK_FRAME_NON_STANDARD(vmx_handle_external_intr);\n\nstatic bool vmx_has_high_real_mode_segbase(void)\n{\n\treturn enable_unrestricted_guest || emulate_invalid_guest_state;\n}\n\nstatic bool vmx_mpx_supported(void)\n{\n\treturn (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&\n\t\t(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);\n}\n\nstatic bool vmx_xsaves_supported(void)\n{\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_XSAVES;\n}\n\nstatic void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info;\n\tbool unblock_nmi;\n\tu8 vector;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tif (enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\t\treturn;\n\t\t/*\n\t\t * Can't use vmx->exit_intr_info since we're not sure what\n\t\t * the exit reason is.\n\t\t */\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\t\tunblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;\n\t\tvector = exit_intr_info & INTR_INFO_VECTOR_MASK;\n\t\t/*\n\t\t * SDM 3: 27.7.1.2 (September 2008)\n\t\t * Re-set bit \"block by NMI\" before VM entry if vmexit caused by\n\t\t * a guest IRET fault.\n\t\t * SDM 3: 23.2.2 (September 2008)\n\t\t * Bit 12 is undefined in any of the following cases:\n\t\t *  If the VM exit sets the valid bit in the IDT-vectoring\n\t\t *   information field.\n\t\t *  If the VM exit is due to a double fault.\n\t\t */\n\t\tif ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&\n\t\t    vector != DF_VECTOR && !idtv_info_valid)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)\n\t\t\t\t  & GUEST_INTR_STATE_NMI);\n\t} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->vnmi_blocked_time +=\n\t\t\tktime_to_ns(ktime_sub(ktime_get(),\n\t\t\t\t\t      vmx->loaded_vmcs->entry_time));\n}\n\nstatic void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,\n\t\t\t\t      u32 idt_vectoring_info,\n\t\t\t\t      int instr_len_field,\n\t\t\t\t      int error_code_field)\n{\n\tu8 vector;\n\tint type;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tif (!idtv_info_valid)\n\t\treturn;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;\n\ttype = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;\n\n\tswitch (type) {\n\tcase INTR_TYPE_NMI_INTR:\n\t\tvcpu->arch.nmi_injected = true;\n\t\t/*\n\t\t * SDM 3: 27.7.1.2 (September 2008)\n\t\t * Clear bit \"block by NMI\" before VM entry if a NMI\n\t\t * delivery faulted.\n\t\t */\n\t\tvmx_set_nmi_mask(vcpu, false);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\t/* fall through */\n\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\tif (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\tu32 err = vmcs_read32(error_code_field);\n\t\t\tkvm_requeue_exception_e(vcpu, vector, err);\n\t\t} else\n\t\t\tkvm_requeue_exception(vcpu, vector);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_INTR:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\t/* fall through */\n\tcase INTR_TYPE_EXT_INTR:\n\t\tkvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void vmx_complete_interrupts(struct vcpu_vmx *vmx)\n{\n\t__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,\n\t\t\t\t  VM_EXIT_INSTRUCTION_LEN,\n\t\t\t\t  IDT_VECTORING_ERROR_CODE);\n}\n\nstatic void vmx_cancel_injection(struct kvm_vcpu *vcpu)\n{\n\t__vmx_complete_interrupts(vcpu,\n\t\t\t\t  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t\t\t\t  VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t\t  VM_ENTRY_EXCEPTION_ERROR_CODE);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n}\n\nstatic void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)\n{\n\tint i, nr_msrs;\n\tstruct perf_guest_switch_msr *msrs;\n\n\tmsrs = perf_guest_get_msrs(&nr_msrs);\n\n\tif (!msrs)\n\t\treturn;\n\n\tfor (i = 0; i < nr_msrs; i++)\n\t\tif (msrs[i].host == msrs[i].guest)\n\t\t\tclear_atomic_switch_msr(vmx, msrs[i].msr);\n\t\telse\n\t\t\tadd_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,\n\t\t\t\t\tmsrs[i].host);\n}\n\nstatic void vmx_arm_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 tscl;\n\tu32 delta_tsc;\n\n\tif (vmx->hv_deadline_tsc == -1)\n\t\treturn;\n\n\ttscl = rdtsc();\n\tif (vmx->hv_deadline_tsc > tscl)\n\t\t/* sure to be 32 bit only because checked on set_hv_timer */\n\t\tdelta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>\n\t\t\tcpu_preemption_timer_multi);\n\telse\n\t\tdelta_tsc = 0;\n\n\tvmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);\n}\n\nstatic void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long cr3, cr4, evmcs_rsp;\n\n\t/* Record the guest's net vcpu time for enforced NMI injections. */\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->entry_time = ktime_get();\n\n\t/* Don't enter VMX if guest state is invalid, let the exit handler\n\t   start emulation until we arrive back to a valid state */\n\tif (vmx->emulation_required)\n\t\treturn;\n\n\tif (vmx->ple_window_dirty) {\n\t\tvmx->ple_window_dirty = false;\n\t\tvmcs_write32(PLE_WINDOW, vmx->ple_window);\n\t}\n\n\tif (vmx->nested.sync_shadow_vmcs) {\n\t\tcopy_vmcs12_to_shadow(vmx);\n\t\tvmx->nested.sync_shadow_vmcs = false;\n\t}\n\n\tif (test_bit(VCPU_REGS_RSP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n\tif (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))\n\t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n\n\tcr3 = __get_current_cr3_fast();\n\tif (unlikely(cr3 != vmx->loaded_vmcs->vmcs_host_cr3)) {\n\t\tvmcs_writel(HOST_CR3, cr3);\n\t\tvmx->loaded_vmcs->vmcs_host_cr3 = cr3;\n\t}\n\n\tcr4 = cr4_read_shadow();\n\tif (unlikely(cr4 != vmx->loaded_vmcs->vmcs_host_cr4)) {\n\t\tvmcs_writel(HOST_CR4, cr4);\n\t\tvmx->loaded_vmcs->vmcs_host_cr4 = cr4;\n\t}\n\n\t/* When single-stepping over STI and MOV SS, we must clear the\n\t * corresponding interruptibility bits in the guest state. Otherwise\n\t * vmentry fails as it then expects bit 14 (BS) in pending debug\n\t * exceptions being set, but that's not correct for the guest debugging\n\t * case. */\n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    kvm_read_cr4_bits(vcpu, X86_CR4_PKE) &&\n\t    vcpu->arch.pkru != vmx->host_pkru)\n\t\t__write_pkru(vcpu->arch.pkru);\n\n\tatomic_switch_perf_msrs(vmx);\n\n\tvmx_arm_hv_timer(vcpu);\n\n\t/*\n\t * If this vCPU has touched SPEC_CTRL, restore the guest's value if\n\t * it's non-zero. Since vmentry is serialising on affected CPUs, there\n\t * is no need to worry about the conditional branch over the wrmsr\n\t * being speculatively taken.\n\t */\n\tif (vmx->spec_ctrl)\n\t\tnative_wrmsrl(MSR_IA32_SPEC_CTRL, vmx->spec_ctrl);\n\n\tvmx->__launched = vmx->loaded_vmcs->launched;\n\n\tevmcs_rsp = static_branch_unlikely(&enable_evmcs) ?\n\t\t(unsigned long)&current_evmcs->host_rsp : 0;\n\n\tasm(\n\t\t/* Store host registers */\n\t\t\"push %%\" _ASM_DX \"; push %%\" _ASM_BP \";\"\n\t\t\"push %%\" _ASM_CX \" \\n\\t\" /* placeholder for guest rcx */\n\t\t\"push %%\" _ASM_CX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t\"je 1f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", %c[host_rsp](%0) \\n\\t\"\n\t\t/* Avoid VMWRITE when Enlightened VMCS is in use */\n\t\t\"test %%\" _ASM_SI \", %%\" _ASM_SI \" \\n\\t\"\n\t\t\"jz 2f \\n\\t\"\n\t\t\"mov %%\" _ASM_SP \", (%%\" _ASM_SI \") \\n\\t\"\n\t\t\"jmp 1f \\n\\t\"\n\t\t\"2: \\n\\t\"\n\t\t__ex(ASM_VMX_VMWRITE_RSP_RDX) \"\\n\\t\"\n\t\t\"1: \\n\\t\"\n\t\t/* Reload cr2 if changed */\n\t\t\"mov %c[cr2](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %%cr2, %%\" _ASM_DX \" \\n\\t\"\n\t\t\"cmp %%\" _ASM_AX \", %%\" _ASM_DX \" \\n\\t\"\n\t\t\"je 3f \\n\\t\"\n\t\t\"mov %%\" _ASM_AX\", %%cr2 \\n\\t\"\n\t\t\"3: \\n\\t\"\n\t\t/* Check if vmlaunch of vmresume is needed */\n\t\t\"cmpl $0, %c[launched](%0) \\n\\t\"\n\t\t/* Load guest registers.  Don't clobber flags. */\n\t\t\"mov %c[rax](%0), %%\" _ASM_AX \" \\n\\t\"\n\t\t\"mov %c[rbx](%0), %%\" _ASM_BX \" \\n\\t\"\n\t\t\"mov %c[rdx](%0), %%\" _ASM_DX \" \\n\\t\"\n\t\t\"mov %c[rsi](%0), %%\" _ASM_SI \" \\n\\t\"\n\t\t\"mov %c[rdi](%0), %%\" _ASM_DI \" \\n\\t\"\n\t\t\"mov %c[rbp](%0), %%\" _ASM_BP \" \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %c[r8](%0),  %%r8  \\n\\t\"\n\t\t\"mov %c[r9](%0),  %%r9  \\n\\t\"\n\t\t\"mov %c[r10](%0), %%r10 \\n\\t\"\n\t\t\"mov %c[r11](%0), %%r11 \\n\\t\"\n\t\t\"mov %c[r12](%0), %%r12 \\n\\t\"\n\t\t\"mov %c[r13](%0), %%r13 \\n\\t\"\n\t\t\"mov %c[r14](%0), %%r14 \\n\\t\"\n\t\t\"mov %c[r15](%0), %%r15 \\n\\t\"\n#endif\n\t\t\"mov %c[rcx](%0), %%\" _ASM_CX \" \\n\\t\" /* kills %0 (ecx) */\n\n\t\t/* Enter guest mode */\n\t\t\"jne 1f \\n\\t\"\n\t\t__ex(ASM_VMX_VMLAUNCH) \"\\n\\t\"\n\t\t\"jmp 2f \\n\\t\"\n\t\t\"1: \" __ex(ASM_VMX_VMRESUME) \"\\n\\t\"\n\t\t\"2: \"\n\t\t/* Save guest registers, load host registers, keep flags */\n\t\t\"mov %0, %c[wordsize](%%\" _ASM_SP \") \\n\\t\"\n\t\t\"pop %0 \\n\\t\"\n\t\t\"setbe %c[fail](%0)\\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[rax](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BX \", %c[rbx](%0) \\n\\t\"\n\t\t__ASM_SIZE(pop) \" %c[rcx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DX \", %c[rdx](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_SI \", %c[rsi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_DI \", %c[rdi](%0) \\n\\t\"\n\t\t\"mov %%\" _ASM_BP \", %c[rbp](%0) \\n\\t\"\n#ifdef CONFIG_X86_64\n\t\t\"mov %%r8,  %c[r8](%0) \\n\\t\"\n\t\t\"mov %%r9,  %c[r9](%0) \\n\\t\"\n\t\t\"mov %%r10, %c[r10](%0) \\n\\t\"\n\t\t\"mov %%r11, %c[r11](%0) \\n\\t\"\n\t\t\"mov %%r12, %c[r12](%0) \\n\\t\"\n\t\t\"mov %%r13, %c[r13](%0) \\n\\t\"\n\t\t\"mov %%r14, %c[r14](%0) \\n\\t\"\n\t\t\"mov %%r15, %c[r15](%0) \\n\\t\"\n\t\t\"xor %%r8d,  %%r8d \\n\\t\"\n\t\t\"xor %%r9d,  %%r9d \\n\\t\"\n\t\t\"xor %%r10d, %%r10d \\n\\t\"\n\t\t\"xor %%r11d, %%r11d \\n\\t\"\n\t\t\"xor %%r12d, %%r12d \\n\\t\"\n\t\t\"xor %%r13d, %%r13d \\n\\t\"\n\t\t\"xor %%r14d, %%r14d \\n\\t\"\n\t\t\"xor %%r15d, %%r15d \\n\\t\"\n#endif\n\t\t\"mov %%cr2, %%\" _ASM_AX \"   \\n\\t\"\n\t\t\"mov %%\" _ASM_AX \", %c[cr2](%0) \\n\\t\"\n\n\t\t\"xor %%eax, %%eax \\n\\t\"\n\t\t\"xor %%ebx, %%ebx \\n\\t\"\n\t\t\"xor %%esi, %%esi \\n\\t\"\n\t\t\"xor %%edi, %%edi \\n\\t\"\n\t\t\"pop  %%\" _ASM_BP \"; pop  %%\" _ASM_DX \" \\n\\t\"\n\t\t\".pushsection .rodata \\n\\t\"\n\t\t\".global vmx_return \\n\\t\"\n\t\t\"vmx_return: \" _ASM_PTR \" 2b \\n\\t\"\n\t\t\".popsection\"\n\t      : : \"c\"(vmx), \"d\"((unsigned long)HOST_RSP), \"S\"(evmcs_rsp),\n\t\t[launched]\"i\"(offsetof(struct vcpu_vmx, __launched)),\n\t\t[fail]\"i\"(offsetof(struct vcpu_vmx, fail)),\n\t\t[host_rsp]\"i\"(offsetof(struct vcpu_vmx, host_rsp)),\n\t\t[rax]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RAX])),\n\t\t[rbx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBX])),\n\t\t[rcx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RCX])),\n\t\t[rdx]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDX])),\n\t\t[rsi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RSI])),\n\t\t[rdi]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RDI])),\n\t\t[rbp]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_RBP])),\n#ifdef CONFIG_X86_64\n\t\t[r8]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R8])),\n\t\t[r9]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R9])),\n\t\t[r10]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R10])),\n\t\t[r11]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R11])),\n\t\t[r12]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R12])),\n\t\t[r13]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R13])),\n\t\t[r14]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R14])),\n\t\t[r15]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.regs[VCPU_REGS_R15])),\n#endif\n\t\t[cr2]\"i\"(offsetof(struct vcpu_vmx, vcpu.arch.cr2)),\n\t\t[wordsize]\"i\"(sizeof(ulong))\n\t      : \"cc\", \"memory\"\n#ifdef CONFIG_X86_64\n\t\t, \"rax\", \"rbx\", \"rdi\"\n\t\t, \"r8\", \"r9\", \"r10\", \"r11\", \"r12\", \"r13\", \"r14\", \"r15\"\n#else\n\t\t, \"eax\", \"ebx\", \"edi\"\n#endif\n\t      );\n\n\t/*\n\t * We do not use IBRS in the kernel. If this vCPU has used the\n\t * SPEC_CTRL MSR it may have left it on; save the value and\n\t * turn it off. This is much more efficient than blindly adding\n\t * it to the atomic save/restore list. Especially as the former\n\t * (Saving guest MSRs on vmexit) doesn't even exist in KVM.\n\t *\n\t * For non-nested case:\n\t * If the L01 MSR bitmap does not intercept the MSR, then we need to\n\t * save it.\n\t *\n\t * For nested case:\n\t * If the L02 MSR bitmap does not intercept the MSR, then we need to\n\t * save it.\n\t */\n\tif (unlikely(!msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL)))\n\t\tvmx->spec_ctrl = native_read_msr(MSR_IA32_SPEC_CTRL);\n\n\tif (vmx->spec_ctrl)\n\t\tnative_wrmsrl(MSR_IA32_SPEC_CTRL, 0);\n\n\t/* Eliminate branch target predictions from guest mode */\n\tvmexit_fill_RSB();\n\n\t/* All fields are clean at this point */\n\tif (static_branch_unlikely(&enable_evmcs))\n\t\tcurrent_evmcs->hv_clean_fields |=\n\t\t\tHV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;\n\n\t/* MSR_IA32_DEBUGCTLMSR is zeroed on vmexit. Restore it if needed */\n\tif (vmx->host_debugctlmsr)\n\t\tupdate_debugctlmsr(vmx->host_debugctlmsr);\n\n#ifndef CONFIG_X86_64\n\t/*\n\t * The sysexit path does not restore ds/es, so we must set them to\n\t * a reasonable value ourselves.\n\t *\n\t * We can't defer this to vmx_load_host_state() since that function\n\t * may be executed in interrupt context, which saves and restore segments\n\t * around it, nullifying its effect.\n\t */\n\tloadsegment(ds, __USER_DS);\n\tloadsegment(es, __USER_DS);\n#endif\n\n\tvcpu->arch.regs_avail = ~((1 << VCPU_REGS_RIP) | (1 << VCPU_REGS_RSP)\n\t\t\t\t  | (1 << VCPU_EXREG_RFLAGS)\n\t\t\t\t  | (1 << VCPU_EXREG_PDPTR)\n\t\t\t\t  | (1 << VCPU_EXREG_SEGMENTS)\n\t\t\t\t  | (1 << VCPU_EXREG_CR3));\n\tvcpu->arch.regs_dirty = 0;\n\n\t/*\n\t * eager fpu is enabled if PKEY is supported and CR4 is switched\n\t * back on host, so it is safe to read guest PKRU from current\n\t * XSAVE.\n\t */\n\tif (static_cpu_has(X86_FEATURE_PKU) &&\n\t    kvm_read_cr4_bits(vcpu, X86_CR4_PKE)) {\n\t\tvcpu->arch.pkru = __read_pkru();\n\t\tif (vcpu->arch.pkru != vmx->host_pkru)\n\t\t\t__write_pkru(vmx->host_pkru);\n\t}\n\n\tvmx->nested.nested_run_pending = 0;\n\tvmx->idt_vectoring_info = 0;\n\n\tvmx->exit_reason = vmx->fail ? 0xdead : vmcs_read32(VM_EXIT_REASON);\n\tif (vmx->fail || (vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\treturn;\n\n\tvmx->loaded_vmcs->launched = 1;\n\tvmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);\n\n\tvmx_complete_atomic_exit(vmx);\n\tvmx_recover_nmi_blocking(vmx);\n\tvmx_complete_interrupts(vmx);\n}\nSTACK_FRAME_NON_STANDARD(vmx_vcpu_run);\n\nstatic struct kvm *vmx_vm_alloc(void)\n{\n\tstruct kvm_vmx *kvm_vmx = vzalloc(sizeof(struct kvm_vmx));\n\treturn &kvm_vmx->kvm;\n}\n\nstatic void vmx_vm_free(struct kvm *kvm)\n{\n\tvfree(to_kvm_vmx(kvm));\n}\n\nstatic void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint cpu;\n\n\tif (vmx->loaded_vmcs == vmcs)\n\t\treturn;\n\n\tcpu = get_cpu();\n\tvmx->loaded_vmcs = vmcs;\n\tvmx_vcpu_put(vcpu);\n\tvmx_vcpu_load(vcpu, cpu);\n\tput_cpu();\n}\n\n/*\n * Ensure that the current vmcs of the logical processor is the\n * vmcs01 of the vcpu before calling free_nested().\n */\nstatic void vmx_free_vcpu_nested(struct kvm_vcpu *vcpu)\n{\n       struct vcpu_vmx *vmx = to_vmx(vcpu);\n\n       vcpu_load(vcpu);\n       vmx_switch_vmcs(vcpu, &vmx->vmcs01);\n       free_nested(vmx);\n       vcpu_put(vcpu);\n}\n\nstatic void vmx_free_vcpu(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (enable_pml)\n\t\tvmx_destroy_pml_buffer(vmx);\n\tfree_vpid(vmx->vpid);\n\tleave_guest_mode(vcpu);\n\tvmx_free_vcpu_nested(vcpu);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\n\tkfree(vmx->guest_msrs);\n\tkvm_vcpu_uninit(vcpu);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n}\n\nstatic struct kvm_vcpu *vmx_create_vcpu(struct kvm *kvm, unsigned int id)\n{\n\tint err;\n\tstruct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tunsigned long *msr_bitmap;\n\tint cpu;\n\n\tif (!vmx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvmx->vpid = allocate_vpid();\n\n\terr = kvm_vcpu_init(&vmx->vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = -ENOMEM;\n\n\t/*\n\t * If PML is turned on, failure on enabling PML just results in failure\n\t * of creating the vcpu, therefore we can simplify PML logic (by\n\t * avoiding dealing with cases, such as enabling PML partially on vcpus\n\t * for the guest, etc.\n\t */\n\tif (enable_pml) {\n\t\tvmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\t\tif (!vmx->pml_pg)\n\t\t\tgoto uninit_vcpu;\n\t}\n\n\tvmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tBUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])\n\t\t     > PAGE_SIZE);\n\n\tif (!vmx->guest_msrs)\n\t\tgoto free_pml;\n\n\terr = alloc_loaded_vmcs(&vmx->vmcs01);\n\tif (err < 0)\n\t\tgoto free_msrs;\n\n\tmsr_bitmap = vmx->vmcs01.msr_bitmap;\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_FS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_GS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(msr_bitmap, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);\n\tvmx->msr_bitmap_mode = 0;\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\tcpu = get_cpu();\n\tvmx_vcpu_load(&vmx->vcpu, cpu);\n\tvmx->vcpu.cpu = cpu;\n\tvmx_vcpu_setup(vmx);\n\tvmx_vcpu_put(&vmx->vcpu);\n\tput_cpu();\n\tif (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {\n\t\terr = alloc_apic_access_page(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (enable_ept && !enable_unrestricted_guest) {\n\t\terr = init_rmode_identity_map(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (nested) {\n\t\tnested_vmx_setup_ctls_msrs(&vmx->nested.msrs,\n\t\t\t\t\t   kvm_vcpu_apicv_active(&vmx->vcpu));\n\t\tvmx->nested.vpid02 = allocate_vpid();\n\t}\n\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\n\tvmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;\n\n\t/*\n\t * Enforce invariant: pi_desc.nv is always either POSTED_INTR_VECTOR\n\t * or POSTED_INTR_WAKEUP_VECTOR.\n\t */\n\tvmx->pi_desc.nv = POSTED_INTR_VECTOR;\n\tvmx->pi_desc.sn = 1;\n\n\treturn &vmx->vcpu;\n\nfree_vmcs:\n\tfree_vpid(vmx->nested.vpid02);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\nfree_msrs:\n\tkfree(vmx->guest_msrs);\nfree_pml:\n\tvmx_destroy_pml_buffer(vmx);\nuninit_vcpu:\n\tkvm_vcpu_uninit(&vmx->vcpu);\nfree_vcpu:\n\tfree_vpid(vmx->vpid);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n\treturn ERR_PTR(err);\n}\n\nstatic int vmx_vm_init(struct kvm *kvm)\n{\n\tif (!ple_gap)\n\t\tkvm->arch.pause_in_guest = true;\n\treturn 0;\n}\n\nstatic void __init vmx_check_processor_compat(void *rtn)\n{\n\tstruct vmcs_config vmcs_conf;\n\n\t*(int *)rtn = 0;\n\tif (setup_vmcs_config(&vmcs_conf) < 0)\n\t\t*(int *)rtn = -EIO;\n\tnested_vmx_setup_ctls_msrs(&vmcs_conf.nested, enable_apicv);\n\tif (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config)) != 0) {\n\t\tprintk(KERN_ERR \"kvm: CPU %d feature inconsistency!\\n\",\n\t\t\t\tsmp_processor_id());\n\t\t*(int *)rtn = -EIO;\n\t}\n}\n\nstatic u64 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)\n{\n\tu8 cache;\n\tu64 ipat = 0;\n\n\t/* For VT-d and EPT combination\n\t * 1. MMIO: always map as UC\n\t * 2. EPT with VT-d:\n\t *   a. VT-d without snooping control feature: can't guarantee the\n\t *\tresult, try to trust guest.\n\t *   b. VT-d with snooping control feature: snooping control feature of\n\t *\tVT-d engine can guarantee the cache correctness. Just set it\n\t *\tto WB to keep consistent with host. So the same as item 3.\n\t * 3. EPT without VT-d: always map as WB and set IPAT=1 to keep\n\t *    consistent with host MTRR\n\t */\n\tif (is_mmio) {\n\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tif (!kvm_arch_has_noncoherent_dma(vcpu->kvm)) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tcache = MTRR_TYPE_WRBACK;\n\t\tgoto exit;\n\t}\n\n\tif (kvm_read_cr0(vcpu) & X86_CR0_CD) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\t\tcache = MTRR_TYPE_WRBACK;\n\t\telse\n\t\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tcache = kvm_mtrr_get_guest_memory_type(vcpu, gfn);\n\nexit:\n\treturn (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;\n}\n\nstatic int vmx_get_lpage_level(void)\n{\n\tif (enable_ept && !cpu_has_vmx_ept_1g_page())\n\t\treturn PT_DIRECTORY_LEVEL;\n\telse\n\t\t/* For shadow and EPT supported 1GB page */\n\t\treturn PT_PDPE_LEVEL;\n}\n\nstatic void vmcs_set_secondary_exec_control(u32 new_ctl)\n{\n\t/*\n\t * These bits in the secondary execution controls field\n\t * are dynamic, the others are mostly based on the hypervisor\n\t * architecture and the guest's CPUID.  Do not touch the\n\t * dynamic bits.\n\t */\n\tu32 mask =\n\t\tSECONDARY_EXEC_SHADOW_VMCS |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\tSECONDARY_EXEC_DESC;\n\n\tu32 cur_ctl = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\n\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL,\n\t\t     (new_ctl & ~mask) | (cur_ctl & mask));\n}\n\n/*\n * Generate MSR_IA32_VMX_CR{0,4}_FIXED1 according to CPUID. Only set bits\n * (indicating \"allowed-1\") if they are supported in the guest's CPUID.\n */\nstatic void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_cpuid_entry2 *entry;\n\n\tvmx->nested.msrs.cr0_fixed1 = 0xffffffff;\n\tvmx->nested.msrs.cr4_fixed1 = X86_CR4_PCE;\n\n#define cr4_fixed1_update(_cr4_mask, _reg, _cpuid_mask) do {\t\t\\\n\tif (entry && (entry->_reg & (_cpuid_mask)))\t\t\t\\\n\t\tvmx->nested.msrs.cr4_fixed1 |= (_cr4_mask);\t\\\n} while (0)\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0x1, 0);\n\tcr4_fixed1_update(X86_CR4_VME,        edx, bit(X86_FEATURE_VME));\n\tcr4_fixed1_update(X86_CR4_PVI,        edx, bit(X86_FEATURE_VME));\n\tcr4_fixed1_update(X86_CR4_TSD,        edx, bit(X86_FEATURE_TSC));\n\tcr4_fixed1_update(X86_CR4_DE,         edx, bit(X86_FEATURE_DE));\n\tcr4_fixed1_update(X86_CR4_PSE,        edx, bit(X86_FEATURE_PSE));\n\tcr4_fixed1_update(X86_CR4_PAE,        edx, bit(X86_FEATURE_PAE));\n\tcr4_fixed1_update(X86_CR4_MCE,        edx, bit(X86_FEATURE_MCE));\n\tcr4_fixed1_update(X86_CR4_PGE,        edx, bit(X86_FEATURE_PGE));\n\tcr4_fixed1_update(X86_CR4_OSFXSR,     edx, bit(X86_FEATURE_FXSR));\n\tcr4_fixed1_update(X86_CR4_OSXMMEXCPT, edx, bit(X86_FEATURE_XMM));\n\tcr4_fixed1_update(X86_CR4_VMXE,       ecx, bit(X86_FEATURE_VMX));\n\tcr4_fixed1_update(X86_CR4_SMXE,       ecx, bit(X86_FEATURE_SMX));\n\tcr4_fixed1_update(X86_CR4_PCIDE,      ecx, bit(X86_FEATURE_PCID));\n\tcr4_fixed1_update(X86_CR4_OSXSAVE,    ecx, bit(X86_FEATURE_XSAVE));\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0x7, 0);\n\tcr4_fixed1_update(X86_CR4_FSGSBASE,   ebx, bit(X86_FEATURE_FSGSBASE));\n\tcr4_fixed1_update(X86_CR4_SMEP,       ebx, bit(X86_FEATURE_SMEP));\n\tcr4_fixed1_update(X86_CR4_SMAP,       ebx, bit(X86_FEATURE_SMAP));\n\tcr4_fixed1_update(X86_CR4_PKE,        ecx, bit(X86_FEATURE_PKU));\n\tcr4_fixed1_update(X86_CR4_UMIP,       ecx, bit(X86_FEATURE_UMIP));\n\n#undef cr4_fixed1_update\n}\n\nstatic void vmx_cpuid_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\tvmx_compute_secondary_exec_control(vmx);\n\t\tvmcs_set_secondary_exec_control(vmx->secondary_exec_control);\n\t}\n\n\tif (nested_vmx_allowed(vcpu))\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\telse\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEATURE_CONTROL_VMXON_ENABLED_OUTSIDE_SMX;\n\n\tif (nested_vmx_allowed(vcpu))\n\t\tnested_vmx_cr_fixed1_bits_update(vcpu);\n}\n\nstatic void vmx_set_supported_cpuid(u32 func, struct kvm_cpuid_entry2 *entry)\n{\n\tif (func == 1 && nested)\n\t\tentry->ecx |= bit(X86_FEATURE_VMX);\n}\n\nstatic void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason;\n\tunsigned long exit_qualification = vcpu->arch.exit_qualification;\n\n\tif (vmx->nested.pml_full) {\n\t\texit_reason = EXIT_REASON_PML_FULL;\n\t\tvmx->nested.pml_full = false;\n\t\texit_qualification &= INTR_INFO_UNBLOCK_NMI;\n\t} else if (fault->error_code & PFERR_RSVD_MASK)\n\t\texit_reason = EXIT_REASON_EPT_MISCONFIG;\n\telse\n\t\texit_reason = EXIT_REASON_EPT_VIOLATION;\n\n\tnested_vmx_vmexit(vcpu, exit_reason, 0, exit_qualification);\n\tvmcs12->guest_physical_address = fault->address;\n}\n\nstatic bool nested_ept_ad_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn nested_ept_get_cr3(vcpu) & VMX_EPTP_AD_ENABLE_BIT;\n}\n\n/* Callbacks for nested_ept_init_mmu_context: */\n\nstatic unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu)\n{\n\t/* return the page table to be shadowed - in our case, EPT12 */\n\treturn get_vmcs12(vcpu)->ept_pointer;\n}\n\nstatic int nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON(mmu_is_nested(vcpu));\n\tif (!valid_ept_address(vcpu, nested_ept_get_cr3(vcpu)))\n\t\treturn 1;\n\n\tkvm_mmu_unload(vcpu);\n\tkvm_init_shadow_ept_mmu(vcpu,\n\t\t\tto_vmx(vcpu)->nested.msrs.ept_caps &\n\t\t\tVMX_EPT_EXECUTE_ONLY_BIT,\n\t\t\tnested_ept_ad_enabled(vcpu));\n\tvcpu->arch.mmu.set_cr3           = vmx_set_cr3;\n\tvcpu->arch.mmu.get_cr3           = nested_ept_get_cr3;\n\tvcpu->arch.mmu.inject_page_fault = nested_ept_inject_page_fault;\n\n\tvcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;\n\treturn 0;\n}\n\nstatic void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.walk_mmu = &vcpu->arch.mmu;\n}\n\nstatic bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,\n\t\t\t\t\t    u16 error_code)\n{\n\tbool inequality, bit;\n\n\tbit = (vmcs12->exception_bitmap & (1u << PF_VECTOR)) != 0;\n\tinequality =\n\t\t(error_code & vmcs12->page_fault_error_code_mask) !=\n\t\t vmcs12->page_fault_error_code_match;\n\treturn inequality ^ bit;\n}\n\nstatic void vmx_inject_page_fault_nested(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tWARN_ON(!is_guest_mode(vcpu));\n\n\tif (nested_vmx_is_page_fault_vmexit(vmcs12, fault->error_code) &&\n\t\t!to_vmx(vcpu)->nested.nested_run_pending) {\n\t\tvmcs12->vm_exit_intr_error_code = fault->error_code;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  PF_VECTOR | INTR_TYPE_HARD_EXCEPTION |\n\t\t\t\t  INTR_INFO_DELIVER_CODE_MASK | INTR_INFO_VALID_MASK,\n\t\t\t\t  fault->address);\n\t} else {\n\t\tkvm_inject_page_fault(vcpu, fault);\n\t}\n}\n\nstatic inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12);\n\nstatic void nested_get_vmcs12_pages(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct page *page;\n\tu64 hpa;\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\t/*\n\t\t * Translate L1 physical address to host physical\n\t\t * address for vmcs02. Keep the page pinned, so this\n\t\t * physical address remains valid. We keep a reference\n\t\t * to it so we can release it later.\n\t\t */\n\t\tif (vmx->nested.apic_access_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\t\tvmx->nested.apic_access_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->apic_access_addr);\n\t\t/*\n\t\t * If translation failed, no matter: This feature asks\n\t\t * to exit when accessing the given address, and if it\n\t\t * can never be accessed, this feature won't do\n\t\t * anything anyway.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.apic_access_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.apic_access_page);\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, hpa);\n\t\t} else {\n\t\t\tvmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES);\n\t\t}\n\t}\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tif (vmx->nested.virtual_apic_page) { /* shouldn't happen */\n\t\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\t\tvmx->nested.virtual_apic_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->virtual_apic_page_addr);\n\n\t\t/*\n\t\t * If translation failed, VM entry will fail because\n\t\t * prepare_vmcs02 set VIRTUAL_APIC_PAGE_ADDR to -1ull.\n\t\t * Failing the vm entry is _not_ what the processor\n\t\t * does but it's basically the only possibility we\n\t\t * have.  We could still enter the guest if CR8 load\n\t\t * exits are enabled, CR8 store exits are enabled, and\n\t\t * virtualize APIC access is disabled; in this case\n\t\t * the processor would never use the TPR shadow and we\n\t\t * could simply clear the bit from the execution\n\t\t * control.  But such a configuration is useless, so\n\t\t * let's keep the code simple.\n\t\t */\n\t\tif (!is_error_page(page)) {\n\t\t\tvmx->nested.virtual_apic_page = page;\n\t\t\thpa = page_to_phys(vmx->nested.virtual_apic_page);\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, hpa);\n\t\t}\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tif (vmx->nested.pi_desc_page) { /* shouldn't happen */\n\t\t\tkunmap(vmx->nested.pi_desc_page);\n\t\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\t\tvmx->nested.pi_desc_page = NULL;\n\t\t}\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->posted_intr_desc_addr);\n\t\tif (is_error_page(page))\n\t\t\treturn;\n\t\tvmx->nested.pi_desc_page = page;\n\t\tvmx->nested.pi_desc = kmap(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc =\n\t\t\t(struct pi_desc *)((void *)vmx->nested.pi_desc +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR,\n\t\t\tpage_to_phys(vmx->nested.pi_desc_page) +\n\t\t\t(unsigned long)(vmcs12->posted_intr_desc_addr &\n\t\t\t(PAGE_SIZE - 1)));\n\t}\n\tif (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))\n\t\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t      CPU_BASED_USE_MSR_BITMAPS);\n\telse\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_USE_MSR_BITMAPS);\n}\n\nstatic void vmx_start_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\tu64 preemption_timeout = get_vmcs12(vcpu)->vmx_preemption_timer_value;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vcpu->arch.virtual_tsc_khz == 0)\n\t\treturn;\n\n\t/* Make sure short timeouts reliably trigger an immediate vmexit.\n\t * hrtimer_start does not guarantee this. */\n\tif (preemption_timeout <= 1) {\n\t\tvmx_preemption_timer_fn(&vmx->nested.preemption_timer);\n\t\treturn;\n\t}\n\n\tpreemption_timeout <<= VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\tpreemption_timeout *= 1000000;\n\tdo_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);\n\thrtimer_start(&vmx->nested.preemption_timer,\n\t\t      ns_to_ktime(preemption_timeout), HRTIMER_MODE_REL);\n}\n\nstatic int nested_vmx_check_io_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->io_bitmap_a) ||\n\t    !page_address_valid(vcpu, vmcs12->io_bitmap_b))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->msr_bitmap))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->virtual_apic_page_addr))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n/*\n * Merge L0's and L1's MSR bitmap, return false to indicate that\n * we do not use the hardware.\n */\nstatic inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tint msr;\n\tstruct page *page;\n\tunsigned long *msr_bitmap_l1;\n\tunsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.vmcs02.msr_bitmap;\n\t/*\n\t * pred_cmd & spec_ctrl are trying to verify two things:\n\t *\n\t * 1. L0 gave a permission to L1 to actually passthrough the MSR. This\n\t *    ensures that we do not accidentally generate an L02 MSR bitmap\n\t *    from the L12 MSR bitmap that is too permissive.\n\t * 2. That L1 or L2s have actually used the MSR. This avoids\n\t *    unnecessarily merging of the bitmap if the MSR is unused. This\n\t *    works properly because we only update the L01 MSR bitmap lazily.\n\t *    So even if L0 should pass L1 these MSRs, the L01 bitmap is only\n\t *    updated to reflect this when L1 (or its L2s) actually write to\n\t *    the MSR.\n\t */\n\tbool pred_cmd = !msr_write_intercepted_l01(vcpu, MSR_IA32_PRED_CMD);\n\tbool spec_ctrl = !msr_write_intercepted_l01(vcpu, MSR_IA32_SPEC_CTRL);\n\n\t/* Nothing to do if the MSR bitmap is not in use.  */\n\tif (!cpu_has_vmx_msr_bitmap() ||\n\t    !nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn false;\n\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    !pred_cmd && !spec_ctrl)\n\t\treturn false;\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->msr_bitmap);\n\tif (is_error_page(page))\n\t\treturn false;\n\n\tmsr_bitmap_l1 = (unsigned long *)kmap(page);\n\tif (nested_cpu_has_apic_reg_virt(vmcs12)) {\n\t\t/*\n\t\t * L0 need not intercept reads for MSRs between 0x800 and 0x8ff, it\n\t\t * just lets the processor take the value from the virtual-APIC page;\n\t\t * take those 256 bits directly from the L1 bitmap.\n\t\t */\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\t\tunsigned word = msr / BITS_PER_LONG;\n\t\t\tmsr_bitmap_l0[word] = msr_bitmap_l1[word];\n\t\t\tmsr_bitmap_l0[word + (0x800 / sizeof(long))] = ~0;\n\t\t}\n\t} else {\n\t\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\t\tunsigned word = msr / BITS_PER_LONG;\n\t\t\tmsr_bitmap_l0[word] = ~0;\n\t\t\tmsr_bitmap_l0[word + (0x800 / sizeof(long))] = ~0;\n\t\t}\n\t}\n\n\tnested_vmx_disable_intercept_for_msr(\n\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\tX2APIC_MSR(APIC_TASKPRI),\n\t\tMSR_TYPE_W);\n\n\tif (nested_cpu_has_vid(vmcs12)) {\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\tX2APIC_MSR(APIC_EOI),\n\t\t\tMSR_TYPE_W);\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\tX2APIC_MSR(APIC_SELF_IPI),\n\t\t\tMSR_TYPE_W);\n\t}\n\n\tif (spec_ctrl)\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tMSR_IA32_SPEC_CTRL,\n\t\t\t\t\tMSR_TYPE_R | MSR_TYPE_W);\n\n\tif (pred_cmd)\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tMSR_IA32_PRED_CMD,\n\t\t\t\t\tMSR_TYPE_W);\n\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\treturn true;\n}\n\nstatic int nested_vmx_check_apic_access_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES) &&\n\t    !page_address_valid(vcpu, vmcs12->apic_access_addr))\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\n\nstatic int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    !nested_cpu_has_apic_reg_virt(vmcs12) &&\n\t    !nested_cpu_has_vid(vmcs12) &&\n\t    !nested_cpu_has_posted_intr(vmcs12))\n\t\treturn 0;\n\n\t/*\n\t * If virtualize x2apic mode is enabled,\n\t * virtualize apic access must be disabled.\n\t */\n\tif (nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\treturn -EINVAL;\n\n\t/*\n\t * If virtual interrupt delivery is enabled,\n\t * we must exit on external interrupts.\n\t */\n\tif (nested_cpu_has_vid(vmcs12) &&\n\t   !nested_exit_on_intr(vcpu))\n\t\treturn -EINVAL;\n\n\t/*\n\t * bits 15:8 should be zero in posted_intr_nv,\n\t * the descriptor address has been already checked\n\t * in nested_get_vmcs12_pages.\n\t */\n\tif (nested_cpu_has_posted_intr(vmcs12) &&\n\t   (!nested_cpu_has_vid(vmcs12) ||\n\t    !nested_exit_intr_ack_set(vcpu) ||\n\t    vmcs12->posted_intr_nv & 0xff00))\n\t\treturn -EINVAL;\n\n\t/* tpr shadow is needed by all apicv features. */\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_switch(struct kvm_vcpu *vcpu,\n\t\t\t\t       unsigned long count_field,\n\t\t\t\t       unsigned long addr_field)\n{\n\tint maxphyaddr;\n\tu64 count, addr;\n\n\tif (vmcs12_read_any(vcpu, count_field, &count) ||\n\t    vmcs12_read_any(vcpu, addr_field, &addr)) {\n\t\tWARN_ON(1);\n\t\treturn -EINVAL;\n\t}\n\tif (count == 0)\n\t\treturn 0;\n\tmaxphyaddr = cpuid_maxphyaddr(vcpu);\n\tif (!IS_ALIGNED(addr, 16) || addr >> maxphyaddr ||\n\t    (addr + count * sizeof(struct vmx_msr_entry) - 1) >> maxphyaddr) {\n\t\tpr_debug_ratelimited(\n\t\t\t\"nVMX: invalid MSR switch (0x%lx, %d, %llu, 0x%08llx)\",\n\t\t\taddr_field, maxphyaddr, count, addr);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_switch_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (vmcs12->vm_exit_msr_load_count == 0 &&\n\t    vmcs12->vm_exit_msr_store_count == 0 &&\n\t    vmcs12->vm_entry_msr_load_count == 0)\n\t\treturn 0; /* Fast path */\n\tif (nested_vmx_check_msr_switch(vcpu, VM_EXIT_MSR_LOAD_COUNT,\n\t\t\t\t\tVM_EXIT_MSR_LOAD_ADDR) ||\n\t    nested_vmx_check_msr_switch(vcpu, VM_EXIT_MSR_STORE_COUNT,\n\t\t\t\t\tVM_EXIT_MSR_STORE_ADDR) ||\n\t    nested_vmx_check_msr_switch(vcpu, VM_ENTRY_MSR_LOAD_COUNT,\n\t\t\t\t\tVM_ENTRY_MSR_LOAD_ADDR))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_check_pml_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tu64 address = vmcs12->pml_address;\n\tint maxphyaddr = cpuid_maxphyaddr(vcpu);\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_PML)) {\n\t\tif (!nested_cpu_has_ept(vmcs12) ||\n\t\t    !IS_ALIGNED(address, 4096)  ||\n\t\t    address >> maxphyaddr)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmx_msr_entry *e)\n{\n\t/* x2APIC MSR accesses are not allowed */\n\tif (vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8)\n\t\treturn -EINVAL;\n\tif (e->index == MSR_IA32_UCODE_WRITE || /* SDM Table 35-2 */\n\t    e->index == MSR_IA32_UCODE_REV)\n\t\treturn -EINVAL;\n\tif (e->reserved != 0)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_load_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmx_msr_entry *e)\n{\n\tif (e->index == MSR_FS_BASE ||\n\t    e->index == MSR_GS_BASE ||\n\t    e->index == MSR_IA32_SMM_MONITOR_CTL || /* SMM is not supported */\n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_store_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmx_msr_entry *e)\n{\n\tif (e->index == MSR_IA32_SMBASE || /* SMM is not supported */\n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\n/*\n * Load guest's/host's msr at nested entry/exit.\n * return 0 for success, entry index for failure.\n */\nstatic u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\tstruct msr_data msr;\n\n\tmsr.host_initiated = false;\n\tfor (i = 0; i < count; i++) {\n\t\tif (kvm_vcpu_read_guest(vcpu, gpa + i * sizeof(e),\n\t\t\t\t\t&e, sizeof(e))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa + i * sizeof(e));\n\t\t\tgoto fail;\n\t\t}\n\t\tif (nested_vmx_load_msr_check(vcpu, &e)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index, e.reserved);\n\t\t\tgoto fail;\n\t\t}\n\t\tmsr.index = e.index;\n\t\tmsr.data = e.value;\n\t\tif (kvm_set_msr(vcpu, &msr)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, e.value);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\nfail:\n\treturn i + 1;\n}\n\nstatic int nested_vmx_store_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct msr_data msr_info;\n\t\tif (kvm_vcpu_read_guest(vcpu,\n\t\t\t\t\tgpa + i * sizeof(e),\n\t\t\t\t\t&e, 2 * sizeof(u32))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa + i * sizeof(e));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (nested_vmx_store_msr_check(vcpu, &e)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index, e.reserved);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsr_info.host_initiated = false;\n\t\tmsr_info.index = e.index;\n\t\tif (kvm_get_msr(vcpu, &msr_info)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR (%u, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (kvm_vcpu_write_guest(vcpu,\n\t\t\t\t\t gpa + i * sizeof(e) +\n\t\t\t\t\t     offsetof(struct vmx_msr_entry, value),\n\t\t\t\t\t &msr_info.data, sizeof(msr_info.data))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, msr_info.data);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool nested_cr3_valid(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tunsigned long invalid_mask;\n\n\tinvalid_mask = (~0ULL) << cpuid_maxphyaddr(vcpu);\n\treturn (val & invalid_mask) == 0;\n}\n\n/*\n * Load guest's/host's cr3 at nested entry/exit. nested_ept is true if we are\n * emulating VM entry into a guest with EPT enabled.\n * Returns 0 on success, 1 on failure. Invalid state exit qualification code\n * is assigned to entry_failure_code on failure.\n */\nstatic int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3, bool nested_ept,\n\t\t\t       u32 *entry_failure_code)\n{\n\tif (cr3 != kvm_read_cr3(vcpu) || (!nested_ept && pdptrs_changed(vcpu))) {\n\t\tif (!nested_cr3_valid(vcpu, cr3)) {\n\t\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\t\treturn 1;\n\t\t}\n\n\t\t/*\n\t\t * If PAE paging and EPT are both on, CR3 is not used by the CPU and\n\t\t * must not be dereferenced.\n\t\t */\n\t\tif (!is_long_mode(vcpu) && is_pae(vcpu) && is_paging(vcpu) &&\n\t\t    !nested_ept) {\n\t\t\tif (!load_pdptrs(vcpu, vcpu->arch.walk_mmu, cr3)) {\n\t\t\t\t*entry_failure_code = ENTRY_FAIL_PDPTE;\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\n\t\tvcpu->arch.cr3 = cr3;\n\t\t__set_bit(VCPU_EXREG_CR3, (ulong *)&vcpu->arch.regs_avail);\n\t}\n\n\tkvm_mmu_reset_context(vcpu);\n\treturn 0;\n}\n\nstatic void prepare_vmcs02_full(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs_write16(GUEST_ES_SELECTOR, vmcs12->guest_es_selector);\n\tvmcs_write16(GUEST_SS_SELECTOR, vmcs12->guest_ss_selector);\n\tvmcs_write16(GUEST_DS_SELECTOR, vmcs12->guest_ds_selector);\n\tvmcs_write16(GUEST_FS_SELECTOR, vmcs12->guest_fs_selector);\n\tvmcs_write16(GUEST_GS_SELECTOR, vmcs12->guest_gs_selector);\n\tvmcs_write16(GUEST_LDTR_SELECTOR, vmcs12->guest_ldtr_selector);\n\tvmcs_write16(GUEST_TR_SELECTOR, vmcs12->guest_tr_selector);\n\tvmcs_write32(GUEST_ES_LIMIT, vmcs12->guest_es_limit);\n\tvmcs_write32(GUEST_SS_LIMIT, vmcs12->guest_ss_limit);\n\tvmcs_write32(GUEST_DS_LIMIT, vmcs12->guest_ds_limit);\n\tvmcs_write32(GUEST_FS_LIMIT, vmcs12->guest_fs_limit);\n\tvmcs_write32(GUEST_GS_LIMIT, vmcs12->guest_gs_limit);\n\tvmcs_write32(GUEST_LDTR_LIMIT, vmcs12->guest_ldtr_limit);\n\tvmcs_write32(GUEST_TR_LIMIT, vmcs12->guest_tr_limit);\n\tvmcs_write32(GUEST_GDTR_LIMIT, vmcs12->guest_gdtr_limit);\n\tvmcs_write32(GUEST_IDTR_LIMIT, vmcs12->guest_idtr_limit);\n\tvmcs_write32(GUEST_ES_AR_BYTES, vmcs12->guest_es_ar_bytes);\n\tvmcs_write32(GUEST_SS_AR_BYTES, vmcs12->guest_ss_ar_bytes);\n\tvmcs_write32(GUEST_DS_AR_BYTES, vmcs12->guest_ds_ar_bytes);\n\tvmcs_write32(GUEST_FS_AR_BYTES, vmcs12->guest_fs_ar_bytes);\n\tvmcs_write32(GUEST_GS_AR_BYTES, vmcs12->guest_gs_ar_bytes);\n\tvmcs_write32(GUEST_LDTR_AR_BYTES, vmcs12->guest_ldtr_ar_bytes);\n\tvmcs_write32(GUEST_TR_AR_BYTES, vmcs12->guest_tr_ar_bytes);\n\tvmcs_writel(GUEST_SS_BASE, vmcs12->guest_ss_base);\n\tvmcs_writel(GUEST_DS_BASE, vmcs12->guest_ds_base);\n\tvmcs_writel(GUEST_FS_BASE, vmcs12->guest_fs_base);\n\tvmcs_writel(GUEST_GS_BASE, vmcs12->guest_gs_base);\n\tvmcs_writel(GUEST_LDTR_BASE, vmcs12->guest_ldtr_base);\n\tvmcs_writel(GUEST_TR_BASE, vmcs12->guest_tr_base);\n\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->guest_gdtr_base);\n\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->guest_idtr_base);\n\n\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->guest_sysenter_cs);\n\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS,\n\t\tvmcs12->guest_pending_dbg_exceptions);\n\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->guest_sysenter_esp);\n\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->guest_sysenter_eip);\n\n\tif (nested_cpu_has_xsaves(vmcs12))\n\t\tvmcs_write64(XSS_EXIT_BITMAP, vmcs12->xss_exit_bitmap);\n\tvmcs_write64(VMCS_LINK_POINTER, -1ull);\n\n\tif (cpu_has_vmx_posted_intr())\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_NESTED_VECTOR);\n\n\t/*\n\t * Whether page-faults are trapped is determined by a combination of\n\t * 3 settings: PFEC_MASK, PFEC_MATCH and EXCEPTION_BITMAP.PF.\n\t * If enable_ept, L0 doesn't care about page faults and we should\n\t * set all of these to L1's desires. However, if !enable_ept, L0 does\n\t * care about (at least some) page faults, and because it is not easy\n\t * (if at all possible?) to merge L0 and L1's desires, we simply ask\n\t * to exit on each and every L2 page fault. This is done by setting\n\t * MASK=MATCH=0 and (see below) EB.PF=1.\n\t * Note that below we don't need special code to set EB.PF beyond the\n\t * \"or\"ing of the EB of vmcs01 and vmcs12, because when enable_ept,\n\t * vmcs01's EB.PF is 0 so the \"or\" will take vmcs12's value, and when\n\t * !enable_ept, EB.PF is 1, so the \"or\" will always be 1.\n\t */\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK,\n\t\tenable_ept ? vmcs12->page_fault_error_code_mask : 0);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH,\n\t\tenable_ept ? vmcs12->page_fault_error_code_match : 0);\n\n\t/* All VMFUNCs are currently emulated through L0 vmexits.  */\n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tif (cpu_has_vmx_apicv()) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, vmcs12->eoi_exit_bitmap2);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, vmcs12->eoi_exit_bitmap3);\n\t}\n\n\t/*\n\t * Set host-state according to L0's settings (vmcs12 is irrelevant here)\n\t * Some constant fields are set here by vmx_set_constant_host_state().\n\t * Other fields are different per CPU, and will be set later when\n\t * vmx_vcpu_load() is called, and when vmx_save_host_state() is called.\n\t */\n\tvmx_set_constant_host_state(vmx);\n\n\t/*\n\t * Set the MSR load/store lists to match L0's settings.\n\t */\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest));\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tif (vmx_mpx_supported())\n\t\tvmcs_write64(GUEST_BNDCFGS, vmcs12->guest_bndcfgs);\n\n\tif (enable_vpid) {\n\t\tif (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02)\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->nested.vpid02);\n\t\telse\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\t}\n\n\t/*\n\t * L1 may access the L2's PDPTR, so save them to construct vmcs12\n\t */\n\tif (enable_ept) {\n\t\tvmcs_write64(GUEST_PDPTR0, vmcs12->guest_pdptr0);\n\t\tvmcs_write64(GUEST_PDPTR1, vmcs12->guest_pdptr1);\n\t\tvmcs_write64(GUEST_PDPTR2, vmcs12->guest_pdptr2);\n\t\tvmcs_write64(GUEST_PDPTR3, vmcs12->guest_pdptr3);\n\t}\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));\n}\n\n/*\n * prepare_vmcs02 is called when the L1 guest hypervisor runs its nested\n * L2 guest. L1 has a vmcs for L2 (vmcs12), and this function \"merges\" it\n * with L0's requirements for its guest (a.k.a. vmcs01), so we can run the L2\n * guest in a way that will both be appropriate to L1's requests, and our\n * needs. In addition to modifying the active vmcs (which is vmcs02), this\n * function also has additional necessary side-effects, like setting various\n * vcpu->arch fields.\n * Returns 0 on success, 1 on failure. Invalid state exit qualification code\n * is assigned to entry_failure_code on failure.\n */\nstatic int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t  u32 *entry_failure_code)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exec_control, vmcs12_exec_ctrl;\n\n\tif (vmx->nested.dirty_vmcs12) {\n\t\tprepare_vmcs02_full(vcpu, vmcs12);\n\t\tvmx->nested.dirty_vmcs12 = false;\n\t}\n\n\t/*\n\t * First, the fields that are shadowed.  This must be kept in sync\n\t * with vmx_shadow_fields.h.\n\t */\n\n\tvmcs_write16(GUEST_CS_SELECTOR, vmcs12->guest_cs_selector);\n\tvmcs_write32(GUEST_CS_LIMIT, vmcs12->guest_cs_limit);\n\tvmcs_write32(GUEST_CS_AR_BYTES, vmcs12->guest_cs_ar_bytes);\n\tvmcs_writel(GUEST_ES_BASE, vmcs12->guest_es_base);\n\tvmcs_writel(GUEST_CS_BASE, vmcs12->guest_cs_base);\n\n\t/*\n\t * Not in vmcs02: GUEST_PML_INDEX, HOST_FS_SELECTOR, HOST_GS_SELECTOR,\n\t * HOST_FS_BASE, HOST_GS_BASE.\n\t */\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS)) {\n\t\tkvm_set_dr(vcpu, 7, vmcs12->guest_dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmcs12->guest_ia32_debugctl);\n\t} else {\n\t\tkvm_set_dr(vcpu, 7, vcpu->arch.dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmx->nested.vmcs01_debugctl);\n\t}\n\tif (vmx->nested.nested_run_pending) {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\t     vmcs12->vm_entry_intr_info_field);\n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE,\n\t\t\t     vmcs12->vm_entry_exception_error_code);\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmcs12->vm_entry_instruction_len);\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t     vmcs12->guest_interruptibility_info);\n\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t!(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);\n\t} else {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n\t}\n\tvmx_set_rflags(vcpu, vmcs12->guest_rflags);\n\n\texec_control = vmcs12->pin_based_vm_exec_control;\n\n\t/* Preemption timer setting is only taken from vmcs01.  */\n\texec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\texec_control |= vmcs_config.pin_based_exec_ctrl;\n\tif (vmx->hv_deadline_tsc == -1)\n\t\texec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\n\t/* Posted interrupts setting is only taken from vmcs12.  */\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tvmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;\n\t\tvmx->nested.pi_pending = false;\n\t} else {\n\t\texec_control &= ~PIN_BASED_POSTED_INTR;\n\t}\n\n\tvmcs_write32(PIN_BASED_VM_EXEC_CONTROL, exec_control);\n\n\tvmx->nested.preemption_timer_expired = false;\n\tif (nested_cpu_has_preemption_timer(vmcs12))\n\t\tvmx_start_preemption_timer(vcpu);\n\n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\texec_control = vmx->secondary_exec_control;\n\n\t\t/* Take the following fields only from vmcs12 */\n\t\texec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_INVPCID |\n\t\t\t\t  SECONDARY_EXEC_RDTSCP |\n\t\t\t\t  SECONDARY_EXEC_XSAVES |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\t\t\t  SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_VMFUNC);\n\t\tif (nested_cpu_has(vmcs12,\n\t\t\t\t   CPU_BASED_ACTIVATE_SECONDARY_CONTROLS)) {\n\t\t\tvmcs12_exec_ctrl = vmcs12->secondary_vm_exec_control &\n\t\t\t\t~SECONDARY_EXEC_ENABLE_PML;\n\t\t\texec_control |= vmcs12_exec_ctrl;\n\t\t}\n\n\t\tif (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\t\tvmcs_write16(GUEST_INTR_STATUS,\n\t\t\t\tvmcs12->guest_intr_status);\n\n\t\t/*\n\t\t * Write an illegal value to APIC_ACCESS_ADDR. Later,\n\t\t * nested_get_vmcs12_pages will either fix it up or\n\t\t * remove the VM execution control.\n\t\t */\n\t\tif (exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, -1ull);\n\n\t\tvmcs_write32(SECONDARY_VM_EXEC_CONTROL, exec_control);\n\t}\n\n\t/*\n\t * HOST_RSP is normally set correctly in vmx_vcpu_run() just before\n\t * entry, but only if the current (host) sp changed from the value\n\t * we wrote last (vmx->host_rsp). This cache is no longer relevant\n\t * if we switch vmcs, and rather than hold a separate cache per vmcs,\n\t * here we just force the write to happen on entry.\n\t */\n\tvmx->host_rsp = 0;\n\n\texec_control = vmx_exec_control(vmx); /* L0's desires */\n\texec_control &= ~CPU_BASED_VIRTUAL_INTR_PENDING;\n\texec_control &= ~CPU_BASED_VIRTUAL_NMI_PENDING;\n\texec_control &= ~CPU_BASED_TPR_SHADOW;\n\texec_control |= vmcs12->cpu_based_vm_exec_control;\n\n\t/*\n\t * Write an illegal value to VIRTUAL_APIC_PAGE_ADDR. Later, if\n\t * nested_get_vmcs12_pages can't fix it up, the illegal value\n\t * will result in a VM entry failure.\n\t */\n\tif (exec_control & CPU_BASED_TPR_SHADOW) {\n\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, -1ull);\n\t\tvmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);\n\t} else {\n#ifdef CONFIG_X86_64\n\t\texec_control |= CPU_BASED_CR8_LOAD_EXITING |\n\t\t\t\tCPU_BASED_CR8_STORE_EXITING;\n#endif\n\t}\n\n\t/*\n\t * A vmexit (to either L1 hypervisor or L0 userspace) is always needed\n\t * for I/O port accesses.\n\t */\n\texec_control &= ~CPU_BASED_USE_IO_BITMAPS;\n\texec_control |= CPU_BASED_UNCOND_IO_EXITING;\n\n\tvmcs_write32(CPU_BASED_VM_EXEC_CONTROL, exec_control);\n\n\t/* EXCEPTION_BITMAP and CR0_GUEST_HOST_MASK should basically be the\n\t * bitwise-or of what L1 wants to trap for L2, and what we want to\n\t * trap. Note that CR0.TS also needs updating - we do this later.\n\t */\n\tupdate_exception_bitmap(vcpu);\n\tvcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);\n\n\t/* L2->L1 exit controls are emulated - the hardware exit is to L0 so\n\t * we should use its exit controls. Note that VM_EXIT_LOAD_IA32_EFER\n\t * bits are further modified by vmx_set_efer() below.\n\t */\n\tvmcs_write32(VM_EXIT_CONTROLS, vmcs_config.vmexit_ctrl);\n\n\t/* vmcs12's VM_ENTRY_LOAD_IA32_EFER and VM_ENTRY_IA32E_MODE are\n\t * emulated by vmx_set_efer(), below.\n\t */\n\tvm_entry_controls_init(vmx, \n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_LOAD_IA32_EFER &\n\t\t\t~VM_ENTRY_IA32E_MODE) |\n\t\t(vmcs_config.vmentry_ctrl & ~VM_ENTRY_IA32E_MODE));\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT)) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->guest_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->guest_ia32_pat;\n\t} else if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\t}\n\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\n\tif (kvm_has_tsc_control)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tif (enable_vpid) {\n\t\t/*\n\t\t * There is no direct mapping between vpid02 and vpid12, the\n\t\t * vpid02 is per-vCPU for L0 and reused while the value of\n\t\t * vpid12 is changed w/ one invvpid during nested vmentry.\n\t\t * The vpid12 is allocated by L1 for L2, so it will not\n\t\t * influence global bitmap(for vpid01 and vpid02 allocation)\n\t\t * even if spawn a lot of nested vCPUs.\n\t\t */\n\t\tif (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02) {\n\t\t\tif (vmcs12->virtual_processor_id != vmx->nested.last_vpid) {\n\t\t\t\tvmx->nested.last_vpid = vmcs12->virtual_processor_id;\n\t\t\t\t__vmx_flush_tlb(vcpu, vmx->nested.vpid02, true);\n\t\t\t}\n\t\t} else {\n\t\t\tvmx_flush_tlb(vcpu, true);\n\t\t}\n\t}\n\n\tif (enable_pml) {\n\t\t/*\n\t\t * Conceptually we want to copy the PML address and index from\n\t\t * vmcs01 here, and then back to vmcs01 on nested vmexit. But,\n\t\t * since we always flush the log on each vmexit, this happens\n\t\t * to be equivalent to simply resetting the fields in vmcs02.\n\t\t */\n\t\tASSERT(vmx->pml_pg);\n\t\tvmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));\n\t\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n\t}\n\n\tif (nested_cpu_has_ept(vmcs12)) {\n\t\tif (nested_ept_init_mmu_context(vcpu)) {\n\t\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\t\treturn 1;\n\t\t}\n\t} else if (nested_cpu_has2(vmcs12,\n\t\t\t\t   SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\t/*\n\t * This sets GUEST_CR0 to vmcs12->guest_cr0, possibly modifying those\n\t * bits which we consider mandatory enabled.\n\t * The CR0_READ_SHADOW is what L2 should have expected to read given\n\t * the specifications by L1; It's not enough to take\n\t * vmcs12->cr0_read_shadow because on our cr0_guest_host_mask we we\n\t * have more bits than L1 expected.\n\t */\n\tvmx_set_cr0(vcpu, vmcs12->guest_cr0);\n\tvmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));\n\n\tvmx_set_cr4(vcpu, vmcs12->guest_cr4);\n\tvmcs_writel(CR4_READ_SHADOW, nested_read_cr4(vmcs12));\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER))\n\t\tvcpu->arch.efer = vmcs12->guest_ia32_efer;\n\telse if (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE)\n\t\tvcpu->arch.efer |= (EFER_LMA | EFER_LME);\n\telse\n\t\tvcpu->arch.efer &= ~(EFER_LMA | EFER_LME);\n\t/* Note: modifies VM_ENTRY/EXIT_CONTROLS and GUEST/HOST_IA32_EFER */\n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\t/*\n\t * Guest state is invalid and unrestricted guest is disabled,\n\t * which means L1 attempted VMEntry to L2 with invalid state.\n\t * Fail the VMEntry.\n\t */\n\tif (vmx->emulation_required) {\n\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\treturn 1;\n\t}\n\n\t/* Shadow page tables on either EPT or shadow page tables. */\n\tif (nested_vmx_load_cr3(vcpu, vmcs12->guest_cr3, nested_cpu_has_ept(vmcs12),\n\t\t\t\tentry_failure_code))\n\t\treturn 1;\n\n\tif (!enable_ept)\n\t\tvcpu->arch.walk_mmu->inject_page_fault = vmx_inject_page_fault_nested;\n\n\tkvm_register_write(vcpu, VCPU_REGS_RSP, vmcs12->guest_rsp);\n\tkvm_register_write(vcpu, VCPU_REGS_RIP, vmcs12->guest_rip);\n\treturn 0;\n}\n\nstatic int nested_vmx_check_nmi_controls(struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_nmi_exiting(vmcs12) &&\n\t    nested_cpu_has_virtual_nmis(vmcs12))\n\t\treturn -EINVAL;\n\n\tif (!nested_cpu_has_virtual_nmis(vmcs12) &&\n\t    nested_cpu_has(vmcs12, CPU_BASED_VIRTUAL_NMI_PENDING))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int check_vmentry_prereqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vmcs12->guest_activity_state != GUEST_ACTIVITY_ACTIVE &&\n\t    vmcs12->guest_activity_state != GUEST_ACTIVITY_HLT)\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_io_bitmap_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_msr_bitmap_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_apic_access_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_tpr_shadow_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_apicv_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_msr_switch_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_pml_controls(vcpu, vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (!vmx_control_verify(vmcs12->cpu_based_vm_exec_control,\n\t\t\t\tvmx->nested.msrs.procbased_ctls_low,\n\t\t\t\tvmx->nested.msrs.procbased_ctls_high) ||\n\t    (nested_cpu_has(vmcs12, CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) &&\n\t     !vmx_control_verify(vmcs12->secondary_vm_exec_control,\n\t\t\t\t vmx->nested.msrs.secondary_ctls_low,\n\t\t\t\t vmx->nested.msrs.secondary_ctls_high)) ||\n\t    !vmx_control_verify(vmcs12->pin_based_vm_exec_control,\n\t\t\t\tvmx->nested.msrs.pinbased_ctls_low,\n\t\t\t\tvmx->nested.msrs.pinbased_ctls_high) ||\n\t    !vmx_control_verify(vmcs12->vm_exit_controls,\n\t\t\t\tvmx->nested.msrs.exit_ctls_low,\n\t\t\t\tvmx->nested.msrs.exit_ctls_high) ||\n\t    !vmx_control_verify(vmcs12->vm_entry_controls,\n\t\t\t\tvmx->nested.msrs.entry_ctls_low,\n\t\t\t\tvmx->nested.msrs.entry_ctls_high))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_vmx_check_nmi_controls(vmcs12))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (nested_cpu_has_vmfunc(vmcs12)) {\n\t\tif (vmcs12->vm_function_control &\n\t\t    ~vmx->nested.msrs.vmfunc_controls)\n\t\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\t\tif (nested_cpu_has_eptp_switching(vmcs12)) {\n\t\t\tif (!nested_cpu_has_ept(vmcs12) ||\n\t\t\t    !page_address_valid(vcpu, vmcs12->eptp_list_address))\n\t\t\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\t\t}\n\t}\n\n\tif (vmcs12->cr3_target_count > nested_cpu_vmx_misc_cr3_count(vcpu))\n\t\treturn VMXERR_ENTRY_INVALID_CONTROL_FIELD;\n\n\tif (!nested_host_cr0_valid(vcpu, vmcs12->host_cr0) ||\n\t    !nested_host_cr4_valid(vcpu, vmcs12->host_cr4) ||\n\t    !nested_cr3_valid(vcpu, vmcs12->host_cr3))\n\t\treturn VMXERR_ENTRY_INVALID_HOST_STATE_FIELD;\n\n\treturn 0;\n}\n\nstatic int check_vmentry_postreqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t\t  u32 *exit_qual)\n{\n\tbool ia32e;\n\n\t*exit_qual = ENTRY_FAIL_DEFAULT;\n\n\tif (!nested_guest_cr0_valid(vcpu, vmcs12->guest_cr0) ||\n\t    !nested_guest_cr4_valid(vcpu, vmcs12->guest_cr4))\n\t\treturn 1;\n\n\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_SHADOW_VMCS) &&\n\t    vmcs12->vmcs_link_pointer != -1ull) {\n\t\t*exit_qual = ENTRY_FAIL_VMCS_LINK_PTR;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-entry control is 1, the following checks\n\t * are performed on the field for the IA32_EFER MSR:\n\t * - Bits reserved in the IA32_EFER MSR must be 0.\n\t * - Bit 10 (corresponding to IA32_EFER.LMA) must equal the value of\n\t *   the IA-32e mode guest VM-exit control. It must also be identical\n\t *   to bit 8 (LME) if bit 31 in the CR0 field (corresponding to\n\t *   CR0.PG) is 1.\n\t */\n\tif (to_vmx(vcpu)->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER)) {\n\t\tia32e = (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->guest_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->guest_ia32_efer & EFER_LMA) ||\n\t\t    ((vmcs12->guest_cr0 & X86_CR0_PG) &&\n\t\t     ia32e != !!(vmcs12->guest_ia32_efer & EFER_LME)))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-exit control is 1, bits reserved in the\n\t * IA32_EFER MSR must be 0 in the field for that register. In addition,\n\t * the values of the LMA and LME bits in the field must each be that of\n\t * the host address-space size VM-exit control.\n\t */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER) {\n\t\tia32e = (vmcs12->vm_exit_controls &\n\t\t\t VM_EXIT_HOST_ADDR_SPACE_SIZE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->host_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LMA) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS) &&\n\t\t(is_noncanonical_address(vmcs12->guest_bndcfgs & PAGE_MASK, vcpu) ||\n\t\t(vmcs12->guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD)))\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int enter_vmx_non_root_mode(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 msr_entry_idx;\n\tu32 exit_qual;\n\tint r;\n\n\tenter_guest_mode(vcpu);\n\n\tif (!(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS))\n\t\tvmx->nested.vmcs01_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\n\tvmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);\n\tvmx_segment_cache_clear(vmx);\n\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset += vmcs12->tsc_offset;\n\n\tr = EXIT_REASON_INVALID_STATE;\n\tif (prepare_vmcs02(vcpu, vmcs12, &exit_qual))\n\t\tgoto fail;\n\n\tnested_get_vmcs12_pages(vcpu, vmcs12);\n\n\tr = EXIT_REASON_MSR_LOAD_FAIL;\n\tmsr_entry_idx = nested_vmx_load_msr(vcpu,\n\t\t\t\t\t    vmcs12->vm_entry_msr_load_addr,\n\t\t\t\t\t    vmcs12->vm_entry_msr_load_count);\n\tif (msr_entry_idx)\n\t\tgoto fail;\n\n\t/*\n\t * Note no nested_vmx_succeed or nested_vmx_fail here. At this point\n\t * we are no longer running L1, and VMLAUNCH/VMRESUME has not yet\n\t * returned as far as L1 is concerned. It will only return (and set\n\t * the success flag) when L2 exits (see nested_vmx_vmexit()).\n\t */\n\treturn 0;\n\nfail:\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset -= vmcs12->tsc_offset;\n\tleave_guest_mode(vcpu);\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\tnested_vmx_entry_failure(vcpu, vmcs12, r, exit_qual);\n\treturn 1;\n}\n\n/*\n * nested_vmx_run() handles a nested entry, i.e., a VMLAUNCH or VMRESUME on L1\n * for running an L2 nested guest.\n */\nstatic int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)\n{\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 interrupt_shadow = vmx_get_interrupt_shadow(vcpu);\n\tu32 exit_qual;\n\tint ret;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\tgoto out;\n\n\tvmcs12 = get_vmcs12(vcpu);\n\n\tif (enable_shadow_vmcs)\n\t\tcopy_shadow_to_vmcs12(vmx);\n\n\t/*\n\t * The nested entry process starts with enforcing various prerequisites\n\t * on vmcs12 as required by the Intel SDM, and act appropriately when\n\t * they fail: As the SDM explains, some conditions should cause the\n\t * instruction to fail, while others will cause the instruction to seem\n\t * to succeed, but return an EXIT_REASON_INVALID_STATE.\n\t * To speed up the normal (success) code path, we should avoid checking\n\t * for misconfigurations which will anyway be caught by the processor\n\t * when using the merged vmcs02.\n\t */\n\tif (interrupt_shadow & KVM_X86_SHADOW_INT_MOV_SS) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\t\t     VMXERR_ENTRY_EVENTS_BLOCKED_BY_MOV_SS);\n\t\tgoto out;\n\t}\n\n\tif (vmcs12->launch_state == launch) {\n\t\tnested_vmx_failValid(vcpu,\n\t\t\tlaunch ? VMXERR_VMLAUNCH_NONCLEAR_VMCS\n\t\t\t       : VMXERR_VMRESUME_NONLAUNCHED_VMCS);\n\t\tgoto out;\n\t}\n\n\tret = check_vmentry_prereqs(vcpu, vmcs12);\n\tif (ret) {\n\t\tnested_vmx_failValid(vcpu, ret);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * After this point, the trap flag no longer triggers a singlestep trap\n\t * on the vm entry instructions; don't call kvm_skip_emulated_instruction.\n\t * This is not 100% correct; for performance reasons, we delegate most\n\t * of the checks on host state to the processor.  If those fail,\n\t * the singlestep trap is missed.\n\t */\n\tskip_emulated_instruction(vcpu);\n\n\tret = check_vmentry_postreqs(vcpu, vmcs12, &exit_qual);\n\tif (ret) {\n\t\tnested_vmx_entry_failure(vcpu, vmcs12,\n\t\t\t\t\t EXIT_REASON_INVALID_STATE, exit_qual);\n\t\treturn 1;\n\t}\n\n\t/*\n\t * We're finally done with prerequisite checking, and can start with\n\t * the nested entry.\n\t */\n\n\tvmx->nested.nested_run_pending = 1;\n\tret = enter_vmx_non_root_mode(vcpu);\n\tif (ret) {\n\t\tvmx->nested.nested_run_pending = 0;\n\t\treturn ret;\n\t}\n\n\t/*\n\t * If we're entering a halted L2 vcpu and the L2 vcpu won't be woken\n\t * by event injection, halt vcpu.\n\t */\n\tif ((vmcs12->guest_activity_state == GUEST_ACTIVITY_HLT) &&\n\t    !(vmcs12->vm_entry_intr_info_field & INTR_INFO_VALID_MASK)) {\n\t\tvmx->nested.nested_run_pending = 0;\n\t\treturn kvm_vcpu_halt(vcpu);\n\t}\n\treturn 1;\n\nout:\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\n/*\n * On a nested exit from L2 to L1, vmcs12.guest_cr0 might not be up-to-date\n * because L2 may have changed some cr0 bits directly (CRO_GUEST_HOST_MASK).\n * This function returns the new value we should put in vmcs12.guest_cr0.\n * It's not enough to just return the vmcs02 GUEST_CR0. Rather,\n *  1. Bits that neither L0 nor L1 trapped, were set directly by L2 and are now\n *     available in vmcs02 GUEST_CR0. (Note: It's enough to check that L0\n *     didn't trap the bit, because if L1 did, so would L0).\n *  2. Bits that L1 asked to trap (and therefore L0 also did) could not have\n *     been modified by L2, and L1 knows it. So just leave the old value of\n *     the bit from vmcs12.guest_cr0. Note that the bit from vmcs02 GUEST_CR0\n *     isn't relevant, because if L0 traps this bit it can set it to anything.\n *  3. Bits that L1 didn't trap, but L0 did. L1 believes the guest could have\n *     changed these bits, and therefore they need to be updated, but L0\n *     didn't necessarily allow them to be changed in GUEST_CR0 - and rather\n *     put them in vmcs02 CR0_READ_SHADOW. So take these bits from there.\n */\nstatic inline unsigned long\nvmcs12_guest_cr0(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t/*1*/\t(vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |\n\t/*2*/\t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask) |\n\t/*3*/\t(vmcs_readl(CR0_READ_SHADOW) & ~(vmcs12->cr0_guest_host_mask |\n\t\t\tvcpu->arch.cr0_guest_owned_bits));\n}\n\nstatic inline unsigned long\nvmcs12_guest_cr4(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t/*1*/\t(vmcs_readl(GUEST_CR4) & vcpu->arch.cr4_guest_owned_bits) |\n\t/*2*/\t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask) |\n\t/*3*/\t(vmcs_readl(CR4_READ_SHADOW) & ~(vmcs12->cr4_guest_host_mask |\n\t\t\tvcpu->arch.cr4_guest_owned_bits));\n}\n\nstatic void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tu32 idt_vectoring;\n\tunsigned int nr;\n\n\tif (vcpu->arch.exception.injected) {\n\t\tnr = vcpu->arch.exception.nr;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (kvm_exception_is_soft(nr)) {\n\t\t\tvmcs12->vm_exit_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_EXCEPTION;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_HARD_EXCEPTION;\n\n\t\tif (vcpu->arch.exception.has_error_code) {\n\t\t\tidt_vectoring |= VECTORING_INFO_DELIVER_CODE_MASK;\n\t\t\tvmcs12->idt_vectoring_error_code =\n\t\t\t\tvcpu->arch.exception.error_code;\n\t\t}\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t} else if (vcpu->arch.nmi_injected) {\n\t\tvmcs12->idt_vectoring_info_field =\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR;\n\t} else if (vcpu->arch.interrupt.injected) {\n\t\tnr = vcpu->arch.interrupt.nr;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (vcpu->arch.interrupt.soft) {\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_INTR;\n\t\t\tvmcs12->vm_entry_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_EXT_INTR;\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t}\n}\n\nstatic int vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qual;\n\tbool block_nested_events =\n\t    vmx->nested.nested_run_pending || kvm_event_needs_reinjection(vcpu);\n\n\tif (vcpu->arch.exception.pending &&\n\t\tnested_vmx_check_exception(vcpu, &exit_qual)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_inject_exception_vmexit(vcpu, exit_qual);\n\t\treturn 0;\n\t}\n\n\tif (nested_cpu_has_preemption_timer(get_vmcs12(vcpu)) &&\n\t    vmx->nested.preemption_timer_expired) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && nested_exit_on_nmi(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  NMI_VECTOR | INTR_TYPE_NMI_INTR |\n\t\t\t\t  INTR_INFO_VALID_MASK, 0);\n\t\t/*\n\t\t * The NMI-triggered VM exit counts as injection:\n\t\t * clear this one and block further NMIs.\n\t\t */\n\t\tvcpu->arch.nmi_pending = 0;\n\t\tvmx_set_nmi_mask(vcpu, true);\n\t\treturn 0;\n\t}\n\n\tif ((kvm_cpu_has_interrupt(vcpu) || external_intr) &&\n\t    nested_exit_on_intr(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);\n\t\treturn 0;\n\t}\n\n\tvmx_complete_nested_posted_interrupt(vcpu);\n\treturn 0;\n}\n\nstatic u32 vmx_get_preemption_timer_value(struct kvm_vcpu *vcpu)\n{\n\tktime_t remaining =\n\t\thrtimer_get_remaining(&to_vmx(vcpu)->nested.preemption_timer);\n\tu64 value;\n\n\tif (ktime_to_ns(remaining) <= 0)\n\t\treturn 0;\n\n\tvalue = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;\n\tdo_div(value, 1000000);\n\treturn value >> VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n}\n\n/*\n * Update the guest state fields of vmcs12 to reflect changes that\n * occurred while L2 was running. (The \"IA-32e mode guest\" bit of the\n * VM-entry controls is also updated, since this is really a guest\n * state bit.)\n */\nstatic void sync_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tvmcs12->guest_cr0 = vmcs12_guest_cr0(vcpu, vmcs12);\n\tvmcs12->guest_cr4 = vmcs12_guest_cr4(vcpu, vmcs12);\n\n\tvmcs12->guest_rsp = kvm_register_read(vcpu, VCPU_REGS_RSP);\n\tvmcs12->guest_rip = kvm_register_read(vcpu, VCPU_REGS_RIP);\n\tvmcs12->guest_rflags = vmcs_readl(GUEST_RFLAGS);\n\n\tvmcs12->guest_es_selector = vmcs_read16(GUEST_ES_SELECTOR);\n\tvmcs12->guest_cs_selector = vmcs_read16(GUEST_CS_SELECTOR);\n\tvmcs12->guest_ss_selector = vmcs_read16(GUEST_SS_SELECTOR);\n\tvmcs12->guest_ds_selector = vmcs_read16(GUEST_DS_SELECTOR);\n\tvmcs12->guest_fs_selector = vmcs_read16(GUEST_FS_SELECTOR);\n\tvmcs12->guest_gs_selector = vmcs_read16(GUEST_GS_SELECTOR);\n\tvmcs12->guest_ldtr_selector = vmcs_read16(GUEST_LDTR_SELECTOR);\n\tvmcs12->guest_tr_selector = vmcs_read16(GUEST_TR_SELECTOR);\n\tvmcs12->guest_es_limit = vmcs_read32(GUEST_ES_LIMIT);\n\tvmcs12->guest_cs_limit = vmcs_read32(GUEST_CS_LIMIT);\n\tvmcs12->guest_ss_limit = vmcs_read32(GUEST_SS_LIMIT);\n\tvmcs12->guest_ds_limit = vmcs_read32(GUEST_DS_LIMIT);\n\tvmcs12->guest_fs_limit = vmcs_read32(GUEST_FS_LIMIT);\n\tvmcs12->guest_gs_limit = vmcs_read32(GUEST_GS_LIMIT);\n\tvmcs12->guest_ldtr_limit = vmcs_read32(GUEST_LDTR_LIMIT);\n\tvmcs12->guest_tr_limit = vmcs_read32(GUEST_TR_LIMIT);\n\tvmcs12->guest_gdtr_limit = vmcs_read32(GUEST_GDTR_LIMIT);\n\tvmcs12->guest_idtr_limit = vmcs_read32(GUEST_IDTR_LIMIT);\n\tvmcs12->guest_es_ar_bytes = vmcs_read32(GUEST_ES_AR_BYTES);\n\tvmcs12->guest_cs_ar_bytes = vmcs_read32(GUEST_CS_AR_BYTES);\n\tvmcs12->guest_ss_ar_bytes = vmcs_read32(GUEST_SS_AR_BYTES);\n\tvmcs12->guest_ds_ar_bytes = vmcs_read32(GUEST_DS_AR_BYTES);\n\tvmcs12->guest_fs_ar_bytes = vmcs_read32(GUEST_FS_AR_BYTES);\n\tvmcs12->guest_gs_ar_bytes = vmcs_read32(GUEST_GS_AR_BYTES);\n\tvmcs12->guest_ldtr_ar_bytes = vmcs_read32(GUEST_LDTR_AR_BYTES);\n\tvmcs12->guest_tr_ar_bytes = vmcs_read32(GUEST_TR_AR_BYTES);\n\tvmcs12->guest_es_base = vmcs_readl(GUEST_ES_BASE);\n\tvmcs12->guest_cs_base = vmcs_readl(GUEST_CS_BASE);\n\tvmcs12->guest_ss_base = vmcs_readl(GUEST_SS_BASE);\n\tvmcs12->guest_ds_base = vmcs_readl(GUEST_DS_BASE);\n\tvmcs12->guest_fs_base = vmcs_readl(GUEST_FS_BASE);\n\tvmcs12->guest_gs_base = vmcs_readl(GUEST_GS_BASE);\n\tvmcs12->guest_ldtr_base = vmcs_readl(GUEST_LDTR_BASE);\n\tvmcs12->guest_tr_base = vmcs_readl(GUEST_TR_BASE);\n\tvmcs12->guest_gdtr_base = vmcs_readl(GUEST_GDTR_BASE);\n\tvmcs12->guest_idtr_base = vmcs_readl(GUEST_IDTR_BASE);\n\n\tvmcs12->guest_interruptibility_info =\n\t\tvmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tvmcs12->guest_pending_dbg_exceptions =\n\t\tvmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_HLT;\n\telse\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_ACTIVE;\n\n\tif (nested_cpu_has_preemption_timer(vmcs12)) {\n\t\tif (vmcs12->vm_exit_controls &\n\t\t    VM_EXIT_SAVE_VMX_PREEMPTION_TIMER)\n\t\t\tvmcs12->vmx_preemption_timer_value =\n\t\t\t\tvmx_get_preemption_timer_value(vcpu);\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\t}\n\n\t/*\n\t * In some cases (usually, nested EPT), L2 is allowed to change its\n\t * own CR3 without exiting. If it has changed it, we must keep it.\n\t * Of course, if L0 is using shadow page tables, GUEST_CR3 was defined\n\t * by L0, not L1 or L2, so we mustn't unconditionally copy it to vmcs12.\n\t *\n\t * Additionally, restore L2's PDPTR to vmcs12.\n\t */\n\tif (enable_ept) {\n\t\tvmcs12->guest_cr3 = vmcs_readl(GUEST_CR3);\n\t\tvmcs12->guest_pdptr0 = vmcs_read64(GUEST_PDPTR0);\n\t\tvmcs12->guest_pdptr1 = vmcs_read64(GUEST_PDPTR1);\n\t\tvmcs12->guest_pdptr2 = vmcs_read64(GUEST_PDPTR2);\n\t\tvmcs12->guest_pdptr3 = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\tvmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);\n\n\tif (nested_cpu_has_vid(vmcs12))\n\t\tvmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);\n\n\tvmcs12->vm_entry_controls =\n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_IA32E_MODE) |\n\t\t(vm_entry_controls_get(to_vmx(vcpu)) & VM_ENTRY_IA32E_MODE);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_DEBUG_CONTROLS) {\n\t\tkvm_get_dr(vcpu, 7, (unsigned long *)&vmcs12->guest_dr7);\n\t\tvmcs12->guest_ia32_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\t}\n\n\t/* TODO: These cannot have changed unless we have MSR bitmaps and\n\t * the relevant bit asks not to trap the change */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)\n\t\tvmcs12->guest_ia32_pat = vmcs_read64(GUEST_IA32_PAT);\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_EFER)\n\t\tvmcs12->guest_ia32_efer = vcpu->arch.efer;\n\tvmcs12->guest_sysenter_cs = vmcs_read32(GUEST_SYSENTER_CS);\n\tvmcs12->guest_sysenter_esp = vmcs_readl(GUEST_SYSENTER_ESP);\n\tvmcs12->guest_sysenter_eip = vmcs_readl(GUEST_SYSENTER_EIP);\n\tif (kvm_mpx_supported())\n\t\tvmcs12->guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);\n}\n\n/*\n * prepare_vmcs12 is part of what we need to do when the nested L2 guest exits\n * and we want to prepare to run its L1 parent. L1 keeps a vmcs for L2 (vmcs12),\n * and this function updates it to reflect the changes to the guest state while\n * L2 was running (and perhaps made some exits which were handled directly by L0\n * without going back to L1), and to reflect the exit reason.\n * Note that we do not have to copy here all VMCS fields, just those that\n * could have changed by the L2 guest or the exit - i.e., the guest-state and\n * exit-information fields only. Other fields are modified by L1 with VMWRITE,\n * which already writes to vmcs12 directly.\n */\nstatic void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t   u32 exit_reason, u32 exit_intr_info,\n\t\t\t   unsigned long exit_qualification)\n{\n\t/* update guest state fields: */\n\tsync_vmcs12(vcpu, vmcs12);\n\n\t/* update exit information fields: */\n\n\tvmcs12->vm_exit_reason = exit_reason;\n\tvmcs12->exit_qualification = exit_qualification;\n\tvmcs12->vm_exit_intr_info = exit_intr_info;\n\n\tvmcs12->idt_vectoring_info_field = 0;\n\tvmcs12->vm_exit_instruction_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tvmcs12->vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\n\tif (!(vmcs12->vm_exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY)) {\n\t\tvmcs12->launch_state = 1;\n\n\t\t/* vm_entry_intr_info_field is cleared on exit. Emulate this\n\t\t * instead of reading the real value. */\n\t\tvmcs12->vm_entry_intr_info_field &= ~INTR_INFO_VALID_MASK;\n\n\t\t/*\n\t\t * Transfer the event that L0 or L1 may wanted to inject into\n\t\t * L2 to IDT_VECTORING_INFO_FIELD.\n\t\t */\n\t\tvmcs12_save_pending_event(vcpu, vmcs12);\n\t}\n\n\t/*\n\t * Drop what we picked up for L2 via vmx_complete_interrupts. It is\n\t * preserved above and would only end up incorrectly in L1.\n\t */\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n}\n\nstatic void load_vmcs12_mmu_host_state(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12)\n{\n\tu32 entry_failure_code;\n\n\tnested_ept_uninit_mmu_context(vcpu);\n\n\t/*\n\t * Only PDPTE load can fail as the value of cr3 was checked on entry and\n\t * couldn't have changed.\n\t */\n\tif (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, &entry_failure_code))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);\n\n\tif (!enable_ept)\n\t\tvcpu->arch.walk_mmu->inject_page_fault = kvm_inject_page_fault;\n}\n\n/*\n * A part of what we need to when the nested L2 guest exits and we want to\n * run its L1 parent, is to reset L1's guest state to the host state specified\n * in vmcs12.\n * This function is to be called not only on normal nested exit, but also on\n * a nested entry failure, as explained in Intel's spec, 3B.23.7 (\"VM-Entry\n * Failures During or After Loading Guest State\").\n * This function should be called when the active VMCS is L1's (vmcs01).\n */\nstatic void load_vmcs12_host_state(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tstruct kvm_segment seg;\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER)\n\t\tvcpu->arch.efer = vmcs12->host_ia32_efer;\n\telse if (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tvcpu->arch.efer |= (EFER_LMA | EFER_LME);\n\telse\n\t\tvcpu->arch.efer &= ~(EFER_LMA | EFER_LME);\n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\tkvm_register_write(vcpu, VCPU_REGS_RSP, vmcs12->host_rsp);\n\tkvm_register_write(vcpu, VCPU_REGS_RIP, vmcs12->host_rip);\n\tvmx_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\t/*\n\t * Note that calling vmx_set_cr0 is important, even if cr0 hasn't\n\t * actually changed, because vmx_set_cr0 refers to efer set above.\n\t *\n\t * CR0_GUEST_HOST_MASK is already set in the original vmcs01\n\t * (KVM doesn't change it);\n\t */\n\tvcpu->arch.cr0_guest_owned_bits = X86_CR0_TS;\n\tvmx_set_cr0(vcpu, vmcs12->host_cr0);\n\n\t/* Same as above - no reason to call set_cr4_guest_host_mask().  */\n\tvcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);\n\tvmx_set_cr4(vcpu, vmcs12->host_cr4);\n\n\tload_vmcs12_mmu_host_state(vcpu, vmcs12);\n\n\t/*\n\t * If vmcs01 don't use VPID, CPU flushes TLB on every\n\t * VMEntry/VMExit. Thus, no need to flush TLB.\n\t *\n\t * If vmcs12 uses VPID, TLB entries populated by L2 are\n\t * tagged with vmx->nested.vpid02 while L1 entries are tagged\n\t * with vmx->vpid. Thus, no need to flush TLB.\n\t *\n\t * Therefore, flush TLB only in case vmcs01 uses VPID and\n\t * vmcs12 don't use VPID as in this case L1 & L2 TLB entries\n\t * are both tagged with vmx->vpid.\n\t */\n\tif (enable_vpid &&\n\t    !(nested_cpu_has_vpid(vmcs12) && to_vmx(vcpu)->nested.vpid02)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->host_ia32_sysenter_cs);\n\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->host_ia32_sysenter_esp);\n\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->host_ia32_sysenter_eip);\n\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->host_idtr_base);\n\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->host_gdtr_base);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xFFFF);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xFFFF);\n\n\t/* If not VM_EXIT_CLEAR_BNDCFGS, the L2 value propagates to L1.  */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_CLEAR_BNDCFGS)\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->host_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->host_ia32_pat;\n\t}\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tvmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL,\n\t\t\tvmcs12->host_ia32_perf_global_ctrl);\n\n\t/* Set L1 segment info according to Intel SDM\n\t    27.5.2 Loading Host Segment and Descriptor-Table Registers */\n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.selector = vmcs12->host_cs_selector,\n\t\t.type = 11,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.g = 1\n\t};\n\tif (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tseg.l = 1;\n\telse\n\t\tseg.db = 1;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_CS);\n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.type = 3,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.db = 1,\n\t\t.g = 1\n\t};\n\tseg.selector = vmcs12->host_ds_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_DS);\n\tseg.selector = vmcs12->host_es_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_ES);\n\tseg.selector = vmcs12->host_ss_selector;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_SS);\n\tseg.selector = vmcs12->host_fs_selector;\n\tseg.base = vmcs12->host_fs_base;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_FS);\n\tseg.selector = vmcs12->host_gs_selector;\n\tseg.base = vmcs12->host_gs_base;\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_GS);\n\tseg = (struct kvm_segment) {\n\t\t.base = vmcs12->host_tr_base,\n\t\t.limit = 0x67,\n\t\t.selector = vmcs12->host_tr_selector,\n\t\t.type = 11,\n\t\t.present = 1\n\t};\n\tvmx_set_segment(vcpu, &seg, VCPU_SREG_TR);\n\n\tkvm_set_dr(vcpu, 7, 0x400);\n\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_update_msr_bitmap(vcpu);\n\n\tif (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,\n\t\t\t\tvmcs12->vm_exit_msr_load_count))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);\n}\n\n/*\n * Emulate an exit from nested guest (L2) to L1, i.e., prepare to run L1\n * and modify vmcs12 to make it see what it would expect to see there if\n * L2 was its real guest. Must only be called when in L2 (is_guest_mode())\n */\nstatic void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,\n\t\t\t      u32 exit_intr_info,\n\t\t\t      unsigned long exit_qualification)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t/* trying to cancel vmlaunch/vmresume is a bug */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\t/*\n\t * The only expected VM-instruction error is \"VM entry with\n\t * invalid control field(s).\" Anything else indicates a\n\t * problem with L0.\n\t */\n\tWARN_ON_ONCE(vmx->fail && (vmcs_read32(VM_INSTRUCTION_ERROR) !=\n\t\t\t\t   VMXERR_ENTRY_INVALID_CONTROL_FIELD));\n\n\tleave_guest_mode(vcpu);\n\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETING)\n\t\tvcpu->arch.tsc_offset -= vmcs12->tsc_offset;\n\n\tif (likely(!vmx->fail)) {\n\t\tif (exit_reason == -1)\n\t\t\tsync_vmcs12(vcpu, vmcs12);\n\t\telse\n\t\t\tprepare_vmcs12(vcpu, vmcs12, exit_reason, exit_intr_info,\n\t\t\t\t       exit_qualification);\n\n\t\tif (nested_vmx_store_msr(vcpu, vmcs12->vm_exit_msr_store_addr,\n\t\t\t\t\t vmcs12->vm_exit_msr_store_count))\n\t\t\tnested_vmx_abort(vcpu, VMX_ABORT_SAVE_GUEST_MSR_FAIL);\n\t}\n\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\tvm_entry_controls_reset_shadow(vmx);\n\tvm_exit_controls_reset_shadow(vmx);\n\tvmx_segment_cache_clear(vmx);\n\n\t/* Update any VMCS fields that might have changed while L2 ran */\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.nr);\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (vmx->hv_deadline_tsc == -1)\n\t\tvmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n\telse\n\t\tvmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t      PIN_BASED_VMX_PREEMPTION_TIMER);\n\tif (kvm_has_tsc_control)\n\t\tdecache_tsc_multiplier(vmx);\n\n\tif (vmx->nested.change_vmcs01_virtual_apic_mode) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = false;\n\t\tvmx_set_virtual_apic_mode(vcpu);\n\t} else if (!nested_cpu_has_ept(vmcs12) &&\n\t\t   nested_cpu_has2(vmcs12,\n\t\t\t\t   SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\tvmx_flush_tlb(vcpu, true);\n\t}\n\n\t/* This is needed for same reason as it was needed in prepare_vmcs02 */\n\tvmx->host_rsp = 0;\n\n\t/* Unpin physical memory we referred to in vmcs02 */\n\tif (vmx->nested.apic_access_page) {\n\t\tkvm_release_page_dirty(vmx->nested.apic_access_page);\n\t\tvmx->nested.apic_access_page = NULL;\n\t}\n\tif (vmx->nested.virtual_apic_page) {\n\t\tkvm_release_page_dirty(vmx->nested.virtual_apic_page);\n\t\tvmx->nested.virtual_apic_page = NULL;\n\t}\n\tif (vmx->nested.pi_desc_page) {\n\t\tkunmap(vmx->nested.pi_desc_page);\n\t\tkvm_release_page_dirty(vmx->nested.pi_desc_page);\n\t\tvmx->nested.pi_desc_page = NULL;\n\t\tvmx->nested.pi_desc = NULL;\n\t}\n\n\t/*\n\t * We are now running in L2, mmu_notifier will force to reload the\n\t * page's hpa for L2 vmcs. Need to reload it for L1 before entering L1.\n\t */\n\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\tif (enable_shadow_vmcs && exit_reason != -1)\n\t\tvmx->nested.sync_shadow_vmcs = true;\n\n\t/* in case we halted in L2 */\n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tif (likely(!vmx->fail)) {\n\t\t/*\n\t\t * TODO: SDM says that with acknowledge interrupt on\n\t\t * exit, bit 31 of the VM-exit interrupt information\n\t\t * (valid interrupt) is always set to 1 on\n\t\t * EXIT_REASON_EXTERNAL_INTERRUPT, so we shouldn't\n\t\t * need kvm_cpu_has_interrupt().  See the commit\n\t\t * message for details.\n\t\t */\n\t\tif (nested_exit_intr_ack_set(vcpu) &&\n\t\t    exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&\n\t\t    kvm_cpu_has_interrupt(vcpu)) {\n\t\t\tint irq = kvm_cpu_get_interrupt(vcpu);\n\t\t\tWARN_ON(irq < 0);\n\t\t\tvmcs12->vm_exit_intr_info = irq |\n\t\t\t\tINTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;\n\t\t}\n\n\t\tif (exit_reason != -1)\n\t\t\ttrace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,\n\t\t\t\t\t\t       vmcs12->exit_qualification,\n\t\t\t\t\t\t       vmcs12->idt_vectoring_info_field,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_info,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_error_code,\n\t\t\t\t\t\t       KVM_ISA_VMX);\n\n\t\tload_vmcs12_host_state(vcpu, vmcs12);\n\n\t\treturn;\n\t}\n\t\n\t/*\n\t * After an early L2 VM-entry failure, we're now back\n\t * in L1 which thinks it just finished a VMLAUNCH or\n\t * VMRESUME instruction, so we need to set the failure\n\t * flag and the VM-instruction error field of the VMCS\n\t * accordingly.\n\t */\n\tnested_vmx_failValid(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\tload_vmcs12_mmu_host_state(vcpu, vmcs12);\n\n\t/*\n\t * The emulated instruction was already skipped in\n\t * nested_vmx_run, but the updated RIP was never\n\t * written back to the vmcs01.\n\t */\n\tskip_emulated_instruction(vcpu);\n\tvmx->fail = 0;\n}\n\n/*\n * Forcibly leave nested mode in order to be able to reset the VCPU later on.\n */\nstatic void vmx_leave_nested(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.nested_run_pending = 0;\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\t}\n\tfree_nested(to_vmx(vcpu));\n}\n\n/*\n * L1's failure to enter L2 is a subset of a normal exit, as explained in\n * 23.7 \"VM-entry failures during or after loading guest state\" (this also\n * lists the acceptable exit-reason and exit-qualification parameters).\n * It should only be called before L2 actually succeeded to run, and when\n * vmcs01 is current (it doesn't leave_guest_mode() or switch vmcss).\n */\nstatic void nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification)\n{\n\tload_vmcs12_host_state(vcpu, vmcs12);\n\tvmcs12->vm_exit_reason = reason | VMX_EXIT_REASONS_FAILED_VMENTRY;\n\tvmcs12->exit_qualification = qualification;\n\tnested_vmx_succeed(vcpu);\n\tif (enable_shadow_vmcs)\n\t\tto_vmx(vcpu)->nested.sync_shadow_vmcs = true;\n}\n\nstatic int vmx_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct x86_emulate_ctxt *ctxt = &vcpu->arch.emulate_ctxt;\n\n\t/*\n\t * RDPID causes #UD if disabled through secondary execution controls.\n\t * Because it is marked as EmulateOnUD, we need to intercept it here.\n\t */\n\tif (info->intercept == x86_intercept_rdtscp &&\n\t    !nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDTSCP)) {\n\t\tctxt->exception.vector = UD_VECTOR;\n\t\tctxt->exception.error_code_valid = false;\n\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t}\n\n\t/* TODO: check more intercepts... */\n\treturn X86EMUL_CONTINUE;\n}\n\n#ifdef CONFIG_X86_64\n/* (a << shift) / divisor, return 1 if overflow otherwise 0 */\nstatic inline int u64_shl_div_u64(u64 a, unsigned int shift,\n\t\t\t\t  u64 divisor, u64 *result)\n{\n\tu64 low = a << shift, high = a >> (64 - shift);\n\n\t/* To avoid the overflow on divq */\n\tif (high >= divisor)\n\t\treturn 1;\n\n\t/* Low hold the result, high hold rem which is discarded */\n\tasm(\"divq %2\\n\\t\" : \"=a\" (low), \"=d\" (high) :\n\t    \"rm\" (divisor), \"0\" (low), \"1\" (high));\n\t*result = low;\n\n\treturn 0;\n}\n\nstatic int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc)\n{\n\tstruct vcpu_vmx *vmx;\n\tu64 tscl, guest_tscl, delta_tsc, lapic_timer_advance_cycles;\n\n\tif (kvm_mwait_in_guest(vcpu->kvm))\n\t\treturn -EOPNOTSUPP;\n\n\tvmx = to_vmx(vcpu);\n\ttscl = rdtsc();\n\tguest_tscl = kvm_read_l1_tsc(vcpu, tscl);\n\tdelta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;\n\tlapic_timer_advance_cycles = nsec_to_cycles(vcpu, lapic_timer_advance_ns);\n\n\tif (delta_tsc > lapic_timer_advance_cycles)\n\t\tdelta_tsc -= lapic_timer_advance_cycles;\n\telse\n\t\tdelta_tsc = 0;\n\n\t/* Convert to host delta tsc if tsc scaling is enabled */\n\tif (vcpu->arch.tsc_scaling_ratio != kvm_default_tsc_scaling_ratio &&\n\t\t\tu64_shl_div_u64(delta_tsc,\n\t\t\t\tkvm_tsc_scaling_ratio_frac_bits,\n\t\t\t\tvcpu->arch.tsc_scaling_ratio,\n\t\t\t\t&delta_tsc))\n\t\treturn -ERANGE;\n\n\t/*\n\t * If the delta tsc can't fit in the 32 bit after the multi shift,\n\t * we can't use the preemption timer.\n\t * It's possible that it fits on later vmentries, but checking\n\t * on every vmentry is costly so we just use an hrtimer.\n\t */\n\tif (delta_tsc >> (cpu_preemption_timer_multi + 32))\n\t\treturn -ERANGE;\n\n\tvmx->hv_deadline_tsc = tscl + delta_tsc;\n\tvmcs_set_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n\n\treturn delta_tsc == 0;\n}\n\nstatic void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tvmx->hv_deadline_tsc = -1;\n\tvmcs_clear_bits(PIN_BASED_VM_EXEC_CONTROL,\n\t\t\tPIN_BASED_VMX_PREEMPTION_TIMER);\n}\n#endif\n\nstatic void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tshrink_ple_window(vcpu);\n}\n\nstatic void vmx_slot_enable_log_dirty(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *slot)\n{\n\tkvm_mmu_slot_leaf_clear_dirty(kvm, slot);\n\tkvm_mmu_slot_largepage_remove_write_access(kvm, slot);\n}\n\nstatic void vmx_slot_disable_log_dirty(struct kvm *kvm,\n\t\t\t\t       struct kvm_memory_slot *slot)\n{\n\tkvm_mmu_slot_set_dirty(kvm, slot);\n}\n\nstatic void vmx_flush_log_dirty(struct kvm *kvm)\n{\n\tkvm_flush_pml_buffers(kvm);\n}\n\nstatic int vmx_write_pml_buffer(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t gpa;\n\tstruct page *page = NULL;\n\tu64 *pml_address;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tWARN_ON_ONCE(vmx->nested.pml_full);\n\n\t\t/*\n\t\t * Check if PML is enabled for the nested guest.\n\t\t * Whether eptp bit 6 is set is already checked\n\t\t * as part of A/D emulation.\n\t\t */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tif (!nested_cpu_has_pml(vmcs12))\n\t\t\treturn 0;\n\n\t\tif (vmcs12->guest_pml_index >= PML_ENTITY_NUM) {\n\t\t\tvmx->nested.pml_full = true;\n\t\t\treturn 1;\n\t\t}\n\n\t\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS) & ~0xFFFull;\n\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->pml_address);\n\t\tif (is_error_page(page))\n\t\t\treturn 0;\n\n\t\tpml_address = kmap(page);\n\t\tpml_address[vmcs12->guest_pml_index--] = gpa;\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t}\n\n\treturn 0;\n}\n\nstatic void vmx_enable_log_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t\t   struct kvm_memory_slot *memslot,\n\t\t\t\t\t   gfn_t offset, unsigned long mask)\n{\n\tkvm_mmu_clear_dirty_pt_masked(kvm, memslot, offset, mask);\n}\n\nstatic void __pi_post_block(struct kvm_vcpu *vcpu)\n{\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\tstruct pi_desc old, new;\n\tunsigned int dest;\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\t\tWARN(old.nv != POSTED_INTR_WAKEUP_VECTOR,\n\t\t     \"Wakeup handler not enabled while the VCPU is blocked\\n\");\n\n\t\tdest = cpu_physical_id(vcpu->cpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'notification vector' */\n\t\tnew.nv = POSTED_INTR_VECTOR;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n\n\tif (!WARN_ON_ONCE(vcpu->pre_pcpu == -1)) {\n\t\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tlist_del(&vcpu->blocked_vcpu_list);\n\t\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tvcpu->pre_pcpu = -1;\n\t}\n}\n\n/*\n * This routine does the following things for vCPU which is going\n * to be blocked if VT-d PI is enabled.\n * - Store the vCPU to the wakeup list, so when interrupts happen\n *   we can find the right vCPU to wake up.\n * - Change the Posted-interrupt descriptor as below:\n *      'NDST' <-- vcpu->pre_pcpu\n *      'NV' <-- POSTED_INTR_WAKEUP_VECTOR\n * - If 'ON' is set during this process, which means at least one\n *   interrupt is posted for this vCPU, we cannot block it, in\n *   this case, return 1, otherwise, return 0.\n *\n */\nstatic int pi_pre_block(struct kvm_vcpu *vcpu)\n{\n\tunsigned int dest;\n\tstruct pi_desc old, new;\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn 0;\n\n\tWARN_ON(irqs_disabled());\n\tlocal_irq_disable();\n\tif (!WARN_ON_ONCE(vcpu->pre_pcpu != -1)) {\n\t\tvcpu->pre_pcpu = vcpu->cpu;\n\t\tspin_lock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t\tlist_add_tail(&vcpu->blocked_vcpu_list,\n\t\t\t      &per_cpu(blocked_vcpu_on_cpu,\n\t\t\t\t       vcpu->pre_pcpu));\n\t\tspin_unlock(&per_cpu(blocked_vcpu_on_cpu_lock, vcpu->pre_pcpu));\n\t}\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\tWARN((pi_desc->sn == 1),\n\t\t     \"Warning: SN field of posted-interrupts \"\n\t\t     \"is set before blocking\\n\");\n\n\t\t/*\n\t\t * Since vCPU can be preempted during this process,\n\t\t * vcpu->cpu could be different with pre_pcpu, we\n\t\t * need to set pre_pcpu as the destination of wakeup\n\t\t * notification event, then we can find the right vCPU\n\t\t * to wakeup in wakeup handler if interrupts happen\n\t\t * when the vCPU is in blocked state.\n\t\t */\n\t\tdest = cpu_physical_id(vcpu->pre_pcpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'wakeup vector' */\n\t\tnew.nv = POSTED_INTR_WAKEUP_VECTOR;\n\t} while (cmpxchg64(&pi_desc->control, old.control,\n\t\t\t   new.control) != old.control);\n\n\t/* We should not block the vCPU if an interrupt is posted for it.  */\n\tif (pi_test_on(pi_desc) == 1)\n\t\t__pi_post_block(vcpu);\n\n\tlocal_irq_enable();\n\treturn (vcpu->pre_pcpu == -1);\n}\n\nstatic int vmx_pre_block(struct kvm_vcpu *vcpu)\n{\n\tif (pi_pre_block(vcpu))\n\t\treturn 1;\n\n\tif (kvm_lapic_hv_timer_in_use(vcpu))\n\t\tkvm_lapic_switch_to_sw_timer(vcpu);\n\n\treturn 0;\n}\n\nstatic void pi_post_block(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->pre_pcpu == -1)\n\t\treturn;\n\n\tWARN_ON(irqs_disabled());\n\tlocal_irq_disable();\n\t__pi_post_block(vcpu);\n\tlocal_irq_enable();\n}\n\nstatic void vmx_post_block(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_x86_ops->set_hv_timer)\n\t\tkvm_lapic_switch_to_hv_timer(vcpu);\n\n\tpi_post_block(vcpu);\n}\n\n/*\n * vmx_update_pi_irte - set IRTE for Posted-Interrupts\n *\n * @kvm: kvm\n * @host_irq: host irq of the interrupt\n * @guest_irq: gsi of the interrupt\n * @set: set or unset PI\n * returns 0 on success, < 0 on failure\n */\nstatic int vmx_update_pi_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\t      uint32_t guest_irq, bool set)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_irq_routing_table *irq_rt;\n\tstruct kvm_lapic_irq irq;\n\tstruct kvm_vcpu *vcpu;\n\tstruct vcpu_data vcpu_info;\n\tint idx, ret = 0;\n\n\tif (!kvm_arch_has_assigned_device(kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP) ||\n\t\t!kvm_vcpu_apicv_active(kvm->vcpus[0]))\n\t\treturn 0;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n\tif (guest_irq >= irq_rt->nr_rt_entries ||\n\t    hlist_empty(&irq_rt->map[guest_irq])) {\n\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",\n\t\t\t     guest_irq, irq_rt->nr_rt_entries);\n\t\tgoto out;\n\t}\n\n\thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n\t\tif (e->type != KVM_IRQ_ROUTING_MSI)\n\t\t\tcontinue;\n\t\t/*\n\t\t * VT-d PI cannot support posting multicast/broadcast\n\t\t * interrupts to a vCPU, we still use interrupt remapping\n\t\t * for these kind of interrupts.\n\t\t *\n\t\t * For lowest-priority interrupts, we only support\n\t\t * those with single CPU as the destination, e.g. user\n\t\t * configures the interrupts via /proc/irq or uses\n\t\t * irqbalance to make the interrupts single-CPU.\n\t\t *\n\t\t * We will support full lowest-priority interrupt later.\n\t\t */\n\n\t\tkvm_set_msi_irq(kvm, e, &irq);\n\t\tif (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu)) {\n\t\t\t/*\n\t\t\t * Make sure the IRTE is in remapped mode if\n\t\t\t * we don't handle it in posted mode.\n\t\t\t */\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\t\t\tif (ret < 0) {\n\t\t\t\tprintk(KERN_INFO\n\t\t\t\t   \"failed to back to remapped mode, irq: %u\\n\",\n\t\t\t\t   host_irq);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tvcpu_info.pi_desc_addr = __pa(vcpu_to_pi_desc(vcpu));\n\t\tvcpu_info.vector = irq.vector;\n\n\t\ttrace_kvm_pi_irte_update(host_irq, vcpu->vcpu_id, e->gsi,\n\t\t\t\tvcpu_info.vector, vcpu_info.pi_desc_addr, set);\n\n\t\tif (set)\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &vcpu_info);\n\t\telse\n\t\t\tret = irq_set_vcpu_affinity(host_irq, NULL);\n\n\t\tif (ret < 0) {\n\t\t\tprintk(KERN_INFO \"%s: failed to update PI IRTE\\n\",\n\t\t\t\t\t__func__);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\treturn ret;\n}\n\nstatic void vmx_setup_mce(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.mcg_cap & MCG_LMCE_P)\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEATURE_CONTROL_LMCE;\n\telse\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEATURE_CONTROL_LMCE;\n}\n\nstatic int vmx_smi_allowed(struct kvm_vcpu *vcpu)\n{\n\t/* we need a nested vmexit to enter SMM, postpone if run is pending */\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int vmx_pre_enter_smm(struct kvm_vcpu *vcpu, char *smstate)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmx->nested.smm.guest_mode = is_guest_mode(vcpu);\n\tif (vmx->nested.smm.guest_mode)\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\n\tvmx->nested.smm.vmxon = vmx->nested.vmxon;\n\tvmx->nested.vmxon = false;\n\tvmx_clear_hlt(vcpu);\n\treturn 0;\n}\n\nstatic int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, u64 smbase)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint ret;\n\n\tif (vmx->nested.smm.vmxon) {\n\t\tvmx->nested.vmxon = true;\n\t\tvmx->nested.smm.vmxon = false;\n\t}\n\n\tif (vmx->nested.smm.guest_mode) {\n\t\tvcpu->arch.hflags &= ~HF_SMM_MASK;\n\t\tret = enter_vmx_non_root_mode(vcpu);\n\t\tvcpu->arch.hflags |= HF_SMM_MASK;\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvmx->nested.smm.guest_mode = false;\n\t}\n\treturn 0;\n}\n\nstatic int enable_smi_window(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nstatic struct kvm_x86_ops vmx_x86_ops __ro_after_init = {\n\t.cpu_has_kvm_support = cpu_has_kvm_support,\n\t.disabled_by_bios = vmx_disabled_by_bios,\n\t.hardware_setup = hardware_setup,\n\t.hardware_unsetup = hardware_unsetup,\n\t.check_processor_compatibility = vmx_check_processor_compat,\n\t.hardware_enable = hardware_enable,\n\t.hardware_disable = hardware_disable,\n\t.cpu_has_accelerated_tpr = report_flexpriority,\n\t.cpu_has_high_real_mode_segbase = vmx_has_high_real_mode_segbase,\n\n\t.vm_init = vmx_vm_init,\n\t.vm_alloc = vmx_vm_alloc,\n\t.vm_free = vmx_vm_free,\n\n\t.vcpu_create = vmx_create_vcpu,\n\t.vcpu_free = vmx_free_vcpu,\n\t.vcpu_reset = vmx_vcpu_reset,\n\n\t.prepare_guest_switch = vmx_save_host_state,\n\t.vcpu_load = vmx_vcpu_load,\n\t.vcpu_put = vmx_vcpu_put,\n\n\t.update_bp_intercept = update_exception_bitmap,\n\t.get_msr_feature = vmx_get_msr_feature,\n\t.get_msr = vmx_get_msr,\n\t.set_msr = vmx_set_msr,\n\t.get_segment_base = vmx_get_segment_base,\n\t.get_segment = vmx_get_segment,\n\t.set_segment = vmx_set_segment,\n\t.get_cpl = vmx_get_cpl,\n\t.get_cs_db_l_bits = vmx_get_cs_db_l_bits,\n\t.decache_cr0_guest_bits = vmx_decache_cr0_guest_bits,\n\t.decache_cr3 = vmx_decache_cr3,\n\t.decache_cr4_guest_bits = vmx_decache_cr4_guest_bits,\n\t.set_cr0 = vmx_set_cr0,\n\t.set_cr3 = vmx_set_cr3,\n\t.set_cr4 = vmx_set_cr4,\n\t.set_efer = vmx_set_efer,\n\t.get_idt = vmx_get_idt,\n\t.set_idt = vmx_set_idt,\n\t.get_gdt = vmx_get_gdt,\n\t.set_gdt = vmx_set_gdt,\n\t.get_dr6 = vmx_get_dr6,\n\t.set_dr6 = vmx_set_dr6,\n\t.set_dr7 = vmx_set_dr7,\n\t.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,\n\t.cache_reg = vmx_cache_reg,\n\t.get_rflags = vmx_get_rflags,\n\t.set_rflags = vmx_set_rflags,\n\n\t.tlb_flush = vmx_flush_tlb,\n\n\t.run = vmx_vcpu_run,\n\t.handle_exit = vmx_handle_exit,\n\t.skip_emulated_instruction = skip_emulated_instruction,\n\t.set_interrupt_shadow = vmx_set_interrupt_shadow,\n\t.get_interrupt_shadow = vmx_get_interrupt_shadow,\n\t.patch_hypercall = vmx_patch_hypercall,\n\t.set_irq = vmx_inject_irq,\n\t.set_nmi = vmx_inject_nmi,\n\t.queue_exception = vmx_queue_exception,\n\t.cancel_injection = vmx_cancel_injection,\n\t.interrupt_allowed = vmx_interrupt_allowed,\n\t.nmi_allowed = vmx_nmi_allowed,\n\t.get_nmi_mask = vmx_get_nmi_mask,\n\t.set_nmi_mask = vmx_set_nmi_mask,\n\t.enable_nmi_window = enable_nmi_window,\n\t.enable_irq_window = enable_irq_window,\n\t.update_cr8_intercept = update_cr8_intercept,\n\t.set_virtual_apic_mode = vmx_set_virtual_apic_mode,\n\t.set_apic_access_page_addr = vmx_set_apic_access_page_addr,\n\t.get_enable_apicv = vmx_get_enable_apicv,\n\t.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,\n\t.load_eoi_exitmap = vmx_load_eoi_exitmap,\n\t.apicv_post_state_restore = vmx_apicv_post_state_restore,\n\t.hwapic_irr_update = vmx_hwapic_irr_update,\n\t.hwapic_isr_update = vmx_hwapic_isr_update,\n\t.sync_pir_to_irr = vmx_sync_pir_to_irr,\n\t.deliver_posted_interrupt = vmx_deliver_posted_interrupt,\n\n\t.set_tss_addr = vmx_set_tss_addr,\n\t.set_identity_map_addr = vmx_set_identity_map_addr,\n\t.get_tdp_level = get_ept_level,\n\t.get_mt_mask = vmx_get_mt_mask,\n\n\t.get_exit_info = vmx_get_exit_info,\n\n\t.get_lpage_level = vmx_get_lpage_level,\n\n\t.cpuid_update = vmx_cpuid_update,\n\n\t.rdtscp_supported = vmx_rdtscp_supported,\n\t.invpcid_supported = vmx_invpcid_supported,\n\n\t.set_supported_cpuid = vmx_set_supported_cpuid,\n\n\t.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,\n\n\t.read_l1_tsc_offset = vmx_read_l1_tsc_offset,\n\t.write_tsc_offset = vmx_write_tsc_offset,\n\n\t.set_tdp_cr3 = vmx_set_cr3,\n\n\t.check_intercept = vmx_check_intercept,\n\t.handle_external_intr = vmx_handle_external_intr,\n\t.mpx_supported = vmx_mpx_supported,\n\t.xsaves_supported = vmx_xsaves_supported,\n\t.umip_emulated = vmx_umip_emulated,\n\n\t.check_nested_events = vmx_check_nested_events,\n\n\t.sched_in = vmx_sched_in,\n\n\t.slot_enable_log_dirty = vmx_slot_enable_log_dirty,\n\t.slot_disable_log_dirty = vmx_slot_disable_log_dirty,\n\t.flush_log_dirty = vmx_flush_log_dirty,\n\t.enable_log_dirty_pt_masked = vmx_enable_log_dirty_pt_masked,\n\t.write_log_dirty = vmx_write_pml_buffer,\n\n\t.pre_block = vmx_pre_block,\n\t.post_block = vmx_post_block,\n\n\t.pmu_ops = &intel_pmu_ops,\n\n\t.update_pi_irte = vmx_update_pi_irte,\n\n#ifdef CONFIG_X86_64\n\t.set_hv_timer = vmx_set_hv_timer,\n\t.cancel_hv_timer = vmx_cancel_hv_timer,\n#endif\n\n\t.setup_mce = vmx_setup_mce,\n\n\t.smi_allowed = vmx_smi_allowed,\n\t.pre_enter_smm = vmx_pre_enter_smm,\n\t.pre_leave_smm = vmx_pre_leave_smm,\n\t.enable_smi_window = enable_smi_window,\n};\n\nstatic int __init vmx_init(void)\n{\n\tint r;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\t/*\n\t * Enlightened VMCS usage should be recommended and the host needs\n\t * to support eVMCS v1 or above. We can also disable eVMCS support\n\t * with module parameter.\n\t */\n\tif (enlightened_vmcs &&\n\t    ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&\n\t    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=\n\t    KVM_EVMCS_VERSION) {\n\t\tint cpu;\n\n\t\t/* Check that we have assist pages on all online CPUs */\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tif (!hv_get_vp_assist_page(cpu)) {\n\t\t\t\tenlightened_vmcs = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (enlightened_vmcs) {\n\t\t\tpr_info(\"KVM: vmx: using Hyper-V Enlightened VMCS\\n\");\n\t\t\tstatic_branch_enable(&enable_evmcs);\n\t\t}\n\t} else {\n\t\tenlightened_vmcs = false;\n\t}\n#endif\n\n\tr = kvm_init(&vmx_x86_ops, sizeof(struct vcpu_vmx),\n                     __alignof__(struct vcpu_vmx), THIS_MODULE);\n\tif (r)\n\t\treturn r;\n\n#ifdef CONFIG_KEXEC_CORE\n\trcu_assign_pointer(crash_vmclear_loaded_vmcss,\n\t\t\t   crash_vmclear_local_loaded_vmcss);\n#endif\n\tvmx_check_vmcs12_offsets();\n\n\treturn 0;\n}\n\nstatic void __exit vmx_exit(void)\n{\n#ifdef CONFIG_KEXEC_CORE\n\tRCU_INIT_POINTER(crash_vmclear_loaded_vmcss, NULL);\n\tsynchronize_rcu();\n#endif\n\n\tkvm_exit();\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tif (static_branch_unlikely(&enable_evmcs)) {\n\t\tint cpu;\n\t\tstruct hv_vp_assist_page *vp_ap;\n\t\t/*\n\t\t * Reset everything to support using non-enlightened VMCS\n\t\t * access later (e.g. when we reload the module with\n\t\t * enlightened_vmcs=0)\n\t\t */\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tvp_ap =\thv_get_vp_assist_page(cpu);\n\n\t\t\tif (!vp_ap)\n\t\t\t\tcontinue;\n\n\t\t\tvp_ap->current_nested_vmcs = 0;\n\t\t\tvp_ap->enlighten_vmentry = 0;\n\t\t}\n\n\t\tstatic_branch_disable(&enable_evmcs);\n\t}\n#endif\n}\n\nmodule_init(vmx_init)\nmodule_exit(vmx_exit)\n"], "filenames": ["arch/x86/kvm/vmx.c"], "buggy_code_start_loc": [7907], "buggy_code_end_loc": [8452], "fixing_code_start_loc": [7908], "fixing_code_end_loc": [8463], "type": "NVD-CWE-noinfo", "message": "In arch/x86/kvm/vmx.c in the Linux kernel before 4.17.2, when nested virtualization is used, local attackers could cause L1 KVM guests to VMEXIT, potentially allowing privilege escalations and denial of service attacks due to lack of checking of CPL.", "other": {"cve": {"id": "CVE-2018-12904", "sourceIdentifier": "cve@mitre.org", "published": "2018-06-27T11:29:00.237", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In arch/x86/kvm/vmx.c in the Linux kernel before 4.17.2, when nested virtualization is used, local attackers could cause L1 KVM guests to VMEXIT, potentially allowing privilege escalations and denial of service attacks due to lack of checking of CPL."}, {"lang": "es", "value": "En arch/x86/kvm/vmx.c en el kernel de Linux en versiones anteriores a la 4.17.2, cuando se emplea la virtualizaci\u00f3n anidada, los atacantes locales podr\u00edan hacer que los invitados L1 KVM realizasen un VMEXIT, permitiendo escalados de privilegios y ataques de denegaci\u00f3n de servicio (DoS) debido a la falta de comprobaci\u00f3n de CPL."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:N/UI:N/S:U/C:L/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.4, "impactScore": 3.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.17.2", "matchCriteriaId": "E8618AFC-8179-4431-9436-5CE1FBD9B32F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=727ba748e110b4de50d142edca9d6a9b7e6111d8", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1589", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.17.2", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/727ba748e110b4de50d142edca9d6a9b7e6111d8", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3752-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.exploit-db.com/exploits/44944/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/727ba748e110b4de50d142edca9d6a9b7e6111d8"}}