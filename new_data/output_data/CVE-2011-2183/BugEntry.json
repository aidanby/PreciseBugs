{"buggy_code": ["/*\n * Memory merging support.\n *\n * This code enables dynamic sharing of identical pages found in different\n * memory areas, even if they are not shared by fork()\n *\n * Copyright (C) 2008-2009 Red Hat, Inc.\n * Authors:\n *\tIzik Eidus\n *\tAndrea Arcangeli\n *\tChris Wright\n *\tHugh Dickins\n *\n * This work is licensed under the terms of the GNU GPL, version 2.\n */\n\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/mman.h>\n#include <linux/sched.h>\n#include <linux/rwsem.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/spinlock.h>\n#include <linux/jhash.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/rbtree.h>\n#include <linux/memory.h>\n#include <linux/mmu_notifier.h>\n#include <linux/swap.h>\n#include <linux/ksm.h>\n#include <linux/hash.h>\n#include <linux/freezer.h>\n#include <linux/oom.h>\n\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n/*\n * A few notes about the KSM scanning process,\n * to make it easier to understand the data structures below:\n *\n * In order to reduce excessive scanning, KSM sorts the memory pages by their\n * contents into a data structure that holds pointers to the pages' locations.\n *\n * Since the contents of the pages may change at any moment, KSM cannot just\n * insert the pages into a normal sorted tree and expect it to find anything.\n * Therefore KSM uses two data structures - the stable and the unstable tree.\n *\n * The stable tree holds pointers to all the merged pages (ksm pages), sorted\n * by their contents.  Because each such page is write-protected, searching on\n * this tree is fully assured to be working (except when pages are unmapped),\n * and therefore this tree is called the stable tree.\n *\n * In addition to the stable tree, KSM uses a second data structure called the\n * unstable tree: this tree holds pointers to pages which have been found to\n * be \"unchanged for a period of time\".  The unstable tree sorts these pages\n * by their contents, but since they are not write-protected, KSM cannot rely\n * upon the unstable tree to work correctly - the unstable tree is liable to\n * be corrupted as its contents are modified, and so it is called unstable.\n *\n * KSM solves this problem by several techniques:\n *\n * 1) The unstable tree is flushed every time KSM completes scanning all\n *    memory areas, and then the tree is rebuilt again from the beginning.\n * 2) KSM will only insert into the unstable tree, pages whose hash value\n *    has not changed since the previous scan of all memory areas.\n * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the\n *    colors of the nodes and not on their contents, assuring that even when\n *    the tree gets \"corrupted\" it won't get out of balance, so scanning time\n *    remains the same (also, searching and inserting nodes in an rbtree uses\n *    the same algorithm, so we have no overhead when we flush and rebuild).\n * 4) KSM never flushes the stable tree, which means that even if it were to\n *    take 10 attempts to find a page in the unstable tree, once it is found,\n *    it is secured in the stable tree.  (When we scan a new page, we first\n *    compare it against the stable tree, and then against the unstable tree.)\n */\n\n/**\n * struct mm_slot - ksm information per mm that is being scanned\n * @link: link to the mm_slots hash list\n * @mm_list: link into the mm_slots list, rooted in ksm_mm_head\n * @rmap_list: head for this mm_slot's singly-linked list of rmap_items\n * @mm: the mm that this information is valid for\n */\nstruct mm_slot {\n\tstruct hlist_node link;\n\tstruct list_head mm_list;\n\tstruct rmap_item *rmap_list;\n\tstruct mm_struct *mm;\n};\n\n/**\n * struct ksm_scan - cursor for scanning\n * @mm_slot: the current mm_slot we are scanning\n * @address: the next address inside that to be scanned\n * @rmap_list: link to the next rmap to be scanned in the rmap_list\n * @seqnr: count of completed full scans (needed when removing unstable node)\n *\n * There is only the one ksm_scan instance of this cursor structure.\n */\nstruct ksm_scan {\n\tstruct mm_slot *mm_slot;\n\tunsigned long address;\n\tstruct rmap_item **rmap_list;\n\tunsigned long seqnr;\n};\n\n/**\n * struct stable_node - node of the stable rbtree\n * @node: rb node of this ksm page in the stable tree\n * @hlist: hlist head of rmap_items using this ksm page\n * @kpfn: page frame number of this ksm page\n */\nstruct stable_node {\n\tstruct rb_node node;\n\tstruct hlist_head hlist;\n\tunsigned long kpfn;\n};\n\n/**\n * struct rmap_item - reverse mapping item for virtual addresses\n * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list\n * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree\n * @mm: the memory structure this rmap_item is pointing into\n * @address: the virtual address this rmap_item tracks (+ flags in low bits)\n * @oldchecksum: previous checksum of the page at that virtual address\n * @node: rb node of this rmap_item in the unstable tree\n * @head: pointer to stable_node heading this list in the stable tree\n * @hlist: link into hlist of rmap_items hanging off that stable_node\n */\nstruct rmap_item {\n\tstruct rmap_item *rmap_list;\n\tstruct anon_vma *anon_vma;\t/* when stable */\n\tstruct mm_struct *mm;\n\tunsigned long address;\t\t/* + low bits used for flags below */\n\tunsigned int oldchecksum;\t/* when unstable */\n\tunion {\n\t\tstruct rb_node node;\t/* when node of unstable tree */\n\t\tstruct {\t\t/* when listed from stable tree */\n\t\t\tstruct stable_node *head;\n\t\t\tstruct hlist_node hlist;\n\t\t};\n\t};\n};\n\n#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */\n#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */\n#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */\n\n/* The stable and unstable tree heads */\nstatic struct rb_root root_stable_tree = RB_ROOT;\nstatic struct rb_root root_unstable_tree = RB_ROOT;\n\n#define MM_SLOTS_HASH_SHIFT 10\n#define MM_SLOTS_HASH_HEADS (1 << MM_SLOTS_HASH_SHIFT)\nstatic struct hlist_head mm_slots_hash[MM_SLOTS_HASH_HEADS];\n\nstatic struct mm_slot ksm_mm_head = {\n\t.mm_list = LIST_HEAD_INIT(ksm_mm_head.mm_list),\n};\nstatic struct ksm_scan ksm_scan = {\n\t.mm_slot = &ksm_mm_head,\n};\n\nstatic struct kmem_cache *rmap_item_cache;\nstatic struct kmem_cache *stable_node_cache;\nstatic struct kmem_cache *mm_slot_cache;\n\n/* The number of nodes in the stable tree */\nstatic unsigned long ksm_pages_shared;\n\n/* The number of page slots additionally sharing those nodes */\nstatic unsigned long ksm_pages_sharing;\n\n/* The number of nodes in the unstable tree */\nstatic unsigned long ksm_pages_unshared;\n\n/* The number of rmap_items in use: to calculate pages_volatile */\nstatic unsigned long ksm_rmap_items;\n\n/* Number of pages ksmd should scan in one batch */\nstatic unsigned int ksm_thread_pages_to_scan = 100;\n\n/* Milliseconds ksmd should sleep between batches */\nstatic unsigned int ksm_thread_sleep_millisecs = 20;\n\n#define KSM_RUN_STOP\t0\n#define KSM_RUN_MERGE\t1\n#define KSM_RUN_UNMERGE\t2\nstatic unsigned int ksm_run = KSM_RUN_STOP;\n\nstatic DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);\nstatic DEFINE_MUTEX(ksm_thread_mutex);\nstatic DEFINE_SPINLOCK(ksm_mmlist_lock);\n\n#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(\"ksm_\"#__struct,\\\n\t\tsizeof(struct __struct), __alignof__(struct __struct),\\\n\t\t(__flags), NULL)\n\nstatic int __init ksm_slab_init(void)\n{\n\trmap_item_cache = KSM_KMEM_CACHE(rmap_item, 0);\n\tif (!rmap_item_cache)\n\t\tgoto out;\n\n\tstable_node_cache = KSM_KMEM_CACHE(stable_node, 0);\n\tif (!stable_node_cache)\n\t\tgoto out_free1;\n\n\tmm_slot_cache = KSM_KMEM_CACHE(mm_slot, 0);\n\tif (!mm_slot_cache)\n\t\tgoto out_free2;\n\n\treturn 0;\n\nout_free2:\n\tkmem_cache_destroy(stable_node_cache);\nout_free1:\n\tkmem_cache_destroy(rmap_item_cache);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void __init ksm_slab_free(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n\tkmem_cache_destroy(stable_node_cache);\n\tkmem_cache_destroy(rmap_item_cache);\n\tmm_slot_cache = NULL;\n}\n\nstatic inline struct rmap_item *alloc_rmap_item(void)\n{\n\tstruct rmap_item *rmap_item;\n\n\trmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL);\n\tif (rmap_item)\n\t\tksm_rmap_items++;\n\treturn rmap_item;\n}\n\nstatic inline void free_rmap_item(struct rmap_item *rmap_item)\n{\n\tksm_rmap_items--;\n\trmap_item->mm = NULL;\t/* debug safety */\n\tkmem_cache_free(rmap_item_cache, rmap_item);\n}\n\nstatic inline struct stable_node *alloc_stable_node(void)\n{\n\treturn kmem_cache_alloc(stable_node_cache, GFP_KERNEL);\n}\n\nstatic inline void free_stable_node(struct stable_node *stable_node)\n{\n\tkmem_cache_free(stable_node_cache, stable_node);\n}\n\nstatic inline struct mm_slot *alloc_mm_slot(void)\n{\n\tif (!mm_slot_cache)\t/* initialization failed */\n\t\treturn NULL;\n\treturn kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);\n}\n\nstatic inline void free_mm_slot(struct mm_slot *mm_slot)\n{\n\tkmem_cache_free(mm_slot_cache, mm_slot);\n}\n\nstatic struct mm_slot *get_mm_slot(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct hlist_head *bucket;\n\tstruct hlist_node *node;\n\n\tbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\n\thlist_for_each_entry(mm_slot, node, bucket, link) {\n\t\tif (mm == mm_slot->mm)\n\t\t\treturn mm_slot;\n\t}\n\treturn NULL;\n}\n\nstatic void insert_to_mm_slots_hash(struct mm_struct *mm,\n\t\t\t\t    struct mm_slot *mm_slot)\n{\n\tstruct hlist_head *bucket;\n\n\tbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\n\tmm_slot->mm = mm;\n\thlist_add_head(&mm_slot->link, bucket);\n}\n\nstatic inline int in_stable_tree(struct rmap_item *rmap_item)\n{\n\treturn rmap_item->address & STABLE_FLAG;\n}\n\n/*\n * ksmd, and unmerge_and_remove_all_rmap_items(), must not touch an mm's\n * page tables after it has passed through ksm_exit() - which, if necessary,\n * takes mmap_sem briefly to serialize against them.  ksm_exit() does not set\n * a special flag: they can just back out as soon as mm_users goes to zero.\n * ksm_test_exit() is used throughout to make this test for exit: in some\n * places for correctness, in some places just to avoid unnecessary work.\n */\nstatic inline bool ksm_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\n/*\n * We use break_ksm to break COW on a ksm page: it's a stripped down\n *\n *\tif (get_user_pages(current, mm, addr, 1, 1, 1, &page, NULL) == 1)\n *\t\tput_page(page);\n *\n * but taking great care only to touch a ksm page, in a VM_MERGEABLE vma,\n * in case the application has unmapped and remapped mm,addr meanwhile.\n * Could a ksm page appear anywhere else?  Actually yes, in a VM_PFNMAP\n * mmap of /dev/mem or /dev/kmem, where we would not want to touch it.\n */\nstatic int break_ksm(struct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\tint ret = 0;\n\n\tdo {\n\t\tcond_resched();\n\t\tpage = follow_page(vma, addr, FOLL_GET);\n\t\tif (IS_ERR_OR_NULL(page))\n\t\t\tbreak;\n\t\tif (PageKsm(page))\n\t\t\tret = handle_mm_fault(vma->vm_mm, vma, addr,\n\t\t\t\t\t\t\tFAULT_FLAG_WRITE);\n\t\telse\n\t\t\tret = VM_FAULT_WRITE;\n\t\tput_page(page);\n\t} while (!(ret & (VM_FAULT_WRITE | VM_FAULT_SIGBUS | VM_FAULT_OOM)));\n\t/*\n\t * We must loop because handle_mm_fault() may back out if there's\n\t * any difficulty e.g. if pte accessed bit gets updated concurrently.\n\t *\n\t * VM_FAULT_WRITE is what we have been hoping for: it indicates that\n\t * COW has been broken, even if the vma does not permit VM_WRITE;\n\t * but note that a concurrent fault might break PageKsm for us.\n\t *\n\t * VM_FAULT_SIGBUS could occur if we race with truncation of the\n\t * backing file, which also invalidates anonymous pages: that's\n\t * okay, that truncation will have unmapped the PageKsm for us.\n\t *\n\t * VM_FAULT_OOM: at the time of writing (late July 2009), setting\n\t * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the\n\t * current task has TIF_MEMDIE set, and will be OOM killed on return\n\t * to user; and ksmd, having no mm, would never be chosen for that.\n\t *\n\t * But if the mm is in a limited mem_cgroup, then the fault may fail\n\t * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and\n\t * even ksmd can fail in this way - though it's usually breaking ksm\n\t * just to undo a merge it made a moment before, so unlikely to oom.\n\t *\n\t * That's a pity: we might therefore have more kernel pages allocated\n\t * than we're counting as nodes in the stable tree; but ksm_do_scan\n\t * will retry to break_cow on each pass, so should recover the page\n\t * in due course.  The important thing is to not let VM_MERGEABLE\n\t * be cleared while any such pages might remain in the area.\n\t */\n\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;\n}\n\nstatic void break_cow(struct rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\n\t/*\n\t * It is not an accident that whenever we want to break COW\n\t * to undo, we also need to drop a reference to the anon_vma.\n\t */\n\tput_anon_vma(rmap_item->anon_vma);\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, addr);\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto out;\n\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\tgoto out;\n\tbreak_ksm(vma, addr);\nout:\n\tup_read(&mm->mmap_sem);\n}\n\nstatic struct page *page_trans_compound_anon(struct page *page)\n{\n\tif (PageTransCompound(page)) {\n\t\tstruct page *head = compound_trans_head(page);\n\t\t/*\n\t\t * head may actually be splitted and freed from under\n\t\t * us but it's ok here.\n\t\t */\n\t\tif (PageAnon(head))\n\t\t\treturn head;\n\t}\n\treturn NULL;\n}\n\nstatic struct page *get_mergeable_page(struct rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\tstruct page *page;\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, addr);\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto out;\n\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\tgoto out;\n\n\tpage = follow_page(vma, addr, FOLL_GET);\n\tif (IS_ERR_OR_NULL(page))\n\t\tgoto out;\n\tif (PageAnon(page) || page_trans_compound_anon(page)) {\n\t\tflush_anon_page(vma, page, addr);\n\t\tflush_dcache_page(page);\n\t} else {\n\t\tput_page(page);\nout:\t\tpage = NULL;\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn page;\n}\n\nstatic void remove_node_from_stable_tree(struct stable_node *stable_node)\n{\n\tstruct rmap_item *rmap_item;\n\tstruct hlist_node *hlist;\n\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tif (rmap_item->hlist.next)\n\t\t\tksm_pages_sharing--;\n\t\telse\n\t\t\tksm_pages_shared--;\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->address &= PAGE_MASK;\n\t\tcond_resched();\n\t}\n\n\trb_erase(&stable_node->node, &root_stable_tree);\n\tfree_stable_node(stable_node);\n}\n\n/*\n * get_ksm_page: checks if the page indicated by the stable node\n * is still its ksm page, despite having held no reference to it.\n * In which case we can trust the content of the page, and it\n * returns the gotten page; but if the page has now been zapped,\n * remove the stale node from the stable tree and return NULL.\n *\n * You would expect the stable_node to hold a reference to the ksm page.\n * But if it increments the page's count, swapping out has to wait for\n * ksmd to come around again before it can free the page, which may take\n * seconds or even minutes: much too unresponsive.  So instead we use a\n * \"keyhole reference\": access to the ksm page from the stable node peeps\n * out through its keyhole to see if that page still holds the right key,\n * pointing back to this stable node.  This relies on freeing a PageAnon\n * page to reset its page->mapping to NULL, and relies on no other use of\n * a page to put something that might look like our key in page->mapping.\n *\n * include/linux/pagemap.h page_cache_get_speculative() is a good reference,\n * but this is different - made simpler by ksm_thread_mutex being held, but\n * interesting for assuming that no other use of the struct page could ever\n * put our expected_mapping into page->mapping (or a field of the union which\n * coincides with page->mapping).  The RCU calls are not for KSM at all, but\n * to keep the page_count protocol described with page_cache_get_speculative.\n *\n * Note: it is possible that get_ksm_page() will return NULL one moment,\n * then page the next, if the page is in between page_freeze_refs() and\n * page_unfreeze_refs(): this shouldn't be a problem anywhere, the page\n * is on its way to being freed; but it is an anomaly to bear in mind.\n */\nstatic struct page *get_ksm_page(struct stable_node *stable_node)\n{\n\tstruct page *page;\n\tvoid *expected_mapping;\n\n\tpage = pfn_to_page(stable_node->kpfn);\n\texpected_mapping = (void *)stable_node +\n\t\t\t\t(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);\n\trcu_read_lock();\n\tif (page->mapping != expected_mapping)\n\t\tgoto stale;\n\tif (!get_page_unless_zero(page))\n\t\tgoto stale;\n\tif (page->mapping != expected_mapping) {\n\t\tput_page(page);\n\t\tgoto stale;\n\t}\n\trcu_read_unlock();\n\treturn page;\nstale:\n\trcu_read_unlock();\n\tremove_node_from_stable_tree(stable_node);\n\treturn NULL;\n}\n\n/*\n * Removing rmap_item from stable or unstable tree.\n * This function will clean the information from the stable/unstable tree.\n */\nstatic void remove_rmap_item_from_tree(struct rmap_item *rmap_item)\n{\n\tif (rmap_item->address & STABLE_FLAG) {\n\t\tstruct stable_node *stable_node;\n\t\tstruct page *page;\n\n\t\tstable_node = rmap_item->head;\n\t\tpage = get_ksm_page(stable_node);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tlock_page(page);\n\t\thlist_del(&rmap_item->hlist);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (stable_node->hlist.first)\n\t\t\tksm_pages_sharing--;\n\t\telse\n\t\t\tksm_pages_shared--;\n\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->address &= PAGE_MASK;\n\n\t} else if (rmap_item->address & UNSTABLE_FLAG) {\n\t\tunsigned char age;\n\t\t/*\n\t\t * Usually ksmd can and must skip the rb_erase, because\n\t\t * root_unstable_tree was already reset to RB_ROOT.\n\t\t * But be careful when an mm is exiting: do the rb_erase\n\t\t * if this rmap_item was inserted by this scan, rather\n\t\t * than left over from before.\n\t\t */\n\t\tage = (unsigned char)(ksm_scan.seqnr - rmap_item->address);\n\t\tBUG_ON(age > 1);\n\t\tif (!age)\n\t\t\trb_erase(&rmap_item->node, &root_unstable_tree);\n\n\t\tksm_pages_unshared--;\n\t\trmap_item->address &= PAGE_MASK;\n\t}\nout:\n\tcond_resched();\t\t/* we're called from many long loops */\n}\n\nstatic void remove_trailing_rmap_items(struct mm_slot *mm_slot,\n\t\t\t\t       struct rmap_item **rmap_list)\n{\n\twhile (*rmap_list) {\n\t\tstruct rmap_item *rmap_item = *rmap_list;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n}\n\n/*\n * Though it's very tempting to unmerge in_stable_tree(rmap_item)s rather\n * than check every pte of a given vma, the locking doesn't quite work for\n * that - an rmap_item is assigned to the stable tree after inserting ksm\n * page and upping mmap_sem.  Nor does it fit with the way we skip dup'ing\n * rmap_items from parent to child at fork time (so as not to waste time\n * if exit comes before the next scan reaches it).\n *\n * Similarly, although we'd like to remove rmap_items (so updating counts\n * and freeing memory) when unmerging an area, it's easier to leave that\n * to the next pass of ksmd - consider, for example, how ksmd might be\n * in cmp_and_merge_page on one of the rmap_items we would be removing.\n */\nstatic int unmerge_ksm_pages(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tunsigned long addr;\n\tint err = 0;\n\n\tfor (addr = start; addr < end && !err; addr += PAGE_SIZE) {\n\t\tif (ksm_test_exit(vma->vm_mm))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\terr = -ERESTARTSYS;\n\t\telse\n\t\t\terr = break_ksm(vma, addr);\n\t}\n\treturn err;\n}\n\n#ifdef CONFIG_SYSFS\n/*\n * Only called through the sysfs control interface:\n */\nstatic int unmerge_and_remove_all_rmap_items(void)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint err = 0;\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tfor (mm_slot = ksm_scan.mm_slot;\n\t\t\tmm_slot != &ksm_mm_head; mm_slot = ksm_scan.mm_slot) {\n\t\tmm = mm_slot->mm;\n\t\tdown_read(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\t\t\tcontinue;\n\t\t\terr = unmerge_ksm_pages(vma,\n\t\t\t\t\t\tvma->vm_start, vma->vm_end);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tremove_trailing_rmap_items(mm_slot, &mm_slot->rmap_list);\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tksm_scan.mm_slot = list_entry(mm_slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\t\tif (ksm_test_exit(mm)) {\n\t\t\thlist_del(&mm_slot->link);\n\t\t\tlist_del(&mm_slot->mm_list);\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\t\tfree_mm_slot(mm_slot);\n\t\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\tmmdrop(mm);\n\t\t} else {\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\t\t\tup_read(&mm->mmap_sem);\n\t\t}\n\t}\n\n\tksm_scan.seqnr = 0;\n\treturn 0;\n\nerror:\n\tup_read(&mm->mmap_sem);\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = &ksm_mm_head;\n\tspin_unlock(&ksm_mmlist_lock);\n\treturn err;\n}\n#endif /* CONFIG_SYSFS */\n\nstatic u32 calc_checksum(struct page *page)\n{\n\tu32 checksum;\n\tvoid *addr = kmap_atomic(page, KM_USER0);\n\tchecksum = jhash2(addr, PAGE_SIZE / 4, 17);\n\tkunmap_atomic(addr, KM_USER0);\n\treturn checksum;\n}\n\nstatic int memcmp_pages(struct page *page1, struct page *page2)\n{\n\tchar *addr1, *addr2;\n\tint ret;\n\n\taddr1 = kmap_atomic(page1, KM_USER0);\n\taddr2 = kmap_atomic(page2, KM_USER1);\n\tret = memcmp(addr1, addr2, PAGE_SIZE);\n\tkunmap_atomic(addr2, KM_USER1);\n\tkunmap_atomic(addr1, KM_USER0);\n\treturn ret;\n}\n\nstatic inline int pages_identical(struct page *page1, struct page *page2)\n{\n\treturn !memcmp_pages(page1, page2);\n}\n\nstatic int write_protect_page(struct vm_area_struct *vma, struct page *page,\n\t\t\t      pte_t *orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long addr;\n\tpte_t *ptep;\n\tspinlock_t *ptl;\n\tint swapped;\n\tint err = -EFAULT;\n\n\taddr = page_address_in_vma(page, vma);\n\tif (addr == -EFAULT)\n\t\tgoto out;\n\n\tBUG_ON(PageTransCompound(page));\n\tptep = page_check_address(page, mm, addr, &ptl, 0);\n\tif (!ptep)\n\t\tgoto out;\n\n\tif (pte_write(*ptep) || pte_dirty(*ptep)) {\n\t\tpte_t entry;\n\n\t\tswapped = PageSwapCache(page);\n\t\tflush_cache_page(vma, addr, page_to_pfn(page));\n\t\t/*\n\t\t * Ok this is tricky, when get_user_pages_fast() run it doesn't\n\t\t * take any lock, therefore the check that we are going to make\n\t\t * with the pagecount against the mapcount is racey and\n\t\t * O_DIRECT can happen right after the check.\n\t\t * So we clear the pte and flush the tlb before the check\n\t\t * this assure us that no O_DIRECT can happen after the check\n\t\t * or in the middle of the check.\n\t\t */\n\t\tentry = ptep_clear_flush(vma, addr, ptep);\n\t\t/*\n\t\t * Check that no O_DIRECT or similar I/O is in progress on the\n\t\t * page\n\t\t */\n\t\tif (page_mapcount(page) + 1 + swapped != page_count(page)) {\n\t\t\tset_pte_at(mm, addr, ptep, entry);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (pte_dirty(entry))\n\t\t\tset_page_dirty(page);\n\t\tentry = pte_mkclean(pte_wrprotect(entry));\n\t\tset_pte_at_notify(mm, addr, ptep, entry);\n\t}\n\t*orig_pte = *ptep;\n\terr = 0;\n\nout_unlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn err;\n}\n\n/**\n * replace_page - replace page in vma by new ksm page\n * @vma:      vma that holds the pte pointing to page\n * @page:     the page we are replacing by kpage\n * @kpage:    the ksm page we replace page by\n * @orig_pte: the original value of the pte\n *\n * Returns 0 on success, -EFAULT on failure.\n */\nstatic int replace_page(struct vm_area_struct *vma, struct page *page,\n\t\t\tstruct page *kpage, pte_t orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *ptep;\n\tspinlock_t *ptl;\n\tunsigned long addr;\n\tint err = -EFAULT;\n\n\taddr = page_address_in_vma(page, vma);\n\tif (addr == -EFAULT)\n\t\tgoto out;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, addr);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, addr);\n\tBUG_ON(pmd_trans_huge(*pmd));\n\tif (!pmd_present(*pmd))\n\t\tgoto out;\n\n\tptep = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tif (!pte_same(*ptep, orig_pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tgoto out;\n\t}\n\n\tget_page(kpage);\n\tpage_add_anon_rmap(kpage, vma, addr);\n\n\tflush_cache_page(vma, addr, pte_pfn(*ptep));\n\tptep_clear_flush(vma, addr, ptep);\n\tset_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma->vm_page_prot));\n\n\tpage_remove_rmap(page);\n\tif (!page_mapped(page))\n\t\ttry_to_free_swap(page);\n\tput_page(page);\n\n\tpte_unmap_unlock(ptep, ptl);\n\terr = 0;\nout:\n\treturn err;\n}\n\nstatic int page_trans_compound_anon_split(struct page *page)\n{\n\tint ret = 0;\n\tstruct page *transhuge_head = page_trans_compound_anon(page);\n\tif (transhuge_head) {\n\t\t/* Get the reference on the head to split it. */\n\t\tif (get_page_unless_zero(transhuge_head)) {\n\t\t\t/*\n\t\t\t * Recheck we got the reference while the head\n\t\t\t * was still anonymous.\n\t\t\t */\n\t\t\tif (PageAnon(transhuge_head))\n\t\t\t\tret = split_huge_page(transhuge_head);\n\t\t\telse\n\t\t\t\t/*\n\t\t\t\t * Retry later if split_huge_page run\n\t\t\t\t * from under us.\n\t\t\t\t */\n\t\t\t\tret = 1;\n\t\t\tput_page(transhuge_head);\n\t\t} else\n\t\t\t/* Retry later if split_huge_page run from under us. */\n\t\t\tret = 1;\n\t}\n\treturn ret;\n}\n\n/*\n * try_to_merge_one_page - take two pages and merge them into one\n * @vma: the vma that holds the pte pointing to page\n * @page: the PageAnon page that we want to replace with kpage\n * @kpage: the PageKsm page that we want to map instead of page,\n *         or NULL the first time when we want to use page as kpage.\n *\n * This function returns 0 if the pages were merged, -EFAULT otherwise.\n */\nstatic int try_to_merge_one_page(struct vm_area_struct *vma,\n\t\t\t\t struct page *page, struct page *kpage)\n{\n\tpte_t orig_pte = __pte(0);\n\tint err = -EFAULT;\n\n\tif (page == kpage)\t\t\t/* ksm page forked */\n\t\treturn 0;\n\n\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\tgoto out;\n\tif (PageTransCompound(page) && page_trans_compound_anon_split(page))\n\t\tgoto out;\n\tBUG_ON(PageTransCompound(page));\n\tif (!PageAnon(page))\n\t\tgoto out;\n\n\t/*\n\t * We need the page lock to read a stable PageSwapCache in\n\t * write_protect_page().  We use trylock_page() instead of\n\t * lock_page() because we don't want to wait here - we\n\t * prefer to continue scanning and merging different pages,\n\t * then come back to this page when it is unlocked.\n\t */\n\tif (!trylock_page(page))\n\t\tgoto out;\n\t/*\n\t * If this anonymous page is mapped only here, its pte may need\n\t * to be write-protected.  If it's mapped elsewhere, all of its\n\t * ptes are necessarily already write-protected.  But in either\n\t * case, we need to lock and check page_count is not raised.\n\t */\n\tif (write_protect_page(vma, page, &orig_pte) == 0) {\n\t\tif (!kpage) {\n\t\t\t/*\n\t\t\t * While we hold page lock, upgrade page from\n\t\t\t * PageAnon+anon_vma to PageKsm+NULL stable_node:\n\t\t\t * stable_tree_insert() will update stable_node.\n\t\t\t */\n\t\t\tset_page_stable_node(page, NULL);\n\t\t\tmark_page_accessed(page);\n\t\t\terr = 0;\n\t\t} else if (pages_identical(page, kpage))\n\t\t\terr = replace_page(vma, page, kpage, orig_pte);\n\t}\n\n\tif ((vma->vm_flags & VM_LOCKED) && kpage && !err) {\n\t\tmunlock_vma_page(page);\n\t\tif (!PageMlocked(kpage)) {\n\t\t\tunlock_page(page);\n\t\t\tlock_page(kpage);\n\t\t\tmlock_vma_page(kpage);\n\t\t\tpage = kpage;\t\t/* for final unlock */\n\t\t}\n\t}\n\n\tunlock_page(page);\nout:\n\treturn err;\n}\n\n/*\n * try_to_merge_with_ksm_page - like try_to_merge_two_pages,\n * but no new kernel page is allocated: kpage must already be a ksm page.\n *\n * This function returns 0 if the pages were merged, -EFAULT otherwise.\n */\nstatic int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,\n\t\t\t\t      struct page *page, struct page *kpage)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tstruct vm_area_struct *vma;\n\tint err = -EFAULT;\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, rmap_item->address);\n\tif (!vma || vma->vm_start > rmap_item->address)\n\t\tgoto out;\n\n\terr = try_to_merge_one_page(vma, page, kpage);\n\tif (err)\n\t\tgoto out;\n\n\t/* Must get reference to anon_vma while still holding mmap_sem */\n\trmap_item->anon_vma = vma->anon_vma;\n\tget_anon_vma(vma->anon_vma);\nout:\n\tup_read(&mm->mmap_sem);\n\treturn err;\n}\n\n/*\n * try_to_merge_two_pages - take two identical pages and prepare them\n * to be merged into one page.\n *\n * This function returns the kpage if we successfully merged two identical\n * pages into one ksm page, NULL otherwise.\n *\n * Note that this function upgrades page to ksm page: if one of the pages\n * is already a ksm page, try_to_merge_with_ksm_page should be used.\n */\nstatic struct page *try_to_merge_two_pages(struct rmap_item *rmap_item,\n\t\t\t\t\t   struct page *page,\n\t\t\t\t\t   struct rmap_item *tree_rmap_item,\n\t\t\t\t\t   struct page *tree_page)\n{\n\tint err;\n\n\terr = try_to_merge_with_ksm_page(rmap_item, page, NULL);\n\tif (!err) {\n\t\terr = try_to_merge_with_ksm_page(tree_rmap_item,\n\t\t\t\t\t\t\ttree_page, page);\n\t\t/*\n\t\t * If that fails, we have a ksm page with only one pte\n\t\t * pointing to it: so break it.\n\t\t */\n\t\tif (err)\n\t\t\tbreak_cow(rmap_item);\n\t}\n\treturn err ? NULL : page;\n}\n\n/*\n * stable_tree_search - search for page inside the stable tree\n *\n * This function checks if there is a page inside the stable tree\n * with identical content to the page that we are scanning right now.\n *\n * This function returns the stable tree node of identical content if found,\n * NULL otherwise.\n */\nstatic struct page *stable_tree_search(struct page *page)\n{\n\tstruct rb_node *node = root_stable_tree.rb_node;\n\tstruct stable_node *stable_node;\n\n\tstable_node = page_stable_node(page);\n\tif (stable_node) {\t\t\t/* ksm page forked */\n\t\tget_page(page);\n\t\treturn page;\n\t}\n\n\twhile (node) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(node, struct stable_node, node);\n\t\ttree_page = get_ksm_page(stable_node);\n\t\tif (!tree_page)\n\t\t\treturn NULL;\n\n\t\tret = memcmp_pages(page, tree_page);\n\n\t\tif (ret < 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnode = node->rb_left;\n\t\t} else if (ret > 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnode = node->rb_right;\n\t\t} else\n\t\t\treturn tree_page;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * stable_tree_insert - insert rmap_item pointing to new ksm page\n * into the stable tree.\n *\n * This function returns the stable tree node just allocated on success,\n * NULL otherwise.\n */\nstatic struct stable_node *stable_tree_insert(struct page *kpage)\n{\n\tstruct rb_node **new = &root_stable_tree.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct stable_node *stable_node;\n\n\twhile (*new) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(*new, struct stable_node, node);\n\t\ttree_page = get_ksm_page(stable_node);\n\t\tif (!tree_page)\n\t\t\treturn NULL;\n\n\t\tret = memcmp_pages(kpage, tree_page);\n\t\tput_page(tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0)\n\t\t\tnew = &parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tnew = &parent->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * It is not a bug that stable_tree_search() didn't\n\t\t\t * find this node: because at that time our page was\n\t\t\t * not yet write-protected, so may have changed since.\n\t\t\t */\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tstable_node = alloc_stable_node();\n\tif (!stable_node)\n\t\treturn NULL;\n\n\trb_link_node(&stable_node->node, parent, new);\n\trb_insert_color(&stable_node->node, &root_stable_tree);\n\n\tINIT_HLIST_HEAD(&stable_node->hlist);\n\n\tstable_node->kpfn = page_to_pfn(kpage);\n\tset_page_stable_node(kpage, stable_node);\n\n\treturn stable_node;\n}\n\n/*\n * unstable_tree_search_insert - search for identical page,\n * else insert rmap_item into the unstable tree.\n *\n * This function searches for a page in the unstable tree identical to the\n * page currently being scanned; and if no identical page is found in the\n * tree, we insert rmap_item as a new object into the unstable tree.\n *\n * This function returns pointer to rmap_item found to be identical\n * to the currently scanned page, NULL otherwise.\n *\n * This function does both searching and inserting, because they share\n * the same walking algorithm in an rbtree.\n */\nstatic\nstruct rmap_item *unstable_tree_search_insert(struct rmap_item *rmap_item,\n\t\t\t\t\t      struct page *page,\n\t\t\t\t\t      struct page **tree_pagep)\n\n{\n\tstruct rb_node **new = &root_unstable_tree.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*new) {\n\t\tstruct rmap_item *tree_rmap_item;\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\ttree_rmap_item = rb_entry(*new, struct rmap_item, node);\n\t\ttree_page = get_mergeable_page(tree_rmap_item);\n\t\tif (IS_ERR_OR_NULL(tree_page))\n\t\t\treturn NULL;\n\n\t\t/*\n\t\t * Don't substitute a ksm page for a forked page.\n\t\t */\n\t\tif (page == tree_page) {\n\t\t\tput_page(tree_page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tret = memcmp_pages(page, tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_left;\n\t\t} else if (ret > 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_right;\n\t\t} else {\n\t\t\t*tree_pagep = tree_page;\n\t\t\treturn tree_rmap_item;\n\t\t}\n\t}\n\n\trmap_item->address |= UNSTABLE_FLAG;\n\trmap_item->address |= (ksm_scan.seqnr & SEQNR_MASK);\n\trb_link_node(&rmap_item->node, parent, new);\n\trb_insert_color(&rmap_item->node, &root_unstable_tree);\n\n\tksm_pages_unshared++;\n\treturn NULL;\n}\n\n/*\n * stable_tree_append - add another rmap_item to the linked list of\n * rmap_items hanging off a given node of the stable tree, all sharing\n * the same ksm page.\n */\nstatic void stable_tree_append(struct rmap_item *rmap_item,\n\t\t\t       struct stable_node *stable_node)\n{\n\trmap_item->head = stable_node;\n\trmap_item->address |= STABLE_FLAG;\n\thlist_add_head(&rmap_item->hlist, &stable_node->hlist);\n\n\tif (rmap_item->hlist.next)\n\t\tksm_pages_sharing++;\n\telse\n\t\tksm_pages_shared++;\n}\n\n/*\n * cmp_and_merge_page - first see if page can be merged into the stable tree;\n * if not, compare checksum to previous and if it's the same, see if page can\n * be inserted into the unstable tree, or merged with a page already there and\n * both transferred to the stable tree.\n *\n * @page: the page that we are searching identical page to.\n * @rmap_item: the reverse mapping into the virtual address of this page\n */\nstatic void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)\n{\n\tstruct rmap_item *tree_rmap_item;\n\tstruct page *tree_page = NULL;\n\tstruct stable_node *stable_node;\n\tstruct page *kpage;\n\tunsigned int checksum;\n\tint err;\n\n\tremove_rmap_item_from_tree(rmap_item);\n\n\t/* We first start with searching the page inside the stable tree */\n\tkpage = stable_tree_search(page);\n\tif (kpage) {\n\t\terr = try_to_merge_with_ksm_page(rmap_item, page, kpage);\n\t\tif (!err) {\n\t\t\t/*\n\t\t\t * The page was successfully merged:\n\t\t\t * add its rmap_item to the stable tree.\n\t\t\t */\n\t\t\tlock_page(kpage);\n\t\t\tstable_tree_append(rmap_item, page_stable_node(kpage));\n\t\t\tunlock_page(kpage);\n\t\t}\n\t\tput_page(kpage);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the hash value of the page has changed from the last time\n\t * we calculated it, this page is changing frequently: therefore we\n\t * don't want to insert it in the unstable tree, and we don't want\n\t * to waste our time searching for something identical to it there.\n\t */\n\tchecksum = calc_checksum(page);\n\tif (rmap_item->oldchecksum != checksum) {\n\t\trmap_item->oldchecksum = checksum;\n\t\treturn;\n\t}\n\n\ttree_rmap_item =\n\t\tunstable_tree_search_insert(rmap_item, page, &tree_page);\n\tif (tree_rmap_item) {\n\t\tkpage = try_to_merge_two_pages(rmap_item, page,\n\t\t\t\t\t\ttree_rmap_item, tree_page);\n\t\tput_page(tree_page);\n\t\t/*\n\t\t * As soon as we merge this page, we want to remove the\n\t\t * rmap_item of the page we have merged with from the unstable\n\t\t * tree, and insert it instead as new node in the stable tree.\n\t\t */\n\t\tif (kpage) {\n\t\t\tremove_rmap_item_from_tree(tree_rmap_item);\n\n\t\t\tlock_page(kpage);\n\t\t\tstable_node = stable_tree_insert(kpage);\n\t\t\tif (stable_node) {\n\t\t\t\tstable_tree_append(tree_rmap_item, stable_node);\n\t\t\t\tstable_tree_append(rmap_item, stable_node);\n\t\t\t}\n\t\t\tunlock_page(kpage);\n\n\t\t\t/*\n\t\t\t * If we fail to insert the page into the stable tree,\n\t\t\t * we will have 2 virtual addresses that are pointing\n\t\t\t * to a ksm page left outside the stable tree,\n\t\t\t * in which case we need to break_cow on both.\n\t\t\t */\n\t\t\tif (!stable_node) {\n\t\t\t\tbreak_cow(tree_rmap_item);\n\t\t\t\tbreak_cow(rmap_item);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct rmap_item *get_next_rmap_item(struct mm_slot *mm_slot,\n\t\t\t\t\t    struct rmap_item **rmap_list,\n\t\t\t\t\t    unsigned long addr)\n{\n\tstruct rmap_item *rmap_item;\n\n\twhile (*rmap_list) {\n\t\trmap_item = *rmap_list;\n\t\tif ((rmap_item->address & PAGE_MASK) == addr)\n\t\t\treturn rmap_item;\n\t\tif (rmap_item->address > addr)\n\t\t\tbreak;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n\n\trmap_item = alloc_rmap_item();\n\tif (rmap_item) {\n\t\t/* It has already been zeroed */\n\t\trmap_item->mm = mm_slot->mm;\n\t\trmap_item->address = addr;\n\t\trmap_item->rmap_list = *rmap_list;\n\t\t*rmap_list = rmap_item;\n\t}\n\treturn rmap_item;\n}\n\nstatic struct rmap_item *scan_get_next_rmap_item(struct page **page)\n{\n\tstruct mm_struct *mm;\n\tstruct mm_slot *slot;\n\tstruct vm_area_struct *vma;\n\tstruct rmap_item *rmap_item;\n\n\tif (list_empty(&ksm_mm_head.mm_list))\n\t\treturn NULL;\n\n\tslot = ksm_scan.mm_slot;\n\tif (slot == &ksm_mm_head) {\n\t\t/*\n\t\t * A number of pages can hang around indefinitely on per-cpu\n\t\t * pagevecs, raised page count preventing write_protect_page\n\t\t * from merging them.  Though it doesn't really matter much,\n\t\t * it is puzzling to see some stuck in pages_volatile until\n\t\t * other activity jostles them out, and they also prevented\n\t\t * LTP's KSM test from succeeding deterministically; so drain\n\t\t * them here (here rather than on entry to ksm_do_scan(),\n\t\t * so we don't IPI too often when pages_to_scan is set low).\n\t\t */\n\t\tlru_add_drain_all();\n\n\t\troot_unstable_tree = RB_ROOT;\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n\t\tksm_scan.mm_slot = slot;\n\t\tspin_unlock(&ksm_mmlist_lock);\nnext_mm:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\n\tmm = slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, ksm_scan.address);\n\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\t\tcontinue;\n\t\tif (ksm_scan.address < vma->vm_start)\n\t\t\tksm_scan.address = vma->vm_start;\n\t\tif (!vma->anon_vma)\n\t\t\tksm_scan.address = vma->vm_end;\n\n\t\twhile (ksm_scan.address < vma->vm_end) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\t*page = follow_page(vma, ksm_scan.address, FOLL_GET);\n\t\t\tif (IS_ERR_OR_NULL(*page)) {\n\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (PageAnon(*page) ||\n\t\t\t    page_trans_compound_anon(*page)) {\n\t\t\t\tflush_anon_page(vma, *page, ksm_scan.address);\n\t\t\t\tflush_dcache_page(*page);\n\t\t\t\trmap_item = get_next_rmap_item(slot,\n\t\t\t\t\tksm_scan.rmap_list, ksm_scan.address);\n\t\t\t\tif (rmap_item) {\n\t\t\t\t\tksm_scan.rmap_list =\n\t\t\t\t\t\t\t&rmap_item->rmap_list;\n\t\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\t} else\n\t\t\t\t\tput_page(*page);\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\treturn rmap_item;\n\t\t\t}\n\t\t\tput_page(*page);\n\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif (ksm_test_exit(mm)) {\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\t/*\n\t * Nuke all the rmap_items that are above this current rmap:\n\t * because there were no VM_MERGEABLE vmas with such addresses.\n\t */\n\tremove_trailing_rmap_items(slot, ksm_scan.rmap_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tif (ksm_scan.address == 0) {\n\t\t/*\n\t\t * We've completed a full scan of all vmas, holding mmap_sem\n\t\t * throughout, and found no VM_MERGEABLE: so do the same as\n\t\t * __ksm_exit does to remove this mm from all our lists now.\n\t\t * This applies either when cleaning up after __ksm_exit\n\t\t * (but beware: we can reach here even before __ksm_exit),\n\t\t * or when all VM_MERGEABLE areas have been unmapped (and\n\t\t * mmap_sem then protects against race with MADV_MERGEABLE).\n\t\t */\n\t\thlist_del(&slot->link);\n\t\tlist_del(&slot->mm_list);\n\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\tfree_mm_slot(slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmdrop(mm);\n\t} else {\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\tup_read(&mm->mmap_sem);\n\t}\n\n\t/* Repeat until we've completed scanning the whole list */\n\tslot = ksm_scan.mm_slot;\n\tif (slot != &ksm_mm_head)\n\t\tgoto next_mm;\n\n\tksm_scan.seqnr++;\n\treturn NULL;\n}\n\n/**\n * ksm_do_scan  - the ksm scanner main worker function.\n * @scan_npages - number of pages we want to scan before we return.\n */\nstatic void ksm_do_scan(unsigned int scan_npages)\n{\n\tstruct rmap_item *rmap_item;\n\tstruct page *uninitialized_var(page);\n\n\twhile (scan_npages-- && likely(!freezing(current))) {\n\t\tcond_resched();\n\t\trmap_item = scan_get_next_rmap_item(&page);\n\t\tif (!rmap_item)\n\t\t\treturn;\n\t\tif (!PageKsm(page) || !in_stable_tree(rmap_item))\n\t\t\tcmp_and_merge_page(page, rmap_item);\n\t\tput_page(page);\n\t}\n}\n\nstatic int ksmd_should_run(void)\n{\n\treturn (ksm_run & KSM_RUN_MERGE) && !list_empty(&ksm_mm_head.mm_list);\n}\n\nstatic int ksm_scan_thread(void *nothing)\n{\n\tset_freezable();\n\tset_user_nice(current, 5);\n\n\twhile (!kthread_should_stop()) {\n\t\tmutex_lock(&ksm_thread_mutex);\n\t\tif (ksmd_should_run())\n\t\t\tksm_do_scan(ksm_thread_pages_to_scan);\n\t\tmutex_unlock(&ksm_thread_mutex);\n\n\t\ttry_to_freeze();\n\n\t\tif (ksmd_should_run()) {\n\t\t\tschedule_timeout_interruptible(\n\t\t\t\tmsecs_to_jiffies(ksm_thread_sleep_millisecs));\n\t\t} else {\n\t\t\twait_event_freezable(ksm_thread_wait,\n\t\t\t\tksmd_should_run() || kthread_should_stop());\n\t\t}\n\t}\n\treturn 0;\n}\n\nint ksm_madvise(struct vm_area_struct *vma, unsigned long start,\n\t\tunsigned long end, int advice, unsigned long *vm_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint err;\n\n\tswitch (advice) {\n\tcase MADV_MERGEABLE:\n\t\t/*\n\t\t * Be somewhat over-protective for now!\n\t\t */\n\t\tif (*vm_flags & (VM_MERGEABLE | VM_SHARED  | VM_MAYSHARE   |\n\t\t\t\t VM_PFNMAP    | VM_IO      | VM_DONTEXPAND |\n\t\t\t\t VM_RESERVED  | VM_HUGETLB | VM_INSERTPAGE |\n\t\t\t\t VM_NONLINEAR | VM_MIXEDMAP | VM_SAO))\n\t\t\treturn 0;\t\t/* just ignore the advice */\n\n\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {\n\t\t\terr = __ksm_enter(mm);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags |= VM_MERGEABLE;\n\t\tbreak;\n\n\tcase MADV_UNMERGEABLE:\n\t\tif (!(*vm_flags & VM_MERGEABLE))\n\t\t\treturn 0;\t\t/* just ignore the advice */\n\n\t\tif (vma->anon_vma) {\n\t\t\terr = unmerge_ksm_pages(vma, start, end);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags &= ~VM_MERGEABLE;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint __ksm_enter(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint needs_wakeup;\n\n\tmm_slot = alloc_mm_slot();\n\tif (!mm_slot)\n\t\treturn -ENOMEM;\n\n\t/* Check ksm_run too?  Would need tighter locking */\n\tneeds_wakeup = list_empty(&ksm_mm_head.mm_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tinsert_to_mm_slots_hash(mm, mm_slot);\n\t/*\n\t * Insert just behind the scanning cursor, to let the area settle\n\t * down a little; when fork is followed by immediate exec, we don't\n\t * want ksmd to waste time setting up and tearing down an rmap_list.\n\t */\n\tlist_add_tail(&mm_slot->mm_list, &ksm_scan.mm_slot->mm_list);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tset_bit(MMF_VM_MERGEABLE, &mm->flags);\n\tatomic_inc(&mm->mm_count);\n\n\tif (needs_wakeup)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\treturn 0;\n}\n\nvoid __ksm_exit(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint easy_to_free = 0;\n\n\t/*\n\t * This process is exiting: if it's straightforward (as is the\n\t * case when ksmd was never running), free mm_slot immediately.\n\t * But if it's at the cursor or has rmap_items linked to it, use\n\t * mmap_sem to synchronize with any break_cows before pagetables\n\t * are freed, and leave the mm_slot on the list for ksmd to free.\n\t * Beware: ksm may already have noticed it exiting and freed the slot.\n\t */\n\n\tspin_lock(&ksm_mmlist_lock);\n\tmm_slot = get_mm_slot(mm);\n\tif (mm_slot && ksm_scan.mm_slot != mm_slot) {\n\t\tif (!mm_slot->rmap_list) {\n\t\t\thlist_del(&mm_slot->link);\n\t\t\tlist_del(&mm_slot->mm_list);\n\t\t\teasy_to_free = 1;\n\t\t} else {\n\t\t\tlist_move(&mm_slot->mm_list,\n\t\t\t\t  &ksm_scan.mm_slot->mm_list);\n\t\t}\n\t}\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tif (easy_to_free) {\n\t\tfree_mm_slot(mm_slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n}\n\nstruct page *ksm_does_need_to_copy(struct page *page,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct page *new_page;\n\n\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);\n\tif (new_page) {\n\t\tcopy_user_highpage(new_page, page, address, vma);\n\n\t\tSetPageDirty(new_page);\n\t\t__SetPageUptodate(new_page);\n\t\tSetPageSwapBacked(new_page);\n\t\t__set_page_locked(new_page);\n\n\t\tif (page_evictable(new_page, vma))\n\t\t\tlru_cache_add_lru(new_page, LRU_ACTIVE_ANON);\n\t\telse\n\t\t\tadd_page_to_unevictable_list(new_page);\n\t}\n\n\treturn new_page;\n}\n\nint page_referenced_ksm(struct page *page, struct mem_cgroup *memcg,\n\t\t\tunsigned long *vm_flags)\n{\n\tstruct stable_node *stable_node;\n\tstruct rmap_item *rmap_item;\n\tstruct hlist_node *hlist;\n\tunsigned int mapcount = page_mapcount(page);\n\tint referenced = 0;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn 0;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\n\t\t\t\tcontinue;\n\n\t\t\treferenced += page_referenced_one(page, vma,\n\t\t\t\trmap_item->address, &mapcount, vm_flags);\n\t\t\tif (!search_new_forks || !mapcount)\n\t\t\t\tbreak;\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t\tif (!mapcount)\n\t\t\tgoto out;\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn referenced;\n}\n\nint try_to_unmap_ksm(struct page *page, enum ttu_flags flags)\n{\n\tstruct stable_node *stable_node;\n\tstruct hlist_node *hlist;\n\tstruct rmap_item *rmap_item;\n\tint ret = SWAP_AGAIN;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn SWAP_FAIL;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tret = try_to_unmap_one(page, vma,\n\t\t\t\t\trmap_item->address, flags);\n\t\t\tif (ret != SWAP_AGAIN || !page_mapped(page)) {\n\t\t\t\tanon_vma_unlock(anon_vma);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn ret;\n}\n\n#ifdef CONFIG_MIGRATION\nint rmap_walk_ksm(struct page *page, int (*rmap_one)(struct page *,\n\t\t  struct vm_area_struct *, unsigned long, void *), void *arg)\n{\n\tstruct stable_node *stable_node;\n\tstruct hlist_node *hlist;\n\tstruct rmap_item *rmap_item;\n\tint ret = SWAP_AGAIN;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn ret;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tret = rmap_one(page, vma, rmap_item->address, arg);\n\t\t\tif (ret != SWAP_AGAIN) {\n\t\t\t\tanon_vma_unlock(anon_vma);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn ret;\n}\n\nvoid ksm_migrate_page(struct page *newpage, struct page *oldpage)\n{\n\tstruct stable_node *stable_node;\n\n\tVM_BUG_ON(!PageLocked(oldpage));\n\tVM_BUG_ON(!PageLocked(newpage));\n\tVM_BUG_ON(newpage->mapping != oldpage->mapping);\n\n\tstable_node = page_stable_node(newpage);\n\tif (stable_node) {\n\t\tVM_BUG_ON(stable_node->kpfn != page_to_pfn(oldpage));\n\t\tstable_node->kpfn = page_to_pfn(newpage);\n\t}\n}\n#endif /* CONFIG_MIGRATION */\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\nstatic struct stable_node *ksm_check_stable_tree(unsigned long start_pfn,\n\t\t\t\t\t\t unsigned long end_pfn)\n{\n\tstruct rb_node *node;\n\n\tfor (node = rb_first(&root_stable_tree); node; node = rb_next(node)) {\n\t\tstruct stable_node *stable_node;\n\n\t\tstable_node = rb_entry(node, struct stable_node, node);\n\t\tif (stable_node->kpfn >= start_pfn &&\n\t\t    stable_node->kpfn < end_pfn)\n\t\t\treturn stable_node;\n\t}\n\treturn NULL;\n}\n\nstatic int ksm_memory_callback(struct notifier_block *self,\n\t\t\t       unsigned long action, void *arg)\n{\n\tstruct memory_notify *mn = arg;\n\tstruct stable_node *stable_node;\n\n\tswitch (action) {\n\tcase MEM_GOING_OFFLINE:\n\t\t/*\n\t\t * Keep it very simple for now: just lock out ksmd and\n\t\t * MADV_UNMERGEABLE while any memory is going offline.\n\t\t * mutex_lock_nested() is necessary because lockdep was alarmed\n\t\t * that here we take ksm_thread_mutex inside notifier chain\n\t\t * mutex, and later take notifier chain mutex inside\n\t\t * ksm_thread_mutex to unlock it.   But that's safe because both\n\t\t * are inside mem_hotplug_mutex.\n\t\t */\n\t\tmutex_lock_nested(&ksm_thread_mutex, SINGLE_DEPTH_NESTING);\n\t\tbreak;\n\n\tcase MEM_OFFLINE:\n\t\t/*\n\t\t * Most of the work is done by page migration; but there might\n\t\t * be a few stable_nodes left over, still pointing to struct\n\t\t * pages which have been offlined: prune those from the tree.\n\t\t */\n\t\twhile ((stable_node = ksm_check_stable_tree(mn->start_pfn,\n\t\t\t\t\tmn->start_pfn + mn->nr_pages)) != NULL)\n\t\t\tremove_node_from_stable_tree(stable_node);\n\t\t/* fallthrough */\n\n\tcase MEM_CANCEL_OFFLINE:\n\t\tmutex_unlock(&ksm_thread_mutex);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n#endif /* CONFIG_MEMORY_HOTREMOVE */\n\n#ifdef CONFIG_SYSFS\n/*\n * This all compiles without CONFIG_SYSFS, but is a waste of space.\n */\n\n#define KSM_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n#define KSM_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic ssize_t sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_thread_sleep_millisecs);\n}\n\nstatic ssize_t sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tksm_thread_sleep_millisecs = msecs;\n\n\treturn count;\n}\nKSM_ATTR(sleep_millisecs);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_thread_pages_to_scan);\n}\n\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long nr_pages;\n\n\terr = strict_strtoul(buf, 10, &nr_pages);\n\tif (err || nr_pages > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tksm_thread_pages_to_scan = nr_pages;\n\n\treturn count;\n}\nKSM_ATTR(pages_to_scan);\n\nstatic ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\tchar *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_run);\n}\n\nstatic ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long flags;\n\n\terr = strict_strtoul(buf, 10, &flags);\n\tif (err || flags > UINT_MAX)\n\t\treturn -EINVAL;\n\tif (flags > KSM_RUN_UNMERGE)\n\t\treturn -EINVAL;\n\n\t/*\n\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.\n\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,\n\t * breaking COW to free the pages_shared (but leaves mm_slots\n\t * on the list for when ksmd may be set running again).\n\t */\n\n\tmutex_lock(&ksm_thread_mutex);\n\tif (ksm_run != flags) {\n\t\tksm_run = flags;\n\t\tif (flags & KSM_RUN_UNMERGE) {\n\t\t\tint oom_score_adj;\n\n\t\t\toom_score_adj = test_set_oom_score_adj(OOM_SCORE_ADJ_MAX);\n\t\t\terr = unmerge_and_remove_all_rmap_items();\n\t\t\ttest_set_oom_score_adj(oom_score_adj);\n\t\t\tif (err) {\n\t\t\t\tksm_run = KSM_RUN_STOP;\n\t\t\t\tcount = err;\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&ksm_thread_mutex);\n\n\tif (flags & KSM_RUN_MERGE)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\treturn count;\n}\nKSM_ATTR(run);\n\nstatic ssize_t pages_shared_show(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_shared);\n}\nKSM_ATTR_RO(pages_shared);\n\nstatic ssize_t pages_sharing_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_sharing);\n}\nKSM_ATTR_RO(pages_sharing);\n\nstatic ssize_t pages_unshared_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_unshared);\n}\nKSM_ATTR_RO(pages_unshared);\n\nstatic ssize_t pages_volatile_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\tlong ksm_pages_volatile;\n\n\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared\n\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;\n\t/*\n\t * It was not worth any locking to calculate that statistic,\n\t * but it might therefore sometimes be negative: conceal that.\n\t */\n\tif (ksm_pages_volatile < 0)\n\t\tksm_pages_volatile = 0;\n\treturn sprintf(buf, \"%ld\\n\", ksm_pages_volatile);\n}\nKSM_ATTR_RO(pages_volatile);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_scan.seqnr);\n}\nKSM_ATTR_RO(full_scans);\n\nstatic struct attribute *ksm_attrs[] = {\n\t&sleep_millisecs_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&run_attr.attr,\n\t&pages_shared_attr.attr,\n\t&pages_sharing_attr.attr,\n\t&pages_unshared_attr.attr,\n\t&pages_volatile_attr.attr,\n\t&full_scans_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group ksm_attr_group = {\n\t.attrs = ksm_attrs,\n\t.name = \"ksm\",\n};\n#endif /* CONFIG_SYSFS */\n\nstatic int __init ksm_init(void)\n{\n\tstruct task_struct *ksm_thread;\n\tint err;\n\n\terr = ksm_slab_init();\n\tif (err)\n\t\tgoto out;\n\n\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");\n\tif (IS_ERR(ksm_thread)) {\n\t\tprintk(KERN_ERR \"ksm: creating kthread failed\\n\");\n\t\terr = PTR_ERR(ksm_thread);\n\t\tgoto out_free;\n\t}\n\n#ifdef CONFIG_SYSFS\n\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"ksm: register sysfs failed\\n\");\n\t\tkthread_stop(ksm_thread);\n\t\tgoto out_free;\n\t}\n#else\n\tksm_run = KSM_RUN_MERGE;\t/* no way for user to start it */\n\n#endif /* CONFIG_SYSFS */\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n\t/*\n\t * Choose a high priority since the callback takes ksm_thread_mutex:\n\t * later callbacks could only be taking locks which nest within that.\n\t */\n\thotplug_memory_notifier(ksm_memory_callback, 100);\n#endif\n\treturn 0;\n\nout_free:\n\tksm_slab_free();\nout:\n\treturn err;\n}\nmodule_init(ksm_init)\n"], "fixing_code": ["/*\n * Memory merging support.\n *\n * This code enables dynamic sharing of identical pages found in different\n * memory areas, even if they are not shared by fork()\n *\n * Copyright (C) 2008-2009 Red Hat, Inc.\n * Authors:\n *\tIzik Eidus\n *\tAndrea Arcangeli\n *\tChris Wright\n *\tHugh Dickins\n *\n * This work is licensed under the terms of the GNU GPL, version 2.\n */\n\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/mman.h>\n#include <linux/sched.h>\n#include <linux/rwsem.h>\n#include <linux/pagemap.h>\n#include <linux/rmap.h>\n#include <linux/spinlock.h>\n#include <linux/jhash.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/rbtree.h>\n#include <linux/memory.h>\n#include <linux/mmu_notifier.h>\n#include <linux/swap.h>\n#include <linux/ksm.h>\n#include <linux/hash.h>\n#include <linux/freezer.h>\n#include <linux/oom.h>\n\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n/*\n * A few notes about the KSM scanning process,\n * to make it easier to understand the data structures below:\n *\n * In order to reduce excessive scanning, KSM sorts the memory pages by their\n * contents into a data structure that holds pointers to the pages' locations.\n *\n * Since the contents of the pages may change at any moment, KSM cannot just\n * insert the pages into a normal sorted tree and expect it to find anything.\n * Therefore KSM uses two data structures - the stable and the unstable tree.\n *\n * The stable tree holds pointers to all the merged pages (ksm pages), sorted\n * by their contents.  Because each such page is write-protected, searching on\n * this tree is fully assured to be working (except when pages are unmapped),\n * and therefore this tree is called the stable tree.\n *\n * In addition to the stable tree, KSM uses a second data structure called the\n * unstable tree: this tree holds pointers to pages which have been found to\n * be \"unchanged for a period of time\".  The unstable tree sorts these pages\n * by their contents, but since they are not write-protected, KSM cannot rely\n * upon the unstable tree to work correctly - the unstable tree is liable to\n * be corrupted as its contents are modified, and so it is called unstable.\n *\n * KSM solves this problem by several techniques:\n *\n * 1) The unstable tree is flushed every time KSM completes scanning all\n *    memory areas, and then the tree is rebuilt again from the beginning.\n * 2) KSM will only insert into the unstable tree, pages whose hash value\n *    has not changed since the previous scan of all memory areas.\n * 3) The unstable tree is a RedBlack Tree - so its balancing is based on the\n *    colors of the nodes and not on their contents, assuring that even when\n *    the tree gets \"corrupted\" it won't get out of balance, so scanning time\n *    remains the same (also, searching and inserting nodes in an rbtree uses\n *    the same algorithm, so we have no overhead when we flush and rebuild).\n * 4) KSM never flushes the stable tree, which means that even if it were to\n *    take 10 attempts to find a page in the unstable tree, once it is found,\n *    it is secured in the stable tree.  (When we scan a new page, we first\n *    compare it against the stable tree, and then against the unstable tree.)\n */\n\n/**\n * struct mm_slot - ksm information per mm that is being scanned\n * @link: link to the mm_slots hash list\n * @mm_list: link into the mm_slots list, rooted in ksm_mm_head\n * @rmap_list: head for this mm_slot's singly-linked list of rmap_items\n * @mm: the mm that this information is valid for\n */\nstruct mm_slot {\n\tstruct hlist_node link;\n\tstruct list_head mm_list;\n\tstruct rmap_item *rmap_list;\n\tstruct mm_struct *mm;\n};\n\n/**\n * struct ksm_scan - cursor for scanning\n * @mm_slot: the current mm_slot we are scanning\n * @address: the next address inside that to be scanned\n * @rmap_list: link to the next rmap to be scanned in the rmap_list\n * @seqnr: count of completed full scans (needed when removing unstable node)\n *\n * There is only the one ksm_scan instance of this cursor structure.\n */\nstruct ksm_scan {\n\tstruct mm_slot *mm_slot;\n\tunsigned long address;\n\tstruct rmap_item **rmap_list;\n\tunsigned long seqnr;\n};\n\n/**\n * struct stable_node - node of the stable rbtree\n * @node: rb node of this ksm page in the stable tree\n * @hlist: hlist head of rmap_items using this ksm page\n * @kpfn: page frame number of this ksm page\n */\nstruct stable_node {\n\tstruct rb_node node;\n\tstruct hlist_head hlist;\n\tunsigned long kpfn;\n};\n\n/**\n * struct rmap_item - reverse mapping item for virtual addresses\n * @rmap_list: next rmap_item in mm_slot's singly-linked rmap_list\n * @anon_vma: pointer to anon_vma for this mm,address, when in stable tree\n * @mm: the memory structure this rmap_item is pointing into\n * @address: the virtual address this rmap_item tracks (+ flags in low bits)\n * @oldchecksum: previous checksum of the page at that virtual address\n * @node: rb node of this rmap_item in the unstable tree\n * @head: pointer to stable_node heading this list in the stable tree\n * @hlist: link into hlist of rmap_items hanging off that stable_node\n */\nstruct rmap_item {\n\tstruct rmap_item *rmap_list;\n\tstruct anon_vma *anon_vma;\t/* when stable */\n\tstruct mm_struct *mm;\n\tunsigned long address;\t\t/* + low bits used for flags below */\n\tunsigned int oldchecksum;\t/* when unstable */\n\tunion {\n\t\tstruct rb_node node;\t/* when node of unstable tree */\n\t\tstruct {\t\t/* when listed from stable tree */\n\t\t\tstruct stable_node *head;\n\t\t\tstruct hlist_node hlist;\n\t\t};\n\t};\n};\n\n#define SEQNR_MASK\t0x0ff\t/* low bits of unstable tree seqnr */\n#define UNSTABLE_FLAG\t0x100\t/* is a node of the unstable tree */\n#define STABLE_FLAG\t0x200\t/* is listed from the stable tree */\n\n/* The stable and unstable tree heads */\nstatic struct rb_root root_stable_tree = RB_ROOT;\nstatic struct rb_root root_unstable_tree = RB_ROOT;\n\n#define MM_SLOTS_HASH_SHIFT 10\n#define MM_SLOTS_HASH_HEADS (1 << MM_SLOTS_HASH_SHIFT)\nstatic struct hlist_head mm_slots_hash[MM_SLOTS_HASH_HEADS];\n\nstatic struct mm_slot ksm_mm_head = {\n\t.mm_list = LIST_HEAD_INIT(ksm_mm_head.mm_list),\n};\nstatic struct ksm_scan ksm_scan = {\n\t.mm_slot = &ksm_mm_head,\n};\n\nstatic struct kmem_cache *rmap_item_cache;\nstatic struct kmem_cache *stable_node_cache;\nstatic struct kmem_cache *mm_slot_cache;\n\n/* The number of nodes in the stable tree */\nstatic unsigned long ksm_pages_shared;\n\n/* The number of page slots additionally sharing those nodes */\nstatic unsigned long ksm_pages_sharing;\n\n/* The number of nodes in the unstable tree */\nstatic unsigned long ksm_pages_unshared;\n\n/* The number of rmap_items in use: to calculate pages_volatile */\nstatic unsigned long ksm_rmap_items;\n\n/* Number of pages ksmd should scan in one batch */\nstatic unsigned int ksm_thread_pages_to_scan = 100;\n\n/* Milliseconds ksmd should sleep between batches */\nstatic unsigned int ksm_thread_sleep_millisecs = 20;\n\n#define KSM_RUN_STOP\t0\n#define KSM_RUN_MERGE\t1\n#define KSM_RUN_UNMERGE\t2\nstatic unsigned int ksm_run = KSM_RUN_STOP;\n\nstatic DECLARE_WAIT_QUEUE_HEAD(ksm_thread_wait);\nstatic DEFINE_MUTEX(ksm_thread_mutex);\nstatic DEFINE_SPINLOCK(ksm_mmlist_lock);\n\n#define KSM_KMEM_CACHE(__struct, __flags) kmem_cache_create(\"ksm_\"#__struct,\\\n\t\tsizeof(struct __struct), __alignof__(struct __struct),\\\n\t\t(__flags), NULL)\n\nstatic int __init ksm_slab_init(void)\n{\n\trmap_item_cache = KSM_KMEM_CACHE(rmap_item, 0);\n\tif (!rmap_item_cache)\n\t\tgoto out;\n\n\tstable_node_cache = KSM_KMEM_CACHE(stable_node, 0);\n\tif (!stable_node_cache)\n\t\tgoto out_free1;\n\n\tmm_slot_cache = KSM_KMEM_CACHE(mm_slot, 0);\n\tif (!mm_slot_cache)\n\t\tgoto out_free2;\n\n\treturn 0;\n\nout_free2:\n\tkmem_cache_destroy(stable_node_cache);\nout_free1:\n\tkmem_cache_destroy(rmap_item_cache);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void __init ksm_slab_free(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n\tkmem_cache_destroy(stable_node_cache);\n\tkmem_cache_destroy(rmap_item_cache);\n\tmm_slot_cache = NULL;\n}\n\nstatic inline struct rmap_item *alloc_rmap_item(void)\n{\n\tstruct rmap_item *rmap_item;\n\n\trmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL);\n\tif (rmap_item)\n\t\tksm_rmap_items++;\n\treturn rmap_item;\n}\n\nstatic inline void free_rmap_item(struct rmap_item *rmap_item)\n{\n\tksm_rmap_items--;\n\trmap_item->mm = NULL;\t/* debug safety */\n\tkmem_cache_free(rmap_item_cache, rmap_item);\n}\n\nstatic inline struct stable_node *alloc_stable_node(void)\n{\n\treturn kmem_cache_alloc(stable_node_cache, GFP_KERNEL);\n}\n\nstatic inline void free_stable_node(struct stable_node *stable_node)\n{\n\tkmem_cache_free(stable_node_cache, stable_node);\n}\n\nstatic inline struct mm_slot *alloc_mm_slot(void)\n{\n\tif (!mm_slot_cache)\t/* initialization failed */\n\t\treturn NULL;\n\treturn kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);\n}\n\nstatic inline void free_mm_slot(struct mm_slot *mm_slot)\n{\n\tkmem_cache_free(mm_slot_cache, mm_slot);\n}\n\nstatic struct mm_slot *get_mm_slot(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct hlist_head *bucket;\n\tstruct hlist_node *node;\n\n\tbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\n\thlist_for_each_entry(mm_slot, node, bucket, link) {\n\t\tif (mm == mm_slot->mm)\n\t\t\treturn mm_slot;\n\t}\n\treturn NULL;\n}\n\nstatic void insert_to_mm_slots_hash(struct mm_struct *mm,\n\t\t\t\t    struct mm_slot *mm_slot)\n{\n\tstruct hlist_head *bucket;\n\n\tbucket = &mm_slots_hash[hash_ptr(mm, MM_SLOTS_HASH_SHIFT)];\n\tmm_slot->mm = mm;\n\thlist_add_head(&mm_slot->link, bucket);\n}\n\nstatic inline int in_stable_tree(struct rmap_item *rmap_item)\n{\n\treturn rmap_item->address & STABLE_FLAG;\n}\n\n/*\n * ksmd, and unmerge_and_remove_all_rmap_items(), must not touch an mm's\n * page tables after it has passed through ksm_exit() - which, if necessary,\n * takes mmap_sem briefly to serialize against them.  ksm_exit() does not set\n * a special flag: they can just back out as soon as mm_users goes to zero.\n * ksm_test_exit() is used throughout to make this test for exit: in some\n * places for correctness, in some places just to avoid unnecessary work.\n */\nstatic inline bool ksm_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\n/*\n * We use break_ksm to break COW on a ksm page: it's a stripped down\n *\n *\tif (get_user_pages(current, mm, addr, 1, 1, 1, &page, NULL) == 1)\n *\t\tput_page(page);\n *\n * but taking great care only to touch a ksm page, in a VM_MERGEABLE vma,\n * in case the application has unmapped and remapped mm,addr meanwhile.\n * Could a ksm page appear anywhere else?  Actually yes, in a VM_PFNMAP\n * mmap of /dev/mem or /dev/kmem, where we would not want to touch it.\n */\nstatic int break_ksm(struct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\tint ret = 0;\n\n\tdo {\n\t\tcond_resched();\n\t\tpage = follow_page(vma, addr, FOLL_GET);\n\t\tif (IS_ERR_OR_NULL(page))\n\t\t\tbreak;\n\t\tif (PageKsm(page))\n\t\t\tret = handle_mm_fault(vma->vm_mm, vma, addr,\n\t\t\t\t\t\t\tFAULT_FLAG_WRITE);\n\t\telse\n\t\t\tret = VM_FAULT_WRITE;\n\t\tput_page(page);\n\t} while (!(ret & (VM_FAULT_WRITE | VM_FAULT_SIGBUS | VM_FAULT_OOM)));\n\t/*\n\t * We must loop because handle_mm_fault() may back out if there's\n\t * any difficulty e.g. if pte accessed bit gets updated concurrently.\n\t *\n\t * VM_FAULT_WRITE is what we have been hoping for: it indicates that\n\t * COW has been broken, even if the vma does not permit VM_WRITE;\n\t * but note that a concurrent fault might break PageKsm for us.\n\t *\n\t * VM_FAULT_SIGBUS could occur if we race with truncation of the\n\t * backing file, which also invalidates anonymous pages: that's\n\t * okay, that truncation will have unmapped the PageKsm for us.\n\t *\n\t * VM_FAULT_OOM: at the time of writing (late July 2009), setting\n\t * aside mem_cgroup limits, VM_FAULT_OOM would only be set if the\n\t * current task has TIF_MEMDIE set, and will be OOM killed on return\n\t * to user; and ksmd, having no mm, would never be chosen for that.\n\t *\n\t * But if the mm is in a limited mem_cgroup, then the fault may fail\n\t * with VM_FAULT_OOM even if the current task is not TIF_MEMDIE; and\n\t * even ksmd can fail in this way - though it's usually breaking ksm\n\t * just to undo a merge it made a moment before, so unlikely to oom.\n\t *\n\t * That's a pity: we might therefore have more kernel pages allocated\n\t * than we're counting as nodes in the stable tree; but ksm_do_scan\n\t * will retry to break_cow on each pass, so should recover the page\n\t * in due course.  The important thing is to not let VM_MERGEABLE\n\t * be cleared while any such pages might remain in the area.\n\t */\n\treturn (ret & VM_FAULT_OOM) ? -ENOMEM : 0;\n}\n\nstatic void break_cow(struct rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\n\t/*\n\t * It is not an accident that whenever we want to break COW\n\t * to undo, we also need to drop a reference to the anon_vma.\n\t */\n\tput_anon_vma(rmap_item->anon_vma);\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, addr);\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto out;\n\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\tgoto out;\n\tbreak_ksm(vma, addr);\nout:\n\tup_read(&mm->mmap_sem);\n}\n\nstatic struct page *page_trans_compound_anon(struct page *page)\n{\n\tif (PageTransCompound(page)) {\n\t\tstruct page *head = compound_trans_head(page);\n\t\t/*\n\t\t * head may actually be splitted and freed from under\n\t\t * us but it's ok here.\n\t\t */\n\t\tif (PageAnon(head))\n\t\t\treturn head;\n\t}\n\treturn NULL;\n}\n\nstatic struct page *get_mergeable_page(struct rmap_item *rmap_item)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tunsigned long addr = rmap_item->address;\n\tstruct vm_area_struct *vma;\n\tstruct page *page;\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, addr);\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto out;\n\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\tgoto out;\n\n\tpage = follow_page(vma, addr, FOLL_GET);\n\tif (IS_ERR_OR_NULL(page))\n\t\tgoto out;\n\tif (PageAnon(page) || page_trans_compound_anon(page)) {\n\t\tflush_anon_page(vma, page, addr);\n\t\tflush_dcache_page(page);\n\t} else {\n\t\tput_page(page);\nout:\t\tpage = NULL;\n\t}\n\tup_read(&mm->mmap_sem);\n\treturn page;\n}\n\nstatic void remove_node_from_stable_tree(struct stable_node *stable_node)\n{\n\tstruct rmap_item *rmap_item;\n\tstruct hlist_node *hlist;\n\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tif (rmap_item->hlist.next)\n\t\t\tksm_pages_sharing--;\n\t\telse\n\t\t\tksm_pages_shared--;\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->address &= PAGE_MASK;\n\t\tcond_resched();\n\t}\n\n\trb_erase(&stable_node->node, &root_stable_tree);\n\tfree_stable_node(stable_node);\n}\n\n/*\n * get_ksm_page: checks if the page indicated by the stable node\n * is still its ksm page, despite having held no reference to it.\n * In which case we can trust the content of the page, and it\n * returns the gotten page; but if the page has now been zapped,\n * remove the stale node from the stable tree and return NULL.\n *\n * You would expect the stable_node to hold a reference to the ksm page.\n * But if it increments the page's count, swapping out has to wait for\n * ksmd to come around again before it can free the page, which may take\n * seconds or even minutes: much too unresponsive.  So instead we use a\n * \"keyhole reference\": access to the ksm page from the stable node peeps\n * out through its keyhole to see if that page still holds the right key,\n * pointing back to this stable node.  This relies on freeing a PageAnon\n * page to reset its page->mapping to NULL, and relies on no other use of\n * a page to put something that might look like our key in page->mapping.\n *\n * include/linux/pagemap.h page_cache_get_speculative() is a good reference,\n * but this is different - made simpler by ksm_thread_mutex being held, but\n * interesting for assuming that no other use of the struct page could ever\n * put our expected_mapping into page->mapping (or a field of the union which\n * coincides with page->mapping).  The RCU calls are not for KSM at all, but\n * to keep the page_count protocol described with page_cache_get_speculative.\n *\n * Note: it is possible that get_ksm_page() will return NULL one moment,\n * then page the next, if the page is in between page_freeze_refs() and\n * page_unfreeze_refs(): this shouldn't be a problem anywhere, the page\n * is on its way to being freed; but it is an anomaly to bear in mind.\n */\nstatic struct page *get_ksm_page(struct stable_node *stable_node)\n{\n\tstruct page *page;\n\tvoid *expected_mapping;\n\n\tpage = pfn_to_page(stable_node->kpfn);\n\texpected_mapping = (void *)stable_node +\n\t\t\t\t(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);\n\trcu_read_lock();\n\tif (page->mapping != expected_mapping)\n\t\tgoto stale;\n\tif (!get_page_unless_zero(page))\n\t\tgoto stale;\n\tif (page->mapping != expected_mapping) {\n\t\tput_page(page);\n\t\tgoto stale;\n\t}\n\trcu_read_unlock();\n\treturn page;\nstale:\n\trcu_read_unlock();\n\tremove_node_from_stable_tree(stable_node);\n\treturn NULL;\n}\n\n/*\n * Removing rmap_item from stable or unstable tree.\n * This function will clean the information from the stable/unstable tree.\n */\nstatic void remove_rmap_item_from_tree(struct rmap_item *rmap_item)\n{\n\tif (rmap_item->address & STABLE_FLAG) {\n\t\tstruct stable_node *stable_node;\n\t\tstruct page *page;\n\n\t\tstable_node = rmap_item->head;\n\t\tpage = get_ksm_page(stable_node);\n\t\tif (!page)\n\t\t\tgoto out;\n\n\t\tlock_page(page);\n\t\thlist_del(&rmap_item->hlist);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (stable_node->hlist.first)\n\t\t\tksm_pages_sharing--;\n\t\telse\n\t\t\tksm_pages_shared--;\n\n\t\tput_anon_vma(rmap_item->anon_vma);\n\t\trmap_item->address &= PAGE_MASK;\n\n\t} else if (rmap_item->address & UNSTABLE_FLAG) {\n\t\tunsigned char age;\n\t\t/*\n\t\t * Usually ksmd can and must skip the rb_erase, because\n\t\t * root_unstable_tree was already reset to RB_ROOT.\n\t\t * But be careful when an mm is exiting: do the rb_erase\n\t\t * if this rmap_item was inserted by this scan, rather\n\t\t * than left over from before.\n\t\t */\n\t\tage = (unsigned char)(ksm_scan.seqnr - rmap_item->address);\n\t\tBUG_ON(age > 1);\n\t\tif (!age)\n\t\t\trb_erase(&rmap_item->node, &root_unstable_tree);\n\n\t\tksm_pages_unshared--;\n\t\trmap_item->address &= PAGE_MASK;\n\t}\nout:\n\tcond_resched();\t\t/* we're called from many long loops */\n}\n\nstatic void remove_trailing_rmap_items(struct mm_slot *mm_slot,\n\t\t\t\t       struct rmap_item **rmap_list)\n{\n\twhile (*rmap_list) {\n\t\tstruct rmap_item *rmap_item = *rmap_list;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n}\n\n/*\n * Though it's very tempting to unmerge in_stable_tree(rmap_item)s rather\n * than check every pte of a given vma, the locking doesn't quite work for\n * that - an rmap_item is assigned to the stable tree after inserting ksm\n * page and upping mmap_sem.  Nor does it fit with the way we skip dup'ing\n * rmap_items from parent to child at fork time (so as not to waste time\n * if exit comes before the next scan reaches it).\n *\n * Similarly, although we'd like to remove rmap_items (so updating counts\n * and freeing memory) when unmerging an area, it's easier to leave that\n * to the next pass of ksmd - consider, for example, how ksmd might be\n * in cmp_and_merge_page on one of the rmap_items we would be removing.\n */\nstatic int unmerge_ksm_pages(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tunsigned long addr;\n\tint err = 0;\n\n\tfor (addr = start; addr < end && !err; addr += PAGE_SIZE) {\n\t\tif (ksm_test_exit(vma->vm_mm))\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\terr = -ERESTARTSYS;\n\t\telse\n\t\t\terr = break_ksm(vma, addr);\n\t}\n\treturn err;\n}\n\n#ifdef CONFIG_SYSFS\n/*\n * Only called through the sysfs control interface:\n */\nstatic int unmerge_and_remove_all_rmap_items(void)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint err = 0;\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tfor (mm_slot = ksm_scan.mm_slot;\n\t\t\tmm_slot != &ksm_mm_head; mm_slot = ksm_scan.mm_slot) {\n\t\tmm = mm_slot->mm;\n\t\tdown_read(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\tif (!(vma->vm_flags & VM_MERGEABLE) || !vma->anon_vma)\n\t\t\t\tcontinue;\n\t\t\terr = unmerge_ksm_pages(vma,\n\t\t\t\t\t\tvma->vm_start, vma->vm_end);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t}\n\n\t\tremove_trailing_rmap_items(mm_slot, &mm_slot->rmap_list);\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tksm_scan.mm_slot = list_entry(mm_slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\t\tif (ksm_test_exit(mm)) {\n\t\t\thlist_del(&mm_slot->link);\n\t\t\tlist_del(&mm_slot->mm_list);\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\t\tfree_mm_slot(mm_slot);\n\t\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\t\tup_read(&mm->mmap_sem);\n\t\t\tmmdrop(mm);\n\t\t} else {\n\t\t\tspin_unlock(&ksm_mmlist_lock);\n\t\t\tup_read(&mm->mmap_sem);\n\t\t}\n\t}\n\n\tksm_scan.seqnr = 0;\n\treturn 0;\n\nerror:\n\tup_read(&mm->mmap_sem);\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = &ksm_mm_head;\n\tspin_unlock(&ksm_mmlist_lock);\n\treturn err;\n}\n#endif /* CONFIG_SYSFS */\n\nstatic u32 calc_checksum(struct page *page)\n{\n\tu32 checksum;\n\tvoid *addr = kmap_atomic(page, KM_USER0);\n\tchecksum = jhash2(addr, PAGE_SIZE / 4, 17);\n\tkunmap_atomic(addr, KM_USER0);\n\treturn checksum;\n}\n\nstatic int memcmp_pages(struct page *page1, struct page *page2)\n{\n\tchar *addr1, *addr2;\n\tint ret;\n\n\taddr1 = kmap_atomic(page1, KM_USER0);\n\taddr2 = kmap_atomic(page2, KM_USER1);\n\tret = memcmp(addr1, addr2, PAGE_SIZE);\n\tkunmap_atomic(addr2, KM_USER1);\n\tkunmap_atomic(addr1, KM_USER0);\n\treturn ret;\n}\n\nstatic inline int pages_identical(struct page *page1, struct page *page2)\n{\n\treturn !memcmp_pages(page1, page2);\n}\n\nstatic int write_protect_page(struct vm_area_struct *vma, struct page *page,\n\t\t\t      pte_t *orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long addr;\n\tpte_t *ptep;\n\tspinlock_t *ptl;\n\tint swapped;\n\tint err = -EFAULT;\n\n\taddr = page_address_in_vma(page, vma);\n\tif (addr == -EFAULT)\n\t\tgoto out;\n\n\tBUG_ON(PageTransCompound(page));\n\tptep = page_check_address(page, mm, addr, &ptl, 0);\n\tif (!ptep)\n\t\tgoto out;\n\n\tif (pte_write(*ptep) || pte_dirty(*ptep)) {\n\t\tpte_t entry;\n\n\t\tswapped = PageSwapCache(page);\n\t\tflush_cache_page(vma, addr, page_to_pfn(page));\n\t\t/*\n\t\t * Ok this is tricky, when get_user_pages_fast() run it doesn't\n\t\t * take any lock, therefore the check that we are going to make\n\t\t * with the pagecount against the mapcount is racey and\n\t\t * O_DIRECT can happen right after the check.\n\t\t * So we clear the pte and flush the tlb before the check\n\t\t * this assure us that no O_DIRECT can happen after the check\n\t\t * or in the middle of the check.\n\t\t */\n\t\tentry = ptep_clear_flush(vma, addr, ptep);\n\t\t/*\n\t\t * Check that no O_DIRECT or similar I/O is in progress on the\n\t\t * page\n\t\t */\n\t\tif (page_mapcount(page) + 1 + swapped != page_count(page)) {\n\t\t\tset_pte_at(mm, addr, ptep, entry);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (pte_dirty(entry))\n\t\t\tset_page_dirty(page);\n\t\tentry = pte_mkclean(pte_wrprotect(entry));\n\t\tset_pte_at_notify(mm, addr, ptep, entry);\n\t}\n\t*orig_pte = *ptep;\n\terr = 0;\n\nout_unlock:\n\tpte_unmap_unlock(ptep, ptl);\nout:\n\treturn err;\n}\n\n/**\n * replace_page - replace page in vma by new ksm page\n * @vma:      vma that holds the pte pointing to page\n * @page:     the page we are replacing by kpage\n * @kpage:    the ksm page we replace page by\n * @orig_pte: the original value of the pte\n *\n * Returns 0 on success, -EFAULT on failure.\n */\nstatic int replace_page(struct vm_area_struct *vma, struct page *page,\n\t\t\tstruct page *kpage, pte_t orig_pte)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *ptep;\n\tspinlock_t *ptl;\n\tunsigned long addr;\n\tint err = -EFAULT;\n\n\taddr = page_address_in_vma(page, vma);\n\tif (addr == -EFAULT)\n\t\tgoto out;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, addr);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, addr);\n\tBUG_ON(pmd_trans_huge(*pmd));\n\tif (!pmd_present(*pmd))\n\t\tgoto out;\n\n\tptep = pte_offset_map_lock(mm, pmd, addr, &ptl);\n\tif (!pte_same(*ptep, orig_pte)) {\n\t\tpte_unmap_unlock(ptep, ptl);\n\t\tgoto out;\n\t}\n\n\tget_page(kpage);\n\tpage_add_anon_rmap(kpage, vma, addr);\n\n\tflush_cache_page(vma, addr, pte_pfn(*ptep));\n\tptep_clear_flush(vma, addr, ptep);\n\tset_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma->vm_page_prot));\n\n\tpage_remove_rmap(page);\n\tif (!page_mapped(page))\n\t\ttry_to_free_swap(page);\n\tput_page(page);\n\n\tpte_unmap_unlock(ptep, ptl);\n\terr = 0;\nout:\n\treturn err;\n}\n\nstatic int page_trans_compound_anon_split(struct page *page)\n{\n\tint ret = 0;\n\tstruct page *transhuge_head = page_trans_compound_anon(page);\n\tif (transhuge_head) {\n\t\t/* Get the reference on the head to split it. */\n\t\tif (get_page_unless_zero(transhuge_head)) {\n\t\t\t/*\n\t\t\t * Recheck we got the reference while the head\n\t\t\t * was still anonymous.\n\t\t\t */\n\t\t\tif (PageAnon(transhuge_head))\n\t\t\t\tret = split_huge_page(transhuge_head);\n\t\t\telse\n\t\t\t\t/*\n\t\t\t\t * Retry later if split_huge_page run\n\t\t\t\t * from under us.\n\t\t\t\t */\n\t\t\t\tret = 1;\n\t\t\tput_page(transhuge_head);\n\t\t} else\n\t\t\t/* Retry later if split_huge_page run from under us. */\n\t\t\tret = 1;\n\t}\n\treturn ret;\n}\n\n/*\n * try_to_merge_one_page - take two pages and merge them into one\n * @vma: the vma that holds the pte pointing to page\n * @page: the PageAnon page that we want to replace with kpage\n * @kpage: the PageKsm page that we want to map instead of page,\n *         or NULL the first time when we want to use page as kpage.\n *\n * This function returns 0 if the pages were merged, -EFAULT otherwise.\n */\nstatic int try_to_merge_one_page(struct vm_area_struct *vma,\n\t\t\t\t struct page *page, struct page *kpage)\n{\n\tpte_t orig_pte = __pte(0);\n\tint err = -EFAULT;\n\n\tif (page == kpage)\t\t\t/* ksm page forked */\n\t\treturn 0;\n\n\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\tgoto out;\n\tif (PageTransCompound(page) && page_trans_compound_anon_split(page))\n\t\tgoto out;\n\tBUG_ON(PageTransCompound(page));\n\tif (!PageAnon(page))\n\t\tgoto out;\n\n\t/*\n\t * We need the page lock to read a stable PageSwapCache in\n\t * write_protect_page().  We use trylock_page() instead of\n\t * lock_page() because we don't want to wait here - we\n\t * prefer to continue scanning and merging different pages,\n\t * then come back to this page when it is unlocked.\n\t */\n\tif (!trylock_page(page))\n\t\tgoto out;\n\t/*\n\t * If this anonymous page is mapped only here, its pte may need\n\t * to be write-protected.  If it's mapped elsewhere, all of its\n\t * ptes are necessarily already write-protected.  But in either\n\t * case, we need to lock and check page_count is not raised.\n\t */\n\tif (write_protect_page(vma, page, &orig_pte) == 0) {\n\t\tif (!kpage) {\n\t\t\t/*\n\t\t\t * While we hold page lock, upgrade page from\n\t\t\t * PageAnon+anon_vma to PageKsm+NULL stable_node:\n\t\t\t * stable_tree_insert() will update stable_node.\n\t\t\t */\n\t\t\tset_page_stable_node(page, NULL);\n\t\t\tmark_page_accessed(page);\n\t\t\terr = 0;\n\t\t} else if (pages_identical(page, kpage))\n\t\t\terr = replace_page(vma, page, kpage, orig_pte);\n\t}\n\n\tif ((vma->vm_flags & VM_LOCKED) && kpage && !err) {\n\t\tmunlock_vma_page(page);\n\t\tif (!PageMlocked(kpage)) {\n\t\t\tunlock_page(page);\n\t\t\tlock_page(kpage);\n\t\t\tmlock_vma_page(kpage);\n\t\t\tpage = kpage;\t\t/* for final unlock */\n\t\t}\n\t}\n\n\tunlock_page(page);\nout:\n\treturn err;\n}\n\n/*\n * try_to_merge_with_ksm_page - like try_to_merge_two_pages,\n * but no new kernel page is allocated: kpage must already be a ksm page.\n *\n * This function returns 0 if the pages were merged, -EFAULT otherwise.\n */\nstatic int try_to_merge_with_ksm_page(struct rmap_item *rmap_item,\n\t\t\t\t      struct page *page, struct page *kpage)\n{\n\tstruct mm_struct *mm = rmap_item->mm;\n\tstruct vm_area_struct *vma;\n\tint err = -EFAULT;\n\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tgoto out;\n\tvma = find_vma(mm, rmap_item->address);\n\tif (!vma || vma->vm_start > rmap_item->address)\n\t\tgoto out;\n\n\terr = try_to_merge_one_page(vma, page, kpage);\n\tif (err)\n\t\tgoto out;\n\n\t/* Must get reference to anon_vma while still holding mmap_sem */\n\trmap_item->anon_vma = vma->anon_vma;\n\tget_anon_vma(vma->anon_vma);\nout:\n\tup_read(&mm->mmap_sem);\n\treturn err;\n}\n\n/*\n * try_to_merge_two_pages - take two identical pages and prepare them\n * to be merged into one page.\n *\n * This function returns the kpage if we successfully merged two identical\n * pages into one ksm page, NULL otherwise.\n *\n * Note that this function upgrades page to ksm page: if one of the pages\n * is already a ksm page, try_to_merge_with_ksm_page should be used.\n */\nstatic struct page *try_to_merge_two_pages(struct rmap_item *rmap_item,\n\t\t\t\t\t   struct page *page,\n\t\t\t\t\t   struct rmap_item *tree_rmap_item,\n\t\t\t\t\t   struct page *tree_page)\n{\n\tint err;\n\n\terr = try_to_merge_with_ksm_page(rmap_item, page, NULL);\n\tif (!err) {\n\t\terr = try_to_merge_with_ksm_page(tree_rmap_item,\n\t\t\t\t\t\t\ttree_page, page);\n\t\t/*\n\t\t * If that fails, we have a ksm page with only one pte\n\t\t * pointing to it: so break it.\n\t\t */\n\t\tif (err)\n\t\t\tbreak_cow(rmap_item);\n\t}\n\treturn err ? NULL : page;\n}\n\n/*\n * stable_tree_search - search for page inside the stable tree\n *\n * This function checks if there is a page inside the stable tree\n * with identical content to the page that we are scanning right now.\n *\n * This function returns the stable tree node of identical content if found,\n * NULL otherwise.\n */\nstatic struct page *stable_tree_search(struct page *page)\n{\n\tstruct rb_node *node = root_stable_tree.rb_node;\n\tstruct stable_node *stable_node;\n\n\tstable_node = page_stable_node(page);\n\tif (stable_node) {\t\t\t/* ksm page forked */\n\t\tget_page(page);\n\t\treturn page;\n\t}\n\n\twhile (node) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(node, struct stable_node, node);\n\t\ttree_page = get_ksm_page(stable_node);\n\t\tif (!tree_page)\n\t\t\treturn NULL;\n\n\t\tret = memcmp_pages(page, tree_page);\n\n\t\tif (ret < 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnode = node->rb_left;\n\t\t} else if (ret > 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnode = node->rb_right;\n\t\t} else\n\t\t\treturn tree_page;\n\t}\n\n\treturn NULL;\n}\n\n/*\n * stable_tree_insert - insert rmap_item pointing to new ksm page\n * into the stable tree.\n *\n * This function returns the stable tree node just allocated on success,\n * NULL otherwise.\n */\nstatic struct stable_node *stable_tree_insert(struct page *kpage)\n{\n\tstruct rb_node **new = &root_stable_tree.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct stable_node *stable_node;\n\n\twhile (*new) {\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\tstable_node = rb_entry(*new, struct stable_node, node);\n\t\ttree_page = get_ksm_page(stable_node);\n\t\tif (!tree_page)\n\t\t\treturn NULL;\n\n\t\tret = memcmp_pages(kpage, tree_page);\n\t\tput_page(tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0)\n\t\t\tnew = &parent->rb_left;\n\t\telse if (ret > 0)\n\t\t\tnew = &parent->rb_right;\n\t\telse {\n\t\t\t/*\n\t\t\t * It is not a bug that stable_tree_search() didn't\n\t\t\t * find this node: because at that time our page was\n\t\t\t * not yet write-protected, so may have changed since.\n\t\t\t */\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tstable_node = alloc_stable_node();\n\tif (!stable_node)\n\t\treturn NULL;\n\n\trb_link_node(&stable_node->node, parent, new);\n\trb_insert_color(&stable_node->node, &root_stable_tree);\n\n\tINIT_HLIST_HEAD(&stable_node->hlist);\n\n\tstable_node->kpfn = page_to_pfn(kpage);\n\tset_page_stable_node(kpage, stable_node);\n\n\treturn stable_node;\n}\n\n/*\n * unstable_tree_search_insert - search for identical page,\n * else insert rmap_item into the unstable tree.\n *\n * This function searches for a page in the unstable tree identical to the\n * page currently being scanned; and if no identical page is found in the\n * tree, we insert rmap_item as a new object into the unstable tree.\n *\n * This function returns pointer to rmap_item found to be identical\n * to the currently scanned page, NULL otherwise.\n *\n * This function does both searching and inserting, because they share\n * the same walking algorithm in an rbtree.\n */\nstatic\nstruct rmap_item *unstable_tree_search_insert(struct rmap_item *rmap_item,\n\t\t\t\t\t      struct page *page,\n\t\t\t\t\t      struct page **tree_pagep)\n\n{\n\tstruct rb_node **new = &root_unstable_tree.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*new) {\n\t\tstruct rmap_item *tree_rmap_item;\n\t\tstruct page *tree_page;\n\t\tint ret;\n\n\t\tcond_resched();\n\t\ttree_rmap_item = rb_entry(*new, struct rmap_item, node);\n\t\ttree_page = get_mergeable_page(tree_rmap_item);\n\t\tif (IS_ERR_OR_NULL(tree_page))\n\t\t\treturn NULL;\n\n\t\t/*\n\t\t * Don't substitute a ksm page for a forked page.\n\t\t */\n\t\tif (page == tree_page) {\n\t\t\tput_page(tree_page);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tret = memcmp_pages(page, tree_page);\n\n\t\tparent = *new;\n\t\tif (ret < 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_left;\n\t\t} else if (ret > 0) {\n\t\t\tput_page(tree_page);\n\t\t\tnew = &parent->rb_right;\n\t\t} else {\n\t\t\t*tree_pagep = tree_page;\n\t\t\treturn tree_rmap_item;\n\t\t}\n\t}\n\n\trmap_item->address |= UNSTABLE_FLAG;\n\trmap_item->address |= (ksm_scan.seqnr & SEQNR_MASK);\n\trb_link_node(&rmap_item->node, parent, new);\n\trb_insert_color(&rmap_item->node, &root_unstable_tree);\n\n\tksm_pages_unshared++;\n\treturn NULL;\n}\n\n/*\n * stable_tree_append - add another rmap_item to the linked list of\n * rmap_items hanging off a given node of the stable tree, all sharing\n * the same ksm page.\n */\nstatic void stable_tree_append(struct rmap_item *rmap_item,\n\t\t\t       struct stable_node *stable_node)\n{\n\trmap_item->head = stable_node;\n\trmap_item->address |= STABLE_FLAG;\n\thlist_add_head(&rmap_item->hlist, &stable_node->hlist);\n\n\tif (rmap_item->hlist.next)\n\t\tksm_pages_sharing++;\n\telse\n\t\tksm_pages_shared++;\n}\n\n/*\n * cmp_and_merge_page - first see if page can be merged into the stable tree;\n * if not, compare checksum to previous and if it's the same, see if page can\n * be inserted into the unstable tree, or merged with a page already there and\n * both transferred to the stable tree.\n *\n * @page: the page that we are searching identical page to.\n * @rmap_item: the reverse mapping into the virtual address of this page\n */\nstatic void cmp_and_merge_page(struct page *page, struct rmap_item *rmap_item)\n{\n\tstruct rmap_item *tree_rmap_item;\n\tstruct page *tree_page = NULL;\n\tstruct stable_node *stable_node;\n\tstruct page *kpage;\n\tunsigned int checksum;\n\tint err;\n\n\tremove_rmap_item_from_tree(rmap_item);\n\n\t/* We first start with searching the page inside the stable tree */\n\tkpage = stable_tree_search(page);\n\tif (kpage) {\n\t\terr = try_to_merge_with_ksm_page(rmap_item, page, kpage);\n\t\tif (!err) {\n\t\t\t/*\n\t\t\t * The page was successfully merged:\n\t\t\t * add its rmap_item to the stable tree.\n\t\t\t */\n\t\t\tlock_page(kpage);\n\t\t\tstable_tree_append(rmap_item, page_stable_node(kpage));\n\t\t\tunlock_page(kpage);\n\t\t}\n\t\tput_page(kpage);\n\t\treturn;\n\t}\n\n\t/*\n\t * If the hash value of the page has changed from the last time\n\t * we calculated it, this page is changing frequently: therefore we\n\t * don't want to insert it in the unstable tree, and we don't want\n\t * to waste our time searching for something identical to it there.\n\t */\n\tchecksum = calc_checksum(page);\n\tif (rmap_item->oldchecksum != checksum) {\n\t\trmap_item->oldchecksum = checksum;\n\t\treturn;\n\t}\n\n\ttree_rmap_item =\n\t\tunstable_tree_search_insert(rmap_item, page, &tree_page);\n\tif (tree_rmap_item) {\n\t\tkpage = try_to_merge_two_pages(rmap_item, page,\n\t\t\t\t\t\ttree_rmap_item, tree_page);\n\t\tput_page(tree_page);\n\t\t/*\n\t\t * As soon as we merge this page, we want to remove the\n\t\t * rmap_item of the page we have merged with from the unstable\n\t\t * tree, and insert it instead as new node in the stable tree.\n\t\t */\n\t\tif (kpage) {\n\t\t\tremove_rmap_item_from_tree(tree_rmap_item);\n\n\t\t\tlock_page(kpage);\n\t\t\tstable_node = stable_tree_insert(kpage);\n\t\t\tif (stable_node) {\n\t\t\t\tstable_tree_append(tree_rmap_item, stable_node);\n\t\t\t\tstable_tree_append(rmap_item, stable_node);\n\t\t\t}\n\t\t\tunlock_page(kpage);\n\n\t\t\t/*\n\t\t\t * If we fail to insert the page into the stable tree,\n\t\t\t * we will have 2 virtual addresses that are pointing\n\t\t\t * to a ksm page left outside the stable tree,\n\t\t\t * in which case we need to break_cow on both.\n\t\t\t */\n\t\t\tif (!stable_node) {\n\t\t\t\tbreak_cow(tree_rmap_item);\n\t\t\t\tbreak_cow(rmap_item);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic struct rmap_item *get_next_rmap_item(struct mm_slot *mm_slot,\n\t\t\t\t\t    struct rmap_item **rmap_list,\n\t\t\t\t\t    unsigned long addr)\n{\n\tstruct rmap_item *rmap_item;\n\n\twhile (*rmap_list) {\n\t\trmap_item = *rmap_list;\n\t\tif ((rmap_item->address & PAGE_MASK) == addr)\n\t\t\treturn rmap_item;\n\t\tif (rmap_item->address > addr)\n\t\t\tbreak;\n\t\t*rmap_list = rmap_item->rmap_list;\n\t\tremove_rmap_item_from_tree(rmap_item);\n\t\tfree_rmap_item(rmap_item);\n\t}\n\n\trmap_item = alloc_rmap_item();\n\tif (rmap_item) {\n\t\t/* It has already been zeroed */\n\t\trmap_item->mm = mm_slot->mm;\n\t\trmap_item->address = addr;\n\t\trmap_item->rmap_list = *rmap_list;\n\t\t*rmap_list = rmap_item;\n\t}\n\treturn rmap_item;\n}\n\nstatic struct rmap_item *scan_get_next_rmap_item(struct page **page)\n{\n\tstruct mm_struct *mm;\n\tstruct mm_slot *slot;\n\tstruct vm_area_struct *vma;\n\tstruct rmap_item *rmap_item;\n\n\tif (list_empty(&ksm_mm_head.mm_list))\n\t\treturn NULL;\n\n\tslot = ksm_scan.mm_slot;\n\tif (slot == &ksm_mm_head) {\n\t\t/*\n\t\t * A number of pages can hang around indefinitely on per-cpu\n\t\t * pagevecs, raised page count preventing write_protect_page\n\t\t * from merging them.  Though it doesn't really matter much,\n\t\t * it is puzzling to see some stuck in pages_volatile until\n\t\t * other activity jostles them out, and they also prevented\n\t\t * LTP's KSM test from succeeding deterministically; so drain\n\t\t * them here (here rather than on entry to ksm_do_scan(),\n\t\t * so we don't IPI too often when pages_to_scan is set low).\n\t\t */\n\t\tlru_add_drain_all();\n\n\t\troot_unstable_tree = RB_ROOT;\n\n\t\tspin_lock(&ksm_mmlist_lock);\n\t\tslot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);\n\t\tksm_scan.mm_slot = slot;\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\t/*\n\t\t * Although we tested list_empty() above, a racing __ksm_exit\n\t\t * of the last mm on the list may have removed it since then.\n\t\t */\n\t\tif (slot == &ksm_mm_head)\n\t\t\treturn NULL;\nnext_mm:\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\n\tmm = slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (ksm_test_exit(mm))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, ksm_scan.address);\n\n\tfor (; vma; vma = vma->vm_next) {\n\t\tif (!(vma->vm_flags & VM_MERGEABLE))\n\t\t\tcontinue;\n\t\tif (ksm_scan.address < vma->vm_start)\n\t\t\tksm_scan.address = vma->vm_start;\n\t\tif (!vma->anon_vma)\n\t\t\tksm_scan.address = vma->vm_end;\n\n\t\twhile (ksm_scan.address < vma->vm_end) {\n\t\t\tif (ksm_test_exit(mm))\n\t\t\t\tbreak;\n\t\t\t*page = follow_page(vma, ksm_scan.address, FOLL_GET);\n\t\t\tif (IS_ERR_OR_NULL(*page)) {\n\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (PageAnon(*page) ||\n\t\t\t    page_trans_compound_anon(*page)) {\n\t\t\t\tflush_anon_page(vma, *page, ksm_scan.address);\n\t\t\t\tflush_dcache_page(*page);\n\t\t\t\trmap_item = get_next_rmap_item(slot,\n\t\t\t\t\tksm_scan.rmap_list, ksm_scan.address);\n\t\t\t\tif (rmap_item) {\n\t\t\t\t\tksm_scan.rmap_list =\n\t\t\t\t\t\t\t&rmap_item->rmap_list;\n\t\t\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\t\t} else\n\t\t\t\t\tput_page(*page);\n\t\t\t\tup_read(&mm->mmap_sem);\n\t\t\t\treturn rmap_item;\n\t\t\t}\n\t\t\tput_page(*page);\n\t\t\tksm_scan.address += PAGE_SIZE;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif (ksm_test_exit(mm)) {\n\t\tksm_scan.address = 0;\n\t\tksm_scan.rmap_list = &slot->rmap_list;\n\t}\n\t/*\n\t * Nuke all the rmap_items that are above this current rmap:\n\t * because there were no VM_MERGEABLE vmas with such addresses.\n\t */\n\tremove_trailing_rmap_items(slot, ksm_scan.rmap_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tksm_scan.mm_slot = list_entry(slot->mm_list.next,\n\t\t\t\t\t\tstruct mm_slot, mm_list);\n\tif (ksm_scan.address == 0) {\n\t\t/*\n\t\t * We've completed a full scan of all vmas, holding mmap_sem\n\t\t * throughout, and found no VM_MERGEABLE: so do the same as\n\t\t * __ksm_exit does to remove this mm from all our lists now.\n\t\t * This applies either when cleaning up after __ksm_exit\n\t\t * (but beware: we can reach here even before __ksm_exit),\n\t\t * or when all VM_MERGEABLE areas have been unmapped (and\n\t\t * mmap_sem then protects against race with MADV_MERGEABLE).\n\t\t */\n\t\thlist_del(&slot->link);\n\t\tlist_del(&slot->mm_list);\n\t\tspin_unlock(&ksm_mmlist_lock);\n\n\t\tfree_mm_slot(slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmdrop(mm);\n\t} else {\n\t\tspin_unlock(&ksm_mmlist_lock);\n\t\tup_read(&mm->mmap_sem);\n\t}\n\n\t/* Repeat until we've completed scanning the whole list */\n\tslot = ksm_scan.mm_slot;\n\tif (slot != &ksm_mm_head)\n\t\tgoto next_mm;\n\n\tksm_scan.seqnr++;\n\treturn NULL;\n}\n\n/**\n * ksm_do_scan  - the ksm scanner main worker function.\n * @scan_npages - number of pages we want to scan before we return.\n */\nstatic void ksm_do_scan(unsigned int scan_npages)\n{\n\tstruct rmap_item *rmap_item;\n\tstruct page *uninitialized_var(page);\n\n\twhile (scan_npages-- && likely(!freezing(current))) {\n\t\tcond_resched();\n\t\trmap_item = scan_get_next_rmap_item(&page);\n\t\tif (!rmap_item)\n\t\t\treturn;\n\t\tif (!PageKsm(page) || !in_stable_tree(rmap_item))\n\t\t\tcmp_and_merge_page(page, rmap_item);\n\t\tput_page(page);\n\t}\n}\n\nstatic int ksmd_should_run(void)\n{\n\treturn (ksm_run & KSM_RUN_MERGE) && !list_empty(&ksm_mm_head.mm_list);\n}\n\nstatic int ksm_scan_thread(void *nothing)\n{\n\tset_freezable();\n\tset_user_nice(current, 5);\n\n\twhile (!kthread_should_stop()) {\n\t\tmutex_lock(&ksm_thread_mutex);\n\t\tif (ksmd_should_run())\n\t\t\tksm_do_scan(ksm_thread_pages_to_scan);\n\t\tmutex_unlock(&ksm_thread_mutex);\n\n\t\ttry_to_freeze();\n\n\t\tif (ksmd_should_run()) {\n\t\t\tschedule_timeout_interruptible(\n\t\t\t\tmsecs_to_jiffies(ksm_thread_sleep_millisecs));\n\t\t} else {\n\t\t\twait_event_freezable(ksm_thread_wait,\n\t\t\t\tksmd_should_run() || kthread_should_stop());\n\t\t}\n\t}\n\treturn 0;\n}\n\nint ksm_madvise(struct vm_area_struct *vma, unsigned long start,\n\t\tunsigned long end, int advice, unsigned long *vm_flags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint err;\n\n\tswitch (advice) {\n\tcase MADV_MERGEABLE:\n\t\t/*\n\t\t * Be somewhat over-protective for now!\n\t\t */\n\t\tif (*vm_flags & (VM_MERGEABLE | VM_SHARED  | VM_MAYSHARE   |\n\t\t\t\t VM_PFNMAP    | VM_IO      | VM_DONTEXPAND |\n\t\t\t\t VM_RESERVED  | VM_HUGETLB | VM_INSERTPAGE |\n\t\t\t\t VM_NONLINEAR | VM_MIXEDMAP | VM_SAO))\n\t\t\treturn 0;\t\t/* just ignore the advice */\n\n\t\tif (!test_bit(MMF_VM_MERGEABLE, &mm->flags)) {\n\t\t\terr = __ksm_enter(mm);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags |= VM_MERGEABLE;\n\t\tbreak;\n\n\tcase MADV_UNMERGEABLE:\n\t\tif (!(*vm_flags & VM_MERGEABLE))\n\t\t\treturn 0;\t\t/* just ignore the advice */\n\n\t\tif (vma->anon_vma) {\n\t\t\terr = unmerge_ksm_pages(vma, start, end);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\t*vm_flags &= ~VM_MERGEABLE;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint __ksm_enter(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint needs_wakeup;\n\n\tmm_slot = alloc_mm_slot();\n\tif (!mm_slot)\n\t\treturn -ENOMEM;\n\n\t/* Check ksm_run too?  Would need tighter locking */\n\tneeds_wakeup = list_empty(&ksm_mm_head.mm_list);\n\n\tspin_lock(&ksm_mmlist_lock);\n\tinsert_to_mm_slots_hash(mm, mm_slot);\n\t/*\n\t * Insert just behind the scanning cursor, to let the area settle\n\t * down a little; when fork is followed by immediate exec, we don't\n\t * want ksmd to waste time setting up and tearing down an rmap_list.\n\t */\n\tlist_add_tail(&mm_slot->mm_list, &ksm_scan.mm_slot->mm_list);\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tset_bit(MMF_VM_MERGEABLE, &mm->flags);\n\tatomic_inc(&mm->mm_count);\n\n\tif (needs_wakeup)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\treturn 0;\n}\n\nvoid __ksm_exit(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint easy_to_free = 0;\n\n\t/*\n\t * This process is exiting: if it's straightforward (as is the\n\t * case when ksmd was never running), free mm_slot immediately.\n\t * But if it's at the cursor or has rmap_items linked to it, use\n\t * mmap_sem to synchronize with any break_cows before pagetables\n\t * are freed, and leave the mm_slot on the list for ksmd to free.\n\t * Beware: ksm may already have noticed it exiting and freed the slot.\n\t */\n\n\tspin_lock(&ksm_mmlist_lock);\n\tmm_slot = get_mm_slot(mm);\n\tif (mm_slot && ksm_scan.mm_slot != mm_slot) {\n\t\tif (!mm_slot->rmap_list) {\n\t\t\thlist_del(&mm_slot->link);\n\t\t\tlist_del(&mm_slot->mm_list);\n\t\t\teasy_to_free = 1;\n\t\t} else {\n\t\t\tlist_move(&mm_slot->mm_list,\n\t\t\t\t  &ksm_scan.mm_slot->mm_list);\n\t\t}\n\t}\n\tspin_unlock(&ksm_mmlist_lock);\n\n\tif (easy_to_free) {\n\t\tfree_mm_slot(mm_slot);\n\t\tclear_bit(MMF_VM_MERGEABLE, &mm->flags);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t}\n}\n\nstruct page *ksm_does_need_to_copy(struct page *page,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct page *new_page;\n\n\tnew_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);\n\tif (new_page) {\n\t\tcopy_user_highpage(new_page, page, address, vma);\n\n\t\tSetPageDirty(new_page);\n\t\t__SetPageUptodate(new_page);\n\t\tSetPageSwapBacked(new_page);\n\t\t__set_page_locked(new_page);\n\n\t\tif (page_evictable(new_page, vma))\n\t\t\tlru_cache_add_lru(new_page, LRU_ACTIVE_ANON);\n\t\telse\n\t\t\tadd_page_to_unevictable_list(new_page);\n\t}\n\n\treturn new_page;\n}\n\nint page_referenced_ksm(struct page *page, struct mem_cgroup *memcg,\n\t\t\tunsigned long *vm_flags)\n{\n\tstruct stable_node *stable_node;\n\tstruct rmap_item *rmap_item;\n\tstruct hlist_node *hlist;\n\tunsigned int mapcount = page_mapcount(page);\n\tint referenced = 0;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn 0;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tif (memcg && !mm_match_cgroup(vma->vm_mm, memcg))\n\t\t\t\tcontinue;\n\n\t\t\treferenced += page_referenced_one(page, vma,\n\t\t\t\trmap_item->address, &mapcount, vm_flags);\n\t\t\tif (!search_new_forks || !mapcount)\n\t\t\t\tbreak;\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t\tif (!mapcount)\n\t\t\tgoto out;\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn referenced;\n}\n\nint try_to_unmap_ksm(struct page *page, enum ttu_flags flags)\n{\n\tstruct stable_node *stable_node;\n\tstruct hlist_node *hlist;\n\tstruct rmap_item *rmap_item;\n\tint ret = SWAP_AGAIN;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn SWAP_FAIL;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tret = try_to_unmap_one(page, vma,\n\t\t\t\t\trmap_item->address, flags);\n\t\t\tif (ret != SWAP_AGAIN || !page_mapped(page)) {\n\t\t\t\tanon_vma_unlock(anon_vma);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn ret;\n}\n\n#ifdef CONFIG_MIGRATION\nint rmap_walk_ksm(struct page *page, int (*rmap_one)(struct page *,\n\t\t  struct vm_area_struct *, unsigned long, void *), void *arg)\n{\n\tstruct stable_node *stable_node;\n\tstruct hlist_node *hlist;\n\tstruct rmap_item *rmap_item;\n\tint ret = SWAP_AGAIN;\n\tint search_new_forks = 0;\n\n\tVM_BUG_ON(!PageKsm(page));\n\tVM_BUG_ON(!PageLocked(page));\n\n\tstable_node = page_stable_node(page);\n\tif (!stable_node)\n\t\treturn ret;\nagain:\n\thlist_for_each_entry(rmap_item, hlist, &stable_node->hlist, hlist) {\n\t\tstruct anon_vma *anon_vma = rmap_item->anon_vma;\n\t\tstruct anon_vma_chain *vmac;\n\t\tstruct vm_area_struct *vma;\n\n\t\tanon_vma_lock(anon_vma);\n\t\tlist_for_each_entry(vmac, &anon_vma->head, same_anon_vma) {\n\t\t\tvma = vmac->vma;\n\t\t\tif (rmap_item->address < vma->vm_start ||\n\t\t\t    rmap_item->address >= vma->vm_end)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Initially we examine only the vma which covers this\n\t\t\t * rmap_item; but later, if there is still work to do,\n\t\t\t * we examine covering vmas in other mms: in case they\n\t\t\t * were forked from the original since ksmd passed.\n\t\t\t */\n\t\t\tif ((rmap_item->mm == vma->vm_mm) == search_new_forks)\n\t\t\t\tcontinue;\n\n\t\t\tret = rmap_one(page, vma, rmap_item->address, arg);\n\t\t\tif (ret != SWAP_AGAIN) {\n\t\t\t\tanon_vma_unlock(anon_vma);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tanon_vma_unlock(anon_vma);\n\t}\n\tif (!search_new_forks++)\n\t\tgoto again;\nout:\n\treturn ret;\n}\n\nvoid ksm_migrate_page(struct page *newpage, struct page *oldpage)\n{\n\tstruct stable_node *stable_node;\n\n\tVM_BUG_ON(!PageLocked(oldpage));\n\tVM_BUG_ON(!PageLocked(newpage));\n\tVM_BUG_ON(newpage->mapping != oldpage->mapping);\n\n\tstable_node = page_stable_node(newpage);\n\tif (stable_node) {\n\t\tVM_BUG_ON(stable_node->kpfn != page_to_pfn(oldpage));\n\t\tstable_node->kpfn = page_to_pfn(newpage);\n\t}\n}\n#endif /* CONFIG_MIGRATION */\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\nstatic struct stable_node *ksm_check_stable_tree(unsigned long start_pfn,\n\t\t\t\t\t\t unsigned long end_pfn)\n{\n\tstruct rb_node *node;\n\n\tfor (node = rb_first(&root_stable_tree); node; node = rb_next(node)) {\n\t\tstruct stable_node *stable_node;\n\n\t\tstable_node = rb_entry(node, struct stable_node, node);\n\t\tif (stable_node->kpfn >= start_pfn &&\n\t\t    stable_node->kpfn < end_pfn)\n\t\t\treturn stable_node;\n\t}\n\treturn NULL;\n}\n\nstatic int ksm_memory_callback(struct notifier_block *self,\n\t\t\t       unsigned long action, void *arg)\n{\n\tstruct memory_notify *mn = arg;\n\tstruct stable_node *stable_node;\n\n\tswitch (action) {\n\tcase MEM_GOING_OFFLINE:\n\t\t/*\n\t\t * Keep it very simple for now: just lock out ksmd and\n\t\t * MADV_UNMERGEABLE while any memory is going offline.\n\t\t * mutex_lock_nested() is necessary because lockdep was alarmed\n\t\t * that here we take ksm_thread_mutex inside notifier chain\n\t\t * mutex, and later take notifier chain mutex inside\n\t\t * ksm_thread_mutex to unlock it.   But that's safe because both\n\t\t * are inside mem_hotplug_mutex.\n\t\t */\n\t\tmutex_lock_nested(&ksm_thread_mutex, SINGLE_DEPTH_NESTING);\n\t\tbreak;\n\n\tcase MEM_OFFLINE:\n\t\t/*\n\t\t * Most of the work is done by page migration; but there might\n\t\t * be a few stable_nodes left over, still pointing to struct\n\t\t * pages which have been offlined: prune those from the tree.\n\t\t */\n\t\twhile ((stable_node = ksm_check_stable_tree(mn->start_pfn,\n\t\t\t\t\tmn->start_pfn + mn->nr_pages)) != NULL)\n\t\t\tremove_node_from_stable_tree(stable_node);\n\t\t/* fallthrough */\n\n\tcase MEM_CANCEL_OFFLINE:\n\t\tmutex_unlock(&ksm_thread_mutex);\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n#endif /* CONFIG_MEMORY_HOTREMOVE */\n\n#ifdef CONFIG_SYSFS\n/*\n * This all compiles without CONFIG_SYSFS, but is a waste of space.\n */\n\n#define KSM_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n#define KSM_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic ssize_t sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_thread_sleep_millisecs);\n}\n\nstatic ssize_t sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tksm_thread_sleep_millisecs = msecs;\n\n\treturn count;\n}\nKSM_ATTR(sleep_millisecs);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_thread_pages_to_scan);\n}\n\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long nr_pages;\n\n\terr = strict_strtoul(buf, 10, &nr_pages);\n\tif (err || nr_pages > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tksm_thread_pages_to_scan = nr_pages;\n\n\treturn count;\n}\nKSM_ATTR(pages_to_scan);\n\nstatic ssize_t run_show(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\tchar *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", ksm_run);\n}\n\nstatic ssize_t run_store(struct kobject *kobj, struct kobj_attribute *attr,\n\t\t\t const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long flags;\n\n\terr = strict_strtoul(buf, 10, &flags);\n\tif (err || flags > UINT_MAX)\n\t\treturn -EINVAL;\n\tif (flags > KSM_RUN_UNMERGE)\n\t\treturn -EINVAL;\n\n\t/*\n\t * KSM_RUN_MERGE sets ksmd running, and 0 stops it running.\n\t * KSM_RUN_UNMERGE stops it running and unmerges all rmap_items,\n\t * breaking COW to free the pages_shared (but leaves mm_slots\n\t * on the list for when ksmd may be set running again).\n\t */\n\n\tmutex_lock(&ksm_thread_mutex);\n\tif (ksm_run != flags) {\n\t\tksm_run = flags;\n\t\tif (flags & KSM_RUN_UNMERGE) {\n\t\t\tint oom_score_adj;\n\n\t\t\toom_score_adj = test_set_oom_score_adj(OOM_SCORE_ADJ_MAX);\n\t\t\terr = unmerge_and_remove_all_rmap_items();\n\t\t\ttest_set_oom_score_adj(oom_score_adj);\n\t\t\tif (err) {\n\t\t\t\tksm_run = KSM_RUN_STOP;\n\t\t\t\tcount = err;\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&ksm_thread_mutex);\n\n\tif (flags & KSM_RUN_MERGE)\n\t\twake_up_interruptible(&ksm_thread_wait);\n\n\treturn count;\n}\nKSM_ATTR(run);\n\nstatic ssize_t pages_shared_show(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_shared);\n}\nKSM_ATTR_RO(pages_shared);\n\nstatic ssize_t pages_sharing_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_sharing);\n}\nKSM_ATTR_RO(pages_sharing);\n\nstatic ssize_t pages_unshared_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_pages_unshared);\n}\nKSM_ATTR_RO(pages_unshared);\n\nstatic ssize_t pages_volatile_show(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\tlong ksm_pages_volatile;\n\n\tksm_pages_volatile = ksm_rmap_items - ksm_pages_shared\n\t\t\t\t- ksm_pages_sharing - ksm_pages_unshared;\n\t/*\n\t * It was not worth any locking to calculate that statistic,\n\t * but it might therefore sometimes be negative: conceal that.\n\t */\n\tif (ksm_pages_volatile < 0)\n\t\tksm_pages_volatile = 0;\n\treturn sprintf(buf, \"%ld\\n\", ksm_pages_volatile);\n}\nKSM_ATTR_RO(pages_volatile);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", ksm_scan.seqnr);\n}\nKSM_ATTR_RO(full_scans);\n\nstatic struct attribute *ksm_attrs[] = {\n\t&sleep_millisecs_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&run_attr.attr,\n\t&pages_shared_attr.attr,\n\t&pages_sharing_attr.attr,\n\t&pages_unshared_attr.attr,\n\t&pages_volatile_attr.attr,\n\t&full_scans_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group ksm_attr_group = {\n\t.attrs = ksm_attrs,\n\t.name = \"ksm\",\n};\n#endif /* CONFIG_SYSFS */\n\nstatic int __init ksm_init(void)\n{\n\tstruct task_struct *ksm_thread;\n\tint err;\n\n\terr = ksm_slab_init();\n\tif (err)\n\t\tgoto out;\n\n\tksm_thread = kthread_run(ksm_scan_thread, NULL, \"ksmd\");\n\tif (IS_ERR(ksm_thread)) {\n\t\tprintk(KERN_ERR \"ksm: creating kthread failed\\n\");\n\t\terr = PTR_ERR(ksm_thread);\n\t\tgoto out_free;\n\t}\n\n#ifdef CONFIG_SYSFS\n\terr = sysfs_create_group(mm_kobj, &ksm_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"ksm: register sysfs failed\\n\");\n\t\tkthread_stop(ksm_thread);\n\t\tgoto out_free;\n\t}\n#else\n\tksm_run = KSM_RUN_MERGE;\t/* no way for user to start it */\n\n#endif /* CONFIG_SYSFS */\n\n#ifdef CONFIG_MEMORY_HOTREMOVE\n\t/*\n\t * Choose a high priority since the callback takes ksm_thread_mutex:\n\t * later callbacks could only be taking locks which nest within that.\n\t */\n\thotplug_memory_notifier(ksm_memory_callback, 100);\n#endif\n\treturn 0;\n\nout_free:\n\tksm_slab_free();\nout:\n\treturn err;\n}\nmodule_init(ksm_init)\n"], "filenames": ["mm/ksm.c"], "buggy_code_start_loc": [1304], "buggy_code_end_loc": [1304], "fixing_code_start_loc": [1305], "fixing_code_end_loc": [1311], "type": "CWE-362", "message": "Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application.", "other": {"cve": {"id": "CVE-2011-2183", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-13T10:24:54.420", "lastModified": "2023-02-13T01:19:35.923", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Race condition in the scan_get_next_rmap_item function in mm/ksm.c in the Linux kernel before 2.6.39.3, when Kernel SamePage Merging (KSM) is enabled, allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a crafted application."}, {"lang": "es", "value": "Condici\u00f3n de carrera en la funci\u00f3n scan_get_next_rmap_item de mm/ksm.c del kernel de Linux en versiones anteriores a la 2.6.39.3, si \"Kernel SamePage Merging\" (KSM) est\u00e1 habilitado, permite a usuarios locales provocar una denegaci\u00f3n de servicio (Resoluci\u00f3n de puntero NULL) o posiblemente tener otros impactos sin especificar a trav\u00e9s de una aplicaci\u00f3n modificada."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.6.39.2", "matchCriteriaId": "AF919361-5DEF-4659-B374-AFD39FBBA986"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:*:*:*:*:*:*:*", "matchCriteriaId": "EAAF54BC-6282-492F-BA52-8792223320ED"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc1:*:*:*:*:*:*", "matchCriteriaId": "2493C2FB-2BA1-4DB3-BC04-E282C9CD399D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc2:*:*:*:*:*:*", "matchCriteriaId": "18DBC8AF-18ED-4879-8888-23022E494D14"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc3:*:*:*:*:*:*", "matchCriteriaId": "75EB375B-8ADF-4EAB-A3FB-ED5D35E5E719"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc4:*:*:*:*:*:*", "matchCriteriaId": "D49BB231-622E-4F20-97C8-E6289933912C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc5:*:*:*:*:*:*", "matchCriteriaId": "2D6859AA-DA7F-4AF9-8443-05962171D6E3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc6:*:*:*:*:*:*", "matchCriteriaId": "9B79A89D-F048-48C5-B148-4B38A6C3953B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39:rc7:*:*:*:*:*:*", "matchCriteriaId": "BEBA9217-9105-4BA3-BE1E-FE387FECEF87"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.39.1:*:*:*:*:*:*:*", "matchCriteriaId": "1713DDF4-33F1-4716-84D7-FCFECCF9BCCA"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.39.3", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=2b472611a32a72f4a118c069c2d62a1a3f087afd", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/06/06/1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=710338", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/2b472611a32a72f4a118c069c2d62a1a3f087afd"}}