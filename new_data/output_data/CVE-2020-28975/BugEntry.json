{"buggy_code": [".. include:: _contributors.rst\n\n.. currentmodule:: sklearn\n\n.. _changes_1_0_1:\n\nVersion 1.0.1\n=============\n\n**In Development**\n\nChangelog\n---------\n\nFixed models\n------------\n\n- |Fix| Non-fit methods in the following classes do not raise a UserWarning\n  when fitted on DataFrames with valid feature names:\n  :class:`covariance.EllipticEnvelope`, :class:`ensemble.IsolationForest`,\n  :class:`ensemble.AdaBoostClassifier`, :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.KNeighborsRegressor`,\n  :class:`neighbors.RadiusNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsRegressor`. :pr:`21199` by `Thomas Fan`_.\n\n:mod:`sklearn.calibration`\n..........................\n\n- |Fix| Fixed :class:`calibration.CalibratedClassifierCV` to take into account\n  `sample_weight` when computing the base estimator prediction when\n  `ensemble=False`.\n  :pr:`20638` by :user:`Julien Bohn\u00e9 <JulienB-78>`.\n\n- |Fix| Fixed a bug in :class:`calibration.CalibratedClassifierCV` with\n  `method=\"sigmoid\"` that was ignoring the `sample_weight` when computing the\n  the Bayesian priors.\n  :pr:`21179` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n:mod:`sklearn.cluster`\n......................\n\n- |Fix| Fixed a bug in :class:`cluster.KMeans`, ensuring reproducibility and equivalence\n  between sparse and dense input. :pr:`21195`\n  by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.linear_model`\n...........................\n\n- |Fix| Improves stability of :class:`linear_model.LassoLars` for different\n  versions of openblas. :pr:`21340` by `Thomas Fan`_.\n\n:mod:`sklearn.neighbors`\n........................\n\n- |Fix| :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.KNeighborsRegressor`,\n  :class:`neighbors.RadiusNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsRegressor` with `metric=\"precomputed\"` raises\n  an error for `bsr` and `dok` sparse matrices in methods: `fit`, `kneighbors`\n  and `radius_neighbors`, due to handling of explicit zeros in `bsr` and `dok`\n  :term:`sparse graph` formats. :pr:`21199` by `Thomas Fan`_.\n\n:mod:`sklearn.pipeline`\n.......................\n\n- |Fix| :meth:`pipeline.Pipeline.get_feature_names_out` correctly passes feature\n  names out from one step of a pipeline to the next. :pr:`21351` by\n  `Thomas Fan`_.\n\n.. _changes_1_0:\n\nVersion 1.0.0\n=============\n\n**September 2021**\n\nFor a short description of the main highlights of the release, please\nrefer to\n:ref:`sphx_glr_auto_examples_release_highlights_plot_release_highlights_1_0_0.py`.\n\n.. include:: changelog_legend.inc\n\nMinimal dependencies\n--------------------\n\nVersion 1.0.0 of scikit-learn requires python 3.7+, numpy 1.14.6+ and\nscipy 1.1.0+. Optional minimal dependency is matplotlib 2.2.2+.\n\nEnforcing keyword-only arguments\n--------------------------------\n\nIn an effort to promote clear and non-ambiguous use of the library, most\nconstructor and function parameters must now be passed as keyword arguments\n(i.e. using the `param=value` syntax) instead of positional. If a keyword-only\nparameter is used as positional, a `TypeError` is now raised.\n:issue:`15005` :pr:`20002` by `Joel Nothman`_, `Adrin Jalali`_, `Thomas Fan`_,\n`Nicolas Hug`_, and `Tom Dupre la Tour`_. See `SLEP009\n<https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep009/proposal.html>`_\nfor more details.\n\nChanged models\n--------------\n\nThe following estimators and functions, when fit with the same data and\nparameters, may produce different models from the previous version. This often\noccurs due to changes in the modelling logic (bug fixes or enhancements), or in\nrandom sampling procedures.\n\n- |Fix| :class:`manifold.TSNE` now avoids numerical underflow issues during\n  affinity matrix computation.\n\n- |Fix| :class:`manifold.Isomap` now connects disconnected components of the\n  neighbors graph along some minimum distance pairs, instead of changing\n  every infinite distances to zero.\n\n- |Fix| The splitting criterion of :class:`tree.DecisionTreeClassifier` and\n  :class:`tree.DecisionTreeRegressor` can be impacted by a fix in the handling\n  of rounding errors. Previously some extra spurious splits could occur.\n\nDetails are listed in the changelog below.\n\n(While we are trying to better inform users by providing this information, we\ncannot assure that this list is complete.)\n\n\nChangelog\n---------\n\n..\n    Entries should be grouped by module (in alphabetic order) and prefixed with\n    one of the labels: |MajorFeature|, |Feature|, |Efficiency|, |Enhancement|,\n    |Fix| or |API| (see whats_new.rst for descriptions).\n    Entries should be ordered by those labels (e.g. |Fix| after |Efficiency|).\n    Changes not specific to a module should be listed under *Multiple Modules*\n    or *Miscellaneous*.\n    Entries should end with:\n    :pr:`123456` by :user:`Joe Bloggs <joeongithub>`.\n    where 123456 is the *pull request* number, not the issue number.\n\n- |API| The option for using the squared error via ``loss`` and\n  ``criterion`` parameters was made more consistent. The preferred way is by\n  setting the value to `\"squared_error\"`. Old option names are still valid,\n  produce the same models, but are deprecated and will be removed in version\n  1.2.\n  :pr:`19310` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n  - For :class:`ensemble.ExtraTreesRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.GradientBoostingRegressor`, `loss=\"ls\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.RandomForestRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.HistGradientBoostingRegressor`, `loss=\"least_squares\"`\n    is deprecated, use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`linear_model.RANSACRegressor`, `loss=\"squared_loss\"` is\n    deprecated, use `\"squared_error\"` instead.\n\n  - For :class:`linear_model.SGDRegressor`, `loss=\"squared_loss\"` is\n    deprecated, use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`tree.DecisionTreeRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`tree.ExtraTreeRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n- |API| The option for using the absolute error via ``loss`` and\n  ``criterion`` parameters was made more consistent. The preferred way is by\n  setting the value to `\"absolute_error\"`. Old option names are still valid,\n  produce the same models, but are deprecated and will be removed in version\n  1.2.\n  :pr:`19733` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n  - For :class:`ensemble.ExtraTreesRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.GradientBoostingRegressor`, `loss=\"lad\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.RandomForestRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.HistGradientBoostingRegressor`,\n    `loss=\"least_absolute_deviation\"` is deprecated, use `\"absolute_error\"`\n    instead.\n\n  - For :class:`linear_model.RANSACRegressor`, `loss=\"absolute_loss\"` is\n    deprecated, use `\"absolute_error\"` instead which is now the default.\n\n  - For :class:`tree.DecisionTreeRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`tree.ExtraTreeRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n- |API| `np.matrix` usage is deprecated in 1.0 and will raise a `TypeError` in\n  1.2. :pr:`20165` by `Thomas Fan`_.\n\n- |API| :term:`get_feature_names_out` has been added to the transformer API\n  to get the names of the output features. :term:`get_feature_names` has in\n  turn been deprecated. :pr:`18444` by `Thomas Fan`_.\n\n- |API| All estimators store `feature_names_in_` when fitted on pandas Dataframes.\n  These feature names are compared to names seen in non-`fit` methods, e.g.\n  `transform` and will raise a `FutureWarning` if they are not consistent.\n  These ``FutureWarning`` s will become ``ValueError`` s in 1.2. :pr:`18010` by\n  `Thomas Fan`_.\n\n:mod:`sklearn.base`\n...................\n\n- |Fix| :func:`config_context` is now threadsafe. :pr:`18736` by `Thomas Fan`_.\n\n:mod:`sklearn.calibration`\n..........................\n\n- |Feature| :func:`calibration.CalibrationDisplay` added to plot\n  calibration curves. :pr:`17443` by :user:`Lucy Liu <lucyleeow>`.\n\n- |Fix| The ``predict`` and ``predict_proba`` methods of\n  :class:`calibration.CalibratedClassifierCV` can now properly be used on\n  prefitted pipelines. :pr:`19641` by :user:`Alek Lefebvre <AlekLefebvre>`.\n\n- |Fix| Fixed an error when using a :class:`ensemble.VotingClassifier`\n  as `base_estimator` in :class:`calibration.CalibratedClassifierCV`.\n  :pr:`20087` by :user:`Cl\u00e9ment Fauchereau <clement-f>`.\n\n\n:mod:`sklearn.cluster`\n......................\n\n- |Efficiency| The ``\"k-means++\"`` initialization of :class:`cluster.KMeans`\n  and :class:`cluster.MiniBatchKMeans`\u00a0is now faster, especially in multicore\n  settings. :pr:`19002` by :user:`Jon Crall <Erotemic>` and :user:`J\u00e9r\u00e9mie du\n  Boisberranger <jeremiedbb>`.\n\n- |Efficiency| :class:`cluster.KMeans` with `algorithm='elkan'` is now faster\n  in multicore settings. :pr:`19052` by\n  :user:`Yusuke Nagasaka <YusukeNagasaka>`.\n\n- |Efficiency| :class:`cluster.MiniBatchKMeans` is now faster in multicore\n  settings. :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Efficiency| :class:`cluster.OPTICS` can now cache the output of the\n  computation of the tree, using the `memory` parameter.  :pr:`19024` by\n  :user:`Frankie Robertson <frankier>`.\n\n- |Enhancement| The `predict` and `fit_predict` methods of\n  :class:`cluster.AffinityPropagation` now accept sparse data type for input\n  data.\n  :pr:`20117` by :user:`Venkatachalam Natchiappan <venkyyuvy>`\n\n- |Fix| Fixed a bug in :class:`cluster.MiniBatchKMeans` where the sample\n  weights were partially ignored when the input is sparse. :pr:`17622` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| Improved convergence detection based on center change in\n  :class:`cluster.MiniBatchKMeans` which was almost never achievable.\n  :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |FIX| :class:`cluster.AgglomerativeClustering` now supports readonly\n  memory-mapped datasets.\n  :pr:`19883` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |Fix| :class:`cluster.AgglomerativeClustering` correctly connects components\n  when connectivity and affinity are both precomputed and the number\n  of connected components is greater than 1. :pr:`20597` by\n  `Thomas Fan`_.\n\n- |Fix| :class:`cluster.FeatureAgglomeration` does not accept a ``**params`` kwarg in\n  the ``fit`` function anymore, resulting in a more concise error message. :pr:`20899`\n  by :user:`Adam Li <adam2392>`.\n\n- |Fix| Fixed a bug in :class:`cluster.KMeans`, ensuring reproducibility and equivalence\n  between sparse and dense input. :pr:`20200`\n  by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| :class:`cluster.Birch` attributes, `fit_` and `partial_fit_`, are\n  deprecated and will be removed in 1.2. :pr:`19297` by `Thomas Fan`_.\n\n- |API| the default value for the `batch_size` parameter of\n  :class:`cluster.MiniBatchKMeans` was changed from 100 to 1024 due to\n  efficiency reasons. The `n_iter_` attribute of\n  :class:`cluster.MiniBatchKMeans` now reports the number of started epochs and\n  the `n_steps_` attribute reports the number of mini batches processed.\n  :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| :func:`cluster.spectral_clustering` raises an improved error when passed\n  a `np.matrix`. :pr:`20560` by `Thomas Fan`_.\n\n:mod:`sklearn.compose`\n......................\n\n- |Enhancement| :class:`compose.ColumnTransformer` now records the output\n  of each transformer in `output_indices_`. :pr:`18393` by\n  :user:`Luca Bittarello <lbittarello>`.\n\n- |Enhancement| :class:`compose.ColumnTransformer` now allows DataFrame input to\n  have its columns appear in a changed order in `transform`. Further, columns that\n  are dropped will not be required in transform, and additional columns will be\n  ignored if `remainder='drop'`. :pr:`19263` by `Thomas Fan`_.\n\n- |Enhancement| Adds `**predict_params` keyword argument to\n  :meth:`compose.TransformedTargetRegressor.predict` that passes keyword\n  argument to the regressor.\n  :pr:`19244` by :user:`Ricardo <ricardojnf>`.\n\n- |FIX| :meth:`compose.ColumnTransformer.get_feature_names` supports\n  non-string feature names returned by any of its transformers. However, note\n  that ``get_feature_names`` is deprecated, use ``get_feature_names_out``\n  instead. :pr:`18459` by :user:`Albert Villanova del Moral <albertvillanova>`\n  and :user:`Alonso Silva Allende <alonsosilvaallende>`.\n\n- |Fix| :class:`compose.TransformedTargetRegressor` now takes nD targets with\n  an adequate transformer.\n  :pr:`18898` by :user:`Oras Phongpanagnam <panangam>`.\n\n- |API| Adds `verbose_feature_names_out` to :class:`compose.ColumnTransformer`.\n  This flag controls the prefixing of feature names out in\n  :term:`get_feature_names_out`. :pr:`18444` and :pr:`21080` by `Thomas Fan`_.\n\n:mod:`sklearn.covariance`\n.........................\n\n- |Fix| Adds arrays check to :func:`covariance.ledoit_wolf` and\n  :func:`covariance.ledoit_wolf_shrinkage`. :pr:`20416` by :user:`Hugo Defois\n  <defoishugo>`.\n\n- |API| Deprecates the following keys in `cv_results_`: `'mean_score'`,\n  `'std_score'`, and `'split(k)_score'` in favor of `'mean_test_score'`\n  `'std_test_score'`, and `'split(k)_test_score'`. :pr:`20583` by `Thomas Fan`_.\n\n:mod:`sklearn.datasets`\n.......................\n\n- |Enhancement| :func:`datasets.fetch_openml` now supports categories with\n  missing values when returning a pandas dataframe. :pr:`19365` by\n  `Thomas Fan`_ and :user:`Amanda Dsouza <amy12xx>` and\n  :user:`EL-ATEIF Sara <elateifsara>`.\n\n- |Enhancement| :func:`datasets.fetch_kddcup99` raises a better message\n  when the cached file is invalid. :pr:`19669` `Thomas Fan`_.\n\n- |Enhancement| Replace usages of ``__file__`` related to resource file I/O\n  with ``importlib.resources`` to avoid the assumption that these resource\n  files (e.g. ``iris.csv``) already exist on a filesystem, and by extension\n  to enable compatibility with tools such as ``PyOxidizer``.\n  :pr:`20297` by :user:`Jack Liu <jackzyliu>`.\n\n- |Fix| Shorten data file names in the openml tests to better support\n  installing on Windows and its default 260 character limit on file names.\n  :pr:`20209` by `Thomas Fan`_.\n\n- |Fix| :func:`datasets.fetch_kddcup99` returns dataframes when\n  `return_X_y=True` and `as_frame=True`. :pr:`19011` by `Thomas Fan`_.\n\n- |API| Deprecates :func:`datasets.load_boston` in 1.0 and it will be removed\n  in 1.2. Alternative code snippets to load similar datasets are provided.\n  Please report to the docstring of the function for details.\n  :pr:`20729` by `Guillaume Lemaitre`_.\n\n\n:mod:`sklearn.decomposition`\n............................\n\n- |Enhancement| added a new approximate solver (randomized SVD, available with\n  `eigen_solver='randomized'`) to :class:`decomposition.KernelPCA`. This\n  significantly accelerates computation when the number of samples is much\n  larger than the desired number of components.\n  :pr:`12069` by :user:`Sylvain Mari\u00e9 <smarie>`.\n\n- |Fix| Fixes incorrect multiple data-conversion warnings when clustering\n  boolean data. :pr:`19046` by :user:`Surya Prakash <jdsurya>`.\n\n- |Fix| Fixed :func:`dict_learning`, used by\n  :class:`decomposition.DictionaryLearning`, to ensure determinism of the\n  output. Achieved by flipping signs of the SVD output which is used to\n  initialize the code. :pr:`18433` by :user:`Bruno Charron <brcharron>`.\n\n- |Fix| Fixed a bug in :class:`decomposition.MiniBatchDictionaryLearning`,\n  :class:`decomposition.MiniBatchSparsePCA` and\n  :func:`decomposition.dict_learning_online` where the update of the dictionary\n  was incorrect. :pr:`19198` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| Fixed a bug in :class:`decomposition.DictionaryLearning`,\n  :class:`decomposition.SparsePCA`,\n  :class:`decomposition.MiniBatchDictionaryLearning`,\n  :class:`decomposition.MiniBatchSparsePCA`,\n  :func:`decomposition.dict_learning` and\n  :func:`decomposition.dict_learning_online` where the restart of unused atoms\n  during the dictionary update was not working as expected. :pr:`19198` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| In :class:`decomposition.DictionaryLearning`,\n  :class:`decomposition.MiniBatchDictionaryLearning`,\n  :func:`decomposition.dict_learning` and\n  :func:`decomposition.dict_learning_online`, `transform_alpha` will be equal\n  to `alpha` instead of 1.0 by default starting from version 1.2 :pr:`19159` by\n  :user:`Beno\u00eet Mal\u00e9zieux <bmalezieux>`.\n\n- |API| Rename variable names in :class:`KernelPCA` to improve\n  readability. `lambdas_` and `alphas_` are renamed to `eigenvalues_`\n  and `eigenvectors_`, respectively. `lambdas_` and `alphas_` are\n  deprecated and will be removed in 1.2.\n  :pr:`19908` by :user:`Kei Ishikawa <kstoneriv3>`.\n\n- |API| The `alpha` and `regularization` parameters of :class:`decomposition.NMF` and\n  :func:`decomposition.non_negative_factorization` are deprecated and will be removed\n  in 1.2. Use the new parameters `alpha_W` and `alpha_H` instead. :pr:`20512` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.dummy`\n....................\n\n- |API| Attribute `n_features_in_` in :class:`dummy.DummyRegressor` and\n  :class:`dummy.DummyRegressor` is deprecated and will be removed in 1.2.\n  :pr:`20960` by `Thomas Fan`_.\n\n:mod:`sklearn.ensemble`\n.......................\n\n- |Enhancement| :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` take cgroups quotas\n  into account when deciding the number of threads used by OpenMP. This\n  avoids performance problems caused by over-subscription when using those\n  classes in a docker container for instance. :pr:`20477`\n  by `Thomas Fan`_.\n\n- |Enhancement| :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` are no longer\n  experimental. They are now considered stable and are subject to the same\n  deprecation cycles as all other estimators. :pr:`19799` by `Nicolas Hug`_.\n\n- |Enhancement| Improve the HTML rendering of the\n  :class:`ensemble.StackingClassifier` and :class:`ensemble.StackingRegressor`.\n  :pr:`19564` by `Thomas Fan`_.\n\n- |Enhancement| Added Poisson criterion to\n  :class:`ensemble.RandomForestRegressor`. :pr:`19836` by :user:`Brian Sun\n  <bsun94>`.\n\n- |Fix| Do not allow to compute out-of-bag (OOB) score in\n  :class:`ensemble.RandomForestClassifier` and\n  :class:`ensemble.ExtraTreesClassifier` with multiclass-multioutput target\n  since scikit-learn does not provide any metric supporting this type of\n  target. Additional private refactoring was performed.\n  :pr:`19162` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |Fix| Improve numerical precision for weights boosting in\n  :class:`ensemble.AdaBoostClassifier` and :class:`ensemble.AdaBoostRegressor`\n  to avoid underflows.\n  :pr:`10096` by :user:`Fenil Suchak <fenilsuchak>`.\n\n- |Fix| Fixed the range of the argument ``max_samples`` to be ``(0.0, 1.0]``\n  in :class:`ensemble.RandomForestClassifier`,\n  :class:`ensemble.RandomForestRegressor`, where `max_samples=1.0` is\n  interpreted as using all `n_samples` for bootstrapping. :pr:`20159` by\n  :user:`murata-yu`.\n\n- |Fix| Fixed a bug in :class:`ensemble.AdaBoostClassifier` and\n  :class:`ensemble.AdaBoostRegressor` where the `sample_weight` parameter\n  got overwritten during `fit`.\n  :pr:`20534` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |API| Removes `tol=None` option in\n  :class:`ensemble.HistGradientBoostingClassifier` and\n  :class:`ensemble.HistGradientBoostingRegressor`. Please use `tol=0` for\n  the same behavior. :pr:`19296` by `Thomas Fan`_.\n\n:mod:`sklearn.feature_extraction`\n.................................\n\n- |Fix| Fixed a bug in :class:`feature_extraction.text.HashingVectorizer`\n  where some input strings would result in negative indices in the transformed\n  data. :pr:`19035` by :user:`Liu Yu <ly648499246>`.\n\n- |Fix| Fixed a bug in :class:`feature_extraction.DictVectorizer` by raising an\n  error with unsupported value type.\n  :pr:`19520` by :user:`Jeff Zhao <kamiyaa>`.\n\n- |Fix| Fixed a bug in :func:`feature_extraction.image.img_to_graph`\n  and :func:`feature_extraction.image.grid_to_graph` where singleton connected\n  components were not handled properly, resulting in a wrong vertex indexing.\n  :pr:`18964` by `Bertrand Thirion`_.\n\n- |Fix| Raise a warning in :class:`feature_extraction.text.CountVectorizer`\n  with `lowercase=True` when there are vocabulary entries with uppercase\n  characters to avoid silent misses in the resulting feature vectors.\n  :pr:`19401` by :user:`Zito Relova <zitorelova>`\n\n:mod:`sklearn.feature_selection`\n................................\n\n- |Feature| :func:`feature_selection.r_regression` computes Pearson's R\n  correlation coefficients between the features and the target.\n  :pr:`17169` by :user:`Dmytro Lituiev <DSLituiev>`\n  and :user:`Julien Jerphanion <jjerphan>`.\n\n- |Enhancement| :func:`feature_selection.RFE.fit` accepts additional estimator\n  parameters that are passed directly to the estimator's `fit` method.\n  :pr:`20380` by :user:`Iv\u00e1n Pulido <ijpulidos>`, :user:`Felipe Bidu <fbidu>`,\n  :user:`Gil Rutter <g-rutter>`, and :user:`Adrin Jalali <adrinjalali>`.\n\n- |FIX| Fix a bug in :func:`isotonic.isotonic_regression` where the\n  `sample_weight` passed by a user were overwritten during ``fit``.\n  :pr:`20515` by :user:`Carsten Allefeld <allefeld>`.\n\n- |Fix| Change :func:`feature_selection.SequentialFeatureSelector` to\n  allow for unsupervised modelling so that the `fit` signature need not\n  do any `y` validation and allow for `y=None`.\n  :pr:`19568` by :user:`Shyam Desai <ShyamDesai>`.\n\n- |API| Raises an error in :class:`feature_selection.VarianceThreshold`\n  when the variance threshold is negative.\n  :pr:`20207` by :user:`Tomohiro Endo <europeanplaice>`\n\n- |API| Deprecates `grid_scores_` in favor of split scores in `cv_results_` in\n  :class:`feature_selection.RFECV`. `grid_scores_` will be removed in\n  version 1.2.\n  :pr:`20161` by :user:`Shuhei Kayawari <wowry>` and :user:`arka204`.\n\n:mod:`sklearn.inspection`\n.........................\n\n- |Enhancement| Add `max_samples` parameter in\n  :func:`inspection.permutation_importance`. It enables to draw a subset of the\n  samples to compute the permutation importance. This is useful to keep the\n  method tractable when evaluating feature importance on large datasets.\n  :pr:`20431` by :user:`Oliver Pfaffel <o1iv3r>`.\n\n- |Enhancement| Add kwargs to format ICE and PD lines separately in partial\n  dependence plots :func:`inspection.plot_partial_dependence` and\n  :meth:`inspection.PartialDependenceDisplay.plot`. :pr:`19428` by :user:`Mehdi\n  Hamoumi <mhham>`.\n\n- |Fix| Allow multiple scorers input to\n  :func:`inspection.permutation_importance`. :pr:`19411` by :user:`Simona\n  Maggio <simonamaggio>`.\n\n- |API| :class:`inspection.PartialDependenceDisplay` exposes a class method:\n  :func:`~inspection.PartialDependenceDisplay.from_estimator`.\n  :func:`inspection.plot_partial_dependence` is deprecated in favor of the\n  class method and will be removed in 1.2. :pr:`20959` by `Thomas Fan`_.\n\n:mod:`sklearn.kernel_approximation`\n...................................\n\n- |Fix| Fix a bug in :class:`kernel_approximation.Nystroem`\n  where the attribute `component_indices_` did not correspond to the subset of\n  sample indices used to generate the approximated kernel. :pr:`20554` by\n  :user:`Xiangyin Kong <kxytim>`.\n\n:mod:`sklearn.linear_model`\n...........................\n\n- |Feature| Added :class:`linear_model.QuantileRegressor` which implements\n  linear quantile regression with L1 penalty.\n  :pr:`9978` by :user:`David Dale <avidale>` and\n  :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature| The new :class:`linear_model.SGDOneClassSVM` provides an SGD\n  implementation of the linear One-Class SVM. Combined with kernel\n  approximation techniques, this implementation approximates the solution of\n  a kernelized One Class SVM while benefitting from a linear\n  complexity in the number of samples.\n  :pr:`10027` by :user:`Albert Thomas <albertcthomas>`.\n\n- |Feature| Added `sample_weight` parameter to\n  :class:`linear_model.LassoCV` and :class:`linear_model.ElasticNetCV`.\n  :pr:`16449` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature| Added new solver `lbfgs` (available with `solver=\"lbfgs\"`)\n  and `positive` argument to :class:`linear_model.Ridge`. When `positive` is\n  set to `True`, forces the coefficients to be positive (only supported by\n  `lbfgs`). :pr:`20231` by :user:`Toshihiro Nakae <tnakae>`.\n\n- |Efficiency| The implementation of :class:`linear_model.LogisticRegression`\n  has been optimised for dense matrices when using `solver='newton-cg'` and\n  `multi_class!='multinomial'`.\n  :pr:`19571` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |Enhancement| `fit` method preserves dtype for numpy.float32 in\n  :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,\n  :class:`linear_model.LassoLars`, :class:`linear_model.LarsCV` and\n  :class:`linear_model.LassoLarsCV`. :pr:`20155` by :user:`Takeshi Oura\n  <takoika>`.\n\n- |Enhancement| Validate user-supplied gram matrix passed to linear models\n  via the `precompute` argument. :pr:`19004` by :user:`Adam Midvidy <amidvidy>`.\n\n- |Fix| :meth:`linear_model.ElasticNet.fit` no longer modifies `sample_weight`\n  in place. :pr:`19055` by `Thomas Fan`_.\n\n- |Fix| :class:`linear_model.Lasso` and :class:`linear_model.ElasticNet` no\n  longer have a `dual_gap_` not corresponding to their objective. :pr:`19172`\n  by :user:`Mathurin Massias <mathurinm>`\n\n- |Fix| `sample_weight` are now fully taken into account in linear models\n  when `normalize=True` for both feature centering and feature\n  scaling.\n  :pr:`19426` by :user:`Alexandre Gramfort <agramfort>` and\n  :user:`Maria Telenczuk <maikia>`.\n\n- |Fix| Points with residuals equal to  ``residual_threshold`` are now considered\n  as inliers for :class:`linear_model.RANSACRegressor`. This allows fitting\n  a model perfectly on some datasets when `residual_threshold=0`.\n  :pr:`19499` by :user:`Gregory Strubel <gregorystrubel>`.\n\n- |Fix| Sample weight invariance for :class:`linear_model.Ridge` was fixed in\n  :pr:`19616` by :user:`Oliver Grisel <ogrisel>` and :user:`Christian Lorentzen\n  <lorentzenchr>`.\n\n- |Fix| The dictionary `params` in :func:`linear_model.enet_path` and\n  :func:`linear_model.lasso_path` should only contain parameter of the\n  coordinate descent solver. Otherwise, an error will be raised.\n  :pr:`19391` by :user:`Shao Yang Hong <hongshaoyang>`.\n\n- |API| Raise a warning in :class:`linear_model.RANSACRegressor` that from\n  version 1.2, `min_samples` need to be set explicitly for models other than\n  :class:`linear_model.LinearRegression`. :pr:`19390` by :user:`Shao Yang Hong\n  <hongshaoyang>`.\n\n- |API|: The parameter ``normalize`` of :class:`linear_model.LinearRegression`\n  is deprecated and will be removed in 1.2. Motivation for this deprecation:\n  ``normalize`` parameter did not take any effect if ``fit_intercept`` was set\n  to False and therefore was deemed confusing. The behavior of the deprecated\n  ``LinearModel(normalize=True)`` can be reproduced with a\n  :class:`~sklearn.pipeline.Pipeline` with ``LinearModel`` (where\n  ``LinearModel`` is :class:`~linear_model.LinearRegression`,\n  :class:`~linear_model.Ridge`, :class:`~linear_model.RidgeClassifier`,\n  :class:`~linear_model.RidgeCV` or :class:`~linear_model.RidgeClassifierCV`)\n  as follows: ``make_pipeline(StandardScaler(with_mean=False),\n  LinearModel())``. The ``normalize`` parameter in\n  :class:`~linear_model.LinearRegression` was deprecated in :pr:`17743` by\n  :user:`Maria Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n  Same for :class:`~linear_model.Ridge`,\n  :class:`~linear_model.RidgeClassifier`, :class:`~linear_model.RidgeCV`, and\n  :class:`~linear_model.RidgeClassifierCV`, in: :pr:`17772` by :user:`Maria\n  Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`. Same for\n  :class:`~linear_model.BayesianRidge`, :class:`~linear_model.ARDRegression`\n  in: :pr:`17746` by :user:`Maria Telenczuk <maikia>`. Same for\n  :class:`~linear_model.Lasso`, :class:`~linear_model.LassoCV`,\n  :class:`~linear_model.ElasticNet`, :class:`~linear_model.ElasticNetCV`,\n  :class:`~linear_model.MultiTaskLasso`,\n  :class:`~linear_model.MultiTaskLassoCV`,\n  :class:`~linear_model.MultiTaskElasticNet`,\n  :class:`~linear_model.MultiTaskElasticNetCV`, in: :pr:`17785` by :user:`Maria\n  Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n\n- |API| The ``normalize`` parameter of\n  :class:`~linear_model.OrthogonalMatchingPursuit` and\n  :class:`~linear_model.OrthogonalMatchingPursuitCV` will default to False in\n  1.2 and will be removed in 1.4. :pr:`17750` by :user:`Maria Telenczuk\n  <maikia>` and :user:`Alexandre Gramfort <agramfort>`. Same for\n  :class:`~linear_model.Lars` :class:`~linear_model.LarsCV`\n  :class:`~linear_model.LassoLars` :class:`~linear_model.LassoLarsCV`\n  :class:`~linear_model.LassoLarsIC`, in :pr:`17769` by :user:`Maria Telenczuk\n  <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n\n- |API| Keyword validation has moved from `__init__` and `set_params` to `fit`\n  for the following estimators conforming to scikit-learn's conventions:\n  :class:`~linear_model.SGDClassifier`,\n  :class:`~linear_model.SGDRegressor`,\n  :class:`~linear_model.SGDOneClassSVM`,\n  :class:`~linear_model.PassiveAggressiveClassifier`, and\n  :class:`~linear_model.PassiveAggressiveRegressor`.\n  :pr:`20683` by `Guillaume Lemaitre`_.\n\n:mod:`sklearn.manifold`\n.......................\n\n- |Enhancement| Implement `'auto'` heuristic for the `learning_rate` in\n  :class:`manifold.TSNE`. It will become default in 1.2. The default\n  initialization will change to `pca` in 1.2. PCA initialization will\n  be scaled to have standard deviation 1e-4 in 1.2.\n  :pr:`19491` by :user:`Dmitry Kobak <dkobak>`.\n\n- |Fix| Change numerical precision to prevent underflow issues\n  during affinity matrix computation for :class:`manifold.TSNE`.\n  :pr:`19472` by :user:`Dmitry Kobak <dkobak>`.\n\n- |Fix| :class:`manifold.Isomap` now uses `scipy.sparse.csgraph.shortest_path`\n  to compute the graph shortest path. It also connects disconnected components\n  of the neighbors graph along some minimum distance pairs, instead of changing\n  every infinite distances to zero. :pr:`20531` by `Roman Yurchak`_ and `Tom\n  Dupre la Tour`_.\n\n- |Fix| Decrease the numerical default tolerance in the lobpcg call\n  in :func:`manifold.spectral_embedding` to prevent numerical instability.\n  :pr:`21194` by :user:`Andrew Knyazev <lobpcg>`.\n\n:mod:`sklearn.metrics`\n......................\n\n- |Feature| :func:`metrics.mean_pinball_loss` exposes the pinball loss for\n  quantile regression. :pr:`19415` by :user:`Xavier Dupr\u00e9 <sdpython>`\n  and :user:`Oliver Grisel <ogrisel>`.\n\n- |Feature| :func:`metrics.d2_tweedie_score` calculates the D^2 regression\n  score for Tweedie deviances with power parameter ``power``. This is a\n  generalization of the `r2_score` and can be interpreted as percentage of\n  Tweedie deviance explained.\n  :pr:`17036` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature|  :func:`metrics.mean_squared_log_error` now supports\n  `squared=False`.\n  :pr:`20326` by :user:`Uttam kumar <helper-uttam>`.\n\n- |Efficiency| Improved speed of :func:`metrics.confusion_matrix` when labels\n  are integral.\n  :pr:`9843` by :user:`Jon Crall <Erotemic>`.\n\n- |Enhancement| A fix to raise an error in :func:`metrics.hinge_loss` when\n  ``pred_decision`` is 1d whereas it is a multiclass classification or when\n  ``pred_decision`` parameter is not consistent with the ``labels`` parameter.\n  :pr:`19643` by :user:`Pierre Attard <PierreAttard>`.\n\n- |Fix| :meth:`metrics.ConfusionMatrixDisplay.plot` uses the correct max\n  for colormap. :pr:`19784` by `Thomas Fan`_.\n\n- |Fix| Samples with zero `sample_weight` values do not affect the results\n  from :func:`metrics.det_curve`, :func:`metrics.precision_recall_curve`\n  and :func:`metrics.roc_curve`.\n  :pr:`18328` by :user:`Albert Villanova del Moral <albertvillanova>` and\n  :user:`Alonso Silva Allende <alonsosilvaallende>`.\n\n- |Fix| avoid overflow in :func:`metrics.cluster.adjusted_rand_score` with\n  large amount of data. :pr:`20312` by :user:`Divyanshu Deoli\n  <divyanshudeoli>`.\n\n- |API| :class:`metrics.ConfusionMatrixDisplay` exposes two class methods\n  :func:`~metrics.ConfusionMatrixDisplay.from_estimator` and\n  :func:`~metrics.ConfusionMatrixDisplay.from_predictions` allowing to create\n  a confusion matrix plot using an estimator or the predictions.\n  :func:`metrics.plot_confusion_matrix` is deprecated in favor of these two\n  class methods and will be removed in 1.2.\n  :pr:`18543` by `Guillaume Lemaitre`_.\n\n- |API| :class:`metrics.PrecisionRecallDisplay` exposes two class methods\n  :func:`~metrics.PrecisionRecallDisplay.from_estimator` and\n  :func:`~metrics.PrecisionRecallDisplay.from_predictions` allowing to create\n  a precision-recall curve using an estimator or the predictions.\n  :func:`metrics.plot_precision_recall_curve` is deprecated in favor of these\n  two class methods and will be removed in 1.2.\n  :pr:`20552` by `Guillaume Lemaitre`_.\n\n- |API| :class:`metrics.DetCurveDisplay` exposes two class methods\n  :func:`~metrics.DetCurveDisplay.from_estimator` and\n  :func:`~metrics.DetCurveDisplay.from_predictions` allowing to create\n  a confusion matrix plot using an estimator or the predictions.\n  :func:`metrics.plot_det_curve` is deprecated in favor of these two\n  class methods and will be removed in 1.2.\n  :pr:`19278` by `Guillaume Lemaitre`_.\n\n:mod:`sklearn.mixture`\n......................\n\n- |Fix| Ensure that the best parameters are set appropriately\n  in the case of divergency for :class:`mixture.GaussianMixture` and\n  :class:`mixture.BayesianGaussianMixture`.\n  :pr:`20030` by :user:`Tingshan Liu <tliu68>` and\n  :user:`Benjamin Pedigo <bdpedigo>`.\n\n:mod:`sklearn.model_selection`\n..............................\n\n- |Feature| added :class:`model_selection.StratifiedGroupKFold`, that combines\n  :class:`model_selection.StratifiedKFold` and\n  :class:`model_selection.GroupKFold`, providing an ability to split data\n  preserving the distribution of classes in each split while keeping each\n  group within a single split.\n  :pr:`18649` by :user:`Leandro Hermida <hermidalc>` and\n  :user:`Rodion Martynov <marrodion>`.\n\n- |Enhancement| warn only once in the main process for per-split fit failures\n  in cross-validation. :pr:`20619` by :user:`Lo\u00efc Est\u00e8ve <lesteve>`\n\n- |Enhancement| The :class:`model_selection.BaseShuffleSplit` base class is\n  now public. :pr:`20056` by :user:`pabloduque0`.\n\n- |Fix| Avoid premature overflow in :func:`model_selection.train_test_split`.\n  :pr:`20904` by :user:`Tomasz Jakubek <t-jakubek>`.\n\n:mod:`sklearn.naive_bayes`\n..........................\n\n- |Fix| The `fit` and `partial_fit` methods of the discrete naive Bayes\n  classifiers (:class:`naive_bayes.BernoulliNB`,\n  :class:`naive_bayes.CategoricalNB`, :class:`naive_bayes.ComplementNB`,\n  and :class:`naive_bayes.MultinomialNB`) now correctly handle the degenerate\n  case of a single class in the training set.\n  :pr:`18925` by :user:`David Poznik <dpoznik>`.\n\n- |API| The attribute ``sigma_`` is now deprecated in\n  :class:`naive_bayes.GaussianNB` and will be removed in 1.2.\n  Use ``var_`` instead.\n  :pr:`18842` by :user:`Hong Shao Yang <hongshaoyang>`.\n\n:mod:`sklearn.neighbors`\n........................\n\n- |Enhancement| The creation of :class:`neighbors.KDTree` and\n  :class:`neighbors.BallTree` has been improved for their worst-cases time\n  complexity from :math:`\\mathcal{O}(n^2)` to :math:`\\mathcal{O}(n)`.\n  :pr:`19473` by :user:`jiefangxuanyan <jiefangxuanyan>` and\n  :user:`Julien Jerphanion <jjerphan>`.\n\n- |FIX| :class:`neighbors.DistanceMetric` subclasses now support readonly\n  memory-mapped datasets. :pr:`19883` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |FIX| :class:`neighbors.NearestNeighbors`, :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsClassifier`, :class:`neighbors.KNeighborsRegressor`\n  and :class:`neighbors.RadiusNeighborsRegressor` do not validate `weights` in\n  `__init__` and validates `weights` in `fit` instead. :pr:`20072` by\n  :user:`Juan Carlos Alfaro Jim\u00e9nez <alfaro96>`.\n\n- |API| The parameter `kwargs` of :class:`neighbors.RadiusNeighborsClassifier` is\n  deprecated and will be removed in 1.2.\n  :pr:`20842` by :user:`Juan Mart\u00edn Loyola <jmloyola>`.\n\n:mod:`sklearn.neural_network`\n.............................\n\n- |Fix| :class:`neural_network.MLPClassifier` and\n  :class:`neural_network.MLPRegressor` now correctly support continued training\n  when loading from a pickled file. :pr:`19631` by `Thomas Fan`_.\n\n:mod:`sklearn.pipeline`\n.......................\n\n- |API| The `predict_proba` and `predict_log_proba` methods of the\n  :class:`pipeline.Pipeline` now support passing prediction kwargs to the final\n  estimator. :pr:`19790` by :user:`Christopher Flynn <crflynn>`.\n\n:mod:`sklearn.preprocessing`\n............................\n\n- |Feature| The new :class:`preprocessing.SplineTransformer` is a feature\n  preprocessing tool for the generation of B-splines, parametrized by the\n  polynomial ``degree`` of the splines, number of knots ``n_knots`` and knot\n  positioning strategy ``knots``.\n  :pr:`18368` by :user:`Christian Lorentzen <lorentzenchr>`.\n  :class:`preprocessing.SplineTransformer` also supports periodic\n  splines via the ``extrapolation`` argument.\n  :pr:`19483` by :user:`Malte Londschien <mlondschien>`.\n  :class:`preprocessing.SplineTransformer` supports sample weights for\n  knot position strategy ``\"quantile\"``.\n  :pr:`20526` by :user:`Malte Londschien <mlondschien>`.\n\n- |Feature| :class:`preprocessing.OrdinalEncoder` supports passing through\n  missing values by default. :pr:`19069` by `Thomas Fan`_.\n\n- |Feature| :class:`preprocessing.OneHotEncoder` now supports\n  `handle_unknown='ignore'` and dropping categories. :pr:`19041` by\n  `Thomas Fan`_.\n\n- |Feature| :class:`preprocessing.PolynomialFeatures` now supports passing\n  a tuple to `degree`, i.e. `degree=(min_degree, max_degree)`.\n  :pr:`20250` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Efficiency| :class:`preprocessing.StandardScaler` is faster and more memory\n  efficient. :pr:`20652` by `Thomas Fan`_.\n\n- |Efficiency| Changed ``algorithm`` argument for :class:`cluster.KMeans` in\n  :class:`preprocessing.KBinsDiscretizer` from ``auto`` to ``full``.\n  :pr:`19934` by :user:`Gleb Levitskiy <GLevV>`.\n\n- |Efficiency| The implementation of `fit` for\n  :class:`preprocessing.PolynomialFeatures` transformer is now faster. This is\n  especially noticeable on large sparse input. :pr:`19734` by :user:`Fred\n  Robinson <frrad>`.\n\n- |Fix| The :func:`preprocessing.StandardScaler.inverse_transform` method\n  now raises error when the input data is 1D. :pr:`19752` by :user:`Zhehao Liu\n  <Max1993Liu>`.\n\n- |Fix| :func:`preprocessing.scale`, :class:`preprocessing.StandardScaler`\n  and similar scalers detect near-constant features to avoid scaling them to\n  very large values. This problem happens in particular when using a scaler on\n  sparse data with a constant column with sample weights, in which case\n  centering is typically disabled. :pr:`19527` by :user:`Oliver Grisel\n  <ogrisel>` and :user:`Maria Telenczuk <maikia>` and :pr:`19788` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| :meth:`preprocessing.StandardScaler.inverse_transform` now\n  correctly handles integer dtypes. :pr:`19356` by :user:`makoeppel`.\n\n- |Fix| :meth:`preprocessing.OrdinalEncoder.inverse_transform` is not\n  supporting sparse matrix and raises the appropriate error message.\n  :pr:`19879` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |Fix| The `fit` method of :class:`preprocessing.OrdinalEncoder` will not\n  raise error when `handle_unknown='ignore'` and unknown categories are given\n  to `fit`.\n  :pr:`19906` by :user:`Zhehao Liu <MaxwellLZH>`.\n\n- |Fix| Fix a regression in :class:`preprocessing.OrdinalEncoder` where large\n  Python numeric would raise an error due to overflow when casted to C type\n  (`np.float64` or `np.int64`).\n  :pr:`20727` by `Guillaume Lemaitre`_.\n\n- |Fix| :class:`preprocessing.FunctionTransformer` does not set `n_features_in_`\n  based on the input to `inverse_transform`. :pr:`20961` by `Thomas Fan`_.\n\n- |API| The `n_input_features_` attribute of\n  :class:`preprocessing.PolynomialFeatures` is deprecated in favor of\n  `n_features_in_` and will be removed in 1.2. :pr:`20240` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.svm`\n...................\n\n- |API| The parameter `**params` of :func:`svm.OneClassSVM.fit` is\n  deprecated and will be removed in 1.2.\n  :pr:`20843` by :user:`Juan Mart\u00edn Loyola <jmloyola>`.\n\n:mod:`sklearn.tree`\n...................\n\n- |Enhancement| Add `fontname` argument in :func:`tree.export_graphviz`\n  for non-English characters. :pr:`18959` by :user:`Zero <Zeroto521>`\n  and :user:`wstates <wstates>`.\n\n- |Fix| Improves compatibility of :func:`tree.plot_tree` with high DPI screens.\n  :pr:`20023` by `Thomas Fan`_.\n\n- |Fix| Fixed a bug in :class:`tree.DecisionTreeClassifier`,\n  :class:`tree.DecisionTreeRegressor` where a node could be split whereas it\n  should not have been due to incorrect handling of rounding errors.\n  :pr:`19336` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| The `n_features_` attribute of :class:`tree.DecisionTreeClassifier`,\n  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeClassifier` and\n  :class:`tree.ExtraTreeRegressor` is deprecated in favor of `n_features_in_`\n  and will be removed in 1.2. :pr:`20272` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.utils`\n....................\n\n- |Enhancement| Deprecated the default value of the `random_state=0` in\n  :func:`~sklearn.utils.extmath.randomized_svd`. Starting in 1.2,\n  the default value of `random_state` will be set to `None`.\n  :pr:`19459` by :user:`Cindy Bezuidenhout <cinbez>` and\n  :user:`Clifford Akai-Nettey<cliffordEmmanuel>`.\n\n- |Enhancement| Added helper decorator :func:`utils.metaestimators.available_if`\n  to provide flexiblity in metaestimators making methods available or\n  unavailable on the basis of state, in a more readable way.\n  :pr:`19948` by `Joel Nothman`_.\n\n- |Enhancement| :func:`utils.validation.check_is_fitted` now uses\n  ``__sklearn_is_fitted__`` if available, instead of checking for attributes\n  ending with an underscore. This also makes :class:`pipeline.Pipeline` and\n  :class:`preprocessing.FunctionTransformer` pass\n  ``check_is_fitted(estimator)``. :pr:`20657` by `Adrin Jalali`_.\n\n- |Fix| Fixed a bug in :func:`utils.sparsefuncs.mean_variance_axis` where the\n  precision of the computed variance was very poor when the real variance is\n  exactly zero. :pr:`19766` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| The docstrings of propreties that are decorated with\n  :func:`utils.deprecated` are now properly wrapped. :pr:`20385` by `Thomas\n  Fan`_.\n\n- |Fix| :func:`utils.stats._weighted_percentile` now correctly ignores\n  zero-weighted observations smaller than the smallest observation with\n  positive weight for ``percentile=0``. Affected classes are\n  :class:`dummy.DummyRegressor` for ``quantile=0`` and\n  :class:`ensemble.HuberLossFunction` and :class:`ensemble.HuberLossFunction`\n  for ``alpha=0``. :pr:`20528` by :user:`Malte Londschien <mlondschien>`.\n\n- |Fix| :func:`utils._safe_indexing` explicitly takes a dataframe copy when\n  integer indices are provided avoiding to raise a warning from Pandas. This\n  warning was previously raised in resampling utilities and functions using\n  those utilities (e.g. :func:`model_selection.train_test_split`,\n  :func:`model_selection.cross_validate`,\n  :func:`model_selection.cross_val_score`,\n  :func:`model_selection.cross_val_predict`).\n  :pr:`20673` by :user:`Joris Van den Bossche  <jorisvandenbossche>`.\n\n- |Fix| Fix a regression in :func:`utils.is_scalar_nan` where large Python\n  numbers would raise an error due to overflow in C types (`np.float64` or\n  `np.int64`).\n  :pr:`20727` by `Guillaume Lemaitre`_.\n\n- |Fix| Support for `np.matrix` is deprecated in\n  :func:`~sklearn.utils.check_array` in 1.0 and will raise a `TypeError` in\n  1.2. :pr:`20165` by `Thomas Fan`_.\n\n- |API| :func:`utils._testing.assert_warns` and\n  :func:`utils._testing.assert_warns_message` are deprecated in 1.0 and will\n  be removed in 1.2. Used `pytest.warns` context manager instead. Note that\n  these functions were not documented and part from the public API.\n  :pr:`20521` by :user:`Olivier Grisel <ogrisel>`.\n\n- |API| Fixed several bugs in :func:`utils.graph.graph_shortest_path`, which is\n  now deprecated. Use `scipy.sparse.csgraph.shortest_path` instead. :pr:`20531`\n  by `Tom Dupre la Tour`_.\n\nCode and Documentation Contributors\n-----------------------------------\n\nThanks to everyone who has contributed to the maintenance and improvement of\nthe project since version 0.24, including:\n\nAbdulelah S. Al Mesfer, Abhinav Gupta, Adam J. Stewart, Adam Li, Adam Midvidy,\nAdrian Garcia Badaracco, Adrian Sad\u0142ocha, Adrin Jalali, Agamemnon Krasoulis,\nAlberto Rubiales, Albert Thomas, Albert Villanova del Moral, Alek Lefebvre,\nAlessia Marcolini, Alexandr Fonari, Alihan Zihna, Aline Ribeiro de Almeida,\nAmanda, Amanda Dsouza, Amol Deshmukh, Ana Pessoa, Anavelyz, Andreas Mueller,\nAndrew Delong, Ashish, Ashvith Shetty, Atsushi Nukariya, Avi Gupta, Ayush\nSingh, baam, BaptBillard, Benjamin Pedigo, Bertrand Thirion, Bharat\nRaghunathan, bmalezieux, Brian Rice, Brian Sun, Bruno Charron, Bryan Chen,\nbumblebee, caherrera-meli, Carsten Allefeld, CeeThinwa, Chiara Marmo,\nchrissobel, Christian Lorentzen, Christopher Yeh, Chuliang Xiao, Cl\u00e9ment\nFauchereau, cliffordEmmanuel, Conner Shen, Connor Tann, David Dale, David Katz,\nDavid Poznik, Dimitri Papadopoulos Orfanos, Divyanshu Deoli, dmallia17,\nDmitry Kobak, DS_anas, Eduardo Jardim, EdwinWenink, EL-ATEIF Sara, Eleni\nMarkou, EricEllwanger, Eric Fiegel, Erich Schubert, Ezri-Mudde, Fatos Morina,\nFelipe Rodrigues, Felix Hafner, Fenil Suchak, flyingdutchman23, Flynn, Fortune\nUwha, Francois Berenger, Frankie Robertson, Frans Larsson, Frederick Robinson,\nfrellwan, Gabriel S Vicente, Gael Varoquaux, genvalen, Geoffrey Thomas,\ngeroldcsendes, Gleb Levitskiy, Glen, Gl\u00f2ria Maci\u00e0 Mu\u00f1oz, gregorystrubel,\ngroceryheist, Guillaume Lemaitre, guiweber, Haidar Almubarak, Hans Moritz\nG\u00fcnther, Haoyin Xu, Harris Mirza, Harry Wei, Harutaka Kawamura, Hassan\nAlsawadi, Helder Geovane Gomes de Lima, Hugo DEFOIS, Igor Ilic, Ikko Ashimine,\nIsaack Mungui, Ishaan Bhat, Ishan Mishra, Iv\u00e1n Pulido, iwhalvic, J Alexander,\nJack Liu, James Alan Preiss, James Budarz, James Lamb, Jannik, Jeff Zhao,\nJennifer Maldonado, J\u00e9r\u00e9mie du Boisberranger, Jesse Lima, Jianzhu Guo, jnboehm,\nJoel Nothman, JohanWork, John Paton, Jonathan Schneider, Jon Crall, Jon Haitz\nLegarreta Gorro\u00f1o, Joris Van den Bossche, Jos\u00e9 Manuel N\u00e1poles Duarte, Juan\nCarlos Alfaro Jim\u00e9nez, Juan Martin Loyola, Julien Jerphanion, Julio Batista\nSilva, julyrashchenko, JVM, Kadatatlu Kishore, Karen Palacio, Kei Ishikawa,\nkmatt10, kobaski, Kot271828, Kunj, KurumeYuta, kxytim, lacrosse91, LalliAcqua,\nLaveen Bagai, Leonardo Rocco, Leonardo Uieda, Leopoldo Corona, Loic Esteve,\nLSturtew, Luca Bittarello, Luccas Quadros, Lucy Jim\u00e9nez, Lucy Liu, ly648499246,\nMabu Manaileng, Manimaran, makoeppel, Marco Gorelli, Maren Westermann,\nMariangela, Maria Telenczuk, marielaraj, Martin Hirzel, Mateo Nore\u00f1a, Mathieu\nBlondel, Mathis Batoul, mathurinm, Matthew Calcote, Maxime Prieur, Maxwell,\nMehdi Hamoumi, Mehmet Ali \u00d6zer, Miao Cai, Michal Karbownik, michalkrawczyk,\nMitzi, mlondschien, Mohamed Haseeb, Mohamed Khoualed, Muhammad Jarir Kanji,\nmurata-yu, Nadim Kawwa, Nanshan Li, naozin555, Nate Parsons, Neal Fultz, Nic\nAnnau, Nicolas Hug, Nicolas Miller, Nico Stefani, Nigel Bosch, Nikita Titov,\nNodar Okroshiashvili, Norbert Preining, novaya, Ogbonna Chibuike Stephen,\nOGordon100, Oliver Pfaffel, Olivier Grisel, Oras Phongpanangam, Pablo Duque,\nPablo Ibieta-Jimenez, Patric Lacouth, Paulo S. Costa, Pawe\u0142 Olszewski, Peter\nDye, PierreAttard, Pierre-Yves Le Borgne, PranayAnchuri, Prince Canuma,\nputschblos, qdeffense, RamyaNP, ranjanikrishnan, Ray Bell, Rene Jean Corneille,\nReshama Shaikh, ricardojnf, RichardScottOZ, Rodion Martynov, Rohan Paul, Roman\nLutz, Roman Yurchak, Samuel Brice, Sandy Khosasi, Sean Benhur J, Sebastian\nFlores, Sebastian P\u00f6lsterl, Shao Yang Hong, shinehide, shinnar, shivamgargsya,\nShooter23, Shuhei Kayawari, Shyam Desai, simonamaggio, Sina Tootoonian,\nsolosilence, Steven Kolawole, Steve Stagg, Surya Prakash, swpease, Sylvain\nMari\u00e9, Takeshi Oura, Terence Honles, TFiFiE, Thomas A Caswell, Thomas J. Fan,\nTim Gates, TimotheeMathieu, Timothy Wolodzko, Tim Vink, t-jakubek, t-kusanagi,\ntliu68, Tobias Uhmann, tom1092, Tom\u00e1s Moreyra, Tom\u00e1s Ronald Hughes, Tom\nDupr\u00e9 la Tour, Tommaso Di Noto, Tomohiro Endo, TONY GEORGE, Toshihiro NAKAE,\ntsuga, Uttam kumar, vadim-ushtanit, Vangelis Gkiastas, Venkatachalam N, Vil\u00e9m\nZouhar, Vinicius Rios Fuck, Vlasovets, waijean, Whidou, xavier dupr\u00e9,\nxiaoyuchai, Yasmeen Alsaedy, yoch, Yosuke KOBAYASHI, Yu Feng, YusukeNagasaka,\nyzhenman, Zero, ZeyuSun, ZhaoweiWang, Zito, Zito Relova\n", "import numpy as np\nimport scipy.sparse as sp\nimport warnings\nfrom abc import ABCMeta, abstractmethod\n\n# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'\n# (and same for other imports)\nfrom . import _libsvm as libsvm  # type: ignore\nfrom . import _liblinear as liblinear  # type: ignore\nfrom . import _libsvm_sparse as libsvm_sparse  # type: ignore\nfrom ..base import BaseEstimator, ClassifierMixin\nfrom ..preprocessing import LabelEncoder\nfrom ..utils.multiclass import _ovr_decision_function\nfrom ..utils import check_array, check_random_state\nfrom ..utils import column_or_1d\nfrom ..utils import compute_class_weight\nfrom ..utils.metaestimators import available_if\nfrom ..utils.deprecation import deprecated\nfrom ..utils.extmath import safe_sparse_dot\nfrom ..utils.validation import check_is_fitted, _check_large_sparse\nfrom ..utils.validation import _num_samples\nfrom ..utils.validation import _check_sample_weight, check_consistent_length\nfrom ..utils.multiclass import check_classification_targets\nfrom ..exceptions import ConvergenceWarning\nfrom ..exceptions import NotFittedError\n\n\nLIBSVM_IMPL = [\"c_svc\", \"nu_svc\", \"one_class\", \"epsilon_svr\", \"nu_svr\"]\n\n\ndef _one_vs_one_coef(dual_coef, n_support, support_vectors):\n    \"\"\"Generate primal coefficients from dual coefficients\n    for the one-vs-one multi class LibSVM in the case\n    of a linear kernel.\"\"\"\n\n    # get 1vs1 weights for all n*(n-1) classifiers.\n    # this is somewhat messy.\n    # shape of dual_coef_ is nSV * (n_classes -1)\n    # see docs for details\n    n_class = dual_coef.shape[0] + 1\n\n    # XXX we could do preallocation of coef but\n    # would have to take care in the sparse case\n    coef = []\n    sv_locs = np.cumsum(np.hstack([[0], n_support]))\n    for class1 in range(n_class):\n        # SVs for class1:\n        sv1 = support_vectors[sv_locs[class1] : sv_locs[class1 + 1], :]\n        for class2 in range(class1 + 1, n_class):\n            # SVs for class1:\n            sv2 = support_vectors[sv_locs[class2] : sv_locs[class2 + 1], :]\n\n            # dual coef for class1 SVs:\n            alpha1 = dual_coef[class2 - 1, sv_locs[class1] : sv_locs[class1 + 1]]\n            # dual coef for class2 SVs:\n            alpha2 = dual_coef[class1, sv_locs[class2] : sv_locs[class2 + 1]]\n            # build weight for class1 vs class2\n\n            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))\n    return coef\n\n\nclass BaseLibSVM(BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Base class for estimators that use libsvm as backing library.\n\n    This implements support vector machine classification and regression.\n\n    Parameter documentation is in the derived `SVC` class.\n    \"\"\"\n\n    # The order of these must match the integer values in LibSVM.\n    # XXX These are actually the same in the dense case. Need to factor\n    # this out.\n    _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n\n    @abstractmethod\n    def __init__(\n        self,\n        kernel,\n        degree,\n        gamma,\n        coef0,\n        tol,\n        C,\n        nu,\n        epsilon,\n        shrinking,\n        probability,\n        cache_size,\n        class_weight,\n        verbose,\n        max_iter,\n        random_state,\n    ):\n\n        if self._impl not in LIBSVM_IMPL:\n            raise ValueError(\n                \"impl should be one of %s, %s was given\" % (LIBSVM_IMPL, self._impl)\n            )\n\n        if gamma == 0:\n            msg = (\n                \"The gamma value of 0.0 is invalid. Use 'auto' to set\"\n                \" gamma to a value of 1 / n_features.\"\n            )\n            raise ValueError(msg)\n\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.nu = nu\n        self.epsilon = epsilon\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def _more_tags(self):\n        # Used by cross_val_score.\n        return {\"pairwise\": self.kernel == \"precomputed\"}\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute `_pairwise` was deprecated in \"\n        \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def _pairwise(self):\n        # Used by cross_val_score.\n        return self.kernel == \"precomputed\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the SVM model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\\n                or (n_samples, n_samples)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples, n_samples).\n\n        y : array-like of shape (n_samples,)\n            Target values (class labels in classification, real numbers in\n            regression).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Per-sample weights. Rescale C per sample. Higher weights\n            force the classifier to put more emphasis on these points.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        If X and y are not C-ordered and contiguous arrays of np.float64 and\n        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n\n        If X is a dense array, then the other methods will not support sparse\n        matrices as input.\n        \"\"\"\n\n        rnd = check_random_state(self.random_state)\n\n        sparse = sp.isspmatrix(X)\n        if sparse and self.kernel == \"precomputed\":\n            raise TypeError(\"Sparse precomputed kernels are not supported.\")\n        self._sparse = sparse and not callable(self.kernel)\n\n        if hasattr(self, \"decision_function_shape\"):\n            if self.decision_function_shape not in (\"ovr\", \"ovo\"):\n                raise ValueError(\n                    \"decision_function_shape must be either 'ovr' or 'ovo', \"\n                    f\"got {self.decision_function_shape}.\"\n                )\n\n        if callable(self.kernel):\n            check_consistent_length(X, y)\n        else:\n            X, y = self._validate_data(\n                X,\n                y,\n                dtype=np.float64,\n                order=\"C\",\n                accept_sparse=\"csr\",\n                accept_large_sparse=False,\n            )\n\n        y = self._validate_targets(y)\n\n        sample_weight = np.asarray(\n            [] if sample_weight is None else sample_weight, dtype=np.float64\n        )\n        solver_type = LIBSVM_IMPL.index(self._impl)\n\n        # input validation\n        n_samples = _num_samples(X)\n        if solver_type != 2 and n_samples != y.shape[0]:\n            raise ValueError(\n                \"X and y have incompatible shapes.\\n\"\n                + \"X has %s samples, but y has %s.\" % (n_samples, y.shape[0])\n            )\n\n        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:\n            raise ValueError(\n                \"Precomputed matrix must be a square matrix.\"\n                \" Input is a {}x{} matrix.\".format(X.shape[0], X.shape[1])\n            )\n\n        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:\n            raise ValueError(\n                \"sample_weight and X have incompatible shapes: \"\n                \"%r vs %r\\n\"\n                \"Note: Sparse matrices cannot be indexed w/\"\n                \"boolean masks (use `indices=True` in CV).\"\n                % (sample_weight.shape, X.shape)\n            )\n\n        kernel = \"precomputed\" if callable(self.kernel) else self.kernel\n\n        if kernel == \"precomputed\":\n            # unused but needs to be a float for cython code that ignores\n            # it anyway\n            self._gamma = 0.0\n        elif isinstance(self.gamma, str):\n            if self.gamma == \"scale\":\n                # var = E[X^2] - E[X]^2 if sparse\n                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()\n                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n            elif self.gamma == \"auto\":\n                self._gamma = 1.0 / X.shape[1]\n            else:\n                raise ValueError(\n                    \"When 'gamma' is a string, it should be either 'scale' or \"\n                    \"'auto'. Got '{}' instead.\".format(self.gamma)\n                )\n        else:\n            self._gamma = self.gamma\n\n        fit = self._sparse_fit if self._sparse else self._dense_fit\n        if self.verbose:\n            print(\"[LibSVM]\", end=\"\")\n\n        seed = rnd.randint(np.iinfo(\"i\").max)\n        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n        # see comment on the other call to np.iinfo in this file\n\n        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function. Use self._intercept_ and self._dual_coef_\n        # internally.\n        self._intercept_ = self.intercept_.copy()\n        self._dual_coef_ = self.dual_coef_\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            self.intercept_ *= -1\n            self.dual_coef_ = -self.dual_coef_\n\n        return self\n\n    def _validate_targets(self, y):\n        \"\"\"Validation of y and class_weight.\n\n        Default implementation for SVR and one-class; overridden in BaseSVC.\n        \"\"\"\n        # XXX this is ugly.\n        # Regression models should not have a class_weight_ attribute.\n        self.class_weight_ = np.empty(0)\n        return column_or_1d(y, warn=True).astype(np.float64, copy=False)\n\n    def _warn_from_fit_status(self):\n        assert self.fit_status_ in (0, 1)\n        if self.fit_status_ == 1:\n            warnings.warn(\n                \"Solver terminated early (max_iter=%i).\"\n                \"  Consider pre-processing your data with\"\n                \" StandardScaler or MinMaxScaler.\"\n                % self.max_iter,\n                ConvergenceWarning,\n            )\n\n    def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        if callable(self.kernel):\n            # you must store a reference to X to compute the kernel in predict\n            # TODO: add keyword copy to copy on demand\n            self.__Xfit = X\n            X = self._compute_kernel(X)\n\n            if X.shape[0] != X.shape[1]:\n                raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")\n\n        libsvm.set_verbosity_wrap(self.verbose)\n\n        # we don't pass **self.get_params() to allow subclasses to\n        # add other parameters to __init__\n        (\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self.dual_coef_,\n            self.intercept_,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n        ) = libsvm.fit(\n            X,\n            y,\n            svm_type=solver_type,\n            sample_weight=sample_weight,\n            class_weight=self.class_weight_,\n            kernel=kernel,\n            C=self.C,\n            nu=self.nu,\n            probability=self.probability,\n            degree=self.degree,\n            shrinking=self.shrinking,\n            tol=self.tol,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            epsilon=self.epsilon,\n            max_iter=self.max_iter,\n            random_seed=random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n        X.sort_indices()\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        libsvm_sparse.set_verbosity_wrap(self.verbose)\n\n        (\n            self.support_,\n            self.support_vectors_,\n            dual_coef_data,\n            self.intercept_,\n            self._n_support,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n        ) = libsvm_sparse.libsvm_sparse_train(\n            X.shape[1],\n            X.data,\n            X.indices,\n            X.indptr,\n            y,\n            solver_type,\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            sample_weight,\n            self.nu,\n            self.cache_size,\n            self.epsilon,\n            int(self.shrinking),\n            int(self.probability),\n            self.max_iter,\n            random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n        if hasattr(self, \"classes_\"):\n            n_class = len(self.classes_) - 1\n        else:  # regression\n            n_class = 1\n        n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        if not n_SV:\n            self.dual_coef_ = sp.csr_matrix([])\n        else:\n            dual_coef_indptr = np.arange(\n                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class\n            )\n            self.dual_coef_ = sp.csr_matrix(\n                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n            )\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        predict = self._sparse_predict if self._sparse else self._dense_predict\n        return predict(X)\n\n    def _dense_predict(self, X):\n        X = self._compute_kernel(X)\n        if X.ndim == 1:\n            X = check_array(X, order=\"C\", accept_large_sparse=False)\n\n        kernel = self.kernel\n        if callable(self.kernel):\n            kernel = \"precomputed\"\n            if X.shape[1] != self.shape_fit_[0]:\n                raise ValueError(\n                    \"X.shape[1] = %d should be equal to %d, \"\n                    \"the number of samples at training time\"\n                    % (X.shape[1], self.shape_fit_[0])\n                )\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n\n        return libsvm.predict(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            cache_size=self.cache_size,\n        )\n\n    def _sparse_predict(self, X):\n        # Precondition: X is a csr_matrix of dtype np.float64.\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        C = 0.0  # C is not useful here\n\n        return libsvm_sparse.libsvm_sparse_predict(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _compute_kernel(self, X):\n        \"\"\"Return the data transformed by a callable kernel\"\"\"\n        if callable(self.kernel):\n            # in the case of precomputed kernel given as a function, we\n            # have to compute explicitly the kernel matrix\n            kernel = self.kernel(X, self.__Xfit)\n            if sp.issparse(kernel):\n                kernel = kernel.toarray()\n            X = np.asarray(kernel, dtype=np.float64, order=\"C\")\n        return X\n\n    def _decision_function(self, X):\n        \"\"\"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like of shape (n_samples, n_class * (n_class-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n        \"\"\"\n        # NOTE: _validate_for_predict contains check for is_fitted\n        # hence must be placed before any other attributes are used.\n        X = self._validate_for_predict(X)\n        X = self._compute_kernel(X)\n\n        if self._sparse:\n            dec_func = self._sparse_decision_function(X)\n        else:\n            dec_func = self._dense_decision_function(X)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function.\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            return -dec_func.ravel()\n\n        return dec_func\n\n    def _dense_decision_function(self, X):\n        X = check_array(X, dtype=np.float64, order=\"C\", accept_large_sparse=False)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        return libsvm.decision_function(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=LIBSVM_IMPL.index(self._impl),\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n    def _sparse_decision_function(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if hasattr(kernel, \"__call__\"):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_decision_function(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _validate_for_predict(self, X):\n        check_is_fitted(self)\n\n        if not callable(self.kernel):\n            X = self._validate_data(\n                X,\n                accept_sparse=\"csr\",\n                dtype=np.float64,\n                order=\"C\",\n                accept_large_sparse=False,\n                reset=False,\n            )\n\n        if self._sparse and not sp.isspmatrix(X):\n            X = sp.csr_matrix(X)\n        if self._sparse:\n            X.sort_indices()\n\n        if sp.issparse(X) and not self._sparse and not callable(self.kernel):\n            raise ValueError(\n                \"cannot use sparse input in %r trained on dense data\"\n                % type(self).__name__\n            )\n\n        if self.kernel == \"precomputed\":\n            if X.shape[1] != self.shape_fit_[0]:\n                raise ValueError(\n                    \"X.shape[1] = %d should be equal to %d, \"\n                    \"the number of samples at training time\"\n                    % (X.shape[1], self.shape_fit_[0])\n                )\n        return X\n\n    @property\n    def coef_(self):\n        \"\"\"Weights assigned to the features when `kernel=\"linear\"`.\n\n        Returns\n        -------\n        ndarray of shape (n_features, n_classes)\n        \"\"\"\n        if self.kernel != \"linear\":\n            raise AttributeError(\"coef_ is only available when using a linear kernel\")\n\n        coef = self._get_coef()\n\n        # coef_ being a read-only property, it's better to mark the value as\n        # immutable to avoid hiding potential bugs for the unsuspecting user.\n        if sp.issparse(coef):\n            # sparse matrix do not have global flags\n            coef.data.flags.writeable = False\n        else:\n            # regular dense array\n            coef.flags.writeable = False\n        return coef\n\n    def _get_coef(self):\n        return safe_sparse_dot(self._dual_coef_, self.support_vectors_)\n\n    @property\n    def n_support_(self):\n        \"\"\"Number of support vectors for each class.\"\"\"\n        try:\n            check_is_fitted(self)\n        except NotFittedError:\n            raise AttributeError\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        if svm_type in (0, 1):\n            return self._n_support\n        else:\n            # SVR and OneClass\n            # _n_support has size 2, we make it size 1\n            return np.array([self._n_support[0]])\n\n\nclass BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):\n    \"\"\"ABC for LibSVM-based classifiers.\"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        kernel,\n        degree,\n        gamma,\n        coef0,\n        tol,\n        C,\n        nu,\n        shrinking,\n        probability,\n        cache_size,\n        class_weight,\n        verbose,\n        max_iter,\n        decision_function_shape,\n        random_state,\n        break_ties,\n    ):\n        self.decision_function_shape = decision_function_shape\n        self.break_ties = break_ties\n        super().__init__(\n            kernel=kernel,\n            degree=degree,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            C=C,\n            nu=nu,\n            epsilon=0.0,\n            shrinking=shrinking,\n            probability=probability,\n            cache_size=cache_size,\n            class_weight=class_weight,\n            verbose=verbose,\n            max_iter=max_iter,\n            random_state=random_state,\n        )\n\n    def _validate_targets(self, y):\n        y_ = column_or_1d(y, warn=True)\n        check_classification_targets(y)\n        cls, y = np.unique(y_, return_inverse=True)\n        self.class_weight_ = compute_class_weight(self.class_weight, classes=cls, y=y_)\n        if len(cls) < 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % len(cls)\n            )\n\n        self.classes_ = cls\n\n        return np.asarray(y, dtype=np.float64, order=\"C\")\n\n    def decision_function(self, X):\n        \"\"\"Evaluate the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n            If decision_function_shape='ovr', the shape is (n_samples,\n            n_classes).\n\n        Notes\n        -----\n        If decision_function_shape='ovo', the function values are proportional\n        to the distance of the samples X to the separating hyperplane. If the\n        exact distances are required, divide the function values by the norm of\n        the weight vector (``coef_``). See also `this question\n        <https://stats.stackexchange.com/questions/14876/\n        interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n        If decision_function_shape='ovr', the decision function is a monotonic\n        transformation of ovo decision function.\n        \"\"\"\n        dec = self._decision_function(X)\n        if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:\n            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))\n        return dec\n\n    def predict(self, X):\n        \"\"\"Perform classification on samples in X.\n\n        For an one-class model, +1 or -1 is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples_test, n_samples_train)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Class labels for samples in X.\n        \"\"\"\n        check_is_fitted(self)\n        if self.break_ties and self.decision_function_shape == \"ovo\":\n            raise ValueError(\n                \"break_ties must be False when decision_function_shape is 'ovo'\"\n            )\n\n        if (\n            self.break_ties\n            and self.decision_function_shape == \"ovr\"\n            and len(self.classes_) > 2\n        ):\n            y = np.argmax(self.decision_function(X), axis=1)\n        else:\n            y = super().predict(X)\n        return self.classes_.take(np.asarray(y, dtype=np.intp))\n\n    # Hacky way of getting predict_proba to raise an AttributeError when\n    # probability=False using properties. Do not use this in new code; when\n    # probabilities are not available depending on a setting, introduce two\n    # estimators.\n    def _check_proba(self):\n        if not self.probability:\n            raise AttributeError(\n                \"predict_proba is not available when  probability=False\"\n            )\n        if self._impl not in (\"c_svc\", \"nu_svc\"):\n            raise AttributeError(\"predict_proba only implemented for SVC and NuSVC\")\n        return True\n\n    @available_if(_check_proba)\n    def predict_proba(self, X):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        The model need to have probability information computed at training\n        time: fit with attribute `probability` set to True.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        T : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the sample for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n\n        Notes\n        -----\n        The probability model is created using cross validation, so\n        the results can be slightly different than those obtained by\n        predict. Also, it will produce meaningless results on very small\n        datasets.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        if self.probA_.size == 0 or self.probB_.size == 0:\n            raise NotFittedError(\n                \"predict_proba is not available when fitted with probability=False\"\n            )\n        pred_proba = (\n            self._sparse_predict_proba if self._sparse else self._dense_predict_proba\n        )\n        return pred_proba(X)\n\n    @available_if(_check_proba)\n    def predict_log_proba(self, X):\n        \"\"\"Compute log probabilities of possible outcomes for samples in X.\n\n        The model need to have probability information computed at training\n        time: fit with attribute `probability` set to True.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples_test, n_samples_train)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        T : ndarray of shape (n_samples, n_classes)\n            Returns the log-probabilities of the sample for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n\n        Notes\n        -----\n        The probability model is created using cross validation, so\n        the results can be slightly different than those obtained by\n        predict. Also, it will produce meaningless results on very small\n        datasets.\n        \"\"\"\n        return np.log(self.predict_proba(X))\n\n    def _dense_predict_proba(self, X):\n        X = self._compute_kernel(X)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        pprob = libsvm.predict_proba(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n        return pprob\n\n    def _sparse_predict_proba(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_predict_proba(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _get_coef(self):\n        if self.dual_coef_.shape[0] == 1:\n            # binary classifier\n            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)\n        else:\n            # 1vs1 classifier\n            coef = _one_vs_one_coef(\n                self.dual_coef_, self._n_support, self.support_vectors_\n            )\n            if sp.issparse(coef[0]):\n                coef = sp.vstack(coef).tocsr()\n            else:\n                coef = np.vstack(coef)\n\n        return coef\n\n    @property\n    def probA_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probA\n\n    @property\n    def probB_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probB\n\n\ndef _get_liblinear_solver_type(multi_class, penalty, loss, dual):\n    \"\"\"Find the liblinear magic number for the solver.\n\n    This number depends on the values of the following attributes:\n      - multi_class\n      - penalty\n      - loss\n      - dual\n\n    The same number is also internally used by LibLinear to determine\n    which solver to use.\n    \"\"\"\n    # nested dicts containing level 1: available loss functions,\n    # level2: available penalties for the given loss function,\n    # level3: whether the dual solver is available for the specified\n    # combination of loss function and penalty\n    _solver_type_dict = {\n        \"logistic_regression\": {\"l1\": {False: 6}, \"l2\": {False: 0, True: 7}},\n        \"hinge\": {\"l2\": {True: 3}},\n        \"squared_hinge\": {\"l1\": {False: 5}, \"l2\": {False: 2, True: 1}},\n        \"epsilon_insensitive\": {\"l2\": {True: 13}},\n        \"squared_epsilon_insensitive\": {\"l2\": {False: 11, True: 12}},\n        \"crammer_singer\": 4,\n    }\n\n    if multi_class == \"crammer_singer\":\n        return _solver_type_dict[multi_class]\n    elif multi_class != \"ovr\":\n        raise ValueError(\n            \"`multi_class` must be one of `ovr`, `crammer_singer`, got %r\" % multi_class\n        )\n\n    _solver_pen = _solver_type_dict.get(loss, None)\n    if _solver_pen is None:\n        error_string = \"loss='%s' is not supported\" % loss\n    else:\n        _solver_dual = _solver_pen.get(penalty, None)\n        if _solver_dual is None:\n            error_string = (\n                \"The combination of penalty='%s' and loss='%s' is not supported\"\n                % (penalty, loss)\n            )\n        else:\n            solver_num = _solver_dual.get(dual, None)\n            if solver_num is None:\n                error_string = (\n                    \"The combination of penalty='%s' and \"\n                    \"loss='%s' are not supported when dual=%s\" % (penalty, loss, dual)\n                )\n            else:\n                return solver_num\n    raise ValueError(\n        \"Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r\"\n        % (error_string, penalty, loss, dual)\n    )\n\n\ndef _fit_liblinear(\n    X,\n    y,\n    C,\n    fit_intercept,\n    intercept_scaling,\n    class_weight,\n    penalty,\n    dual,\n    verbose,\n    max_iter,\n    tol,\n    random_state=None,\n    multi_class=\"ovr\",\n    loss=\"logistic_regression\",\n    epsilon=0.1,\n    sample_weight=None,\n):\n    \"\"\"Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.\n\n    Preprocessing is done in this function before supplying it to liblinear.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training vector, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    y : array-like of shape (n_samples,)\n        Target vector relative to X\n\n    C : float\n        Inverse of cross-validation parameter. Lower the C, the more\n        the penalization.\n\n    fit_intercept : bool\n        Whether or not to fit the intercept, that is to add a intercept\n        term to the decision function.\n\n    intercept_scaling : float\n        LibLinear internally penalizes the intercept and this term is subject\n        to regularization just like the other terms of the feature vector.\n        In order to avoid this, one should increase the intercept_scaling.\n        such that the feature vector becomes [x, intercept_scaling].\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    penalty : {'l1', 'l2'}\n        The norm of the penalty used in regularization.\n\n    dual : bool\n        Dual or primal formulation,\n\n    verbose : int\n        Set verbose to any positive number for verbosity.\n\n    max_iter : int\n        Number of iterations.\n\n    tol : float\n        Stopping condition.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n        `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`\n        optimizes a joint objective over all classes.\n        While `crammer_singer` is interesting from an theoretical perspective\n        as it is consistent it is seldom used in practice and rarely leads to\n        better accuracy and is more expensive to compute.\n        If `crammer_singer` is chosen, the options loss, penalty and dual will\n        be ignored.\n\n    loss : {'logistic_regression', 'hinge', 'squared_hinge', \\\n            'epsilon_insensitive', 'squared_epsilon_insensitive}, \\\n            default='logistic_regression'\n        The loss function used to fit the model.\n\n    epsilon : float, default=0.1\n        Epsilon parameter in the epsilon-insensitive loss function. Note\n        that the value of this parameter depends on the scale of the target\n        variable y. If unsure, set epsilon=0.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights assigned to each sample.\n\n    Returns\n    -------\n    coef_ : ndarray of shape (n_features, n_features + 1)\n        The coefficient vector got by minimizing the objective function.\n\n    intercept_ : float\n        The intercept term added to the vector.\n\n    n_iter_ : int\n        Maximum number of iterations run across all classes.\n    \"\"\"\n    if loss not in [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"]:\n        enc = LabelEncoder()\n        y_ind = enc.fit_transform(y)\n        classes_ = enc.classes_\n        if len(classes_) < 2:\n            raise ValueError(\n                \"This solver needs samples of at least 2 classes\"\n                \" in the data, but the data contains only one\"\n                \" class: %r\"\n                % classes_[0]\n            )\n\n        class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)\n    else:\n        class_weight_ = np.empty(0, dtype=np.float64)\n        y_ind = y\n    liblinear.set_verbosity_wrap(verbose)\n    rnd = check_random_state(random_state)\n    if verbose:\n        print(\"[LibLinear]\", end=\"\")\n\n    # LinearSVC breaks when intercept_scaling is <= 0\n    bias = -1.0\n    if fit_intercept:\n        if intercept_scaling <= 0:\n            raise ValueError(\n                \"Intercept scaling is %r but needs to be greater \"\n                \"than 0. To disable fitting an intercept,\"\n                \" set fit_intercept=False.\" % intercept_scaling\n            )\n        else:\n            bias = intercept_scaling\n\n    libsvm.set_verbosity_wrap(verbose)\n    libsvm_sparse.set_verbosity_wrap(verbose)\n    liblinear.set_verbosity_wrap(verbose)\n\n    # Liblinear doesn't support 64bit sparse matrix indices yet\n    if sp.issparse(X):\n        _check_large_sparse(X)\n\n    # LibLinear wants targets as doubles, even for classification\n    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()\n    y_ind = np.require(y_ind, requirements=\"W\")\n\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)\n\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n    raw_coef_, n_iter_ = liblinear.train_wrap(\n        X,\n        y_ind,\n        sp.isspmatrix(X),\n        solver_type,\n        tol,\n        bias,\n        C,\n        class_weight_,\n        max_iter,\n        rnd.randint(np.iinfo(\"i\").max),\n        epsilon,\n        sample_weight,\n    )\n    # Regarding rnd.randint(..) in the above signature:\n    # seed for srand in range [0..INT_MAX); due to limitations in Numpy\n    # on 32-bit platforms, we can't get to the UINT_MAX limit that\n    # srand supports\n    n_iter_ = max(n_iter_)\n    if n_iter_ >= max_iter:\n        warnings.warn(\n            \"Liblinear failed to converge, increase the number of iterations.\",\n            ConvergenceWarning,\n        )\n\n    if fit_intercept:\n        coef_ = raw_coef_[:, :-1]\n        intercept_ = intercept_scaling * raw_coef_[:, -1]\n    else:\n        coef_ = raw_coef_\n        intercept_ = 0.0\n\n    return coef_, intercept_, n_iter_\n", "\"\"\"\nTesting for Support Vector Machine module (sklearn.svm)\n\nTODO: remove hard coded numerical results when possible\n\"\"\"\nimport numpy as np\nimport itertools\nimport pytest\nimport re\n\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_allclose\nfrom scipy import sparse\nfrom sklearn import svm, linear_model, datasets, metrics, base\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification, make_blobs\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.utils.validation import _num_samples\nfrom sklearn.utils import shuffle\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.exceptions import NotFittedError, UndefinedMetricWarning\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# mypy error: Module 'sklearn.svm' has no attribute '_libsvm'\nfrom sklearn.svm import _libsvm  # type: ignore\n\n# toy sample\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\nY = [1, 1, 1, 2, 2, 2]\nT = [[-1, -1], [2, 2], [3, 2]]\ntrue_result = [1, 2, 2]\n\n# also load the iris dataset\niris = datasets.load_iris()\nrng = check_random_state(42)\nperm = rng.permutation(iris.target.size)\niris.data = iris.data[perm]\niris.target = iris.target[perm]\n\n\ndef test_libsvm_parameters():\n    # Test parameters on classes that make use of libsvm.\n    clf = svm.SVC(kernel=\"linear\").fit(X, Y)\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.support_, [1, 3])\n    assert_array_equal(clf.support_vectors_, (X[1], X[3]))\n    assert_array_equal(clf.intercept_, [0.0])\n    assert_array_equal(clf.predict(X), Y)\n\n\ndef test_libsvm_iris():\n    # Check consistency on dataset iris.\n\n    # shuffle the dataset so that labels are not ordered\n    for k in (\"linear\", \"rbf\"):\n        clf = svm.SVC(kernel=k).fit(iris.data, iris.target)\n        assert np.mean(clf.predict(iris.data) == iris.target) > 0.9\n        assert hasattr(clf, \"coef_\") == (k == \"linear\")\n\n    assert_array_equal(clf.classes_, np.sort(clf.classes_))\n\n    # check also the low-level API\n    model = _libsvm.fit(iris.data, iris.target.astype(np.float64))\n    pred = _libsvm.predict(iris.data, *model)\n    assert np.mean(pred == iris.target) > 0.95\n\n    model = _libsvm.fit(iris.data, iris.target.astype(np.float64), kernel=\"linear\")\n    pred = _libsvm.predict(iris.data, *model, kernel=\"linear\")\n    assert np.mean(pred == iris.target) > 0.95\n\n    pred = _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n    assert np.mean(pred == iris.target) > 0.95\n\n    # If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence\n    # we should get deterministic results (assuming that there is no other\n    # thread calling this wrapper calling `srand` concurrently).\n    pred2 = _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n    assert_array_equal(pred, pred2)\n\n\ndef test_precomputed():\n    # SVC with a precomputed kernel.\n    # We test it with a toy dataset and with iris.\n    clf = svm.SVC(kernel=\"precomputed\")\n    # Gram matrix for train data (square matrix)\n    # (we use just a linear kernel)\n    K = np.dot(X, np.array(X).T)\n    clf.fit(K, Y)\n    # Gram matrix for test data (rectangular matrix)\n    KT = np.dot(T, np.array(X).T)\n    pred = clf.predict(KT)\n    with pytest.raises(ValueError):\n        clf.predict(KT.T)\n\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.support_, [1, 3])\n    assert_array_equal(clf.intercept_, [0])\n    assert_array_almost_equal(clf.support_, [1, 3])\n    assert_array_equal(pred, true_result)\n\n    # Gram matrix for test data but compute KT[i,j]\n    # for support vectors j only.\n    KT = np.zeros_like(KT)\n    for i in range(len(T)):\n        for j in clf.support_:\n            KT[i, j] = np.dot(T[i], X[j])\n\n    pred = clf.predict(KT)\n    assert_array_equal(pred, true_result)\n\n    # same as before, but using a callable function instead of the kernel\n    # matrix. kernel is just a linear kernel\n\n    def kfunc(x, y):\n        return np.dot(x, y.T)\n\n    clf = svm.SVC(kernel=kfunc)\n    clf.fit(np.array(X), Y)\n    pred = clf.predict(T)\n\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.intercept_, [0])\n    assert_array_almost_equal(clf.support_, [1, 3])\n    assert_array_equal(pred, true_result)\n\n    # test a precomputed kernel with the iris dataset\n    # and check parameters against a linear SVC\n    clf = svm.SVC(kernel=\"precomputed\")\n    clf2 = svm.SVC(kernel=\"linear\")\n    K = np.dot(iris.data, iris.data.T)\n    clf.fit(K, iris.target)\n    clf2.fit(iris.data, iris.target)\n    pred = clf.predict(K)\n    assert_array_almost_equal(clf.support_, clf2.support_)\n    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)\n    assert_array_almost_equal(clf.intercept_, clf2.intercept_)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n    # Gram matrix for test data but compute KT[i,j]\n    # for support vectors j only.\n    K = np.zeros_like(K)\n    for i in range(len(iris.data)):\n        for j in clf.support_:\n            K[i, j] = np.dot(iris.data[i], iris.data[j])\n\n    pred = clf.predict(K)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n    clf = svm.SVC(kernel=kfunc)\n    clf.fit(iris.data, iris.target)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n\ndef test_svr():\n    # Test Support Vector Regression\n\n    diabetes = datasets.load_diabetes()\n    for clf in (\n        svm.NuSVR(kernel=\"linear\", nu=0.4, C=1.0),\n        svm.NuSVR(kernel=\"linear\", nu=0.4, C=10.0),\n        svm.SVR(kernel=\"linear\", C=10.0),\n        svm.LinearSVR(C=10.0),\n        svm.LinearSVR(C=10.0),\n    ):\n        clf.fit(diabetes.data, diabetes.target)\n        assert clf.score(diabetes.data, diabetes.target) > 0.02\n\n    # non-regression test; previously, BaseLibSVM would check that\n    # len(np.unique(y)) < 2, which must only be done for SVC\n    svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n\n\ndef test_linearsvr():\n    # check that SVR(kernel='linear') and LinearSVC() give\n    # comparable results\n    diabetes = datasets.load_diabetes()\n    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)\n    score1 = lsvr.score(diabetes.data, diabetes.target)\n\n    svr = svm.SVR(kernel=\"linear\", C=1e3).fit(diabetes.data, diabetes.target)\n    score2 = svr.score(diabetes.data, diabetes.target)\n\n    assert_allclose(np.linalg.norm(lsvr.coef_), np.linalg.norm(svr.coef_), 1, 0.0001)\n    assert_almost_equal(score1, score2, 2)\n\n\ndef test_linearsvr_fit_sampleweight():\n    # check correct result when sample_weight is 1\n    # check that SVR(kernel='linear') and LinearSVC() give\n    # comparable results\n    diabetes = datasets.load_diabetes()\n    n_samples = len(diabetes.target)\n    unit_weight = np.ones(n_samples)\n    lsvr = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target, sample_weight=unit_weight\n    )\n    score1 = lsvr.score(diabetes.data, diabetes.target)\n\n    lsvr_no_weight = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target\n    )\n    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)\n\n    assert_allclose(\n        np.linalg.norm(lsvr.coef_), np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001\n    )\n    assert_almost_equal(score1, score2, 2)\n\n    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where\n    # X = X1 repeated n1 times, X2 repeated n2 times and so forth\n    random_state = check_random_state(0)\n    random_weight = random_state.randint(0, 10, n_samples)\n    lsvr_unflat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target, sample_weight=random_weight\n    )\n    score3 = lsvr_unflat.score(\n        diabetes.data, diabetes.target, sample_weight=random_weight\n    )\n\n    X_flat = np.repeat(diabetes.data, random_weight, axis=0)\n    y_flat = np.repeat(diabetes.target, random_weight, axis=0)\n    lsvr_flat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)\n    score4 = lsvr_flat.score(X_flat, y_flat)\n\n    assert_almost_equal(score3, score4, 2)\n\n\ndef test_svr_errors():\n    X = [[0.0], [1.0]]\n    y = [0.0, 0.5]\n\n    # Bad kernel\n    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))\n    clf.fit(X, y)\n    with pytest.raises(ValueError):\n        clf.predict(X)\n\n\ndef test_oneclass():\n    # Test OneClassSVM\n    clf = svm.OneClassSVM()\n    clf.fit(X)\n    pred = clf.predict(T)\n\n    assert_array_equal(pred, [1, -1, -1])\n    assert pred.dtype == np.dtype(\"intp\")\n    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)\n    assert_array_almost_equal(clf.dual_coef_, [[0.750, 0.750, 0.750, 0.750]], decimal=3)\n    with pytest.raises(AttributeError):\n        (lambda: clf.coef_)()\n\n\ndef test_oneclass_decision_function():\n    # Test OneClassSVM decision function\n    clf = svm.OneClassSVM()\n    rnd = check_random_state(2)\n\n    # Generate train data\n    X = 0.3 * rnd.randn(100, 2)\n    X_train = np.r_[X + 2, X - 2]\n\n    # Generate some regular novel observations\n    X = 0.3 * rnd.randn(20, 2)\n    X_test = np.r_[X + 2, X - 2]\n    # Generate some abnormal novel observations\n    X_outliers = rnd.uniform(low=-4, high=4, size=(20, 2))\n\n    # fit the model\n    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n    clf.fit(X_train)\n\n    # predict things\n    y_pred_test = clf.predict(X_test)\n    assert np.mean(y_pred_test == 1) > 0.9\n    y_pred_outliers = clf.predict(X_outliers)\n    assert np.mean(y_pred_outliers == -1) > 0.9\n    dec_func_test = clf.decision_function(X_test)\n    assert_array_equal((dec_func_test > 0).ravel(), y_pred_test == 1)\n    dec_func_outliers = clf.decision_function(X_outliers)\n    assert_array_equal((dec_func_outliers > 0).ravel(), y_pred_outliers == 1)\n\n\ndef test_oneclass_score_samples():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = svm.OneClassSVM(gamma=1).fit(X_train)\n    assert_array_equal(\n        clf.score_samples([[2.0, 2.0]]),\n        clf.decision_function([[2.0, 2.0]]) + clf.offset_,\n    )\n\n\n# TODO: Remove in v1.2\ndef test_oneclass_fit_params_is_deprecated():\n    clf = svm.OneClassSVM()\n    params = {\n        \"unused_param\": \"\",\n        \"extra_param\": None,\n    }\n    msg = (\n        \"Passing additional keyword parameters has no effect and is deprecated \"\n        \"in 1.0. An error will be raised from 1.2 and beyond. The ignored \"\n        f\"keyword parameter(s) are: {params.keys()}.\"\n    )\n    with pytest.warns(FutureWarning, match=re.escape(msg)):\n        clf.fit(X, **params)\n\n\ndef test_tweak_params():\n    # Make sure some tweaking of parameters works.\n    # We change clf.dual_coef_ at run time and expect .predict() to change\n    # accordingly. Notice that this is not trivial since it involves a lot\n    # of C/Python copying in the libsvm bindings.\n    # The success of this test ensures that the mapping between libsvm and\n    # the python classifier is complete.\n    clf = svm.SVC(kernel=\"linear\", C=1.0)\n    clf.fit(X, Y)\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.predict([[-0.1, -0.1]]), [1])\n    clf._dual_coef_ = np.array([[0.0, 1.0]])\n    assert_array_equal(clf.predict([[-0.1, -0.1]]), [2])\n\n\ndef test_probability():\n    # Predict probabilities using SVC\n    # This uses cross validation, so we use a slightly bigger testing set.\n\n    for clf in (\n        svm.SVC(probability=True, random_state=0, C=1.0),\n        svm.NuSVC(probability=True, random_state=0),\n    ):\n        clf.fit(iris.data, iris.target)\n\n        prob_predict = clf.predict_proba(iris.data)\n        assert_array_almost_equal(np.sum(prob_predict, 1), np.ones(iris.data.shape[0]))\n        assert np.mean(np.argmax(prob_predict, 1) == clf.predict(iris.data)) > 0.9\n\n        assert_almost_equal(\n            clf.predict_proba(iris.data), np.exp(clf.predict_log_proba(iris.data)), 8\n        )\n\n\ndef test_decision_function():\n    # Test decision_function\n    # Sanity check, test that decision_function implemented in python\n    # returns the same as the one in libsvm\n    # multi class:\n    clf = svm.SVC(kernel=\"linear\", C=0.1, decision_function_shape=\"ovo\").fit(\n        iris.data, iris.target\n    )\n\n    dec = np.dot(iris.data, clf.coef_.T) + clf.intercept_\n\n    assert_array_almost_equal(dec, clf.decision_function(iris.data))\n\n    # binary:\n    clf.fit(X, Y)\n    dec = np.dot(X, clf.coef_.T) + clf.intercept_\n    prediction = clf.predict(X)\n    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))\n    assert_array_almost_equal(\n        prediction, clf.classes_[(clf.decision_function(X) > 0).astype(int)]\n    )\n    expected = np.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])\n    assert_array_almost_equal(clf.decision_function(X), expected, 2)\n\n    # kernel binary:\n    clf = svm.SVC(kernel=\"rbf\", gamma=1, decision_function_shape=\"ovo\")\n    clf.fit(X, Y)\n\n    rbfs = rbf_kernel(X, clf.support_vectors_, gamma=clf.gamma)\n    dec = np.dot(rbfs, clf.dual_coef_.T) + clf.intercept_\n    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))\n\n\n@pytest.mark.parametrize(\"SVM\", (svm.SVC, svm.NuSVC))\ndef test_decision_function_shape(SVM):\n    # check that decision_function_shape='ovr' or 'ovo' gives\n    # correct shape and is consistent with predict\n\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(\n        iris.data, iris.target\n    )\n    dec = clf.decision_function(iris.data)\n    assert dec.shape == (len(iris.data), 3)\n    assert_array_equal(clf.predict(iris.data), np.argmax(dec, axis=1))\n\n    # with five classes:\n    X, y = make_blobs(n_samples=80, centers=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(X_train, y_train)\n    dec = clf.decision_function(X_test)\n    assert dec.shape == (len(X_test), 5)\n    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))\n\n    # check shape of ovo_decition_function=True\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovo\").fit(X_train, y_train)\n    dec = clf.decision_function(X_train)\n    assert dec.shape == (len(X_train), 10)\n\n    with pytest.raises(ValueError, match=\"must be either 'ovr' or 'ovo'\"):\n        SVM(decision_function_shape=\"bad\").fit(X_train, y_train)\n\n\ndef test_svr_predict():\n    # Test SVR's decision_function\n    # Sanity check, test that predict implemented in python\n    # returns the same as the one in libsvm\n\n    X = iris.data\n    y = iris.target\n\n    # linear kernel\n    reg = svm.SVR(kernel=\"linear\", C=0.1).fit(X, y)\n\n    dec = np.dot(X, reg.coef_.T) + reg.intercept_\n    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())\n\n    # rbf kernel\n    reg = svm.SVR(kernel=\"rbf\", gamma=1).fit(X, y)\n\n    rbfs = rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)\n    dec = np.dot(rbfs, reg.dual_coef_.T) + reg.intercept_\n    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())\n\n\ndef test_weight():\n    # Test class weights\n    clf = svm.SVC(class_weight={1: 0.1})\n    # we give a small weights to class 1\n    clf.fit(X, Y)\n    # so all predicted values belong to class 2\n    assert_array_almost_equal(clf.predict(X), [2] * 6)\n\n    X_, y_ = make_classification(\n        n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2\n    )\n\n    for clf in (\n        linear_model.LogisticRegression(),\n        svm.LinearSVC(random_state=0),\n        svm.SVC(),\n    ):\n        clf.set_params(class_weight={0: 0.1, 1: 10})\n        clf.fit(X_[:100], y_[:100])\n        y_pred = clf.predict(X_[100:])\n        assert f1_score(y_[100:], y_pred) > 0.3\n\n\n@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])\ndef test_svm_classifier_sided_sample_weight(estimator):\n    # fit a linear SVM and check that giving more weight to opposed samples\n    # in the space will flip the decision toward these samples.\n    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]\n    estimator.set_params(kernel=\"linear\")\n\n    # check that with unit weights, a sample is supposed to be predicted on\n    # the boundary\n    sample_weight = [1] * 6\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred == pytest.approx(0)\n\n    # give more weights to opposed samples\n    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred < 0\n\n    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred > 0\n\n\n@pytest.mark.parametrize(\"estimator\", [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)])\ndef test_svm_regressor_sided_sample_weight(estimator):\n    # similar test to test_svm_classifier_sided_sample_weight but for\n    # SVM regressors\n    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]\n    estimator.set_params(kernel=\"linear\")\n\n    # check that with unit weights, a sample is supposed to be predicted on\n    # the boundary\n    sample_weight = [1] * 6\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred == pytest.approx(1.5)\n\n    # give more weights to opposed samples\n    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred < 1.5\n\n    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred > 1.5\n\n\ndef test_svm_equivalence_sample_weight_C():\n    # test that rescaling all samples is the same as changing C\n    clf = svm.SVC()\n    clf.fit(X, Y)\n    dual_coef_no_weight = clf.dual_coef_\n    clf.set_params(C=100)\n    clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))\n    assert_allclose(dual_coef_no_weight, clf.dual_coef_)\n\n\n@pytest.mark.parametrize(\n    \"Estimator, err_msg\",\n    [\n        (svm.SVC, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.NuSVC, \"(negative dimensions are not allowed|nu is infeasible)\"),\n        (svm.SVR, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.NuSVR, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.OneClassSVM, \"Invalid input - all samples have zero or negative weights.\"),\n    ],\n    ids=[\"SVC\", \"NuSVC\", \"SVR\", \"NuSVR\", \"OneClassSVM\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[0] * len(Y), [-0.3] * len(Y)],\n    ids=[\"weights-are-zero\", \"weights-are-negative\"],\n)\ndef test_negative_sample_weights_mask_all_samples(Estimator, err_msg, sample_weight):\n    est = Estimator(kernel=\"linear\")\n    with pytest.raises(ValueError, match=err_msg):\n        est.fit(X, Y, sample_weight=sample_weight)\n\n\n@pytest.mark.parametrize(\n    \"Classifier, err_msg\",\n    [\n        (\n            svm.SVC,\n            \"Invalid input - all samples with positive weights have the same label\",\n        ),\n        (svm.NuSVC, \"specified nu is infeasible\"),\n    ],\n    ids=[\"SVC\", \"NuSVC\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[0, -0.5, 0, 1, 1, 1], [1, 1, 1, 0, -0.1, -0.3]],\n    ids=[\"mask-label-1\", \"mask-label-2\"],\n)\ndef test_negative_weights_svc_leave_just_one_label(Classifier, err_msg, sample_weight):\n    clf = Classifier(kernel=\"linear\")\n    with pytest.raises(ValueError, match=err_msg):\n        clf.fit(X, Y, sample_weight=sample_weight)\n\n\n@pytest.mark.parametrize(\n    \"Classifier, model\",\n    [\n        (svm.SVC, {\"when-left\": [0.3998, 0.4], \"when-right\": [0.4, 0.3999]}),\n        (svm.NuSVC, {\"when-left\": [0.3333, 0.3333], \"when-right\": [0.3333, 0.3333]}),\n    ],\n    ids=[\"SVC\", \"NuSVC\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight, mask_side\",\n    [([1, -0.5, 1, 1, 1, 1], \"when-left\"), ([1, 1, 1, 0, 1, 1], \"when-right\")],\n    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],\n)\ndef test_negative_weights_svc_leave_two_labels(\n    Classifier, model, sample_weight, mask_side\n):\n    clf = Classifier(kernel=\"linear\")\n    clf.fit(X, Y, sample_weight=sample_weight)\n    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)\n\n\n@pytest.mark.parametrize(\n    \"Estimator\", [svm.SVC, svm.NuSVC, svm.NuSVR], ids=[\"SVC\", \"NuSVC\", \"NuSVR\"]\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],\n    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],\n)\ndef test_negative_weight_equal_coeffs(Estimator, sample_weight):\n    # model generates equal coefficients\n    est = Estimator(kernel=\"linear\")\n    est.fit(X, Y, sample_weight=sample_weight)\n    coef = np.abs(est.coef_).ravel()\n    assert coef[0] == pytest.approx(coef[1], rel=1e-3)\n\n\n@ignore_warnings(category=UndefinedMetricWarning)\ndef test_auto_weight():\n    # Test class weights for imbalanced data\n    from sklearn.linear_model import LogisticRegression\n\n    # We take as dataset the two-dimensional projection of iris so\n    # that it is not separable and remove half of predictors from\n    # class 1.\n    # We add one to the targets as a non-regression test:\n    # class_weight=\"balanced\"\n    # used to work only when the labels where a range [0..K).\n    from sklearn.utils import compute_class_weight\n\n    X, y = iris.data[:, :2], iris.target + 1\n    unbalanced = np.delete(np.arange(y.size), np.where(y > 2)[0][::2])\n\n    classes = np.unique(y[unbalanced])\n    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y[unbalanced])\n    assert np.argmax(class_weights) == 2\n\n    for clf in (\n        svm.SVC(kernel=\"linear\"),\n        svm.LinearSVC(random_state=0),\n        LogisticRegression(),\n    ):\n        # check that score is better when class='balanced' is set.\n        y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)\n        clf.set_params(class_weight=\"balanced\")\n        y_pred_balanced = clf.fit(\n            X[unbalanced],\n            y[unbalanced],\n        ).predict(X)\n        assert metrics.f1_score(y, y_pred, average=\"macro\") <= metrics.f1_score(\n            y, y_pred_balanced, average=\"macro\"\n        )\n\n\ndef test_bad_input():\n    # Test that it gives proper exception on deficient input\n    # impossible value of C\n    with pytest.raises(ValueError):\n        svm.SVC(C=-1).fit(X, Y)\n\n    # impossible value of nu\n    clf = svm.NuSVC(nu=0.0)\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    Y2 = Y[:-1]  # wrong dimensions for labels\n    with pytest.raises(ValueError):\n        clf.fit(X, Y2)\n\n    # Test with arrays that are non-contiguous.\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\n        Xf = np.asfortranarray(X)\n        assert not Xf.flags[\"C_CONTIGUOUS\"]\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\n        yf = yf[:, -1]\n        assert not yf.flags[\"F_CONTIGUOUS\"]\n        assert not yf.flags[\"C_CONTIGUOUS\"]\n        clf.fit(Xf, yf)\n        assert_array_equal(clf.predict(T), true_result)\n\n    # error for precomputed kernelsx\n    clf = svm.SVC(kernel=\"precomputed\")\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    # predict with sparse input when trained with dense\n    clf = svm.SVC().fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(sparse.lil_matrix(X))\n\n    Xt = np.array(X).T\n    clf.fit(np.dot(X, Xt), Y)\n    with pytest.raises(ValueError):\n        clf.predict(X)\n\n    clf = svm.SVC()\n    clf.fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(Xt)\n\n\n@pytest.mark.parametrize(\n    \"Estimator, data\",\n    [\n        (svm.SVC, datasets.load_iris(return_X_y=True)),\n        (svm.NuSVC, datasets.load_iris(return_X_y=True)),\n        (svm.SVR, datasets.load_diabetes(return_X_y=True)),\n        (svm.NuSVR, datasets.load_diabetes(return_X_y=True)),\n        (svm.OneClassSVM, datasets.load_iris(return_X_y=True)),\n    ],\n)\ndef test_svm_gamma_error(Estimator, data):\n    X, y = data\n    est = Estimator(gamma=\"auto_deprecated\")\n    err_msg = \"When 'gamma' is a string, it should be either 'scale' or 'auto'\"\n    with pytest.raises(ValueError, match=err_msg):\n        est.fit(X, y)\n\n\ndef test_unicode_kernel():\n    # Test that a unicode kernel name does not cause a TypeError\n    clf = svm.SVC(kernel=\"linear\", probability=True)\n    clf.fit(X, Y)\n    clf.predict_proba(T)\n    _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n\n\ndef test_sparse_precomputed():\n    clf = svm.SVC(kernel=\"precomputed\")\n    sparse_gram = sparse.csr_matrix([[1, 0], [0, 1]])\n    with pytest.raises(TypeError, match=\"Sparse precomputed\"):\n        clf.fit(sparse_gram, [0, 1])\n\n\ndef test_sparse_fit_support_vectors_empty():\n    # Regression test for #14893\n    X_train = sparse.csr_matrix(\n        [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    y_train = np.array([0.04, 0.04, 0.10, 0.16])\n    model = svm.SVR(kernel=\"linear\")\n    model.fit(X_train, y_train)\n    assert not model.support_vectors_.data.size\n    assert not model.dual_coef_.data.size\n\n\ndef test_linearsvc_parameters():\n    # Test possible parameter combinations in LinearSVC\n    # Generate list of possible parameter combinations\n    losses = [\"hinge\", \"squared_hinge\", \"logistic_regression\", \"foo\"]\n    penalties, duals = [\"l1\", \"l2\", \"bar\"], [True, False]\n\n    X, y = make_classification(n_samples=5, n_features=5)\n\n    for loss, penalty, dual in itertools.product(losses, penalties, duals):\n        clf = svm.LinearSVC(penalty=penalty, loss=loss, dual=dual)\n        if (\n            (loss, penalty) == (\"hinge\", \"l1\")\n            or (loss, penalty, dual) == (\"hinge\", \"l2\", False)\n            or (penalty, dual) == (\"l1\", True)\n            or loss == \"foo\"\n            or penalty == \"bar\"\n        ):\n\n            with pytest.raises(\n                ValueError,\n                match=\"Unsupported set of arguments.*penalty='%s.*loss='%s.*dual=%s\"\n                % (penalty, loss, dual),\n            ):\n                clf.fit(X, y)\n        else:\n            clf.fit(X, y)\n\n    # Incorrect loss value - test if explicit error message is raised\n    with pytest.raises(ValueError, match=\".*loss='l3' is not supported.*\"):\n        svm.LinearSVC(loss=\"l3\").fit(X, y)\n\n\ndef test_linear_svx_uppercase_loss_penality_raises_error():\n    # Check if Upper case notation raises error at _fit_liblinear\n    # which is called by fit\n\n    X, y = [[0.0], [1.0]], [0, 1]\n\n    msg = \"loss='SQuared_hinge' is not supported\"\n    with pytest.raises(ValueError, match=msg):\n        svm.LinearSVC(loss=\"SQuared_hinge\").fit(X, y)\n\n    msg = \"The combination of penalty='L2' and loss='squared_hinge' is not supported\"\n    with pytest.raises(ValueError, match=msg):\n        svm.LinearSVC(penalty=\"L2\").fit(X, y)\n\n\ndef test_linearsvc():\n    # Test basic routines using LinearSVC\n    clf = svm.LinearSVC(random_state=0).fit(X, Y)\n\n    # by default should have intercept\n    assert clf.fit_intercept\n\n    assert_array_equal(clf.predict(T), true_result)\n    assert_array_almost_equal(clf.intercept_, [0], decimal=3)\n\n    # the same with l1 penalty\n    clf = svm.LinearSVC(\n        penalty=\"l1\", loss=\"squared_hinge\", dual=False, random_state=0\n    ).fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # l2 penalty with dual formulation\n    clf = svm.LinearSVC(penalty=\"l2\", dual=True, random_state=0).fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # l2 penalty, l1 loss\n    clf = svm.LinearSVC(penalty=\"l2\", loss=\"hinge\", dual=True, random_state=0)\n    clf.fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # test also decision function\n    dec = clf.decision_function(T)\n    res = (dec > 0).astype(int) + 1\n    assert_array_equal(res, true_result)\n\n\ndef test_linearsvc_crammer_singer():\n    # Test LinearSVC with crammer_singer multi-class svm\n    ovr_clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)\n    cs_clf = svm.LinearSVC(multi_class=\"crammer_singer\", random_state=0)\n    cs_clf.fit(iris.data, iris.target)\n\n    # similar prediction for ovr and crammer-singer:\n    assert (ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > 0.9\n\n    # classifiers shouldn't be the same\n    assert (ovr_clf.coef_ != cs_clf.coef_).all()\n\n    # test decision function\n    assert_array_equal(\n        cs_clf.predict(iris.data),\n        np.argmax(cs_clf.decision_function(iris.data), axis=1),\n    )\n    dec_func = np.dot(iris.data, cs_clf.coef_.T) + cs_clf.intercept_\n    assert_array_almost_equal(dec_func, cs_clf.decision_function(iris.data))\n\n\ndef test_linearsvc_fit_sampleweight():\n    # check correct result when sample_weight is 1\n    n_samples = len(X)\n    unit_weight = np.ones(n_samples)\n    clf = svm.LinearSVC(random_state=0).fit(X, Y)\n    clf_unitweight = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X, Y, sample_weight=unit_weight\n    )\n\n    # check if same as sample_weight=None\n    assert_array_equal(clf_unitweight.predict(T), clf.predict(T))\n    assert_allclose(clf.coef_, clf_unitweight.coef_, 1, 0.0001)\n\n    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where\n    # X = X1 repeated n1 times, X2 repeated n2 times and so forth\n\n    random_state = check_random_state(0)\n    random_weight = random_state.randint(0, 10, n_samples)\n    lsvc_unflat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X, Y, sample_weight=random_weight\n    )\n    pred1 = lsvc_unflat.predict(T)\n\n    X_flat = np.repeat(X, random_weight, axis=0)\n    y_flat = np.repeat(Y, random_weight, axis=0)\n    lsvc_flat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X_flat, y_flat\n    )\n    pred2 = lsvc_flat.predict(T)\n\n    assert_array_equal(pred1, pred2)\n    assert_allclose(lsvc_unflat.coef_, lsvc_flat.coef_, 1, 0.0001)\n\n\ndef test_crammer_singer_binary():\n    # Test Crammer-Singer formulation in the binary case\n    X, y = make_classification(n_classes=2, random_state=0)\n\n    for fit_intercept in (True, False):\n        acc = (\n            svm.LinearSVC(\n                fit_intercept=fit_intercept,\n                multi_class=\"crammer_singer\",\n                random_state=0,\n            )\n            .fit(X, y)\n            .score(X, y)\n        )\n        assert acc > 0.9\n\n\ndef test_linearsvc_iris():\n    # Test that LinearSVC gives plausible predictions on the iris dataset\n    # Also, test symbolic class names (classes_).\n    target = iris.target_names[iris.target]\n    clf = svm.LinearSVC(random_state=0).fit(iris.data, target)\n    assert set(clf.classes_) == set(iris.target_names)\n    assert np.mean(clf.predict(iris.data) == target) > 0.8\n\n    dec = clf.decision_function(iris.data)\n    pred = iris.target_names[np.argmax(dec, 1)]\n    assert_array_equal(pred, clf.predict(iris.data))\n\n\ndef test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):\n    # Test that dense liblinear honours intercept_scaling param\n    X = [[2, 1], [3, 1], [1, 3], [2, 3]]\n    y = [0, 0, 1, 1]\n    clf = classifier(\n        fit_intercept=True,\n        penalty=\"l1\",\n        loss=\"squared_hinge\",\n        dual=False,\n        C=4,\n        tol=1e-7,\n        random_state=0,\n    )\n    assert clf.intercept_scaling == 1, clf.intercept_scaling\n    assert clf.fit_intercept\n\n    # when intercept_scaling is low the intercept value is highly \"penalized\"\n    # by regularization\n    clf.intercept_scaling = 1\n    clf.fit(X, y)\n    assert_almost_equal(clf.intercept_, 0, decimal=5)\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # is not affected by regularization\n    clf.intercept_scaling = 100\n    clf.fit(X, y)\n    intercept1 = clf.intercept_\n    assert intercept1 < -1\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # doesn't depend on intercept_scaling value\n    clf.intercept_scaling = 1000\n    clf.fit(X, y)\n    intercept2 = clf.intercept_\n    assert_array_almost_equal(intercept1, intercept2, decimal=2)\n\n\ndef test_liblinear_set_coef():\n    # multi-class case\n    clf = svm.LinearSVC().fit(iris.data, iris.target)\n    values = clf.decision_function(iris.data)\n    clf.coef_ = clf.coef_.copy()\n    clf.intercept_ = clf.intercept_.copy()\n    values2 = clf.decision_function(iris.data)\n    assert_array_almost_equal(values, values2)\n\n    # binary-class case\n    X = [[2, 1], [3, 1], [1, 3], [2, 3]]\n    y = [0, 0, 1, 1]\n\n    clf = svm.LinearSVC().fit(X, y)\n    values = clf.decision_function(X)\n    clf.coef_ = clf.coef_.copy()\n    clf.intercept_ = clf.intercept_.copy()\n    values2 = clf.decision_function(X)\n    assert_array_equal(values, values2)\n\n\ndef test_immutable_coef_property():\n    # Check that primal coef modification are not silently ignored\n    svms = [\n        svm.SVC(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.NuSVC(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.SVR(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.NuSVR(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.OneClassSVM(kernel=\"linear\").fit(iris.data),\n    ]\n    for clf in svms:\n        with pytest.raises(AttributeError):\n            clf.__setattr__(\"coef_\", np.arange(3))\n        with pytest.raises((RuntimeError, ValueError)):\n            clf.coef_.__setitem__((0, 0), 0)\n\n\ndef test_linearsvc_verbose():\n    # stdout: redirect\n    import os\n\n    stdout = os.dup(1)  # save original stdout\n    os.dup2(os.pipe()[1], 1)  # replace it\n\n    # actual call\n    clf = svm.LinearSVC(verbose=1)\n    clf.fit(X, Y)\n\n    # stdout: restore\n    os.dup2(stdout, 1)  # restore original stdout\n\n\ndef test_svc_clone_with_callable_kernel():\n    # create SVM with callable linear kernel, check that results are the same\n    # as with built-in linear kernel\n    svm_callable = svm.SVC(\n        kernel=lambda x, y: np.dot(x, y.T),\n        probability=True,\n        random_state=0,\n        decision_function_shape=\"ovr\",\n    )\n    # clone for checking clonability with lambda functions..\n    svm_cloned = base.clone(svm_callable)\n    svm_cloned.fit(iris.data, iris.target)\n\n    svm_builtin = svm.SVC(\n        kernel=\"linear\", probability=True, random_state=0, decision_function_shape=\"ovr\"\n    )\n    svm_builtin.fit(iris.data, iris.target)\n\n    assert_array_almost_equal(svm_cloned.dual_coef_, svm_builtin.dual_coef_)\n    assert_array_almost_equal(svm_cloned.intercept_, svm_builtin.intercept_)\n    assert_array_equal(svm_cloned.predict(iris.data), svm_builtin.predict(iris.data))\n\n    assert_array_almost_equal(\n        svm_cloned.predict_proba(iris.data),\n        svm_builtin.predict_proba(iris.data),\n        decimal=4,\n    )\n    assert_array_almost_equal(\n        svm_cloned.decision_function(iris.data),\n        svm_builtin.decision_function(iris.data),\n    )\n\n\ndef test_svc_bad_kernel():\n    svc = svm.SVC(kernel=lambda x, y: x)\n    with pytest.raises(ValueError):\n        svc.fit(X, Y)\n\n\ndef test_timeout():\n    a = svm.SVC(\n        kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, max_iter=1\n    )\n    warning_msg = (\n        r\"Solver terminated early \\(max_iter=1\\).  Consider pre-processing \"\n        r\"your data with StandardScaler or MinMaxScaler.\"\n    )\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        a.fit(np.array(X), Y)\n\n\ndef test_unfitted():\n    X = \"foo!\"  # input validation not required when SVM not fitted\n\n    clf = svm.SVC()\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)\n\n    clf = svm.NuSVR()\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)\n\n\n# ignore convergence warnings from max_iter=1\n@ignore_warnings\ndef test_consistent_proba():\n    a = svm.SVC(probability=True, max_iter=1, random_state=0)\n    proba_1 = a.fit(X, Y).predict_proba(X)\n    a = svm.SVC(probability=True, max_iter=1, random_state=0)\n    proba_2 = a.fit(X, Y).predict_proba(X)\n    assert_array_almost_equal(proba_1, proba_2)\n\n\ndef test_linear_svm_convergence_warnings():\n    # Test that warnings are raised if model does not converge\n\n    lsvc = svm.LinearSVC(random_state=0, max_iter=2)\n    warning_msg = \"Liblinear failed to converge, increase the number of iterations.\"\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        lsvc.fit(X, Y)\n    assert lsvc.n_iter_ == 2\n\n    lsvr = svm.LinearSVR(random_state=0, max_iter=2)\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        lsvr.fit(iris.data, iris.target)\n    assert lsvr.n_iter_ == 2\n\n\ndef test_svr_coef_sign():\n    # Test that SVR(kernel=\"linear\") has coef_ with the right sign.\n    # Non-regression test for #2933.\n    X = np.random.RandomState(21).randn(10, 3)\n    y = np.random.RandomState(12).randn(10)\n\n    for svr in [svm.SVR(kernel=\"linear\"), svm.NuSVR(kernel=\"linear\"), svm.LinearSVR()]:\n        svr.fit(X, y)\n        assert_array_almost_equal(\n            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_\n        )\n\n\ndef test_linear_svc_intercept_scaling():\n    # Test that the right error message is thrown when intercept_scaling <= 0\n\n    for i in [-1, 0]:\n        lsvc = svm.LinearSVC(intercept_scaling=i)\n\n        msg = (\n            \"Intercept scaling is %r but needs to be greater than 0.\"\n            \" To disable fitting an intercept,\"\n            \" set fit_intercept=False.\"\n            % lsvc.intercept_scaling\n        )\n        with pytest.raises(ValueError, match=msg):\n            lsvc.fit(X, Y)\n\n\ndef test_lsvc_intercept_scaling_zero():\n    # Test that intercept_scaling is ignored when fit_intercept is False\n\n    lsvc = svm.LinearSVC(fit_intercept=False)\n    lsvc.fit(X, Y)\n    assert lsvc.intercept_ == 0.0\n\n\ndef test_hasattr_predict_proba():\n    # Method must be (un)available before or after fit, switched by\n    # `probability` param\n\n    G = svm.SVC(probability=True)\n    assert hasattr(G, \"predict_proba\")\n    G.fit(iris.data, iris.target)\n    assert hasattr(G, \"predict_proba\")\n\n    G = svm.SVC(probability=False)\n    assert not hasattr(G, \"predict_proba\")\n    G.fit(iris.data, iris.target)\n    assert not hasattr(G, \"predict_proba\")\n\n    # Switching to `probability=True` after fitting should make\n    # predict_proba available, but calling it must not work:\n    G.probability = True\n    assert hasattr(G, \"predict_proba\")\n    msg = \"predict_proba is not available when fitted with probability=False\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        G.predict_proba(iris.data)\n\n\ndef test_decision_function_shape_two_class():\n    for n_classes in [2, 3]:\n        X, y = make_blobs(centers=n_classes, random_state=0)\n        for estimator in [svm.SVC, svm.NuSVC]:\n            clf = OneVsRestClassifier(estimator(decision_function_shape=\"ovr\")).fit(\n                X, y\n            )\n            assert len(clf.predict(X)) == len(y)\n\n\ndef test_ovr_decision_function():\n    # One point from each quadrant represents one class\n    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])\n    y_train = [0, 1, 2, 3]\n\n    # First point is closer to the decision boundaries than the second point\n    base_points = np.array([[5, 5], [10, 10]])\n\n    # For all the quadrants (classes)\n    X_test = np.vstack(\n        (\n            base_points * [1, 1],  # Q1\n            base_points * [-1, 1],  # Q2\n            base_points * [-1, -1],  # Q3\n            base_points * [1, -1],  # Q4\n        )\n    )\n\n    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2\n\n    clf = svm.SVC(kernel=\"linear\", decision_function_shape=\"ovr\")\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    # Test if the prediction is the same as y\n    assert_array_equal(y_pred, y_test)\n\n    deci_val = clf.decision_function(X_test)\n\n    # Assert that the predicted class has the maximum value\n    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)\n\n    # Get decision value at test points for the predicted class\n    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))\n\n    # Assert pred_class_deci_val > 0 here\n    assert np.min(pred_class_deci_val) > 0.0\n\n    # Test if the first point has lower decision value on every quadrant\n    # compared to the second point\n    assert np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1])\n\n\n@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])\ndef test_svc_invalid_break_ties_param(SVCClass):\n    X, y = make_blobs(random_state=42)\n\n    svm = SVCClass(\n        kernel=\"linear\", decision_function_shape=\"ovo\", break_ties=True, random_state=42\n    ).fit(X, y)\n\n    with pytest.raises(ValueError, match=\"break_ties must be False\"):\n        svm.predict(y)\n\n\n@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])\ndef test_svc_ovr_tie_breaking(SVCClass):\n    \"\"\"Test if predict breaks ties in OVR mode.\n    Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277\n    \"\"\"\n    X, y = make_blobs(random_state=27)\n\n    xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 1000)\n    ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 1000)\n    xx, yy = np.meshgrid(xs, ys)\n\n    svm = SVCClass(\n        kernel=\"linear\",\n        decision_function_shape=\"ovr\",\n        break_ties=False,\n        random_state=42,\n    ).fit(X, y)\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    assert not np.all(pred == np.argmax(dv, axis=1))\n\n    svm = SVCClass(\n        kernel=\"linear\", decision_function_shape=\"ovr\", break_ties=True, random_state=42\n    ).fit(X, y)\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    assert np.all(pred == np.argmax(dv, axis=1))\n\n\ndef test_gamma_auto():\n    X, y = [[0.0, 1.2], [1.0, 1.3]], [0, 1]\n\n    with pytest.warns(None) as record:\n        svm.SVC(kernel=\"linear\").fit(X, y)\n    assert not len(record)\n\n    with pytest.warns(None) as record:\n        svm.SVC(kernel=\"precomputed\").fit(X, y)\n    assert not len(record)\n\n\ndef test_gamma_scale():\n    X, y = [[0.0], [1.0]], [0, 1]\n\n    clf = svm.SVC()\n    with pytest.warns(None) as record:\n        clf.fit(X, y)\n    assert not len(record)\n    assert_almost_equal(clf._gamma, 4)\n\n    # X_var ~= 1 shouldn't raise warning, for when\n    # gamma is not explicitly set.\n    X, y = [[1, 2], [3, 2 * np.sqrt(6) / 3 + 2]], [0, 1]\n    with pytest.warns(None) as record:\n        clf.fit(X, y)\n    assert not len(record)\n\n\n@pytest.mark.parametrize(\n    \"SVM, params\",\n    [\n        (LinearSVC, {\"penalty\": \"l1\", \"loss\": \"squared_hinge\", \"dual\": False}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": True}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": False}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"hinge\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"epsilon_insensitive\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),\n    ],\n)\ndef test_linearsvm_liblinear_sample_weight(SVM, params):\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.dtype(\"float\"),\n    )\n    y = np.array(\n        [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype(\"int\")\n    )\n\n    X2 = np.vstack([X, X])\n    y2 = np.hstack([y, 3 - y])\n    sample_weight = np.ones(shape=len(y) * 2)\n    sample_weight[len(y) :] = 0\n    X2, y2, sample_weight = shuffle(X2, y2, sample_weight, random_state=0)\n\n    base_estimator = SVM(random_state=42)\n    base_estimator.set_params(**params)\n    base_estimator.set_params(tol=1e-12, max_iter=1000)\n    est_no_weight = base.clone(base_estimator).fit(X, y)\n    est_with_weight = base.clone(base_estimator).fit(\n        X2, y2, sample_weight=sample_weight\n    )\n\n    for method in (\"predict\", \"decision_function\"):\n        if hasattr(base_estimator, method):\n            X_est_no_weight = getattr(est_no_weight, method)(X)\n            X_est_with_weight = getattr(est_with_weight, method)(X)\n            assert_allclose(X_est_no_weight, X_est_with_weight)\n\n\ndef test_n_support_oneclass_svr():\n    # Make n_support is correct for oneclass and SVR (used to be\n    # non-initialized)\n    # this is a non regression test for issue #14774\n    X = np.array([[0], [0.44], [0.45], [0.46], [1]])\n    clf = svm.OneClassSVM()\n    assert not hasattr(clf, \"n_support_\")\n    clf.fit(X)\n    assert clf.n_support_ == clf.support_vectors_.shape[0]\n    assert clf.n_support_.size == 1\n    assert clf.n_support_ == 3\n\n    y = np.arange(X.shape[0])\n    reg = svm.SVR().fit(X, y)\n    assert reg.n_support_ == reg.support_vectors_.shape[0]\n    assert reg.n_support_.size == 1\n    assert reg.n_support_ == 4\n\n\n@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])\ndef test_custom_kernel_not_array_input(Estimator):\n    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"\n    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]\n    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding\n    y = np.array([1, 1, 2, 2, 1])\n\n    def string_kernel(X1, X2):\n        assert isinstance(X1[0], str)\n        n_samples1 = _num_samples(X1)\n        n_samples2 = _num_samples(X2)\n        K = np.zeros((n_samples1, n_samples2))\n        for ii in range(n_samples1):\n            for jj in range(ii, n_samples2):\n                K[ii, jj] = X1[ii].count(\"A\") * X2[jj].count(\"A\")\n                K[ii, jj] += X1[ii].count(\"B\") * X2[jj].count(\"B\")\n                K[jj, ii] = K[ii, jj]\n        return K\n\n    K = string_kernel(data, data)\n    assert_array_equal(np.dot(X, X.T), K)\n\n    svc1 = Estimator(kernel=string_kernel).fit(data, y)\n    svc2 = Estimator(kernel=\"linear\").fit(X, y)\n    svc3 = Estimator(kernel=\"precomputed\").fit(K, y)\n\n    assert svc1.score(data, y) == svc3.score(K, y)\n    assert svc1.score(data, y) == svc2.score(X, y)\n    if hasattr(svc1, \"decision_function\"):  # classifier\n        assert_allclose(svc1.decision_function(data), svc2.decision_function(X))\n        assert_allclose(svc1.decision_function(data), svc3.decision_function(K))\n        assert_array_equal(svc1.predict(data), svc2.predict(X))\n        assert_array_equal(svc1.predict(data), svc3.predict(K))\n    else:  # regressor\n        assert_allclose(svc1.predict(data), svc2.predict(X))\n        assert_allclose(svc1.predict(data), svc3.predict(K))\n"], "fixing_code": [".. include:: _contributors.rst\n\n.. currentmodule:: sklearn\n\n.. _changes_1_0_1:\n\nVersion 1.0.1\n=============\n\n**In Development**\n\nChangelog\n---------\n\nFixed models\n------------\n\n- |Fix| Non-fit methods in the following classes do not raise a UserWarning\n  when fitted on DataFrames with valid feature names:\n  :class:`covariance.EllipticEnvelope`, :class:`ensemble.IsolationForest`,\n  :class:`ensemble.AdaBoostClassifier`, :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.KNeighborsRegressor`,\n  :class:`neighbors.RadiusNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsRegressor`. :pr:`21199` by `Thomas Fan`_.\n\n:mod:`sklearn.calibration`\n..........................\n\n- |Fix| Fixed :class:`calibration.CalibratedClassifierCV` to take into account\n  `sample_weight` when computing the base estimator prediction when\n  `ensemble=False`.\n  :pr:`20638` by :user:`Julien Bohn\u00e9 <JulienB-78>`.\n\n- |Fix| Fixed a bug in :class:`calibration.CalibratedClassifierCV` with\n  `method=\"sigmoid\"` that was ignoring the `sample_weight` when computing the\n  the Bayesian priors.\n  :pr:`21179` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n:mod:`sklearn.cluster`\n......................\n\n- |Fix| Fixed a bug in :class:`cluster.KMeans`, ensuring reproducibility and equivalence\n  between sparse and dense input. :pr:`21195`\n  by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.linear_model`\n...........................\n\n- |Fix| Improves stability of :class:`linear_model.LassoLars` for different\n  versions of openblas. :pr:`21340` by `Thomas Fan`_.\n\n:mod:`sklearn.neighbors`\n........................\n\n- |Fix| :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.KNeighborsRegressor`,\n  :class:`neighbors.RadiusNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsRegressor` with `metric=\"precomputed\"` raises\n  an error for `bsr` and `dok` sparse matrices in methods: `fit`, `kneighbors`\n  and `radius_neighbors`, due to handling of explicit zeros in `bsr` and `dok`\n  :term:`sparse graph` formats. :pr:`21199` by `Thomas Fan`_.\n\n:mod:`sklearn.pipeline`\n.......................\n\n- |Fix| :meth:`pipeline.Pipeline.get_feature_names_out` correctly passes feature\n  names out from one step of a pipeline to the next. :pr:`21351` by\n  `Thomas Fan`_.\n\n:mod:`sklearn.svm`\n..................\n\n- |Fix| :class:`svm.SVC` and :class:`svm.SVR` check for an inconsistency\n  in its internal representation and raise an error instead of segfaulting.\n  This fix also resolves\n  `CVE-2020-28975 <https://nvd.nist.gov/vuln/detail/CVE-2020-28975>`__.\n  :pr:`21336` by `Thomas Fan`_.\n\n.. _changes_1_0:\n\nVersion 1.0.0\n=============\n\n**September 2021**\n\nFor a short description of the main highlights of the release, please\nrefer to\n:ref:`sphx_glr_auto_examples_release_highlights_plot_release_highlights_1_0_0.py`.\n\n.. include:: changelog_legend.inc\n\nMinimal dependencies\n--------------------\n\nVersion 1.0.0 of scikit-learn requires python 3.7+, numpy 1.14.6+ and\nscipy 1.1.0+. Optional minimal dependency is matplotlib 2.2.2+.\n\nEnforcing keyword-only arguments\n--------------------------------\n\nIn an effort to promote clear and non-ambiguous use of the library, most\nconstructor and function parameters must now be passed as keyword arguments\n(i.e. using the `param=value` syntax) instead of positional. If a keyword-only\nparameter is used as positional, a `TypeError` is now raised.\n:issue:`15005` :pr:`20002` by `Joel Nothman`_, `Adrin Jalali`_, `Thomas Fan`_,\n`Nicolas Hug`_, and `Tom Dupre la Tour`_. See `SLEP009\n<https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep009/proposal.html>`_\nfor more details.\n\nChanged models\n--------------\n\nThe following estimators and functions, when fit with the same data and\nparameters, may produce different models from the previous version. This often\noccurs due to changes in the modelling logic (bug fixes or enhancements), or in\nrandom sampling procedures.\n\n- |Fix| :class:`manifold.TSNE` now avoids numerical underflow issues during\n  affinity matrix computation.\n\n- |Fix| :class:`manifold.Isomap` now connects disconnected components of the\n  neighbors graph along some minimum distance pairs, instead of changing\n  every infinite distances to zero.\n\n- |Fix| The splitting criterion of :class:`tree.DecisionTreeClassifier` and\n  :class:`tree.DecisionTreeRegressor` can be impacted by a fix in the handling\n  of rounding errors. Previously some extra spurious splits could occur.\n\nDetails are listed in the changelog below.\n\n(While we are trying to better inform users by providing this information, we\ncannot assure that this list is complete.)\n\n\nChangelog\n---------\n\n..\n    Entries should be grouped by module (in alphabetic order) and prefixed with\n    one of the labels: |MajorFeature|, |Feature|, |Efficiency|, |Enhancement|,\n    |Fix| or |API| (see whats_new.rst for descriptions).\n    Entries should be ordered by those labels (e.g. |Fix| after |Efficiency|).\n    Changes not specific to a module should be listed under *Multiple Modules*\n    or *Miscellaneous*.\n    Entries should end with:\n    :pr:`123456` by :user:`Joe Bloggs <joeongithub>`.\n    where 123456 is the *pull request* number, not the issue number.\n\n- |API| The option for using the squared error via ``loss`` and\n  ``criterion`` parameters was made more consistent. The preferred way is by\n  setting the value to `\"squared_error\"`. Old option names are still valid,\n  produce the same models, but are deprecated and will be removed in version\n  1.2.\n  :pr:`19310` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n  - For :class:`ensemble.ExtraTreesRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.GradientBoostingRegressor`, `loss=\"ls\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.RandomForestRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`ensemble.HistGradientBoostingRegressor`, `loss=\"least_squares\"`\n    is deprecated, use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`linear_model.RANSACRegressor`, `loss=\"squared_loss\"` is\n    deprecated, use `\"squared_error\"` instead.\n\n  - For :class:`linear_model.SGDRegressor`, `loss=\"squared_loss\"` is\n    deprecated, use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`tree.DecisionTreeRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n  - For :class:`tree.ExtraTreeRegressor`, `criterion=\"mse\"` is deprecated,\n    use `\"squared_error\"` instead which is now the default.\n\n- |API| The option for using the absolute error via ``loss`` and\n  ``criterion`` parameters was made more consistent. The preferred way is by\n  setting the value to `\"absolute_error\"`. Old option names are still valid,\n  produce the same models, but are deprecated and will be removed in version\n  1.2.\n  :pr:`19733` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n  - For :class:`ensemble.ExtraTreesRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.GradientBoostingRegressor`, `loss=\"lad\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.RandomForestRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`ensemble.HistGradientBoostingRegressor`,\n    `loss=\"least_absolute_deviation\"` is deprecated, use `\"absolute_error\"`\n    instead.\n\n  - For :class:`linear_model.RANSACRegressor`, `loss=\"absolute_loss\"` is\n    deprecated, use `\"absolute_error\"` instead which is now the default.\n\n  - For :class:`tree.DecisionTreeRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n  - For :class:`tree.ExtraTreeRegressor`, `criterion=\"mae\"` is deprecated,\n    use `\"absolute_error\"` instead.\n\n- |API| `np.matrix` usage is deprecated in 1.0 and will raise a `TypeError` in\n  1.2. :pr:`20165` by `Thomas Fan`_.\n\n- |API| :term:`get_feature_names_out` has been added to the transformer API\n  to get the names of the output features. :term:`get_feature_names` has in\n  turn been deprecated. :pr:`18444` by `Thomas Fan`_.\n\n- |API| All estimators store `feature_names_in_` when fitted on pandas Dataframes.\n  These feature names are compared to names seen in non-`fit` methods, e.g.\n  `transform` and will raise a `FutureWarning` if they are not consistent.\n  These ``FutureWarning`` s will become ``ValueError`` s in 1.2. :pr:`18010` by\n  `Thomas Fan`_.\n\n:mod:`sklearn.base`\n...................\n\n- |Fix| :func:`config_context` is now threadsafe. :pr:`18736` by `Thomas Fan`_.\n\n:mod:`sklearn.calibration`\n..........................\n\n- |Feature| :func:`calibration.CalibrationDisplay` added to plot\n  calibration curves. :pr:`17443` by :user:`Lucy Liu <lucyleeow>`.\n\n- |Fix| The ``predict`` and ``predict_proba`` methods of\n  :class:`calibration.CalibratedClassifierCV` can now properly be used on\n  prefitted pipelines. :pr:`19641` by :user:`Alek Lefebvre <AlekLefebvre>`.\n\n- |Fix| Fixed an error when using a :class:`ensemble.VotingClassifier`\n  as `base_estimator` in :class:`calibration.CalibratedClassifierCV`.\n  :pr:`20087` by :user:`Cl\u00e9ment Fauchereau <clement-f>`.\n\n\n:mod:`sklearn.cluster`\n......................\n\n- |Efficiency| The ``\"k-means++\"`` initialization of :class:`cluster.KMeans`\n  and :class:`cluster.MiniBatchKMeans`\u00a0is now faster, especially in multicore\n  settings. :pr:`19002` by :user:`Jon Crall <Erotemic>` and :user:`J\u00e9r\u00e9mie du\n  Boisberranger <jeremiedbb>`.\n\n- |Efficiency| :class:`cluster.KMeans` with `algorithm='elkan'` is now faster\n  in multicore settings. :pr:`19052` by\n  :user:`Yusuke Nagasaka <YusukeNagasaka>`.\n\n- |Efficiency| :class:`cluster.MiniBatchKMeans` is now faster in multicore\n  settings. :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Efficiency| :class:`cluster.OPTICS` can now cache the output of the\n  computation of the tree, using the `memory` parameter.  :pr:`19024` by\n  :user:`Frankie Robertson <frankier>`.\n\n- |Enhancement| The `predict` and `fit_predict` methods of\n  :class:`cluster.AffinityPropagation` now accept sparse data type for input\n  data.\n  :pr:`20117` by :user:`Venkatachalam Natchiappan <venkyyuvy>`\n\n- |Fix| Fixed a bug in :class:`cluster.MiniBatchKMeans` where the sample\n  weights were partially ignored when the input is sparse. :pr:`17622` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| Improved convergence detection based on center change in\n  :class:`cluster.MiniBatchKMeans` which was almost never achievable.\n  :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |FIX| :class:`cluster.AgglomerativeClustering` now supports readonly\n  memory-mapped datasets.\n  :pr:`19883` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |Fix| :class:`cluster.AgglomerativeClustering` correctly connects components\n  when connectivity and affinity are both precomputed and the number\n  of connected components is greater than 1. :pr:`20597` by\n  `Thomas Fan`_.\n\n- |Fix| :class:`cluster.FeatureAgglomeration` does not accept a ``**params`` kwarg in\n  the ``fit`` function anymore, resulting in a more concise error message. :pr:`20899`\n  by :user:`Adam Li <adam2392>`.\n\n- |Fix| Fixed a bug in :class:`cluster.KMeans`, ensuring reproducibility and equivalence\n  between sparse and dense input. :pr:`20200`\n  by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| :class:`cluster.Birch` attributes, `fit_` and `partial_fit_`, are\n  deprecated and will be removed in 1.2. :pr:`19297` by `Thomas Fan`_.\n\n- |API| the default value for the `batch_size` parameter of\n  :class:`cluster.MiniBatchKMeans` was changed from 100 to 1024 due to\n  efficiency reasons. The `n_iter_` attribute of\n  :class:`cluster.MiniBatchKMeans` now reports the number of started epochs and\n  the `n_steps_` attribute reports the number of mini batches processed.\n  :pr:`17622` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| :func:`cluster.spectral_clustering` raises an improved error when passed\n  a `np.matrix`. :pr:`20560` by `Thomas Fan`_.\n\n:mod:`sklearn.compose`\n......................\n\n- |Enhancement| :class:`compose.ColumnTransformer` now records the output\n  of each transformer in `output_indices_`. :pr:`18393` by\n  :user:`Luca Bittarello <lbittarello>`.\n\n- |Enhancement| :class:`compose.ColumnTransformer` now allows DataFrame input to\n  have its columns appear in a changed order in `transform`. Further, columns that\n  are dropped will not be required in transform, and additional columns will be\n  ignored if `remainder='drop'`. :pr:`19263` by `Thomas Fan`_.\n\n- |Enhancement| Adds `**predict_params` keyword argument to\n  :meth:`compose.TransformedTargetRegressor.predict` that passes keyword\n  argument to the regressor.\n  :pr:`19244` by :user:`Ricardo <ricardojnf>`.\n\n- |FIX| :meth:`compose.ColumnTransformer.get_feature_names` supports\n  non-string feature names returned by any of its transformers. However, note\n  that ``get_feature_names`` is deprecated, use ``get_feature_names_out``\n  instead. :pr:`18459` by :user:`Albert Villanova del Moral <albertvillanova>`\n  and :user:`Alonso Silva Allende <alonsosilvaallende>`.\n\n- |Fix| :class:`compose.TransformedTargetRegressor` now takes nD targets with\n  an adequate transformer.\n  :pr:`18898` by :user:`Oras Phongpanagnam <panangam>`.\n\n- |API| Adds `verbose_feature_names_out` to :class:`compose.ColumnTransformer`.\n  This flag controls the prefixing of feature names out in\n  :term:`get_feature_names_out`. :pr:`18444` and :pr:`21080` by `Thomas Fan`_.\n\n:mod:`sklearn.covariance`\n.........................\n\n- |Fix| Adds arrays check to :func:`covariance.ledoit_wolf` and\n  :func:`covariance.ledoit_wolf_shrinkage`. :pr:`20416` by :user:`Hugo Defois\n  <defoishugo>`.\n\n- |API| Deprecates the following keys in `cv_results_`: `'mean_score'`,\n  `'std_score'`, and `'split(k)_score'` in favor of `'mean_test_score'`\n  `'std_test_score'`, and `'split(k)_test_score'`. :pr:`20583` by `Thomas Fan`_.\n\n:mod:`sklearn.datasets`\n.......................\n\n- |Enhancement| :func:`datasets.fetch_openml` now supports categories with\n  missing values when returning a pandas dataframe. :pr:`19365` by\n  `Thomas Fan`_ and :user:`Amanda Dsouza <amy12xx>` and\n  :user:`EL-ATEIF Sara <elateifsara>`.\n\n- |Enhancement| :func:`datasets.fetch_kddcup99` raises a better message\n  when the cached file is invalid. :pr:`19669` `Thomas Fan`_.\n\n- |Enhancement| Replace usages of ``__file__`` related to resource file I/O\n  with ``importlib.resources`` to avoid the assumption that these resource\n  files (e.g. ``iris.csv``) already exist on a filesystem, and by extension\n  to enable compatibility with tools such as ``PyOxidizer``.\n  :pr:`20297` by :user:`Jack Liu <jackzyliu>`.\n\n- |Fix| Shorten data file names in the openml tests to better support\n  installing on Windows and its default 260 character limit on file names.\n  :pr:`20209` by `Thomas Fan`_.\n\n- |Fix| :func:`datasets.fetch_kddcup99` returns dataframes when\n  `return_X_y=True` and `as_frame=True`. :pr:`19011` by `Thomas Fan`_.\n\n- |API| Deprecates :func:`datasets.load_boston` in 1.0 and it will be removed\n  in 1.2. Alternative code snippets to load similar datasets are provided.\n  Please report to the docstring of the function for details.\n  :pr:`20729` by `Guillaume Lemaitre`_.\n\n\n:mod:`sklearn.decomposition`\n............................\n\n- |Enhancement| added a new approximate solver (randomized SVD, available with\n  `eigen_solver='randomized'`) to :class:`decomposition.KernelPCA`. This\n  significantly accelerates computation when the number of samples is much\n  larger than the desired number of components.\n  :pr:`12069` by :user:`Sylvain Mari\u00e9 <smarie>`.\n\n- |Fix| Fixes incorrect multiple data-conversion warnings when clustering\n  boolean data. :pr:`19046` by :user:`Surya Prakash <jdsurya>`.\n\n- |Fix| Fixed :func:`dict_learning`, used by\n  :class:`decomposition.DictionaryLearning`, to ensure determinism of the\n  output. Achieved by flipping signs of the SVD output which is used to\n  initialize the code. :pr:`18433` by :user:`Bruno Charron <brcharron>`.\n\n- |Fix| Fixed a bug in :class:`decomposition.MiniBatchDictionaryLearning`,\n  :class:`decomposition.MiniBatchSparsePCA` and\n  :func:`decomposition.dict_learning_online` where the update of the dictionary\n  was incorrect. :pr:`19198` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| Fixed a bug in :class:`decomposition.DictionaryLearning`,\n  :class:`decomposition.SparsePCA`,\n  :class:`decomposition.MiniBatchDictionaryLearning`,\n  :class:`decomposition.MiniBatchSparsePCA`,\n  :func:`decomposition.dict_learning` and\n  :func:`decomposition.dict_learning_online` where the restart of unused atoms\n  during the dictionary update was not working as expected. :pr:`19198` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| In :class:`decomposition.DictionaryLearning`,\n  :class:`decomposition.MiniBatchDictionaryLearning`,\n  :func:`decomposition.dict_learning` and\n  :func:`decomposition.dict_learning_online`, `transform_alpha` will be equal\n  to `alpha` instead of 1.0 by default starting from version 1.2 :pr:`19159` by\n  :user:`Beno\u00eet Mal\u00e9zieux <bmalezieux>`.\n\n- |API| Rename variable names in :class:`KernelPCA` to improve\n  readability. `lambdas_` and `alphas_` are renamed to `eigenvalues_`\n  and `eigenvectors_`, respectively. `lambdas_` and `alphas_` are\n  deprecated and will be removed in 1.2.\n  :pr:`19908` by :user:`Kei Ishikawa <kstoneriv3>`.\n\n- |API| The `alpha` and `regularization` parameters of :class:`decomposition.NMF` and\n  :func:`decomposition.non_negative_factorization` are deprecated and will be removed\n  in 1.2. Use the new parameters `alpha_W` and `alpha_H` instead. :pr:`20512` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.dummy`\n....................\n\n- |API| Attribute `n_features_in_` in :class:`dummy.DummyRegressor` and\n  :class:`dummy.DummyRegressor` is deprecated and will be removed in 1.2.\n  :pr:`20960` by `Thomas Fan`_.\n\n:mod:`sklearn.ensemble`\n.......................\n\n- |Enhancement| :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` take cgroups quotas\n  into account when deciding the number of threads used by OpenMP. This\n  avoids performance problems caused by over-subscription when using those\n  classes in a docker container for instance. :pr:`20477`\n  by `Thomas Fan`_.\n\n- |Enhancement| :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` are no longer\n  experimental. They are now considered stable and are subject to the same\n  deprecation cycles as all other estimators. :pr:`19799` by `Nicolas Hug`_.\n\n- |Enhancement| Improve the HTML rendering of the\n  :class:`ensemble.StackingClassifier` and :class:`ensemble.StackingRegressor`.\n  :pr:`19564` by `Thomas Fan`_.\n\n- |Enhancement| Added Poisson criterion to\n  :class:`ensemble.RandomForestRegressor`. :pr:`19836` by :user:`Brian Sun\n  <bsun94>`.\n\n- |Fix| Do not allow to compute out-of-bag (OOB) score in\n  :class:`ensemble.RandomForestClassifier` and\n  :class:`ensemble.ExtraTreesClassifier` with multiclass-multioutput target\n  since scikit-learn does not provide any metric supporting this type of\n  target. Additional private refactoring was performed.\n  :pr:`19162` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |Fix| Improve numerical precision for weights boosting in\n  :class:`ensemble.AdaBoostClassifier` and :class:`ensemble.AdaBoostRegressor`\n  to avoid underflows.\n  :pr:`10096` by :user:`Fenil Suchak <fenilsuchak>`.\n\n- |Fix| Fixed the range of the argument ``max_samples`` to be ``(0.0, 1.0]``\n  in :class:`ensemble.RandomForestClassifier`,\n  :class:`ensemble.RandomForestRegressor`, where `max_samples=1.0` is\n  interpreted as using all `n_samples` for bootstrapping. :pr:`20159` by\n  :user:`murata-yu`.\n\n- |Fix| Fixed a bug in :class:`ensemble.AdaBoostClassifier` and\n  :class:`ensemble.AdaBoostRegressor` where the `sample_weight` parameter\n  got overwritten during `fit`.\n  :pr:`20534` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |API| Removes `tol=None` option in\n  :class:`ensemble.HistGradientBoostingClassifier` and\n  :class:`ensemble.HistGradientBoostingRegressor`. Please use `tol=0` for\n  the same behavior. :pr:`19296` by `Thomas Fan`_.\n\n:mod:`sklearn.feature_extraction`\n.................................\n\n- |Fix| Fixed a bug in :class:`feature_extraction.text.HashingVectorizer`\n  where some input strings would result in negative indices in the transformed\n  data. :pr:`19035` by :user:`Liu Yu <ly648499246>`.\n\n- |Fix| Fixed a bug in :class:`feature_extraction.DictVectorizer` by raising an\n  error with unsupported value type.\n  :pr:`19520` by :user:`Jeff Zhao <kamiyaa>`.\n\n- |Fix| Fixed a bug in :func:`feature_extraction.image.img_to_graph`\n  and :func:`feature_extraction.image.grid_to_graph` where singleton connected\n  components were not handled properly, resulting in a wrong vertex indexing.\n  :pr:`18964` by `Bertrand Thirion`_.\n\n- |Fix| Raise a warning in :class:`feature_extraction.text.CountVectorizer`\n  with `lowercase=True` when there are vocabulary entries with uppercase\n  characters to avoid silent misses in the resulting feature vectors.\n  :pr:`19401` by :user:`Zito Relova <zitorelova>`\n\n:mod:`sklearn.feature_selection`\n................................\n\n- |Feature| :func:`feature_selection.r_regression` computes Pearson's R\n  correlation coefficients between the features and the target.\n  :pr:`17169` by :user:`Dmytro Lituiev <DSLituiev>`\n  and :user:`Julien Jerphanion <jjerphan>`.\n\n- |Enhancement| :func:`feature_selection.RFE.fit` accepts additional estimator\n  parameters that are passed directly to the estimator's `fit` method.\n  :pr:`20380` by :user:`Iv\u00e1n Pulido <ijpulidos>`, :user:`Felipe Bidu <fbidu>`,\n  :user:`Gil Rutter <g-rutter>`, and :user:`Adrin Jalali <adrinjalali>`.\n\n- |FIX| Fix a bug in :func:`isotonic.isotonic_regression` where the\n  `sample_weight` passed by a user were overwritten during ``fit``.\n  :pr:`20515` by :user:`Carsten Allefeld <allefeld>`.\n\n- |Fix| Change :func:`feature_selection.SequentialFeatureSelector` to\n  allow for unsupervised modelling so that the `fit` signature need not\n  do any `y` validation and allow for `y=None`.\n  :pr:`19568` by :user:`Shyam Desai <ShyamDesai>`.\n\n- |API| Raises an error in :class:`feature_selection.VarianceThreshold`\n  when the variance threshold is negative.\n  :pr:`20207` by :user:`Tomohiro Endo <europeanplaice>`\n\n- |API| Deprecates `grid_scores_` in favor of split scores in `cv_results_` in\n  :class:`feature_selection.RFECV`. `grid_scores_` will be removed in\n  version 1.2.\n  :pr:`20161` by :user:`Shuhei Kayawari <wowry>` and :user:`arka204`.\n\n:mod:`sklearn.inspection`\n.........................\n\n- |Enhancement| Add `max_samples` parameter in\n  :func:`inspection.permutation_importance`. It enables to draw a subset of the\n  samples to compute the permutation importance. This is useful to keep the\n  method tractable when evaluating feature importance on large datasets.\n  :pr:`20431` by :user:`Oliver Pfaffel <o1iv3r>`.\n\n- |Enhancement| Add kwargs to format ICE and PD lines separately in partial\n  dependence plots :func:`inspection.plot_partial_dependence` and\n  :meth:`inspection.PartialDependenceDisplay.plot`. :pr:`19428` by :user:`Mehdi\n  Hamoumi <mhham>`.\n\n- |Fix| Allow multiple scorers input to\n  :func:`inspection.permutation_importance`. :pr:`19411` by :user:`Simona\n  Maggio <simonamaggio>`.\n\n- |API| :class:`inspection.PartialDependenceDisplay` exposes a class method:\n  :func:`~inspection.PartialDependenceDisplay.from_estimator`.\n  :func:`inspection.plot_partial_dependence` is deprecated in favor of the\n  class method and will be removed in 1.2. :pr:`20959` by `Thomas Fan`_.\n\n:mod:`sklearn.kernel_approximation`\n...................................\n\n- |Fix| Fix a bug in :class:`kernel_approximation.Nystroem`\n  where the attribute `component_indices_` did not correspond to the subset of\n  sample indices used to generate the approximated kernel. :pr:`20554` by\n  :user:`Xiangyin Kong <kxytim>`.\n\n:mod:`sklearn.linear_model`\n...........................\n\n- |Feature| Added :class:`linear_model.QuantileRegressor` which implements\n  linear quantile regression with L1 penalty.\n  :pr:`9978` by :user:`David Dale <avidale>` and\n  :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature| The new :class:`linear_model.SGDOneClassSVM` provides an SGD\n  implementation of the linear One-Class SVM. Combined with kernel\n  approximation techniques, this implementation approximates the solution of\n  a kernelized One Class SVM while benefitting from a linear\n  complexity in the number of samples.\n  :pr:`10027` by :user:`Albert Thomas <albertcthomas>`.\n\n- |Feature| Added `sample_weight` parameter to\n  :class:`linear_model.LassoCV` and :class:`linear_model.ElasticNetCV`.\n  :pr:`16449` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature| Added new solver `lbfgs` (available with `solver=\"lbfgs\"`)\n  and `positive` argument to :class:`linear_model.Ridge`. When `positive` is\n  set to `True`, forces the coefficients to be positive (only supported by\n  `lbfgs`). :pr:`20231` by :user:`Toshihiro Nakae <tnakae>`.\n\n- |Efficiency| The implementation of :class:`linear_model.LogisticRegression`\n  has been optimised for dense matrices when using `solver='newton-cg'` and\n  `multi_class!='multinomial'`.\n  :pr:`19571` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |Enhancement| `fit` method preserves dtype for numpy.float32 in\n  :class:`linear_model.Lars`, :class:`linear_model.LassoLars`,\n  :class:`linear_model.LassoLars`, :class:`linear_model.LarsCV` and\n  :class:`linear_model.LassoLarsCV`. :pr:`20155` by :user:`Takeshi Oura\n  <takoika>`.\n\n- |Enhancement| Validate user-supplied gram matrix passed to linear models\n  via the `precompute` argument. :pr:`19004` by :user:`Adam Midvidy <amidvidy>`.\n\n- |Fix| :meth:`linear_model.ElasticNet.fit` no longer modifies `sample_weight`\n  in place. :pr:`19055` by `Thomas Fan`_.\n\n- |Fix| :class:`linear_model.Lasso` and :class:`linear_model.ElasticNet` no\n  longer have a `dual_gap_` not corresponding to their objective. :pr:`19172`\n  by :user:`Mathurin Massias <mathurinm>`\n\n- |Fix| `sample_weight` are now fully taken into account in linear models\n  when `normalize=True` for both feature centering and feature\n  scaling.\n  :pr:`19426` by :user:`Alexandre Gramfort <agramfort>` and\n  :user:`Maria Telenczuk <maikia>`.\n\n- |Fix| Points with residuals equal to  ``residual_threshold`` are now considered\n  as inliers for :class:`linear_model.RANSACRegressor`. This allows fitting\n  a model perfectly on some datasets when `residual_threshold=0`.\n  :pr:`19499` by :user:`Gregory Strubel <gregorystrubel>`.\n\n- |Fix| Sample weight invariance for :class:`linear_model.Ridge` was fixed in\n  :pr:`19616` by :user:`Oliver Grisel <ogrisel>` and :user:`Christian Lorentzen\n  <lorentzenchr>`.\n\n- |Fix| The dictionary `params` in :func:`linear_model.enet_path` and\n  :func:`linear_model.lasso_path` should only contain parameter of the\n  coordinate descent solver. Otherwise, an error will be raised.\n  :pr:`19391` by :user:`Shao Yang Hong <hongshaoyang>`.\n\n- |API| Raise a warning in :class:`linear_model.RANSACRegressor` that from\n  version 1.2, `min_samples` need to be set explicitly for models other than\n  :class:`linear_model.LinearRegression`. :pr:`19390` by :user:`Shao Yang Hong\n  <hongshaoyang>`.\n\n- |API|: The parameter ``normalize`` of :class:`linear_model.LinearRegression`\n  is deprecated and will be removed in 1.2. Motivation for this deprecation:\n  ``normalize`` parameter did not take any effect if ``fit_intercept`` was set\n  to False and therefore was deemed confusing. The behavior of the deprecated\n  ``LinearModel(normalize=True)`` can be reproduced with a\n  :class:`~sklearn.pipeline.Pipeline` with ``LinearModel`` (where\n  ``LinearModel`` is :class:`~linear_model.LinearRegression`,\n  :class:`~linear_model.Ridge`, :class:`~linear_model.RidgeClassifier`,\n  :class:`~linear_model.RidgeCV` or :class:`~linear_model.RidgeClassifierCV`)\n  as follows: ``make_pipeline(StandardScaler(with_mean=False),\n  LinearModel())``. The ``normalize`` parameter in\n  :class:`~linear_model.LinearRegression` was deprecated in :pr:`17743` by\n  :user:`Maria Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n  Same for :class:`~linear_model.Ridge`,\n  :class:`~linear_model.RidgeClassifier`, :class:`~linear_model.RidgeCV`, and\n  :class:`~linear_model.RidgeClassifierCV`, in: :pr:`17772` by :user:`Maria\n  Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`. Same for\n  :class:`~linear_model.BayesianRidge`, :class:`~linear_model.ARDRegression`\n  in: :pr:`17746` by :user:`Maria Telenczuk <maikia>`. Same for\n  :class:`~linear_model.Lasso`, :class:`~linear_model.LassoCV`,\n  :class:`~linear_model.ElasticNet`, :class:`~linear_model.ElasticNetCV`,\n  :class:`~linear_model.MultiTaskLasso`,\n  :class:`~linear_model.MultiTaskLassoCV`,\n  :class:`~linear_model.MultiTaskElasticNet`,\n  :class:`~linear_model.MultiTaskElasticNetCV`, in: :pr:`17785` by :user:`Maria\n  Telenczuk <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n\n- |API| The ``normalize`` parameter of\n  :class:`~linear_model.OrthogonalMatchingPursuit` and\n  :class:`~linear_model.OrthogonalMatchingPursuitCV` will default to False in\n  1.2 and will be removed in 1.4. :pr:`17750` by :user:`Maria Telenczuk\n  <maikia>` and :user:`Alexandre Gramfort <agramfort>`. Same for\n  :class:`~linear_model.Lars` :class:`~linear_model.LarsCV`\n  :class:`~linear_model.LassoLars` :class:`~linear_model.LassoLarsCV`\n  :class:`~linear_model.LassoLarsIC`, in :pr:`17769` by :user:`Maria Telenczuk\n  <maikia>` and :user:`Alexandre Gramfort <agramfort>`.\n\n- |API| Keyword validation has moved from `__init__` and `set_params` to `fit`\n  for the following estimators conforming to scikit-learn's conventions:\n  :class:`~linear_model.SGDClassifier`,\n  :class:`~linear_model.SGDRegressor`,\n  :class:`~linear_model.SGDOneClassSVM`,\n  :class:`~linear_model.PassiveAggressiveClassifier`, and\n  :class:`~linear_model.PassiveAggressiveRegressor`.\n  :pr:`20683` by `Guillaume Lemaitre`_.\n\n:mod:`sklearn.manifold`\n.......................\n\n- |Enhancement| Implement `'auto'` heuristic for the `learning_rate` in\n  :class:`manifold.TSNE`. It will become default in 1.2. The default\n  initialization will change to `pca` in 1.2. PCA initialization will\n  be scaled to have standard deviation 1e-4 in 1.2.\n  :pr:`19491` by :user:`Dmitry Kobak <dkobak>`.\n\n- |Fix| Change numerical precision to prevent underflow issues\n  during affinity matrix computation for :class:`manifold.TSNE`.\n  :pr:`19472` by :user:`Dmitry Kobak <dkobak>`.\n\n- |Fix| :class:`manifold.Isomap` now uses `scipy.sparse.csgraph.shortest_path`\n  to compute the graph shortest path. It also connects disconnected components\n  of the neighbors graph along some minimum distance pairs, instead of changing\n  every infinite distances to zero. :pr:`20531` by `Roman Yurchak`_ and `Tom\n  Dupre la Tour`_.\n\n- |Fix| Decrease the numerical default tolerance in the lobpcg call\n  in :func:`manifold.spectral_embedding` to prevent numerical instability.\n  :pr:`21194` by :user:`Andrew Knyazev <lobpcg>`.\n\n:mod:`sklearn.metrics`\n......................\n\n- |Feature| :func:`metrics.mean_pinball_loss` exposes the pinball loss for\n  quantile regression. :pr:`19415` by :user:`Xavier Dupr\u00e9 <sdpython>`\n  and :user:`Oliver Grisel <ogrisel>`.\n\n- |Feature| :func:`metrics.d2_tweedie_score` calculates the D^2 regression\n  score for Tweedie deviances with power parameter ``power``. This is a\n  generalization of the `r2_score` and can be interpreted as percentage of\n  Tweedie deviance explained.\n  :pr:`17036` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Feature|  :func:`metrics.mean_squared_log_error` now supports\n  `squared=False`.\n  :pr:`20326` by :user:`Uttam kumar <helper-uttam>`.\n\n- |Efficiency| Improved speed of :func:`metrics.confusion_matrix` when labels\n  are integral.\n  :pr:`9843` by :user:`Jon Crall <Erotemic>`.\n\n- |Enhancement| A fix to raise an error in :func:`metrics.hinge_loss` when\n  ``pred_decision`` is 1d whereas it is a multiclass classification or when\n  ``pred_decision`` parameter is not consistent with the ``labels`` parameter.\n  :pr:`19643` by :user:`Pierre Attard <PierreAttard>`.\n\n- |Fix| :meth:`metrics.ConfusionMatrixDisplay.plot` uses the correct max\n  for colormap. :pr:`19784` by `Thomas Fan`_.\n\n- |Fix| Samples with zero `sample_weight` values do not affect the results\n  from :func:`metrics.det_curve`, :func:`metrics.precision_recall_curve`\n  and :func:`metrics.roc_curve`.\n  :pr:`18328` by :user:`Albert Villanova del Moral <albertvillanova>` and\n  :user:`Alonso Silva Allende <alonsosilvaallende>`.\n\n- |Fix| avoid overflow in :func:`metrics.cluster.adjusted_rand_score` with\n  large amount of data. :pr:`20312` by :user:`Divyanshu Deoli\n  <divyanshudeoli>`.\n\n- |API| :class:`metrics.ConfusionMatrixDisplay` exposes two class methods\n  :func:`~metrics.ConfusionMatrixDisplay.from_estimator` and\n  :func:`~metrics.ConfusionMatrixDisplay.from_predictions` allowing to create\n  a confusion matrix plot using an estimator or the predictions.\n  :func:`metrics.plot_confusion_matrix` is deprecated in favor of these two\n  class methods and will be removed in 1.2.\n  :pr:`18543` by `Guillaume Lemaitre`_.\n\n- |API| :class:`metrics.PrecisionRecallDisplay` exposes two class methods\n  :func:`~metrics.PrecisionRecallDisplay.from_estimator` and\n  :func:`~metrics.PrecisionRecallDisplay.from_predictions` allowing to create\n  a precision-recall curve using an estimator or the predictions.\n  :func:`metrics.plot_precision_recall_curve` is deprecated in favor of these\n  two class methods and will be removed in 1.2.\n  :pr:`20552` by `Guillaume Lemaitre`_.\n\n- |API| :class:`metrics.DetCurveDisplay` exposes two class methods\n  :func:`~metrics.DetCurveDisplay.from_estimator` and\n  :func:`~metrics.DetCurveDisplay.from_predictions` allowing to create\n  a confusion matrix plot using an estimator or the predictions.\n  :func:`metrics.plot_det_curve` is deprecated in favor of these two\n  class methods and will be removed in 1.2.\n  :pr:`19278` by `Guillaume Lemaitre`_.\n\n:mod:`sklearn.mixture`\n......................\n\n- |Fix| Ensure that the best parameters are set appropriately\n  in the case of divergency for :class:`mixture.GaussianMixture` and\n  :class:`mixture.BayesianGaussianMixture`.\n  :pr:`20030` by :user:`Tingshan Liu <tliu68>` and\n  :user:`Benjamin Pedigo <bdpedigo>`.\n\n:mod:`sklearn.model_selection`\n..............................\n\n- |Feature| added :class:`model_selection.StratifiedGroupKFold`, that combines\n  :class:`model_selection.StratifiedKFold` and\n  :class:`model_selection.GroupKFold`, providing an ability to split data\n  preserving the distribution of classes in each split while keeping each\n  group within a single split.\n  :pr:`18649` by :user:`Leandro Hermida <hermidalc>` and\n  :user:`Rodion Martynov <marrodion>`.\n\n- |Enhancement| warn only once in the main process for per-split fit failures\n  in cross-validation. :pr:`20619` by :user:`Lo\u00efc Est\u00e8ve <lesteve>`\n\n- |Enhancement| The :class:`model_selection.BaseShuffleSplit` base class is\n  now public. :pr:`20056` by :user:`pabloduque0`.\n\n- |Fix| Avoid premature overflow in :func:`model_selection.train_test_split`.\n  :pr:`20904` by :user:`Tomasz Jakubek <t-jakubek>`.\n\n:mod:`sklearn.naive_bayes`\n..........................\n\n- |Fix| The `fit` and `partial_fit` methods of the discrete naive Bayes\n  classifiers (:class:`naive_bayes.BernoulliNB`,\n  :class:`naive_bayes.CategoricalNB`, :class:`naive_bayes.ComplementNB`,\n  and :class:`naive_bayes.MultinomialNB`) now correctly handle the degenerate\n  case of a single class in the training set.\n  :pr:`18925` by :user:`David Poznik <dpoznik>`.\n\n- |API| The attribute ``sigma_`` is now deprecated in\n  :class:`naive_bayes.GaussianNB` and will be removed in 1.2.\n  Use ``var_`` instead.\n  :pr:`18842` by :user:`Hong Shao Yang <hongshaoyang>`.\n\n:mod:`sklearn.neighbors`\n........................\n\n- |Enhancement| The creation of :class:`neighbors.KDTree` and\n  :class:`neighbors.BallTree` has been improved for their worst-cases time\n  complexity from :math:`\\mathcal{O}(n^2)` to :math:`\\mathcal{O}(n)`.\n  :pr:`19473` by :user:`jiefangxuanyan <jiefangxuanyan>` and\n  :user:`Julien Jerphanion <jjerphan>`.\n\n- |FIX| :class:`neighbors.DistanceMetric` subclasses now support readonly\n  memory-mapped datasets. :pr:`19883` by :user:`Julien Jerphanion <jjerphan>`.\n\n- |FIX| :class:`neighbors.NearestNeighbors`, :class:`neighbors.KNeighborsClassifier`,\n  :class:`neighbors.RadiusNeighborsClassifier`, :class:`neighbors.KNeighborsRegressor`\n  and :class:`neighbors.RadiusNeighborsRegressor` do not validate `weights` in\n  `__init__` and validates `weights` in `fit` instead. :pr:`20072` by\n  :user:`Juan Carlos Alfaro Jim\u00e9nez <alfaro96>`.\n\n- |API| The parameter `kwargs` of :class:`neighbors.RadiusNeighborsClassifier` is\n  deprecated and will be removed in 1.2.\n  :pr:`20842` by :user:`Juan Mart\u00edn Loyola <jmloyola>`.\n\n:mod:`sklearn.neural_network`\n.............................\n\n- |Fix| :class:`neural_network.MLPClassifier` and\n  :class:`neural_network.MLPRegressor` now correctly support continued training\n  when loading from a pickled file. :pr:`19631` by `Thomas Fan`_.\n\n:mod:`sklearn.pipeline`\n.......................\n\n- |API| The `predict_proba` and `predict_log_proba` methods of the\n  :class:`pipeline.Pipeline` now support passing prediction kwargs to the final\n  estimator. :pr:`19790` by :user:`Christopher Flynn <crflynn>`.\n\n:mod:`sklearn.preprocessing`\n............................\n\n- |Feature| The new :class:`preprocessing.SplineTransformer` is a feature\n  preprocessing tool for the generation of B-splines, parametrized by the\n  polynomial ``degree`` of the splines, number of knots ``n_knots`` and knot\n  positioning strategy ``knots``.\n  :pr:`18368` by :user:`Christian Lorentzen <lorentzenchr>`.\n  :class:`preprocessing.SplineTransformer` also supports periodic\n  splines via the ``extrapolation`` argument.\n  :pr:`19483` by :user:`Malte Londschien <mlondschien>`.\n  :class:`preprocessing.SplineTransformer` supports sample weights for\n  knot position strategy ``\"quantile\"``.\n  :pr:`20526` by :user:`Malte Londschien <mlondschien>`.\n\n- |Feature| :class:`preprocessing.OrdinalEncoder` supports passing through\n  missing values by default. :pr:`19069` by `Thomas Fan`_.\n\n- |Feature| :class:`preprocessing.OneHotEncoder` now supports\n  `handle_unknown='ignore'` and dropping categories. :pr:`19041` by\n  `Thomas Fan`_.\n\n- |Feature| :class:`preprocessing.PolynomialFeatures` now supports passing\n  a tuple to `degree`, i.e. `degree=(min_degree, max_degree)`.\n  :pr:`20250` by :user:`Christian Lorentzen <lorentzenchr>`.\n\n- |Efficiency| :class:`preprocessing.StandardScaler` is faster and more memory\n  efficient. :pr:`20652` by `Thomas Fan`_.\n\n- |Efficiency| Changed ``algorithm`` argument for :class:`cluster.KMeans` in\n  :class:`preprocessing.KBinsDiscretizer` from ``auto`` to ``full``.\n  :pr:`19934` by :user:`Gleb Levitskiy <GLevV>`.\n\n- |Efficiency| The implementation of `fit` for\n  :class:`preprocessing.PolynomialFeatures` transformer is now faster. This is\n  especially noticeable on large sparse input. :pr:`19734` by :user:`Fred\n  Robinson <frrad>`.\n\n- |Fix| The :func:`preprocessing.StandardScaler.inverse_transform` method\n  now raises error when the input data is 1D. :pr:`19752` by :user:`Zhehao Liu\n  <Max1993Liu>`.\n\n- |Fix| :func:`preprocessing.scale`, :class:`preprocessing.StandardScaler`\n  and similar scalers detect near-constant features to avoid scaling them to\n  very large values. This problem happens in particular when using a scaler on\n  sparse data with a constant column with sample weights, in which case\n  centering is typically disabled. :pr:`19527` by :user:`Oliver Grisel\n  <ogrisel>` and :user:`Maria Telenczuk <maikia>` and :pr:`19788` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| :meth:`preprocessing.StandardScaler.inverse_transform` now\n  correctly handles integer dtypes. :pr:`19356` by :user:`makoeppel`.\n\n- |Fix| :meth:`preprocessing.OrdinalEncoder.inverse_transform` is not\n  supporting sparse matrix and raises the appropriate error message.\n  :pr:`19879` by :user:`Guillaume Lemaitre <glemaitre>`.\n\n- |Fix| The `fit` method of :class:`preprocessing.OrdinalEncoder` will not\n  raise error when `handle_unknown='ignore'` and unknown categories are given\n  to `fit`.\n  :pr:`19906` by :user:`Zhehao Liu <MaxwellLZH>`.\n\n- |Fix| Fix a regression in :class:`preprocessing.OrdinalEncoder` where large\n  Python numeric would raise an error due to overflow when casted to C type\n  (`np.float64` or `np.int64`).\n  :pr:`20727` by `Guillaume Lemaitre`_.\n\n- |Fix| :class:`preprocessing.FunctionTransformer` does not set `n_features_in_`\n  based on the input to `inverse_transform`. :pr:`20961` by `Thomas Fan`_.\n\n- |API| The `n_input_features_` attribute of\n  :class:`preprocessing.PolynomialFeatures` is deprecated in favor of\n  `n_features_in_` and will be removed in 1.2. :pr:`20240` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.svm`\n...................\n\n- |API| The parameter `**params` of :func:`svm.OneClassSVM.fit` is\n  deprecated and will be removed in 1.2.\n  :pr:`20843` by :user:`Juan Mart\u00edn Loyola <jmloyola>`.\n\n:mod:`sklearn.tree`\n...................\n\n- |Enhancement| Add `fontname` argument in :func:`tree.export_graphviz`\n  for non-English characters. :pr:`18959` by :user:`Zero <Zeroto521>`\n  and :user:`wstates <wstates>`.\n\n- |Fix| Improves compatibility of :func:`tree.plot_tree` with high DPI screens.\n  :pr:`20023` by `Thomas Fan`_.\n\n- |Fix| Fixed a bug in :class:`tree.DecisionTreeClassifier`,\n  :class:`tree.DecisionTreeRegressor` where a node could be split whereas it\n  should not have been due to incorrect handling of rounding errors.\n  :pr:`19336` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |API| The `n_features_` attribute of :class:`tree.DecisionTreeClassifier`,\n  :class:`tree.DecisionTreeRegressor`, :class:`tree.ExtraTreeClassifier` and\n  :class:`tree.ExtraTreeRegressor` is deprecated in favor of `n_features_in_`\n  and will be removed in 1.2. :pr:`20272` by\n  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n:mod:`sklearn.utils`\n....................\n\n- |Enhancement| Deprecated the default value of the `random_state=0` in\n  :func:`~sklearn.utils.extmath.randomized_svd`. Starting in 1.2,\n  the default value of `random_state` will be set to `None`.\n  :pr:`19459` by :user:`Cindy Bezuidenhout <cinbez>` and\n  :user:`Clifford Akai-Nettey<cliffordEmmanuel>`.\n\n- |Enhancement| Added helper decorator :func:`utils.metaestimators.available_if`\n  to provide flexiblity in metaestimators making methods available or\n  unavailable on the basis of state, in a more readable way.\n  :pr:`19948` by `Joel Nothman`_.\n\n- |Enhancement| :func:`utils.validation.check_is_fitted` now uses\n  ``__sklearn_is_fitted__`` if available, instead of checking for attributes\n  ending with an underscore. This also makes :class:`pipeline.Pipeline` and\n  :class:`preprocessing.FunctionTransformer` pass\n  ``check_is_fitted(estimator)``. :pr:`20657` by `Adrin Jalali`_.\n\n- |Fix| Fixed a bug in :func:`utils.sparsefuncs.mean_variance_axis` where the\n  precision of the computed variance was very poor when the real variance is\n  exactly zero. :pr:`19766` by :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n\n- |Fix| The docstrings of propreties that are decorated with\n  :func:`utils.deprecated` are now properly wrapped. :pr:`20385` by `Thomas\n  Fan`_.\n\n- |Fix| :func:`utils.stats._weighted_percentile` now correctly ignores\n  zero-weighted observations smaller than the smallest observation with\n  positive weight for ``percentile=0``. Affected classes are\n  :class:`dummy.DummyRegressor` for ``quantile=0`` and\n  :class:`ensemble.HuberLossFunction` and :class:`ensemble.HuberLossFunction`\n  for ``alpha=0``. :pr:`20528` by :user:`Malte Londschien <mlondschien>`.\n\n- |Fix| :func:`utils._safe_indexing` explicitly takes a dataframe copy when\n  integer indices are provided avoiding to raise a warning from Pandas. This\n  warning was previously raised in resampling utilities and functions using\n  those utilities (e.g. :func:`model_selection.train_test_split`,\n  :func:`model_selection.cross_validate`,\n  :func:`model_selection.cross_val_score`,\n  :func:`model_selection.cross_val_predict`).\n  :pr:`20673` by :user:`Joris Van den Bossche  <jorisvandenbossche>`.\n\n- |Fix| Fix a regression in :func:`utils.is_scalar_nan` where large Python\n  numbers would raise an error due to overflow in C types (`np.float64` or\n  `np.int64`).\n  :pr:`20727` by `Guillaume Lemaitre`_.\n\n- |Fix| Support for `np.matrix` is deprecated in\n  :func:`~sklearn.utils.check_array` in 1.0 and will raise a `TypeError` in\n  1.2. :pr:`20165` by `Thomas Fan`_.\n\n- |API| :func:`utils._testing.assert_warns` and\n  :func:`utils._testing.assert_warns_message` are deprecated in 1.0 and will\n  be removed in 1.2. Used `pytest.warns` context manager instead. Note that\n  these functions were not documented and part from the public API.\n  :pr:`20521` by :user:`Olivier Grisel <ogrisel>`.\n\n- |API| Fixed several bugs in :func:`utils.graph.graph_shortest_path`, which is\n  now deprecated. Use `scipy.sparse.csgraph.shortest_path` instead. :pr:`20531`\n  by `Tom Dupre la Tour`_.\n\nCode and Documentation Contributors\n-----------------------------------\n\nThanks to everyone who has contributed to the maintenance and improvement of\nthe project since version 0.24, including:\n\nAbdulelah S. Al Mesfer, Abhinav Gupta, Adam J. Stewart, Adam Li, Adam Midvidy,\nAdrian Garcia Badaracco, Adrian Sad\u0142ocha, Adrin Jalali, Agamemnon Krasoulis,\nAlberto Rubiales, Albert Thomas, Albert Villanova del Moral, Alek Lefebvre,\nAlessia Marcolini, Alexandr Fonari, Alihan Zihna, Aline Ribeiro de Almeida,\nAmanda, Amanda Dsouza, Amol Deshmukh, Ana Pessoa, Anavelyz, Andreas Mueller,\nAndrew Delong, Ashish, Ashvith Shetty, Atsushi Nukariya, Avi Gupta, Ayush\nSingh, baam, BaptBillard, Benjamin Pedigo, Bertrand Thirion, Bharat\nRaghunathan, bmalezieux, Brian Rice, Brian Sun, Bruno Charron, Bryan Chen,\nbumblebee, caherrera-meli, Carsten Allefeld, CeeThinwa, Chiara Marmo,\nchrissobel, Christian Lorentzen, Christopher Yeh, Chuliang Xiao, Cl\u00e9ment\nFauchereau, cliffordEmmanuel, Conner Shen, Connor Tann, David Dale, David Katz,\nDavid Poznik, Dimitri Papadopoulos Orfanos, Divyanshu Deoli, dmallia17,\nDmitry Kobak, DS_anas, Eduardo Jardim, EdwinWenink, EL-ATEIF Sara, Eleni\nMarkou, EricEllwanger, Eric Fiegel, Erich Schubert, Ezri-Mudde, Fatos Morina,\nFelipe Rodrigues, Felix Hafner, Fenil Suchak, flyingdutchman23, Flynn, Fortune\nUwha, Francois Berenger, Frankie Robertson, Frans Larsson, Frederick Robinson,\nfrellwan, Gabriel S Vicente, Gael Varoquaux, genvalen, Geoffrey Thomas,\ngeroldcsendes, Gleb Levitskiy, Glen, Gl\u00f2ria Maci\u00e0 Mu\u00f1oz, gregorystrubel,\ngroceryheist, Guillaume Lemaitre, guiweber, Haidar Almubarak, Hans Moritz\nG\u00fcnther, Haoyin Xu, Harris Mirza, Harry Wei, Harutaka Kawamura, Hassan\nAlsawadi, Helder Geovane Gomes de Lima, Hugo DEFOIS, Igor Ilic, Ikko Ashimine,\nIsaack Mungui, Ishaan Bhat, Ishan Mishra, Iv\u00e1n Pulido, iwhalvic, J Alexander,\nJack Liu, James Alan Preiss, James Budarz, James Lamb, Jannik, Jeff Zhao,\nJennifer Maldonado, J\u00e9r\u00e9mie du Boisberranger, Jesse Lima, Jianzhu Guo, jnboehm,\nJoel Nothman, JohanWork, John Paton, Jonathan Schneider, Jon Crall, Jon Haitz\nLegarreta Gorro\u00f1o, Joris Van den Bossche, Jos\u00e9 Manuel N\u00e1poles Duarte, Juan\nCarlos Alfaro Jim\u00e9nez, Juan Martin Loyola, Julien Jerphanion, Julio Batista\nSilva, julyrashchenko, JVM, Kadatatlu Kishore, Karen Palacio, Kei Ishikawa,\nkmatt10, kobaski, Kot271828, Kunj, KurumeYuta, kxytim, lacrosse91, LalliAcqua,\nLaveen Bagai, Leonardo Rocco, Leonardo Uieda, Leopoldo Corona, Loic Esteve,\nLSturtew, Luca Bittarello, Luccas Quadros, Lucy Jim\u00e9nez, Lucy Liu, ly648499246,\nMabu Manaileng, Manimaran, makoeppel, Marco Gorelli, Maren Westermann,\nMariangela, Maria Telenczuk, marielaraj, Martin Hirzel, Mateo Nore\u00f1a, Mathieu\nBlondel, Mathis Batoul, mathurinm, Matthew Calcote, Maxime Prieur, Maxwell,\nMehdi Hamoumi, Mehmet Ali \u00d6zer, Miao Cai, Michal Karbownik, michalkrawczyk,\nMitzi, mlondschien, Mohamed Haseeb, Mohamed Khoualed, Muhammad Jarir Kanji,\nmurata-yu, Nadim Kawwa, Nanshan Li, naozin555, Nate Parsons, Neal Fultz, Nic\nAnnau, Nicolas Hug, Nicolas Miller, Nico Stefani, Nigel Bosch, Nikita Titov,\nNodar Okroshiashvili, Norbert Preining, novaya, Ogbonna Chibuike Stephen,\nOGordon100, Oliver Pfaffel, Olivier Grisel, Oras Phongpanangam, Pablo Duque,\nPablo Ibieta-Jimenez, Patric Lacouth, Paulo S. Costa, Pawe\u0142 Olszewski, Peter\nDye, PierreAttard, Pierre-Yves Le Borgne, PranayAnchuri, Prince Canuma,\nputschblos, qdeffense, RamyaNP, ranjanikrishnan, Ray Bell, Rene Jean Corneille,\nReshama Shaikh, ricardojnf, RichardScottOZ, Rodion Martynov, Rohan Paul, Roman\nLutz, Roman Yurchak, Samuel Brice, Sandy Khosasi, Sean Benhur J, Sebastian\nFlores, Sebastian P\u00f6lsterl, Shao Yang Hong, shinehide, shinnar, shivamgargsya,\nShooter23, Shuhei Kayawari, Shyam Desai, simonamaggio, Sina Tootoonian,\nsolosilence, Steven Kolawole, Steve Stagg, Surya Prakash, swpease, Sylvain\nMari\u00e9, Takeshi Oura, Terence Honles, TFiFiE, Thomas A Caswell, Thomas J. Fan,\nTim Gates, TimotheeMathieu, Timothy Wolodzko, Tim Vink, t-jakubek, t-kusanagi,\ntliu68, Tobias Uhmann, tom1092, Tom\u00e1s Moreyra, Tom\u00e1s Ronald Hughes, Tom\nDupr\u00e9 la Tour, Tommaso Di Noto, Tomohiro Endo, TONY GEORGE, Toshihiro NAKAE,\ntsuga, Uttam kumar, vadim-ushtanit, Vangelis Gkiastas, Venkatachalam N, Vil\u00e9m\nZouhar, Vinicius Rios Fuck, Vlasovets, waijean, Whidou, xavier dupr\u00e9,\nxiaoyuchai, Yasmeen Alsaedy, yoch, Yosuke KOBAYASHI, Yu Feng, YusukeNagasaka,\nyzhenman, Zero, ZeyuSun, ZhaoweiWang, Zito, Zito Relova\n", "import numpy as np\nimport scipy.sparse as sp\nimport warnings\nfrom abc import ABCMeta, abstractmethod\n\n# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'\n# (and same for other imports)\nfrom . import _libsvm as libsvm  # type: ignore\nfrom . import _liblinear as liblinear  # type: ignore\nfrom . import _libsvm_sparse as libsvm_sparse  # type: ignore\nfrom ..base import BaseEstimator, ClassifierMixin\nfrom ..preprocessing import LabelEncoder\nfrom ..utils.multiclass import _ovr_decision_function\nfrom ..utils import check_array, check_random_state\nfrom ..utils import column_or_1d\nfrom ..utils import compute_class_weight\nfrom ..utils.metaestimators import available_if\nfrom ..utils.deprecation import deprecated\nfrom ..utils.extmath import safe_sparse_dot\nfrom ..utils.validation import check_is_fitted, _check_large_sparse\nfrom ..utils.validation import _num_samples\nfrom ..utils.validation import _check_sample_weight, check_consistent_length\nfrom ..utils.multiclass import check_classification_targets\nfrom ..exceptions import ConvergenceWarning\nfrom ..exceptions import NotFittedError\n\n\nLIBSVM_IMPL = [\"c_svc\", \"nu_svc\", \"one_class\", \"epsilon_svr\", \"nu_svr\"]\n\n\ndef _one_vs_one_coef(dual_coef, n_support, support_vectors):\n    \"\"\"Generate primal coefficients from dual coefficients\n    for the one-vs-one multi class LibSVM in the case\n    of a linear kernel.\"\"\"\n\n    # get 1vs1 weights for all n*(n-1) classifiers.\n    # this is somewhat messy.\n    # shape of dual_coef_ is nSV * (n_classes -1)\n    # see docs for details\n    n_class = dual_coef.shape[0] + 1\n\n    # XXX we could do preallocation of coef but\n    # would have to take care in the sparse case\n    coef = []\n    sv_locs = np.cumsum(np.hstack([[0], n_support]))\n    for class1 in range(n_class):\n        # SVs for class1:\n        sv1 = support_vectors[sv_locs[class1] : sv_locs[class1 + 1], :]\n        for class2 in range(class1 + 1, n_class):\n            # SVs for class1:\n            sv2 = support_vectors[sv_locs[class2] : sv_locs[class2 + 1], :]\n\n            # dual coef for class1 SVs:\n            alpha1 = dual_coef[class2 - 1, sv_locs[class1] : sv_locs[class1 + 1]]\n            # dual coef for class2 SVs:\n            alpha2 = dual_coef[class1, sv_locs[class2] : sv_locs[class2 + 1]]\n            # build weight for class1 vs class2\n\n            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))\n    return coef\n\n\nclass BaseLibSVM(BaseEstimator, metaclass=ABCMeta):\n    \"\"\"Base class for estimators that use libsvm as backing library.\n\n    This implements support vector machine classification and regression.\n\n    Parameter documentation is in the derived `SVC` class.\n    \"\"\"\n\n    # The order of these must match the integer values in LibSVM.\n    # XXX These are actually the same in the dense case. Need to factor\n    # this out.\n    _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]\n\n    @abstractmethod\n    def __init__(\n        self,\n        kernel,\n        degree,\n        gamma,\n        coef0,\n        tol,\n        C,\n        nu,\n        epsilon,\n        shrinking,\n        probability,\n        cache_size,\n        class_weight,\n        verbose,\n        max_iter,\n        random_state,\n    ):\n\n        if self._impl not in LIBSVM_IMPL:\n            raise ValueError(\n                \"impl should be one of %s, %s was given\" % (LIBSVM_IMPL, self._impl)\n            )\n\n        if gamma == 0:\n            msg = (\n                \"The gamma value of 0.0 is invalid. Use 'auto' to set\"\n                \" gamma to a value of 1 / n_features.\"\n            )\n            raise ValueError(msg)\n\n        self.kernel = kernel\n        self.degree = degree\n        self.gamma = gamma\n        self.coef0 = coef0\n        self.tol = tol\n        self.C = C\n        self.nu = nu\n        self.epsilon = epsilon\n        self.shrinking = shrinking\n        self.probability = probability\n        self.cache_size = cache_size\n        self.class_weight = class_weight\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.random_state = random_state\n\n    def _more_tags(self):\n        # Used by cross_val_score.\n        return {\"pairwise\": self.kernel == \"precomputed\"}\n\n    # TODO: Remove in 1.1\n    # mypy error: Decorated property not supported\n    @deprecated(  # type: ignore\n        \"Attribute `_pairwise` was deprecated in \"\n        \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\"\n    )\n    @property\n    def _pairwise(self):\n        # Used by cross_val_score.\n        return self.kernel == \"precomputed\"\n\n    def fit(self, X, y, sample_weight=None):\n        \"\"\"Fit the SVM model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\\n                or (n_samples, n_samples)\n            Training vectors, where `n_samples` is the number of samples\n            and `n_features` is the number of features.\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples, n_samples).\n\n        y : array-like of shape (n_samples,)\n            Target values (class labels in classification, real numbers in\n            regression).\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Per-sample weights. Rescale C per sample. Higher weights\n            force the classifier to put more emphasis on these points.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n\n        Notes\n        -----\n        If X and y are not C-ordered and contiguous arrays of np.float64 and\n        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n\n        If X is a dense array, then the other methods will not support sparse\n        matrices as input.\n        \"\"\"\n\n        rnd = check_random_state(self.random_state)\n\n        sparse = sp.isspmatrix(X)\n        if sparse and self.kernel == \"precomputed\":\n            raise TypeError(\"Sparse precomputed kernels are not supported.\")\n        self._sparse = sparse and not callable(self.kernel)\n\n        if hasattr(self, \"decision_function_shape\"):\n            if self.decision_function_shape not in (\"ovr\", \"ovo\"):\n                raise ValueError(\n                    \"decision_function_shape must be either 'ovr' or 'ovo', \"\n                    f\"got {self.decision_function_shape}.\"\n                )\n\n        if callable(self.kernel):\n            check_consistent_length(X, y)\n        else:\n            X, y = self._validate_data(\n                X,\n                y,\n                dtype=np.float64,\n                order=\"C\",\n                accept_sparse=\"csr\",\n                accept_large_sparse=False,\n            )\n\n        y = self._validate_targets(y)\n\n        sample_weight = np.asarray(\n            [] if sample_weight is None else sample_weight, dtype=np.float64\n        )\n        solver_type = LIBSVM_IMPL.index(self._impl)\n\n        # input validation\n        n_samples = _num_samples(X)\n        if solver_type != 2 and n_samples != y.shape[0]:\n            raise ValueError(\n                \"X and y have incompatible shapes.\\n\"\n                + \"X has %s samples, but y has %s.\" % (n_samples, y.shape[0])\n            )\n\n        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:\n            raise ValueError(\n                \"Precomputed matrix must be a square matrix.\"\n                \" Input is a {}x{} matrix.\".format(X.shape[0], X.shape[1])\n            )\n\n        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:\n            raise ValueError(\n                \"sample_weight and X have incompatible shapes: \"\n                \"%r vs %r\\n\"\n                \"Note: Sparse matrices cannot be indexed w/\"\n                \"boolean masks (use `indices=True` in CV).\"\n                % (sample_weight.shape, X.shape)\n            )\n\n        kernel = \"precomputed\" if callable(self.kernel) else self.kernel\n\n        if kernel == \"precomputed\":\n            # unused but needs to be a float for cython code that ignores\n            # it anyway\n            self._gamma = 0.0\n        elif isinstance(self.gamma, str):\n            if self.gamma == \"scale\":\n                # var = E[X^2] - E[X]^2 if sparse\n                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()\n                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0\n            elif self.gamma == \"auto\":\n                self._gamma = 1.0 / X.shape[1]\n            else:\n                raise ValueError(\n                    \"When 'gamma' is a string, it should be either 'scale' or \"\n                    \"'auto'. Got '{}' instead.\".format(self.gamma)\n                )\n        else:\n            self._gamma = self.gamma\n\n        fit = self._sparse_fit if self._sparse else self._dense_fit\n        if self.verbose:\n            print(\"[LibSVM]\", end=\"\")\n\n        seed = rnd.randint(np.iinfo(\"i\").max)\n        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\n        # see comment on the other call to np.iinfo in this file\n\n        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function. Use self._intercept_ and self._dual_coef_\n        # internally.\n        self._intercept_ = self.intercept_.copy()\n        self._dual_coef_ = self.dual_coef_\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            self.intercept_ *= -1\n            self.dual_coef_ = -self.dual_coef_\n\n        return self\n\n    def _validate_targets(self, y):\n        \"\"\"Validation of y and class_weight.\n\n        Default implementation for SVR and one-class; overridden in BaseSVC.\n        \"\"\"\n        # XXX this is ugly.\n        # Regression models should not have a class_weight_ attribute.\n        self.class_weight_ = np.empty(0)\n        return column_or_1d(y, warn=True).astype(np.float64, copy=False)\n\n    def _warn_from_fit_status(self):\n        assert self.fit_status_ in (0, 1)\n        if self.fit_status_ == 1:\n            warnings.warn(\n                \"Solver terminated early (max_iter=%i).\"\n                \"  Consider pre-processing your data with\"\n                \" StandardScaler or MinMaxScaler.\"\n                % self.max_iter,\n                ConvergenceWarning,\n            )\n\n    def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        if callable(self.kernel):\n            # you must store a reference to X to compute the kernel in predict\n            # TODO: add keyword copy to copy on demand\n            self.__Xfit = X\n            X = self._compute_kernel(X)\n\n            if X.shape[0] != X.shape[1]:\n                raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")\n\n        libsvm.set_verbosity_wrap(self.verbose)\n\n        # we don't pass **self.get_params() to allow subclasses to\n        # add other parameters to __init__\n        (\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self.dual_coef_,\n            self.intercept_,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n        ) = libsvm.fit(\n            X,\n            y,\n            svm_type=solver_type,\n            sample_weight=sample_weight,\n            class_weight=self.class_weight_,\n            kernel=kernel,\n            C=self.C,\n            nu=self.nu,\n            probability=self.probability,\n            degree=self.degree,\n            shrinking=self.shrinking,\n            tol=self.tol,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            epsilon=self.epsilon,\n            max_iter=self.max_iter,\n            random_seed=random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n        X.sort_indices()\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        libsvm_sparse.set_verbosity_wrap(self.verbose)\n\n        (\n            self.support_,\n            self.support_vectors_,\n            dual_coef_data,\n            self.intercept_,\n            self._n_support,\n            self._probA,\n            self._probB,\n            self.fit_status_,\n        ) = libsvm_sparse.libsvm_sparse_train(\n            X.shape[1],\n            X.data,\n            X.indices,\n            X.indptr,\n            y,\n            solver_type,\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            sample_weight,\n            self.nu,\n            self.cache_size,\n            self.epsilon,\n            int(self.shrinking),\n            int(self.probability),\n            self.max_iter,\n            random_seed,\n        )\n\n        self._warn_from_fit_status()\n\n        if hasattr(self, \"classes_\"):\n            n_class = len(self.classes_) - 1\n        else:  # regression\n            n_class = 1\n        n_SV = self.support_vectors_.shape[0]\n\n        dual_coef_indices = np.tile(np.arange(n_SV), n_class)\n        if not n_SV:\n            self.dual_coef_ = sp.csr_matrix([])\n        else:\n            dual_coef_indptr = np.arange(\n                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class\n            )\n            self.dual_coef_ = sp.csr_matrix(\n                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\n            )\n\n    def predict(self, X):\n        \"\"\"Perform regression on samples in X.\n\n        For an one-class model, +1 (inlier) or -1 (outlier) is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted values.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        predict = self._sparse_predict if self._sparse else self._dense_predict\n        return predict(X)\n\n    def _dense_predict(self, X):\n        X = self._compute_kernel(X)\n        if X.ndim == 1:\n            X = check_array(X, order=\"C\", accept_large_sparse=False)\n\n        kernel = self.kernel\n        if callable(self.kernel):\n            kernel = \"precomputed\"\n            if X.shape[1] != self.shape_fit_[0]:\n                raise ValueError(\n                    \"X.shape[1] = %d should be equal to %d, \"\n                    \"the number of samples at training time\"\n                    % (X.shape[1], self.shape_fit_[0])\n                )\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n\n        return libsvm.predict(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            coef0=self.coef0,\n            gamma=self._gamma,\n            cache_size=self.cache_size,\n        )\n\n    def _sparse_predict(self, X):\n        # Precondition: X is a csr_matrix of dtype np.float64.\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        C = 0.0  # C is not useful here\n\n        return libsvm_sparse.libsvm_sparse_predict(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _compute_kernel(self, X):\n        \"\"\"Return the data transformed by a callable kernel\"\"\"\n        if callable(self.kernel):\n            # in the case of precomputed kernel given as a function, we\n            # have to compute explicitly the kernel matrix\n            kernel = self.kernel(X, self.__Xfit)\n            if sp.issparse(kernel):\n                kernel = kernel.toarray()\n            X = np.asarray(kernel, dtype=np.float64, order=\"C\")\n        return X\n\n    def _decision_function(self, X):\n        \"\"\"Evaluates the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n\n        Returns\n        -------\n        X : array-like of shape (n_samples, n_class * (n_class-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n        \"\"\"\n        # NOTE: _validate_for_predict contains check for is_fitted\n        # hence must be placed before any other attributes are used.\n        X = self._validate_for_predict(X)\n        X = self._compute_kernel(X)\n\n        if self._sparse:\n            dec_func = self._sparse_decision_function(X)\n        else:\n            dec_func = self._dense_decision_function(X)\n\n        # In binary case, we need to flip the sign of coef, intercept and\n        # decision function.\n        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:\n            return -dec_func.ravel()\n\n        return dec_func\n\n    def _dense_decision_function(self, X):\n        X = check_array(X, dtype=np.float64, order=\"C\", accept_large_sparse=False)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        return libsvm.decision_function(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=LIBSVM_IMPL.index(self._impl),\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n    def _sparse_decision_function(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if hasattr(kernel, \"__call__\"):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_decision_function(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _validate_for_predict(self, X):\n        check_is_fitted(self)\n\n        if not callable(self.kernel):\n            X = self._validate_data(\n                X,\n                accept_sparse=\"csr\",\n                dtype=np.float64,\n                order=\"C\",\n                accept_large_sparse=False,\n                reset=False,\n            )\n\n        if self._sparse and not sp.isspmatrix(X):\n            X = sp.csr_matrix(X)\n        if self._sparse:\n            X.sort_indices()\n\n        if sp.issparse(X) and not self._sparse and not callable(self.kernel):\n            raise ValueError(\n                \"cannot use sparse input in %r trained on dense data\"\n                % type(self).__name__\n            )\n\n        if self.kernel == \"precomputed\":\n            if X.shape[1] != self.shape_fit_[0]:\n                raise ValueError(\n                    \"X.shape[1] = %d should be equal to %d, \"\n                    \"the number of samples at training time\"\n                    % (X.shape[1], self.shape_fit_[0])\n                )\n        # Fixes https://nvd.nist.gov/vuln/detail/CVE-2020-28975\n        # Check that _n_support is consistent with support_vectors\n        sv = self.support_vectors_\n        if not self._sparse and sv.size > 0 and self.n_support_.sum() != sv.shape[0]:\n            raise ValueError(\n                f\"The internal representation of {self.__class__.__name__} was altered\"\n            )\n        return X\n\n    @property\n    def coef_(self):\n        \"\"\"Weights assigned to the features when `kernel=\"linear\"`.\n\n        Returns\n        -------\n        ndarray of shape (n_features, n_classes)\n        \"\"\"\n        if self.kernel != \"linear\":\n            raise AttributeError(\"coef_ is only available when using a linear kernel\")\n\n        coef = self._get_coef()\n\n        # coef_ being a read-only property, it's better to mark the value as\n        # immutable to avoid hiding potential bugs for the unsuspecting user.\n        if sp.issparse(coef):\n            # sparse matrix do not have global flags\n            coef.data.flags.writeable = False\n        else:\n            # regular dense array\n            coef.flags.writeable = False\n        return coef\n\n    def _get_coef(self):\n        return safe_sparse_dot(self._dual_coef_, self.support_vectors_)\n\n    @property\n    def n_support_(self):\n        \"\"\"Number of support vectors for each class.\"\"\"\n        try:\n            check_is_fitted(self)\n        except NotFittedError:\n            raise AttributeError\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        if svm_type in (0, 1):\n            return self._n_support\n        else:\n            # SVR and OneClass\n            # _n_support has size 2, we make it size 1\n            return np.array([self._n_support[0]])\n\n\nclass BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):\n    \"\"\"ABC for LibSVM-based classifiers.\"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        kernel,\n        degree,\n        gamma,\n        coef0,\n        tol,\n        C,\n        nu,\n        shrinking,\n        probability,\n        cache_size,\n        class_weight,\n        verbose,\n        max_iter,\n        decision_function_shape,\n        random_state,\n        break_ties,\n    ):\n        self.decision_function_shape = decision_function_shape\n        self.break_ties = break_ties\n        super().__init__(\n            kernel=kernel,\n            degree=degree,\n            gamma=gamma,\n            coef0=coef0,\n            tol=tol,\n            C=C,\n            nu=nu,\n            epsilon=0.0,\n            shrinking=shrinking,\n            probability=probability,\n            cache_size=cache_size,\n            class_weight=class_weight,\n            verbose=verbose,\n            max_iter=max_iter,\n            random_state=random_state,\n        )\n\n    def _validate_targets(self, y):\n        y_ = column_or_1d(y, warn=True)\n        check_classification_targets(y)\n        cls, y = np.unique(y_, return_inverse=True)\n        self.class_weight_ = compute_class_weight(self.class_weight, classes=cls, y=y_)\n        if len(cls) < 2:\n            raise ValueError(\n                \"The number of classes has to be greater than one; got %d class\"\n                % len(cls)\n            )\n\n        self.classes_ = cls\n\n        return np.asarray(y, dtype=np.float64, order=\"C\")\n\n    def decision_function(self, X):\n        \"\"\"Evaluate the decision function for the samples in X.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples.\n\n        Returns\n        -------\n        X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n            Returns the decision function of the sample for each class\n            in the model.\n            If decision_function_shape='ovr', the shape is (n_samples,\n            n_classes).\n\n        Notes\n        -----\n        If decision_function_shape='ovo', the function values are proportional\n        to the distance of the samples X to the separating hyperplane. If the\n        exact distances are required, divide the function values by the norm of\n        the weight vector (``coef_``). See also `this question\n        <https://stats.stackexchange.com/questions/14876/\n        interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n        If decision_function_shape='ovr', the decision function is a monotonic\n        transformation of ovo decision function.\n        \"\"\"\n        dec = self._decision_function(X)\n        if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:\n            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))\n        return dec\n\n    def predict(self, X):\n        \"\"\"Perform classification on samples in X.\n\n        For an one-class model, +1 or -1 is returned.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\\n                (n_samples_test, n_samples_train)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            Class labels for samples in X.\n        \"\"\"\n        check_is_fitted(self)\n        if self.break_ties and self.decision_function_shape == \"ovo\":\n            raise ValueError(\n                \"break_ties must be False when decision_function_shape is 'ovo'\"\n            )\n\n        if (\n            self.break_ties\n            and self.decision_function_shape == \"ovr\"\n            and len(self.classes_) > 2\n        ):\n            y = np.argmax(self.decision_function(X), axis=1)\n        else:\n            y = super().predict(X)\n        return self.classes_.take(np.asarray(y, dtype=np.intp))\n\n    # Hacky way of getting predict_proba to raise an AttributeError when\n    # probability=False using properties. Do not use this in new code; when\n    # probabilities are not available depending on a setting, introduce two\n    # estimators.\n    def _check_proba(self):\n        if not self.probability:\n            raise AttributeError(\n                \"predict_proba is not available when  probability=False\"\n            )\n        if self._impl not in (\"c_svc\", \"nu_svc\"):\n            raise AttributeError(\"predict_proba only implemented for SVC and NuSVC\")\n        return True\n\n    @available_if(_check_proba)\n    def predict_proba(self, X):\n        \"\"\"Compute probabilities of possible outcomes for samples in X.\n\n        The model need to have probability information computed at training\n        time: fit with attribute `probability` set to True.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        T : ndarray of shape (n_samples, n_classes)\n            Returns the probability of the sample for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n\n        Notes\n        -----\n        The probability model is created using cross validation, so\n        the results can be slightly different than those obtained by\n        predict. Also, it will produce meaningless results on very small\n        datasets.\n        \"\"\"\n        X = self._validate_for_predict(X)\n        if self.probA_.size == 0 or self.probB_.size == 0:\n            raise NotFittedError(\n                \"predict_proba is not available when fitted with probability=False\"\n            )\n        pred_proba = (\n            self._sparse_predict_proba if self._sparse else self._dense_predict_proba\n        )\n        return pred_proba(X)\n\n    @available_if(_check_proba)\n    def predict_log_proba(self, X):\n        \"\"\"Compute log probabilities of possible outcomes for samples in X.\n\n        The model need to have probability information computed at training\n        time: fit with attribute `probability` set to True.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features) or \\\n                (n_samples_test, n_samples_train)\n            For kernel=\"precomputed\", the expected shape of X is\n            (n_samples_test, n_samples_train).\n\n        Returns\n        -------\n        T : ndarray of shape (n_samples, n_classes)\n            Returns the log-probabilities of the sample for each class in\n            the model. The columns correspond to the classes in sorted\n            order, as they appear in the attribute :term:`classes_`.\n\n        Notes\n        -----\n        The probability model is created using cross validation, so\n        the results can be slightly different than those obtained by\n        predict. Also, it will produce meaningless results on very small\n        datasets.\n        \"\"\"\n        return np.log(self.predict_proba(X))\n\n    def _dense_predict_proba(self, X):\n        X = self._compute_kernel(X)\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        svm_type = LIBSVM_IMPL.index(self._impl)\n        pprob = libsvm.predict_proba(\n            X,\n            self.support_,\n            self.support_vectors_,\n            self._n_support,\n            self._dual_coef_,\n            self._intercept_,\n            self._probA,\n            self._probB,\n            svm_type=svm_type,\n            kernel=kernel,\n            degree=self.degree,\n            cache_size=self.cache_size,\n            coef0=self.coef0,\n            gamma=self._gamma,\n        )\n\n        return pprob\n\n    def _sparse_predict_proba(self, X):\n        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")\n\n        kernel = self.kernel\n        if callable(kernel):\n            kernel = \"precomputed\"\n\n        kernel_type = self._sparse_kernels.index(kernel)\n\n        return libsvm_sparse.libsvm_sparse_predict_proba(\n            X.data,\n            X.indices,\n            X.indptr,\n            self.support_vectors_.data,\n            self.support_vectors_.indices,\n            self.support_vectors_.indptr,\n            self._dual_coef_.data,\n            self._intercept_,\n            LIBSVM_IMPL.index(self._impl),\n            kernel_type,\n            self.degree,\n            self._gamma,\n            self.coef0,\n            self.tol,\n            self.C,\n            self.class_weight_,\n            self.nu,\n            self.epsilon,\n            self.shrinking,\n            self.probability,\n            self._n_support,\n            self._probA,\n            self._probB,\n        )\n\n    def _get_coef(self):\n        if self.dual_coef_.shape[0] == 1:\n            # binary classifier\n            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)\n        else:\n            # 1vs1 classifier\n            coef = _one_vs_one_coef(\n                self.dual_coef_, self._n_support, self.support_vectors_\n            )\n            if sp.issparse(coef[0]):\n                coef = sp.vstack(coef).tocsr()\n            else:\n                coef = np.vstack(coef)\n\n        return coef\n\n    @property\n    def probA_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probA\n\n    @property\n    def probB_(self):\n        \"\"\"Parameter learned in Platt scaling when `probability=True`.\n\n        Returns\n        -------\n        ndarray of shape  (n_classes * (n_classes - 1) / 2)\n        \"\"\"\n        return self._probB\n\n\ndef _get_liblinear_solver_type(multi_class, penalty, loss, dual):\n    \"\"\"Find the liblinear magic number for the solver.\n\n    This number depends on the values of the following attributes:\n      - multi_class\n      - penalty\n      - loss\n      - dual\n\n    The same number is also internally used by LibLinear to determine\n    which solver to use.\n    \"\"\"\n    # nested dicts containing level 1: available loss functions,\n    # level2: available penalties for the given loss function,\n    # level3: whether the dual solver is available for the specified\n    # combination of loss function and penalty\n    _solver_type_dict = {\n        \"logistic_regression\": {\"l1\": {False: 6}, \"l2\": {False: 0, True: 7}},\n        \"hinge\": {\"l2\": {True: 3}},\n        \"squared_hinge\": {\"l1\": {False: 5}, \"l2\": {False: 2, True: 1}},\n        \"epsilon_insensitive\": {\"l2\": {True: 13}},\n        \"squared_epsilon_insensitive\": {\"l2\": {False: 11, True: 12}},\n        \"crammer_singer\": 4,\n    }\n\n    if multi_class == \"crammer_singer\":\n        return _solver_type_dict[multi_class]\n    elif multi_class != \"ovr\":\n        raise ValueError(\n            \"`multi_class` must be one of `ovr`, `crammer_singer`, got %r\" % multi_class\n        )\n\n    _solver_pen = _solver_type_dict.get(loss, None)\n    if _solver_pen is None:\n        error_string = \"loss='%s' is not supported\" % loss\n    else:\n        _solver_dual = _solver_pen.get(penalty, None)\n        if _solver_dual is None:\n            error_string = (\n                \"The combination of penalty='%s' and loss='%s' is not supported\"\n                % (penalty, loss)\n            )\n        else:\n            solver_num = _solver_dual.get(dual, None)\n            if solver_num is None:\n                error_string = (\n                    \"The combination of penalty='%s' and \"\n                    \"loss='%s' are not supported when dual=%s\" % (penalty, loss, dual)\n                )\n            else:\n                return solver_num\n    raise ValueError(\n        \"Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r\"\n        % (error_string, penalty, loss, dual)\n    )\n\n\ndef _fit_liblinear(\n    X,\n    y,\n    C,\n    fit_intercept,\n    intercept_scaling,\n    class_weight,\n    penalty,\n    dual,\n    verbose,\n    max_iter,\n    tol,\n    random_state=None,\n    multi_class=\"ovr\",\n    loss=\"logistic_regression\",\n    epsilon=0.1,\n    sample_weight=None,\n):\n    \"\"\"Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.\n\n    Preprocessing is done in this function before supplying it to liblinear.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Training vector, where `n_samples` is the number of samples and\n        `n_features` is the number of features.\n\n    y : array-like of shape (n_samples,)\n        Target vector relative to X\n\n    C : float\n        Inverse of cross-validation parameter. Lower the C, the more\n        the penalization.\n\n    fit_intercept : bool\n        Whether or not to fit the intercept, that is to add a intercept\n        term to the decision function.\n\n    intercept_scaling : float\n        LibLinear internally penalizes the intercept and this term is subject\n        to regularization just like the other terms of the feature vector.\n        In order to avoid this, one should increase the intercept_scaling.\n        such that the feature vector becomes [x, intercept_scaling].\n\n    class_weight : dict or 'balanced', default=None\n        Weights associated with classes in the form ``{class_label: weight}``.\n        If not given, all classes are supposed to have weight one. For\n        multi-output problems, a list of dicts can be provided in the same\n        order as the columns of y.\n\n        The \"balanced\" mode uses the values of y to automatically adjust\n        weights inversely proportional to class frequencies in the input data\n        as ``n_samples / (n_classes * np.bincount(y))``\n\n    penalty : {'l1', 'l2'}\n        The norm of the penalty used in regularization.\n\n    dual : bool\n        Dual or primal formulation,\n\n    verbose : int\n        Set verbose to any positive number for verbosity.\n\n    max_iter : int\n        Number of iterations.\n\n    tol : float\n        Stopping condition.\n\n    random_state : int, RandomState instance or None, default=None\n        Controls the pseudo random number generation for shuffling the data.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n        `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`\n        optimizes a joint objective over all classes.\n        While `crammer_singer` is interesting from an theoretical perspective\n        as it is consistent it is seldom used in practice and rarely leads to\n        better accuracy and is more expensive to compute.\n        If `crammer_singer` is chosen, the options loss, penalty and dual will\n        be ignored.\n\n    loss : {'logistic_regression', 'hinge', 'squared_hinge', \\\n            'epsilon_insensitive', 'squared_epsilon_insensitive}, \\\n            default='logistic_regression'\n        The loss function used to fit the model.\n\n    epsilon : float, default=0.1\n        Epsilon parameter in the epsilon-insensitive loss function. Note\n        that the value of this parameter depends on the scale of the target\n        variable y. If unsure, set epsilon=0.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weights assigned to each sample.\n\n    Returns\n    -------\n    coef_ : ndarray of shape (n_features, n_features + 1)\n        The coefficient vector got by minimizing the objective function.\n\n    intercept_ : float\n        The intercept term added to the vector.\n\n    n_iter_ : int\n        Maximum number of iterations run across all classes.\n    \"\"\"\n    if loss not in [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"]:\n        enc = LabelEncoder()\n        y_ind = enc.fit_transform(y)\n        classes_ = enc.classes_\n        if len(classes_) < 2:\n            raise ValueError(\n                \"This solver needs samples of at least 2 classes\"\n                \" in the data, but the data contains only one\"\n                \" class: %r\"\n                % classes_[0]\n            )\n\n        class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)\n    else:\n        class_weight_ = np.empty(0, dtype=np.float64)\n        y_ind = y\n    liblinear.set_verbosity_wrap(verbose)\n    rnd = check_random_state(random_state)\n    if verbose:\n        print(\"[LibLinear]\", end=\"\")\n\n    # LinearSVC breaks when intercept_scaling is <= 0\n    bias = -1.0\n    if fit_intercept:\n        if intercept_scaling <= 0:\n            raise ValueError(\n                \"Intercept scaling is %r but needs to be greater \"\n                \"than 0. To disable fitting an intercept,\"\n                \" set fit_intercept=False.\" % intercept_scaling\n            )\n        else:\n            bias = intercept_scaling\n\n    libsvm.set_verbosity_wrap(verbose)\n    libsvm_sparse.set_verbosity_wrap(verbose)\n    liblinear.set_verbosity_wrap(verbose)\n\n    # Liblinear doesn't support 64bit sparse matrix indices yet\n    if sp.issparse(X):\n        _check_large_sparse(X)\n\n    # LibLinear wants targets as doubles, even for classification\n    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()\n    y_ind = np.require(y_ind, requirements=\"W\")\n\n    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)\n\n    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n    raw_coef_, n_iter_ = liblinear.train_wrap(\n        X,\n        y_ind,\n        sp.isspmatrix(X),\n        solver_type,\n        tol,\n        bias,\n        C,\n        class_weight_,\n        max_iter,\n        rnd.randint(np.iinfo(\"i\").max),\n        epsilon,\n        sample_weight,\n    )\n    # Regarding rnd.randint(..) in the above signature:\n    # seed for srand in range [0..INT_MAX); due to limitations in Numpy\n    # on 32-bit platforms, we can't get to the UINT_MAX limit that\n    # srand supports\n    n_iter_ = max(n_iter_)\n    if n_iter_ >= max_iter:\n        warnings.warn(\n            \"Liblinear failed to converge, increase the number of iterations.\",\n            ConvergenceWarning,\n        )\n\n    if fit_intercept:\n        coef_ = raw_coef_[:, :-1]\n        intercept_ = intercept_scaling * raw_coef_[:, -1]\n    else:\n        coef_ = raw_coef_\n        intercept_ = 0.0\n\n    return coef_, intercept_, n_iter_\n", "\"\"\"\nTesting for Support Vector Machine module (sklearn.svm)\n\nTODO: remove hard coded numerical results when possible\n\"\"\"\nimport numpy as np\nimport itertools\nimport pytest\nimport re\n\nfrom numpy.testing import assert_array_equal, assert_array_almost_equal\nfrom numpy.testing import assert_almost_equal\nfrom numpy.testing import assert_allclose\nfrom scipy import sparse\nfrom sklearn import svm, linear_model, datasets, metrics, base\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification, make_blobs\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.utils.validation import _num_samples\nfrom sklearn.utils import shuffle\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.exceptions import NotFittedError, UndefinedMetricWarning\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# mypy error: Module 'sklearn.svm' has no attribute '_libsvm'\nfrom sklearn.svm import _libsvm  # type: ignore\n\n# toy sample\nX = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\nY = [1, 1, 1, 2, 2, 2]\nT = [[-1, -1], [2, 2], [3, 2]]\ntrue_result = [1, 2, 2]\n\n# also load the iris dataset\niris = datasets.load_iris()\nrng = check_random_state(42)\nperm = rng.permutation(iris.target.size)\niris.data = iris.data[perm]\niris.target = iris.target[perm]\n\n\ndef test_libsvm_parameters():\n    # Test parameters on classes that make use of libsvm.\n    clf = svm.SVC(kernel=\"linear\").fit(X, Y)\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.support_, [1, 3])\n    assert_array_equal(clf.support_vectors_, (X[1], X[3]))\n    assert_array_equal(clf.intercept_, [0.0])\n    assert_array_equal(clf.predict(X), Y)\n\n\ndef test_libsvm_iris():\n    # Check consistency on dataset iris.\n\n    # shuffle the dataset so that labels are not ordered\n    for k in (\"linear\", \"rbf\"):\n        clf = svm.SVC(kernel=k).fit(iris.data, iris.target)\n        assert np.mean(clf.predict(iris.data) == iris.target) > 0.9\n        assert hasattr(clf, \"coef_\") == (k == \"linear\")\n\n    assert_array_equal(clf.classes_, np.sort(clf.classes_))\n\n    # check also the low-level API\n    model = _libsvm.fit(iris.data, iris.target.astype(np.float64))\n    pred = _libsvm.predict(iris.data, *model)\n    assert np.mean(pred == iris.target) > 0.95\n\n    model = _libsvm.fit(iris.data, iris.target.astype(np.float64), kernel=\"linear\")\n    pred = _libsvm.predict(iris.data, *model, kernel=\"linear\")\n    assert np.mean(pred == iris.target) > 0.95\n\n    pred = _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n    assert np.mean(pred == iris.target) > 0.95\n\n    # If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence\n    # we should get deterministic results (assuming that there is no other\n    # thread calling this wrapper calling `srand` concurrently).\n    pred2 = _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n    assert_array_equal(pred, pred2)\n\n\ndef test_precomputed():\n    # SVC with a precomputed kernel.\n    # We test it with a toy dataset and with iris.\n    clf = svm.SVC(kernel=\"precomputed\")\n    # Gram matrix for train data (square matrix)\n    # (we use just a linear kernel)\n    K = np.dot(X, np.array(X).T)\n    clf.fit(K, Y)\n    # Gram matrix for test data (rectangular matrix)\n    KT = np.dot(T, np.array(X).T)\n    pred = clf.predict(KT)\n    with pytest.raises(ValueError):\n        clf.predict(KT.T)\n\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.support_, [1, 3])\n    assert_array_equal(clf.intercept_, [0])\n    assert_array_almost_equal(clf.support_, [1, 3])\n    assert_array_equal(pred, true_result)\n\n    # Gram matrix for test data but compute KT[i,j]\n    # for support vectors j only.\n    KT = np.zeros_like(KT)\n    for i in range(len(T)):\n        for j in clf.support_:\n            KT[i, j] = np.dot(T[i], X[j])\n\n    pred = clf.predict(KT)\n    assert_array_equal(pred, true_result)\n\n    # same as before, but using a callable function instead of the kernel\n    # matrix. kernel is just a linear kernel\n\n    def kfunc(x, y):\n        return np.dot(x, y.T)\n\n    clf = svm.SVC(kernel=kfunc)\n    clf.fit(np.array(X), Y)\n    pred = clf.predict(T)\n\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.intercept_, [0])\n    assert_array_almost_equal(clf.support_, [1, 3])\n    assert_array_equal(pred, true_result)\n\n    # test a precomputed kernel with the iris dataset\n    # and check parameters against a linear SVC\n    clf = svm.SVC(kernel=\"precomputed\")\n    clf2 = svm.SVC(kernel=\"linear\")\n    K = np.dot(iris.data, iris.data.T)\n    clf.fit(K, iris.target)\n    clf2.fit(iris.data, iris.target)\n    pred = clf.predict(K)\n    assert_array_almost_equal(clf.support_, clf2.support_)\n    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)\n    assert_array_almost_equal(clf.intercept_, clf2.intercept_)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n    # Gram matrix for test data but compute KT[i,j]\n    # for support vectors j only.\n    K = np.zeros_like(K)\n    for i in range(len(iris.data)):\n        for j in clf.support_:\n            K[i, j] = np.dot(iris.data[i], iris.data[j])\n\n    pred = clf.predict(K)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n    clf = svm.SVC(kernel=kfunc)\n    clf.fit(iris.data, iris.target)\n    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)\n\n\ndef test_svr():\n    # Test Support Vector Regression\n\n    diabetes = datasets.load_diabetes()\n    for clf in (\n        svm.NuSVR(kernel=\"linear\", nu=0.4, C=1.0),\n        svm.NuSVR(kernel=\"linear\", nu=0.4, C=10.0),\n        svm.SVR(kernel=\"linear\", C=10.0),\n        svm.LinearSVR(C=10.0),\n        svm.LinearSVR(C=10.0),\n    ):\n        clf.fit(diabetes.data, diabetes.target)\n        assert clf.score(diabetes.data, diabetes.target) > 0.02\n\n    # non-regression test; previously, BaseLibSVM would check that\n    # len(np.unique(y)) < 2, which must only be done for SVC\n    svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n\n\ndef test_linearsvr():\n    # check that SVR(kernel='linear') and LinearSVC() give\n    # comparable results\n    diabetes = datasets.load_diabetes()\n    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)\n    score1 = lsvr.score(diabetes.data, diabetes.target)\n\n    svr = svm.SVR(kernel=\"linear\", C=1e3).fit(diabetes.data, diabetes.target)\n    score2 = svr.score(diabetes.data, diabetes.target)\n\n    assert_allclose(np.linalg.norm(lsvr.coef_), np.linalg.norm(svr.coef_), 1, 0.0001)\n    assert_almost_equal(score1, score2, 2)\n\n\ndef test_linearsvr_fit_sampleweight():\n    # check correct result when sample_weight is 1\n    # check that SVR(kernel='linear') and LinearSVC() give\n    # comparable results\n    diabetes = datasets.load_diabetes()\n    n_samples = len(diabetes.target)\n    unit_weight = np.ones(n_samples)\n    lsvr = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target, sample_weight=unit_weight\n    )\n    score1 = lsvr.score(diabetes.data, diabetes.target)\n\n    lsvr_no_weight = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target\n    )\n    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)\n\n    assert_allclose(\n        np.linalg.norm(lsvr.coef_), np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001\n    )\n    assert_almost_equal(score1, score2, 2)\n\n    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where\n    # X = X1 repeated n1 times, X2 repeated n2 times and so forth\n    random_state = check_random_state(0)\n    random_weight = random_state.randint(0, 10, n_samples)\n    lsvr_unflat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(\n        diabetes.data, diabetes.target, sample_weight=random_weight\n    )\n    score3 = lsvr_unflat.score(\n        diabetes.data, diabetes.target, sample_weight=random_weight\n    )\n\n    X_flat = np.repeat(diabetes.data, random_weight, axis=0)\n    y_flat = np.repeat(diabetes.target, random_weight, axis=0)\n    lsvr_flat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)\n    score4 = lsvr_flat.score(X_flat, y_flat)\n\n    assert_almost_equal(score3, score4, 2)\n\n\ndef test_svr_errors():\n    X = [[0.0], [1.0]]\n    y = [0.0, 0.5]\n\n    # Bad kernel\n    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))\n    clf.fit(X, y)\n    with pytest.raises(ValueError):\n        clf.predict(X)\n\n\ndef test_oneclass():\n    # Test OneClassSVM\n    clf = svm.OneClassSVM()\n    clf.fit(X)\n    pred = clf.predict(T)\n\n    assert_array_equal(pred, [1, -1, -1])\n    assert pred.dtype == np.dtype(\"intp\")\n    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)\n    assert_array_almost_equal(clf.dual_coef_, [[0.750, 0.750, 0.750, 0.750]], decimal=3)\n    with pytest.raises(AttributeError):\n        (lambda: clf.coef_)()\n\n\ndef test_oneclass_decision_function():\n    # Test OneClassSVM decision function\n    clf = svm.OneClassSVM()\n    rnd = check_random_state(2)\n\n    # Generate train data\n    X = 0.3 * rnd.randn(100, 2)\n    X_train = np.r_[X + 2, X - 2]\n\n    # Generate some regular novel observations\n    X = 0.3 * rnd.randn(20, 2)\n    X_test = np.r_[X + 2, X - 2]\n    # Generate some abnormal novel observations\n    X_outliers = rnd.uniform(low=-4, high=4, size=(20, 2))\n\n    # fit the model\n    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n    clf.fit(X_train)\n\n    # predict things\n    y_pred_test = clf.predict(X_test)\n    assert np.mean(y_pred_test == 1) > 0.9\n    y_pred_outliers = clf.predict(X_outliers)\n    assert np.mean(y_pred_outliers == -1) > 0.9\n    dec_func_test = clf.decision_function(X_test)\n    assert_array_equal((dec_func_test > 0).ravel(), y_pred_test == 1)\n    dec_func_outliers = clf.decision_function(X_outliers)\n    assert_array_equal((dec_func_outliers > 0).ravel(), y_pred_outliers == 1)\n\n\ndef test_oneclass_score_samples():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = svm.OneClassSVM(gamma=1).fit(X_train)\n    assert_array_equal(\n        clf.score_samples([[2.0, 2.0]]),\n        clf.decision_function([[2.0, 2.0]]) + clf.offset_,\n    )\n\n\n# TODO: Remove in v1.2\ndef test_oneclass_fit_params_is_deprecated():\n    clf = svm.OneClassSVM()\n    params = {\n        \"unused_param\": \"\",\n        \"extra_param\": None,\n    }\n    msg = (\n        \"Passing additional keyword parameters has no effect and is deprecated \"\n        \"in 1.0. An error will be raised from 1.2 and beyond. The ignored \"\n        f\"keyword parameter(s) are: {params.keys()}.\"\n    )\n    with pytest.warns(FutureWarning, match=re.escape(msg)):\n        clf.fit(X, **params)\n\n\ndef test_tweak_params():\n    # Make sure some tweaking of parameters works.\n    # We change clf.dual_coef_ at run time and expect .predict() to change\n    # accordingly. Notice that this is not trivial since it involves a lot\n    # of C/Python copying in the libsvm bindings.\n    # The success of this test ensures that the mapping between libsvm and\n    # the python classifier is complete.\n    clf = svm.SVC(kernel=\"linear\", C=1.0)\n    clf.fit(X, Y)\n    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])\n    assert_array_equal(clf.predict([[-0.1, -0.1]]), [1])\n    clf._dual_coef_ = np.array([[0.0, 1.0]])\n    assert_array_equal(clf.predict([[-0.1, -0.1]]), [2])\n\n\ndef test_probability():\n    # Predict probabilities using SVC\n    # This uses cross validation, so we use a slightly bigger testing set.\n\n    for clf in (\n        svm.SVC(probability=True, random_state=0, C=1.0),\n        svm.NuSVC(probability=True, random_state=0),\n    ):\n        clf.fit(iris.data, iris.target)\n\n        prob_predict = clf.predict_proba(iris.data)\n        assert_array_almost_equal(np.sum(prob_predict, 1), np.ones(iris.data.shape[0]))\n        assert np.mean(np.argmax(prob_predict, 1) == clf.predict(iris.data)) > 0.9\n\n        assert_almost_equal(\n            clf.predict_proba(iris.data), np.exp(clf.predict_log_proba(iris.data)), 8\n        )\n\n\ndef test_decision_function():\n    # Test decision_function\n    # Sanity check, test that decision_function implemented in python\n    # returns the same as the one in libsvm\n    # multi class:\n    clf = svm.SVC(kernel=\"linear\", C=0.1, decision_function_shape=\"ovo\").fit(\n        iris.data, iris.target\n    )\n\n    dec = np.dot(iris.data, clf.coef_.T) + clf.intercept_\n\n    assert_array_almost_equal(dec, clf.decision_function(iris.data))\n\n    # binary:\n    clf.fit(X, Y)\n    dec = np.dot(X, clf.coef_.T) + clf.intercept_\n    prediction = clf.predict(X)\n    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))\n    assert_array_almost_equal(\n        prediction, clf.classes_[(clf.decision_function(X) > 0).astype(int)]\n    )\n    expected = np.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])\n    assert_array_almost_equal(clf.decision_function(X), expected, 2)\n\n    # kernel binary:\n    clf = svm.SVC(kernel=\"rbf\", gamma=1, decision_function_shape=\"ovo\")\n    clf.fit(X, Y)\n\n    rbfs = rbf_kernel(X, clf.support_vectors_, gamma=clf.gamma)\n    dec = np.dot(rbfs, clf.dual_coef_.T) + clf.intercept_\n    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))\n\n\n@pytest.mark.parametrize(\"SVM\", (svm.SVC, svm.NuSVC))\ndef test_decision_function_shape(SVM):\n    # check that decision_function_shape='ovr' or 'ovo' gives\n    # correct shape and is consistent with predict\n\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(\n        iris.data, iris.target\n    )\n    dec = clf.decision_function(iris.data)\n    assert dec.shape == (len(iris.data), 3)\n    assert_array_equal(clf.predict(iris.data), np.argmax(dec, axis=1))\n\n    # with five classes:\n    X, y = make_blobs(n_samples=80, centers=5, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(X_train, y_train)\n    dec = clf.decision_function(X_test)\n    assert dec.shape == (len(X_test), 5)\n    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))\n\n    # check shape of ovo_decition_function=True\n    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovo\").fit(X_train, y_train)\n    dec = clf.decision_function(X_train)\n    assert dec.shape == (len(X_train), 10)\n\n    with pytest.raises(ValueError, match=\"must be either 'ovr' or 'ovo'\"):\n        SVM(decision_function_shape=\"bad\").fit(X_train, y_train)\n\n\ndef test_svr_predict():\n    # Test SVR's decision_function\n    # Sanity check, test that predict implemented in python\n    # returns the same as the one in libsvm\n\n    X = iris.data\n    y = iris.target\n\n    # linear kernel\n    reg = svm.SVR(kernel=\"linear\", C=0.1).fit(X, y)\n\n    dec = np.dot(X, reg.coef_.T) + reg.intercept_\n    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())\n\n    # rbf kernel\n    reg = svm.SVR(kernel=\"rbf\", gamma=1).fit(X, y)\n\n    rbfs = rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)\n    dec = np.dot(rbfs, reg.dual_coef_.T) + reg.intercept_\n    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())\n\n\ndef test_weight():\n    # Test class weights\n    clf = svm.SVC(class_weight={1: 0.1})\n    # we give a small weights to class 1\n    clf.fit(X, Y)\n    # so all predicted values belong to class 2\n    assert_array_almost_equal(clf.predict(X), [2] * 6)\n\n    X_, y_ = make_classification(\n        n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2\n    )\n\n    for clf in (\n        linear_model.LogisticRegression(),\n        svm.LinearSVC(random_state=0),\n        svm.SVC(),\n    ):\n        clf.set_params(class_weight={0: 0.1, 1: 10})\n        clf.fit(X_[:100], y_[:100])\n        y_pred = clf.predict(X_[100:])\n        assert f1_score(y_[100:], y_pred) > 0.3\n\n\n@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])\ndef test_svm_classifier_sided_sample_weight(estimator):\n    # fit a linear SVM and check that giving more weight to opposed samples\n    # in the space will flip the decision toward these samples.\n    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]\n    estimator.set_params(kernel=\"linear\")\n\n    # check that with unit weights, a sample is supposed to be predicted on\n    # the boundary\n    sample_weight = [1] * 6\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred == pytest.approx(0)\n\n    # give more weights to opposed samples\n    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred < 0\n\n    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.decision_function([[-1.0, 1.0]])\n    assert y_pred > 0\n\n\n@pytest.mark.parametrize(\"estimator\", [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)])\ndef test_svm_regressor_sided_sample_weight(estimator):\n    # similar test to test_svm_classifier_sided_sample_weight but for\n    # SVM regressors\n    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]\n    estimator.set_params(kernel=\"linear\")\n\n    # check that with unit weights, a sample is supposed to be predicted on\n    # the boundary\n    sample_weight = [1] * 6\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred == pytest.approx(1.5)\n\n    # give more weights to opposed samples\n    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred < 1.5\n\n    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]\n    estimator.fit(X, Y, sample_weight=sample_weight)\n    y_pred = estimator.predict([[-1.0, 1.0]])\n    assert y_pred > 1.5\n\n\ndef test_svm_equivalence_sample_weight_C():\n    # test that rescaling all samples is the same as changing C\n    clf = svm.SVC()\n    clf.fit(X, Y)\n    dual_coef_no_weight = clf.dual_coef_\n    clf.set_params(C=100)\n    clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))\n    assert_allclose(dual_coef_no_weight, clf.dual_coef_)\n\n\n@pytest.mark.parametrize(\n    \"Estimator, err_msg\",\n    [\n        (svm.SVC, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.NuSVC, \"(negative dimensions are not allowed|nu is infeasible)\"),\n        (svm.SVR, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.NuSVR, \"Invalid input - all samples have zero or negative weights.\"),\n        (svm.OneClassSVM, \"Invalid input - all samples have zero or negative weights.\"),\n    ],\n    ids=[\"SVC\", \"NuSVC\", \"SVR\", \"NuSVR\", \"OneClassSVM\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[0] * len(Y), [-0.3] * len(Y)],\n    ids=[\"weights-are-zero\", \"weights-are-negative\"],\n)\ndef test_negative_sample_weights_mask_all_samples(Estimator, err_msg, sample_weight):\n    est = Estimator(kernel=\"linear\")\n    with pytest.raises(ValueError, match=err_msg):\n        est.fit(X, Y, sample_weight=sample_weight)\n\n\n@pytest.mark.parametrize(\n    \"Classifier, err_msg\",\n    [\n        (\n            svm.SVC,\n            \"Invalid input - all samples with positive weights have the same label\",\n        ),\n        (svm.NuSVC, \"specified nu is infeasible\"),\n    ],\n    ids=[\"SVC\", \"NuSVC\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[0, -0.5, 0, 1, 1, 1], [1, 1, 1, 0, -0.1, -0.3]],\n    ids=[\"mask-label-1\", \"mask-label-2\"],\n)\ndef test_negative_weights_svc_leave_just_one_label(Classifier, err_msg, sample_weight):\n    clf = Classifier(kernel=\"linear\")\n    with pytest.raises(ValueError, match=err_msg):\n        clf.fit(X, Y, sample_weight=sample_weight)\n\n\n@pytest.mark.parametrize(\n    \"Classifier, model\",\n    [\n        (svm.SVC, {\"when-left\": [0.3998, 0.4], \"when-right\": [0.4, 0.3999]}),\n        (svm.NuSVC, {\"when-left\": [0.3333, 0.3333], \"when-right\": [0.3333, 0.3333]}),\n    ],\n    ids=[\"SVC\", \"NuSVC\"],\n)\n@pytest.mark.parametrize(\n    \"sample_weight, mask_side\",\n    [([1, -0.5, 1, 1, 1, 1], \"when-left\"), ([1, 1, 1, 0, 1, 1], \"when-right\")],\n    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],\n)\ndef test_negative_weights_svc_leave_two_labels(\n    Classifier, model, sample_weight, mask_side\n):\n    clf = Classifier(kernel=\"linear\")\n    clf.fit(X, Y, sample_weight=sample_weight)\n    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)\n\n\n@pytest.mark.parametrize(\n    \"Estimator\", [svm.SVC, svm.NuSVC, svm.NuSVR], ids=[\"SVC\", \"NuSVC\", \"NuSVR\"]\n)\n@pytest.mark.parametrize(\n    \"sample_weight\",\n    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],\n    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],\n)\ndef test_negative_weight_equal_coeffs(Estimator, sample_weight):\n    # model generates equal coefficients\n    est = Estimator(kernel=\"linear\")\n    est.fit(X, Y, sample_weight=sample_weight)\n    coef = np.abs(est.coef_).ravel()\n    assert coef[0] == pytest.approx(coef[1], rel=1e-3)\n\n\n@ignore_warnings(category=UndefinedMetricWarning)\ndef test_auto_weight():\n    # Test class weights for imbalanced data\n    from sklearn.linear_model import LogisticRegression\n\n    # We take as dataset the two-dimensional projection of iris so\n    # that it is not separable and remove half of predictors from\n    # class 1.\n    # We add one to the targets as a non-regression test:\n    # class_weight=\"balanced\"\n    # used to work only when the labels where a range [0..K).\n    from sklearn.utils import compute_class_weight\n\n    X, y = iris.data[:, :2], iris.target + 1\n    unbalanced = np.delete(np.arange(y.size), np.where(y > 2)[0][::2])\n\n    classes = np.unique(y[unbalanced])\n    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y[unbalanced])\n    assert np.argmax(class_weights) == 2\n\n    for clf in (\n        svm.SVC(kernel=\"linear\"),\n        svm.LinearSVC(random_state=0),\n        LogisticRegression(),\n    ):\n        # check that score is better when class='balanced' is set.\n        y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)\n        clf.set_params(class_weight=\"balanced\")\n        y_pred_balanced = clf.fit(\n            X[unbalanced],\n            y[unbalanced],\n        ).predict(X)\n        assert metrics.f1_score(y, y_pred, average=\"macro\") <= metrics.f1_score(\n            y, y_pred_balanced, average=\"macro\"\n        )\n\n\ndef test_bad_input():\n    # Test that it gives proper exception on deficient input\n    # impossible value of C\n    with pytest.raises(ValueError):\n        svm.SVC(C=-1).fit(X, Y)\n\n    # impossible value of nu\n    clf = svm.NuSVC(nu=0.0)\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    Y2 = Y[:-1]  # wrong dimensions for labels\n    with pytest.raises(ValueError):\n        clf.fit(X, Y2)\n\n    # Test with arrays that are non-contiguous.\n    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):\n        Xf = np.asfortranarray(X)\n        assert not Xf.flags[\"C_CONTIGUOUS\"]\n        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)\n        yf = yf[:, -1]\n        assert not yf.flags[\"F_CONTIGUOUS\"]\n        assert not yf.flags[\"C_CONTIGUOUS\"]\n        clf.fit(Xf, yf)\n        assert_array_equal(clf.predict(T), true_result)\n\n    # error for precomputed kernelsx\n    clf = svm.SVC(kernel=\"precomputed\")\n    with pytest.raises(ValueError):\n        clf.fit(X, Y)\n\n    # predict with sparse input when trained with dense\n    clf = svm.SVC().fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(sparse.lil_matrix(X))\n\n    Xt = np.array(X).T\n    clf.fit(np.dot(X, Xt), Y)\n    with pytest.raises(ValueError):\n        clf.predict(X)\n\n    clf = svm.SVC()\n    clf.fit(X, Y)\n    with pytest.raises(ValueError):\n        clf.predict(Xt)\n\n\n@pytest.mark.parametrize(\n    \"Estimator, data\",\n    [\n        (svm.SVC, datasets.load_iris(return_X_y=True)),\n        (svm.NuSVC, datasets.load_iris(return_X_y=True)),\n        (svm.SVR, datasets.load_diabetes(return_X_y=True)),\n        (svm.NuSVR, datasets.load_diabetes(return_X_y=True)),\n        (svm.OneClassSVM, datasets.load_iris(return_X_y=True)),\n    ],\n)\ndef test_svm_gamma_error(Estimator, data):\n    X, y = data\n    est = Estimator(gamma=\"auto_deprecated\")\n    err_msg = \"When 'gamma' is a string, it should be either 'scale' or 'auto'\"\n    with pytest.raises(ValueError, match=err_msg):\n        est.fit(X, y)\n\n\ndef test_unicode_kernel():\n    # Test that a unicode kernel name does not cause a TypeError\n    clf = svm.SVC(kernel=\"linear\", probability=True)\n    clf.fit(X, Y)\n    clf.predict_proba(T)\n    _libsvm.cross_validation(\n        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0\n    )\n\n\ndef test_sparse_precomputed():\n    clf = svm.SVC(kernel=\"precomputed\")\n    sparse_gram = sparse.csr_matrix([[1, 0], [0, 1]])\n    with pytest.raises(TypeError, match=\"Sparse precomputed\"):\n        clf.fit(sparse_gram, [0, 1])\n\n\ndef test_sparse_fit_support_vectors_empty():\n    # Regression test for #14893\n    X_train = sparse.csr_matrix(\n        [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    y_train = np.array([0.04, 0.04, 0.10, 0.16])\n    model = svm.SVR(kernel=\"linear\")\n    model.fit(X_train, y_train)\n    assert not model.support_vectors_.data.size\n    assert not model.dual_coef_.data.size\n\n\ndef test_linearsvc_parameters():\n    # Test possible parameter combinations in LinearSVC\n    # Generate list of possible parameter combinations\n    losses = [\"hinge\", \"squared_hinge\", \"logistic_regression\", \"foo\"]\n    penalties, duals = [\"l1\", \"l2\", \"bar\"], [True, False]\n\n    X, y = make_classification(n_samples=5, n_features=5)\n\n    for loss, penalty, dual in itertools.product(losses, penalties, duals):\n        clf = svm.LinearSVC(penalty=penalty, loss=loss, dual=dual)\n        if (\n            (loss, penalty) == (\"hinge\", \"l1\")\n            or (loss, penalty, dual) == (\"hinge\", \"l2\", False)\n            or (penalty, dual) == (\"l1\", True)\n            or loss == \"foo\"\n            or penalty == \"bar\"\n        ):\n\n            with pytest.raises(\n                ValueError,\n                match=\"Unsupported set of arguments.*penalty='%s.*loss='%s.*dual=%s\"\n                % (penalty, loss, dual),\n            ):\n                clf.fit(X, y)\n        else:\n            clf.fit(X, y)\n\n    # Incorrect loss value - test if explicit error message is raised\n    with pytest.raises(ValueError, match=\".*loss='l3' is not supported.*\"):\n        svm.LinearSVC(loss=\"l3\").fit(X, y)\n\n\ndef test_linear_svx_uppercase_loss_penality_raises_error():\n    # Check if Upper case notation raises error at _fit_liblinear\n    # which is called by fit\n\n    X, y = [[0.0], [1.0]], [0, 1]\n\n    msg = \"loss='SQuared_hinge' is not supported\"\n    with pytest.raises(ValueError, match=msg):\n        svm.LinearSVC(loss=\"SQuared_hinge\").fit(X, y)\n\n    msg = \"The combination of penalty='L2' and loss='squared_hinge' is not supported\"\n    with pytest.raises(ValueError, match=msg):\n        svm.LinearSVC(penalty=\"L2\").fit(X, y)\n\n\ndef test_linearsvc():\n    # Test basic routines using LinearSVC\n    clf = svm.LinearSVC(random_state=0).fit(X, Y)\n\n    # by default should have intercept\n    assert clf.fit_intercept\n\n    assert_array_equal(clf.predict(T), true_result)\n    assert_array_almost_equal(clf.intercept_, [0], decimal=3)\n\n    # the same with l1 penalty\n    clf = svm.LinearSVC(\n        penalty=\"l1\", loss=\"squared_hinge\", dual=False, random_state=0\n    ).fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # l2 penalty with dual formulation\n    clf = svm.LinearSVC(penalty=\"l2\", dual=True, random_state=0).fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # l2 penalty, l1 loss\n    clf = svm.LinearSVC(penalty=\"l2\", loss=\"hinge\", dual=True, random_state=0)\n    clf.fit(X, Y)\n    assert_array_equal(clf.predict(T), true_result)\n\n    # test also decision function\n    dec = clf.decision_function(T)\n    res = (dec > 0).astype(int) + 1\n    assert_array_equal(res, true_result)\n\n\ndef test_linearsvc_crammer_singer():\n    # Test LinearSVC with crammer_singer multi-class svm\n    ovr_clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)\n    cs_clf = svm.LinearSVC(multi_class=\"crammer_singer\", random_state=0)\n    cs_clf.fit(iris.data, iris.target)\n\n    # similar prediction for ovr and crammer-singer:\n    assert (ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > 0.9\n\n    # classifiers shouldn't be the same\n    assert (ovr_clf.coef_ != cs_clf.coef_).all()\n\n    # test decision function\n    assert_array_equal(\n        cs_clf.predict(iris.data),\n        np.argmax(cs_clf.decision_function(iris.data), axis=1),\n    )\n    dec_func = np.dot(iris.data, cs_clf.coef_.T) + cs_clf.intercept_\n    assert_array_almost_equal(dec_func, cs_clf.decision_function(iris.data))\n\n\ndef test_linearsvc_fit_sampleweight():\n    # check correct result when sample_weight is 1\n    n_samples = len(X)\n    unit_weight = np.ones(n_samples)\n    clf = svm.LinearSVC(random_state=0).fit(X, Y)\n    clf_unitweight = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X, Y, sample_weight=unit_weight\n    )\n\n    # check if same as sample_weight=None\n    assert_array_equal(clf_unitweight.predict(T), clf.predict(T))\n    assert_allclose(clf.coef_, clf_unitweight.coef_, 1, 0.0001)\n\n    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where\n    # X = X1 repeated n1 times, X2 repeated n2 times and so forth\n\n    random_state = check_random_state(0)\n    random_weight = random_state.randint(0, 10, n_samples)\n    lsvc_unflat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X, Y, sample_weight=random_weight\n    )\n    pred1 = lsvc_unflat.predict(T)\n\n    X_flat = np.repeat(X, random_weight, axis=0)\n    y_flat = np.repeat(Y, random_weight, axis=0)\n    lsvc_flat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(\n        X_flat, y_flat\n    )\n    pred2 = lsvc_flat.predict(T)\n\n    assert_array_equal(pred1, pred2)\n    assert_allclose(lsvc_unflat.coef_, lsvc_flat.coef_, 1, 0.0001)\n\n\ndef test_crammer_singer_binary():\n    # Test Crammer-Singer formulation in the binary case\n    X, y = make_classification(n_classes=2, random_state=0)\n\n    for fit_intercept in (True, False):\n        acc = (\n            svm.LinearSVC(\n                fit_intercept=fit_intercept,\n                multi_class=\"crammer_singer\",\n                random_state=0,\n            )\n            .fit(X, y)\n            .score(X, y)\n        )\n        assert acc > 0.9\n\n\ndef test_linearsvc_iris():\n    # Test that LinearSVC gives plausible predictions on the iris dataset\n    # Also, test symbolic class names (classes_).\n    target = iris.target_names[iris.target]\n    clf = svm.LinearSVC(random_state=0).fit(iris.data, target)\n    assert set(clf.classes_) == set(iris.target_names)\n    assert np.mean(clf.predict(iris.data) == target) > 0.8\n\n    dec = clf.decision_function(iris.data)\n    pred = iris.target_names[np.argmax(dec, 1)]\n    assert_array_equal(pred, clf.predict(iris.data))\n\n\ndef test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):\n    # Test that dense liblinear honours intercept_scaling param\n    X = [[2, 1], [3, 1], [1, 3], [2, 3]]\n    y = [0, 0, 1, 1]\n    clf = classifier(\n        fit_intercept=True,\n        penalty=\"l1\",\n        loss=\"squared_hinge\",\n        dual=False,\n        C=4,\n        tol=1e-7,\n        random_state=0,\n    )\n    assert clf.intercept_scaling == 1, clf.intercept_scaling\n    assert clf.fit_intercept\n\n    # when intercept_scaling is low the intercept value is highly \"penalized\"\n    # by regularization\n    clf.intercept_scaling = 1\n    clf.fit(X, y)\n    assert_almost_equal(clf.intercept_, 0, decimal=5)\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # is not affected by regularization\n    clf.intercept_scaling = 100\n    clf.fit(X, y)\n    intercept1 = clf.intercept_\n    assert intercept1 < -1\n\n    # when intercept_scaling is sufficiently high, the intercept value\n    # doesn't depend on intercept_scaling value\n    clf.intercept_scaling = 1000\n    clf.fit(X, y)\n    intercept2 = clf.intercept_\n    assert_array_almost_equal(intercept1, intercept2, decimal=2)\n\n\ndef test_liblinear_set_coef():\n    # multi-class case\n    clf = svm.LinearSVC().fit(iris.data, iris.target)\n    values = clf.decision_function(iris.data)\n    clf.coef_ = clf.coef_.copy()\n    clf.intercept_ = clf.intercept_.copy()\n    values2 = clf.decision_function(iris.data)\n    assert_array_almost_equal(values, values2)\n\n    # binary-class case\n    X = [[2, 1], [3, 1], [1, 3], [2, 3]]\n    y = [0, 0, 1, 1]\n\n    clf = svm.LinearSVC().fit(X, y)\n    values = clf.decision_function(X)\n    clf.coef_ = clf.coef_.copy()\n    clf.intercept_ = clf.intercept_.copy()\n    values2 = clf.decision_function(X)\n    assert_array_equal(values, values2)\n\n\ndef test_immutable_coef_property():\n    # Check that primal coef modification are not silently ignored\n    svms = [\n        svm.SVC(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.NuSVC(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.SVR(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.NuSVR(kernel=\"linear\").fit(iris.data, iris.target),\n        svm.OneClassSVM(kernel=\"linear\").fit(iris.data),\n    ]\n    for clf in svms:\n        with pytest.raises(AttributeError):\n            clf.__setattr__(\"coef_\", np.arange(3))\n        with pytest.raises((RuntimeError, ValueError)):\n            clf.coef_.__setitem__((0, 0), 0)\n\n\ndef test_linearsvc_verbose():\n    # stdout: redirect\n    import os\n\n    stdout = os.dup(1)  # save original stdout\n    os.dup2(os.pipe()[1], 1)  # replace it\n\n    # actual call\n    clf = svm.LinearSVC(verbose=1)\n    clf.fit(X, Y)\n\n    # stdout: restore\n    os.dup2(stdout, 1)  # restore original stdout\n\n\ndef test_svc_clone_with_callable_kernel():\n    # create SVM with callable linear kernel, check that results are the same\n    # as with built-in linear kernel\n    svm_callable = svm.SVC(\n        kernel=lambda x, y: np.dot(x, y.T),\n        probability=True,\n        random_state=0,\n        decision_function_shape=\"ovr\",\n    )\n    # clone for checking clonability with lambda functions..\n    svm_cloned = base.clone(svm_callable)\n    svm_cloned.fit(iris.data, iris.target)\n\n    svm_builtin = svm.SVC(\n        kernel=\"linear\", probability=True, random_state=0, decision_function_shape=\"ovr\"\n    )\n    svm_builtin.fit(iris.data, iris.target)\n\n    assert_array_almost_equal(svm_cloned.dual_coef_, svm_builtin.dual_coef_)\n    assert_array_almost_equal(svm_cloned.intercept_, svm_builtin.intercept_)\n    assert_array_equal(svm_cloned.predict(iris.data), svm_builtin.predict(iris.data))\n\n    assert_array_almost_equal(\n        svm_cloned.predict_proba(iris.data),\n        svm_builtin.predict_proba(iris.data),\n        decimal=4,\n    )\n    assert_array_almost_equal(\n        svm_cloned.decision_function(iris.data),\n        svm_builtin.decision_function(iris.data),\n    )\n\n\ndef test_svc_bad_kernel():\n    svc = svm.SVC(kernel=lambda x, y: x)\n    with pytest.raises(ValueError):\n        svc.fit(X, Y)\n\n\ndef test_timeout():\n    a = svm.SVC(\n        kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, max_iter=1\n    )\n    warning_msg = (\n        r\"Solver terminated early \\(max_iter=1\\).  Consider pre-processing \"\n        r\"your data with StandardScaler or MinMaxScaler.\"\n    )\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        a.fit(np.array(X), Y)\n\n\ndef test_unfitted():\n    X = \"foo!\"  # input validation not required when SVM not fitted\n\n    clf = svm.SVC()\n    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)\n\n    clf = svm.NuSVR()\n    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):\n        clf.predict(X)\n\n\n# ignore convergence warnings from max_iter=1\n@ignore_warnings\ndef test_consistent_proba():\n    a = svm.SVC(probability=True, max_iter=1, random_state=0)\n    proba_1 = a.fit(X, Y).predict_proba(X)\n    a = svm.SVC(probability=True, max_iter=1, random_state=0)\n    proba_2 = a.fit(X, Y).predict_proba(X)\n    assert_array_almost_equal(proba_1, proba_2)\n\n\ndef test_linear_svm_convergence_warnings():\n    # Test that warnings are raised if model does not converge\n\n    lsvc = svm.LinearSVC(random_state=0, max_iter=2)\n    warning_msg = \"Liblinear failed to converge, increase the number of iterations.\"\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        lsvc.fit(X, Y)\n    assert lsvc.n_iter_ == 2\n\n    lsvr = svm.LinearSVR(random_state=0, max_iter=2)\n    with pytest.warns(ConvergenceWarning, match=warning_msg):\n        lsvr.fit(iris.data, iris.target)\n    assert lsvr.n_iter_ == 2\n\n\ndef test_svr_coef_sign():\n    # Test that SVR(kernel=\"linear\") has coef_ with the right sign.\n    # Non-regression test for #2933.\n    X = np.random.RandomState(21).randn(10, 3)\n    y = np.random.RandomState(12).randn(10)\n\n    for svr in [svm.SVR(kernel=\"linear\"), svm.NuSVR(kernel=\"linear\"), svm.LinearSVR()]:\n        svr.fit(X, y)\n        assert_array_almost_equal(\n            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_\n        )\n\n\ndef test_linear_svc_intercept_scaling():\n    # Test that the right error message is thrown when intercept_scaling <= 0\n\n    for i in [-1, 0]:\n        lsvc = svm.LinearSVC(intercept_scaling=i)\n\n        msg = (\n            \"Intercept scaling is %r but needs to be greater than 0.\"\n            \" To disable fitting an intercept,\"\n            \" set fit_intercept=False.\"\n            % lsvc.intercept_scaling\n        )\n        with pytest.raises(ValueError, match=msg):\n            lsvc.fit(X, Y)\n\n\ndef test_lsvc_intercept_scaling_zero():\n    # Test that intercept_scaling is ignored when fit_intercept is False\n\n    lsvc = svm.LinearSVC(fit_intercept=False)\n    lsvc.fit(X, Y)\n    assert lsvc.intercept_ == 0.0\n\n\ndef test_hasattr_predict_proba():\n    # Method must be (un)available before or after fit, switched by\n    # `probability` param\n\n    G = svm.SVC(probability=True)\n    assert hasattr(G, \"predict_proba\")\n    G.fit(iris.data, iris.target)\n    assert hasattr(G, \"predict_proba\")\n\n    G = svm.SVC(probability=False)\n    assert not hasattr(G, \"predict_proba\")\n    G.fit(iris.data, iris.target)\n    assert not hasattr(G, \"predict_proba\")\n\n    # Switching to `probability=True` after fitting should make\n    # predict_proba available, but calling it must not work:\n    G.probability = True\n    assert hasattr(G, \"predict_proba\")\n    msg = \"predict_proba is not available when fitted with probability=False\"\n\n    with pytest.raises(NotFittedError, match=msg):\n        G.predict_proba(iris.data)\n\n\ndef test_decision_function_shape_two_class():\n    for n_classes in [2, 3]:\n        X, y = make_blobs(centers=n_classes, random_state=0)\n        for estimator in [svm.SVC, svm.NuSVC]:\n            clf = OneVsRestClassifier(estimator(decision_function_shape=\"ovr\")).fit(\n                X, y\n            )\n            assert len(clf.predict(X)) == len(y)\n\n\ndef test_ovr_decision_function():\n    # One point from each quadrant represents one class\n    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])\n    y_train = [0, 1, 2, 3]\n\n    # First point is closer to the decision boundaries than the second point\n    base_points = np.array([[5, 5], [10, 10]])\n\n    # For all the quadrants (classes)\n    X_test = np.vstack(\n        (\n            base_points * [1, 1],  # Q1\n            base_points * [-1, 1],  # Q2\n            base_points * [-1, -1],  # Q3\n            base_points * [1, -1],  # Q4\n        )\n    )\n\n    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2\n\n    clf = svm.SVC(kernel=\"linear\", decision_function_shape=\"ovr\")\n    clf.fit(X_train, y_train)\n\n    y_pred = clf.predict(X_test)\n\n    # Test if the prediction is the same as y\n    assert_array_equal(y_pred, y_test)\n\n    deci_val = clf.decision_function(X_test)\n\n    # Assert that the predicted class has the maximum value\n    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)\n\n    # Get decision value at test points for the predicted class\n    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))\n\n    # Assert pred_class_deci_val > 0 here\n    assert np.min(pred_class_deci_val) > 0.0\n\n    # Test if the first point has lower decision value on every quadrant\n    # compared to the second point\n    assert np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1])\n\n\n@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])\ndef test_svc_invalid_break_ties_param(SVCClass):\n    X, y = make_blobs(random_state=42)\n\n    svm = SVCClass(\n        kernel=\"linear\", decision_function_shape=\"ovo\", break_ties=True, random_state=42\n    ).fit(X, y)\n\n    with pytest.raises(ValueError, match=\"break_ties must be False\"):\n        svm.predict(y)\n\n\n@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])\ndef test_svc_ovr_tie_breaking(SVCClass):\n    \"\"\"Test if predict breaks ties in OVR mode.\n    Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277\n    \"\"\"\n    X, y = make_blobs(random_state=27)\n\n    xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 1000)\n    ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 1000)\n    xx, yy = np.meshgrid(xs, ys)\n\n    svm = SVCClass(\n        kernel=\"linear\",\n        decision_function_shape=\"ovr\",\n        break_ties=False,\n        random_state=42,\n    ).fit(X, y)\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    assert not np.all(pred == np.argmax(dv, axis=1))\n\n    svm = SVCClass(\n        kernel=\"linear\", decision_function_shape=\"ovr\", break_ties=True, random_state=42\n    ).fit(X, y)\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    assert np.all(pred == np.argmax(dv, axis=1))\n\n\ndef test_gamma_auto():\n    X, y = [[0.0, 1.2], [1.0, 1.3]], [0, 1]\n\n    with pytest.warns(None) as record:\n        svm.SVC(kernel=\"linear\").fit(X, y)\n    assert not len(record)\n\n    with pytest.warns(None) as record:\n        svm.SVC(kernel=\"precomputed\").fit(X, y)\n    assert not len(record)\n\n\ndef test_gamma_scale():\n    X, y = [[0.0], [1.0]], [0, 1]\n\n    clf = svm.SVC()\n    with pytest.warns(None) as record:\n        clf.fit(X, y)\n    assert not len(record)\n    assert_almost_equal(clf._gamma, 4)\n\n    # X_var ~= 1 shouldn't raise warning, for when\n    # gamma is not explicitly set.\n    X, y = [[1, 2], [3, 2 * np.sqrt(6) / 3 + 2]], [0, 1]\n    with pytest.warns(None) as record:\n        clf.fit(X, y)\n    assert not len(record)\n\n\n@pytest.mark.parametrize(\n    \"SVM, params\",\n    [\n        (LinearSVC, {\"penalty\": \"l1\", \"loss\": \"squared_hinge\", \"dual\": False}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": True}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": False}),\n        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"hinge\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"epsilon_insensitive\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),\n        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),\n    ],\n)\ndef test_linearsvm_liblinear_sample_weight(SVM, params):\n    X = np.array(\n        [\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [1, 3],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [2, 1],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [3, 3],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n            [4, 1],\n        ],\n        dtype=np.dtype(\"float\"),\n    )\n    y = np.array(\n        [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype(\"int\")\n    )\n\n    X2 = np.vstack([X, X])\n    y2 = np.hstack([y, 3 - y])\n    sample_weight = np.ones(shape=len(y) * 2)\n    sample_weight[len(y) :] = 0\n    X2, y2, sample_weight = shuffle(X2, y2, sample_weight, random_state=0)\n\n    base_estimator = SVM(random_state=42)\n    base_estimator.set_params(**params)\n    base_estimator.set_params(tol=1e-12, max_iter=1000)\n    est_no_weight = base.clone(base_estimator).fit(X, y)\n    est_with_weight = base.clone(base_estimator).fit(\n        X2, y2, sample_weight=sample_weight\n    )\n\n    for method in (\"predict\", \"decision_function\"):\n        if hasattr(base_estimator, method):\n            X_est_no_weight = getattr(est_no_weight, method)(X)\n            X_est_with_weight = getattr(est_with_weight, method)(X)\n            assert_allclose(X_est_no_weight, X_est_with_weight)\n\n\ndef test_n_support_oneclass_svr():\n    # Make n_support is correct for oneclass and SVR (used to be\n    # non-initialized)\n    # this is a non regression test for issue #14774\n    X = np.array([[0], [0.44], [0.45], [0.46], [1]])\n    clf = svm.OneClassSVM()\n    assert not hasattr(clf, \"n_support_\")\n    clf.fit(X)\n    assert clf.n_support_ == clf.support_vectors_.shape[0]\n    assert clf.n_support_.size == 1\n    assert clf.n_support_ == 3\n\n    y = np.arange(X.shape[0])\n    reg = svm.SVR().fit(X, y)\n    assert reg.n_support_ == reg.support_vectors_.shape[0]\n    assert reg.n_support_.size == 1\n    assert reg.n_support_ == 4\n\n\n@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])\ndef test_custom_kernel_not_array_input(Estimator):\n    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"\n    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]\n    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding\n    y = np.array([1, 1, 2, 2, 1])\n\n    def string_kernel(X1, X2):\n        assert isinstance(X1[0], str)\n        n_samples1 = _num_samples(X1)\n        n_samples2 = _num_samples(X2)\n        K = np.zeros((n_samples1, n_samples2))\n        for ii in range(n_samples1):\n            for jj in range(ii, n_samples2):\n                K[ii, jj] = X1[ii].count(\"A\") * X2[jj].count(\"A\")\n                K[ii, jj] += X1[ii].count(\"B\") * X2[jj].count(\"B\")\n                K[jj, ii] = K[ii, jj]\n        return K\n\n    K = string_kernel(data, data)\n    assert_array_equal(np.dot(X, X.T), K)\n\n    svc1 = Estimator(kernel=string_kernel).fit(data, y)\n    svc2 = Estimator(kernel=\"linear\").fit(X, y)\n    svc3 = Estimator(kernel=\"precomputed\").fit(K, y)\n\n    assert svc1.score(data, y) == svc3.score(K, y)\n    assert svc1.score(data, y) == svc2.score(X, y)\n    if hasattr(svc1, \"decision_function\"):  # classifier\n        assert_allclose(svc1.decision_function(data), svc2.decision_function(X))\n        assert_allclose(svc1.decision_function(data), svc3.decision_function(K))\n        assert_array_equal(svc1.predict(data), svc2.predict(X))\n        assert_array_equal(svc1.predict(data), svc3.predict(K))\n    else:  # regressor\n        assert_allclose(svc1.predict(data), svc2.predict(X))\n        assert_allclose(svc1.predict(data), svc3.predict(K))\n\n\ndef test_svc_raises_error_internal_representation():\n    \"\"\"Check that SVC raises error when internal representation is altered.\n\n    Non-regression test for #18891 and https://nvd.nist.gov/vuln/detail/CVE-2020-28975\n    \"\"\"\n    clf = svm.SVC(kernel=\"linear\").fit(X, Y)\n    clf._n_support[0] = 1000000\n\n    msg = \"The internal representation of SVC was altered\"\n    with pytest.raises(ValueError, match=msg):\n        clf.predict(X)\n"], "filenames": ["doc/whats_new/v1.0.rst", "sklearn/svm/_base.py", "sklearn/svm/tests/test_svm.py"], "buggy_code_start_loc": [68, 618, 1373], "buggy_code_end_loc": [68, 618, 1373], "fixing_code_start_loc": [69, 619, 1374], "fixing_code_end_loc": [78, 626, 1387], "type": "NVD-CWE-noinfo", "message": "** DISPUTED ** svm_predict_values in svm.cpp in Libsvm v324, as used in scikit-learn 0.23.2 and other products, allows attackers to cause a denial of service (segmentation fault) via a crafted model SVM (introduced via pickle, json, or any other model permanence standard) with a large value in the _n_support array. NOTE: the scikit-learn vendor's position is that the behavior can only occur if the library's API is violated by an application that changes a private attribute.", "other": {"cve": {"id": "CVE-2020-28975", "sourceIdentifier": "cve@mitre.org", "published": "2020-11-21T21:15:10.680", "lastModified": "2023-01-17T18:24:01.210", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "** DISPUTED ** svm_predict_values in svm.cpp in Libsvm v324, as used in scikit-learn 0.23.2 and other products, allows attackers to cause a denial of service (segmentation fault) via a crafted model SVM (introduced via pickle, json, or any other model permanence standard) with a large value in the _n_support array. NOTE: the scikit-learn vendor's position is that the behavior can only occur if the library's API is violated by an application that changes a private attribute."}, {"lang": "es", "value": "**EN DISPUTA** La funci\u00f3n svm_predict_values en el archivo svm.cpp en Libsvm versi\u00f3n v324, como es usado en scikit-learn versiones 0.23.2 y otros productos, permite a atacantes causar una denegaci\u00f3n de servicio (fallo de segmentaci\u00f3n) por medio de un modelo SVM dise\u00f1ado (introducido por medio de pickle, json o cualquier otro modelo est\u00e1ndar de permanencia) con un valor grande en la matriz _n_supportNOTA: la posici\u00f3n del proveedor de scikit-learn es que el comportamiento s\u00f3lo puede ocurrir si la API de la biblioteca es violada por una aplicaci\u00f3n que cambia un atributo privado"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:scikit-learn:scikit-learn:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.23.2", "versionEndExcluding": "1.0.1", "matchCriteriaId": "4320862C-5961-4410-A723-8AC2475C9C51"}]}]}], "references": [{"url": "http://packetstormsecurity.com/files/160281/SciKit-Learn-0.23.2-Denial-Of-Service.html", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}, {"url": "http://seclists.org/fulldisclosure/2020/Nov/44", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://github.com/cjlin1/libsvm/blob/9a3a9708926dec87d382c43b203f2ca19c2d56a0/svm.cpp#L2501", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/scikit-learn/scikit-learn/commit/1bf13d567d3cd74854aa8343fd25b61dd768bb85", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/scikit-learn/scikit-learn/issues/18891", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/202301-03", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/scikit-learn/scikit-learn/commit/1bf13d567d3cd74854aa8343fd25b61dd768bb85"}}