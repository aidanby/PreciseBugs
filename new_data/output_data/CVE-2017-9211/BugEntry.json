{"buggy_code": ["/*\n * Symmetric key cipher operations.\n *\n * Generic encrypt/decrypt wrapper for ciphers, handles operations across\n * multiple page boundaries by using temporary blocks.  In user context,\n * the kernel is given a chance to schedule us once per page.\n *\n * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <linux/bug.h>\n#include <linux/cryptouser.h>\n#include <linux/compiler.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/seq_file.h>\n#include <net/netlink.h>\n\n#include \"internal.h\"\n\nenum {\n\tSKCIPHER_WALK_PHYS = 1 << 0,\n\tSKCIPHER_WALK_SLOW = 1 << 1,\n\tSKCIPHER_WALK_COPY = 1 << 2,\n\tSKCIPHER_WALK_DIFF = 1 << 3,\n\tSKCIPHER_WALK_SLEEP = 1 << 4,\n};\n\nstruct skcipher_walk_buffer {\n\tstruct list_head entry;\n\tstruct scatter_walk dst;\n\tunsigned int len;\n\tu8 *data;\n\tu8 buffer[];\n};\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk);\n\nstatic inline void skcipher_unmap(struct scatter_walk *walk, void *vaddr)\n{\n\tif (PageHighMem(scatterwalk_page(walk)))\n\t\tkunmap_atomic(vaddr);\n}\n\nstatic inline void *skcipher_map(struct scatter_walk *walk)\n{\n\tstruct page *page = scatterwalk_page(walk);\n\n\treturn (PageHighMem(page) ? kmap_atomic(page) : page_address(page)) +\n\t       offset_in_page(walk->offset);\n}\n\nstatic inline void skcipher_map_src(struct skcipher_walk *walk)\n{\n\twalk->src.virt.addr = skcipher_map(&walk->in);\n}\n\nstatic inline void skcipher_map_dst(struct skcipher_walk *walk)\n{\n\twalk->dst.virt.addr = skcipher_map(&walk->out);\n}\n\nstatic inline void skcipher_unmap_src(struct skcipher_walk *walk)\n{\n\tskcipher_unmap(&walk->in, walk->src.virt.addr);\n}\n\nstatic inline void skcipher_unmap_dst(struct skcipher_walk *walk)\n{\n\tskcipher_unmap(&walk->out, walk->dst.virt.addr);\n}\n\nstatic inline gfp_t skcipher_walk_gfp(struct skcipher_walk *walk)\n{\n\treturn walk->flags & SKCIPHER_WALK_SLEEP ? GFP_KERNEL : GFP_ATOMIC;\n}\n\n/* Get a spot of the specified length that does not straddle a page.\n * The caller needs to ensure that there is enough space for this operation.\n */\nstatic inline u8 *skcipher_get_spot(u8 *start, unsigned int len)\n{\n\tu8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);\n\n\treturn max(start, end_page);\n}\n\nstatic int skcipher_done_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tu8 *addr;\n\n\taddr = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);\n\taddr = skcipher_get_spot(addr, bsize);\n\tscatterwalk_copychunks(addr, &walk->out, bsize,\n\t\t\t       (walk->flags & SKCIPHER_WALK_PHYS) ? 2 : 1);\n\treturn 0;\n}\n\nint skcipher_walk_done(struct skcipher_walk *walk, int err)\n{\n\tunsigned int n = walk->nbytes - err;\n\tunsigned int nbytes;\n\n\tnbytes = walk->total - n;\n\n\tif (unlikely(err < 0)) {\n\t\tnbytes = 0;\n\t\tn = 0;\n\t} else if (likely(!(walk->flags & (SKCIPHER_WALK_PHYS |\n\t\t\t\t\t   SKCIPHER_WALK_SLOW |\n\t\t\t\t\t   SKCIPHER_WALK_COPY |\n\t\t\t\t\t   SKCIPHER_WALK_DIFF)))) {\nunmap_src:\n\t\tskcipher_unmap_src(walk);\n\t} else if (walk->flags & SKCIPHER_WALK_DIFF) {\n\t\tskcipher_unmap_dst(walk);\n\t\tgoto unmap_src;\n\t} else if (walk->flags & SKCIPHER_WALK_COPY) {\n\t\tskcipher_map_dst(walk);\n\t\tmemcpy(walk->dst.virt.addr, walk->page, n);\n\t\tskcipher_unmap_dst(walk);\n\t} else if (unlikely(walk->flags & SKCIPHER_WALK_SLOW)) {\n\t\tif (WARN_ON(err)) {\n\t\t\terr = -EINVAL;\n\t\t\tnbytes = 0;\n\t\t} else\n\t\t\tn = skcipher_done_slow(walk, n);\n\t}\n\n\tif (err > 0)\n\t\terr = 0;\n\n\twalk->total = nbytes;\n\twalk->nbytes = nbytes;\n\n\tscatterwalk_advance(&walk->in, n);\n\tscatterwalk_advance(&walk->out, n);\n\tscatterwalk_done(&walk->in, 0, nbytes);\n\tscatterwalk_done(&walk->out, 1, nbytes);\n\n\tif (nbytes) {\n\t\tcrypto_yield(walk->flags & SKCIPHER_WALK_SLEEP ?\n\t\t\t     CRYPTO_TFM_REQ_MAY_SLEEP : 0);\n\t\treturn skcipher_walk_next(walk);\n\t}\n\n\t/* Short-circuit for the common/fast path. */\n\tif (!((unsigned long)walk->buffer | (unsigned long)walk->page))\n\t\tgoto out;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tgoto out;\n\n\tif (walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_done);\n\nvoid skcipher_walk_complete(struct skcipher_walk *walk, int err)\n{\n\tstruct skcipher_walk_buffer *p, *tmp;\n\n\tlist_for_each_entry_safe(p, tmp, &walk->buffers, entry) {\n\t\tu8 *data;\n\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\tdata = p->data;\n\t\tif (!data) {\n\t\t\tdata = PTR_ALIGN(&p->buffer[0], walk->alignmask + 1);\n\t\t\tdata = skcipher_get_spot(data, walk->stride);\n\t\t}\n\n\t\tscatterwalk_copychunks(data, &p->dst, p->len, 1);\n\n\t\tif (offset_in_page(p->data) + p->len + walk->stride >\n\t\t    PAGE_SIZE)\n\t\t\tfree_page((unsigned long)p->data);\n\ndone:\n\t\tlist_del(&p->entry);\n\t\tkfree(p);\n\t}\n\n\tif (!err && walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_complete);\n\nstatic void skcipher_queue_write(struct skcipher_walk *walk,\n\t\t\t\t struct skcipher_walk_buffer *p)\n{\n\tp->dst = walk->out;\n\tlist_add_tail(&p->entry, &walk->buffers);\n}\n\nstatic int skcipher_next_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tbool phys = walk->flags & SKCIPHER_WALK_PHYS;\n\tunsigned alignmask = walk->alignmask;\n\tstruct skcipher_walk_buffer *p;\n\tunsigned a;\n\tunsigned n;\n\tu8 *buffer;\n\tvoid *v;\n\n\tif (!phys) {\n\t\tif (!walk->buffer)\n\t\t\twalk->buffer = walk->page;\n\t\tbuffer = walk->buffer;\n\t\tif (buffer)\n\t\t\tgoto ok;\n\t}\n\n\t/* Start with the minimum alignment of kmalloc. */\n\ta = crypto_tfm_ctx_alignment() - 1;\n\tn = bsize;\n\n\tif (phys) {\n\t\t/* Calculate the minimum alignment of p->buffer. */\n\t\ta &= (sizeof(*p) ^ (sizeof(*p) - 1)) >> 1;\n\t\tn += sizeof(*p);\n\t}\n\n\t/* Minimum size to align p->buffer by alignmask. */\n\tn += alignmask & ~a;\n\n\t/* Minimum size to ensure p->buffer does not straddle a page. */\n\tn += (bsize - 1) & ~(alignmask | a);\n\n\tv = kzalloc(n, skcipher_walk_gfp(walk));\n\tif (!v)\n\t\treturn skcipher_walk_done(walk, -ENOMEM);\n\n\tif (phys) {\n\t\tp = v;\n\t\tp->len = bsize;\n\t\tskcipher_queue_write(walk, p);\n\t\tbuffer = p->buffer;\n\t} else {\n\t\twalk->buffer = v;\n\t\tbuffer = v;\n\t}\n\nok:\n\twalk->dst.virt.addr = PTR_ALIGN(buffer, alignmask + 1);\n\twalk->dst.virt.addr = skcipher_get_spot(walk->dst.virt.addr, bsize);\n\twalk->src.virt.addr = walk->dst.virt.addr;\n\n\tscatterwalk_copychunks(walk->src.virt.addr, &walk->in, bsize, 0);\n\n\twalk->nbytes = bsize;\n\twalk->flags |= SKCIPHER_WALK_SLOW;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_copy(struct skcipher_walk *walk)\n{\n\tstruct skcipher_walk_buffer *p;\n\tu8 *tmp = walk->page;\n\n\tskcipher_map_src(walk);\n\tmemcpy(tmp, walk->src.virt.addr, walk->nbytes);\n\tskcipher_unmap_src(walk);\n\n\twalk->src.virt.addr = tmp;\n\twalk->dst.virt.addr = tmp;\n\n\tif (!(walk->flags & SKCIPHER_WALK_PHYS))\n\t\treturn 0;\n\n\tp = kmalloc(sizeof(*p), skcipher_walk_gfp(walk));\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tp->data = walk->page;\n\tp->len = walk->nbytes;\n\tskcipher_queue_write(walk, p);\n\n\tif (offset_in_page(walk->page) + walk->nbytes + walk->stride >\n\t    PAGE_SIZE)\n\t\twalk->page = NULL;\n\telse\n\t\twalk->page += walk->nbytes;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_fast(struct skcipher_walk *walk)\n{\n\tunsigned long diff;\n\n\twalk->src.phys.page = scatterwalk_page(&walk->in);\n\twalk->src.phys.offset = offset_in_page(walk->in.offset);\n\twalk->dst.phys.page = scatterwalk_page(&walk->out);\n\twalk->dst.phys.offset = offset_in_page(walk->out.offset);\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\treturn 0;\n\n\tdiff = walk->src.phys.offset - walk->dst.phys.offset;\n\tdiff |= walk->src.virt.page - walk->dst.virt.page;\n\n\tskcipher_map_src(walk);\n\twalk->dst.virt.addr = walk->src.virt.addr;\n\n\tif (diff) {\n\t\twalk->flags |= SKCIPHER_WALK_DIFF;\n\t\tskcipher_map_dst(walk);\n\t}\n\n\treturn 0;\n}\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk)\n{\n\tunsigned int bsize;\n\tunsigned int n;\n\tint err;\n\n\twalk->flags &= ~(SKCIPHER_WALK_SLOW | SKCIPHER_WALK_COPY |\n\t\t\t SKCIPHER_WALK_DIFF);\n\n\tn = walk->total;\n\tbsize = min(walk->stride, max(n, walk->blocksize));\n\tn = scatterwalk_clamp(&walk->in, n);\n\tn = scatterwalk_clamp(&walk->out, n);\n\n\tif (unlikely(n < bsize)) {\n\t\tif (unlikely(walk->total < walk->blocksize))\n\t\t\treturn skcipher_walk_done(walk, -EINVAL);\n\nslow_path:\n\t\terr = skcipher_next_slow(walk, bsize);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\tif (unlikely((walk->in.offset | walk->out.offset) & walk->alignmask)) {\n\t\tif (!walk->page) {\n\t\t\tgfp_t gfp = skcipher_walk_gfp(walk);\n\n\t\t\twalk->page = (void *)__get_free_page(gfp);\n\t\t\tif (!walk->page)\n\t\t\t\tgoto slow_path;\n\t\t}\n\n\t\twalk->nbytes = min_t(unsigned, n,\n\t\t\t\t     PAGE_SIZE - offset_in_page(walk->page));\n\t\twalk->flags |= SKCIPHER_WALK_COPY;\n\t\terr = skcipher_next_copy(walk);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\twalk->nbytes = n;\n\n\treturn skcipher_next_fast(walk);\n\nset_phys_lowmem:\n\tif (!err && (walk->flags & SKCIPHER_WALK_PHYS)) {\n\t\twalk->src.phys.page = virt_to_page(walk->src.virt.addr);\n\t\twalk->dst.phys.page = virt_to_page(walk->dst.virt.addr);\n\t\twalk->src.phys.offset &= PAGE_SIZE - 1;\n\t\twalk->dst.phys.offset &= PAGE_SIZE - 1;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_next);\n\nstatic int skcipher_copy_iv(struct skcipher_walk *walk)\n{\n\tunsigned a = crypto_tfm_ctx_alignment() - 1;\n\tunsigned alignmask = walk->alignmask;\n\tunsigned ivsize = walk->ivsize;\n\tunsigned bs = walk->stride;\n\tunsigned aligned_bs;\n\tunsigned size;\n\tu8 *iv;\n\n\taligned_bs = ALIGN(bs, alignmask);\n\n\t/* Minimum size to align buffer by alignmask. */\n\tsize = alignmask & ~a;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tsize += ivsize;\n\telse {\n\t\tsize += aligned_bs + ivsize;\n\n\t\t/* Minimum size to ensure buffer does not straddle a page. */\n\t\tsize += (bs - 1) & ~(alignmask | a);\n\t}\n\n\twalk->buffer = kmalloc(size, skcipher_walk_gfp(walk));\n\tif (!walk->buffer)\n\t\treturn -ENOMEM;\n\n\tiv = PTR_ALIGN(walk->buffer, alignmask + 1);\n\tiv = skcipher_get_spot(iv, bs) + aligned_bs;\n\n\twalk->iv = memcpy(iv, walk->iv, walk->ivsize);\n\treturn 0;\n}\n\nstatic int skcipher_walk_first(struct skcipher_walk *walk)\n{\n\twalk->nbytes = 0;\n\n\tif (WARN_ON_ONCE(in_irq()))\n\t\treturn -EDEADLK;\n\n\tif (unlikely(!walk->total))\n\t\treturn 0;\n\n\twalk->buffer = NULL;\n\tif (unlikely(((unsigned long)walk->iv & walk->alignmask))) {\n\t\tint err = skcipher_copy_iv(walk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\twalk->page = NULL;\n\twalk->nbytes = walk->total;\n\n\treturn skcipher_walk_next(walk);\n}\n\nstatic int skcipher_walk_skcipher(struct skcipher_walk *walk,\n\t\t\t\t  struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\twalk->total = req->cryptlen;\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\twalk->flags |= req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\n\t\t       SKCIPHER_WALK_SLEEP : 0;\n\n\twalk->blocksize = crypto_skcipher_blocksize(tfm);\n\twalk->stride = crypto_skcipher_walksize(tfm);\n\twalk->ivsize = crypto_skcipher_ivsize(tfm);\n\twalk->alignmask = crypto_skcipher_alignmask(tfm);\n\n\treturn skcipher_walk_first(walk);\n}\n\nint skcipher_walk_virt(struct skcipher_walk *walk,\n\t\t       struct skcipher_request *req, bool atomic)\n{\n\tint err;\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\terr = skcipher_walk_skcipher(walk, req);\n\n\twalk->flags &= atomic ? ~SKCIPHER_WALK_SLEEP : ~0;\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_virt);\n\nvoid skcipher_walk_atomise(struct skcipher_walk *walk)\n{\n\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_atomise);\n\nint skcipher_walk_async(struct skcipher_walk *walk,\n\t\t\tstruct skcipher_request *req)\n{\n\twalk->flags |= SKCIPHER_WALK_PHYS;\n\n\tINIT_LIST_HEAD(&walk->buffers);\n\n\treturn skcipher_walk_skcipher(walk, req);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_async);\n\nstatic int skcipher_walk_aead_common(struct skcipher_walk *walk,\n\t\t\t\t     struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tint err;\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\tscatterwalk_copychunks(NULL, &walk->in, req->assoclen, 2);\n\tscatterwalk_copychunks(NULL, &walk->out, req->assoclen, 2);\n\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\tif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP)\n\t\twalk->flags |= SKCIPHER_WALK_SLEEP;\n\telse\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\twalk->blocksize = crypto_aead_blocksize(tfm);\n\twalk->stride = crypto_aead_chunksize(tfm);\n\twalk->ivsize = crypto_aead_ivsize(tfm);\n\twalk->alignmask = crypto_aead_alignmask(tfm);\n\n\terr = skcipher_walk_first(walk);\n\n\tif (atomic)\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\treturn err;\n}\n\nint skcipher_walk_aead(struct skcipher_walk *walk, struct aead_request *req,\n\t\t       bool atomic)\n{\n\twalk->total = req->cryptlen;\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead);\n\nint skcipher_walk_aead_encrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\twalk->total = req->cryptlen;\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_encrypt);\n\nint skcipher_walk_aead_decrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\n\twalk->total = req->cryptlen - crypto_aead_authsize(tfm);\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_decrypt);\n\nstatic unsigned int crypto_skcipher_extsize(struct crypto_alg *alg)\n{\n\tif (alg->cra_type == &crypto_blkcipher_type)\n\t\treturn sizeof(struct crypto_blkcipher *);\n\n\tif (alg->cra_type == &crypto_ablkcipher_type ||\n\t    alg->cra_type == &crypto_givcipher_type)\n\t\treturn sizeof(struct crypto_ablkcipher *);\n\n\treturn crypto_alg_extsize(alg);\n}\n\nstatic int skcipher_setkey_blkcipher(struct crypto_skcipher *tfm,\n\t\t\t\t     const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_blkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_blkcipher *blkcipher = *ctx;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(blkcipher, ~0);\n\tcrypto_blkcipher_set_flags(blkcipher, crypto_skcipher_get_flags(tfm) &\n\t\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(blkcipher, key, keylen);\n\tcrypto_skcipher_set_flags(tfm, crypto_blkcipher_get_flags(blkcipher) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int skcipher_crypt_blkcipher(struct skcipher_request *req,\n\t\t\t\t    int (*crypt)(struct blkcipher_desc *,\n\t\t\t\t\t\t struct scatterlist *,\n\t\t\t\t\t\t struct scatterlist *,\n\t\t\t\t\t\t unsigned int))\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_blkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = *ctx,\n\t\t.info = req->iv,\n\t\t.flags = req->base.flags,\n\t};\n\n\n\treturn crypt(&desc, req->dst, req->src, req->cryptlen);\n}\n\nstatic int skcipher_encrypt_blkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\n\n\treturn skcipher_crypt_blkcipher(req, alg->encrypt);\n}\n\nstatic int skcipher_decrypt_blkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\n\n\treturn skcipher_crypt_blkcipher(req, alg->decrypt);\n}\n\nstatic void crypto_exit_skcipher_ops_blkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_blkcipher **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_blkcipher(*ctx);\n}\n\nstatic int crypto_init_skcipher_ops_blkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct crypto_blkcipher **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *blkcipher;\n\tstruct crypto_tfm *btfm;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tbtfm = __crypto_alloc_tfm(calg, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(btfm)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(btfm);\n\t}\n\n\tblkcipher = __crypto_blkcipher_cast(btfm);\n\t*ctx = blkcipher;\n\ttfm->exit = crypto_exit_skcipher_ops_blkcipher;\n\n\tskcipher->setkey = skcipher_setkey_blkcipher;\n\tskcipher->encrypt = skcipher_encrypt_blkcipher;\n\tskcipher->decrypt = skcipher_decrypt_blkcipher;\n\n\tskcipher->ivsize = crypto_blkcipher_ivsize(blkcipher);\n\tskcipher->keysize = calg->cra_blkcipher.max_keysize;\n\n\treturn 0;\n}\n\nstatic int skcipher_setkey_ablkcipher(struct crypto_skcipher *tfm,\n\t\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_ablkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_ablkcipher *ablkcipher = *ctx;\n\tint err;\n\n\tcrypto_ablkcipher_clear_flags(ablkcipher, ~0);\n\tcrypto_ablkcipher_set_flags(ablkcipher,\n\t\t\t\t    crypto_skcipher_get_flags(tfm) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(ablkcipher, key, keylen);\n\tcrypto_skcipher_set_flags(tfm,\n\t\t\t\t  crypto_ablkcipher_get_flags(ablkcipher) &\n\t\t\t\t  CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int skcipher_crypt_ablkcipher(struct skcipher_request *req,\n\t\t\t\t     int (*crypt)(struct ablkcipher_request *))\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_ablkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct ablkcipher_request *subreq = skcipher_request_ctx(req);\n\n\tablkcipher_request_set_tfm(subreq, *ctx);\n\tablkcipher_request_set_callback(subreq, skcipher_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\n\t\t\t\t     req->iv);\n\n\treturn crypt(subreq);\n}\n\nstatic int skcipher_encrypt_ablkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct ablkcipher_alg *alg = &tfm->__crt_alg->cra_ablkcipher;\n\n\treturn skcipher_crypt_ablkcipher(req, alg->encrypt);\n}\n\nstatic int skcipher_decrypt_ablkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct ablkcipher_alg *alg = &tfm->__crt_alg->cra_ablkcipher;\n\n\treturn skcipher_crypt_ablkcipher(req, alg->decrypt);\n}\n\nstatic void crypto_exit_skcipher_ops_ablkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ablkcipher(*ctx);\n}\n\nstatic int crypto_init_skcipher_ops_ablkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct crypto_ablkcipher **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *ablkcipher;\n\tstruct crypto_tfm *abtfm;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tabtfm = __crypto_alloc_tfm(calg, 0, 0);\n\tif (IS_ERR(abtfm)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(abtfm);\n\t}\n\n\tablkcipher = __crypto_ablkcipher_cast(abtfm);\n\t*ctx = ablkcipher;\n\ttfm->exit = crypto_exit_skcipher_ops_ablkcipher;\n\n\tskcipher->setkey = skcipher_setkey_ablkcipher;\n\tskcipher->encrypt = skcipher_encrypt_ablkcipher;\n\tskcipher->decrypt = skcipher_decrypt_ablkcipher;\n\n\tskcipher->ivsize = crypto_ablkcipher_ivsize(ablkcipher);\n\tskcipher->reqsize = crypto_ablkcipher_reqsize(ablkcipher) +\n\t\t\t    sizeof(struct ablkcipher_request);\n\tskcipher->keysize = calg->cra_ablkcipher.max_keysize;\n\n\treturn 0;\n}\n\nstatic void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\talg->exit(skcipher);\n}\n\nstatic int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = alg->setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}\n\nstatic void crypto_skcipher_free_instance(struct crypto_instance *inst)\n{\n\tstruct skcipher_instance *skcipher =\n\t\tcontainer_of(inst, struct skcipher_instance, s.base);\n\n\tskcipher->free(skcipher);\n}\n\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n\t__maybe_unused;\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n{\n\tstruct skcipher_alg *skcipher = container_of(alg, struct skcipher_alg,\n\t\t\t\t\t\t     base);\n\n\tseq_printf(m, \"type         : skcipher\\n\");\n\tseq_printf(m, \"async        : %s\\n\",\n\t\t   alg->cra_flags & CRYPTO_ALG_ASYNC ?  \"yes\" : \"no\");\n\tseq_printf(m, \"blocksize    : %u\\n\", alg->cra_blocksize);\n\tseq_printf(m, \"min keysize  : %u\\n\", skcipher->min_keysize);\n\tseq_printf(m, \"max keysize  : %u\\n\", skcipher->max_keysize);\n\tseq_printf(m, \"ivsize       : %u\\n\", skcipher->ivsize);\n\tseq_printf(m, \"chunksize    : %u\\n\", skcipher->chunksize);\n\tseq_printf(m, \"walksize     : %u\\n\", skcipher->walksize);\n}\n\n#ifdef CONFIG_NET\nstatic int crypto_skcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_blkcipher rblkcipher;\n\tstruct skcipher_alg *skcipher = container_of(alg, struct skcipher_alg,\n\t\t\t\t\t\t     base);\n\n\tstrncpy(rblkcipher.type, \"skcipher\", sizeof(rblkcipher.type));\n\tstrncpy(rblkcipher.geniv, \"<none>\", sizeof(rblkcipher.geniv));\n\n\trblkcipher.blocksize = alg->cra_blocksize;\n\trblkcipher.min_keysize = skcipher->min_keysize;\n\trblkcipher.max_keysize = skcipher->max_keysize;\n\trblkcipher.ivsize = skcipher->ivsize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_BLKCIPHER,\n\t\t    sizeof(struct crypto_report_blkcipher), &rblkcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n#else\nstatic int crypto_skcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\treturn -ENOSYS;\n}\n#endif\n\nstatic const struct crypto_type crypto_skcipher_type2 = {\n\t.extsize = crypto_skcipher_extsize,\n\t.init_tfm = crypto_skcipher_init_tfm,\n\t.free = crypto_skcipher_free_instance,\n#ifdef CONFIG_PROC_FS\n\t.show = crypto_skcipher_show,\n#endif\n\t.report = crypto_skcipher_report,\n\t.maskclear = ~CRYPTO_ALG_TYPE_MASK,\n\t.maskset = CRYPTO_ALG_TYPE_BLKCIPHER_MASK,\n\t.type = CRYPTO_ALG_TYPE_SKCIPHER,\n\t.tfmsize = offsetof(struct crypto_skcipher, base),\n};\n\nint crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn,\n\t\t\t  const char *name, u32 type, u32 mask)\n{\n\tspawn->base.frontend = &crypto_skcipher_type2;\n\treturn crypto_grab_spawn(&spawn->base, name, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_grab_skcipher);\n\nstruct crypto_skcipher *crypto_alloc_skcipher(const char *alg_name,\n\t\t\t\t\t      u32 type, u32 mask)\n{\n\treturn crypto_alloc_tfm(alg_name, &crypto_skcipher_type2, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_skcipher);\n\nint crypto_has_skcipher2(const char *alg_name, u32 type, u32 mask)\n{\n\treturn crypto_type_has_alg(alg_name, &crypto_skcipher_type2,\n\t\t\t\t   type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_has_skcipher2);\n\nstatic int skcipher_prepare_alg(struct skcipher_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\n\tif (alg->ivsize > PAGE_SIZE / 8 || alg->chunksize > PAGE_SIZE / 8 ||\n\t    alg->walksize > PAGE_SIZE / 8)\n\t\treturn -EINVAL;\n\n\tif (!alg->chunksize)\n\t\talg->chunksize = base->cra_blocksize;\n\tif (!alg->walksize)\n\t\talg->walksize = alg->chunksize;\n\n\tbase->cra_type = &crypto_skcipher_type2;\n\tbase->cra_flags &= ~CRYPTO_ALG_TYPE_MASK;\n\tbase->cra_flags |= CRYPTO_ALG_TYPE_SKCIPHER;\n\n\treturn 0;\n}\n\nint crypto_register_skcipher(struct skcipher_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\tint err;\n\n\terr = skcipher_prepare_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_alg(base);\n}\nEXPORT_SYMBOL_GPL(crypto_register_skcipher);\n\nvoid crypto_unregister_skcipher(struct skcipher_alg *alg)\n{\n\tcrypto_unregister_alg(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skcipher);\n\nint crypto_register_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_skcipher(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_skciphers);\n\nvoid crypto_unregister_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skciphers);\n\nint skcipher_register_instance(struct crypto_template *tmpl,\n\t\t\t   struct skcipher_instance *inst)\n{\n\tint err;\n\n\terr = skcipher_prepare_alg(&inst->alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_instance(tmpl, skcipher_crypto_instance(inst));\n}\nEXPORT_SYMBOL_GPL(skcipher_register_instance);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Symmetric key cipher type\");\n"], "fixing_code": ["/*\n * Symmetric key cipher operations.\n *\n * Generic encrypt/decrypt wrapper for ciphers, handles operations across\n * multiple page boundaries by using temporary blocks.  In user context,\n * the kernel is given a chance to schedule us once per page.\n *\n * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the Free\n * Software Foundation; either version 2 of the License, or (at your option)\n * any later version.\n *\n */\n\n#include <crypto/internal/aead.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/scatterwalk.h>\n#include <linux/bug.h>\n#include <linux/cryptouser.h>\n#include <linux/compiler.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/rtnetlink.h>\n#include <linux/seq_file.h>\n#include <net/netlink.h>\n\n#include \"internal.h\"\n\nenum {\n\tSKCIPHER_WALK_PHYS = 1 << 0,\n\tSKCIPHER_WALK_SLOW = 1 << 1,\n\tSKCIPHER_WALK_COPY = 1 << 2,\n\tSKCIPHER_WALK_DIFF = 1 << 3,\n\tSKCIPHER_WALK_SLEEP = 1 << 4,\n};\n\nstruct skcipher_walk_buffer {\n\tstruct list_head entry;\n\tstruct scatter_walk dst;\n\tunsigned int len;\n\tu8 *data;\n\tu8 buffer[];\n};\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk);\n\nstatic inline void skcipher_unmap(struct scatter_walk *walk, void *vaddr)\n{\n\tif (PageHighMem(scatterwalk_page(walk)))\n\t\tkunmap_atomic(vaddr);\n}\n\nstatic inline void *skcipher_map(struct scatter_walk *walk)\n{\n\tstruct page *page = scatterwalk_page(walk);\n\n\treturn (PageHighMem(page) ? kmap_atomic(page) : page_address(page)) +\n\t       offset_in_page(walk->offset);\n}\n\nstatic inline void skcipher_map_src(struct skcipher_walk *walk)\n{\n\twalk->src.virt.addr = skcipher_map(&walk->in);\n}\n\nstatic inline void skcipher_map_dst(struct skcipher_walk *walk)\n{\n\twalk->dst.virt.addr = skcipher_map(&walk->out);\n}\n\nstatic inline void skcipher_unmap_src(struct skcipher_walk *walk)\n{\n\tskcipher_unmap(&walk->in, walk->src.virt.addr);\n}\n\nstatic inline void skcipher_unmap_dst(struct skcipher_walk *walk)\n{\n\tskcipher_unmap(&walk->out, walk->dst.virt.addr);\n}\n\nstatic inline gfp_t skcipher_walk_gfp(struct skcipher_walk *walk)\n{\n\treturn walk->flags & SKCIPHER_WALK_SLEEP ? GFP_KERNEL : GFP_ATOMIC;\n}\n\n/* Get a spot of the specified length that does not straddle a page.\n * The caller needs to ensure that there is enough space for this operation.\n */\nstatic inline u8 *skcipher_get_spot(u8 *start, unsigned int len)\n{\n\tu8 *end_page = (u8 *)(((unsigned long)(start + len - 1)) & PAGE_MASK);\n\n\treturn max(start, end_page);\n}\n\nstatic int skcipher_done_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tu8 *addr;\n\n\taddr = (u8 *)ALIGN((unsigned long)walk->buffer, walk->alignmask + 1);\n\taddr = skcipher_get_spot(addr, bsize);\n\tscatterwalk_copychunks(addr, &walk->out, bsize,\n\t\t\t       (walk->flags & SKCIPHER_WALK_PHYS) ? 2 : 1);\n\treturn 0;\n}\n\nint skcipher_walk_done(struct skcipher_walk *walk, int err)\n{\n\tunsigned int n = walk->nbytes - err;\n\tunsigned int nbytes;\n\n\tnbytes = walk->total - n;\n\n\tif (unlikely(err < 0)) {\n\t\tnbytes = 0;\n\t\tn = 0;\n\t} else if (likely(!(walk->flags & (SKCIPHER_WALK_PHYS |\n\t\t\t\t\t   SKCIPHER_WALK_SLOW |\n\t\t\t\t\t   SKCIPHER_WALK_COPY |\n\t\t\t\t\t   SKCIPHER_WALK_DIFF)))) {\nunmap_src:\n\t\tskcipher_unmap_src(walk);\n\t} else if (walk->flags & SKCIPHER_WALK_DIFF) {\n\t\tskcipher_unmap_dst(walk);\n\t\tgoto unmap_src;\n\t} else if (walk->flags & SKCIPHER_WALK_COPY) {\n\t\tskcipher_map_dst(walk);\n\t\tmemcpy(walk->dst.virt.addr, walk->page, n);\n\t\tskcipher_unmap_dst(walk);\n\t} else if (unlikely(walk->flags & SKCIPHER_WALK_SLOW)) {\n\t\tif (WARN_ON(err)) {\n\t\t\terr = -EINVAL;\n\t\t\tnbytes = 0;\n\t\t} else\n\t\t\tn = skcipher_done_slow(walk, n);\n\t}\n\n\tif (err > 0)\n\t\terr = 0;\n\n\twalk->total = nbytes;\n\twalk->nbytes = nbytes;\n\n\tscatterwalk_advance(&walk->in, n);\n\tscatterwalk_advance(&walk->out, n);\n\tscatterwalk_done(&walk->in, 0, nbytes);\n\tscatterwalk_done(&walk->out, 1, nbytes);\n\n\tif (nbytes) {\n\t\tcrypto_yield(walk->flags & SKCIPHER_WALK_SLEEP ?\n\t\t\t     CRYPTO_TFM_REQ_MAY_SLEEP : 0);\n\t\treturn skcipher_walk_next(walk);\n\t}\n\n\t/* Short-circuit for the common/fast path. */\n\tif (!((unsigned long)walk->buffer | (unsigned long)walk->page))\n\t\tgoto out;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tgoto out;\n\n\tif (walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_done);\n\nvoid skcipher_walk_complete(struct skcipher_walk *walk, int err)\n{\n\tstruct skcipher_walk_buffer *p, *tmp;\n\n\tlist_for_each_entry_safe(p, tmp, &walk->buffers, entry) {\n\t\tu8 *data;\n\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\tdata = p->data;\n\t\tif (!data) {\n\t\t\tdata = PTR_ALIGN(&p->buffer[0], walk->alignmask + 1);\n\t\t\tdata = skcipher_get_spot(data, walk->stride);\n\t\t}\n\n\t\tscatterwalk_copychunks(data, &p->dst, p->len, 1);\n\n\t\tif (offset_in_page(p->data) + p->len + walk->stride >\n\t\t    PAGE_SIZE)\n\t\t\tfree_page((unsigned long)p->data);\n\ndone:\n\t\tlist_del(&p->entry);\n\t\tkfree(p);\n\t}\n\n\tif (!err && walk->iv != walk->oiv)\n\t\tmemcpy(walk->oiv, walk->iv, walk->ivsize);\n\tif (walk->buffer != walk->page)\n\t\tkfree(walk->buffer);\n\tif (walk->page)\n\t\tfree_page((unsigned long)walk->page);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_complete);\n\nstatic void skcipher_queue_write(struct skcipher_walk *walk,\n\t\t\t\t struct skcipher_walk_buffer *p)\n{\n\tp->dst = walk->out;\n\tlist_add_tail(&p->entry, &walk->buffers);\n}\n\nstatic int skcipher_next_slow(struct skcipher_walk *walk, unsigned int bsize)\n{\n\tbool phys = walk->flags & SKCIPHER_WALK_PHYS;\n\tunsigned alignmask = walk->alignmask;\n\tstruct skcipher_walk_buffer *p;\n\tunsigned a;\n\tunsigned n;\n\tu8 *buffer;\n\tvoid *v;\n\n\tif (!phys) {\n\t\tif (!walk->buffer)\n\t\t\twalk->buffer = walk->page;\n\t\tbuffer = walk->buffer;\n\t\tif (buffer)\n\t\t\tgoto ok;\n\t}\n\n\t/* Start with the minimum alignment of kmalloc. */\n\ta = crypto_tfm_ctx_alignment() - 1;\n\tn = bsize;\n\n\tif (phys) {\n\t\t/* Calculate the minimum alignment of p->buffer. */\n\t\ta &= (sizeof(*p) ^ (sizeof(*p) - 1)) >> 1;\n\t\tn += sizeof(*p);\n\t}\n\n\t/* Minimum size to align p->buffer by alignmask. */\n\tn += alignmask & ~a;\n\n\t/* Minimum size to ensure p->buffer does not straddle a page. */\n\tn += (bsize - 1) & ~(alignmask | a);\n\n\tv = kzalloc(n, skcipher_walk_gfp(walk));\n\tif (!v)\n\t\treturn skcipher_walk_done(walk, -ENOMEM);\n\n\tif (phys) {\n\t\tp = v;\n\t\tp->len = bsize;\n\t\tskcipher_queue_write(walk, p);\n\t\tbuffer = p->buffer;\n\t} else {\n\t\twalk->buffer = v;\n\t\tbuffer = v;\n\t}\n\nok:\n\twalk->dst.virt.addr = PTR_ALIGN(buffer, alignmask + 1);\n\twalk->dst.virt.addr = skcipher_get_spot(walk->dst.virt.addr, bsize);\n\twalk->src.virt.addr = walk->dst.virt.addr;\n\n\tscatterwalk_copychunks(walk->src.virt.addr, &walk->in, bsize, 0);\n\n\twalk->nbytes = bsize;\n\twalk->flags |= SKCIPHER_WALK_SLOW;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_copy(struct skcipher_walk *walk)\n{\n\tstruct skcipher_walk_buffer *p;\n\tu8 *tmp = walk->page;\n\n\tskcipher_map_src(walk);\n\tmemcpy(tmp, walk->src.virt.addr, walk->nbytes);\n\tskcipher_unmap_src(walk);\n\n\twalk->src.virt.addr = tmp;\n\twalk->dst.virt.addr = tmp;\n\n\tif (!(walk->flags & SKCIPHER_WALK_PHYS))\n\t\treturn 0;\n\n\tp = kmalloc(sizeof(*p), skcipher_walk_gfp(walk));\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tp->data = walk->page;\n\tp->len = walk->nbytes;\n\tskcipher_queue_write(walk, p);\n\n\tif (offset_in_page(walk->page) + walk->nbytes + walk->stride >\n\t    PAGE_SIZE)\n\t\twalk->page = NULL;\n\telse\n\t\twalk->page += walk->nbytes;\n\n\treturn 0;\n}\n\nstatic int skcipher_next_fast(struct skcipher_walk *walk)\n{\n\tunsigned long diff;\n\n\twalk->src.phys.page = scatterwalk_page(&walk->in);\n\twalk->src.phys.offset = offset_in_page(walk->in.offset);\n\twalk->dst.phys.page = scatterwalk_page(&walk->out);\n\twalk->dst.phys.offset = offset_in_page(walk->out.offset);\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\treturn 0;\n\n\tdiff = walk->src.phys.offset - walk->dst.phys.offset;\n\tdiff |= walk->src.virt.page - walk->dst.virt.page;\n\n\tskcipher_map_src(walk);\n\twalk->dst.virt.addr = walk->src.virt.addr;\n\n\tif (diff) {\n\t\twalk->flags |= SKCIPHER_WALK_DIFF;\n\t\tskcipher_map_dst(walk);\n\t}\n\n\treturn 0;\n}\n\nstatic int skcipher_walk_next(struct skcipher_walk *walk)\n{\n\tunsigned int bsize;\n\tunsigned int n;\n\tint err;\n\n\twalk->flags &= ~(SKCIPHER_WALK_SLOW | SKCIPHER_WALK_COPY |\n\t\t\t SKCIPHER_WALK_DIFF);\n\n\tn = walk->total;\n\tbsize = min(walk->stride, max(n, walk->blocksize));\n\tn = scatterwalk_clamp(&walk->in, n);\n\tn = scatterwalk_clamp(&walk->out, n);\n\n\tif (unlikely(n < bsize)) {\n\t\tif (unlikely(walk->total < walk->blocksize))\n\t\t\treturn skcipher_walk_done(walk, -EINVAL);\n\nslow_path:\n\t\terr = skcipher_next_slow(walk, bsize);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\tif (unlikely((walk->in.offset | walk->out.offset) & walk->alignmask)) {\n\t\tif (!walk->page) {\n\t\t\tgfp_t gfp = skcipher_walk_gfp(walk);\n\n\t\t\twalk->page = (void *)__get_free_page(gfp);\n\t\t\tif (!walk->page)\n\t\t\t\tgoto slow_path;\n\t\t}\n\n\t\twalk->nbytes = min_t(unsigned, n,\n\t\t\t\t     PAGE_SIZE - offset_in_page(walk->page));\n\t\twalk->flags |= SKCIPHER_WALK_COPY;\n\t\terr = skcipher_next_copy(walk);\n\t\tgoto set_phys_lowmem;\n\t}\n\n\twalk->nbytes = n;\n\n\treturn skcipher_next_fast(walk);\n\nset_phys_lowmem:\n\tif (!err && (walk->flags & SKCIPHER_WALK_PHYS)) {\n\t\twalk->src.phys.page = virt_to_page(walk->src.virt.addr);\n\t\twalk->dst.phys.page = virt_to_page(walk->dst.virt.addr);\n\t\twalk->src.phys.offset &= PAGE_SIZE - 1;\n\t\twalk->dst.phys.offset &= PAGE_SIZE - 1;\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_next);\n\nstatic int skcipher_copy_iv(struct skcipher_walk *walk)\n{\n\tunsigned a = crypto_tfm_ctx_alignment() - 1;\n\tunsigned alignmask = walk->alignmask;\n\tunsigned ivsize = walk->ivsize;\n\tunsigned bs = walk->stride;\n\tunsigned aligned_bs;\n\tunsigned size;\n\tu8 *iv;\n\n\taligned_bs = ALIGN(bs, alignmask);\n\n\t/* Minimum size to align buffer by alignmask. */\n\tsize = alignmask & ~a;\n\n\tif (walk->flags & SKCIPHER_WALK_PHYS)\n\t\tsize += ivsize;\n\telse {\n\t\tsize += aligned_bs + ivsize;\n\n\t\t/* Minimum size to ensure buffer does not straddle a page. */\n\t\tsize += (bs - 1) & ~(alignmask | a);\n\t}\n\n\twalk->buffer = kmalloc(size, skcipher_walk_gfp(walk));\n\tif (!walk->buffer)\n\t\treturn -ENOMEM;\n\n\tiv = PTR_ALIGN(walk->buffer, alignmask + 1);\n\tiv = skcipher_get_spot(iv, bs) + aligned_bs;\n\n\twalk->iv = memcpy(iv, walk->iv, walk->ivsize);\n\treturn 0;\n}\n\nstatic int skcipher_walk_first(struct skcipher_walk *walk)\n{\n\twalk->nbytes = 0;\n\n\tif (WARN_ON_ONCE(in_irq()))\n\t\treturn -EDEADLK;\n\n\tif (unlikely(!walk->total))\n\t\treturn 0;\n\n\twalk->buffer = NULL;\n\tif (unlikely(((unsigned long)walk->iv & walk->alignmask))) {\n\t\tint err = skcipher_copy_iv(walk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\twalk->page = NULL;\n\twalk->nbytes = walk->total;\n\n\treturn skcipher_walk_next(walk);\n}\n\nstatic int skcipher_walk_skcipher(struct skcipher_walk *walk,\n\t\t\t\t  struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\twalk->total = req->cryptlen;\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\twalk->flags |= req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP ?\n\t\t       SKCIPHER_WALK_SLEEP : 0;\n\n\twalk->blocksize = crypto_skcipher_blocksize(tfm);\n\twalk->stride = crypto_skcipher_walksize(tfm);\n\twalk->ivsize = crypto_skcipher_ivsize(tfm);\n\twalk->alignmask = crypto_skcipher_alignmask(tfm);\n\n\treturn skcipher_walk_first(walk);\n}\n\nint skcipher_walk_virt(struct skcipher_walk *walk,\n\t\t       struct skcipher_request *req, bool atomic)\n{\n\tint err;\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\terr = skcipher_walk_skcipher(walk, req);\n\n\twalk->flags &= atomic ? ~SKCIPHER_WALK_SLEEP : ~0;\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_virt);\n\nvoid skcipher_walk_atomise(struct skcipher_walk *walk)\n{\n\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_atomise);\n\nint skcipher_walk_async(struct skcipher_walk *walk,\n\t\t\tstruct skcipher_request *req)\n{\n\twalk->flags |= SKCIPHER_WALK_PHYS;\n\n\tINIT_LIST_HEAD(&walk->buffers);\n\n\treturn skcipher_walk_skcipher(walk, req);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_async);\n\nstatic int skcipher_walk_aead_common(struct skcipher_walk *walk,\n\t\t\t\t     struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tint err;\n\n\twalk->flags &= ~SKCIPHER_WALK_PHYS;\n\n\tscatterwalk_start(&walk->in, req->src);\n\tscatterwalk_start(&walk->out, req->dst);\n\n\tscatterwalk_copychunks(NULL, &walk->in, req->assoclen, 2);\n\tscatterwalk_copychunks(NULL, &walk->out, req->assoclen, 2);\n\n\twalk->iv = req->iv;\n\twalk->oiv = req->iv;\n\n\tif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP)\n\t\twalk->flags |= SKCIPHER_WALK_SLEEP;\n\telse\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\twalk->blocksize = crypto_aead_blocksize(tfm);\n\twalk->stride = crypto_aead_chunksize(tfm);\n\twalk->ivsize = crypto_aead_ivsize(tfm);\n\twalk->alignmask = crypto_aead_alignmask(tfm);\n\n\terr = skcipher_walk_first(walk);\n\n\tif (atomic)\n\t\twalk->flags &= ~SKCIPHER_WALK_SLEEP;\n\n\treturn err;\n}\n\nint skcipher_walk_aead(struct skcipher_walk *walk, struct aead_request *req,\n\t\t       bool atomic)\n{\n\twalk->total = req->cryptlen;\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead);\n\nint skcipher_walk_aead_encrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\twalk->total = req->cryptlen;\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_encrypt);\n\nint skcipher_walk_aead_decrypt(struct skcipher_walk *walk,\n\t\t\t       struct aead_request *req, bool atomic)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\n\twalk->total = req->cryptlen - crypto_aead_authsize(tfm);\n\n\treturn skcipher_walk_aead_common(walk, req, atomic);\n}\nEXPORT_SYMBOL_GPL(skcipher_walk_aead_decrypt);\n\nstatic unsigned int crypto_skcipher_extsize(struct crypto_alg *alg)\n{\n\tif (alg->cra_type == &crypto_blkcipher_type)\n\t\treturn sizeof(struct crypto_blkcipher *);\n\n\tif (alg->cra_type == &crypto_ablkcipher_type ||\n\t    alg->cra_type == &crypto_givcipher_type)\n\t\treturn sizeof(struct crypto_ablkcipher *);\n\n\treturn crypto_alg_extsize(alg);\n}\n\nstatic int skcipher_setkey_blkcipher(struct crypto_skcipher *tfm,\n\t\t\t\t     const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_blkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_blkcipher *blkcipher = *ctx;\n\tint err;\n\n\tcrypto_blkcipher_clear_flags(blkcipher, ~0);\n\tcrypto_blkcipher_set_flags(blkcipher, crypto_skcipher_get_flags(tfm) &\n\t\t\t\t\t      CRYPTO_TFM_REQ_MASK);\n\terr = crypto_blkcipher_setkey(blkcipher, key, keylen);\n\tcrypto_skcipher_set_flags(tfm, crypto_blkcipher_get_flags(blkcipher) &\n\t\t\t\t       CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int skcipher_crypt_blkcipher(struct skcipher_request *req,\n\t\t\t\t    int (*crypt)(struct blkcipher_desc *,\n\t\t\t\t\t\t struct scatterlist *,\n\t\t\t\t\t\t struct scatterlist *,\n\t\t\t\t\t\t unsigned int))\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_blkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct blkcipher_desc desc = {\n\t\t.tfm = *ctx,\n\t\t.info = req->iv,\n\t\t.flags = req->base.flags,\n\t};\n\n\n\treturn crypt(&desc, req->dst, req->src, req->cryptlen);\n}\n\nstatic int skcipher_encrypt_blkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\n\n\treturn skcipher_crypt_blkcipher(req, alg->encrypt);\n}\n\nstatic int skcipher_decrypt_blkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct blkcipher_alg *alg = &tfm->__crt_alg->cra_blkcipher;\n\n\treturn skcipher_crypt_blkcipher(req, alg->decrypt);\n}\n\nstatic void crypto_exit_skcipher_ops_blkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_blkcipher **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_blkcipher(*ctx);\n}\n\nstatic int crypto_init_skcipher_ops_blkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct crypto_blkcipher **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_blkcipher *blkcipher;\n\tstruct crypto_tfm *btfm;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tbtfm = __crypto_alloc_tfm(calg, CRYPTO_ALG_TYPE_BLKCIPHER,\n\t\t\t\t\tCRYPTO_ALG_TYPE_MASK);\n\tif (IS_ERR(btfm)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(btfm);\n\t}\n\n\tblkcipher = __crypto_blkcipher_cast(btfm);\n\t*ctx = blkcipher;\n\ttfm->exit = crypto_exit_skcipher_ops_blkcipher;\n\n\tskcipher->setkey = skcipher_setkey_blkcipher;\n\tskcipher->encrypt = skcipher_encrypt_blkcipher;\n\tskcipher->decrypt = skcipher_decrypt_blkcipher;\n\n\tskcipher->ivsize = crypto_blkcipher_ivsize(blkcipher);\n\tskcipher->keysize = calg->cra_blkcipher.max_keysize;\n\n\treturn 0;\n}\n\nstatic int skcipher_setkey_ablkcipher(struct crypto_skcipher *tfm,\n\t\t\t\t      const u8 *key, unsigned int keylen)\n{\n\tstruct crypto_ablkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct crypto_ablkcipher *ablkcipher = *ctx;\n\tint err;\n\n\tcrypto_ablkcipher_clear_flags(ablkcipher, ~0);\n\tcrypto_ablkcipher_set_flags(ablkcipher,\n\t\t\t\t    crypto_skcipher_get_flags(tfm) &\n\t\t\t\t    CRYPTO_TFM_REQ_MASK);\n\terr = crypto_ablkcipher_setkey(ablkcipher, key, keylen);\n\tcrypto_skcipher_set_flags(tfm,\n\t\t\t\t  crypto_ablkcipher_get_flags(ablkcipher) &\n\t\t\t\t  CRYPTO_TFM_RES_MASK);\n\n\treturn err;\n}\n\nstatic int skcipher_crypt_ablkcipher(struct skcipher_request *req,\n\t\t\t\t     int (*crypt)(struct ablkcipher_request *))\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_ablkcipher **ctx = crypto_skcipher_ctx(tfm);\n\tstruct ablkcipher_request *subreq = skcipher_request_ctx(req);\n\n\tablkcipher_request_set_tfm(subreq, *ctx);\n\tablkcipher_request_set_callback(subreq, skcipher_request_flags(req),\n\t\t\t\t\treq->base.complete, req->base.data);\n\tablkcipher_request_set_crypt(subreq, req->src, req->dst, req->cryptlen,\n\t\t\t\t     req->iv);\n\n\treturn crypt(subreq);\n}\n\nstatic int skcipher_encrypt_ablkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct ablkcipher_alg *alg = &tfm->__crt_alg->cra_ablkcipher;\n\n\treturn skcipher_crypt_ablkcipher(req, alg->encrypt);\n}\n\nstatic int skcipher_decrypt_ablkcipher(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *skcipher = crypto_skcipher_reqtfm(req);\n\tstruct crypto_tfm *tfm = crypto_skcipher_tfm(skcipher);\n\tstruct ablkcipher_alg *alg = &tfm->__crt_alg->cra_ablkcipher;\n\n\treturn skcipher_crypt_ablkcipher(req, alg->decrypt);\n}\n\nstatic void crypto_exit_skcipher_ops_ablkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_ablkcipher **ctx = crypto_tfm_ctx(tfm);\n\n\tcrypto_free_ablkcipher(*ctx);\n}\n\nstatic int crypto_init_skcipher_ops_ablkcipher(struct crypto_tfm *tfm)\n{\n\tstruct crypto_alg *calg = tfm->__crt_alg;\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct crypto_ablkcipher **ctx = crypto_tfm_ctx(tfm);\n\tstruct crypto_ablkcipher *ablkcipher;\n\tstruct crypto_tfm *abtfm;\n\n\tif (!crypto_mod_get(calg))\n\t\treturn -EAGAIN;\n\n\tabtfm = __crypto_alloc_tfm(calg, 0, 0);\n\tif (IS_ERR(abtfm)) {\n\t\tcrypto_mod_put(calg);\n\t\treturn PTR_ERR(abtfm);\n\t}\n\n\tablkcipher = __crypto_ablkcipher_cast(abtfm);\n\t*ctx = ablkcipher;\n\ttfm->exit = crypto_exit_skcipher_ops_ablkcipher;\n\n\tskcipher->setkey = skcipher_setkey_ablkcipher;\n\tskcipher->encrypt = skcipher_encrypt_ablkcipher;\n\tskcipher->decrypt = skcipher_decrypt_ablkcipher;\n\n\tskcipher->ivsize = crypto_ablkcipher_ivsize(ablkcipher);\n\tskcipher->reqsize = crypto_ablkcipher_reqsize(ablkcipher) +\n\t\t\t    sizeof(struct ablkcipher_request);\n\tskcipher->keysize = calg->cra_ablkcipher.max_keysize;\n\n\treturn 0;\n}\n\nstatic int skcipher_setkey_unaligned(struct crypto_skcipher *tfm,\n\t\t\t\t     const u8 *key, unsigned int keylen)\n{\n\tunsigned long alignmask = crypto_skcipher_alignmask(tfm);\n\tstruct skcipher_alg *cipher = crypto_skcipher_alg(tfm);\n\tu8 *buffer, *alignbuffer;\n\tunsigned long absize;\n\tint ret;\n\n\tabsize = keylen + alignmask;\n\tbuffer = kmalloc(absize, GFP_ATOMIC);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\talignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);\n\tmemcpy(alignbuffer, key, keylen);\n\tret = cipher->setkey(tfm, alignbuffer, keylen);\n\tkzfree(buffer);\n\treturn ret;\n}\n\nstatic int skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t   unsigned int keylen)\n{\n\tstruct skcipher_alg *cipher = crypto_skcipher_alg(tfm);\n\tunsigned long alignmask = crypto_skcipher_alignmask(tfm);\n\n\tif (keylen < cipher->min_keysize || keylen > cipher->max_keysize) {\n\t\tcrypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((unsigned long)key & alignmask)\n\t\treturn skcipher_setkey_unaligned(tfm, key, keylen);\n\n\treturn cipher->setkey(tfm, key, keylen);\n}\n\nstatic void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\talg->exit(skcipher);\n}\n\nstatic int crypto_skcipher_init_tfm(struct crypto_tfm *tfm)\n{\n\tstruct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);\n\tstruct skcipher_alg *alg = crypto_skcipher_alg(skcipher);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_blkcipher_type)\n\t\treturn crypto_init_skcipher_ops_blkcipher(tfm);\n\n\tif (tfm->__crt_alg->cra_type == &crypto_ablkcipher_type ||\n\t    tfm->__crt_alg->cra_type == &crypto_givcipher_type)\n\t\treturn crypto_init_skcipher_ops_ablkcipher(tfm);\n\n\tskcipher->setkey = skcipher_setkey;\n\tskcipher->encrypt = alg->encrypt;\n\tskcipher->decrypt = alg->decrypt;\n\tskcipher->ivsize = alg->ivsize;\n\tskcipher->keysize = alg->max_keysize;\n\n\tif (alg->exit)\n\t\tskcipher->base.exit = crypto_skcipher_exit_tfm;\n\n\tif (alg->init)\n\t\treturn alg->init(skcipher);\n\n\treturn 0;\n}\n\nstatic void crypto_skcipher_free_instance(struct crypto_instance *inst)\n{\n\tstruct skcipher_instance *skcipher =\n\t\tcontainer_of(inst, struct skcipher_instance, s.base);\n\n\tskcipher->free(skcipher);\n}\n\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n\t__maybe_unused;\nstatic void crypto_skcipher_show(struct seq_file *m, struct crypto_alg *alg)\n{\n\tstruct skcipher_alg *skcipher = container_of(alg, struct skcipher_alg,\n\t\t\t\t\t\t     base);\n\n\tseq_printf(m, \"type         : skcipher\\n\");\n\tseq_printf(m, \"async        : %s\\n\",\n\t\t   alg->cra_flags & CRYPTO_ALG_ASYNC ?  \"yes\" : \"no\");\n\tseq_printf(m, \"blocksize    : %u\\n\", alg->cra_blocksize);\n\tseq_printf(m, \"min keysize  : %u\\n\", skcipher->min_keysize);\n\tseq_printf(m, \"max keysize  : %u\\n\", skcipher->max_keysize);\n\tseq_printf(m, \"ivsize       : %u\\n\", skcipher->ivsize);\n\tseq_printf(m, \"chunksize    : %u\\n\", skcipher->chunksize);\n\tseq_printf(m, \"walksize     : %u\\n\", skcipher->walksize);\n}\n\n#ifdef CONFIG_NET\nstatic int crypto_skcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\tstruct crypto_report_blkcipher rblkcipher;\n\tstruct skcipher_alg *skcipher = container_of(alg, struct skcipher_alg,\n\t\t\t\t\t\t     base);\n\n\tstrncpy(rblkcipher.type, \"skcipher\", sizeof(rblkcipher.type));\n\tstrncpy(rblkcipher.geniv, \"<none>\", sizeof(rblkcipher.geniv));\n\n\trblkcipher.blocksize = alg->cra_blocksize;\n\trblkcipher.min_keysize = skcipher->min_keysize;\n\trblkcipher.max_keysize = skcipher->max_keysize;\n\trblkcipher.ivsize = skcipher->ivsize;\n\n\tif (nla_put(skb, CRYPTOCFGA_REPORT_BLKCIPHER,\n\t\t    sizeof(struct crypto_report_blkcipher), &rblkcipher))\n\t\tgoto nla_put_failure;\n\treturn 0;\n\nnla_put_failure:\n\treturn -EMSGSIZE;\n}\n#else\nstatic int crypto_skcipher_report(struct sk_buff *skb, struct crypto_alg *alg)\n{\n\treturn -ENOSYS;\n}\n#endif\n\nstatic const struct crypto_type crypto_skcipher_type2 = {\n\t.extsize = crypto_skcipher_extsize,\n\t.init_tfm = crypto_skcipher_init_tfm,\n\t.free = crypto_skcipher_free_instance,\n#ifdef CONFIG_PROC_FS\n\t.show = crypto_skcipher_show,\n#endif\n\t.report = crypto_skcipher_report,\n\t.maskclear = ~CRYPTO_ALG_TYPE_MASK,\n\t.maskset = CRYPTO_ALG_TYPE_BLKCIPHER_MASK,\n\t.type = CRYPTO_ALG_TYPE_SKCIPHER,\n\t.tfmsize = offsetof(struct crypto_skcipher, base),\n};\n\nint crypto_grab_skcipher(struct crypto_skcipher_spawn *spawn,\n\t\t\t  const char *name, u32 type, u32 mask)\n{\n\tspawn->base.frontend = &crypto_skcipher_type2;\n\treturn crypto_grab_spawn(&spawn->base, name, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_grab_skcipher);\n\nstruct crypto_skcipher *crypto_alloc_skcipher(const char *alg_name,\n\t\t\t\t\t      u32 type, u32 mask)\n{\n\treturn crypto_alloc_tfm(alg_name, &crypto_skcipher_type2, type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_alloc_skcipher);\n\nint crypto_has_skcipher2(const char *alg_name, u32 type, u32 mask)\n{\n\treturn crypto_type_has_alg(alg_name, &crypto_skcipher_type2,\n\t\t\t\t   type, mask);\n}\nEXPORT_SYMBOL_GPL(crypto_has_skcipher2);\n\nstatic int skcipher_prepare_alg(struct skcipher_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\n\tif (alg->ivsize > PAGE_SIZE / 8 || alg->chunksize > PAGE_SIZE / 8 ||\n\t    alg->walksize > PAGE_SIZE / 8)\n\t\treturn -EINVAL;\n\n\tif (!alg->chunksize)\n\t\talg->chunksize = base->cra_blocksize;\n\tif (!alg->walksize)\n\t\talg->walksize = alg->chunksize;\n\n\tbase->cra_type = &crypto_skcipher_type2;\n\tbase->cra_flags &= ~CRYPTO_ALG_TYPE_MASK;\n\tbase->cra_flags |= CRYPTO_ALG_TYPE_SKCIPHER;\n\n\treturn 0;\n}\n\nint crypto_register_skcipher(struct skcipher_alg *alg)\n{\n\tstruct crypto_alg *base = &alg->base;\n\tint err;\n\n\terr = skcipher_prepare_alg(alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_alg(base);\n}\nEXPORT_SYMBOL_GPL(crypto_register_skcipher);\n\nvoid crypto_unregister_skcipher(struct skcipher_alg *alg)\n{\n\tcrypto_unregister_alg(&alg->base);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skcipher);\n\nint crypto_register_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\tret = crypto_register_skcipher(&algs[i]);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\tfor (--i; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(crypto_register_skciphers);\n\nvoid crypto_unregister_skciphers(struct skcipher_alg *algs, int count)\n{\n\tint i;\n\n\tfor (i = count - 1; i >= 0; --i)\n\t\tcrypto_unregister_skcipher(&algs[i]);\n}\nEXPORT_SYMBOL_GPL(crypto_unregister_skciphers);\n\nint skcipher_register_instance(struct crypto_template *tmpl,\n\t\t\t   struct skcipher_instance *inst)\n{\n\tint err;\n\n\terr = skcipher_prepare_alg(&inst->alg);\n\tif (err)\n\t\treturn err;\n\n\treturn crypto_register_instance(tmpl, skcipher_crypto_instance(inst));\n}\nEXPORT_SYMBOL_GPL(skcipher_register_instance);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Symmetric key cipher type\");\n"], "filenames": ["crypto/skcipher.c"], "buggy_code_start_loc": [766], "buggy_code_end_loc": [788], "fixing_code_start_loc": [767], "fixing_code_end_loc": [826], "type": "CWE-476", "message": "The crypto_skcipher_init_tfm function in crypto/skcipher.c in the Linux kernel through 4.11.2 relies on a setkey function that lacks a key-size check, which allows local users to cause a denial of service (NULL pointer dereference) via a crafted application.", "other": {"cve": {"id": "CVE-2017-9211", "sourceIdentifier": "cve@mitre.org", "published": "2017-05-23T05:29:00.247", "lastModified": "2017-06-08T12:07:35.823", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The crypto_skcipher_init_tfm function in crypto/skcipher.c in the Linux kernel through 4.11.2 relies on a setkey function that lacks a key-size check, which allows local users to cause a denial of service (NULL pointer dereference) via a crafted application."}, {"lang": "es", "value": "La funci\u00f3n crypto_skcipher_init_tfm en el archivo crypto/skcipher.c en el kernel de Linux hasta versi\u00f3n 4.11.2, se basa en una funci\u00f3n setkey que carece de una comprobaci\u00f3n de tama\u00f1o de clave, que permite a los usuarios locales causar una denegaci\u00f3n de servicio (desreferencia de puntero NULL) por medio de una aplicaci\u00f3n especialmente dise\u00f1ada."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.11.2", "matchCriteriaId": "14A37919-F5C8-4F53-895A-69FD63F4FD5D"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=9933e113c2e87a9f46a40fde8dafbf801dca1ab9", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/9933e113c2e87a9f46a40fde8dafbf801dca1ab9", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://patchwork.kernel.org/patch/9718933/", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9933e113c2e87a9f46a40fde8dafbf801dca1ab9"}}