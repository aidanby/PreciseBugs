{"buggy_code": ["/*\n * Copyright (c) 2009-2017 Nicira, Inc.\n * Copyright (c) 2010 Jean Tourrilhes - HP-Labs.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at:\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <config.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <stdbool.h>\n#include <stdlib.h>\n#include <unistd.h>\n\n#include \"bitmap.h\"\n#include \"bundles.h\"\n#include \"byte-order.h\"\n#include \"classifier.h\"\n#include \"connectivity.h\"\n#include \"connmgr.h\"\n#include \"coverage.h\"\n#include \"dp-packet.h\"\n#include \"hash.h\"\n#include \"openvswitch/hmap.h\"\n#include \"netdev.h\"\n#include \"nx-match.h\"\n#include \"ofproto.h\"\n#include \"ofproto-provider.h\"\n#include \"openflow/nicira-ext.h\"\n#include \"openflow/openflow.h\"\n#include \"openvswitch/dynamic-string.h\"\n#include \"openvswitch/meta-flow.h\"\n#include \"openvswitch/ofp-actions.h\"\n#include \"openvswitch/ofp-errors.h\"\n#include \"openvswitch/ofp-msgs.h\"\n#include \"openvswitch/ofp-print.h\"\n#include \"openvswitch/ofp-util.h\"\n#include \"openvswitch/ofpbuf.h\"\n#include \"openvswitch/vlog.h\"\n#include \"ovs-rcu.h\"\n#include \"packets.h\"\n#include \"pinsched.h\"\n#include \"poll-loop.h\"\n#include \"random.h\"\n#include \"seq.h\"\n#include \"openvswitch/shash.h\"\n#include \"simap.h\"\n#include \"smap.h\"\n#include \"sset.h\"\n#include \"timeval.h\"\n#include \"tun-metadata.h\"\n#include \"unaligned.h\"\n#include \"unixctl.h\"\n#include \"util.h\"\n\nVLOG_DEFINE_THIS_MODULE(ofproto);\n\nCOVERAGE_DEFINE(ofproto_flush);\nCOVERAGE_DEFINE(ofproto_packet_out);\nCOVERAGE_DEFINE(ofproto_queue_req);\nCOVERAGE_DEFINE(ofproto_recv_openflow);\nCOVERAGE_DEFINE(ofproto_reinit_ports);\nCOVERAGE_DEFINE(ofproto_update_port);\n\n/* Default fields to use for prefix tries in each flow table, unless something\n * else is configured. */\nconst enum mf_field_id default_prefix_fields[2] =\n    { MFF_IPV4_DST, MFF_IPV4_SRC };\n\n/* oftable. */\nstatic void oftable_init(struct oftable *);\nstatic void oftable_destroy(struct oftable *);\n\nstatic void oftable_set_name(struct oftable *, const char *name);\n\nstatic enum ofperr evict_rules_from_table(struct oftable *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void oftable_configure_eviction(struct oftable *,\n                                       unsigned int eviction,\n                                       const struct mf_subfield *fields,\n                                       size_t n_fields)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* This is the only combination of OpenFlow eviction flags that OVS supports: a\n * combination of OF1.4+ importance, the remaining lifetime of the flow, and\n * fairness based on user-specified fields. */\n#define OFPROTO_EVICTION_FLAGS \\\n    (OFPTMPEF14_OTHER | OFPTMPEF14_IMPORTANCE | OFPTMPEF14_LIFETIME)\n\n/* A set of rules within a single OpenFlow table (oftable) that have the same\n * values for the oftable's eviction_fields.  A rule to be evicted, when one is\n * needed, is taken from the eviction group that contains the greatest number\n * of rules.\n *\n * An oftable owns any number of eviction groups, each of which contains any\n * number of rules.\n *\n * Membership in an eviction group is imprecise, based on the hash of the\n * oftable's eviction_fields (in the eviction_group's id_node.hash member).\n * That is, if two rules have different eviction_fields, but those\n * eviction_fields hash to the same value, then they will belong to the same\n * eviction_group anyway.\n *\n * (When eviction is not enabled on an oftable, we don't track any eviction\n * groups, to save time and space.) */\nstruct eviction_group {\n    struct hmap_node id_node;   /* In oftable's \"eviction_groups_by_id\". */\n    struct heap_node size_node; /* In oftable's \"eviction_groups_by_size\". */\n    struct heap rules;          /* Contains \"struct rule\"s. */\n};\n\nstatic bool choose_rule_to_evict(struct oftable *table, struct rule **rulep)\n    OVS_REQUIRES(ofproto_mutex);\nstatic uint64_t rule_eviction_priority(struct ofproto *ofproto, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void eviction_group_add_rule(struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void eviction_group_remove_rule(struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void rule_criteria_init(struct rule_criteria *, uint8_t table_id,\n                               const struct match *match, int priority,\n                               ovs_version_t version,\n                               ovs_be64 cookie, ovs_be64 cookie_mask,\n                               ofp_port_t out_port, uint32_t out_group);\nstatic void rule_criteria_require_rw(struct rule_criteria *,\n                                     bool can_write_readonly);\nstatic void rule_criteria_destroy(struct rule_criteria *);\n\nstatic enum ofperr collect_rules_loose(struct ofproto *,\n                                       const struct rule_criteria *,\n                                       struct rule_collection *);\n\nstruct learned_cookie {\n    union {\n        /* In struct ofproto's 'learned_cookies' hmap. */\n        struct hmap_node hmap_node OVS_GUARDED_BY(ofproto_mutex);\n\n        /* In 'dead_cookies' list when removed from hmap. */\n        struct ovs_list list_node;\n    } u;\n\n    /* Key. */\n    ovs_be64 cookie OVS_GUARDED_BY(ofproto_mutex);\n    uint8_t table_id OVS_GUARDED_BY(ofproto_mutex);\n\n    /* Number of references from \"learn\" actions.\n     *\n     * When this drops to 0, all of the flows in 'table_id' with the specified\n     * 'cookie' are deleted. */\n    int n OVS_GUARDED_BY(ofproto_mutex);\n};\n\nstatic const struct ofpact_learn *next_learn_with_delete(\n    const struct rule_actions *, const struct ofpact_learn *start);\n\nstatic void learned_cookies_inc(struct ofproto *, const struct rule_actions *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void learned_cookies_dec(struct ofproto *, const struct rule_actions *,\n                                struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void learned_cookies_flush(struct ofproto *, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* ofport. */\nstatic void ofport_destroy__(struct ofport *) OVS_EXCLUDED(ofproto_mutex);\nstatic void ofport_destroy(struct ofport *, bool del);\nstatic bool ofport_is_mtu_overridden(const struct ofproto *,\n                                     const struct ofport *);\n\nstatic int update_port(struct ofproto *, const char *devname);\nstatic int init_ports(struct ofproto *);\nstatic void reinit_ports(struct ofproto *);\n\nstatic long long int ofport_get_usage(const struct ofproto *,\n                                      ofp_port_t ofp_port);\nstatic void ofport_set_usage(struct ofproto *, ofp_port_t ofp_port,\n                             long long int last_used);\nstatic void ofport_remove_usage(struct ofproto *, ofp_port_t ofp_port);\n\n/* Ofport usage.\n *\n * Keeps track of the currently used and recently used ofport values and is\n * used to prevent immediate recycling of ofport values. */\nstruct ofport_usage {\n    struct hmap_node hmap_node; /* In struct ofproto's \"ofport_usage\" hmap. */\n    ofp_port_t ofp_port;        /* OpenFlow port number. */\n    long long int last_used;    /* Last time the 'ofp_port' was used. LLONG_MAX\n                                   represents in-use ofports. */\n};\n\n/* rule. */\nstatic void ofproto_rule_send_removed(struct rule *)\n        OVS_EXCLUDED(ofproto_mutex);\nstatic bool rule_is_readonly(const struct rule *);\nstatic void ofproto_rule_insert__(struct ofproto *, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_rule_remove__(struct ofproto *, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* The source of an OpenFlow request.\n *\n * A table modification request can be generated externally, via OpenFlow, or\n * internally through a function call.  This structure indicates the source of\n * an OpenFlow-generated table modification.  For an internal flow_mod, it\n * isn't meaningful and thus supplied as NULL. */\nstruct openflow_mod_requester {\n    struct ofconn *ofconn;      /* Connection on which flow_mod arrived. */\n    const struct ofp_header *request;\n};\n\n/* OpenFlow. */\nstatic enum ofperr ofproto_rule_create(struct ofproto *, struct cls_rule *,\n                                       uint8_t table_id, ovs_be64 new_cookie,\n                                       uint16_t idle_timeout,\n                                       uint16_t hard_timeout,\n                                       enum ofputil_flow_mod_flags flags,\n                                       uint16_t importance,\n                                       const struct ofpact *ofpacts,\n                                       size_t ofpacts_len,\n                                       uint64_t match_tlv_bitmap,\n                                       uint64_t ofpacts_tlv_bitmap,\n                                       struct rule **new_rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS;\n\nstatic void replace_rule_start(struct ofproto *, struct ofproto_flow_mod *,\n                               struct rule *old_rule, struct rule *new_rule)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void replace_rule_revert(struct ofproto *, struct rule *old_rule,\n                                struct rule *new_rule)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void replace_rule_finish(struct ofproto *, struct ofproto_flow_mod *,\n                                const struct openflow_mod_requester *,\n                                struct rule *old_rule, struct rule *new_rule,\n                                struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void delete_flows__(struct rule_collection *,\n                           enum ofp_flow_removed_reason,\n                           const struct openflow_mod_requester *)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic bool ofproto_group_exists(const struct ofproto *, uint32_t group_id);\nstatic void handle_openflow(struct ofconn *, const struct ofpbuf *);\nstatic enum ofperr ofproto_flow_mod_init(struct ofproto *,\n                                         struct ofproto_flow_mod *,\n                                         const struct ofputil_flow_mod *fm,\n                                         struct rule *)\n    OVS_EXCLUDED(ofproto_mutex);\nstatic enum ofperr ofproto_flow_mod_start(struct ofproto *,\n                                          struct ofproto_flow_mod *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_flow_mod_revert(struct ofproto *,\n                                    struct ofproto_flow_mod *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_flow_mod_finish(struct ofproto *,\n                                    struct ofproto_flow_mod *,\n                                    const struct openflow_mod_requester *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic enum ofperr handle_flow_mod__(struct ofproto *,\n                                     const struct ofputil_flow_mod *,\n                                     const struct openflow_mod_requester *)\n    OVS_EXCLUDED(ofproto_mutex);\nstatic void calc_duration(long long int start, long long int now,\n                          uint32_t *sec, uint32_t *nsec);\n\n/* ofproto. */\nstatic uint64_t pick_datapath_id(const struct ofproto *);\nstatic uint64_t pick_fallback_dpid(void);\nstatic void ofproto_destroy__(struct ofproto *);\nstatic void update_mtu(struct ofproto *, struct ofport *);\nstatic void update_mtu_ofproto(struct ofproto *);\nstatic void meter_delete(struct ofproto *, uint32_t first, uint32_t last);\nstatic void meter_insert_rule(struct rule *);\n\n/* unixctl. */\nstatic void ofproto_unixctl_init(void);\n\n/* All registered ofproto classes, in probe order. */\nstatic const struct ofproto_class **ofproto_classes;\nstatic size_t n_ofproto_classes;\nstatic size_t allocated_ofproto_classes;\n\n/* Global lock that protects all flow table operations. */\nstruct ovs_mutex ofproto_mutex = OVS_MUTEX_INITIALIZER;\n\nunsigned ofproto_flow_limit = OFPROTO_FLOW_LIMIT_DEFAULT;\nunsigned ofproto_max_idle = OFPROTO_MAX_IDLE_DEFAULT;\n\nsize_t n_handlers, n_revalidators;\nchar *pmd_cpu_mask;\n\n/* Map from datapath name to struct ofproto, for use by unixctl commands. */\nstatic struct hmap all_ofprotos = HMAP_INITIALIZER(&all_ofprotos);\n\n/* Initial mappings of port to OpenFlow number mappings. */\nstatic struct shash init_ofp_ports = SHASH_INITIALIZER(&init_ofp_ports);\n\nstatic struct vlog_rate_limit rl = VLOG_RATE_LIMIT_INIT(1, 5);\n\n/* The default value of true waits for flow restore. */\nstatic bool flow_restore_wait = true;\n\n/* Must be called to initialize the ofproto library.\n *\n * The caller may pass in 'iface_hints', which contains an shash of\n * \"iface_hint\" elements indexed by the interface's name.  The provider\n * may use these hints to describe the startup configuration in order to\n * reinitialize its state.  The caller owns the provided data, so a\n * provider will make copies of anything required.  An ofproto provider\n * will remove any existing state that is not described by the hint, and\n * may choose to remove it all. */\nvoid\nofproto_init(const struct shash *iface_hints)\n{\n    struct shash_node *node;\n    size_t i;\n\n    ofproto_class_register(&ofproto_dpif_class);\n\n    /* Make a local copy, since we don't own 'iface_hints' elements. */\n    SHASH_FOR_EACH(node, iface_hints) {\n        const struct iface_hint *orig_hint = node->data;\n        struct iface_hint *new_hint = xmalloc(sizeof *new_hint);\n        const char *br_type = ofproto_normalize_type(orig_hint->br_type);\n\n        new_hint->br_name = xstrdup(orig_hint->br_name);\n        new_hint->br_type = xstrdup(br_type);\n        new_hint->ofp_port = orig_hint->ofp_port;\n\n        shash_add(&init_ofp_ports, node->name, new_hint);\n    }\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        ofproto_classes[i]->init(&init_ofp_ports);\n    }\n\n    ofproto_unixctl_init();\n}\n\n/* 'type' should be a normalized datapath type, as returned by\n * ofproto_normalize_type().  Returns the corresponding ofproto_class\n * structure, or a null pointer if there is none registered for 'type'. */\nstatic const struct ofproto_class *\nofproto_class_find__(const char *type)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        const struct ofproto_class *class = ofproto_classes[i];\n        struct sset types;\n        bool found;\n\n        sset_init(&types);\n        class->enumerate_types(&types);\n        found = sset_contains(&types, type);\n        sset_destroy(&types);\n\n        if (found) {\n            return class;\n        }\n    }\n    VLOG_WARN(\"unknown datapath type %s\", type);\n    return NULL;\n}\n\n/* Registers a new ofproto class.  After successful registration, new ofprotos\n * of that type can be created using ofproto_create(). */\nint\nofproto_class_register(const struct ofproto_class *new_class)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        if (ofproto_classes[i] == new_class) {\n            return EEXIST;\n        }\n    }\n\n    if (n_ofproto_classes >= allocated_ofproto_classes) {\n        ofproto_classes = x2nrealloc(ofproto_classes,\n                                     &allocated_ofproto_classes,\n                                     sizeof *ofproto_classes);\n    }\n    ofproto_classes[n_ofproto_classes++] = new_class;\n    return 0;\n}\n\n/* Unregisters a datapath provider.  'type' must have been previously\n * registered and not currently be in use by any ofprotos.  After\n * unregistration new datapaths of that type cannot be opened using\n * ofproto_create(). */\nint\nofproto_class_unregister(const struct ofproto_class *class)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        if (ofproto_classes[i] == class) {\n            for (i++; i < n_ofproto_classes; i++) {\n                ofproto_classes[i - 1] = ofproto_classes[i];\n            }\n            n_ofproto_classes--;\n            return 0;\n        }\n    }\n    VLOG_WARN(\"attempted to unregister an ofproto class that is not \"\n              \"registered\");\n    return EAFNOSUPPORT;\n}\n\n/* Clears 'types' and enumerates all registered ofproto types into it.  The\n * caller must first initialize the sset. */\nvoid\nofproto_enumerate_types(struct sset *types)\n{\n    size_t i;\n\n    sset_clear(types);\n    for (i = 0; i < n_ofproto_classes; i++) {\n        ofproto_classes[i]->enumerate_types(types);\n    }\n}\n\n/* Returns the fully spelled out name for the given ofproto 'type'.\n *\n * Normalized type string can be compared with strcmp().  Unnormalized type\n * string might be the same even if they have different spellings. */\nconst char *\nofproto_normalize_type(const char *type)\n{\n    return type && type[0] ? type : \"system\";\n}\n\n/* Clears 'names' and enumerates the names of all known created ofprotos with\n * the given 'type'.  The caller must first initialize the sset.  Returns 0 if\n * successful, otherwise a positive errno value.\n *\n * Some kinds of datapaths might not be practically enumerable.  This is not\n * considered an error. */\nint\nofproto_enumerate_names(const char *type, struct sset *names)\n{\n    const struct ofproto_class *class = ofproto_class_find__(type);\n    return class ? class->enumerate_names(type, names) : EAFNOSUPPORT;\n}\n\nstatic void\nofproto_bump_tables_version(struct ofproto *ofproto)\n{\n    ++ofproto->tables_version;\n    ofproto->ofproto_class->set_tables_version(ofproto,\n                                               ofproto->tables_version);\n}\n\nint\nofproto_create(const char *datapath_name, const char *datapath_type,\n               struct ofproto **ofprotop)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct ofproto_class *class;\n    struct ofproto *ofproto;\n    int error;\n    int i;\n\n    *ofprotop = NULL;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n    if (!class) {\n        VLOG_WARN(\"could not create datapath %s of unknown type %s\",\n                  datapath_name, datapath_type);\n        return EAFNOSUPPORT;\n    }\n\n    ofproto = class->alloc();\n    if (!ofproto) {\n        VLOG_ERR(\"failed to allocate datapath %s of type %s\",\n                 datapath_name, datapath_type);\n        return ENOMEM;\n    }\n\n    /* Initialize. */\n    ovs_mutex_lock(&ofproto_mutex);\n    memset(ofproto, 0, sizeof *ofproto);\n    ofproto->ofproto_class = class;\n    ofproto->name = xstrdup(datapath_name);\n    ofproto->type = xstrdup(datapath_type);\n    hmap_insert(&all_ofprotos, &ofproto->hmap_node,\n                hash_string(ofproto->name, 0));\n    ofproto->datapath_id = 0;\n    ofproto->forward_bpdu = false;\n    ofproto->fallback_dpid = pick_fallback_dpid();\n    ofproto->mfr_desc = NULL;\n    ofproto->hw_desc = NULL;\n    ofproto->sw_desc = NULL;\n    ofproto->serial_desc = NULL;\n    ofproto->dp_desc = NULL;\n    ofproto->frag_handling = OFPUTIL_FRAG_NORMAL;\n    hmap_init(&ofproto->ports);\n    hmap_init(&ofproto->ofport_usage);\n    shash_init(&ofproto->port_by_name);\n    simap_init(&ofproto->ofp_requests);\n    ofproto->max_ports = ofp_to_u16(OFPP_MAX);\n    ofproto->eviction_group_timer = LLONG_MIN;\n    ofproto->tables = NULL;\n    ofproto->n_tables = 0;\n    ofproto->tables_version = OVS_VERSION_MIN;\n    hindex_init(&ofproto->cookies);\n    hmap_init(&ofproto->learned_cookies);\n    ovs_list_init(&ofproto->expirable);\n    ofproto->connmgr = connmgr_create(ofproto, datapath_name, datapath_name);\n    ofproto->min_mtu = INT_MAX;\n    cmap_init(&ofproto->groups);\n    ovs_mutex_unlock(&ofproto_mutex);\n    ofproto->ogf.types = 0xf;\n    ofproto->ogf.capabilities = OFPGFC_CHAINING | OFPGFC_SELECT_LIVENESS |\n                                OFPGFC_SELECT_WEIGHT;\n    for (i = 0; i < 4; i++) {\n        ofproto->ogf.max_groups[i] = OFPG_MAX;\n        ofproto->ogf.ofpacts[i] = (UINT64_C(1) << N_OFPACTS) - 1;\n    }\n    ovsrcu_set(&ofproto->metadata_tab, tun_metadata_alloc(NULL));\n\n    ovs_mutex_init(&ofproto->vl_mff_map.mutex);\n    cmap_init(&ofproto->vl_mff_map.cmap);\n\n    error = ofproto->ofproto_class->construct(ofproto);\n    if (error) {\n        VLOG_ERR(\"failed to open datapath %s: %s\",\n                 datapath_name, ovs_strerror(error));\n        ovs_mutex_lock(&ofproto_mutex);\n        connmgr_destroy(ofproto->connmgr);\n        ofproto->connmgr = NULL;\n        ovs_mutex_unlock(&ofproto_mutex);\n        ofproto_destroy__(ofproto);\n        return error;\n    }\n\n    /* Check that hidden tables, if any, are at the end. */\n    ovs_assert(ofproto->n_tables);\n    for (i = 0; i + 1 < ofproto->n_tables; i++) {\n        enum oftable_flags flags = ofproto->tables[i].flags;\n        enum oftable_flags next_flags = ofproto->tables[i + 1].flags;\n\n        ovs_assert(!(flags & OFTABLE_HIDDEN) || next_flags & OFTABLE_HIDDEN);\n    }\n\n    ofproto->datapath_id = pick_datapath_id(ofproto);\n    init_ports(ofproto);\n\n    /* Initialize meters table. */\n    if (ofproto->ofproto_class->meter_get_features) {\n        ofproto->ofproto_class->meter_get_features(ofproto,\n                                                   &ofproto->meter_features);\n    } else {\n        memset(&ofproto->meter_features, 0, sizeof ofproto->meter_features);\n    }\n    ofproto->meters = xzalloc((ofproto->meter_features.max_meters + 1)\n                              * sizeof(struct meter *));\n\n    /* Set the initial tables version. */\n    ofproto_bump_tables_version(ofproto);\n\n    *ofprotop = ofproto;\n    return 0;\n}\n\n/* Must be called (only) by an ofproto implementation in its constructor\n * function.  See the large comment on 'construct' in struct ofproto_class for\n * details. */\nvoid\nofproto_init_tables(struct ofproto *ofproto, int n_tables)\n{\n    struct oftable *table;\n\n    ovs_assert(!ofproto->n_tables);\n    ovs_assert(n_tables >= 1 && n_tables <= 255);\n\n    ofproto->n_tables = n_tables;\n    ofproto->tables = xmalloc(n_tables * sizeof *ofproto->tables);\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        oftable_init(table);\n    }\n}\n\n/* To be optionally called (only) by an ofproto implementation in its\n * constructor function.  See the large comment on 'construct' in struct\n * ofproto_class for details.\n *\n * Sets the maximum number of ports to 'max_ports'.  The ofproto generic layer\n * will then ensure that actions passed into the ofproto implementation will\n * not refer to OpenFlow ports numbered 'max_ports' or higher.  If this\n * function is not called, there will be no such restriction.\n *\n * Reserved ports numbered OFPP_MAX and higher are special and not subject to\n * the 'max_ports' restriction. */\nvoid\nofproto_init_max_ports(struct ofproto *ofproto, uint16_t max_ports)\n{\n    ovs_assert(max_ports <= ofp_to_u16(OFPP_MAX));\n    ofproto->max_ports = max_ports;\n}\n\nuint64_t\nofproto_get_datapath_id(const struct ofproto *ofproto)\n{\n    return ofproto->datapath_id;\n}\n\nvoid\nofproto_set_datapath_id(struct ofproto *p, uint64_t datapath_id)\n{\n    uint64_t old_dpid = p->datapath_id;\n    p->datapath_id = datapath_id ? datapath_id : pick_datapath_id(p);\n    if (p->datapath_id != old_dpid) {\n        /* Force all active connections to reconnect, since there is no way to\n         * notify a controller that the datapath ID has changed. */\n        ofproto_reconnect_controllers(p);\n    }\n}\n\nvoid\nofproto_set_controllers(struct ofproto *p,\n                        const struct ofproto_controller *controllers,\n                        size_t n_controllers, uint32_t allowed_versions)\n{\n    connmgr_set_controllers(p->connmgr, controllers, n_controllers,\n                            allowed_versions);\n}\n\nvoid\nofproto_set_fail_mode(struct ofproto *p, enum ofproto_fail_mode fail_mode)\n{\n    connmgr_set_fail_mode(p->connmgr, fail_mode);\n}\n\n/* Drops the connections between 'ofproto' and all of its controllers, forcing\n * them to reconnect. */\nvoid\nofproto_reconnect_controllers(struct ofproto *ofproto)\n{\n    connmgr_reconnect(ofproto->connmgr);\n}\n\n/* Sets the 'n' TCP port addresses in 'extras' as ones to which 'ofproto''s\n * in-band control should guarantee access, in the same way that in-band\n * control guarantees access to OpenFlow controllers. */\nvoid\nofproto_set_extra_in_band_remotes(struct ofproto *ofproto,\n                                  const struct sockaddr_in *extras, size_t n)\n{\n    connmgr_set_extra_in_band_remotes(ofproto->connmgr, extras, n);\n}\n\n/* Sets the OpenFlow queue used by flows set up by in-band control on\n * 'ofproto' to 'queue_id'.  If 'queue_id' is negative, then in-band control\n * flows will use the default queue. */\nvoid\nofproto_set_in_band_queue(struct ofproto *ofproto, int queue_id)\n{\n    connmgr_set_in_band_queue(ofproto->connmgr, queue_id);\n}\n\n/* Sets the number of flows at which eviction from the kernel flow table\n * will occur. */\nvoid\nofproto_set_flow_limit(unsigned limit)\n{\n    ofproto_flow_limit = limit;\n}\n\n/* Sets the maximum idle time for flows in the datapath before they are\n * expired. */\nvoid\nofproto_set_max_idle(unsigned max_idle)\n{\n    ofproto_max_idle = max_idle;\n}\n\n/* If forward_bpdu is true, the NORMAL action will forward frames with\n * reserved (e.g. STP) destination Ethernet addresses. if forward_bpdu is false,\n * the NORMAL action will drop these frames. */\nvoid\nofproto_set_forward_bpdu(struct ofproto *ofproto, bool forward_bpdu)\n{\n    bool old_val = ofproto->forward_bpdu;\n    ofproto->forward_bpdu = forward_bpdu;\n    if (old_val != ofproto->forward_bpdu) {\n        if (ofproto->ofproto_class->forward_bpdu_changed) {\n            ofproto->ofproto_class->forward_bpdu_changed(ofproto);\n        }\n    }\n}\n\n/* Sets the MAC aging timeout for the OFPP_NORMAL action on 'ofproto' to\n * 'idle_time', in seconds, and the maximum number of MAC table entries to\n * 'max_entries'. */\nvoid\nofproto_set_mac_table_config(struct ofproto *ofproto, unsigned idle_time,\n                             size_t max_entries)\n{\n    if (ofproto->ofproto_class->set_mac_table_config) {\n        ofproto->ofproto_class->set_mac_table_config(ofproto, idle_time,\n                                                     max_entries);\n    }\n}\n\n/* Multicast snooping configuration. */\n\n/* Configures multicast snooping on 'ofproto' using the settings\n * defined in 's'.  If 's' is NULL, disables multicast snooping.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_mcast_snooping(struct ofproto *ofproto,\n                           const struct ofproto_mcast_snooping_settings *s)\n{\n    return (ofproto->ofproto_class->set_mcast_snooping\n            ? ofproto->ofproto_class->set_mcast_snooping(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Configures multicast snooping flood settings on 'ofp_port' of 'ofproto'.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_mcast_snooping(struct ofproto *ofproto, void *aux,\n                           const struct ofproto_mcast_snooping_port_settings *s)\n{\n    return (ofproto->ofproto_class->set_mcast_snooping_port\n            ? ofproto->ofproto_class->set_mcast_snooping_port(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\nvoid\nofproto_set_cpu_mask(const char *cmask)\n{\n    free(pmd_cpu_mask);\n    pmd_cpu_mask = nullable_xstrdup(cmask);\n}\n\nvoid\nofproto_set_threads(int n_handlers_, int n_revalidators_)\n{\n    int threads = MAX(count_cpu_cores(), 2);\n\n    n_revalidators = MAX(n_revalidators_, 0);\n    n_handlers = MAX(n_handlers_, 0);\n\n    if (!n_revalidators) {\n        n_revalidators = n_handlers\n            ? MAX(threads - (int) n_handlers, 1)\n            : threads / 4 + 1;\n    }\n\n    if (!n_handlers) {\n        n_handlers = MAX(threads - (int) n_revalidators, 1);\n    }\n}\n\nvoid\nofproto_set_dp_desc(struct ofproto *p, const char *dp_desc)\n{\n    free(p->dp_desc);\n    p->dp_desc = nullable_xstrdup(dp_desc);\n}\n\nint\nofproto_set_snoops(struct ofproto *ofproto, const struct sset *snoops)\n{\n    return connmgr_set_snoops(ofproto->connmgr, snoops);\n}\n\nint\nofproto_set_netflow(struct ofproto *ofproto,\n                    const struct netflow_options *nf_options)\n{\n    if (nf_options && sset_is_empty(&nf_options->collectors)) {\n        nf_options = NULL;\n    }\n\n    if (ofproto->ofproto_class->set_netflow) {\n        return ofproto->ofproto_class->set_netflow(ofproto, nf_options);\n    } else {\n        return nf_options ? EOPNOTSUPP : 0;\n    }\n}\n\nint\nofproto_set_sflow(struct ofproto *ofproto,\n                  const struct ofproto_sflow_options *oso)\n{\n    if (oso && sset_is_empty(&oso->targets)) {\n        oso = NULL;\n    }\n\n    if (ofproto->ofproto_class->set_sflow) {\n        return ofproto->ofproto_class->set_sflow(ofproto, oso);\n    } else {\n        return oso ? EOPNOTSUPP : 0;\n    }\n}\n\nint\nofproto_set_ipfix(struct ofproto *ofproto,\n                  const struct ofproto_ipfix_bridge_exporter_options *bo,\n                  const struct ofproto_ipfix_flow_exporter_options *fo,\n                  size_t n_fo)\n{\n    if (ofproto->ofproto_class->set_ipfix) {\n        return ofproto->ofproto_class->set_ipfix(ofproto, bo, fo, n_fo);\n    } else {\n        return (bo || fo) ? EOPNOTSUPP : 0;\n    }\n}\n\nstatic int\nofproto_get_ipfix_stats(struct ofproto *ofproto,\n                        bool bridge_ipfix,\n                        struct ovs_list *replies)\n{\n    int error;\n\n    if (ofproto->ofproto_class->get_ipfix_stats) {\n        error = ofproto->ofproto_class->get_ipfix_stats(ofproto,\n                                                          bridge_ipfix,\n                                                          replies);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_ipfix_bridge_stats_request(struct ofconn *ofconn,\n                                  const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    enum ofperr error;\n\n    ofpmp_init(&replies, request);\n    error = ofproto_get_ipfix_stats(ofproto, true, &replies);\n\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_ipfix_flow_stats_request(struct ofconn *ofconn,\n                                const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    enum ofperr error;\n\n    ofpmp_init(&replies, request);\n    error = ofproto_get_ipfix_stats(ofproto, false, &replies);\n\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_nxt_ct_flush_zone(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    const struct nx_zone_id *nzi = ofpmsg_body(oh);\n\n    if (!is_all_zeros(nzi->zero, sizeof nzi->zero)) {\n        return OFPERR_NXBRC_MUST_BE_ZERO;\n    }\n\n    uint16_t zone = ntohs(nzi->zone_id);\n    if (ofproto->ofproto_class->ct_flush) {\n        ofproto->ofproto_class->ct_flush(ofproto, &zone);\n    } else {\n        return EOPNOTSUPP;\n    }\n\n    return 0;\n}\n\nvoid\nofproto_set_flow_restore_wait(bool flow_restore_wait_db)\n{\n    flow_restore_wait = flow_restore_wait_db;\n}\n\nbool\nofproto_get_flow_restore_wait(void)\n{\n    return flow_restore_wait;\n}\n\n\f\n/* Spanning Tree Protocol (STP) configuration. */\n\n/* Configures STP on 'ofproto' using the settings defined in 's'.  If\n * 's' is NULL, disables STP.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_stp(struct ofproto *ofproto,\n                const struct ofproto_stp_settings *s)\n{\n    return (ofproto->ofproto_class->set_stp\n            ? ofproto->ofproto_class->set_stp(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP status of 'ofproto' and stores it in 's'.  If the\n * 'enabled' member of 's' is false, then the other members are not\n * meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_get_stp_status(struct ofproto *ofproto,\n                       struct ofproto_stp_status *s)\n{\n    return (ofproto->ofproto_class->get_stp_status\n            ? ofproto->ofproto_class->get_stp_status(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Configures STP on 'ofp_port' of 'ofproto' using the settings defined\n * in 's'.  The caller is responsible for assigning STP port numbers\n * (using the 'port_num' member in the range of 1 through 255, inclusive)\n * and ensuring there are no duplicates.  If the 's' is NULL, then STP\n * is disabled on the port.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_stp(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct ofproto_port_stp_settings *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure STP on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->set_stp_port\n            ? ofproto->ofproto_class->set_stp_port(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP port status of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_stp_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct ofproto_port_stp_status *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get STP status on nonexistent \"\n                     \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->get_stp_port_status\n            ? ofproto->ofproto_class->get_stp_port_status(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP port statistics of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_stp_stats(struct ofproto *ofproto, ofp_port_t ofp_port,\n                           struct ofproto_port_stp_stats *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get STP stats on nonexistent \"\n                     \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->get_stp_port_stats\n            ? ofproto->ofproto_class->get_stp_port_stats(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Rapid Spanning Tree Protocol (RSTP) configuration. */\n\n/* Configures RSTP on 'ofproto' using the settings defined in 's'.  If\n * 's' is NULL, disables RSTP.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_rstp(struct ofproto *ofproto,\n                 const struct ofproto_rstp_settings *s)\n{\n    if (!ofproto->ofproto_class->set_rstp) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_rstp(ofproto, s);\n    return 0;\n}\n\n/* Retrieves RSTP status of 'ofproto' and stores it in 's'.  If the\n * 'enabled' member of 's' is false, then the other members are not\n * meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_get_rstp_status(struct ofproto *ofproto,\n                        struct ofproto_rstp_status *s)\n{\n    if (!ofproto->ofproto_class->get_rstp_status) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->get_rstp_status(ofproto, s);\n    return 0;\n}\n\n/* Configures RSTP on 'ofp_port' of 'ofproto' using the settings defined\n * in 's'.  The caller is responsible for assigning RSTP port numbers\n * (using the 'port_num' member in the range of 1 through 255, inclusive)\n * and ensuring there are no duplicates.  If the 's' is NULL, then RSTP\n * is disabled on the port.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_rstp(struct ofproto *ofproto, ofp_port_t ofp_port,\n                      const struct ofproto_port_rstp_settings *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure RSTP on nonexistent port %\"PRIu32,\n                ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    if (!ofproto->ofproto_class->set_rstp_port) {\n        return  EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_rstp_port(ofport, s);\n    return 0;\n}\n\n/* Retrieves RSTP port status of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_rstp_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                             struct ofproto_port_rstp_status *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get RSTP status on nonexistent \"\n                \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    if (!ofproto->ofproto_class->get_rstp_port_status) {\n        return  EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->get_rstp_port_status(ofport, s);\n    return 0;\n}\n\f\n/* Queue DSCP configuration. */\n\n/* Registers meta-data associated with the 'n_qdscp' Qualities of Service\n * 'queues' attached to 'ofport'.  This data is not intended to be sufficient\n * to implement QoS.  Instead, it is used to implement features which require\n * knowledge of what queues exist on a port, and some basic information about\n * them.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_port_set_queues(struct ofproto *ofproto, ofp_port_t ofp_port,\n                        const struct ofproto_port_queue *queues,\n                        size_t n_queues)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot set queues on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->set_queues\n            ? ofproto->ofproto_class->set_queues(ofport, queues, n_queues)\n            : EOPNOTSUPP);\n}\n\f\n/* LLDP configuration. */\nvoid\nofproto_port_set_lldp(struct ofproto *ofproto,\n                      ofp_port_t ofp_port,\n                      const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure LLDP on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n    error = (ofproto->ofproto_class->set_lldp\n             ? ofproto->ofproto_class->set_lldp(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: lldp configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\nint\nofproto_set_aa(struct ofproto *ofproto, void *aux OVS_UNUSED,\n               const struct aa_settings *s)\n{\n    if (!ofproto->ofproto_class->set_aa) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_aa(ofproto, s);\n    return 0;\n}\n\nint\nofproto_aa_mapping_register(struct ofproto *ofproto, void *aux,\n                            const struct aa_mapping_settings *s)\n{\n    if (!ofproto->ofproto_class->aa_mapping_set) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_mapping_set(ofproto, aux, s);\n    return 0;\n}\n\nint\nofproto_aa_mapping_unregister(struct ofproto *ofproto, void *aux)\n{\n    if (!ofproto->ofproto_class->aa_mapping_unset) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_mapping_unset(ofproto, aux);\n    return 0;\n}\n\nint\nofproto_aa_vlan_get_queued(struct ofproto *ofproto,\n                           struct ovs_list *list)\n{\n    if (!ofproto->ofproto_class->aa_vlan_get_queued) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_vlan_get_queued(ofproto, list);\n    return 0;\n}\n\nunsigned int\nofproto_aa_vlan_get_queue_size(struct ofproto *ofproto)\n{\n    if (!ofproto->ofproto_class->aa_vlan_get_queue_size) {\n        return EOPNOTSUPP;\n    }\n    return ofproto->ofproto_class->aa_vlan_get_queue_size(ofproto);\n}\n\n/* Connectivity Fault Management configuration. */\n\n/* Clears the CFM configuration from 'ofp_port' on 'ofproto'. */\nvoid\nofproto_port_clear_cfm(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (ofport && ofproto->ofproto_class->set_cfm) {\n        ofproto->ofproto_class->set_cfm(ofport, NULL);\n    }\n}\n\n/* Configures connectivity fault management on 'ofp_port' in 'ofproto'.  Takes\n * basic configuration from the configuration members in 'cfm', and the remote\n * maintenance point ID from  remote_mpid.  Ignores the statistics members of\n * 'cfm'.\n *\n * This function has no effect if 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_cfm(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct cfm_settings *s)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure CFM on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    /* XXX: For configuration simplicity, we only support one remote_mpid\n     * outside of the CFM module.  It's not clear if this is the correct long\n     * term solution or not. */\n    error = (ofproto->ofproto_class->set_cfm\n             ? ofproto->ofproto_class->set_cfm(ofport, s)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: CFM configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n/* Configures BFD on 'ofp_port' in 'ofproto'.  This function has no effect if\n * 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_bfd(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure bfd on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    error = (ofproto->ofproto_class->set_bfd\n             ? ofproto->ofproto_class->set_bfd(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: bfd configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n/* Checks the status change of BFD on 'ofport'.\n *\n * Returns true if 'ofproto_class' does not support 'bfd_status_changed'. */\nbool\nofproto_port_bfd_status_changed(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->bfd_status_changed\n            ? ofproto->ofproto_class->bfd_status_changed(ofport)\n            : true);\n}\n\n/* Populates 'status' with the status of BFD on 'ofport'.  Returns 0 on\n * success.  Returns a positive errno otherwise.  Has no effect if 'ofp_port'\n * is not an OpenFlow port in 'ofproto'.\n *\n * The caller must provide and own '*status'. */\nint\nofproto_port_get_bfd_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct smap *status)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->get_bfd_status\n            ? ofproto->ofproto_class->get_bfd_status(ofport, status)\n            : EOPNOTSUPP);\n}\n\n/* Checks the status of LACP negotiation for 'ofp_port' within ofproto.\n * Returns 1 if LACP partner information for 'ofp_port' is up-to-date,\n * 0 if LACP partner information is not current (generally indicating a\n * connectivity problem), or -1 if LACP is not enabled on 'ofp_port'. */\nint\nofproto_port_is_lacp_current(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->port_is_lacp_current\n            ? ofproto->ofproto_class->port_is_lacp_current(ofport)\n            : -1);\n}\n\nint\nofproto_port_get_lacp_stats(const struct ofport *port, struct lacp_slave_stats *stats)\n{\n    struct ofproto *ofproto = port->ofproto;\n    int error;\n\n    if (ofproto->ofproto_class->port_get_lacp_stats) {\n        error = ofproto->ofproto_class->port_get_lacp_stats(port, stats);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\f\n/* Bundles. */\n\n/* Registers a \"bundle\" associated with client data pointer 'aux' in 'ofproto'.\n * A bundle is the same concept as a Port in OVSDB, that is, it consists of one\n * or more \"slave\" devices (Interfaces, in OVSDB) along with a VLAN\n * configuration plus, if there is more than one slave, a bonding\n * configuration.\n *\n * If 'aux' is already registered then this function updates its configuration\n * to 's'.  Otherwise, this function registers a new bundle.\n *\n * Bundles only affect the NXAST_AUTOPATH action and output to the OFPP_NORMAL\n * port. */\nint\nofproto_bundle_register(struct ofproto *ofproto, void *aux,\n                        const struct ofproto_bundle_settings *s)\n{\n    return (ofproto->ofproto_class->bundle_set\n            ? ofproto->ofproto_class->bundle_set(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\n/* Unregisters the bundle registered on 'ofproto' with auxiliary data 'aux'.\n * If no such bundle has been registered, this has no effect. */\nint\nofproto_bundle_unregister(struct ofproto *ofproto, void *aux)\n{\n    return ofproto_bundle_register(ofproto, aux, NULL);\n}\n\n\f\n/* Registers a mirror associated with client data pointer 'aux' in 'ofproto'.\n * If 'aux' is already registered then this function updates its configuration\n * to 's'.  Otherwise, this function registers a new mirror. */\nint\nofproto_mirror_register(struct ofproto *ofproto, void *aux,\n                        const struct ofproto_mirror_settings *s)\n{\n    return (ofproto->ofproto_class->mirror_set\n            ? ofproto->ofproto_class->mirror_set(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\n/* Unregisters the mirror registered on 'ofproto' with auxiliary data 'aux'.\n * If no mirror has been registered, this has no effect. */\nint\nofproto_mirror_unregister(struct ofproto *ofproto, void *aux)\n{\n    return ofproto_mirror_register(ofproto, aux, NULL);\n}\n\n/* Retrieves statistics from mirror associated with client data pointer\n * 'aux' in 'ofproto'.  Stores packet and byte counts in 'packets' and\n * 'bytes', respectively.  If a particular counters is not supported,\n * the appropriate argument is set to UINT64_MAX.\n */\nint\nofproto_mirror_get_stats(struct ofproto *ofproto, void *aux,\n                         uint64_t *packets, uint64_t *bytes)\n{\n    if (!ofproto->ofproto_class->mirror_get_stats) {\n        *packets = *bytes = UINT64_MAX;\n        return EOPNOTSUPP;\n    }\n\n    return ofproto->ofproto_class->mirror_get_stats(ofproto, aux,\n                                                    packets, bytes);\n}\n\n/* Configures the VLANs whose bits are set to 1 in 'flood_vlans' as VLANs on\n * which all packets are flooded, instead of using MAC learning.  If\n * 'flood_vlans' is NULL, then MAC learning applies to all VLANs.\n *\n * Flood VLANs affect only the treatment of packets output to the OFPP_NORMAL\n * port. */\nint\nofproto_set_flood_vlans(struct ofproto *ofproto, unsigned long *flood_vlans)\n{\n    return (ofproto->ofproto_class->set_flood_vlans\n            ? ofproto->ofproto_class->set_flood_vlans(ofproto, flood_vlans)\n            : EOPNOTSUPP);\n}\n\n/* Returns true if 'aux' is a registered bundle that is currently in use as the\n * output for a mirror. */\nbool\nofproto_is_mirror_output_bundle(const struct ofproto *ofproto, void *aux)\n{\n    return (ofproto->ofproto_class->is_mirror_output_bundle\n            ? ofproto->ofproto_class->is_mirror_output_bundle(ofproto, aux)\n            : false);\n}\n\f\n/* Configuration of OpenFlow tables. */\n\n/* Returns the number of OpenFlow tables in 'ofproto'. */\nint\nofproto_get_n_tables(const struct ofproto *ofproto)\n{\n    return ofproto->n_tables;\n}\n\n/* Returns the number of Controller visible OpenFlow tables\n * in 'ofproto'. This number will exclude Hidden tables.\n * This funtion's return value should be less or equal to that of\n * ofproto_get_n_tables() . */\nuint8_t\nofproto_get_n_visible_tables(const struct ofproto *ofproto)\n{\n    uint8_t n = ofproto->n_tables;\n\n    /* Count only non-hidden tables in the number of tables.  (Hidden tables,\n     * if present, are always at the end.) */\n    while(n && (ofproto->tables[n - 1].flags & OFTABLE_HIDDEN)) {\n        n--;\n    }\n\n    return n;\n}\n\n/* Configures the OpenFlow table in 'ofproto' with id 'table_id' with the\n * settings from 's'.  'table_id' must be in the range 0 through the number of\n * OpenFlow tables in 'ofproto' minus 1, inclusive.\n *\n * For read-only tables, only the name may be configured. */\nvoid\nofproto_configure_table(struct ofproto *ofproto, int table_id,\n                        const struct ofproto_table_settings *s)\n{\n    struct oftable *table;\n\n    ovs_assert(table_id >= 0 && table_id < ofproto->n_tables);\n    table = &ofproto->tables[table_id];\n\n    oftable_set_name(table, s->name);\n\n    if (table->flags & OFTABLE_READONLY) {\n        return;\n    }\n\n    if (classifier_set_prefix_fields(&table->cls,\n                                     s->prefix_fields, s->n_prefix_fields)) {\n        /* XXX: Trigger revalidation. */\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    unsigned int new_eviction = (s->enable_eviction\n                                 ? table->eviction | EVICTION_CLIENT\n                                 : table->eviction & ~EVICTION_CLIENT);\n    oftable_configure_eviction(table, new_eviction, s->groups, s->n_groups);\n    table->max_flows = s->max_flows;\n    evict_rules_from_table(table);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\f\nbool\nofproto_has_snoops(const struct ofproto *ofproto)\n{\n    return connmgr_has_snoops(ofproto->connmgr);\n}\n\nvoid\nofproto_get_snoops(const struct ofproto *ofproto, struct sset *snoops)\n{\n    connmgr_get_snoops(ofproto->connmgr, snoops);\n}\n\n/* Deletes 'rule' from 'ofproto'.\n *\n * Within an ofproto implementation, this function allows an ofproto\n * implementation to destroy any rules that remain when its ->destruct()\n * function is called.  This function is not suitable for use elsewhere in an\n * ofproto implementation.\n *\n * This function implements steps 4.4 and 4.5 in the section titled \"Rule Life\n * Cycle\" in ofproto-provider.h. */\nvoid\nofproto_rule_delete(struct ofproto *ofproto, struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    /* This skips the ofmonitor and flow-removed notifications because the\n     * switch is being deleted and any OpenFlow channels have been or soon will\n     * be killed. */\n    ovs_mutex_lock(&ofproto_mutex);\n\n    if (rule->state == RULE_INSERTED) {\n        /* Make sure there is no postponed removal of the rule. */\n        ovs_assert(cls_rule_visible_in_version(&rule->cr, OVS_VERSION_MAX));\n\n        if (!classifier_remove(&rule->ofproto->tables[rule->table_id].cls,\n                               &rule->cr)) {\n            OVS_NOT_REACHED();\n        }\n        ofproto_rule_remove__(rule->ofproto, rule);\n        if (ofproto->ofproto_class->rule_delete) {\n            ofproto->ofproto_class->rule_delete(rule);\n        }\n\n        /* This may not be the last reference to the rule. */\n        ofproto_rule_unref(rule);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic void\nofproto_flush__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n\n    /* This will flush all datapath flows. */\n    if (ofproto->ofproto_class->flush) {\n        ofproto->ofproto_class->flush(ofproto);\n    }\n\n    /* XXX: There is a small race window here, where new datapath flows can be\n     * created by upcall handlers based on the existing flow table.  We can not\n     * call ofproto class flush while holding 'ofproto_mutex' to prevent this,\n     * as then we could deadlock on syncing with the handler threads waiting on\n     * the same mutex. */\n\n    ovs_mutex_lock(&ofproto_mutex);\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        struct rule_collection rules;\n        struct rule *rule;\n\n        if (table->flags & OFTABLE_HIDDEN) {\n            continue;\n        }\n\n        rule_collection_init(&rules);\n\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            rule_collection_add(&rules, rule);\n        }\n        delete_flows__(&rules, OFPRR_DELETE, NULL);\n    }\n    /* XXX: Concurrent handler threads may insert new learned flows based on\n     * learn actions of the now deleted flows right after we release\n     * 'ofproto_mutex'. */\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic void\nofproto_destroy__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n\n    cmap_destroy(&ofproto->groups);\n\n    hmap_remove(&all_ofprotos, &ofproto->hmap_node);\n\n    free(ofproto->name);\n    free(ofproto->type);\n    free(ofproto->mfr_desc);\n    free(ofproto->hw_desc);\n    free(ofproto->sw_desc);\n    free(ofproto->serial_desc);\n    free(ofproto->dp_desc);\n    hmap_destroy(&ofproto->ports);\n    hmap_destroy(&ofproto->ofport_usage);\n    shash_destroy(&ofproto->port_by_name);\n    simap_destroy(&ofproto->ofp_requests);\n\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        oftable_destroy(table);\n    }\n    free(ofproto->tables);\n\n    ovs_mutex_lock(&ofproto->vl_mff_map.mutex);\n    mf_vl_mff_map_clear(&ofproto->vl_mff_map, true);\n    ovs_mutex_unlock(&ofproto->vl_mff_map.mutex);\n    cmap_destroy(&ofproto->vl_mff_map.cmap);\n    ovs_mutex_destroy(&ofproto->vl_mff_map.mutex);\n    tun_metadata_free(ovsrcu_get_protected(struct tun_table *,\n                                           &ofproto->metadata_tab));\n\n    ovs_assert(hindex_is_empty(&ofproto->cookies));\n    hindex_destroy(&ofproto->cookies);\n\n    ovs_assert(hmap_is_empty(&ofproto->learned_cookies));\n    hmap_destroy(&ofproto->learned_cookies);\n\n    ofproto->ofproto_class->dealloc(ofproto);\n}\n\n/* Destroying rules is doubly deferred, must have 'ofproto' around for them.\n * - 1st we defer the removal of the rules from the classifier\n * - 2nd we defer the actual destruction of the rules. */\nstatic void\nofproto_destroy_defer__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovsrcu_postpone(ofproto_destroy__, ofproto);\n}\n\nvoid\nofproto_destroy(struct ofproto *p, bool del)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofport *ofport, *next_ofport;\n    struct ofport_usage *usage;\n\n    if (!p) {\n        return;\n    }\n\n    if (p->meters) {\n        meter_delete(p, 1, p->meter_features.max_meters);\n        p->meter_features.max_meters = 0;\n        free(p->meters);\n        p->meters = NULL;\n    }\n\n    ofproto_flush__(p);\n    HMAP_FOR_EACH_SAFE (ofport, next_ofport, hmap_node, &p->ports) {\n        ofport_destroy(ofport, del);\n    }\n\n    HMAP_FOR_EACH_POP (usage, hmap_node, &p->ofport_usage) {\n        free(usage);\n    }\n\n    p->ofproto_class->destruct(p, del);\n\n    /* We should not postpone this because it involves deleting a listening\n     * socket which we may want to reopen soon. 'connmgr' may be used by other\n     * threads only if they take the ofproto_mutex and read a non-NULL\n     * 'ofproto->connmgr'. */\n    ovs_mutex_lock(&ofproto_mutex);\n    connmgr_destroy(p->connmgr);\n    p->connmgr = NULL;\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    /* Destroying rules is deferred, must have 'ofproto' around for them. */\n    ovsrcu_postpone(ofproto_destroy_defer__, p);\n}\n\n/* Destroys the datapath with the respective 'name' and 'type'.  With the Linux\n * kernel datapath, for example, this destroys the datapath in the kernel, and\n * with the netdev-based datapath, it tears down the data structures that\n * represent the datapath.\n *\n * The datapath should not be currently open as an ofproto. */\nint\nofproto_delete(const char *name, const char *type)\n{\n    const struct ofproto_class *class = ofproto_class_find__(type);\n    return (!class ? EAFNOSUPPORT\n            : !class->del ? EACCES\n            : class->del(type, name));\n}\n\nstatic void\nprocess_port_change(struct ofproto *ofproto, int error, char *devname)\n{\n    if (error == ENOBUFS) {\n        reinit_ports(ofproto);\n    } else if (!error) {\n        update_port(ofproto, devname);\n        free(devname);\n    }\n}\n\nint\nofproto_type_run(const char *datapath_type)\n{\n    const struct ofproto_class *class;\n    int error;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    error = class->type_run ? class->type_run(datapath_type) : 0;\n    if (error && error != EAGAIN) {\n        VLOG_ERR_RL(&rl, \"%s: type_run failed (%s)\",\n                    datapath_type, ovs_strerror(error));\n    }\n    return error;\n}\n\nvoid\nofproto_type_wait(const char *datapath_type)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    if (class->type_wait) {\n        class->type_wait(datapath_type);\n    }\n}\n\nint\nofproto_run(struct ofproto *p)\n{\n    int error;\n    uint64_t new_seq;\n\n    error = p->ofproto_class->run(p);\n    if (error && error != EAGAIN) {\n        VLOG_ERR_RL(&rl, \"%s: run failed (%s)\", p->name, ovs_strerror(error));\n    }\n\n    /* Restore the eviction group heap invariant occasionally. */\n    if (p->eviction_group_timer < time_msec()) {\n        size_t i;\n\n        p->eviction_group_timer = time_msec() + 1000;\n\n        for (i = 0; i < p->n_tables; i++) {\n            struct oftable *table = &p->tables[i];\n            struct eviction_group *evg;\n            struct rule *rule;\n\n            if (!table->eviction) {\n                continue;\n            }\n\n            if (table->n_flows > 100000) {\n                static struct vlog_rate_limit count_rl =\n                    VLOG_RATE_LIMIT_INIT(1, 1);\n                VLOG_WARN_RL(&count_rl, \"Table %\"PRIuSIZE\" has an excessive\"\n                             \" number of rules: %d\", i, table->n_flows);\n            }\n\n            ovs_mutex_lock(&ofproto_mutex);\n            CLS_FOR_EACH (rule, cr, &table->cls) {\n                if (rule->idle_timeout || rule->hard_timeout) {\n                    if (!rule->eviction_group) {\n                        eviction_group_add_rule(rule);\n                    } else {\n                        heap_raw_change(&rule->evg_node,\n                                        rule_eviction_priority(p, rule));\n                    }\n                }\n            }\n\n            HEAP_FOR_EACH (evg, size_node, &table->eviction_groups_by_size) {\n                heap_rebuild(&evg->rules);\n            }\n            ovs_mutex_unlock(&ofproto_mutex);\n        }\n    }\n\n    if (p->ofproto_class->port_poll) {\n        char *devname;\n\n        while ((error = p->ofproto_class->port_poll(p, &devname)) != EAGAIN) {\n            process_port_change(p, error, devname);\n        }\n    }\n\n    new_seq = seq_read(connectivity_seq_get());\n    if (new_seq != p->change_seq) {\n        struct sset devnames;\n        const char *devname;\n        struct ofport *ofport;\n\n        /* Update OpenFlow port status for any port whose netdev has changed.\n         *\n         * Refreshing a given 'ofport' can cause an arbitrary ofport to be\n         * destroyed, so it's not safe to update ports directly from the\n         * HMAP_FOR_EACH loop, or even to use HMAP_FOR_EACH_SAFE.  Instead, we\n         * need this two-phase approach. */\n        sset_init(&devnames);\n        HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n            uint64_t port_change_seq;\n\n            port_change_seq = netdev_get_change_seq(ofport->netdev);\n            if (ofport->change_seq != port_change_seq) {\n                ofport->change_seq = port_change_seq;\n                sset_add(&devnames, netdev_get_name(ofport->netdev));\n            }\n        }\n        SSET_FOR_EACH (devname, &devnames) {\n            update_port(p, devname);\n        }\n        sset_destroy(&devnames);\n\n        p->change_seq = new_seq;\n    }\n\n    connmgr_run(p->connmgr, handle_openflow);\n\n    return error;\n}\n\nvoid\nofproto_wait(struct ofproto *p)\n{\n    p->ofproto_class->wait(p);\n    if (p->ofproto_class->port_poll_wait) {\n        p->ofproto_class->port_poll_wait(p);\n    }\n    seq_wait(connectivity_seq_get(), p->change_seq);\n    connmgr_wait(p->connmgr);\n}\n\nbool\nofproto_is_alive(const struct ofproto *p)\n{\n    return connmgr_has_controllers(p->connmgr);\n}\n\n/* Adds some memory usage statistics for 'ofproto' into 'usage', for use with\n * memory_report(). */\nvoid\nofproto_get_memory_usage(const struct ofproto *ofproto, struct simap *usage)\n{\n    const struct oftable *table;\n    unsigned int n_rules;\n\n    simap_increase(usage, \"ports\", hmap_count(&ofproto->ports));\n\n    n_rules = 0;\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        n_rules += table->n_flows;\n    }\n    simap_increase(usage, \"rules\", n_rules);\n\n    if (ofproto->ofproto_class->get_memory_usage) {\n        ofproto->ofproto_class->get_memory_usage(ofproto, usage);\n    }\n\n    connmgr_get_memory_usage(ofproto->connmgr, usage);\n}\n\nvoid\nofproto_type_get_memory_usage(const char *datapath_type, struct simap *usage)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    if (class && class->type_get_memory_usage) {\n        class->type_get_memory_usage(datapath_type, usage);\n    }\n}\n\nvoid\nofproto_get_ofproto_controller_info(const struct ofproto *ofproto,\n                                    struct shash *info)\n{\n    connmgr_get_controller_info(ofproto->connmgr, info);\n}\n\nvoid\nofproto_free_ofproto_controller_info(struct shash *info)\n{\n    connmgr_free_controller_info(info);\n}\n\n/* Makes a deep copy of 'old' into 'port'. */\nvoid\nofproto_port_clone(struct ofproto_port *port, const struct ofproto_port *old)\n{\n    port->name = xstrdup(old->name);\n    port->type = xstrdup(old->type);\n    port->ofp_port = old->ofp_port;\n}\n\n/* Frees memory allocated to members of 'ofproto_port'.\n *\n * Do not call this function on an ofproto_port obtained from\n * ofproto_port_dump_next(): that function retains ownership of the data in the\n * ofproto_port. */\nvoid\nofproto_port_destroy(struct ofproto_port *ofproto_port)\n{\n    free(ofproto_port->name);\n    free(ofproto_port->type);\n}\n\n/* Initializes 'dump' to begin dumping the ports in an ofproto.\n *\n * This function provides no status indication.  An error status for the entire\n * dump operation is provided when it is completed by calling\n * ofproto_port_dump_done().\n */\nvoid\nofproto_port_dump_start(struct ofproto_port_dump *dump,\n                        const struct ofproto *ofproto)\n{\n    dump->ofproto = ofproto;\n    dump->error = ofproto->ofproto_class->port_dump_start(ofproto,\n                                                          &dump->state);\n}\n\n/* Attempts to retrieve another port from 'dump', which must have been created\n * with ofproto_port_dump_start().  On success, stores a new ofproto_port into\n * 'port' and returns true.  On failure, returns false.\n *\n * Failure might indicate an actual error or merely that the last port has been\n * dumped.  An error status for the entire dump operation is provided when it\n * is completed by calling ofproto_port_dump_done().\n *\n * The ofproto owns the data stored in 'port'.  It will remain valid until at\n * least the next time 'dump' is passed to ofproto_port_dump_next() or\n * ofproto_port_dump_done(). */\nbool\nofproto_port_dump_next(struct ofproto_port_dump *dump,\n                       struct ofproto_port *port)\n{\n    const struct ofproto *ofproto = dump->ofproto;\n\n    if (dump->error) {\n        return false;\n    }\n\n    dump->error = ofproto->ofproto_class->port_dump_next(ofproto, dump->state,\n                                                         port);\n    if (dump->error) {\n        ofproto->ofproto_class->port_dump_done(ofproto, dump->state);\n        return false;\n    }\n    return true;\n}\n\n/* Completes port table dump operation 'dump', which must have been created\n * with ofproto_port_dump_start().  Returns 0 if the dump operation was\n * error-free, otherwise a positive errno value describing the problem. */\nint\nofproto_port_dump_done(struct ofproto_port_dump *dump)\n{\n    const struct ofproto *ofproto = dump->ofproto;\n    if (!dump->error) {\n        dump->error = ofproto->ofproto_class->port_dump_done(ofproto,\n                                                             dump->state);\n    }\n    return dump->error == EOF ? 0 : dump->error;\n}\n\n/* Returns the type to pass to netdev_open() when a datapath of type\n * 'datapath_type' has a port of type 'port_type', for a few special\n * cases when a netdev type differs from a port type.  For example, when\n * using the userspace datapath, a port of type \"internal\" needs to be\n * opened as \"tap\".\n *\n * Returns either 'type' itself or a string literal, which must not be\n * freed. */\nconst char *\nofproto_port_open_type(const char *datapath_type, const char *port_type)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n    if (!class) {\n        return port_type;\n    }\n\n    return (class->port_open_type\n            ? class->port_open_type(datapath_type, port_type)\n            : port_type);\n}\n\n/* Attempts to add 'netdev' as a port on 'ofproto'.  If 'ofp_portp' is\n * non-null and '*ofp_portp' is not OFPP_NONE, attempts to use that as\n * the port's OpenFlow port number.\n *\n * If successful, returns 0 and sets '*ofp_portp' to the new port's\n * OpenFlow port number (if 'ofp_portp' is non-null).  On failure,\n * returns a positive errno value and sets '*ofp_portp' to OFPP_NONE (if\n * 'ofp_portp' is non-null). */\nint\nofproto_port_add(struct ofproto *ofproto, struct netdev *netdev,\n                 ofp_port_t *ofp_portp)\n{\n    ofp_port_t ofp_port = ofp_portp ? *ofp_portp : OFPP_NONE;\n    int error;\n\n    error = ofproto->ofproto_class->port_add(ofproto, netdev);\n    if (!error) {\n        const char *netdev_name = netdev_get_name(netdev);\n\n        simap_put(&ofproto->ofp_requests, netdev_name,\n                  ofp_to_u16(ofp_port));\n        error = update_port(ofproto, netdev_name);\n    }\n    if (ofp_portp) {\n        *ofp_portp = OFPP_NONE;\n        if (!error) {\n            struct ofproto_port ofproto_port;\n\n            error = ofproto_port_query_by_name(ofproto,\n                                               netdev_get_name(netdev),\n                                               &ofproto_port);\n            if (!error) {\n                *ofp_portp = ofproto_port.ofp_port;\n                ofproto_port_destroy(&ofproto_port);\n            }\n        }\n    }\n    return error;\n}\n\n/* Looks up a port named 'devname' in 'ofproto'.  On success, returns 0 and\n * initializes '*port' appropriately; on failure, returns a positive errno\n * value.\n *\n * The caller owns the data in 'ofproto_port' and must free it with\n * ofproto_port_destroy() when it is no longer needed. */\nint\nofproto_port_query_by_name(const struct ofproto *ofproto, const char *devname,\n                           struct ofproto_port *port)\n{\n    int error;\n\n    error = ofproto->ofproto_class->port_query_by_name(ofproto, devname, port);\n    if (error) {\n        memset(port, 0, sizeof *port);\n    }\n    return error;\n}\n\n/* Deletes port number 'ofp_port' from the datapath for 'ofproto'.\n * Returns 0 if successful, otherwise a positive errno. */\nint\nofproto_port_del(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    const char *name = ofport ? netdev_get_name(ofport->netdev) : \"<unknown>\";\n    struct simap_node *ofp_request_node;\n    int error;\n\n    ofp_request_node = simap_find(&ofproto->ofp_requests, name);\n    if (ofp_request_node) {\n        simap_delete(&ofproto->ofp_requests, ofp_request_node);\n    }\n\n    error = ofproto->ofproto_class->port_del(ofproto, ofp_port);\n    if (!error && ofport) {\n        /* 'name' is the netdev's name and update_port() is going to close the\n         * netdev.  Just in case update_port() refers to 'name' after it\n         * destroys 'ofport', make a copy of it around the update_port()\n         * call. */\n        char *devname = xstrdup(name);\n        update_port(ofproto, devname);\n        free(devname);\n    }\n    return error;\n}\n\n/* Refreshes datapath configuration of port number 'ofp_port' in 'ofproto'.\n *\n * This function has no effect if 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_config(struct ofproto *ofproto, ofp_port_t ofp_port,\n                        const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure datapath on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    error = (ofproto->ofproto_class->port_set_config\n             ? ofproto->ofproto_class->port_set_config(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: datapath configuration on port %\"PRIu32\n                  \" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n\nstatic void\nflow_mod_init(struct ofputil_flow_mod *fm,\n              const struct match *match, int priority,\n              const struct ofpact *ofpacts, size_t ofpacts_len,\n              enum ofp_flow_mod_command command)\n{\n    *fm = (struct ofputil_flow_mod) {\n        .match = *match,\n        .priority = priority,\n        .table_id = 0,\n        .command = command,\n        .buffer_id = UINT32_MAX,\n        .out_port = OFPP_ANY,\n        .out_group = OFPG_ANY,\n        .ofpacts = CONST_CAST(struct ofpact *, ofpacts),\n        .ofpacts_len = ofpacts_len,\n    };\n}\n\nstatic int\nsimple_flow_mod(struct ofproto *ofproto,\n                const struct match *match, int priority,\n                const struct ofpact *ofpacts, size_t ofpacts_len,\n                enum ofp_flow_mod_command command)\n{\n    struct ofputil_flow_mod fm;\n\n    flow_mod_init(&fm, match, priority, ofpacts, ofpacts_len, command);\n\n    return handle_flow_mod__(ofproto, &fm, NULL);\n}\n\n/* Adds a flow to OpenFlow flow table 0 in 'p' that matches 'cls_rule' and\n * performs the 'n_actions' actions in 'actions'.  The new flow will not\n * timeout.\n *\n * If cls_rule->priority is in the range of priorities supported by OpenFlow\n * (0...65535, inclusive) then the flow will be visible to OpenFlow\n * controllers; otherwise, it will be hidden.\n *\n * The caller retains ownership of 'cls_rule' and 'ofpacts'.\n *\n * This is a helper function for in-band control and fail-open. */\nvoid\nofproto_add_flow(struct ofproto *ofproto, const struct match *match,\n                 int priority,\n                 const struct ofpact *ofpacts, size_t ofpacts_len)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct rule *rule;\n    bool must_add;\n\n    /* First do a cheap check whether the rule we're looking for already exists\n     * with the actions that we want.  If it does, then we're done. */\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  &ofproto->tables[0].cls, match, priority,\n                                  OVS_VERSION_MAX));\n    if (rule) {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        must_add = !ofpacts_equal(actions->ofpacts, actions->ofpacts_len,\n                                  ofpacts, ofpacts_len);\n    } else {\n        must_add = true;\n    }\n\n    /* If there's no such rule or the rule doesn't have the actions we want,\n     * fall back to a executing a full flow mod.  We can't optimize this at\n     * all because we didn't take enough locks above to ensure that the flow\n     * table didn't already change beneath us.  */\n    if (must_add) {\n        simple_flow_mod(ofproto, match, priority, ofpacts, ofpacts_len,\n                        OFPFC_MODIFY_STRICT);\n    }\n}\n\n/* Executes the flow modification specified in 'fm'.  Returns 0 on success, or\n * an OFPERR_* OpenFlow error code on failure.\n *\n * This is a helper function for in-band control and fail-open. */\nenum ofperr\nofproto_flow_mod(struct ofproto *ofproto, const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    return handle_flow_mod__(ofproto, fm, NULL);\n}\n\n/* Searches for a rule with matching criteria exactly equal to 'target' in\n * ofproto's table 0 and, if it finds one, deletes it.\n *\n * This is a helper function for in-band control and fail-open. */\nvoid\nofproto_delete_flow(struct ofproto *ofproto,\n                    const struct match *target, int priority)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct classifier *cls = &ofproto->tables[0].cls;\n    struct rule *rule;\n\n    /* First do a cheap check whether the rule we're looking for has already\n     * been deleted.  If so, then we're done. */\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  cls, target, priority, OVS_VERSION_MAX));\n    if (!rule) {\n        return;\n    }\n\n    struct rule_collection rules;\n\n    rule_collection_init(&rules);\n    rule_collection_add(&rules, rule);\n    delete_flows__(&rules, OFPRR_DELETE, NULL);\n    rule_collection_destroy(&rules);\n}\n\n/* Delete all of the flows from all of ofproto's flow tables, then reintroduce\n * the flows required by in-band control and fail-open.  */\nvoid\nofproto_flush_flows(struct ofproto *ofproto)\n{\n    COVERAGE_INC(ofproto_flush);\n    ofproto_flush__(ofproto);\n    connmgr_flushed(ofproto->connmgr);\n}\n\f\nstatic void\nreinit_ports(struct ofproto *p)\n{\n    struct ofproto_port_dump dump;\n    struct sset devnames;\n    struct ofport *ofport;\n    struct ofproto_port ofproto_port;\n    const char *devname;\n\n    COVERAGE_INC(ofproto_reinit_ports);\n\n    sset_init(&devnames);\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        sset_add(&devnames, netdev_get_name(ofport->netdev));\n    }\n    OFPROTO_PORT_FOR_EACH (&ofproto_port, &dump, p) {\n        sset_add(&devnames, ofproto_port.name);\n    }\n\n    SSET_FOR_EACH (devname, &devnames) {\n        update_port(p, devname);\n    }\n    sset_destroy(&devnames);\n}\n\nstatic ofp_port_t\nalloc_ofp_port(struct ofproto *ofproto, const char *netdev_name)\n{\n    uint16_t port_idx;\n\n    port_idx = simap_get(&ofproto->ofp_requests, netdev_name);\n    port_idx = port_idx ? port_idx : UINT16_MAX;\n\n    if (port_idx >= ofproto->max_ports\n        || ofport_get_usage(ofproto, u16_to_ofp(port_idx)) == LLONG_MAX) {\n        uint16_t lru_ofport = 0, end_port_no = ofproto->alloc_port_no;\n        long long int last_used_at, lru = LLONG_MAX;\n\n        /* Search for a free OpenFlow port number.  We try not to\n         * immediately reuse them to prevent problems due to old\n         * flows.\n         *\n         * We limit the automatically assigned port numbers to the lower half\n         * of the port range, to reserve the upper half for assignment by\n         * controllers. */\n        for (;;) {\n            if (++ofproto->alloc_port_no >= MIN(ofproto->max_ports, 32768)) {\n                ofproto->alloc_port_no = 1;\n            }\n            last_used_at = ofport_get_usage(ofproto,\n                                         u16_to_ofp(ofproto->alloc_port_no));\n            if (!last_used_at) {\n                port_idx = ofproto->alloc_port_no;\n                break;\n            } else if ( last_used_at < time_msec() - 60*60*1000) {\n                /* If the port with ofport 'ofproto->alloc_port_no' was deleted\n                 * more than an hour ago, consider it usable. */\n                ofport_remove_usage(ofproto,\n                    u16_to_ofp(ofproto->alloc_port_no));\n                port_idx = ofproto->alloc_port_no;\n                break;\n            } else if (last_used_at < lru) {\n                lru = last_used_at;\n                lru_ofport = ofproto->alloc_port_no;\n            }\n\n            if (ofproto->alloc_port_no == end_port_no) {\n                if (lru_ofport) {\n                    port_idx = lru_ofport;\n                    break;\n                }\n                return OFPP_NONE;\n            }\n        }\n    }\n    ofport_set_usage(ofproto, u16_to_ofp(port_idx), LLONG_MAX);\n    return u16_to_ofp(port_idx);\n}\n\nstatic void\ndealloc_ofp_port(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    if (ofp_to_u16(ofp_port) < ofproto->max_ports) {\n        ofport_set_usage(ofproto, ofp_port, time_msec());\n    }\n}\n\n/* Opens and returns a netdev for 'ofproto_port' in 'ofproto', or a null\n * pointer if the netdev cannot be opened.  On success, also fills in\n * '*pp'.  */\nstatic struct netdev *\nofport_open(struct ofproto *ofproto,\n            struct ofproto_port *ofproto_port,\n            struct ofputil_phy_port *pp)\n{\n    enum netdev_flags flags;\n    struct netdev *netdev;\n    int error;\n\n    error = netdev_open(ofproto_port->name, ofproto_port->type, &netdev);\n    if (error) {\n        VLOG_WARN_RL(&rl, \"%s: ignoring port %s (%\"PRIu32\") because netdev %s \"\n                     \"cannot be opened (%s)\",\n                     ofproto->name,\n                     ofproto_port->name, ofproto_port->ofp_port,\n                     ofproto_port->name, ovs_strerror(error));\n        return NULL;\n    }\n\n    if (ofproto_port->ofp_port == OFPP_NONE) {\n        if (!strcmp(ofproto->name, ofproto_port->name)) {\n            ofproto_port->ofp_port = OFPP_LOCAL;\n        } else {\n            ofproto_port->ofp_port = alloc_ofp_port(ofproto,\n                                                    ofproto_port->name);\n        }\n    }\n    pp->port_no = ofproto_port->ofp_port;\n    netdev_get_etheraddr(netdev, &pp->hw_addr);\n    ovs_strlcpy(pp->name, ofproto_port->name, sizeof pp->name);\n    netdev_get_flags(netdev, &flags);\n    pp->config = flags & NETDEV_UP ? 0 : OFPUTIL_PC_PORT_DOWN;\n    pp->state = netdev_get_carrier(netdev) ? 0 : OFPUTIL_PS_LINK_DOWN;\n    netdev_get_features(netdev, &pp->curr, &pp->advertised,\n                        &pp->supported, &pp->peer);\n    pp->curr_speed = netdev_features_to_bps(pp->curr, 0) / 1000;\n    pp->max_speed = netdev_features_to_bps(pp->supported, 0) / 1000;\n\n    return netdev;\n}\n\n/* Returns true if most fields of 'a' and 'b' are equal.  Differences in name,\n * port number, and 'config' bits other than OFPUTIL_PC_PORT_DOWN are\n * disregarded. */\nstatic bool\nofport_equal(const struct ofputil_phy_port *a,\n             const struct ofputil_phy_port *b)\n{\n    return (eth_addr_equals(a->hw_addr, b->hw_addr)\n            && a->state == b->state\n            && !((a->config ^ b->config) & OFPUTIL_PC_PORT_DOWN)\n            && a->curr == b->curr\n            && a->advertised == b->advertised\n            && a->supported == b->supported\n            && a->peer == b->peer\n            && a->curr_speed == b->curr_speed\n            && a->max_speed == b->max_speed);\n}\n\n/* Adds an ofport to 'p' initialized based on the given 'netdev' and 'opp'.\n * The caller must ensure that 'p' does not have a conflicting ofport (that is,\n * one with the same name or port number). */\nstatic int\nofport_install(struct ofproto *p,\n               struct netdev *netdev, const struct ofputil_phy_port *pp)\n{\n    const char *netdev_name = netdev_get_name(netdev);\n    struct ofport *ofport;\n    int error;\n\n    /* Create ofport. */\n    ofport = p->ofproto_class->port_alloc();\n    if (!ofport) {\n        error = ENOMEM;\n        goto error;\n    }\n    ofport->ofproto = p;\n    ofport->netdev = netdev;\n    ofport->change_seq = netdev_get_change_seq(netdev);\n    ofport->pp = *pp;\n    ofport->ofp_port = pp->port_no;\n    ofport->created = time_msec();\n\n    /* Add port to 'p'. */\n    hmap_insert(&p->ports, &ofport->hmap_node,\n                hash_ofp_port(ofport->ofp_port));\n    shash_add(&p->port_by_name, netdev_name, ofport);\n\n    update_mtu(p, ofport);\n\n    /* Let the ofproto_class initialize its private data. */\n    error = p->ofproto_class->port_construct(ofport);\n    if (error) {\n        goto error;\n    }\n    connmgr_send_port_status(p->connmgr, NULL, pp, OFPPR_ADD);\n    return 0;\n\nerror:\n    VLOG_WARN_RL(&rl, \"%s: could not add port %s (%s)\",\n                 p->name, netdev_name, ovs_strerror(error));\n    if (ofport) {\n        ofport_destroy__(ofport);\n    } else {\n        netdev_close(netdev);\n    }\n    return error;\n}\n\n/* Removes 'ofport' from 'p' and destroys it. */\nstatic void\nofport_remove(struct ofport *ofport)\n{\n    struct ofproto *p = ofport->ofproto;\n    bool is_mtu_overridden = ofport_is_mtu_overridden(p, ofport);\n\n    connmgr_send_port_status(ofport->ofproto->connmgr, NULL, &ofport->pp,\n                             OFPPR_DELETE);\n    ofport_destroy(ofport, true);\n    if (!is_mtu_overridden) {\n        update_mtu_ofproto(p);\n    }\n}\n\n/* If 'ofproto' contains an ofport named 'name', removes it from 'ofproto' and\n * destroys it. */\nstatic void\nofport_remove_with_name(struct ofproto *ofproto, const char *name)\n{\n    struct ofport *port = shash_find_data(&ofproto->port_by_name, name);\n    if (port) {\n        ofport_remove(port);\n    }\n}\n\n/* Updates 'port' with new 'pp' description.\n *\n * Does not handle a name or port number change.  The caller must implement\n * such a change as a delete followed by an add.  */\nstatic void\nofport_modified(struct ofport *port, struct ofputil_phy_port *pp)\n{\n    port->pp.hw_addr = pp->hw_addr;\n    port->pp.config = ((port->pp.config & ~OFPUTIL_PC_PORT_DOWN)\n                        | (pp->config & OFPUTIL_PC_PORT_DOWN));\n    port->pp.state = ((port->pp.state & ~OFPUTIL_PS_LINK_DOWN)\n                      | (pp->state & OFPUTIL_PS_LINK_DOWN));\n    port->pp.curr = pp->curr;\n    port->pp.advertised = pp->advertised;\n    port->pp.supported = pp->supported;\n    port->pp.peer = pp->peer;\n    port->pp.curr_speed = pp->curr_speed;\n    port->pp.max_speed = pp->max_speed;\n\n    connmgr_send_port_status(port->ofproto->connmgr, NULL,\n                             &port->pp, OFPPR_MODIFY);\n}\n\n/* Update OpenFlow 'state' in 'port' and notify controller. */\nvoid\nofproto_port_set_state(struct ofport *port, enum ofputil_port_state state)\n{\n    if (port->pp.state != state) {\n        port->pp.state = state;\n        connmgr_send_port_status(port->ofproto->connmgr, NULL,\n                                 &port->pp, OFPPR_MODIFY);\n    }\n}\n\nvoid\nofproto_port_unregister(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *port = ofproto_get_port(ofproto, ofp_port);\n    if (port) {\n        if (port->ofproto->ofproto_class->set_stp_port) {\n            port->ofproto->ofproto_class->set_stp_port(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->set_rstp_port) {\n            port->ofproto->ofproto_class->set_rstp_port(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->set_cfm) {\n            port->ofproto->ofproto_class->set_cfm(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->bundle_remove) {\n            port->ofproto->ofproto_class->bundle_remove(port);\n        }\n    }\n}\n\nstatic void\nofport_destroy__(struct ofport *port)\n{\n    struct ofproto *ofproto = port->ofproto;\n    const char *name = netdev_get_name(port->netdev);\n\n    hmap_remove(&ofproto->ports, &port->hmap_node);\n    shash_delete(&ofproto->port_by_name,\n                 shash_find(&ofproto->port_by_name, name));\n\n    netdev_close(port->netdev);\n    ofproto->ofproto_class->port_dealloc(port);\n}\n\nstatic void\nofport_destroy(struct ofport *port, bool del)\n{\n    if (port) {\n        dealloc_ofp_port(port->ofproto, port->ofp_port);\n        port->ofproto->ofproto_class->port_destruct(port, del);\n        ofport_destroy__(port);\n     }\n}\n\nstruct ofport *\nofproto_get_port(const struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *port;\n\n    HMAP_FOR_EACH_IN_BUCKET (port, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ports) {\n        if (port->ofp_port == ofp_port) {\n            return port;\n        }\n    }\n    return NULL;\n}\n\nstatic long long int\nofport_get_usage(const struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport_usage *usage;\n\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            return usage->last_used;\n        }\n    }\n    return 0;\n}\n\nstatic void\nofport_set_usage(struct ofproto *ofproto, ofp_port_t ofp_port,\n                 long long int last_used)\n{\n    struct ofport_usage *usage;\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            usage->last_used = last_used;\n            return;\n        }\n    }\n    ovs_assert(last_used == LLONG_MAX);\n\n    usage = xmalloc(sizeof *usage);\n    usage->ofp_port = ofp_port;\n    usage->last_used = last_used;\n    hmap_insert(&ofproto->ofport_usage, &usage->hmap_node,\n                hash_ofp_port(ofp_port));\n}\n\nstatic void\nofport_remove_usage(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport_usage *usage;\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            hmap_remove(&ofproto->ofport_usage, &usage->hmap_node);\n            free(usage);\n            break;\n        }\n    }\n}\n\nint\nofproto_port_get_stats(const struct ofport *port, struct netdev_stats *stats)\n{\n    struct ofproto *ofproto = port->ofproto;\n    int error;\n\n    if (ofproto->ofproto_class->port_get_stats) {\n        error = ofproto->ofproto_class->port_get_stats(port, stats);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\nstatic int\nupdate_port(struct ofproto *ofproto, const char *name)\n{\n    struct ofproto_port ofproto_port;\n    struct ofputil_phy_port pp;\n    struct netdev *netdev;\n    struct ofport *port;\n    int error = 0;\n\n    COVERAGE_INC(ofproto_update_port);\n\n    /* Fetch 'name''s location and properties from the datapath. */\n    netdev = (!ofproto_port_query_by_name(ofproto, name, &ofproto_port)\n              ? ofport_open(ofproto, &ofproto_port, &pp)\n              : NULL);\n\n    if (netdev) {\n        port = ofproto_get_port(ofproto, ofproto_port.ofp_port);\n        if (port && !strcmp(netdev_get_name(port->netdev), name)) {\n            struct netdev *old_netdev = port->netdev;\n\n            /* 'name' hasn't changed location.  Any properties changed? */\n            if (!ofport_equal(&port->pp, &pp)) {\n                ofport_modified(port, &pp);\n            }\n\n            update_mtu(ofproto, port);\n\n            /* Install the newly opened netdev in case it has changed.\n             * Don't close the old netdev yet in case port_modified has to\n             * remove a retained reference to it.*/\n            port->netdev = netdev;\n            port->change_seq = netdev_get_change_seq(netdev);\n\n            if (port->ofproto->ofproto_class->port_modified) {\n                port->ofproto->ofproto_class->port_modified(port);\n            }\n\n            netdev_close(old_netdev);\n        } else {\n            /* If 'port' is nonnull then its name differs from 'name' and thus\n             * we should delete it.  If we think there's a port named 'name'\n             * then its port number must be wrong now so delete it too. */\n            if (port) {\n                ofport_remove(port);\n            }\n            ofport_remove_with_name(ofproto, name);\n            error = ofport_install(ofproto, netdev, &pp);\n        }\n    } else {\n        /* Any port named 'name' is gone now. */\n        ofport_remove_with_name(ofproto, name);\n    }\n    ofproto_port_destroy(&ofproto_port);\n\n    return error;\n}\n\nstatic int\ninit_ports(struct ofproto *p)\n{\n    struct ofproto_port_dump dump;\n    struct ofproto_port ofproto_port;\n    struct shash_node *node, *next;\n\n    OFPROTO_PORT_FOR_EACH (&ofproto_port, &dump, p) {\n        const char *name = ofproto_port.name;\n\n        if (shash_find(&p->port_by_name, name)) {\n            VLOG_WARN_RL(&rl, \"%s: ignoring duplicate device %s in datapath\",\n                         p->name, name);\n        } else {\n            struct ofputil_phy_port pp;\n            struct netdev *netdev;\n\n            /* Check if an OpenFlow port number had been requested. */\n            node = shash_find(&init_ofp_ports, name);\n            if (node) {\n                const struct iface_hint *iface_hint = node->data;\n                simap_put(&p->ofp_requests, name,\n                          ofp_to_u16(iface_hint->ofp_port));\n            }\n\n            netdev = ofport_open(p, &ofproto_port, &pp);\n            if (netdev) {\n                ofport_install(p, netdev, &pp);\n                if (ofp_to_u16(ofproto_port.ofp_port) < p->max_ports) {\n                    p->alloc_port_no = MAX(p->alloc_port_no,\n                                           ofp_to_u16(ofproto_port.ofp_port));\n                }\n            }\n        }\n    }\n\n    SHASH_FOR_EACH_SAFE(node, next, &init_ofp_ports) {\n        struct iface_hint *iface_hint = node->data;\n\n        if (!strcmp(iface_hint->br_name, p->name)) {\n            free(iface_hint->br_name);\n            free(iface_hint->br_type);\n            free(iface_hint);\n            shash_delete(&init_ofp_ports, node);\n        }\n    }\n\n    return 0;\n}\n\nstatic bool\nofport_is_internal_or_patch(const struct ofproto *p, const struct ofport *port)\n{\n    return !strcmp(netdev_get_type(port->netdev),\n                   ofproto_port_open_type(p->type, \"internal\")) ||\n           !strcmp(netdev_get_type(port->netdev),\n                   ofproto_port_open_type(p->type, \"patch\"));\n}\n\n/* If 'port' is internal or patch and if the user didn't explicitly specify an\n * mtu through the database, we have to override it. */\nstatic bool\nofport_is_mtu_overridden(const struct ofproto *p, const struct ofport *port)\n{\n    return ofport_is_internal_or_patch(p, port)\n           && !netdev_mtu_is_user_config(port->netdev);\n}\n\n/* Find the minimum MTU of all non-overridden devices attached to 'p'.\n * Returns ETH_PAYLOAD_MAX or the minimum of the ports. */\nstatic int\nfind_min_mtu(struct ofproto *p)\n{\n    struct ofport *ofport;\n    int mtu = 0;\n\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        struct netdev *netdev = ofport->netdev;\n        int dev_mtu;\n\n        /* Skip any overridden port, since that's what we're trying to set. */\n        if (ofport_is_mtu_overridden(p, ofport)) {\n            continue;\n        }\n\n        if (netdev_get_mtu(netdev, &dev_mtu)) {\n            continue;\n        }\n        if (!mtu || dev_mtu < mtu) {\n            mtu = dev_mtu;\n        }\n    }\n\n    return mtu ? mtu: ETH_PAYLOAD_MAX;\n}\n\n/* Update MTU of all overridden devices on 'p' to the minimum of the\n * non-overridden ports in event of 'port' added or changed. */\nstatic void\nupdate_mtu(struct ofproto *p, struct ofport *port)\n{\n    struct netdev *netdev = port->netdev;\n    int dev_mtu;\n\n    if (netdev_get_mtu(netdev, &dev_mtu)) {\n        port->mtu = 0;\n        return;\n    }\n    if (ofport_is_mtu_overridden(p, port)) {\n        if (dev_mtu > p->min_mtu) {\n            if (!netdev_set_mtu(port->netdev, p->min_mtu)) {\n                dev_mtu = p->min_mtu;\n            }\n        }\n        port->mtu = dev_mtu;\n        return;\n    }\n\n    port->mtu = dev_mtu;\n    /* For non-overridden port find new min mtu. */\n\n    update_mtu_ofproto(p);\n}\n\nstatic void\nupdate_mtu_ofproto(struct ofproto *p)\n{\n    struct ofport *ofport;\n    int old_min = p->min_mtu;\n\n    p->min_mtu = find_min_mtu(p);\n    if (p->min_mtu == old_min) {\n        return;\n    }\n\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        struct netdev *netdev = ofport->netdev;\n\n        if (ofport_is_mtu_overridden(p, ofport)) {\n            if (!netdev_set_mtu(netdev, p->min_mtu)) {\n                ofport->mtu = p->min_mtu;\n            }\n        }\n    }\n}\n\f\nstatic void\nofproto_rule_destroy__(struct rule *rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    cls_rule_destroy(CONST_CAST(struct cls_rule *, &rule->cr));\n    rule_actions_destroy(rule_get_actions(rule));\n    ovs_mutex_destroy(&rule->mutex);\n    rule->ofproto->ofproto_class->rule_dealloc(rule);\n}\n\nstatic void\nrule_destroy_cb(struct rule *rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    /* Send rule removed if needed. */\n    if (rule->flags & OFPUTIL_FF_SEND_FLOW_REM\n        && rule->removed_reason != OVS_OFPRR_NONE\n        && !rule_is_hidden(rule)) {\n        ofproto_rule_send_removed(rule);\n    }\n    rule->ofproto->ofproto_class->rule_destruct(rule);\n    mf_vl_mff_unref(&rule->ofproto->vl_mff_map, rule->match_tlv_bitmap);\n    mf_vl_mff_unref(&rule->ofproto->vl_mff_map, rule->ofpacts_tlv_bitmap);\n    ofproto_rule_destroy__(rule);\n}\n\nvoid\nofproto_rule_ref(struct rule *rule)\n{\n    if (rule) {\n        ovs_refcount_ref(&rule->ref_count);\n    }\n}\n\nbool\nofproto_rule_try_ref(struct rule *rule)\n{\n    if (rule) {\n        return ovs_refcount_try_ref_rcu(&rule->ref_count);\n    }\n    return false;\n}\n\n/* Decrements 'rule''s ref_count and schedules 'rule' to be destroyed if the\n * ref_count reaches 0.\n *\n * Use of RCU allows short term use (between RCU quiescent periods) without\n * keeping a reference.  A reference must be taken if the rule needs to\n * stay around accross the RCU quiescent periods. */\nvoid\nofproto_rule_unref(struct rule *rule)\n{\n    if (rule && ovs_refcount_unref_relaxed(&rule->ref_count) == 1) {\n        ovs_assert(rule->state != RULE_INSERTED);\n        ovsrcu_postpone(rule_destroy_cb, rule);\n    }\n}\n\nstatic void\nremove_rule_rcu__(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = rule->ofproto;\n    struct oftable *table = &ofproto->tables[rule->table_id];\n\n    ovs_assert(!cls_rule_visible_in_version(&rule->cr, OVS_VERSION_MAX));\n    if (!classifier_remove(&table->cls, &rule->cr)) {\n        OVS_NOT_REACHED();\n    }\n    if (ofproto->ofproto_class->rule_delete) {\n        ofproto->ofproto_class->rule_delete(rule);\n    }\n    ofproto_rule_unref(rule);\n}\n\nstatic void\nremove_rule_rcu(struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    remove_rule_rcu__(rule);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Removes and deletes rules from a NULL-terminated array of rule pointers. */\nstatic void\nremove_rules_rcu(struct rule **rules)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct rule **orig_rules = rules;\n\n    if (*rules) {\n        struct ofproto *ofproto = rules[0]->ofproto;\n        unsigned long tables[BITMAP_N_LONGS(256)];\n        struct rule *rule;\n        size_t table_id;\n\n        memset(tables, 0, sizeof tables);\n\n        ovs_mutex_lock(&ofproto_mutex);\n        while ((rule = *rules++)) {\n            /* Defer once for each new table.  This defers the subtable cleanup\n             * until later, so that when removing large number of flows the\n             * operation is faster. */\n            if (!bitmap_is_set(tables, rule->table_id)) {\n                struct classifier *cls = &ofproto->tables[rule->table_id].cls;\n\n                bitmap_set1(tables, rule->table_id);\n                classifier_defer(cls);\n            }\n            remove_rule_rcu__(rule);\n        }\n\n        BITMAP_FOR_EACH_1(table_id, 256, tables) {\n            struct classifier *cls = &ofproto->tables[table_id].cls;\n\n            classifier_publish(cls);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    free(orig_rules);\n}\n\nvoid\nofproto_group_ref(struct ofgroup *group)\n{\n    if (group) {\n        ovs_refcount_ref(&group->ref_count);\n    }\n}\n\nbool\nofproto_group_try_ref(struct ofgroup *group)\n{\n    if (group) {\n        return ovs_refcount_try_ref_rcu(&group->ref_count);\n    }\n    return false;\n}\n\nstatic void\ngroup_destroy_cb(struct ofgroup *group)\n{\n    group->ofproto->ofproto_class->group_destruct(group);\n    ofputil_group_properties_destroy(CONST_CAST(struct ofputil_group_props *,\n                                                &group->props));\n    ofputil_bucket_list_destroy(CONST_CAST(struct ovs_list *,\n                                           &group->buckets));\n    group->ofproto->ofproto_class->group_dealloc(group);\n}\n\nvoid\nofproto_group_unref(struct ofgroup *group)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    if (group && ovs_refcount_unref_relaxed(&group->ref_count) == 1) {\n        ovs_assert(rule_collection_n(&group->rules) == 0);\n        ovsrcu_postpone(group_destroy_cb, group);\n    }\n}\n\nstatic void\nremove_group_rcu__(struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = group->ofproto;\n\n    ovs_assert(!versions_visible_in_version(&group->versions, OVS_VERSION_MAX));\n\n    cmap_remove(&ofproto->groups, &group->cmap_node,\n                hash_int(group->group_id, 0));\n    ofproto_group_unref(group);\n}\n\nstatic void\nremove_group_rcu(struct ofgroup *group)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    remove_group_rcu__(group);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Removes and deletes groups from a NULL-terminated array of group\n * pointers. */\nstatic void\nremove_groups_rcu(struct ofgroup **groups)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    for (struct ofgroup **g = groups; *g; g++) {\n        remove_group_rcu__(*g);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n    free(groups);\n}\n\nstatic uint32_t get_provider_meter_id(const struct ofproto *,\n                                      uint32_t of_meter_id);\n\n/* Creates and returns a new 'struct rule_actions', whose actions are a copy\n * of from the 'ofpacts_len' bytes of 'ofpacts'. */\nconst struct rule_actions *\nrule_actions_create(const struct ofpact *ofpacts, size_t ofpacts_len)\n{\n    struct rule_actions *actions;\n\n    actions = xmalloc(sizeof *actions + ofpacts_len);\n    actions->ofpacts_len = ofpacts_len;\n    memcpy(actions->ofpacts, ofpacts, ofpacts_len);\n    actions->has_meter = ofpacts_get_meter(ofpacts, ofpacts_len) != 0;\n    actions->has_groups =\n        (ofpact_find_type_flattened(ofpacts, OFPACT_GROUP,\n                                    ofpact_end(ofpacts, ofpacts_len))\n         != NULL);\n    actions->has_learn_with_delete = (next_learn_with_delete(actions, NULL)\n                                      != NULL);\n\n    return actions;\n}\n\n/* Free the actions after the RCU quiescent period is reached. */\nvoid\nrule_actions_destroy(const struct rule_actions *actions)\n{\n    if (actions) {\n        ovsrcu_postpone(free, CONST_CAST(struct rule_actions *, actions));\n    }\n}\n\n/* Returns true if 'rule' has an OpenFlow OFPAT_OUTPUT or OFPAT_ENQUEUE action\n * that outputs to 'port' (output to OFPP_FLOOD and OFPP_ALL doesn't count). */\nbool\nofproto_rule_has_out_port(const struct rule *rule, ofp_port_t port)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (port == OFPP_ANY) {\n        return true;\n    } else {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        return ofpacts_output_to_port(actions->ofpacts,\n                                      actions->ofpacts_len, port);\n    }\n}\n\n/* Returns true if 'rule' has group and equals group_id. */\nstatic bool\nofproto_rule_has_out_group(const struct rule *rule, uint32_t group_id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (group_id == OFPG_ANY) {\n        return true;\n    } else {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        return ofpacts_output_to_group(actions->ofpacts,\n                                       actions->ofpacts_len, group_id);\n    }\n}\n\nstatic bool\nrule_is_readonly(const struct rule *rule)\n{\n    const struct oftable *table = &rule->ofproto->tables[rule->table_id];\n    return (table->flags & OFTABLE_READONLY) != 0;\n}\n\f\nstatic uint32_t\nhash_learned_cookie(ovs_be64 cookie_, uint8_t table_id)\n{\n    uint64_t cookie = (OVS_FORCE uint64_t) cookie_;\n    return hash_3words(cookie, cookie >> 32, table_id);\n}\n\nstatic void\nlearned_cookies_update_one__(struct ofproto *ofproto,\n                             const struct ofpact_learn *learn,\n                             int delta, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    uint32_t hash = hash_learned_cookie(learn->cookie, learn->table_id);\n    struct learned_cookie *c;\n\n    HMAP_FOR_EACH_WITH_HASH (c, u.hmap_node, hash, &ofproto->learned_cookies) {\n        if (c->cookie == learn->cookie && c->table_id == learn->table_id) {\n            c->n += delta;\n            ovs_assert(c->n >= 0);\n\n            if (!c->n) {\n                hmap_remove(&ofproto->learned_cookies, &c->u.hmap_node);\n                ovs_list_push_back(dead_cookies, &c->u.list_node);\n            }\n\n            return;\n        }\n    }\n\n    ovs_assert(delta > 0);\n    c = xmalloc(sizeof *c);\n    hmap_insert(&ofproto->learned_cookies, &c->u.hmap_node, hash);\n    c->cookie = learn->cookie;\n    c->table_id = learn->table_id;\n    c->n = delta;\n}\n\nstatic const struct ofpact_learn *\nnext_learn_with_delete(const struct rule_actions *actions,\n                       const struct ofpact_learn *start)\n{\n    const struct ofpact *pos;\n\n    for (pos = start ? ofpact_next(&start->ofpact) : actions->ofpacts;\n         pos < ofpact_end(actions->ofpacts, actions->ofpacts_len);\n         pos = ofpact_next(pos)) {\n        if (pos->type == OFPACT_LEARN) {\n            const struct ofpact_learn *learn = ofpact_get_LEARN(pos);\n            if (learn->flags & NX_LEARN_F_DELETE_LEARNED) {\n                return learn;\n            }\n        }\n    }\n\n    return NULL;\n}\n\nstatic void\nlearned_cookies_update__(struct ofproto *ofproto,\n                         const struct rule_actions *actions,\n                         int delta, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (actions->has_learn_with_delete) {\n        const struct ofpact_learn *learn;\n\n        for (learn = next_learn_with_delete(actions, NULL); learn;\n             learn = next_learn_with_delete(actions, learn)) {\n            learned_cookies_update_one__(ofproto, learn, delta, dead_cookies);\n        }\n    }\n}\n\nstatic void\nlearned_cookies_inc(struct ofproto *ofproto,\n                    const struct rule_actions *actions)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    learned_cookies_update__(ofproto, actions, +1, NULL);\n}\n\nstatic void\nlearned_cookies_dec(struct ofproto *ofproto,\n                    const struct rule_actions *actions,\n                    struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    learned_cookies_update__(ofproto, actions, -1, dead_cookies);\n}\n\nstatic void\nlearned_cookies_flush(struct ofproto *ofproto, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct learned_cookie *c;\n\n    LIST_FOR_EACH_POP (c, u.list_node, dead_cookies) {\n        struct rule_criteria criteria;\n        struct rule_collection rules;\n        struct match match;\n\n        match_init_catchall(&match);\n        rule_criteria_init(&criteria, c->table_id, &match, 0, OVS_VERSION_MAX,\n                           c->cookie, OVS_BE64_MAX, OFPP_ANY, OFPG_ANY);\n        rule_criteria_require_rw(&criteria, false);\n        collect_rules_loose(ofproto, &criteria, &rules);\n        rule_criteria_destroy(&criteria);\n        delete_flows__(&rules, OFPRR_DELETE, NULL);\n\n        free(c);\n    }\n}\n\f\nstatic enum ofperr\nhandle_echo_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    ofconn_send_reply(ofconn, make_echo_reply(oh));\n    return 0;\n}\n\nstatic void\nquery_tables(struct ofproto *ofproto,\n             struct ofputil_table_features **featuresp,\n             struct ofputil_table_stats **statsp)\n{\n    struct mf_bitmap rw_fields = oxm_writable_fields();\n    struct mf_bitmap match = oxm_matchable_fields();\n    struct mf_bitmap mask = oxm_maskable_fields();\n\n    struct ofputil_table_features *features;\n    struct ofputil_table_stats *stats;\n    int i;\n\n    features = *featuresp = xcalloc(ofproto->n_tables, sizeof *features);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        struct ofputil_table_features *f = &features[i];\n\n        f->table_id = i;\n        sprintf(f->name, \"table%d\", i);\n        f->metadata_match = OVS_BE64_MAX;\n        f->metadata_write = OVS_BE64_MAX;\n        atomic_read_relaxed(&ofproto->tables[i].miss_config, &f->miss_config);\n        f->max_entries = 1000000;\n\n        bool more_tables = false;\n        for (int j = i + 1; j < ofproto->n_tables; j++) {\n            if (!(ofproto->tables[j].flags & OFTABLE_HIDDEN)) {\n                bitmap_set1(f->nonmiss.next, j);\n                more_tables = true;\n            }\n        }\n        f->nonmiss.instructions = (1u << N_OVS_INSTRUCTIONS) - 1;\n        if (!more_tables) {\n            f->nonmiss.instructions &= ~(1u << OVSINST_OFPIT11_GOTO_TABLE);\n        }\n        f->nonmiss.write.ofpacts = (UINT64_C(1) << N_OFPACTS) - 1;\n        f->nonmiss.write.set_fields = rw_fields;\n        f->nonmiss.apply = f->nonmiss.write;\n        f->miss = f->nonmiss;\n\n        f->match = match;\n        f->mask = mask;\n        f->wildcard = match;\n    }\n\n    if (statsp) {\n        stats = *statsp = xcalloc(ofproto->n_tables, sizeof *stats);\n        for (i = 0; i < ofproto->n_tables; i++) {\n            struct ofputil_table_stats *s = &stats[i];\n\n            s->table_id = i;\n            s->active_count = ofproto->tables[i].n_flows;\n            if (i == 0) {\n                s->active_count -= connmgr_count_hidden_rules(\n                    ofproto->connmgr);\n            }\n        }\n    } else {\n        stats = NULL;\n    }\n\n    ofproto->ofproto_class->query_tables(ofproto, features, stats);\n\n    for (i = 0; i < ofproto->n_tables; i++) {\n        const struct oftable *table = &ofproto->tables[i];\n        struct ofputil_table_features *f = &features[i];\n\n        if (table->name) {\n            ovs_strzcpy(f->name, table->name, sizeof f->name);\n        }\n\n        if (table->max_flows < f->max_entries) {\n            f->max_entries = table->max_flows;\n        }\n    }\n}\n\nstatic void\nquery_switch_features(struct ofproto *ofproto,\n                      bool *arp_match_ip, uint64_t *ofpacts)\n{\n    struct ofputil_table_features *features, *f;\n\n    *arp_match_ip = false;\n    *ofpacts = 0;\n\n    query_tables(ofproto, &features, NULL);\n    for (f = features; f < &features[ofproto->n_tables]; f++) {\n        *ofpacts |= f->nonmiss.apply.ofpacts | f->miss.apply.ofpacts;\n        if (bitmap_is_set(f->match.bm, MFF_ARP_SPA) ||\n            bitmap_is_set(f->match.bm, MFF_ARP_TPA)) {\n            *arp_match_ip = true;\n        }\n    }\n    free(features);\n\n    /* Sanity check. */\n    ovs_assert(*ofpacts & (UINT64_C(1) << OFPACT_OUTPUT));\n}\n\nstatic enum ofperr\nhandle_features_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_switch_features features;\n    struct ofport *port;\n    bool arp_match_ip;\n    struct ofpbuf *b;\n\n    query_switch_features(ofproto, &arp_match_ip, &features.ofpacts);\n\n    features.datapath_id = ofproto->datapath_id;\n    features.n_buffers = 0;\n    features.n_tables = ofproto_get_n_visible_tables(ofproto);\n    features.capabilities = (OFPUTIL_C_FLOW_STATS | OFPUTIL_C_TABLE_STATS |\n                             OFPUTIL_C_PORT_STATS | OFPUTIL_C_QUEUE_STATS |\n                             OFPUTIL_C_GROUP_STATS | OFPUTIL_C_BUNDLES);\n    if (arp_match_ip) {\n        features.capabilities |= OFPUTIL_C_ARP_MATCH_IP;\n    }\n    /* FIXME: Fill in proper features.auxiliary_id for auxiliary connections */\n    features.auxiliary_id = 0;\n    b = ofputil_encode_switch_features(&features, ofconn_get_protocol(ofconn),\n                                       oh->xid);\n    HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n        ofputil_put_switch_features_port(&port->pp, b);\n    }\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_get_config_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_switch_config config;\n    config.frag = ofconn_get_ofproto(ofconn)->frag_handling;\n    config.invalid_ttl_to_controller\n        = ofconn_get_invalid_ttl_to_controller(ofconn);\n    config.miss_send_len = ofconn_get_miss_send_len(ofconn);\n\n    ofconn_send_reply(ofconn, ofputil_encode_get_config_reply(oh, &config));\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_set_config(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_switch_config config;\n    enum ofperr error;\n\n    error = ofputil_decode_set_config(oh, &config);\n    if (error) {\n        return error;\n    }\n\n    if (ofconn_get_type(ofconn) != OFCONN_PRIMARY\n        || ofconn_get_role(ofconn) != OFPCR12_ROLE_SLAVE) {\n        enum ofputil_frag_handling cur = ofproto->frag_handling;\n        enum ofputil_frag_handling next = config.frag;\n\n        if (cur != next) {\n            if (ofproto->ofproto_class->set_frag_handling(ofproto, next)) {\n                ofproto->frag_handling = next;\n            } else {\n                VLOG_WARN_RL(&rl, \"%s: unsupported fragment handling mode %s\",\n                             ofproto->name,\n                             ofputil_frag_handling_to_string(next));\n            }\n        }\n    }\n\n    if (config.invalid_ttl_to_controller >= 0) {\n        ofconn_set_invalid_ttl_to_controller(ofconn,\n                                             config.invalid_ttl_to_controller);\n    }\n\n    ofconn_set_miss_send_len(ofconn, config.miss_send_len);\n\n    return 0;\n}\n\n/* Checks whether 'ofconn' is a slave controller.  If so, returns an OpenFlow\n * error message code for the caller to propagate upward.  Otherwise, returns\n * 0.\n *\n * The log message mentions 'msg_type'. */\nstatic enum ofperr\nreject_slave_controller(struct ofconn *ofconn)\n{\n    if (ofconn_get_type(ofconn) == OFCONN_PRIMARY\n        && ofconn_get_role(ofconn) == OFPCR12_ROLE_SLAVE) {\n        return OFPERR_OFPBRC_IS_SLAVE;\n    } else {\n        return 0;\n    }\n}\n\n/* Checks that the 'ofpacts_len' bytes of action in 'ofpacts' are appropriate\n * for 'ofproto':\n *\n *    - If they use a meter, then 'ofproto' has that meter configured.\n *\n *    - If they use any groups, then 'ofproto' has that group configured.\n *\n * Returns 0 if successful, otherwise an OpenFlow error.  Caller must hold\n * 'ofproto_mutex' for the result to be valid also after this function\n * returns. */\nenum ofperr\nofproto_check_ofpacts(struct ofproto *ofproto,\n                      const struct ofpact ofpacts[], size_t ofpacts_len)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    uint32_t mid;\n\n    mid = ofpacts_get_meter(ofpacts, ofpacts_len);\n    if (mid && get_provider_meter_id(ofproto, mid) == UINT32_MAX) {\n        return OFPERR_OFPMMFC_INVALID_METER;\n    }\n\n    const struct ofpact_group *a;\n    OFPACT_FOR_EACH_TYPE_FLATTENED (a, GROUP, ofpacts, ofpacts_len) {\n        if (!ofproto_group_exists(ofproto, a->group_id)) {\n            return OFPERR_OFPBAC_BAD_OUT_GROUP;\n        }\n    }\n\n    return 0;\n}\n\nvoid\nofproto_packet_out_uninit(struct ofproto_packet_out *opo)\n{\n    dp_packet_delete(opo->packet);\n    opo->packet = NULL;\n    free(opo->flow);\n    opo->flow = NULL;\n    free(opo->ofpacts);\n    opo->ofpacts = NULL;\n    opo->ofpacts_len = 0;\n    ovs_assert(!opo->aux);\n}\n\n/* Takes ownership of po->ofpacts, which must have been malloc'ed. */\nstatic enum ofperr\nofproto_packet_out_init(struct ofproto *ofproto,\n                        struct ofconn *ofconn,\n                        struct ofproto_packet_out *opo,\n                        const struct ofputil_packet_out *po)\n{\n    enum ofperr error;\n\n    if (ofp_to_u16(po->in_port) >= ofproto->max_ports\n        && ofp_to_u16(po->in_port) < ofp_to_u16(OFPP_MAX)) {\n        return OFPERR_OFPBRC_BAD_PORT;\n    }\n\n    /* Get payload. */\n    if (po->buffer_id != UINT32_MAX) {\n        return OFPERR_OFPBRC_BUFFER_UNKNOWN;\n    }\n\n    /* Ensure that the L3 header is 32-bit aligned. */\n    opo->packet = dp_packet_clone_data_with_headroom(po->packet,\n                                                     po->packet_len, 2);\n    /* Store struct flow. */\n    opo->flow = xmalloc(sizeof *opo->flow);\n    flow_extract(opo->packet, opo->flow);\n    opo->flow->in_port.ofp_port = po->in_port;\n\n    /* Check actions like for flow mods.  We pass a 'table_id' of 0 to\n     * ofproto_check_consistency(), which isn't strictly correct because these\n     * actions aren't in any table.  This is OK as 'table_id' is only used to\n     * check instructions (e.g., goto-table), which can't appear on the action\n     * list of a packet-out. */\n    error = ofpacts_check_consistency(po->ofpacts, po->ofpacts_len,\n                                      opo->flow,\n                                      u16_to_ofp(ofproto->max_ports), 0,\n                                      ofproto->n_tables,\n                                      ofconn_get_protocol(ofconn));\n    if (error) {\n        dp_packet_delete(opo->packet);\n        free(opo->flow);\n        return error;\n    }\n\n    opo->ofpacts = po->ofpacts;\n    opo->ofpacts_len = po->ofpacts_len;\n\n    opo->aux = NULL;\n    return 0;\n}\n\nstatic enum ofperr\nofproto_packet_out_start(struct ofproto *ofproto,\n                         struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    error = ofproto_check_ofpacts(ofproto, opo->ofpacts, opo->ofpacts_len);\n    if (error) {\n        return error;\n    }\n\n    return ofproto->ofproto_class->packet_xlate(ofproto, opo);\n}\n\nstatic void\nofproto_packet_out_revert(struct ofproto *ofproto,\n                          struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto->ofproto_class->packet_xlate_revert(ofproto, opo);\n}\n\nstatic void\nofproto_packet_out_finish(struct ofproto *ofproto,\n                          struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto->ofproto_class->packet_execute(ofproto, opo);\n}\n\nstatic enum ofperr\nhandle_packet_out(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofputil_packet_out po;\n    struct ofproto_packet_out opo;\n    uint64_t ofpacts_stub[1024 / 8];\n    struct ofpbuf ofpacts;\n    enum ofperr error;\n\n    COVERAGE_INC(ofproto_packet_out);\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    /* Decode message. */\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n    error = ofputil_decode_packet_out(&po, oh, &ofpacts);\n    if (error) {\n        ofpbuf_uninit(&ofpacts);\n        return error;\n    }\n\n    po.ofpacts = ofpbuf_steal_data(&ofpacts);   /* Move to heap. */\n    error = ofproto_packet_out_init(p, ofconn, &opo, &po);\n    if (error) {\n        free(po.ofpacts);\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    opo.version = p->tables_version;\n    error = ofproto_packet_out_start(p, &opo);\n    if (!error) {\n        ofproto_packet_out_finish(p, &opo);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ofproto_packet_out_uninit(&opo);\n    return error;\n}\n\nstatic enum ofperr\nhandle_nxt_resume(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_packet_in_private pin;\n    enum ofperr error;\n\n    error = ofputil_decode_packet_in_private(oh, false,\n                                             ofproto_get_tun_tab(ofproto),\n                                             &ofproto->vl_mff_map, &pin, NULL,\n                                             NULL);\n    if (error) {\n        return error;\n    }\n\n    error = (ofproto->ofproto_class->nxt_resume\n             ? ofproto->ofproto_class->nxt_resume(ofproto, &pin)\n             : OFPERR_NXR_NOT_SUPPORTED);\n\n    ofputil_packet_in_private_destroy(&pin);\n\n    return error;\n}\n\nstatic void\nupdate_port_config(struct ofconn *ofconn, struct ofport *port,\n                   enum ofputil_port_config config,\n                   enum ofputil_port_config mask)\n{\n    enum ofputil_port_config toggle = (config ^ port->pp.config) & mask;\n\n    if (toggle & OFPUTIL_PC_PORT_DOWN\n        && (config & OFPUTIL_PC_PORT_DOWN\n            ? netdev_turn_flags_off(port->netdev, NETDEV_UP, NULL)\n            : netdev_turn_flags_on(port->netdev, NETDEV_UP, NULL))) {\n        /* We tried to bring the port up or down, but it failed, so don't\n         * update the \"down\" bit. */\n        toggle &= ~OFPUTIL_PC_PORT_DOWN;\n    }\n\n    if (toggle) {\n        enum ofputil_port_config old_config = port->pp.config;\n        port->pp.config ^= toggle;\n        port->ofproto->ofproto_class->port_reconfigured(port, old_config);\n        connmgr_send_port_status(port->ofproto->connmgr, ofconn, &port->pp,\n                                 OFPPR_MODIFY);\n    }\n}\n\nstatic enum ofperr\nport_mod_start(struct ofconn *ofconn, struct ofputil_port_mod *pm,\n               struct ofport **port)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n\n    *port = ofproto_get_port(p, pm->port_no);\n    if (!*port) {\n        return OFPERR_OFPPMFC_BAD_PORT;\n    }\n    if (!eth_addr_equals((*port)->pp.hw_addr, pm->hw_addr)) {\n        return OFPERR_OFPPMFC_BAD_HW_ADDR;\n    }\n    return 0;\n}\n\nstatic void\nport_mod_finish(struct ofconn *ofconn, struct ofputil_port_mod *pm,\n                struct ofport *port)\n{\n    update_port_config(ofconn, port, pm->config, pm->mask);\n    if (pm->advertise) {\n        netdev_set_advertisements(port->netdev, pm->advertise);\n    }\n}\n\nstatic enum ofperr\nhandle_port_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_port_mod pm;\n    struct ofport *port;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_port_mod(oh, &pm, false);\n    if (error) {\n        return error;\n    }\n\n    error = port_mod_start(ofconn, &pm, &port);\n    if (!error) {\n        port_mod_finish(ofconn, &pm, port);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_desc_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    static const char *default_mfr_desc = \"Nicira, Inc.\";\n    static const char *default_hw_desc = \"Open vSwitch\";\n    static const char *default_sw_desc = VERSION;\n    static const char *default_serial_desc = \"None\";\n    static const char *default_dp_desc = \"None\";\n\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofp_desc_stats *ods;\n    struct ofpbuf *msg;\n\n    msg = ofpraw_alloc_stats_reply(request, 0);\n    ods = ofpbuf_put_zeros(msg, sizeof *ods);\n    ovs_strlcpy(ods->mfr_desc, p->mfr_desc ? p->mfr_desc : default_mfr_desc,\n                sizeof ods->mfr_desc);\n    ovs_strlcpy(ods->hw_desc, p->hw_desc ? p->hw_desc : default_hw_desc,\n                sizeof ods->hw_desc);\n    ovs_strlcpy(ods->sw_desc, p->sw_desc ? p->sw_desc : default_sw_desc,\n                sizeof ods->sw_desc);\n    ovs_strlcpy(ods->serial_num,\n                p->serial_desc ? p->serial_desc : default_serial_desc,\n                sizeof ods->serial_num);\n    ovs_strlcpy(ods->dp_desc, p->dp_desc ? p->dp_desc : default_dp_desc,\n                sizeof ods->dp_desc);\n    ofconn_send_reply(ofconn, msg);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_features *features;\n    struct ofputil_table_stats *stats;\n    struct ofpbuf *reply;\n    size_t i;\n\n    query_tables(ofproto, &features, &stats);\n\n    reply = ofputil_encode_table_stats_reply(request);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_stats_reply(reply, &stats[i], &features[i]);\n        }\n    }\n    ofconn_send_reply(ofconn, reply);\n\n    free(features);\n    free(stats);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_features_request(struct ofconn *ofconn,\n                              const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofpbuf msg = ofpbuf_const_initializer(request,\n                                                 ntohs(request->length));\n    ofpraw_pull_assert(&msg);\n    if (msg.size || ofpmp_more(request)) {\n        return OFPERR_OFPTFFC_EPERM;\n    }\n\n    struct ofputil_table_features *features;\n    query_tables(ofproto, &features, NULL);\n\n    struct ovs_list replies;\n    ofpmp_init(&replies, request);\n    for (size_t i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_features_reply(&features[i], &replies);\n        }\n    }\n    ofconn_send_replies(ofconn, &replies);\n\n    free(features);\n\n    return 0;\n}\n\n/* Returns the vacancy of 'oftable', a number that ranges from 0 (if the table\n * is full) to 100 (if the table is empty).\n *\n * A table without a limit on flows is considered to be empty. */\nstatic uint8_t\noftable_vacancy(const struct oftable *t)\n{\n    return (!t->max_flows ? 100\n            : t->n_flows >= t->max_flows ? 0\n            : (t->max_flows - t->n_flows) * 100.0 / t->max_flows);\n}\n\nstatic void\nquery_table_desc__(struct ofputil_table_desc *td,\n                   struct ofproto *ofproto, uint8_t table_id)\n{\n    const struct oftable *t = &ofproto->tables[table_id];\n\n    td->table_id = table_id;\n    td->eviction = (t->eviction & EVICTION_OPENFLOW\n                    ? OFPUTIL_TABLE_EVICTION_ON\n                    : OFPUTIL_TABLE_EVICTION_OFF);\n    td->eviction_flags = OFPROTO_EVICTION_FLAGS;\n    td->vacancy = (t->vacancy_event\n                   ? OFPUTIL_TABLE_VACANCY_ON\n                   : OFPUTIL_TABLE_VACANCY_OFF);\n    td->table_vacancy.vacancy_down = t->vacancy_down;\n    td->table_vacancy.vacancy_up = t->vacancy_up;\n    td->table_vacancy.vacancy = oftable_vacancy(t);\n}\n\n/* This function queries the database for dumping table-desc. */\nstatic void\nquery_tables_desc(struct ofproto *ofproto, struct ofputil_table_desc **descp)\n{\n    struct ofputil_table_desc *table_desc;\n    size_t i;\n\n    table_desc = *descp = xcalloc(ofproto->n_tables, sizeof *table_desc);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        struct ofputil_table_desc *td = &table_desc[i];\n        query_table_desc__(td, ofproto, i);\n    }\n}\n\n/* Function to handle dump-table-desc request. */\nstatic enum ofperr\nhandle_table_desc_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_desc *table_desc;\n    struct ovs_list replies;\n    size_t i;\n\n    query_tables_desc(ofproto, &table_desc);\n    ofpmp_init(&replies, request);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_desc_reply(&table_desc[i], &replies,\n                                            request->version);\n        }\n    }\n    ofconn_send_replies(ofconn, &replies);\n    free(table_desc);\n    return 0;\n}\n\n/* This function determines and sends the vacancy event, based on the value\n * of current vacancy and threshold vacancy. If the current vacancy is less\n * than or equal to vacancy_down, vacancy up events must be enabled, and when\n * the current vacancy is greater or equal to vacancy_up, vacancy down events\n * must be enabled. */\nstatic void\nsend_table_status(struct ofproto *ofproto, uint8_t table_id)\n{\n    struct oftable *t = &ofproto->tables[table_id];\n    if (!t->vacancy_event) {\n        return;\n    }\n\n    uint8_t vacancy = oftable_vacancy(t);\n    enum ofp14_table_reason event;\n    if (vacancy < t->vacancy_down) {\n        event = OFPTR_VACANCY_DOWN;\n    } else if (vacancy > t->vacancy_up) {\n        event = OFPTR_VACANCY_UP;\n    } else {\n        return;\n    }\n\n    if (event == t->vacancy_event) {\n        struct ofputil_table_desc td;\n        query_table_desc__(&td, ofproto, table_id);\n        connmgr_send_table_status(ofproto->connmgr, &td, event);\n\n        t->vacancy_event = (event == OFPTR_VACANCY_DOWN\n                            ? OFPTR_VACANCY_UP\n                            : OFPTR_VACANCY_DOWN);\n    }\n}\n\nstatic void\nappend_port_stat(struct ofport *port, struct ovs_list *replies)\n{\n    struct ofputil_port_stats ops = { .port_no = port->pp.port_no };\n\n    calc_duration(port->created, time_msec(),\n                  &ops.duration_sec, &ops.duration_nsec);\n\n    /* Intentionally ignore return value, since errors will set\n     * 'stats' to all-1s, which is correct for OpenFlow, and\n     * netdev_get_stats() will log errors. */\n    ofproto_port_get_stats(port, &ops.stats);\n\n    ofputil_append_port_stat(replies, &ops);\n}\n\nstatic void\nhandle_port_request(struct ofconn *ofconn,\n                    const struct ofp_header *request, ofp_port_t port_no,\n                    void (*cb)(struct ofport *, struct ovs_list *replies))\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofport *port;\n    struct ovs_list replies;\n\n    ofpmp_init(&replies, request);\n    if (port_no != OFPP_ANY) {\n        port = ofproto_get_port(ofproto, port_no);\n        if (port) {\n            cb(port, &replies);\n        }\n    } else {\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            cb(port, &replies);\n        }\n    }\n\n    ofconn_send_replies(ofconn, &replies);\n}\n\nstatic enum ofperr\nhandle_port_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    ofp_port_t port_no;\n    enum ofperr error;\n\n    error = ofputil_decode_port_stats_request(request, &port_no);\n    if (!error) {\n        handle_port_request(ofconn, request, port_no, append_port_stat);\n    }\n    return error;\n}\n\nstatic void\nappend_port_desc(struct ofport *port, struct ovs_list *replies)\n{\n    ofputil_append_port_desc_stats_reply(&port->pp, replies);\n}\n\nstatic enum ofperr\nhandle_port_desc_stats_request(struct ofconn *ofconn,\n                               const struct ofp_header *request)\n{\n    ofp_port_t port_no;\n    enum ofperr error;\n\n    error = ofputil_decode_port_desc_stats_request(request, &port_no);\n    if (!error) {\n        handle_port_request(ofconn, request, port_no, append_port_desc);\n    }\n    return error;\n}\n\nstatic uint32_t\nhash_cookie(ovs_be64 cookie)\n{\n    return hash_uint64((OVS_FORCE uint64_t)cookie);\n}\n\nstatic void\ncookies_insert(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    hindex_insert(&ofproto->cookies, &rule->cookie_node,\n                  hash_cookie(rule->flow_cookie));\n}\n\nstatic void\ncookies_remove(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    hindex_remove(&ofproto->cookies, &rule->cookie_node);\n}\n\nstatic void\ncalc_duration(long long int start, long long int now,\n              uint32_t *sec, uint32_t *nsec)\n{\n    long long int msecs = now - start;\n    *sec = msecs / 1000;\n    *nsec = (msecs % 1000) * (1000 * 1000);\n}\n\n/* Checks whether 'table_id' is 0xff or a valid table ID in 'ofproto'.  Returns\n * true if 'table_id' is OK, false otherwise.  */\nstatic bool\ncheck_table_id(const struct ofproto *ofproto, uint8_t table_id)\n{\n    return table_id == OFPTT_ALL || table_id < ofproto->n_tables;\n}\n\nstatic struct oftable *\nnext_visible_table(const struct ofproto *ofproto, uint8_t table_id)\n{\n    struct oftable *table;\n\n    for (table = &ofproto->tables[table_id];\n         table < &ofproto->tables[ofproto->n_tables];\n         table++) {\n        if (!(table->flags & OFTABLE_HIDDEN)) {\n            return table;\n        }\n    }\n\n    return NULL;\n}\n\nstatic struct oftable *\nfirst_matching_table(const struct ofproto *ofproto, uint8_t table_id)\n{\n    if (table_id == 0xff) {\n        return next_visible_table(ofproto, 0);\n    } else if (table_id < ofproto->n_tables) {\n        return &ofproto->tables[table_id];\n    } else {\n        return NULL;\n    }\n}\n\nstatic struct oftable *\nnext_matching_table(const struct ofproto *ofproto,\n                    const struct oftable *table, uint8_t table_id)\n{\n    return (table_id == 0xff\n            ? next_visible_table(ofproto, (table - ofproto->tables) + 1)\n            : NULL);\n}\n\n/* Assigns TABLE to each oftable, in turn, that matches TABLE_ID in OFPROTO:\n *\n *   - If TABLE_ID is 0xff, this iterates over every classifier table in\n *     OFPROTO, skipping tables marked OFTABLE_HIDDEN.\n *\n *   - If TABLE_ID is the number of a table in OFPROTO, then the loop iterates\n *     only once, for that table.  (This can be used to access tables marked\n *     OFTABLE_HIDDEN.)\n *\n *   - Otherwise, TABLE_ID isn't valid for OFPROTO, so the loop won't be\n *     entered at all.  (Perhaps you should have validated TABLE_ID with\n *     check_table_id().)\n *\n * All parameters are evaluated multiple times.\n */\n#define FOR_EACH_MATCHING_TABLE(TABLE, TABLE_ID, OFPROTO)         \\\n    for ((TABLE) = first_matching_table(OFPROTO, TABLE_ID);       \\\n         (TABLE) != NULL;                                         \\\n         (TABLE) = next_matching_table(OFPROTO, TABLE, TABLE_ID))\n\n/* Initializes 'criteria' in a straightforward way based on the other\n * parameters.\n *\n * By default, the criteria include flows that are read-only, on the assumption\n * that the collected flows won't be modified.  Call rule_criteria_require_rw()\n * if flows will be modified.\n *\n * For \"loose\" matching, the 'priority' parameter is unimportant and may be\n * supplied as 0. */\nstatic void\nrule_criteria_init(struct rule_criteria *criteria, uint8_t table_id,\n                   const struct match *match, int priority,\n                   ovs_version_t version, ovs_be64 cookie,\n                   ovs_be64 cookie_mask, ofp_port_t out_port,\n                   uint32_t out_group)\n{\n    criteria->table_id = table_id;\n    cls_rule_init(&criteria->cr, match, priority);\n    criteria->version = version;\n    criteria->cookie = cookie;\n    criteria->cookie_mask = cookie_mask;\n    criteria->out_port = out_port;\n    criteria->out_group = out_group;\n\n    /* We ordinarily want to skip hidden rules, but there has to be a way for\n     * code internal to OVS to modify and delete them, so if the criteria\n     * specify a priority that can only be for a hidden flow, then allow hidden\n     * rules to be selected.  (This doesn't allow OpenFlow clients to meddle\n     * with hidden flows because OpenFlow uses only a 16-bit field to specify\n     * priority.) */\n    criteria->include_hidden = priority > UINT16_MAX;\n\n    /* We assume that the criteria are being used to collect flows for reading\n     * but not modification.  Thus, we should collect read-only flows. */\n    criteria->include_readonly = true;\n}\n\n/* By default, criteria initialized by rule_criteria_init() will match flows\n * that are read-only, on the assumption that the collected flows won't be\n * modified.  Call this function to match only flows that are be modifiable.\n *\n * Specify 'can_write_readonly' as false in ordinary circumstances, true if the\n * caller has special privileges that allow it to modify even \"read-only\"\n * flows. */\nstatic void\nrule_criteria_require_rw(struct rule_criteria *criteria,\n                         bool can_write_readonly)\n{\n    criteria->include_readonly = can_write_readonly;\n}\n\nstatic void\nrule_criteria_destroy(struct rule_criteria *criteria)\n{\n    cls_rule_destroy(&criteria->cr);\n    criteria->version = OVS_VERSION_NOT_REMOVED; /* Mark as destroyed. */\n}\n\n/* Schedules postponed removal of rules, destroys 'rules'. */\nstatic void\nremove_rules_postponed(struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules) > 0) {\n        if (rule_collection_n(rules) == 1) {\n            ovsrcu_postpone(remove_rule_rcu, rule_collection_rules(rules)[0]);\n            rule_collection_init(rules);\n        } else {\n            ovsrcu_postpone(remove_rules_rcu, rule_collection_detach(rules));\n        }\n    }\n}\n\n/* Schedules postponed removal of groups, destroys 'groups'. */\nstatic void\nremove_groups_postponed(struct group_collection *groups)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (group_collection_n(groups) > 0) {\n        if (group_collection_n(groups) == 1) {\n            ovsrcu_postpone(remove_group_rcu,\n                            group_collection_groups(groups)[0]);\n            group_collection_init(groups);\n        } else {\n            ovsrcu_postpone(remove_groups_rcu,\n                            group_collection_detach(groups));\n        }\n    }\n}\n\n/* Checks whether 'rule' matches 'c' and, if so, adds it to 'rules'.  This\n * function verifies most of the criteria in 'c' itself, but the caller must\n * check 'c->cr' itself.\n *\n * Rules that have already been marked for removal are not collected.\n *\n * Increments '*n_readonly' if 'rule' wasn't added because it's read-only (and\n * 'c' only includes modifiable rules). */\nstatic void\ncollect_rule(struct rule *rule, const struct rule_criteria *c,\n             struct rule_collection *rules, size_t *n_readonly)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if ((c->table_id == rule->table_id || c->table_id == 0xff)\n        && ofproto_rule_has_out_port(rule, c->out_port)\n        && ofproto_rule_has_out_group(rule, c->out_group)\n        && !((rule->flow_cookie ^ c->cookie) & c->cookie_mask)\n        && (!rule_is_hidden(rule) || c->include_hidden)\n        && cls_rule_visible_in_version(&rule->cr, c->version)) {\n        /* Rule matches all the criteria... */\n        if (!rule_is_readonly(rule) || c->include_readonly) {\n            /* ...add it. */\n            rule_collection_add(rules, rule);\n        } else {\n            /* ...except it's read-only. */\n            ++*n_readonly;\n        }\n    }\n}\n\n/* Searches 'ofproto' for rules that match the criteria in 'criteria'.  Matches\n * on classifiers rules are done in the \"loose\" way required for OpenFlow\n * OFPFC_MODIFY and OFPFC_DELETE requests.  Puts the selected rules on list\n * 'rules'.\n *\n * Returns 0 on success, otherwise an OpenFlow error code. */\nstatic enum ofperr\ncollect_rules_loose(struct ofproto *ofproto,\n                    const struct rule_criteria *criteria,\n                    struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table;\n    enum ofperr error = 0;\n    size_t n_readonly = 0;\n\n    rule_collection_init(rules);\n\n    if (!check_table_id(ofproto, criteria->table_id)) {\n        error = OFPERR_OFPBRC_BAD_TABLE_ID;\n        goto exit;\n    }\n\n    if (criteria->cookie_mask == OVS_BE64_MAX) {\n        struct rule *rule;\n\n        HINDEX_FOR_EACH_WITH_HASH (rule, cookie_node,\n                                   hash_cookie(criteria->cookie),\n                                   &ofproto->cookies) {\n            if (cls_rule_is_loose_match(&rule->cr, &criteria->cr.match)) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    } else {\n        FOR_EACH_MATCHING_TABLE (table, criteria->table_id, ofproto) {\n            struct rule *rule;\n\n            CLS_FOR_EACH_TARGET (rule, cr, &table->cls, &criteria->cr,\n                                 criteria->version) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    }\n\nexit:\n    if (!error && !rule_collection_n(rules) && n_readonly) {\n        /* We didn't find any rules to modify.  We did find some read-only\n         * rules that we're not allowed to modify, so report that. */\n        error = OFPERR_OFPBRC_EPERM;\n    }\n    if (error) {\n        rule_collection_destroy(rules);\n    }\n    return error;\n}\n\n/* Searches 'ofproto' for rules that match the criteria in 'criteria'.  Matches\n * on classifiers rules are done in the \"strict\" way required for OpenFlow\n * OFPFC_MODIFY_STRICT and OFPFC_DELETE_STRICT requests.  Puts the selected\n * rules on list 'rules'.\n *\n * Returns 0 on success, otherwise an OpenFlow error code. */\nstatic enum ofperr\ncollect_rules_strict(struct ofproto *ofproto,\n                     const struct rule_criteria *criteria,\n                     struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table;\n    size_t n_readonly = 0;\n    enum ofperr error = 0;\n\n    rule_collection_init(rules);\n\n    if (!check_table_id(ofproto, criteria->table_id)) {\n        error = OFPERR_OFPBRC_BAD_TABLE_ID;\n        goto exit;\n    }\n\n    if (criteria->cookie_mask == OVS_BE64_MAX) {\n        struct rule *rule;\n\n        HINDEX_FOR_EACH_WITH_HASH (rule, cookie_node,\n                                   hash_cookie(criteria->cookie),\n                                   &ofproto->cookies) {\n            if (cls_rule_equal(&rule->cr, &criteria->cr)) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    } else {\n        FOR_EACH_MATCHING_TABLE (table, criteria->table_id, ofproto) {\n            struct rule *rule;\n\n            rule = rule_from_cls_rule(classifier_find_rule_exactly(\n                                          &table->cls, &criteria->cr,\n                                          criteria->version));\n            if (rule) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    }\n\nexit:\n    if (!error && !rule_collection_n(rules) && n_readonly) {\n        /* We didn't find any rules to modify.  We did find some read-only\n         * rules that we're not allowed to modify, so report that. */\n        error = OFPERR_OFPBRC_EPERM;\n    }\n    if (error) {\n        rule_collection_destroy(rules);\n    }\n    return error;\n}\n\n/* Returns 'age_ms' (a duration in milliseconds), converted to seconds and\n * forced into the range of a uint16_t. */\nstatic int\nage_secs(long long int age_ms)\n{\n    return (age_ms < 0 ? 0\n            : age_ms >= UINT16_MAX * 1000 ? UINT16_MAX\n            : (unsigned int) age_ms / 1000);\n}\n\nstatic enum ofperr\nhandle_flow_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_stats_request fsr;\n    struct rule_criteria criteria;\n    struct rule_collection rules;\n    struct ovs_list replies;\n    enum ofperr error;\n\n    error = ofputil_decode_flow_stats_request(&fsr, request,\n                                              ofproto_get_tun_tab(ofproto),\n                                              &ofproto->vl_mff_map);\n    if (error) {\n        return error;\n    }\n\n    rule_criteria_init(&criteria, fsr.table_id, &fsr.match, 0, OVS_VERSION_MAX,\n                       fsr.cookie, fsr.cookie_mask, fsr.out_port,\n                       fsr.out_group);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = collect_rules_loose(ofproto, &criteria, &rules);\n    rule_criteria_destroy(&criteria);\n    if (!error) {\n        rule_collection_ref(&rules);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    if (error) {\n        return error;\n    }\n\n    ofpmp_init(&replies, request);\n    struct rule *rule;\n    RULE_COLLECTION_FOR_EACH (rule, &rules) {\n        long long int now = time_msec();\n        struct ofputil_flow_stats fs;\n        long long int created, used, modified;\n        const struct rule_actions *actions;\n        enum ofputil_flow_mod_flags flags;\n\n        ovs_mutex_lock(&rule->mutex);\n        fs.cookie = rule->flow_cookie;\n        fs.idle_timeout = rule->idle_timeout;\n        fs.hard_timeout = rule->hard_timeout;\n        fs.importance = rule->importance;\n        created = rule->created;\n        modified = rule->modified;\n        actions = rule_get_actions(rule);\n        flags = rule->flags;\n        ovs_mutex_unlock(&rule->mutex);\n\n        ofproto->ofproto_class->rule_get_stats(rule, &fs.packet_count,\n                                               &fs.byte_count, &used);\n\n        minimatch_expand(&rule->cr.match, &fs.match);\n        fs.table_id = rule->table_id;\n        calc_duration(created, now, &fs.duration_sec, &fs.duration_nsec);\n        fs.priority = rule->cr.priority;\n        fs.idle_age = age_secs(now - used);\n        fs.hard_age = age_secs(now - modified);\n        fs.ofpacts = actions->ofpacts;\n        fs.ofpacts_len = actions->ofpacts_len;\n\n        fs.flags = flags;\n        ofputil_append_flow_stats_reply(&fs, &replies,\n                                        ofproto_get_tun_tab(ofproto));\n    }\n\n    rule_collection_unref(&rules);\n    rule_collection_destroy(&rules);\n\n    ofconn_send_replies(ofconn, &replies);\n\n    return 0;\n}\n\nstatic void\nflow_stats_ds(struct ofproto *ofproto, struct rule *rule, struct ds *results)\n{\n    uint64_t packet_count, byte_count;\n    const struct rule_actions *actions;\n    long long int created, used;\n\n    rule->ofproto->ofproto_class->rule_get_stats(rule, &packet_count,\n                                                 &byte_count, &used);\n\n    ovs_mutex_lock(&rule->mutex);\n    actions = rule_get_actions(rule);\n    created = rule->created;\n    ovs_mutex_unlock(&rule->mutex);\n\n    if (rule->table_id != 0) {\n        ds_put_format(results, \"table_id=%\"PRIu8\", \", rule->table_id);\n    }\n    ds_put_format(results, \"duration=%llds, \", (time_msec() - created) / 1000);\n    ds_put_format(results, \"n_packets=%\"PRIu64\", \", packet_count);\n    ds_put_format(results, \"n_bytes=%\"PRIu64\", \", byte_count);\n    cls_rule_format(&rule->cr, ofproto_get_tun_tab(ofproto), results);\n    ds_put_char(results, ',');\n\n    ds_put_cstr(results, \"actions=\");\n    ofpacts_format(actions->ofpacts, actions->ofpacts_len, results);\n\n    ds_put_cstr(results, \"\\n\");\n}\n\n/* Adds a pretty-printed description of all flows to 'results', including\n * hidden flows (e.g., set up by in-band control). */\nvoid\nofproto_get_all_flows(struct ofproto *p, struct ds *results)\n{\n    struct oftable *table;\n\n    OFPROTO_FOR_EACH_TABLE (table, p) {\n        struct rule *rule;\n\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            flow_stats_ds(p, rule, results);\n        }\n    }\n}\n\n/* Obtains the NetFlow engine type and engine ID for 'ofproto' into\n * '*engine_type' and '*engine_id', respectively. */\nvoid\nofproto_get_netflow_ids(const struct ofproto *ofproto,\n                        uint8_t *engine_type, uint8_t *engine_id)\n{\n    ofproto->ofproto_class->get_netflow_ids(ofproto, engine_type, engine_id);\n}\n\n/* Checks the status change of CFM on 'ofport'.\n *\n * Returns true if 'ofproto_class' does not support 'cfm_status_changed'. */\nbool\nofproto_port_cfm_status_changed(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->cfm_status_changed\n            ? ofproto->ofproto_class->cfm_status_changed(ofport)\n            : true);\n}\n\n/* Checks the status of CFM configured on 'ofp_port' within 'ofproto'.\n * Returns 0 if the port's CFM status was successfully stored into\n * '*status'.  Returns positive errno if the port did not have CFM\n * configured.\n *\n * The caller must provide and own '*status', and must free 'status->rmps'.\n * '*status' is indeterminate if the return value is non-zero. */\nint\nofproto_port_get_cfm_status(const struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct cfm_status *status)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->get_cfm_status\n            ? ofproto->ofproto_class->get_cfm_status(ofport, status)\n            : EOPNOTSUPP);\n}\n\nstatic enum ofperr\nhandle_aggregate_stats_request(struct ofconn *ofconn,\n                               const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_stats_request request;\n    struct ofputil_aggregate_stats stats;\n    bool unknown_packets, unknown_bytes;\n    struct rule_criteria criteria;\n    struct rule_collection rules;\n    struct ofpbuf *reply;\n    enum ofperr error;\n\n    error = ofputil_decode_flow_stats_request(&request, oh,\n                                              ofproto_get_tun_tab(ofproto),\n                                              &ofproto->vl_mff_map);\n    if (error) {\n        return error;\n    }\n\n    rule_criteria_init(&criteria, request.table_id, &request.match, 0,\n                       OVS_VERSION_MAX, request.cookie, request.cookie_mask,\n                       request.out_port, request.out_group);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = collect_rules_loose(ofproto, &criteria, &rules);\n    rule_criteria_destroy(&criteria);\n    if (!error) {\n        rule_collection_ref(&rules);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    if (error) {\n        return error;\n    }\n\n    memset(&stats, 0, sizeof stats);\n    unknown_packets = unknown_bytes = false;\n\n    struct rule *rule;\n    RULE_COLLECTION_FOR_EACH (rule, &rules) {\n        uint64_t packet_count;\n        uint64_t byte_count;\n        long long int used;\n\n        ofproto->ofproto_class->rule_get_stats(rule, &packet_count,\n                                               &byte_count, &used);\n\n        if (packet_count == UINT64_MAX) {\n            unknown_packets = true;\n        } else {\n            stats.packet_count += packet_count;\n        }\n\n        if (byte_count == UINT64_MAX) {\n            unknown_bytes = true;\n        } else {\n            stats.byte_count += byte_count;\n        }\n\n        stats.flow_count++;\n    }\n    if (unknown_packets) {\n        stats.packet_count = UINT64_MAX;\n    }\n    if (unknown_bytes) {\n        stats.byte_count = UINT64_MAX;\n    }\n\n    rule_collection_unref(&rules);\n    rule_collection_destroy(&rules);\n\n    reply = ofputil_encode_aggregate_stats_reply(&stats, oh);\n    ofconn_send_reply(ofconn, reply);\n\n    return 0;\n}\n\nstruct queue_stats_cbdata {\n    struct ofport *ofport;\n    struct ovs_list replies;\n    long long int now;\n};\n\nstatic void\nput_queue_stats(struct queue_stats_cbdata *cbdata, uint32_t queue_id,\n                const struct netdev_queue_stats *stats)\n{\n    struct ofputil_queue_stats oqs;\n\n    oqs.port_no = cbdata->ofport->pp.port_no;\n    oqs.queue_id = queue_id;\n    oqs.tx_bytes = stats->tx_bytes;\n    oqs.tx_packets = stats->tx_packets;\n    oqs.tx_errors = stats->tx_errors;\n    if (stats->created != LLONG_MIN) {\n        calc_duration(stats->created, cbdata->now,\n                      &oqs.duration_sec, &oqs.duration_nsec);\n    } else {\n        oqs.duration_sec = oqs.duration_nsec = UINT32_MAX;\n    }\n    ofputil_append_queue_stat(&cbdata->replies, &oqs);\n}\n\nstatic void\nhandle_queue_stats_dump_cb(uint32_t queue_id,\n                           struct netdev_queue_stats *stats,\n                           void *cbdata_)\n{\n    struct queue_stats_cbdata *cbdata = cbdata_;\n\n    put_queue_stats(cbdata, queue_id, stats);\n}\n\nstatic enum ofperr\nhandle_queue_stats_for_port(struct ofport *port, uint32_t queue_id,\n                            struct queue_stats_cbdata *cbdata)\n{\n    cbdata->ofport = port;\n    if (queue_id == OFPQ_ALL) {\n        netdev_dump_queue_stats(port->netdev,\n                                handle_queue_stats_dump_cb, cbdata);\n    } else {\n        struct netdev_queue_stats stats;\n\n        if (!netdev_get_queue_stats(port->netdev, queue_id, &stats)) {\n            put_queue_stats(cbdata, queue_id, &stats);\n        } else {\n            return OFPERR_OFPQOFC_BAD_QUEUE;\n        }\n    }\n    return 0;\n}\n\nstatic enum ofperr\nhandle_queue_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *rq)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct queue_stats_cbdata cbdata;\n    struct ofport *port;\n    enum ofperr error;\n    struct ofputil_queue_stats_request oqsr;\n\n    COVERAGE_INC(ofproto_queue_req);\n\n    ofpmp_init(&cbdata.replies, rq);\n    cbdata.now = time_msec();\n\n    error = ofputil_decode_queue_stats_request(rq, &oqsr);\n    if (error) {\n        return error;\n    }\n\n    if (oqsr.port_no == OFPP_ANY) {\n        error = OFPERR_OFPQOFC_BAD_QUEUE;\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            if (!handle_queue_stats_for_port(port, oqsr.queue_id, &cbdata)) {\n                error = 0;\n            }\n        }\n    } else {\n        port = ofproto_get_port(ofproto, oqsr.port_no);\n        error = (port\n                 ? handle_queue_stats_for_port(port, oqsr.queue_id, &cbdata)\n                 : OFPERR_OFPQOFC_BAD_PORT);\n    }\n    if (!error) {\n        ofconn_send_replies(ofconn, &cbdata.replies);\n    } else {\n        ofpbuf_list_delete(&cbdata.replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nevict_rules_from_table(struct oftable *table)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error = 0;\n    struct rule_collection rules;\n    unsigned int count = table->n_flows;\n    unsigned int max_flows = table->max_flows;\n\n    rule_collection_init(&rules);\n\n    while (count-- > max_flows) {\n        struct rule *rule;\n\n        if (!choose_rule_to_evict(table, &rule)) {\n            error = OFPERR_OFPFMFC_TABLE_FULL;\n            break;\n        } else {\n            eviction_group_remove_rule(rule);\n            rule_collection_add(&rules, rule);\n        }\n    }\n    delete_flows__(&rules, OFPRR_EVICTION, NULL);\n\n    return error;\n}\n\nstatic void\nget_conjunctions(const struct ofputil_flow_mod *fm,\n                 struct cls_conjunction **conjsp, size_t *n_conjsp)\n{\n    struct cls_conjunction *conjs = NULL;\n    int n_conjs = 0;\n\n    const struct ofpact *ofpact;\n    OFPACT_FOR_EACH (ofpact, fm->ofpacts, fm->ofpacts_len) {\n        if (ofpact->type == OFPACT_CONJUNCTION) {\n            n_conjs++;\n        } else if (ofpact->type != OFPACT_NOTE) {\n            /* \"conjunction\" may appear with \"note\" actions but not with any\n             * other type of actions. */\n            ovs_assert(!n_conjs);\n            break;\n        }\n    }\n    if (n_conjs) {\n        int i = 0;\n\n        conjs = xzalloc(n_conjs * sizeof *conjs);\n        OFPACT_FOR_EACH (ofpact, fm->ofpacts, fm->ofpacts_len) {\n            if (ofpact->type == OFPACT_CONJUNCTION) {\n                struct ofpact_conjunction *oc = ofpact_get_CONJUNCTION(ofpact);\n                conjs[i].clause = oc->clause;\n                conjs[i].n_clauses = oc->n_clauses;\n                conjs[i].id = oc->id;\n                i++;\n            }\n        }\n    }\n\n    *conjsp = conjs;\n    *n_conjsp = n_conjs;\n}\n\n/* add_flow_init(), add_flow_start(), add_flow_revert(), and add_flow_finish()\n * implement OFPFC_ADD and the cases for OFPFC_MODIFY and OFPFC_MODIFY_STRICT\n * in which no matching flow already exists in the flow table.\n *\n * add_flow_init() creates a new flow according to 'fm' and stores it to 'ofm'\n * for later reference.  If the flow replaces other flow, it will be updated to\n * match modify semantics later by add_flow_start() (by calling\n * replace_rule_start()).\n *\n * Returns 0 on success, or an OpenFlow error code on failure.\n *\n * On successful return the caller must complete the operation by calling\n * add_flow_start(), and if that succeeds, then either add_flow_finish(), or\n * add_flow_revert() if the operation needs to be reverted due to a later\n * failure.\n */\nstatic enum ofperr\nadd_flow_init(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n              const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n    struct cls_rule cr;\n    uint8_t table_id;\n    enum ofperr error;\n\n    if (!check_table_id(ofproto, fm->table_id)) {\n        return OFPERR_OFPBRC_BAD_TABLE_ID;\n    }\n\n    /* Pick table. */\n    if (fm->table_id == 0xff) {\n        if (ofproto->ofproto_class->rule_choose_table) {\n            error = ofproto->ofproto_class->rule_choose_table(ofproto,\n                                                              &fm->match,\n                                                              &table_id);\n            if (error) {\n                return error;\n            }\n            ovs_assert(table_id < ofproto->n_tables);\n        } else {\n            table_id = 0;\n        }\n    } else if (fm->table_id < ofproto->n_tables) {\n        table_id = fm->table_id;\n    } else {\n        return OFPERR_OFPBRC_BAD_TABLE_ID;\n    }\n\n    table = &ofproto->tables[table_id];\n    if (table->flags & OFTABLE_READONLY\n        && !(fm->flags & OFPUTIL_FF_NO_READONLY)) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    if (!(fm->flags & OFPUTIL_FF_HIDDEN_FIELDS)\n        && !match_has_default_hidden_fields(&fm->match)) {\n        VLOG_WARN_RL(&rl, \"%s: (add_flow) only internal flows can set \"\n                     \"non-default values to hidden fields\", ofproto->name);\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    if (!ofm->temp_rule) {\n        cls_rule_init(&cr, &fm->match, fm->priority);\n\n        /* Allocate new rule.  Destroys 'cr'. */\n        error = ofproto_rule_create(ofproto, &cr, table - ofproto->tables,\n                                    fm->new_cookie, fm->idle_timeout,\n                                    fm->hard_timeout, fm->flags,\n                                    fm->importance, fm->ofpacts,\n                                    fm->ofpacts_len,\n                                    fm->match.flow.tunnel.metadata.present.map,\n                                    fm->ofpacts_tlv_bitmap, &ofm->temp_rule);\n        if (error) {\n            return error;\n        }\n\n        get_conjunctions(fm, &ofm->conjs, &ofm->n_conjs);\n    }\n    return 0;\n}\n\n/* ofm->temp_rule is consumed only in the successful case. */\nstatic enum ofperr\nadd_flow_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = NULL;\n    struct rule *new_rule = ofm->temp_rule;\n    const struct rule_actions *actions = rule_get_actions(new_rule);\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n    enum ofperr error;\n\n    /* Must check actions while holding ofproto_mutex to avoid a race. */\n    error = ofproto_check_ofpacts(ofproto, actions->ofpacts,\n                                  actions->ofpacts_len);\n    if (error) {\n        return error;\n    }\n\n    /* Check for the existence of an identical rule.\n     * This will not return rules earlier marked for removal. */\n    old_rule = rule_from_cls_rule(classifier_find_rule_exactly(&table->cls,\n                                                               &new_rule->cr,\n                                                               ofm->version));\n    if (!old_rule) {\n        /* Check for overlap, if requested. */\n        if (new_rule->flags & OFPUTIL_FF_CHECK_OVERLAP\n            && classifier_rule_overlaps(&table->cls, &new_rule->cr,\n                                        ofm->version)) {\n            return OFPERR_OFPFMFC_OVERLAP;\n        }\n\n        /* If necessary, evict an existing rule to clear out space. */\n        if (table->n_flows >= table->max_flows) {\n            if (!choose_rule_to_evict(table, &old_rule)) {\n                return OFPERR_OFPFMFC_TABLE_FULL;\n            }\n            eviction_group_remove_rule(old_rule);\n            /* Marks 'old_rule' as an evicted rule rather than replaced rule.\n             */\n            old_rule->removed_reason = OFPRR_EVICTION;\n        }\n    } else {\n        ofm->modify_cookie = true;\n    }\n\n    if (old_rule) {\n        rule_collection_add(&ofm->old_rules, old_rule);\n    }\n    /* Take ownership of the temp_rule. */\n    rule_collection_add(&ofm->new_rules, new_rule);\n    ofm->temp_rule = NULL;\n\n    replace_rule_start(ofproto, ofm, old_rule, new_rule);\n    return 0;\n}\n\n/* Revert the effects of add_flow_start(). */\nstatic void\nadd_flow_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = rule_collection_n(&ofm->old_rules)\n        ? rule_collection_rules(&ofm->old_rules)[0] : NULL;\n    struct rule *new_rule = rule_collection_rules(&ofm->new_rules)[0];\n\n    replace_rule_revert(ofproto, old_rule, new_rule);\n}\n\n/* To be called after version bump. */\nstatic void\nadd_flow_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = rule_collection_n(&ofm->old_rules)\n        ? rule_collection_rules(&ofm->old_rules)[0] : NULL;\n    struct rule *new_rule = rule_collection_rules(&ofm->new_rules)[0];\n    struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n\n    replace_rule_finish(ofproto, ofm, req, old_rule, new_rule, &dead_cookies);\n    learned_cookies_flush(ofproto, &dead_cookies);\n\n    if (old_rule) {\n        ovsrcu_postpone(remove_rule_rcu, old_rule);\n    } else {\n        ofmonitor_report(ofproto->connmgr, new_rule, NXFME_ADDED, 0,\n                         req ? req->ofconn : NULL,\n                         req ? req->request->xid : 0, NULL);\n\n        /* Send Vacancy Events for OF1.4+. */\n        send_table_status(ofproto, new_rule->table_id);\n    }\n}\n\f\n/* OFPFC_MODIFY and OFPFC_MODIFY_STRICT. */\n\n/* Create a new rule.  Note that the rule is NOT inserted into a any data\n * structures yet.  Takes ownership of 'cr'.  Only assigns '*new_rule' if\n * successful. */\nstatic enum ofperr\nofproto_rule_create(struct ofproto *ofproto, struct cls_rule *cr,\n                    uint8_t table_id, ovs_be64 new_cookie,\n                    uint16_t idle_timeout, uint16_t hard_timeout,\n                    enum ofputil_flow_mod_flags flags, uint16_t importance,\n                    const struct ofpact *ofpacts, size_t ofpacts_len,\n                    uint64_t match_tlv_bitmap, uint64_t ofpacts_tlv_bitmap,\n                    struct rule **new_rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    struct rule *rule;\n    enum ofperr error;\n\n    /* Allocate new rule. */\n    rule = ofproto->ofproto_class->rule_alloc();\n    if (!rule) {\n        cls_rule_destroy(cr);\n        VLOG_WARN_RL(&rl, \"%s: failed to allocate a rule.\", ofproto->name);\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Initialize base state. */\n    *CONST_CAST(struct ofproto **, &rule->ofproto) = ofproto;\n    cls_rule_move(CONST_CAST(struct cls_rule *, &rule->cr), cr);\n    ovs_refcount_init(&rule->ref_count);\n\n    ovs_mutex_init(&rule->mutex);\n    ovs_mutex_lock(&rule->mutex);\n    *CONST_CAST(ovs_be64 *, &rule->flow_cookie) = new_cookie;\n    rule->created = rule->modified = time_msec();\n    rule->idle_timeout = idle_timeout;\n    rule->hard_timeout = hard_timeout;\n    *CONST_CAST(uint16_t *, &rule->importance) = importance;\n    rule->removed_reason = OVS_OFPRR_NONE;\n\n    *CONST_CAST(uint8_t *, &rule->table_id) = table_id;\n    rule->flags = flags & OFPUTIL_FF_STATE;\n\n    *CONST_CAST(const struct rule_actions **, &rule->actions)\n        = rule_actions_create(ofpacts, ofpacts_len);\n\n    ovs_list_init(&rule->meter_list_node);\n    rule->eviction_group = NULL;\n    rule->monitor_flags = 0;\n    rule->add_seqno = 0;\n    rule->modify_seqno = 0;\n    ovs_list_init(&rule->expirable);\n    ovs_mutex_unlock(&rule->mutex);\n\n    /* Construct rule, initializing derived state. */\n    error = ofproto->ofproto_class->rule_construct(rule);\n    if (error) {\n        ofproto_rule_destroy__(rule);\n        return error;\n    }\n\n    rule->state = RULE_INITIALIZED;\n    rule->match_tlv_bitmap = match_tlv_bitmap;\n    rule->ofpacts_tlv_bitmap = ofpacts_tlv_bitmap;\n    mf_vl_mff_ref(&rule->ofproto->vl_mff_map, match_tlv_bitmap);\n    mf_vl_mff_ref(&rule->ofproto->vl_mff_map, ofpacts_tlv_bitmap);\n\n    *new_rule = rule;\n    return 0;\n}\n\n/* Initialize 'ofm' for a learn action.  If the rule already existed, reference\n * to that rule is taken, otherwise a new rule is created.  'ofm' keeps the\n * rule reference in both.  This does not take the global 'ofproto_mutex'. */\nenum ofperr\nofproto_flow_mod_init_for_learn(struct ofproto *ofproto,\n                                const struct ofputil_flow_mod *fm,\n                                struct ofproto_flow_mod *ofm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    /* Reject flow mods that do not look like they were generated by a learn\n     * action. */\n    if (fm->command != OFPFC_MODIFY_STRICT || fm->table_id == OFPTT_ALL\n        || fm->flags & OFPUTIL_FF_RESET_COUNTS\n        || fm->buffer_id != UINT32_MAX) {\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Check if the rule already exists, and we can get a reference to it. */\n    struct oftable *table = &ofproto->tables[fm->table_id];\n    struct rule *rule;\n\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  &table->cls, &fm->match, fm->priority,\n                                  OVS_VERSION_MAX));\n    if (rule) {\n        /* Check if the rule's attributes match as well. */\n        const struct rule_actions *actions;\n\n        ovs_mutex_lock(&rule->mutex);\n        actions = rule_get_actions(rule);\n        if (rule->idle_timeout == fm->idle_timeout\n            && rule->hard_timeout == fm->hard_timeout\n            && rule->importance == fm->importance\n            && rule->flags == (fm->flags & OFPUTIL_FF_STATE)\n            && (!fm->modify_cookie || (fm->new_cookie == rule->flow_cookie))\n            && ofpacts_equal(fm->ofpacts, fm->ofpacts_len,\n                             actions->ofpacts, actions->ofpacts_len)) {\n            /* Rule already exists and need not change, except for the modified\n             * timestamp.  Get a reference to the existing rule. */\n            ovs_mutex_unlock(&rule->mutex);\n            if (!ofproto_rule_try_ref(rule)) {\n                rule = NULL; /* Pretend it did not exist. */\n            }\n        } else {\n            ovs_mutex_unlock(&rule->mutex);\n            rule = NULL;\n        }\n    }\n\n    return ofproto_flow_mod_init(ofproto, ofm, fm, rule);\n}\n\nenum ofperr\nofproto_flow_mod_learn_refresh(struct ofproto_flow_mod *ofm)\n{\n    enum ofperr error = 0;\n\n    /* ofm->temp_rule is our reference to the learned rule.  We have a\n     * reference to an existing rule, if it already was in the classifier,\n     * otherwise we may have a fresh rule that we need to insert. */\n    struct rule *rule = ofm->temp_rule;\n    if (!rule) {\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Create a new rule if the current one has been removed from the\n     * classifier.  We need to do this since RCU does not allow a current rule\n     * to be reinserted before all threads have quiesced.\n     *\n     * It is possible that the rule is removed asynchronously, e.g., right\n     * after we have read the 'rule->state' below.  In this case the next time\n     * this function is executed the rule will be reinstated. */\n    if (rule->state == RULE_REMOVED) {\n        struct cls_rule cr;\n\n        cls_rule_clone(&cr, &rule->cr);\n        ovs_mutex_lock(&rule->mutex);\n        error = ofproto_rule_create(rule->ofproto, &cr, rule->table_id,\n                                    rule->flow_cookie,\n                                    rule->idle_timeout,\n                                    rule->hard_timeout, rule->flags,\n                                    rule->importance,\n                                    rule->actions->ofpacts,\n                                    rule->actions->ofpacts_len,\n                                    rule->match_tlv_bitmap,\n                                    rule->ofpacts_tlv_bitmap,\n                                    &ofm->temp_rule);\n        ovs_mutex_unlock(&rule->mutex);\n        if (!error) {\n            ofproto_rule_unref(rule);   /* Release old reference. */\n        }\n    } else {\n        /* Refresh the existing rule. */\n        ovs_mutex_lock(&rule->mutex);\n        rule->modified = time_msec();\n        ovs_mutex_unlock(&rule->mutex);\n    }\n    return error;\n}\n\nenum ofperr\nofproto_flow_mod_learn_start(struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = ofm->temp_rule;\n\n    /* ofproto_flow_mod_start() consumes the reference, so we\n     * take a new one. */\n    ofproto_rule_ref(rule);\n    enum ofperr error = ofproto_flow_mod_start(rule->ofproto, ofm);\n    ofm->temp_rule = rule;\n\n    return error;\n}\n\nvoid\nofproto_flow_mod_learn_revert(struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = rule_collection_rules(&ofm->new_rules)[0];\n    ofproto_flow_mod_revert(rule->ofproto, ofm);\n}\n\nvoid\nofproto_flow_mod_learn_finish(struct ofproto_flow_mod *ofm,\n                              struct ofproto *orig_ofproto)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = rule_collection_rules(&ofm->new_rules)[0];\n\n    /* If learning on a different bridge, must bump its version\n     * number and flush connmgr afterwards. */\n    if (rule->ofproto != orig_ofproto) {\n        ofproto_bump_tables_version(rule->ofproto);\n    }\n    ofproto_flow_mod_finish(rule->ofproto, ofm, NULL);\n    if (rule->ofproto != orig_ofproto) {\n        ofmonitor_flush(rule->ofproto->connmgr);\n    }\n}\n\n/* Refresh 'ofm->temp_rule', for which the caller holds a reference, if already\n * in the classifier, insert it otherwise.  If the rule has already been\n * removed from the classifier, a new rule is created using 'ofm->temp_rule' as\n * a template and the reference to the old 'ofm->temp_rule' is freed.  If\n * 'keep_ref' is true, then a reference to the current rule is held, otherwise\n * it is released and 'ofm->temp_rule' is set to NULL.\n *\n * Caller needs to be the exclusive owner of 'ofm' as it is being manipulated\n * during the call. */\nenum ofperr\nofproto_flow_mod_learn(struct ofproto_flow_mod *ofm, bool keep_ref)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error = ofproto_flow_mod_learn_refresh(ofm);\n    struct rule *rule = ofm->temp_rule;\n\n    /* Do we need to insert the rule? */\n    if (!error && rule->state == RULE_INITIALIZED) {\n        ovs_mutex_lock(&ofproto_mutex);\n        ofm->version = rule->ofproto->tables_version + 1;\n        error = ofproto_flow_mod_learn_start(ofm);\n        if (!error) {\n            ofproto_flow_mod_learn_finish(ofm, NULL);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    if (!keep_ref) {\n        ofproto_rule_unref(rule);\n        ofm->temp_rule = NULL;\n    }\n    return error;\n}\n\nstatic void\nreplace_rule_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                   struct rule *old_rule, struct rule *new_rule)\n{\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n\n    /* 'old_rule' may be either an evicted rule or replaced rule. */\n    if (old_rule) {\n        /* Copy values from old rule for modify semantics. */\n        if (old_rule->removed_reason != OFPRR_EVICTION) {\n            bool change_cookie = (ofm->modify_cookie\n                                  && new_rule->flow_cookie != OVS_BE64_MAX\n                                  && new_rule->flow_cookie != old_rule->flow_cookie);\n\n            ovs_mutex_lock(&new_rule->mutex);\n            ovs_mutex_lock(&old_rule->mutex);\n            if (ofm->command != OFPFC_ADD) {\n                new_rule->idle_timeout = old_rule->idle_timeout;\n                new_rule->hard_timeout = old_rule->hard_timeout;\n                *CONST_CAST(uint16_t *, &new_rule->importance) = old_rule->importance;\n                new_rule->flags = old_rule->flags;\n                new_rule->created = old_rule->created;\n            }\n            if (!change_cookie) {\n                *CONST_CAST(ovs_be64 *, &new_rule->flow_cookie)\n                    = old_rule->flow_cookie;\n            }\n            ovs_mutex_unlock(&old_rule->mutex);\n            ovs_mutex_unlock(&new_rule->mutex);\n        }\n\n        /* Mark the old rule for removal in the next version. */\n        cls_rule_make_invisible_in_version(&old_rule->cr, ofm->version);\n\n        /* Remove the old rule from data structures. */\n        ofproto_rule_remove__(ofproto, old_rule);\n    } else {\n        table->n_flows++;\n    }\n    /* Insert flow to ofproto data structures, so that later flow_mods may\n     * relate to it.  This is reversible, in case later errors require this to\n     * be reverted. */\n    ofproto_rule_insert__(ofproto, new_rule);\n    /* Make the new rule visible for classifier lookups only from the next\n     * version. */\n    classifier_insert(&table->cls, &new_rule->cr, ofm->version, ofm->conjs,\n                      ofm->n_conjs);\n}\n\nstatic void\nreplace_rule_revert(struct ofproto *ofproto,\n                    struct rule *old_rule, struct rule *new_rule)\n{\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n\n    if (old_rule) {\n        if (old_rule->removed_reason == OFPRR_EVICTION) {\n            /* Revert the eviction. */\n            eviction_group_add_rule(old_rule);\n        }\n\n        /* Restore the old rule to data structures. */\n        ofproto_rule_insert__(ofproto, old_rule);\n\n        /* Restore the original visibility of the old rule. */\n        cls_rule_restore_visibility(&old_rule->cr);\n    } else {\n        /* Restore table's rule count. */\n        table->n_flows--;\n    }\n\n    /* Remove the new rule immediately.  It was never visible to lookups. */\n    if (!classifier_remove(&table->cls, &new_rule->cr)) {\n        OVS_NOT_REACHED();\n    }\n    ofproto_rule_remove__(ofproto, new_rule);\n    ofproto_rule_unref(new_rule);\n}\n\n/* Adds the 'new_rule', replacing the 'old_rule'. */\nstatic void\nreplace_rule_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req,\n                    struct rule *old_rule, struct rule *new_rule,\n                    struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *replaced_rule;\n\n    replaced_rule = (old_rule && old_rule->removed_reason != OFPRR_EVICTION)\n        ? old_rule : NULL;\n\n    /* Insert the new flow to the ofproto provider.  A non-NULL 'replaced_rule'\n     * is a duplicate rule the 'new_rule' is replacing.  The provider should\n     * link the packet and byte counts from the old rule to the new one if\n     * 'modify_keep_counts' is 'true'.  The 'replaced_rule' will be deleted\n     * right after this call. */\n    ofproto->ofproto_class->rule_insert(new_rule, replaced_rule,\n                                        ofm->modify_keep_counts);\n    learned_cookies_inc(ofproto, rule_get_actions(new_rule));\n\n    if (old_rule) {\n        const struct rule_actions *old_actions = rule_get_actions(old_rule);\n        const struct rule_actions *new_actions = rule_get_actions(new_rule);\n\n        learned_cookies_dec(ofproto, old_actions, dead_cookies);\n\n        if (replaced_rule) {\n            enum nx_flow_update_event event = ofm->command == OFPFC_ADD\n                ? NXFME_ADDED : NXFME_MODIFIED;\n\n            bool changed_cookie = (new_rule->flow_cookie\n                                   != old_rule->flow_cookie);\n\n            bool changed_actions = !ofpacts_equal(new_actions->ofpacts,\n                                                  new_actions->ofpacts_len,\n                                                  old_actions->ofpacts,\n                                                  old_actions->ofpacts_len);\n\n            if (event != NXFME_MODIFIED || changed_actions\n                || changed_cookie) {\n                ofmonitor_report(ofproto->connmgr, new_rule, event, 0,\n                                 req ? req->ofconn : NULL,\n                                 req ? req->request->xid : 0,\n                                 changed_actions ? old_actions : NULL);\n            }\n        } else {\n            /* XXX: This is slight duplication with delete_flows_finish__() */\n            ofmonitor_report(ofproto->connmgr, old_rule, NXFME_DELETED,\n                             OFPRR_EVICTION,\n                             req ? req->ofconn : NULL,\n                             req ? req->request->xid : 0, NULL);\n        }\n    }\n}\n\n/* ofm->temp_rule is consumed only in the successful case. */\nstatic enum ofperr\nmodify_flows_start__(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n    enum ofperr error;\n\n    if (rule_collection_n(old_rules) > 0) {\n        /* Create a new 'modified' rule for each old rule. */\n        struct rule *old_rule, *new_rule;\n        const struct rule_actions *actions = rule_get_actions(ofm->temp_rule);\n\n        /* Must check actions while holding ofproto_mutex to avoid a race. */\n        error = ofproto_check_ofpacts(ofproto, actions->ofpacts,\n                                      actions->ofpacts_len);\n        if (error) {\n            return error;\n        }\n\n        /* Use the temp rule as the first new rule, and as the template for\n         * the rest. */\n        struct rule *temp = ofm->temp_rule;\n        ofm->temp_rule = NULL;   /* We consume the template. */\n\n        bool first = true;\n        RULE_COLLECTION_FOR_EACH (old_rule, old_rules) {\n            if (first) {\n                /* The template rule's match is possibly a loose one, so it\n                 * must be replaced with the old rule's match so that the new\n                 * rule actually replaces the old one. */\n                cls_rule_destroy(CONST_CAST(struct cls_rule *, &temp->cr));\n                cls_rule_clone(CONST_CAST(struct cls_rule *, &temp->cr),\n                               &old_rule->cr);\n                if (temp->match_tlv_bitmap != old_rule->match_tlv_bitmap) {\n                    mf_vl_mff_unref(&temp->ofproto->vl_mff_map,\n                                    temp->match_tlv_bitmap);\n                    temp->match_tlv_bitmap = old_rule->match_tlv_bitmap;\n                    mf_vl_mff_ref(&temp->ofproto->vl_mff_map,\n                                  temp->match_tlv_bitmap);\n                }\n                *CONST_CAST(uint8_t *, &temp->table_id) = old_rule->table_id;\n                rule_collection_add(new_rules, temp);\n                first = false;\n            } else {\n                struct cls_rule cr;\n                cls_rule_clone(&cr, &old_rule->cr);\n                error = ofproto_rule_create(ofproto, &cr, old_rule->table_id,\n                                            temp->flow_cookie,\n                                            temp->idle_timeout,\n                                            temp->hard_timeout, temp->flags,\n                                            temp->importance,\n                                            temp->actions->ofpacts,\n                                            temp->actions->ofpacts_len,\n                                            old_rule->match_tlv_bitmap,\n                                            temp->ofpacts_tlv_bitmap,\n                                            &new_rule);\n                if (!error) {\n                    rule_collection_add(new_rules, new_rule);\n                } else {\n                    /* Return the template rule in place in the error case. */\n                    ofm->temp_rule = temp;\n                    rule_collection_rules(new_rules)[0] = NULL;\n\n                    rule_collection_unref(new_rules);\n                    rule_collection_destroy(new_rules);\n                    return error;\n                }\n            }\n        }\n        ovs_assert(rule_collection_n(new_rules)\n                   == rule_collection_n(old_rules));\n\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_start(ofproto, ofm, old_rule, new_rule);\n        }\n    } else if (ofm->modify_may_add_flow) {\n        /* No match, add a new flow, consumes 'temp'. */\n        error = add_flow_start(ofproto, ofm);\n    } else {\n        /* No flow to modify and may not add a flow. */\n        ofproto_rule_unref(ofm->temp_rule);\n        ofm->temp_rule = NULL;   /* We consume the template. */\n        error = 0;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nmodify_flows_init_loose(struct ofproto *ofproto,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, 0,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask, OFPP_ANY,\n                       OFPG_ANY);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    /* Must create a new flow in advance for the case that no matches are\n     * found.  Also used for template for multiple modified flows. */\n    add_flow_init(ofproto, ofm, fm);\n\n    return 0;\n}\n\n/* Implements OFPFC_MODIFY.  Returns 0 on success or an OpenFlow error code on\n * failure. */\nstatic enum ofperr\nmodify_flows_start_loose(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_loose(ofproto, &ofm->criteria, old_rules);\n\n    if (!error) {\n        error = modify_flows_start__(ofproto, ofm);\n    }\n\n    if (error) {\n        rule_collection_destroy(old_rules);\n    }\n\n    return error;\n}\n\nstatic void\nmodify_flows_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n\n    /* Old rules were not changed yet, only need to revert new rules. */\n    if (rule_collection_n(old_rules) > 0) {\n        struct rule *old_rule, *new_rule;\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_revert(ofproto, old_rule, new_rule);\n        }\n        rule_collection_destroy(new_rules);\n        rule_collection_destroy(old_rules);\n    }\n}\n\nstatic void\nmodify_flows_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n\n    if (rule_collection_n(old_rules) == 0\n        && rule_collection_n(new_rules) == 1) {\n        add_flow_finish(ofproto, ofm, req);\n    } else if (rule_collection_n(old_rules) > 0) {\n        struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n\n        ovs_assert(rule_collection_n(new_rules)\n                   == rule_collection_n(old_rules));\n\n        struct rule *old_rule, *new_rule;\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_finish(ofproto, ofm, req, old_rule, new_rule,\n                                &dead_cookies);\n        }\n        learned_cookies_flush(ofproto, &dead_cookies);\n        remove_rules_postponed(old_rules);\n    }\n}\n\nstatic enum ofperr\nmodify_flow_init_strict(struct ofproto *ofproto OVS_UNUSED,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, fm->priority,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask, OFPP_ANY,\n                       OFPG_ANY);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    /* Must create a new flow in advance for the case that no matches are\n     * found.  Also used for template for multiple modified flows. */\n    add_flow_init(ofproto, ofm, fm);\n\n    return 0;\n}\n\n/* Implements OFPFC_MODIFY_STRICT.  Returns 0 on success or an OpenFlow error\n * code on failure. */\nstatic enum ofperr\nmodify_flow_start_strict(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_strict(ofproto, &ofm->criteria, old_rules);\n\n    if (!error) {\n        /* collect_rules_strict() can return max 1 rule. */\n        error = modify_flows_start__(ofproto, ofm);\n    }\n\n    return error;\n}\n\f\n/* OFPFC_DELETE implementation. */\n\nstatic void\ndelete_flows_start__(struct ofproto *ofproto, ovs_version_t version,\n                     const struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        struct oftable *table = &ofproto->tables[rule->table_id];\n\n        table->n_flows--;\n        cls_rule_make_invisible_in_version(&rule->cr, version);\n\n        /* Remove rule from ofproto data structures. */\n        ofproto_rule_remove__(ofproto, rule);\n    }\n}\n\nstatic void\ndelete_flows_revert__(struct ofproto *ofproto,\n                      const struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        struct oftable *table = &ofproto->tables[rule->table_id];\n\n        /* Add rule back to ofproto data structures. */\n        ofproto_rule_insert__(ofproto, rule);\n\n        /* Restore table's rule count. */\n        table->n_flows++;\n\n        /* Restore the original visibility of the rule. */\n        cls_rule_restore_visibility(&rule->cr);\n    }\n}\n\nstatic void\ndelete_flows_finish__(struct ofproto *ofproto,\n                      struct rule_collection *rules,\n                      enum ofp_flow_removed_reason reason,\n                      const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules)) {\n        struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n        struct rule *rule;\n\n        RULE_COLLECTION_FOR_EACH (rule, rules) {\n            /* This value will be used to send the flow removed message right\n             * before the rule is actually destroyed. */\n            rule->removed_reason = reason;\n\n            ofmonitor_report(ofproto->connmgr, rule, NXFME_DELETED, reason,\n                             req ? req->ofconn : NULL,\n                             req ? req->request->xid : 0, NULL);\n\n            /* Send Vacancy Event for OF1.4+. */\n            send_table_status(ofproto, rule->table_id);\n\n            learned_cookies_dec(ofproto, rule_get_actions(rule),\n                                &dead_cookies);\n        }\n        remove_rules_postponed(rules);\n\n        learned_cookies_flush(ofproto, &dead_cookies);\n    }\n}\n\n/* Deletes the rules listed in 'rules'.\n * The deleted rules will become invisible to the lookups in the next version.\n * Destroys 'rules'. */\nstatic void\ndelete_flows__(struct rule_collection *rules,\n               enum ofp_flow_removed_reason reason,\n               const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules)) {\n        struct ofproto *ofproto = rule_collection_rules(rules)[0]->ofproto;\n\n        delete_flows_start__(ofproto, ofproto->tables_version + 1, rules);\n        ofproto_bump_tables_version(ofproto);\n        delete_flows_finish__(ofproto, rules, reason, req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n}\n\nstatic enum ofperr\ndelete_flows_init_loose(struct ofproto *ofproto OVS_UNUSED,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, 0,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask,\n                       fm->out_port, fm->out_group);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    return 0;\n}\n\n/* Implements OFPFC_DELETE. */\nstatic enum ofperr\ndelete_flows_start_loose(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_loose(ofproto, &ofm->criteria, rules);\n\n    if (!error) {\n        delete_flows_start__(ofproto, ofm->version, rules);\n    }\n\n    return error;\n}\n\nstatic void\ndelete_flows_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    delete_flows_revert__(ofproto, &ofm->old_rules);\n}\n\nstatic void\ndelete_flows_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    delete_flows_finish__(ofproto, &ofm->old_rules, OFPRR_DELETE, req);\n}\n\nstatic enum ofperr\ndelete_flows_init_strict(struct ofproto *ofproto OVS_UNUSED,\n                         struct ofproto_flow_mod *ofm,\n                         const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, fm->priority,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask,\n                       fm->out_port, fm->out_group);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    return 0;\n}\n\n/* Implements OFPFC_DELETE_STRICT. */\nstatic enum ofperr\ndelete_flow_start_strict(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_strict(ofproto, &ofm->criteria, rules);\n\n    if (!error) {\n        delete_flows_start__(ofproto, ofm->version, rules);\n    }\n\n    return error;\n}\n\n/* This may only be called by rule_destroy_cb()! */\nstatic void\nofproto_rule_send_removed(struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofputil_flow_removed fr;\n    long long int used;\n\n    minimatch_expand(&rule->cr.match, &fr.match);\n    fr.priority = rule->cr.priority;\n\n    /* Synchronize with connmgr_destroy() calls to prevent connmgr disappearing\n     * while we use it. */\n    ovs_mutex_lock(&ofproto_mutex);\n    struct connmgr *connmgr = rule->ofproto->connmgr;\n    if (!connmgr) {\n        ovs_mutex_unlock(&ofproto_mutex);\n        return;\n    }\n\n    fr.cookie = rule->flow_cookie;\n    fr.reason = rule->removed_reason;\n    fr.table_id = rule->table_id;\n    calc_duration(rule->created, time_msec(),\n                  &fr.duration_sec, &fr.duration_nsec);\n    ovs_mutex_lock(&rule->mutex);\n    fr.idle_timeout = rule->idle_timeout;\n    fr.hard_timeout = rule->hard_timeout;\n    ovs_mutex_unlock(&rule->mutex);\n    rule->ofproto->ofproto_class->rule_get_stats(rule, &fr.packet_count,\n                                                 &fr.byte_count, &used);\n    connmgr_send_flow_removed(connmgr, &fr);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Sends an OpenFlow \"flow removed\" message with the given 'reason' (either\n * OFPRR_HARD_TIMEOUT or OFPRR_IDLE_TIMEOUT), and then removes 'rule' from its\n * ofproto.\n *\n * ofproto implementation ->run() functions should use this function to expire\n * OpenFlow flows. */\nvoid\nofproto_rule_expire(struct rule *rule, uint8_t reason)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection rules;\n\n    rule_collection_init(&rules);\n    rule_collection_add(&rules, rule);\n    delete_flows__(&rules, reason, NULL);\n}\n\n/* Reduces '*timeout' to no more than 'max'.  A value of zero in either case\n * means \"infinite\". */\nstatic void\nreduce_timeout(uint16_t max, uint16_t *timeout)\n{\n    if (max && (!*timeout || *timeout > max)) {\n        *timeout = max;\n    }\n}\n\n/* If 'idle_timeout' is nonzero, and 'rule' has no idle timeout or an idle\n * timeout greater than 'idle_timeout', lowers 'rule''s idle timeout to\n * 'idle_timeout' seconds.  Similarly for 'hard_timeout'.\n *\n * Suitable for implementing OFPACT_FIN_TIMEOUT. */\nvoid\nofproto_rule_reduce_timeouts__(struct rule *rule,\n                               uint16_t idle_timeout, uint16_t hard_timeout)\n    OVS_REQUIRES(ofproto_mutex)\n    OVS_EXCLUDED(rule->mutex)\n{\n    if (!idle_timeout && !hard_timeout) {\n        return;\n    }\n\n    if (ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_insert(&rule->ofproto->expirable, &rule->expirable);\n    }\n\n    ovs_mutex_lock(&rule->mutex);\n    reduce_timeout(idle_timeout, &rule->idle_timeout);\n    reduce_timeout(hard_timeout, &rule->hard_timeout);\n    ovs_mutex_unlock(&rule->mutex);\n}\n\nvoid\nofproto_rule_reduce_timeouts(struct rule *rule,\n                             uint16_t idle_timeout, uint16_t hard_timeout)\n    OVS_EXCLUDED(ofproto_mutex, rule->mutex)\n{\n    if (!idle_timeout && !hard_timeout) {\n        return;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    if (ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_insert(&rule->ofproto->expirable, &rule->expirable);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ovs_mutex_lock(&rule->mutex);\n    reduce_timeout(idle_timeout, &rule->idle_timeout);\n    reduce_timeout(hard_timeout, &rule->hard_timeout);\n    ovs_mutex_unlock(&rule->mutex);\n}\n\f\nstatic enum ofperr\nhandle_flow_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_mod fm;\n    uint64_t ofpacts_stub[1024 / 8];\n    struct ofpbuf ofpacts;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n    error = ofputil_decode_flow_mod(&fm, oh, ofconn_get_protocol(ofconn),\n                                    ofproto_get_tun_tab(ofproto),\n                                    &ofproto->vl_mff_map, &ofpacts,\n                                    u16_to_ofp(ofproto->max_ports),\n                                    ofproto->n_tables);\n    if (!error) {\n        struct openflow_mod_requester req = { ofconn, oh };\n        error = handle_flow_mod__(ofproto, &fm, &req);\n    }\n\n    ofpbuf_uninit(&ofpacts);\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_mod__(struct ofproto *ofproto, const struct ofputil_flow_mod *fm,\n                  const struct openflow_mod_requester *req)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto_flow_mod ofm;\n    enum ofperr error;\n\n    error = ofproto_flow_mod_init(ofproto, &ofm, fm, NULL);\n    if (error) {\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ofm.version = ofproto->tables_version + 1;\n    error = ofproto_flow_mod_start(ofproto, &ofm);\n    if (!error) {\n        ofproto_bump_tables_version(ofproto);\n        ofproto_flow_mod_finish(ofproto, &ofm, req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_role_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_role_request request;\n    struct ofputil_role_request reply;\n    struct ofpbuf *buf;\n    enum ofperr error;\n\n    error = ofputil_decode_role_message(oh, &request);\n    if (error) {\n        return error;\n    }\n\n    if (request.role != OFPCR12_ROLE_NOCHANGE) {\n        if (request.role != OFPCR12_ROLE_EQUAL\n            && request.have_generation_id\n            && !ofconn_set_master_election_id(ofconn, request.generation_id)) {\n                return OFPERR_OFPRRFC_STALE;\n        }\n\n        ofconn_set_role(ofconn, request.role);\n    }\n\n    reply.role = ofconn_get_role(ofconn);\n    reply.have_generation_id = ofconn_get_master_election_id(\n        ofconn, &reply.generation_id);\n    buf = ofputil_encode_role_reply(oh, &reply);\n    ofconn_send_reply(ofconn, buf);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_flow_mod_table_id(struct ofconn *ofconn,\n                             const struct ofp_header *oh)\n{\n    const struct nx_flow_mod_table_id *msg = ofpmsg_body(oh);\n    enum ofputil_protocol cur, next;\n\n    cur = ofconn_get_protocol(ofconn);\n    next = ofputil_protocol_set_tid(cur, msg->set != 0);\n    ofconn_set_protocol(ofconn, next);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_flow_format(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    const struct nx_set_flow_format *msg = ofpmsg_body(oh);\n    enum ofputil_protocol cur, next;\n    enum ofputil_protocol next_base;\n\n    next_base = ofputil_nx_flow_format_to_protocol(ntohl(msg->format));\n    if (!next_base) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    cur = ofconn_get_protocol(ofconn);\n    next = ofputil_protocol_set_base(cur, next_base);\n    ofconn_set_protocol(ofconn, next);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_packet_in_format(struct ofconn *ofconn,\n                                const struct ofp_header *oh)\n{\n    const struct nx_set_packet_in_format *msg = ofpmsg_body(oh);\n    uint32_t format;\n\n    format = ntohl(msg->format);\n    if (!ofputil_packet_in_format_is_valid(format)) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    ofconn_set_packet_in_format(ofconn, format);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_async_config(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_async_cfg basis = ofconn_get_async_config(ofconn);\n    struct ofputil_async_cfg ac;\n    enum ofperr error;\n\n    error = ofputil_decode_set_async_config(oh, false, &basis, &ac);\n    if (error) {\n        return error;\n    }\n\n    ofconn_set_async_config(ofconn, &ac);\n    if (ofconn_get_type(ofconn) == OFCONN_SERVICE &&\n        !ofconn_get_miss_send_len(ofconn)) {\n        ofconn_set_miss_send_len(ofconn, OFP_DEFAULT_MISS_SEND_LEN);\n    }\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_get_async_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_async_cfg ac = ofconn_get_async_config(ofconn);\n    ofconn_send_reply(ofconn, ofputil_encode_get_async_reply(oh, &ac));\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_controller_id(struct ofconn *ofconn,\n                             const struct ofp_header *oh)\n{\n    const struct nx_controller_id *nci = ofpmsg_body(oh);\n\n    if (!is_all_zeros(nci->zero, sizeof nci->zero)) {\n        return OFPERR_NXBRC_MUST_BE_ZERO;\n    }\n\n    ofconn_set_controller_id(ofconn, ntohs(nci->controller_id));\n    return 0;\n}\n\nstatic enum ofperr\nhandle_barrier_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofpbuf *buf;\n\n    buf = ofpraw_alloc_reply((oh->version == OFP10_VERSION\n                              ? OFPRAW_OFPT10_BARRIER_REPLY\n                              : OFPRAW_OFPT11_BARRIER_REPLY), oh, 0);\n    ofconn_send_reply(ofconn, buf);\n    return 0;\n}\n\nstatic void\nofproto_compose_flow_refresh_update(const struct rule *rule,\n                                    enum nx_flow_monitor_flags flags,\n                                    struct ovs_list *msgs,\n                                    const struct tun_table *tun_table)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct rule_actions *actions;\n    struct ofputil_flow_update fu;\n\n    fu.event = (flags & (NXFMF_INITIAL | NXFMF_ADD)\n                ? NXFME_ADDED : NXFME_MODIFIED);\n    fu.reason = 0;\n    ovs_mutex_lock(&rule->mutex);\n    fu.idle_timeout = rule->idle_timeout;\n    fu.hard_timeout = rule->hard_timeout;\n    ovs_mutex_unlock(&rule->mutex);\n    fu.table_id = rule->table_id;\n    fu.cookie = rule->flow_cookie;\n    minimatch_expand(&rule->cr.match, &fu.match);\n    fu.priority = rule->cr.priority;\n\n    actions = flags & NXFMF_ACTIONS ? rule_get_actions(rule) : NULL;\n    fu.ofpacts = actions ? actions->ofpacts : NULL;\n    fu.ofpacts_len = actions ? actions->ofpacts_len : 0;\n\n    if (ovs_list_is_empty(msgs)) {\n        ofputil_start_flow_update(msgs);\n    }\n    ofputil_append_flow_update(&fu, msgs, tun_table);\n}\n\nvoid\nofmonitor_compose_refresh_updates(struct rule_collection *rules,\n                                  struct ovs_list *msgs)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        enum nx_flow_monitor_flags flags = rule->monitor_flags;\n        rule->monitor_flags = 0;\n\n        ofproto_compose_flow_refresh_update(rule, flags, msgs,\n                ofproto_get_tun_tab(rule->ofproto));\n    }\n}\n\nstatic void\nofproto_collect_ofmonitor_refresh_rule(const struct ofmonitor *m,\n                                       struct rule *rule, uint64_t seqno,\n                                       struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum nx_flow_monitor_flags update;\n\n    if (rule_is_hidden(rule)) {\n        return;\n    }\n\n    if (!ofproto_rule_has_out_port(rule, m->out_port)) {\n        return;\n    }\n\n    if (seqno) {\n        if (rule->add_seqno > seqno) {\n            update = NXFMF_ADD | NXFMF_MODIFY;\n        } else if (rule->modify_seqno > seqno) {\n            update = NXFMF_MODIFY;\n        } else {\n            return;\n        }\n\n        if (!(m->flags & update)) {\n            return;\n        }\n    } else {\n        update = NXFMF_INITIAL;\n    }\n\n    if (!rule->monitor_flags) {\n        rule_collection_add(rules, rule);\n    }\n    rule->monitor_flags |= update | (m->flags & NXFMF_ACTIONS);\n}\n\nstatic void\nofproto_collect_ofmonitor_refresh_rules(const struct ofmonitor *m,\n                                        uint64_t seqno,\n                                        struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct ofproto *ofproto = ofconn_get_ofproto(m->ofconn);\n    const struct oftable *table;\n    struct cls_rule target;\n\n    cls_rule_init_from_minimatch(&target, &m->match, 0);\n    FOR_EACH_MATCHING_TABLE (table, m->table_id, ofproto) {\n        struct rule *rule;\n\n        CLS_FOR_EACH_TARGET (rule, cr, &table->cls, &target, OVS_VERSION_MAX) {\n            ofproto_collect_ofmonitor_refresh_rule(m, rule, seqno, rules);\n        }\n    }\n    cls_rule_destroy(&target);\n}\n\nstatic void\nofproto_collect_ofmonitor_initial_rules(struct ofmonitor *m,\n                                        struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (m->flags & NXFMF_INITIAL) {\n        ofproto_collect_ofmonitor_refresh_rules(m, 0, rules);\n    }\n}\n\nvoid\nofmonitor_collect_resume_rules(struct ofmonitor *m,\n                               uint64_t seqno, struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto_collect_ofmonitor_refresh_rules(m, seqno, rules);\n}\n\nstatic enum ofperr\nflow_monitor_delete(struct ofconn *ofconn, uint32_t id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofmonitor *m;\n    enum ofperr error;\n\n    m = ofmonitor_lookup(ofconn, id);\n    if (m) {\n        ofmonitor_destroy(m);\n        error = 0;\n    } else {\n        error = OFPERR_OFPMOFC_UNKNOWN_MONITOR;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_monitor_request(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n\n    struct ofpbuf b = ofpbuf_const_initializer(oh, ntohs(oh->length));\n\n    struct ofmonitor **monitors = NULL;\n    size_t allocated_monitors = 0;\n    size_t n_monitors = 0;\n\n    enum ofperr error;\n\n    ovs_mutex_lock(&ofproto_mutex);\n    for (;;) {\n        struct ofputil_flow_monitor_request request;\n        struct ofmonitor *m;\n        int retval;\n\n        retval = ofputil_decode_flow_monitor_request(&request, &b);\n        if (retval == EOF) {\n            break;\n        } else if (retval) {\n            error = retval;\n            goto error;\n        }\n\n        if (request.table_id != 0xff\n            && request.table_id >= ofproto->n_tables) {\n            error = OFPERR_OFPBRC_BAD_TABLE_ID;\n            goto error;\n        }\n\n        error = ofmonitor_create(&request, ofconn, &m);\n        if (error) {\n            goto error;\n        }\n\n        if (n_monitors >= allocated_monitors) {\n            monitors = x2nrealloc(monitors, &allocated_monitors,\n                                  sizeof *monitors);\n        }\n        monitors[n_monitors++] = m;\n    }\n\n    struct rule_collection rules;\n    rule_collection_init(&rules);\n    for (size_t i = 0; i < n_monitors; i++) {\n        ofproto_collect_ofmonitor_initial_rules(monitors[i], &rules);\n    }\n\n    struct ovs_list replies;\n    ofpmp_init(&replies, oh);\n    ofmonitor_compose_refresh_updates(&rules, &replies);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    rule_collection_destroy(&rules);\n\n    ofconn_send_replies(ofconn, &replies);\n    free(monitors);\n\n    return 0;\n\nerror:\n    for (size_t i = 0; i < n_monitors; i++) {\n        ofmonitor_destroy(monitors[i]);\n    }\n    free(monitors);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_monitor_cancel(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error;\n    uint32_t id;\n\n    id = ofputil_decode_flow_monitor_cancel(oh);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = flow_monitor_delete(ofconn, id);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\n/* Meters implementation.\n *\n * Meter table entry, indexed by the OpenFlow meter_id.\n * 'created' is used to compute the duration for meter stats.\n * 'list rules' is needed so that we can delete the dependent rules when the\n * meter table entry is deleted.\n * 'provider_meter_id' is for the provider's private use.\n */\nstruct meter {\n    long long int created;      /* Time created. */\n    struct ovs_list rules;      /* List of \"struct rule_dpif\"s. */\n    ofproto_meter_id provider_meter_id;\n    uint16_t flags;             /* Meter flags. */\n    uint16_t n_bands;           /* Number of meter bands. */\n    struct ofputil_meter_band *bands;\n};\n\n/*\n * This is used in instruction validation at flow set-up time,\n * as flows may not use non-existing meters.\n * Return value of UINT32_MAX signifies an invalid meter.\n */\nstatic uint32_t\nget_provider_meter_id(const struct ofproto *ofproto, uint32_t of_meter_id)\n{\n    if (of_meter_id && of_meter_id <= ofproto->meter_features.max_meters) {\n        const struct meter *meter = ofproto->meters[of_meter_id];\n        if (meter) {\n            return meter->provider_meter_id.uint32;\n        }\n    }\n    return UINT32_MAX;\n}\n\n/* Finds the meter invoked by 'rule''s actions and adds 'rule' to the meter's\n * list of rules. */\nstatic void\nmeter_insert_rule(struct rule *rule)\n{\n    const struct rule_actions *a = rule_get_actions(rule);\n    uint32_t meter_id = ofpacts_get_meter(a->ofpacts, a->ofpacts_len);\n    struct meter *meter = rule->ofproto->meters[meter_id];\n\n    ovs_list_insert(&meter->rules, &rule->meter_list_node);\n}\n\nstatic void\nmeter_update(struct meter *meter, const struct ofputil_meter_config *config)\n{\n    free(meter->bands);\n\n    meter->flags = config->flags;\n    meter->n_bands = config->n_bands;\n    meter->bands = xmemdup(config->bands,\n                           config->n_bands * sizeof *meter->bands);\n}\n\nstatic struct meter *\nmeter_create(const struct ofputil_meter_config *config,\n             ofproto_meter_id provider_meter_id)\n{\n    struct meter *meter;\n\n    meter = xzalloc(sizeof *meter);\n    meter->provider_meter_id = provider_meter_id;\n    meter->created = time_msec();\n    ovs_list_init(&meter->rules);\n\n    meter_update(meter, config);\n\n    return meter;\n}\n\nstatic void\nmeter_delete(struct ofproto *ofproto, uint32_t first, uint32_t last)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    for (uint32_t mid = first; mid <= last; ++mid) {\n        struct meter *meter = ofproto->meters[mid];\n        if (meter) {\n            /* First delete the rules that use this meter. */\n            if (!ovs_list_is_empty(&meter->rules)) {\n                struct rule_collection rules;\n                struct rule *rule;\n\n                rule_collection_init(&rules);\n\n                LIST_FOR_EACH (rule, meter_list_node, &meter->rules) {\n                    rule_collection_add(&rules, rule);\n                }\n                delete_flows__(&rules, OFPRR_METER_DELETE, NULL);\n            }\n\n            ofproto->meters[mid] = NULL;\n            ofproto->ofproto_class->meter_del(ofproto,\n                                              meter->provider_meter_id);\n            free(meter->bands);\n            free(meter);\n        }\n    }\n}\n\nstatic enum ofperr\nhandle_add_meter(struct ofproto *ofproto, struct ofputil_meter_mod *mm)\n{\n    ofproto_meter_id provider_meter_id = { UINT32_MAX };\n    struct meter **meterp = &ofproto->meters[mm->meter.meter_id];\n    enum ofperr error;\n\n    if (*meterp) {\n        return OFPERR_OFPMMFC_METER_EXISTS;\n    }\n\n    error = ofproto->ofproto_class->meter_set(ofproto, &provider_meter_id,\n                                              &mm->meter);\n    if (!error) {\n        ovs_assert(provider_meter_id.uint32 != UINT32_MAX);\n        *meterp = meter_create(&mm->meter, provider_meter_id);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_modify_meter(struct ofproto *ofproto, struct ofputil_meter_mod *mm)\n{\n    struct meter *meter = ofproto->meters[mm->meter.meter_id];\n    enum ofperr error;\n    uint32_t provider_meter_id;\n\n    if (!meter) {\n        return OFPERR_OFPMMFC_UNKNOWN_METER;\n    }\n\n    provider_meter_id = meter->provider_meter_id.uint32;\n    error = ofproto->ofproto_class->meter_set(ofproto,\n                                              &meter->provider_meter_id,\n                                              &mm->meter);\n    ovs_assert(meter->provider_meter_id.uint32 == provider_meter_id);\n    if (!error) {\n        meter_update(meter, &mm->meter);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_delete_meter(struct ofconn *ofconn, struct ofputil_meter_mod *mm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    uint32_t meter_id = mm->meter.meter_id;\n    enum ofperr error = 0;\n    uint32_t first, last;\n\n    if (meter_id == OFPM13_ALL) {\n        first = 1;\n        last = ofproto->meter_features.max_meters;\n    } else {\n        if (!meter_id || meter_id > ofproto->meter_features.max_meters) {\n            return 0;\n        }\n        first = last = meter_id;\n    }\n\n    /* Delete the meters. */\n    ovs_mutex_lock(&ofproto_mutex);\n    meter_delete(ofproto, first, last);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_meter_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_meter_mod mm;\n    uint64_t bands_stub[256 / 8];\n    struct ofpbuf bands;\n    uint32_t meter_id;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    ofpbuf_use_stub(&bands, bands_stub, sizeof bands_stub);\n\n    error = ofputil_decode_meter_mod(oh, &mm, &bands);\n    if (error) {\n        goto exit_free_bands;\n    }\n\n    meter_id = mm.meter.meter_id;\n\n    if (mm.command != OFPMC13_DELETE) {\n        /* Fails also when meters are not implemented by the provider. */\n        if (meter_id == 0 || meter_id > OFPM13_MAX) {\n            error = OFPERR_OFPMMFC_INVALID_METER;\n            goto exit_free_bands;\n        } else if (meter_id > ofproto->meter_features.max_meters) {\n            error = OFPERR_OFPMMFC_OUT_OF_METERS;\n            goto exit_free_bands;\n        }\n        if (mm.meter.n_bands > ofproto->meter_features.max_bands) {\n            error = OFPERR_OFPMMFC_OUT_OF_BANDS;\n            goto exit_free_bands;\n        }\n    }\n\n    switch (mm.command) {\n    case OFPMC13_ADD:\n        error = handle_add_meter(ofproto, &mm);\n        break;\n\n    case OFPMC13_MODIFY:\n        error = handle_modify_meter(ofproto, &mm);\n        break;\n\n    case OFPMC13_DELETE:\n        error = handle_delete_meter(ofconn, &mm);\n        break;\n\n    default:\n        error = OFPERR_OFPMMFC_BAD_COMMAND;\n        break;\n    }\n\n    if (!error) {\n        struct ofputil_requestforward rf;\n        rf.xid = oh->xid;\n        rf.reason = OFPRFR_METER_MOD;\n        rf.meter_mod = &mm;\n        connmgr_send_requestforward(ofproto->connmgr, ofconn, &rf);\n    }\n\nexit_free_bands:\n    ofpbuf_uninit(&bands);\n    return error;\n}\n\nstatic enum ofperr\nhandle_meter_features_request(struct ofconn *ofconn,\n                              const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_meter_features features;\n    struct ofpbuf *b;\n\n    if (ofproto->ofproto_class->meter_get_features) {\n        ofproto->ofproto_class->meter_get_features(ofproto, &features);\n    } else {\n        memset(&features, 0, sizeof features);\n    }\n    b = ofputil_encode_meter_features_reply(&features, request);\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_meter_request(struct ofconn *ofconn, const struct ofp_header *request,\n                     enum ofptype type)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    uint64_t bands_stub[256 / 8];\n    struct ofpbuf bands;\n    uint32_t meter_id, first, last;\n\n    ofputil_decode_meter_request(request, &meter_id);\n\n    if (meter_id == OFPM13_ALL) {\n        first = 1;\n        last = ofproto->meter_features.max_meters;\n    } else {\n        if (!meter_id || meter_id > ofproto->meter_features.max_meters ||\n            !ofproto->meters[meter_id]) {\n            return OFPERR_OFPMMFC_UNKNOWN_METER;\n        }\n        first = last = meter_id;\n    }\n\n    ofpbuf_use_stub(&bands, bands_stub, sizeof bands_stub);\n    ofpmp_init(&replies, request);\n\n    for (meter_id = first; meter_id <= last; ++meter_id) {\n        struct meter *meter = ofproto->meters[meter_id];\n        if (!meter) {\n            continue; /* Skip non-existing meters. */\n        }\n        if (type == OFPTYPE_METER_STATS_REQUEST) {\n            struct ofputil_meter_stats stats;\n\n            stats.meter_id = meter_id;\n\n            /* Provider sets the packet and byte counts, we do the rest. */\n            stats.flow_count = ovs_list_size(&meter->rules);\n            calc_duration(meter->created, time_msec(),\n                          &stats.duration_sec, &stats.duration_nsec);\n            stats.n_bands = meter->n_bands;\n            ofpbuf_clear(&bands);\n            stats.bands\n                = ofpbuf_put_uninit(&bands,\n                                    meter->n_bands * sizeof *stats.bands);\n\n            if (!ofproto->ofproto_class->meter_get(ofproto,\n                                                   meter->provider_meter_id,\n                                                   &stats)) {\n                ofputil_append_meter_stats(&replies, &stats);\n            }\n        } else { /* type == OFPTYPE_METER_CONFIG_REQUEST */\n            struct ofputil_meter_config config;\n\n            config.meter_id = meter_id;\n            config.flags = meter->flags;\n            config.n_bands = meter->n_bands;\n            config.bands = meter->bands;\n            ofputil_append_meter_config(&replies, &config);\n        }\n    }\n\n    ofconn_send_replies(ofconn, &replies);\n    ofpbuf_uninit(&bands);\n    return 0;\n}\n\n/* Returned group is RCU protected. */\nstatic struct ofgroup *\nofproto_group_lookup__(const struct ofproto *ofproto, uint32_t group_id,\n                       ovs_version_t version)\n{\n    struct ofgroup *group;\n\n    CMAP_FOR_EACH_WITH_HASH (group, cmap_node, hash_int(group_id, 0),\n                             &ofproto->groups) {\n        if (group->group_id == group_id\n            && versions_visible_in_version(&group->versions, version)) {\n            return group;\n        }\n    }\n\n    return NULL;\n}\n\n/* If the group exists, this function increments the groups's reference count.\n *\n * Make sure to call ofproto_group_unref() after no longer needing to maintain\n * a reference to the group. */\nstruct ofgroup *\nofproto_group_lookup(const struct ofproto *ofproto, uint32_t group_id,\n                     ovs_version_t version, bool take_ref)\n{\n    struct ofgroup *group;\n\n    group = ofproto_group_lookup__(ofproto, group_id, version);\n    if (group && take_ref) {\n        /* Not holding a lock, so it is possible that another thread releases\n         * the last reference just before we manage to get one. */\n        return ofproto_group_try_ref(group) ? group : NULL;\n    }\n    return group;\n}\n\n/* Caller should hold 'ofproto_mutex' if it is important that the\n * group is not removed by someone else. */\nstatic bool\nofproto_group_exists(const struct ofproto *ofproto, uint32_t group_id)\n{\n    return ofproto_group_lookup__(ofproto, group_id, OVS_VERSION_MAX) != NULL;\n}\n\nstatic void\ngroup_add_rule(struct ofgroup *group, struct rule *rule)\n{\n    rule_collection_add(&group->rules, rule);\n}\n\nstatic void\ngroup_remove_rule(struct ofgroup *group, struct rule *rule)\n{\n    rule_collection_remove(&group->rules, rule);\n}\n\nstatic void\nappend_group_stats(struct ofgroup *group, struct ovs_list *replies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofputil_group_stats ogs;\n    const struct ofproto *ofproto = group->ofproto;\n    long long int now = time_msec();\n    int error;\n\n    ogs.bucket_stats = xmalloc(group->n_buckets * sizeof *ogs.bucket_stats);\n\n    /* Provider sets the packet and byte counts, we do the rest. */\n    ogs.ref_count = rule_collection_n(&group->rules);\n    ogs.n_buckets = group->n_buckets;\n\n    error = (ofproto->ofproto_class->group_get_stats\n             ? ofproto->ofproto_class->group_get_stats(group, &ogs)\n             : EOPNOTSUPP);\n    if (error) {\n        ogs.packet_count = UINT64_MAX;\n        ogs.byte_count = UINT64_MAX;\n        memset(ogs.bucket_stats, 0xff,\n               ogs.n_buckets * sizeof *ogs.bucket_stats);\n    }\n\n    ogs.group_id = group->group_id;\n    calc_duration(group->created, now, &ogs.duration_sec, &ogs.duration_nsec);\n\n    ofputil_append_group_stats(replies, &ogs);\n\n    free(ogs.bucket_stats);\n}\n\nstatic void\nhandle_group_request(struct ofconn *ofconn,\n                     const struct ofp_header *request, uint32_t group_id,\n                     void (*cb)(struct ofgroup *, struct ovs_list *replies))\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofgroup *group;\n    struct ovs_list replies;\n\n    ofpmp_init(&replies, request);\n    /* Must exclude modifications to guarantee iterating groups. */\n    ovs_mutex_lock(&ofproto_mutex);\n    if (group_id == OFPG_ALL) {\n        CMAP_FOR_EACH (group, cmap_node, &ofproto->groups) {\n            if (versions_visible_in_version(&group->versions,\n                                            OVS_VERSION_MAX)) {\n                cb(group, &replies);\n            }\n        }\n    } else {\n        group = ofproto_group_lookup__(ofproto, group_id, OVS_VERSION_MAX);\n        if (group) {\n            cb(group, &replies);\n        }\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n    ofconn_send_replies(ofconn, &replies);\n}\n\nstatic enum ofperr\nhandle_group_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *request)\n{\n    uint32_t group_id;\n    enum ofperr error;\n\n    error = ofputil_decode_group_stats_request(request, &group_id);\n    if (error) {\n        return error;\n    }\n\n    handle_group_request(ofconn, request, group_id, append_group_stats);\n    return 0;\n}\n\nstatic void\nappend_group_desc(struct ofgroup *group, struct ovs_list *replies)\n{\n    struct ofputil_group_desc gds;\n\n    gds.group_id = group->group_id;\n    gds.type = group->type;\n    gds.props = group->props;\n\n    ofputil_append_group_desc_reply(&gds, &group->buckets, replies);\n}\n\nstatic enum ofperr\nhandle_group_desc_stats_request(struct ofconn *ofconn,\n                                const struct ofp_header *request)\n{\n    handle_group_request(ofconn, request,\n                         ofputil_decode_group_desc_request(request),\n                         append_group_desc);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_group_features_stats_request(struct ofconn *ofconn,\n                                    const struct ofp_header *request)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofpbuf *msg;\n\n    msg = ofputil_encode_group_features_reply(&p->ogf, request);\n    if (msg) {\n        ofconn_send_reply(ofconn, msg);\n    }\n\n    return 0;\n}\n\nstatic void\nput_queue_get_config_reply(struct ofport *port, uint32_t queue,\n                           struct ovs_list *replies)\n{\n    struct ofputil_queue_config qc;\n\n    /* None of the existing queues have compatible properties, so we hard-code\n     * omitting min_rate and max_rate. */\n    qc.port = port->ofp_port;\n    qc.queue = queue;\n    qc.min_rate = UINT16_MAX;\n    qc.max_rate = UINT16_MAX;\n    ofputil_append_queue_get_config_reply(&qc, replies);\n}\n\nstatic int\nhandle_queue_get_config_request_for_port(struct ofport *port, uint32_t queue,\n                                         struct ovs_list *replies)\n{\n    struct smap details = SMAP_INITIALIZER(&details);\n    if (queue != OFPQ_ALL) {\n        int error = netdev_get_queue(port->netdev, queue, &details);\n        switch (error) {\n        case 0:\n            put_queue_get_config_reply(port, queue, replies);\n            break;\n        case EOPNOTSUPP:\n        case EINVAL:\n            return OFPERR_OFPQOFC_BAD_QUEUE;\n        default:\n            return OFPERR_NXQOFC_QUEUE_ERROR;\n        }\n    } else {\n        struct netdev_queue_dump queue_dump;\n        uint32_t queue_id;\n\n        NETDEV_QUEUE_FOR_EACH (&queue_id, &details, &queue_dump,\n                               port->netdev) {\n            put_queue_get_config_reply(port, queue_id, replies);\n        }\n    }\n    smap_destroy(&details);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_queue_get_config_request(struct ofconn *ofconn,\n                                const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    struct ofport *port;\n    ofp_port_t req_port;\n    uint32_t req_queue;\n    enum ofperr error;\n\n    error = ofputil_decode_queue_get_config_request(oh, &req_port, &req_queue);\n    if (error) {\n        return error;\n    }\n\n    ofputil_start_queue_get_config_reply(oh, &replies);\n    if (req_port == OFPP_ANY) {\n        error = OFPERR_OFPQOFC_BAD_QUEUE;\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            if (!handle_queue_get_config_request_for_port(port, req_queue,\n                                                          &replies)) {\n                error = 0;\n            }\n        }\n    } else {\n        port = ofproto_get_port(ofproto, req_port);\n        error = (port\n                 ? handle_queue_get_config_request_for_port(port, req_queue,\n                                                            &replies)\n                 : OFPERR_OFPQOFC_BAD_PORT);\n    }\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\n/* Allocates, initializes, and constructs a new group in 'ofproto', obtaining\n * all the attributes for it from 'gm', and stores a pointer to it in\n * '*ofgroup'.  Makes the new group visible from the flow table starting from\n * 'version'.\n *\n * Returns 0 if successful, otherwise an error code.  If there is an error then\n * '*ofgroup' is indeterminate upon return. */\nstatic enum ofperr\ninit_group(struct ofproto *ofproto, const struct ofputil_group_mod *gm,\n           ovs_version_t version, struct ofgroup **ofgroup)\n{\n    enum ofperr error;\n    const long long int now = time_msec();\n\n    if (gm->group_id > OFPG_MAX) {\n        return OFPERR_OFPGMFC_INVALID_GROUP;\n    }\n    if (gm->type > OFPGT11_FF) {\n        return OFPERR_OFPGMFC_BAD_TYPE;\n    }\n\n    *ofgroup = ofproto->ofproto_class->group_alloc();\n    if (!*ofgroup) {\n        VLOG_WARN_RL(&rl, \"%s: failed to allocate group\", ofproto->name);\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    *CONST_CAST(struct ofproto **, &(*ofgroup)->ofproto) = ofproto;\n    *CONST_CAST(uint32_t *, &((*ofgroup)->group_id)) = gm->group_id;\n    *CONST_CAST(enum ofp11_group_type *, &(*ofgroup)->type) = gm->type;\n    *CONST_CAST(long long int *, &((*ofgroup)->created)) = now;\n    *CONST_CAST(long long int *, &((*ofgroup)->modified)) = now;\n    ovs_refcount_init(&(*ofgroup)->ref_count);\n    (*ofgroup)->being_deleted = false;\n\n    ovs_list_init(CONST_CAST(struct ovs_list *, &(*ofgroup)->buckets));\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &(*ofgroup)->buckets),\n                              &gm->buckets, NULL);\n\n    *CONST_CAST(uint32_t *, &(*ofgroup)->n_buckets) =\n        ovs_list_size(&(*ofgroup)->buckets);\n\n    ofputil_group_properties_copy(CONST_CAST(struct ofputil_group_props *,\n                                             &(*ofgroup)->props),\n                                  &gm->props);\n    rule_collection_init(&(*ofgroup)->rules);\n\n    /* Make group visible from 'version'. */\n    (*ofgroup)->versions = VERSIONS_INITIALIZER(version,\n                                                OVS_VERSION_NOT_REMOVED);\n\n    /* Construct called BEFORE any locks are held. */\n    error = ofproto->ofproto_class->group_construct(*ofgroup);\n    if (error) {\n        ofputil_group_properties_destroy(CONST_CAST(struct ofputil_group_props *,\n                                                    &(*ofgroup)->props));\n        ofputil_bucket_list_destroy(CONST_CAST(struct ovs_list *,\n                                               &(*ofgroup)->buckets));\n        ofproto->ofproto_class->group_dealloc(*ofgroup);\n    }\n    return error;\n}\n\n/* Implements the OFPGC11_ADD operation specified by 'gm', adding a group to\n * 'ofproto''s group table.  Returns 0 on success or an OpenFlow error code on\n * failure. */\nstatic enum ofperr\nadd_group_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    if (ofproto_group_exists(ofproto, ogm->gm.group_id)) {\n        return OFPERR_OFPGMFC_GROUP_EXISTS;\n    }\n\n    if (ofproto->n_groups[ogm->gm.type]\n        >= ofproto->ogf.max_groups[ogm->gm.type]) {\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    /* Allocate new group and initialize it. */\n    error = init_group(ofproto, &ogm->gm, ogm->version, &ogm->new_group);\n    if (!error) {\n        /* Insert new group. */\n        cmap_insert(&ofproto->groups, &ogm->new_group->cmap_node,\n                    hash_int(ogm->new_group->group_id, 0));\n        ofproto->n_groups[ogm->new_group->type]++;\n    }\n    return error;\n}\n\n/* Adds all of the buckets from 'ofgroup' to 'new_ofgroup'.  The buckets\n * already in 'new_ofgroup' will be placed just after the (copy of the) bucket\n * in 'ofgroup' with bucket ID 'command_bucket_id'.  Special\n * 'command_bucket_id' values OFPG15_BUCKET_FIRST and OFPG15_BUCKET_LAST are\n * also honored. */\nstatic enum ofperr\ncopy_buckets_for_insert_bucket(const struct ofgroup *ofgroup,\n                               struct ofgroup *new_ofgroup,\n                               uint32_t command_bucket_id)\n{\n    struct ofputil_bucket *last = NULL;\n\n    if (command_bucket_id <= OFPG15_BUCKET_MAX) {\n        /* Check here to ensure that a bucket corresponding to\n         * command_bucket_id exists in the old bucket list.\n         *\n         * The subsequent search of below of new_ofgroup covers\n         * both buckets in the old bucket list and buckets added\n         * by the insert buckets group mod message this function processes. */\n        if (!ofputil_bucket_find(&ofgroup->buckets, command_bucket_id)) {\n            return OFPERR_OFPGMFC_UNKNOWN_BUCKET;\n        }\n\n        if (!ovs_list_is_empty(&new_ofgroup->buckets)) {\n            last = ofputil_bucket_list_back(&new_ofgroup->buckets);\n        }\n    }\n\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &new_ofgroup->buckets),\n                              &ofgroup->buckets, NULL);\n\n    if (ofputil_bucket_check_duplicate_id(&new_ofgroup->buckets)) {\n            VLOG_INFO_RL(&rl, \"Duplicate bucket id\");\n            return OFPERR_OFPGMFC_BUCKET_EXISTS;\n    }\n\n    /* Rearrange list according to command_bucket_id */\n    if (command_bucket_id == OFPG15_BUCKET_LAST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            struct ofputil_bucket *new_first;\n            const struct ofputil_bucket *first;\n\n            first = ofputil_bucket_list_front(&ofgroup->buckets);\n            new_first = ofputil_bucket_find(&new_ofgroup->buckets,\n                                            first->bucket_id);\n\n            ovs_list_splice(new_ofgroup->buckets.next, &new_first->list_node,\n                            CONST_CAST(struct ovs_list *,\n                                       &new_ofgroup->buckets));\n        }\n    } else if (command_bucket_id <= OFPG15_BUCKET_MAX && last) {\n        struct ofputil_bucket *after;\n\n        /* Presence of bucket is checked above so after should never be NULL */\n        after = ofputil_bucket_find(&new_ofgroup->buckets, command_bucket_id);\n\n        ovs_list_splice(after->list_node.next, new_ofgroup->buckets.next,\n                    last->list_node.next);\n    }\n\n    return 0;\n}\n\n/* Appends all of the a copy of all the buckets from 'ofgroup' to 'new_ofgroup'\n * with the exception of the bucket whose bucket id is 'command_bucket_id'.\n * Special 'command_bucket_id' values OFPG15_BUCKET_FIRST, OFPG15_BUCKET_LAST\n * and OFPG15_BUCKET_ALL are also honored. */\nstatic enum ofperr\ncopy_buckets_for_remove_bucket(const struct ofgroup *ofgroup,\n                               struct ofgroup *new_ofgroup,\n                               uint32_t command_bucket_id)\n{\n    const struct ofputil_bucket *skip = NULL;\n\n    if (command_bucket_id == OFPG15_BUCKET_ALL) {\n        return 0;\n    }\n\n    if (command_bucket_id == OFPG15_BUCKET_FIRST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            skip = ofputil_bucket_list_front(&ofgroup->buckets);\n        }\n    } else if (command_bucket_id == OFPG15_BUCKET_LAST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            skip = ofputil_bucket_list_back(&ofgroup->buckets);\n        }\n    } else {\n        skip = ofputil_bucket_find(&ofgroup->buckets, command_bucket_id);\n        if (!skip) {\n            return OFPERR_OFPGMFC_UNKNOWN_BUCKET;\n        }\n    }\n\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &new_ofgroup->buckets),\n                              &ofgroup->buckets, skip);\n\n    return 0;\n}\n\n/* Implements OFPGC11_MODIFY, OFPGC15_INSERT_BUCKET and\n * OFPGC15_REMOVE_BUCKET.  Returns 0 on success or an OpenFlow error code\n * on failure.\n *\n * Note that the group is re-created and then replaces the old group in\n * ofproto's ofgroup hash map. Thus, the group is never altered while users of\n * the xlate module hold a pointer to the group. */\nstatic enum ofperr\nmodify_group_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *old_group;          /* Modified group. */\n    struct ofgroup *new_group;\n    enum ofperr error;\n\n    old_group = ofproto_group_lookup__(ofproto, ogm->gm.group_id,\n                                       OVS_VERSION_MAX);\n    if (!old_group) {\n        return OFPERR_OFPGMFC_UNKNOWN_GROUP;\n    }\n\n    /* Inserting or deleting a bucket should not change the group's type or\n     * properties, so change the group mod so that these aspects match the old\n     * group.  (See EXT-570.)  */\n    if (ogm->gm.command == OFPGC15_INSERT_BUCKET ||\n        ogm->gm.command == OFPGC15_REMOVE_BUCKET) {\n        ogm->gm.type = old_group->type;\n        ofputil_group_properties_destroy(&ogm->gm.props);\n        ofputil_group_properties_copy(&ogm->gm.props, &old_group->props);\n    }\n\n    if (old_group->type != ogm->gm.type\n        && (ofproto->n_groups[ogm->gm.type]\n            >= ofproto->ogf.max_groups[ogm->gm.type])) {\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    error = init_group(ofproto, &ogm->gm, ogm->version, &ogm->new_group);\n    if (error) {\n        return error;\n    }\n    new_group = ogm->new_group;\n\n    /* Manipulate bucket list for bucket commands */\n    if (ogm->gm.command == OFPGC15_INSERT_BUCKET) {\n        error = copy_buckets_for_insert_bucket(old_group, new_group,\n                                               ogm->gm.command_bucket_id);\n    } else if (ogm->gm.command == OFPGC15_REMOVE_BUCKET) {\n        error = copy_buckets_for_remove_bucket(old_group, new_group,\n                                               ogm->gm.command_bucket_id);\n    }\n    if (error) {\n        goto out;\n    }\n\n    /* The group creation time does not change during modification. */\n    *CONST_CAST(long long int *, &(new_group->created)) = old_group->created;\n    *CONST_CAST(long long int *, &(new_group->modified)) = time_msec();\n\n    group_collection_add(&ogm->old_groups, old_group);\n\n    /* Mark the old group for deletion. */\n    versions_set_remove_version(&old_group->versions, ogm->version);\n    /* Insert replacement group. */\n    cmap_insert(&ofproto->groups, &new_group->cmap_node,\n                    hash_int(new_group->group_id, 0));\n    /* Transfer rules. */\n    rule_collection_move(&new_group->rules, &old_group->rules);\n\n    if (old_group->type != new_group->type) {\n        ofproto->n_groups[old_group->type]--;\n        ofproto->n_groups[new_group->type]++;\n    }\n    return 0;\n\nout:\n    ofproto_group_unref(new_group);\n    return error;\n}\n\n/* Implements the OFPGC11_ADD_OR_MOD command which creates the group when it does not\n * exist yet and modifies it otherwise */\nstatic enum ofperr\nadd_or_modify_group_start(struct ofproto *ofproto,\n                          struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    if (!ofproto_group_exists(ofproto, ogm->gm.group_id)) {\n        error = add_group_start(ofproto, ogm);\n    } else {\n        error = modify_group_start(ofproto, ogm);\n    }\n\n    return error;\n}\n\nstatic void\ndelete_group_start(struct ofproto *ofproto, ovs_version_t version,\n                   struct group_collection *groups, struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Makes flow deletion code leave the rule pointers in 'group->rules'\n     * intact, so that we can later refer to the rules deleted due to the group\n     * deletion.  Rule pointers will be removed from all other groups, if any,\n     * so we will never try to delete the same rule twice. */\n    group->being_deleted = true;\n\n    /* Mark all the referring groups for deletion. */\n    delete_flows_start__(ofproto, version, &group->rules);\n    group_collection_add(groups, group);\n    versions_set_remove_version(&group->versions, version);\n    ofproto->n_groups[group->type]--;\n}\n\nstatic void\ndelete_group_finish(struct ofproto *ofproto, struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Finish deletion of all flow entries containing this group in a group\n     * action. */\n    delete_flows_finish__(ofproto, &group->rules, OFPRR_GROUP_DELETE, NULL);\n\n    /* Group removal is postponed by the caller. */\n}\n\n/* Implements OFPGC11_DELETE. */\nstatic void\ndelete_groups_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *group;\n\n    if (ogm->gm.group_id == OFPG_ALL) {\n        CMAP_FOR_EACH (group, cmap_node, &ofproto->groups) {\n            if (versions_visible_in_version(&group->versions, ogm->version)) {\n                delete_group_start(ofproto, ogm->version, &ogm->old_groups,\n                                   group);\n            }\n        }\n    } else {\n        group = ofproto_group_lookup__(ofproto, ogm->gm.group_id, ogm->version);\n        if (group) {\n            delete_group_start(ofproto, ogm->version, &ogm->old_groups, group);\n        }\n    }\n}\n\nstatic enum ofperr\nofproto_group_mod_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    ogm->new_group = NULL;\n    group_collection_init(&ogm->old_groups);\n\n    switch (ogm->gm.command) {\n    case OFPGC11_ADD:\n        error = add_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_MODIFY:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_ADD_OR_MOD:\n        error = add_or_modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_DELETE:\n        delete_groups_start(ofproto, ogm);\n        error = 0;\n        break;\n\n    case OFPGC15_INSERT_BUCKET:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC15_REMOVE_BUCKET:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    default:\n        if (ogm->gm.command > OFPGC11_DELETE) {\n            VLOG_INFO_RL(&rl, \"%s: Invalid group_mod command type %d\",\n                         ofproto->name, ogm->gm.command);\n        }\n        error = OFPERR_OFPGMFC_BAD_COMMAND;\n        break;\n    }\n    return error;\n}\n\nstatic void\nofproto_group_mod_revert(struct ofproto *ofproto,\n                         struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *new_group = ogm->new_group;\n    struct ofgroup *old_group;\n\n    /* Restore replaced or deleted groups. */\n    GROUP_COLLECTION_FOR_EACH (old_group, &ogm->old_groups) {\n        ofproto->n_groups[old_group->type]++;\n        if (new_group) {\n            ovs_assert(group_collection_n(&ogm->old_groups) == 1);\n            /* Transfer rules back. */\n            rule_collection_move(&old_group->rules, &new_group->rules);\n        } else {\n            old_group->being_deleted = false;\n            /* Revert rule deletion. */\n            delete_flows_revert__(ofproto, &old_group->rules);\n        }\n        /* Restore visibility. */\n        versions_set_remove_version(&old_group->versions,\n                                    OVS_VERSION_NOT_REMOVED);\n    }\n    if (new_group) {\n        /* Remove the new group immediately.  It was never visible to\n         * lookups. */\n        cmap_remove(&ofproto->groups, &new_group->cmap_node,\n                    hash_int(new_group->group_id, 0));\n        ofproto->n_groups[new_group->type]--;\n        ofproto_group_unref(new_group);\n    }\n}\n\nstatic void\nofproto_group_mod_finish(struct ofproto *ofproto,\n                         struct ofproto_group_mod *ogm,\n                         const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *new_group = ogm->new_group;\n    struct ofgroup *old_group;\n\n    if (new_group && group_collection_n(&ogm->old_groups) &&\n        ofproto->ofproto_class->group_modify) {\n        /* Modify a group. */\n        ovs_assert(group_collection_n(&ogm->old_groups) == 1);\n\n        /* XXX: OK to lose old group's stats? */\n        ofproto->ofproto_class->group_modify(new_group);\n    }\n\n    /* Delete old groups. */\n    GROUP_COLLECTION_FOR_EACH(old_group, &ogm->old_groups) {\n        delete_group_finish(ofproto, old_group);\n    }\n    remove_groups_postponed(&ogm->old_groups);\n\n    if (req) {\n        struct ofputil_requestforward rf;\n        rf.xid = req->request->xid;\n        rf.reason = OFPRFR_GROUP_MOD;\n        rf.group_mod = &ogm->gm;\n        connmgr_send_requestforward(ofproto->connmgr, req->ofconn, &rf);\n    }\n}\n\n/* Delete all groups from 'ofproto'.\n *\n * This is intended for use within an ofproto provider's 'destruct'\n * function. */\nvoid\nofproto_group_delete_all(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto_group_mod ogm;\n\n    ogm.gm.command = OFPGC11_DELETE;\n    ogm.gm.group_id = OFPG_ALL;\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ogm.version = ofproto->tables_version + 1;\n    ofproto_group_mod_start(ofproto, &ogm);\n    ofproto_bump_tables_version(ofproto);\n    ofproto_group_mod_finish(ofproto, &ogm, NULL);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic enum ofperr\nhandle_group_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofproto_group_mod ogm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_group_mod(oh, &ogm.gm);\n    if (error) {\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ogm.version = ofproto->tables_version + 1;\n    error = ofproto_group_mod_start(ofproto, &ogm);\n    if (!error) {\n        struct openflow_mod_requester req = { ofconn, oh };\n\n        ofproto_bump_tables_version(ofproto);\n        ofproto_group_mod_finish(ofproto, &ogm, &req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ofputil_uninit_group_mod(&ogm.gm);\n\n    return error;\n}\n\nenum ofputil_table_miss\nofproto_table_get_miss_config(const struct ofproto *ofproto, uint8_t table_id)\n{\n    enum ofputil_table_miss miss;\n\n    atomic_read_relaxed(&ofproto->tables[table_id].miss_config, &miss);\n    return miss;\n}\n\nstatic void\ntable_mod__(struct oftable *oftable,\n            const struct ofputil_table_mod *tm)\n{\n    if (tm->miss == OFPUTIL_TABLE_MISS_DEFAULT) {\n        /* This is how an OFPT_TABLE_MOD decodes if it doesn't specify any\n         * table-miss configuration (because the protocol used doesn't have\n         * such a concept), so there's nothing to do. */\n    } else {\n        atomic_store_relaxed(&oftable->miss_config, tm->miss);\n    }\n\n    unsigned int new_eviction = oftable->eviction;\n    if (tm->eviction == OFPUTIL_TABLE_EVICTION_ON) {\n        new_eviction |= EVICTION_OPENFLOW;\n    } else if (tm->eviction == OFPUTIL_TABLE_EVICTION_OFF) {\n        new_eviction &= ~EVICTION_OPENFLOW;\n    }\n\n    if (new_eviction != oftable->eviction) {\n        ovs_mutex_lock(&ofproto_mutex);\n        oftable_configure_eviction(oftable, new_eviction,\n                                   oftable->eviction_fields,\n                                   oftable->n_eviction_fields);\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    if (tm->vacancy != OFPUTIL_TABLE_VACANCY_DEFAULT) {\n        ovs_mutex_lock(&ofproto_mutex);\n        oftable->vacancy_down = tm->table_vacancy.vacancy_down;\n        oftable->vacancy_up = tm->table_vacancy.vacancy_up;\n        if (tm->vacancy == OFPUTIL_TABLE_VACANCY_OFF) {\n            oftable->vacancy_event = 0;\n        } else if (!oftable->vacancy_event) {\n            uint8_t vacancy = oftable_vacancy(oftable);\n            oftable->vacancy_event = (vacancy < oftable->vacancy_up\n                                      ? OFPTR_VACANCY_UP\n                                      : OFPTR_VACANCY_DOWN);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n}\n\nstatic enum ofperr\ntable_mod(struct ofproto *ofproto, const struct ofputil_table_mod *tm)\n{\n    if (!check_table_id(ofproto, tm->table_id)) {\n        return OFPERR_OFPTMFC_BAD_TABLE;\n    }\n\n    /* Don't allow the eviction flags to be changed (except to the only fixed\n     * value that OVS supports).  OF1.4 says this is normal: \"The\n     * OFPTMPT_EVICTION property usually cannot be modified using a\n     * OFP_TABLE_MOD request, because the eviction mechanism is switch\n     * defined\". */\n    if (tm->eviction_flags != UINT32_MAX\n        && tm->eviction_flags != OFPROTO_EVICTION_FLAGS) {\n        return OFPERR_OFPTMFC_BAD_CONFIG;\n    }\n\n    if (tm->table_id == OFPTT_ALL) {\n        struct oftable *oftable;\n        OFPROTO_FOR_EACH_TABLE (oftable, ofproto) {\n            if (!(oftable->flags & (OFTABLE_HIDDEN | OFTABLE_READONLY))) {\n                table_mod__(oftable, tm);\n            }\n        }\n    } else {\n        struct oftable *oftable = &ofproto->tables[tm->table_id];\n        if (oftable->flags & OFTABLE_READONLY) {\n            return OFPERR_OFPTMFC_EPERM;\n        }\n        table_mod__(oftable, tm);\n    }\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_mod tm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_table_mod(oh, &tm);\n    if (error) {\n        return error;\n    }\n\n    return table_mod(ofproto, &tm);\n}\n\n/* Free resources that may be allocated by ofproto_flow_mod_init(). */\nvoid\nofproto_flow_mod_uninit(struct ofproto_flow_mod *ofm)\n{\n    if (ofm->temp_rule) {\n        ofproto_rule_unref(ofm->temp_rule);\n        ofm->temp_rule = NULL;\n    }\n    if (ofm->criteria.version != OVS_VERSION_NOT_REMOVED) {\n        rule_criteria_destroy(&ofm->criteria);\n    }\n    if (ofm->conjs) {\n        free(ofm->conjs);\n        ofm->conjs = NULL;\n        ofm->n_conjs = 0;\n    }\n}\n\n/* Initializes 'ofm' with 'ofproto', 'fm', and 'rule'.  'rule' may be null, but\n * if it is nonnull then the caller must own a reference to it, which on\n * success is transferred to 'ofm' and on failure is unreffed. */\nstatic enum ofperr\nofproto_flow_mod_init(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                      const struct ofputil_flow_mod *fm, struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error;\n\n    /* Forward flow mod fields we need later. */\n    ofm->command = fm->command;\n    ofm->modify_cookie = fm->modify_cookie;\n\n    ofm->modify_may_add_flow = (fm->new_cookie != OVS_BE64_MAX\n                                && fm->cookie_mask == htonll(0));\n    /* Old flags must be kept when modifying a flow, but we still must\n     * honor the reset counts flag if present in the flow mod. */\n    ofm->modify_keep_counts = !(fm->flags & OFPUTIL_FF_RESET_COUNTS);\n\n    /* Initialize state needed by ofproto_flow_mod_uninit(). */\n    ofm->temp_rule = rule;\n    ofm->criteria.version = OVS_VERSION_NOT_REMOVED;\n    ofm->conjs = NULL;\n    ofm->n_conjs = 0;\n\n    bool check_buffer_id = false;\n\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        check_buffer_id = true;\n        error = add_flow_init(ofproto, ofm, fm);\n        break;\n    case OFPFC_MODIFY:\n        check_buffer_id = true;\n        error = modify_flows_init_loose(ofproto, ofm, fm);\n        break;\n    case OFPFC_MODIFY_STRICT:\n        check_buffer_id = true;\n        error = modify_flow_init_strict(ofproto, ofm, fm);\n        break;\n    case OFPFC_DELETE:\n        error = delete_flows_init_loose(ofproto, ofm, fm);\n        break;\n    case OFPFC_DELETE_STRICT:\n        error = delete_flows_init_strict(ofproto, ofm, fm);\n        break;\n    default:\n        error = OFPERR_OFPFMFC_BAD_COMMAND;\n        break;\n    }\n    if (!error && check_buffer_id && fm->buffer_id != UINT32_MAX) {\n        error = OFPERR_OFPBRC_BUFFER_UNKNOWN;\n    }\n\n    if (error) {\n        ofproto_flow_mod_uninit(ofm);\n    }\n    return error;\n}\n\nstatic enum ofperr\nofproto_flow_mod_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    rule_collection_init(&ofm->old_rules);\n    rule_collection_init(&ofm->new_rules);\n\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        error = add_flow_start(ofproto, ofm);\n        break;\n    case OFPFC_MODIFY:\n        error = modify_flows_start_loose(ofproto, ofm);\n        break;\n    case OFPFC_MODIFY_STRICT:\n        error = modify_flow_start_strict(ofproto, ofm);\n        break;\n    case OFPFC_DELETE:\n        error = delete_flows_start_loose(ofproto, ofm);\n        break;\n    case OFPFC_DELETE_STRICT:\n        error = delete_flow_start_strict(ofproto, ofm);\n        break;\n    default:\n        OVS_NOT_REACHED();\n    }\n    /* Release resources not needed after start. */\n    ofproto_flow_mod_uninit(ofm);\n\n    if (error) {\n        rule_collection_destroy(&ofm->old_rules);\n        rule_collection_destroy(&ofm->new_rules);\n    }\n    return error;\n}\n\nstatic void\nofproto_flow_mod_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        add_flow_revert(ofproto, ofm);\n        break;\n\n    case OFPFC_MODIFY:\n    case OFPFC_MODIFY_STRICT:\n        modify_flows_revert(ofproto, ofm);\n        break;\n\n    case OFPFC_DELETE:\n    case OFPFC_DELETE_STRICT:\n        delete_flows_revert(ofproto, ofm);\n        break;\n\n    default:\n        break;\n    }\n\n    rule_collection_destroy(&ofm->old_rules);\n    rule_collection_destroy(&ofm->new_rules);\n}\n\nstatic void\nofproto_flow_mod_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                        const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        add_flow_finish(ofproto, ofm, req);\n        break;\n\n    case OFPFC_MODIFY:\n    case OFPFC_MODIFY_STRICT:\n        modify_flows_finish(ofproto, ofm, req);\n        break;\n\n    case OFPFC_DELETE:\n    case OFPFC_DELETE_STRICT:\n        delete_flows_finish(ofproto, ofm, req);\n        break;\n\n    default:\n        break;\n    }\n\n    rule_collection_destroy(&ofm->old_rules);\n    rule_collection_destroy(&ofm->new_rules);\n\n    if (req) {\n        ofconn_report_flow_mod(req->ofconn, ofm->command);\n    }\n}\n\n/* Commit phases (all while locking ofproto_mutex):\n *\n * 1. Begin: Gather resources and make changes visible in the next version.\n *           - Mark affected rules for removal in the next version.\n *           - Create new replacement rules, make visible in the next\n *             version.\n *           - Do not send any events or notifications.\n *\n * 2. Revert: Fail if any errors are found.  After this point no errors are\n * possible.  No visible changes were made, so rollback is minimal (remove\n * added invisible rules, restore visibility of rules marked for removal).\n *\n * 3. Finish: Make the changes visible for lookups. Insert replacement rules to\n * the ofproto provider. Remove replaced and deleted rules from ofproto data\n * structures, and Schedule postponed removal of deleted rules from the\n * classifier.  Send notifications, buffered packets, etc.\n */\nstatic enum ofperr\ndo_bundle_commit(struct ofconn *ofconn, uint32_t id, uint16_t flags)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    ovs_version_t version = ofproto->tables_version + 1;\n    struct ofp_bundle *bundle;\n    struct ofp_bundle_entry *be;\n    enum ofperr error;\n\n    bundle = ofconn_get_bundle(ofconn, id);\n\n    if (!bundle) {\n        return OFPERR_OFPBFC_BAD_ID;\n    }\n    if (bundle->flags != flags) {\n        error = OFPERR_OFPBFC_BAD_FLAGS;\n    } else {\n        bool prev_is_port_mod = false;\n\n        error = 0;\n        ovs_mutex_lock(&ofproto_mutex);\n\n        /* 1. Begin. */\n        LIST_FOR_EACH (be, node, &bundle->msg_list) {\n            if (be->type == OFPTYPE_PORT_MOD) {\n                /* Our port mods are not atomic. */\n                if (flags & OFPBF_ATOMIC) {\n                    error = OFPERR_OFPBFC_MSG_FAILED;\n                } else {\n                    prev_is_port_mod = true;\n                    error = port_mod_start(ofconn, &be->opm.pm, &be->opm.port);\n                }\n            } else {\n                /* Flow & group mods between port mods are applied as a single\n                 * version, but the versions are published only after we know\n                 * the commit is successful. */\n                if (prev_is_port_mod) {\n                    prev_is_port_mod = false;\n                    ++version;\n                }\n                if (be->type == OFPTYPE_FLOW_MOD) {\n                    /* Store the version in which the changes should take\n                     * effect. */\n                    be->ofm.version = version;\n                    error = ofproto_flow_mod_start(ofproto, &be->ofm);\n                } else if (be->type == OFPTYPE_GROUP_MOD) {\n                    /* Store the version in which the changes should take\n                     * effect. */\n                    be->ogm.version = version;\n                    error = ofproto_group_mod_start(ofproto, &be->ogm);\n                } else if (be->type == OFPTYPE_PACKET_OUT) {\n                    be->opo.version = version;\n                    error = ofproto_packet_out_start(ofproto, &be->opo);\n                } else {\n                    OVS_NOT_REACHED();\n                }\n            }\n            if (error) {\n                break;\n            }\n        }\n\n        if (error) {\n            /* Send error referring to the original message. */\n            if (error) {\n                ofconn_send_error(ofconn, &be->ofp_msg, error);\n                error = OFPERR_OFPBFC_MSG_FAILED;\n            }\n\n            /* 2. Revert.  Undo all the changes made above. */\n            LIST_FOR_EACH_REVERSE_CONTINUE(be, node, &bundle->msg_list) {\n                if (be->type == OFPTYPE_FLOW_MOD) {\n                    ofproto_flow_mod_revert(ofproto, &be->ofm);\n                } else if (be->type == OFPTYPE_GROUP_MOD) {\n                    ofproto_group_mod_revert(ofproto, &be->ogm);\n                } else if (be->type == OFPTYPE_PACKET_OUT) {\n                    ofproto_packet_out_revert(ofproto, &be->opo);\n                }\n                /* Nothing needs to be reverted for a port mod. */\n            }\n        } else {\n            /* 4. Finish. */\n            LIST_FOR_EACH (be, node, &bundle->msg_list) {\n                if (be->type == OFPTYPE_PORT_MOD) {\n                    /* Perform the actual port mod. This is not atomic, i.e.,\n                     * the effects will be immediately seen by upcall\n                     * processing regardless of the lookup version.  It should\n                     * be noted that port configuration changes can originate\n                     * also from OVSDB changes asynchronously to all upcall\n                     * processing. */\n                    port_mod_finish(ofconn, &be->opm.pm, be->opm.port);\n                } else {\n                    version =\n                        (be->type == OFPTYPE_FLOW_MOD) ? be->ofm.version :\n                        (be->type == OFPTYPE_GROUP_MOD) ? be->ogm.version :\n                        (be->type == OFPTYPE_PACKET_OUT) ? be->opo.version :\n                        version;\n\n                    /* Bump the lookup version to the one of the current\n                     * message.  This makes all the changes in the bundle at\n                     * this version visible to lookups at once. */\n                    if (ofproto->tables_version < version) {\n                        ofproto->tables_version = version;\n                        ofproto->ofproto_class->set_tables_version(\n                            ofproto, ofproto->tables_version);\n                    }\n\n                    struct openflow_mod_requester req = { ofconn,\n                                                          &be->ofp_msg };\n\n                    if (be->type == OFPTYPE_FLOW_MOD) {\n                        ofproto_flow_mod_finish(ofproto, &be->ofm, &req);\n                    } else if (be->type == OFPTYPE_GROUP_MOD) {\n                        ofproto_group_mod_finish(ofproto, &be->ogm, &req);\n                    } else if (be->type == OFPTYPE_PACKET_OUT) {\n                        ofproto_packet_out_finish(ofproto, &be->opo);\n                    }\n                }\n            }\n        }\n\n        ofmonitor_flush(ofproto->connmgr);\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    /* The bundle is discarded regardless the outcome. */\n    ofp_bundle_remove__(ofconn, bundle);\n    return error;\n}\n\nstatic enum ofperr\nhandle_bundle_control(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_bundle_ctrl_msg bctrl;\n    struct ofputil_bundle_ctrl_msg reply;\n    struct ofpbuf *buf;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_bundle_ctrl(oh, &bctrl);\n    if (error) {\n        return error;\n    }\n    reply.flags = 0;\n    reply.bundle_id = bctrl.bundle_id;\n\n    switch (bctrl.type) {\n    case OFPBCT_OPEN_REQUEST:\n        error = ofp_bundle_open(ofconn, bctrl.bundle_id, bctrl.flags, oh);\n        reply.type = OFPBCT_OPEN_REPLY;\n        break;\n    case OFPBCT_CLOSE_REQUEST:\n        error = ofp_bundle_close(ofconn, bctrl.bundle_id, bctrl.flags);\n        reply.type = OFPBCT_CLOSE_REPLY;\n        break;\n    case OFPBCT_COMMIT_REQUEST:\n        error = do_bundle_commit(ofconn, bctrl.bundle_id, bctrl.flags);\n        reply.type = OFPBCT_COMMIT_REPLY;\n        break;\n    case OFPBCT_DISCARD_REQUEST:\n        error = ofp_bundle_discard(ofconn, bctrl.bundle_id);\n        reply.type = OFPBCT_DISCARD_REPLY;\n        break;\n\n    case OFPBCT_OPEN_REPLY:\n    case OFPBCT_CLOSE_REPLY:\n    case OFPBCT_COMMIT_REPLY:\n    case OFPBCT_DISCARD_REPLY:\n        return OFPERR_OFPBFC_BAD_TYPE;\n        break;\n    }\n\n    if (!error) {\n        buf = ofputil_encode_bundle_ctrl_reply(oh, &reply);\n        ofconn_send_reply(ofconn, buf);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_bundle_add(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    enum ofperr error;\n    struct ofputil_bundle_add_msg badd;\n    struct ofp_bundle_entry *bmsg;\n    enum ofptype type;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_bundle_add(oh, &badd, &type);\n    if (error) {\n        return error;\n    }\n\n    bmsg = ofp_bundle_entry_alloc(type, badd.msg);\n\n    struct ofpbuf ofpacts;\n    uint64_t ofpacts_stub[1024 / 8];\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n\n    if (type == OFPTYPE_PORT_MOD) {\n        error = ofputil_decode_port_mod(badd.msg, &bmsg->opm.pm, false);\n    } else if (type == OFPTYPE_FLOW_MOD) {\n        struct ofputil_flow_mod fm;\n\n        error = ofputil_decode_flow_mod(&fm, badd.msg,\n                                        ofconn_get_protocol(ofconn),\n                                        ofproto_get_tun_tab(ofproto),\n                                        &ofproto->vl_mff_map, &ofpacts,\n                                        u16_to_ofp(ofproto->max_ports),\n                                        ofproto->n_tables);\n        if (!error) {\n            error = ofproto_flow_mod_init(ofproto, &bmsg->ofm, &fm, NULL);\n        }\n    } else if (type == OFPTYPE_GROUP_MOD) {\n        error = ofputil_decode_group_mod(badd.msg, &bmsg->ogm.gm);\n    } else if (type == OFPTYPE_PACKET_OUT) {\n        struct ofputil_packet_out po;\n\n        COVERAGE_INC(ofproto_packet_out);\n\n        /* Decode message. */\n        error = ofputil_decode_packet_out(&po, badd.msg, &ofpacts);\n        if (!error) {\n            po.ofpacts = ofpbuf_steal_data(&ofpacts);   /* Move to heap. */\n            error = ofproto_packet_out_init(ofproto, ofconn, &bmsg->opo, &po);\n        }\n    } else {\n        OVS_NOT_REACHED();\n    }\n\n    ofpbuf_uninit(&ofpacts);\n\n    if (!error) {\n        error = ofp_bundle_add_message(ofconn, badd.bundle_id, badd.flags,\n                                       bmsg, oh);\n    }\n\n    if (error) {\n        ofp_bundle_entry_free(bmsg);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_tlv_table_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct tun_table *old_tab, *new_tab;\n    struct ofputil_tlv_table_mod ttm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_tlv_table_mod(oh, &ttm);\n    if (error) {\n        return error;\n    }\n\n    old_tab = ovsrcu_get_protected(struct tun_table *, &ofproto->metadata_tab);\n    error = tun_metadata_table_mod(&ttm, old_tab, &new_tab);\n    if (!error) {\n        ovs_mutex_lock(&ofproto->vl_mff_map.mutex);\n        error = mf_vl_mff_map_mod_from_tun_metadata(&ofproto->vl_mff_map,\n                                                    &ttm);\n        ovs_mutex_unlock(&ofproto->vl_mff_map.mutex);\n        if (!error) {\n            ovsrcu_set(&ofproto->metadata_tab, new_tab);\n            tun_metadata_postpone_free(old_tab);\n        }\n    }\n\n    ofputil_uninit_tlv_table(&ttm.mappings);\n    return error;\n}\n\nstatic enum ofperr\nhandle_tlv_table_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    const struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_tlv_table_reply ttr;\n    struct ofpbuf *b;\n\n    tun_metadata_table_request(ofproto_get_tun_tab(ofproto), &ttr);\n\n    b = ofputil_encode_tlv_table_reply(oh, &ttr);\n    ofputil_uninit_tlv_table(&ttr.mappings);\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_openflow__(struct ofconn *ofconn, const struct ofpbuf *msg)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct ofp_header *oh = msg->data;\n    enum ofptype type;\n    enum ofperr error;\n\n    error = ofptype_decode(&type, oh);\n    if (error) {\n        return error;\n    }\n    if (oh->version >= OFP13_VERSION && ofpmsg_is_stat_request(oh)\n        && ofpmp_more(oh)) {\n        /* We have no buffer implementation for multipart requests.\n         * Report overflow for requests which consists of multiple\n         * messages. */\n        return OFPERR_OFPBRC_MULTIPART_BUFFER_OVERFLOW;\n    }\n\n    switch (type) {\n        /* OpenFlow requests. */\n    case OFPTYPE_ECHO_REQUEST:\n        return handle_echo_request(ofconn, oh);\n\n    case OFPTYPE_FEATURES_REQUEST:\n        return handle_features_request(ofconn, oh);\n\n    case OFPTYPE_GET_CONFIG_REQUEST:\n        return handle_get_config_request(ofconn, oh);\n\n    case OFPTYPE_SET_CONFIG:\n        return handle_set_config(ofconn, oh);\n\n    case OFPTYPE_PACKET_OUT:\n        return handle_packet_out(ofconn, oh);\n\n    case OFPTYPE_PORT_MOD:\n        return handle_port_mod(ofconn, oh);\n\n    case OFPTYPE_FLOW_MOD:\n        return handle_flow_mod(ofconn, oh);\n\n    case OFPTYPE_GROUP_MOD:\n        return handle_group_mod(ofconn, oh);\n\n    case OFPTYPE_TABLE_MOD:\n        return handle_table_mod(ofconn, oh);\n\n    case OFPTYPE_METER_MOD:\n        return handle_meter_mod(ofconn, oh);\n\n    case OFPTYPE_BARRIER_REQUEST:\n        return handle_barrier_request(ofconn, oh);\n\n    case OFPTYPE_ROLE_REQUEST:\n        return handle_role_request(ofconn, oh);\n\n        /* OpenFlow replies. */\n    case OFPTYPE_ECHO_REPLY:\n        return 0;\n\n        /* Nicira extension requests. */\n    case OFPTYPE_FLOW_MOD_TABLE_ID:\n        return handle_nxt_flow_mod_table_id(ofconn, oh);\n\n    case OFPTYPE_SET_FLOW_FORMAT:\n        return handle_nxt_set_flow_format(ofconn, oh);\n\n    case OFPTYPE_SET_PACKET_IN_FORMAT:\n        return handle_nxt_set_packet_in_format(ofconn, oh);\n\n    case OFPTYPE_SET_CONTROLLER_ID:\n        return handle_nxt_set_controller_id(ofconn, oh);\n\n    case OFPTYPE_FLOW_AGE:\n        /* Nothing to do. */\n        return 0;\n\n    case OFPTYPE_FLOW_MONITOR_CANCEL:\n        return handle_flow_monitor_cancel(ofconn, oh);\n\n    case OFPTYPE_SET_ASYNC_CONFIG:\n        return handle_nxt_set_async_config(ofconn, oh);\n\n    case OFPTYPE_GET_ASYNC_REQUEST:\n        return handle_nxt_get_async_request(ofconn, oh);\n\n    case OFPTYPE_NXT_RESUME:\n        return handle_nxt_resume(ofconn, oh);\n\n        /* Statistics requests. */\n    case OFPTYPE_DESC_STATS_REQUEST:\n        return handle_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_FLOW_STATS_REQUEST:\n        return handle_flow_stats_request(ofconn, oh);\n\n    case OFPTYPE_AGGREGATE_STATS_REQUEST:\n        return handle_aggregate_stats_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_STATS_REQUEST:\n        return handle_table_stats_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_FEATURES_STATS_REQUEST:\n        return handle_table_features_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_DESC_REQUEST:\n        return handle_table_desc_request(ofconn, oh);\n\n    case OFPTYPE_PORT_STATS_REQUEST:\n        return handle_port_stats_request(ofconn, oh);\n\n    case OFPTYPE_QUEUE_STATS_REQUEST:\n        return handle_queue_stats_request(ofconn, oh);\n\n    case OFPTYPE_PORT_DESC_STATS_REQUEST:\n        return handle_port_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_FLOW_MONITOR_STATS_REQUEST:\n        return handle_flow_monitor_request(ofconn, oh);\n\n    case OFPTYPE_METER_STATS_REQUEST:\n    case OFPTYPE_METER_CONFIG_STATS_REQUEST:\n        return handle_meter_request(ofconn, oh, type);\n\n    case OFPTYPE_METER_FEATURES_STATS_REQUEST:\n        return handle_meter_features_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_STATS_REQUEST:\n        return handle_group_stats_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_DESC_STATS_REQUEST:\n        return handle_group_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_FEATURES_STATS_REQUEST:\n        return handle_group_features_stats_request(ofconn, oh);\n\n    case OFPTYPE_QUEUE_GET_CONFIG_REQUEST:\n        return handle_queue_get_config_request(ofconn, oh);\n\n    case OFPTYPE_BUNDLE_CONTROL:\n        return handle_bundle_control(ofconn, oh);\n\n    case OFPTYPE_BUNDLE_ADD_MESSAGE:\n        return handle_bundle_add(ofconn, oh);\n\n    case OFPTYPE_NXT_TLV_TABLE_MOD:\n        return handle_tlv_table_mod(ofconn, oh);\n\n    case OFPTYPE_NXT_TLV_TABLE_REQUEST:\n        return handle_tlv_table_request(ofconn, oh);\n\n    case OFPTYPE_IPFIX_BRIDGE_STATS_REQUEST:\n        return handle_ipfix_bridge_stats_request(ofconn, oh);\n\n    case OFPTYPE_IPFIX_FLOW_STATS_REQUEST:\n        return handle_ipfix_flow_stats_request(ofconn, oh);\n\n    case OFPTYPE_CT_FLUSH_ZONE:\n        return handle_nxt_ct_flush_zone(ofconn, oh);\n\n    case OFPTYPE_HELLO:\n    case OFPTYPE_ERROR:\n    case OFPTYPE_FEATURES_REPLY:\n    case OFPTYPE_GET_CONFIG_REPLY:\n    case OFPTYPE_PACKET_IN:\n    case OFPTYPE_FLOW_REMOVED:\n    case OFPTYPE_PORT_STATUS:\n    case OFPTYPE_BARRIER_REPLY:\n    case OFPTYPE_QUEUE_GET_CONFIG_REPLY:\n    case OFPTYPE_DESC_STATS_REPLY:\n    case OFPTYPE_FLOW_STATS_REPLY:\n    case OFPTYPE_QUEUE_STATS_REPLY:\n    case OFPTYPE_PORT_STATS_REPLY:\n    case OFPTYPE_TABLE_STATS_REPLY:\n    case OFPTYPE_AGGREGATE_STATS_REPLY:\n    case OFPTYPE_PORT_DESC_STATS_REPLY:\n    case OFPTYPE_ROLE_REPLY:\n    case OFPTYPE_FLOW_MONITOR_PAUSED:\n    case OFPTYPE_FLOW_MONITOR_RESUMED:\n    case OFPTYPE_FLOW_MONITOR_STATS_REPLY:\n    case OFPTYPE_GET_ASYNC_REPLY:\n    case OFPTYPE_GROUP_STATS_REPLY:\n    case OFPTYPE_GROUP_DESC_STATS_REPLY:\n    case OFPTYPE_GROUP_FEATURES_STATS_REPLY:\n    case OFPTYPE_METER_STATS_REPLY:\n    case OFPTYPE_METER_CONFIG_STATS_REPLY:\n    case OFPTYPE_METER_FEATURES_STATS_REPLY:\n    case OFPTYPE_TABLE_FEATURES_STATS_REPLY:\n    case OFPTYPE_TABLE_DESC_REPLY:\n    case OFPTYPE_ROLE_STATUS:\n    case OFPTYPE_REQUESTFORWARD:\n    case OFPTYPE_TABLE_STATUS:\n    case OFPTYPE_NXT_TLV_TABLE_REPLY:\n    case OFPTYPE_IPFIX_BRIDGE_STATS_REPLY:\n    case OFPTYPE_IPFIX_FLOW_STATS_REPLY:\n    default:\n        if (ofpmsg_is_stat_request(oh)) {\n            return OFPERR_OFPBRC_BAD_STAT;\n        } else {\n            return OFPERR_OFPBRC_BAD_TYPE;\n        }\n    }\n}\n\nstatic void\nhandle_openflow(struct ofconn *ofconn, const struct ofpbuf *ofp_msg)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error = handle_openflow__(ofconn, ofp_msg);\n\n    if (error) {\n        ofconn_send_error(ofconn, ofp_msg->data, error);\n    }\n    COVERAGE_INC(ofproto_recv_openflow);\n}\n\f\nstatic uint64_t\npick_datapath_id(const struct ofproto *ofproto)\n{\n    const struct ofport *port;\n\n    port = ofproto_get_port(ofproto, OFPP_LOCAL);\n    if (port) {\n        struct eth_addr ea;\n        int error;\n\n        error = netdev_get_etheraddr(port->netdev, &ea);\n        if (!error) {\n            return eth_addr_to_uint64(ea);\n        }\n        VLOG_WARN(\"%s: could not get MAC address for %s (%s)\",\n                  ofproto->name, netdev_get_name(port->netdev),\n                  ovs_strerror(error));\n    }\n    return ofproto->fallback_dpid;\n}\n\nstatic uint64_t\npick_fallback_dpid(void)\n{\n    struct eth_addr ea;\n    eth_addr_nicira_random(&ea);\n    return eth_addr_to_uint64(ea);\n}\n\f\n/* Table overflow policy. */\n\n/* Chooses and updates 'rulep' with a rule to evict from 'table'.  Sets 'rulep'\n * to NULL if the table is not configured to evict rules or if the table\n * contains no evictable rules.  (Rules with a readlock on their evict rwlock,\n * or with no timeouts are not evictable.) */\nstatic bool\nchoose_rule_to_evict(struct oftable *table, struct rule **rulep)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct eviction_group *evg;\n\n    *rulep = NULL;\n    if (!table->eviction) {\n        return false;\n    }\n\n    /* In the common case, the outer and inner loops here will each be entered\n     * exactly once:\n     *\n     *   - The inner loop normally \"return\"s in its first iteration.  If the\n     *     eviction group has any evictable rules, then it always returns in\n     *     some iteration.\n     *\n     *   - The outer loop only iterates more than once if the largest eviction\n     *     group has no evictable rules.\n     *\n     *   - The outer loop can exit only if table's 'max_flows' is all filled up\n     *     by unevictable rules. */\n    HEAP_FOR_EACH (evg, size_node, &table->eviction_groups_by_size) {\n        struct rule *rule;\n\n        HEAP_FOR_EACH (rule, evg_node, &evg->rules) {\n            *rulep = rule;\n            return true;\n        }\n    }\n\n    return false;\n}\n\f\n/* Eviction groups. */\n\n/* Returns the priority to use for an eviction_group that contains 'n_rules'\n * rules.  The priority contains low-order random bits to ensure that eviction\n * groups with the same number of rules are prioritized randomly. */\nstatic uint32_t\neviction_group_priority(size_t n_rules)\n{\n    uint16_t size = MIN(UINT16_MAX, n_rules);\n    return (size << 16) | random_uint16();\n}\n\n/* Updates 'evg', an eviction_group within 'table', following a change that\n * adds or removes rules in 'evg'. */\nstatic void\neviction_group_resized(struct oftable *table, struct eviction_group *evg)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    heap_change(&table->eviction_groups_by_size, &evg->size_node,\n                eviction_group_priority(heap_count(&evg->rules)));\n}\n\n/* Destroys 'evg', an eviction_group within 'table':\n *\n *   - Removes all the rules, if any, from 'evg'.  (It doesn't destroy the\n *     rules themselves, just removes them from the eviction group.)\n *\n *   - Removes 'evg' from 'table'.\n *\n *   - Frees 'evg'. */\nstatic void\neviction_group_destroy(struct oftable *table, struct eviction_group *evg)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    while (!heap_is_empty(&evg->rules)) {\n        struct rule *rule;\n\n        rule = CONTAINER_OF(heap_pop(&evg->rules), struct rule, evg_node);\n        rule->eviction_group = NULL;\n    }\n    hmap_remove(&table->eviction_groups_by_id, &evg->id_node);\n    heap_remove(&table->eviction_groups_by_size, &evg->size_node);\n    heap_destroy(&evg->rules);\n    free(evg);\n}\n\n/* Removes 'rule' from its eviction group, if any. */\nstatic void\neviction_group_remove_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule->eviction_group) {\n        struct oftable *table = &rule->ofproto->tables[rule->table_id];\n        struct eviction_group *evg = rule->eviction_group;\n\n        rule->eviction_group = NULL;\n        heap_remove(&evg->rules, &rule->evg_node);\n        if (heap_is_empty(&evg->rules)) {\n            eviction_group_destroy(table, evg);\n        } else {\n            eviction_group_resized(table, evg);\n        }\n    }\n}\n\n/* Hashes the 'rule''s values for the eviction_fields of 'rule''s table, and\n * returns the hash value. */\nstatic uint32_t\neviction_group_hash_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table = &rule->ofproto->tables[rule->table_id];\n    const struct mf_subfield *sf;\n    struct flow flow;\n    uint32_t hash;\n\n    hash = table->eviction_group_id_basis;\n    miniflow_expand(rule->cr.match.flow, &flow);\n    for (sf = table->eviction_fields;\n         sf < &table->eviction_fields[table->n_eviction_fields];\n         sf++)\n    {\n        if (mf_are_prereqs_ok(sf->field, &flow, NULL)) {\n            union mf_value value;\n\n            mf_get_value(sf->field, &flow, &value);\n            if (sf->ofs) {\n                bitwise_zero(&value, sf->field->n_bytes, 0, sf->ofs);\n            }\n            if (sf->ofs + sf->n_bits < sf->field->n_bytes * 8) {\n                unsigned int start = sf->ofs + sf->n_bits;\n                bitwise_zero(&value, sf->field->n_bytes, start,\n                             sf->field->n_bytes * 8 - start);\n            }\n            hash = hash_bytes(&value, sf->field->n_bytes, hash);\n        } else {\n            hash = hash_int(hash, 0);\n        }\n    }\n\n    return hash;\n}\n\n/* Returns an eviction group within 'table' with the given 'id', creating one\n * if necessary. */\nstatic struct eviction_group *\neviction_group_find(struct oftable *table, uint32_t id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct eviction_group *evg;\n\n    HMAP_FOR_EACH_WITH_HASH (evg, id_node, id, &table->eviction_groups_by_id) {\n        return evg;\n    }\n\n    evg = xmalloc(sizeof *evg);\n    hmap_insert(&table->eviction_groups_by_id, &evg->id_node, id);\n    heap_insert(&table->eviction_groups_by_size, &evg->size_node,\n                eviction_group_priority(0));\n    heap_init(&evg->rules);\n\n    return evg;\n}\n\n/* Returns an eviction priority for 'rule'.  The return value should be\n * interpreted so that higher priorities make a rule a more attractive\n * candidate for eviction. */\nstatic uint64_t\nrule_eviction_priority(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Calculate absolute time when this flow will expire.  If it will never\n     * expire, then return 0 to make it unevictable.  */\n    long long int expiration = LLONG_MAX;\n    if (rule->hard_timeout) {\n        /* 'modified' needs protection even when we hold 'ofproto_mutex'. */\n        ovs_mutex_lock(&rule->mutex);\n        long long int modified = rule->modified;\n        ovs_mutex_unlock(&rule->mutex);\n\n        expiration = modified + rule->hard_timeout * 1000;\n    }\n    if (rule->idle_timeout) {\n        uint64_t packets, bytes;\n        long long int used;\n        long long int idle_expiration;\n\n        ofproto->ofproto_class->rule_get_stats(rule, &packets, &bytes, &used);\n        idle_expiration = used + rule->idle_timeout * 1000;\n        expiration = MIN(expiration, idle_expiration);\n    }\n    if (expiration == LLONG_MAX) {\n        return 0;\n    }\n\n    /* Calculate the time of expiration as a number of (approximate) seconds\n     * after program startup.\n     *\n     * This should work OK for program runs that last UINT32_MAX seconds or\n     * less.  Therefore, please restart OVS at least once every 136 years. */\n    uint32_t expiration_ofs = (expiration >> 10) - (time_boot_msec() >> 10);\n\n    /* Combine expiration time with OpenFlow \"importance\" to form a single\n     * priority value.  We want flows with relatively low \"importance\" to be\n     * evicted before even considering expiration time, so put \"importance\" in\n     * the most significant bits and expiration time in the least significant\n     * bits.\n     *\n     * Small 'priority' should be evicted before those with large 'priority'.\n     * The caller expects the opposite convention (a large return value being\n     * more attractive for eviction) so we invert it before returning. */\n    uint64_t priority = ((uint64_t) rule->importance << 32) + expiration_ofs;\n    return UINT64_MAX - priority;\n}\n\n/* Adds 'rule' to an appropriate eviction group for its oftable's\n * configuration.  Does nothing if 'rule''s oftable doesn't have eviction\n * enabled, or if 'rule' is a permanent rule (one that will never expire on its\n * own).\n *\n * The caller must ensure that 'rule' is not already in an eviction group. */\nstatic void\neviction_group_add_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = rule->ofproto;\n    struct oftable *table = &ofproto->tables[rule->table_id];\n    bool has_timeout;\n\n    /* Timeouts may be modified only when holding 'ofproto_mutex'.  We have it\n     * so no additional protection is needed. */\n    has_timeout = rule->hard_timeout || rule->idle_timeout;\n\n    if (table->eviction && has_timeout) {\n        struct eviction_group *evg;\n\n        evg = eviction_group_find(table, eviction_group_hash_rule(rule));\n\n        rule->eviction_group = evg;\n        heap_insert(&evg->rules, &rule->evg_node,\n                    rule_eviction_priority(ofproto, rule));\n        eviction_group_resized(table, evg);\n    }\n}\n\f\n/* oftables. */\n\n/* Initializes 'table'. */\nstatic void\noftable_init(struct oftable *table)\n{\n    memset(table, 0, sizeof *table);\n    classifier_init(&table->cls, flow_segment_u64s);\n    table->max_flows = UINT_MAX;\n    table->n_flows = 0;\n    hmap_init(&table->eviction_groups_by_id);\n    heap_init(&table->eviction_groups_by_size);\n    atomic_init(&table->miss_config, OFPUTIL_TABLE_MISS_DEFAULT);\n\n    classifier_set_prefix_fields(&table->cls, default_prefix_fields,\n                                 ARRAY_SIZE(default_prefix_fields));\n\n    atomic_init(&table->n_matched, 0);\n    atomic_init(&table->n_missed, 0);\n}\n\n/* Destroys 'table', including its classifier and eviction groups.\n *\n * The caller is responsible for freeing 'table' itself. */\nstatic void\noftable_destroy(struct oftable *table)\n{\n    ovs_assert(classifier_is_empty(&table->cls));\n\n    ovs_mutex_lock(&ofproto_mutex);\n    oftable_configure_eviction(table, 0, NULL, 0);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    hmap_destroy(&table->eviction_groups_by_id);\n    heap_destroy(&table->eviction_groups_by_size);\n    classifier_destroy(&table->cls);\n    free(table->name);\n}\n\n/* Changes the name of 'table' to 'name'.  If 'name' is NULL or the empty\n * string, then 'table' will use its default name.\n *\n * This only affects the name exposed for a table exposed through the OpenFlow\n * OFPST_TABLE (as printed by \"ovs-ofctl dump-tables\"). */\nstatic void\noftable_set_name(struct oftable *table, const char *name)\n{\n    if (name && name[0]) {\n        int len = strnlen(name, OFP_MAX_TABLE_NAME_LEN);\n        if (!table->name || strncmp(name, table->name, len)) {\n            free(table->name);\n            table->name = xmemdup0(name, len);\n        }\n    } else {\n        free(table->name);\n        table->name = NULL;\n    }\n}\n\n/* oftables support a choice of two policies when adding a rule would cause the\n * number of flows in the table to exceed the configured maximum number: either\n * they can refuse to add the new flow or they can evict some existing flow.\n * This function configures the latter policy on 'table', with fairness based\n * on the values of the 'n_fields' fields specified in 'fields'.  (Specifying\n * 'n_fields' as 0 disables fairness.) */\nstatic void\noftable_configure_eviction(struct oftable *table, unsigned int eviction,\n                           const struct mf_subfield *fields, size_t n_fields)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    if ((table->eviction != 0) == (eviction != 0)\n        && n_fields == table->n_eviction_fields\n        && (!n_fields\n            || !memcmp(fields, table->eviction_fields,\n                       n_fields * sizeof *fields))) {\n        /* The set of eviction fields did not change.  If 'eviction' changed,\n         * it remains nonzero, so that we can just update table->eviction\n         * without fussing with the eviction groups. */\n        table->eviction = eviction;\n        return;\n    }\n\n    /* Destroy existing eviction groups, then destroy and recreate data\n     * structures to recover memory. */\n    struct eviction_group *evg, *next;\n    HMAP_FOR_EACH_SAFE (evg, next, id_node, &table->eviction_groups_by_id) {\n        eviction_group_destroy(table, evg);\n    }\n    hmap_destroy(&table->eviction_groups_by_id);\n    hmap_init(&table->eviction_groups_by_id);\n    heap_destroy(&table->eviction_groups_by_size);\n    heap_init(&table->eviction_groups_by_size);\n\n    /* Replace eviction groups by the new ones, if there is a change.  Free the\n     * old fields only after allocating the new ones, because 'fields ==\n     * table->eviction_fields' is possible. */\n    struct mf_subfield *old_fields = table->eviction_fields;\n    table->n_eviction_fields = n_fields;\n    table->eviction_fields = (fields\n                              ? xmemdup(fields, n_fields * sizeof *fields)\n                              : NULL);\n    free(old_fields);\n\n    /* Add the new eviction groups, if enabled. */\n    table->eviction = eviction;\n    if (table->eviction) {\n        table->eviction_group_id_basis = random_uint32();\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            eviction_group_add_rule(rule);\n        }\n    }\n}\n\n/* Inserts 'rule' from the ofproto data structures BEFORE caller has inserted\n * it to the classifier. */\nstatic void\nofproto_rule_insert__(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct rule_actions *actions = rule_get_actions(rule);\n\n    /* A rule may not be reinserted. */\n    ovs_assert(rule->state == RULE_INITIALIZED);\n\n    if (rule->hard_timeout || rule->idle_timeout) {\n        ovs_list_insert(&ofproto->expirable, &rule->expirable);\n    }\n    cookies_insert(ofproto, rule);\n    eviction_group_add_rule(rule);\n    if (actions->has_meter) {\n        meter_insert_rule(rule);\n    }\n    if (actions->has_groups) {\n        const struct ofpact_group *a;\n        OFPACT_FOR_EACH_TYPE_FLATTENED (a, GROUP, actions->ofpacts,\n                                        actions->ofpacts_len) {\n            struct ofgroup *group;\n\n            group = ofproto_group_lookup(ofproto, a->group_id, OVS_VERSION_MAX,\n                                         false);\n            ovs_assert(group != NULL);\n            group_add_rule(group, rule);\n        }\n    }\n\n    rule->state = RULE_INSERTED;\n}\n\n/* Removes 'rule' from the ofproto data structures.  Caller may have deferred\n * the removal from the classifier. */\nstatic void\nofproto_rule_remove__(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ovs_assert(rule->state == RULE_INSERTED);\n\n    cookies_remove(ofproto, rule);\n\n    eviction_group_remove_rule(rule);\n    if (!ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_remove(&rule->expirable);\n    }\n    if (!ovs_list_is_empty(&rule->meter_list_node)) {\n        ovs_list_remove(&rule->meter_list_node);\n        ovs_list_init(&rule->meter_list_node);\n    }\n\n    /* Remove the rule from any groups, except from the group that is being\n     * deleted, if any. */\n    const struct rule_actions *actions = rule_get_actions(rule);\n\n    if (actions->has_groups) {\n        const struct ofpact_group *a;\n\n        OFPACT_FOR_EACH_TYPE_FLATTENED(a, GROUP, actions->ofpacts,\n                                        actions->ofpacts_len) {\n            struct ofgroup *group;\n\n            group = ofproto_group_lookup(ofproto, a->group_id, OVS_VERSION_MAX,\n                                         false);\n            ovs_assert(group);\n\n            /* Leave the rule for the group that is being deleted, if any,\n             * as we still need the list of rules for clean-up. */\n            if (!group->being_deleted) {\n                group_remove_rule(group, rule);\n            }\n        }\n    }\n\n    rule->state = RULE_REMOVED;\n}\n\f\n/* unixctl commands. */\n\nstruct ofproto *\nofproto_lookup(const char *name)\n{\n    struct ofproto *ofproto;\n\n    HMAP_FOR_EACH_WITH_HASH (ofproto, hmap_node, hash_string(name, 0),\n                             &all_ofprotos) {\n        if (!strcmp(ofproto->name, name)) {\n            return ofproto;\n        }\n    }\n    return NULL;\n}\n\nstatic void\nofproto_unixctl_list(struct unixctl_conn *conn, int argc OVS_UNUSED,\n                     const char *argv[] OVS_UNUSED, void *aux OVS_UNUSED)\n{\n    struct ofproto *ofproto;\n    struct ds results;\n\n    ds_init(&results);\n    HMAP_FOR_EACH (ofproto, hmap_node, &all_ofprotos) {\n        ds_put_format(&results, \"%s\\n\", ofproto->name);\n    }\n    unixctl_command_reply(conn, ds_cstr(&results));\n    ds_destroy(&results);\n}\n\nstatic void\nofproto_unixctl_init(void)\n{\n    static bool registered;\n    if (registered) {\n        return;\n    }\n    registered = true;\n\n    unixctl_command_register(\"ofproto/list\", \"\", 0, 0,\n                             ofproto_unixctl_list, NULL);\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2009-2017 Nicira, Inc.\n * Copyright (c) 2010 Jean Tourrilhes - HP-Labs.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at:\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n#include <config.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <stdbool.h>\n#include <stdlib.h>\n#include <unistd.h>\n\n#include \"bitmap.h\"\n#include \"bundles.h\"\n#include \"byte-order.h\"\n#include \"classifier.h\"\n#include \"connectivity.h\"\n#include \"connmgr.h\"\n#include \"coverage.h\"\n#include \"dp-packet.h\"\n#include \"hash.h\"\n#include \"openvswitch/hmap.h\"\n#include \"netdev.h\"\n#include \"nx-match.h\"\n#include \"ofproto.h\"\n#include \"ofproto-provider.h\"\n#include \"openflow/nicira-ext.h\"\n#include \"openflow/openflow.h\"\n#include \"openvswitch/dynamic-string.h\"\n#include \"openvswitch/meta-flow.h\"\n#include \"openvswitch/ofp-actions.h\"\n#include \"openvswitch/ofp-errors.h\"\n#include \"openvswitch/ofp-msgs.h\"\n#include \"openvswitch/ofp-print.h\"\n#include \"openvswitch/ofp-util.h\"\n#include \"openvswitch/ofpbuf.h\"\n#include \"openvswitch/vlog.h\"\n#include \"ovs-rcu.h\"\n#include \"packets.h\"\n#include \"pinsched.h\"\n#include \"poll-loop.h\"\n#include \"random.h\"\n#include \"seq.h\"\n#include \"openvswitch/shash.h\"\n#include \"simap.h\"\n#include \"smap.h\"\n#include \"sset.h\"\n#include \"timeval.h\"\n#include \"tun-metadata.h\"\n#include \"unaligned.h\"\n#include \"unixctl.h\"\n#include \"util.h\"\n\nVLOG_DEFINE_THIS_MODULE(ofproto);\n\nCOVERAGE_DEFINE(ofproto_flush);\nCOVERAGE_DEFINE(ofproto_packet_out);\nCOVERAGE_DEFINE(ofproto_queue_req);\nCOVERAGE_DEFINE(ofproto_recv_openflow);\nCOVERAGE_DEFINE(ofproto_reinit_ports);\nCOVERAGE_DEFINE(ofproto_update_port);\n\n/* Default fields to use for prefix tries in each flow table, unless something\n * else is configured. */\nconst enum mf_field_id default_prefix_fields[2] =\n    { MFF_IPV4_DST, MFF_IPV4_SRC };\n\n/* oftable. */\nstatic void oftable_init(struct oftable *);\nstatic void oftable_destroy(struct oftable *);\n\nstatic void oftable_set_name(struct oftable *, const char *name);\n\nstatic enum ofperr evict_rules_from_table(struct oftable *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void oftable_configure_eviction(struct oftable *,\n                                       unsigned int eviction,\n                                       const struct mf_subfield *fields,\n                                       size_t n_fields)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* This is the only combination of OpenFlow eviction flags that OVS supports: a\n * combination of OF1.4+ importance, the remaining lifetime of the flow, and\n * fairness based on user-specified fields. */\n#define OFPROTO_EVICTION_FLAGS \\\n    (OFPTMPEF14_OTHER | OFPTMPEF14_IMPORTANCE | OFPTMPEF14_LIFETIME)\n\n/* A set of rules within a single OpenFlow table (oftable) that have the same\n * values for the oftable's eviction_fields.  A rule to be evicted, when one is\n * needed, is taken from the eviction group that contains the greatest number\n * of rules.\n *\n * An oftable owns any number of eviction groups, each of which contains any\n * number of rules.\n *\n * Membership in an eviction group is imprecise, based on the hash of the\n * oftable's eviction_fields (in the eviction_group's id_node.hash member).\n * That is, if two rules have different eviction_fields, but those\n * eviction_fields hash to the same value, then they will belong to the same\n * eviction_group anyway.\n *\n * (When eviction is not enabled on an oftable, we don't track any eviction\n * groups, to save time and space.) */\nstruct eviction_group {\n    struct hmap_node id_node;   /* In oftable's \"eviction_groups_by_id\". */\n    struct heap_node size_node; /* In oftable's \"eviction_groups_by_size\". */\n    struct heap rules;          /* Contains \"struct rule\"s. */\n};\n\nstatic bool choose_rule_to_evict(struct oftable *table, struct rule **rulep)\n    OVS_REQUIRES(ofproto_mutex);\nstatic uint64_t rule_eviction_priority(struct ofproto *ofproto, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void eviction_group_add_rule(struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void eviction_group_remove_rule(struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void rule_criteria_init(struct rule_criteria *, uint8_t table_id,\n                               const struct match *match, int priority,\n                               ovs_version_t version,\n                               ovs_be64 cookie, ovs_be64 cookie_mask,\n                               ofp_port_t out_port, uint32_t out_group);\nstatic void rule_criteria_require_rw(struct rule_criteria *,\n                                     bool can_write_readonly);\nstatic void rule_criteria_destroy(struct rule_criteria *);\n\nstatic enum ofperr collect_rules_loose(struct ofproto *,\n                                       const struct rule_criteria *,\n                                       struct rule_collection *);\n\nstruct learned_cookie {\n    union {\n        /* In struct ofproto's 'learned_cookies' hmap. */\n        struct hmap_node hmap_node OVS_GUARDED_BY(ofproto_mutex);\n\n        /* In 'dead_cookies' list when removed from hmap. */\n        struct ovs_list list_node;\n    } u;\n\n    /* Key. */\n    ovs_be64 cookie OVS_GUARDED_BY(ofproto_mutex);\n    uint8_t table_id OVS_GUARDED_BY(ofproto_mutex);\n\n    /* Number of references from \"learn\" actions.\n     *\n     * When this drops to 0, all of the flows in 'table_id' with the specified\n     * 'cookie' are deleted. */\n    int n OVS_GUARDED_BY(ofproto_mutex);\n};\n\nstatic const struct ofpact_learn *next_learn_with_delete(\n    const struct rule_actions *, const struct ofpact_learn *start);\n\nstatic void learned_cookies_inc(struct ofproto *, const struct rule_actions *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void learned_cookies_dec(struct ofproto *, const struct rule_actions *,\n                                struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void learned_cookies_flush(struct ofproto *, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* ofport. */\nstatic void ofport_destroy__(struct ofport *) OVS_EXCLUDED(ofproto_mutex);\nstatic void ofport_destroy(struct ofport *, bool del);\nstatic bool ofport_is_mtu_overridden(const struct ofproto *,\n                                     const struct ofport *);\n\nstatic int update_port(struct ofproto *, const char *devname);\nstatic int init_ports(struct ofproto *);\nstatic void reinit_ports(struct ofproto *);\n\nstatic long long int ofport_get_usage(const struct ofproto *,\n                                      ofp_port_t ofp_port);\nstatic void ofport_set_usage(struct ofproto *, ofp_port_t ofp_port,\n                             long long int last_used);\nstatic void ofport_remove_usage(struct ofproto *, ofp_port_t ofp_port);\n\n/* Ofport usage.\n *\n * Keeps track of the currently used and recently used ofport values and is\n * used to prevent immediate recycling of ofport values. */\nstruct ofport_usage {\n    struct hmap_node hmap_node; /* In struct ofproto's \"ofport_usage\" hmap. */\n    ofp_port_t ofp_port;        /* OpenFlow port number. */\n    long long int last_used;    /* Last time the 'ofp_port' was used. LLONG_MAX\n                                   represents in-use ofports. */\n};\n\n/* rule. */\nstatic void ofproto_rule_send_removed(struct rule *)\n        OVS_EXCLUDED(ofproto_mutex);\nstatic bool rule_is_readonly(const struct rule *);\nstatic void ofproto_rule_insert__(struct ofproto *, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_rule_remove__(struct ofproto *, struct rule *)\n    OVS_REQUIRES(ofproto_mutex);\n\n/* The source of an OpenFlow request.\n *\n * A table modification request can be generated externally, via OpenFlow, or\n * internally through a function call.  This structure indicates the source of\n * an OpenFlow-generated table modification.  For an internal flow_mod, it\n * isn't meaningful and thus supplied as NULL. */\nstruct openflow_mod_requester {\n    struct ofconn *ofconn;      /* Connection on which flow_mod arrived. */\n    const struct ofp_header *request;\n};\n\n/* OpenFlow. */\nstatic enum ofperr ofproto_rule_create(struct ofproto *, struct cls_rule *,\n                                       uint8_t table_id, ovs_be64 new_cookie,\n                                       uint16_t idle_timeout,\n                                       uint16_t hard_timeout,\n                                       enum ofputil_flow_mod_flags flags,\n                                       uint16_t importance,\n                                       const struct ofpact *ofpacts,\n                                       size_t ofpacts_len,\n                                       uint64_t match_tlv_bitmap,\n                                       uint64_t ofpacts_tlv_bitmap,\n                                       struct rule **new_rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS;\n\nstatic void replace_rule_start(struct ofproto *, struct ofproto_flow_mod *,\n                               struct rule *old_rule, struct rule *new_rule)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void replace_rule_revert(struct ofproto *, struct rule *old_rule,\n                                struct rule *new_rule)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic void replace_rule_finish(struct ofproto *, struct ofproto_flow_mod *,\n                                const struct openflow_mod_requester *,\n                                struct rule *old_rule, struct rule *new_rule,\n                                struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void delete_flows__(struct rule_collection *,\n                           enum ofp_flow_removed_reason,\n                           const struct openflow_mod_requester *)\n    OVS_REQUIRES(ofproto_mutex);\n\nstatic bool ofproto_group_exists(const struct ofproto *, uint32_t group_id);\nstatic void handle_openflow(struct ofconn *, const struct ofpbuf *);\nstatic enum ofperr ofproto_flow_mod_init(struct ofproto *,\n                                         struct ofproto_flow_mod *,\n                                         const struct ofputil_flow_mod *fm,\n                                         struct rule *)\n    OVS_EXCLUDED(ofproto_mutex);\nstatic enum ofperr ofproto_flow_mod_start(struct ofproto *,\n                                          struct ofproto_flow_mod *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_flow_mod_revert(struct ofproto *,\n                                    struct ofproto_flow_mod *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic void ofproto_flow_mod_finish(struct ofproto *,\n                                    struct ofproto_flow_mod *,\n                                    const struct openflow_mod_requester *)\n    OVS_REQUIRES(ofproto_mutex);\nstatic enum ofperr handle_flow_mod__(struct ofproto *,\n                                     const struct ofputil_flow_mod *,\n                                     const struct openflow_mod_requester *)\n    OVS_EXCLUDED(ofproto_mutex);\nstatic void calc_duration(long long int start, long long int now,\n                          uint32_t *sec, uint32_t *nsec);\n\n/* ofproto. */\nstatic uint64_t pick_datapath_id(const struct ofproto *);\nstatic uint64_t pick_fallback_dpid(void);\nstatic void ofproto_destroy__(struct ofproto *);\nstatic void update_mtu(struct ofproto *, struct ofport *);\nstatic void update_mtu_ofproto(struct ofproto *);\nstatic void meter_delete(struct ofproto *, uint32_t first, uint32_t last);\nstatic void meter_insert_rule(struct rule *);\n\n/* unixctl. */\nstatic void ofproto_unixctl_init(void);\n\n/* All registered ofproto classes, in probe order. */\nstatic const struct ofproto_class **ofproto_classes;\nstatic size_t n_ofproto_classes;\nstatic size_t allocated_ofproto_classes;\n\n/* Global lock that protects all flow table operations. */\nstruct ovs_mutex ofproto_mutex = OVS_MUTEX_INITIALIZER;\n\nunsigned ofproto_flow_limit = OFPROTO_FLOW_LIMIT_DEFAULT;\nunsigned ofproto_max_idle = OFPROTO_MAX_IDLE_DEFAULT;\n\nsize_t n_handlers, n_revalidators;\nchar *pmd_cpu_mask;\n\n/* Map from datapath name to struct ofproto, for use by unixctl commands. */\nstatic struct hmap all_ofprotos = HMAP_INITIALIZER(&all_ofprotos);\n\n/* Initial mappings of port to OpenFlow number mappings. */\nstatic struct shash init_ofp_ports = SHASH_INITIALIZER(&init_ofp_ports);\n\nstatic struct vlog_rate_limit rl = VLOG_RATE_LIMIT_INIT(1, 5);\n\n/* The default value of true waits for flow restore. */\nstatic bool flow_restore_wait = true;\n\n/* Must be called to initialize the ofproto library.\n *\n * The caller may pass in 'iface_hints', which contains an shash of\n * \"iface_hint\" elements indexed by the interface's name.  The provider\n * may use these hints to describe the startup configuration in order to\n * reinitialize its state.  The caller owns the provided data, so a\n * provider will make copies of anything required.  An ofproto provider\n * will remove any existing state that is not described by the hint, and\n * may choose to remove it all. */\nvoid\nofproto_init(const struct shash *iface_hints)\n{\n    struct shash_node *node;\n    size_t i;\n\n    ofproto_class_register(&ofproto_dpif_class);\n\n    /* Make a local copy, since we don't own 'iface_hints' elements. */\n    SHASH_FOR_EACH(node, iface_hints) {\n        const struct iface_hint *orig_hint = node->data;\n        struct iface_hint *new_hint = xmalloc(sizeof *new_hint);\n        const char *br_type = ofproto_normalize_type(orig_hint->br_type);\n\n        new_hint->br_name = xstrdup(orig_hint->br_name);\n        new_hint->br_type = xstrdup(br_type);\n        new_hint->ofp_port = orig_hint->ofp_port;\n\n        shash_add(&init_ofp_ports, node->name, new_hint);\n    }\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        ofproto_classes[i]->init(&init_ofp_ports);\n    }\n\n    ofproto_unixctl_init();\n}\n\n/* 'type' should be a normalized datapath type, as returned by\n * ofproto_normalize_type().  Returns the corresponding ofproto_class\n * structure, or a null pointer if there is none registered for 'type'. */\nstatic const struct ofproto_class *\nofproto_class_find__(const char *type)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        const struct ofproto_class *class = ofproto_classes[i];\n        struct sset types;\n        bool found;\n\n        sset_init(&types);\n        class->enumerate_types(&types);\n        found = sset_contains(&types, type);\n        sset_destroy(&types);\n\n        if (found) {\n            return class;\n        }\n    }\n    VLOG_WARN(\"unknown datapath type %s\", type);\n    return NULL;\n}\n\n/* Registers a new ofproto class.  After successful registration, new ofprotos\n * of that type can be created using ofproto_create(). */\nint\nofproto_class_register(const struct ofproto_class *new_class)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        if (ofproto_classes[i] == new_class) {\n            return EEXIST;\n        }\n    }\n\n    if (n_ofproto_classes >= allocated_ofproto_classes) {\n        ofproto_classes = x2nrealloc(ofproto_classes,\n                                     &allocated_ofproto_classes,\n                                     sizeof *ofproto_classes);\n    }\n    ofproto_classes[n_ofproto_classes++] = new_class;\n    return 0;\n}\n\n/* Unregisters a datapath provider.  'type' must have been previously\n * registered and not currently be in use by any ofprotos.  After\n * unregistration new datapaths of that type cannot be opened using\n * ofproto_create(). */\nint\nofproto_class_unregister(const struct ofproto_class *class)\n{\n    size_t i;\n\n    for (i = 0; i < n_ofproto_classes; i++) {\n        if (ofproto_classes[i] == class) {\n            for (i++; i < n_ofproto_classes; i++) {\n                ofproto_classes[i - 1] = ofproto_classes[i];\n            }\n            n_ofproto_classes--;\n            return 0;\n        }\n    }\n    VLOG_WARN(\"attempted to unregister an ofproto class that is not \"\n              \"registered\");\n    return EAFNOSUPPORT;\n}\n\n/* Clears 'types' and enumerates all registered ofproto types into it.  The\n * caller must first initialize the sset. */\nvoid\nofproto_enumerate_types(struct sset *types)\n{\n    size_t i;\n\n    sset_clear(types);\n    for (i = 0; i < n_ofproto_classes; i++) {\n        ofproto_classes[i]->enumerate_types(types);\n    }\n}\n\n/* Returns the fully spelled out name for the given ofproto 'type'.\n *\n * Normalized type string can be compared with strcmp().  Unnormalized type\n * string might be the same even if they have different spellings. */\nconst char *\nofproto_normalize_type(const char *type)\n{\n    return type && type[0] ? type : \"system\";\n}\n\n/* Clears 'names' and enumerates the names of all known created ofprotos with\n * the given 'type'.  The caller must first initialize the sset.  Returns 0 if\n * successful, otherwise a positive errno value.\n *\n * Some kinds of datapaths might not be practically enumerable.  This is not\n * considered an error. */\nint\nofproto_enumerate_names(const char *type, struct sset *names)\n{\n    const struct ofproto_class *class = ofproto_class_find__(type);\n    return class ? class->enumerate_names(type, names) : EAFNOSUPPORT;\n}\n\nstatic void\nofproto_bump_tables_version(struct ofproto *ofproto)\n{\n    ++ofproto->tables_version;\n    ofproto->ofproto_class->set_tables_version(ofproto,\n                                               ofproto->tables_version);\n}\n\nint\nofproto_create(const char *datapath_name, const char *datapath_type,\n               struct ofproto **ofprotop)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct ofproto_class *class;\n    struct ofproto *ofproto;\n    int error;\n    int i;\n\n    *ofprotop = NULL;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n    if (!class) {\n        VLOG_WARN(\"could not create datapath %s of unknown type %s\",\n                  datapath_name, datapath_type);\n        return EAFNOSUPPORT;\n    }\n\n    ofproto = class->alloc();\n    if (!ofproto) {\n        VLOG_ERR(\"failed to allocate datapath %s of type %s\",\n                 datapath_name, datapath_type);\n        return ENOMEM;\n    }\n\n    /* Initialize. */\n    ovs_mutex_lock(&ofproto_mutex);\n    memset(ofproto, 0, sizeof *ofproto);\n    ofproto->ofproto_class = class;\n    ofproto->name = xstrdup(datapath_name);\n    ofproto->type = xstrdup(datapath_type);\n    hmap_insert(&all_ofprotos, &ofproto->hmap_node,\n                hash_string(ofproto->name, 0));\n    ofproto->datapath_id = 0;\n    ofproto->forward_bpdu = false;\n    ofproto->fallback_dpid = pick_fallback_dpid();\n    ofproto->mfr_desc = NULL;\n    ofproto->hw_desc = NULL;\n    ofproto->sw_desc = NULL;\n    ofproto->serial_desc = NULL;\n    ofproto->dp_desc = NULL;\n    ofproto->frag_handling = OFPUTIL_FRAG_NORMAL;\n    hmap_init(&ofproto->ports);\n    hmap_init(&ofproto->ofport_usage);\n    shash_init(&ofproto->port_by_name);\n    simap_init(&ofproto->ofp_requests);\n    ofproto->max_ports = ofp_to_u16(OFPP_MAX);\n    ofproto->eviction_group_timer = LLONG_MIN;\n    ofproto->tables = NULL;\n    ofproto->n_tables = 0;\n    ofproto->tables_version = OVS_VERSION_MIN;\n    hindex_init(&ofproto->cookies);\n    hmap_init(&ofproto->learned_cookies);\n    ovs_list_init(&ofproto->expirable);\n    ofproto->connmgr = connmgr_create(ofproto, datapath_name, datapath_name);\n    ofproto->min_mtu = INT_MAX;\n    cmap_init(&ofproto->groups);\n    ovs_mutex_unlock(&ofproto_mutex);\n    ofproto->ogf.types = 0xf;\n    ofproto->ogf.capabilities = OFPGFC_CHAINING | OFPGFC_SELECT_LIVENESS |\n                                OFPGFC_SELECT_WEIGHT;\n    for (i = 0; i < 4; i++) {\n        ofproto->ogf.max_groups[i] = OFPG_MAX;\n        ofproto->ogf.ofpacts[i] = (UINT64_C(1) << N_OFPACTS) - 1;\n    }\n    ovsrcu_set(&ofproto->metadata_tab, tun_metadata_alloc(NULL));\n\n    ovs_mutex_init(&ofproto->vl_mff_map.mutex);\n    cmap_init(&ofproto->vl_mff_map.cmap);\n\n    error = ofproto->ofproto_class->construct(ofproto);\n    if (error) {\n        VLOG_ERR(\"failed to open datapath %s: %s\",\n                 datapath_name, ovs_strerror(error));\n        ovs_mutex_lock(&ofproto_mutex);\n        connmgr_destroy(ofproto->connmgr);\n        ofproto->connmgr = NULL;\n        ovs_mutex_unlock(&ofproto_mutex);\n        ofproto_destroy__(ofproto);\n        return error;\n    }\n\n    /* Check that hidden tables, if any, are at the end. */\n    ovs_assert(ofproto->n_tables);\n    for (i = 0; i + 1 < ofproto->n_tables; i++) {\n        enum oftable_flags flags = ofproto->tables[i].flags;\n        enum oftable_flags next_flags = ofproto->tables[i + 1].flags;\n\n        ovs_assert(!(flags & OFTABLE_HIDDEN) || next_flags & OFTABLE_HIDDEN);\n    }\n\n    ofproto->datapath_id = pick_datapath_id(ofproto);\n    init_ports(ofproto);\n\n    /* Initialize meters table. */\n    if (ofproto->ofproto_class->meter_get_features) {\n        ofproto->ofproto_class->meter_get_features(ofproto,\n                                                   &ofproto->meter_features);\n    } else {\n        memset(&ofproto->meter_features, 0, sizeof ofproto->meter_features);\n    }\n    ofproto->meters = xzalloc((ofproto->meter_features.max_meters + 1)\n                              * sizeof(struct meter *));\n\n    /* Set the initial tables version. */\n    ofproto_bump_tables_version(ofproto);\n\n    *ofprotop = ofproto;\n    return 0;\n}\n\n/* Must be called (only) by an ofproto implementation in its constructor\n * function.  See the large comment on 'construct' in struct ofproto_class for\n * details. */\nvoid\nofproto_init_tables(struct ofproto *ofproto, int n_tables)\n{\n    struct oftable *table;\n\n    ovs_assert(!ofproto->n_tables);\n    ovs_assert(n_tables >= 1 && n_tables <= 255);\n\n    ofproto->n_tables = n_tables;\n    ofproto->tables = xmalloc(n_tables * sizeof *ofproto->tables);\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        oftable_init(table);\n    }\n}\n\n/* To be optionally called (only) by an ofproto implementation in its\n * constructor function.  See the large comment on 'construct' in struct\n * ofproto_class for details.\n *\n * Sets the maximum number of ports to 'max_ports'.  The ofproto generic layer\n * will then ensure that actions passed into the ofproto implementation will\n * not refer to OpenFlow ports numbered 'max_ports' or higher.  If this\n * function is not called, there will be no such restriction.\n *\n * Reserved ports numbered OFPP_MAX and higher are special and not subject to\n * the 'max_ports' restriction. */\nvoid\nofproto_init_max_ports(struct ofproto *ofproto, uint16_t max_ports)\n{\n    ovs_assert(max_ports <= ofp_to_u16(OFPP_MAX));\n    ofproto->max_ports = max_ports;\n}\n\nuint64_t\nofproto_get_datapath_id(const struct ofproto *ofproto)\n{\n    return ofproto->datapath_id;\n}\n\nvoid\nofproto_set_datapath_id(struct ofproto *p, uint64_t datapath_id)\n{\n    uint64_t old_dpid = p->datapath_id;\n    p->datapath_id = datapath_id ? datapath_id : pick_datapath_id(p);\n    if (p->datapath_id != old_dpid) {\n        /* Force all active connections to reconnect, since there is no way to\n         * notify a controller that the datapath ID has changed. */\n        ofproto_reconnect_controllers(p);\n    }\n}\n\nvoid\nofproto_set_controllers(struct ofproto *p,\n                        const struct ofproto_controller *controllers,\n                        size_t n_controllers, uint32_t allowed_versions)\n{\n    connmgr_set_controllers(p->connmgr, controllers, n_controllers,\n                            allowed_versions);\n}\n\nvoid\nofproto_set_fail_mode(struct ofproto *p, enum ofproto_fail_mode fail_mode)\n{\n    connmgr_set_fail_mode(p->connmgr, fail_mode);\n}\n\n/* Drops the connections between 'ofproto' and all of its controllers, forcing\n * them to reconnect. */\nvoid\nofproto_reconnect_controllers(struct ofproto *ofproto)\n{\n    connmgr_reconnect(ofproto->connmgr);\n}\n\n/* Sets the 'n' TCP port addresses in 'extras' as ones to which 'ofproto''s\n * in-band control should guarantee access, in the same way that in-band\n * control guarantees access to OpenFlow controllers. */\nvoid\nofproto_set_extra_in_band_remotes(struct ofproto *ofproto,\n                                  const struct sockaddr_in *extras, size_t n)\n{\n    connmgr_set_extra_in_band_remotes(ofproto->connmgr, extras, n);\n}\n\n/* Sets the OpenFlow queue used by flows set up by in-band control on\n * 'ofproto' to 'queue_id'.  If 'queue_id' is negative, then in-band control\n * flows will use the default queue. */\nvoid\nofproto_set_in_band_queue(struct ofproto *ofproto, int queue_id)\n{\n    connmgr_set_in_band_queue(ofproto->connmgr, queue_id);\n}\n\n/* Sets the number of flows at which eviction from the kernel flow table\n * will occur. */\nvoid\nofproto_set_flow_limit(unsigned limit)\n{\n    ofproto_flow_limit = limit;\n}\n\n/* Sets the maximum idle time for flows in the datapath before they are\n * expired. */\nvoid\nofproto_set_max_idle(unsigned max_idle)\n{\n    ofproto_max_idle = max_idle;\n}\n\n/* If forward_bpdu is true, the NORMAL action will forward frames with\n * reserved (e.g. STP) destination Ethernet addresses. if forward_bpdu is false,\n * the NORMAL action will drop these frames. */\nvoid\nofproto_set_forward_bpdu(struct ofproto *ofproto, bool forward_bpdu)\n{\n    bool old_val = ofproto->forward_bpdu;\n    ofproto->forward_bpdu = forward_bpdu;\n    if (old_val != ofproto->forward_bpdu) {\n        if (ofproto->ofproto_class->forward_bpdu_changed) {\n            ofproto->ofproto_class->forward_bpdu_changed(ofproto);\n        }\n    }\n}\n\n/* Sets the MAC aging timeout for the OFPP_NORMAL action on 'ofproto' to\n * 'idle_time', in seconds, and the maximum number of MAC table entries to\n * 'max_entries'. */\nvoid\nofproto_set_mac_table_config(struct ofproto *ofproto, unsigned idle_time,\n                             size_t max_entries)\n{\n    if (ofproto->ofproto_class->set_mac_table_config) {\n        ofproto->ofproto_class->set_mac_table_config(ofproto, idle_time,\n                                                     max_entries);\n    }\n}\n\n/* Multicast snooping configuration. */\n\n/* Configures multicast snooping on 'ofproto' using the settings\n * defined in 's'.  If 's' is NULL, disables multicast snooping.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_mcast_snooping(struct ofproto *ofproto,\n                           const struct ofproto_mcast_snooping_settings *s)\n{\n    return (ofproto->ofproto_class->set_mcast_snooping\n            ? ofproto->ofproto_class->set_mcast_snooping(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Configures multicast snooping flood settings on 'ofp_port' of 'ofproto'.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_mcast_snooping(struct ofproto *ofproto, void *aux,\n                           const struct ofproto_mcast_snooping_port_settings *s)\n{\n    return (ofproto->ofproto_class->set_mcast_snooping_port\n            ? ofproto->ofproto_class->set_mcast_snooping_port(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\nvoid\nofproto_set_cpu_mask(const char *cmask)\n{\n    free(pmd_cpu_mask);\n    pmd_cpu_mask = nullable_xstrdup(cmask);\n}\n\nvoid\nofproto_set_threads(int n_handlers_, int n_revalidators_)\n{\n    int threads = MAX(count_cpu_cores(), 2);\n\n    n_revalidators = MAX(n_revalidators_, 0);\n    n_handlers = MAX(n_handlers_, 0);\n\n    if (!n_revalidators) {\n        n_revalidators = n_handlers\n            ? MAX(threads - (int) n_handlers, 1)\n            : threads / 4 + 1;\n    }\n\n    if (!n_handlers) {\n        n_handlers = MAX(threads - (int) n_revalidators, 1);\n    }\n}\n\nvoid\nofproto_set_dp_desc(struct ofproto *p, const char *dp_desc)\n{\n    free(p->dp_desc);\n    p->dp_desc = nullable_xstrdup(dp_desc);\n}\n\nint\nofproto_set_snoops(struct ofproto *ofproto, const struct sset *snoops)\n{\n    return connmgr_set_snoops(ofproto->connmgr, snoops);\n}\n\nint\nofproto_set_netflow(struct ofproto *ofproto,\n                    const struct netflow_options *nf_options)\n{\n    if (nf_options && sset_is_empty(&nf_options->collectors)) {\n        nf_options = NULL;\n    }\n\n    if (ofproto->ofproto_class->set_netflow) {\n        return ofproto->ofproto_class->set_netflow(ofproto, nf_options);\n    } else {\n        return nf_options ? EOPNOTSUPP : 0;\n    }\n}\n\nint\nofproto_set_sflow(struct ofproto *ofproto,\n                  const struct ofproto_sflow_options *oso)\n{\n    if (oso && sset_is_empty(&oso->targets)) {\n        oso = NULL;\n    }\n\n    if (ofproto->ofproto_class->set_sflow) {\n        return ofproto->ofproto_class->set_sflow(ofproto, oso);\n    } else {\n        return oso ? EOPNOTSUPP : 0;\n    }\n}\n\nint\nofproto_set_ipfix(struct ofproto *ofproto,\n                  const struct ofproto_ipfix_bridge_exporter_options *bo,\n                  const struct ofproto_ipfix_flow_exporter_options *fo,\n                  size_t n_fo)\n{\n    if (ofproto->ofproto_class->set_ipfix) {\n        return ofproto->ofproto_class->set_ipfix(ofproto, bo, fo, n_fo);\n    } else {\n        return (bo || fo) ? EOPNOTSUPP : 0;\n    }\n}\n\nstatic int\nofproto_get_ipfix_stats(struct ofproto *ofproto,\n                        bool bridge_ipfix,\n                        struct ovs_list *replies)\n{\n    int error;\n\n    if (ofproto->ofproto_class->get_ipfix_stats) {\n        error = ofproto->ofproto_class->get_ipfix_stats(ofproto,\n                                                          bridge_ipfix,\n                                                          replies);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_ipfix_bridge_stats_request(struct ofconn *ofconn,\n                                  const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    enum ofperr error;\n\n    ofpmp_init(&replies, request);\n    error = ofproto_get_ipfix_stats(ofproto, true, &replies);\n\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_ipfix_flow_stats_request(struct ofconn *ofconn,\n                                const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    enum ofperr error;\n\n    ofpmp_init(&replies, request);\n    error = ofproto_get_ipfix_stats(ofproto, false, &replies);\n\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_nxt_ct_flush_zone(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    const struct nx_zone_id *nzi = ofpmsg_body(oh);\n\n    if (!is_all_zeros(nzi->zero, sizeof nzi->zero)) {\n        return OFPERR_NXBRC_MUST_BE_ZERO;\n    }\n\n    uint16_t zone = ntohs(nzi->zone_id);\n    if (ofproto->ofproto_class->ct_flush) {\n        ofproto->ofproto_class->ct_flush(ofproto, &zone);\n    } else {\n        return EOPNOTSUPP;\n    }\n\n    return 0;\n}\n\nvoid\nofproto_set_flow_restore_wait(bool flow_restore_wait_db)\n{\n    flow_restore_wait = flow_restore_wait_db;\n}\n\nbool\nofproto_get_flow_restore_wait(void)\n{\n    return flow_restore_wait;\n}\n\n\f\n/* Spanning Tree Protocol (STP) configuration. */\n\n/* Configures STP on 'ofproto' using the settings defined in 's'.  If\n * 's' is NULL, disables STP.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_stp(struct ofproto *ofproto,\n                const struct ofproto_stp_settings *s)\n{\n    return (ofproto->ofproto_class->set_stp\n            ? ofproto->ofproto_class->set_stp(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP status of 'ofproto' and stores it in 's'.  If the\n * 'enabled' member of 's' is false, then the other members are not\n * meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_get_stp_status(struct ofproto *ofproto,\n                       struct ofproto_stp_status *s)\n{\n    return (ofproto->ofproto_class->get_stp_status\n            ? ofproto->ofproto_class->get_stp_status(ofproto, s)\n            : EOPNOTSUPP);\n}\n\n/* Configures STP on 'ofp_port' of 'ofproto' using the settings defined\n * in 's'.  The caller is responsible for assigning STP port numbers\n * (using the 'port_num' member in the range of 1 through 255, inclusive)\n * and ensuring there are no duplicates.  If the 's' is NULL, then STP\n * is disabled on the port.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_stp(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct ofproto_port_stp_settings *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure STP on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->set_stp_port\n            ? ofproto->ofproto_class->set_stp_port(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP port status of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_stp_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct ofproto_port_stp_status *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get STP status on nonexistent \"\n                     \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->get_stp_port_status\n            ? ofproto->ofproto_class->get_stp_port_status(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Retrieves STP port statistics of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_stp_stats(struct ofproto *ofproto, ofp_port_t ofp_port,\n                           struct ofproto_port_stp_stats *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get STP stats on nonexistent \"\n                     \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->get_stp_port_stats\n            ? ofproto->ofproto_class->get_stp_port_stats(ofport, s)\n            : EOPNOTSUPP);\n}\n\n/* Rapid Spanning Tree Protocol (RSTP) configuration. */\n\n/* Configures RSTP on 'ofproto' using the settings defined in 's'.  If\n * 's' is NULL, disables RSTP.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_set_rstp(struct ofproto *ofproto,\n                 const struct ofproto_rstp_settings *s)\n{\n    if (!ofproto->ofproto_class->set_rstp) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_rstp(ofproto, s);\n    return 0;\n}\n\n/* Retrieves RSTP status of 'ofproto' and stores it in 's'.  If the\n * 'enabled' member of 's' is false, then the other members are not\n * meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_get_rstp_status(struct ofproto *ofproto,\n                        struct ofproto_rstp_status *s)\n{\n    if (!ofproto->ofproto_class->get_rstp_status) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->get_rstp_status(ofproto, s);\n    return 0;\n}\n\n/* Configures RSTP on 'ofp_port' of 'ofproto' using the settings defined\n * in 's'.  The caller is responsible for assigning RSTP port numbers\n * (using the 'port_num' member in the range of 1 through 255, inclusive)\n * and ensuring there are no duplicates.  If the 's' is NULL, then RSTP\n * is disabled on the port.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_set_rstp(struct ofproto *ofproto, ofp_port_t ofp_port,\n                      const struct ofproto_port_rstp_settings *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure RSTP on nonexistent port %\"PRIu32,\n                ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    if (!ofproto->ofproto_class->set_rstp_port) {\n        return  EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_rstp_port(ofport, s);\n    return 0;\n}\n\n/* Retrieves RSTP port status of 'ofp_port' on 'ofproto' and stores it in\n * 's'.  If the 'enabled' member in 's' is false, then the other members\n * are not meaningful.\n *\n * Returns 0 if successful, otherwise a positive errno value.*/\nint\nofproto_port_get_rstp_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                             struct ofproto_port_rstp_status *s)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN_RL(&rl, \"%s: cannot get RSTP status on nonexistent \"\n                \"port %\"PRIu32, ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    if (!ofproto->ofproto_class->get_rstp_port_status) {\n        return  EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->get_rstp_port_status(ofport, s);\n    return 0;\n}\n\f\n/* Queue DSCP configuration. */\n\n/* Registers meta-data associated with the 'n_qdscp' Qualities of Service\n * 'queues' attached to 'ofport'.  This data is not intended to be sufficient\n * to implement QoS.  Instead, it is used to implement features which require\n * knowledge of what queues exist on a port, and some basic information about\n * them.\n *\n * Returns 0 if successful, otherwise a positive errno value. */\nint\nofproto_port_set_queues(struct ofproto *ofproto, ofp_port_t ofp_port,\n                        const struct ofproto_port_queue *queues,\n                        size_t n_queues)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot set queues on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return ENODEV;\n    }\n\n    return (ofproto->ofproto_class->set_queues\n            ? ofproto->ofproto_class->set_queues(ofport, queues, n_queues)\n            : EOPNOTSUPP);\n}\n\f\n/* LLDP configuration. */\nvoid\nofproto_port_set_lldp(struct ofproto *ofproto,\n                      ofp_port_t ofp_port,\n                      const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure LLDP on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n    error = (ofproto->ofproto_class->set_lldp\n             ? ofproto->ofproto_class->set_lldp(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: lldp configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\nint\nofproto_set_aa(struct ofproto *ofproto, void *aux OVS_UNUSED,\n               const struct aa_settings *s)\n{\n    if (!ofproto->ofproto_class->set_aa) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->set_aa(ofproto, s);\n    return 0;\n}\n\nint\nofproto_aa_mapping_register(struct ofproto *ofproto, void *aux,\n                            const struct aa_mapping_settings *s)\n{\n    if (!ofproto->ofproto_class->aa_mapping_set) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_mapping_set(ofproto, aux, s);\n    return 0;\n}\n\nint\nofproto_aa_mapping_unregister(struct ofproto *ofproto, void *aux)\n{\n    if (!ofproto->ofproto_class->aa_mapping_unset) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_mapping_unset(ofproto, aux);\n    return 0;\n}\n\nint\nofproto_aa_vlan_get_queued(struct ofproto *ofproto,\n                           struct ovs_list *list)\n{\n    if (!ofproto->ofproto_class->aa_vlan_get_queued) {\n        return EOPNOTSUPP;\n    }\n    ofproto->ofproto_class->aa_vlan_get_queued(ofproto, list);\n    return 0;\n}\n\nunsigned int\nofproto_aa_vlan_get_queue_size(struct ofproto *ofproto)\n{\n    if (!ofproto->ofproto_class->aa_vlan_get_queue_size) {\n        return EOPNOTSUPP;\n    }\n    return ofproto->ofproto_class->aa_vlan_get_queue_size(ofproto);\n}\n\n/* Connectivity Fault Management configuration. */\n\n/* Clears the CFM configuration from 'ofp_port' on 'ofproto'. */\nvoid\nofproto_port_clear_cfm(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    if (ofport && ofproto->ofproto_class->set_cfm) {\n        ofproto->ofproto_class->set_cfm(ofport, NULL);\n    }\n}\n\n/* Configures connectivity fault management on 'ofp_port' in 'ofproto'.  Takes\n * basic configuration from the configuration members in 'cfm', and the remote\n * maintenance point ID from  remote_mpid.  Ignores the statistics members of\n * 'cfm'.\n *\n * This function has no effect if 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_cfm(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct cfm_settings *s)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure CFM on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    /* XXX: For configuration simplicity, we only support one remote_mpid\n     * outside of the CFM module.  It's not clear if this is the correct long\n     * term solution or not. */\n    error = (ofproto->ofproto_class->set_cfm\n             ? ofproto->ofproto_class->set_cfm(ofport, s)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: CFM configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n/* Configures BFD on 'ofp_port' in 'ofproto'.  This function has no effect if\n * 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_bfd(struct ofproto *ofproto, ofp_port_t ofp_port,\n                     const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure bfd on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    error = (ofproto->ofproto_class->set_bfd\n             ? ofproto->ofproto_class->set_bfd(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: bfd configuration on port %\"PRIu32\" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n/* Checks the status change of BFD on 'ofport'.\n *\n * Returns true if 'ofproto_class' does not support 'bfd_status_changed'. */\nbool\nofproto_port_bfd_status_changed(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->bfd_status_changed\n            ? ofproto->ofproto_class->bfd_status_changed(ofport)\n            : true);\n}\n\n/* Populates 'status' with the status of BFD on 'ofport'.  Returns 0 on\n * success.  Returns a positive errno otherwise.  Has no effect if 'ofp_port'\n * is not an OpenFlow port in 'ofproto'.\n *\n * The caller must provide and own '*status'. */\nint\nofproto_port_get_bfd_status(struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct smap *status)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->get_bfd_status\n            ? ofproto->ofproto_class->get_bfd_status(ofport, status)\n            : EOPNOTSUPP);\n}\n\n/* Checks the status of LACP negotiation for 'ofp_port' within ofproto.\n * Returns 1 if LACP partner information for 'ofp_port' is up-to-date,\n * 0 if LACP partner information is not current (generally indicating a\n * connectivity problem), or -1 if LACP is not enabled on 'ofp_port'. */\nint\nofproto_port_is_lacp_current(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->port_is_lacp_current\n            ? ofproto->ofproto_class->port_is_lacp_current(ofport)\n            : -1);\n}\n\nint\nofproto_port_get_lacp_stats(const struct ofport *port, struct lacp_slave_stats *stats)\n{\n    struct ofproto *ofproto = port->ofproto;\n    int error;\n\n    if (ofproto->ofproto_class->port_get_lacp_stats) {\n        error = ofproto->ofproto_class->port_get_lacp_stats(port, stats);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\f\n/* Bundles. */\n\n/* Registers a \"bundle\" associated with client data pointer 'aux' in 'ofproto'.\n * A bundle is the same concept as a Port in OVSDB, that is, it consists of one\n * or more \"slave\" devices (Interfaces, in OVSDB) along with a VLAN\n * configuration plus, if there is more than one slave, a bonding\n * configuration.\n *\n * If 'aux' is already registered then this function updates its configuration\n * to 's'.  Otherwise, this function registers a new bundle.\n *\n * Bundles only affect the NXAST_AUTOPATH action and output to the OFPP_NORMAL\n * port. */\nint\nofproto_bundle_register(struct ofproto *ofproto, void *aux,\n                        const struct ofproto_bundle_settings *s)\n{\n    return (ofproto->ofproto_class->bundle_set\n            ? ofproto->ofproto_class->bundle_set(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\n/* Unregisters the bundle registered on 'ofproto' with auxiliary data 'aux'.\n * If no such bundle has been registered, this has no effect. */\nint\nofproto_bundle_unregister(struct ofproto *ofproto, void *aux)\n{\n    return ofproto_bundle_register(ofproto, aux, NULL);\n}\n\n\f\n/* Registers a mirror associated with client data pointer 'aux' in 'ofproto'.\n * If 'aux' is already registered then this function updates its configuration\n * to 's'.  Otherwise, this function registers a new mirror. */\nint\nofproto_mirror_register(struct ofproto *ofproto, void *aux,\n                        const struct ofproto_mirror_settings *s)\n{\n    return (ofproto->ofproto_class->mirror_set\n            ? ofproto->ofproto_class->mirror_set(ofproto, aux, s)\n            : EOPNOTSUPP);\n}\n\n/* Unregisters the mirror registered on 'ofproto' with auxiliary data 'aux'.\n * If no mirror has been registered, this has no effect. */\nint\nofproto_mirror_unregister(struct ofproto *ofproto, void *aux)\n{\n    return ofproto_mirror_register(ofproto, aux, NULL);\n}\n\n/* Retrieves statistics from mirror associated with client data pointer\n * 'aux' in 'ofproto'.  Stores packet and byte counts in 'packets' and\n * 'bytes', respectively.  If a particular counters is not supported,\n * the appropriate argument is set to UINT64_MAX.\n */\nint\nofproto_mirror_get_stats(struct ofproto *ofproto, void *aux,\n                         uint64_t *packets, uint64_t *bytes)\n{\n    if (!ofproto->ofproto_class->mirror_get_stats) {\n        *packets = *bytes = UINT64_MAX;\n        return EOPNOTSUPP;\n    }\n\n    return ofproto->ofproto_class->mirror_get_stats(ofproto, aux,\n                                                    packets, bytes);\n}\n\n/* Configures the VLANs whose bits are set to 1 in 'flood_vlans' as VLANs on\n * which all packets are flooded, instead of using MAC learning.  If\n * 'flood_vlans' is NULL, then MAC learning applies to all VLANs.\n *\n * Flood VLANs affect only the treatment of packets output to the OFPP_NORMAL\n * port. */\nint\nofproto_set_flood_vlans(struct ofproto *ofproto, unsigned long *flood_vlans)\n{\n    return (ofproto->ofproto_class->set_flood_vlans\n            ? ofproto->ofproto_class->set_flood_vlans(ofproto, flood_vlans)\n            : EOPNOTSUPP);\n}\n\n/* Returns true if 'aux' is a registered bundle that is currently in use as the\n * output for a mirror. */\nbool\nofproto_is_mirror_output_bundle(const struct ofproto *ofproto, void *aux)\n{\n    return (ofproto->ofproto_class->is_mirror_output_bundle\n            ? ofproto->ofproto_class->is_mirror_output_bundle(ofproto, aux)\n            : false);\n}\n\f\n/* Configuration of OpenFlow tables. */\n\n/* Returns the number of OpenFlow tables in 'ofproto'. */\nint\nofproto_get_n_tables(const struct ofproto *ofproto)\n{\n    return ofproto->n_tables;\n}\n\n/* Returns the number of Controller visible OpenFlow tables\n * in 'ofproto'. This number will exclude Hidden tables.\n * This funtion's return value should be less or equal to that of\n * ofproto_get_n_tables() . */\nuint8_t\nofproto_get_n_visible_tables(const struct ofproto *ofproto)\n{\n    uint8_t n = ofproto->n_tables;\n\n    /* Count only non-hidden tables in the number of tables.  (Hidden tables,\n     * if present, are always at the end.) */\n    while(n && (ofproto->tables[n - 1].flags & OFTABLE_HIDDEN)) {\n        n--;\n    }\n\n    return n;\n}\n\n/* Configures the OpenFlow table in 'ofproto' with id 'table_id' with the\n * settings from 's'.  'table_id' must be in the range 0 through the number of\n * OpenFlow tables in 'ofproto' minus 1, inclusive.\n *\n * For read-only tables, only the name may be configured. */\nvoid\nofproto_configure_table(struct ofproto *ofproto, int table_id,\n                        const struct ofproto_table_settings *s)\n{\n    struct oftable *table;\n\n    ovs_assert(table_id >= 0 && table_id < ofproto->n_tables);\n    table = &ofproto->tables[table_id];\n\n    oftable_set_name(table, s->name);\n\n    if (table->flags & OFTABLE_READONLY) {\n        return;\n    }\n\n    if (classifier_set_prefix_fields(&table->cls,\n                                     s->prefix_fields, s->n_prefix_fields)) {\n        /* XXX: Trigger revalidation. */\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    unsigned int new_eviction = (s->enable_eviction\n                                 ? table->eviction | EVICTION_CLIENT\n                                 : table->eviction & ~EVICTION_CLIENT);\n    oftable_configure_eviction(table, new_eviction, s->groups, s->n_groups);\n    table->max_flows = s->max_flows;\n    evict_rules_from_table(table);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\f\nbool\nofproto_has_snoops(const struct ofproto *ofproto)\n{\n    return connmgr_has_snoops(ofproto->connmgr);\n}\n\nvoid\nofproto_get_snoops(const struct ofproto *ofproto, struct sset *snoops)\n{\n    connmgr_get_snoops(ofproto->connmgr, snoops);\n}\n\n/* Deletes 'rule' from 'ofproto'.\n *\n * Within an ofproto implementation, this function allows an ofproto\n * implementation to destroy any rules that remain when its ->destruct()\n * function is called.  This function is not suitable for use elsewhere in an\n * ofproto implementation.\n *\n * This function implements steps 4.4 and 4.5 in the section titled \"Rule Life\n * Cycle\" in ofproto-provider.h. */\nvoid\nofproto_rule_delete(struct ofproto *ofproto, struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    /* This skips the ofmonitor and flow-removed notifications because the\n     * switch is being deleted and any OpenFlow channels have been or soon will\n     * be killed. */\n    ovs_mutex_lock(&ofproto_mutex);\n\n    if (rule->state == RULE_INSERTED) {\n        /* Make sure there is no postponed removal of the rule. */\n        ovs_assert(cls_rule_visible_in_version(&rule->cr, OVS_VERSION_MAX));\n\n        if (!classifier_remove(&rule->ofproto->tables[rule->table_id].cls,\n                               &rule->cr)) {\n            OVS_NOT_REACHED();\n        }\n        ofproto_rule_remove__(rule->ofproto, rule);\n        if (ofproto->ofproto_class->rule_delete) {\n            ofproto->ofproto_class->rule_delete(rule);\n        }\n\n        /* This may not be the last reference to the rule. */\n        ofproto_rule_unref(rule);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic void\nofproto_flush__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n\n    /* This will flush all datapath flows. */\n    if (ofproto->ofproto_class->flush) {\n        ofproto->ofproto_class->flush(ofproto);\n    }\n\n    /* XXX: There is a small race window here, where new datapath flows can be\n     * created by upcall handlers based on the existing flow table.  We can not\n     * call ofproto class flush while holding 'ofproto_mutex' to prevent this,\n     * as then we could deadlock on syncing with the handler threads waiting on\n     * the same mutex. */\n\n    ovs_mutex_lock(&ofproto_mutex);\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        struct rule_collection rules;\n        struct rule *rule;\n\n        if (table->flags & OFTABLE_HIDDEN) {\n            continue;\n        }\n\n        rule_collection_init(&rules);\n\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            rule_collection_add(&rules, rule);\n        }\n        delete_flows__(&rules, OFPRR_DELETE, NULL);\n    }\n    /* XXX: Concurrent handler threads may insert new learned flows based on\n     * learn actions of the now deleted flows right after we release\n     * 'ofproto_mutex'. */\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic void\nofproto_destroy__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n\n    cmap_destroy(&ofproto->groups);\n\n    hmap_remove(&all_ofprotos, &ofproto->hmap_node);\n\n    free(ofproto->name);\n    free(ofproto->type);\n    free(ofproto->mfr_desc);\n    free(ofproto->hw_desc);\n    free(ofproto->sw_desc);\n    free(ofproto->serial_desc);\n    free(ofproto->dp_desc);\n    hmap_destroy(&ofproto->ports);\n    hmap_destroy(&ofproto->ofport_usage);\n    shash_destroy(&ofproto->port_by_name);\n    simap_destroy(&ofproto->ofp_requests);\n\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        oftable_destroy(table);\n    }\n    free(ofproto->tables);\n\n    ovs_mutex_lock(&ofproto->vl_mff_map.mutex);\n    mf_vl_mff_map_clear(&ofproto->vl_mff_map, true);\n    ovs_mutex_unlock(&ofproto->vl_mff_map.mutex);\n    cmap_destroy(&ofproto->vl_mff_map.cmap);\n    ovs_mutex_destroy(&ofproto->vl_mff_map.mutex);\n    tun_metadata_free(ovsrcu_get_protected(struct tun_table *,\n                                           &ofproto->metadata_tab));\n\n    ovs_assert(hindex_is_empty(&ofproto->cookies));\n    hindex_destroy(&ofproto->cookies);\n\n    ovs_assert(hmap_is_empty(&ofproto->learned_cookies));\n    hmap_destroy(&ofproto->learned_cookies);\n\n    ofproto->ofproto_class->dealloc(ofproto);\n}\n\n/* Destroying rules is doubly deferred, must have 'ofproto' around for them.\n * - 1st we defer the removal of the rules from the classifier\n * - 2nd we defer the actual destruction of the rules. */\nstatic void\nofproto_destroy_defer__(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovsrcu_postpone(ofproto_destroy__, ofproto);\n}\n\nvoid\nofproto_destroy(struct ofproto *p, bool del)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofport *ofport, *next_ofport;\n    struct ofport_usage *usage;\n\n    if (!p) {\n        return;\n    }\n\n    if (p->meters) {\n        meter_delete(p, 1, p->meter_features.max_meters);\n        p->meter_features.max_meters = 0;\n        free(p->meters);\n        p->meters = NULL;\n    }\n\n    ofproto_flush__(p);\n    HMAP_FOR_EACH_SAFE (ofport, next_ofport, hmap_node, &p->ports) {\n        ofport_destroy(ofport, del);\n    }\n\n    HMAP_FOR_EACH_POP (usage, hmap_node, &p->ofport_usage) {\n        free(usage);\n    }\n\n    p->ofproto_class->destruct(p, del);\n\n    /* We should not postpone this because it involves deleting a listening\n     * socket which we may want to reopen soon. 'connmgr' may be used by other\n     * threads only if they take the ofproto_mutex and read a non-NULL\n     * 'ofproto->connmgr'. */\n    ovs_mutex_lock(&ofproto_mutex);\n    connmgr_destroy(p->connmgr);\n    p->connmgr = NULL;\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    /* Destroying rules is deferred, must have 'ofproto' around for them. */\n    ovsrcu_postpone(ofproto_destroy_defer__, p);\n}\n\n/* Destroys the datapath with the respective 'name' and 'type'.  With the Linux\n * kernel datapath, for example, this destroys the datapath in the kernel, and\n * with the netdev-based datapath, it tears down the data structures that\n * represent the datapath.\n *\n * The datapath should not be currently open as an ofproto. */\nint\nofproto_delete(const char *name, const char *type)\n{\n    const struct ofproto_class *class = ofproto_class_find__(type);\n    return (!class ? EAFNOSUPPORT\n            : !class->del ? EACCES\n            : class->del(type, name));\n}\n\nstatic void\nprocess_port_change(struct ofproto *ofproto, int error, char *devname)\n{\n    if (error == ENOBUFS) {\n        reinit_ports(ofproto);\n    } else if (!error) {\n        update_port(ofproto, devname);\n        free(devname);\n    }\n}\n\nint\nofproto_type_run(const char *datapath_type)\n{\n    const struct ofproto_class *class;\n    int error;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    error = class->type_run ? class->type_run(datapath_type) : 0;\n    if (error && error != EAGAIN) {\n        VLOG_ERR_RL(&rl, \"%s: type_run failed (%s)\",\n                    datapath_type, ovs_strerror(error));\n    }\n    return error;\n}\n\nvoid\nofproto_type_wait(const char *datapath_type)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    if (class->type_wait) {\n        class->type_wait(datapath_type);\n    }\n}\n\nint\nofproto_run(struct ofproto *p)\n{\n    int error;\n    uint64_t new_seq;\n\n    error = p->ofproto_class->run(p);\n    if (error && error != EAGAIN) {\n        VLOG_ERR_RL(&rl, \"%s: run failed (%s)\", p->name, ovs_strerror(error));\n    }\n\n    /* Restore the eviction group heap invariant occasionally. */\n    if (p->eviction_group_timer < time_msec()) {\n        size_t i;\n\n        p->eviction_group_timer = time_msec() + 1000;\n\n        for (i = 0; i < p->n_tables; i++) {\n            struct oftable *table = &p->tables[i];\n            struct eviction_group *evg;\n            struct rule *rule;\n\n            if (!table->eviction) {\n                continue;\n            }\n\n            if (table->n_flows > 100000) {\n                static struct vlog_rate_limit count_rl =\n                    VLOG_RATE_LIMIT_INIT(1, 1);\n                VLOG_WARN_RL(&count_rl, \"Table %\"PRIuSIZE\" has an excessive\"\n                             \" number of rules: %d\", i, table->n_flows);\n            }\n\n            ovs_mutex_lock(&ofproto_mutex);\n            CLS_FOR_EACH (rule, cr, &table->cls) {\n                if (rule->idle_timeout || rule->hard_timeout) {\n                    if (!rule->eviction_group) {\n                        eviction_group_add_rule(rule);\n                    } else {\n                        heap_raw_change(&rule->evg_node,\n                                        rule_eviction_priority(p, rule));\n                    }\n                }\n            }\n\n            HEAP_FOR_EACH (evg, size_node, &table->eviction_groups_by_size) {\n                heap_rebuild(&evg->rules);\n            }\n            ovs_mutex_unlock(&ofproto_mutex);\n        }\n    }\n\n    if (p->ofproto_class->port_poll) {\n        char *devname;\n\n        while ((error = p->ofproto_class->port_poll(p, &devname)) != EAGAIN) {\n            process_port_change(p, error, devname);\n        }\n    }\n\n    new_seq = seq_read(connectivity_seq_get());\n    if (new_seq != p->change_seq) {\n        struct sset devnames;\n        const char *devname;\n        struct ofport *ofport;\n\n        /* Update OpenFlow port status for any port whose netdev has changed.\n         *\n         * Refreshing a given 'ofport' can cause an arbitrary ofport to be\n         * destroyed, so it's not safe to update ports directly from the\n         * HMAP_FOR_EACH loop, or even to use HMAP_FOR_EACH_SAFE.  Instead, we\n         * need this two-phase approach. */\n        sset_init(&devnames);\n        HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n            uint64_t port_change_seq;\n\n            port_change_seq = netdev_get_change_seq(ofport->netdev);\n            if (ofport->change_seq != port_change_seq) {\n                ofport->change_seq = port_change_seq;\n                sset_add(&devnames, netdev_get_name(ofport->netdev));\n            }\n        }\n        SSET_FOR_EACH (devname, &devnames) {\n            update_port(p, devname);\n        }\n        sset_destroy(&devnames);\n\n        p->change_seq = new_seq;\n    }\n\n    connmgr_run(p->connmgr, handle_openflow);\n\n    return error;\n}\n\nvoid\nofproto_wait(struct ofproto *p)\n{\n    p->ofproto_class->wait(p);\n    if (p->ofproto_class->port_poll_wait) {\n        p->ofproto_class->port_poll_wait(p);\n    }\n    seq_wait(connectivity_seq_get(), p->change_seq);\n    connmgr_wait(p->connmgr);\n}\n\nbool\nofproto_is_alive(const struct ofproto *p)\n{\n    return connmgr_has_controllers(p->connmgr);\n}\n\n/* Adds some memory usage statistics for 'ofproto' into 'usage', for use with\n * memory_report(). */\nvoid\nofproto_get_memory_usage(const struct ofproto *ofproto, struct simap *usage)\n{\n    const struct oftable *table;\n    unsigned int n_rules;\n\n    simap_increase(usage, \"ports\", hmap_count(&ofproto->ports));\n\n    n_rules = 0;\n    OFPROTO_FOR_EACH_TABLE (table, ofproto) {\n        n_rules += table->n_flows;\n    }\n    simap_increase(usage, \"rules\", n_rules);\n\n    if (ofproto->ofproto_class->get_memory_usage) {\n        ofproto->ofproto_class->get_memory_usage(ofproto, usage);\n    }\n\n    connmgr_get_memory_usage(ofproto->connmgr, usage);\n}\n\nvoid\nofproto_type_get_memory_usage(const char *datapath_type, struct simap *usage)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n\n    if (class && class->type_get_memory_usage) {\n        class->type_get_memory_usage(datapath_type, usage);\n    }\n}\n\nvoid\nofproto_get_ofproto_controller_info(const struct ofproto *ofproto,\n                                    struct shash *info)\n{\n    connmgr_get_controller_info(ofproto->connmgr, info);\n}\n\nvoid\nofproto_free_ofproto_controller_info(struct shash *info)\n{\n    connmgr_free_controller_info(info);\n}\n\n/* Makes a deep copy of 'old' into 'port'. */\nvoid\nofproto_port_clone(struct ofproto_port *port, const struct ofproto_port *old)\n{\n    port->name = xstrdup(old->name);\n    port->type = xstrdup(old->type);\n    port->ofp_port = old->ofp_port;\n}\n\n/* Frees memory allocated to members of 'ofproto_port'.\n *\n * Do not call this function on an ofproto_port obtained from\n * ofproto_port_dump_next(): that function retains ownership of the data in the\n * ofproto_port. */\nvoid\nofproto_port_destroy(struct ofproto_port *ofproto_port)\n{\n    free(ofproto_port->name);\n    free(ofproto_port->type);\n}\n\n/* Initializes 'dump' to begin dumping the ports in an ofproto.\n *\n * This function provides no status indication.  An error status for the entire\n * dump operation is provided when it is completed by calling\n * ofproto_port_dump_done().\n */\nvoid\nofproto_port_dump_start(struct ofproto_port_dump *dump,\n                        const struct ofproto *ofproto)\n{\n    dump->ofproto = ofproto;\n    dump->error = ofproto->ofproto_class->port_dump_start(ofproto,\n                                                          &dump->state);\n}\n\n/* Attempts to retrieve another port from 'dump', which must have been created\n * with ofproto_port_dump_start().  On success, stores a new ofproto_port into\n * 'port' and returns true.  On failure, returns false.\n *\n * Failure might indicate an actual error or merely that the last port has been\n * dumped.  An error status for the entire dump operation is provided when it\n * is completed by calling ofproto_port_dump_done().\n *\n * The ofproto owns the data stored in 'port'.  It will remain valid until at\n * least the next time 'dump' is passed to ofproto_port_dump_next() or\n * ofproto_port_dump_done(). */\nbool\nofproto_port_dump_next(struct ofproto_port_dump *dump,\n                       struct ofproto_port *port)\n{\n    const struct ofproto *ofproto = dump->ofproto;\n\n    if (dump->error) {\n        return false;\n    }\n\n    dump->error = ofproto->ofproto_class->port_dump_next(ofproto, dump->state,\n                                                         port);\n    if (dump->error) {\n        ofproto->ofproto_class->port_dump_done(ofproto, dump->state);\n        return false;\n    }\n    return true;\n}\n\n/* Completes port table dump operation 'dump', which must have been created\n * with ofproto_port_dump_start().  Returns 0 if the dump operation was\n * error-free, otherwise a positive errno value describing the problem. */\nint\nofproto_port_dump_done(struct ofproto_port_dump *dump)\n{\n    const struct ofproto *ofproto = dump->ofproto;\n    if (!dump->error) {\n        dump->error = ofproto->ofproto_class->port_dump_done(ofproto,\n                                                             dump->state);\n    }\n    return dump->error == EOF ? 0 : dump->error;\n}\n\n/* Returns the type to pass to netdev_open() when a datapath of type\n * 'datapath_type' has a port of type 'port_type', for a few special\n * cases when a netdev type differs from a port type.  For example, when\n * using the userspace datapath, a port of type \"internal\" needs to be\n * opened as \"tap\".\n *\n * Returns either 'type' itself or a string literal, which must not be\n * freed. */\nconst char *\nofproto_port_open_type(const char *datapath_type, const char *port_type)\n{\n    const struct ofproto_class *class;\n\n    datapath_type = ofproto_normalize_type(datapath_type);\n    class = ofproto_class_find__(datapath_type);\n    if (!class) {\n        return port_type;\n    }\n\n    return (class->port_open_type\n            ? class->port_open_type(datapath_type, port_type)\n            : port_type);\n}\n\n/* Attempts to add 'netdev' as a port on 'ofproto'.  If 'ofp_portp' is\n * non-null and '*ofp_portp' is not OFPP_NONE, attempts to use that as\n * the port's OpenFlow port number.\n *\n * If successful, returns 0 and sets '*ofp_portp' to the new port's\n * OpenFlow port number (if 'ofp_portp' is non-null).  On failure,\n * returns a positive errno value and sets '*ofp_portp' to OFPP_NONE (if\n * 'ofp_portp' is non-null). */\nint\nofproto_port_add(struct ofproto *ofproto, struct netdev *netdev,\n                 ofp_port_t *ofp_portp)\n{\n    ofp_port_t ofp_port = ofp_portp ? *ofp_portp : OFPP_NONE;\n    int error;\n\n    error = ofproto->ofproto_class->port_add(ofproto, netdev);\n    if (!error) {\n        const char *netdev_name = netdev_get_name(netdev);\n\n        simap_put(&ofproto->ofp_requests, netdev_name,\n                  ofp_to_u16(ofp_port));\n        error = update_port(ofproto, netdev_name);\n    }\n    if (ofp_portp) {\n        *ofp_portp = OFPP_NONE;\n        if (!error) {\n            struct ofproto_port ofproto_port;\n\n            error = ofproto_port_query_by_name(ofproto,\n                                               netdev_get_name(netdev),\n                                               &ofproto_port);\n            if (!error) {\n                *ofp_portp = ofproto_port.ofp_port;\n                ofproto_port_destroy(&ofproto_port);\n            }\n        }\n    }\n    return error;\n}\n\n/* Looks up a port named 'devname' in 'ofproto'.  On success, returns 0 and\n * initializes '*port' appropriately; on failure, returns a positive errno\n * value.\n *\n * The caller owns the data in 'ofproto_port' and must free it with\n * ofproto_port_destroy() when it is no longer needed. */\nint\nofproto_port_query_by_name(const struct ofproto *ofproto, const char *devname,\n                           struct ofproto_port *port)\n{\n    int error;\n\n    error = ofproto->ofproto_class->port_query_by_name(ofproto, devname, port);\n    if (error) {\n        memset(port, 0, sizeof *port);\n    }\n    return error;\n}\n\n/* Deletes port number 'ofp_port' from the datapath for 'ofproto'.\n * Returns 0 if successful, otherwise a positive errno. */\nint\nofproto_port_del(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    const char *name = ofport ? netdev_get_name(ofport->netdev) : \"<unknown>\";\n    struct simap_node *ofp_request_node;\n    int error;\n\n    ofp_request_node = simap_find(&ofproto->ofp_requests, name);\n    if (ofp_request_node) {\n        simap_delete(&ofproto->ofp_requests, ofp_request_node);\n    }\n\n    error = ofproto->ofproto_class->port_del(ofproto, ofp_port);\n    if (!error && ofport) {\n        /* 'name' is the netdev's name and update_port() is going to close the\n         * netdev.  Just in case update_port() refers to 'name' after it\n         * destroys 'ofport', make a copy of it around the update_port()\n         * call. */\n        char *devname = xstrdup(name);\n        update_port(ofproto, devname);\n        free(devname);\n    }\n    return error;\n}\n\n/* Refreshes datapath configuration of port number 'ofp_port' in 'ofproto'.\n *\n * This function has no effect if 'ofproto' does not have a port 'ofp_port'. */\nvoid\nofproto_port_set_config(struct ofproto *ofproto, ofp_port_t ofp_port,\n                        const struct smap *cfg)\n{\n    struct ofport *ofport;\n    int error;\n\n    ofport = ofproto_get_port(ofproto, ofp_port);\n    if (!ofport) {\n        VLOG_WARN(\"%s: cannot configure datapath on nonexistent port %\"PRIu32,\n                  ofproto->name, ofp_port);\n        return;\n    }\n\n    error = (ofproto->ofproto_class->port_set_config\n             ? ofproto->ofproto_class->port_set_config(ofport, cfg)\n             : EOPNOTSUPP);\n    if (error) {\n        VLOG_WARN(\"%s: datapath configuration on port %\"PRIu32\n                  \" (%s) failed (%s)\",\n                  ofproto->name, ofp_port, netdev_get_name(ofport->netdev),\n                  ovs_strerror(error));\n    }\n}\n\n\nstatic void\nflow_mod_init(struct ofputil_flow_mod *fm,\n              const struct match *match, int priority,\n              const struct ofpact *ofpacts, size_t ofpacts_len,\n              enum ofp_flow_mod_command command)\n{\n    *fm = (struct ofputil_flow_mod) {\n        .match = *match,\n        .priority = priority,\n        .table_id = 0,\n        .command = command,\n        .buffer_id = UINT32_MAX,\n        .out_port = OFPP_ANY,\n        .out_group = OFPG_ANY,\n        .ofpacts = CONST_CAST(struct ofpact *, ofpacts),\n        .ofpacts_len = ofpacts_len,\n    };\n}\n\nstatic int\nsimple_flow_mod(struct ofproto *ofproto,\n                const struct match *match, int priority,\n                const struct ofpact *ofpacts, size_t ofpacts_len,\n                enum ofp_flow_mod_command command)\n{\n    struct ofputil_flow_mod fm;\n\n    flow_mod_init(&fm, match, priority, ofpacts, ofpacts_len, command);\n\n    return handle_flow_mod__(ofproto, &fm, NULL);\n}\n\n/* Adds a flow to OpenFlow flow table 0 in 'p' that matches 'cls_rule' and\n * performs the 'n_actions' actions in 'actions'.  The new flow will not\n * timeout.\n *\n * If cls_rule->priority is in the range of priorities supported by OpenFlow\n * (0...65535, inclusive) then the flow will be visible to OpenFlow\n * controllers; otherwise, it will be hidden.\n *\n * The caller retains ownership of 'cls_rule' and 'ofpacts'.\n *\n * This is a helper function for in-band control and fail-open. */\nvoid\nofproto_add_flow(struct ofproto *ofproto, const struct match *match,\n                 int priority,\n                 const struct ofpact *ofpacts, size_t ofpacts_len)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct rule *rule;\n    bool must_add;\n\n    /* First do a cheap check whether the rule we're looking for already exists\n     * with the actions that we want.  If it does, then we're done. */\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  &ofproto->tables[0].cls, match, priority,\n                                  OVS_VERSION_MAX));\n    if (rule) {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        must_add = !ofpacts_equal(actions->ofpacts, actions->ofpacts_len,\n                                  ofpacts, ofpacts_len);\n    } else {\n        must_add = true;\n    }\n\n    /* If there's no such rule or the rule doesn't have the actions we want,\n     * fall back to a executing a full flow mod.  We can't optimize this at\n     * all because we didn't take enough locks above to ensure that the flow\n     * table didn't already change beneath us.  */\n    if (must_add) {\n        simple_flow_mod(ofproto, match, priority, ofpacts, ofpacts_len,\n                        OFPFC_MODIFY_STRICT);\n    }\n}\n\n/* Executes the flow modification specified in 'fm'.  Returns 0 on success, or\n * an OFPERR_* OpenFlow error code on failure.\n *\n * This is a helper function for in-band control and fail-open. */\nenum ofperr\nofproto_flow_mod(struct ofproto *ofproto, const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    return handle_flow_mod__(ofproto, fm, NULL);\n}\n\n/* Searches for a rule with matching criteria exactly equal to 'target' in\n * ofproto's table 0 and, if it finds one, deletes it.\n *\n * This is a helper function for in-band control and fail-open. */\nvoid\nofproto_delete_flow(struct ofproto *ofproto,\n                    const struct match *target, int priority)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct classifier *cls = &ofproto->tables[0].cls;\n    struct rule *rule;\n\n    /* First do a cheap check whether the rule we're looking for has already\n     * been deleted.  If so, then we're done. */\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  cls, target, priority, OVS_VERSION_MAX));\n    if (!rule) {\n        return;\n    }\n\n    struct rule_collection rules;\n\n    rule_collection_init(&rules);\n    rule_collection_add(&rules, rule);\n    delete_flows__(&rules, OFPRR_DELETE, NULL);\n    rule_collection_destroy(&rules);\n}\n\n/* Delete all of the flows from all of ofproto's flow tables, then reintroduce\n * the flows required by in-band control and fail-open.  */\nvoid\nofproto_flush_flows(struct ofproto *ofproto)\n{\n    COVERAGE_INC(ofproto_flush);\n    ofproto_flush__(ofproto);\n    connmgr_flushed(ofproto->connmgr);\n}\n\f\nstatic void\nreinit_ports(struct ofproto *p)\n{\n    struct ofproto_port_dump dump;\n    struct sset devnames;\n    struct ofport *ofport;\n    struct ofproto_port ofproto_port;\n    const char *devname;\n\n    COVERAGE_INC(ofproto_reinit_ports);\n\n    sset_init(&devnames);\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        sset_add(&devnames, netdev_get_name(ofport->netdev));\n    }\n    OFPROTO_PORT_FOR_EACH (&ofproto_port, &dump, p) {\n        sset_add(&devnames, ofproto_port.name);\n    }\n\n    SSET_FOR_EACH (devname, &devnames) {\n        update_port(p, devname);\n    }\n    sset_destroy(&devnames);\n}\n\nstatic ofp_port_t\nalloc_ofp_port(struct ofproto *ofproto, const char *netdev_name)\n{\n    uint16_t port_idx;\n\n    port_idx = simap_get(&ofproto->ofp_requests, netdev_name);\n    port_idx = port_idx ? port_idx : UINT16_MAX;\n\n    if (port_idx >= ofproto->max_ports\n        || ofport_get_usage(ofproto, u16_to_ofp(port_idx)) == LLONG_MAX) {\n        uint16_t lru_ofport = 0, end_port_no = ofproto->alloc_port_no;\n        long long int last_used_at, lru = LLONG_MAX;\n\n        /* Search for a free OpenFlow port number.  We try not to\n         * immediately reuse them to prevent problems due to old\n         * flows.\n         *\n         * We limit the automatically assigned port numbers to the lower half\n         * of the port range, to reserve the upper half for assignment by\n         * controllers. */\n        for (;;) {\n            if (++ofproto->alloc_port_no >= MIN(ofproto->max_ports, 32768)) {\n                ofproto->alloc_port_no = 1;\n            }\n            last_used_at = ofport_get_usage(ofproto,\n                                         u16_to_ofp(ofproto->alloc_port_no));\n            if (!last_used_at) {\n                port_idx = ofproto->alloc_port_no;\n                break;\n            } else if ( last_used_at < time_msec() - 60*60*1000) {\n                /* If the port with ofport 'ofproto->alloc_port_no' was deleted\n                 * more than an hour ago, consider it usable. */\n                ofport_remove_usage(ofproto,\n                    u16_to_ofp(ofproto->alloc_port_no));\n                port_idx = ofproto->alloc_port_no;\n                break;\n            } else if (last_used_at < lru) {\n                lru = last_used_at;\n                lru_ofport = ofproto->alloc_port_no;\n            }\n\n            if (ofproto->alloc_port_no == end_port_no) {\n                if (lru_ofport) {\n                    port_idx = lru_ofport;\n                    break;\n                }\n                return OFPP_NONE;\n            }\n        }\n    }\n    ofport_set_usage(ofproto, u16_to_ofp(port_idx), LLONG_MAX);\n    return u16_to_ofp(port_idx);\n}\n\nstatic void\ndealloc_ofp_port(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    if (ofp_to_u16(ofp_port) < ofproto->max_ports) {\n        ofport_set_usage(ofproto, ofp_port, time_msec());\n    }\n}\n\n/* Opens and returns a netdev for 'ofproto_port' in 'ofproto', or a null\n * pointer if the netdev cannot be opened.  On success, also fills in\n * '*pp'.  */\nstatic struct netdev *\nofport_open(struct ofproto *ofproto,\n            struct ofproto_port *ofproto_port,\n            struct ofputil_phy_port *pp)\n{\n    enum netdev_flags flags;\n    struct netdev *netdev;\n    int error;\n\n    error = netdev_open(ofproto_port->name, ofproto_port->type, &netdev);\n    if (error) {\n        VLOG_WARN_RL(&rl, \"%s: ignoring port %s (%\"PRIu32\") because netdev %s \"\n                     \"cannot be opened (%s)\",\n                     ofproto->name,\n                     ofproto_port->name, ofproto_port->ofp_port,\n                     ofproto_port->name, ovs_strerror(error));\n        return NULL;\n    }\n\n    if (ofproto_port->ofp_port == OFPP_NONE) {\n        if (!strcmp(ofproto->name, ofproto_port->name)) {\n            ofproto_port->ofp_port = OFPP_LOCAL;\n        } else {\n            ofproto_port->ofp_port = alloc_ofp_port(ofproto,\n                                                    ofproto_port->name);\n        }\n    }\n    pp->port_no = ofproto_port->ofp_port;\n    netdev_get_etheraddr(netdev, &pp->hw_addr);\n    ovs_strlcpy(pp->name, ofproto_port->name, sizeof pp->name);\n    netdev_get_flags(netdev, &flags);\n    pp->config = flags & NETDEV_UP ? 0 : OFPUTIL_PC_PORT_DOWN;\n    pp->state = netdev_get_carrier(netdev) ? 0 : OFPUTIL_PS_LINK_DOWN;\n    netdev_get_features(netdev, &pp->curr, &pp->advertised,\n                        &pp->supported, &pp->peer);\n    pp->curr_speed = netdev_features_to_bps(pp->curr, 0) / 1000;\n    pp->max_speed = netdev_features_to_bps(pp->supported, 0) / 1000;\n\n    return netdev;\n}\n\n/* Returns true if most fields of 'a' and 'b' are equal.  Differences in name,\n * port number, and 'config' bits other than OFPUTIL_PC_PORT_DOWN are\n * disregarded. */\nstatic bool\nofport_equal(const struct ofputil_phy_port *a,\n             const struct ofputil_phy_port *b)\n{\n    return (eth_addr_equals(a->hw_addr, b->hw_addr)\n            && a->state == b->state\n            && !((a->config ^ b->config) & OFPUTIL_PC_PORT_DOWN)\n            && a->curr == b->curr\n            && a->advertised == b->advertised\n            && a->supported == b->supported\n            && a->peer == b->peer\n            && a->curr_speed == b->curr_speed\n            && a->max_speed == b->max_speed);\n}\n\n/* Adds an ofport to 'p' initialized based on the given 'netdev' and 'opp'.\n * The caller must ensure that 'p' does not have a conflicting ofport (that is,\n * one with the same name or port number). */\nstatic int\nofport_install(struct ofproto *p,\n               struct netdev *netdev, const struct ofputil_phy_port *pp)\n{\n    const char *netdev_name = netdev_get_name(netdev);\n    struct ofport *ofport;\n    int error;\n\n    /* Create ofport. */\n    ofport = p->ofproto_class->port_alloc();\n    if (!ofport) {\n        error = ENOMEM;\n        goto error;\n    }\n    ofport->ofproto = p;\n    ofport->netdev = netdev;\n    ofport->change_seq = netdev_get_change_seq(netdev);\n    ofport->pp = *pp;\n    ofport->ofp_port = pp->port_no;\n    ofport->created = time_msec();\n\n    /* Add port to 'p'. */\n    hmap_insert(&p->ports, &ofport->hmap_node,\n                hash_ofp_port(ofport->ofp_port));\n    shash_add(&p->port_by_name, netdev_name, ofport);\n\n    update_mtu(p, ofport);\n\n    /* Let the ofproto_class initialize its private data. */\n    error = p->ofproto_class->port_construct(ofport);\n    if (error) {\n        goto error;\n    }\n    connmgr_send_port_status(p->connmgr, NULL, pp, OFPPR_ADD);\n    return 0;\n\nerror:\n    VLOG_WARN_RL(&rl, \"%s: could not add port %s (%s)\",\n                 p->name, netdev_name, ovs_strerror(error));\n    if (ofport) {\n        ofport_destroy__(ofport);\n    } else {\n        netdev_close(netdev);\n    }\n    return error;\n}\n\n/* Removes 'ofport' from 'p' and destroys it. */\nstatic void\nofport_remove(struct ofport *ofport)\n{\n    struct ofproto *p = ofport->ofproto;\n    bool is_mtu_overridden = ofport_is_mtu_overridden(p, ofport);\n\n    connmgr_send_port_status(ofport->ofproto->connmgr, NULL, &ofport->pp,\n                             OFPPR_DELETE);\n    ofport_destroy(ofport, true);\n    if (!is_mtu_overridden) {\n        update_mtu_ofproto(p);\n    }\n}\n\n/* If 'ofproto' contains an ofport named 'name', removes it from 'ofproto' and\n * destroys it. */\nstatic void\nofport_remove_with_name(struct ofproto *ofproto, const char *name)\n{\n    struct ofport *port = shash_find_data(&ofproto->port_by_name, name);\n    if (port) {\n        ofport_remove(port);\n    }\n}\n\n/* Updates 'port' with new 'pp' description.\n *\n * Does not handle a name or port number change.  The caller must implement\n * such a change as a delete followed by an add.  */\nstatic void\nofport_modified(struct ofport *port, struct ofputil_phy_port *pp)\n{\n    port->pp.hw_addr = pp->hw_addr;\n    port->pp.config = ((port->pp.config & ~OFPUTIL_PC_PORT_DOWN)\n                        | (pp->config & OFPUTIL_PC_PORT_DOWN));\n    port->pp.state = ((port->pp.state & ~OFPUTIL_PS_LINK_DOWN)\n                      | (pp->state & OFPUTIL_PS_LINK_DOWN));\n    port->pp.curr = pp->curr;\n    port->pp.advertised = pp->advertised;\n    port->pp.supported = pp->supported;\n    port->pp.peer = pp->peer;\n    port->pp.curr_speed = pp->curr_speed;\n    port->pp.max_speed = pp->max_speed;\n\n    connmgr_send_port_status(port->ofproto->connmgr, NULL,\n                             &port->pp, OFPPR_MODIFY);\n}\n\n/* Update OpenFlow 'state' in 'port' and notify controller. */\nvoid\nofproto_port_set_state(struct ofport *port, enum ofputil_port_state state)\n{\n    if (port->pp.state != state) {\n        port->pp.state = state;\n        connmgr_send_port_status(port->ofproto->connmgr, NULL,\n                                 &port->pp, OFPPR_MODIFY);\n    }\n}\n\nvoid\nofproto_port_unregister(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *port = ofproto_get_port(ofproto, ofp_port);\n    if (port) {\n        if (port->ofproto->ofproto_class->set_stp_port) {\n            port->ofproto->ofproto_class->set_stp_port(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->set_rstp_port) {\n            port->ofproto->ofproto_class->set_rstp_port(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->set_cfm) {\n            port->ofproto->ofproto_class->set_cfm(port, NULL);\n        }\n        if (port->ofproto->ofproto_class->bundle_remove) {\n            port->ofproto->ofproto_class->bundle_remove(port);\n        }\n    }\n}\n\nstatic void\nofport_destroy__(struct ofport *port)\n{\n    struct ofproto *ofproto = port->ofproto;\n    const char *name = netdev_get_name(port->netdev);\n\n    hmap_remove(&ofproto->ports, &port->hmap_node);\n    shash_delete(&ofproto->port_by_name,\n                 shash_find(&ofproto->port_by_name, name));\n\n    netdev_close(port->netdev);\n    ofproto->ofproto_class->port_dealloc(port);\n}\n\nstatic void\nofport_destroy(struct ofport *port, bool del)\n{\n    if (port) {\n        dealloc_ofp_port(port->ofproto, port->ofp_port);\n        port->ofproto->ofproto_class->port_destruct(port, del);\n        ofport_destroy__(port);\n     }\n}\n\nstruct ofport *\nofproto_get_port(const struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *port;\n\n    HMAP_FOR_EACH_IN_BUCKET (port, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ports) {\n        if (port->ofp_port == ofp_port) {\n            return port;\n        }\n    }\n    return NULL;\n}\n\nstatic long long int\nofport_get_usage(const struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport_usage *usage;\n\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            return usage->last_used;\n        }\n    }\n    return 0;\n}\n\nstatic void\nofport_set_usage(struct ofproto *ofproto, ofp_port_t ofp_port,\n                 long long int last_used)\n{\n    struct ofport_usage *usage;\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            usage->last_used = last_used;\n            return;\n        }\n    }\n    ovs_assert(last_used == LLONG_MAX);\n\n    usage = xmalloc(sizeof *usage);\n    usage->ofp_port = ofp_port;\n    usage->last_used = last_used;\n    hmap_insert(&ofproto->ofport_usage, &usage->hmap_node,\n                hash_ofp_port(ofp_port));\n}\n\nstatic void\nofport_remove_usage(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport_usage *usage;\n    HMAP_FOR_EACH_IN_BUCKET (usage, hmap_node, hash_ofp_port(ofp_port),\n                             &ofproto->ofport_usage) {\n        if (usage->ofp_port == ofp_port) {\n            hmap_remove(&ofproto->ofport_usage, &usage->hmap_node);\n            free(usage);\n            break;\n        }\n    }\n}\n\nint\nofproto_port_get_stats(const struct ofport *port, struct netdev_stats *stats)\n{\n    struct ofproto *ofproto = port->ofproto;\n    int error;\n\n    if (ofproto->ofproto_class->port_get_stats) {\n        error = ofproto->ofproto_class->port_get_stats(port, stats);\n    } else {\n        error = EOPNOTSUPP;\n    }\n\n    return error;\n}\n\nstatic int\nupdate_port(struct ofproto *ofproto, const char *name)\n{\n    struct ofproto_port ofproto_port;\n    struct ofputil_phy_port pp;\n    struct netdev *netdev;\n    struct ofport *port;\n    int error = 0;\n\n    COVERAGE_INC(ofproto_update_port);\n\n    /* Fetch 'name''s location and properties from the datapath. */\n    netdev = (!ofproto_port_query_by_name(ofproto, name, &ofproto_port)\n              ? ofport_open(ofproto, &ofproto_port, &pp)\n              : NULL);\n\n    if (netdev) {\n        port = ofproto_get_port(ofproto, ofproto_port.ofp_port);\n        if (port && !strcmp(netdev_get_name(port->netdev), name)) {\n            struct netdev *old_netdev = port->netdev;\n\n            /* 'name' hasn't changed location.  Any properties changed? */\n            if (!ofport_equal(&port->pp, &pp)) {\n                ofport_modified(port, &pp);\n            }\n\n            update_mtu(ofproto, port);\n\n            /* Install the newly opened netdev in case it has changed.\n             * Don't close the old netdev yet in case port_modified has to\n             * remove a retained reference to it.*/\n            port->netdev = netdev;\n            port->change_seq = netdev_get_change_seq(netdev);\n\n            if (port->ofproto->ofproto_class->port_modified) {\n                port->ofproto->ofproto_class->port_modified(port);\n            }\n\n            netdev_close(old_netdev);\n        } else {\n            /* If 'port' is nonnull then its name differs from 'name' and thus\n             * we should delete it.  If we think there's a port named 'name'\n             * then its port number must be wrong now so delete it too. */\n            if (port) {\n                ofport_remove(port);\n            }\n            ofport_remove_with_name(ofproto, name);\n            error = ofport_install(ofproto, netdev, &pp);\n        }\n    } else {\n        /* Any port named 'name' is gone now. */\n        ofport_remove_with_name(ofproto, name);\n    }\n    ofproto_port_destroy(&ofproto_port);\n\n    return error;\n}\n\nstatic int\ninit_ports(struct ofproto *p)\n{\n    struct ofproto_port_dump dump;\n    struct ofproto_port ofproto_port;\n    struct shash_node *node, *next;\n\n    OFPROTO_PORT_FOR_EACH (&ofproto_port, &dump, p) {\n        const char *name = ofproto_port.name;\n\n        if (shash_find(&p->port_by_name, name)) {\n            VLOG_WARN_RL(&rl, \"%s: ignoring duplicate device %s in datapath\",\n                         p->name, name);\n        } else {\n            struct ofputil_phy_port pp;\n            struct netdev *netdev;\n\n            /* Check if an OpenFlow port number had been requested. */\n            node = shash_find(&init_ofp_ports, name);\n            if (node) {\n                const struct iface_hint *iface_hint = node->data;\n                simap_put(&p->ofp_requests, name,\n                          ofp_to_u16(iface_hint->ofp_port));\n            }\n\n            netdev = ofport_open(p, &ofproto_port, &pp);\n            if (netdev) {\n                ofport_install(p, netdev, &pp);\n                if (ofp_to_u16(ofproto_port.ofp_port) < p->max_ports) {\n                    p->alloc_port_no = MAX(p->alloc_port_no,\n                                           ofp_to_u16(ofproto_port.ofp_port));\n                }\n            }\n        }\n    }\n\n    SHASH_FOR_EACH_SAFE(node, next, &init_ofp_ports) {\n        struct iface_hint *iface_hint = node->data;\n\n        if (!strcmp(iface_hint->br_name, p->name)) {\n            free(iface_hint->br_name);\n            free(iface_hint->br_type);\n            free(iface_hint);\n            shash_delete(&init_ofp_ports, node);\n        }\n    }\n\n    return 0;\n}\n\nstatic bool\nofport_is_internal_or_patch(const struct ofproto *p, const struct ofport *port)\n{\n    return !strcmp(netdev_get_type(port->netdev),\n                   ofproto_port_open_type(p->type, \"internal\")) ||\n           !strcmp(netdev_get_type(port->netdev),\n                   ofproto_port_open_type(p->type, \"patch\"));\n}\n\n/* If 'port' is internal or patch and if the user didn't explicitly specify an\n * mtu through the database, we have to override it. */\nstatic bool\nofport_is_mtu_overridden(const struct ofproto *p, const struct ofport *port)\n{\n    return ofport_is_internal_or_patch(p, port)\n           && !netdev_mtu_is_user_config(port->netdev);\n}\n\n/* Find the minimum MTU of all non-overridden devices attached to 'p'.\n * Returns ETH_PAYLOAD_MAX or the minimum of the ports. */\nstatic int\nfind_min_mtu(struct ofproto *p)\n{\n    struct ofport *ofport;\n    int mtu = 0;\n\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        struct netdev *netdev = ofport->netdev;\n        int dev_mtu;\n\n        /* Skip any overridden port, since that's what we're trying to set. */\n        if (ofport_is_mtu_overridden(p, ofport)) {\n            continue;\n        }\n\n        if (netdev_get_mtu(netdev, &dev_mtu)) {\n            continue;\n        }\n        if (!mtu || dev_mtu < mtu) {\n            mtu = dev_mtu;\n        }\n    }\n\n    return mtu ? mtu: ETH_PAYLOAD_MAX;\n}\n\n/* Update MTU of all overridden devices on 'p' to the minimum of the\n * non-overridden ports in event of 'port' added or changed. */\nstatic void\nupdate_mtu(struct ofproto *p, struct ofport *port)\n{\n    struct netdev *netdev = port->netdev;\n    int dev_mtu;\n\n    if (netdev_get_mtu(netdev, &dev_mtu)) {\n        port->mtu = 0;\n        return;\n    }\n    if (ofport_is_mtu_overridden(p, port)) {\n        if (dev_mtu > p->min_mtu) {\n            if (!netdev_set_mtu(port->netdev, p->min_mtu)) {\n                dev_mtu = p->min_mtu;\n            }\n        }\n        port->mtu = dev_mtu;\n        return;\n    }\n\n    port->mtu = dev_mtu;\n    /* For non-overridden port find new min mtu. */\n\n    update_mtu_ofproto(p);\n}\n\nstatic void\nupdate_mtu_ofproto(struct ofproto *p)\n{\n    struct ofport *ofport;\n    int old_min = p->min_mtu;\n\n    p->min_mtu = find_min_mtu(p);\n    if (p->min_mtu == old_min) {\n        return;\n    }\n\n    HMAP_FOR_EACH (ofport, hmap_node, &p->ports) {\n        struct netdev *netdev = ofport->netdev;\n\n        if (ofport_is_mtu_overridden(p, ofport)) {\n            if (!netdev_set_mtu(netdev, p->min_mtu)) {\n                ofport->mtu = p->min_mtu;\n            }\n        }\n    }\n}\n\f\nstatic void\nofproto_rule_destroy__(struct rule *rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    cls_rule_destroy(CONST_CAST(struct cls_rule *, &rule->cr));\n    rule_actions_destroy(rule_get_actions(rule));\n    ovs_mutex_destroy(&rule->mutex);\n    rule->ofproto->ofproto_class->rule_dealloc(rule);\n}\n\nstatic void\nrule_destroy_cb(struct rule *rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    /* Send rule removed if needed. */\n    if (rule->flags & OFPUTIL_FF_SEND_FLOW_REM\n        && rule->removed_reason != OVS_OFPRR_NONE\n        && !rule_is_hidden(rule)) {\n        ofproto_rule_send_removed(rule);\n    }\n    rule->ofproto->ofproto_class->rule_destruct(rule);\n    mf_vl_mff_unref(&rule->ofproto->vl_mff_map, rule->match_tlv_bitmap);\n    mf_vl_mff_unref(&rule->ofproto->vl_mff_map, rule->ofpacts_tlv_bitmap);\n    ofproto_rule_destroy__(rule);\n}\n\nvoid\nofproto_rule_ref(struct rule *rule)\n{\n    if (rule) {\n        ovs_refcount_ref(&rule->ref_count);\n    }\n}\n\nbool\nofproto_rule_try_ref(struct rule *rule)\n{\n    if (rule) {\n        return ovs_refcount_try_ref_rcu(&rule->ref_count);\n    }\n    return false;\n}\n\n/* Decrements 'rule''s ref_count and schedules 'rule' to be destroyed if the\n * ref_count reaches 0.\n *\n * Use of RCU allows short term use (between RCU quiescent periods) without\n * keeping a reference.  A reference must be taken if the rule needs to\n * stay around accross the RCU quiescent periods. */\nvoid\nofproto_rule_unref(struct rule *rule)\n{\n    if (rule && ovs_refcount_unref_relaxed(&rule->ref_count) == 1) {\n        ovs_assert(rule->state != RULE_INSERTED);\n        ovsrcu_postpone(rule_destroy_cb, rule);\n    }\n}\n\nstatic void\nremove_rule_rcu__(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = rule->ofproto;\n    struct oftable *table = &ofproto->tables[rule->table_id];\n\n    ovs_assert(!cls_rule_visible_in_version(&rule->cr, OVS_VERSION_MAX));\n    if (!classifier_remove(&table->cls, &rule->cr)) {\n        OVS_NOT_REACHED();\n    }\n    if (ofproto->ofproto_class->rule_delete) {\n        ofproto->ofproto_class->rule_delete(rule);\n    }\n    ofproto_rule_unref(rule);\n}\n\nstatic void\nremove_rule_rcu(struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    remove_rule_rcu__(rule);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Removes and deletes rules from a NULL-terminated array of rule pointers. */\nstatic void\nremove_rules_rcu(struct rule **rules)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct rule **orig_rules = rules;\n\n    if (*rules) {\n        struct ofproto *ofproto = rules[0]->ofproto;\n        unsigned long tables[BITMAP_N_LONGS(256)];\n        struct rule *rule;\n        size_t table_id;\n\n        memset(tables, 0, sizeof tables);\n\n        ovs_mutex_lock(&ofproto_mutex);\n        while ((rule = *rules++)) {\n            /* Defer once for each new table.  This defers the subtable cleanup\n             * until later, so that when removing large number of flows the\n             * operation is faster. */\n            if (!bitmap_is_set(tables, rule->table_id)) {\n                struct classifier *cls = &ofproto->tables[rule->table_id].cls;\n\n                bitmap_set1(tables, rule->table_id);\n                classifier_defer(cls);\n            }\n            remove_rule_rcu__(rule);\n        }\n\n        BITMAP_FOR_EACH_1(table_id, 256, tables) {\n            struct classifier *cls = &ofproto->tables[table_id].cls;\n\n            classifier_publish(cls);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    free(orig_rules);\n}\n\nvoid\nofproto_group_ref(struct ofgroup *group)\n{\n    if (group) {\n        ovs_refcount_ref(&group->ref_count);\n    }\n}\n\nbool\nofproto_group_try_ref(struct ofgroup *group)\n{\n    if (group) {\n        return ovs_refcount_try_ref_rcu(&group->ref_count);\n    }\n    return false;\n}\n\nstatic void\ngroup_destroy_cb(struct ofgroup *group)\n{\n    group->ofproto->ofproto_class->group_destruct(group);\n    ofputil_group_properties_destroy(CONST_CAST(struct ofputil_group_props *,\n                                                &group->props));\n    ofputil_bucket_list_destroy(CONST_CAST(struct ovs_list *,\n                                           &group->buckets));\n    group->ofproto->ofproto_class->group_dealloc(group);\n}\n\nvoid\nofproto_group_unref(struct ofgroup *group)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    if (group && ovs_refcount_unref_relaxed(&group->ref_count) == 1) {\n        ovs_assert(rule_collection_n(&group->rules) == 0);\n        ovsrcu_postpone(group_destroy_cb, group);\n    }\n}\n\nstatic void\nremove_group_rcu__(struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = group->ofproto;\n\n    ovs_assert(!versions_visible_in_version(&group->versions, OVS_VERSION_MAX));\n\n    cmap_remove(&ofproto->groups, &group->cmap_node,\n                hash_int(group->group_id, 0));\n    ofproto_group_unref(group);\n}\n\nstatic void\nremove_group_rcu(struct ofgroup *group)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    remove_group_rcu__(group);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Removes and deletes groups from a NULL-terminated array of group\n * pointers. */\nstatic void\nremove_groups_rcu(struct ofgroup **groups)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    ovs_mutex_lock(&ofproto_mutex);\n    for (struct ofgroup **g = groups; *g; g++) {\n        remove_group_rcu__(*g);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n    free(groups);\n}\n\nstatic uint32_t get_provider_meter_id(const struct ofproto *,\n                                      uint32_t of_meter_id);\n\n/* Creates and returns a new 'struct rule_actions', whose actions are a copy\n * of from the 'ofpacts_len' bytes of 'ofpacts'. */\nconst struct rule_actions *\nrule_actions_create(const struct ofpact *ofpacts, size_t ofpacts_len)\n{\n    struct rule_actions *actions;\n\n    actions = xmalloc(sizeof *actions + ofpacts_len);\n    actions->ofpacts_len = ofpacts_len;\n    memcpy(actions->ofpacts, ofpacts, ofpacts_len);\n    actions->has_meter = ofpacts_get_meter(ofpacts, ofpacts_len) != 0;\n    actions->has_groups =\n        (ofpact_find_type_flattened(ofpacts, OFPACT_GROUP,\n                                    ofpact_end(ofpacts, ofpacts_len))\n         != NULL);\n    actions->has_learn_with_delete = (next_learn_with_delete(actions, NULL)\n                                      != NULL);\n\n    return actions;\n}\n\n/* Free the actions after the RCU quiescent period is reached. */\nvoid\nrule_actions_destroy(const struct rule_actions *actions)\n{\n    if (actions) {\n        ovsrcu_postpone(free, CONST_CAST(struct rule_actions *, actions));\n    }\n}\n\n/* Returns true if 'rule' has an OpenFlow OFPAT_OUTPUT or OFPAT_ENQUEUE action\n * that outputs to 'port' (output to OFPP_FLOOD and OFPP_ALL doesn't count). */\nbool\nofproto_rule_has_out_port(const struct rule *rule, ofp_port_t port)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (port == OFPP_ANY) {\n        return true;\n    } else {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        return ofpacts_output_to_port(actions->ofpacts,\n                                      actions->ofpacts_len, port);\n    }\n}\n\n/* Returns true if 'rule' has group and equals group_id. */\nstatic bool\nofproto_rule_has_out_group(const struct rule *rule, uint32_t group_id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (group_id == OFPG_ANY) {\n        return true;\n    } else {\n        const struct rule_actions *actions = rule_get_actions(rule);\n        return ofpacts_output_to_group(actions->ofpacts,\n                                       actions->ofpacts_len, group_id);\n    }\n}\n\nstatic bool\nrule_is_readonly(const struct rule *rule)\n{\n    const struct oftable *table = &rule->ofproto->tables[rule->table_id];\n    return (table->flags & OFTABLE_READONLY) != 0;\n}\n\f\nstatic uint32_t\nhash_learned_cookie(ovs_be64 cookie_, uint8_t table_id)\n{\n    uint64_t cookie = (OVS_FORCE uint64_t) cookie_;\n    return hash_3words(cookie, cookie >> 32, table_id);\n}\n\nstatic void\nlearned_cookies_update_one__(struct ofproto *ofproto,\n                             const struct ofpact_learn *learn,\n                             int delta, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    uint32_t hash = hash_learned_cookie(learn->cookie, learn->table_id);\n    struct learned_cookie *c;\n\n    HMAP_FOR_EACH_WITH_HASH (c, u.hmap_node, hash, &ofproto->learned_cookies) {\n        if (c->cookie == learn->cookie && c->table_id == learn->table_id) {\n            c->n += delta;\n            ovs_assert(c->n >= 0);\n\n            if (!c->n) {\n                hmap_remove(&ofproto->learned_cookies, &c->u.hmap_node);\n                ovs_list_push_back(dead_cookies, &c->u.list_node);\n            }\n\n            return;\n        }\n    }\n\n    ovs_assert(delta > 0);\n    c = xmalloc(sizeof *c);\n    hmap_insert(&ofproto->learned_cookies, &c->u.hmap_node, hash);\n    c->cookie = learn->cookie;\n    c->table_id = learn->table_id;\n    c->n = delta;\n}\n\nstatic const struct ofpact_learn *\nnext_learn_with_delete(const struct rule_actions *actions,\n                       const struct ofpact_learn *start)\n{\n    const struct ofpact *pos;\n\n    for (pos = start ? ofpact_next(&start->ofpact) : actions->ofpacts;\n         pos < ofpact_end(actions->ofpacts, actions->ofpacts_len);\n         pos = ofpact_next(pos)) {\n        if (pos->type == OFPACT_LEARN) {\n            const struct ofpact_learn *learn = ofpact_get_LEARN(pos);\n            if (learn->flags & NX_LEARN_F_DELETE_LEARNED) {\n                return learn;\n            }\n        }\n    }\n\n    return NULL;\n}\n\nstatic void\nlearned_cookies_update__(struct ofproto *ofproto,\n                         const struct rule_actions *actions,\n                         int delta, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (actions->has_learn_with_delete) {\n        const struct ofpact_learn *learn;\n\n        for (learn = next_learn_with_delete(actions, NULL); learn;\n             learn = next_learn_with_delete(actions, learn)) {\n            learned_cookies_update_one__(ofproto, learn, delta, dead_cookies);\n        }\n    }\n}\n\nstatic void\nlearned_cookies_inc(struct ofproto *ofproto,\n                    const struct rule_actions *actions)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    learned_cookies_update__(ofproto, actions, +1, NULL);\n}\n\nstatic void\nlearned_cookies_dec(struct ofproto *ofproto,\n                    const struct rule_actions *actions,\n                    struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    learned_cookies_update__(ofproto, actions, -1, dead_cookies);\n}\n\nstatic void\nlearned_cookies_flush(struct ofproto *ofproto, struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct learned_cookie *c;\n\n    LIST_FOR_EACH_POP (c, u.list_node, dead_cookies) {\n        struct rule_criteria criteria;\n        struct rule_collection rules;\n        struct match match;\n\n        match_init_catchall(&match);\n        rule_criteria_init(&criteria, c->table_id, &match, 0, OVS_VERSION_MAX,\n                           c->cookie, OVS_BE64_MAX, OFPP_ANY, OFPG_ANY);\n        rule_criteria_require_rw(&criteria, false);\n        collect_rules_loose(ofproto, &criteria, &rules);\n        rule_criteria_destroy(&criteria);\n        delete_flows__(&rules, OFPRR_DELETE, NULL);\n\n        free(c);\n    }\n}\n\f\nstatic enum ofperr\nhandle_echo_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    ofconn_send_reply(ofconn, make_echo_reply(oh));\n    return 0;\n}\n\nstatic void\nquery_tables(struct ofproto *ofproto,\n             struct ofputil_table_features **featuresp,\n             struct ofputil_table_stats **statsp)\n{\n    struct mf_bitmap rw_fields = oxm_writable_fields();\n    struct mf_bitmap match = oxm_matchable_fields();\n    struct mf_bitmap mask = oxm_maskable_fields();\n\n    struct ofputil_table_features *features;\n    struct ofputil_table_stats *stats;\n    int i;\n\n    features = *featuresp = xcalloc(ofproto->n_tables, sizeof *features);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        struct ofputil_table_features *f = &features[i];\n\n        f->table_id = i;\n        sprintf(f->name, \"table%d\", i);\n        f->metadata_match = OVS_BE64_MAX;\n        f->metadata_write = OVS_BE64_MAX;\n        atomic_read_relaxed(&ofproto->tables[i].miss_config, &f->miss_config);\n        f->max_entries = 1000000;\n\n        bool more_tables = false;\n        for (int j = i + 1; j < ofproto->n_tables; j++) {\n            if (!(ofproto->tables[j].flags & OFTABLE_HIDDEN)) {\n                bitmap_set1(f->nonmiss.next, j);\n                more_tables = true;\n            }\n        }\n        f->nonmiss.instructions = (1u << N_OVS_INSTRUCTIONS) - 1;\n        if (!more_tables) {\n            f->nonmiss.instructions &= ~(1u << OVSINST_OFPIT11_GOTO_TABLE);\n        }\n        f->nonmiss.write.ofpacts = (UINT64_C(1) << N_OFPACTS) - 1;\n        f->nonmiss.write.set_fields = rw_fields;\n        f->nonmiss.apply = f->nonmiss.write;\n        f->miss = f->nonmiss;\n\n        f->match = match;\n        f->mask = mask;\n        f->wildcard = match;\n    }\n\n    if (statsp) {\n        stats = *statsp = xcalloc(ofproto->n_tables, sizeof *stats);\n        for (i = 0; i < ofproto->n_tables; i++) {\n            struct ofputil_table_stats *s = &stats[i];\n\n            s->table_id = i;\n            s->active_count = ofproto->tables[i].n_flows;\n            if (i == 0) {\n                s->active_count -= connmgr_count_hidden_rules(\n                    ofproto->connmgr);\n            }\n        }\n    } else {\n        stats = NULL;\n    }\n\n    ofproto->ofproto_class->query_tables(ofproto, features, stats);\n\n    for (i = 0; i < ofproto->n_tables; i++) {\n        const struct oftable *table = &ofproto->tables[i];\n        struct ofputil_table_features *f = &features[i];\n\n        if (table->name) {\n            ovs_strzcpy(f->name, table->name, sizeof f->name);\n        }\n\n        if (table->max_flows < f->max_entries) {\n            f->max_entries = table->max_flows;\n        }\n    }\n}\n\nstatic void\nquery_switch_features(struct ofproto *ofproto,\n                      bool *arp_match_ip, uint64_t *ofpacts)\n{\n    struct ofputil_table_features *features, *f;\n\n    *arp_match_ip = false;\n    *ofpacts = 0;\n\n    query_tables(ofproto, &features, NULL);\n    for (f = features; f < &features[ofproto->n_tables]; f++) {\n        *ofpacts |= f->nonmiss.apply.ofpacts | f->miss.apply.ofpacts;\n        if (bitmap_is_set(f->match.bm, MFF_ARP_SPA) ||\n            bitmap_is_set(f->match.bm, MFF_ARP_TPA)) {\n            *arp_match_ip = true;\n        }\n    }\n    free(features);\n\n    /* Sanity check. */\n    ovs_assert(*ofpacts & (UINT64_C(1) << OFPACT_OUTPUT));\n}\n\nstatic enum ofperr\nhandle_features_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_switch_features features;\n    struct ofport *port;\n    bool arp_match_ip;\n    struct ofpbuf *b;\n\n    query_switch_features(ofproto, &arp_match_ip, &features.ofpacts);\n\n    features.datapath_id = ofproto->datapath_id;\n    features.n_buffers = 0;\n    features.n_tables = ofproto_get_n_visible_tables(ofproto);\n    features.capabilities = (OFPUTIL_C_FLOW_STATS | OFPUTIL_C_TABLE_STATS |\n                             OFPUTIL_C_PORT_STATS | OFPUTIL_C_QUEUE_STATS |\n                             OFPUTIL_C_GROUP_STATS | OFPUTIL_C_BUNDLES);\n    if (arp_match_ip) {\n        features.capabilities |= OFPUTIL_C_ARP_MATCH_IP;\n    }\n    /* FIXME: Fill in proper features.auxiliary_id for auxiliary connections */\n    features.auxiliary_id = 0;\n    b = ofputil_encode_switch_features(&features, ofconn_get_protocol(ofconn),\n                                       oh->xid);\n    HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n        ofputil_put_switch_features_port(&port->pp, b);\n    }\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_get_config_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_switch_config config;\n    config.frag = ofconn_get_ofproto(ofconn)->frag_handling;\n    config.invalid_ttl_to_controller\n        = ofconn_get_invalid_ttl_to_controller(ofconn);\n    config.miss_send_len = ofconn_get_miss_send_len(ofconn);\n\n    ofconn_send_reply(ofconn, ofputil_encode_get_config_reply(oh, &config));\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_set_config(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_switch_config config;\n    enum ofperr error;\n\n    error = ofputil_decode_set_config(oh, &config);\n    if (error) {\n        return error;\n    }\n\n    if (ofconn_get_type(ofconn) != OFCONN_PRIMARY\n        || ofconn_get_role(ofconn) != OFPCR12_ROLE_SLAVE) {\n        enum ofputil_frag_handling cur = ofproto->frag_handling;\n        enum ofputil_frag_handling next = config.frag;\n\n        if (cur != next) {\n            if (ofproto->ofproto_class->set_frag_handling(ofproto, next)) {\n                ofproto->frag_handling = next;\n            } else {\n                VLOG_WARN_RL(&rl, \"%s: unsupported fragment handling mode %s\",\n                             ofproto->name,\n                             ofputil_frag_handling_to_string(next));\n            }\n        }\n    }\n\n    if (config.invalid_ttl_to_controller >= 0) {\n        ofconn_set_invalid_ttl_to_controller(ofconn,\n                                             config.invalid_ttl_to_controller);\n    }\n\n    ofconn_set_miss_send_len(ofconn, config.miss_send_len);\n\n    return 0;\n}\n\n/* Checks whether 'ofconn' is a slave controller.  If so, returns an OpenFlow\n * error message code for the caller to propagate upward.  Otherwise, returns\n * 0.\n *\n * The log message mentions 'msg_type'. */\nstatic enum ofperr\nreject_slave_controller(struct ofconn *ofconn)\n{\n    if (ofconn_get_type(ofconn) == OFCONN_PRIMARY\n        && ofconn_get_role(ofconn) == OFPCR12_ROLE_SLAVE) {\n        return OFPERR_OFPBRC_IS_SLAVE;\n    } else {\n        return 0;\n    }\n}\n\n/* Checks that the 'ofpacts_len' bytes of action in 'ofpacts' are appropriate\n * for 'ofproto':\n *\n *    - If they use a meter, then 'ofproto' has that meter configured.\n *\n *    - If they use any groups, then 'ofproto' has that group configured.\n *\n * Returns 0 if successful, otherwise an OpenFlow error.  Caller must hold\n * 'ofproto_mutex' for the result to be valid also after this function\n * returns. */\nenum ofperr\nofproto_check_ofpacts(struct ofproto *ofproto,\n                      const struct ofpact ofpacts[], size_t ofpacts_len)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    uint32_t mid;\n\n    mid = ofpacts_get_meter(ofpacts, ofpacts_len);\n    if (mid && get_provider_meter_id(ofproto, mid) == UINT32_MAX) {\n        return OFPERR_OFPMMFC_INVALID_METER;\n    }\n\n    const struct ofpact_group *a;\n    OFPACT_FOR_EACH_TYPE_FLATTENED (a, GROUP, ofpacts, ofpacts_len) {\n        if (!ofproto_group_exists(ofproto, a->group_id)) {\n            return OFPERR_OFPBAC_BAD_OUT_GROUP;\n        }\n    }\n\n    return 0;\n}\n\nvoid\nofproto_packet_out_uninit(struct ofproto_packet_out *opo)\n{\n    dp_packet_delete(opo->packet);\n    opo->packet = NULL;\n    free(opo->flow);\n    opo->flow = NULL;\n    free(opo->ofpacts);\n    opo->ofpacts = NULL;\n    opo->ofpacts_len = 0;\n    ovs_assert(!opo->aux);\n}\n\n/* Takes ownership of po->ofpacts, which must have been malloc'ed. */\nstatic enum ofperr\nofproto_packet_out_init(struct ofproto *ofproto,\n                        struct ofconn *ofconn,\n                        struct ofproto_packet_out *opo,\n                        const struct ofputil_packet_out *po)\n{\n    enum ofperr error;\n\n    if (ofp_to_u16(po->in_port) >= ofproto->max_ports\n        && ofp_to_u16(po->in_port) < ofp_to_u16(OFPP_MAX)) {\n        return OFPERR_OFPBRC_BAD_PORT;\n    }\n\n    /* Get payload. */\n    if (po->buffer_id != UINT32_MAX) {\n        return OFPERR_OFPBRC_BUFFER_UNKNOWN;\n    }\n\n    /* Ensure that the L3 header is 32-bit aligned. */\n    opo->packet = dp_packet_clone_data_with_headroom(po->packet,\n                                                     po->packet_len, 2);\n    /* Store struct flow. */\n    opo->flow = xmalloc(sizeof *opo->flow);\n    flow_extract(opo->packet, opo->flow);\n    opo->flow->in_port.ofp_port = po->in_port;\n\n    /* Check actions like for flow mods.  We pass a 'table_id' of 0 to\n     * ofproto_check_consistency(), which isn't strictly correct because these\n     * actions aren't in any table.  This is OK as 'table_id' is only used to\n     * check instructions (e.g., goto-table), which can't appear on the action\n     * list of a packet-out. */\n    error = ofpacts_check_consistency(po->ofpacts, po->ofpacts_len,\n                                      opo->flow,\n                                      u16_to_ofp(ofproto->max_ports), 0,\n                                      ofproto->n_tables,\n                                      ofconn_get_protocol(ofconn));\n    if (error) {\n        dp_packet_delete(opo->packet);\n        free(opo->flow);\n        return error;\n    }\n\n    opo->ofpacts = po->ofpacts;\n    opo->ofpacts_len = po->ofpacts_len;\n\n    opo->aux = NULL;\n    return 0;\n}\n\nstatic enum ofperr\nofproto_packet_out_start(struct ofproto *ofproto,\n                         struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    error = ofproto_check_ofpacts(ofproto, opo->ofpacts, opo->ofpacts_len);\n    if (error) {\n        return error;\n    }\n\n    return ofproto->ofproto_class->packet_xlate(ofproto, opo);\n}\n\nstatic void\nofproto_packet_out_revert(struct ofproto *ofproto,\n                          struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto->ofproto_class->packet_xlate_revert(ofproto, opo);\n}\n\nstatic void\nofproto_packet_out_finish(struct ofproto *ofproto,\n                          struct ofproto_packet_out *opo)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto->ofproto_class->packet_execute(ofproto, opo);\n}\n\nstatic enum ofperr\nhandle_packet_out(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofputil_packet_out po;\n    struct ofproto_packet_out opo;\n    uint64_t ofpacts_stub[1024 / 8];\n    struct ofpbuf ofpacts;\n    enum ofperr error;\n\n    COVERAGE_INC(ofproto_packet_out);\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    /* Decode message. */\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n    error = ofputil_decode_packet_out(&po, oh, &ofpacts);\n    if (error) {\n        ofpbuf_uninit(&ofpacts);\n        return error;\n    }\n\n    po.ofpacts = ofpbuf_steal_data(&ofpacts);   /* Move to heap. */\n    error = ofproto_packet_out_init(p, ofconn, &opo, &po);\n    if (error) {\n        free(po.ofpacts);\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    opo.version = p->tables_version;\n    error = ofproto_packet_out_start(p, &opo);\n    if (!error) {\n        ofproto_packet_out_finish(p, &opo);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ofproto_packet_out_uninit(&opo);\n    return error;\n}\n\nstatic enum ofperr\nhandle_nxt_resume(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_packet_in_private pin;\n    enum ofperr error;\n\n    error = ofputil_decode_packet_in_private(oh, false,\n                                             ofproto_get_tun_tab(ofproto),\n                                             &ofproto->vl_mff_map, &pin, NULL,\n                                             NULL);\n    if (error) {\n        return error;\n    }\n\n    error = (ofproto->ofproto_class->nxt_resume\n             ? ofproto->ofproto_class->nxt_resume(ofproto, &pin)\n             : OFPERR_NXR_NOT_SUPPORTED);\n\n    ofputil_packet_in_private_destroy(&pin);\n\n    return error;\n}\n\nstatic void\nupdate_port_config(struct ofconn *ofconn, struct ofport *port,\n                   enum ofputil_port_config config,\n                   enum ofputil_port_config mask)\n{\n    enum ofputil_port_config toggle = (config ^ port->pp.config) & mask;\n\n    if (toggle & OFPUTIL_PC_PORT_DOWN\n        && (config & OFPUTIL_PC_PORT_DOWN\n            ? netdev_turn_flags_off(port->netdev, NETDEV_UP, NULL)\n            : netdev_turn_flags_on(port->netdev, NETDEV_UP, NULL))) {\n        /* We tried to bring the port up or down, but it failed, so don't\n         * update the \"down\" bit. */\n        toggle &= ~OFPUTIL_PC_PORT_DOWN;\n    }\n\n    if (toggle) {\n        enum ofputil_port_config old_config = port->pp.config;\n        port->pp.config ^= toggle;\n        port->ofproto->ofproto_class->port_reconfigured(port, old_config);\n        connmgr_send_port_status(port->ofproto->connmgr, ofconn, &port->pp,\n                                 OFPPR_MODIFY);\n    }\n}\n\nstatic enum ofperr\nport_mod_start(struct ofconn *ofconn, struct ofputil_port_mod *pm,\n               struct ofport **port)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n\n    *port = ofproto_get_port(p, pm->port_no);\n    if (!*port) {\n        return OFPERR_OFPPMFC_BAD_PORT;\n    }\n    if (!eth_addr_equals((*port)->pp.hw_addr, pm->hw_addr)) {\n        return OFPERR_OFPPMFC_BAD_HW_ADDR;\n    }\n    return 0;\n}\n\nstatic void\nport_mod_finish(struct ofconn *ofconn, struct ofputil_port_mod *pm,\n                struct ofport *port)\n{\n    update_port_config(ofconn, port, pm->config, pm->mask);\n    if (pm->advertise) {\n        netdev_set_advertisements(port->netdev, pm->advertise);\n    }\n}\n\nstatic enum ofperr\nhandle_port_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_port_mod pm;\n    struct ofport *port;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_port_mod(oh, &pm, false);\n    if (error) {\n        return error;\n    }\n\n    error = port_mod_start(ofconn, &pm, &port);\n    if (!error) {\n        port_mod_finish(ofconn, &pm, port);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_desc_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    static const char *default_mfr_desc = \"Nicira, Inc.\";\n    static const char *default_hw_desc = \"Open vSwitch\";\n    static const char *default_sw_desc = VERSION;\n    static const char *default_serial_desc = \"None\";\n    static const char *default_dp_desc = \"None\";\n\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofp_desc_stats *ods;\n    struct ofpbuf *msg;\n\n    msg = ofpraw_alloc_stats_reply(request, 0);\n    ods = ofpbuf_put_zeros(msg, sizeof *ods);\n    ovs_strlcpy(ods->mfr_desc, p->mfr_desc ? p->mfr_desc : default_mfr_desc,\n                sizeof ods->mfr_desc);\n    ovs_strlcpy(ods->hw_desc, p->hw_desc ? p->hw_desc : default_hw_desc,\n                sizeof ods->hw_desc);\n    ovs_strlcpy(ods->sw_desc, p->sw_desc ? p->sw_desc : default_sw_desc,\n                sizeof ods->sw_desc);\n    ovs_strlcpy(ods->serial_num,\n                p->serial_desc ? p->serial_desc : default_serial_desc,\n                sizeof ods->serial_num);\n    ovs_strlcpy(ods->dp_desc, p->dp_desc ? p->dp_desc : default_dp_desc,\n                sizeof ods->dp_desc);\n    ofconn_send_reply(ofconn, msg);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_features *features;\n    struct ofputil_table_stats *stats;\n    struct ofpbuf *reply;\n    size_t i;\n\n    query_tables(ofproto, &features, &stats);\n\n    reply = ofputil_encode_table_stats_reply(request);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_stats_reply(reply, &stats[i], &features[i]);\n        }\n    }\n    ofconn_send_reply(ofconn, reply);\n\n    free(features);\n    free(stats);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_features_request(struct ofconn *ofconn,\n                              const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofpbuf msg = ofpbuf_const_initializer(request,\n                                                 ntohs(request->length));\n    ofpraw_pull_assert(&msg);\n    if (msg.size || ofpmp_more(request)) {\n        return OFPERR_OFPTFFC_EPERM;\n    }\n\n    struct ofputil_table_features *features;\n    query_tables(ofproto, &features, NULL);\n\n    struct ovs_list replies;\n    ofpmp_init(&replies, request);\n    for (size_t i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_features_reply(&features[i], &replies);\n        }\n    }\n    ofconn_send_replies(ofconn, &replies);\n\n    free(features);\n\n    return 0;\n}\n\n/* Returns the vacancy of 'oftable', a number that ranges from 0 (if the table\n * is full) to 100 (if the table is empty).\n *\n * A table without a limit on flows is considered to be empty. */\nstatic uint8_t\noftable_vacancy(const struct oftable *t)\n{\n    return (!t->max_flows ? 100\n            : t->n_flows >= t->max_flows ? 0\n            : (t->max_flows - t->n_flows) * 100.0 / t->max_flows);\n}\n\nstatic void\nquery_table_desc__(struct ofputil_table_desc *td,\n                   struct ofproto *ofproto, uint8_t table_id)\n{\n    const struct oftable *t = &ofproto->tables[table_id];\n\n    td->table_id = table_id;\n    td->eviction = (t->eviction & EVICTION_OPENFLOW\n                    ? OFPUTIL_TABLE_EVICTION_ON\n                    : OFPUTIL_TABLE_EVICTION_OFF);\n    td->eviction_flags = OFPROTO_EVICTION_FLAGS;\n    td->vacancy = (t->vacancy_event\n                   ? OFPUTIL_TABLE_VACANCY_ON\n                   : OFPUTIL_TABLE_VACANCY_OFF);\n    td->table_vacancy.vacancy_down = t->vacancy_down;\n    td->table_vacancy.vacancy_up = t->vacancy_up;\n    td->table_vacancy.vacancy = oftable_vacancy(t);\n}\n\n/* This function queries the database for dumping table-desc. */\nstatic void\nquery_tables_desc(struct ofproto *ofproto, struct ofputil_table_desc **descp)\n{\n    struct ofputil_table_desc *table_desc;\n    size_t i;\n\n    table_desc = *descp = xcalloc(ofproto->n_tables, sizeof *table_desc);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        struct ofputil_table_desc *td = &table_desc[i];\n        query_table_desc__(td, ofproto, i);\n    }\n}\n\n/* Function to handle dump-table-desc request. */\nstatic enum ofperr\nhandle_table_desc_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_desc *table_desc;\n    struct ovs_list replies;\n    size_t i;\n\n    query_tables_desc(ofproto, &table_desc);\n    ofpmp_init(&replies, request);\n    for (i = 0; i < ofproto->n_tables; i++) {\n        if (!(ofproto->tables[i].flags & OFTABLE_HIDDEN)) {\n            ofputil_append_table_desc_reply(&table_desc[i], &replies,\n                                            request->version);\n        }\n    }\n    ofconn_send_replies(ofconn, &replies);\n    free(table_desc);\n    return 0;\n}\n\n/* This function determines and sends the vacancy event, based on the value\n * of current vacancy and threshold vacancy. If the current vacancy is less\n * than or equal to vacancy_down, vacancy up events must be enabled, and when\n * the current vacancy is greater or equal to vacancy_up, vacancy down events\n * must be enabled. */\nstatic void\nsend_table_status(struct ofproto *ofproto, uint8_t table_id)\n{\n    struct oftable *t = &ofproto->tables[table_id];\n    if (!t->vacancy_event) {\n        return;\n    }\n\n    uint8_t vacancy = oftable_vacancy(t);\n    enum ofp14_table_reason event;\n    if (vacancy < t->vacancy_down) {\n        event = OFPTR_VACANCY_DOWN;\n    } else if (vacancy > t->vacancy_up) {\n        event = OFPTR_VACANCY_UP;\n    } else {\n        return;\n    }\n\n    if (event == t->vacancy_event) {\n        struct ofputil_table_desc td;\n        query_table_desc__(&td, ofproto, table_id);\n        connmgr_send_table_status(ofproto->connmgr, &td, event);\n\n        t->vacancy_event = (event == OFPTR_VACANCY_DOWN\n                            ? OFPTR_VACANCY_UP\n                            : OFPTR_VACANCY_DOWN);\n    }\n}\n\nstatic void\nappend_port_stat(struct ofport *port, struct ovs_list *replies)\n{\n    struct ofputil_port_stats ops = { .port_no = port->pp.port_no };\n\n    calc_duration(port->created, time_msec(),\n                  &ops.duration_sec, &ops.duration_nsec);\n\n    /* Intentionally ignore return value, since errors will set\n     * 'stats' to all-1s, which is correct for OpenFlow, and\n     * netdev_get_stats() will log errors. */\n    ofproto_port_get_stats(port, &ops.stats);\n\n    ofputil_append_port_stat(replies, &ops);\n}\n\nstatic void\nhandle_port_request(struct ofconn *ofconn,\n                    const struct ofp_header *request, ofp_port_t port_no,\n                    void (*cb)(struct ofport *, struct ovs_list *replies))\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofport *port;\n    struct ovs_list replies;\n\n    ofpmp_init(&replies, request);\n    if (port_no != OFPP_ANY) {\n        port = ofproto_get_port(ofproto, port_no);\n        if (port) {\n            cb(port, &replies);\n        }\n    } else {\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            cb(port, &replies);\n        }\n    }\n\n    ofconn_send_replies(ofconn, &replies);\n}\n\nstatic enum ofperr\nhandle_port_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n{\n    ofp_port_t port_no;\n    enum ofperr error;\n\n    error = ofputil_decode_port_stats_request(request, &port_no);\n    if (!error) {\n        handle_port_request(ofconn, request, port_no, append_port_stat);\n    }\n    return error;\n}\n\nstatic void\nappend_port_desc(struct ofport *port, struct ovs_list *replies)\n{\n    ofputil_append_port_desc_stats_reply(&port->pp, replies);\n}\n\nstatic enum ofperr\nhandle_port_desc_stats_request(struct ofconn *ofconn,\n                               const struct ofp_header *request)\n{\n    ofp_port_t port_no;\n    enum ofperr error;\n\n    error = ofputil_decode_port_desc_stats_request(request, &port_no);\n    if (!error) {\n        handle_port_request(ofconn, request, port_no, append_port_desc);\n    }\n    return error;\n}\n\nstatic uint32_t\nhash_cookie(ovs_be64 cookie)\n{\n    return hash_uint64((OVS_FORCE uint64_t)cookie);\n}\n\nstatic void\ncookies_insert(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    hindex_insert(&ofproto->cookies, &rule->cookie_node,\n                  hash_cookie(rule->flow_cookie));\n}\n\nstatic void\ncookies_remove(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    hindex_remove(&ofproto->cookies, &rule->cookie_node);\n}\n\nstatic void\ncalc_duration(long long int start, long long int now,\n              uint32_t *sec, uint32_t *nsec)\n{\n    long long int msecs = now - start;\n    *sec = msecs / 1000;\n    *nsec = (msecs % 1000) * (1000 * 1000);\n}\n\n/* Checks whether 'table_id' is 0xff or a valid table ID in 'ofproto'.  Returns\n * true if 'table_id' is OK, false otherwise.  */\nstatic bool\ncheck_table_id(const struct ofproto *ofproto, uint8_t table_id)\n{\n    return table_id == OFPTT_ALL || table_id < ofproto->n_tables;\n}\n\nstatic struct oftable *\nnext_visible_table(const struct ofproto *ofproto, uint8_t table_id)\n{\n    struct oftable *table;\n\n    for (table = &ofproto->tables[table_id];\n         table < &ofproto->tables[ofproto->n_tables];\n         table++) {\n        if (!(table->flags & OFTABLE_HIDDEN)) {\n            return table;\n        }\n    }\n\n    return NULL;\n}\n\nstatic struct oftable *\nfirst_matching_table(const struct ofproto *ofproto, uint8_t table_id)\n{\n    if (table_id == 0xff) {\n        return next_visible_table(ofproto, 0);\n    } else if (table_id < ofproto->n_tables) {\n        return &ofproto->tables[table_id];\n    } else {\n        return NULL;\n    }\n}\n\nstatic struct oftable *\nnext_matching_table(const struct ofproto *ofproto,\n                    const struct oftable *table, uint8_t table_id)\n{\n    return (table_id == 0xff\n            ? next_visible_table(ofproto, (table - ofproto->tables) + 1)\n            : NULL);\n}\n\n/* Assigns TABLE to each oftable, in turn, that matches TABLE_ID in OFPROTO:\n *\n *   - If TABLE_ID is 0xff, this iterates over every classifier table in\n *     OFPROTO, skipping tables marked OFTABLE_HIDDEN.\n *\n *   - If TABLE_ID is the number of a table in OFPROTO, then the loop iterates\n *     only once, for that table.  (This can be used to access tables marked\n *     OFTABLE_HIDDEN.)\n *\n *   - Otherwise, TABLE_ID isn't valid for OFPROTO, so the loop won't be\n *     entered at all.  (Perhaps you should have validated TABLE_ID with\n *     check_table_id().)\n *\n * All parameters are evaluated multiple times.\n */\n#define FOR_EACH_MATCHING_TABLE(TABLE, TABLE_ID, OFPROTO)         \\\n    for ((TABLE) = first_matching_table(OFPROTO, TABLE_ID);       \\\n         (TABLE) != NULL;                                         \\\n         (TABLE) = next_matching_table(OFPROTO, TABLE, TABLE_ID))\n\n/* Initializes 'criteria' in a straightforward way based on the other\n * parameters.\n *\n * By default, the criteria include flows that are read-only, on the assumption\n * that the collected flows won't be modified.  Call rule_criteria_require_rw()\n * if flows will be modified.\n *\n * For \"loose\" matching, the 'priority' parameter is unimportant and may be\n * supplied as 0. */\nstatic void\nrule_criteria_init(struct rule_criteria *criteria, uint8_t table_id,\n                   const struct match *match, int priority,\n                   ovs_version_t version, ovs_be64 cookie,\n                   ovs_be64 cookie_mask, ofp_port_t out_port,\n                   uint32_t out_group)\n{\n    criteria->table_id = table_id;\n    cls_rule_init(&criteria->cr, match, priority);\n    criteria->version = version;\n    criteria->cookie = cookie;\n    criteria->cookie_mask = cookie_mask;\n    criteria->out_port = out_port;\n    criteria->out_group = out_group;\n\n    /* We ordinarily want to skip hidden rules, but there has to be a way for\n     * code internal to OVS to modify and delete them, so if the criteria\n     * specify a priority that can only be for a hidden flow, then allow hidden\n     * rules to be selected.  (This doesn't allow OpenFlow clients to meddle\n     * with hidden flows because OpenFlow uses only a 16-bit field to specify\n     * priority.) */\n    criteria->include_hidden = priority > UINT16_MAX;\n\n    /* We assume that the criteria are being used to collect flows for reading\n     * but not modification.  Thus, we should collect read-only flows. */\n    criteria->include_readonly = true;\n}\n\n/* By default, criteria initialized by rule_criteria_init() will match flows\n * that are read-only, on the assumption that the collected flows won't be\n * modified.  Call this function to match only flows that are be modifiable.\n *\n * Specify 'can_write_readonly' as false in ordinary circumstances, true if the\n * caller has special privileges that allow it to modify even \"read-only\"\n * flows. */\nstatic void\nrule_criteria_require_rw(struct rule_criteria *criteria,\n                         bool can_write_readonly)\n{\n    criteria->include_readonly = can_write_readonly;\n}\n\nstatic void\nrule_criteria_destroy(struct rule_criteria *criteria)\n{\n    cls_rule_destroy(&criteria->cr);\n    criteria->version = OVS_VERSION_NOT_REMOVED; /* Mark as destroyed. */\n}\n\n/* Schedules postponed removal of rules, destroys 'rules'. */\nstatic void\nremove_rules_postponed(struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules) > 0) {\n        if (rule_collection_n(rules) == 1) {\n            ovsrcu_postpone(remove_rule_rcu, rule_collection_rules(rules)[0]);\n            rule_collection_init(rules);\n        } else {\n            ovsrcu_postpone(remove_rules_rcu, rule_collection_detach(rules));\n        }\n    }\n}\n\n/* Schedules postponed removal of groups, destroys 'groups'. */\nstatic void\nremove_groups_postponed(struct group_collection *groups)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (group_collection_n(groups) > 0) {\n        if (group_collection_n(groups) == 1) {\n            ovsrcu_postpone(remove_group_rcu,\n                            group_collection_groups(groups)[0]);\n            group_collection_init(groups);\n        } else {\n            ovsrcu_postpone(remove_groups_rcu,\n                            group_collection_detach(groups));\n        }\n    }\n}\n\n/* Checks whether 'rule' matches 'c' and, if so, adds it to 'rules'.  This\n * function verifies most of the criteria in 'c' itself, but the caller must\n * check 'c->cr' itself.\n *\n * Rules that have already been marked for removal are not collected.\n *\n * Increments '*n_readonly' if 'rule' wasn't added because it's read-only (and\n * 'c' only includes modifiable rules). */\nstatic void\ncollect_rule(struct rule *rule, const struct rule_criteria *c,\n             struct rule_collection *rules, size_t *n_readonly)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if ((c->table_id == rule->table_id || c->table_id == 0xff)\n        && ofproto_rule_has_out_port(rule, c->out_port)\n        && ofproto_rule_has_out_group(rule, c->out_group)\n        && !((rule->flow_cookie ^ c->cookie) & c->cookie_mask)\n        && (!rule_is_hidden(rule) || c->include_hidden)\n        && cls_rule_visible_in_version(&rule->cr, c->version)) {\n        /* Rule matches all the criteria... */\n        if (!rule_is_readonly(rule) || c->include_readonly) {\n            /* ...add it. */\n            rule_collection_add(rules, rule);\n        } else {\n            /* ...except it's read-only. */\n            ++*n_readonly;\n        }\n    }\n}\n\n/* Searches 'ofproto' for rules that match the criteria in 'criteria'.  Matches\n * on classifiers rules are done in the \"loose\" way required for OpenFlow\n * OFPFC_MODIFY and OFPFC_DELETE requests.  Puts the selected rules on list\n * 'rules'.\n *\n * Returns 0 on success, otherwise an OpenFlow error code. */\nstatic enum ofperr\ncollect_rules_loose(struct ofproto *ofproto,\n                    const struct rule_criteria *criteria,\n                    struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table;\n    enum ofperr error = 0;\n    size_t n_readonly = 0;\n\n    rule_collection_init(rules);\n\n    if (!check_table_id(ofproto, criteria->table_id)) {\n        error = OFPERR_OFPBRC_BAD_TABLE_ID;\n        goto exit;\n    }\n\n    if (criteria->cookie_mask == OVS_BE64_MAX) {\n        struct rule *rule;\n\n        HINDEX_FOR_EACH_WITH_HASH (rule, cookie_node,\n                                   hash_cookie(criteria->cookie),\n                                   &ofproto->cookies) {\n            if (cls_rule_is_loose_match(&rule->cr, &criteria->cr.match)) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    } else {\n        FOR_EACH_MATCHING_TABLE (table, criteria->table_id, ofproto) {\n            struct rule *rule;\n\n            CLS_FOR_EACH_TARGET (rule, cr, &table->cls, &criteria->cr,\n                                 criteria->version) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    }\n\nexit:\n    if (!error && !rule_collection_n(rules) && n_readonly) {\n        /* We didn't find any rules to modify.  We did find some read-only\n         * rules that we're not allowed to modify, so report that. */\n        error = OFPERR_OFPBRC_EPERM;\n    }\n    if (error) {\n        rule_collection_destroy(rules);\n    }\n    return error;\n}\n\n/* Searches 'ofproto' for rules that match the criteria in 'criteria'.  Matches\n * on classifiers rules are done in the \"strict\" way required for OpenFlow\n * OFPFC_MODIFY_STRICT and OFPFC_DELETE_STRICT requests.  Puts the selected\n * rules on list 'rules'.\n *\n * Returns 0 on success, otherwise an OpenFlow error code. */\nstatic enum ofperr\ncollect_rules_strict(struct ofproto *ofproto,\n                     const struct rule_criteria *criteria,\n                     struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table;\n    size_t n_readonly = 0;\n    enum ofperr error = 0;\n\n    rule_collection_init(rules);\n\n    if (!check_table_id(ofproto, criteria->table_id)) {\n        error = OFPERR_OFPBRC_BAD_TABLE_ID;\n        goto exit;\n    }\n\n    if (criteria->cookie_mask == OVS_BE64_MAX) {\n        struct rule *rule;\n\n        HINDEX_FOR_EACH_WITH_HASH (rule, cookie_node,\n                                   hash_cookie(criteria->cookie),\n                                   &ofproto->cookies) {\n            if (cls_rule_equal(&rule->cr, &criteria->cr)) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    } else {\n        FOR_EACH_MATCHING_TABLE (table, criteria->table_id, ofproto) {\n            struct rule *rule;\n\n            rule = rule_from_cls_rule(classifier_find_rule_exactly(\n                                          &table->cls, &criteria->cr,\n                                          criteria->version));\n            if (rule) {\n                collect_rule(rule, criteria, rules, &n_readonly);\n            }\n        }\n    }\n\nexit:\n    if (!error && !rule_collection_n(rules) && n_readonly) {\n        /* We didn't find any rules to modify.  We did find some read-only\n         * rules that we're not allowed to modify, so report that. */\n        error = OFPERR_OFPBRC_EPERM;\n    }\n    if (error) {\n        rule_collection_destroy(rules);\n    }\n    return error;\n}\n\n/* Returns 'age_ms' (a duration in milliseconds), converted to seconds and\n * forced into the range of a uint16_t. */\nstatic int\nage_secs(long long int age_ms)\n{\n    return (age_ms < 0 ? 0\n            : age_ms >= UINT16_MAX * 1000 ? UINT16_MAX\n            : (unsigned int) age_ms / 1000);\n}\n\nstatic enum ofperr\nhandle_flow_stats_request(struct ofconn *ofconn,\n                          const struct ofp_header *request)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_stats_request fsr;\n    struct rule_criteria criteria;\n    struct rule_collection rules;\n    struct ovs_list replies;\n    enum ofperr error;\n\n    error = ofputil_decode_flow_stats_request(&fsr, request,\n                                              ofproto_get_tun_tab(ofproto),\n                                              &ofproto->vl_mff_map);\n    if (error) {\n        return error;\n    }\n\n    rule_criteria_init(&criteria, fsr.table_id, &fsr.match, 0, OVS_VERSION_MAX,\n                       fsr.cookie, fsr.cookie_mask, fsr.out_port,\n                       fsr.out_group);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = collect_rules_loose(ofproto, &criteria, &rules);\n    rule_criteria_destroy(&criteria);\n    if (!error) {\n        rule_collection_ref(&rules);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    if (error) {\n        return error;\n    }\n\n    ofpmp_init(&replies, request);\n    struct rule *rule;\n    RULE_COLLECTION_FOR_EACH (rule, &rules) {\n        long long int now = time_msec();\n        struct ofputil_flow_stats fs;\n        long long int created, used, modified;\n        const struct rule_actions *actions;\n        enum ofputil_flow_mod_flags flags;\n\n        ovs_mutex_lock(&rule->mutex);\n        fs.cookie = rule->flow_cookie;\n        fs.idle_timeout = rule->idle_timeout;\n        fs.hard_timeout = rule->hard_timeout;\n        fs.importance = rule->importance;\n        created = rule->created;\n        modified = rule->modified;\n        actions = rule_get_actions(rule);\n        flags = rule->flags;\n        ovs_mutex_unlock(&rule->mutex);\n\n        ofproto->ofproto_class->rule_get_stats(rule, &fs.packet_count,\n                                               &fs.byte_count, &used);\n\n        minimatch_expand(&rule->cr.match, &fs.match);\n        fs.table_id = rule->table_id;\n        calc_duration(created, now, &fs.duration_sec, &fs.duration_nsec);\n        fs.priority = rule->cr.priority;\n        fs.idle_age = age_secs(now - used);\n        fs.hard_age = age_secs(now - modified);\n        fs.ofpacts = actions->ofpacts;\n        fs.ofpacts_len = actions->ofpacts_len;\n\n        fs.flags = flags;\n        ofputil_append_flow_stats_reply(&fs, &replies,\n                                        ofproto_get_tun_tab(ofproto));\n    }\n\n    rule_collection_unref(&rules);\n    rule_collection_destroy(&rules);\n\n    ofconn_send_replies(ofconn, &replies);\n\n    return 0;\n}\n\nstatic void\nflow_stats_ds(struct ofproto *ofproto, struct rule *rule, struct ds *results)\n{\n    uint64_t packet_count, byte_count;\n    const struct rule_actions *actions;\n    long long int created, used;\n\n    rule->ofproto->ofproto_class->rule_get_stats(rule, &packet_count,\n                                                 &byte_count, &used);\n\n    ovs_mutex_lock(&rule->mutex);\n    actions = rule_get_actions(rule);\n    created = rule->created;\n    ovs_mutex_unlock(&rule->mutex);\n\n    if (rule->table_id != 0) {\n        ds_put_format(results, \"table_id=%\"PRIu8\", \", rule->table_id);\n    }\n    ds_put_format(results, \"duration=%llds, \", (time_msec() - created) / 1000);\n    ds_put_format(results, \"n_packets=%\"PRIu64\", \", packet_count);\n    ds_put_format(results, \"n_bytes=%\"PRIu64\", \", byte_count);\n    cls_rule_format(&rule->cr, ofproto_get_tun_tab(ofproto), results);\n    ds_put_char(results, ',');\n\n    ds_put_cstr(results, \"actions=\");\n    ofpacts_format(actions->ofpacts, actions->ofpacts_len, results);\n\n    ds_put_cstr(results, \"\\n\");\n}\n\n/* Adds a pretty-printed description of all flows to 'results', including\n * hidden flows (e.g., set up by in-band control). */\nvoid\nofproto_get_all_flows(struct ofproto *p, struct ds *results)\n{\n    struct oftable *table;\n\n    OFPROTO_FOR_EACH_TABLE (table, p) {\n        struct rule *rule;\n\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            flow_stats_ds(p, rule, results);\n        }\n    }\n}\n\n/* Obtains the NetFlow engine type and engine ID for 'ofproto' into\n * '*engine_type' and '*engine_id', respectively. */\nvoid\nofproto_get_netflow_ids(const struct ofproto *ofproto,\n                        uint8_t *engine_type, uint8_t *engine_id)\n{\n    ofproto->ofproto_class->get_netflow_ids(ofproto, engine_type, engine_id);\n}\n\n/* Checks the status change of CFM on 'ofport'.\n *\n * Returns true if 'ofproto_class' does not support 'cfm_status_changed'. */\nbool\nofproto_port_cfm_status_changed(struct ofproto *ofproto, ofp_port_t ofp_port)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->cfm_status_changed\n            ? ofproto->ofproto_class->cfm_status_changed(ofport)\n            : true);\n}\n\n/* Checks the status of CFM configured on 'ofp_port' within 'ofproto'.\n * Returns 0 if the port's CFM status was successfully stored into\n * '*status'.  Returns positive errno if the port did not have CFM\n * configured.\n *\n * The caller must provide and own '*status', and must free 'status->rmps'.\n * '*status' is indeterminate if the return value is non-zero. */\nint\nofproto_port_get_cfm_status(const struct ofproto *ofproto, ofp_port_t ofp_port,\n                            struct cfm_status *status)\n{\n    struct ofport *ofport = ofproto_get_port(ofproto, ofp_port);\n    return (ofport && ofproto->ofproto_class->get_cfm_status\n            ? ofproto->ofproto_class->get_cfm_status(ofport, status)\n            : EOPNOTSUPP);\n}\n\nstatic enum ofperr\nhandle_aggregate_stats_request(struct ofconn *ofconn,\n                               const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_stats_request request;\n    struct ofputil_aggregate_stats stats;\n    bool unknown_packets, unknown_bytes;\n    struct rule_criteria criteria;\n    struct rule_collection rules;\n    struct ofpbuf *reply;\n    enum ofperr error;\n\n    error = ofputil_decode_flow_stats_request(&request, oh,\n                                              ofproto_get_tun_tab(ofproto),\n                                              &ofproto->vl_mff_map);\n    if (error) {\n        return error;\n    }\n\n    rule_criteria_init(&criteria, request.table_id, &request.match, 0,\n                       OVS_VERSION_MAX, request.cookie, request.cookie_mask,\n                       request.out_port, request.out_group);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = collect_rules_loose(ofproto, &criteria, &rules);\n    rule_criteria_destroy(&criteria);\n    if (!error) {\n        rule_collection_ref(&rules);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    if (error) {\n        return error;\n    }\n\n    memset(&stats, 0, sizeof stats);\n    unknown_packets = unknown_bytes = false;\n\n    struct rule *rule;\n    RULE_COLLECTION_FOR_EACH (rule, &rules) {\n        uint64_t packet_count;\n        uint64_t byte_count;\n        long long int used;\n\n        ofproto->ofproto_class->rule_get_stats(rule, &packet_count,\n                                               &byte_count, &used);\n\n        if (packet_count == UINT64_MAX) {\n            unknown_packets = true;\n        } else {\n            stats.packet_count += packet_count;\n        }\n\n        if (byte_count == UINT64_MAX) {\n            unknown_bytes = true;\n        } else {\n            stats.byte_count += byte_count;\n        }\n\n        stats.flow_count++;\n    }\n    if (unknown_packets) {\n        stats.packet_count = UINT64_MAX;\n    }\n    if (unknown_bytes) {\n        stats.byte_count = UINT64_MAX;\n    }\n\n    rule_collection_unref(&rules);\n    rule_collection_destroy(&rules);\n\n    reply = ofputil_encode_aggregate_stats_reply(&stats, oh);\n    ofconn_send_reply(ofconn, reply);\n\n    return 0;\n}\n\nstruct queue_stats_cbdata {\n    struct ofport *ofport;\n    struct ovs_list replies;\n    long long int now;\n};\n\nstatic void\nput_queue_stats(struct queue_stats_cbdata *cbdata, uint32_t queue_id,\n                const struct netdev_queue_stats *stats)\n{\n    struct ofputil_queue_stats oqs;\n\n    oqs.port_no = cbdata->ofport->pp.port_no;\n    oqs.queue_id = queue_id;\n    oqs.tx_bytes = stats->tx_bytes;\n    oqs.tx_packets = stats->tx_packets;\n    oqs.tx_errors = stats->tx_errors;\n    if (stats->created != LLONG_MIN) {\n        calc_duration(stats->created, cbdata->now,\n                      &oqs.duration_sec, &oqs.duration_nsec);\n    } else {\n        oqs.duration_sec = oqs.duration_nsec = UINT32_MAX;\n    }\n    ofputil_append_queue_stat(&cbdata->replies, &oqs);\n}\n\nstatic void\nhandle_queue_stats_dump_cb(uint32_t queue_id,\n                           struct netdev_queue_stats *stats,\n                           void *cbdata_)\n{\n    struct queue_stats_cbdata *cbdata = cbdata_;\n\n    put_queue_stats(cbdata, queue_id, stats);\n}\n\nstatic enum ofperr\nhandle_queue_stats_for_port(struct ofport *port, uint32_t queue_id,\n                            struct queue_stats_cbdata *cbdata)\n{\n    cbdata->ofport = port;\n    if (queue_id == OFPQ_ALL) {\n        netdev_dump_queue_stats(port->netdev,\n                                handle_queue_stats_dump_cb, cbdata);\n    } else {\n        struct netdev_queue_stats stats;\n\n        if (!netdev_get_queue_stats(port->netdev, queue_id, &stats)) {\n            put_queue_stats(cbdata, queue_id, &stats);\n        } else {\n            return OFPERR_OFPQOFC_BAD_QUEUE;\n        }\n    }\n    return 0;\n}\n\nstatic enum ofperr\nhandle_queue_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *rq)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct queue_stats_cbdata cbdata;\n    struct ofport *port;\n    enum ofperr error;\n    struct ofputil_queue_stats_request oqsr;\n\n    COVERAGE_INC(ofproto_queue_req);\n\n    ofpmp_init(&cbdata.replies, rq);\n    cbdata.now = time_msec();\n\n    error = ofputil_decode_queue_stats_request(rq, &oqsr);\n    if (error) {\n        return error;\n    }\n\n    if (oqsr.port_no == OFPP_ANY) {\n        error = OFPERR_OFPQOFC_BAD_QUEUE;\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            if (!handle_queue_stats_for_port(port, oqsr.queue_id, &cbdata)) {\n                error = 0;\n            }\n        }\n    } else {\n        port = ofproto_get_port(ofproto, oqsr.port_no);\n        error = (port\n                 ? handle_queue_stats_for_port(port, oqsr.queue_id, &cbdata)\n                 : OFPERR_OFPQOFC_BAD_PORT);\n    }\n    if (!error) {\n        ofconn_send_replies(ofconn, &cbdata.replies);\n    } else {\n        ofpbuf_list_delete(&cbdata.replies);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nevict_rules_from_table(struct oftable *table)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error = 0;\n    struct rule_collection rules;\n    unsigned int count = table->n_flows;\n    unsigned int max_flows = table->max_flows;\n\n    rule_collection_init(&rules);\n\n    while (count-- > max_flows) {\n        struct rule *rule;\n\n        if (!choose_rule_to_evict(table, &rule)) {\n            error = OFPERR_OFPFMFC_TABLE_FULL;\n            break;\n        } else {\n            eviction_group_remove_rule(rule);\n            rule_collection_add(&rules, rule);\n        }\n    }\n    delete_flows__(&rules, OFPRR_EVICTION, NULL);\n\n    return error;\n}\n\nstatic void\nget_conjunctions(const struct ofputil_flow_mod *fm,\n                 struct cls_conjunction **conjsp, size_t *n_conjsp)\n{\n    struct cls_conjunction *conjs = NULL;\n    int n_conjs = 0;\n\n    const struct ofpact *ofpact;\n    OFPACT_FOR_EACH (ofpact, fm->ofpacts, fm->ofpacts_len) {\n        if (ofpact->type == OFPACT_CONJUNCTION) {\n            n_conjs++;\n        } else if (ofpact->type != OFPACT_NOTE) {\n            /* \"conjunction\" may appear with \"note\" actions but not with any\n             * other type of actions. */\n            ovs_assert(!n_conjs);\n            break;\n        }\n    }\n    if (n_conjs) {\n        int i = 0;\n\n        conjs = xzalloc(n_conjs * sizeof *conjs);\n        OFPACT_FOR_EACH (ofpact, fm->ofpacts, fm->ofpacts_len) {\n            if (ofpact->type == OFPACT_CONJUNCTION) {\n                struct ofpact_conjunction *oc = ofpact_get_CONJUNCTION(ofpact);\n                conjs[i].clause = oc->clause;\n                conjs[i].n_clauses = oc->n_clauses;\n                conjs[i].id = oc->id;\n                i++;\n            }\n        }\n    }\n\n    *conjsp = conjs;\n    *n_conjsp = n_conjs;\n}\n\n/* add_flow_init(), add_flow_start(), add_flow_revert(), and add_flow_finish()\n * implement OFPFC_ADD and the cases for OFPFC_MODIFY and OFPFC_MODIFY_STRICT\n * in which no matching flow already exists in the flow table.\n *\n * add_flow_init() creates a new flow according to 'fm' and stores it to 'ofm'\n * for later reference.  If the flow replaces other flow, it will be updated to\n * match modify semantics later by add_flow_start() (by calling\n * replace_rule_start()).\n *\n * Returns 0 on success, or an OpenFlow error code on failure.\n *\n * On successful return the caller must complete the operation by calling\n * add_flow_start(), and if that succeeds, then either add_flow_finish(), or\n * add_flow_revert() if the operation needs to be reverted due to a later\n * failure.\n */\nstatic enum ofperr\nadd_flow_init(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n              const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct oftable *table;\n    struct cls_rule cr;\n    uint8_t table_id;\n    enum ofperr error;\n\n    if (!check_table_id(ofproto, fm->table_id)) {\n        return OFPERR_OFPBRC_BAD_TABLE_ID;\n    }\n\n    /* Pick table. */\n    if (fm->table_id == 0xff) {\n        if (ofproto->ofproto_class->rule_choose_table) {\n            error = ofproto->ofproto_class->rule_choose_table(ofproto,\n                                                              &fm->match,\n                                                              &table_id);\n            if (error) {\n                return error;\n            }\n            ovs_assert(table_id < ofproto->n_tables);\n        } else {\n            table_id = 0;\n        }\n    } else if (fm->table_id < ofproto->n_tables) {\n        table_id = fm->table_id;\n    } else {\n        return OFPERR_OFPBRC_BAD_TABLE_ID;\n    }\n\n    table = &ofproto->tables[table_id];\n    if (table->flags & OFTABLE_READONLY\n        && !(fm->flags & OFPUTIL_FF_NO_READONLY)) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    if (!(fm->flags & OFPUTIL_FF_HIDDEN_FIELDS)\n        && !match_has_default_hidden_fields(&fm->match)) {\n        VLOG_WARN_RL(&rl, \"%s: (add_flow) only internal flows can set \"\n                     \"non-default values to hidden fields\", ofproto->name);\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    if (!ofm->temp_rule) {\n        cls_rule_init(&cr, &fm->match, fm->priority);\n\n        /* Allocate new rule.  Destroys 'cr'. */\n        error = ofproto_rule_create(ofproto, &cr, table - ofproto->tables,\n                                    fm->new_cookie, fm->idle_timeout,\n                                    fm->hard_timeout, fm->flags,\n                                    fm->importance, fm->ofpacts,\n                                    fm->ofpacts_len,\n                                    fm->match.flow.tunnel.metadata.present.map,\n                                    fm->ofpacts_tlv_bitmap, &ofm->temp_rule);\n        if (error) {\n            return error;\n        }\n\n        get_conjunctions(fm, &ofm->conjs, &ofm->n_conjs);\n    }\n    return 0;\n}\n\n/* ofm->temp_rule is consumed only in the successful case. */\nstatic enum ofperr\nadd_flow_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = NULL;\n    struct rule *new_rule = ofm->temp_rule;\n    const struct rule_actions *actions = rule_get_actions(new_rule);\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n    enum ofperr error;\n\n    /* Must check actions while holding ofproto_mutex to avoid a race. */\n    error = ofproto_check_ofpacts(ofproto, actions->ofpacts,\n                                  actions->ofpacts_len);\n    if (error) {\n        return error;\n    }\n\n    /* Check for the existence of an identical rule.\n     * This will not return rules earlier marked for removal. */\n    old_rule = rule_from_cls_rule(classifier_find_rule_exactly(&table->cls,\n                                                               &new_rule->cr,\n                                                               ofm->version));\n    if (!old_rule) {\n        /* Check for overlap, if requested. */\n        if (new_rule->flags & OFPUTIL_FF_CHECK_OVERLAP\n            && classifier_rule_overlaps(&table->cls, &new_rule->cr,\n                                        ofm->version)) {\n            return OFPERR_OFPFMFC_OVERLAP;\n        }\n\n        /* If necessary, evict an existing rule to clear out space. */\n        if (table->n_flows >= table->max_flows) {\n            if (!choose_rule_to_evict(table, &old_rule)) {\n                return OFPERR_OFPFMFC_TABLE_FULL;\n            }\n            eviction_group_remove_rule(old_rule);\n            /* Marks 'old_rule' as an evicted rule rather than replaced rule.\n             */\n            old_rule->removed_reason = OFPRR_EVICTION;\n        }\n    } else {\n        ofm->modify_cookie = true;\n    }\n\n    if (old_rule) {\n        rule_collection_add(&ofm->old_rules, old_rule);\n    }\n    /* Take ownership of the temp_rule. */\n    rule_collection_add(&ofm->new_rules, new_rule);\n    ofm->temp_rule = NULL;\n\n    replace_rule_start(ofproto, ofm, old_rule, new_rule);\n    return 0;\n}\n\n/* Revert the effects of add_flow_start(). */\nstatic void\nadd_flow_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = rule_collection_n(&ofm->old_rules)\n        ? rule_collection_rules(&ofm->old_rules)[0] : NULL;\n    struct rule *new_rule = rule_collection_rules(&ofm->new_rules)[0];\n\n    replace_rule_revert(ofproto, old_rule, new_rule);\n}\n\n/* To be called after version bump. */\nstatic void\nadd_flow_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *old_rule = rule_collection_n(&ofm->old_rules)\n        ? rule_collection_rules(&ofm->old_rules)[0] : NULL;\n    struct rule *new_rule = rule_collection_rules(&ofm->new_rules)[0];\n    struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n\n    replace_rule_finish(ofproto, ofm, req, old_rule, new_rule, &dead_cookies);\n    learned_cookies_flush(ofproto, &dead_cookies);\n\n    if (old_rule) {\n        ovsrcu_postpone(remove_rule_rcu, old_rule);\n    } else {\n        ofmonitor_report(ofproto->connmgr, new_rule, NXFME_ADDED, 0,\n                         req ? req->ofconn : NULL,\n                         req ? req->request->xid : 0, NULL);\n\n        /* Send Vacancy Events for OF1.4+. */\n        send_table_status(ofproto, new_rule->table_id);\n    }\n}\n\f\n/* OFPFC_MODIFY and OFPFC_MODIFY_STRICT. */\n\n/* Create a new rule.  Note that the rule is NOT inserted into a any data\n * structures yet.  Takes ownership of 'cr'.  Only assigns '*new_rule' if\n * successful. */\nstatic enum ofperr\nofproto_rule_create(struct ofproto *ofproto, struct cls_rule *cr,\n                    uint8_t table_id, ovs_be64 new_cookie,\n                    uint16_t idle_timeout, uint16_t hard_timeout,\n                    enum ofputil_flow_mod_flags flags, uint16_t importance,\n                    const struct ofpact *ofpacts, size_t ofpacts_len,\n                    uint64_t match_tlv_bitmap, uint64_t ofpacts_tlv_bitmap,\n                    struct rule **new_rule)\n    OVS_NO_THREAD_SAFETY_ANALYSIS\n{\n    struct rule *rule;\n    enum ofperr error;\n\n    /* Allocate new rule. */\n    rule = ofproto->ofproto_class->rule_alloc();\n    if (!rule) {\n        cls_rule_destroy(cr);\n        VLOG_WARN_RL(&rl, \"%s: failed to allocate a rule.\", ofproto->name);\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Initialize base state. */\n    *CONST_CAST(struct ofproto **, &rule->ofproto) = ofproto;\n    cls_rule_move(CONST_CAST(struct cls_rule *, &rule->cr), cr);\n    ovs_refcount_init(&rule->ref_count);\n\n    ovs_mutex_init(&rule->mutex);\n    ovs_mutex_lock(&rule->mutex);\n    *CONST_CAST(ovs_be64 *, &rule->flow_cookie) = new_cookie;\n    rule->created = rule->modified = time_msec();\n    rule->idle_timeout = idle_timeout;\n    rule->hard_timeout = hard_timeout;\n    *CONST_CAST(uint16_t *, &rule->importance) = importance;\n    rule->removed_reason = OVS_OFPRR_NONE;\n\n    *CONST_CAST(uint8_t *, &rule->table_id) = table_id;\n    rule->flags = flags & OFPUTIL_FF_STATE;\n\n    *CONST_CAST(const struct rule_actions **, &rule->actions)\n        = rule_actions_create(ofpacts, ofpacts_len);\n\n    ovs_list_init(&rule->meter_list_node);\n    rule->eviction_group = NULL;\n    rule->monitor_flags = 0;\n    rule->add_seqno = 0;\n    rule->modify_seqno = 0;\n    ovs_list_init(&rule->expirable);\n    ovs_mutex_unlock(&rule->mutex);\n\n    /* Construct rule, initializing derived state. */\n    error = ofproto->ofproto_class->rule_construct(rule);\n    if (error) {\n        ofproto_rule_destroy__(rule);\n        return error;\n    }\n\n    rule->state = RULE_INITIALIZED;\n    rule->match_tlv_bitmap = match_tlv_bitmap;\n    rule->ofpacts_tlv_bitmap = ofpacts_tlv_bitmap;\n    mf_vl_mff_ref(&rule->ofproto->vl_mff_map, match_tlv_bitmap);\n    mf_vl_mff_ref(&rule->ofproto->vl_mff_map, ofpacts_tlv_bitmap);\n\n    *new_rule = rule;\n    return 0;\n}\n\n/* Initialize 'ofm' for a learn action.  If the rule already existed, reference\n * to that rule is taken, otherwise a new rule is created.  'ofm' keeps the\n * rule reference in both.  This does not take the global 'ofproto_mutex'. */\nenum ofperr\nofproto_flow_mod_init_for_learn(struct ofproto *ofproto,\n                                const struct ofputil_flow_mod *fm,\n                                struct ofproto_flow_mod *ofm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    /* Reject flow mods that do not look like they were generated by a learn\n     * action. */\n    if (fm->command != OFPFC_MODIFY_STRICT || fm->table_id == OFPTT_ALL\n        || fm->flags & OFPUTIL_FF_RESET_COUNTS\n        || fm->buffer_id != UINT32_MAX) {\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Check if the rule already exists, and we can get a reference to it. */\n    struct oftable *table = &ofproto->tables[fm->table_id];\n    struct rule *rule;\n\n    rule = rule_from_cls_rule(classifier_find_match_exactly(\n                                  &table->cls, &fm->match, fm->priority,\n                                  OVS_VERSION_MAX));\n    if (rule) {\n        /* Check if the rule's attributes match as well. */\n        const struct rule_actions *actions;\n\n        ovs_mutex_lock(&rule->mutex);\n        actions = rule_get_actions(rule);\n        if (rule->idle_timeout == fm->idle_timeout\n            && rule->hard_timeout == fm->hard_timeout\n            && rule->importance == fm->importance\n            && rule->flags == (fm->flags & OFPUTIL_FF_STATE)\n            && (!fm->modify_cookie || (fm->new_cookie == rule->flow_cookie))\n            && ofpacts_equal(fm->ofpacts, fm->ofpacts_len,\n                             actions->ofpacts, actions->ofpacts_len)) {\n            /* Rule already exists and need not change, except for the modified\n             * timestamp.  Get a reference to the existing rule. */\n            ovs_mutex_unlock(&rule->mutex);\n            if (!ofproto_rule_try_ref(rule)) {\n                rule = NULL; /* Pretend it did not exist. */\n            }\n        } else {\n            ovs_mutex_unlock(&rule->mutex);\n            rule = NULL;\n        }\n    }\n\n    return ofproto_flow_mod_init(ofproto, ofm, fm, rule);\n}\n\nenum ofperr\nofproto_flow_mod_learn_refresh(struct ofproto_flow_mod *ofm)\n{\n    enum ofperr error = 0;\n\n    /* ofm->temp_rule is our reference to the learned rule.  We have a\n     * reference to an existing rule, if it already was in the classifier,\n     * otherwise we may have a fresh rule that we need to insert. */\n    struct rule *rule = ofm->temp_rule;\n    if (!rule) {\n        return OFPERR_OFPFMFC_UNKNOWN;\n    }\n\n    /* Create a new rule if the current one has been removed from the\n     * classifier.  We need to do this since RCU does not allow a current rule\n     * to be reinserted before all threads have quiesced.\n     *\n     * It is possible that the rule is removed asynchronously, e.g., right\n     * after we have read the 'rule->state' below.  In this case the next time\n     * this function is executed the rule will be reinstated. */\n    if (rule->state == RULE_REMOVED) {\n        struct cls_rule cr;\n\n        cls_rule_clone(&cr, &rule->cr);\n        ovs_mutex_lock(&rule->mutex);\n        error = ofproto_rule_create(rule->ofproto, &cr, rule->table_id,\n                                    rule->flow_cookie,\n                                    rule->idle_timeout,\n                                    rule->hard_timeout, rule->flags,\n                                    rule->importance,\n                                    rule->actions->ofpacts,\n                                    rule->actions->ofpacts_len,\n                                    rule->match_tlv_bitmap,\n                                    rule->ofpacts_tlv_bitmap,\n                                    &ofm->temp_rule);\n        ovs_mutex_unlock(&rule->mutex);\n        if (!error) {\n            ofproto_rule_unref(rule);   /* Release old reference. */\n        }\n    } else {\n        /* Refresh the existing rule. */\n        ovs_mutex_lock(&rule->mutex);\n        rule->modified = time_msec();\n        ovs_mutex_unlock(&rule->mutex);\n    }\n    return error;\n}\n\nenum ofperr\nofproto_flow_mod_learn_start(struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = ofm->temp_rule;\n\n    /* ofproto_flow_mod_start() consumes the reference, so we\n     * take a new one. */\n    ofproto_rule_ref(rule);\n    enum ofperr error = ofproto_flow_mod_start(rule->ofproto, ofm);\n    ofm->temp_rule = rule;\n\n    return error;\n}\n\nvoid\nofproto_flow_mod_learn_revert(struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = rule_collection_rules(&ofm->new_rules)[0];\n    ofproto_flow_mod_revert(rule->ofproto, ofm);\n}\n\nvoid\nofproto_flow_mod_learn_finish(struct ofproto_flow_mod *ofm,\n                              struct ofproto *orig_ofproto)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule = rule_collection_rules(&ofm->new_rules)[0];\n\n    /* If learning on a different bridge, must bump its version\n     * number and flush connmgr afterwards. */\n    if (rule->ofproto != orig_ofproto) {\n        ofproto_bump_tables_version(rule->ofproto);\n    }\n    ofproto_flow_mod_finish(rule->ofproto, ofm, NULL);\n    if (rule->ofproto != orig_ofproto) {\n        ofmonitor_flush(rule->ofproto->connmgr);\n    }\n}\n\n/* Refresh 'ofm->temp_rule', for which the caller holds a reference, if already\n * in the classifier, insert it otherwise.  If the rule has already been\n * removed from the classifier, a new rule is created using 'ofm->temp_rule' as\n * a template and the reference to the old 'ofm->temp_rule' is freed.  If\n * 'keep_ref' is true, then a reference to the current rule is held, otherwise\n * it is released and 'ofm->temp_rule' is set to NULL.\n *\n * Caller needs to be the exclusive owner of 'ofm' as it is being manipulated\n * during the call. */\nenum ofperr\nofproto_flow_mod_learn(struct ofproto_flow_mod *ofm, bool keep_ref)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error = ofproto_flow_mod_learn_refresh(ofm);\n    struct rule *rule = ofm->temp_rule;\n\n    /* Do we need to insert the rule? */\n    if (!error && rule->state == RULE_INITIALIZED) {\n        ovs_mutex_lock(&ofproto_mutex);\n        ofm->version = rule->ofproto->tables_version + 1;\n        error = ofproto_flow_mod_learn_start(ofm);\n        if (!error) {\n            ofproto_flow_mod_learn_finish(ofm, NULL);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    if (!keep_ref) {\n        ofproto_rule_unref(rule);\n        ofm->temp_rule = NULL;\n    }\n    return error;\n}\n\nstatic void\nreplace_rule_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                   struct rule *old_rule, struct rule *new_rule)\n{\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n\n    /* 'old_rule' may be either an evicted rule or replaced rule. */\n    if (old_rule) {\n        /* Copy values from old rule for modify semantics. */\n        if (old_rule->removed_reason != OFPRR_EVICTION) {\n            bool change_cookie = (ofm->modify_cookie\n                                  && new_rule->flow_cookie != OVS_BE64_MAX\n                                  && new_rule->flow_cookie != old_rule->flow_cookie);\n\n            ovs_mutex_lock(&new_rule->mutex);\n            ovs_mutex_lock(&old_rule->mutex);\n            if (ofm->command != OFPFC_ADD) {\n                new_rule->idle_timeout = old_rule->idle_timeout;\n                new_rule->hard_timeout = old_rule->hard_timeout;\n                *CONST_CAST(uint16_t *, &new_rule->importance) = old_rule->importance;\n                new_rule->flags = old_rule->flags;\n                new_rule->created = old_rule->created;\n            }\n            if (!change_cookie) {\n                *CONST_CAST(ovs_be64 *, &new_rule->flow_cookie)\n                    = old_rule->flow_cookie;\n            }\n            ovs_mutex_unlock(&old_rule->mutex);\n            ovs_mutex_unlock(&new_rule->mutex);\n        }\n\n        /* Mark the old rule for removal in the next version. */\n        cls_rule_make_invisible_in_version(&old_rule->cr, ofm->version);\n\n        /* Remove the old rule from data structures. */\n        ofproto_rule_remove__(ofproto, old_rule);\n    } else {\n        table->n_flows++;\n    }\n    /* Insert flow to ofproto data structures, so that later flow_mods may\n     * relate to it.  This is reversible, in case later errors require this to\n     * be reverted. */\n    ofproto_rule_insert__(ofproto, new_rule);\n    /* Make the new rule visible for classifier lookups only from the next\n     * version. */\n    classifier_insert(&table->cls, &new_rule->cr, ofm->version, ofm->conjs,\n                      ofm->n_conjs);\n}\n\nstatic void\nreplace_rule_revert(struct ofproto *ofproto,\n                    struct rule *old_rule, struct rule *new_rule)\n{\n    struct oftable *table = &ofproto->tables[new_rule->table_id];\n\n    if (old_rule) {\n        if (old_rule->removed_reason == OFPRR_EVICTION) {\n            /* Revert the eviction. */\n            eviction_group_add_rule(old_rule);\n        }\n\n        /* Restore the old rule to data structures. */\n        ofproto_rule_insert__(ofproto, old_rule);\n\n        /* Restore the original visibility of the old rule. */\n        cls_rule_restore_visibility(&old_rule->cr);\n    } else {\n        /* Restore table's rule count. */\n        table->n_flows--;\n    }\n\n    /* Remove the new rule immediately.  It was never visible to lookups. */\n    if (!classifier_remove(&table->cls, &new_rule->cr)) {\n        OVS_NOT_REACHED();\n    }\n    ofproto_rule_remove__(ofproto, new_rule);\n    ofproto_rule_unref(new_rule);\n}\n\n/* Adds the 'new_rule', replacing the 'old_rule'. */\nstatic void\nreplace_rule_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req,\n                    struct rule *old_rule, struct rule *new_rule,\n                    struct ovs_list *dead_cookies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *replaced_rule;\n\n    replaced_rule = (old_rule && old_rule->removed_reason != OFPRR_EVICTION)\n        ? old_rule : NULL;\n\n    /* Insert the new flow to the ofproto provider.  A non-NULL 'replaced_rule'\n     * is a duplicate rule the 'new_rule' is replacing.  The provider should\n     * link the packet and byte counts from the old rule to the new one if\n     * 'modify_keep_counts' is 'true'.  The 'replaced_rule' will be deleted\n     * right after this call. */\n    ofproto->ofproto_class->rule_insert(new_rule, replaced_rule,\n                                        ofm->modify_keep_counts);\n    learned_cookies_inc(ofproto, rule_get_actions(new_rule));\n\n    if (old_rule) {\n        const struct rule_actions *old_actions = rule_get_actions(old_rule);\n        const struct rule_actions *new_actions = rule_get_actions(new_rule);\n\n        learned_cookies_dec(ofproto, old_actions, dead_cookies);\n\n        if (replaced_rule) {\n            enum nx_flow_update_event event = ofm->command == OFPFC_ADD\n                ? NXFME_ADDED : NXFME_MODIFIED;\n\n            bool changed_cookie = (new_rule->flow_cookie\n                                   != old_rule->flow_cookie);\n\n            bool changed_actions = !ofpacts_equal(new_actions->ofpacts,\n                                                  new_actions->ofpacts_len,\n                                                  old_actions->ofpacts,\n                                                  old_actions->ofpacts_len);\n\n            if (event != NXFME_MODIFIED || changed_actions\n                || changed_cookie) {\n                ofmonitor_report(ofproto->connmgr, new_rule, event, 0,\n                                 req ? req->ofconn : NULL,\n                                 req ? req->request->xid : 0,\n                                 changed_actions ? old_actions : NULL);\n            }\n        } else {\n            /* XXX: This is slight duplication with delete_flows_finish__() */\n            ofmonitor_report(ofproto->connmgr, old_rule, NXFME_DELETED,\n                             OFPRR_EVICTION,\n                             req ? req->ofconn : NULL,\n                             req ? req->request->xid : 0, NULL);\n        }\n    }\n}\n\n/* ofm->temp_rule is consumed only in the successful case. */\nstatic enum ofperr\nmodify_flows_start__(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n    enum ofperr error;\n\n    if (rule_collection_n(old_rules) > 0) {\n        /* Create a new 'modified' rule for each old rule. */\n        struct rule *old_rule, *new_rule;\n        const struct rule_actions *actions = rule_get_actions(ofm->temp_rule);\n\n        /* Must check actions while holding ofproto_mutex to avoid a race. */\n        error = ofproto_check_ofpacts(ofproto, actions->ofpacts,\n                                      actions->ofpacts_len);\n        if (error) {\n            return error;\n        }\n\n        /* Use the temp rule as the first new rule, and as the template for\n         * the rest. */\n        struct rule *temp = ofm->temp_rule;\n        ofm->temp_rule = NULL;   /* We consume the template. */\n\n        bool first = true;\n        RULE_COLLECTION_FOR_EACH (old_rule, old_rules) {\n            if (first) {\n                /* The template rule's match is possibly a loose one, so it\n                 * must be replaced with the old rule's match so that the new\n                 * rule actually replaces the old one. */\n                cls_rule_destroy(CONST_CAST(struct cls_rule *, &temp->cr));\n                cls_rule_clone(CONST_CAST(struct cls_rule *, &temp->cr),\n                               &old_rule->cr);\n                if (temp->match_tlv_bitmap != old_rule->match_tlv_bitmap) {\n                    mf_vl_mff_unref(&temp->ofproto->vl_mff_map,\n                                    temp->match_tlv_bitmap);\n                    temp->match_tlv_bitmap = old_rule->match_tlv_bitmap;\n                    mf_vl_mff_ref(&temp->ofproto->vl_mff_map,\n                                  temp->match_tlv_bitmap);\n                }\n                *CONST_CAST(uint8_t *, &temp->table_id) = old_rule->table_id;\n                rule_collection_add(new_rules, temp);\n                first = false;\n            } else {\n                struct cls_rule cr;\n                cls_rule_clone(&cr, &old_rule->cr);\n                error = ofproto_rule_create(ofproto, &cr, old_rule->table_id,\n                                            temp->flow_cookie,\n                                            temp->idle_timeout,\n                                            temp->hard_timeout, temp->flags,\n                                            temp->importance,\n                                            temp->actions->ofpacts,\n                                            temp->actions->ofpacts_len,\n                                            old_rule->match_tlv_bitmap,\n                                            temp->ofpacts_tlv_bitmap,\n                                            &new_rule);\n                if (!error) {\n                    rule_collection_add(new_rules, new_rule);\n                } else {\n                    /* Return the template rule in place in the error case. */\n                    ofm->temp_rule = temp;\n                    rule_collection_rules(new_rules)[0] = NULL;\n\n                    rule_collection_unref(new_rules);\n                    rule_collection_destroy(new_rules);\n                    return error;\n                }\n            }\n        }\n        ovs_assert(rule_collection_n(new_rules)\n                   == rule_collection_n(old_rules));\n\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_start(ofproto, ofm, old_rule, new_rule);\n        }\n    } else if (ofm->modify_may_add_flow) {\n        /* No match, add a new flow, consumes 'temp'. */\n        error = add_flow_start(ofproto, ofm);\n    } else {\n        /* No flow to modify and may not add a flow. */\n        ofproto_rule_unref(ofm->temp_rule);\n        ofm->temp_rule = NULL;   /* We consume the template. */\n        error = 0;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nmodify_flows_init_loose(struct ofproto *ofproto,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, 0,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask, OFPP_ANY,\n                       OFPG_ANY);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    /* Must create a new flow in advance for the case that no matches are\n     * found.  Also used for template for multiple modified flows. */\n    add_flow_init(ofproto, ofm, fm);\n\n    return 0;\n}\n\n/* Implements OFPFC_MODIFY.  Returns 0 on success or an OpenFlow error code on\n * failure. */\nstatic enum ofperr\nmodify_flows_start_loose(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_loose(ofproto, &ofm->criteria, old_rules);\n\n    if (!error) {\n        error = modify_flows_start__(ofproto, ofm);\n    }\n\n    if (error) {\n        rule_collection_destroy(old_rules);\n    }\n\n    return error;\n}\n\nstatic void\nmodify_flows_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n\n    /* Old rules were not changed yet, only need to revert new rules. */\n    if (rule_collection_n(old_rules) > 0) {\n        struct rule *old_rule, *new_rule;\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_revert(ofproto, old_rule, new_rule);\n        }\n        rule_collection_destroy(new_rules);\n        rule_collection_destroy(old_rules);\n    }\n}\n\nstatic void\nmodify_flows_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    struct rule_collection *new_rules = &ofm->new_rules;\n\n    if (rule_collection_n(old_rules) == 0\n        && rule_collection_n(new_rules) == 1) {\n        add_flow_finish(ofproto, ofm, req);\n    } else if (rule_collection_n(old_rules) > 0) {\n        struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n\n        ovs_assert(rule_collection_n(new_rules)\n                   == rule_collection_n(old_rules));\n\n        struct rule *old_rule, *new_rule;\n        RULE_COLLECTIONS_FOR_EACH (old_rule, new_rule, old_rules, new_rules) {\n            replace_rule_finish(ofproto, ofm, req, old_rule, new_rule,\n                                &dead_cookies);\n        }\n        learned_cookies_flush(ofproto, &dead_cookies);\n        remove_rules_postponed(old_rules);\n    }\n}\n\nstatic enum ofperr\nmodify_flow_init_strict(struct ofproto *ofproto OVS_UNUSED,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, fm->priority,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask, OFPP_ANY,\n                       OFPG_ANY);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    /* Must create a new flow in advance for the case that no matches are\n     * found.  Also used for template for multiple modified flows. */\n    add_flow_init(ofproto, ofm, fm);\n\n    return 0;\n}\n\n/* Implements OFPFC_MODIFY_STRICT.  Returns 0 on success or an OpenFlow error\n * code on failure. */\nstatic enum ofperr\nmodify_flow_start_strict(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *old_rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_strict(ofproto, &ofm->criteria, old_rules);\n\n    if (!error) {\n        /* collect_rules_strict() can return max 1 rule. */\n        error = modify_flows_start__(ofproto, ofm);\n    }\n\n    return error;\n}\n\f\n/* OFPFC_DELETE implementation. */\n\nstatic void\ndelete_flows_start__(struct ofproto *ofproto, ovs_version_t version,\n                     const struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        struct oftable *table = &ofproto->tables[rule->table_id];\n\n        table->n_flows--;\n        cls_rule_make_invisible_in_version(&rule->cr, version);\n\n        /* Remove rule from ofproto data structures. */\n        ofproto_rule_remove__(ofproto, rule);\n    }\n}\n\nstatic void\ndelete_flows_revert__(struct ofproto *ofproto,\n                      const struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        struct oftable *table = &ofproto->tables[rule->table_id];\n\n        /* Add rule back to ofproto data structures. */\n        ofproto_rule_insert__(ofproto, rule);\n\n        /* Restore table's rule count. */\n        table->n_flows++;\n\n        /* Restore the original visibility of the rule. */\n        cls_rule_restore_visibility(&rule->cr);\n    }\n}\n\nstatic void\ndelete_flows_finish__(struct ofproto *ofproto,\n                      struct rule_collection *rules,\n                      enum ofp_flow_removed_reason reason,\n                      const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules)) {\n        struct ovs_list dead_cookies = OVS_LIST_INITIALIZER(&dead_cookies);\n        struct rule *rule;\n\n        RULE_COLLECTION_FOR_EACH (rule, rules) {\n            /* This value will be used to send the flow removed message right\n             * before the rule is actually destroyed. */\n            rule->removed_reason = reason;\n\n            ofmonitor_report(ofproto->connmgr, rule, NXFME_DELETED, reason,\n                             req ? req->ofconn : NULL,\n                             req ? req->request->xid : 0, NULL);\n\n            /* Send Vacancy Event for OF1.4+. */\n            send_table_status(ofproto, rule->table_id);\n\n            learned_cookies_dec(ofproto, rule_get_actions(rule),\n                                &dead_cookies);\n        }\n        remove_rules_postponed(rules);\n\n        learned_cookies_flush(ofproto, &dead_cookies);\n    }\n}\n\n/* Deletes the rules listed in 'rules'.\n * The deleted rules will become invisible to the lookups in the next version.\n * Destroys 'rules'. */\nstatic void\ndelete_flows__(struct rule_collection *rules,\n               enum ofp_flow_removed_reason reason,\n               const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule_collection_n(rules)) {\n        struct ofproto *ofproto = rule_collection_rules(rules)[0]->ofproto;\n\n        delete_flows_start__(ofproto, ofproto->tables_version + 1, rules);\n        ofproto_bump_tables_version(ofproto);\n        delete_flows_finish__(ofproto, rules, reason, req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n}\n\nstatic enum ofperr\ndelete_flows_init_loose(struct ofproto *ofproto OVS_UNUSED,\n                        struct ofproto_flow_mod *ofm,\n                        const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, 0,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask,\n                       fm->out_port, fm->out_group);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    return 0;\n}\n\n/* Implements OFPFC_DELETE. */\nstatic enum ofperr\ndelete_flows_start_loose(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_loose(ofproto, &ofm->criteria, rules);\n\n    if (!error) {\n        delete_flows_start__(ofproto, ofm->version, rules);\n    }\n\n    return error;\n}\n\nstatic void\ndelete_flows_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    delete_flows_revert__(ofproto, &ofm->old_rules);\n}\n\nstatic void\ndelete_flows_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                    const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    delete_flows_finish__(ofproto, &ofm->old_rules, OFPRR_DELETE, req);\n}\n\nstatic enum ofperr\ndelete_flows_init_strict(struct ofproto *ofproto OVS_UNUSED,\n                         struct ofproto_flow_mod *ofm,\n                         const struct ofputil_flow_mod *fm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    rule_criteria_init(&ofm->criteria, fm->table_id, &fm->match, fm->priority,\n                       OVS_VERSION_MAX, fm->cookie, fm->cookie_mask,\n                       fm->out_port, fm->out_group);\n    rule_criteria_require_rw(&ofm->criteria,\n                             (fm->flags & OFPUTIL_FF_NO_READONLY) != 0);\n    return 0;\n}\n\n/* Implements OFPFC_DELETE_STRICT. */\nstatic enum ofperr\ndelete_flow_start_strict(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection *rules = &ofm->old_rules;\n    enum ofperr error;\n\n    error = collect_rules_strict(ofproto, &ofm->criteria, rules);\n\n    if (!error) {\n        delete_flows_start__(ofproto, ofm->version, rules);\n    }\n\n    return error;\n}\n\n/* This may only be called by rule_destroy_cb()! */\nstatic void\nofproto_rule_send_removed(struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofputil_flow_removed fr;\n    long long int used;\n\n    minimatch_expand(&rule->cr.match, &fr.match);\n    fr.priority = rule->cr.priority;\n\n    /* Synchronize with connmgr_destroy() calls to prevent connmgr disappearing\n     * while we use it. */\n    ovs_mutex_lock(&ofproto_mutex);\n    struct connmgr *connmgr = rule->ofproto->connmgr;\n    if (!connmgr) {\n        ovs_mutex_unlock(&ofproto_mutex);\n        return;\n    }\n\n    fr.cookie = rule->flow_cookie;\n    fr.reason = rule->removed_reason;\n    fr.table_id = rule->table_id;\n    calc_duration(rule->created, time_msec(),\n                  &fr.duration_sec, &fr.duration_nsec);\n    ovs_mutex_lock(&rule->mutex);\n    fr.idle_timeout = rule->idle_timeout;\n    fr.hard_timeout = rule->hard_timeout;\n    ovs_mutex_unlock(&rule->mutex);\n    rule->ofproto->ofproto_class->rule_get_stats(rule, &fr.packet_count,\n                                                 &fr.byte_count, &used);\n    connmgr_send_flow_removed(connmgr, &fr);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\n/* Sends an OpenFlow \"flow removed\" message with the given 'reason' (either\n * OFPRR_HARD_TIMEOUT or OFPRR_IDLE_TIMEOUT), and then removes 'rule' from its\n * ofproto.\n *\n * ofproto implementation ->run() functions should use this function to expire\n * OpenFlow flows. */\nvoid\nofproto_rule_expire(struct rule *rule, uint8_t reason)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule_collection rules;\n\n    rule_collection_init(&rules);\n    rule_collection_add(&rules, rule);\n    delete_flows__(&rules, reason, NULL);\n}\n\n/* Reduces '*timeout' to no more than 'max'.  A value of zero in either case\n * means \"infinite\". */\nstatic void\nreduce_timeout(uint16_t max, uint16_t *timeout)\n{\n    if (max && (!*timeout || *timeout > max)) {\n        *timeout = max;\n    }\n}\n\n/* If 'idle_timeout' is nonzero, and 'rule' has no idle timeout or an idle\n * timeout greater than 'idle_timeout', lowers 'rule''s idle timeout to\n * 'idle_timeout' seconds.  Similarly for 'hard_timeout'.\n *\n * Suitable for implementing OFPACT_FIN_TIMEOUT. */\nvoid\nofproto_rule_reduce_timeouts__(struct rule *rule,\n                               uint16_t idle_timeout, uint16_t hard_timeout)\n    OVS_REQUIRES(ofproto_mutex)\n    OVS_EXCLUDED(rule->mutex)\n{\n    if (!idle_timeout && !hard_timeout) {\n        return;\n    }\n\n    if (ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_insert(&rule->ofproto->expirable, &rule->expirable);\n    }\n\n    ovs_mutex_lock(&rule->mutex);\n    reduce_timeout(idle_timeout, &rule->idle_timeout);\n    reduce_timeout(hard_timeout, &rule->hard_timeout);\n    ovs_mutex_unlock(&rule->mutex);\n}\n\nvoid\nofproto_rule_reduce_timeouts(struct rule *rule,\n                             uint16_t idle_timeout, uint16_t hard_timeout)\n    OVS_EXCLUDED(ofproto_mutex, rule->mutex)\n{\n    if (!idle_timeout && !hard_timeout) {\n        return;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    if (ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_insert(&rule->ofproto->expirable, &rule->expirable);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ovs_mutex_lock(&rule->mutex);\n    reduce_timeout(idle_timeout, &rule->idle_timeout);\n    reduce_timeout(hard_timeout, &rule->hard_timeout);\n    ovs_mutex_unlock(&rule->mutex);\n}\n\f\nstatic enum ofperr\nhandle_flow_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_flow_mod fm;\n    uint64_t ofpacts_stub[1024 / 8];\n    struct ofpbuf ofpacts;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n    error = ofputil_decode_flow_mod(&fm, oh, ofconn_get_protocol(ofconn),\n                                    ofproto_get_tun_tab(ofproto),\n                                    &ofproto->vl_mff_map, &ofpacts,\n                                    u16_to_ofp(ofproto->max_ports),\n                                    ofproto->n_tables);\n    if (!error) {\n        struct openflow_mod_requester req = { ofconn, oh };\n        error = handle_flow_mod__(ofproto, &fm, &req);\n    }\n\n    ofpbuf_uninit(&ofpacts);\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_mod__(struct ofproto *ofproto, const struct ofputil_flow_mod *fm,\n                  const struct openflow_mod_requester *req)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto_flow_mod ofm;\n    enum ofperr error;\n\n    error = ofproto_flow_mod_init(ofproto, &ofm, fm, NULL);\n    if (error) {\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ofm.version = ofproto->tables_version + 1;\n    error = ofproto_flow_mod_start(ofproto, &ofm);\n    if (!error) {\n        ofproto_bump_tables_version(ofproto);\n        ofproto_flow_mod_finish(ofproto, &ofm, req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_role_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_role_request request;\n    struct ofputil_role_request reply;\n    struct ofpbuf *buf;\n    enum ofperr error;\n\n    error = ofputil_decode_role_message(oh, &request);\n    if (error) {\n        return error;\n    }\n\n    if (request.role != OFPCR12_ROLE_NOCHANGE) {\n        if (request.role != OFPCR12_ROLE_EQUAL\n            && request.have_generation_id\n            && !ofconn_set_master_election_id(ofconn, request.generation_id)) {\n                return OFPERR_OFPRRFC_STALE;\n        }\n\n        ofconn_set_role(ofconn, request.role);\n    }\n\n    reply.role = ofconn_get_role(ofconn);\n    reply.have_generation_id = ofconn_get_master_election_id(\n        ofconn, &reply.generation_id);\n    buf = ofputil_encode_role_reply(oh, &reply);\n    ofconn_send_reply(ofconn, buf);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_flow_mod_table_id(struct ofconn *ofconn,\n                             const struct ofp_header *oh)\n{\n    const struct nx_flow_mod_table_id *msg = ofpmsg_body(oh);\n    enum ofputil_protocol cur, next;\n\n    cur = ofconn_get_protocol(ofconn);\n    next = ofputil_protocol_set_tid(cur, msg->set != 0);\n    ofconn_set_protocol(ofconn, next);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_flow_format(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    const struct nx_set_flow_format *msg = ofpmsg_body(oh);\n    enum ofputil_protocol cur, next;\n    enum ofputil_protocol next_base;\n\n    next_base = ofputil_nx_flow_format_to_protocol(ntohl(msg->format));\n    if (!next_base) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    cur = ofconn_get_protocol(ofconn);\n    next = ofputil_protocol_set_base(cur, next_base);\n    ofconn_set_protocol(ofconn, next);\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_packet_in_format(struct ofconn *ofconn,\n                                const struct ofp_header *oh)\n{\n    const struct nx_set_packet_in_format *msg = ofpmsg_body(oh);\n    uint32_t format;\n\n    format = ntohl(msg->format);\n    if (!ofputil_packet_in_format_is_valid(format)) {\n        return OFPERR_OFPBRC_EPERM;\n    }\n\n    ofconn_set_packet_in_format(ofconn, format);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_async_config(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_async_cfg basis = ofconn_get_async_config(ofconn);\n    struct ofputil_async_cfg ac;\n    enum ofperr error;\n\n    error = ofputil_decode_set_async_config(oh, false, &basis, &ac);\n    if (error) {\n        return error;\n    }\n\n    ofconn_set_async_config(ofconn, &ac);\n    if (ofconn_get_type(ofconn) == OFCONN_SERVICE &&\n        !ofconn_get_miss_send_len(ofconn)) {\n        ofconn_set_miss_send_len(ofconn, OFP_DEFAULT_MISS_SEND_LEN);\n    }\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_get_async_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_async_cfg ac = ofconn_get_async_config(ofconn);\n    ofconn_send_reply(ofconn, ofputil_encode_get_async_reply(oh, &ac));\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_nxt_set_controller_id(struct ofconn *ofconn,\n                             const struct ofp_header *oh)\n{\n    const struct nx_controller_id *nci = ofpmsg_body(oh);\n\n    if (!is_all_zeros(nci->zero, sizeof nci->zero)) {\n        return OFPERR_NXBRC_MUST_BE_ZERO;\n    }\n\n    ofconn_set_controller_id(ofconn, ntohs(nci->controller_id));\n    return 0;\n}\n\nstatic enum ofperr\nhandle_barrier_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofpbuf *buf;\n\n    buf = ofpraw_alloc_reply((oh->version == OFP10_VERSION\n                              ? OFPRAW_OFPT10_BARRIER_REPLY\n                              : OFPRAW_OFPT11_BARRIER_REPLY), oh, 0);\n    ofconn_send_reply(ofconn, buf);\n    return 0;\n}\n\nstatic void\nofproto_compose_flow_refresh_update(const struct rule *rule,\n                                    enum nx_flow_monitor_flags flags,\n                                    struct ovs_list *msgs,\n                                    const struct tun_table *tun_table)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct rule_actions *actions;\n    struct ofputil_flow_update fu;\n\n    fu.event = (flags & (NXFMF_INITIAL | NXFMF_ADD)\n                ? NXFME_ADDED : NXFME_MODIFIED);\n    fu.reason = 0;\n    ovs_mutex_lock(&rule->mutex);\n    fu.idle_timeout = rule->idle_timeout;\n    fu.hard_timeout = rule->hard_timeout;\n    ovs_mutex_unlock(&rule->mutex);\n    fu.table_id = rule->table_id;\n    fu.cookie = rule->flow_cookie;\n    minimatch_expand(&rule->cr.match, &fu.match);\n    fu.priority = rule->cr.priority;\n\n    actions = flags & NXFMF_ACTIONS ? rule_get_actions(rule) : NULL;\n    fu.ofpacts = actions ? actions->ofpacts : NULL;\n    fu.ofpacts_len = actions ? actions->ofpacts_len : 0;\n\n    if (ovs_list_is_empty(msgs)) {\n        ofputil_start_flow_update(msgs);\n    }\n    ofputil_append_flow_update(&fu, msgs, tun_table);\n}\n\nvoid\nofmonitor_compose_refresh_updates(struct rule_collection *rules,\n                                  struct ovs_list *msgs)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    RULE_COLLECTION_FOR_EACH (rule, rules) {\n        enum nx_flow_monitor_flags flags = rule->monitor_flags;\n        rule->monitor_flags = 0;\n\n        ofproto_compose_flow_refresh_update(rule, flags, msgs,\n                ofproto_get_tun_tab(rule->ofproto));\n    }\n}\n\nstatic void\nofproto_collect_ofmonitor_refresh_rule(const struct ofmonitor *m,\n                                       struct rule *rule, uint64_t seqno,\n                                       struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum nx_flow_monitor_flags update;\n\n    if (rule_is_hidden(rule)) {\n        return;\n    }\n\n    if (!ofproto_rule_has_out_port(rule, m->out_port)) {\n        return;\n    }\n\n    if (seqno) {\n        if (rule->add_seqno > seqno) {\n            update = NXFMF_ADD | NXFMF_MODIFY;\n        } else if (rule->modify_seqno > seqno) {\n            update = NXFMF_MODIFY;\n        } else {\n            return;\n        }\n\n        if (!(m->flags & update)) {\n            return;\n        }\n    } else {\n        update = NXFMF_INITIAL;\n    }\n\n    if (!rule->monitor_flags) {\n        rule_collection_add(rules, rule);\n    }\n    rule->monitor_flags |= update | (m->flags & NXFMF_ACTIONS);\n}\n\nstatic void\nofproto_collect_ofmonitor_refresh_rules(const struct ofmonitor *m,\n                                        uint64_t seqno,\n                                        struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct ofproto *ofproto = ofconn_get_ofproto(m->ofconn);\n    const struct oftable *table;\n    struct cls_rule target;\n\n    cls_rule_init_from_minimatch(&target, &m->match, 0);\n    FOR_EACH_MATCHING_TABLE (table, m->table_id, ofproto) {\n        struct rule *rule;\n\n        CLS_FOR_EACH_TARGET (rule, cr, &table->cls, &target, OVS_VERSION_MAX) {\n            ofproto_collect_ofmonitor_refresh_rule(m, rule, seqno, rules);\n        }\n    }\n    cls_rule_destroy(&target);\n}\n\nstatic void\nofproto_collect_ofmonitor_initial_rules(struct ofmonitor *m,\n                                        struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (m->flags & NXFMF_INITIAL) {\n        ofproto_collect_ofmonitor_refresh_rules(m, 0, rules);\n    }\n}\n\nvoid\nofmonitor_collect_resume_rules(struct ofmonitor *m,\n                               uint64_t seqno, struct rule_collection *rules)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ofproto_collect_ofmonitor_refresh_rules(m, seqno, rules);\n}\n\nstatic enum ofperr\nflow_monitor_delete(struct ofconn *ofconn, uint32_t id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofmonitor *m;\n    enum ofperr error;\n\n    m = ofmonitor_lookup(ofconn, id);\n    if (m) {\n        ofmonitor_destroy(m);\n        error = 0;\n    } else {\n        error = OFPERR_OFPMOFC_UNKNOWN_MONITOR;\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_monitor_request(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n\n    struct ofpbuf b = ofpbuf_const_initializer(oh, ntohs(oh->length));\n\n    struct ofmonitor **monitors = NULL;\n    size_t allocated_monitors = 0;\n    size_t n_monitors = 0;\n\n    enum ofperr error;\n\n    ovs_mutex_lock(&ofproto_mutex);\n    for (;;) {\n        struct ofputil_flow_monitor_request request;\n        struct ofmonitor *m;\n        int retval;\n\n        retval = ofputil_decode_flow_monitor_request(&request, &b);\n        if (retval == EOF) {\n            break;\n        } else if (retval) {\n            error = retval;\n            goto error;\n        }\n\n        if (request.table_id != 0xff\n            && request.table_id >= ofproto->n_tables) {\n            error = OFPERR_OFPBRC_BAD_TABLE_ID;\n            goto error;\n        }\n\n        error = ofmonitor_create(&request, ofconn, &m);\n        if (error) {\n            goto error;\n        }\n\n        if (n_monitors >= allocated_monitors) {\n            monitors = x2nrealloc(monitors, &allocated_monitors,\n                                  sizeof *monitors);\n        }\n        monitors[n_monitors++] = m;\n    }\n\n    struct rule_collection rules;\n    rule_collection_init(&rules);\n    for (size_t i = 0; i < n_monitors; i++) {\n        ofproto_collect_ofmonitor_initial_rules(monitors[i], &rules);\n    }\n\n    struct ovs_list replies;\n    ofpmp_init(&replies, oh);\n    ofmonitor_compose_refresh_updates(&rules, &replies);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    rule_collection_destroy(&rules);\n\n    ofconn_send_replies(ofconn, &replies);\n    free(monitors);\n\n    return 0;\n\nerror:\n    for (size_t i = 0; i < n_monitors; i++) {\n        ofmonitor_destroy(monitors[i]);\n    }\n    free(monitors);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_flow_monitor_cancel(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error;\n    uint32_t id;\n\n    id = ofputil_decode_flow_monitor_cancel(oh);\n\n    ovs_mutex_lock(&ofproto_mutex);\n    error = flow_monitor_delete(ofconn, id);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\n/* Meters implementation.\n *\n * Meter table entry, indexed by the OpenFlow meter_id.\n * 'created' is used to compute the duration for meter stats.\n * 'list rules' is needed so that we can delete the dependent rules when the\n * meter table entry is deleted.\n * 'provider_meter_id' is for the provider's private use.\n */\nstruct meter {\n    long long int created;      /* Time created. */\n    struct ovs_list rules;      /* List of \"struct rule_dpif\"s. */\n    ofproto_meter_id provider_meter_id;\n    uint16_t flags;             /* Meter flags. */\n    uint16_t n_bands;           /* Number of meter bands. */\n    struct ofputil_meter_band *bands;\n};\n\n/*\n * This is used in instruction validation at flow set-up time,\n * as flows may not use non-existing meters.\n * Return value of UINT32_MAX signifies an invalid meter.\n */\nstatic uint32_t\nget_provider_meter_id(const struct ofproto *ofproto, uint32_t of_meter_id)\n{\n    if (of_meter_id && of_meter_id <= ofproto->meter_features.max_meters) {\n        const struct meter *meter = ofproto->meters[of_meter_id];\n        if (meter) {\n            return meter->provider_meter_id.uint32;\n        }\n    }\n    return UINT32_MAX;\n}\n\n/* Finds the meter invoked by 'rule''s actions and adds 'rule' to the meter's\n * list of rules. */\nstatic void\nmeter_insert_rule(struct rule *rule)\n{\n    const struct rule_actions *a = rule_get_actions(rule);\n    uint32_t meter_id = ofpacts_get_meter(a->ofpacts, a->ofpacts_len);\n    struct meter *meter = rule->ofproto->meters[meter_id];\n\n    ovs_list_insert(&meter->rules, &rule->meter_list_node);\n}\n\nstatic void\nmeter_update(struct meter *meter, const struct ofputil_meter_config *config)\n{\n    free(meter->bands);\n\n    meter->flags = config->flags;\n    meter->n_bands = config->n_bands;\n    meter->bands = xmemdup(config->bands,\n                           config->n_bands * sizeof *meter->bands);\n}\n\nstatic struct meter *\nmeter_create(const struct ofputil_meter_config *config,\n             ofproto_meter_id provider_meter_id)\n{\n    struct meter *meter;\n\n    meter = xzalloc(sizeof *meter);\n    meter->provider_meter_id = provider_meter_id;\n    meter->created = time_msec();\n    ovs_list_init(&meter->rules);\n\n    meter_update(meter, config);\n\n    return meter;\n}\n\nstatic void\nmeter_delete(struct ofproto *ofproto, uint32_t first, uint32_t last)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    for (uint32_t mid = first; mid <= last; ++mid) {\n        struct meter *meter = ofproto->meters[mid];\n        if (meter) {\n            /* First delete the rules that use this meter. */\n            if (!ovs_list_is_empty(&meter->rules)) {\n                struct rule_collection rules;\n                struct rule *rule;\n\n                rule_collection_init(&rules);\n\n                LIST_FOR_EACH (rule, meter_list_node, &meter->rules) {\n                    rule_collection_add(&rules, rule);\n                }\n                delete_flows__(&rules, OFPRR_METER_DELETE, NULL);\n            }\n\n            ofproto->meters[mid] = NULL;\n            ofproto->ofproto_class->meter_del(ofproto,\n                                              meter->provider_meter_id);\n            free(meter->bands);\n            free(meter);\n        }\n    }\n}\n\nstatic enum ofperr\nhandle_add_meter(struct ofproto *ofproto, struct ofputil_meter_mod *mm)\n{\n    ofproto_meter_id provider_meter_id = { UINT32_MAX };\n    struct meter **meterp = &ofproto->meters[mm->meter.meter_id];\n    enum ofperr error;\n\n    if (*meterp) {\n        return OFPERR_OFPMMFC_METER_EXISTS;\n    }\n\n    error = ofproto->ofproto_class->meter_set(ofproto, &provider_meter_id,\n                                              &mm->meter);\n    if (!error) {\n        ovs_assert(provider_meter_id.uint32 != UINT32_MAX);\n        *meterp = meter_create(&mm->meter, provider_meter_id);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_modify_meter(struct ofproto *ofproto, struct ofputil_meter_mod *mm)\n{\n    struct meter *meter = ofproto->meters[mm->meter.meter_id];\n    enum ofperr error;\n    uint32_t provider_meter_id;\n\n    if (!meter) {\n        return OFPERR_OFPMMFC_UNKNOWN_METER;\n    }\n\n    provider_meter_id = meter->provider_meter_id.uint32;\n    error = ofproto->ofproto_class->meter_set(ofproto,\n                                              &meter->provider_meter_id,\n                                              &mm->meter);\n    ovs_assert(meter->provider_meter_id.uint32 == provider_meter_id);\n    if (!error) {\n        meter_update(meter, &mm->meter);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_delete_meter(struct ofconn *ofconn, struct ofputil_meter_mod *mm)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    uint32_t meter_id = mm->meter.meter_id;\n    enum ofperr error = 0;\n    uint32_t first, last;\n\n    if (meter_id == OFPM13_ALL) {\n        first = 1;\n        last = ofproto->meter_features.max_meters;\n    } else {\n        if (!meter_id || meter_id > ofproto->meter_features.max_meters) {\n            return 0;\n        }\n        first = last = meter_id;\n    }\n\n    /* Delete the meters. */\n    ovs_mutex_lock(&ofproto_mutex);\n    meter_delete(ofproto, first, last);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_meter_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_meter_mod mm;\n    uint64_t bands_stub[256 / 8];\n    struct ofpbuf bands;\n    uint32_t meter_id;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    ofpbuf_use_stub(&bands, bands_stub, sizeof bands_stub);\n\n    error = ofputil_decode_meter_mod(oh, &mm, &bands);\n    if (error) {\n        goto exit_free_bands;\n    }\n\n    meter_id = mm.meter.meter_id;\n\n    if (mm.command != OFPMC13_DELETE) {\n        /* Fails also when meters are not implemented by the provider. */\n        if (meter_id == 0 || meter_id > OFPM13_MAX) {\n            error = OFPERR_OFPMMFC_INVALID_METER;\n            goto exit_free_bands;\n        } else if (meter_id > ofproto->meter_features.max_meters) {\n            error = OFPERR_OFPMMFC_OUT_OF_METERS;\n            goto exit_free_bands;\n        }\n        if (mm.meter.n_bands > ofproto->meter_features.max_bands) {\n            error = OFPERR_OFPMMFC_OUT_OF_BANDS;\n            goto exit_free_bands;\n        }\n    }\n\n    switch (mm.command) {\n    case OFPMC13_ADD:\n        error = handle_add_meter(ofproto, &mm);\n        break;\n\n    case OFPMC13_MODIFY:\n        error = handle_modify_meter(ofproto, &mm);\n        break;\n\n    case OFPMC13_DELETE:\n        error = handle_delete_meter(ofconn, &mm);\n        break;\n\n    default:\n        error = OFPERR_OFPMMFC_BAD_COMMAND;\n        break;\n    }\n\n    if (!error) {\n        struct ofputil_requestforward rf;\n        rf.xid = oh->xid;\n        rf.reason = OFPRFR_METER_MOD;\n        rf.meter_mod = &mm;\n        connmgr_send_requestforward(ofproto->connmgr, ofconn, &rf);\n    }\n\nexit_free_bands:\n    ofpbuf_uninit(&bands);\n    return error;\n}\n\nstatic enum ofperr\nhandle_meter_features_request(struct ofconn *ofconn,\n                              const struct ofp_header *request)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_meter_features features;\n    struct ofpbuf *b;\n\n    if (ofproto->ofproto_class->meter_get_features) {\n        ofproto->ofproto_class->meter_get_features(ofproto, &features);\n    } else {\n        memset(&features, 0, sizeof features);\n    }\n    b = ofputil_encode_meter_features_reply(&features, request);\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_meter_request(struct ofconn *ofconn, const struct ofp_header *request,\n                     enum ofptype type)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    uint64_t bands_stub[256 / 8];\n    struct ofpbuf bands;\n    uint32_t meter_id, first, last;\n\n    ofputil_decode_meter_request(request, &meter_id);\n\n    if (meter_id == OFPM13_ALL) {\n        first = 1;\n        last = ofproto->meter_features.max_meters;\n    } else {\n        if (!meter_id || meter_id > ofproto->meter_features.max_meters ||\n            !ofproto->meters[meter_id]) {\n            return OFPERR_OFPMMFC_UNKNOWN_METER;\n        }\n        first = last = meter_id;\n    }\n\n    ofpbuf_use_stub(&bands, bands_stub, sizeof bands_stub);\n    ofpmp_init(&replies, request);\n\n    for (meter_id = first; meter_id <= last; ++meter_id) {\n        struct meter *meter = ofproto->meters[meter_id];\n        if (!meter) {\n            continue; /* Skip non-existing meters. */\n        }\n        if (type == OFPTYPE_METER_STATS_REQUEST) {\n            struct ofputil_meter_stats stats;\n\n            stats.meter_id = meter_id;\n\n            /* Provider sets the packet and byte counts, we do the rest. */\n            stats.flow_count = ovs_list_size(&meter->rules);\n            calc_duration(meter->created, time_msec(),\n                          &stats.duration_sec, &stats.duration_nsec);\n            stats.n_bands = meter->n_bands;\n            ofpbuf_clear(&bands);\n            stats.bands\n                = ofpbuf_put_uninit(&bands,\n                                    meter->n_bands * sizeof *stats.bands);\n\n            if (!ofproto->ofproto_class->meter_get(ofproto,\n                                                   meter->provider_meter_id,\n                                                   &stats)) {\n                ofputil_append_meter_stats(&replies, &stats);\n            }\n        } else { /* type == OFPTYPE_METER_CONFIG_REQUEST */\n            struct ofputil_meter_config config;\n\n            config.meter_id = meter_id;\n            config.flags = meter->flags;\n            config.n_bands = meter->n_bands;\n            config.bands = meter->bands;\n            ofputil_append_meter_config(&replies, &config);\n        }\n    }\n\n    ofconn_send_replies(ofconn, &replies);\n    ofpbuf_uninit(&bands);\n    return 0;\n}\n\n/* Returned group is RCU protected. */\nstatic struct ofgroup *\nofproto_group_lookup__(const struct ofproto *ofproto, uint32_t group_id,\n                       ovs_version_t version)\n{\n    struct ofgroup *group;\n\n    CMAP_FOR_EACH_WITH_HASH (group, cmap_node, hash_int(group_id, 0),\n                             &ofproto->groups) {\n        if (group->group_id == group_id\n            && versions_visible_in_version(&group->versions, version)) {\n            return group;\n        }\n    }\n\n    return NULL;\n}\n\n/* If the group exists, this function increments the groups's reference count.\n *\n * Make sure to call ofproto_group_unref() after no longer needing to maintain\n * a reference to the group. */\nstruct ofgroup *\nofproto_group_lookup(const struct ofproto *ofproto, uint32_t group_id,\n                     ovs_version_t version, bool take_ref)\n{\n    struct ofgroup *group;\n\n    group = ofproto_group_lookup__(ofproto, group_id, version);\n    if (group && take_ref) {\n        /* Not holding a lock, so it is possible that another thread releases\n         * the last reference just before we manage to get one. */\n        return ofproto_group_try_ref(group) ? group : NULL;\n    }\n    return group;\n}\n\n/* Caller should hold 'ofproto_mutex' if it is important that the\n * group is not removed by someone else. */\nstatic bool\nofproto_group_exists(const struct ofproto *ofproto, uint32_t group_id)\n{\n    return ofproto_group_lookup__(ofproto, group_id, OVS_VERSION_MAX) != NULL;\n}\n\nstatic void\ngroup_add_rule(struct ofgroup *group, struct rule *rule)\n{\n    rule_collection_add(&group->rules, rule);\n}\n\nstatic void\ngroup_remove_rule(struct ofgroup *group, struct rule *rule)\n{\n    rule_collection_remove(&group->rules, rule);\n}\n\nstatic void\nappend_group_stats(struct ofgroup *group, struct ovs_list *replies)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofputil_group_stats ogs;\n    const struct ofproto *ofproto = group->ofproto;\n    long long int now = time_msec();\n    int error;\n\n    ogs.bucket_stats = xmalloc(group->n_buckets * sizeof *ogs.bucket_stats);\n\n    /* Provider sets the packet and byte counts, we do the rest. */\n    ogs.ref_count = rule_collection_n(&group->rules);\n    ogs.n_buckets = group->n_buckets;\n\n    error = (ofproto->ofproto_class->group_get_stats\n             ? ofproto->ofproto_class->group_get_stats(group, &ogs)\n             : EOPNOTSUPP);\n    if (error) {\n        ogs.packet_count = UINT64_MAX;\n        ogs.byte_count = UINT64_MAX;\n        memset(ogs.bucket_stats, 0xff,\n               ogs.n_buckets * sizeof *ogs.bucket_stats);\n    }\n\n    ogs.group_id = group->group_id;\n    calc_duration(group->created, now, &ogs.duration_sec, &ogs.duration_nsec);\n\n    ofputil_append_group_stats(replies, &ogs);\n\n    free(ogs.bucket_stats);\n}\n\nstatic void\nhandle_group_request(struct ofconn *ofconn,\n                     const struct ofp_header *request, uint32_t group_id,\n                     void (*cb)(struct ofgroup *, struct ovs_list *replies))\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofgroup *group;\n    struct ovs_list replies;\n\n    ofpmp_init(&replies, request);\n    /* Must exclude modifications to guarantee iterating groups. */\n    ovs_mutex_lock(&ofproto_mutex);\n    if (group_id == OFPG_ALL) {\n        CMAP_FOR_EACH (group, cmap_node, &ofproto->groups) {\n            if (versions_visible_in_version(&group->versions,\n                                            OVS_VERSION_MAX)) {\n                cb(group, &replies);\n            }\n        }\n    } else {\n        group = ofproto_group_lookup__(ofproto, group_id, OVS_VERSION_MAX);\n        if (group) {\n            cb(group, &replies);\n        }\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n    ofconn_send_replies(ofconn, &replies);\n}\n\nstatic enum ofperr\nhandle_group_stats_request(struct ofconn *ofconn,\n                           const struct ofp_header *request)\n{\n    uint32_t group_id;\n    enum ofperr error;\n\n    error = ofputil_decode_group_stats_request(request, &group_id);\n    if (error) {\n        return error;\n    }\n\n    handle_group_request(ofconn, request, group_id, append_group_stats);\n    return 0;\n}\n\nstatic void\nappend_group_desc(struct ofgroup *group, struct ovs_list *replies)\n{\n    struct ofputil_group_desc gds;\n\n    gds.group_id = group->group_id;\n    gds.type = group->type;\n    gds.props = group->props;\n\n    ofputil_append_group_desc_reply(&gds, &group->buckets, replies);\n}\n\nstatic enum ofperr\nhandle_group_desc_stats_request(struct ofconn *ofconn,\n                                const struct ofp_header *request)\n{\n    handle_group_request(ofconn, request,\n                         ofputil_decode_group_desc_request(request),\n                         append_group_desc);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_group_features_stats_request(struct ofconn *ofconn,\n                                    const struct ofp_header *request)\n{\n    struct ofproto *p = ofconn_get_ofproto(ofconn);\n    struct ofpbuf *msg;\n\n    msg = ofputil_encode_group_features_reply(&p->ogf, request);\n    if (msg) {\n        ofconn_send_reply(ofconn, msg);\n    }\n\n    return 0;\n}\n\nstatic void\nput_queue_get_config_reply(struct ofport *port, uint32_t queue,\n                           struct ovs_list *replies)\n{\n    struct ofputil_queue_config qc;\n\n    /* None of the existing queues have compatible properties, so we hard-code\n     * omitting min_rate and max_rate. */\n    qc.port = port->ofp_port;\n    qc.queue = queue;\n    qc.min_rate = UINT16_MAX;\n    qc.max_rate = UINT16_MAX;\n    ofputil_append_queue_get_config_reply(&qc, replies);\n}\n\nstatic int\nhandle_queue_get_config_request_for_port(struct ofport *port, uint32_t queue,\n                                         struct ovs_list *replies)\n{\n    struct smap details = SMAP_INITIALIZER(&details);\n    if (queue != OFPQ_ALL) {\n        int error = netdev_get_queue(port->netdev, queue, &details);\n        switch (error) {\n        case 0:\n            put_queue_get_config_reply(port, queue, replies);\n            break;\n        case EOPNOTSUPP:\n        case EINVAL:\n            return OFPERR_OFPQOFC_BAD_QUEUE;\n        default:\n            return OFPERR_NXQOFC_QUEUE_ERROR;\n        }\n    } else {\n        struct netdev_queue_dump queue_dump;\n        uint32_t queue_id;\n\n        NETDEV_QUEUE_FOR_EACH (&queue_id, &details, &queue_dump,\n                               port->netdev) {\n            put_queue_get_config_reply(port, queue_id, replies);\n        }\n    }\n    smap_destroy(&details);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_queue_get_config_request(struct ofconn *ofconn,\n                                const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ovs_list replies;\n    struct ofport *port;\n    ofp_port_t req_port;\n    uint32_t req_queue;\n    enum ofperr error;\n\n    error = ofputil_decode_queue_get_config_request(oh, &req_port, &req_queue);\n    if (error) {\n        return error;\n    }\n\n    ofputil_start_queue_get_config_reply(oh, &replies);\n    if (req_port == OFPP_ANY) {\n        error = OFPERR_OFPQOFC_BAD_QUEUE;\n        HMAP_FOR_EACH (port, hmap_node, &ofproto->ports) {\n            if (!handle_queue_get_config_request_for_port(port, req_queue,\n                                                          &replies)) {\n                error = 0;\n            }\n        }\n    } else {\n        port = ofproto_get_port(ofproto, req_port);\n        error = (port\n                 ? handle_queue_get_config_request_for_port(port, req_queue,\n                                                            &replies)\n                 : OFPERR_OFPQOFC_BAD_PORT);\n    }\n    if (!error) {\n        ofconn_send_replies(ofconn, &replies);\n    } else {\n        ofpbuf_list_delete(&replies);\n    }\n\n    return error;\n}\n\n/* Allocates, initializes, and constructs a new group in 'ofproto', obtaining\n * all the attributes for it from 'gm', and stores a pointer to it in\n * '*ofgroup'.  Makes the new group visible from the flow table starting from\n * 'version'.\n *\n * Returns 0 if successful, otherwise an error code.  If there is an error then\n * '*ofgroup' is indeterminate upon return. */\nstatic enum ofperr\ninit_group(struct ofproto *ofproto, const struct ofputil_group_mod *gm,\n           ovs_version_t version, struct ofgroup **ofgroup)\n{\n    enum ofperr error;\n    const long long int now = time_msec();\n\n    if (gm->group_id > OFPG_MAX) {\n        return OFPERR_OFPGMFC_INVALID_GROUP;\n    }\n    if (gm->type > OFPGT11_FF) {\n        return OFPERR_OFPGMFC_BAD_TYPE;\n    }\n\n    *ofgroup = ofproto->ofproto_class->group_alloc();\n    if (!*ofgroup) {\n        VLOG_WARN_RL(&rl, \"%s: failed to allocate group\", ofproto->name);\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    *CONST_CAST(struct ofproto **, &(*ofgroup)->ofproto) = ofproto;\n    *CONST_CAST(uint32_t *, &((*ofgroup)->group_id)) = gm->group_id;\n    *CONST_CAST(enum ofp11_group_type *, &(*ofgroup)->type) = gm->type;\n    *CONST_CAST(long long int *, &((*ofgroup)->created)) = now;\n    *CONST_CAST(long long int *, &((*ofgroup)->modified)) = now;\n    ovs_refcount_init(&(*ofgroup)->ref_count);\n    (*ofgroup)->being_deleted = false;\n\n    ovs_list_init(CONST_CAST(struct ovs_list *, &(*ofgroup)->buckets));\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &(*ofgroup)->buckets),\n                              &gm->buckets, NULL);\n\n    *CONST_CAST(uint32_t *, &(*ofgroup)->n_buckets) =\n        ovs_list_size(&(*ofgroup)->buckets);\n\n    ofputil_group_properties_copy(CONST_CAST(struct ofputil_group_props *,\n                                             &(*ofgroup)->props),\n                                  &gm->props);\n    rule_collection_init(&(*ofgroup)->rules);\n\n    /* Make group visible from 'version'. */\n    (*ofgroup)->versions = VERSIONS_INITIALIZER(version,\n                                                OVS_VERSION_NOT_REMOVED);\n\n    /* Construct called BEFORE any locks are held. */\n    error = ofproto->ofproto_class->group_construct(*ofgroup);\n    if (error) {\n        ofputil_group_properties_destroy(CONST_CAST(struct ofputil_group_props *,\n                                                    &(*ofgroup)->props));\n        ofputil_bucket_list_destroy(CONST_CAST(struct ovs_list *,\n                                               &(*ofgroup)->buckets));\n        ofproto->ofproto_class->group_dealloc(*ofgroup);\n    }\n    return error;\n}\n\n/* Implements the OFPGC11_ADD operation specified by 'gm', adding a group to\n * 'ofproto''s group table.  Returns 0 on success or an OpenFlow error code on\n * failure. */\nstatic enum ofperr\nadd_group_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    if (ofproto_group_exists(ofproto, ogm->gm.group_id)) {\n        return OFPERR_OFPGMFC_GROUP_EXISTS;\n    }\n\n    if (ofproto->n_groups[ogm->gm.type]\n        >= ofproto->ogf.max_groups[ogm->gm.type]) {\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    /* Allocate new group and initialize it. */\n    error = init_group(ofproto, &ogm->gm, ogm->version, &ogm->new_group);\n    if (!error) {\n        /* Insert new group. */\n        cmap_insert(&ofproto->groups, &ogm->new_group->cmap_node,\n                    hash_int(ogm->new_group->group_id, 0));\n        ofproto->n_groups[ogm->new_group->type]++;\n    }\n    return error;\n}\n\n/* Adds all of the buckets from 'ofgroup' to 'new_ofgroup'.  The buckets\n * already in 'new_ofgroup' will be placed just after the (copy of the) bucket\n * in 'ofgroup' with bucket ID 'command_bucket_id'.  Special\n * 'command_bucket_id' values OFPG15_BUCKET_FIRST and OFPG15_BUCKET_LAST are\n * also honored. */\nstatic enum ofperr\ncopy_buckets_for_insert_bucket(const struct ofgroup *ofgroup,\n                               struct ofgroup *new_ofgroup,\n                               uint32_t command_bucket_id)\n{\n    struct ofputil_bucket *last = NULL;\n\n    if (command_bucket_id <= OFPG15_BUCKET_MAX) {\n        /* Check here to ensure that a bucket corresponding to\n         * command_bucket_id exists in the old bucket list.\n         *\n         * The subsequent search of below of new_ofgroup covers\n         * both buckets in the old bucket list and buckets added\n         * by the insert buckets group mod message this function processes. */\n        if (!ofputil_bucket_find(&ofgroup->buckets, command_bucket_id)) {\n            return OFPERR_OFPGMFC_UNKNOWN_BUCKET;\n        }\n\n        if (!ovs_list_is_empty(&new_ofgroup->buckets)) {\n            last = ofputil_bucket_list_back(&new_ofgroup->buckets);\n        }\n    }\n\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &new_ofgroup->buckets),\n                              &ofgroup->buckets, NULL);\n\n    if (ofputil_bucket_check_duplicate_id(&new_ofgroup->buckets)) {\n            VLOG_INFO_RL(&rl, \"Duplicate bucket id\");\n            return OFPERR_OFPGMFC_BUCKET_EXISTS;\n    }\n\n    /* Rearrange list according to command_bucket_id */\n    if (command_bucket_id == OFPG15_BUCKET_LAST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            struct ofputil_bucket *new_first;\n            const struct ofputil_bucket *first;\n\n            first = ofputil_bucket_list_front(&ofgroup->buckets);\n            new_first = ofputil_bucket_find(&new_ofgroup->buckets,\n                                            first->bucket_id);\n\n            ovs_list_splice(new_ofgroup->buckets.next, &new_first->list_node,\n                            CONST_CAST(struct ovs_list *,\n                                       &new_ofgroup->buckets));\n        }\n    } else if (command_bucket_id <= OFPG15_BUCKET_MAX && last) {\n        struct ofputil_bucket *after;\n\n        /* Presence of bucket is checked above so after should never be NULL */\n        after = ofputil_bucket_find(&new_ofgroup->buckets, command_bucket_id);\n\n        ovs_list_splice(after->list_node.next, new_ofgroup->buckets.next,\n                    last->list_node.next);\n    }\n\n    return 0;\n}\n\n/* Appends all of the a copy of all the buckets from 'ofgroup' to 'new_ofgroup'\n * with the exception of the bucket whose bucket id is 'command_bucket_id'.\n * Special 'command_bucket_id' values OFPG15_BUCKET_FIRST, OFPG15_BUCKET_LAST\n * and OFPG15_BUCKET_ALL are also honored. */\nstatic enum ofperr\ncopy_buckets_for_remove_bucket(const struct ofgroup *ofgroup,\n                               struct ofgroup *new_ofgroup,\n                               uint32_t command_bucket_id)\n{\n    const struct ofputil_bucket *skip = NULL;\n\n    if (command_bucket_id == OFPG15_BUCKET_ALL) {\n        return 0;\n    }\n\n    if (command_bucket_id == OFPG15_BUCKET_FIRST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            skip = ofputil_bucket_list_front(&ofgroup->buckets);\n        }\n    } else if (command_bucket_id == OFPG15_BUCKET_LAST) {\n        if (!ovs_list_is_empty(&ofgroup->buckets)) {\n            skip = ofputil_bucket_list_back(&ofgroup->buckets);\n        }\n    } else {\n        skip = ofputil_bucket_find(&ofgroup->buckets, command_bucket_id);\n        if (!skip) {\n            return OFPERR_OFPGMFC_UNKNOWN_BUCKET;\n        }\n    }\n\n    ofputil_bucket_clone_list(CONST_CAST(struct ovs_list *,\n                                         &new_ofgroup->buckets),\n                              &ofgroup->buckets, skip);\n\n    return 0;\n}\n\n/* Implements OFPGC11_MODIFY, OFPGC15_INSERT_BUCKET and\n * OFPGC15_REMOVE_BUCKET.  Returns 0 on success or an OpenFlow error code\n * on failure.\n *\n * Note that the group is re-created and then replaces the old group in\n * ofproto's ofgroup hash map. Thus, the group is never altered while users of\n * the xlate module hold a pointer to the group. */\nstatic enum ofperr\nmodify_group_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *old_group;          /* Modified group. */\n    struct ofgroup *new_group;\n    enum ofperr error;\n\n    old_group = ofproto_group_lookup__(ofproto, ogm->gm.group_id,\n                                       OVS_VERSION_MAX);\n    if (!old_group) {\n        return OFPERR_OFPGMFC_UNKNOWN_GROUP;\n    }\n\n    /* Inserting or deleting a bucket should not change the group's type or\n     * properties, so change the group mod so that these aspects match the old\n     * group.  (See EXT-570.)  */\n    if (ogm->gm.command == OFPGC15_INSERT_BUCKET ||\n        ogm->gm.command == OFPGC15_REMOVE_BUCKET) {\n        ogm->gm.type = old_group->type;\n        ofputil_group_properties_destroy(&ogm->gm.props);\n        ofputil_group_properties_copy(&ogm->gm.props, &old_group->props);\n    }\n\n    if (old_group->type != ogm->gm.type\n        && (ofproto->n_groups[ogm->gm.type]\n            >= ofproto->ogf.max_groups[ogm->gm.type])) {\n        return OFPERR_OFPGMFC_OUT_OF_GROUPS;\n    }\n\n    error = init_group(ofproto, &ogm->gm, ogm->version, &ogm->new_group);\n    if (error) {\n        return error;\n    }\n    new_group = ogm->new_group;\n\n    /* Manipulate bucket list for bucket commands */\n    if (ogm->gm.command == OFPGC15_INSERT_BUCKET) {\n        error = copy_buckets_for_insert_bucket(old_group, new_group,\n                                               ogm->gm.command_bucket_id);\n    } else if (ogm->gm.command == OFPGC15_REMOVE_BUCKET) {\n        error = copy_buckets_for_remove_bucket(old_group, new_group,\n                                               ogm->gm.command_bucket_id);\n    }\n    if (error) {\n        goto out;\n    }\n\n    /* The group creation time does not change during modification. */\n    *CONST_CAST(long long int *, &(new_group->created)) = old_group->created;\n    *CONST_CAST(long long int *, &(new_group->modified)) = time_msec();\n\n    group_collection_add(&ogm->old_groups, old_group);\n\n    /* Mark the old group for deletion. */\n    versions_set_remove_version(&old_group->versions, ogm->version);\n    /* Insert replacement group. */\n    cmap_insert(&ofproto->groups, &new_group->cmap_node,\n                    hash_int(new_group->group_id, 0));\n    /* Transfer rules. */\n    rule_collection_move(&new_group->rules, &old_group->rules);\n\n    if (old_group->type != new_group->type) {\n        ofproto->n_groups[old_group->type]--;\n        ofproto->n_groups[new_group->type]++;\n    }\n    return 0;\n\nout:\n    ofproto_group_unref(new_group);\n    return error;\n}\n\n/* Implements the OFPGC11_ADD_OR_MOD command which creates the group when it does not\n * exist yet and modifies it otherwise */\nstatic enum ofperr\nadd_or_modify_group_start(struct ofproto *ofproto,\n                          struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    if (!ofproto_group_exists(ofproto, ogm->gm.group_id)) {\n        error = add_group_start(ofproto, ogm);\n    } else {\n        error = modify_group_start(ofproto, ogm);\n    }\n\n    return error;\n}\n\nstatic void\ndelete_group_start(struct ofproto *ofproto, ovs_version_t version,\n                   struct group_collection *groups, struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Makes flow deletion code leave the rule pointers in 'group->rules'\n     * intact, so that we can later refer to the rules deleted due to the group\n     * deletion.  Rule pointers will be removed from all other groups, if any,\n     * so we will never try to delete the same rule twice. */\n    group->being_deleted = true;\n\n    /* Mark all the referring groups for deletion. */\n    delete_flows_start__(ofproto, version, &group->rules);\n    group_collection_add(groups, group);\n    versions_set_remove_version(&group->versions, version);\n    ofproto->n_groups[group->type]--;\n}\n\nstatic void\ndelete_group_finish(struct ofproto *ofproto, struct ofgroup *group)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Finish deletion of all flow entries containing this group in a group\n     * action. */\n    delete_flows_finish__(ofproto, &group->rules, OFPRR_GROUP_DELETE, NULL);\n\n    /* Group removal is postponed by the caller. */\n}\n\n/* Implements OFPGC11_DELETE. */\nstatic void\ndelete_groups_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *group;\n\n    if (ogm->gm.group_id == OFPG_ALL) {\n        CMAP_FOR_EACH (group, cmap_node, &ofproto->groups) {\n            if (versions_visible_in_version(&group->versions, ogm->version)) {\n                delete_group_start(ofproto, ogm->version, &ogm->old_groups,\n                                   group);\n            }\n        }\n    } else {\n        group = ofproto_group_lookup__(ofproto, ogm->gm.group_id, ogm->version);\n        if (group) {\n            delete_group_start(ofproto, ogm->version, &ogm->old_groups, group);\n        }\n    }\n}\n\nstatic enum ofperr\nofproto_group_mod_start(struct ofproto *ofproto, struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    ogm->new_group = NULL;\n    group_collection_init(&ogm->old_groups);\n\n    switch (ogm->gm.command) {\n    case OFPGC11_ADD:\n        error = add_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_MODIFY:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_ADD_OR_MOD:\n        error = add_or_modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC11_DELETE:\n        delete_groups_start(ofproto, ogm);\n        error = 0;\n        break;\n\n    case OFPGC15_INSERT_BUCKET:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    case OFPGC15_REMOVE_BUCKET:\n        error = modify_group_start(ofproto, ogm);\n        break;\n\n    default:\n        if (ogm->gm.command > OFPGC11_DELETE) {\n            VLOG_INFO_RL(&rl, \"%s: Invalid group_mod command type %d\",\n                         ofproto->name, ogm->gm.command);\n        }\n        error = OFPERR_OFPGMFC_BAD_COMMAND;\n        break;\n    }\n    return error;\n}\n\nstatic void\nofproto_group_mod_revert(struct ofproto *ofproto,\n                         struct ofproto_group_mod *ogm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *new_group = ogm->new_group;\n    struct ofgroup *old_group;\n\n    /* Restore replaced or deleted groups. */\n    GROUP_COLLECTION_FOR_EACH (old_group, &ogm->old_groups) {\n        ofproto->n_groups[old_group->type]++;\n        if (new_group) {\n            ovs_assert(group_collection_n(&ogm->old_groups) == 1);\n            /* Transfer rules back. */\n            rule_collection_move(&old_group->rules, &new_group->rules);\n        } else {\n            old_group->being_deleted = false;\n            /* Revert rule deletion. */\n            delete_flows_revert__(ofproto, &old_group->rules);\n        }\n        /* Restore visibility. */\n        versions_set_remove_version(&old_group->versions,\n                                    OVS_VERSION_NOT_REMOVED);\n    }\n    if (new_group) {\n        /* Remove the new group immediately.  It was never visible to\n         * lookups. */\n        cmap_remove(&ofproto->groups, &new_group->cmap_node,\n                    hash_int(new_group->group_id, 0));\n        ofproto->n_groups[new_group->type]--;\n        ofproto_group_unref(new_group);\n    }\n}\n\nstatic void\nofproto_group_mod_finish(struct ofproto *ofproto,\n                         struct ofproto_group_mod *ogm,\n                         const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofgroup *new_group = ogm->new_group;\n    struct ofgroup *old_group;\n\n    if (new_group && group_collection_n(&ogm->old_groups) &&\n        ofproto->ofproto_class->group_modify) {\n        /* Modify a group. */\n        ovs_assert(group_collection_n(&ogm->old_groups) == 1);\n\n        /* XXX: OK to lose old group's stats? */\n        ofproto->ofproto_class->group_modify(new_group);\n    }\n\n    /* Delete old groups. */\n    GROUP_COLLECTION_FOR_EACH(old_group, &ogm->old_groups) {\n        delete_group_finish(ofproto, old_group);\n    }\n    remove_groups_postponed(&ogm->old_groups);\n\n    if (req) {\n        struct ofputil_requestforward rf;\n        rf.xid = req->request->xid;\n        rf.reason = OFPRFR_GROUP_MOD;\n        rf.group_mod = &ogm->gm;\n        connmgr_send_requestforward(ofproto->connmgr, req->ofconn, &rf);\n    }\n}\n\n/* Delete all groups from 'ofproto'.\n *\n * This is intended for use within an ofproto provider's 'destruct'\n * function. */\nvoid\nofproto_group_delete_all(struct ofproto *ofproto)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto_group_mod ogm;\n\n    ogm.gm.command = OFPGC11_DELETE;\n    ogm.gm.group_id = OFPG_ALL;\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ogm.version = ofproto->tables_version + 1;\n    ofproto_group_mod_start(ofproto, &ogm);\n    ofproto_bump_tables_version(ofproto);\n    ofproto_group_mod_finish(ofproto, &ogm, NULL);\n    ovs_mutex_unlock(&ofproto_mutex);\n}\n\nstatic enum ofperr\nhandle_group_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofproto_group_mod ogm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_group_mod(oh, &ogm.gm);\n    if (error) {\n        return error;\n    }\n\n    ovs_mutex_lock(&ofproto_mutex);\n    ogm.version = ofproto->tables_version + 1;\n    error = ofproto_group_mod_start(ofproto, &ogm);\n    if (!error) {\n        struct openflow_mod_requester req = { ofconn, oh };\n\n        ofproto_bump_tables_version(ofproto);\n        ofproto_group_mod_finish(ofproto, &ogm, &req);\n        ofmonitor_flush(ofproto->connmgr);\n    }\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    ofputil_uninit_group_mod(&ogm.gm);\n\n    return error;\n}\n\nenum ofputil_table_miss\nofproto_table_get_miss_config(const struct ofproto *ofproto, uint8_t table_id)\n{\n    enum ofputil_table_miss miss;\n\n    atomic_read_relaxed(&ofproto->tables[table_id].miss_config, &miss);\n    return miss;\n}\n\nstatic void\ntable_mod__(struct oftable *oftable,\n            const struct ofputil_table_mod *tm)\n{\n    if (tm->miss == OFPUTIL_TABLE_MISS_DEFAULT) {\n        /* This is how an OFPT_TABLE_MOD decodes if it doesn't specify any\n         * table-miss configuration (because the protocol used doesn't have\n         * such a concept), so there's nothing to do. */\n    } else {\n        atomic_store_relaxed(&oftable->miss_config, tm->miss);\n    }\n\n    unsigned int new_eviction = oftable->eviction;\n    if (tm->eviction == OFPUTIL_TABLE_EVICTION_ON) {\n        new_eviction |= EVICTION_OPENFLOW;\n    } else if (tm->eviction == OFPUTIL_TABLE_EVICTION_OFF) {\n        new_eviction &= ~EVICTION_OPENFLOW;\n    }\n\n    if (new_eviction != oftable->eviction) {\n        ovs_mutex_lock(&ofproto_mutex);\n        oftable_configure_eviction(oftable, new_eviction,\n                                   oftable->eviction_fields,\n                                   oftable->n_eviction_fields);\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    if (tm->vacancy != OFPUTIL_TABLE_VACANCY_DEFAULT) {\n        ovs_mutex_lock(&ofproto_mutex);\n        oftable->vacancy_down = tm->table_vacancy.vacancy_down;\n        oftable->vacancy_up = tm->table_vacancy.vacancy_up;\n        if (tm->vacancy == OFPUTIL_TABLE_VACANCY_OFF) {\n            oftable->vacancy_event = 0;\n        } else if (!oftable->vacancy_event) {\n            uint8_t vacancy = oftable_vacancy(oftable);\n            oftable->vacancy_event = (vacancy < oftable->vacancy_up\n                                      ? OFPTR_VACANCY_UP\n                                      : OFPTR_VACANCY_DOWN);\n        }\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n}\n\nstatic enum ofperr\ntable_mod(struct ofproto *ofproto, const struct ofputil_table_mod *tm)\n{\n    if (!check_table_id(ofproto, tm->table_id)) {\n        return OFPERR_OFPTMFC_BAD_TABLE;\n    }\n\n    /* Don't allow the eviction flags to be changed (except to the only fixed\n     * value that OVS supports).  OF1.4 says this is normal: \"The\n     * OFPTMPT_EVICTION property usually cannot be modified using a\n     * OFP_TABLE_MOD request, because the eviction mechanism is switch\n     * defined\". */\n    if (tm->eviction_flags != UINT32_MAX\n        && tm->eviction_flags != OFPROTO_EVICTION_FLAGS) {\n        return OFPERR_OFPTMFC_BAD_CONFIG;\n    }\n\n    if (tm->table_id == OFPTT_ALL) {\n        struct oftable *oftable;\n        OFPROTO_FOR_EACH_TABLE (oftable, ofproto) {\n            if (!(oftable->flags & (OFTABLE_HIDDEN | OFTABLE_READONLY))) {\n                table_mod__(oftable, tm);\n            }\n        }\n    } else {\n        struct oftable *oftable = &ofproto->tables[tm->table_id];\n        if (oftable->flags & OFTABLE_READONLY) {\n            return OFPERR_OFPTMFC_EPERM;\n        }\n        table_mod__(oftable, tm);\n    }\n\n    return 0;\n}\n\nstatic enum ofperr\nhandle_table_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_table_mod tm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_table_mod(oh, &tm);\n    if (error) {\n        return error;\n    }\n\n    return table_mod(ofproto, &tm);\n}\n\n/* Free resources that may be allocated by ofproto_flow_mod_init(). */\nvoid\nofproto_flow_mod_uninit(struct ofproto_flow_mod *ofm)\n{\n    if (ofm->temp_rule) {\n        ofproto_rule_unref(ofm->temp_rule);\n        ofm->temp_rule = NULL;\n    }\n    if (ofm->criteria.version != OVS_VERSION_NOT_REMOVED) {\n        rule_criteria_destroy(&ofm->criteria);\n    }\n    if (ofm->conjs) {\n        free(ofm->conjs);\n        ofm->conjs = NULL;\n        ofm->n_conjs = 0;\n    }\n}\n\n/* Initializes 'ofm' with 'ofproto', 'fm', and 'rule'.  'rule' may be null, but\n * if it is nonnull then the caller must own a reference to it, which on\n * success is transferred to 'ofm' and on failure is unreffed. */\nstatic enum ofperr\nofproto_flow_mod_init(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                      const struct ofputil_flow_mod *fm, struct rule *rule)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error;\n\n    /* Forward flow mod fields we need later. */\n    ofm->command = fm->command;\n    ofm->modify_cookie = fm->modify_cookie;\n\n    ofm->modify_may_add_flow = (fm->new_cookie != OVS_BE64_MAX\n                                && fm->cookie_mask == htonll(0));\n    /* Old flags must be kept when modifying a flow, but we still must\n     * honor the reset counts flag if present in the flow mod. */\n    ofm->modify_keep_counts = !(fm->flags & OFPUTIL_FF_RESET_COUNTS);\n\n    /* Initialize state needed by ofproto_flow_mod_uninit(). */\n    ofm->temp_rule = rule;\n    ofm->criteria.version = OVS_VERSION_NOT_REMOVED;\n    ofm->conjs = NULL;\n    ofm->n_conjs = 0;\n\n    bool check_buffer_id = false;\n\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        check_buffer_id = true;\n        error = add_flow_init(ofproto, ofm, fm);\n        break;\n    case OFPFC_MODIFY:\n        check_buffer_id = true;\n        error = modify_flows_init_loose(ofproto, ofm, fm);\n        break;\n    case OFPFC_MODIFY_STRICT:\n        check_buffer_id = true;\n        error = modify_flow_init_strict(ofproto, ofm, fm);\n        break;\n    case OFPFC_DELETE:\n        error = delete_flows_init_loose(ofproto, ofm, fm);\n        break;\n    case OFPFC_DELETE_STRICT:\n        error = delete_flows_init_strict(ofproto, ofm, fm);\n        break;\n    default:\n        error = OFPERR_OFPFMFC_BAD_COMMAND;\n        break;\n    }\n    if (!error && check_buffer_id && fm->buffer_id != UINT32_MAX) {\n        error = OFPERR_OFPBRC_BUFFER_UNKNOWN;\n    }\n\n    if (error) {\n        ofproto_flow_mod_uninit(ofm);\n    }\n    return error;\n}\n\nstatic enum ofperr\nofproto_flow_mod_start(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    enum ofperr error;\n\n    rule_collection_init(&ofm->old_rules);\n    rule_collection_init(&ofm->new_rules);\n\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        error = add_flow_start(ofproto, ofm);\n        break;\n    case OFPFC_MODIFY:\n        error = modify_flows_start_loose(ofproto, ofm);\n        break;\n    case OFPFC_MODIFY_STRICT:\n        error = modify_flow_start_strict(ofproto, ofm);\n        break;\n    case OFPFC_DELETE:\n        error = delete_flows_start_loose(ofproto, ofm);\n        break;\n    case OFPFC_DELETE_STRICT:\n        error = delete_flow_start_strict(ofproto, ofm);\n        break;\n    default:\n        OVS_NOT_REACHED();\n    }\n    /* Release resources not needed after start. */\n    ofproto_flow_mod_uninit(ofm);\n\n    if (error) {\n        rule_collection_destroy(&ofm->old_rules);\n        rule_collection_destroy(&ofm->new_rules);\n    }\n    return error;\n}\n\nstatic void\nofproto_flow_mod_revert(struct ofproto *ofproto, struct ofproto_flow_mod *ofm)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        add_flow_revert(ofproto, ofm);\n        break;\n\n    case OFPFC_MODIFY:\n    case OFPFC_MODIFY_STRICT:\n        modify_flows_revert(ofproto, ofm);\n        break;\n\n    case OFPFC_DELETE:\n    case OFPFC_DELETE_STRICT:\n        delete_flows_revert(ofproto, ofm);\n        break;\n\n    default:\n        break;\n    }\n\n    rule_collection_destroy(&ofm->old_rules);\n    rule_collection_destroy(&ofm->new_rules);\n}\n\nstatic void\nofproto_flow_mod_finish(struct ofproto *ofproto, struct ofproto_flow_mod *ofm,\n                        const struct openflow_mod_requester *req)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    switch (ofm->command) {\n    case OFPFC_ADD:\n        add_flow_finish(ofproto, ofm, req);\n        break;\n\n    case OFPFC_MODIFY:\n    case OFPFC_MODIFY_STRICT:\n        modify_flows_finish(ofproto, ofm, req);\n        break;\n\n    case OFPFC_DELETE:\n    case OFPFC_DELETE_STRICT:\n        delete_flows_finish(ofproto, ofm, req);\n        break;\n\n    default:\n        break;\n    }\n\n    rule_collection_destroy(&ofm->old_rules);\n    rule_collection_destroy(&ofm->new_rules);\n\n    if (req) {\n        ofconn_report_flow_mod(req->ofconn, ofm->command);\n    }\n}\n\n/* Commit phases (all while locking ofproto_mutex):\n *\n * 1. Begin: Gather resources and make changes visible in the next version.\n *           - Mark affected rules for removal in the next version.\n *           - Create new replacement rules, make visible in the next\n *             version.\n *           - Do not send any events or notifications.\n *\n * 2. Revert: Fail if any errors are found.  After this point no errors are\n * possible.  No visible changes were made, so rollback is minimal (remove\n * added invisible rules, restore visibility of rules marked for removal).\n *\n * 3. Finish: Make the changes visible for lookups. Insert replacement rules to\n * the ofproto provider. Remove replaced and deleted rules from ofproto data\n * structures, and Schedule postponed removal of deleted rules from the\n * classifier.  Send notifications, buffered packets, etc.\n */\nstatic enum ofperr\ndo_bundle_commit(struct ofconn *ofconn, uint32_t id, uint16_t flags)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    ovs_version_t version = ofproto->tables_version + 1;\n    struct ofp_bundle *bundle;\n    struct ofp_bundle_entry *be;\n    enum ofperr error;\n\n    bundle = ofconn_get_bundle(ofconn, id);\n\n    if (!bundle) {\n        return OFPERR_OFPBFC_BAD_ID;\n    }\n    if (bundle->flags != flags) {\n        error = OFPERR_OFPBFC_BAD_FLAGS;\n    } else {\n        bool prev_is_port_mod = false;\n\n        error = 0;\n        ovs_mutex_lock(&ofproto_mutex);\n\n        /* 1. Begin. */\n        LIST_FOR_EACH (be, node, &bundle->msg_list) {\n            if (be->type == OFPTYPE_PORT_MOD) {\n                /* Our port mods are not atomic. */\n                if (flags & OFPBF_ATOMIC) {\n                    error = OFPERR_OFPBFC_MSG_FAILED;\n                } else {\n                    prev_is_port_mod = true;\n                    error = port_mod_start(ofconn, &be->opm.pm, &be->opm.port);\n                }\n            } else {\n                /* Flow & group mods between port mods are applied as a single\n                 * version, but the versions are published only after we know\n                 * the commit is successful. */\n                if (prev_is_port_mod) {\n                    prev_is_port_mod = false;\n                    ++version;\n                }\n                if (be->type == OFPTYPE_FLOW_MOD) {\n                    /* Store the version in which the changes should take\n                     * effect. */\n                    be->ofm.version = version;\n                    error = ofproto_flow_mod_start(ofproto, &be->ofm);\n                } else if (be->type == OFPTYPE_GROUP_MOD) {\n                    /* Store the version in which the changes should take\n                     * effect. */\n                    be->ogm.version = version;\n                    error = ofproto_group_mod_start(ofproto, &be->ogm);\n                } else if (be->type == OFPTYPE_PACKET_OUT) {\n                    be->opo.version = version;\n                    error = ofproto_packet_out_start(ofproto, &be->opo);\n                } else {\n                    OVS_NOT_REACHED();\n                }\n            }\n            if (error) {\n                break;\n            }\n        }\n\n        if (error) {\n            /* Send error referring to the original message. */\n            if (error) {\n                ofconn_send_error(ofconn, &be->ofp_msg, error);\n                error = OFPERR_OFPBFC_MSG_FAILED;\n            }\n\n            /* 2. Revert.  Undo all the changes made above. */\n            LIST_FOR_EACH_REVERSE_CONTINUE(be, node, &bundle->msg_list) {\n                if (be->type == OFPTYPE_FLOW_MOD) {\n                    ofproto_flow_mod_revert(ofproto, &be->ofm);\n                } else if (be->type == OFPTYPE_GROUP_MOD) {\n                    ofproto_group_mod_revert(ofproto, &be->ogm);\n                } else if (be->type == OFPTYPE_PACKET_OUT) {\n                    ofproto_packet_out_revert(ofproto, &be->opo);\n                }\n                /* Nothing needs to be reverted for a port mod. */\n            }\n        } else {\n            /* 4. Finish. */\n            LIST_FOR_EACH (be, node, &bundle->msg_list) {\n                if (be->type == OFPTYPE_PORT_MOD) {\n                    /* Perform the actual port mod. This is not atomic, i.e.,\n                     * the effects will be immediately seen by upcall\n                     * processing regardless of the lookup version.  It should\n                     * be noted that port configuration changes can originate\n                     * also from OVSDB changes asynchronously to all upcall\n                     * processing. */\n                    port_mod_finish(ofconn, &be->opm.pm, be->opm.port);\n                } else {\n                    version =\n                        (be->type == OFPTYPE_FLOW_MOD) ? be->ofm.version :\n                        (be->type == OFPTYPE_GROUP_MOD) ? be->ogm.version :\n                        (be->type == OFPTYPE_PACKET_OUT) ? be->opo.version :\n                        version;\n\n                    /* Bump the lookup version to the one of the current\n                     * message.  This makes all the changes in the bundle at\n                     * this version visible to lookups at once. */\n                    if (ofproto->tables_version < version) {\n                        ofproto->tables_version = version;\n                        ofproto->ofproto_class->set_tables_version(\n                            ofproto, ofproto->tables_version);\n                    }\n\n                    struct openflow_mod_requester req = { ofconn,\n                                                          &be->ofp_msg };\n\n                    if (be->type == OFPTYPE_FLOW_MOD) {\n                        ofproto_flow_mod_finish(ofproto, &be->ofm, &req);\n                    } else if (be->type == OFPTYPE_GROUP_MOD) {\n                        ofproto_group_mod_finish(ofproto, &be->ogm, &req);\n                    } else if (be->type == OFPTYPE_PACKET_OUT) {\n                        ofproto_packet_out_finish(ofproto, &be->opo);\n                    }\n                }\n            }\n        }\n\n        ofmonitor_flush(ofproto->connmgr);\n        ovs_mutex_unlock(&ofproto_mutex);\n    }\n\n    /* The bundle is discarded regardless the outcome. */\n    ofp_bundle_remove__(ofconn, bundle);\n    return error;\n}\n\nstatic enum ofperr\nhandle_bundle_control(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofputil_bundle_ctrl_msg bctrl;\n    struct ofputil_bundle_ctrl_msg reply;\n    struct ofpbuf *buf;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_bundle_ctrl(oh, &bctrl);\n    if (error) {\n        return error;\n    }\n    reply.flags = 0;\n    reply.bundle_id = bctrl.bundle_id;\n\n    switch (bctrl.type) {\n    case OFPBCT_OPEN_REQUEST:\n        error = ofp_bundle_open(ofconn, bctrl.bundle_id, bctrl.flags, oh);\n        reply.type = OFPBCT_OPEN_REPLY;\n        break;\n    case OFPBCT_CLOSE_REQUEST:\n        error = ofp_bundle_close(ofconn, bctrl.bundle_id, bctrl.flags);\n        reply.type = OFPBCT_CLOSE_REPLY;\n        break;\n    case OFPBCT_COMMIT_REQUEST:\n        error = do_bundle_commit(ofconn, bctrl.bundle_id, bctrl.flags);\n        reply.type = OFPBCT_COMMIT_REPLY;\n        break;\n    case OFPBCT_DISCARD_REQUEST:\n        error = ofp_bundle_discard(ofconn, bctrl.bundle_id);\n        reply.type = OFPBCT_DISCARD_REPLY;\n        break;\n\n    case OFPBCT_OPEN_REPLY:\n    case OFPBCT_CLOSE_REPLY:\n    case OFPBCT_COMMIT_REPLY:\n    case OFPBCT_DISCARD_REPLY:\n        return OFPERR_OFPBFC_BAD_TYPE;\n        break;\n    }\n\n    if (!error) {\n        buf = ofputil_encode_bundle_ctrl_reply(oh, &reply);\n        ofconn_send_reply(ofconn, buf);\n    }\n    return error;\n}\n\nstatic enum ofperr\nhandle_bundle_add(struct ofconn *ofconn, const struct ofp_header *oh)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    enum ofperr error;\n    struct ofputil_bundle_add_msg badd;\n    struct ofp_bundle_entry *bmsg;\n    enum ofptype type;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_bundle_add(oh, &badd, &type);\n    if (error) {\n        return error;\n    }\n\n    bmsg = ofp_bundle_entry_alloc(type, badd.msg);\n\n    struct ofpbuf ofpacts;\n    uint64_t ofpacts_stub[1024 / 8];\n    ofpbuf_use_stub(&ofpacts, ofpacts_stub, sizeof ofpacts_stub);\n\n    if (type == OFPTYPE_PORT_MOD) {\n        error = ofputil_decode_port_mod(badd.msg, &bmsg->opm.pm, false);\n    } else if (type == OFPTYPE_FLOW_MOD) {\n        struct ofputil_flow_mod fm;\n\n        error = ofputil_decode_flow_mod(&fm, badd.msg,\n                                        ofconn_get_protocol(ofconn),\n                                        ofproto_get_tun_tab(ofproto),\n                                        &ofproto->vl_mff_map, &ofpacts,\n                                        u16_to_ofp(ofproto->max_ports),\n                                        ofproto->n_tables);\n        if (!error) {\n            error = ofproto_flow_mod_init(ofproto, &bmsg->ofm, &fm, NULL);\n        }\n    } else if (type == OFPTYPE_GROUP_MOD) {\n        error = ofputil_decode_group_mod(badd.msg, &bmsg->ogm.gm);\n    } else if (type == OFPTYPE_PACKET_OUT) {\n        struct ofputil_packet_out po;\n\n        COVERAGE_INC(ofproto_packet_out);\n\n        /* Decode message. */\n        error = ofputil_decode_packet_out(&po, badd.msg, &ofpacts);\n        if (!error) {\n            po.ofpacts = ofpbuf_steal_data(&ofpacts);   /* Move to heap. */\n            error = ofproto_packet_out_init(ofproto, ofconn, &bmsg->opo, &po);\n        }\n    } else {\n        OVS_NOT_REACHED();\n    }\n\n    ofpbuf_uninit(&ofpacts);\n\n    if (!error) {\n        error = ofp_bundle_add_message(ofconn, badd.bundle_id, badd.flags,\n                                       bmsg, oh);\n    }\n\n    if (error) {\n        ofp_bundle_entry_free(bmsg);\n    }\n\n    return error;\n}\n\nstatic enum ofperr\nhandle_tlv_table_mod(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct tun_table *old_tab, *new_tab;\n    struct ofputil_tlv_table_mod ttm;\n    enum ofperr error;\n\n    error = reject_slave_controller(ofconn);\n    if (error) {\n        return error;\n    }\n\n    error = ofputil_decode_tlv_table_mod(oh, &ttm);\n    if (error) {\n        return error;\n    }\n\n    old_tab = ovsrcu_get_protected(struct tun_table *, &ofproto->metadata_tab);\n    error = tun_metadata_table_mod(&ttm, old_tab, &new_tab);\n    if (!error) {\n        ovs_mutex_lock(&ofproto->vl_mff_map.mutex);\n        error = mf_vl_mff_map_mod_from_tun_metadata(&ofproto->vl_mff_map,\n                                                    &ttm);\n        ovs_mutex_unlock(&ofproto->vl_mff_map.mutex);\n        if (!error) {\n            ovsrcu_set(&ofproto->metadata_tab, new_tab);\n            tun_metadata_postpone_free(old_tab);\n        }\n    }\n\n    ofputil_uninit_tlv_table(&ttm.mappings);\n    return error;\n}\n\nstatic enum ofperr\nhandle_tlv_table_request(struct ofconn *ofconn, const struct ofp_header *oh)\n{\n    const struct ofproto *ofproto = ofconn_get_ofproto(ofconn);\n    struct ofputil_tlv_table_reply ttr;\n    struct ofpbuf *b;\n\n    tun_metadata_table_request(ofproto_get_tun_tab(ofproto), &ttr);\n\n    b = ofputil_encode_tlv_table_reply(oh, &ttr);\n    ofputil_uninit_tlv_table(&ttr.mappings);\n\n    ofconn_send_reply(ofconn, b);\n    return 0;\n}\n\nstatic enum ofperr\nhandle_openflow__(struct ofconn *ofconn, const struct ofpbuf *msg)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    const struct ofp_header *oh = msg->data;\n    enum ofptype type;\n    enum ofperr error;\n\n    error = ofptype_decode(&type, oh);\n    if (error) {\n        return error;\n    }\n    if (oh->version >= OFP13_VERSION && ofpmsg_is_stat_request(oh)\n        && ofpmp_more(oh)) {\n        /* We have no buffer implementation for multipart requests.\n         * Report overflow for requests which consists of multiple\n         * messages. */\n        return OFPERR_OFPBRC_MULTIPART_BUFFER_OVERFLOW;\n    }\n\n    switch (type) {\n        /* OpenFlow requests. */\n    case OFPTYPE_ECHO_REQUEST:\n        return handle_echo_request(ofconn, oh);\n\n    case OFPTYPE_FEATURES_REQUEST:\n        return handle_features_request(ofconn, oh);\n\n    case OFPTYPE_GET_CONFIG_REQUEST:\n        return handle_get_config_request(ofconn, oh);\n\n    case OFPTYPE_SET_CONFIG:\n        return handle_set_config(ofconn, oh);\n\n    case OFPTYPE_PACKET_OUT:\n        return handle_packet_out(ofconn, oh);\n\n    case OFPTYPE_PORT_MOD:\n        return handle_port_mod(ofconn, oh);\n\n    case OFPTYPE_FLOW_MOD:\n        return handle_flow_mod(ofconn, oh);\n\n    case OFPTYPE_GROUP_MOD:\n        return handle_group_mod(ofconn, oh);\n\n    case OFPTYPE_TABLE_MOD:\n        return handle_table_mod(ofconn, oh);\n\n    case OFPTYPE_METER_MOD:\n        return handle_meter_mod(ofconn, oh);\n\n    case OFPTYPE_BARRIER_REQUEST:\n        return handle_barrier_request(ofconn, oh);\n\n    case OFPTYPE_ROLE_REQUEST:\n        return handle_role_request(ofconn, oh);\n\n        /* OpenFlow replies. */\n    case OFPTYPE_ECHO_REPLY:\n        return 0;\n\n        /* Nicira extension requests. */\n    case OFPTYPE_FLOW_MOD_TABLE_ID:\n        return handle_nxt_flow_mod_table_id(ofconn, oh);\n\n    case OFPTYPE_SET_FLOW_FORMAT:\n        return handle_nxt_set_flow_format(ofconn, oh);\n\n    case OFPTYPE_SET_PACKET_IN_FORMAT:\n        return handle_nxt_set_packet_in_format(ofconn, oh);\n\n    case OFPTYPE_SET_CONTROLLER_ID:\n        return handle_nxt_set_controller_id(ofconn, oh);\n\n    case OFPTYPE_FLOW_AGE:\n        /* Nothing to do. */\n        return 0;\n\n    case OFPTYPE_FLOW_MONITOR_CANCEL:\n        return handle_flow_monitor_cancel(ofconn, oh);\n\n    case OFPTYPE_SET_ASYNC_CONFIG:\n        return handle_nxt_set_async_config(ofconn, oh);\n\n    case OFPTYPE_GET_ASYNC_REQUEST:\n        return handle_nxt_get_async_request(ofconn, oh);\n\n    case OFPTYPE_NXT_RESUME:\n        return handle_nxt_resume(ofconn, oh);\n\n        /* Statistics requests. */\n    case OFPTYPE_DESC_STATS_REQUEST:\n        return handle_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_FLOW_STATS_REQUEST:\n        return handle_flow_stats_request(ofconn, oh);\n\n    case OFPTYPE_AGGREGATE_STATS_REQUEST:\n        return handle_aggregate_stats_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_STATS_REQUEST:\n        return handle_table_stats_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_FEATURES_STATS_REQUEST:\n        return handle_table_features_request(ofconn, oh);\n\n    case OFPTYPE_TABLE_DESC_REQUEST:\n        return handle_table_desc_request(ofconn, oh);\n\n    case OFPTYPE_PORT_STATS_REQUEST:\n        return handle_port_stats_request(ofconn, oh);\n\n    case OFPTYPE_QUEUE_STATS_REQUEST:\n        return handle_queue_stats_request(ofconn, oh);\n\n    case OFPTYPE_PORT_DESC_STATS_REQUEST:\n        return handle_port_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_FLOW_MONITOR_STATS_REQUEST:\n        return handle_flow_monitor_request(ofconn, oh);\n\n    case OFPTYPE_METER_STATS_REQUEST:\n    case OFPTYPE_METER_CONFIG_STATS_REQUEST:\n        return handle_meter_request(ofconn, oh, type);\n\n    case OFPTYPE_METER_FEATURES_STATS_REQUEST:\n        return handle_meter_features_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_STATS_REQUEST:\n        return handle_group_stats_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_DESC_STATS_REQUEST:\n        return handle_group_desc_stats_request(ofconn, oh);\n\n    case OFPTYPE_GROUP_FEATURES_STATS_REQUEST:\n        return handle_group_features_stats_request(ofconn, oh);\n\n    case OFPTYPE_QUEUE_GET_CONFIG_REQUEST:\n        return handle_queue_get_config_request(ofconn, oh);\n\n    case OFPTYPE_BUNDLE_CONTROL:\n        return handle_bundle_control(ofconn, oh);\n\n    case OFPTYPE_BUNDLE_ADD_MESSAGE:\n        return handle_bundle_add(ofconn, oh);\n\n    case OFPTYPE_NXT_TLV_TABLE_MOD:\n        return handle_tlv_table_mod(ofconn, oh);\n\n    case OFPTYPE_NXT_TLV_TABLE_REQUEST:\n        return handle_tlv_table_request(ofconn, oh);\n\n    case OFPTYPE_IPFIX_BRIDGE_STATS_REQUEST:\n        return handle_ipfix_bridge_stats_request(ofconn, oh);\n\n    case OFPTYPE_IPFIX_FLOW_STATS_REQUEST:\n        return handle_ipfix_flow_stats_request(ofconn, oh);\n\n    case OFPTYPE_CT_FLUSH_ZONE:\n        return handle_nxt_ct_flush_zone(ofconn, oh);\n\n    case OFPTYPE_HELLO:\n    case OFPTYPE_ERROR:\n    case OFPTYPE_FEATURES_REPLY:\n    case OFPTYPE_GET_CONFIG_REPLY:\n    case OFPTYPE_PACKET_IN:\n    case OFPTYPE_FLOW_REMOVED:\n    case OFPTYPE_PORT_STATUS:\n    case OFPTYPE_BARRIER_REPLY:\n    case OFPTYPE_QUEUE_GET_CONFIG_REPLY:\n    case OFPTYPE_DESC_STATS_REPLY:\n    case OFPTYPE_FLOW_STATS_REPLY:\n    case OFPTYPE_QUEUE_STATS_REPLY:\n    case OFPTYPE_PORT_STATS_REPLY:\n    case OFPTYPE_TABLE_STATS_REPLY:\n    case OFPTYPE_AGGREGATE_STATS_REPLY:\n    case OFPTYPE_PORT_DESC_STATS_REPLY:\n    case OFPTYPE_ROLE_REPLY:\n    case OFPTYPE_FLOW_MONITOR_PAUSED:\n    case OFPTYPE_FLOW_MONITOR_RESUMED:\n    case OFPTYPE_FLOW_MONITOR_STATS_REPLY:\n    case OFPTYPE_GET_ASYNC_REPLY:\n    case OFPTYPE_GROUP_STATS_REPLY:\n    case OFPTYPE_GROUP_DESC_STATS_REPLY:\n    case OFPTYPE_GROUP_FEATURES_STATS_REPLY:\n    case OFPTYPE_METER_STATS_REPLY:\n    case OFPTYPE_METER_CONFIG_STATS_REPLY:\n    case OFPTYPE_METER_FEATURES_STATS_REPLY:\n    case OFPTYPE_TABLE_FEATURES_STATS_REPLY:\n    case OFPTYPE_TABLE_DESC_REPLY:\n    case OFPTYPE_ROLE_STATUS:\n    case OFPTYPE_REQUESTFORWARD:\n    case OFPTYPE_TABLE_STATUS:\n    case OFPTYPE_NXT_TLV_TABLE_REPLY:\n    case OFPTYPE_IPFIX_BRIDGE_STATS_REPLY:\n    case OFPTYPE_IPFIX_FLOW_STATS_REPLY:\n    default:\n        if (ofpmsg_is_stat_request(oh)) {\n            return OFPERR_OFPBRC_BAD_STAT;\n        } else {\n            return OFPERR_OFPBRC_BAD_TYPE;\n        }\n    }\n}\n\nstatic void\nhandle_openflow(struct ofconn *ofconn, const struct ofpbuf *ofp_msg)\n    OVS_EXCLUDED(ofproto_mutex)\n{\n    enum ofperr error = handle_openflow__(ofconn, ofp_msg);\n\n    if (error) {\n        ofconn_send_error(ofconn, ofp_msg->data, error);\n    }\n    COVERAGE_INC(ofproto_recv_openflow);\n}\n\f\nstatic uint64_t\npick_datapath_id(const struct ofproto *ofproto)\n{\n    const struct ofport *port;\n\n    port = ofproto_get_port(ofproto, OFPP_LOCAL);\n    if (port) {\n        struct eth_addr ea;\n        int error;\n\n        error = netdev_get_etheraddr(port->netdev, &ea);\n        if (!error) {\n            return eth_addr_to_uint64(ea);\n        }\n        VLOG_WARN(\"%s: could not get MAC address for %s (%s)\",\n                  ofproto->name, netdev_get_name(port->netdev),\n                  ovs_strerror(error));\n    }\n    return ofproto->fallback_dpid;\n}\n\nstatic uint64_t\npick_fallback_dpid(void)\n{\n    struct eth_addr ea;\n    eth_addr_nicira_random(&ea);\n    return eth_addr_to_uint64(ea);\n}\n\f\n/* Table overflow policy. */\n\n/* Chooses and updates 'rulep' with a rule to evict from 'table'.  Sets 'rulep'\n * to NULL if the table is not configured to evict rules or if the table\n * contains no evictable rules.  (Rules with a readlock on their evict rwlock,\n * or with no timeouts are not evictable.) */\nstatic bool\nchoose_rule_to_evict(struct oftable *table, struct rule **rulep)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct eviction_group *evg;\n\n    *rulep = NULL;\n    if (!table->eviction) {\n        return false;\n    }\n\n    /* In the common case, the outer and inner loops here will each be entered\n     * exactly once:\n     *\n     *   - The inner loop normally \"return\"s in its first iteration.  If the\n     *     eviction group has any evictable rules, then it always returns in\n     *     some iteration.\n     *\n     *   - The outer loop only iterates more than once if the largest eviction\n     *     group has no evictable rules.\n     *\n     *   - The outer loop can exit only if table's 'max_flows' is all filled up\n     *     by unevictable rules. */\n    HEAP_FOR_EACH (evg, size_node, &table->eviction_groups_by_size) {\n        struct rule *rule;\n\n        HEAP_FOR_EACH (rule, evg_node, &evg->rules) {\n            *rulep = rule;\n            return true;\n        }\n    }\n\n    return false;\n}\n\f\n/* Eviction groups. */\n\n/* Returns the priority to use for an eviction_group that contains 'n_rules'\n * rules.  The priority contains low-order random bits to ensure that eviction\n * groups with the same number of rules are prioritized randomly. */\nstatic uint32_t\neviction_group_priority(size_t n_rules)\n{\n    uint16_t size = MIN(UINT16_MAX, n_rules);\n    return (size << 16) | random_uint16();\n}\n\n/* Updates 'evg', an eviction_group within 'table', following a change that\n * adds or removes rules in 'evg'. */\nstatic void\neviction_group_resized(struct oftable *table, struct eviction_group *evg)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    heap_change(&table->eviction_groups_by_size, &evg->size_node,\n                eviction_group_priority(heap_count(&evg->rules)));\n}\n\n/* Destroys 'evg', an eviction_group within 'table':\n *\n *   - Removes all the rules, if any, from 'evg'.  (It doesn't destroy the\n *     rules themselves, just removes them from the eviction group.)\n *\n *   - Removes 'evg' from 'table'.\n *\n *   - Frees 'evg'. */\nstatic void\neviction_group_destroy(struct oftable *table, struct eviction_group *evg)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    while (!heap_is_empty(&evg->rules)) {\n        struct rule *rule;\n\n        rule = CONTAINER_OF(heap_pop(&evg->rules), struct rule, evg_node);\n        rule->eviction_group = NULL;\n    }\n    hmap_remove(&table->eviction_groups_by_id, &evg->id_node);\n    heap_remove(&table->eviction_groups_by_size, &evg->size_node);\n    heap_destroy(&evg->rules);\n    free(evg);\n}\n\n/* Removes 'rule' from its eviction group, if any. */\nstatic void\neviction_group_remove_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    if (rule->eviction_group) {\n        struct oftable *table = &rule->ofproto->tables[rule->table_id];\n        struct eviction_group *evg = rule->eviction_group;\n\n        rule->eviction_group = NULL;\n        heap_remove(&evg->rules, &rule->evg_node);\n        if (heap_is_empty(&evg->rules)) {\n            eviction_group_destroy(table, evg);\n        } else {\n            eviction_group_resized(table, evg);\n        }\n    }\n}\n\n/* Hashes the 'rule''s values for the eviction_fields of 'rule''s table, and\n * returns the hash value. */\nstatic uint32_t\neviction_group_hash_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct oftable *table = &rule->ofproto->tables[rule->table_id];\n    const struct mf_subfield *sf;\n    struct flow flow;\n    uint32_t hash;\n\n    hash = table->eviction_group_id_basis;\n    miniflow_expand(rule->cr.match.flow, &flow);\n    for (sf = table->eviction_fields;\n         sf < &table->eviction_fields[table->n_eviction_fields];\n         sf++)\n    {\n        if (mf_are_prereqs_ok(sf->field, &flow, NULL)) {\n            union mf_value value;\n\n            mf_get_value(sf->field, &flow, &value);\n            if (sf->ofs) {\n                bitwise_zero(&value, sf->field->n_bytes, 0, sf->ofs);\n            }\n            if (sf->ofs + sf->n_bits < sf->field->n_bytes * 8) {\n                unsigned int start = sf->ofs + sf->n_bits;\n                bitwise_zero(&value, sf->field->n_bytes, start,\n                             sf->field->n_bytes * 8 - start);\n            }\n            hash = hash_bytes(&value, sf->field->n_bytes, hash);\n        } else {\n            hash = hash_int(hash, 0);\n        }\n    }\n\n    return hash;\n}\n\n/* Returns an eviction group within 'table' with the given 'id', creating one\n * if necessary. */\nstatic struct eviction_group *\neviction_group_find(struct oftable *table, uint32_t id)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct eviction_group *evg;\n\n    HMAP_FOR_EACH_WITH_HASH (evg, id_node, id, &table->eviction_groups_by_id) {\n        return evg;\n    }\n\n    evg = xmalloc(sizeof *evg);\n    hmap_insert(&table->eviction_groups_by_id, &evg->id_node, id);\n    heap_insert(&table->eviction_groups_by_size, &evg->size_node,\n                eviction_group_priority(0));\n    heap_init(&evg->rules);\n\n    return evg;\n}\n\n/* Returns an eviction priority for 'rule'.  The return value should be\n * interpreted so that higher priorities make a rule a more attractive\n * candidate for eviction. */\nstatic uint64_t\nrule_eviction_priority(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    /* Calculate absolute time when this flow will expire.  If it will never\n     * expire, then return 0 to make it unevictable.  */\n    long long int expiration = LLONG_MAX;\n    if (rule->hard_timeout) {\n        /* 'modified' needs protection even when we hold 'ofproto_mutex'. */\n        ovs_mutex_lock(&rule->mutex);\n        long long int modified = rule->modified;\n        ovs_mutex_unlock(&rule->mutex);\n\n        expiration = modified + rule->hard_timeout * 1000;\n    }\n    if (rule->idle_timeout) {\n        uint64_t packets, bytes;\n        long long int used;\n        long long int idle_expiration;\n\n        ofproto->ofproto_class->rule_get_stats(rule, &packets, &bytes, &used);\n        idle_expiration = used + rule->idle_timeout * 1000;\n        expiration = MIN(expiration, idle_expiration);\n    }\n    if (expiration == LLONG_MAX) {\n        return 0;\n    }\n\n    /* Calculate the time of expiration as a number of (approximate) seconds\n     * after program startup.\n     *\n     * This should work OK for program runs that last UINT32_MAX seconds or\n     * less.  Therefore, please restart OVS at least once every 136 years. */\n    uint32_t expiration_ofs = (expiration >> 10) - (time_boot_msec() >> 10);\n\n    /* Combine expiration time with OpenFlow \"importance\" to form a single\n     * priority value.  We want flows with relatively low \"importance\" to be\n     * evicted before even considering expiration time, so put \"importance\" in\n     * the most significant bits and expiration time in the least significant\n     * bits.\n     *\n     * Small 'priority' should be evicted before those with large 'priority'.\n     * The caller expects the opposite convention (a large return value being\n     * more attractive for eviction) so we invert it before returning. */\n    uint64_t priority = ((uint64_t) rule->importance << 32) + expiration_ofs;\n    return UINT64_MAX - priority;\n}\n\n/* Adds 'rule' to an appropriate eviction group for its oftable's\n * configuration.  Does nothing if 'rule''s oftable doesn't have eviction\n * enabled, or if 'rule' is a permanent rule (one that will never expire on its\n * own).\n *\n * The caller must ensure that 'rule' is not already in an eviction group. */\nstatic void\neviction_group_add_rule(struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct ofproto *ofproto = rule->ofproto;\n    struct oftable *table = &ofproto->tables[rule->table_id];\n    bool has_timeout;\n\n    /* Timeouts may be modified only when holding 'ofproto_mutex'.  We have it\n     * so no additional protection is needed. */\n    has_timeout = rule->hard_timeout || rule->idle_timeout;\n\n    if (table->eviction && has_timeout) {\n        struct eviction_group *evg;\n\n        evg = eviction_group_find(table, eviction_group_hash_rule(rule));\n\n        rule->eviction_group = evg;\n        heap_insert(&evg->rules, &rule->evg_node,\n                    rule_eviction_priority(ofproto, rule));\n        eviction_group_resized(table, evg);\n    }\n}\n\f\n/* oftables. */\n\n/* Initializes 'table'. */\nstatic void\noftable_init(struct oftable *table)\n{\n    memset(table, 0, sizeof *table);\n    classifier_init(&table->cls, flow_segment_u64s);\n    table->max_flows = UINT_MAX;\n    table->n_flows = 0;\n    hmap_init(&table->eviction_groups_by_id);\n    heap_init(&table->eviction_groups_by_size);\n    atomic_init(&table->miss_config, OFPUTIL_TABLE_MISS_DEFAULT);\n\n    classifier_set_prefix_fields(&table->cls, default_prefix_fields,\n                                 ARRAY_SIZE(default_prefix_fields));\n\n    atomic_init(&table->n_matched, 0);\n    atomic_init(&table->n_missed, 0);\n}\n\n/* Destroys 'table', including its classifier and eviction groups.\n *\n * The caller is responsible for freeing 'table' itself. */\nstatic void\noftable_destroy(struct oftable *table)\n{\n    ovs_assert(classifier_is_empty(&table->cls));\n\n    ovs_mutex_lock(&ofproto_mutex);\n    oftable_configure_eviction(table, 0, NULL, 0);\n    ovs_mutex_unlock(&ofproto_mutex);\n\n    hmap_destroy(&table->eviction_groups_by_id);\n    heap_destroy(&table->eviction_groups_by_size);\n    classifier_destroy(&table->cls);\n    free(table->name);\n}\n\n/* Changes the name of 'table' to 'name'.  If 'name' is NULL or the empty\n * string, then 'table' will use its default name.\n *\n * This only affects the name exposed for a table exposed through the OpenFlow\n * OFPST_TABLE (as printed by \"ovs-ofctl dump-tables\"). */\nstatic void\noftable_set_name(struct oftable *table, const char *name)\n{\n    if (name && name[0]) {\n        int len = strnlen(name, OFP_MAX_TABLE_NAME_LEN);\n        if (!table->name || strncmp(name, table->name, len)) {\n            free(table->name);\n            table->name = xmemdup0(name, len);\n        }\n    } else {\n        free(table->name);\n        table->name = NULL;\n    }\n}\n\n/* oftables support a choice of two policies when adding a rule would cause the\n * number of flows in the table to exceed the configured maximum number: either\n * they can refuse to add the new flow or they can evict some existing flow.\n * This function configures the latter policy on 'table', with fairness based\n * on the values of the 'n_fields' fields specified in 'fields'.  (Specifying\n * 'n_fields' as 0 disables fairness.) */\nstatic void\noftable_configure_eviction(struct oftable *table, unsigned int eviction,\n                           const struct mf_subfield *fields, size_t n_fields)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    struct rule *rule;\n\n    if ((table->eviction != 0) == (eviction != 0)\n        && n_fields == table->n_eviction_fields\n        && (!n_fields\n            || !memcmp(fields, table->eviction_fields,\n                       n_fields * sizeof *fields))) {\n        /* The set of eviction fields did not change.  If 'eviction' changed,\n         * it remains nonzero, so that we can just update table->eviction\n         * without fussing with the eviction groups. */\n        table->eviction = eviction;\n        return;\n    }\n\n    /* Destroy existing eviction groups, then destroy and recreate data\n     * structures to recover memory. */\n    struct eviction_group *evg, *next;\n    HMAP_FOR_EACH_SAFE (evg, next, id_node, &table->eviction_groups_by_id) {\n        eviction_group_destroy(table, evg);\n    }\n    hmap_destroy(&table->eviction_groups_by_id);\n    hmap_init(&table->eviction_groups_by_id);\n    heap_destroy(&table->eviction_groups_by_size);\n    heap_init(&table->eviction_groups_by_size);\n\n    /* Replace eviction groups by the new ones, if there is a change.  Free the\n     * old fields only after allocating the new ones, because 'fields ==\n     * table->eviction_fields' is possible. */\n    struct mf_subfield *old_fields = table->eviction_fields;\n    table->n_eviction_fields = n_fields;\n    table->eviction_fields = (fields\n                              ? xmemdup(fields, n_fields * sizeof *fields)\n                              : NULL);\n    free(old_fields);\n\n    /* Add the new eviction groups, if enabled. */\n    table->eviction = eviction;\n    if (table->eviction) {\n        table->eviction_group_id_basis = random_uint32();\n        CLS_FOR_EACH (rule, cr, &table->cls) {\n            eviction_group_add_rule(rule);\n        }\n    }\n}\n\n/* Inserts 'rule' from the ofproto data structures BEFORE caller has inserted\n * it to the classifier. */\nstatic void\nofproto_rule_insert__(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    const struct rule_actions *actions = rule_get_actions(rule);\n\n    /* A rule may not be reinserted. */\n    ovs_assert(rule->state != RULE_INSERTED);\n\n    if (rule->hard_timeout || rule->idle_timeout) {\n        ovs_list_insert(&ofproto->expirable, &rule->expirable);\n    }\n    cookies_insert(ofproto, rule);\n    eviction_group_add_rule(rule);\n    if (actions->has_meter) {\n        meter_insert_rule(rule);\n    }\n    if (actions->has_groups) {\n        const struct ofpact_group *a;\n        OFPACT_FOR_EACH_TYPE_FLATTENED (a, GROUP, actions->ofpacts,\n                                        actions->ofpacts_len) {\n            struct ofgroup *group;\n\n            group = ofproto_group_lookup(ofproto, a->group_id, OVS_VERSION_MAX,\n                                         false);\n            ovs_assert(group != NULL);\n            group_add_rule(group, rule);\n        }\n    }\n\n    rule->state = RULE_INSERTED;\n}\n\n/* Removes 'rule' from the ofproto data structures.  Caller may have deferred\n * the removal from the classifier. */\nstatic void\nofproto_rule_remove__(struct ofproto *ofproto, struct rule *rule)\n    OVS_REQUIRES(ofproto_mutex)\n{\n    ovs_assert(rule->state == RULE_INSERTED);\n\n    cookies_remove(ofproto, rule);\n\n    eviction_group_remove_rule(rule);\n    if (!ovs_list_is_empty(&rule->expirable)) {\n        ovs_list_remove(&rule->expirable);\n    }\n    if (!ovs_list_is_empty(&rule->meter_list_node)) {\n        ovs_list_remove(&rule->meter_list_node);\n        ovs_list_init(&rule->meter_list_node);\n    }\n\n    /* Remove the rule from any groups, except from the group that is being\n     * deleted, if any. */\n    const struct rule_actions *actions = rule_get_actions(rule);\n\n    if (actions->has_groups) {\n        const struct ofpact_group *a;\n\n        OFPACT_FOR_EACH_TYPE_FLATTENED(a, GROUP, actions->ofpacts,\n                                        actions->ofpacts_len) {\n            struct ofgroup *group;\n\n            group = ofproto_group_lookup(ofproto, a->group_id, OVS_VERSION_MAX,\n                                         false);\n            ovs_assert(group);\n\n            /* Leave the rule for the group that is being deleted, if any,\n             * as we still need the list of rules for clean-up. */\n            if (!group->being_deleted) {\n                group_remove_rule(group, rule);\n            }\n        }\n    }\n\n    rule->state = RULE_REMOVED;\n}\n\f\n/* unixctl commands. */\n\nstruct ofproto *\nofproto_lookup(const char *name)\n{\n    struct ofproto *ofproto;\n\n    HMAP_FOR_EACH_WITH_HASH (ofproto, hmap_node, hash_string(name, 0),\n                             &all_ofprotos) {\n        if (!strcmp(ofproto->name, name)) {\n            return ofproto;\n        }\n    }\n    return NULL;\n}\n\nstatic void\nofproto_unixctl_list(struct unixctl_conn *conn, int argc OVS_UNUSED,\n                     const char *argv[] OVS_UNUSED, void *aux OVS_UNUSED)\n{\n    struct ofproto *ofproto;\n    struct ds results;\n\n    ds_init(&results);\n    HMAP_FOR_EACH (ofproto, hmap_node, &all_ofprotos) {\n        ds_put_format(&results, \"%s\\n\", ofproto->name);\n    }\n    unixctl_command_reply(conn, ds_cstr(&results));\n    ds_destroy(&results);\n}\n\nstatic void\nofproto_unixctl_init(void)\n{\n    static bool registered;\n    if (registered) {\n        return;\n    }\n    registered = true;\n\n    unixctl_command_register(\"ofproto/list\", \"\", 0, 0,\n                             ofproto_unixctl_list, NULL);\n}\n"], "filenames": ["ofproto/ofproto.c"], "buggy_code_start_loc": [8486], "buggy_code_end_loc": [8487], "fixing_code_start_loc": [8486], "fixing_code_end_loc": [8487], "type": "CWE-617", "message": "An issue was discovered in Open vSwitch (OvS) 2.7.x through 2.7.6, affecting ofproto_rule_insert__ in ofproto/ofproto.c. During bundle commit, flows that are added in a bundle are applied to ofproto in order. If a flow cannot be added (e.g., the flow action is a go-to for a group id that does not exist), OvS tries to revert back all previous flows that were successfully applied from the same bundle. This is possible since OvS maintains list of old flows that were replaced by flows from the bundle. While reinserting old flows, OvS has an assertion failure due to a check on rule state != RULE_INITIALIZED. This would work for new flows, but for an old flow the rule state is RULE_REMOVED. The assertion failure causes an OvS crash.", "other": {"cve": {"id": "CVE-2018-17205", "sourceIdentifier": "cve@mitre.org", "published": "2018-09-19T16:29:01.003", "lastModified": "2021-08-04T17:14:46.777", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in Open vSwitch (OvS) 2.7.x through 2.7.6, affecting ofproto_rule_insert__ in ofproto/ofproto.c. During bundle commit, flows that are added in a bundle are applied to ofproto in order. If a flow cannot be added (e.g., the flow action is a go-to for a group id that does not exist), OvS tries to revert back all previous flows that were successfully applied from the same bundle. This is possible since OvS maintains list of old flows that were replaced by flows from the bundle. While reinserting old flows, OvS has an assertion failure due to a check on rule state != RULE_INITIALIZED. This would work for new flows, but for an old flow the rule state is RULE_REMOVED. The assertion failure causes an OvS crash."}, {"lang": "es", "value": "Se ha descubierto un problema en Open vSwitch (OvS) en versiones 2.7.x hasta la 2.7.6 que afecta a ofproto_rule_insert__ en ofproto/ofproto.c. Durante el commit bundle, los flujos que se a\u00f1aden a un bundle se aplican a ofproto en orden. Si un flujo no se puede a\u00f1adir (por ejemplo, la acci\u00f3n flow es un go-to para un ID de grupo que no existe), OvS intenta revertir todos los flujos anteriores cuando se aplican con \u00e9xito desde el mismo bundle. Esto es posible porque OvS mantiene una lista de flujos antiguos que se reemplazaron por flujos del bundle. Cuando se vuelve a insertar flujos antiguos, OvS tiene un fallo de aserci\u00f3n debido a una comprobaci\u00f3n en la declaraci\u00f3n de regla != RULE_INITIALIZED. Esto funcionar\u00eda para los nuevos flujos, pero para un flujo antiguo, la sentencia es RULE_REMOVED. El fallo de aserci\u00f3n provoca el cierre inesperado de OvS."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:openvswitch:openvswitch:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndIncluding": "2.7.6", "matchCriteriaId": "682FD155-6A56-43D5-9953-852BB3AA0109"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:openstack:10:*:*:*:*:*:*:*", "matchCriteriaId": "E722FEF7-58A6-47AD-B1D0-DB0B71B0C7AA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:redhat:openstack:13:*:*:*:*:*:*:*", "matchCriteriaId": "704CFA1A-953E-4105-BFBE-406034B83DED"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}], "references": [{"url": "https://access.redhat.com/errata/RHSA-2018:3500", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0053", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:0081", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/openvswitch/ovs/commit/0befd1f3745055c32940f5faf9559be6a14395e6", "source": "cve@mitre.org", "tags": ["Exploit", "Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3873-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/openvswitch/ovs/commit/0befd1f3745055c32940f5faf9559be6a14395e6"}}