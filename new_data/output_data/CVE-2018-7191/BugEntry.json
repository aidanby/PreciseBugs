{"buggy_code": ["/*\n *  TUN - Universal TUN/TAP device driver.\n *  Copyright (C) 1999-2002 Maxim Krasnyansky <maxk@qualcomm.com>\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n *  GNU General Public License for more details.\n *\n *  $Id: tun.c,v 1.15 2002/03/01 02:44:24 maxk Exp $\n */\n\n/*\n *  Changes:\n *\n *  Mike Kershaw <dragorn@kismetwireless.net> 2005/08/14\n *    Add TUNSETLINK ioctl to set the link encapsulation\n *\n *  Mark Smith <markzzzsmith@yahoo.com.au>\n *    Use eth_random_addr() for tap MAC address.\n *\n *  Harald Roelle <harald.roelle@ifi.lmu.de>  2004/04/20\n *    Fixes in packet dropping, queue length setting and queue wakeup.\n *    Increased default tx queue length.\n *    Added ethtool API.\n *    Minor cleanups\n *\n *  Daniel Podlejski <underley@underley.eu.org>\n *    Modifications for 2.3.99-pre5 kernel.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#define DRV_NAME\t\"tun\"\n#define DRV_VERSION\t\"1.6\"\n#define DRV_DESCRIPTION\t\"Universal TUN/TAP device driver\"\n#define DRV_COPYRIGHT\t\"(C) 1999-2004 Max Krasnyansky <maxk@qualcomm.com>\"\n\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/major.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/miscdevice.h>\n#include <linux/ethtool.h>\n#include <linux/rtnetlink.h>\n#include <linux/compat.h>\n#include <linux/if.h>\n#include <linux/if_arp.h>\n#include <linux/if_ether.h>\n#include <linux/if_tun.h>\n#include <linux/if_vlan.h>\n#include <linux/crc32.h>\n#include <linux/nsproxy.h>\n#include <linux/virtio_net.h>\n#include <linux/rcupdate.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/rtnetlink.h>\n#include <net/sock.h>\n#include <linux/seq_file.h>\n#include <linux/uio.h>\n#include <linux/skb_array.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n\n#include <linux/uaccess.h>\n\n/* Uncomment to enable debugging */\n/* #define TUN_DEBUG 1 */\n\n#ifdef TUN_DEBUG\nstatic int debug;\n\n#define tun_debug(level, tun, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (tun->debug)\t\t\t\t\t\t\\\n\t\tnetdev_printk(level, tun->dev, fmt, ##args);\t\\\n} while (0)\n#define DBG1(level, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (debug == 2)\t\t\t\t\t\t\\\n\t\tprintk(level fmt, ##args);\t\t\t\\\n} while (0)\n#else\n#define tun_debug(level, tun, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(level, tun->dev, fmt, ##args);\t\\\n} while (0)\n#define DBG1(level, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tprintk(level fmt, ##args);\t\t\t\\\n} while (0)\n#endif\n\n#define TUN_HEADROOM 256\n#define TUN_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)\n\n/* TUN device flags */\n\n/* IFF_ATTACH_QUEUE is never stored in device flags,\n * overload it to mean fasync when stored there.\n */\n#define TUN_FASYNC\tIFF_ATTACH_QUEUE\n/* High bits in flags field are unused. */\n#define TUN_VNET_LE     0x80000000\n#define TUN_VNET_BE     0x40000000\n\n#define TUN_FEATURES (IFF_NO_PI | IFF_ONE_QUEUE | IFF_VNET_HDR | \\\n\t\t      IFF_MULTI_QUEUE)\n#define GOODCOPY_LEN 128\n\n#define FLT_EXACT_COUNT 8\nstruct tap_filter {\n\tunsigned int    count;    /* Number of addrs. Zero means disabled */\n\tu32             mask[2];  /* Mask of the hashed addrs */\n\tunsigned char\taddr[FLT_EXACT_COUNT][ETH_ALEN];\n};\n\n/* MAX_TAP_QUEUES 256 is chosen to allow rx/tx queues to be equal\n * to max number of VCPUs in guest. */\n#define MAX_TAP_QUEUES 256\n#define MAX_TAP_FLOWS  4096\n\n#define TUN_FLOW_EXPIRE (3 * HZ)\n\nstruct tun_pcpu_stats {\n\tu64 rx_packets;\n\tu64 rx_bytes;\n\tu64 tx_packets;\n\tu64 tx_bytes;\n\tstruct u64_stats_sync syncp;\n\tu32 rx_dropped;\n\tu32 tx_dropped;\n\tu32 rx_frame_errors;\n};\n\n/* A tun_file connects an open character device to a tuntap netdevice. It\n * also contains all socket related structures (except sock_fprog and tap_filter)\n * to serve as one transmit queue for tuntap device. The sock_fprog and\n * tap_filter were kept in tun_struct since they were used for filtering for the\n * netdevice not for a specific queue (at least I didn't see the requirement for\n * this).\n *\n * RCU usage:\n * The tun_file and tun_struct are loosely coupled, the pointer from one to the\n * other can only be read while rcu_read_lock or rtnl_lock is held.\n */\nstruct tun_file {\n\tstruct sock sk;\n\tstruct socket socket;\n\tstruct socket_wq wq;\n\tstruct tun_struct __rcu *tun;\n\tstruct fasync_struct *fasync;\n\t/* only used for fasnyc */\n\tunsigned int flags;\n\tunion {\n\t\tu16 queue_index;\n\t\tunsigned int ifindex;\n\t};\n\tstruct list_head next;\n\tstruct tun_struct *detached;\n\tstruct skb_array tx_array;\n};\n\nstruct tun_flow_entry {\n\tstruct hlist_node hash_link;\n\tstruct rcu_head rcu;\n\tstruct tun_struct *tun;\n\n\tu32 rxhash;\n\tu32 rps_rxhash;\n\tint queue_index;\n\tunsigned long updated;\n};\n\n#define TUN_NUM_FLOW_ENTRIES 1024\n\n/* Since the socket were moved to tun_file, to preserve the behavior of persist\n * device, socket filter, sndbuf and vnet header size were restore when the\n * file were attached to a persist device.\n */\nstruct tun_struct {\n\tstruct tun_file __rcu\t*tfiles[MAX_TAP_QUEUES];\n\tunsigned int            numqueues;\n\tunsigned int \t\tflags;\n\tkuid_t\t\t\towner;\n\tkgid_t\t\t\tgroup;\n\n\tstruct net_device\t*dev;\n\tnetdev_features_t\tset_features;\n#define TUN_USER_FEATURES (NETIF_F_HW_CSUM|NETIF_F_TSO_ECN|NETIF_F_TSO| \\\n\t\t\t  NETIF_F_TSO6)\n\n\tint\t\t\talign;\n\tint\t\t\tvnet_hdr_sz;\n\tint\t\t\tsndbuf;\n\tstruct tap_filter\ttxflt;\n\tstruct sock_fprog\tfprog;\n\t/* protected by rtnl lock */\n\tbool\t\t\tfilter_attached;\n#ifdef TUN_DEBUG\n\tint debug;\n#endif\n\tspinlock_t lock;\n\tstruct hlist_head flows[TUN_NUM_FLOW_ENTRIES];\n\tstruct timer_list flow_gc_timer;\n\tunsigned long ageing_time;\n\tunsigned int numdisabled;\n\tstruct list_head disabled;\n\tvoid *security;\n\tu32 flow_count;\n\tu32 rx_batched;\n\tstruct tun_pcpu_stats __percpu *pcpu_stats;\n\tstruct bpf_prog __rcu *xdp_prog;\n};\n\n#ifdef CONFIG_TUN_VNET_CROSS_LE\nstatic inline bool tun_legacy_is_little_endian(struct tun_struct *tun)\n{\n\treturn tun->flags & TUN_VNET_BE ? false :\n\t\tvirtio_legacy_is_little_endian();\n}\n\nstatic long tun_get_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\tint be = !!(tun->flags & TUN_VNET_BE);\n\n\tif (put_user(be, argp))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long tun_set_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\tint be;\n\n\tif (get_user(be, argp))\n\t\treturn -EFAULT;\n\n\tif (be)\n\t\ttun->flags |= TUN_VNET_BE;\n\telse\n\t\ttun->flags &= ~TUN_VNET_BE;\n\n\treturn 0;\n}\n#else\nstatic inline bool tun_legacy_is_little_endian(struct tun_struct *tun)\n{\n\treturn virtio_legacy_is_little_endian();\n}\n\nstatic long tun_get_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\treturn -EINVAL;\n}\n\nstatic long tun_set_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\treturn -EINVAL;\n}\n#endif /* CONFIG_TUN_VNET_CROSS_LE */\n\nstatic inline bool tun_is_little_endian(struct tun_struct *tun)\n{\n\treturn tun->flags & TUN_VNET_LE ||\n\t\ttun_legacy_is_little_endian(tun);\n}\n\nstatic inline u16 tun16_to_cpu(struct tun_struct *tun, __virtio16 val)\n{\n\treturn __virtio16_to_cpu(tun_is_little_endian(tun), val);\n}\n\nstatic inline __virtio16 cpu_to_tun16(struct tun_struct *tun, u16 val)\n{\n\treturn __cpu_to_virtio16(tun_is_little_endian(tun), val);\n}\n\nstatic inline u32 tun_hashfn(u32 rxhash)\n{\n\treturn rxhash & 0x3ff;\n}\n\nstatic struct tun_flow_entry *tun_flow_find(struct hlist_head *head, u32 rxhash)\n{\n\tstruct tun_flow_entry *e;\n\n\thlist_for_each_entry_rcu(e, head, hash_link) {\n\t\tif (e->rxhash == rxhash)\n\t\t\treturn e;\n\t}\n\treturn NULL;\n}\n\nstatic struct tun_flow_entry *tun_flow_create(struct tun_struct *tun,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      u32 rxhash, u16 queue_index)\n{\n\tstruct tun_flow_entry *e = kmalloc(sizeof(*e), GFP_ATOMIC);\n\n\tif (e) {\n\t\ttun_debug(KERN_INFO, tun, \"create flow: hash %u index %u\\n\",\n\t\t\t  rxhash, queue_index);\n\t\te->updated = jiffies;\n\t\te->rxhash = rxhash;\n\t\te->rps_rxhash = 0;\n\t\te->queue_index = queue_index;\n\t\te->tun = tun;\n\t\thlist_add_head_rcu(&e->hash_link, head);\n\t\t++tun->flow_count;\n\t}\n\treturn e;\n}\n\nstatic void tun_flow_delete(struct tun_struct *tun, struct tun_flow_entry *e)\n{\n\ttun_debug(KERN_INFO, tun, \"delete flow: hash %u index %u\\n\",\n\t\t  e->rxhash, e->queue_index);\n\thlist_del_rcu(&e->hash_link);\n\tkfree_rcu(e, rcu);\n\t--tun->flow_count;\n}\n\nstatic void tun_flow_flush(struct tun_struct *tun)\n{\n\tint i;\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link)\n\t\t\ttun_flow_delete(tun, e);\n\t}\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_delete_by_queue(struct tun_struct *tun, u16 queue_index)\n{\n\tint i;\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link) {\n\t\t\tif (e->queue_index == queue_index)\n\t\t\t\ttun_flow_delete(tun, e);\n\t\t}\n\t}\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_cleanup(unsigned long data)\n{\n\tstruct tun_struct *tun = (struct tun_struct *)data;\n\tunsigned long delay = tun->ageing_time;\n\tunsigned long next_timer = jiffies + delay;\n\tunsigned long count = 0;\n\tint i;\n\n\ttun_debug(KERN_INFO, tun, \"tun_flow_cleanup\\n\");\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link) {\n\t\t\tunsigned long this_timer;\n\t\t\tcount++;\n\t\t\tthis_timer = e->updated + delay;\n\t\t\tif (time_before_eq(this_timer, jiffies))\n\t\t\t\ttun_flow_delete(tun, e);\n\t\t\telse if (time_before(this_timer, next_timer))\n\t\t\t\tnext_timer = this_timer;\n\t\t}\n\t}\n\n\tif (count)\n\t\tmod_timer(&tun->flow_gc_timer, round_jiffies_up(next_timer));\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_update(struct tun_struct *tun, u32 rxhash,\n\t\t\t    struct tun_file *tfile)\n{\n\tstruct hlist_head *head;\n\tstruct tun_flow_entry *e;\n\tunsigned long delay = tun->ageing_time;\n\tu16 queue_index = tfile->queue_index;\n\n\tif (!rxhash)\n\t\treturn;\n\telse\n\t\thead = &tun->flows[tun_hashfn(rxhash)];\n\n\trcu_read_lock();\n\n\t/* We may get a very small possibility of OOO during switching, not\n\t * worth to optimize.*/\n\tif (tun->numqueues == 1 || tfile->detached)\n\t\tgoto unlock;\n\n\te = tun_flow_find(head, rxhash);\n\tif (likely(e)) {\n\t\t/* TODO: keep queueing to old queue until it's empty? */\n\t\te->queue_index = queue_index;\n\t\te->updated = jiffies;\n\t\tsock_rps_record_flow_hash(e->rps_rxhash);\n\t} else {\n\t\tspin_lock_bh(&tun->lock);\n\t\tif (!tun_flow_find(head, rxhash) &&\n\t\t    tun->flow_count < MAX_TAP_FLOWS)\n\t\t\ttun_flow_create(tun, head, rxhash, queue_index);\n\n\t\tif (!timer_pending(&tun->flow_gc_timer))\n\t\t\tmod_timer(&tun->flow_gc_timer,\n\t\t\t\t  round_jiffies_up(jiffies + delay));\n\t\tspin_unlock_bh(&tun->lock);\n\t}\n\nunlock:\n\trcu_read_unlock();\n}\n\n/**\n * Save the hash received in the stack receive path and update the\n * flow_hash table accordingly.\n */\nstatic inline void tun_flow_save_rps_rxhash(struct tun_flow_entry *e, u32 hash)\n{\n\tif (unlikely(e->rps_rxhash != hash))\n\t\te->rps_rxhash = hash;\n}\n\n/* We try to identify a flow through its rxhash first. The reason that\n * we do not check rxq no. is because some cards(e.g 82599), chooses\n * the rxq based on the txq where the last packet of the flow comes. As\n * the userspace application move between processors, we may get a\n * different rxq no. here. If we could not get rxhash, then we would\n * hope the rxq no. may help here.\n */\nstatic u16 tun_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    void *accel_priv, select_queue_fallback_t fallback)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct tun_flow_entry *e;\n\tu32 txq = 0;\n\tu32 numqueues = 0;\n\n\trcu_read_lock();\n\tnumqueues = ACCESS_ONCE(tun->numqueues);\n\n\ttxq = __skb_get_hash_symmetric(skb);\n\tif (txq) {\n\t\te = tun_flow_find(&tun->flows[tun_hashfn(txq)], txq);\n\t\tif (e) {\n\t\t\ttun_flow_save_rps_rxhash(e, txq);\n\t\t\ttxq = e->queue_index;\n\t\t} else\n\t\t\t/* use multiply and shift instead of expensive divide */\n\t\t\ttxq = ((u64)txq * numqueues) >> 32;\n\t} else if (likely(skb_rx_queue_recorded(skb))) {\n\t\ttxq = skb_get_rx_queue(skb);\n\t\twhile (unlikely(txq >= numqueues))\n\t\t\ttxq -= numqueues;\n\t}\n\n\trcu_read_unlock();\n\treturn txq;\n}\n\nstatic inline bool tun_not_capable(struct tun_struct *tun)\n{\n\tconst struct cred *cred = current_cred();\n\tstruct net *net = dev_net(tun->dev);\n\n\treturn ((uid_valid(tun->owner) && !uid_eq(cred->euid, tun->owner)) ||\n\t\t  (gid_valid(tun->group) && !in_egroup_p(tun->group))) &&\n\t\t!ns_capable(net->user_ns, CAP_NET_ADMIN);\n}\n\nstatic void tun_set_real_num_queues(struct tun_struct *tun)\n{\n\tnetif_set_real_num_tx_queues(tun->dev, tun->numqueues);\n\tnetif_set_real_num_rx_queues(tun->dev, tun->numqueues);\n}\n\nstatic void tun_disable_queue(struct tun_struct *tun, struct tun_file *tfile)\n{\n\ttfile->detached = tun;\n\tlist_add_tail(&tfile->next, &tun->disabled);\n\t++tun->numdisabled;\n}\n\nstatic struct tun_struct *tun_enable_queue(struct tun_file *tfile)\n{\n\tstruct tun_struct *tun = tfile->detached;\n\n\ttfile->detached = NULL;\n\tlist_del_init(&tfile->next);\n\t--tun->numdisabled;\n\treturn tun;\n}\n\nstatic void tun_queue_purge(struct tun_file *tfile)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_array_consume(&tfile->tx_array)) != NULL)\n\t\tkfree_skb(skb);\n\n\tskb_queue_purge(&tfile->sk.sk_write_queue);\n\tskb_queue_purge(&tfile->sk.sk_error_queue);\n}\n\nstatic void __tun_detach(struct tun_file *tfile, bool clean)\n{\n\tstruct tun_file *ntfile;\n\tstruct tun_struct *tun;\n\n\ttun = rtnl_dereference(tfile->tun);\n\n\tif (tun && !tfile->detached) {\n\t\tu16 index = tfile->queue_index;\n\t\tBUG_ON(index >= tun->numqueues);\n\n\t\trcu_assign_pointer(tun->tfiles[index],\n\t\t\t\t   tun->tfiles[tun->numqueues - 1]);\n\t\tntfile = rtnl_dereference(tun->tfiles[index]);\n\t\tntfile->queue_index = index;\n\n\t\t--tun->numqueues;\n\t\tif (clean) {\n\t\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t\t\tsock_put(&tfile->sk);\n\t\t} else\n\t\t\ttun_disable_queue(tun, tfile);\n\n\t\tsynchronize_net();\n\t\ttun_flow_delete_by_queue(tun, tun->numqueues + 1);\n\t\t/* Drop read queue */\n\t\ttun_queue_purge(tfile);\n\t\ttun_set_real_num_queues(tun);\n\t} else if (tfile->detached && clean) {\n\t\ttun = tun_enable_queue(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\n\tif (clean) {\n\t\tif (tun && tun->numqueues == 0 && tun->numdisabled == 0) {\n\t\t\tnetif_carrier_off(tun->dev);\n\n\t\t\tif (!(tun->flags & IFF_PERSIST) &&\n\t\t\t    tun->dev->reg_state == NETREG_REGISTERED)\n\t\t\t\tunregister_netdevice(tun->dev);\n\t\t}\n\t\tif (tun)\n\t\t\tskb_array_cleanup(&tfile->tx_array);\n\t\tsock_put(&tfile->sk);\n\t}\n}\n\nstatic void tun_detach(struct tun_file *tfile, bool clean)\n{\n\trtnl_lock();\n\t__tun_detach(tfile, clean);\n\trtnl_unlock();\n}\n\nstatic void tun_detach_all(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct bpf_prog *xdp_prog = rtnl_dereference(tun->xdp_prog);\n\tstruct tun_file *tfile, *tmp;\n\tint i, n = tun->numqueues;\n\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tBUG_ON(!tfile);\n\t\ttfile->socket.sk->sk_shutdown = RCV_SHUTDOWN;\n\t\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t\t--tun->numqueues;\n\t}\n\tlist_for_each_entry(tfile, &tun->disabled, next) {\n\t\ttfile->socket.sk->sk_shutdown = RCV_SHUTDOWN;\n\t\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t}\n\tBUG_ON(tun->numqueues != 0);\n\n\tsynchronize_net();\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\t/* Drop read queue */\n\t\ttun_queue_purge(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\tlist_for_each_entry_safe(tfile, tmp, &tun->disabled, next) {\n\t\ttun_enable_queue(tfile);\n\t\ttun_queue_purge(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\tBUG_ON(tun->numdisabled != 0);\n\n\tif (xdp_prog)\n\t\tbpf_prog_put(xdp_prog);\n\n\tif (tun->flags & IFF_PERSIST)\n\t\tmodule_put(THIS_MODULE);\n}\n\nstatic int tun_attach(struct tun_struct *tun, struct file *file, bool skip_filter)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev = tun->dev;\n\tint err;\n\n\terr = security_tun_dev_attach(tfile->socket.sk, tun->security);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -EINVAL;\n\tif (rtnl_dereference(tfile->tun) && !tfile->detached)\n\t\tgoto out;\n\n\terr = -EBUSY;\n\tif (!(tun->flags & IFF_MULTI_QUEUE) && tun->numqueues == 1)\n\t\tgoto out;\n\n\terr = -E2BIG;\n\tif (!tfile->detached &&\n\t    tun->numqueues + tun->numdisabled == MAX_TAP_QUEUES)\n\t\tgoto out;\n\n\terr = 0;\n\n\t/* Re-attach the filter to persist device */\n\tif (!skip_filter && (tun->filter_attached == true)) {\n\t\tlock_sock(tfile->socket.sk);\n\t\terr = sk_attach_filter(&tun->fprog, tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t\tif (!err)\n\t\t\tgoto out;\n\t}\n\n\tif (!tfile->detached &&\n\t    skb_array_init(&tfile->tx_array, dev->tx_queue_len, GFP_KERNEL)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\ttfile->queue_index = tun->numqueues;\n\ttfile->socket.sk->sk_shutdown &= ~RCV_SHUTDOWN;\n\trcu_assign_pointer(tfile->tun, tun);\n\trcu_assign_pointer(tun->tfiles[tun->numqueues], tfile);\n\ttun->numqueues++;\n\n\tif (tfile->detached)\n\t\ttun_enable_queue(tfile);\n\telse\n\t\tsock_hold(&tfile->sk);\n\n\ttun_set_real_num_queues(tun);\n\n\t/* device is allowed to go away first, so no need to hold extra\n\t * refcnt.\n\t */\n\nout:\n\treturn err;\n}\n\nstatic struct tun_struct *__tun_get(struct tun_file *tfile)\n{\n\tstruct tun_struct *tun;\n\n\trcu_read_lock();\n\ttun = rcu_dereference(tfile->tun);\n\tif (tun)\n\t\tdev_hold(tun->dev);\n\trcu_read_unlock();\n\n\treturn tun;\n}\n\nstatic struct tun_struct *tun_get(struct file *file)\n{\n\treturn __tun_get(file->private_data);\n}\n\nstatic void tun_put(struct tun_struct *tun)\n{\n\tdev_put(tun->dev);\n}\n\n/* TAP filtering */\nstatic void addr_hash_set(u32 *mask, const u8 *addr)\n{\n\tint n = ether_crc(ETH_ALEN, addr) >> 26;\n\tmask[n >> 5] |= (1 << (n & 31));\n}\n\nstatic unsigned int addr_hash_test(const u32 *mask, const u8 *addr)\n{\n\tint n = ether_crc(ETH_ALEN, addr) >> 26;\n\treturn mask[n >> 5] & (1 << (n & 31));\n}\n\nstatic int update_filter(struct tap_filter *filter, void __user *arg)\n{\n\tstruct { u8 u[ETH_ALEN]; } *addr;\n\tstruct tun_filter uf;\n\tint err, alen, n, nexact;\n\n\tif (copy_from_user(&uf, arg, sizeof(uf)))\n\t\treturn -EFAULT;\n\n\tif (!uf.count) {\n\t\t/* Disabled */\n\t\tfilter->count = 0;\n\t\treturn 0;\n\t}\n\n\talen = ETH_ALEN * uf.count;\n\taddr = memdup_user(arg + sizeof(uf), alen);\n\tif (IS_ERR(addr))\n\t\treturn PTR_ERR(addr);\n\n\t/* The filter is updated without holding any locks. Which is\n\t * perfectly safe. We disable it first and in the worst\n\t * case we'll accept a few undesired packets. */\n\tfilter->count = 0;\n\twmb();\n\n\t/* Use first set of addresses as an exact filter */\n\tfor (n = 0; n < uf.count && n < FLT_EXACT_COUNT; n++)\n\t\tmemcpy(filter->addr[n], addr[n].u, ETH_ALEN);\n\n\tnexact = n;\n\n\t/* Remaining multicast addresses are hashed,\n\t * unicast will leave the filter disabled. */\n\tmemset(filter->mask, 0, sizeof(filter->mask));\n\tfor (; n < uf.count; n++) {\n\t\tif (!is_multicast_ether_addr(addr[n].u)) {\n\t\t\terr = 0; /* no filter */\n\t\t\tgoto free_addr;\n\t\t}\n\t\taddr_hash_set(filter->mask, addr[n].u);\n\t}\n\n\t/* For ALLMULTI just set the mask to all ones.\n\t * This overrides the mask populated above. */\n\tif ((uf.flags & TUN_FLT_ALLMULTI))\n\t\tmemset(filter->mask, ~0, sizeof(filter->mask));\n\n\t/* Now enable the filter */\n\twmb();\n\tfilter->count = nexact;\n\n\t/* Return the number of exact filters */\n\terr = nexact;\nfree_addr:\n\tkfree(addr);\n\treturn err;\n}\n\n/* Returns: 0 - drop, !=0 - accept */\nstatic int run_filter(struct tap_filter *filter, const struct sk_buff *skb)\n{\n\t/* Cannot use eth_hdr(skb) here because skb_mac_hdr() is incorrect\n\t * at this point. */\n\tstruct ethhdr *eh = (struct ethhdr *) skb->data;\n\tint i;\n\n\t/* Exact match */\n\tfor (i = 0; i < filter->count; i++)\n\t\tif (ether_addr_equal(eh->h_dest, filter->addr[i]))\n\t\t\treturn 1;\n\n\t/* Inexact match (multicast only) */\n\tif (is_multicast_ether_addr(eh->h_dest))\n\t\treturn addr_hash_test(filter->mask, eh->h_dest);\n\n\treturn 0;\n}\n\n/*\n * Checks whether the packet is accepted or not.\n * Returns: 0 - drop, !=0 - accept\n */\nstatic int check_filter(struct tap_filter *filter, const struct sk_buff *skb)\n{\n\tif (!filter->count)\n\t\treturn 1;\n\n\treturn run_filter(filter, skb);\n}\n\n/* Network device part of the driver */\n\nstatic const struct ethtool_ops tun_ethtool_ops;\n\n/* Net device detach from fd. */\nstatic void tun_net_uninit(struct net_device *dev)\n{\n\ttun_detach_all(dev);\n}\n\n/* Net device open. */\nstatic int tun_net_open(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tint i;\n\n\tnetif_tx_start_all_queues(dev);\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\tstruct tun_file *tfile;\n\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\ttfile->socket.sk->sk_write_space(tfile->socket.sk);\n\t}\n\n\treturn 0;\n}\n\n/* Net device close. */\nstatic int tun_net_close(struct net_device *dev)\n{\n\tnetif_tx_stop_all_queues(dev);\n\treturn 0;\n}\n\n/* Net device start xmit */\nstatic netdev_tx_t tun_net_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tint txq = skb->queue_mapping;\n\tstruct tun_file *tfile;\n\tu32 numqueues = 0;\n\n\trcu_read_lock();\n\ttfile = rcu_dereference(tun->tfiles[txq]);\n\tnumqueues = ACCESS_ONCE(tun->numqueues);\n\n\t/* Drop packet if interface is not attached */\n\tif (txq >= numqueues)\n\t\tgoto drop;\n\n#ifdef CONFIG_RPS\n\tif (numqueues == 1 && static_key_false(&rps_needed)) {\n\t\t/* Select queue was not called for the skbuff, so we extract the\n\t\t * RPS hash and save it into the flow_table here.\n\t\t */\n\t\t__u32 rxhash;\n\n\t\trxhash = __skb_get_hash_symmetric(skb);\n\t\tif (rxhash) {\n\t\t\tstruct tun_flow_entry *e;\n\t\t\te = tun_flow_find(&tun->flows[tun_hashfn(rxhash)],\n\t\t\t\t\trxhash);\n\t\t\tif (e)\n\t\t\t\ttun_flow_save_rps_rxhash(e, rxhash);\n\t\t}\n\t}\n#endif\n\n\ttun_debug(KERN_INFO, tun, \"tun_net_xmit %d\\n\", skb->len);\n\n\tBUG_ON(!tfile);\n\n\t/* Drop if the filter does not like it.\n\t * This is a noop if the filter is disabled.\n\t * Filter can be enabled only for the TAP devices. */\n\tif (!check_filter(&tun->txflt, skb))\n\t\tgoto drop;\n\n\tif (tfile->socket.sk->sk_filter &&\n\t    sk_filter(tfile->socket.sk, skb))\n\t\tgoto drop;\n\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\tgoto drop;\n\n\tskb_tx_timestamp(skb);\n\n\t/* Orphan the skb - required as we might hang on to it\n\t * for indefinite time.\n\t */\n\tskb_orphan(skb);\n\n\tnf_reset(skb);\n\n\tif (skb_array_produce(&tfile->tx_array, skb))\n\t\tgoto drop;\n\n\t/* Notify and wake up reader process */\n\tif (tfile->flags & TUN_FASYNC)\n\t\tkill_fasync(&tfile->fasync, SIGIO, POLL_IN);\n\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\n\trcu_read_unlock();\n\treturn NETDEV_TX_OK;\n\ndrop:\n\tthis_cpu_inc(tun->pcpu_stats->tx_dropped);\n\tskb_tx_error(skb);\n\tkfree_skb(skb);\n\trcu_read_unlock();\n\treturn NET_XMIT_DROP;\n}\n\nstatic void tun_net_mclist(struct net_device *dev)\n{\n\t/*\n\t * This callback is supposed to deal with mc filter in\n\t * _rx_ path and has nothing to do with the _tx_ path.\n\t * In rx path we always accept everything userspace gives us.\n\t */\n}\n\nstatic netdev_features_t tun_net_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\treturn (features & tun->set_features) | (features & ~TUN_USER_FEATURES);\n}\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void tun_poll_controller(struct net_device *dev)\n{\n\t/*\n\t * Tun only receives frames when:\n\t * 1) the char device endpoint gets data from user space\n\t * 2) the tun socket gets a sendmsg call from user space\n\t * Since both of those are synchronous operations, we are guaranteed\n\t * never to have pending data when we poll for it\n\t * so there is nothing to do here but return.\n\t * We need this though so netpoll recognizes us as an interface that\n\t * supports polling, which enables bridge devices in virt setups to\n\t * still use netconsole\n\t */\n\treturn;\n}\n#endif\n\nstatic void tun_set_headroom(struct net_device *dev, int new_hr)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (new_hr < NET_SKB_PAD)\n\t\tnew_hr = NET_SKB_PAD;\n\n\ttun->align = new_hr;\n}\n\nstatic void\ntun_net_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tu32 rx_dropped = 0, tx_dropped = 0, rx_frame_errors = 0;\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct tun_pcpu_stats *p;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tu64 rxpackets, rxbytes, txpackets, txbytes;\n\t\tunsigned int start;\n\n\t\tp = per_cpu_ptr(tun->pcpu_stats, i);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&p->syncp);\n\t\t\trxpackets\t= p->rx_packets;\n\t\t\trxbytes\t\t= p->rx_bytes;\n\t\t\ttxpackets\t= p->tx_packets;\n\t\t\ttxbytes\t\t= p->tx_bytes;\n\t\t} while (u64_stats_fetch_retry(&p->syncp, start));\n\n\t\tstats->rx_packets\t+= rxpackets;\n\t\tstats->rx_bytes\t\t+= rxbytes;\n\t\tstats->tx_packets\t+= txpackets;\n\t\tstats->tx_bytes\t\t+= txbytes;\n\n\t\t/* u32 counters */\n\t\trx_dropped\t+= p->rx_dropped;\n\t\trx_frame_errors\t+= p->rx_frame_errors;\n\t\ttx_dropped\t+= p->tx_dropped;\n\t}\n\tstats->rx_dropped  = rx_dropped;\n\tstats->rx_frame_errors = rx_frame_errors;\n\tstats->tx_dropped = tx_dropped;\n}\n\nstatic int tun_xdp_set(struct net_device *dev, struct bpf_prog *prog,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\n\told_prog = rtnl_dereference(tun->xdp_prog);\n\trcu_assign_pointer(tun->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\treturn 0;\n}\n\nstatic u32 tun_xdp_query(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tconst struct bpf_prog *xdp_prog;\n\n\txdp_prog = rtnl_dereference(tun->xdp_prog);\n\tif (xdp_prog)\n\t\treturn xdp_prog->aux->id;\n\n\treturn 0;\n}\n\nstatic int tun_xdp(struct net_device *dev, struct netdev_xdp *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn tun_xdp_set(dev, xdp->prog, xdp->extack);\n\tcase XDP_QUERY_PROG:\n\t\txdp->prog_id = tun_xdp_query(dev);\n\t\txdp->prog_attached = !!xdp->prog_id;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct net_device_ops tun_netdev_ops = {\n\t.ndo_uninit\t\t= tun_net_uninit,\n\t.ndo_open\t\t= tun_net_open,\n\t.ndo_stop\t\t= tun_net_close,\n\t.ndo_start_xmit\t\t= tun_net_xmit,\n\t.ndo_fix_features\t= tun_net_fix_features,\n\t.ndo_select_queue\t= tun_select_queue,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tun_poll_controller,\n#endif\n\t.ndo_set_rx_headroom\t= tun_set_headroom,\n\t.ndo_get_stats64\t= tun_net_get_stats64,\n};\n\nstatic const struct net_device_ops tap_netdev_ops = {\n\t.ndo_uninit\t\t= tun_net_uninit,\n\t.ndo_open\t\t= tun_net_open,\n\t.ndo_stop\t\t= tun_net_close,\n\t.ndo_start_xmit\t\t= tun_net_xmit,\n\t.ndo_fix_features\t= tun_net_fix_features,\n\t.ndo_set_rx_mode\t= tun_net_mclist,\n\t.ndo_set_mac_address\t= eth_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_select_queue\t= tun_select_queue,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tun_poll_controller,\n#endif\n\t.ndo_features_check\t= passthru_features_check,\n\t.ndo_set_rx_headroom\t= tun_set_headroom,\n\t.ndo_get_stats64\t= tun_net_get_stats64,\n\t.ndo_xdp\t\t= tun_xdp,\n};\n\nstatic void tun_flow_init(struct tun_struct *tun)\n{\n\tint i;\n\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++)\n\t\tINIT_HLIST_HEAD(&tun->flows[i]);\n\n\ttun->ageing_time = TUN_FLOW_EXPIRE;\n\tsetup_timer(&tun->flow_gc_timer, tun_flow_cleanup, (unsigned long)tun);\n\tmod_timer(&tun->flow_gc_timer,\n\t\t  round_jiffies_up(jiffies + tun->ageing_time));\n}\n\nstatic void tun_flow_uninit(struct tun_struct *tun)\n{\n\tdel_timer_sync(&tun->flow_gc_timer);\n\ttun_flow_flush(tun);\n}\n\n#define MIN_MTU 68\n#define MAX_MTU 65535\n\n/* Initialize net device. */\nstatic void tun_net_init(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tdev->netdev_ops = &tun_netdev_ops;\n\n\t\t/* Point-to-Point TUN Device */\n\t\tdev->hard_header_len = 0;\n\t\tdev->addr_len = 0;\n\t\tdev->mtu = 1500;\n\n\t\t/* Zero header length */\n\t\tdev->type = ARPHRD_NONE;\n\t\tdev->flags = IFF_POINTOPOINT | IFF_NOARP | IFF_MULTICAST;\n\t\tbreak;\n\n\tcase IFF_TAP:\n\t\tdev->netdev_ops = &tap_netdev_ops;\n\t\t/* Ethernet TAP Device */\n\t\tether_setup(dev);\n\t\tdev->priv_flags &= ~IFF_TX_SKB_SHARING;\n\t\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\n\t\teth_hw_addr_random(dev);\n\n\t\tbreak;\n\t}\n\n\tdev->min_mtu = MIN_MTU;\n\tdev->max_mtu = MAX_MTU - dev->hard_header_len;\n}\n\n/* Character device part */\n\n/* Poll */\nstatic unsigned int tun_chr_poll(struct file *file, poll_table *wait)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tstruct sock *sk;\n\tunsigned int mask = 0;\n\n\tif (!tun)\n\t\treturn POLLERR;\n\n\tsk = tfile->socket.sk;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_poll\\n\");\n\n\tpoll_wait(file, sk_sleep(sk), wait);\n\n\tif (!skb_array_empty(&tfile->tx_array))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\tif (tun->dev->flags & IFF_UP &&\n\t    (sock_writeable(sk) ||\n\t     (!test_and_set_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags) &&\n\t      sock_writeable(sk))))\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\tif (tun->dev->reg_state != NETREG_REGISTERED)\n\t\tmask = POLLERR;\n\n\ttun_put(tun);\n\treturn mask;\n}\n\n/* prepad is the amount to reserve at front.  len is length after that.\n * linear is a hint as to how much to copy (usually headers). */\nstatic struct sk_buff *tun_alloc_skb(struct tun_file *tfile,\n\t\t\t\t     size_t prepad, size_t len,\n\t\t\t\t     size_t linear, int noblock)\n{\n\tstruct sock *sk = tfile->socket.sk;\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   &err, 0);\n\tif (!skb)\n\t\treturn ERR_PTR(err);\n\n\tskb_reserve(skb, prepad);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic void tun_rx_batched(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t   struct sk_buff *skb, int more)\n{\n\tstruct sk_buff_head *queue = &tfile->sk.sk_write_queue;\n\tstruct sk_buff_head process_queue;\n\tu32 rx_batched = tun->rx_batched;\n\tbool rcv = false;\n\n\tif (!rx_batched || (!more && skb_queue_empty(queue))) {\n\t\tlocal_bh_disable();\n\t\tnetif_receive_skb(skb);\n\t\tlocal_bh_enable();\n\t\treturn;\n\t}\n\n\tspin_lock(&queue->lock);\n\tif (!more || skb_queue_len(queue) == rx_batched) {\n\t\t__skb_queue_head_init(&process_queue);\n\t\tskb_queue_splice_tail_init(queue, &process_queue);\n\t\trcv = true;\n\t} else {\n\t\t__skb_queue_tail(queue, skb);\n\t}\n\tspin_unlock(&queue->lock);\n\n\tif (rcv) {\n\t\tstruct sk_buff *nskb;\n\n\t\tlocal_bh_disable();\n\t\twhile ((nskb = __skb_dequeue(&process_queue)))\n\t\t\tnetif_receive_skb(nskb);\n\t\tnetif_receive_skb(skb);\n\t\tlocal_bh_enable();\n\t}\n}\n\nstatic bool tun_can_build_skb(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t      int len, int noblock, bool zerocopy)\n{\n\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\treturn false;\n\n\tif (tfile->socket.sk->sk_sndbuf != INT_MAX)\n\t\treturn false;\n\n\tif (!noblock)\n\t\treturn false;\n\n\tif (zerocopy)\n\t\treturn false;\n\n\tif (SKB_DATA_ALIGN(len + TUN_RX_PAD) +\n\t    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) > PAGE_SIZE)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct sk_buff *tun_build_skb(struct tun_struct *tun,\n\t\t\t\t     struct tun_file *tfile,\n\t\t\t\t     struct iov_iter *from,\n\t\t\t\t     struct virtio_net_hdr *hdr,\n\t\t\t\t     int len, int *skb_xdp)\n{\n\tstruct page_frag *alloc_frag = &current->task_frag;\n\tstruct sk_buff *skb;\n\tstruct bpf_prog *xdp_prog;\n\tint buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tunsigned int delta = 0;\n\tchar *buf;\n\tsize_t copied;\n\tbool xdp_xmit = false;\n\tint err, pad = TUN_RX_PAD;\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(tun->xdp_prog);\n\tif (xdp_prog)\n\t\tpad += TUN_HEADROOM;\n\tbuflen += SKB_DATA_ALIGN(len + pad);\n\trcu_read_unlock();\n\n\tif (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf = (char *)page_address(alloc_frag->page) + alloc_frag->offset;\n\tcopied = copy_page_from_iter(alloc_frag->page,\n\t\t\t\t     alloc_frag->offset + pad,\n\t\t\t\t     len, from);\n\tif (copied != len)\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* There's a small window that XDP may be set after the check\n\t * of xdp_prog above, this should be rare and for simplicity\n\t * we do XDP on skb in case the headroom is not enough.\n\t */\n\tif (hdr->gso_type || !xdp_prog)\n\t\t*skb_xdp = 1;\n\telse\n\t\t*skb_xdp = 0;\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(tun->xdp_prog);\n\tif (xdp_prog && !*skb_xdp) {\n\t\tstruct xdp_buff xdp;\n\t\tvoid *orig_data;\n\t\tu32 act;\n\n\t\txdp.data_hard_start = buf;\n\t\txdp.data = buf + pad;\n\t\txdp.data_end = xdp.data + len;\n\t\torig_data = xdp.data;\n\t\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\t\tswitch (act) {\n\t\tcase XDP_REDIRECT:\n\t\t\tget_page(alloc_frag->page);\n\t\t\talloc_frag->offset += buflen;\n\t\t\terr = xdp_do_redirect(tun->dev, &xdp, xdp_prog);\n\t\t\tif (err)\n\t\t\t\tgoto err_redirect;\n\t\t\treturn NULL;\n\t\tcase XDP_TX:\n\t\t\txdp_xmit = true;\n\t\t\t/* fall through */\n\t\tcase XDP_PASS:\n\t\t\tdelta = orig_data - xdp.data;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(act);\n\t\t\t/* fall through */\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(tun->dev, xdp_prog, act);\n\t\t\t/* fall through */\n\t\tcase XDP_DROP:\n\t\t\tgoto err_xdp;\n\t\t}\n\t}\n\n\tskb = build_skb(buf, buflen);\n\tif (!skb) {\n\t\trcu_read_unlock();\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tskb_reserve(skb, pad - delta);\n\tskb_put(skb, len + delta);\n\tget_page(alloc_frag->page);\n\talloc_frag->offset += buflen;\n\n\tif (xdp_xmit) {\n\t\tskb->dev = tun->dev;\n\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\trcu_read_lock();\n\t\treturn NULL;\n\t}\n\n\trcu_read_unlock();\n\n\treturn skb;\n\nerr_redirect:\n\tput_page(alloc_frag->page);\nerr_xdp:\n\trcu_read_unlock();\n\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\treturn NULL;\n}\n\n/* Get packet from user space buffer */\nstatic ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t    void *msg_control, struct iov_iter *from,\n\t\t\t    int noblock, bool more)\n{\n\tstruct tun_pi pi = { 0, cpu_to_be16(ETH_P_IP) };\n\tstruct sk_buff *skb;\n\tsize_t total_len = iov_iter_count(from);\n\tsize_t len = total_len, align = tun->align, linear;\n\tstruct virtio_net_hdr gso = { 0 };\n\tstruct tun_pcpu_stats *stats;\n\tint good_linear;\n\tint copylen;\n\tbool zerocopy = false;\n\tint err;\n\tu32 rxhash;\n\tint skb_xdp = 1;\n\n\tif (!(tun->dev->flags & IFF_UP))\n\t\treturn -EIO;\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (len < sizeof(pi))\n\t\t\treturn -EINVAL;\n\t\tlen -= sizeof(pi);\n\n\t\tif (!copy_from_iter_full(&pi, sizeof(pi), from))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (tun->flags & IFF_VNET_HDR) {\n\t\tint vnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\t\tif (len < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\t\tlen -= vnet_hdr_sz;\n\n\t\tif (!copy_from_iter_full(&gso, sizeof(gso), from))\n\t\t\treturn -EFAULT;\n\n\t\tif ((gso.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2 > tun16_to_cpu(tun, gso.hdr_len))\n\t\t\tgso.hdr_len = cpu_to_tun16(tun, tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2);\n\n\t\tif (tun16_to_cpu(tun, gso.hdr_len) > len)\n\t\t\treturn -EINVAL;\n\t\tiov_iter_advance(from, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif ((tun->flags & TUN_TYPE_MASK) == IFF_TAP) {\n\t\talign += NET_IP_ALIGN;\n\t\tif (unlikely(len < ETH_HLEN ||\n\t\t\t     (gso.hdr_len && tun16_to_cpu(tun, gso.hdr_len) < ETH_HLEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tgood_linear = SKB_MAX_HEAD(align);\n\n\tif (msg_control) {\n\t\tstruct iov_iter i = *from;\n\n\t\t/* There are 256 bytes to be copied in skb, so there is\n\t\t * enough room for skb expand head in case it is used.\n\t\t * The rest of the buffer is mapped from userspace.\n\t\t */\n\t\tcopylen = gso.hdr_len ? tun16_to_cpu(tun, gso.hdr_len) : GOODCOPY_LEN;\n\t\tif (copylen > good_linear)\n\t\t\tcopylen = good_linear;\n\t\tlinear = copylen;\n\t\tiov_iter_advance(&i, copylen);\n\t\tif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\n\t\t\tzerocopy = true;\n\t}\n\n\tif (tun_can_build_skb(tun, tfile, len, noblock, zerocopy)) {\n\t\t/* For the packet that is not easy to be processed\n\t\t * (e.g gso or jumbo packet), we will do it at after\n\t\t * skb was created with generic XDP routine.\n\t\t */\n\t\tskb = tun_build_skb(tun, tfile, from, &gso, len, &skb_xdp);\n\t\tif (IS_ERR(skb)) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\t\tif (!skb)\n\t\t\treturn total_len;\n\t} else {\n\t\tif (!zerocopy) {\n\t\t\tcopylen = len;\n\t\t\tif (tun16_to_cpu(tun, gso.hdr_len) > good_linear)\n\t\t\t\tlinear = good_linear;\n\t\t\telse\n\t\t\t\tlinear = tun16_to_cpu(tun, gso.hdr_len);\n\t\t}\n\n\t\tskb = tun_alloc_skb(tfile, align, copylen, linear, noblock);\n\t\tif (IS_ERR(skb)) {\n\t\t\tif (PTR_ERR(skb) != -EAGAIN)\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\n\t\tif (zerocopy)\n\t\t\terr = zerocopy_sg_from_iter(skb, from);\n\t\telse\n\t\t\terr = skb_copy_datagram_from_iter(skb, 0, from, len);\n\n\t\tif (err) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {\n\t\tthis_cpu_inc(tun->pcpu_stats->rx_frame_errors);\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tif (tun->flags & IFF_NO_PI) {\n\t\t\tu8 ip_version = skb->len ? (skb->data[0] >> 4) : 0;\n\n\t\t\tswitch (ip_version) {\n\t\t\tcase 4:\n\t\t\t\tpi.proto = htons(ETH_P_IP);\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tpi.proto = htons(ETH_P_IPV6);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = pi.proto;\n\t\tskb->dev = tun->dev;\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n\t\tbreak;\n\t}\n\n\t/* copy skb_ubuf_info for callback when skb has no error */\n\tif (zerocopy) {\n\t\tskb_shinfo(skb)->destructor_arg = msg_control;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_DEV_ZEROCOPY;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\t} else if (msg_control) {\n\t\tstruct ubuf_info *uarg = msg_control;\n\t\tuarg->callback(uarg, false);\n\t}\n\n\tskb_reset_network_header(skb);\n\tskb_probe_transport_header(skb, 0);\n\n\tif (skb_xdp) {\n\t\tstruct bpf_prog *xdp_prog;\n\t\tint ret;\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(tun->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tret = do_xdp_generic(xdp_prog, skb);\n\t\t\tif (ret != XDP_PASS) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn total_len;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\trxhash = __skb_get_hash_symmetric(skb);\n#ifndef CONFIG_4KSTACKS\n\ttun_rx_batched(tun, tfile, skb, more);\n#else\n\tnetif_rx_ni(skb);\n#endif\n\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->rx_packets++;\n\tstats->rx_bytes += len;\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(stats);\n\n\ttun_flow_update(tun, rxhash, tfile);\n\treturn total_len;\n}\n\nstatic ssize_t tun_chr_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tun_struct *tun = tun_get(file);\n\tstruct tun_file *tfile = file->private_data;\n\tssize_t result;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tresult = tun_get_user(tun, tfile, NULL, from,\n\t\t\t      file->f_flags & O_NONBLOCK, false);\n\n\ttun_put(tun);\n\treturn result;\n}\n\n/* Put packet to the user space buffer */\nstatic ssize_t tun_put_user(struct tun_struct *tun,\n\t\t\t    struct tun_file *tfile,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct iov_iter *iter)\n{\n\tstruct tun_pi pi = { 0, skb->protocol };\n\tstruct tun_pcpu_stats *stats;\n\tssize_t total;\n\tint vlan_offset = 0;\n\tint vlan_hlen = 0;\n\tint vnet_hdr_sz = 0;\n\n\tif (skb_vlan_tag_present(skb))\n\t\tvlan_hlen = VLAN_HLEN;\n\n\tif (tun->flags & IFF_VNET_HDR)\n\t\tvnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\ttotal = skb->len + vlan_hlen + vnet_hdr_sz;\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (iov_iter_count(iter) < sizeof(pi))\n\t\t\treturn -EINVAL;\n\n\t\ttotal += sizeof(pi);\n\t\tif (iov_iter_count(iter) < total) {\n\t\t\t/* Packet will be striped */\n\t\t\tpi.flags |= TUN_PKT_STRIP;\n\t\t}\n\n\t\tif (copy_to_iter(&pi, sizeof(pi), iter) != sizeof(pi))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (vnet_hdr_sz) {\n\t\tstruct virtio_net_hdr gso;\n\n\t\tif (iov_iter_count(iter) < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\n\t\tif (virtio_net_hdr_from_skb(skb, &gso,\n\t\t\t\t\t    tun_is_little_endian(tun), true)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\t\t\tpr_err(\"unexpected GSO type: \"\n\t\t\t       \"0x%x, gso_size %d, hdr_len %d\\n\",\n\t\t\t       sinfo->gso_type, tun16_to_cpu(tun, gso.gso_size),\n\t\t\t       tun16_to_cpu(tun, gso.hdr_len));\n\t\t\tprint_hex_dump(KERN_ERR, \"tun: \",\n\t\t\t\t       DUMP_PREFIX_NONE,\n\t\t\t\t       16, 1, skb->head,\n\t\t\t\t       min((int)tun16_to_cpu(tun, gso.hdr_len), 64), true);\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (copy_to_iter(&gso, sizeof(gso), iter) != sizeof(gso))\n\t\t\treturn -EFAULT;\n\n\t\tiov_iter_advance(iter, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif (vlan_hlen) {\n\t\tint ret;\n\t\tstruct {\n\t\t\t__be16 h_vlan_proto;\n\t\t\t__be16 h_vlan_TCI;\n\t\t} veth;\n\n\t\tveth.h_vlan_proto = skb->vlan_proto;\n\t\tveth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));\n\n\t\tvlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);\n\n\t\tret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);\n\t\tif (ret || !iov_iter_count(iter))\n\t\t\tgoto done;\n\n\t\tret = copy_to_iter(&veth, sizeof(veth), iter);\n\t\tif (ret != sizeof(veth) || !iov_iter_count(iter))\n\t\t\tgoto done;\n\t}\n\n\tskb_copy_datagram_iter(skb, vlan_offset, iter, skb->len - vlan_offset);\n\ndone:\n\t/* caller is in process context, */\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->tx_packets++;\n\tstats->tx_bytes += skb->len + vlan_hlen;\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(tun->pcpu_stats);\n\n\treturn total;\n}\n\nstatic struct sk_buff *tun_ring_recv(struct tun_file *tfile, int noblock,\n\t\t\t\t     int *err)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct sk_buff *skb = NULL;\n\tint error = 0;\n\n\tskb = skb_array_consume(&tfile->tx_array);\n\tif (skb)\n\t\tgoto out;\n\tif (noblock) {\n\t\terror = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tadd_wait_queue(&tfile->wq.wait, &wait);\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\n\twhile (1) {\n\t\tskb = skb_array_consume(&tfile->tx_array);\n\t\tif (skb)\n\t\t\tbreak;\n\t\tif (signal_pending(current)) {\n\t\t\terror = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tfile->socket.sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\terror = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t}\n\n\tcurrent->state = TASK_RUNNING;\n\tremove_wait_queue(&tfile->wq.wait, &wait);\n\nout:\n\t*err = error;\n\treturn skb;\n}\n\nstatic ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t   struct iov_iter *to,\n\t\t\t   int noblock, struct sk_buff *skb)\n{\n\tssize_t ret;\n\tint err;\n\n\ttun_debug(KERN_INFO, tun, \"tun_do_read\\n\");\n\n\tif (!iov_iter_count(to))\n\t\treturn 0;\n\n\tif (!skb) {\n\t\t/* Read frames from ring */\n\t\tskb = tun_ring_recv(tfile, noblock, &err);\n\t\tif (!skb)\n\t\t\treturn err;\n\t}\n\n\tret = tun_put_user(tun, tfile, skb, to);\n\tif (unlikely(ret < 0))\n\t\tkfree_skb(skb);\n\telse\n\t\tconsume_skb(skb);\n\n\treturn ret;\n}\n\nstatic ssize_t tun_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tssize_t len = iov_iter_count(to), ret;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\tret = tun_do_read(tun, tfile, to, file->f_flags & O_NONBLOCK, NULL);\n\tret = min_t(ssize_t, ret, len);\n\tif (ret > 0)\n\t\tiocb->ki_pos = ret;\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic void tun_free_netdev(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tBUG_ON(!(list_empty(&tun->disabled)));\n\tfree_percpu(tun->pcpu_stats);\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\n}\n\nstatic void tun_setup(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\ttun->owner = INVALID_UID;\n\ttun->group = INVALID_GID;\n\n\tdev->ethtool_ops = &tun_ethtool_ops;\n\tdev->needs_free_netdev = true;\n\tdev->priv_destructor = tun_free_netdev;\n\t/* We prefer our own queue length */\n\tdev->tx_queue_len = TUN_READQ_SIZE;\n}\n\n/* Trivial set of netlink ops to allow deleting tun or tap\n * device with netlink.\n */\nstatic int tun_validate(struct nlattr *tb[], struct nlattr *data[],\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\treturn -EINVAL;\n}\n\nstatic struct rtnl_link_ops tun_link_ops __read_mostly = {\n\t.kind\t\t= DRV_NAME,\n\t.priv_size\t= sizeof(struct tun_struct),\n\t.setup\t\t= tun_setup,\n\t.validate\t= tun_validate,\n};\n\nstatic void tun_sock_write_space(struct sock *sk)\n{\n\tstruct tun_file *tfile;\n\twait_queue_head_t *wqueue;\n\n\tif (!sock_writeable(sk))\n\t\treturn;\n\n\tif (!test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags))\n\t\treturn;\n\n\twqueue = sk_sleep(sk);\n\tif (wqueue && waitqueue_active(wqueue))\n\t\twake_up_interruptible_sync_poll(wqueue, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\ttfile = container_of(sk, struct tun_file, sk);\n\tkill_fasync(&tfile->fasync, SIGIO, POLL_OUT);\n}\n\nstatic int tun_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)\n{\n\tint ret;\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun = __tun_get(tfile);\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tret = tun_get_user(tun, tfile, m->msg_control, &m->msg_iter,\n\t\t\t   m->msg_flags & MSG_DONTWAIT,\n\t\t\t   m->msg_flags & MSG_MORE);\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic int tun_recvmsg(struct socket *sock, struct msghdr *m, size_t total_len,\n\t\t       int flags)\n{\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tint ret;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tif (flags & ~(MSG_DONTWAIT|MSG_TRUNC|MSG_ERRQUEUE)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (flags & MSG_ERRQUEUE) {\n\t\tret = sock_recv_errqueue(sock->sk, m, total_len,\n\t\t\t\t\t SOL_PACKET, TUN_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\tret = tun_do_read(tun, tfile, &m->msg_iter, flags & MSG_DONTWAIT,\n\t\t\t  m->msg_control);\n\tif (ret > (ssize_t)total_len) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tret = flags & MSG_TRUNC ? ret : total_len;\n\t}\nout:\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic int tun_peek_len(struct socket *sock)\n{\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun;\n\tint ret = 0;\n\n\ttun = __tun_get(tfile);\n\tif (!tun)\n\t\treturn 0;\n\n\tret = skb_array_peek_len(&tfile->tx_array);\n\ttun_put(tun);\n\n\treturn ret;\n}\n\n/* Ops structure to mimic raw sockets with tun */\nstatic const struct proto_ops tun_socket_ops = {\n\t.peek_len = tun_peek_len,\n\t.sendmsg = tun_sendmsg,\n\t.recvmsg = tun_recvmsg,\n};\n\nstatic struct proto tun_proto = {\n\t.name\t\t= \"tun\",\n\t.owner\t\t= THIS_MODULE,\n\t.obj_size\t= sizeof(struct tun_file),\n};\n\nstatic int tun_flags(struct tun_struct *tun)\n{\n\treturn tun->flags & (TUN_FEATURES | IFF_PERSIST | IFF_TUN | IFF_TAP);\n}\n\nstatic ssize_t tun_show_flags(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn sprintf(buf, \"0x%x\\n\", tun_flags(tun));\n}\n\nstatic ssize_t tun_show_owner(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn uid_valid(tun->owner)?\n\t\tsprintf(buf, \"%u\\n\",\n\t\t\tfrom_kuid_munged(current_user_ns(), tun->owner)):\n\t\tsprintf(buf, \"-1\\n\");\n}\n\nstatic ssize_t tun_show_group(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn gid_valid(tun->group) ?\n\t\tsprintf(buf, \"%u\\n\",\n\t\t\tfrom_kgid_munged(current_user_ns(), tun->group)):\n\t\tsprintf(buf, \"-1\\n\");\n}\n\nstatic DEVICE_ATTR(tun_flags, 0444, tun_show_flags, NULL);\nstatic DEVICE_ATTR(owner, 0444, tun_show_owner, NULL);\nstatic DEVICE_ATTR(group, 0444, tun_show_group, NULL);\n\nstatic struct attribute *tun_dev_attrs[] = {\n\t&dev_attr_tun_flags.attr,\n\t&dev_attr_owner.attr,\n\t&dev_attr_group.attr,\n\tNULL\n};\n\nstatic const struct attribute_group tun_attr_group = {\n\t.attrs = tun_dev_attrs\n};\n\nstatic int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}\n\nstatic void tun_get_iff(struct net *net, struct tun_struct *tun,\n\t\t       struct ifreq *ifr)\n{\n\ttun_debug(KERN_INFO, tun, \"tun_get_iff\\n\");\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\n\tifr->ifr_flags = tun_flags(tun);\n\n}\n\n/* This is like a cut-down ethtool ops, except done via tun fd so no\n * privs required. */\nstatic int set_offload(struct tun_struct *tun, unsigned long arg)\n{\n\tnetdev_features_t features = 0;\n\n\tif (arg & TUN_F_CSUM) {\n\t\tfeatures |= NETIF_F_HW_CSUM;\n\t\targ &= ~TUN_F_CSUM;\n\n\t\tif (arg & (TUN_F_TSO4|TUN_F_TSO6)) {\n\t\t\tif (arg & TUN_F_TSO_ECN) {\n\t\t\t\tfeatures |= NETIF_F_TSO_ECN;\n\t\t\t\targ &= ~TUN_F_TSO_ECN;\n\t\t\t}\n\t\t\tif (arg & TUN_F_TSO4)\n\t\t\t\tfeatures |= NETIF_F_TSO;\n\t\t\tif (arg & TUN_F_TSO6)\n\t\t\t\tfeatures |= NETIF_F_TSO6;\n\t\t\targ &= ~(TUN_F_TSO4|TUN_F_TSO6);\n\t\t}\n\t}\n\n\t/* This gives the user a way to test for new features in future by\n\t * trying to set them. */\n\tif (arg)\n\t\treturn -EINVAL;\n\n\ttun->set_features = features;\n\ttun->dev->wanted_features &= ~TUN_USER_FEATURES;\n\ttun->dev->wanted_features |= features;\n\tnetdev_update_features(tun->dev);\n\n\treturn 0;\n}\n\nstatic void tun_detach_filter(struct tun_struct *tun, int n)\n{\n\tint i;\n\tstruct tun_file *tfile;\n\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tlock_sock(tfile->socket.sk);\n\t\tsk_detach_filter(tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t}\n\n\ttun->filter_attached = false;\n}\n\nstatic int tun_attach_filter(struct tun_struct *tun)\n{\n\tint i, ret = 0;\n\tstruct tun_file *tfile;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tlock_sock(tfile->socket.sk);\n\t\tret = sk_attach_filter(&tun->fprog, tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t\tif (ret) {\n\t\t\ttun_detach_filter(tun, i);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\ttun->filter_attached = true;\n\treturn ret;\n}\n\nstatic void tun_set_sndbuf(struct tun_struct *tun)\n{\n\tstruct tun_file *tfile;\n\tint i;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\ttfile->socket.sk->sk_sndbuf = tun->sndbuf;\n\t}\n}\n\nstatic int tun_set_queue(struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tint ret = 0;\n\n\trtnl_lock();\n\n\tif (ifr->ifr_flags & IFF_ATTACH_QUEUE) {\n\t\ttun = tfile->detached;\n\t\tif (!tun) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\tret = security_tun_dev_attach_queue(tun->security);\n\t\tif (ret < 0)\n\t\t\tgoto unlock;\n\t\tret = tun_attach(tun, file, false);\n\t} else if (ifr->ifr_flags & IFF_DETACH_QUEUE) {\n\t\ttun = rtnl_dereference(tfile->tun);\n\t\tif (!tun || !(tun->flags & IFF_MULTI_QUEUE) || tfile->detached)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\t__tun_detach(tfile, false);\n\t} else\n\t\tret = -EINVAL;\n\nunlock:\n\trtnl_unlock();\n\treturn ret;\n}\n\nstatic long __tun_chr_ioctl(struct file *file, unsigned int cmd,\n\t\t\t    unsigned long arg, int ifreq_len)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tvoid __user* argp = (void __user*)arg;\n\tstruct ifreq ifr;\n\tkuid_t owner;\n\tkgid_t group;\n\tint sndbuf;\n\tint vnet_hdr_sz;\n\tunsigned int ifindex;\n\tint le;\n\tint ret;\n\n\tif (cmd == TUNSETIFF || cmd == TUNSETQUEUE || _IOC_TYPE(cmd) == SOCK_IOC_TYPE) {\n\t\tif (copy_from_user(&ifr, argp, ifreq_len))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tmemset(&ifr, 0, sizeof(ifr));\n\t}\n\tif (cmd == TUNGETFEATURES) {\n\t\t/* Currently this just means: \"what IFF flags are valid?\".\n\t\t * This is needed because we never checked for invalid flags on\n\t\t * TUNSETIFF.\n\t\t */\n\t\treturn put_user(IFF_TUN | IFF_TAP | TUN_FEATURES,\n\t\t\t\t(unsigned int __user*)argp);\n\t} else if (cmd == TUNSETQUEUE)\n\t\treturn tun_set_queue(file, &ifr);\n\n\tret = 0;\n\trtnl_lock();\n\n\ttun = __tun_get(tfile);\n\tif (cmd == TUNSETIFF) {\n\t\tret = -EEXIST;\n\t\tif (tun)\n\t\t\tgoto unlock;\n\n\t\tifr.ifr_name[IFNAMSIZ-1] = '\\0';\n\n\t\tret = tun_set_iff(sock_net(&tfile->sk), file, &ifr);\n\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tgoto unlock;\n\t}\n\tif (cmd == TUNSETIFINDEX) {\n\t\tret = -EPERM;\n\t\tif (tun)\n\t\t\tgoto unlock;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&ifindex, argp, sizeof(ifindex)))\n\t\t\tgoto unlock;\n\n\t\tret = 0;\n\t\ttfile->ifindex = ifindex;\n\t\tgoto unlock;\n\t}\n\n\tret = -EBADFD;\n\tif (!tun)\n\t\tgoto unlock;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_ioctl cmd %u\\n\", cmd);\n\n\tret = 0;\n\tswitch (cmd) {\n\tcase TUNGETIFF:\n\t\ttun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\n\t\tif (tfile->detached)\n\t\t\tifr.ifr_flags |= IFF_DETACH_QUEUE;\n\t\tif (!tfile->socket.sk->sk_filter)\n\t\t\tifr.ifr_flags |= IFF_NOFILTER;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETNOCSUM:\n\t\t/* Disable/Enable checksum */\n\n\t\t/* [unimplemented] */\n\t\ttun_debug(KERN_INFO, tun, \"ignored: set checksum %s\\n\",\n\t\t\t  arg ? \"disabled\" : \"enabled\");\n\t\tbreak;\n\n\tcase TUNSETPERSIST:\n\t\t/* Disable/Enable persist mode. Keep an extra reference to the\n\t\t * module to prevent the module being unprobed.\n\t\t */\n\t\tif (arg && !(tun->flags & IFF_PERSIST)) {\n\t\t\ttun->flags |= IFF_PERSIST;\n\t\t\t__module_get(THIS_MODULE);\n\t\t}\n\t\tif (!arg && (tun->flags & IFF_PERSIST)) {\n\t\t\ttun->flags &= ~IFF_PERSIST;\n\t\t\tmodule_put(THIS_MODULE);\n\t\t}\n\n\t\ttun_debug(KERN_INFO, tun, \"persist %s\\n\",\n\t\t\t  arg ? \"enabled\" : \"disabled\");\n\t\tbreak;\n\n\tcase TUNSETOWNER:\n\t\t/* Set owner of the device */\n\t\towner = make_kuid(current_user_ns(), arg);\n\t\tif (!uid_valid(owner)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttun->owner = owner;\n\t\ttun_debug(KERN_INFO, tun, \"owner set to %u\\n\",\n\t\t\t  from_kuid(&init_user_ns, tun->owner));\n\t\tbreak;\n\n\tcase TUNSETGROUP:\n\t\t/* Set group of the device */\n\t\tgroup = make_kgid(current_user_ns(), arg);\n\t\tif (!gid_valid(group)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttun->group = group;\n\t\ttun_debug(KERN_INFO, tun, \"group set to %u\\n\",\n\t\t\t  from_kgid(&init_user_ns, tun->group));\n\t\tbreak;\n\n\tcase TUNSETLINK:\n\t\t/* Only allow setting the type when the interface is down */\n\t\tif (tun->dev->flags & IFF_UP) {\n\t\t\ttun_debug(KERN_INFO, tun,\n\t\t\t\t  \"Linktype set failed because interface is up\\n\");\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\ttun->dev->type = (int) arg;\n\t\t\ttun_debug(KERN_INFO, tun, \"linktype set to %d\\n\",\n\t\t\t\t  tun->dev->type);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n#ifdef TUN_DEBUG\n\tcase TUNSETDEBUG:\n\t\ttun->debug = arg;\n\t\tbreak;\n#endif\n\tcase TUNSETOFFLOAD:\n\t\tret = set_offload(tun, arg);\n\t\tbreak;\n\n\tcase TUNSETTXFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = update_filter(&tun->txflt, (void __user *)arg);\n\t\tbreak;\n\n\tcase SIOCGIFHWADDR:\n\t\t/* Get hw address */\n\t\tmemcpy(ifr.ifr_hwaddr.sa_data, tun->dev->dev_addr, ETH_ALEN);\n\t\tifr.ifr_hwaddr.sa_family = tun->dev->type;\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCSIFHWADDR:\n\t\t/* Set hw address */\n\t\ttun_debug(KERN_DEBUG, tun, \"set hw address: %pM\\n\",\n\t\t\t  ifr.ifr_hwaddr.sa_data);\n\n\t\tret = dev_set_mac_address(tun->dev, &ifr.ifr_hwaddr);\n\t\tbreak;\n\n\tcase TUNGETSNDBUF:\n\t\tsndbuf = tfile->socket.sk->sk_sndbuf;\n\t\tif (copy_to_user(argp, &sndbuf, sizeof(sndbuf)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETSNDBUF:\n\t\tif (copy_from_user(&sndbuf, argp, sizeof(sndbuf))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->sndbuf = sndbuf;\n\t\ttun_set_sndbuf(tun);\n\t\tbreak;\n\n\tcase TUNGETVNETHDRSZ:\n\t\tvnet_hdr_sz = tun->vnet_hdr_sz;\n\t\tif (copy_to_user(argp, &vnet_hdr_sz, sizeof(vnet_hdr_sz)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETHDRSZ:\n\t\tif (copy_from_user(&vnet_hdr_sz, argp, sizeof(vnet_hdr_sz))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (vnet_hdr_sz < (int)sizeof(struct virtio_net_hdr)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->vnet_hdr_sz = vnet_hdr_sz;\n\t\tbreak;\n\n\tcase TUNGETVNETLE:\n\t\tle = !!(tun->flags & TUN_VNET_LE);\n\t\tif (put_user(le, (int __user *)argp))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETLE:\n\t\tif (get_user(le, (int __user *)argp)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (le)\n\t\t\ttun->flags |= TUN_VNET_LE;\n\t\telse\n\t\t\ttun->flags &= ~TUN_VNET_LE;\n\t\tbreak;\n\n\tcase TUNGETVNETBE:\n\t\tret = tun_get_vnet_be(tun, argp);\n\t\tbreak;\n\n\tcase TUNSETVNETBE:\n\t\tret = tun_set_vnet_be(tun, argp);\n\t\tbreak;\n\n\tcase TUNATTACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&tun->fprog, argp, sizeof(tun->fprog)))\n\t\t\tbreak;\n\n\t\tret = tun_attach_filter(tun);\n\t\tbreak;\n\n\tcase TUNDETACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = 0;\n\t\ttun_detach_filter(tun, tun->numqueues);\n\t\tbreak;\n\n\tcase TUNGETFILTER:\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_to_user(argp, &tun->fprog, sizeof(tun->fprog)))\n\t\t\tbreak;\n\t\tret = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tif (tun)\n\t\ttun_put(tun);\n\treturn ret;\n}\n\nstatic long tun_chr_ioctl(struct file *file,\n\t\t\t  unsigned int cmd, unsigned long arg)\n{\n\treturn __tun_chr_ioctl(file, cmd, arg, sizeof (struct ifreq));\n}\n\n#ifdef CONFIG_COMPAT\nstatic long tun_chr_compat_ioctl(struct file *file,\n\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase TUNSETIFF:\n\tcase TUNGETIFF:\n\tcase TUNSETTXFILTER:\n\tcase TUNGETSNDBUF:\n\tcase TUNSETSNDBUF:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCSIFHWADDR:\n\t\targ = (unsigned long)compat_ptr(arg);\n\t\tbreak;\n\tdefault:\n\t\targ = (compat_ulong_t)arg;\n\t\tbreak;\n\t}\n\n\t/*\n\t * compat_ifreq is shorter than ifreq, so we must not access beyond\n\t * the end of that structure. All fields that are used in this\n\t * driver are compatible though, we don't need to convert the\n\t * contents.\n\t */\n\treturn __tun_chr_ioctl(file, cmd, arg, sizeof(struct compat_ifreq));\n}\n#endif /* CONFIG_COMPAT */\n\nstatic int tun_chr_fasync(int fd, struct file *file, int on)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tint ret;\n\n\tif ((ret = fasync_helper(fd, file, on, &tfile->fasync)) < 0)\n\t\tgoto out;\n\n\tif (on) {\n\t\t__f_setown(file, task_pid(current), PIDTYPE_PID, 0);\n\t\ttfile->flags |= TUN_FASYNC;\n\t} else\n\t\ttfile->flags &= ~TUN_FASYNC;\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic int tun_chr_open(struct inode *inode, struct file * file)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct tun_file *tfile;\n\n\tDBG1(KERN_INFO, \"tunX: tun_chr_open\\n\");\n\n\ttfile = (struct tun_file *)sk_alloc(net, AF_UNSPEC, GFP_KERNEL,\n\t\t\t\t\t    &tun_proto, 0);\n\tif (!tfile)\n\t\treturn -ENOMEM;\n\tRCU_INIT_POINTER(tfile->tun, NULL);\n\ttfile->flags = 0;\n\ttfile->ifindex = 0;\n\n\tinit_waitqueue_head(&tfile->wq.wait);\n\tRCU_INIT_POINTER(tfile->socket.wq, &tfile->wq);\n\n\ttfile->socket.file = file;\n\ttfile->socket.ops = &tun_socket_ops;\n\n\tsock_init_data(&tfile->socket, &tfile->sk);\n\n\ttfile->sk.sk_write_space = tun_sock_write_space;\n\ttfile->sk.sk_sndbuf = INT_MAX;\n\n\tfile->private_data = tfile;\n\tINIT_LIST_HEAD(&tfile->next);\n\n\tsock_set_flag(&tfile->sk, SOCK_ZEROCOPY);\n\n\treturn 0;\n}\n\nstatic int tun_chr_close(struct inode *inode, struct file *file)\n{\n\tstruct tun_file *tfile = file->private_data;\n\n\ttun_detach(tfile, true);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void tun_chr_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct tun_struct *tun;\n\tstruct ifreq ifr;\n\n\tmemset(&ifr, 0, sizeof(ifr));\n\n\trtnl_lock();\n\ttun = tun_get(f);\n\tif (tun)\n\t\ttun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\trtnl_unlock();\n\n\tif (tun)\n\t\ttun_put(tun);\n\n\tseq_printf(m, \"iff:\\t%s\\n\", ifr.ifr_name);\n}\n#endif\n\nstatic const struct file_operations tun_fops = {\n\t.owner\t= THIS_MODULE,\n\t.llseek = no_llseek,\n\t.read_iter  = tun_chr_read_iter,\n\t.write_iter = tun_chr_write_iter,\n\t.poll\t= tun_chr_poll,\n\t.unlocked_ioctl\t= tun_chr_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = tun_chr_compat_ioctl,\n#endif\n\t.open\t= tun_chr_open,\n\t.release = tun_chr_close,\n\t.fasync = tun_chr_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo = tun_chr_show_fdinfo,\n#endif\n};\n\nstatic struct miscdevice tun_miscdev = {\n\t.minor = TUN_MINOR,\n\t.name = \"tun\",\n\t.nodename = \"net/tun\",\n\t.fops = &tun_fops,\n};\n\n/* ethtool interface */\n\nstatic int tun_get_link_ksettings(struct net_device *dev,\n\t\t\t\t  struct ethtool_link_ksettings *cmd)\n{\n\tethtool_link_ksettings_zero_link_mode(cmd, supported);\n\tethtool_link_ksettings_zero_link_mode(cmd, advertising);\n\tcmd->base.speed\t\t= SPEED_10;\n\tcmd->base.duplex\t= DUPLEX_FULL;\n\tcmd->base.port\t\t= PORT_TP;\n\tcmd->base.phy_address\t= 0;\n\tcmd->base.autoneg\t= AUTONEG_DISABLE;\n\treturn 0;\n}\n\nstatic void tun_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tstrlcpy(info->bus_info, \"tun\", sizeof(info->bus_info));\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tstrlcpy(info->bus_info, \"tap\", sizeof(info->bus_info));\n\t\tbreak;\n\t}\n}\n\nstatic u32 tun_get_msglevel(struct net_device *dev)\n{\n#ifdef TUN_DEBUG\n\tstruct tun_struct *tun = netdev_priv(dev);\n\treturn tun->debug;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic void tun_set_msglevel(struct net_device *dev, u32 value)\n{\n#ifdef TUN_DEBUG\n\tstruct tun_struct *tun = netdev_priv(dev);\n\ttun->debug = value;\n#endif\n}\n\nstatic int tun_get_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *ec)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tec->rx_max_coalesced_frames = tun->rx_batched;\n\n\treturn 0;\n}\n\nstatic int tun_set_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *ec)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (ec->rx_max_coalesced_frames > NAPI_POLL_WEIGHT)\n\t\ttun->rx_batched = NAPI_POLL_WEIGHT;\n\telse\n\t\ttun->rx_batched = ec->rx_max_coalesced_frames;\n\n\treturn 0;\n}\n\nstatic const struct ethtool_ops tun_ethtool_ops = {\n\t.get_drvinfo\t= tun_get_drvinfo,\n\t.get_msglevel\t= tun_get_msglevel,\n\t.set_msglevel\t= tun_set_msglevel,\n\t.get_link\t= ethtool_op_get_link,\n\t.get_ts_info\t= ethtool_op_get_ts_info,\n\t.get_coalesce   = tun_get_coalesce,\n\t.set_coalesce   = tun_set_coalesce,\n\t.get_link_ksettings = tun_get_link_ksettings,\n};\n\nstatic int tun_queue_resize(struct tun_struct *tun)\n{\n\tstruct net_device *dev = tun->dev;\n\tstruct tun_file *tfile;\n\tstruct skb_array **arrays;\n\tint n = tun->numqueues + tun->numdisabled;\n\tint ret, i;\n\n\tarrays = kmalloc_array(n, sizeof(*arrays), GFP_KERNEL);\n\tif (!arrays)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tarrays[i] = &tfile->tx_array;\n\t}\n\tlist_for_each_entry(tfile, &tun->disabled, next)\n\t\tarrays[i++] = &tfile->tx_array;\n\n\tret = skb_array_resize_multiple(arrays, n,\n\t\t\t\t\tdev->tx_queue_len, GFP_KERNEL);\n\n\tkfree(arrays);\n\treturn ret;\n}\n\nstatic int tun_device_event(struct notifier_block *unused,\n\t\t\t    unsigned long event, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (dev->rtnl_link_ops != &tun_link_ops)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (event) {\n\tcase NETDEV_CHANGE_TX_QUEUE_LEN:\n\t\tif (tun_queue_resize(tun))\n\t\t\treturn NOTIFY_BAD;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block tun_notifier_block __read_mostly = {\n\t.notifier_call\t= tun_device_event,\n};\n\nstatic int __init tun_init(void)\n{\n\tint ret = 0;\n\n\tpr_info(\"%s, %s\\n\", DRV_DESCRIPTION, DRV_VERSION);\n\n\tret = rtnl_link_register(&tun_link_ops);\n\tif (ret) {\n\t\tpr_err(\"Can't register link_ops\\n\");\n\t\tgoto err_linkops;\n\t}\n\n\tret = misc_register(&tun_miscdev);\n\tif (ret) {\n\t\tpr_err(\"Can't register misc device %d\\n\", TUN_MINOR);\n\t\tgoto err_misc;\n\t}\n\n\tret = register_netdevice_notifier(&tun_notifier_block);\n\tif (ret) {\n\t\tpr_err(\"Can't register netdevice notifier\\n\");\n\t\tgoto err_notifier;\n\t}\n\n\treturn  0;\n\nerr_notifier:\n\tmisc_deregister(&tun_miscdev);\nerr_misc:\n\trtnl_link_unregister(&tun_link_ops);\nerr_linkops:\n\treturn ret;\n}\n\nstatic void tun_cleanup(void)\n{\n\tmisc_deregister(&tun_miscdev);\n\trtnl_link_unregister(&tun_link_ops);\n\tunregister_netdevice_notifier(&tun_notifier_block);\n}\n\n/* Get an underlying socket object from tun file.  Returns error unless file is\n * attached to a device.  The returned object works like a packet socket, it\n * can be used for sock_sendmsg/sock_recvmsg.  The caller is responsible for\n * holding a reference to the file for as long as the socket is in use. */\nstruct socket *tun_get_socket(struct file *file)\n{\n\tstruct tun_file *tfile;\n\tif (file->f_op != &tun_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfile = file->private_data;\n\tif (!tfile)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &tfile->socket;\n}\nEXPORT_SYMBOL_GPL(tun_get_socket);\n\nstruct skb_array *tun_get_skb_array(struct file *file)\n{\n\tstruct tun_file *tfile;\n\n\tif (file->f_op != &tun_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfile = file->private_data;\n\tif (!tfile)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &tfile->tx_array;\n}\nEXPORT_SYMBOL_GPL(tun_get_skb_array);\n\nmodule_init(tun_init);\nmodule_exit(tun_cleanup);\nMODULE_DESCRIPTION(DRV_DESCRIPTION);\nMODULE_AUTHOR(DRV_COPYRIGHT);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_MISCDEV(TUN_MINOR);\nMODULE_ALIAS(\"devname:net/tun\");\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n#include <linux/hashtable.h>\n\nstruct netpoll_info;\nstruct device;\nstruct phy_device;\nstruct dsa_switch_tree;\n\n/* 802.11 specific */\nstruct wireless_dev;\n/* 802.15.4 specific */\nstruct wpan_dev;\nstruct mpls_dev;\n/* UDP Tunnel offloads */\nstruct udp_tunnel_info;\nstruct bpf_prog;\nstruct xdp_buff;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously; in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), and all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tCompute the worst-case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key rps_needed;\nextern struct static_key rfs_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tunsigned int\thh_len;\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n/* Reserve HH_DATA_MOD byte-aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer; they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds boot-time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nint __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-CPU poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tunsigned int\t\tgro_count;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tint\t\t\tpoll_owner;\n#endif\n\tstruct net_device\t*dev;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n\tstruct hrtimer\t\ttimer;\n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n\tunsigned int\t\tnapi_id;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_MISSED,\t/* reschedule a napi */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n\tNAPI_STATE_HASHED,\t/* In NAPI hash (busy polling possible) */\n\tNAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */\n\tNAPI_STATE_IN_BUSY_POLL,/* sk_busy_loop() owns this NAPI */\n};\n\nenum {\n\tNAPIF_STATE_SCHED\t = BIT(NAPI_STATE_SCHED),\n\tNAPIF_STATE_MISSED\t = BIT(NAPI_STATE_MISSED),\n\tNAPIF_STATE_DISABLE\t = BIT(NAPI_STATE_DISABLE),\n\tNAPIF_STATE_NPSVC\t = BIT(NAPI_STATE_NPSVC),\n\tNAPIF_STATE_HASHED\t = BIT(NAPI_STATE_HASHED),\n\tNAPIF_STATE_NO_BUSY_POLL = BIT(NAPI_STATE_NO_BUSY_POLL),\n\tNAPIF_STATE_IN_BUSY_POLL = BIT(NAPI_STATE_IN_BUSY_POLL),\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n\tGRO_CONSUMED,\n};\ntypedef enum gro_result gro_result_t;\n\n/*\n * enum rx_handler_result - Possible return values for rx_handlers.\n * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it\n * further.\n * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in\n * case skb->dev was changed by rx_handler.\n * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.\n * @RX_HANDLER_PASS: Do nothing, pass the skb as if no rx_handler was called.\n *\n * rx_handlers are functions called from inside __netif_receive_skb(), to do\n * special processing of the skb, prior to delivery to protocol handlers.\n *\n * Currently, a net_device can only have a single rx_handler registered. Trying\n * to register a second rx_handler will return -EBUSY.\n *\n * To register a rx_handler on a net_device, use netdev_rx_handler_register().\n * To unregister a rx_handler on a net_device, use\n * netdev_rx_handler_unregister().\n *\n * Upon return, rx_handler is expected to tell __netif_receive_skb() what to\n * do with the skb.\n *\n * If the rx_handler consumed the skb in some way, it should return\n * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for\n * the skb to be delivered in some other way.\n *\n * If the rx_handler changed skb->dev, to divert the skb to another\n * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the\n * new device will be called if it exists.\n *\n * If the rx_handler decides the skb should be ignored, it should return\n * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that\n * are registered on exact device (ptype->dev == skb->dev).\n *\n * If the rx_handler didn't change skb->dev, but wants the skb to be normally\n * delivered, it should return RX_HANDLER_PASS.\n *\n * A device without a registered rx_handler will behave as if rx_handler\n * returned RX_HANDLER_PASS.\n */\n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\nbool napi_schedule_prep(struct napi_struct *n);\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/**\n *\tnapi_schedule_irqoff - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Variant of napi_schedule(), assuming hard irqs are masked.\n */\nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool napi_complete_done(struct napi_struct *n, int work_done);\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: NAPI context\n *\n * Mark NAPI processing as complete.\n * Consider using napi_complete_done() instead.\n * Return false if device should avoid rearming interrupts.\n */\nstatic inline bool napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\n/**\n *\tnapi_hash_del - remove a NAPI from global table\n *\t@napi: NAPI context\n *\n * Warning: caller must observe RCU grace period\n * before freeing memory containing @napi, if\n * this function returns true.\n * Note: core networking stack automatically calls it\n * from netif_napi_del().\n * Drivers might want to call this helper to combine all\n * the needed RCU grace periods into a single one.\n */\nbool napi_hash_del(struct napi_struct *napi);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: NAPI context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nvoid napi_disable(struct napi_struct *n);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n\tclear_bit(NAPI_STATE_NPSVC, &n->state);\n}\n\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: NAPI context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n/*\n * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The\n * netif_tx_* functions below are used to manipulate this flag.  The\n * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit\n * queue independently.  The netif_xmit_*stopped functions below are called\n * to check if the queue has been stopped by the driver or stack (either\n * of the XOFF bits are set in the state).  Drivers should not need to call\n * netif_xmit*stopped functions, they should only be using netif_tx_*.\n */\n\nstruct netdev_queue {\n/*\n * read-mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n\tunsigned long\t\ttx_maxrate;\n\t/*\n\t * Number of TX timeouts for this queue\n\t * (/sys/class/net/DEV/Q/trans_timeout)\n\t */\n\tunsigned long\t\ttrans_timeout;\n/*\n * write-mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * Time (in jiffies) of last Tx\n\t */\n\tunsigned long\t\ttrans_start;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU, the\n * tail pointer for that CPU's input queue at the time of last enqueue, and\n * a hardware filter index.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n * Each entry is a 32bit value. Upper part is the high-order bits\n * of flow hash, lower part is CPU number.\n * rps_cpu_mask is used to partition the space, depending on number of\n * possible CPUs : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1\n * For example, if 64 CPUs are possible, rps_cpu_mask = 0x3f,\n * meaning we use 32-6=26 bits for the hash.\n */\nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[0] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t/* We only give a hint, preemption can change CPU under us */\n\t\tval |= raw_smp_processor_id();\n\n\t\tif (table->ents[index] != val)\n\t\t\ttable->ents[index] = val;\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif /* CONFIG_RPS */\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n#ifdef CONFIG_RPS\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n#endif\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n\n/*\n * RX queue sysfs structures and functions.\n */\nstruct rx_queue_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct netdev_rx_queue *queue, char *buf);\n\tssize_t (*store)(struct netdev_rx_queue *queue,\n\t\t\t const char *buf, size_t len);\n};\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE(_tcs) (sizeof(struct xps_dev_maps) +\t\t\\\n\t(nr_cpu_ids * (_tcs) * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n/* HW offloaded queuing disciplines txq count and offset maps */\nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n/*\n * This structure is to hold information about the device\n * configured to run FCoE protocol stack.\n */\nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n/* This structure holds a unique identifier to identify some\n * physical item (port for example) used by a netdevice.\n */\nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb);\n\nenum tc_setup_type {\n\tTC_SETUP_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n\tTC_SETUP_CLSMATCHALL,\n\tTC_SETUP_CLSBPF,\n};\n\n/* These structures hold the attributes of xdp state that are being passed\n * to the netdevice through the xdp op.\n */\nenum xdp_netdev_command {\n\t/* Set or clear a bpf program used in the earliest stages of packet\n\t * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee\n\t * is responsible for calling bpf_prog_put on any old progs that are\n\t * stored. In case of error, the callee need not release the new prog\n\t * reference, but on success it takes ownership and must bpf_prog_put\n\t * when it is no longer used.\n\t */\n\tXDP_SETUP_PROG,\n\tXDP_SETUP_PROG_HW,\n\t/* Check if a bpf program is set on the device.  The callee should\n\t * set @prog_attached to one of XDP_ATTACHED_* values, note that \"true\"\n\t * is equivalent to XDP_ATTACHED_DRV.\n\t */\n\tXDP_QUERY_PROG,\n};\n\nstruct netlink_ext_ack;\n\nstruct netdev_xdp {\n\tenum xdp_netdev_command command;\n\tunion {\n\t\t/* XDP_SETUP_PROG */\n\t\tstruct {\n\t\t\tu32 flags;\n\t\t\tstruct bpf_prog *prog;\n\t\t\tstruct netlink_ext_ack *extack;\n\t\t};\n\t\t/* XDP_QUERY_PROG */\n\t\tstruct {\n\t\t\tu8 prog_attached;\n\t\t\tu32 prog_id;\n\t\t};\n\t};\n};\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstruct xfrmdev_ops {\n\tint\t(*xdo_dev_state_add) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_delete) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_free) (struct xfrm_state *x);\n\tbool\t(*xdo_dev_offload_ok) (struct sk_buff *skb,\n\t\t\t\t       struct xfrm_state *x);\n};\n#endif\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when a network device is registered.\n *     The network device can use this for any late stage initialization\n *     or semantic validation. It can fail with an error code which will\n *     be propagated back to register_netdev.\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when a network device transitions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when a network device transitions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tReturns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop\n *\tthe queue before that can happen; it's for obsolete devices and weird\n *\tcorner cases, but the stack really does a non-trivial amount\n *\tof useless work if you return NETDEV_TX_BUSY.\n *\tRequired; cannot be NULL.\n *\n * netdev_features_t (*ndo_features_check)(struct sk_buff *skb,\n *\t\t\t\t\t   struct net_device *dev\n *\t\t\t\t\t   netdev_features_t features);\n *\tCalled by core transmit path to determine if device is capable of\n *\tperforming offload operations on a given packet. This is to give\n *\tthe device an opportunity to implement any restrictions that cannot\n *\tbe otherwise expressed by feature flags. The check is called with\n *\tthe set of features that the stack has calculated and it returns\n *\tthose the driver believes to be appropriate.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,\n *                         void *accel_priv, select_queue_fallback_t fallback);\n *\tCalled to decide which queue to use when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscuous is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\tIf driver handles unicast address filtering, it should set\n *\tIFF_UNICAST_FLT in its priv_flags.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tMAC address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user requests an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctls return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reasons; new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback used when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * void (*ndo_get_stats64)(struct net_device *dev,\n *                         struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * bool (*ndo_has_offload_stats)(const struct net_device *dev, int attr_id)\n *\tReturn true if this device supports offload stats of this attr_id.\n *\n * int (*ndo_get_offload_stats)(int attr_id, const struct net_device *dev,\n *\tvoid *attr_data)\n *\tGet statistics for offload operations by attr_id. Write it into the\n *\tattr_data pointer.\n *\n * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is registered.\n *\n * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan,\n *\t\t\t  u8 qos, __be16 proto);\n * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,\n *\t\t\t  int max_tx_rate);\n * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n *\n *      Enable or disable the VF ability to query its RSS Redirection Table and\n *      Hash Key. This is needed since on some devices VF share this information\n *      with PF and querying it may introduce a theoretical security risk.\n * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n * int (*ndo_setup_tc)(struct net_device *dev, enum tc_setup_type type,\n *\t\t       void *type_data);\n *\tCalled to setup any 'tc' scheduler, classifier or action on @dev.\n *\tThis is always called from the stack with the rtnl lock held and netif\n *\ttx queues stopped. This allows the netdevice to perform queue\n *\tmanagement safely.\n *\n *\tFiber Channel over Ethernet (FCoE) offload functions.\n * int (*ndo_fcoe_enable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to start using LLD for FCoE\n *\tso the underlying device can perform whatever needed configuration or\n *\tinitialization to support acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_disable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to stop using LLD for FCoE\n *\tso the underlying device can perform whatever needed clean-ups to\n *\tstop supporting acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,\n *\t\t\t     struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Initiator wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);\n *\tCalled when the FCoE Initiator/Target is done with the DDPed I/O as\n *\tindicated by the FC exchange id 'xid', so the underlying device can\n *\tclean up and reuse resources for later DDP requests.\n *\n * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,\n *\t\t\t      struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Target wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n *\t\t\t       struct netdev_fcoe_hbainfo *hbainfo);\n *\tCalled when the FCoE Protocol stack wants information on the underlying\n *\tdevice. This information is utilized by the FCoE protocol stack to\n *\tregister attributes with Fiber Channel management service as per the\n *\tFC-GS Fabric Device Management Information(FDMI) specification.\n *\n * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);\n *\tCalled when the underlying device wants to override default World Wide\n *\tName (WWN) generation mechanism in FCoE protocol stack to pass its own\n *\tWorld Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE\n *\tprotocol stack to use.\n *\n *\tRFS acceleration.\n * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,\n *\t\t\t    u16 rxq_index, u32 flow_id);\n *\tSet hardware filter for RFS.  rxq_index is the target queue index;\n *\tflow_id is a flow ID to be passed to rps_may_expire_flow() later.\n *\tReturn the filter ID on success, or a negative error code.\n *\n *\tSlave management functions (for bridge, bonding, etc).\n * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to make another netdev an underling.\n *\n * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to release previously enslaved netdev.\n *\n *      Feature/offload setting functions.\n * netdev_features_t (*ndo_fix_features)(struct net_device *dev,\n *\t\tnetdev_features_t features);\n *\tAdjusts the requested feature flags according to device-specific\n *\tconstraints, and returns the resulting flags. Must not modify\n *\tthe device state.\n *\n * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);\n *\tCalled to update device configuration to new features. Passed\n *\tfeature set might be less than what was returned by ndo_fix_features()).\n *\tMust return >0 or -errno if it changed dev->features itself.\n *\n * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid, u16 flags)\n *\tAdds an FDB entry to dev for addr.\n * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid)\n *\tDeletes the FDB entry from dev coresponding to addr.\n * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,\n *\t\t       struct net_device *dev, struct net_device *filter_dev,\n *\t\t       int *idx)\n *\tUsed to add FDB entries to dump requests. Implementers should add\n *\tentries to skb and update idx with the number of entries.\n *\n * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags)\n * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,\n *\t\t\t     struct net_device *dev, u32 filter_mask,\n *\t\t\t     int nlflags)\n * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags);\n *\n * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);\n *\tCalled to change device carrier. Soft-devices (like dummy, team, etc)\n *\twhich do not represent real hardware may define this to allow their\n *\tuserspace components to manage their virtual carrier state. Devices\n *\tthat determine carrier state from physical hardware properties (eg\n *\tnetwork cables) or protocol-dependent mechanisms (eg\n *\tUSB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.\n *\n * int (*ndo_get_phys_port_id)(struct net_device *dev,\n *\t\t\t       struct netdev_phys_item_id *ppid);\n *\tCalled to get ID of physical port of this device. If driver does\n *\tnot implement this, it is assumed that the hw is not able to have\n *\tmultiple net devices on single physical port.\n *\n * void (*ndo_udp_tunnel_add)(struct net_device *dev,\n *\t\t\t      struct udp_tunnel_info *ti);\n *\tCalled by UDP tunnel to notify a driver about the UDP port and socket\n *\taddress family that a UDP tunnel is listnening to. It is called only\n *\twhen a new port starts listening. The operation is protected by the\n *\tRTNL.\n *\n * void (*ndo_udp_tunnel_del)(struct net_device *dev,\n *\t\t\t      struct udp_tunnel_info *ti);\n *\tCalled by UDP tunnel to notify the driver about a UDP port and socket\n *\taddress family that the UDP tunnel is not listening to anymore. The\n *\toperation is protected by the RTNL.\n *\n * void* (*ndo_dfwd_add_station)(struct net_device *pdev,\n *\t\t\t\t struct net_device *dev)\n *\tCalled by upper layer devices to accelerate switching or other\n *\tstation functionality into hardware. 'pdev is the lowerdev\n *\tto use for the offload and 'dev' is the net device that will\n *\tback the offload. Returns a pointer to the private structure\n *\tthe upper layer will maintain.\n * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)\n *\tCalled by upper layer device to delete the station created\n *\tby 'ndo_dfwd_add_station'. 'pdev' is the net device backing\n *\tthe station and priv is the structure returned by the add\n *\toperation.\n * int (*ndo_set_tx_maxrate)(struct net_device *dev,\n *\t\t\t     int queue_index, u32 maxrate);\n *\tCalled when a user wants to set a max-rate limitation of specific\n *\tTX queue.\n * int (*ndo_get_iflink)(const struct net_device *dev);\n *\tCalled to get the iflink value of this device.\n * void (*ndo_change_proto_down)(struct net_device *dev,\n *\t\t\t\t bool proto_down);\n *\tThis function is used to pass protocol port error state information\n *\tto the switch driver. The switch driver can react to the proto_down\n *      by doing a phys down on the associated switch port.\n * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);\n *\tThis function is used to get egress tunnel information for given skb.\n *\tThis is useful for retrieving outer tunnel header parameters while\n *\tsampling packet.\n * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);\n *\tThis function is used to specify the headroom that the skb must\n *\tconsider when allocation skb during packet reception. Setting\n *\tappropriate rx headroom value allows avoiding skb head copy on\n *\tforward. Setting a negative value resets the rx headroom to the\n *\tdefault value.\n * int (*ndo_xdp)(struct net_device *dev, struct netdev_xdp *xdp);\n *\tThis function is used to set or query state related to XDP on the\n *\tnetdevice. See definition of enum xdp_netdev_command for details.\n * int (*ndo_xdp_xmit)(struct net_device *dev, struct xdp_buff *xdp);\n *\tThis function is used to submit a XDP packet for transmit on a\n *\tnetdevice.\n * void (*ndo_xdp_flush)(struct net_device *dev);\n *\tThis function is used to inform the driver to flush a particular\n *\txdp tx queue. Must be called on same CPU as xdp_xmit.\n */\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    void *accel_priv,\n\t\t\t\t\t\t    select_queue_fallback_t fallback);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tvoid\t\t\t(*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t   struct rtnl_link_stats64 *storage);\n\tbool\t\t\t(*ndo_has_offload_stats)(const struct net_device *dev, int attr_id);\n\tint\t\t\t(*ndo_get_offload_stats)(int attr_id,\n\t\t\t\t\t\t\t const struct net_device *dev,\n\t\t\t\t\t\t\t void *attr_data);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan,\n\t\t\t\t\t\t   u8 qos, __be16 proto);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_set_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, u64 guid,\n\t\t\t\t\t\t   int guid_type);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tenum tc_setup_type type,\n\t\t\t\t\t\tvoid *type_data);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct net_device *dev,\n\t\t\t\t\t\t       struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct net_device *dev,\n\t\t\t\t\t\t     struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint *idx);\n\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid\t\t\t(*ndo_udp_tunnel_add)(struct net_device *dev,\n\t\t\t\t\t\t      struct udp_tunnel_info *ti);\n\tvoid\t\t\t(*ndo_udp_tunnel_del)(struct net_device *dev,\n\t\t\t\t\t\t      struct udp_tunnel_info *ti);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tint\t\t\t(*ndo_get_lock_subclass)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_change_proto_down)(struct net_device *dev,\n\t\t\t\t\t\t\t bool proto_down);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n\tint\t\t\t(*ndo_xdp)(struct net_device *dev,\n\t\t\t\t\t   struct netdev_xdp *xdp);\n\tint\t\t\t(*ndo_xdp_xmit)(struct net_device *dev,\n\t\t\t\t\t\tstruct xdp_buff *xdp);\n\tvoid\t\t\t(*ndo_xdp_flush)(struct net_device *dev);\n};\n\n/**\n * enum net_device_priv_flags - &struct net_device priv_flags\n *\n * These are the &struct net_device, they are only set internally\n * by drivers and used in the kernel. These flags are invisible to\n * userspace; this means that the order of these flags can change\n * during any kernel release.\n *\n * You should have a pretty good reason to be extending these flags.\n *\n * @IFF_802_1Q_VLAN: 802.1Q VLAN device\n * @IFF_EBRIDGE: Ethernet bridging device\n * @IFF_BONDING: bonding master or slave\n * @IFF_ISATAP: ISATAP interface (RFC4214)\n * @IFF_WAN_HDLC: WAN HDLC device\n * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to\n *\trelease skb->dst\n * @IFF_DONT_BRIDGE: disallow bridging this ether dev\n * @IFF_DISABLE_NETPOLL: disable netpoll at run-time\n * @IFF_MACVLAN_PORT: device used as macvlan port\n * @IFF_BRIDGE_PORT: device used as bridge port\n * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port\n * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit\n * @IFF_UNICAST_FLT: Supports unicast filtering\n * @IFF_TEAM_PORT: device used as team port\n * @IFF_SUPP_NOFCS: device supports sending custom FCS\n * @IFF_LIVE_ADDR_CHANGE: device supports hardware address\n *\tchange when it's running\n * @IFF_MACVLAN: Macvlan device\n * @IFF_XMIT_DST_RELEASE_PERM: IFF_XMIT_DST_RELEASE not taking into account\n *\tunderlying stacked devices\n * @IFF_IPVLAN_MASTER: IPvlan master device\n * @IFF_IPVLAN_SLAVE: IPvlan slave device\n * @IFF_L3MDEV_MASTER: device is an L3 master device\n * @IFF_NO_QUEUE: device can run without qdisc attached\n * @IFF_OPENVSWITCH: device is a Open vSwitch master\n * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device\n * @IFF_TEAM: device is a team device\n * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured\n * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external\n *\tentity (i.e. the master device for bridged veth)\n * @IFF_MACSEC: device is a MACsec device\n */\nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_IPVLAN_MASTER\t\t= 1<<18,\n\tIFF_IPVLAN_SLAVE\t\t= 1<<19,\n\tIFF_L3MDEV_MASTER\t\t= 1<<20,\n\tIFF_NO_QUEUE\t\t\t= 1<<21,\n\tIFF_OPENVSWITCH\t\t\t= 1<<22,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<23,\n\tIFF_TEAM\t\t\t= 1<<24,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<25,\n\tIFF_PHONY_HEADROOM\t\t= 1<<26,\n\tIFF_MACSEC\t\t\t= 1<<27,\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_IPVLAN_MASTER\t\tIFF_IPVLAN_MASTER\n#define IFF_IPVLAN_SLAVE\t\tIFF_IPVLAN_SLAVE\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n\n/**\n *\tstruct net_device - The DEVICE structure.\n *\n *\tActually, this whole structure is a big mistake.  It mixes I/O\n *\tdata with strictly \"high-level\" data, and it has to know about\n *\talmost every data structure used in the INET module.\n *\n *\t@name:\tThis is the first field of the \"visible\" part of this structure\n *\t\t(i.e. as seen by users in the \"Space.c\" file).  It is the name\n *\t\tof the interface.\n *\n *\t@name_hlist: \tDevice name hash chain, please keep it close to name[]\n *\t@ifalias:\tSNMP alias\n *\t@mem_end:\tShared memory end\n *\t@mem_start:\tShared memory start\n *\t@base_addr:\tDevice I/O address\n *\t@irq:\t\tDevice IRQ number\n *\n *\t@carrier_changes:\tStats to monitor carrier on<->off transitions\n *\n *\t@state:\t\tGeneric network queuing layer state, see netdev_state_t\n *\t@dev_list:\tThe global list of network devices\n *\t@napi_list:\tList entry used for polling NAPI devices\n *\t@unreg_list:\tList entry  when we are unregistering the\n *\t\t\tdevice; see the function unregister_netdev\n *\t@close_list:\tList entry used when we are closing the device\n *\t@ptype_all:     Device-specific packet handlers for all protocols\n *\t@ptype_specific: Device-specific, protocol-specific packet handlers\n *\n *\t@adj_list:\tDirectly linked devices, like slaves for bonding\n *\t@features:\tCurrently active device features\n *\t@hw_features:\tUser-changeable features\n *\n *\t@wanted_features:\tUser-requested features\n *\t@vlan_features:\t\tMask of features inheritable by VLAN devices\n *\n *\t@hw_enc_features:\tMask of features inherited by encapsulating devices\n *\t\t\t\tThis field indicates what encapsulation\n *\t\t\t\toffloads the hardware is capable of doing,\n *\t\t\t\tand drivers will need to set them appropriately.\n *\n *\t@mpls_features:\tMask of features inheritable by MPLS\n *\n *\t@ifindex:\tinterface index\n *\t@group:\t\tThe group the device belongs to\n *\n *\t@stats:\t\tStatistics struct, which was left as a legacy, use\n *\t\t\trtnl_link_stats64 instead\n *\n *\t@rx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@tx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@rx_nohandler:\tnohandler dropped packets by core network on\n *\t\t\tinactive devices, do not use this in drivers\n *\n *\t@wireless_handlers:\tList of functions to handle Wireless Extensions,\n *\t\t\t\tinstead of ioctl,\n *\t\t\t\tsee <net/iw_handler.h> for details.\n *\t@wireless_data:\tInstance data managed by the core of wireless extensions\n *\n *\t@netdev_ops:\tIncludes several pointers to callbacks,\n *\t\t\tif one wants to override the ndo_*() functions\n *\t@ethtool_ops:\tManagement operations\n *\t@ndisc_ops:\tIncludes callbacks for different IPv6 neighbour\n *\t\t\tdiscovery handling. Necessary for e.g. 6LoWPAN.\n *\t@header_ops:\tIncludes callbacks for creating,parsing,caching,etc\n *\t\t\tof Layer 2 headers.\n *\n *\t@flags:\t\tInterface flags (a la BSD)\n *\t@priv_flags:\tLike 'flags' but invisible to userspace,\n *\t\t\tsee if.h for the definitions\n *\t@gflags:\tGlobal flags ( kept as legacy )\n *\t@padded:\tHow much padding added by alloc_netdev()\n *\t@operstate:\tRFC2863 operstate\n *\t@link_mode:\tMapping policy to operstate\n *\t@if_port:\tSelectable AUI, TP, ...\n *\t@dma:\t\tDMA channel\n *\t@mtu:\t\tInterface MTU value\n *\t@min_mtu:\tInterface Minimum MTU value\n *\t@max_mtu:\tInterface Maximum MTU value\n *\t@type:\t\tInterface hardware type\n *\t@hard_header_len: Maximum hardware header length.\n *\t@min_header_len:  Minimum hardware header length\n *\n *\t@needed_headroom: Extra headroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed\n *\t@needed_tailroom: Extra tailroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed. Some cases also use\n *\t\t\t  LL_MAX_HEADER instead to allocate the skb\n *\n *\tinterface address info:\n *\n * \t@perm_addr:\t\tPermanent hw address\n * \t@addr_assign_type:\tHw address assignment type\n * \t@addr_len:\t\tHardware address length\n *\t@neigh_priv_len:\tUsed in neigh_alloc()\n * \t@dev_id:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same link layer address\n * \t@dev_port:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same function\n *\t@addr_list_lock:\tXXX: need comments on this one\n *\t@uc_promisc:\t\tCounter that indicates promiscuous mode\n *\t\t\t\thas been enabled due to the need to listen to\n *\t\t\t\tadditional unicast addresses in a device that\n *\t\t\t\tdoes not implement ndo_set_rx_mode()\n *\t@uc:\t\t\tunicast mac addresses\n *\t@mc:\t\t\tmulticast mac addresses\n *\t@dev_addrs:\t\tlist of device hw addresses\n *\t@queues_kset:\t\tGroup of all Kobjects in the Tx and RX queues\n *\t@promiscuity:\t\tNumber of times the NIC is told to work in\n *\t\t\t\tpromiscuous mode; if it becomes 0 the NIC will\n *\t\t\t\texit promiscuous mode\n *\t@allmulti:\t\tCounter, enables or disables allmulticast mode\n *\n *\t@vlan_info:\tVLAN info\n *\t@dsa_ptr:\tdsa specific data\n *\t@tipc_ptr:\tTIPC specific data\n *\t@atalk_ptr:\tAppleTalk link\n *\t@ip_ptr:\tIPv4 specific data\n *\t@dn_ptr:\tDECnet specific data\n *\t@ip6_ptr:\tIPv6 specific data\n *\t@ax25_ptr:\tAX.25 specific data\n *\t@ieee80211_ptr:\tIEEE 802.11 specific data, assign before registering\n *\n *\t@dev_addr:\tHw address (before bcast,\n *\t\t\tbecause most packets are unicast)\n *\n *\t@_rx:\t\t\tArray of RX queues\n *\t@num_rx_queues:\t\tNumber of RX queues\n *\t\t\t\tallocated at register_netdev() time\n *\t@real_num_rx_queues: \tNumber of RX queues currently active in device\n *\n *\t@rx_handler:\t\thandler for received packets\n *\t@rx_handler_data: \tXXX: need comments on this one\n *\t@ingress_queue:\t\tXXX: need comments on this one\n *\t@broadcast:\t\thw bcast address\n *\n *\t@rx_cpu_rmap:\tCPU reverse-mapping for RX completion interrupts,\n *\t\t\tindexed by RX queue number. Assigned by driver.\n *\t\t\tThis must only be set if the ndo_rx_flow_steer\n *\t\t\toperation is defined\n *\t@index_hlist:\t\tDevice index hash chain\n *\n *\t@_tx:\t\t\tArray of TX queues\n *\t@num_tx_queues:\t\tNumber of TX queues allocated at alloc_netdev_mq() time\n *\t@real_num_tx_queues: \tNumber of TX queues currently active in device\n *\t@qdisc:\t\t\tRoot qdisc from userspace point of view\n *\t@tx_queue_len:\t\tMax frames per queue allowed\n *\t@tx_global_lock: \tXXX: need comments on this one\n *\n *\t@xps_maps:\tXXX: need comments on this one\n *\n *\t@watchdog_timeo:\tRepresents the timeout that is used by\n *\t\t\t\tthe watchdog (see dev_watchdog())\n *\t@watchdog_timer:\tList of timers\n *\n *\t@pcpu_refcnt:\t\tNumber of references to this device\n *\t@todo_list:\t\tDelayed register/unregister\n *\t@link_watch_list:\tXXX: need comments on this one\n *\n *\t@reg_state:\t\tRegister/unregister state machine\n *\t@dismantle:\t\tDevice is going to be freed\n *\t@rtnl_link_state:\tThis enum represents the phases of creating\n *\t\t\t\ta new link\n *\n *\t@needs_free_netdev:\tShould unregister perform free_netdev?\n *\t@priv_destructor:\tCalled from unregister\n *\t@npinfo:\t\tXXX: need comments on this one\n * \t@nd_net:\t\tNetwork namespace this network device is inside\n *\n * \t@ml_priv:\tMid-layer private\n * \t@lstats:\tLoopback statistics\n * \t@tstats:\tTunnel statistics\n * \t@dstats:\tDummy statistics\n * \t@vstats:\tVirtual ethernet statistics\n *\n *\t@garp_port:\tGARP\n *\t@mrp_port:\tMRP\n *\n *\t@dev:\t\tClass/net/name entry\n *\t@sysfs_groups:\tSpace for optional device, statistics and wireless\n *\t\t\tsysfs groups\n *\n *\t@sysfs_rx_queue_group:\tSpace for optional per-rx queue attributes\n *\t@rtnl_link_ops:\tRtnl_link_ops\n *\n *\t@gso_max_size:\tMaximum size of generic segmentation offload\n *\t@gso_max_segs:\tMaximum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\n *\t@dcbnl_ops:\tData Center Bridging netlink ops\n *\t@num_tc:\tNumber of traffic classes in the net device\n *\t@tc_to_txq:\tXXX: need comments on this one\n *\t@prio_tc_map:\tXXX: need comments on this one\n *\n *\t@fcoe_ddp_xid:\tMax exchange id for FCoE LRO by ddp\n *\n *\t@priomap:\tXXX: need comments on this one\n *\t@phydev:\tPhysical device may attach itself\n *\t\t\tfor hardware timestamping\n *\n *\t@qdisc_tx_busylock: lockdep class annotating Qdisc->busylock spinlock\n *\t@qdisc_running_key: lockdep class annotating Qdisc->running seqcount\n *\n *\t@proto_down:\tprotocol port state information can be sent to the\n *\t\t\tswitch driver and used to set the phys state of the\n *\t\t\tswitch port.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct hlist_node\tname_hlist;\n\tchar \t\t\t*ifalias;\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\tint\t\t\tirq;\n\n\tatomic_t\t\tcarrier_changes;\n\n\t/*\n\t *\tSome hardware also needs these fields (state,dev_list,\n\t *\tnapi_list,unreg_list,close_list) but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\tnetdev_features_t\tgso_partial_features;\n\n\tint\t\t\tifindex;\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats;\n\n\tatomic_long_t\t\trx_dropped;\n\tatomic_long_t\t\ttx_dropped;\n\tatomic_long_t\t\trx_nohandler;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *wireless_handlers;\n\tstruct iw_public_data\t*wireless_data;\n#endif\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_SWITCHDEV\n\tconst struct switchdev_ops *switchdev_ops;\n#endif\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tconst struct ndisc_ops *ndisc_ops;\n#endif\n\n#ifdef CONFIG_XFRM\n\tconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\n\tunsigned int\t\tpriv_flags;\n\n\tunsigned short\t\tgflags;\n\tunsigned short\t\tpadded;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\tunsigned int\t\tmtu;\n\tunsigned int\t\tmin_mtu;\n\tunsigned int\t\tmax_mtu;\n\tunsigned short\t\ttype;\n\tunsigned short\t\thard_header_len;\n\tunsigned char\t\tmin_header_len;\n\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tspinlock_t\t\taddr_list_lock;\n\tunsigned char\t\tname_assign_type;\n\tbool\t\t\tuc_promisc;\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol-specific pointers */\n\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_switch_tree\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n\tvoid \t\t\t*atalk_ptr;\n\tstruct in_device __rcu\t*ip_ptr;\n\tstruct dn_dev __rcu     *dn_ptr;\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n\tvoid\t\t\t*ax25_ptr;\n\tstruct wireless_dev\t*ieee80211_ptr;\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\n\n#ifdef CONFIG_SYSFS\n\tstruct netdev_rx_queue\t*_rx;\n\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n#endif\n\n\tstruct bpf_prog __rcu\t*xdp_prog;\n\tunsigned long\t\tgro_flush_timeout;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *ingress_cl_list;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc\t\t*qdisc;\n#ifdef CONFIG_NET_SCHED\n\tDECLARE_HASHTABLE\t(qdisc_hash, 4);\n#endif\n\tunsigned int\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\tint\t\t\twatchdog_timeo;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *egress_cl_list;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\tstruct timer_list\twatchdog_timer;\n\n\tint __percpu\t\t*pcpu_refcnt;\n\tstruct list_head\ttodo_list;\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tbool needs_free_netdev;\n\tvoid (*priv_destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t\tstruct pcpu_vstats __percpu\t\t*vstats;\n\t};\n\n#if IS_ENABLED(CONFIG_GARP)\n\tstruct garp_port __rcu\t*garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\n\tstruct mrp_port __rcu\t*mrp_port;\n#endif\n\n\tstruct device\t\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n#define GSO_MAX_SEGS\t\t65535\n\tu16\t\t\tgso_max_segs;\n\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\tu8\t\t\tnum_tc;\n\tstruct netdev_tc_txq\ttc_to_txq[TC_MAX_QUEUE];\n\tu8\t\t\tprio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device\t*phydev;\n\tstruct lock_class_key\t*qdisc_tx_busylock;\n\tstruct lock_class_key\t*qdisc_running_key;\n\tbool\t\t\tproto_down;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\nstatic inline bool netif_elide_gro(const struct net_device *dev)\n{\n\tif (!(dev->features & NETIF_F_GRO) || dev->xdp_prog)\n\t\treturn true;\n\treturn false;\n}\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq);\nvoid netdev_reset_tc(struct net_device *dev);\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset);\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc);\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n#define netdev_lockdep_set_classes(dev)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key qdisc_tx_busylock_key;\t\\\n\tstatic struct lock_class_key qdisc_running_key;\t\t\\\n\tstatic struct lock_class_key qdisc_xmit_lock_key;\t\\\n\tstatic struct lock_class_key dev_addr_list_lock_key;\t\\\n\tunsigned int i;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t(dev)->qdisc_tx_busylock = &qdisc_tx_busylock_key;\t\\\n\t(dev)->qdisc_running_key = &qdisc_running_key;\t\t\\\n\tlockdep_set_class(&(dev)->addr_list_lock,\t\t\\\n\t\t\t  &dev_addr_list_lock_key); \t\t\\\n\tfor (i = 0; i < (dev)->num_tx_queues; i++)\t\t\\\n\t\tlockdep_set_class(&(dev)->_tx[i]._xmit_lock,\t\\\n\t\t\t\t  &qdisc_xmit_lock_key);\t\\\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv);\n\n/* returns the headroom that the master device needs to take in account\n * when forwarding to this dev\n */\nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n/* set the device rx headroom to the dev's default */\nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fine-grained identification of different network device types. For\n * example Ethernet, Wireless LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/* Default NAPI poll() weight\n * Device drivers are strongly advised to not use bigger value\n */\n#define NAPI_POLL_WEIGHT 64\n\n/**\n *\tnetif_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a NAPI context prior to calling\n * *any* of the other NAPI-related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *\tnetif_tx_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * This variant of netif_napi_add() should be used from drivers using NAPI\n * to exclusively poll a TX queue.\n * This will avoid we add it into napi_hash[], thus polluting this hash table.\n */\nstatic inline void netif_tx_napi_add(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int),\n\t\t\t\t     int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add(dev, napi, poll, weight);\n}\n\n/**\n *  netif_napi_del - remove a NAPI context\n *  @napi: NAPI context\n *\n *  netif_napi_del() removes a NAPI context from the network device NAPI list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid\t*frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint\tdata_offset;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tu16\tflush;\n\n\t/* Save the IP ID here and check when we get to the transport layer */\n\tu16\tflush_id;\n\n\t/* Number of segments aggregated. */\n\tu16\tcount;\n\n\t/* Start offset for remote checksum offload */\n\tu16\tgro_remcsum_start;\n\n\t/* jiffies when first packet was created/queued */\n\tunsigned long age;\n\n\t/* Used in ipv6_gro_receive() and foo-over-udp */\n\tu16\tproto;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tu8\tsame_flow:1;\n\n\t/* Used in tunnel GRO receive */\n\tu8\tencap_mark:1;\n\n\t/* GRO checksum is valid */\n\tu8\tcsum_valid:1;\n\n\t/* Number of checksums via CHECKSUM_UNNECESSARY */\n\tu8\tcsum_cnt:3;\n\n\t/* Free the skb? */\n\tu8\tfree:2;\n#define NAPI_GRO_FREE\t\t  1\n#define NAPI_GRO_FREE_STOLEN_HEAD 2\n\n\t/* Used in foo-over-udp, set in udp[46]_gro_receive */\n\tu8\tis_ipv6:1;\n\n\t/* Used in GRE, set in fou/gue_gro_receive */\n\tu8\tis_fou:1;\n\n\t/* Used to determine if flush_id can be ignored */\n\tu8\tis_atomic:1;\n\n\t/* Number of gro_receive callbacks this packet already went through */\n\tu8 recursion_counter:4;\n\n\t/* 1 bit hole */\n\n\t/* used to support CHECKSUM_COMPLETE for tunneling protocols */\n\t__wsum\tcsum;\n\n\t/* used in skb_gro_receive() slow path */\n\tstruct sk_buff *last;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\n#define GRO_RECURSION_LIMIT 15\nstatic inline int gro_recursion_inc_test(struct sk_buff *skb)\n{\n\treturn ++NAPI_GRO_CB(skb)->recursion_counter == GRO_RECURSION_LIMIT;\n}\n\ntypedef struct sk_buff **(*gro_receive_t)(struct sk_buff **, struct sk_buff *);\nstatic inline struct sk_buff **call_gro_receive(gro_receive_t cb,\n\t\t\t\t\t\tstruct sk_buff **head,\n\t\t\t\t\t\tstruct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(head, skb);\n}\n\ntypedef struct sk_buff **(*gro_receive_sk_t)(struct sock *, struct sk_buff **,\n\t\t\t\t\t     struct sk_buff *);\nstatic inline struct sk_buff **call_gro_receive_sk(gro_receive_sk_t cb,\n\t\t\t\t\t\t   struct sock *sk,\n\t\t\t\t\t\t   struct sk_buff **head,\n\t\t\t\t\t\t   struct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(sk, head, skb);\n}\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t/* This is really htons(ether_type). */\n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\n/* often modified stats are per-CPU, other are shared (netdev->stats) */\nstruct pcpu_sw_netstats {\n\tu64     rx_packets;\n\tu64     rx_bytes;\n\tu64     tx_packets;\n\tu64     tx_bytes;\n\tstruct u64_stats_sync   syncp;\n};\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n/* netdevice notifier chain. Please remember to update the rtnetlink\n * notification exclusion list in rtnetlink_event() when adding new\n * types.\n */\n#define NETDEV_UP\t0x0001\t/* For now you can't veto a device up/down */\n#define NETDEV_DOWN\t0x0002\n#define NETDEV_REBOOT\t0x0003\t/* Tell a protocol stack a network interface\n\t\t\t\t   detected a hardware crash and restarted\n\t\t\t\t   - we can use this eg to kick tcp sessions\n\t\t\t\t   once done */\n#define NETDEV_CHANGE\t0x0004\t/* Notify device state change */\n#define NETDEV_REGISTER 0x0005\n#define NETDEV_UNREGISTER\t0x0006\n#define NETDEV_CHANGEMTU\t0x0007 /* notify after mtu change happened */\n#define NETDEV_CHANGEADDR\t0x0008\n#define NETDEV_GOING_DOWN\t0x0009\n#define NETDEV_CHANGENAME\t0x000A\n#define NETDEV_FEAT_CHANGE\t0x000B\n#define NETDEV_BONDING_FAILOVER 0x000C\n#define NETDEV_PRE_UP\t\t0x000D\n#define NETDEV_PRE_TYPE_CHANGE\t0x000E\n#define NETDEV_POST_TYPE_CHANGE\t0x000F\n#define NETDEV_POST_INIT\t0x0010\n#define NETDEV_UNREGISTER_FINAL 0x0011\n#define NETDEV_RELEASE\t\t0x0012\n#define NETDEV_NOTIFY_PEERS\t0x0013\n#define NETDEV_JOIN\t\t0x0014\n#define NETDEV_CHANGEUPPER\t0x0015\n#define NETDEV_RESEND_IGMP\t0x0016\n#define NETDEV_PRECHANGEMTU\t0x0017 /* notify before mtu change happened */\n#define NETDEV_CHANGEINFODATA\t0x0018\n#define NETDEV_BONDING_INFO\t0x0019\n#define NETDEV_PRECHANGEUPPER\t0x001A\n#define NETDEV_CHANGELOWERSTATE\t0x001B\n#define NETDEV_UDP_TUNNEL_PUSH_INFO\t0x001C\n#define NETDEV_UDP_TUNNEL_DROP_INFO\t0x001D\n#define NETDEV_CHANGE_TX_QUEUE_LEN\t0x001E\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\n\nstruct netdev_notifier_info {\n\tstruct net_device *dev;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct net_device *upper_dev; /* new upper dev */\n\tbool master; /* is upper dev master */\n\tbool linking; /* is the notification for link or unlink */\n\tvoid *upper_info; /* upper dev info */\n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tvoid *lower_state_info; /* is lower dev state */\n};\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\n\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nunsigned long netdev_boot_base(const char *prefix, int unit);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev);\nvoid dev_close(struct net_device *dev);\nvoid dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nint dev_queue_xmit(struct sk_buff *skb);\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv);\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nvoid synchronize_net(void);\nint init_dummy_netdev(struct net_device *dev);\n\nDECLARE_PER_CPU(int, xmit_recursion);\n#define XMIT_RECURSION_LIMIT\t10\n\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(xmit_recursion);\n}\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id);\nint netdev_get_name(struct net *net, char *name, int ifindex);\nint dev_restart(struct net_device *dev);\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void skb_gro_frag0_invalidate(struct sk_buff *skb)\n{\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tif (!pskb_may_pull(skb, hlen))\n\t\treturn NULL;\n\n\tskb_gro_frag0_invalidate(skb);\n\treturn skb->data + offset;\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline void skb_gro_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t\tconst void *start, unsigned int len)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid)\n\t\tNAPI_GRO_CB(skb)->csum = csum_sub(NAPI_GRO_CB(skb)->csum,\n\t\t\t\t\t\t  csum_partial(start, len, 0));\n}\n\n/* GRO checksum functions. These are logical equivalents of the normal\n * checksum functions (in skbuff.h) except that they operate on the GRO\n * offsets and fields in sk_buff.\n */\n\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb);\n\nstatic inline bool skb_at_gro_remcsum_start(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->gro_remcsum_start == skb_gro_offset(skb));\n}\n\nstatic inline bool __skb_gro_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t      bool zero_okay,\n\t\t\t\t\t\t      __sum16 check)\n{\n\treturn ((skb->ip_summed != CHECKSUM_PARTIAL ||\n\t\tskb_checksum_start_offset(skb) <\n\t\t skb_gro_offset(skb)) &&\n\t\t!skb_at_gro_remcsum_start(skb) &&\n\t\tNAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t(!zero_okay || check));\n}\n\nstatic inline __sum16 __skb_gro_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t\t   __wsum psum)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid &&\n\t    !csum_fold(csum_add(psum, NAPI_GRO_CB(skb)->csum)))\n\t\treturn 0;\n\n\tNAPI_GRO_CB(skb)->csum = psum;\n\n\treturn __skb_gro_checksum_complete(skb);\n}\n\nstatic inline void skb_gro_incr_csum_unnecessary(struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->csum_cnt > 0) {\n\t\t/* Consume a checksum from CHECKSUM_UNNECESSARY */\n\t\tNAPI_GRO_CB(skb)->csum_cnt--;\n\t} else {\n\t\t/* Update skb for CHECKSUM_UNNECESSARY and csum_level when we\n\t\t * verified a new top level checksum or an encapsulated one\n\t\t * during GRO. This saves work if we fallback to normal path.\n\t\t */\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t}\n}\n\n#define __skb_gro_checksum_validate(skb, proto, zero_okay, check,\t\\\n\t\t\t\t    compute_pseudo)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_gro_checksum_validate_complete(skb,\t\\\n\t\t\t\tcompute_pseudo(skb, proto));\t\t\\\n\tif (!__ret)\t\t\t\t\t\t\t\\\n\t\tskb_gro_incr_csum_unnecessary(skb);\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_gro_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, false, 0, compute_pseudo)\n\n#define skb_gro_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t     compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, true, check, compute_pseudo)\n\n#define skb_gro_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_gro_checksum_validate(skb, 0, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_gro_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t!NAPI_GRO_CB(skb)->csum_valid);\n}\n\nstatic inline void __skb_gro_checksum_convert(struct sk_buff *skb,\n\t\t\t\t\t      __sum16 check, __wsum pseudo)\n{\n\tNAPI_GRO_CB(skb)->csum = ~pseudo;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n}\n\n#define skb_gro_checksum_try_convert(skb, proto, check, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_convert_check(skb))\t\t\t\\\n\t\t__skb_gro_checksum_convert(skb, check,\t\t\t\\\n\t\t\t\t\t   compute_pseudo(skb, proto));\t\\\n} while (0)\n\nstruct gro_remcsum {\n\tint offset;\n\t__wsum delta;\n};\n\nstatic inline void skb_gro_remcsum_init(struct gro_remcsum *grc)\n{\n\tgrc->offset = 0;\n\tgrc->delta = 0;\n}\n\nstatic inline void *skb_gro_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t    unsigned int off, size_t hdrlen,\n\t\t\t\t\t    int start, int offset,\n\t\t\t\t\t    struct gro_remcsum *grc,\n\t\t\t\t\t    bool nopartial)\n{\n\t__wsum delta;\n\tsize_t plen = hdrlen + max_t(size_t, offset + sizeof(u16), start);\n\n\tBUG_ON(!NAPI_GRO_CB(skb)->csum_valid);\n\n\tif (!nopartial) {\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = off + hdrlen + start;\n\t\treturn ptr;\n\t}\n\n\tptr = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, off + plen)) {\n\t\tptr = skb_gro_header_slow(skb, off + plen, off);\n\t\tif (!ptr)\n\t\t\treturn NULL;\n\t}\n\n\tdelta = remcsum_adjust(ptr + hdrlen, NAPI_GRO_CB(skb)->csum,\n\t\t\t       start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tNAPI_GRO_CB(skb)->csum = csum_add(NAPI_GRO_CB(skb)->csum, delta);\n\n\tgrc->offset = off + hdrlen + offset;\n\tgrc->delta = delta;\n\n\treturn ptr;\n}\n\nstatic inline void skb_gro_remcsum_cleanup(struct sk_buff *skb,\n\t\t\t\t\t   struct gro_remcsum *grc)\n{\n\tvoid *ptr;\n\tsize_t plen = grc->offset + sizeof(u16);\n\n\tif (!grc->delta)\n\t\treturn;\n\n\tptr = skb_gro_header_fast(skb, grc->offset);\n\tif (skb_gro_header_hard(skb, grc->offset + sizeof(u16))) {\n\t\tptr = skb_gro_header_slow(skb, plen, grc->offset);\n\t\tif (!ptr)\n\t\t\treturn;\n\t}\n\n\tremcsum_unadjust((__sum16 *)ptr, grc->delta);\n}\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff **pp, int flush)\n{\n\tif (PTR_ERR(pp) != -EINPROGRESS)\n\t\tNAPI_GRO_CB(skb)->flush |= flush;\n}\n#else\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff **pp, int flush)\n{\n\tNAPI_GRO_CB(skb)->flush |= flush;\n}\n#endif\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\n/* ll_header must have at least hard_header_len allocated */\nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\tif (len < dev->min_header_len)\n\t\treturn false;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n#define FLOW_LIMIT_HISTORY\t(1 << 7)  /* must be ^2 and !overflow buckets */\nstruct sd_flow_limit {\n\tu64\t\t\tcount;\n\tunsigned int\t\tnum_buckets;\n\tunsigned int\t\thistory_head;\n\tu16\t\t\thistory[FLOW_LIMIT_HISTORY];\n\tu8\t\t\tbuckets[];\n};\n\nextern int netdev_flow_limit_table_len;\n#endif /* CONFIG_NET_FLOW_LIMIT */\n\n/*\n * Incoming packets are placed on per-CPU queues\n */\nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\treceived_rps;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n\n#ifdef CONFIG_RPS\n\t/* input_queue_head should be written by cpu owning this struct,\n\t * and only read by other cpus. Worth using a cache line.\n\t */\n\tunsigned int\t\tinput_queue_head ____cacheline_aligned_in_smp;\n\n\t/* Elements below can be accessed between CPUs for RPS/RFS */\n\tcall_single_data_t\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic __always_inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic __always_inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetdev_txq_bql_enqueue_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their ndo_start_xmit(),\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_complete_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their TX completion path,\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t/*\n\t * The XOFF flag must be set before checking the dql_avail below,\n\t * because in netdev_tx_completed_queue we update the dql_completed\n\t * before checking the XOFF flag.\n\t */\n\tsmp_mb();\n\n\t/* check again in case another CPU has just made room avail */\n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n/**\n * \tnetdev_sent_queue - report the number of bytes queued to hardware\n * \t@dev: network device\n * \t@bytes: number of bytes queued to the hardware device queue\n *\n * \tReport the number of bytes queued for sending/completion to the network\n * \tdevice hardware queue. @bytes should be a good approximation and should\n * \texactly match netdev_completed_queue() @bytes\n */\nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t/*\n\t * Without the memory barrier there is a small possiblity that\n\t * netdev_tx_sent_queue will miss the update and cause the queue to\n\t * be stopped forever\n\t */\n\tsmp_mb();\n\n\tif (dql_avail(&dev_queue->dql) < 0)\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n/**\n * \tnetdev_completed_queue - report bytes and packets completed by device\n * \t@dev: network device\n * \t@pkts: actual number of packets sent over the medium\n * \t@bytes: actual number of bytes sent over the medium\n *\n * \tReport the number of bytes and packets transmitted by the network device\n * \thardware queue over the physical medium, @bytes must exactly match the\n * \t@bytes amount passed to netdev_sent_queue()\n */\nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n/**\n * \tnetdev_reset_queue - reset the packets and bytes count of a network device\n * \t@dev_queue: network device\n *\n * \tReset the bytes and packet count of a network device and clear the\n * \tsoftware flow control OFF bit for this network device\n */\nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n/**\n * \tnetdev_cap_txqueue - check if selected tx queue exceeds device queues\n * \t@dev: network device\n * \t@queue_index: given tx queue index\n *\n * \tReturns 0 if given tx queue index >= number of device tx queues,\n * \totherwise returns the originally passed tx queue index.\n */\nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start,\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_wake_queue(txq);\n}\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n#endif\n\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues);\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SYSFS\nstatic inline unsigned int get_netdev_rx_queue_index(\n\t\tstruct netdev_rx_queue *queue)\n{\n\tstruct net_device *dev = queue->dev;\n\tint index = queue - dev->_rx;\n\n\tBUG_ON(index >= dev->num_rx_queues);\n\treturn index;\n}\n#endif\n\n#define DEFAULT_MAX_NUM_RSS_QUEUES\t(8)\nint netif_get_num_default_rss_queues(void);\n\nenum skb_free_reason {\n\tSKB_REASON_CONSUMED,\n\tSKB_REASON_DROPPED,\n};\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason);\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason);\n\n/*\n * It is not allowed to call kfree_skb() or consume_skb() from hardware\n * interrupt context or with hardware interrupts being disabled.\n * (in_irq() || irqs_disabled())\n *\n * We provide four helpers that can be used in following contexts :\n *\n * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.\n *  Typically used in place of consume_skb(skb) in TX completion path\n *\n * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_any(skb) when caller doesn't know its current irq context,\n *  and consumed a packet. Used in place of consume_skb(skb)\n */\nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_CONSUMED);\n}\n\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog);\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb);\nint netif_rx(struct sk_buff *skb);\nint netif_rx_ni(struct sk_buff *skb);\nint netif_receive_skb(struct sk_buff *skb);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nbool netdev_is_rx_handler_busy(struct net_device *dev);\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *);\nint dev_ethtool(struct net *net, struct ifreq *);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *, unsigned int flags);\nint dev_change_flags(struct net_device *, unsigned int);\nvoid __dev_notify_flags(struct net_device *, unsigned int old_flags,\n\t\t\tunsigned int gchanges);\nint dev_change_name(struct net_device *, const char *);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_change_net_namespace(struct net_device *, struct net *, const char *);\nint __dev_set_mtu(struct net_device *, int);\nint dev_set_mtu(struct net_device *, int);\nvoid dev_set_group(struct net_device *, int);\nint dev_set_mac_address(struct net_device *, struct sockaddr *);\nint dev_change_carrier(struct net_device *, bool new_carrier);\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid);\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len);\nint dev_change_proto_down(struct net_device *dev, bool proto_down);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\n\ntypedef int (*xdp_op_t)(struct net_device *dev, struct netdev_xdp *xdp);\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, u32 flags);\nu8 __dev_xdp_attached(struct net_device *dev, xdp_op_t xdp_op, u32 *prog_id);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(const struct net_device *dev,\n\t\t\tconst struct sk_buff *skb);\n\nstatic __always_inline int ____dev_forward_skb(struct net_device *dev,\n\t\t\t\t\t       struct sk_buff *skb)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!is_skb_forwardable(dev, skb))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, true);\n\tskb->priority = 0;\n\treturn 0;\n}\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev);\n\nextern int\t\tnetdev_budget;\nextern unsigned int\tnetdev_budget_usecs;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nvoid netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tthis_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tthis_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nvoid linkwatch_init_dev(struct net_device *dev);\nvoid linkwatch_fire_event(struct net_device *dev);\nvoid linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\n\nvoid netif_carrier_off(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if device is dormant\n *\t@dev: network device\n *\n * Check if device is dormant.\n */\nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline bool netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline bool __netif_tx_acquire(struct netdev_queue *txq)\n{\n\t__acquire(&txq->_xmit_lock);\n\treturn true;\n}\n\nstatic inline void __netif_tx_release(struct netdev_queue *txq)\n{\n\t__release(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/* legacy drivers only, netdev_start_xmit() sets txq->trans_start */\nstatic inline void netif_trans_update(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\n\tif (txq->trans_start != jiffies)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_acquire(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\t__netif_tx_acquire(txq))\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_release(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_nested(struct net_device *dev)\n{\n\tint subclass = SINGLE_DEPTH_NESTING;\n\n\tif (dev->netdev_ops->ndo_get_lock_subclass)\n\t\tsubclass = dev->netdev_ops->ndo_get_lock_subclass(dev);\n\n\tspin_lock_nested(&dev->addr_list_lock, subclass);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nvoid ether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nvoid dev_addr_flush(struct net_device *dev);\nint dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n/**\n *  __dev_uc_sync - Synchonize device's unicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n/**\n *  __dev_uc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_uc_sync().\n */\nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n/* Functions used for multicast addresses handling */\nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n/**\n *  __dev_mc_sync - Synchonize device's multicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n/**\n *  __dev_mc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_mc_sync().\n */\nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n/* Functions used for secondary unicast and multicast support */\nvoid dev_set_rx_mode(struct net_device *dev);\nvoid __dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tdev_weight_rx_bias;\nextern int\t\tdev_weight_tx_bias;\nextern int\t\tdev_rx_weight;\nextern int\t\tdev_tx_weight;\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *upper_dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data);\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev);\n\nbool netdev_has_any_upper_dev(struct net_device *dev);\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nstruct net_device *netdev_all_lower_get_next(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_lower_get_next_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter);\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\tvoid *data),\n\t\t\t      void *data);\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n/* RSS keys are 40 or 52 bytes long */\n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint dev_get_nest_level(struct net_device *dev);\nint skb_checksum_help(struct sk_buff *skb);\nint skb_crc32c_csum_help(struct sk_buff *skb);\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features);\n\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path);\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\nstatic inline\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn __skb_gso_segment(skb, features, true);\n}\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t/* Assume this is an IP checksum (not SCTP CRC) */\n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t/* Can checksum everything */\n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nint __init dev_proc_init(void);\n#else\n#define dev_proc_init() 0\n#endif\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\tskb->xmit_more = more ? 1 : 0;\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(const struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(const struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nstatic inline int netdev_class_create_file(const struct class_attribute *class_attr)\n{\n\treturn netdev_class_create_file_ns(class_attr, NULL);\n}\n\nstatic inline void netdev_class_remove_file(const struct class_attribute *class_attr)\n{\n\tnetdev_class_remove_file_ns(class_attr, NULL);\n}\n\nextern const struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nvoid linkwatch_run_queue(void);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n/* Allow TSO being used on stacked device :\n * Performing the GSO segmentation before last device\n * is a performance improvement.\n */\nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = (netdev_features_t)gso_type << NETIF_F_GSO_SHIFT;\n\n\t/* check flags correspondence */\n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_FIXEDID != (NETIF_F_TSO_MANGLEID >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP4  != (NETIF_F_GSO_IPXIP4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP6  != (NETIF_F_GSO_IPXIP6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_PARTIAL != (NETIF_F_GSO_PARTIAL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SCTP    != (NETIF_F_GSO_SCTP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_ESP != (NETIF_F_GSO_ESP >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nstatic inline void skb_gso_error_unwind(struct sk_buff *skb, __be16 protocol,\n\t\t\t\t\tint pulled_hlen, u16 mac_offset,\n\t\t\t\t\tint mac_len)\n{\n\tskb->protocol = protocol;\n\tskb->encapsulation = 1;\n\tskb_push(skb, pulled_hlen);\n\tskb_reset_transport_header(skb);\n\tskb->mac_header = mac_offset;\n\tskb->network_header = skb->mac_header + mac_len;\n\tskb->mac_len = mac_len;\n}\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_ipvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_SLAVE;\n}\n\nstatic inline bool netif_is_ipvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_MASTER;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_ovs_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OVS_DATAPATH;\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\n/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */\nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\n/* return true if dev can't cope with mtu frames that need vlan tag insertion */\nstatic inline bool netif_reduces_vlan_mtu(struct net_device *dev)\n{\n\t/* TODO: reserve and use an additional IFF bit, if we get more users */\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline bool netdev_unregistering(const struct net_device *dev)\n{\n\treturn dev->reg_state == NETREG_UNREGISTERING;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n__printf(3, 4)\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...);\n__printf(2, 3)\nvoid netdev_emerg(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_alert(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_crit(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_err(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_warn(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_notice(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_info(const struct net_device *dev, const char *format, ...);\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_netdev_dbg(__dev, format, ##args);\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s\\n\" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_netdev_dbg(netdev, format, ##args);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n/* if @cond then downgrade to debug, else print at @level */\n#define netif_cond_dbg(priv, type, netdev, cond, level, fmt, args...)     \\\n\tdo {                                                              \\\n\t\tif (cond)                                                 \\\n\t\t\tnetif_dbg(priv, type, netdev, fmt, ##args);       \\\n\t\telse                                                      \\\n\t\t\tnetif_ ## level(priv, type, netdev, fmt, ##args); \\\n\t} while (0)\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n *      NET3    Protocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic seqcount_t devnet_rename_seq;\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n *\n *\t\t      Device Boot-time Settings Routines\n *\n ******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n * netdev_boot_setup_check\t- check boot time settings\n * @dev: the netdevice\n *\n * Check boot time settings for the device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq = s[i].map.irq;\n\t\t\tdev->base_addr = s[i].map.base_addr;\n\t\t\tdev->mem_start = s[i].map.mem_start;\n\t\t\tdev->mem_end = s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n * netdev_boot_base\t- get address from boot time settings\n * @prefix: prefix for network device\n * @unit: id for network device\n *\n * Check boot time settings for the base address of device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n *\n *\tThe use of raw_seqcount_begin() and cond_resched() before\n *\tretrying is required as we want to give the writers a chance\n *\tto complete when CONFIG_PREEMPT is not set.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tunsigned int seq;\n\nretry:\n\tseq = raw_seqcount_begin(&devnet_rename_seq);\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(name, dev->name);\n\trcu_read_unlock();\n\tif (read_seqcount_retry(&devnet_rename_seq, seq)) {\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\nstatic int dev_get_valid_name(struct net *net,\n\t\t\t      struct net_device *dev,\n\t\t\t      const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\twrite_seqcount_begin(&devnet_rename_seq);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn ret;\n\t}\n\n\twrite_seqcount_end(&devnet_rename_seq);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del_rcu(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\twrite_seqcount_begin(&devnet_rename_seq);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tchar *new_ifalias;\n\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tkfree(dev->ifalias);\n\t\tdev->ifalias = NULL;\n\t\treturn 0;\n\t}\n\n\tnew_ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!new_ifalias)\n\t\treturn -ENOMEM;\n\tdev->ifalias = new_ifalias;\n\tmemcpy(dev->ifalias, alias, len);\n\tdev->ifalias[len] = 0;\n\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = 0;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\tnetdev_notifier_info_init(&info, dev);\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tgoto outroll;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\n\noutroll:\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\nunlock:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tASSERT_RTNL();\n\tnetdev_notifier_info_init(info, dev);\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\treturn call_netdevice_notifiers_info(val, dev, &info);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n#ifdef CONFIG_NET_INGRESS\nstatic struct static_key ingress_needed __read_mostly;\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_key_slow_inc(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_key_slow_dec(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct static_key egress_needed __read_mostly;\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_key_slow_inc(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_key_slow_dec(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic struct static_key netstamp_needed __read_mostly;\n#ifdef HAVE_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_key_enable(&netstamp_needed);\n\telse\n\t\tstatic_key_disable(&netstamp_needed);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 0)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted + 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_key_slow_inc(&netstamp_needed);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 1)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted - 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_key_slow_dec(&netstamp_needed);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tif (static_key_false(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\\\n\tif (static_key_false(&netstamp_needed)) {\t\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\\\n\t\t\t__net_timestamp(SKB);\t\t\\\n\t}\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tlen = dev->mtu + dev->hard_header_len + VLAN_HLEN;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_XPS\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tRCU_INIT_POINTER(dev_maps->cpu_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev->num_tc ? : 1;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, cpu, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tint cpu, i;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\tfor_each_possible_cpu(cpu)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, cpu,\n\t\t\t\t\t       offset, count);\n\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tfor (i = offset + (count - 1); count--; i--)\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, i),\n\t\t\t\t\t     NUMA_NO_NODE);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map,\n\t\t\t\t      int cpu, u16 index)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add queue to this CPU's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store queue on this CPU's map */\n\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t       cpu_to_node(cpu));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL;\n\tint i, cpu, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tbool active = false;\n\n\tif (dev->num_tc) {\n\t\tnum_tc = dev->num_tc;\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmaps_sz = XPS_DEV_MAPS_SIZE(num_tc);\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\t/* allocate memory for queue storage */\n\tfor_each_cpu_and(cpu, cpu_online_mask, mask) {\n\t\tif (!new_dev_maps)\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\tif (!new_dev_maps) {\n\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttci = cpu * num_tc + tc;\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[tci]) :\n\t\t\t\t NULL;\n\n\t\tmap = expand_xps_map(map, cpu, index);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = tc, tci = cpu * num_tc; dev_maps && i--; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\n\t\t/* We need to explicitly update tci as prevous loop\n\t\t * could break out early if dev_maps is NULL.\n\t\t */\n\t\ttci = cpu * num_tc + tc;\n\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu)) {\n\t\t\t/* add queue to CPU maps */\n\t\t\tint pos = 0;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (numa_node_id == -2)\n\t\t\t\tnuma_node_id = cpu_to_node(cpu);\n\t\t\telse if (numa_node_id != cpu_to_node(cpu))\n\t\t\t\tnuma_node_id = -1;\n#endif\n\t\t} else if (dev_maps) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = num_tc - tc, tci++; dev_maps && --i; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\t}\n\n\trcu_assign_pointer(dev->xps_maps, new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = num_tc, tci = cpu * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tif (map && map != new_map)\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\tkfree_rcu(dev_maps, rcu);\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\t/* update Tx queue numa node */\n\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t     (numa_node_id >= 0) ? numa_node_id :\n\t\t\t\t     NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes queue from unused CPUs */\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = tc, tci = cpu * num_tc; i--; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tif (!cpumask_test_cpu(cpu, mask) || !cpu_online(cpu))\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tfor (i = num_tc - tc, tci++; --i; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t}\n\n\t/* free map if not active */\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = num_tc, tci = cpu * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\tmap = dev_maps ?\n\t\t\t      xmap_dereference(dev_maps->cpu_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tif (txq < dev->real_num_tx_queues) {\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn is_kdump_kernel() ?\n\t\t1 : min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!(txq->state & QUEUE_STATE_ANY_XOFF)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = num_tx_queues;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = dev->tc_to_txq[tc].offset;\n\t\tqcount = dev->tc_to_txq[tc].count;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tWARN(1, \"%s: caps=(%pNF, %pNF) len=%d data_len=%d gso_size=%d \"\n\t     \"gso_type=%d ip_summed=%d\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features,\n\t     skb->len, skb->data_len, skb_shinfo(skb)->gso_size,\n\t     skb_shinfo(skb)->gso_type, skb->ip_summed);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__le32))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_not_inet = 0;\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb_mac_header(skb);\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL;\n\n\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_SGO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tstruct sk_buff *segs;\n\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\t/* We're going to init ->check field in TCP or UDP header */\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\t/* Only report GSO partial support if it will enable us to\n\t * support segmentation on this frame without needing additional\n\t * work.\n\t */\n\tif (features & NETIF_F_GSO_PARTIAL) {\n\t\tnetdev_features_t partial_features = NETIF_F_GSO_ROBUST;\n\t\tstruct net_device *dev = skb->dev;\n\n\t\tpartial_features |= dev->features & dev->gso_partial_features;\n\t\tif (!skb_gso_ok(skb, features | partial_features))\n\t\t\tfeatures &= ~NETIF_F_GSO_PARTIAL;\n\t}\n\n\tBUILD_BUG_ON(SKB_SGO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tsegs = skb_mac_gso_segment(skb, features);\n\n\tif (unlikely(skb_needs_check(skb, tx_path)))\n\t\tskb_warn_bad_offload(skb);\n\n\treturn segs;\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tdma_addr_t addr = page_to_phys(skb_frag_page(frag));\n\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tint tmp;\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, &tmp);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(const struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs)\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (!list_empty(&ptype_all) || !list_empty(&dev->ptype_all))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb->next = NULL;\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_xmit_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb->csum_not_inet))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\treturn !!(features & NETIF_F_CSUM_MASK) ? 0 : skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\tif (validate_xmit_xfrm(skb, features))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tatomic_long_inc(&dev->tx_dropped);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size)  {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\thdr_len += tcp_hdrlen(skb);\n\t\telse\n\t\t\thdr_len += sizeof(struct udphdr);\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list(to_free);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tstruct netprio_map *map = rcu_dereference_bh(skb->dev->priomap);\n\n\tif (!skb->priority && skb->sk && map) {\n\t\tunsigned int prioidx =\n\t\t\tsock_cgroup_prioidx(&skb->sk->sk_cgrp_data);\n\n\t\tif (prioidx < map->priomap_len)\n\t\t\tskb->priority = map->priomap[prioidx];\n\t}\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\nDEFINE_PER_CPU(int, xmit_recursion);\nEXPORT_SYMBOL(xmit_recursion);\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct tcf_proto *cl = rcu_dereference_bh(dev->egress_cl_list);\n\tstruct tcf_result cl_res;\n\n\tif (!cl)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len was already set by the caller. */\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tcf_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\t*ret = NET_XMIT_DROP;\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\tif (dev->num_tc) {\n\t\t\ttci *= dev->num_tc;\n\t\t\ttci += netdev_get_prio_tc_map(dev, skb->priority);\n\t\t}\n\n\t\tmap = rcu_dereference(dev_maps->cpu_map[tci]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse\n\t\t\t\tqueue_index = map->queues[reciprocal_scale(skb_get_hash(skb),\n\t\t\t\t\t\t\t\t\t   map->len)];\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, accel_priv,\n\t\t\t\t\t\t\t    __netdev_pick_tx);\n\t\telse\n\t\t\tqueue_index = __netdev_pick_tx(dev, skb);\n\n\t\tif (!accel_priv)\n\t\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@accel_priv: private data used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_at_ingress = 0;\n# ifdef CONFIG_NET_EGRESS\n\tif (static_key_false(&egress_needed)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\ttxq = netdev_pick_tx(dev, skb, accel_priv);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\t\t\tif (unlikely(__this_cpu_read(xmit_recursion) >\n\t\t\t\t     XMIT_RECURSION_LIMIT))\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv)\n{\n\treturn __dev_queue_xmit(skb, accel_priv);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nunsigned int __read_mostly netdev_budget_usecs = 2000;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = ACCESS_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tstruct xdp_buff xdp;\n\tu32 act = XDP_DROP;\n\tvoid *orig_data;\n\tint hlen, off;\n\tu32 mac_len;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_cloned(skb))\n\t\treturn XDP_PASS;\n\n\tif (skb_linearize(skb))\n\t\tgoto do_drop;\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thlen = skb_headlen(skb) + mac_len;\n\txdp.data = skb->data - mac_len;\n\txdp.data_end = xdp.data + hlen;\n\txdp.data_hard_start = skb->data - skb_headroom(skb);\n\torig_data = xdp.data;\n\n\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\toff = xdp.data - orig_data;\n\tif (off > 0)\n\t\t__skb_pull(skb, off);\n\telse if (off < 0)\n\t\t__skb_push(skb, -off);\n\tskb->mac_header += off;\n\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\t/* fall through */\n\tcase XDP_PASS:\n\t\tbreak;\n\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(act);\n\t\t/* fall through */\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\t/* fall through */\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tkfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL_GPL(generic_xdp_tx);\n\nstatic struct static_key generic_xdp_needed __read_mostly;\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tu32 act = netif_receive_generic_xdp(skb, xdp_prog);\n\t\tint err;\n\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t/* fallthru to submit skb */\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb(skb);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n\n\tif (static_key_false(&generic_xdp_needed)) {\n\t\tint ret;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\t\tret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\n\t\t/* Consider XDP consuming the packet a success from\n\t\t * the netdev point of view we do not want to count\n\t\t * this as an error.\n\t\t */\n\t\tif (ret != XDP_PASS)\n\t\t\treturn NET_RX_SUCCESS;\n\t}\n\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\ttrace_netif_rx_entry(skb);\n\n\treturn netif_rx_internal(skb);\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\n\t\t__kfree_skb_flush();\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tspin_lock(root_lock);\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tspin_unlock(root_lock);\n\t\t}\n\t}\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto *cl = rcu_dereference_bh(skb->dev->ingress_cl_list);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!cl)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tskb->tc_at_ingress = 1;\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tcf_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tskb_do_redirect(skb);\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n#ifdef CONFIG_NETFILTER_INGRESS\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n#endif /* CONFIG_NETFILTER_INGRESS */\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_key_false(&ingress_needed)) {\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_tc(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tif (skb_vlan_tag_get_id(skb))\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\tskb->vlan_tci = 0;\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\telse\n\t\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_core(skb, false);\n\n\treturn ret;\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_xdp *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_key_slow_dec(&generic_xdp_needed);\n\t\t} else if (new && !old) {\n\t\t\tstatic_key_slow_inc(&generic_xdp_needed);\n\t\t\tdev_disable_lro(dev);\n\t\t}\n\t\tbreak;\n\n\tcase XDP_QUERY_PROG:\n\t\txdp->prog_attached = !!old;\n\t\txdp->prog_id = old ? old->aux->id : 0;\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\tif (static_key_false(&generic_xdp_needed)) {\n\t\tint ret;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\t\tret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\n\t\tif (ret != XDP_PASS)\n\t\t\treturn NET_RX_DROP;\n\t}\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\ttrace_netif_receive_skb_entry(skb);\n\n\treturn netif_receive_skb_internal(skb);\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\nDEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tunsigned int cpu;\n\n\tget_online_cpus();\n\n\tfor_each_online_cpu(cpu)\n\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\n\tfor_each_online_cpu(cpu)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tput_online_cpus();\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->callbacks.gro_complete(skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb_internal(skb);\n}\n\n/* napi->gro_list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tstruct sk_buff *skb, *prev = NULL;\n\n\t/* scan list and build reverse chain */\n\tfor (skb = napi->gro_list; skb != NULL; skb = skb->next) {\n\t\tskb->prev = prev;\n\t\tprev = skb;\n\t}\n\n\tfor (skb = prev; skb; skb = prev) {\n\t\tskb->next = NULL;\n\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\n\t\tprev = skb->prev;\n\t\tnapi_gro_complete(skb);\n\t\tnapi->gro_count--;\n\t}\n\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic void gro_list_prepare(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb_mac_header(skb) == skb_tail_pointer(skb) &&\n\t    pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = min_t(unsigned int,\n\t\t\t\t\t\t    skb_frag_size(frag0),\n\t\t\t\t\t\t    skb->end - skb->tail);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tpinfo->frags[0].page_offset += grow;\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (netif_elide_gro(skb->dev))\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = skb_is_gso(skb) || skb_has_frag_list(skb);\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->recursion_counter = 0;\n\t\tNAPI_GRO_CB(skb)->is_fou = 0;\n\t\tNAPI_GRO_CB(skb)->is_atomic = 1;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tif (IS_ERR(pp) && PTR_ERR(pp) == -EINPROGRESS) {\n\t\tret = GRO_CONSUMED;\n\t\tgoto ok;\n\t}\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic void napi_skb_free_stolen_head(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tsecpath_reset(skb);\n\tkmem_cache_free(skbuff_head_cache, skb);\n}\n\nstatic gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\t__kfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(dev_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\tsecpath_reset(skb);\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL && netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\teth = skb_gro_header_fast(skb, 0);\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\treturn napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = dev_rx_weight;\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock(sd);\n\t\tlocal_irq_enable();\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (n->gro_list) {\n\t\tunsigned long timeout = 0;\n\n\t\tif (work_done)\n\t\t\ttimeout = n->dev->gro_flush_timeout;\n\n\t\tif (timeout)\n\t\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t\t      HRTIMER_MODE_REL_PINNED);\n\t\telse\n\t\t\tnapi_gro_flush(n, false);\n\t}\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\n#define BUSY_POLL_BUDGET 8\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock)\n{\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, BUSY_POLL_BUDGET);\n\ttrace_napi_poll(napi, rc, BUSY_POLL_BUDGET);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == BUSY_POLL_BUDGET)\n\t\t__napi_schedule(napi);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL))\n\t\t\t\tgoto count;\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val)\n\t\t\t\tgoto count;\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, BUSY_POLL_BUDGET);\n\t\ttrace_napi_poll(napi, work, BUSY_POLL_BUDGET);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock);\n\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock);\n\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state) ||\n\t    test_and_set_bit(NAPI_STATE_HASHED, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nbool napi_hash_del(struct napi_struct *napi)\n{\n\tbool rcu_sync_needed = false;\n\n\tspin_lock(&napi_hash_lock);\n\n\tif (test_and_clear_bit(NAPI_STATE_HASHED, &napi->state)) {\n\t\trcu_sync_needed = true;\n\t\thlist_del_rcu(&napi->napi_hash_node);\n\t}\n\tspin_unlock(&napi_hash_lock);\n\treturn rcu_sync_needed;\n}\nEXPORT_SYMBOL_GPL(napi_hash_del);\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (napi->gro_list && !napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state))\n\t\t__napi_schedule_irqoff(napi);\n\n\treturn HRTIMER_NORESTART;\n}\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tpr_err_once(\"netif_napi_add() called with weight %d on device %s\\n\",\n\t\t\t    weight, dev->name);\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tnapi_hash_add(napi);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/* Must be called in process context */\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tmight_sleep();\n\tif (napi_hash_del(napi))\n\t\tsynchronize_net();\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tkfree_skb_list(napi->gro_list);\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tvoid *have;\n\tint work, weight;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tWARN_ON_ONCE(work > weight);\n\n\tif (likely(work < weight))\n\t\tgoto out_unlock;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\tgoto out_unlock;\n\t}\n\n\tif (n->gro_list) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\tgoto out_unlock;\n\t}\n\n\tlist_add_tail(&n->poll_list, repoll);\n\nout_unlock:\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(netdev_budget_usecs);\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\nout:\n\t__kfree_skb_flush();\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int __netdev_has_upper_dev(struct net_device *upper_dev, void *data)\n{\n\tstruct net_device *dev = data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, __netdev_has_upper_dev,\n\t\t\t\t\t     upper_dev);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, __netdev_has_upper_dev,\n\t\t\t\t\t       upper_dev);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = netdev_next_upper_dev_rcu(dev, &iter);\n\t     udev;\n\t     udev = netdev_next_upper_dev_rcu(dev, &iter)) {\n\t\t/* first is the upper device itself */\n\t\tret = fn(udev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its upper devices */\n\t\tret = netdev_walk_all_upper_dev_rcu(udev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tvoid *data),\n\t\t\t      void *data)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = netdev_next_lower_dev(dev, &iter);\n\t     ldev;\n\t     ldev = netdev_next_lower_dev(dev, &iter)) {\n\t\t/* first is the lower device itself */\n\t\tret = fn(ldev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its lower devices */\n\t\tret = netdev_walk_all_lower_dev(ldev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic struct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = netdev_next_lower_dev_rcu(dev, &iter);\n\t     ldev;\n\t     ldev = netdev_next_lower_dev_rcu(dev, &iter)) {\n\t\t/* first is the lower device itself */\n\t\tret = fn(ldev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its lower devices */\n\t\tret = netdev_walk_all_lower_dev_rcu(ldev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif (netdev_has_upper_dev(dev, upper_dev))\n\t\treturn -EEXIST;\n\n\tif (master && netdev_master_upper_dev_get(dev))\n\t\treturn -EBUSY;\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = master;\n\tchangeupper_info.linking = true;\n\tchangeupper_info.upper_info = upper_info;\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, false, NULL, NULL);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\tchangeupper_info.linking = false;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info\tinfo;\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO, dev,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\nint dev_get_nest_level(struct net_device *dev)\n{\n\tstruct net_device *lower = NULL;\n\tstruct list_head *iter;\n\tint max_nest = -1;\n\tint nest;\n\n\tASSERT_RTNL();\n\n\tnetdev_for_each_lower_dev(dev, lower, iter) {\n\t\tnest = dev_get_nest_level(lower);\n\t\tif (max_nest < nest)\n\t\t\tmax_nest = nest;\n\t}\n\n\treturn max_nest + 1;\n}\nEXPORT_SYMBOL(dev_get_nest_level);\n\n/**\n * netdev_lower_change - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info;\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE, lower_dev,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\tfrom_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\tfrom_kuid(&init_user_ns, uid),\n\t\t\t\tfrom_kgid(&init_user_ns, gid),\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = changes;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tnet_err_ratelimited(\"%s: Invalid MTU %d requested, hw min %d\\n\",\n\t\t\t\t    dev->name, new_mtu, dev->min_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tnet_err_ratelimited(\"%s: Invalid MTU %d requested, hw max %d\\n\",\n\t\t\t\t    dev->name, new_mtu, dev->max_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\t}\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_name)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_name(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\nu8 __dev_xdp_attached(struct net_device *dev, xdp_op_t xdp_op, u32 *prog_id)\n{\n\tstruct netdev_xdp xdp;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = XDP_QUERY_PROG;\n\n\t/* Query must always succeed. */\n\tWARN_ON(xdp_op(dev, &xdp) < 0);\n\tif (prog_id)\n\t\t*prog_id = xdp.prog_id;\n\n\treturn xdp.prog_attached;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, xdp_op_t xdp_op,\n\t\t\t   struct netlink_ext_ack *extack, u32 flags,\n\t\t\t   struct bpf_prog *prog)\n{\n\tstruct netdev_xdp xdp;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\txdp.command = XDP_SETUP_PROG_HW;\n\telse\n\t\txdp.command = XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\treturn xdp_op(dev, &xdp);\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, u32 flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct bpf_prog *prog = NULL;\n\txdp_op_t xdp_op, xdp_chk;\n\tint err;\n\n\tASSERT_RTNL();\n\n\txdp_op = xdp_chk = ops->ndo_xdp;\n\tif (!xdp_op && (flags & (XDP_FLAGS_DRV_MODE | XDP_FLAGS_HW_MODE)))\n\t\treturn -EOPNOTSUPP;\n\tif (!xdp_op || (flags & XDP_FLAGS_SKB_MODE))\n\t\txdp_op = generic_xdp_install;\n\tif (xdp_op == xdp_chk)\n\t\txdp_chk = generic_xdp_install;\n\n\tif (fd >= 0) {\n\t\tif (xdp_chk && __dev_xdp_attached(dev, xdp_chk, NULL))\n\t\t\treturn -EEXIST;\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) &&\n\t\t    __dev_xdp_attached(dev, xdp_op, NULL))\n\t\t\treturn -EBUSY;\n\n\t\tprog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);\n\t\tif (IS_ERR(prog))\n\t\t\treturn PTR_ERR(prog);\n\t}\n\n\terr = dev_xdp_install(dev, xdp_op, extack, flags, prog);\n\tif (err < 0 && prog)\n\t\tbpf_prog_put(prog);\n\n\treturn err;\n}\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\tnetdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off an an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_SYSFS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= NETIF_F_SOFT_FEATURES;\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->netdev_ops->ndo_udp_tunnel_add) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\trtnl_lock();\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t__rtnl_unlock();\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*netdev_stats));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += (unsigned long)atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += (unsigned long)atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += (unsigned long)atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYSFS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_SYSFS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\tstruct bpf_prog *prog;\n\n\tmight_sleep();\n\tnetif_free_tx_queues(dev);\n#ifdef CONFIG_SYSFS\n\tkvfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\tprog = rcu_dereference_protected(dev->xdp_prog, 1);\n\tif (prog) {\n\t\tbpf_prog_put(prog);\n\t\tstatic_key_slow_dec(&generic_xdp_needed);\n\t}\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t\tlist_del(head);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(net, dev, pat) < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U, GFP_KERNEL);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tdev->ifindex = dev_new_index(net);\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"], "fixing_code": ["/*\n *  TUN - Universal TUN/TAP device driver.\n *  Copyright (C) 1999-2002 Maxim Krasnyansky <maxk@qualcomm.com>\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n *  GNU General Public License for more details.\n *\n *  $Id: tun.c,v 1.15 2002/03/01 02:44:24 maxk Exp $\n */\n\n/*\n *  Changes:\n *\n *  Mike Kershaw <dragorn@kismetwireless.net> 2005/08/14\n *    Add TUNSETLINK ioctl to set the link encapsulation\n *\n *  Mark Smith <markzzzsmith@yahoo.com.au>\n *    Use eth_random_addr() for tap MAC address.\n *\n *  Harald Roelle <harald.roelle@ifi.lmu.de>  2004/04/20\n *    Fixes in packet dropping, queue length setting and queue wakeup.\n *    Increased default tx queue length.\n *    Added ethtool API.\n *    Minor cleanups\n *\n *  Daniel Podlejski <underley@underley.eu.org>\n *    Modifications for 2.3.99-pre5 kernel.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#define DRV_NAME\t\"tun\"\n#define DRV_VERSION\t\"1.6\"\n#define DRV_DESCRIPTION\t\"Universal TUN/TAP device driver\"\n#define DRV_COPYRIGHT\t\"(C) 1999-2004 Max Krasnyansky <maxk@qualcomm.com>\"\n\n#include <linux/module.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/major.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/miscdevice.h>\n#include <linux/ethtool.h>\n#include <linux/rtnetlink.h>\n#include <linux/compat.h>\n#include <linux/if.h>\n#include <linux/if_arp.h>\n#include <linux/if_ether.h>\n#include <linux/if_tun.h>\n#include <linux/if_vlan.h>\n#include <linux/crc32.h>\n#include <linux/nsproxy.h>\n#include <linux/virtio_net.h>\n#include <linux/rcupdate.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n#include <net/rtnetlink.h>\n#include <net/sock.h>\n#include <linux/seq_file.h>\n#include <linux/uio.h>\n#include <linux/skb_array.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n\n#include <linux/uaccess.h>\n\n/* Uncomment to enable debugging */\n/* #define TUN_DEBUG 1 */\n\n#ifdef TUN_DEBUG\nstatic int debug;\n\n#define tun_debug(level, tun, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (tun->debug)\t\t\t\t\t\t\\\n\t\tnetdev_printk(level, tun->dev, fmt, ##args);\t\\\n} while (0)\n#define DBG1(level, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (debug == 2)\t\t\t\t\t\t\\\n\t\tprintk(level fmt, ##args);\t\t\t\\\n} while (0)\n#else\n#define tun_debug(level, tun, fmt, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(level, tun->dev, fmt, ##args);\t\\\n} while (0)\n#define DBG1(level, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tprintk(level fmt, ##args);\t\t\t\\\n} while (0)\n#endif\n\n#define TUN_HEADROOM 256\n#define TUN_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)\n\n/* TUN device flags */\n\n/* IFF_ATTACH_QUEUE is never stored in device flags,\n * overload it to mean fasync when stored there.\n */\n#define TUN_FASYNC\tIFF_ATTACH_QUEUE\n/* High bits in flags field are unused. */\n#define TUN_VNET_LE     0x80000000\n#define TUN_VNET_BE     0x40000000\n\n#define TUN_FEATURES (IFF_NO_PI | IFF_ONE_QUEUE | IFF_VNET_HDR | \\\n\t\t      IFF_MULTI_QUEUE)\n#define GOODCOPY_LEN 128\n\n#define FLT_EXACT_COUNT 8\nstruct tap_filter {\n\tunsigned int    count;    /* Number of addrs. Zero means disabled */\n\tu32             mask[2];  /* Mask of the hashed addrs */\n\tunsigned char\taddr[FLT_EXACT_COUNT][ETH_ALEN];\n};\n\n/* MAX_TAP_QUEUES 256 is chosen to allow rx/tx queues to be equal\n * to max number of VCPUs in guest. */\n#define MAX_TAP_QUEUES 256\n#define MAX_TAP_FLOWS  4096\n\n#define TUN_FLOW_EXPIRE (3 * HZ)\n\nstruct tun_pcpu_stats {\n\tu64 rx_packets;\n\tu64 rx_bytes;\n\tu64 tx_packets;\n\tu64 tx_bytes;\n\tstruct u64_stats_sync syncp;\n\tu32 rx_dropped;\n\tu32 tx_dropped;\n\tu32 rx_frame_errors;\n};\n\n/* A tun_file connects an open character device to a tuntap netdevice. It\n * also contains all socket related structures (except sock_fprog and tap_filter)\n * to serve as one transmit queue for tuntap device. The sock_fprog and\n * tap_filter were kept in tun_struct since they were used for filtering for the\n * netdevice not for a specific queue (at least I didn't see the requirement for\n * this).\n *\n * RCU usage:\n * The tun_file and tun_struct are loosely coupled, the pointer from one to the\n * other can only be read while rcu_read_lock or rtnl_lock is held.\n */\nstruct tun_file {\n\tstruct sock sk;\n\tstruct socket socket;\n\tstruct socket_wq wq;\n\tstruct tun_struct __rcu *tun;\n\tstruct fasync_struct *fasync;\n\t/* only used for fasnyc */\n\tunsigned int flags;\n\tunion {\n\t\tu16 queue_index;\n\t\tunsigned int ifindex;\n\t};\n\tstruct list_head next;\n\tstruct tun_struct *detached;\n\tstruct skb_array tx_array;\n};\n\nstruct tun_flow_entry {\n\tstruct hlist_node hash_link;\n\tstruct rcu_head rcu;\n\tstruct tun_struct *tun;\n\n\tu32 rxhash;\n\tu32 rps_rxhash;\n\tint queue_index;\n\tunsigned long updated;\n};\n\n#define TUN_NUM_FLOW_ENTRIES 1024\n\n/* Since the socket were moved to tun_file, to preserve the behavior of persist\n * device, socket filter, sndbuf and vnet header size were restore when the\n * file were attached to a persist device.\n */\nstruct tun_struct {\n\tstruct tun_file __rcu\t*tfiles[MAX_TAP_QUEUES];\n\tunsigned int            numqueues;\n\tunsigned int \t\tflags;\n\tkuid_t\t\t\towner;\n\tkgid_t\t\t\tgroup;\n\n\tstruct net_device\t*dev;\n\tnetdev_features_t\tset_features;\n#define TUN_USER_FEATURES (NETIF_F_HW_CSUM|NETIF_F_TSO_ECN|NETIF_F_TSO| \\\n\t\t\t  NETIF_F_TSO6)\n\n\tint\t\t\talign;\n\tint\t\t\tvnet_hdr_sz;\n\tint\t\t\tsndbuf;\n\tstruct tap_filter\ttxflt;\n\tstruct sock_fprog\tfprog;\n\t/* protected by rtnl lock */\n\tbool\t\t\tfilter_attached;\n#ifdef TUN_DEBUG\n\tint debug;\n#endif\n\tspinlock_t lock;\n\tstruct hlist_head flows[TUN_NUM_FLOW_ENTRIES];\n\tstruct timer_list flow_gc_timer;\n\tunsigned long ageing_time;\n\tunsigned int numdisabled;\n\tstruct list_head disabled;\n\tvoid *security;\n\tu32 flow_count;\n\tu32 rx_batched;\n\tstruct tun_pcpu_stats __percpu *pcpu_stats;\n\tstruct bpf_prog __rcu *xdp_prog;\n};\n\n#ifdef CONFIG_TUN_VNET_CROSS_LE\nstatic inline bool tun_legacy_is_little_endian(struct tun_struct *tun)\n{\n\treturn tun->flags & TUN_VNET_BE ? false :\n\t\tvirtio_legacy_is_little_endian();\n}\n\nstatic long tun_get_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\tint be = !!(tun->flags & TUN_VNET_BE);\n\n\tif (put_user(be, argp))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long tun_set_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\tint be;\n\n\tif (get_user(be, argp))\n\t\treturn -EFAULT;\n\n\tif (be)\n\t\ttun->flags |= TUN_VNET_BE;\n\telse\n\t\ttun->flags &= ~TUN_VNET_BE;\n\n\treturn 0;\n}\n#else\nstatic inline bool tun_legacy_is_little_endian(struct tun_struct *tun)\n{\n\treturn virtio_legacy_is_little_endian();\n}\n\nstatic long tun_get_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\treturn -EINVAL;\n}\n\nstatic long tun_set_vnet_be(struct tun_struct *tun, int __user *argp)\n{\n\treturn -EINVAL;\n}\n#endif /* CONFIG_TUN_VNET_CROSS_LE */\n\nstatic inline bool tun_is_little_endian(struct tun_struct *tun)\n{\n\treturn tun->flags & TUN_VNET_LE ||\n\t\ttun_legacy_is_little_endian(tun);\n}\n\nstatic inline u16 tun16_to_cpu(struct tun_struct *tun, __virtio16 val)\n{\n\treturn __virtio16_to_cpu(tun_is_little_endian(tun), val);\n}\n\nstatic inline __virtio16 cpu_to_tun16(struct tun_struct *tun, u16 val)\n{\n\treturn __cpu_to_virtio16(tun_is_little_endian(tun), val);\n}\n\nstatic inline u32 tun_hashfn(u32 rxhash)\n{\n\treturn rxhash & 0x3ff;\n}\n\nstatic struct tun_flow_entry *tun_flow_find(struct hlist_head *head, u32 rxhash)\n{\n\tstruct tun_flow_entry *e;\n\n\thlist_for_each_entry_rcu(e, head, hash_link) {\n\t\tif (e->rxhash == rxhash)\n\t\t\treturn e;\n\t}\n\treturn NULL;\n}\n\nstatic struct tun_flow_entry *tun_flow_create(struct tun_struct *tun,\n\t\t\t\t\t      struct hlist_head *head,\n\t\t\t\t\t      u32 rxhash, u16 queue_index)\n{\n\tstruct tun_flow_entry *e = kmalloc(sizeof(*e), GFP_ATOMIC);\n\n\tif (e) {\n\t\ttun_debug(KERN_INFO, tun, \"create flow: hash %u index %u\\n\",\n\t\t\t  rxhash, queue_index);\n\t\te->updated = jiffies;\n\t\te->rxhash = rxhash;\n\t\te->rps_rxhash = 0;\n\t\te->queue_index = queue_index;\n\t\te->tun = tun;\n\t\thlist_add_head_rcu(&e->hash_link, head);\n\t\t++tun->flow_count;\n\t}\n\treturn e;\n}\n\nstatic void tun_flow_delete(struct tun_struct *tun, struct tun_flow_entry *e)\n{\n\ttun_debug(KERN_INFO, tun, \"delete flow: hash %u index %u\\n\",\n\t\t  e->rxhash, e->queue_index);\n\thlist_del_rcu(&e->hash_link);\n\tkfree_rcu(e, rcu);\n\t--tun->flow_count;\n}\n\nstatic void tun_flow_flush(struct tun_struct *tun)\n{\n\tint i;\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link)\n\t\t\ttun_flow_delete(tun, e);\n\t}\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_delete_by_queue(struct tun_struct *tun, u16 queue_index)\n{\n\tint i;\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link) {\n\t\t\tif (e->queue_index == queue_index)\n\t\t\t\ttun_flow_delete(tun, e);\n\t\t}\n\t}\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_cleanup(unsigned long data)\n{\n\tstruct tun_struct *tun = (struct tun_struct *)data;\n\tunsigned long delay = tun->ageing_time;\n\tunsigned long next_timer = jiffies + delay;\n\tunsigned long count = 0;\n\tint i;\n\n\ttun_debug(KERN_INFO, tun, \"tun_flow_cleanup\\n\");\n\n\tspin_lock_bh(&tun->lock);\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++) {\n\t\tstruct tun_flow_entry *e;\n\t\tstruct hlist_node *n;\n\n\t\thlist_for_each_entry_safe(e, n, &tun->flows[i], hash_link) {\n\t\t\tunsigned long this_timer;\n\t\t\tcount++;\n\t\t\tthis_timer = e->updated + delay;\n\t\t\tif (time_before_eq(this_timer, jiffies))\n\t\t\t\ttun_flow_delete(tun, e);\n\t\t\telse if (time_before(this_timer, next_timer))\n\t\t\t\tnext_timer = this_timer;\n\t\t}\n\t}\n\n\tif (count)\n\t\tmod_timer(&tun->flow_gc_timer, round_jiffies_up(next_timer));\n\tspin_unlock_bh(&tun->lock);\n}\n\nstatic void tun_flow_update(struct tun_struct *tun, u32 rxhash,\n\t\t\t    struct tun_file *tfile)\n{\n\tstruct hlist_head *head;\n\tstruct tun_flow_entry *e;\n\tunsigned long delay = tun->ageing_time;\n\tu16 queue_index = tfile->queue_index;\n\n\tif (!rxhash)\n\t\treturn;\n\telse\n\t\thead = &tun->flows[tun_hashfn(rxhash)];\n\n\trcu_read_lock();\n\n\t/* We may get a very small possibility of OOO during switching, not\n\t * worth to optimize.*/\n\tif (tun->numqueues == 1 || tfile->detached)\n\t\tgoto unlock;\n\n\te = tun_flow_find(head, rxhash);\n\tif (likely(e)) {\n\t\t/* TODO: keep queueing to old queue until it's empty? */\n\t\te->queue_index = queue_index;\n\t\te->updated = jiffies;\n\t\tsock_rps_record_flow_hash(e->rps_rxhash);\n\t} else {\n\t\tspin_lock_bh(&tun->lock);\n\t\tif (!tun_flow_find(head, rxhash) &&\n\t\t    tun->flow_count < MAX_TAP_FLOWS)\n\t\t\ttun_flow_create(tun, head, rxhash, queue_index);\n\n\t\tif (!timer_pending(&tun->flow_gc_timer))\n\t\t\tmod_timer(&tun->flow_gc_timer,\n\t\t\t\t  round_jiffies_up(jiffies + delay));\n\t\tspin_unlock_bh(&tun->lock);\n\t}\n\nunlock:\n\trcu_read_unlock();\n}\n\n/**\n * Save the hash received in the stack receive path and update the\n * flow_hash table accordingly.\n */\nstatic inline void tun_flow_save_rps_rxhash(struct tun_flow_entry *e, u32 hash)\n{\n\tif (unlikely(e->rps_rxhash != hash))\n\t\te->rps_rxhash = hash;\n}\n\n/* We try to identify a flow through its rxhash first. The reason that\n * we do not check rxq no. is because some cards(e.g 82599), chooses\n * the rxq based on the txq where the last packet of the flow comes. As\n * the userspace application move between processors, we may get a\n * different rxq no. here. If we could not get rxhash, then we would\n * hope the rxq no. may help here.\n */\nstatic u16 tun_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    void *accel_priv, select_queue_fallback_t fallback)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct tun_flow_entry *e;\n\tu32 txq = 0;\n\tu32 numqueues = 0;\n\n\trcu_read_lock();\n\tnumqueues = ACCESS_ONCE(tun->numqueues);\n\n\ttxq = __skb_get_hash_symmetric(skb);\n\tif (txq) {\n\t\te = tun_flow_find(&tun->flows[tun_hashfn(txq)], txq);\n\t\tif (e) {\n\t\t\ttun_flow_save_rps_rxhash(e, txq);\n\t\t\ttxq = e->queue_index;\n\t\t} else\n\t\t\t/* use multiply and shift instead of expensive divide */\n\t\t\ttxq = ((u64)txq * numqueues) >> 32;\n\t} else if (likely(skb_rx_queue_recorded(skb))) {\n\t\ttxq = skb_get_rx_queue(skb);\n\t\twhile (unlikely(txq >= numqueues))\n\t\t\ttxq -= numqueues;\n\t}\n\n\trcu_read_unlock();\n\treturn txq;\n}\n\nstatic inline bool tun_not_capable(struct tun_struct *tun)\n{\n\tconst struct cred *cred = current_cred();\n\tstruct net *net = dev_net(tun->dev);\n\n\treturn ((uid_valid(tun->owner) && !uid_eq(cred->euid, tun->owner)) ||\n\t\t  (gid_valid(tun->group) && !in_egroup_p(tun->group))) &&\n\t\t!ns_capable(net->user_ns, CAP_NET_ADMIN);\n}\n\nstatic void tun_set_real_num_queues(struct tun_struct *tun)\n{\n\tnetif_set_real_num_tx_queues(tun->dev, tun->numqueues);\n\tnetif_set_real_num_rx_queues(tun->dev, tun->numqueues);\n}\n\nstatic void tun_disable_queue(struct tun_struct *tun, struct tun_file *tfile)\n{\n\ttfile->detached = tun;\n\tlist_add_tail(&tfile->next, &tun->disabled);\n\t++tun->numdisabled;\n}\n\nstatic struct tun_struct *tun_enable_queue(struct tun_file *tfile)\n{\n\tstruct tun_struct *tun = tfile->detached;\n\n\ttfile->detached = NULL;\n\tlist_del_init(&tfile->next);\n\t--tun->numdisabled;\n\treturn tun;\n}\n\nstatic void tun_queue_purge(struct tun_file *tfile)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = skb_array_consume(&tfile->tx_array)) != NULL)\n\t\tkfree_skb(skb);\n\n\tskb_queue_purge(&tfile->sk.sk_write_queue);\n\tskb_queue_purge(&tfile->sk.sk_error_queue);\n}\n\nstatic void __tun_detach(struct tun_file *tfile, bool clean)\n{\n\tstruct tun_file *ntfile;\n\tstruct tun_struct *tun;\n\n\ttun = rtnl_dereference(tfile->tun);\n\n\tif (tun && !tfile->detached) {\n\t\tu16 index = tfile->queue_index;\n\t\tBUG_ON(index >= tun->numqueues);\n\n\t\trcu_assign_pointer(tun->tfiles[index],\n\t\t\t\t   tun->tfiles[tun->numqueues - 1]);\n\t\tntfile = rtnl_dereference(tun->tfiles[index]);\n\t\tntfile->queue_index = index;\n\n\t\t--tun->numqueues;\n\t\tif (clean) {\n\t\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t\t\tsock_put(&tfile->sk);\n\t\t} else\n\t\t\ttun_disable_queue(tun, tfile);\n\n\t\tsynchronize_net();\n\t\ttun_flow_delete_by_queue(tun, tun->numqueues + 1);\n\t\t/* Drop read queue */\n\t\ttun_queue_purge(tfile);\n\t\ttun_set_real_num_queues(tun);\n\t} else if (tfile->detached && clean) {\n\t\ttun = tun_enable_queue(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\n\tif (clean) {\n\t\tif (tun && tun->numqueues == 0 && tun->numdisabled == 0) {\n\t\t\tnetif_carrier_off(tun->dev);\n\n\t\t\tif (!(tun->flags & IFF_PERSIST) &&\n\t\t\t    tun->dev->reg_state == NETREG_REGISTERED)\n\t\t\t\tunregister_netdevice(tun->dev);\n\t\t}\n\t\tif (tun)\n\t\t\tskb_array_cleanup(&tfile->tx_array);\n\t\tsock_put(&tfile->sk);\n\t}\n}\n\nstatic void tun_detach(struct tun_file *tfile, bool clean)\n{\n\trtnl_lock();\n\t__tun_detach(tfile, clean);\n\trtnl_unlock();\n}\n\nstatic void tun_detach_all(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct bpf_prog *xdp_prog = rtnl_dereference(tun->xdp_prog);\n\tstruct tun_file *tfile, *tmp;\n\tint i, n = tun->numqueues;\n\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tBUG_ON(!tfile);\n\t\ttfile->socket.sk->sk_shutdown = RCV_SHUTDOWN;\n\t\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t\t--tun->numqueues;\n\t}\n\tlist_for_each_entry(tfile, &tun->disabled, next) {\n\t\ttfile->socket.sk->sk_shutdown = RCV_SHUTDOWN;\n\t\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\t\tRCU_INIT_POINTER(tfile->tun, NULL);\n\t}\n\tBUG_ON(tun->numqueues != 0);\n\n\tsynchronize_net();\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\t/* Drop read queue */\n\t\ttun_queue_purge(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\tlist_for_each_entry_safe(tfile, tmp, &tun->disabled, next) {\n\t\ttun_enable_queue(tfile);\n\t\ttun_queue_purge(tfile);\n\t\tsock_put(&tfile->sk);\n\t}\n\tBUG_ON(tun->numdisabled != 0);\n\n\tif (xdp_prog)\n\t\tbpf_prog_put(xdp_prog);\n\n\tif (tun->flags & IFF_PERSIST)\n\t\tmodule_put(THIS_MODULE);\n}\n\nstatic int tun_attach(struct tun_struct *tun, struct file *file, bool skip_filter)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev = tun->dev;\n\tint err;\n\n\terr = security_tun_dev_attach(tfile->socket.sk, tun->security);\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = -EINVAL;\n\tif (rtnl_dereference(tfile->tun) && !tfile->detached)\n\t\tgoto out;\n\n\terr = -EBUSY;\n\tif (!(tun->flags & IFF_MULTI_QUEUE) && tun->numqueues == 1)\n\t\tgoto out;\n\n\terr = -E2BIG;\n\tif (!tfile->detached &&\n\t    tun->numqueues + tun->numdisabled == MAX_TAP_QUEUES)\n\t\tgoto out;\n\n\terr = 0;\n\n\t/* Re-attach the filter to persist device */\n\tif (!skip_filter && (tun->filter_attached == true)) {\n\t\tlock_sock(tfile->socket.sk);\n\t\terr = sk_attach_filter(&tun->fprog, tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t\tif (!err)\n\t\t\tgoto out;\n\t}\n\n\tif (!tfile->detached &&\n\t    skb_array_init(&tfile->tx_array, dev->tx_queue_len, GFP_KERNEL)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\ttfile->queue_index = tun->numqueues;\n\ttfile->socket.sk->sk_shutdown &= ~RCV_SHUTDOWN;\n\trcu_assign_pointer(tfile->tun, tun);\n\trcu_assign_pointer(tun->tfiles[tun->numqueues], tfile);\n\ttun->numqueues++;\n\n\tif (tfile->detached)\n\t\ttun_enable_queue(tfile);\n\telse\n\t\tsock_hold(&tfile->sk);\n\n\ttun_set_real_num_queues(tun);\n\n\t/* device is allowed to go away first, so no need to hold extra\n\t * refcnt.\n\t */\n\nout:\n\treturn err;\n}\n\nstatic struct tun_struct *__tun_get(struct tun_file *tfile)\n{\n\tstruct tun_struct *tun;\n\n\trcu_read_lock();\n\ttun = rcu_dereference(tfile->tun);\n\tif (tun)\n\t\tdev_hold(tun->dev);\n\trcu_read_unlock();\n\n\treturn tun;\n}\n\nstatic struct tun_struct *tun_get(struct file *file)\n{\n\treturn __tun_get(file->private_data);\n}\n\nstatic void tun_put(struct tun_struct *tun)\n{\n\tdev_put(tun->dev);\n}\n\n/* TAP filtering */\nstatic void addr_hash_set(u32 *mask, const u8 *addr)\n{\n\tint n = ether_crc(ETH_ALEN, addr) >> 26;\n\tmask[n >> 5] |= (1 << (n & 31));\n}\n\nstatic unsigned int addr_hash_test(const u32 *mask, const u8 *addr)\n{\n\tint n = ether_crc(ETH_ALEN, addr) >> 26;\n\treturn mask[n >> 5] & (1 << (n & 31));\n}\n\nstatic int update_filter(struct tap_filter *filter, void __user *arg)\n{\n\tstruct { u8 u[ETH_ALEN]; } *addr;\n\tstruct tun_filter uf;\n\tint err, alen, n, nexact;\n\n\tif (copy_from_user(&uf, arg, sizeof(uf)))\n\t\treturn -EFAULT;\n\n\tif (!uf.count) {\n\t\t/* Disabled */\n\t\tfilter->count = 0;\n\t\treturn 0;\n\t}\n\n\talen = ETH_ALEN * uf.count;\n\taddr = memdup_user(arg + sizeof(uf), alen);\n\tif (IS_ERR(addr))\n\t\treturn PTR_ERR(addr);\n\n\t/* The filter is updated without holding any locks. Which is\n\t * perfectly safe. We disable it first and in the worst\n\t * case we'll accept a few undesired packets. */\n\tfilter->count = 0;\n\twmb();\n\n\t/* Use first set of addresses as an exact filter */\n\tfor (n = 0; n < uf.count && n < FLT_EXACT_COUNT; n++)\n\t\tmemcpy(filter->addr[n], addr[n].u, ETH_ALEN);\n\n\tnexact = n;\n\n\t/* Remaining multicast addresses are hashed,\n\t * unicast will leave the filter disabled. */\n\tmemset(filter->mask, 0, sizeof(filter->mask));\n\tfor (; n < uf.count; n++) {\n\t\tif (!is_multicast_ether_addr(addr[n].u)) {\n\t\t\terr = 0; /* no filter */\n\t\t\tgoto free_addr;\n\t\t}\n\t\taddr_hash_set(filter->mask, addr[n].u);\n\t}\n\n\t/* For ALLMULTI just set the mask to all ones.\n\t * This overrides the mask populated above. */\n\tif ((uf.flags & TUN_FLT_ALLMULTI))\n\t\tmemset(filter->mask, ~0, sizeof(filter->mask));\n\n\t/* Now enable the filter */\n\twmb();\n\tfilter->count = nexact;\n\n\t/* Return the number of exact filters */\n\terr = nexact;\nfree_addr:\n\tkfree(addr);\n\treturn err;\n}\n\n/* Returns: 0 - drop, !=0 - accept */\nstatic int run_filter(struct tap_filter *filter, const struct sk_buff *skb)\n{\n\t/* Cannot use eth_hdr(skb) here because skb_mac_hdr() is incorrect\n\t * at this point. */\n\tstruct ethhdr *eh = (struct ethhdr *) skb->data;\n\tint i;\n\n\t/* Exact match */\n\tfor (i = 0; i < filter->count; i++)\n\t\tif (ether_addr_equal(eh->h_dest, filter->addr[i]))\n\t\t\treturn 1;\n\n\t/* Inexact match (multicast only) */\n\tif (is_multicast_ether_addr(eh->h_dest))\n\t\treturn addr_hash_test(filter->mask, eh->h_dest);\n\n\treturn 0;\n}\n\n/*\n * Checks whether the packet is accepted or not.\n * Returns: 0 - drop, !=0 - accept\n */\nstatic int check_filter(struct tap_filter *filter, const struct sk_buff *skb)\n{\n\tif (!filter->count)\n\t\treturn 1;\n\n\treturn run_filter(filter, skb);\n}\n\n/* Network device part of the driver */\n\nstatic const struct ethtool_ops tun_ethtool_ops;\n\n/* Net device detach from fd. */\nstatic void tun_net_uninit(struct net_device *dev)\n{\n\ttun_detach_all(dev);\n}\n\n/* Net device open. */\nstatic int tun_net_open(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tint i;\n\n\tnetif_tx_start_all_queues(dev);\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\tstruct tun_file *tfile;\n\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\ttfile->socket.sk->sk_write_space(tfile->socket.sk);\n\t}\n\n\treturn 0;\n}\n\n/* Net device close. */\nstatic int tun_net_close(struct net_device *dev)\n{\n\tnetif_tx_stop_all_queues(dev);\n\treturn 0;\n}\n\n/* Net device start xmit */\nstatic netdev_tx_t tun_net_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tint txq = skb->queue_mapping;\n\tstruct tun_file *tfile;\n\tu32 numqueues = 0;\n\n\trcu_read_lock();\n\ttfile = rcu_dereference(tun->tfiles[txq]);\n\tnumqueues = ACCESS_ONCE(tun->numqueues);\n\n\t/* Drop packet if interface is not attached */\n\tif (txq >= numqueues)\n\t\tgoto drop;\n\n#ifdef CONFIG_RPS\n\tif (numqueues == 1 && static_key_false(&rps_needed)) {\n\t\t/* Select queue was not called for the skbuff, so we extract the\n\t\t * RPS hash and save it into the flow_table here.\n\t\t */\n\t\t__u32 rxhash;\n\n\t\trxhash = __skb_get_hash_symmetric(skb);\n\t\tif (rxhash) {\n\t\t\tstruct tun_flow_entry *e;\n\t\t\te = tun_flow_find(&tun->flows[tun_hashfn(rxhash)],\n\t\t\t\t\trxhash);\n\t\t\tif (e)\n\t\t\t\ttun_flow_save_rps_rxhash(e, rxhash);\n\t\t}\n\t}\n#endif\n\n\ttun_debug(KERN_INFO, tun, \"tun_net_xmit %d\\n\", skb->len);\n\n\tBUG_ON(!tfile);\n\n\t/* Drop if the filter does not like it.\n\t * This is a noop if the filter is disabled.\n\t * Filter can be enabled only for the TAP devices. */\n\tif (!check_filter(&tun->txflt, skb))\n\t\tgoto drop;\n\n\tif (tfile->socket.sk->sk_filter &&\n\t    sk_filter(tfile->socket.sk, skb))\n\t\tgoto drop;\n\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\tgoto drop;\n\n\tskb_tx_timestamp(skb);\n\n\t/* Orphan the skb - required as we might hang on to it\n\t * for indefinite time.\n\t */\n\tskb_orphan(skb);\n\n\tnf_reset(skb);\n\n\tif (skb_array_produce(&tfile->tx_array, skb))\n\t\tgoto drop;\n\n\t/* Notify and wake up reader process */\n\tif (tfile->flags & TUN_FASYNC)\n\t\tkill_fasync(&tfile->fasync, SIGIO, POLL_IN);\n\ttfile->socket.sk->sk_data_ready(tfile->socket.sk);\n\n\trcu_read_unlock();\n\treturn NETDEV_TX_OK;\n\ndrop:\n\tthis_cpu_inc(tun->pcpu_stats->tx_dropped);\n\tskb_tx_error(skb);\n\tkfree_skb(skb);\n\trcu_read_unlock();\n\treturn NET_XMIT_DROP;\n}\n\nstatic void tun_net_mclist(struct net_device *dev)\n{\n\t/*\n\t * This callback is supposed to deal with mc filter in\n\t * _rx_ path and has nothing to do with the _tx_ path.\n\t * In rx path we always accept everything userspace gives us.\n\t */\n}\n\nstatic netdev_features_t tun_net_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\treturn (features & tun->set_features) | (features & ~TUN_USER_FEATURES);\n}\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void tun_poll_controller(struct net_device *dev)\n{\n\t/*\n\t * Tun only receives frames when:\n\t * 1) the char device endpoint gets data from user space\n\t * 2) the tun socket gets a sendmsg call from user space\n\t * Since both of those are synchronous operations, we are guaranteed\n\t * never to have pending data when we poll for it\n\t * so there is nothing to do here but return.\n\t * We need this though so netpoll recognizes us as an interface that\n\t * supports polling, which enables bridge devices in virt setups to\n\t * still use netconsole\n\t */\n\treturn;\n}\n#endif\n\nstatic void tun_set_headroom(struct net_device *dev, int new_hr)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (new_hr < NET_SKB_PAD)\n\t\tnew_hr = NET_SKB_PAD;\n\n\ttun->align = new_hr;\n}\n\nstatic void\ntun_net_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)\n{\n\tu32 rx_dropped = 0, tx_dropped = 0, rx_frame_errors = 0;\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct tun_pcpu_stats *p;\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tu64 rxpackets, rxbytes, txpackets, txbytes;\n\t\tunsigned int start;\n\n\t\tp = per_cpu_ptr(tun->pcpu_stats, i);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&p->syncp);\n\t\t\trxpackets\t= p->rx_packets;\n\t\t\trxbytes\t\t= p->rx_bytes;\n\t\t\ttxpackets\t= p->tx_packets;\n\t\t\ttxbytes\t\t= p->tx_bytes;\n\t\t} while (u64_stats_fetch_retry(&p->syncp, start));\n\n\t\tstats->rx_packets\t+= rxpackets;\n\t\tstats->rx_bytes\t\t+= rxbytes;\n\t\tstats->tx_packets\t+= txpackets;\n\t\tstats->tx_bytes\t\t+= txbytes;\n\n\t\t/* u32 counters */\n\t\trx_dropped\t+= p->rx_dropped;\n\t\trx_frame_errors\t+= p->rx_frame_errors;\n\t\ttx_dropped\t+= p->tx_dropped;\n\t}\n\tstats->rx_dropped  = rx_dropped;\n\tstats->rx_frame_errors = rx_frame_errors;\n\tstats->tx_dropped = tx_dropped;\n}\n\nstatic int tun_xdp_set(struct net_device *dev, struct bpf_prog *prog,\n\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\n\told_prog = rtnl_dereference(tun->xdp_prog);\n\trcu_assign_pointer(tun->xdp_prog, prog);\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\n\treturn 0;\n}\n\nstatic u32 tun_xdp_query(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\tconst struct bpf_prog *xdp_prog;\n\n\txdp_prog = rtnl_dereference(tun->xdp_prog);\n\tif (xdp_prog)\n\t\treturn xdp_prog->aux->id;\n\n\treturn 0;\n}\n\nstatic int tun_xdp(struct net_device *dev, struct netdev_xdp *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn tun_xdp_set(dev, xdp->prog, xdp->extack);\n\tcase XDP_QUERY_PROG:\n\t\txdp->prog_id = tun_xdp_query(dev);\n\t\txdp->prog_attached = !!xdp->prog_id;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct net_device_ops tun_netdev_ops = {\n\t.ndo_uninit\t\t= tun_net_uninit,\n\t.ndo_open\t\t= tun_net_open,\n\t.ndo_stop\t\t= tun_net_close,\n\t.ndo_start_xmit\t\t= tun_net_xmit,\n\t.ndo_fix_features\t= tun_net_fix_features,\n\t.ndo_select_queue\t= tun_select_queue,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tun_poll_controller,\n#endif\n\t.ndo_set_rx_headroom\t= tun_set_headroom,\n\t.ndo_get_stats64\t= tun_net_get_stats64,\n};\n\nstatic const struct net_device_ops tap_netdev_ops = {\n\t.ndo_uninit\t\t= tun_net_uninit,\n\t.ndo_open\t\t= tun_net_open,\n\t.ndo_stop\t\t= tun_net_close,\n\t.ndo_start_xmit\t\t= tun_net_xmit,\n\t.ndo_fix_features\t= tun_net_fix_features,\n\t.ndo_set_rx_mode\t= tun_net_mclist,\n\t.ndo_set_mac_address\t= eth_mac_addr,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_select_queue\t= tun_select_queue,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= tun_poll_controller,\n#endif\n\t.ndo_features_check\t= passthru_features_check,\n\t.ndo_set_rx_headroom\t= tun_set_headroom,\n\t.ndo_get_stats64\t= tun_net_get_stats64,\n\t.ndo_xdp\t\t= tun_xdp,\n};\n\nstatic void tun_flow_init(struct tun_struct *tun)\n{\n\tint i;\n\n\tfor (i = 0; i < TUN_NUM_FLOW_ENTRIES; i++)\n\t\tINIT_HLIST_HEAD(&tun->flows[i]);\n\n\ttun->ageing_time = TUN_FLOW_EXPIRE;\n\tsetup_timer(&tun->flow_gc_timer, tun_flow_cleanup, (unsigned long)tun);\n\tmod_timer(&tun->flow_gc_timer,\n\t\t  round_jiffies_up(jiffies + tun->ageing_time));\n}\n\nstatic void tun_flow_uninit(struct tun_struct *tun)\n{\n\tdel_timer_sync(&tun->flow_gc_timer);\n\ttun_flow_flush(tun);\n}\n\n#define MIN_MTU 68\n#define MAX_MTU 65535\n\n/* Initialize net device. */\nstatic void tun_net_init(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tdev->netdev_ops = &tun_netdev_ops;\n\n\t\t/* Point-to-Point TUN Device */\n\t\tdev->hard_header_len = 0;\n\t\tdev->addr_len = 0;\n\t\tdev->mtu = 1500;\n\n\t\t/* Zero header length */\n\t\tdev->type = ARPHRD_NONE;\n\t\tdev->flags = IFF_POINTOPOINT | IFF_NOARP | IFF_MULTICAST;\n\t\tbreak;\n\n\tcase IFF_TAP:\n\t\tdev->netdev_ops = &tap_netdev_ops;\n\t\t/* Ethernet TAP Device */\n\t\tether_setup(dev);\n\t\tdev->priv_flags &= ~IFF_TX_SKB_SHARING;\n\t\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\n\t\teth_hw_addr_random(dev);\n\n\t\tbreak;\n\t}\n\n\tdev->min_mtu = MIN_MTU;\n\tdev->max_mtu = MAX_MTU - dev->hard_header_len;\n}\n\n/* Character device part */\n\n/* Poll */\nstatic unsigned int tun_chr_poll(struct file *file, poll_table *wait)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tstruct sock *sk;\n\tunsigned int mask = 0;\n\n\tif (!tun)\n\t\treturn POLLERR;\n\n\tsk = tfile->socket.sk;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_poll\\n\");\n\n\tpoll_wait(file, sk_sleep(sk), wait);\n\n\tif (!skb_array_empty(&tfile->tx_array))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\tif (tun->dev->flags & IFF_UP &&\n\t    (sock_writeable(sk) ||\n\t     (!test_and_set_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags) &&\n\t      sock_writeable(sk))))\n\t\tmask |= POLLOUT | POLLWRNORM;\n\n\tif (tun->dev->reg_state != NETREG_REGISTERED)\n\t\tmask = POLLERR;\n\n\ttun_put(tun);\n\treturn mask;\n}\n\n/* prepad is the amount to reserve at front.  len is length after that.\n * linear is a hint as to how much to copy (usually headers). */\nstatic struct sk_buff *tun_alloc_skb(struct tun_file *tfile,\n\t\t\t\t     size_t prepad, size_t len,\n\t\t\t\t     size_t linear, int noblock)\n{\n\tstruct sock *sk = tfile->socket.sk;\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   &err, 0);\n\tif (!skb)\n\t\treturn ERR_PTR(err);\n\n\tskb_reserve(skb, prepad);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic void tun_rx_batched(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t   struct sk_buff *skb, int more)\n{\n\tstruct sk_buff_head *queue = &tfile->sk.sk_write_queue;\n\tstruct sk_buff_head process_queue;\n\tu32 rx_batched = tun->rx_batched;\n\tbool rcv = false;\n\n\tif (!rx_batched || (!more && skb_queue_empty(queue))) {\n\t\tlocal_bh_disable();\n\t\tnetif_receive_skb(skb);\n\t\tlocal_bh_enable();\n\t\treturn;\n\t}\n\n\tspin_lock(&queue->lock);\n\tif (!more || skb_queue_len(queue) == rx_batched) {\n\t\t__skb_queue_head_init(&process_queue);\n\t\tskb_queue_splice_tail_init(queue, &process_queue);\n\t\trcv = true;\n\t} else {\n\t\t__skb_queue_tail(queue, skb);\n\t}\n\tspin_unlock(&queue->lock);\n\n\tif (rcv) {\n\t\tstruct sk_buff *nskb;\n\n\t\tlocal_bh_disable();\n\t\twhile ((nskb = __skb_dequeue(&process_queue)))\n\t\t\tnetif_receive_skb(nskb);\n\t\tnetif_receive_skb(skb);\n\t\tlocal_bh_enable();\n\t}\n}\n\nstatic bool tun_can_build_skb(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t      int len, int noblock, bool zerocopy)\n{\n\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\treturn false;\n\n\tif (tfile->socket.sk->sk_sndbuf != INT_MAX)\n\t\treturn false;\n\n\tif (!noblock)\n\t\treturn false;\n\n\tif (zerocopy)\n\t\treturn false;\n\n\tif (SKB_DATA_ALIGN(len + TUN_RX_PAD) +\n\t    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) > PAGE_SIZE)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct sk_buff *tun_build_skb(struct tun_struct *tun,\n\t\t\t\t     struct tun_file *tfile,\n\t\t\t\t     struct iov_iter *from,\n\t\t\t\t     struct virtio_net_hdr *hdr,\n\t\t\t\t     int len, int *skb_xdp)\n{\n\tstruct page_frag *alloc_frag = &current->task_frag;\n\tstruct sk_buff *skb;\n\tstruct bpf_prog *xdp_prog;\n\tint buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tunsigned int delta = 0;\n\tchar *buf;\n\tsize_t copied;\n\tbool xdp_xmit = false;\n\tint err, pad = TUN_RX_PAD;\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(tun->xdp_prog);\n\tif (xdp_prog)\n\t\tpad += TUN_HEADROOM;\n\tbuflen += SKB_DATA_ALIGN(len + pad);\n\trcu_read_unlock();\n\n\tif (unlikely(!skb_page_frag_refill(buflen, alloc_frag, GFP_KERNEL)))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf = (char *)page_address(alloc_frag->page) + alloc_frag->offset;\n\tcopied = copy_page_from_iter(alloc_frag->page,\n\t\t\t\t     alloc_frag->offset + pad,\n\t\t\t\t     len, from);\n\tif (copied != len)\n\t\treturn ERR_PTR(-EFAULT);\n\n\t/* There's a small window that XDP may be set after the check\n\t * of xdp_prog above, this should be rare and for simplicity\n\t * we do XDP on skb in case the headroom is not enough.\n\t */\n\tif (hdr->gso_type || !xdp_prog)\n\t\t*skb_xdp = 1;\n\telse\n\t\t*skb_xdp = 0;\n\n\trcu_read_lock();\n\txdp_prog = rcu_dereference(tun->xdp_prog);\n\tif (xdp_prog && !*skb_xdp) {\n\t\tstruct xdp_buff xdp;\n\t\tvoid *orig_data;\n\t\tu32 act;\n\n\t\txdp.data_hard_start = buf;\n\t\txdp.data = buf + pad;\n\t\txdp.data_end = xdp.data + len;\n\t\torig_data = xdp.data;\n\t\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\t\tswitch (act) {\n\t\tcase XDP_REDIRECT:\n\t\t\tget_page(alloc_frag->page);\n\t\t\talloc_frag->offset += buflen;\n\t\t\terr = xdp_do_redirect(tun->dev, &xdp, xdp_prog);\n\t\t\tif (err)\n\t\t\t\tgoto err_redirect;\n\t\t\treturn NULL;\n\t\tcase XDP_TX:\n\t\t\txdp_xmit = true;\n\t\t\t/* fall through */\n\t\tcase XDP_PASS:\n\t\t\tdelta = orig_data - xdp.data;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(act);\n\t\t\t/* fall through */\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(tun->dev, xdp_prog, act);\n\t\t\t/* fall through */\n\t\tcase XDP_DROP:\n\t\t\tgoto err_xdp;\n\t\t}\n\t}\n\n\tskb = build_skb(buf, buflen);\n\tif (!skb) {\n\t\trcu_read_unlock();\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tskb_reserve(skb, pad - delta);\n\tskb_put(skb, len + delta);\n\tget_page(alloc_frag->page);\n\talloc_frag->offset += buflen;\n\n\tif (xdp_xmit) {\n\t\tskb->dev = tun->dev;\n\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\trcu_read_lock();\n\t\treturn NULL;\n\t}\n\n\trcu_read_unlock();\n\n\treturn skb;\n\nerr_redirect:\n\tput_page(alloc_frag->page);\nerr_xdp:\n\trcu_read_unlock();\n\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\treturn NULL;\n}\n\n/* Get packet from user space buffer */\nstatic ssize_t tun_get_user(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t    void *msg_control, struct iov_iter *from,\n\t\t\t    int noblock, bool more)\n{\n\tstruct tun_pi pi = { 0, cpu_to_be16(ETH_P_IP) };\n\tstruct sk_buff *skb;\n\tsize_t total_len = iov_iter_count(from);\n\tsize_t len = total_len, align = tun->align, linear;\n\tstruct virtio_net_hdr gso = { 0 };\n\tstruct tun_pcpu_stats *stats;\n\tint good_linear;\n\tint copylen;\n\tbool zerocopy = false;\n\tint err;\n\tu32 rxhash;\n\tint skb_xdp = 1;\n\n\tif (!(tun->dev->flags & IFF_UP))\n\t\treturn -EIO;\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (len < sizeof(pi))\n\t\t\treturn -EINVAL;\n\t\tlen -= sizeof(pi);\n\n\t\tif (!copy_from_iter_full(&pi, sizeof(pi), from))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (tun->flags & IFF_VNET_HDR) {\n\t\tint vnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\t\tif (len < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\t\tlen -= vnet_hdr_sz;\n\n\t\tif (!copy_from_iter_full(&gso, sizeof(gso), from))\n\t\t\treturn -EFAULT;\n\n\t\tif ((gso.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2 > tun16_to_cpu(tun, gso.hdr_len))\n\t\t\tgso.hdr_len = cpu_to_tun16(tun, tun16_to_cpu(tun, gso.csum_start) + tun16_to_cpu(tun, gso.csum_offset) + 2);\n\n\t\tif (tun16_to_cpu(tun, gso.hdr_len) > len)\n\t\t\treturn -EINVAL;\n\t\tiov_iter_advance(from, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif ((tun->flags & TUN_TYPE_MASK) == IFF_TAP) {\n\t\talign += NET_IP_ALIGN;\n\t\tif (unlikely(len < ETH_HLEN ||\n\t\t\t     (gso.hdr_len && tun16_to_cpu(tun, gso.hdr_len) < ETH_HLEN)))\n\t\t\treturn -EINVAL;\n\t}\n\n\tgood_linear = SKB_MAX_HEAD(align);\n\n\tif (msg_control) {\n\t\tstruct iov_iter i = *from;\n\n\t\t/* There are 256 bytes to be copied in skb, so there is\n\t\t * enough room for skb expand head in case it is used.\n\t\t * The rest of the buffer is mapped from userspace.\n\t\t */\n\t\tcopylen = gso.hdr_len ? tun16_to_cpu(tun, gso.hdr_len) : GOODCOPY_LEN;\n\t\tif (copylen > good_linear)\n\t\t\tcopylen = good_linear;\n\t\tlinear = copylen;\n\t\tiov_iter_advance(&i, copylen);\n\t\tif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\n\t\t\tzerocopy = true;\n\t}\n\n\tif (tun_can_build_skb(tun, tfile, len, noblock, zerocopy)) {\n\t\t/* For the packet that is not easy to be processed\n\t\t * (e.g gso or jumbo packet), we will do it at after\n\t\t * skb was created with generic XDP routine.\n\t\t */\n\t\tskb = tun_build_skb(tun, tfile, from, &gso, len, &skb_xdp);\n\t\tif (IS_ERR(skb)) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\t\tif (!skb)\n\t\t\treturn total_len;\n\t} else {\n\t\tif (!zerocopy) {\n\t\t\tcopylen = len;\n\t\t\tif (tun16_to_cpu(tun, gso.hdr_len) > good_linear)\n\t\t\t\tlinear = good_linear;\n\t\t\telse\n\t\t\t\tlinear = tun16_to_cpu(tun, gso.hdr_len);\n\t\t}\n\n\t\tskb = tun_alloc_skb(tfile, align, copylen, linear, noblock);\n\t\tif (IS_ERR(skb)) {\n\t\t\tif (PTR_ERR(skb) != -EAGAIN)\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\treturn PTR_ERR(skb);\n\t\t}\n\n\t\tif (zerocopy)\n\t\t\terr = zerocopy_sg_from_iter(skb, from);\n\t\telse\n\t\t\terr = skb_copy_datagram_from_iter(skb, 0, from, len);\n\n\t\tif (err) {\n\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (virtio_net_hdr_to_skb(skb, &gso, tun_is_little_endian(tun))) {\n\t\tthis_cpu_inc(tun->pcpu_stats->rx_frame_errors);\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tif (tun->flags & IFF_NO_PI) {\n\t\t\tu8 ip_version = skb->len ? (skb->data[0] >> 4) : 0;\n\n\t\t\tswitch (ip_version) {\n\t\t\tcase 4:\n\t\t\t\tpi.proto = htons(ETH_P_IP);\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tpi.proto = htons(ETH_P_IPV6);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthis_cpu_inc(tun->pcpu_stats->rx_dropped);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tskb_reset_mac_header(skb);\n\t\tskb->protocol = pi.proto;\n\t\tskb->dev = tun->dev;\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tskb->protocol = eth_type_trans(skb, tun->dev);\n\t\tbreak;\n\t}\n\n\t/* copy skb_ubuf_info for callback when skb has no error */\n\tif (zerocopy) {\n\t\tskb_shinfo(skb)->destructor_arg = msg_control;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_DEV_ZEROCOPY;\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\t} else if (msg_control) {\n\t\tstruct ubuf_info *uarg = msg_control;\n\t\tuarg->callback(uarg, false);\n\t}\n\n\tskb_reset_network_header(skb);\n\tskb_probe_transport_header(skb, 0);\n\n\tif (skb_xdp) {\n\t\tstruct bpf_prog *xdp_prog;\n\t\tint ret;\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(tun->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tret = do_xdp_generic(xdp_prog, skb);\n\t\t\tif (ret != XDP_PASS) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn total_len;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\trxhash = __skb_get_hash_symmetric(skb);\n#ifndef CONFIG_4KSTACKS\n\ttun_rx_batched(tun, tfile, skb, more);\n#else\n\tnetif_rx_ni(skb);\n#endif\n\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->rx_packets++;\n\tstats->rx_bytes += len;\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(stats);\n\n\ttun_flow_update(tun, rxhash, tfile);\n\treturn total_len;\n}\n\nstatic ssize_t tun_chr_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tun_struct *tun = tun_get(file);\n\tstruct tun_file *tfile = file->private_data;\n\tssize_t result;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tresult = tun_get_user(tun, tfile, NULL, from,\n\t\t\t      file->f_flags & O_NONBLOCK, false);\n\n\ttun_put(tun);\n\treturn result;\n}\n\n/* Put packet to the user space buffer */\nstatic ssize_t tun_put_user(struct tun_struct *tun,\n\t\t\t    struct tun_file *tfile,\n\t\t\t    struct sk_buff *skb,\n\t\t\t    struct iov_iter *iter)\n{\n\tstruct tun_pi pi = { 0, skb->protocol };\n\tstruct tun_pcpu_stats *stats;\n\tssize_t total;\n\tint vlan_offset = 0;\n\tint vlan_hlen = 0;\n\tint vnet_hdr_sz = 0;\n\n\tif (skb_vlan_tag_present(skb))\n\t\tvlan_hlen = VLAN_HLEN;\n\n\tif (tun->flags & IFF_VNET_HDR)\n\t\tvnet_hdr_sz = READ_ONCE(tun->vnet_hdr_sz);\n\n\ttotal = skb->len + vlan_hlen + vnet_hdr_sz;\n\n\tif (!(tun->flags & IFF_NO_PI)) {\n\t\tif (iov_iter_count(iter) < sizeof(pi))\n\t\t\treturn -EINVAL;\n\n\t\ttotal += sizeof(pi);\n\t\tif (iov_iter_count(iter) < total) {\n\t\t\t/* Packet will be striped */\n\t\t\tpi.flags |= TUN_PKT_STRIP;\n\t\t}\n\n\t\tif (copy_to_iter(&pi, sizeof(pi), iter) != sizeof(pi))\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (vnet_hdr_sz) {\n\t\tstruct virtio_net_hdr gso;\n\n\t\tif (iov_iter_count(iter) < vnet_hdr_sz)\n\t\t\treturn -EINVAL;\n\n\t\tif (virtio_net_hdr_from_skb(skb, &gso,\n\t\t\t\t\t    tun_is_little_endian(tun), true)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\t\t\tpr_err(\"unexpected GSO type: \"\n\t\t\t       \"0x%x, gso_size %d, hdr_len %d\\n\",\n\t\t\t       sinfo->gso_type, tun16_to_cpu(tun, gso.gso_size),\n\t\t\t       tun16_to_cpu(tun, gso.hdr_len));\n\t\t\tprint_hex_dump(KERN_ERR, \"tun: \",\n\t\t\t\t       DUMP_PREFIX_NONE,\n\t\t\t\t       16, 1, skb->head,\n\t\t\t\t       min((int)tun16_to_cpu(tun, gso.hdr_len), 64), true);\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (copy_to_iter(&gso, sizeof(gso), iter) != sizeof(gso))\n\t\t\treturn -EFAULT;\n\n\t\tiov_iter_advance(iter, vnet_hdr_sz - sizeof(gso));\n\t}\n\n\tif (vlan_hlen) {\n\t\tint ret;\n\t\tstruct {\n\t\t\t__be16 h_vlan_proto;\n\t\t\t__be16 h_vlan_TCI;\n\t\t} veth;\n\n\t\tveth.h_vlan_proto = skb->vlan_proto;\n\t\tveth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));\n\n\t\tvlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);\n\n\t\tret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);\n\t\tif (ret || !iov_iter_count(iter))\n\t\t\tgoto done;\n\n\t\tret = copy_to_iter(&veth, sizeof(veth), iter);\n\t\tif (ret != sizeof(veth) || !iov_iter_count(iter))\n\t\t\tgoto done;\n\t}\n\n\tskb_copy_datagram_iter(skb, vlan_offset, iter, skb->len - vlan_offset);\n\ndone:\n\t/* caller is in process context, */\n\tstats = get_cpu_ptr(tun->pcpu_stats);\n\tu64_stats_update_begin(&stats->syncp);\n\tstats->tx_packets++;\n\tstats->tx_bytes += skb->len + vlan_hlen;\n\tu64_stats_update_end(&stats->syncp);\n\tput_cpu_ptr(tun->pcpu_stats);\n\n\treturn total;\n}\n\nstatic struct sk_buff *tun_ring_recv(struct tun_file *tfile, int noblock,\n\t\t\t\t     int *err)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct sk_buff *skb = NULL;\n\tint error = 0;\n\n\tskb = skb_array_consume(&tfile->tx_array);\n\tif (skb)\n\t\tgoto out;\n\tif (noblock) {\n\t\terror = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tadd_wait_queue(&tfile->wq.wait, &wait);\n\tcurrent->state = TASK_INTERRUPTIBLE;\n\n\twhile (1) {\n\t\tskb = skb_array_consume(&tfile->tx_array);\n\t\tif (skb)\n\t\t\tbreak;\n\t\tif (signal_pending(current)) {\n\t\t\terror = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (tfile->socket.sk->sk_shutdown & RCV_SHUTDOWN) {\n\t\t\terror = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t}\n\n\tcurrent->state = TASK_RUNNING;\n\tremove_wait_queue(&tfile->wq.wait, &wait);\n\nout:\n\t*err = error;\n\treturn skb;\n}\n\nstatic ssize_t tun_do_read(struct tun_struct *tun, struct tun_file *tfile,\n\t\t\t   struct iov_iter *to,\n\t\t\t   int noblock, struct sk_buff *skb)\n{\n\tssize_t ret;\n\tint err;\n\n\ttun_debug(KERN_INFO, tun, \"tun_do_read\\n\");\n\n\tif (!iov_iter_count(to))\n\t\treturn 0;\n\n\tif (!skb) {\n\t\t/* Read frames from ring */\n\t\tskb = tun_ring_recv(tfile, noblock, &err);\n\t\tif (!skb)\n\t\t\treturn err;\n\t}\n\n\tret = tun_put_user(tun, tfile, skb, to);\n\tif (unlikely(ret < 0))\n\t\tkfree_skb(skb);\n\telse\n\t\tconsume_skb(skb);\n\n\treturn ret;\n}\n\nstatic ssize_t tun_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tssize_t len = iov_iter_count(to), ret;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\tret = tun_do_read(tun, tfile, to, file->f_flags & O_NONBLOCK, NULL);\n\tret = min_t(ssize_t, ret, len);\n\tif (ret > 0)\n\t\tiocb->ki_pos = ret;\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic void tun_free_netdev(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tBUG_ON(!(list_empty(&tun->disabled)));\n\tfree_percpu(tun->pcpu_stats);\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\n}\n\nstatic void tun_setup(struct net_device *dev)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\ttun->owner = INVALID_UID;\n\ttun->group = INVALID_GID;\n\n\tdev->ethtool_ops = &tun_ethtool_ops;\n\tdev->needs_free_netdev = true;\n\tdev->priv_destructor = tun_free_netdev;\n\t/* We prefer our own queue length */\n\tdev->tx_queue_len = TUN_READQ_SIZE;\n}\n\n/* Trivial set of netlink ops to allow deleting tun or tap\n * device with netlink.\n */\nstatic int tun_validate(struct nlattr *tb[], struct nlattr *data[],\n\t\t\tstruct netlink_ext_ack *extack)\n{\n\treturn -EINVAL;\n}\n\nstatic struct rtnl_link_ops tun_link_ops __read_mostly = {\n\t.kind\t\t= DRV_NAME,\n\t.priv_size\t= sizeof(struct tun_struct),\n\t.setup\t\t= tun_setup,\n\t.validate\t= tun_validate,\n};\n\nstatic void tun_sock_write_space(struct sock *sk)\n{\n\tstruct tun_file *tfile;\n\twait_queue_head_t *wqueue;\n\n\tif (!sock_writeable(sk))\n\t\treturn;\n\n\tif (!test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags))\n\t\treturn;\n\n\twqueue = sk_sleep(sk);\n\tif (wqueue && waitqueue_active(wqueue))\n\t\twake_up_interruptible_sync_poll(wqueue, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\ttfile = container_of(sk, struct tun_file, sk);\n\tkill_fasync(&tfile->fasync, SIGIO, POLL_OUT);\n}\n\nstatic int tun_sendmsg(struct socket *sock, struct msghdr *m, size_t total_len)\n{\n\tint ret;\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun = __tun_get(tfile);\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tret = tun_get_user(tun, tfile, m->msg_control, &m->msg_iter,\n\t\t\t   m->msg_flags & MSG_DONTWAIT,\n\t\t\t   m->msg_flags & MSG_MORE);\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic int tun_recvmsg(struct socket *sock, struct msghdr *m, size_t total_len,\n\t\t       int flags)\n{\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun = __tun_get(tfile);\n\tint ret;\n\n\tif (!tun)\n\t\treturn -EBADFD;\n\n\tif (flags & ~(MSG_DONTWAIT|MSG_TRUNC|MSG_ERRQUEUE)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (flags & MSG_ERRQUEUE) {\n\t\tret = sock_recv_errqueue(sock->sk, m, total_len,\n\t\t\t\t\t SOL_PACKET, TUN_TX_TIMESTAMP);\n\t\tgoto out;\n\t}\n\tret = tun_do_read(tun, tfile, &m->msg_iter, flags & MSG_DONTWAIT,\n\t\t\t  m->msg_control);\n\tif (ret > (ssize_t)total_len) {\n\t\tm->msg_flags |= MSG_TRUNC;\n\t\tret = flags & MSG_TRUNC ? ret : total_len;\n\t}\nout:\n\ttun_put(tun);\n\treturn ret;\n}\n\nstatic int tun_peek_len(struct socket *sock)\n{\n\tstruct tun_file *tfile = container_of(sock, struct tun_file, socket);\n\tstruct tun_struct *tun;\n\tint ret = 0;\n\n\ttun = __tun_get(tfile);\n\tif (!tun)\n\t\treturn 0;\n\n\tret = skb_array_peek_len(&tfile->tx_array);\n\ttun_put(tun);\n\n\treturn ret;\n}\n\n/* Ops structure to mimic raw sockets with tun */\nstatic const struct proto_ops tun_socket_ops = {\n\t.peek_len = tun_peek_len,\n\t.sendmsg = tun_sendmsg,\n\t.recvmsg = tun_recvmsg,\n};\n\nstatic struct proto tun_proto = {\n\t.name\t\t= \"tun\",\n\t.owner\t\t= THIS_MODULE,\n\t.obj_size\t= sizeof(struct tun_file),\n};\n\nstatic int tun_flags(struct tun_struct *tun)\n{\n\treturn tun->flags & (TUN_FEATURES | IFF_PERSIST | IFF_TUN | IFF_TAP);\n}\n\nstatic ssize_t tun_show_flags(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn sprintf(buf, \"0x%x\\n\", tun_flags(tun));\n}\n\nstatic ssize_t tun_show_owner(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn uid_valid(tun->owner)?\n\t\tsprintf(buf, \"%u\\n\",\n\t\t\tfrom_kuid_munged(current_user_ns(), tun->owner)):\n\t\tsprintf(buf, \"-1\\n\");\n}\n\nstatic ssize_t tun_show_group(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct tun_struct *tun = netdev_priv(to_net_dev(dev));\n\treturn gid_valid(tun->group) ?\n\t\tsprintf(buf, \"%u\\n\",\n\t\t\tfrom_kgid_munged(current_user_ns(), tun->group)):\n\t\tsprintf(buf, \"-1\\n\");\n}\n\nstatic DEVICE_ATTR(tun_flags, 0444, tun_show_flags, NULL);\nstatic DEVICE_ATTR(owner, 0444, tun_show_owner, NULL);\nstatic DEVICE_ATTR(group, 0444, tun_show_group, NULL);\n\nstatic struct attribute *tun_dev_attrs[] = {\n\t&dev_attr_tun_flags.attr,\n\t&dev_attr_owner.attr,\n\t&dev_attr_group.attr,\n\tNULL\n};\n\nstatic const struct attribute_group tun_attr_group = {\n\t.attrs = tun_dev_attrs\n};\n\nstatic int tun_set_iff(struct net *net, struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_struct *tun;\n\tstruct tun_file *tfile = file->private_data;\n\tstruct net_device *dev;\n\tint err;\n\n\tif (tfile->detached)\n\t\treturn -EINVAL;\n\n\tdev = __dev_get_by_name(net, ifr->ifr_name);\n\tif (dev) {\n\t\tif (ifr->ifr_flags & IFF_TUN_EXCL)\n\t\t\treturn -EBUSY;\n\t\tif ((ifr->ifr_flags & IFF_TUN) && dev->netdev_ops == &tun_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse if ((ifr->ifr_flags & IFF_TAP) && dev->netdev_ops == &tap_netdev_ops)\n\t\t\ttun = netdev_priv(dev);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tif (!!(ifr->ifr_flags & IFF_MULTI_QUEUE) !=\n\t\t    !!(tun->flags & IFF_MULTI_QUEUE))\n\t\t\treturn -EINVAL;\n\n\t\tif (tun_not_capable(tun))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_open(tun->security);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tun_attach(tun, file, ifr->ifr_flags & IFF_NOFILTER);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tif (tun->flags & IFF_MULTI_QUEUE &&\n\t\t    (tun->numqueues + tun->numdisabled > 1)) {\n\t\t\t/* One or more queue has already been attached, no need\n\t\t\t * to initialize the device again.\n\t\t\t */\n\t\t\treturn 0;\n\t\t}\n\t}\n\telse {\n\t\tchar *name;\n\t\tunsigned long flags = 0;\n\t\tint queues = ifr->ifr_flags & IFF_MULTI_QUEUE ?\n\t\t\t     MAX_TAP_QUEUES : 1;\n\n\t\tif (!ns_capable(net->user_ns, CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\terr = security_tun_dev_create();\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t/* Set dev type */\n\t\tif (ifr->ifr_flags & IFF_TUN) {\n\t\t\t/* TUN device */\n\t\t\tflags |= IFF_TUN;\n\t\t\tname = \"tun%d\";\n\t\t} else if (ifr->ifr_flags & IFF_TAP) {\n\t\t\t/* TAP device */\n\t\t\tflags |= IFF_TAP;\n\t\t\tname = \"tap%d\";\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tif (*ifr->ifr_name)\n\t\t\tname = ifr->ifr_name;\n\n\t\tdev = alloc_netdev_mqs(sizeof(struct tun_struct), name,\n\t\t\t\t       NET_NAME_UNKNOWN, tun_setup, queues,\n\t\t\t\t       queues);\n\n\t\tif (!dev)\n\t\t\treturn -ENOMEM;\n\t\terr = dev_get_valid_name(net, dev, name);\n\t\tif (err)\n\t\t\tgoto err_free_dev;\n\n\t\tdev_net_set(dev, net);\n\t\tdev->rtnl_link_ops = &tun_link_ops;\n\t\tdev->ifindex = tfile->ifindex;\n\t\tdev->sysfs_groups[0] = &tun_attr_group;\n\n\t\ttun = netdev_priv(dev);\n\t\ttun->dev = dev;\n\t\ttun->flags = flags;\n\t\ttun->txflt.count = 0;\n\t\ttun->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\n\n\t\ttun->align = NET_SKB_PAD;\n\t\ttun->filter_attached = false;\n\t\ttun->sndbuf = tfile->socket.sk->sk_sndbuf;\n\t\ttun->rx_batched = 0;\n\n\t\ttun->pcpu_stats = netdev_alloc_pcpu_stats(struct tun_pcpu_stats);\n\t\tif (!tun->pcpu_stats) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_free_dev;\n\t\t}\n\n\t\tspin_lock_init(&tun->lock);\n\n\t\terr = security_tun_dev_alloc_security(&tun->security);\n\t\tif (err < 0)\n\t\t\tgoto err_free_stat;\n\n\t\ttun_net_init(dev);\n\t\ttun_flow_init(tun);\n\n\t\tdev->hw_features = NETIF_F_SG | NETIF_F_FRAGLIST |\n\t\t\t\t   TUN_USER_FEATURES | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t   NETIF_F_HW_VLAN_STAG_TX;\n\t\tdev->features = dev->hw_features | NETIF_F_LLTX;\n\t\tdev->vlan_features = dev->features &\n\t\t\t\t     ~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t       NETIF_F_HW_VLAN_STAG_TX);\n\n\t\tINIT_LIST_HEAD(&tun->disabled);\n\t\terr = tun_attach(tun, file, false);\n\t\tif (err < 0)\n\t\t\tgoto err_free_flow;\n\n\t\terr = register_netdevice(tun->dev);\n\t\tif (err < 0)\n\t\t\tgoto err_detach;\n\t}\n\n\tnetif_carrier_on(tun->dev);\n\n\ttun_debug(KERN_INFO, tun, \"tun_set_iff\\n\");\n\n\ttun->flags = (tun->flags & ~TUN_FEATURES) |\n\t\t(ifr->ifr_flags & TUN_FEATURES);\n\n\t/* Make sure persistent devices do not get stuck in\n\t * xoff state.\n\t */\n\tif (netif_running(tun->dev))\n\t\tnetif_tx_wake_all_queues(tun->dev);\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\treturn 0;\n\nerr_detach:\n\ttun_detach_all(dev);\n\t/* register_netdevice() already called tun_free_netdev() */\n\tgoto err_free_dev;\n\nerr_free_flow:\n\ttun_flow_uninit(tun);\n\tsecurity_tun_dev_free_security(tun->security);\nerr_free_stat:\n\tfree_percpu(tun->pcpu_stats);\nerr_free_dev:\n\tfree_netdev(dev);\n\treturn err;\n}\n\nstatic void tun_get_iff(struct net *net, struct tun_struct *tun,\n\t\t       struct ifreq *ifr)\n{\n\ttun_debug(KERN_INFO, tun, \"tun_get_iff\\n\");\n\n\tstrcpy(ifr->ifr_name, tun->dev->name);\n\n\tifr->ifr_flags = tun_flags(tun);\n\n}\n\n/* This is like a cut-down ethtool ops, except done via tun fd so no\n * privs required. */\nstatic int set_offload(struct tun_struct *tun, unsigned long arg)\n{\n\tnetdev_features_t features = 0;\n\n\tif (arg & TUN_F_CSUM) {\n\t\tfeatures |= NETIF_F_HW_CSUM;\n\t\targ &= ~TUN_F_CSUM;\n\n\t\tif (arg & (TUN_F_TSO4|TUN_F_TSO6)) {\n\t\t\tif (arg & TUN_F_TSO_ECN) {\n\t\t\t\tfeatures |= NETIF_F_TSO_ECN;\n\t\t\t\targ &= ~TUN_F_TSO_ECN;\n\t\t\t}\n\t\t\tif (arg & TUN_F_TSO4)\n\t\t\t\tfeatures |= NETIF_F_TSO;\n\t\t\tif (arg & TUN_F_TSO6)\n\t\t\t\tfeatures |= NETIF_F_TSO6;\n\t\t\targ &= ~(TUN_F_TSO4|TUN_F_TSO6);\n\t\t}\n\t}\n\n\t/* This gives the user a way to test for new features in future by\n\t * trying to set them. */\n\tif (arg)\n\t\treturn -EINVAL;\n\n\ttun->set_features = features;\n\ttun->dev->wanted_features &= ~TUN_USER_FEATURES;\n\ttun->dev->wanted_features |= features;\n\tnetdev_update_features(tun->dev);\n\n\treturn 0;\n}\n\nstatic void tun_detach_filter(struct tun_struct *tun, int n)\n{\n\tint i;\n\tstruct tun_file *tfile;\n\n\tfor (i = 0; i < n; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tlock_sock(tfile->socket.sk);\n\t\tsk_detach_filter(tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t}\n\n\ttun->filter_attached = false;\n}\n\nstatic int tun_attach_filter(struct tun_struct *tun)\n{\n\tint i, ret = 0;\n\tstruct tun_file *tfile;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tlock_sock(tfile->socket.sk);\n\t\tret = sk_attach_filter(&tun->fprog, tfile->socket.sk);\n\t\trelease_sock(tfile->socket.sk);\n\t\tif (ret) {\n\t\t\ttun_detach_filter(tun, i);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\ttun->filter_attached = true;\n\treturn ret;\n}\n\nstatic void tun_set_sndbuf(struct tun_struct *tun)\n{\n\tstruct tun_file *tfile;\n\tint i;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\ttfile->socket.sk->sk_sndbuf = tun->sndbuf;\n\t}\n}\n\nstatic int tun_set_queue(struct file *file, struct ifreq *ifr)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tint ret = 0;\n\n\trtnl_lock();\n\n\tif (ifr->ifr_flags & IFF_ATTACH_QUEUE) {\n\t\ttun = tfile->detached;\n\t\tif (!tun) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\tret = security_tun_dev_attach_queue(tun->security);\n\t\tif (ret < 0)\n\t\t\tgoto unlock;\n\t\tret = tun_attach(tun, file, false);\n\t} else if (ifr->ifr_flags & IFF_DETACH_QUEUE) {\n\t\ttun = rtnl_dereference(tfile->tun);\n\t\tif (!tun || !(tun->flags & IFF_MULTI_QUEUE) || tfile->detached)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\t__tun_detach(tfile, false);\n\t} else\n\t\tret = -EINVAL;\n\nunlock:\n\trtnl_unlock();\n\treturn ret;\n}\n\nstatic long __tun_chr_ioctl(struct file *file, unsigned int cmd,\n\t\t\t    unsigned long arg, int ifreq_len)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tstruct tun_struct *tun;\n\tvoid __user* argp = (void __user*)arg;\n\tstruct ifreq ifr;\n\tkuid_t owner;\n\tkgid_t group;\n\tint sndbuf;\n\tint vnet_hdr_sz;\n\tunsigned int ifindex;\n\tint le;\n\tint ret;\n\n\tif (cmd == TUNSETIFF || cmd == TUNSETQUEUE || _IOC_TYPE(cmd) == SOCK_IOC_TYPE) {\n\t\tif (copy_from_user(&ifr, argp, ifreq_len))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tmemset(&ifr, 0, sizeof(ifr));\n\t}\n\tif (cmd == TUNGETFEATURES) {\n\t\t/* Currently this just means: \"what IFF flags are valid?\".\n\t\t * This is needed because we never checked for invalid flags on\n\t\t * TUNSETIFF.\n\t\t */\n\t\treturn put_user(IFF_TUN | IFF_TAP | TUN_FEATURES,\n\t\t\t\t(unsigned int __user*)argp);\n\t} else if (cmd == TUNSETQUEUE)\n\t\treturn tun_set_queue(file, &ifr);\n\n\tret = 0;\n\trtnl_lock();\n\n\ttun = __tun_get(tfile);\n\tif (cmd == TUNSETIFF) {\n\t\tret = -EEXIST;\n\t\tif (tun)\n\t\t\tgoto unlock;\n\n\t\tifr.ifr_name[IFNAMSIZ-1] = '\\0';\n\n\t\tret = tun_set_iff(sock_net(&tfile->sk), file, &ifr);\n\n\t\tif (ret)\n\t\t\tgoto unlock;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tgoto unlock;\n\t}\n\tif (cmd == TUNSETIFINDEX) {\n\t\tret = -EPERM;\n\t\tif (tun)\n\t\t\tgoto unlock;\n\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&ifindex, argp, sizeof(ifindex)))\n\t\t\tgoto unlock;\n\n\t\tret = 0;\n\t\ttfile->ifindex = ifindex;\n\t\tgoto unlock;\n\t}\n\n\tret = -EBADFD;\n\tif (!tun)\n\t\tgoto unlock;\n\n\ttun_debug(KERN_INFO, tun, \"tun_chr_ioctl cmd %u\\n\", cmd);\n\n\tret = 0;\n\tswitch (cmd) {\n\tcase TUNGETIFF:\n\t\ttun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\n\t\tif (tfile->detached)\n\t\t\tifr.ifr_flags |= IFF_DETACH_QUEUE;\n\t\tif (!tfile->socket.sk->sk_filter)\n\t\t\tifr.ifr_flags |= IFF_NOFILTER;\n\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETNOCSUM:\n\t\t/* Disable/Enable checksum */\n\n\t\t/* [unimplemented] */\n\t\ttun_debug(KERN_INFO, tun, \"ignored: set checksum %s\\n\",\n\t\t\t  arg ? \"disabled\" : \"enabled\");\n\t\tbreak;\n\n\tcase TUNSETPERSIST:\n\t\t/* Disable/Enable persist mode. Keep an extra reference to the\n\t\t * module to prevent the module being unprobed.\n\t\t */\n\t\tif (arg && !(tun->flags & IFF_PERSIST)) {\n\t\t\ttun->flags |= IFF_PERSIST;\n\t\t\t__module_get(THIS_MODULE);\n\t\t}\n\t\tif (!arg && (tun->flags & IFF_PERSIST)) {\n\t\t\ttun->flags &= ~IFF_PERSIST;\n\t\t\tmodule_put(THIS_MODULE);\n\t\t}\n\n\t\ttun_debug(KERN_INFO, tun, \"persist %s\\n\",\n\t\t\t  arg ? \"enabled\" : \"disabled\");\n\t\tbreak;\n\n\tcase TUNSETOWNER:\n\t\t/* Set owner of the device */\n\t\towner = make_kuid(current_user_ns(), arg);\n\t\tif (!uid_valid(owner)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttun->owner = owner;\n\t\ttun_debug(KERN_INFO, tun, \"owner set to %u\\n\",\n\t\t\t  from_kuid(&init_user_ns, tun->owner));\n\t\tbreak;\n\n\tcase TUNSETGROUP:\n\t\t/* Set group of the device */\n\t\tgroup = make_kgid(current_user_ns(), arg);\n\t\tif (!gid_valid(group)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttun->group = group;\n\t\ttun_debug(KERN_INFO, tun, \"group set to %u\\n\",\n\t\t\t  from_kgid(&init_user_ns, tun->group));\n\t\tbreak;\n\n\tcase TUNSETLINK:\n\t\t/* Only allow setting the type when the interface is down */\n\t\tif (tun->dev->flags & IFF_UP) {\n\t\t\ttun_debug(KERN_INFO, tun,\n\t\t\t\t  \"Linktype set failed because interface is up\\n\");\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\ttun->dev->type = (int) arg;\n\t\t\ttun_debug(KERN_INFO, tun, \"linktype set to %d\\n\",\n\t\t\t\t  tun->dev->type);\n\t\t\tret = 0;\n\t\t}\n\t\tbreak;\n\n#ifdef TUN_DEBUG\n\tcase TUNSETDEBUG:\n\t\ttun->debug = arg;\n\t\tbreak;\n#endif\n\tcase TUNSETOFFLOAD:\n\t\tret = set_offload(tun, arg);\n\t\tbreak;\n\n\tcase TUNSETTXFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = update_filter(&tun->txflt, (void __user *)arg);\n\t\tbreak;\n\n\tcase SIOCGIFHWADDR:\n\t\t/* Get hw address */\n\t\tmemcpy(ifr.ifr_hwaddr.sa_data, tun->dev->dev_addr, ETH_ALEN);\n\t\tifr.ifr_hwaddr.sa_family = tun->dev->type;\n\t\tif (copy_to_user(argp, &ifr, ifreq_len))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase SIOCSIFHWADDR:\n\t\t/* Set hw address */\n\t\ttun_debug(KERN_DEBUG, tun, \"set hw address: %pM\\n\",\n\t\t\t  ifr.ifr_hwaddr.sa_data);\n\n\t\tret = dev_set_mac_address(tun->dev, &ifr.ifr_hwaddr);\n\t\tbreak;\n\n\tcase TUNGETSNDBUF:\n\t\tsndbuf = tfile->socket.sk->sk_sndbuf;\n\t\tif (copy_to_user(argp, &sndbuf, sizeof(sndbuf)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETSNDBUF:\n\t\tif (copy_from_user(&sndbuf, argp, sizeof(sndbuf))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->sndbuf = sndbuf;\n\t\ttun_set_sndbuf(tun);\n\t\tbreak;\n\n\tcase TUNGETVNETHDRSZ:\n\t\tvnet_hdr_sz = tun->vnet_hdr_sz;\n\t\tif (copy_to_user(argp, &vnet_hdr_sz, sizeof(vnet_hdr_sz)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETHDRSZ:\n\t\tif (copy_from_user(&vnet_hdr_sz, argp, sizeof(vnet_hdr_sz))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (vnet_hdr_sz < (int)sizeof(struct virtio_net_hdr)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\ttun->vnet_hdr_sz = vnet_hdr_sz;\n\t\tbreak;\n\n\tcase TUNGETVNETLE:\n\t\tle = !!(tun->flags & TUN_VNET_LE);\n\t\tif (put_user(le, (int __user *)argp))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\n\tcase TUNSETVNETLE:\n\t\tif (get_user(le, (int __user *)argp)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (le)\n\t\t\ttun->flags |= TUN_VNET_LE;\n\t\telse\n\t\t\ttun->flags &= ~TUN_VNET_LE;\n\t\tbreak;\n\n\tcase TUNGETVNETBE:\n\t\tret = tun_get_vnet_be(tun, argp);\n\t\tbreak;\n\n\tcase TUNSETVNETBE:\n\t\tret = tun_set_vnet_be(tun, argp);\n\t\tbreak;\n\n\tcase TUNATTACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_from_user(&tun->fprog, argp, sizeof(tun->fprog)))\n\t\t\tbreak;\n\n\t\tret = tun_attach_filter(tun);\n\t\tbreak;\n\n\tcase TUNDETACHFILTER:\n\t\t/* Can be set only for TAPs */\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = 0;\n\t\ttun_detach_filter(tun, tun->numqueues);\n\t\tbreak;\n\n\tcase TUNGETFILTER:\n\t\tret = -EINVAL;\n\t\tif ((tun->flags & TUN_TYPE_MASK) != IFF_TAP)\n\t\t\tbreak;\n\t\tret = -EFAULT;\n\t\tif (copy_to_user(argp, &tun->fprog, sizeof(tun->fprog)))\n\t\t\tbreak;\n\t\tret = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\nunlock:\n\trtnl_unlock();\n\tif (tun)\n\t\ttun_put(tun);\n\treturn ret;\n}\n\nstatic long tun_chr_ioctl(struct file *file,\n\t\t\t  unsigned int cmd, unsigned long arg)\n{\n\treturn __tun_chr_ioctl(file, cmd, arg, sizeof (struct ifreq));\n}\n\n#ifdef CONFIG_COMPAT\nstatic long tun_chr_compat_ioctl(struct file *file,\n\t\t\t unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase TUNSETIFF:\n\tcase TUNGETIFF:\n\tcase TUNSETTXFILTER:\n\tcase TUNGETSNDBUF:\n\tcase TUNSETSNDBUF:\n\tcase SIOCGIFHWADDR:\n\tcase SIOCSIFHWADDR:\n\t\targ = (unsigned long)compat_ptr(arg);\n\t\tbreak;\n\tdefault:\n\t\targ = (compat_ulong_t)arg;\n\t\tbreak;\n\t}\n\n\t/*\n\t * compat_ifreq is shorter than ifreq, so we must not access beyond\n\t * the end of that structure. All fields that are used in this\n\t * driver are compatible though, we don't need to convert the\n\t * contents.\n\t */\n\treturn __tun_chr_ioctl(file, cmd, arg, sizeof(struct compat_ifreq));\n}\n#endif /* CONFIG_COMPAT */\n\nstatic int tun_chr_fasync(int fd, struct file *file, int on)\n{\n\tstruct tun_file *tfile = file->private_data;\n\tint ret;\n\n\tif ((ret = fasync_helper(fd, file, on, &tfile->fasync)) < 0)\n\t\tgoto out;\n\n\tif (on) {\n\t\t__f_setown(file, task_pid(current), PIDTYPE_PID, 0);\n\t\ttfile->flags |= TUN_FASYNC;\n\t} else\n\t\ttfile->flags &= ~TUN_FASYNC;\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic int tun_chr_open(struct inode *inode, struct file * file)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct tun_file *tfile;\n\n\tDBG1(KERN_INFO, \"tunX: tun_chr_open\\n\");\n\n\ttfile = (struct tun_file *)sk_alloc(net, AF_UNSPEC, GFP_KERNEL,\n\t\t\t\t\t    &tun_proto, 0);\n\tif (!tfile)\n\t\treturn -ENOMEM;\n\tRCU_INIT_POINTER(tfile->tun, NULL);\n\ttfile->flags = 0;\n\ttfile->ifindex = 0;\n\n\tinit_waitqueue_head(&tfile->wq.wait);\n\tRCU_INIT_POINTER(tfile->socket.wq, &tfile->wq);\n\n\ttfile->socket.file = file;\n\ttfile->socket.ops = &tun_socket_ops;\n\n\tsock_init_data(&tfile->socket, &tfile->sk);\n\n\ttfile->sk.sk_write_space = tun_sock_write_space;\n\ttfile->sk.sk_sndbuf = INT_MAX;\n\n\tfile->private_data = tfile;\n\tINIT_LIST_HEAD(&tfile->next);\n\n\tsock_set_flag(&tfile->sk, SOCK_ZEROCOPY);\n\n\treturn 0;\n}\n\nstatic int tun_chr_close(struct inode *inode, struct file *file)\n{\n\tstruct tun_file *tfile = file->private_data;\n\n\ttun_detach(tfile, true);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void tun_chr_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct tun_struct *tun;\n\tstruct ifreq ifr;\n\n\tmemset(&ifr, 0, sizeof(ifr));\n\n\trtnl_lock();\n\ttun = tun_get(f);\n\tif (tun)\n\t\ttun_get_iff(current->nsproxy->net_ns, tun, &ifr);\n\trtnl_unlock();\n\n\tif (tun)\n\t\ttun_put(tun);\n\n\tseq_printf(m, \"iff:\\t%s\\n\", ifr.ifr_name);\n}\n#endif\n\nstatic const struct file_operations tun_fops = {\n\t.owner\t= THIS_MODULE,\n\t.llseek = no_llseek,\n\t.read_iter  = tun_chr_read_iter,\n\t.write_iter = tun_chr_write_iter,\n\t.poll\t= tun_chr_poll,\n\t.unlocked_ioctl\t= tun_chr_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = tun_chr_compat_ioctl,\n#endif\n\t.open\t= tun_chr_open,\n\t.release = tun_chr_close,\n\t.fasync = tun_chr_fasync,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo = tun_chr_show_fdinfo,\n#endif\n};\n\nstatic struct miscdevice tun_miscdev = {\n\t.minor = TUN_MINOR,\n\t.name = \"tun\",\n\t.nodename = \"net/tun\",\n\t.fops = &tun_fops,\n};\n\n/* ethtool interface */\n\nstatic int tun_get_link_ksettings(struct net_device *dev,\n\t\t\t\t  struct ethtool_link_ksettings *cmd)\n{\n\tethtool_link_ksettings_zero_link_mode(cmd, supported);\n\tethtool_link_ksettings_zero_link_mode(cmd, advertising);\n\tcmd->base.speed\t\t= SPEED_10;\n\tcmd->base.duplex\t= DUPLEX_FULL;\n\tcmd->base.port\t\t= PORT_TP;\n\tcmd->base.phy_address\t= 0;\n\tcmd->base.autoneg\t= AUTONEG_DISABLE;\n\treturn 0;\n}\n\nstatic void tun_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\n\n\tswitch (tun->flags & TUN_TYPE_MASK) {\n\tcase IFF_TUN:\n\t\tstrlcpy(info->bus_info, \"tun\", sizeof(info->bus_info));\n\t\tbreak;\n\tcase IFF_TAP:\n\t\tstrlcpy(info->bus_info, \"tap\", sizeof(info->bus_info));\n\t\tbreak;\n\t}\n}\n\nstatic u32 tun_get_msglevel(struct net_device *dev)\n{\n#ifdef TUN_DEBUG\n\tstruct tun_struct *tun = netdev_priv(dev);\n\treturn tun->debug;\n#else\n\treturn -EOPNOTSUPP;\n#endif\n}\n\nstatic void tun_set_msglevel(struct net_device *dev, u32 value)\n{\n#ifdef TUN_DEBUG\n\tstruct tun_struct *tun = netdev_priv(dev);\n\ttun->debug = value;\n#endif\n}\n\nstatic int tun_get_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *ec)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tec->rx_max_coalesced_frames = tun->rx_batched;\n\n\treturn 0;\n}\n\nstatic int tun_set_coalesce(struct net_device *dev,\n\t\t\t    struct ethtool_coalesce *ec)\n{\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (ec->rx_max_coalesced_frames > NAPI_POLL_WEIGHT)\n\t\ttun->rx_batched = NAPI_POLL_WEIGHT;\n\telse\n\t\ttun->rx_batched = ec->rx_max_coalesced_frames;\n\n\treturn 0;\n}\n\nstatic const struct ethtool_ops tun_ethtool_ops = {\n\t.get_drvinfo\t= tun_get_drvinfo,\n\t.get_msglevel\t= tun_get_msglevel,\n\t.set_msglevel\t= tun_set_msglevel,\n\t.get_link\t= ethtool_op_get_link,\n\t.get_ts_info\t= ethtool_op_get_ts_info,\n\t.get_coalesce   = tun_get_coalesce,\n\t.set_coalesce   = tun_set_coalesce,\n\t.get_link_ksettings = tun_get_link_ksettings,\n};\n\nstatic int tun_queue_resize(struct tun_struct *tun)\n{\n\tstruct net_device *dev = tun->dev;\n\tstruct tun_file *tfile;\n\tstruct skb_array **arrays;\n\tint n = tun->numqueues + tun->numdisabled;\n\tint ret, i;\n\n\tarrays = kmalloc_array(n, sizeof(*arrays), GFP_KERNEL);\n\tif (!arrays)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < tun->numqueues; i++) {\n\t\ttfile = rtnl_dereference(tun->tfiles[i]);\n\t\tarrays[i] = &tfile->tx_array;\n\t}\n\tlist_for_each_entry(tfile, &tun->disabled, next)\n\t\tarrays[i++] = &tfile->tx_array;\n\n\tret = skb_array_resize_multiple(arrays, n,\n\t\t\t\t\tdev->tx_queue_len, GFP_KERNEL);\n\n\tkfree(arrays);\n\treturn ret;\n}\n\nstatic int tun_device_event(struct notifier_block *unused,\n\t\t\t    unsigned long event, void *ptr)\n{\n\tstruct net_device *dev = netdev_notifier_info_to_dev(ptr);\n\tstruct tun_struct *tun = netdev_priv(dev);\n\n\tif (dev->rtnl_link_ops != &tun_link_ops)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (event) {\n\tcase NETDEV_CHANGE_TX_QUEUE_LEN:\n\t\tif (tun_queue_resize(tun))\n\t\t\treturn NOTIFY_BAD;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block tun_notifier_block __read_mostly = {\n\t.notifier_call\t= tun_device_event,\n};\n\nstatic int __init tun_init(void)\n{\n\tint ret = 0;\n\n\tpr_info(\"%s, %s\\n\", DRV_DESCRIPTION, DRV_VERSION);\n\n\tret = rtnl_link_register(&tun_link_ops);\n\tif (ret) {\n\t\tpr_err(\"Can't register link_ops\\n\");\n\t\tgoto err_linkops;\n\t}\n\n\tret = misc_register(&tun_miscdev);\n\tif (ret) {\n\t\tpr_err(\"Can't register misc device %d\\n\", TUN_MINOR);\n\t\tgoto err_misc;\n\t}\n\n\tret = register_netdevice_notifier(&tun_notifier_block);\n\tif (ret) {\n\t\tpr_err(\"Can't register netdevice notifier\\n\");\n\t\tgoto err_notifier;\n\t}\n\n\treturn  0;\n\nerr_notifier:\n\tmisc_deregister(&tun_miscdev);\nerr_misc:\n\trtnl_link_unregister(&tun_link_ops);\nerr_linkops:\n\treturn ret;\n}\n\nstatic void tun_cleanup(void)\n{\n\tmisc_deregister(&tun_miscdev);\n\trtnl_link_unregister(&tun_link_ops);\n\tunregister_netdevice_notifier(&tun_notifier_block);\n}\n\n/* Get an underlying socket object from tun file.  Returns error unless file is\n * attached to a device.  The returned object works like a packet socket, it\n * can be used for sock_sendmsg/sock_recvmsg.  The caller is responsible for\n * holding a reference to the file for as long as the socket is in use. */\nstruct socket *tun_get_socket(struct file *file)\n{\n\tstruct tun_file *tfile;\n\tif (file->f_op != &tun_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfile = file->private_data;\n\tif (!tfile)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &tfile->socket;\n}\nEXPORT_SYMBOL_GPL(tun_get_socket);\n\nstruct skb_array *tun_get_skb_array(struct file *file)\n{\n\tstruct tun_file *tfile;\n\n\tif (file->f_op != &tun_fops)\n\t\treturn ERR_PTR(-EINVAL);\n\ttfile = file->private_data;\n\tif (!tfile)\n\t\treturn ERR_PTR(-EBADFD);\n\treturn &tfile->tx_array;\n}\nEXPORT_SYMBOL_GPL(tun_get_skb_array);\n\nmodule_init(tun_init);\nmodule_exit(tun_cleanup);\nMODULE_DESCRIPTION(DRV_DESCRIPTION);\nMODULE_AUTHOR(DRV_COPYRIGHT);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_MISCDEV(TUN_MINOR);\nMODULE_ALIAS(\"devname:net/tun\");\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tDefinitions for the Interfaces handler.\n *\n * Version:\t@(#)dev.h\t1.0.10\t08/12/93\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tDonald J. Becker, <becker@cesdis.gsfc.nasa.gov>\n *\t\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\t\tBjorn Ekwall. <bj0rn@blox.se>\n *              Pekka Riikonen <priikone@poseidon.pspt.fi>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\t\tMoved to /usr/include/linux for NET3\n */\n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <linux/ethtool.h>\n#include <net/net_namespace.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n#include <linux/hashtable.h>\n\nstruct netpoll_info;\nstruct device;\nstruct phy_device;\nstruct dsa_switch_tree;\n\n/* 802.11 specific */\nstruct wireless_dev;\n/* 802.15.4 specific */\nstruct wpan_dev;\nstruct mpls_dev;\n/* UDP Tunnel offloads */\nstruct udp_tunnel_info;\nstruct bpf_prog;\nstruct xdp_buff;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\n\n/* Backlog congestion levels */\n#define NET_RX_SUCCESS\t\t0\t/* keep 'em coming, baby */\n#define NET_RX_DROP\t\t1\t/* packet dropped */\n\n/*\n * Transmit return codes: transmit return codes originate from three different\n * namespaces:\n *\n * - qdisc return codes\n * - driver transmit return codes\n * - errno values\n *\n * Drivers are allowed to return any one of those in their hard_start_xmit()\n * function. Real network devices commonly used with qdiscs should only return\n * the driver transmit return codes though - when qdiscs are used, the actual\n * transmission happens asynchronously, so the value is not propagated to\n * higher layers. Virtual network devices transmit synchronously; in this case\n * the driver transmit return codes are consumed by dev_queue_xmit(), and all\n * others are propagated to higher layers.\n */\n\n/* qdisc ->enqueue() return codes. */\n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t/* skb dropped\t\t\t*/\n#define NET_XMIT_CN\t\t0x02\t/* congestion notification\t*/\n#define NET_XMIT_MASK\t\t0x0f\t/* qdisc flags in net/sch_generic.h */\n\n/* NET_XMIT_CN is special. It does not guarantee that this packet is lost. It\n * indicates that the device will soon be dropping packets, or already drops\n * some packets of the same priority; prompting us to send less aggressively. */\n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n/* Driver transmit return codes */\n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t/* make sure enum is signed */\n\tNETDEV_TX_OK\t = 0x00,\t/* driver took care of packet */\n\tNETDEV_TX_BUSY\t = 0x10,\t/* driver tx path was busy*/\n};\ntypedef enum netdev_tx netdev_tx_t;\n\n/*\n * Current order: NETDEV_TX_MASK > NET_XMIT_MASK >= 0 is significant;\n * hard_start_xmit() return < NET_XMIT_MASK means skb was consumed.\n */\nstatic inline bool dev_xmit_complete(int rc)\n{\n\t/*\n\t * Positive cases with an skb consumed by a driver:\n\t * - successful transmission (rc == NETDEV_TX_OK)\n\t * - error while transmitting (rc < 0)\n\t * - error while queueing to a different device (rc & NET_XMIT_MASK)\n\t */\n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tCompute the worst-case header length according to the protocols\n *\tused.\n */\n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n/*\n *\tOld network device statistics. Fields are native words\n *\t(unsigned long) so they can be read and written atomically.\n */\n\nstruct net_device_stats {\n\tunsigned long\trx_packets;\n\tunsigned long\ttx_packets;\n\tunsigned long\trx_bytes;\n\tunsigned long\ttx_bytes;\n\tunsigned long\trx_errors;\n\tunsigned long\ttx_errors;\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\tmulticast;\n\tunsigned long\tcollisions;\n\tunsigned long\trx_length_errors;\n\tunsigned long\trx_over_errors;\n\tunsigned long\trx_crc_errors;\n\tunsigned long\trx_frame_errors;\n\tunsigned long\trx_fifo_errors;\n\tunsigned long\trx_missed_errors;\n\tunsigned long\ttx_aborted_errors;\n\tunsigned long\ttx_carrier_errors;\n\tunsigned long\ttx_fifo_errors;\n\tunsigned long\ttx_heartbeat_errors;\n\tunsigned long\ttx_window_errors;\n\tunsigned long\trx_compressed;\n\tunsigned long\ttx_compressed;\n};\n\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key rps_needed;\nextern struct static_key rfs_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_SLAVE\t\t3\n#define NETDEV_HW_ADDR_T_UNICAST\t4\n#define NETDEV_HW_ADDR_T_MULTICAST\t5\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n\nstruct hh_cache {\n\tunsigned int\thh_len;\n\tseqlock_t\thh_lock;\n\n\t/* cached hardware header; allow for machine alignment needs.        */\n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n/* Reserve HH_DATA_MOD byte-aligned hard_header_len, but at least that much.\n * Alternative is:\n *   dev->hard_header_len ? (dev->hard_header_len +\n *                           (HH_DATA_MOD - 1)) & ~(HH_DATA_MOD - 1) : 0\n *\n * We could use other alignment values, but we must maintain the\n * relationship HH alignment <= LL alignment.\n */\n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom)&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len+(dev)->needed_headroom+(extra))&~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n};\n\n/* These flag bits are private to the generic network queueing\n * layer; they may not be explicitly referenced by any other\n * code.\n */\n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n};\n\n\n/*\n * This structure holds boot-time configured netdevice settings. They\n * are then used in the device probing.\n */\nstruct netdev_boot_setup {\n\tchar name[IFNAMSIZ];\n\tstruct ifmap map;\n};\n#define NETDEV_BOOT_SETUP_MAX 8\n\nint __init netdev_boot_setup(char *str);\n\n/*\n * Structure for NAPI scheduling similar to tasklet but with weighting\n */\nstruct napi_struct {\n\t/* The poll_list must only be managed by the entity which\n\t * changes the state of the NAPI_STATE_SCHED bit.  This means\n\t * whoever atomically sets that bit can add this napi_struct\n\t * to the per-CPU poll_list, and whoever clears that bit\n\t * can remove from the list right before clearing the bit.\n\t */\n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tunsigned int\t\tgro_count;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\tint\t\t\tpoll_owner;\n#endif\n\tstruct net_device\t*dev;\n\tstruct sk_buff\t\t*gro_list;\n\tstruct sk_buff\t\t*skb;\n\tstruct hrtimer\t\ttimer;\n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n\tunsigned int\t\tnapi_id;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t/* Poll is scheduled */\n\tNAPI_STATE_MISSED,\t/* reschedule a napi */\n\tNAPI_STATE_DISABLE,\t/* Disable pending */\n\tNAPI_STATE_NPSVC,\t/* Netpoll - don't dequeue from poll_list */\n\tNAPI_STATE_HASHED,\t/* In NAPI hash (busy polling possible) */\n\tNAPI_STATE_NO_BUSY_POLL,/* Do not add in napi_hash, no busy polling */\n\tNAPI_STATE_IN_BUSY_POLL,/* sk_busy_loop() owns this NAPI */\n};\n\nenum {\n\tNAPIF_STATE_SCHED\t = BIT(NAPI_STATE_SCHED),\n\tNAPIF_STATE_MISSED\t = BIT(NAPI_STATE_MISSED),\n\tNAPIF_STATE_DISABLE\t = BIT(NAPI_STATE_DISABLE),\n\tNAPIF_STATE_NPSVC\t = BIT(NAPI_STATE_NPSVC),\n\tNAPIF_STATE_HASHED\t = BIT(NAPI_STATE_HASHED),\n\tNAPIF_STATE_NO_BUSY_POLL = BIT(NAPI_STATE_NO_BUSY_POLL),\n\tNAPIF_STATE_IN_BUSY_POLL = BIT(NAPI_STATE_IN_BUSY_POLL),\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_DROP,\n\tGRO_CONSUMED,\n};\ntypedef enum gro_result gro_result_t;\n\n/*\n * enum rx_handler_result - Possible return values for rx_handlers.\n * @RX_HANDLER_CONSUMED: skb was consumed by rx_handler, do not process it\n * further.\n * @RX_HANDLER_ANOTHER: Do another round in receive path. This is indicated in\n * case skb->dev was changed by rx_handler.\n * @RX_HANDLER_EXACT: Force exact delivery, no wildcard.\n * @RX_HANDLER_PASS: Do nothing, pass the skb as if no rx_handler was called.\n *\n * rx_handlers are functions called from inside __netif_receive_skb(), to do\n * special processing of the skb, prior to delivery to protocol handlers.\n *\n * Currently, a net_device can only have a single rx_handler registered. Trying\n * to register a second rx_handler will return -EBUSY.\n *\n * To register a rx_handler on a net_device, use netdev_rx_handler_register().\n * To unregister a rx_handler on a net_device, use\n * netdev_rx_handler_unregister().\n *\n * Upon return, rx_handler is expected to tell __netif_receive_skb() what to\n * do with the skb.\n *\n * If the rx_handler consumed the skb in some way, it should return\n * RX_HANDLER_CONSUMED. This is appropriate when the rx_handler arranged for\n * the skb to be delivered in some other way.\n *\n * If the rx_handler changed skb->dev, to divert the skb to another\n * net_device, it should return RX_HANDLER_ANOTHER. The rx_handler for the\n * new device will be called if it exists.\n *\n * If the rx_handler decides the skb should be ignored, it should return\n * RX_HANDLER_EXACT. The skb will only be delivered to protocol handlers that\n * are registered on exact device (ptype->dev == skb->dev).\n *\n * If the rx_handler didn't change skb->dev, but wants the skb to be normally\n * delivered, it should return RX_HANDLER_PASS.\n *\n * A device without a registered rx_handler will behave as if rx_handler\n * returned RX_HANDLER_PASS.\n */\n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\nbool napi_schedule_prep(struct napi_struct *n);\n\n/**\n *\tnapi_schedule - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Schedule NAPI poll routine to be called if it is not already\n * running.\n */\nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n/**\n *\tnapi_schedule_irqoff - schedule NAPI poll\n *\t@n: NAPI context\n *\n * Variant of napi_schedule(), assuming hard irqs are masked.\n */\nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n/* Try to reschedule poll. Called by dev->poll() after napi_complete().  */\nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool napi_complete_done(struct napi_struct *n, int work_done);\n/**\n *\tnapi_complete - NAPI processing complete\n *\t@n: NAPI context\n *\n * Mark NAPI processing as complete.\n * Consider using napi_complete_done() instead.\n * Return false if device should avoid rearming interrupts.\n */\nstatic inline bool napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\n/**\n *\tnapi_hash_del - remove a NAPI from global table\n *\t@napi: NAPI context\n *\n * Warning: caller must observe RCU grace period\n * before freeing memory containing @napi, if\n * this function returns true.\n * Note: core networking stack automatically calls it\n * from netif_napi_del().\n * Drivers might want to call this helper to combine all\n * the needed RCU grace periods into a single one.\n */\nbool napi_hash_del(struct napi_struct *napi);\n\n/**\n *\tnapi_disable - prevent NAPI from scheduling\n *\t@n: NAPI context\n *\n * Stop NAPI from being scheduled on this context.\n * Waits till any outstanding processing completes.\n */\nvoid napi_disable(struct napi_struct *n);\n\n/**\n *\tnapi_enable - enable NAPI scheduling\n *\t@n: NAPI context\n *\n * Resume NAPI from being scheduled on this context.\n * Must be paired with napi_disable.\n */\nstatic inline void napi_enable(struct napi_struct *n)\n{\n\tBUG_ON(!test_bit(NAPI_STATE_SCHED, &n->state));\n\tsmp_mb__before_atomic();\n\tclear_bit(NAPI_STATE_SCHED, &n->state);\n\tclear_bit(NAPI_STATE_NPSVC, &n->state);\n}\n\n/**\n *\tnapi_synchronize - wait until NAPI is not running\n *\t@n: NAPI context\n *\n * Wait until NAPI is done being scheduled on this context.\n * Waits till any outstanding processing completes but\n * does not disable future activations.\n */\nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n/*\n * __QUEUE_STATE_DRV_XOFF is used by drivers to stop the transmit queue.  The\n * netif_tx_* functions below are used to manipulate this flag.  The\n * __QUEUE_STATE_STACK_XOFF flag is used by the stack to stop the transmit\n * queue independently.  The netif_xmit_*stopped functions below are called\n * to check if the queue has been stopped by the driver or stack (either\n * of the XOFF bits are set in the state).  Drivers should not need to call\n * netif_xmit*stopped functions, they should only be using netif_tx_*.\n */\n\nstruct netdev_queue {\n/*\n * read-mostly part\n */\n\tstruct net_device\t*dev;\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc\t\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n\tunsigned long\t\ttx_maxrate;\n\t/*\n\t * Number of TX timeouts for this queue\n\t * (/sys/class/net/DEV/Q/trans_timeout)\n\t */\n\tunsigned long\t\ttrans_timeout;\n/*\n * write-mostly part\n */\n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t/*\n\t * Time (in jiffies) of last Tx\n\t */\n\tunsigned long\t\ttrans_start;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n} ____cacheline_aligned_in_smp;\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n/*\n * This structure holds an RPS map which can be of variable length.  The\n * map is an array of CPUs.\n */\nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[0];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n/*\n * The rps_dev_flow structure contains the mapping of a flow to a CPU, the\n * tail pointer for that CPU's input queue at the time of last enqueue, and\n * a hardware filter index.\n */\nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n/*\n * The rps_dev_flow_table structure contains a table of flow mappings.\n */\nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[0];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n/*\n * The rps_sock_flow_table contains mappings of flows to the last CPU\n * on which they were processed by the application (set in recvmsg).\n * Each entry is a 32bit value. Upper part is the high-order bits\n * of flow hash, lower part is CPU number.\n * rps_cpu_mask is used to partition the space, depending on number of\n * possible CPUs : rps_cpu_mask = roundup_pow_of_two(nr_cpu_ids) - 1\n * For example, if 64 CPUs are possible, rps_cpu_mask = 0x3f,\n * meaning we use 32-6=26 bits for the hash.\n */\nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[0] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t/* We only give a hint, preemption can change CPU under us */\n\t\tval |= raw_smp_processor_id();\n\n\t\tif (table->ents[index] != val)\n\t\t\ttable->ents[index] = val;\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif /* CONFIG_RPS */\n\n/* This structure contains an instance of an RX queue. */\nstruct netdev_rx_queue {\n#ifdef CONFIG_RPS\n\tstruct rps_map __rcu\t\t*rps_map;\n\tstruct rps_dev_flow_table __rcu\t*rps_flow_table;\n#endif\n\tstruct kobject\t\t\tkobj;\n\tstruct net_device\t\t*dev;\n} ____cacheline_aligned_in_smp;\n\n/*\n * RX queue sysfs structures and functions.\n */\nstruct rx_queue_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct netdev_rx_queue *queue, char *buf);\n\tssize_t (*store)(struct netdev_rx_queue *queue,\n\t\t\t const char *buf, size_t len);\n};\n\n#ifdef CONFIG_XPS\n/*\n * This structure holds an XPS map which can be of variable length.  The\n * map is an array of queues.\n */\nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[0];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n/*\n * This structure holds all XPS maps for device.  Maps are indexed by CPU.\n */\nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tstruct xps_map __rcu *cpu_map[0];\n};\n#define XPS_DEV_MAPS_SIZE(_tcs) (sizeof(struct xps_dev_maps) +\t\t\\\n\t(nr_cpu_ids * (_tcs) * sizeof(struct xps_map *)))\n#endif /* CONFIG_XPS */\n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n/* HW offloaded queuing disciplines txq count and offset maps */\nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n/*\n * This structure is to hold information about the device\n * configured to run FCoE protocol stack.\n */\nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n/* This structure holds a unique identifier to identify some\n * physical item (port for example) used by a netdevice.\n */\nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb);\n\nenum tc_setup_type {\n\tTC_SETUP_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n\tTC_SETUP_CLSMATCHALL,\n\tTC_SETUP_CLSBPF,\n};\n\n/* These structures hold the attributes of xdp state that are being passed\n * to the netdevice through the xdp op.\n */\nenum xdp_netdev_command {\n\t/* Set or clear a bpf program used in the earliest stages of packet\n\t * rx. The prog will have been loaded as BPF_PROG_TYPE_XDP. The callee\n\t * is responsible for calling bpf_prog_put on any old progs that are\n\t * stored. In case of error, the callee need not release the new prog\n\t * reference, but on success it takes ownership and must bpf_prog_put\n\t * when it is no longer used.\n\t */\n\tXDP_SETUP_PROG,\n\tXDP_SETUP_PROG_HW,\n\t/* Check if a bpf program is set on the device.  The callee should\n\t * set @prog_attached to one of XDP_ATTACHED_* values, note that \"true\"\n\t * is equivalent to XDP_ATTACHED_DRV.\n\t */\n\tXDP_QUERY_PROG,\n};\n\nstruct netlink_ext_ack;\n\nstruct netdev_xdp {\n\tenum xdp_netdev_command command;\n\tunion {\n\t\t/* XDP_SETUP_PROG */\n\t\tstruct {\n\t\t\tu32 flags;\n\t\t\tstruct bpf_prog *prog;\n\t\t\tstruct netlink_ext_ack *extack;\n\t\t};\n\t\t/* XDP_QUERY_PROG */\n\t\tstruct {\n\t\t\tu8 prog_attached;\n\t\t\tu32 prog_id;\n\t\t};\n\t};\n};\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstruct xfrmdev_ops {\n\tint\t(*xdo_dev_state_add) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_delete) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_free) (struct xfrm_state *x);\n\tbool\t(*xdo_dev_offload_ok) (struct sk_buff *skb,\n\t\t\t\t       struct xfrm_state *x);\n};\n#endif\n\n/*\n * This structure defines the management hooks for network devices.\n * The following hooks can be defined; unless noted otherwise, they are\n * optional and can be filled with a null pointer.\n *\n * int (*ndo_init)(struct net_device *dev);\n *     This function is called once when a network device is registered.\n *     The network device can use this for any late stage initialization\n *     or semantic validation. It can fail with an error code which will\n *     be propagated back to register_netdev.\n *\n * void (*ndo_uninit)(struct net_device *dev);\n *     This function is called when device is unregistered or when registration\n *     fails. It is not called if init fails.\n *\n * int (*ndo_open)(struct net_device *dev);\n *     This function is called when a network device transitions to the up\n *     state.\n *\n * int (*ndo_stop)(struct net_device *dev);\n *     This function is called when a network device transitions to the down\n *     state.\n *\n * netdev_tx_t (*ndo_start_xmit)(struct sk_buff *skb,\n *                               struct net_device *dev);\n *\tCalled when a packet needs to be transmitted.\n *\tReturns NETDEV_TX_OK.  Can return NETDEV_TX_BUSY, but you should stop\n *\tthe queue before that can happen; it's for obsolete devices and weird\n *\tcorner cases, but the stack really does a non-trivial amount\n *\tof useless work if you return NETDEV_TX_BUSY.\n *\tRequired; cannot be NULL.\n *\n * netdev_features_t (*ndo_features_check)(struct sk_buff *skb,\n *\t\t\t\t\t   struct net_device *dev\n *\t\t\t\t\t   netdev_features_t features);\n *\tCalled by core transmit path to determine if device is capable of\n *\tperforming offload operations on a given packet. This is to give\n *\tthe device an opportunity to implement any restrictions that cannot\n *\tbe otherwise expressed by feature flags. The check is called with\n *\tthe set of features that the stack has calculated and it returns\n *\tthose the driver believes to be appropriate.\n *\n * u16 (*ndo_select_queue)(struct net_device *dev, struct sk_buff *skb,\n *                         void *accel_priv, select_queue_fallback_t fallback);\n *\tCalled to decide which queue to use when device supports multiple\n *\ttransmit queues.\n *\n * void (*ndo_change_rx_flags)(struct net_device *dev, int flags);\n *\tThis function is called to allow device receiver to make\n *\tchanges to configuration when multicast or promiscuous is enabled.\n *\n * void (*ndo_set_rx_mode)(struct net_device *dev);\n *\tThis function is called device changes address list filtering.\n *\tIf driver handles unicast address filtering, it should set\n *\tIFF_UNICAST_FLT in its priv_flags.\n *\n * int (*ndo_set_mac_address)(struct net_device *dev, void *addr);\n *\tThis function  is called when the Media Access Control address\n *\tneeds to be changed. If this interface is not defined, the\n *\tMAC address can not be changed.\n *\n * int (*ndo_validate_addr)(struct net_device *dev);\n *\tTest if Media Access Control address is valid for the device.\n *\n * int (*ndo_do_ioctl)(struct net_device *dev, struct ifreq *ifr, int cmd);\n *\tCalled when a user requests an ioctl which can't be handled by\n *\tthe generic interface code. If not defined ioctls return\n *\tnot supported error code.\n *\n * int (*ndo_set_config)(struct net_device *dev, struct ifmap *map);\n *\tUsed to set network devices bus interface parameters. This interface\n *\tis retained for legacy reasons; new devices should use the bus\n *\tinterface (PCI) for low level management.\n *\n * int (*ndo_change_mtu)(struct net_device *dev, int new_mtu);\n *\tCalled when a user wants to change the Maximum Transfer Unit\n *\tof a device.\n *\n * void (*ndo_tx_timeout)(struct net_device *dev);\n *\tCallback used when the transmitter has not made any progress\n *\tfor dev->watchdog ticks.\n *\n * void (*ndo_get_stats64)(struct net_device *dev,\n *                         struct rtnl_link_stats64 *storage);\n * struct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n *\tCalled when a user wants to get the network device usage\n *\tstatistics. Drivers must do one of the following:\n *\t1. Define @ndo_get_stats64 to fill in a zero-initialised\n *\t   rtnl_link_stats64 structure passed by the caller.\n *\t2. Define @ndo_get_stats to update a net_device_stats structure\n *\t   (which should normally be dev->stats) and return a pointer to\n *\t   it. The structure may be changed asynchronously only if each\n *\t   field is written atomically.\n *\t3. Update dev->stats asynchronously and atomically, and define\n *\t   neither operation.\n *\n * bool (*ndo_has_offload_stats)(const struct net_device *dev, int attr_id)\n *\tReturn true if this device supports offload stats of this attr_id.\n *\n * int (*ndo_get_offload_stats)(int attr_id, const struct net_device *dev,\n *\tvoid *attr_data)\n *\tGet statistics for offload operations by attr_id. Write it into the\n *\tattr_data pointer.\n *\n * int (*ndo_vlan_rx_add_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is registered.\n *\n * int (*ndo_vlan_rx_kill_vid)(struct net_device *dev, __be16 proto, u16 vid);\n *\tIf device supports VLAN filtering this function is called when a\n *\tVLAN id is unregistered.\n *\n * void (*ndo_poll_controller)(struct net_device *dev);\n *\n *\tSR-IOV management functions.\n * int (*ndo_set_vf_mac)(struct net_device *dev, int vf, u8* mac);\n * int (*ndo_set_vf_vlan)(struct net_device *dev, int vf, u16 vlan,\n *\t\t\t  u8 qos, __be16 proto);\n * int (*ndo_set_vf_rate)(struct net_device *dev, int vf, int min_tx_rate,\n *\t\t\t  int max_tx_rate);\n * int (*ndo_set_vf_spoofchk)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_set_vf_trust)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_config)(struct net_device *dev,\n *\t\t\t    int vf, struct ifla_vf_info *ivf);\n * int (*ndo_set_vf_link_state)(struct net_device *dev, int vf, int link_state);\n * int (*ndo_set_vf_port)(struct net_device *dev, int vf,\n *\t\t\t  struct nlattr *port[]);\n *\n *      Enable or disable the VF ability to query its RSS Redirection Table and\n *      Hash Key. This is needed since on some devices VF share this information\n *      with PF and querying it may introduce a theoretical security risk.\n * int (*ndo_set_vf_rss_query_en)(struct net_device *dev, int vf, bool setting);\n * int (*ndo_get_vf_port)(struct net_device *dev, int vf, struct sk_buff *skb);\n * int (*ndo_setup_tc)(struct net_device *dev, enum tc_setup_type type,\n *\t\t       void *type_data);\n *\tCalled to setup any 'tc' scheduler, classifier or action on @dev.\n *\tThis is always called from the stack with the rtnl lock held and netif\n *\ttx queues stopped. This allows the netdevice to perform queue\n *\tmanagement safely.\n *\n *\tFiber Channel over Ethernet (FCoE) offload functions.\n * int (*ndo_fcoe_enable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to start using LLD for FCoE\n *\tso the underlying device can perform whatever needed configuration or\n *\tinitialization to support acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_disable)(struct net_device *dev);\n *\tCalled when the FCoE protocol stack wants to stop using LLD for FCoE\n *\tso the underlying device can perform whatever needed clean-ups to\n *\tstop supporting acceleration of FCoE traffic.\n *\n * int (*ndo_fcoe_ddp_setup)(struct net_device *dev, u16 xid,\n *\t\t\t     struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Initiator wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_ddp_done)(struct net_device *dev,  u16 xid);\n *\tCalled when the FCoE Initiator/Target is done with the DDPed I/O as\n *\tindicated by the FC exchange id 'xid', so the underlying device can\n *\tclean up and reuse resources for later DDP requests.\n *\n * int (*ndo_fcoe_ddp_target)(struct net_device *dev, u16 xid,\n *\t\t\t      struct scatterlist *sgl, unsigned int sgc);\n *\tCalled when the FCoE Target wants to initialize an I/O that\n *\tis a possible candidate for Direct Data Placement (DDP). The LLD can\n *\tperform necessary setup and returns 1 to indicate the device is set up\n *\tsuccessfully to perform DDP on this I/O, otherwise this returns 0.\n *\n * int (*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n *\t\t\t       struct netdev_fcoe_hbainfo *hbainfo);\n *\tCalled when the FCoE Protocol stack wants information on the underlying\n *\tdevice. This information is utilized by the FCoE protocol stack to\n *\tregister attributes with Fiber Channel management service as per the\n *\tFC-GS Fabric Device Management Information(FDMI) specification.\n *\n * int (*ndo_fcoe_get_wwn)(struct net_device *dev, u64 *wwn, int type);\n *\tCalled when the underlying device wants to override default World Wide\n *\tName (WWN) generation mechanism in FCoE protocol stack to pass its own\n *\tWorld Wide Port Name (WWPN) or World Wide Node Name (WWNN) to the FCoE\n *\tprotocol stack to use.\n *\n *\tRFS acceleration.\n * int (*ndo_rx_flow_steer)(struct net_device *dev, const struct sk_buff *skb,\n *\t\t\t    u16 rxq_index, u32 flow_id);\n *\tSet hardware filter for RFS.  rxq_index is the target queue index;\n *\tflow_id is a flow ID to be passed to rps_may_expire_flow() later.\n *\tReturn the filter ID on success, or a negative error code.\n *\n *\tSlave management functions (for bridge, bonding, etc).\n * int (*ndo_add_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to make another netdev an underling.\n *\n * int (*ndo_del_slave)(struct net_device *dev, struct net_device *slave_dev);\n *\tCalled to release previously enslaved netdev.\n *\n *      Feature/offload setting functions.\n * netdev_features_t (*ndo_fix_features)(struct net_device *dev,\n *\t\tnetdev_features_t features);\n *\tAdjusts the requested feature flags according to device-specific\n *\tconstraints, and returns the resulting flags. Must not modify\n *\tthe device state.\n *\n * int (*ndo_set_features)(struct net_device *dev, netdev_features_t features);\n *\tCalled to update device configuration to new features. Passed\n *\tfeature set might be less than what was returned by ndo_fix_features()).\n *\tMust return >0 or -errno if it changed dev->features itself.\n *\n * int (*ndo_fdb_add)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid, u16 flags)\n *\tAdds an FDB entry to dev for addr.\n * int (*ndo_fdb_del)(struct ndmsg *ndm, struct nlattr *tb[],\n *\t\t      struct net_device *dev,\n *\t\t      const unsigned char *addr, u16 vid)\n *\tDeletes the FDB entry from dev coresponding to addr.\n * int (*ndo_fdb_dump)(struct sk_buff *skb, struct netlink_callback *cb,\n *\t\t       struct net_device *dev, struct net_device *filter_dev,\n *\t\t       int *idx)\n *\tUsed to add FDB entries to dump requests. Implementers should add\n *\tentries to skb and update idx with the number of entries.\n *\n * int (*ndo_bridge_setlink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags)\n * int (*ndo_bridge_getlink)(struct sk_buff *skb, u32 pid, u32 seq,\n *\t\t\t     struct net_device *dev, u32 filter_mask,\n *\t\t\t     int nlflags)\n * int (*ndo_bridge_dellink)(struct net_device *dev, struct nlmsghdr *nlh,\n *\t\t\t     u16 flags);\n *\n * int (*ndo_change_carrier)(struct net_device *dev, bool new_carrier);\n *\tCalled to change device carrier. Soft-devices (like dummy, team, etc)\n *\twhich do not represent real hardware may define this to allow their\n *\tuserspace components to manage their virtual carrier state. Devices\n *\tthat determine carrier state from physical hardware properties (eg\n *\tnetwork cables) or protocol-dependent mechanisms (eg\n *\tUSB_CDC_NOTIFY_NETWORK_CONNECTION) should NOT implement this function.\n *\n * int (*ndo_get_phys_port_id)(struct net_device *dev,\n *\t\t\t       struct netdev_phys_item_id *ppid);\n *\tCalled to get ID of physical port of this device. If driver does\n *\tnot implement this, it is assumed that the hw is not able to have\n *\tmultiple net devices on single physical port.\n *\n * void (*ndo_udp_tunnel_add)(struct net_device *dev,\n *\t\t\t      struct udp_tunnel_info *ti);\n *\tCalled by UDP tunnel to notify a driver about the UDP port and socket\n *\taddress family that a UDP tunnel is listnening to. It is called only\n *\twhen a new port starts listening. The operation is protected by the\n *\tRTNL.\n *\n * void (*ndo_udp_tunnel_del)(struct net_device *dev,\n *\t\t\t      struct udp_tunnel_info *ti);\n *\tCalled by UDP tunnel to notify the driver about a UDP port and socket\n *\taddress family that the UDP tunnel is not listening to anymore. The\n *\toperation is protected by the RTNL.\n *\n * void* (*ndo_dfwd_add_station)(struct net_device *pdev,\n *\t\t\t\t struct net_device *dev)\n *\tCalled by upper layer devices to accelerate switching or other\n *\tstation functionality into hardware. 'pdev is the lowerdev\n *\tto use for the offload and 'dev' is the net device that will\n *\tback the offload. Returns a pointer to the private structure\n *\tthe upper layer will maintain.\n * void (*ndo_dfwd_del_station)(struct net_device *pdev, void *priv)\n *\tCalled by upper layer device to delete the station created\n *\tby 'ndo_dfwd_add_station'. 'pdev' is the net device backing\n *\tthe station and priv is the structure returned by the add\n *\toperation.\n * int (*ndo_set_tx_maxrate)(struct net_device *dev,\n *\t\t\t     int queue_index, u32 maxrate);\n *\tCalled when a user wants to set a max-rate limitation of specific\n *\tTX queue.\n * int (*ndo_get_iflink)(const struct net_device *dev);\n *\tCalled to get the iflink value of this device.\n * void (*ndo_change_proto_down)(struct net_device *dev,\n *\t\t\t\t bool proto_down);\n *\tThis function is used to pass protocol port error state information\n *\tto the switch driver. The switch driver can react to the proto_down\n *      by doing a phys down on the associated switch port.\n * int (*ndo_fill_metadata_dst)(struct net_device *dev, struct sk_buff *skb);\n *\tThis function is used to get egress tunnel information for given skb.\n *\tThis is useful for retrieving outer tunnel header parameters while\n *\tsampling packet.\n * void (*ndo_set_rx_headroom)(struct net_device *dev, int needed_headroom);\n *\tThis function is used to specify the headroom that the skb must\n *\tconsider when allocation skb during packet reception. Setting\n *\tappropriate rx headroom value allows avoiding skb head copy on\n *\tforward. Setting a negative value resets the rx headroom to the\n *\tdefault value.\n * int (*ndo_xdp)(struct net_device *dev, struct netdev_xdp *xdp);\n *\tThis function is used to set or query state related to XDP on the\n *\tnetdevice. See definition of enum xdp_netdev_command for details.\n * int (*ndo_xdp_xmit)(struct net_device *dev, struct xdp_buff *xdp);\n *\tThis function is used to submit a XDP packet for transmit on a\n *\tnetdevice.\n * void (*ndo_xdp_flush)(struct net_device *dev);\n *\tThis function is used to inform the driver to flush a particular\n *\txdp tx queue. Must be called on same CPU as xdp_xmit.\n */\nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    void *accel_priv,\n\t\t\t\t\t\t    select_queue_fallback_t fallback);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev);\n\n\tvoid\t\t\t(*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t   struct rtnl_link_stats64 *storage);\n\tbool\t\t\t(*ndo_has_offload_stats)(const struct net_device *dev, int attr_id);\n\tint\t\t\t(*ndo_get_offload_stats)(int attr_id,\n\t\t\t\t\t\t\t const struct net_device *dev,\n\t\t\t\t\t\t\t void *attr_data);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan,\n\t\t\t\t\t\t   u8 qos, __be16 proto);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_set_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, u64 guid,\n\t\t\t\t\t\t   int guid_type);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tenum tc_setup_type type,\n\t\t\t\t\t\tvoid *type_data);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct net_device *dev,\n\t\t\t\t\t\t       struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct net_device *dev,\n\t\t\t\t\t\t     struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint *idx);\n\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid\t\t\t(*ndo_udp_tunnel_add)(struct net_device *dev,\n\t\t\t\t\t\t      struct udp_tunnel_info *ti);\n\tvoid\t\t\t(*ndo_udp_tunnel_del)(struct net_device *dev,\n\t\t\t\t\t\t      struct udp_tunnel_info *ti);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tint\t\t\t(*ndo_get_lock_subclass)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_change_proto_down)(struct net_device *dev,\n\t\t\t\t\t\t\t bool proto_down);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n\tint\t\t\t(*ndo_xdp)(struct net_device *dev,\n\t\t\t\t\t   struct netdev_xdp *xdp);\n\tint\t\t\t(*ndo_xdp_xmit)(struct net_device *dev,\n\t\t\t\t\t\tstruct xdp_buff *xdp);\n\tvoid\t\t\t(*ndo_xdp_flush)(struct net_device *dev);\n};\n\n/**\n * enum net_device_priv_flags - &struct net_device priv_flags\n *\n * These are the &struct net_device, they are only set internally\n * by drivers and used in the kernel. These flags are invisible to\n * userspace; this means that the order of these flags can change\n * during any kernel release.\n *\n * You should have a pretty good reason to be extending these flags.\n *\n * @IFF_802_1Q_VLAN: 802.1Q VLAN device\n * @IFF_EBRIDGE: Ethernet bridging device\n * @IFF_BONDING: bonding master or slave\n * @IFF_ISATAP: ISATAP interface (RFC4214)\n * @IFF_WAN_HDLC: WAN HDLC device\n * @IFF_XMIT_DST_RELEASE: dev_hard_start_xmit() is allowed to\n *\trelease skb->dst\n * @IFF_DONT_BRIDGE: disallow bridging this ether dev\n * @IFF_DISABLE_NETPOLL: disable netpoll at run-time\n * @IFF_MACVLAN_PORT: device used as macvlan port\n * @IFF_BRIDGE_PORT: device used as bridge port\n * @IFF_OVS_DATAPATH: device used as Open vSwitch datapath port\n * @IFF_TX_SKB_SHARING: The interface supports sharing skbs on transmit\n * @IFF_UNICAST_FLT: Supports unicast filtering\n * @IFF_TEAM_PORT: device used as team port\n * @IFF_SUPP_NOFCS: device supports sending custom FCS\n * @IFF_LIVE_ADDR_CHANGE: device supports hardware address\n *\tchange when it's running\n * @IFF_MACVLAN: Macvlan device\n * @IFF_XMIT_DST_RELEASE_PERM: IFF_XMIT_DST_RELEASE not taking into account\n *\tunderlying stacked devices\n * @IFF_IPVLAN_MASTER: IPvlan master device\n * @IFF_IPVLAN_SLAVE: IPvlan slave device\n * @IFF_L3MDEV_MASTER: device is an L3 master device\n * @IFF_NO_QUEUE: device can run without qdisc attached\n * @IFF_OPENVSWITCH: device is a Open vSwitch master\n * @IFF_L3MDEV_SLAVE: device is enslaved to an L3 master device\n * @IFF_TEAM: device is a team device\n * @IFF_RXFH_CONFIGURED: device has had Rx Flow indirection table configured\n * @IFF_PHONY_HEADROOM: the headroom value is controlled by an external\n *\tentity (i.e. the master device for bridged veth)\n * @IFF_MACSEC: device is a MACsec device\n */\nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_IPVLAN_MASTER\t\t= 1<<18,\n\tIFF_IPVLAN_SLAVE\t\t= 1<<19,\n\tIFF_L3MDEV_MASTER\t\t= 1<<20,\n\tIFF_NO_QUEUE\t\t\t= 1<<21,\n\tIFF_OPENVSWITCH\t\t\t= 1<<22,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<23,\n\tIFF_TEAM\t\t\t= 1<<24,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<25,\n\tIFF_PHONY_HEADROOM\t\t= 1<<26,\n\tIFF_MACSEC\t\t\t= 1<<27,\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_IPVLAN_MASTER\t\tIFF_IPVLAN_MASTER\n#define IFF_IPVLAN_SLAVE\t\tIFF_IPVLAN_SLAVE\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n\n/**\n *\tstruct net_device - The DEVICE structure.\n *\n *\tActually, this whole structure is a big mistake.  It mixes I/O\n *\tdata with strictly \"high-level\" data, and it has to know about\n *\talmost every data structure used in the INET module.\n *\n *\t@name:\tThis is the first field of the \"visible\" part of this structure\n *\t\t(i.e. as seen by users in the \"Space.c\" file).  It is the name\n *\t\tof the interface.\n *\n *\t@name_hlist: \tDevice name hash chain, please keep it close to name[]\n *\t@ifalias:\tSNMP alias\n *\t@mem_end:\tShared memory end\n *\t@mem_start:\tShared memory start\n *\t@base_addr:\tDevice I/O address\n *\t@irq:\t\tDevice IRQ number\n *\n *\t@carrier_changes:\tStats to monitor carrier on<->off transitions\n *\n *\t@state:\t\tGeneric network queuing layer state, see netdev_state_t\n *\t@dev_list:\tThe global list of network devices\n *\t@napi_list:\tList entry used for polling NAPI devices\n *\t@unreg_list:\tList entry  when we are unregistering the\n *\t\t\tdevice; see the function unregister_netdev\n *\t@close_list:\tList entry used when we are closing the device\n *\t@ptype_all:     Device-specific packet handlers for all protocols\n *\t@ptype_specific: Device-specific, protocol-specific packet handlers\n *\n *\t@adj_list:\tDirectly linked devices, like slaves for bonding\n *\t@features:\tCurrently active device features\n *\t@hw_features:\tUser-changeable features\n *\n *\t@wanted_features:\tUser-requested features\n *\t@vlan_features:\t\tMask of features inheritable by VLAN devices\n *\n *\t@hw_enc_features:\tMask of features inherited by encapsulating devices\n *\t\t\t\tThis field indicates what encapsulation\n *\t\t\t\toffloads the hardware is capable of doing,\n *\t\t\t\tand drivers will need to set them appropriately.\n *\n *\t@mpls_features:\tMask of features inheritable by MPLS\n *\n *\t@ifindex:\tinterface index\n *\t@group:\t\tThe group the device belongs to\n *\n *\t@stats:\t\tStatistics struct, which was left as a legacy, use\n *\t\t\trtnl_link_stats64 instead\n *\n *\t@rx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@tx_dropped:\tDropped packets by core network,\n *\t\t\tdo not use this in drivers\n *\t@rx_nohandler:\tnohandler dropped packets by core network on\n *\t\t\tinactive devices, do not use this in drivers\n *\n *\t@wireless_handlers:\tList of functions to handle Wireless Extensions,\n *\t\t\t\tinstead of ioctl,\n *\t\t\t\tsee <net/iw_handler.h> for details.\n *\t@wireless_data:\tInstance data managed by the core of wireless extensions\n *\n *\t@netdev_ops:\tIncludes several pointers to callbacks,\n *\t\t\tif one wants to override the ndo_*() functions\n *\t@ethtool_ops:\tManagement operations\n *\t@ndisc_ops:\tIncludes callbacks for different IPv6 neighbour\n *\t\t\tdiscovery handling. Necessary for e.g. 6LoWPAN.\n *\t@header_ops:\tIncludes callbacks for creating,parsing,caching,etc\n *\t\t\tof Layer 2 headers.\n *\n *\t@flags:\t\tInterface flags (a la BSD)\n *\t@priv_flags:\tLike 'flags' but invisible to userspace,\n *\t\t\tsee if.h for the definitions\n *\t@gflags:\tGlobal flags ( kept as legacy )\n *\t@padded:\tHow much padding added by alloc_netdev()\n *\t@operstate:\tRFC2863 operstate\n *\t@link_mode:\tMapping policy to operstate\n *\t@if_port:\tSelectable AUI, TP, ...\n *\t@dma:\t\tDMA channel\n *\t@mtu:\t\tInterface MTU value\n *\t@min_mtu:\tInterface Minimum MTU value\n *\t@max_mtu:\tInterface Maximum MTU value\n *\t@type:\t\tInterface hardware type\n *\t@hard_header_len: Maximum hardware header length.\n *\t@min_header_len:  Minimum hardware header length\n *\n *\t@needed_headroom: Extra headroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed\n *\t@needed_tailroom: Extra tailroom the hardware may need, but not in all\n *\t\t\t  cases can this be guaranteed. Some cases also use\n *\t\t\t  LL_MAX_HEADER instead to allocate the skb\n *\n *\tinterface address info:\n *\n * \t@perm_addr:\t\tPermanent hw address\n * \t@addr_assign_type:\tHw address assignment type\n * \t@addr_len:\t\tHardware address length\n *\t@neigh_priv_len:\tUsed in neigh_alloc()\n * \t@dev_id:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same link layer address\n * \t@dev_port:\t\tUsed to differentiate devices that share\n * \t\t\t\tthe same function\n *\t@addr_list_lock:\tXXX: need comments on this one\n *\t@uc_promisc:\t\tCounter that indicates promiscuous mode\n *\t\t\t\thas been enabled due to the need to listen to\n *\t\t\t\tadditional unicast addresses in a device that\n *\t\t\t\tdoes not implement ndo_set_rx_mode()\n *\t@uc:\t\t\tunicast mac addresses\n *\t@mc:\t\t\tmulticast mac addresses\n *\t@dev_addrs:\t\tlist of device hw addresses\n *\t@queues_kset:\t\tGroup of all Kobjects in the Tx and RX queues\n *\t@promiscuity:\t\tNumber of times the NIC is told to work in\n *\t\t\t\tpromiscuous mode; if it becomes 0 the NIC will\n *\t\t\t\texit promiscuous mode\n *\t@allmulti:\t\tCounter, enables or disables allmulticast mode\n *\n *\t@vlan_info:\tVLAN info\n *\t@dsa_ptr:\tdsa specific data\n *\t@tipc_ptr:\tTIPC specific data\n *\t@atalk_ptr:\tAppleTalk link\n *\t@ip_ptr:\tIPv4 specific data\n *\t@dn_ptr:\tDECnet specific data\n *\t@ip6_ptr:\tIPv6 specific data\n *\t@ax25_ptr:\tAX.25 specific data\n *\t@ieee80211_ptr:\tIEEE 802.11 specific data, assign before registering\n *\n *\t@dev_addr:\tHw address (before bcast,\n *\t\t\tbecause most packets are unicast)\n *\n *\t@_rx:\t\t\tArray of RX queues\n *\t@num_rx_queues:\t\tNumber of RX queues\n *\t\t\t\tallocated at register_netdev() time\n *\t@real_num_rx_queues: \tNumber of RX queues currently active in device\n *\n *\t@rx_handler:\t\thandler for received packets\n *\t@rx_handler_data: \tXXX: need comments on this one\n *\t@ingress_queue:\t\tXXX: need comments on this one\n *\t@broadcast:\t\thw bcast address\n *\n *\t@rx_cpu_rmap:\tCPU reverse-mapping for RX completion interrupts,\n *\t\t\tindexed by RX queue number. Assigned by driver.\n *\t\t\tThis must only be set if the ndo_rx_flow_steer\n *\t\t\toperation is defined\n *\t@index_hlist:\t\tDevice index hash chain\n *\n *\t@_tx:\t\t\tArray of TX queues\n *\t@num_tx_queues:\t\tNumber of TX queues allocated at alloc_netdev_mq() time\n *\t@real_num_tx_queues: \tNumber of TX queues currently active in device\n *\t@qdisc:\t\t\tRoot qdisc from userspace point of view\n *\t@tx_queue_len:\t\tMax frames per queue allowed\n *\t@tx_global_lock: \tXXX: need comments on this one\n *\n *\t@xps_maps:\tXXX: need comments on this one\n *\n *\t@watchdog_timeo:\tRepresents the timeout that is used by\n *\t\t\t\tthe watchdog (see dev_watchdog())\n *\t@watchdog_timer:\tList of timers\n *\n *\t@pcpu_refcnt:\t\tNumber of references to this device\n *\t@todo_list:\t\tDelayed register/unregister\n *\t@link_watch_list:\tXXX: need comments on this one\n *\n *\t@reg_state:\t\tRegister/unregister state machine\n *\t@dismantle:\t\tDevice is going to be freed\n *\t@rtnl_link_state:\tThis enum represents the phases of creating\n *\t\t\t\ta new link\n *\n *\t@needs_free_netdev:\tShould unregister perform free_netdev?\n *\t@priv_destructor:\tCalled from unregister\n *\t@npinfo:\t\tXXX: need comments on this one\n * \t@nd_net:\t\tNetwork namespace this network device is inside\n *\n * \t@ml_priv:\tMid-layer private\n * \t@lstats:\tLoopback statistics\n * \t@tstats:\tTunnel statistics\n * \t@dstats:\tDummy statistics\n * \t@vstats:\tVirtual ethernet statistics\n *\n *\t@garp_port:\tGARP\n *\t@mrp_port:\tMRP\n *\n *\t@dev:\t\tClass/net/name entry\n *\t@sysfs_groups:\tSpace for optional device, statistics and wireless\n *\t\t\tsysfs groups\n *\n *\t@sysfs_rx_queue_group:\tSpace for optional per-rx queue attributes\n *\t@rtnl_link_ops:\tRtnl_link_ops\n *\n *\t@gso_max_size:\tMaximum size of generic segmentation offload\n *\t@gso_max_segs:\tMaximum number of segments that can be passed to the\n *\t\t\tNIC for GSO\n *\n *\t@dcbnl_ops:\tData Center Bridging netlink ops\n *\t@num_tc:\tNumber of traffic classes in the net device\n *\t@tc_to_txq:\tXXX: need comments on this one\n *\t@prio_tc_map:\tXXX: need comments on this one\n *\n *\t@fcoe_ddp_xid:\tMax exchange id for FCoE LRO by ddp\n *\n *\t@priomap:\tXXX: need comments on this one\n *\t@phydev:\tPhysical device may attach itself\n *\t\t\tfor hardware timestamping\n *\n *\t@qdisc_tx_busylock: lockdep class annotating Qdisc->busylock spinlock\n *\t@qdisc_running_key: lockdep class annotating Qdisc->running seqcount\n *\n *\t@proto_down:\tprotocol port state information can be sent to the\n *\t\t\tswitch driver and used to set the phys state of the\n *\t\t\tswitch port.\n *\n *\tFIXME: cleanup struct net_device such that network protocol info\n *\tmoves out.\n */\n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct hlist_node\tname_hlist;\n\tchar \t\t\t*ifalias;\n\t/*\n\t *\tI/O specific fields\n\t *\tFIXME: Merge these and struct ifmap into one\n\t */\n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\tint\t\t\tirq;\n\n\tatomic_t\t\tcarrier_changes;\n\n\t/*\n\t *\tSome hardware also needs these fields (state,dev_list,\n\t *\tnapi_list,unreg_list,close_list) but they are not\n\t *\tpart of the usual set specified in Space.c.\n\t */\n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\tnetdev_features_t\tgso_partial_features;\n\n\tint\t\t\tifindex;\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats;\n\n\tatomic_long_t\t\trx_dropped;\n\tatomic_long_t\t\ttx_dropped;\n\tatomic_long_t\t\trx_nohandler;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *wireless_handlers;\n\tstruct iw_public_data\t*wireless_data;\n#endif\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_SWITCHDEV\n\tconst struct switchdev_ops *switchdev_ops;\n#endif\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tconst struct ndisc_ops *ndisc_ops;\n#endif\n\n#ifdef CONFIG_XFRM\n\tconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned int\t\tflags;\n\tunsigned int\t\tpriv_flags;\n\n\tunsigned short\t\tgflags;\n\tunsigned short\t\tpadded;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\tunsigned int\t\tmtu;\n\tunsigned int\t\tmin_mtu;\n\tunsigned int\t\tmax_mtu;\n\tunsigned short\t\ttype;\n\tunsigned short\t\thard_header_len;\n\tunsigned char\t\tmin_header_len;\n\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\t/* Interface address info. */\n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tspinlock_t\t\taddr_list_lock;\n\tunsigned char\t\tname_assign_type;\n\tbool\t\t\tuc_promisc;\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\n\n\t/* Protocol-specific pointers */\n\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_switch_tree\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n\tvoid \t\t\t*atalk_ptr;\n\tstruct in_device __rcu\t*ip_ptr;\n\tstruct dn_dev __rcu     *dn_ptr;\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n\tvoid\t\t\t*ax25_ptr;\n\tstruct wireless_dev\t*ieee80211_ptr;\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n\n/*\n * Cache lines mostly used on receive path (including eth_type_trans())\n */\n\t/* Interface address info used in eth_type_trans() */\n\tunsigned char\t\t*dev_addr;\n\n#ifdef CONFIG_SYSFS\n\tstruct netdev_rx_queue\t*_rx;\n\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n#endif\n\n\tstruct bpf_prog __rcu\t*xdp_prog;\n\tunsigned long\t\tgro_flush_timeout;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *ingress_cl_list;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n/*\n * Cache lines mostly used on transmit path\n */\n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc\t\t*qdisc;\n#ifdef CONFIG_NET_SCHED\n\tDECLARE_HASHTABLE\t(qdisc_hash, 4);\n#endif\n\tunsigned int\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\tint\t\t\twatchdog_timeo;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps;\n#endif\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto __rcu  *egress_cl_list;\n#endif\n\n\t/* These may be needed for future network-power-down code. */\n\tstruct timer_list\twatchdog_timer;\n\n\tint __percpu\t\t*pcpu_refcnt;\n\tstruct list_head\ttodo_list;\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t/* completed register_netdevice */\n\t       NETREG_UNREGISTERING,\t/* called unregister_netdevice */\n\t       NETREG_UNREGISTERED,\t/* completed unregister todo */\n\t       NETREG_RELEASED,\t\t/* called free_netdev */\n\t       NETREG_DUMMY,\t\t/* dummy device for NAPI poll */\n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tbool needs_free_netdev;\n\tvoid (*priv_destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t/* mid-layer private */\n\tunion {\n\t\tvoid\t\t\t\t\t*ml_priv;\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t\tstruct pcpu_vstats __percpu\t\t*vstats;\n\t};\n\n#if IS_ENABLED(CONFIG_GARP)\n\tstruct garp_port __rcu\t*garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\n\tstruct mrp_port __rcu\t*mrp_port;\n#endif\n\n\tstruct device\t\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t/* for setting kernel sock attribute on TCP connection setup */\n#define GSO_MAX_SIZE\t\t65536\n\tunsigned int\t\tgso_max_size;\n#define GSO_MAX_SEGS\t\t65535\n\tu16\t\t\tgso_max_segs;\n\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\tu8\t\t\tnum_tc;\n\tstruct netdev_tc_txq\ttc_to_txq[TC_MAX_QUEUE];\n\tu8\t\t\tprio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device\t*phydev;\n\tstruct lock_class_key\t*qdisc_tx_busylock;\n\tstruct lock_class_key\t*qdisc_running_key;\n\tbool\t\t\tproto_down;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\nstatic inline bool netif_elide_gro(const struct net_device *dev)\n{\n\tif (!(dev->features & NETIF_F_GRO) || dev->xdp_prog)\n\t\treturn true;\n\treturn false;\n}\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq);\nvoid netdev_reset_tc(struct net_device *dev);\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset);\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc);\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n#define netdev_lockdep_set_classes(dev)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key qdisc_tx_busylock_key;\t\\\n\tstatic struct lock_class_key qdisc_running_key;\t\t\\\n\tstatic struct lock_class_key qdisc_xmit_lock_key;\t\\\n\tstatic struct lock_class_key dev_addr_list_lock_key;\t\\\n\tunsigned int i;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t(dev)->qdisc_tx_busylock = &qdisc_tx_busylock_key;\t\\\n\t(dev)->qdisc_running_key = &qdisc_running_key;\t\t\\\n\tlockdep_set_class(&(dev)->addr_list_lock,\t\t\\\n\t\t\t  &dev_addr_list_lock_key); \t\t\\\n\tfor (i = 0; i < (dev)->num_tx_queues; i++)\t\t\\\n\t\tlockdep_set_class(&(dev)->_tx[i]._xmit_lock,\t\\\n\t\t\t\t  &qdisc_xmit_lock_key);\t\\\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv);\n\n/* returns the headroom that the master device needs to take in account\n * when forwarding to this dev\n */\nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n/* set the device rx headroom to the dev's default */\nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\n/*\n * Net namespace inlines\n */\nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\n/**\n *\tnetdev_priv - access network device private data\n *\t@dev: network device\n *\n * Get network device private data\n */\nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n/* Set the sysfs physical device reference for the network logical device\n * if set prior to registration will cause a symlink during initialization.\n */\n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n/* Set the sysfs device type for the network logical device to allow\n * fine-grained identification of different network device types. For\n * example Ethernet, Wireless LAN, Bluetooth, WiMAX etc.\n */\n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n/* Default NAPI poll() weight\n * Device drivers are strongly advised to not use bigger value\n */\n#define NAPI_POLL_WEIGHT 64\n\n/**\n *\tnetif_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * netif_napi_add() must be used to initialize a NAPI context prior to calling\n * *any* of the other NAPI-related functions.\n */\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight);\n\n/**\n *\tnetif_tx_napi_add - initialize a NAPI context\n *\t@dev:  network device\n *\t@napi: NAPI context\n *\t@poll: polling function\n *\t@weight: default weight\n *\n * This variant of netif_napi_add() should be used from drivers using NAPI\n * to exclusively poll a TX queue.\n * This will avoid we add it into napi_hash[], thus polluting this hash table.\n */\nstatic inline void netif_tx_napi_add(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int),\n\t\t\t\t     int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add(dev, napi, poll, weight);\n}\n\n/**\n *  netif_napi_del - remove a NAPI context\n *  @napi: NAPI context\n *\n *  netif_napi_del() removes a NAPI context from the network device NAPI list\n */\nvoid netif_napi_del(struct napi_struct *napi);\n\nstruct napi_gro_cb {\n\t/* Virtual address of skb_shinfo(skb)->frags[0].page + offset. */\n\tvoid\t*frag0;\n\n\t/* Length of frag0. */\n\tunsigned int frag0_len;\n\n\t/* This indicates where we are processing relative to skb->data. */\n\tint\tdata_offset;\n\n\t/* This is non-zero if the packet cannot be merged with the new skb. */\n\tu16\tflush;\n\n\t/* Save the IP ID here and check when we get to the transport layer */\n\tu16\tflush_id;\n\n\t/* Number of segments aggregated. */\n\tu16\tcount;\n\n\t/* Start offset for remote checksum offload */\n\tu16\tgro_remcsum_start;\n\n\t/* jiffies when first packet was created/queued */\n\tunsigned long age;\n\n\t/* Used in ipv6_gro_receive() and foo-over-udp */\n\tu16\tproto;\n\n\t/* This is non-zero if the packet may be of the same flow. */\n\tu8\tsame_flow:1;\n\n\t/* Used in tunnel GRO receive */\n\tu8\tencap_mark:1;\n\n\t/* GRO checksum is valid */\n\tu8\tcsum_valid:1;\n\n\t/* Number of checksums via CHECKSUM_UNNECESSARY */\n\tu8\tcsum_cnt:3;\n\n\t/* Free the skb? */\n\tu8\tfree:2;\n#define NAPI_GRO_FREE\t\t  1\n#define NAPI_GRO_FREE_STOLEN_HEAD 2\n\n\t/* Used in foo-over-udp, set in udp[46]_gro_receive */\n\tu8\tis_ipv6:1;\n\n\t/* Used in GRE, set in fou/gue_gro_receive */\n\tu8\tis_fou:1;\n\n\t/* Used to determine if flush_id can be ignored */\n\tu8\tis_atomic:1;\n\n\t/* Number of gro_receive callbacks this packet already went through */\n\tu8 recursion_counter:4;\n\n\t/* 1 bit hole */\n\n\t/* used to support CHECKSUM_COMPLETE for tunneling protocols */\n\t__wsum\tcsum;\n\n\t/* used in skb_gro_receive() slow path */\n\tstruct sk_buff *last;\n};\n\n#define NAPI_GRO_CB(skb) ((struct napi_gro_cb *)(skb)->cb)\n\n#define GRO_RECURSION_LIMIT 15\nstatic inline int gro_recursion_inc_test(struct sk_buff *skb)\n{\n\treturn ++NAPI_GRO_CB(skb)->recursion_counter == GRO_RECURSION_LIMIT;\n}\n\ntypedef struct sk_buff **(*gro_receive_t)(struct sk_buff **, struct sk_buff *);\nstatic inline struct sk_buff **call_gro_receive(gro_receive_t cb,\n\t\t\t\t\t\tstruct sk_buff **head,\n\t\t\t\t\t\tstruct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(head, skb);\n}\n\ntypedef struct sk_buff **(*gro_receive_sk_t)(struct sock *, struct sk_buff **,\n\t\t\t\t\t     struct sk_buff *);\nstatic inline struct sk_buff **call_gro_receive_sk(gro_receive_sk_t cb,\n\t\t\t\t\t\t   struct sock *sk,\n\t\t\t\t\t\t   struct sk_buff **head,\n\t\t\t\t\t\t   struct sk_buff *skb)\n{\n\tif (unlikely(gro_recursion_inc_test(skb))) {\n\t\tNAPI_GRO_CB(skb)->flush |= 1;\n\t\treturn NULL;\n\t}\n\n\treturn cb(sk, head, skb);\n}\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t/* This is really htons(ether_type). */\n\tstruct net_device\t*dev;\t/* NULL is wildcarded here\t     */\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t**(*gro_receive)(struct sk_buff **head,\n\t\t\t\t\t\t struct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t/* This is really htons(ether_type). */\n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\n/* often modified stats are per-CPU, other are shared (netdev->stats) */\nstruct pcpu_sw_netstats {\n\tu64     rx_packets;\n\tu64     rx_bytes;\n\tu64     tx_packets;\n\tu64     tx_bytes;\n\tstruct u64_stats_sync   syncp;\n};\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n/* netdevice notifier chain. Please remember to update the rtnetlink\n * notification exclusion list in rtnetlink_event() when adding new\n * types.\n */\n#define NETDEV_UP\t0x0001\t/* For now you can't veto a device up/down */\n#define NETDEV_DOWN\t0x0002\n#define NETDEV_REBOOT\t0x0003\t/* Tell a protocol stack a network interface\n\t\t\t\t   detected a hardware crash and restarted\n\t\t\t\t   - we can use this eg to kick tcp sessions\n\t\t\t\t   once done */\n#define NETDEV_CHANGE\t0x0004\t/* Notify device state change */\n#define NETDEV_REGISTER 0x0005\n#define NETDEV_UNREGISTER\t0x0006\n#define NETDEV_CHANGEMTU\t0x0007 /* notify after mtu change happened */\n#define NETDEV_CHANGEADDR\t0x0008\n#define NETDEV_GOING_DOWN\t0x0009\n#define NETDEV_CHANGENAME\t0x000A\n#define NETDEV_FEAT_CHANGE\t0x000B\n#define NETDEV_BONDING_FAILOVER 0x000C\n#define NETDEV_PRE_UP\t\t0x000D\n#define NETDEV_PRE_TYPE_CHANGE\t0x000E\n#define NETDEV_POST_TYPE_CHANGE\t0x000F\n#define NETDEV_POST_INIT\t0x0010\n#define NETDEV_UNREGISTER_FINAL 0x0011\n#define NETDEV_RELEASE\t\t0x0012\n#define NETDEV_NOTIFY_PEERS\t0x0013\n#define NETDEV_JOIN\t\t0x0014\n#define NETDEV_CHANGEUPPER\t0x0015\n#define NETDEV_RESEND_IGMP\t0x0016\n#define NETDEV_PRECHANGEMTU\t0x0017 /* notify before mtu change happened */\n#define NETDEV_CHANGEINFODATA\t0x0018\n#define NETDEV_BONDING_INFO\t0x0019\n#define NETDEV_PRECHANGEUPPER\t0x001A\n#define NETDEV_CHANGELOWERSTATE\t0x001B\n#define NETDEV_UDP_TUNNEL_PUSH_INFO\t0x001C\n#define NETDEV_UDP_TUNNEL_DROP_INFO\t0x001D\n#define NETDEV_CHANGE_TX_QUEUE_LEN\t0x001E\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\n\nstruct netdev_notifier_info {\n\tstruct net_device *dev;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct net_device *upper_dev; /* new upper dev */\n\tbool master; /* is upper dev master */\n\tbool linking; /* is the notification for link or unlink */\n\tvoid *upper_info; /* upper dev info */\n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tvoid *lower_state_info; /* is lower dev state */\n};\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\n\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t/* Device list lock */\n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nunsigned long netdev_boot_base(const char *prefix, int unit);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev);\nvoid dev_close(struct net_device *dev);\nvoid dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nint dev_queue_xmit(struct sk_buff *skb);\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv);\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nvoid synchronize_net(void);\nint init_dummy_netdev(struct net_device *dev);\n\nDECLARE_PER_CPU(int, xmit_recursion);\n#define XMIT_RECURSION_LIMIT\t10\n\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(xmit_recursion);\n}\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id);\nint netdev_get_name(struct net *net, char *name, int ifindex);\nint dev_restart(struct net_device *dev);\nint skb_gro_receive(struct sk_buff **head, struct sk_buff *skb);\n\nstatic inline unsigned int skb_gro_offset(const struct sk_buff *skb)\n{\n\treturn NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline unsigned int skb_gro_len(const struct sk_buff *skb)\n{\n\treturn skb->len - NAPI_GRO_CB(skb)->data_offset;\n}\n\nstatic inline void skb_gro_pull(struct sk_buff *skb, unsigned int len)\n{\n\tNAPI_GRO_CB(skb)->data_offset += len;\n}\n\nstatic inline void *skb_gro_header_fast(struct sk_buff *skb,\n\t\t\t\t\tunsigned int offset)\n{\n\treturn NAPI_GRO_CB(skb)->frag0 + offset;\n}\n\nstatic inline int skb_gro_header_hard(struct sk_buff *skb, unsigned int hlen)\n{\n\treturn NAPI_GRO_CB(skb)->frag0_len < hlen;\n}\n\nstatic inline void skb_gro_frag0_invalidate(struct sk_buff *skb)\n{\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n}\n\nstatic inline void *skb_gro_header_slow(struct sk_buff *skb, unsigned int hlen,\n\t\t\t\t\tunsigned int offset)\n{\n\tif (!pskb_may_pull(skb, hlen))\n\t\treturn NULL;\n\n\tskb_gro_frag0_invalidate(skb);\n\treturn skb->data + offset;\n}\n\nstatic inline void *skb_gro_network_header(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->frag0 ?: skb->data) +\n\t       skb_network_offset(skb);\n}\n\nstatic inline void skb_gro_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t\tconst void *start, unsigned int len)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid)\n\t\tNAPI_GRO_CB(skb)->csum = csum_sub(NAPI_GRO_CB(skb)->csum,\n\t\t\t\t\t\t  csum_partial(start, len, 0));\n}\n\n/* GRO checksum functions. These are logical equivalents of the normal\n * checksum functions (in skbuff.h) except that they operate on the GRO\n * offsets and fields in sk_buff.\n */\n\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb);\n\nstatic inline bool skb_at_gro_remcsum_start(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->gro_remcsum_start == skb_gro_offset(skb));\n}\n\nstatic inline bool __skb_gro_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t      bool zero_okay,\n\t\t\t\t\t\t      __sum16 check)\n{\n\treturn ((skb->ip_summed != CHECKSUM_PARTIAL ||\n\t\tskb_checksum_start_offset(skb) <\n\t\t skb_gro_offset(skb)) &&\n\t\t!skb_at_gro_remcsum_start(skb) &&\n\t\tNAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t(!zero_okay || check));\n}\n\nstatic inline __sum16 __skb_gro_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t\t   __wsum psum)\n{\n\tif (NAPI_GRO_CB(skb)->csum_valid &&\n\t    !csum_fold(csum_add(psum, NAPI_GRO_CB(skb)->csum)))\n\t\treturn 0;\n\n\tNAPI_GRO_CB(skb)->csum = psum;\n\n\treturn __skb_gro_checksum_complete(skb);\n}\n\nstatic inline void skb_gro_incr_csum_unnecessary(struct sk_buff *skb)\n{\n\tif (NAPI_GRO_CB(skb)->csum_cnt > 0) {\n\t\t/* Consume a checksum from CHECKSUM_UNNECESSARY */\n\t\tNAPI_GRO_CB(skb)->csum_cnt--;\n\t} else {\n\t\t/* Update skb for CHECKSUM_UNNECESSARY and csum_level when we\n\t\t * verified a new top level checksum or an encapsulated one\n\t\t * during GRO. This saves work if we fallback to normal path.\n\t\t */\n\t\t__skb_incr_checksum_unnecessary(skb);\n\t}\n}\n\n#define __skb_gro_checksum_validate(skb, proto, zero_okay, check,\t\\\n\t\t\t\t    compute_pseudo)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_gro_checksum_validate_complete(skb,\t\\\n\t\t\t\tcompute_pseudo(skb, proto));\t\t\\\n\tif (!__ret)\t\t\t\t\t\t\t\\\n\t\tskb_gro_incr_csum_unnecessary(skb);\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_gro_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, false, 0, compute_pseudo)\n\n#define skb_gro_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t     compute_pseudo)\t\t\\\n\t__skb_gro_checksum_validate(skb, proto, true, check, compute_pseudo)\n\n#define skb_gro_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_gro_checksum_validate(skb, 0, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_gro_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (NAPI_GRO_CB(skb)->csum_cnt == 0 &&\n\t\t!NAPI_GRO_CB(skb)->csum_valid);\n}\n\nstatic inline void __skb_gro_checksum_convert(struct sk_buff *skb,\n\t\t\t\t\t      __sum16 check, __wsum pseudo)\n{\n\tNAPI_GRO_CB(skb)->csum = ~pseudo;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n}\n\n#define skb_gro_checksum_try_convert(skb, proto, check, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_gro_checksum_convert_check(skb))\t\t\t\\\n\t\t__skb_gro_checksum_convert(skb, check,\t\t\t\\\n\t\t\t\t\t   compute_pseudo(skb, proto));\t\\\n} while (0)\n\nstruct gro_remcsum {\n\tint offset;\n\t__wsum delta;\n};\n\nstatic inline void skb_gro_remcsum_init(struct gro_remcsum *grc)\n{\n\tgrc->offset = 0;\n\tgrc->delta = 0;\n}\n\nstatic inline void *skb_gro_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t    unsigned int off, size_t hdrlen,\n\t\t\t\t\t    int start, int offset,\n\t\t\t\t\t    struct gro_remcsum *grc,\n\t\t\t\t\t    bool nopartial)\n{\n\t__wsum delta;\n\tsize_t plen = hdrlen + max_t(size_t, offset + sizeof(u16), start);\n\n\tBUG_ON(!NAPI_GRO_CB(skb)->csum_valid);\n\n\tif (!nopartial) {\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = off + hdrlen + start;\n\t\treturn ptr;\n\t}\n\n\tptr = skb_gro_header_fast(skb, off);\n\tif (skb_gro_header_hard(skb, off + plen)) {\n\t\tptr = skb_gro_header_slow(skb, off + plen, off);\n\t\tif (!ptr)\n\t\t\treturn NULL;\n\t}\n\n\tdelta = remcsum_adjust(ptr + hdrlen, NAPI_GRO_CB(skb)->csum,\n\t\t\t       start, offset);\n\n\t/* Adjust skb->csum since we changed the packet */\n\tNAPI_GRO_CB(skb)->csum = csum_add(NAPI_GRO_CB(skb)->csum, delta);\n\n\tgrc->offset = off + hdrlen + offset;\n\tgrc->delta = delta;\n\n\treturn ptr;\n}\n\nstatic inline void skb_gro_remcsum_cleanup(struct sk_buff *skb,\n\t\t\t\t\t   struct gro_remcsum *grc)\n{\n\tvoid *ptr;\n\tsize_t plen = grc->offset + sizeof(u16);\n\n\tif (!grc->delta)\n\t\treturn;\n\n\tptr = skb_gro_header_fast(skb, grc->offset);\n\tif (skb_gro_header_hard(skb, grc->offset + sizeof(u16))) {\n\t\tptr = skb_gro_header_slow(skb, plen, grc->offset);\n\t\tif (!ptr)\n\t\t\treturn;\n\t}\n\n\tremcsum_unadjust((__sum16 *)ptr, grc->delta);\n}\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff **pp, int flush)\n{\n\tif (PTR_ERR(pp) != -EINPROGRESS)\n\t\tNAPI_GRO_CB(skb)->flush |= flush;\n}\n#else\nstatic inline void skb_gro_flush_final(struct sk_buff *skb, struct sk_buff **pp, int flush)\n{\n\tNAPI_GRO_CB(skb)->flush |= flush;\n}\n#endif\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\n/* ll_header must have at least hard_header_len allocated */\nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\tif (len < dev->min_header_len)\n\t\treturn false;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\ntypedef int gifconf_func_t(struct net_device * dev, char __user * bufptr, int len);\nint register_gifconf(unsigned int family, gifconf_func_t *gifconf);\nstatic inline int unregister_gifconf(unsigned int family)\n{\n\treturn register_gifconf(family, NULL);\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n#define FLOW_LIMIT_HISTORY\t(1 << 7)  /* must be ^2 and !overflow buckets */\nstruct sd_flow_limit {\n\tu64\t\t\tcount;\n\tunsigned int\t\tnum_buckets;\n\tunsigned int\t\thistory_head;\n\tu16\t\t\thistory[FLOW_LIMIT_HISTORY];\n\tu8\t\t\tbuckets[];\n};\n\nextern int netdev_flow_limit_table_len;\n#endif /* CONFIG_NET_FLOW_LIMIT */\n\n/*\n * Incoming packets are placed on per-CPU queues\n */\nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t/* stats */\n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n\tunsigned int\t\treceived_rps;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n\n#ifdef CONFIG_RPS\n\t/* input_queue_head should be written by cpu owning this struct,\n\t * and only read by other cpus. Worth using a cache line.\n\t */\n\tunsigned int\t\tinput_queue_head ____cacheline_aligned_in_smp;\n\n\t/* Elements below can be accessed between CPUs for RPS/RFS */\n\tcall_single_data_t\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic __always_inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_start_queue - allow transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n */\nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n/**\n *\tnetif_wake_queue - restart transmit\n *\t@dev: network device\n *\n *\tAllow upper layers to call the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are available.\n */\nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic __always_inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_stop_queue - stop transmitted packets\n *\t@dev: network device\n *\n *\tStop upper layers calling the device hard_start_xmit routine.\n *\tUsed for flow control when transmit resources are unavailable.\n */\nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n/**\n *\tnetif_queue_stopped - test if transmit queue is flowblocked\n *\t@dev: network device\n *\n *\tTest if transmit queue on device is currently unable to send.\n */\nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n/**\n *\tnetdev_txq_bql_enqueue_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their ndo_start_xmit(),\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n/**\n *\tnetdev_txq_bql_complete_prefetchw - prefetch bql data for write\n *\t@dev_queue: pointer to transmit queue\n *\n * BQL enabled drivers might use this helper in their TX completion path,\n * to give appropriate hint to the CPU.\n */\nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t/*\n\t * The XOFF flag must be set before checking the dql_avail below,\n\t * because in netdev_tx_completed_queue we update the dql_completed\n\t * before checking the XOFF flag.\n\t */\n\tsmp_mb();\n\n\t/* check again in case another CPU has just made room avail */\n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n/**\n * \tnetdev_sent_queue - report the number of bytes queued to hardware\n * \t@dev: network device\n * \t@bytes: number of bytes queued to the hardware device queue\n *\n * \tReport the number of bytes queued for sending/completion to the network\n * \tdevice hardware queue. @bytes should be a good approximation and should\n * \texactly match netdev_completed_queue() @bytes\n */\nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t/*\n\t * Without the memory barrier there is a small possiblity that\n\t * netdev_tx_sent_queue will miss the update and cause the queue to\n\t * be stopped forever\n\t */\n\tsmp_mb();\n\n\tif (dql_avail(&dev_queue->dql) < 0)\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n/**\n * \tnetdev_completed_queue - report bytes and packets completed by device\n * \t@dev: network device\n * \t@pkts: actual number of packets sent over the medium\n * \t@bytes: actual number of bytes sent over the medium\n *\n * \tReport the number of bytes and packets transmitted by the network device\n * \thardware queue over the physical medium, @bytes must exactly match the\n * \t@bytes amount passed to netdev_sent_queue()\n */\nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n/**\n * \tnetdev_reset_queue - reset the packets and bytes count of a network device\n * \t@dev_queue: network device\n *\n * \tReset the bytes and packet count of a network device and clear the\n * \tsoftware flow control OFF bit for this network device\n */\nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n/**\n * \tnetdev_cap_txqueue - check if selected tx queue exceeds device queues\n * \t@dev: network device\n * \t@queue_index: given tx queue index\n *\n * \tReturns 0 if given tx queue index >= number of device tx queues,\n * \totherwise returns the originally passed tx queue index.\n */\nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n/**\n *\tnetif_running - test if up\n *\t@dev: network device\n *\n *\tTest if the device has been brought up.\n */\nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n/*\n * Routines to manage the subqueues on a device.  We only need start,\n * stop, and a check if it's stopped.  All other device management is\n * done at the overall netdevice level.\n * Also test the device if we're multiqueue.\n */\n\n/**\n *\tnetif_start_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Start individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n/**\n *\tnetif_stop_subqueue - stop sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Stop individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n/**\n *\tnetif_subqueue_stopped - test status of subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Check individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n/**\n *\tnetif_wake_subqueue - allow sending packets on subqueue\n *\t@dev: network device\n *\t@queue_index: sub queue index\n *\n * Resume individual transmit queue of a device with multiple transmit queues.\n */\nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_wake_queue(txq);\n}\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n#endif\n\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues);\n\n/*\n * Returns a Tx hash for the given packet when dev->real_num_tx_queues is used\n * as a distribution range limit for the returned value.\n */\nstatic inline u16 skb_tx_hash(const struct net_device *dev,\n\t\t\t      struct sk_buff *skb)\n{\n\treturn __skb_tx_hash(dev, skb, dev->real_num_tx_queues);\n}\n\n/**\n *\tnetif_is_multiqueue - test if device has multiple transmit queues\n *\t@dev: network device\n *\n * Check if device has multiple transmit queues\n */\nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxq)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_SYSFS\nstatic inline unsigned int get_netdev_rx_queue_index(\n\t\tstruct netdev_rx_queue *queue)\n{\n\tstruct net_device *dev = queue->dev;\n\tint index = queue - dev->_rx;\n\n\tBUG_ON(index >= dev->num_rx_queues);\n\treturn index;\n}\n#endif\n\n#define DEFAULT_MAX_NUM_RSS_QUEUES\t(8)\nint netif_get_num_default_rss_queues(void);\n\nenum skb_free_reason {\n\tSKB_REASON_CONSUMED,\n\tSKB_REASON_DROPPED,\n};\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason);\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason);\n\n/*\n * It is not allowed to call kfree_skb() or consume_skb() from hardware\n * interrupt context or with hardware interrupts being disabled.\n * (in_irq() || irqs_disabled())\n *\n * We provide four helpers that can be used in following contexts :\n *\n * dev_kfree_skb_irq(skb) when caller drops a packet from irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_irq(skb) when caller consumes a packet from irq context.\n *  Typically used in place of consume_skb(skb) in TX completion path\n *\n * dev_kfree_skb_any(skb) when caller doesn't know its current irq context,\n *  replacing kfree_skb(skb)\n *\n * dev_consume_skb_any(skb) when caller doesn't know its current irq context,\n *  and consumed a packet. Used in place of consume_skb(skb)\n */\nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_irq(skb, SKB_REASON_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_DROPPED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\t__dev_kfree_skb_any(skb, SKB_REASON_CONSUMED);\n}\n\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog);\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb);\nint netif_rx(struct sk_buff *skb);\nint netif_rx_ni(struct sk_buff *skb);\nint netif_receive_skb(struct sk_buff *skb);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nbool netdev_is_rx_handler_busy(struct net_device *dev);\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nint dev_ioctl(struct net *net, unsigned int cmd, void __user *);\nint dev_ethtool(struct net *net, struct ifreq *);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *, unsigned int flags);\nint dev_change_flags(struct net_device *, unsigned int);\nvoid __dev_notify_flags(struct net_device *, unsigned int old_flags,\n\t\t\tunsigned int gchanges);\nint dev_change_name(struct net_device *, const char *);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_change_net_namespace(struct net_device *, struct net *, const char *);\nint __dev_set_mtu(struct net_device *, int);\nint dev_set_mtu(struct net_device *, int);\nvoid dev_set_group(struct net_device *, int);\nint dev_set_mac_address(struct net_device *, struct sockaddr *);\nint dev_change_carrier(struct net_device *, bool new_carrier);\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid);\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len);\nint dev_change_proto_down(struct net_device *dev, bool proto_down);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\n\ntypedef int (*xdp_op_t)(struct net_device *dev, struct netdev_xdp *xdp);\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, u32 flags);\nu8 __dev_xdp_attached(struct net_device *dev, xdp_op_t xdp_op, u32 *prog_id);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(const struct net_device *dev,\n\t\t\tconst struct sk_buff *skb);\n\nstatic __always_inline int ____dev_forward_skb(struct net_device *dev,\n\t\t\t\t\t       struct sk_buff *skb)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!is_skb_forwardable(dev, skb))) {\n\t\tatomic_long_inc(&dev->rx_dropped);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, true);\n\tskb->priority = 0;\n\treturn 0;\n}\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev);\n\nextern int\t\tnetdev_budget;\nextern unsigned int\tnetdev_budget_usecs;\n\n/* Called by rtnetlink.c:rtnl_unlock() */\nvoid netdev_run_todo(void);\n\n/**\n *\tdev_put - release reference to device\n *\t@dev: network device\n *\n * Release reference to device to allow it to be freed.\n */\nstatic inline void dev_put(struct net_device *dev)\n{\n\tthis_cpu_dec(*dev->pcpu_refcnt);\n}\n\n/**\n *\tdev_hold - get reference to device\n *\t@dev: network device\n *\n * Hold reference to device to keep it from being freed.\n */\nstatic inline void dev_hold(struct net_device *dev)\n{\n\tthis_cpu_inc(*dev->pcpu_refcnt);\n}\n\n/* Carrier loss detection, dial on demand. The functions netif_carrier_on\n * and _off may be called from IRQ context, but it is caller\n * who is responsible for serialization of these calls.\n *\n * The name carrier is inappropriate, these functions should really be\n * called netif_lowerlayer_*() because they represent the state of any\n * kind of lower layer not just hardware media.\n */\n\nvoid linkwatch_init_dev(struct net_device *dev);\nvoid linkwatch_fire_event(struct net_device *dev);\nvoid linkwatch_forget_dev(struct net_device *dev);\n\n/**\n *\tnetif_carrier_ok - test if carrier present\n *\t@dev: network device\n *\n * Check if carrier is present on device\n */\nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\n\nvoid netif_carrier_off(struct net_device *dev);\n\n/**\n *\tnetif_dormant_on - mark device as dormant.\n *\t@dev: network device\n *\n * Mark device as dormant (as per RFC2863).\n *\n * The dormant state indicates that the relevant interface is not\n * actually in a condition to pass packets (i.e., it is not 'up') but is\n * in a \"pending\" state, waiting for some external event.  For \"on-\n * demand\" interfaces, this new state identifies the situation where the\n * interface is waiting for events to place it in the up state.\n */\nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant_off - set device as not dormant.\n *\t@dev: network device\n *\n * Device is not in dormant state.\n */\nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n/**\n *\tnetif_dormant - test if device is dormant\n *\t@dev: network device\n *\n * Check if device is dormant.\n */\nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n/**\n *\tnetif_oper_up - test if device is operational\n *\t@dev: network device\n *\n * Check if carrier is operational\n */\nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN /* backward compat */);\n}\n\n/**\n *\tnetif_device_present - is device available or removed\n *\t@dev: network device\n *\n * Check if device has not been removed from system.\n */\nstatic inline bool netif_device_present(struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n/*\n * Network interface message level settings\n */\n\nenum {\n\tNETIF_MSG_DRV\t\t= 0x0001,\n\tNETIF_MSG_PROBE\t\t= 0x0002,\n\tNETIF_MSG_LINK\t\t= 0x0004,\n\tNETIF_MSG_TIMER\t\t= 0x0008,\n\tNETIF_MSG_IFDOWN\t= 0x0010,\n\tNETIF_MSG_IFUP\t\t= 0x0020,\n\tNETIF_MSG_RX_ERR\t= 0x0040,\n\tNETIF_MSG_TX_ERR\t= 0x0080,\n\tNETIF_MSG_TX_QUEUED\t= 0x0100,\n\tNETIF_MSG_INTR\t\t= 0x0200,\n\tNETIF_MSG_TX_DONE\t= 0x0400,\n\tNETIF_MSG_RX_STATUS\t= 0x0800,\n\tNETIF_MSG_PKTDATA\t= 0x1000,\n\tNETIF_MSG_HW\t\t= 0x2000,\n\tNETIF_MSG_WOL\t\t= 0x4000,\n};\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t/* use default */\n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t/* no output */\n\t\treturn 0;\n\t/* set low N bits */\n\treturn (1 << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = cpu;\n}\n\nstatic inline bool __netif_tx_acquire(struct netdev_queue *txq)\n{\n\t__acquire(&txq->_xmit_lock);\n\treturn true;\n}\n\nstatic inline void __netif_tx_release(struct netdev_queue *txq)\n{\n\t__release(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\ttxq->xmit_lock_owner = smp_processor_id();\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\tif (likely(ok))\n\t\ttxq->xmit_lock_owner = smp_processor_id();\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\ttxq->xmit_lock_owner = -1;\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\ttxq->trans_start = jiffies;\n}\n\n/* legacy drivers only, netdev_start_xmit() sets txq->trans_start */\nstatic inline void netif_trans_update(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\n\tif (txq->trans_start != jiffies)\n\t\ttxq->trans_start = jiffies;\n}\n\n/**\n *\tnetif_tx_lock - grab network device transmit lock\n *\t@dev: network device\n *\n * Get network device transmit lock\n */\nstatic inline void netif_tx_lock(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tspin_lock(&dev->tx_global_lock);\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* We are the only thread of execution doing a\n\t\t * freeze, but we have to grab the _xmit_lock in\n\t\t * order to synchronize with threads which are in\n\t\t * the ->hard_start_xmit() handler and already\n\t\t * checked the frozen bit.\n\t\t */\n\t\t__netif_tx_lock(txq, cpu);\n\t\tset_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nstatic inline void netif_tx_unlock(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t/* No need to grab the _xmit_lock here.  If the\n\t\t * queue is not stopped for another reason, we\n\t\t * force a schedule.\n\t\t */\n\t\tclear_bit(__QUEUE_STATE_FROZEN, &txq->state);\n\t\tnetif_schedule_queue(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n}\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_acquire(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\t__netif_tx_acquire(txq))\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_release(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tspin_lock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_lock_nested(struct net_device *dev)\n{\n\tint subclass = SINGLE_DEPTH_NESTING;\n\n\tif (dev->netdev_ops->ndo_get_lock_subclass)\n\t\tsubclass = dev->netdev_ops->ndo_get_lock_subclass(dev);\n\n\tspin_lock_nested(&dev->addr_list_lock, subclass);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tspin_lock_bh(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n/*\n * dev_addrs walker. Should be used only for read access. Call with\n * rcu_read_lock held.\n */\n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n/* These functions live elsewhere (drivers/net/net_init.c, but related) */\n\nvoid ether_setup(struct net_device *dev);\n\n/* Support for loadable net-drivers */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\nint dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t       const char *name);\n\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\n/* General hardware address lists handling functions */\nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n/* Functions used for device addresses handling */\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nvoid dev_addr_flush(struct net_device *dev);\nint dev_addr_init(struct net_device *dev);\n\n/* Functions used for unicast addresses handling */\nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n/**\n *  __dev_uc_sync - Synchonize device's unicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n/**\n *  __dev_uc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_uc_sync().\n */\nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n/* Functions used for multicast addresses handling */\nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n/**\n *  __dev_mc_sync - Synchonize device's multicast list\n *  @dev:  device to sync\n *  @sync: function to call if address should be added\n *  @unsync: function to call if address should be removed\n *\n *  Add newly added addresses to the interface, and release\n *  addresses that have been deleted.\n */\nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n/**\n *  __dev_mc_unsync - Remove synchronized addresses from device\n *  @dev:  device to sync\n *  @unsync: function to call if address should be removed\n *\n *  Remove all addresses that were added to the device by dev_mc_sync().\n */\nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n/* Functions used for secondary unicast and multicast support */\nvoid dev_set_rx_mode(struct net_device *dev);\nvoid __dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n/* Load a device via the kmod */\nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tnetdev_tstamp_prequeue;\nextern int\t\tweight_p;\nextern int\t\tdev_weight_rx_bias;\nextern int\t\tdev_weight_tx_bias;\nextern int\t\tdev_rx_weight;\nextern int\t\tdev_tx_weight;\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n/* iterate through upper list, must be called under RCU read lock */\n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *upper_dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data);\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev);\n\nbool netdev_has_any_upper_dev(struct net_device *dev);\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nstruct net_device *netdev_all_lower_get_next(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter);\nstruct net_device *netdev_all_lower_get_next_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter);\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\tvoid *data),\n\t\t\t      void *data);\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n/* RSS keys are 40 or 52 bytes long */\n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint dev_get_nest_level(struct net_device *dev);\nint skb_checksum_help(struct sk_buff *skb);\nint skb_crc32c_csum_help(struct sk_buff *skb);\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features);\n\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path);\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info; /* must be first */\n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\nstatic inline\nstruct sk_buff *skb_gso_segment(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn __skb_gso_segment(skb, features, true);\n}\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t/* Assume this is an IP checksum (not SCTP CRC) */\n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t/* Can checksum everything */\n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev)\n{\n}\n#endif\n/* rx skb timestamps */\nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\n#ifdef CONFIG_PROC_FS\nint __init dev_proc_init(void);\n#else\n#define dev_proc_init() 0\n#endif\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\tskb->xmit_more = more ? 1 : 0;\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(const struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(const struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nstatic inline int netdev_class_create_file(const struct class_attribute *class_attr)\n{\n\treturn netdev_class_create_file_ns(class_attr, NULL);\n}\n\nstatic inline void netdev_class_remove_file(const struct class_attribute *class_attr)\n{\n\tnetdev_class_remove_file_ns(class_attr, NULL);\n}\n\nextern const struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nvoid linkwatch_run_queue(void);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n/* Allow TSO being used on stacked device :\n * Performing the GSO segmentation before last device\n * is a performance improvement.\n */\nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = (netdev_features_t)gso_type << NETIF_F_GSO_SHIFT;\n\n\t/* check flags correspondence */\n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_FIXEDID != (NETIF_F_TSO_MANGLEID >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP4  != (NETIF_F_GSO_IPXIP4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP6  != (NETIF_F_GSO_IPXIP6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_PARTIAL != (NETIF_F_GSO_PARTIAL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SCTP    != (NETIF_F_GSO_SCTP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_ESP != (NETIF_F_GSO_ESP >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nstatic inline void netif_set_gso_max_size(struct net_device *dev,\n\t\t\t\t\t  unsigned int size)\n{\n\tdev->gso_max_size = size;\n}\n\nstatic inline void skb_gso_error_unwind(struct sk_buff *skb, __be16 protocol,\n\t\t\t\t\tint pulled_hlen, u16 mac_offset,\n\t\t\t\t\tint mac_len)\n{\n\tskb->protocol = protocol;\n\tskb->encapsulation = 1;\n\tskb_push(skb, pulled_hlen);\n\tskb_reset_transport_header(skb);\n\tskb->mac_header = mac_offset;\n\tskb->network_header = skb->mac_header + mac_len;\n\tskb->mac_len = mac_len;\n}\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_ipvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_SLAVE;\n}\n\nstatic inline bool netif_is_ipvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_IPVLAN_MASTER;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_ovs_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OVS_DATAPATH;\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\n/* This device needs to keep skb dst for qdisc enqueue or ndo_start_xmit() */\nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\n/* return true if dev can't cope with mtu frames that need vlan tag insertion */\nstatic inline bool netif_reduces_vlan_mtu(struct net_device *dev)\n{\n\t/* TODO: reserve and use an additional IFF bit, if we get more users */\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n/* Logging, debugging and troubleshooting/diagnostic helpers. */\n\n/* netdev_printk helpers, similar to dev_printk */\n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline bool netdev_unregistering(const struct net_device *dev)\n{\n\treturn dev->reg_state == NETREG_UNREGISTERING;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n__printf(3, 4)\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...);\n__printf(2, 3)\nvoid netdev_emerg(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_alert(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_crit(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_err(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_warn(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_notice(const struct net_device *dev, const char *format, ...);\n__printf(2, 3)\nvoid netdev_info(const struct net_device *dev, const char *format, ...);\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tdynamic_netdev_dbg(__dev, format, ##args);\t\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n\tnetdev_printk(KERN_DEBUG, __dev, format, ##args)\n#else\n#define netdev_dbg(__dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, __dev, format, ##args); \\\n})\n#endif\n\n#if defined(VERBOSE_DEBUG)\n#define netdev_vdbg\tnetdev_dbg\n#else\n\n#define netdev_vdbg(dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetdev_printk(KERN_DEBUG, dev, format, ##args);\t\\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n * netdev_WARN() acts like dev_printk(), but with the key difference\n * of using a WARN/WARN_ON to get the message out, including the\n * file/line information and a backtrace.\n */\n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s\\n\" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n/* netif printk helpers, similar to netdev_printk */\n\n#define netif_printk(priv, type, level, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t  \t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_printk(level, (dev), fmt, ##args);\t\\\n} while (0)\n\n#define netif_level(level, priv, type, dev, fmt, args...)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tnetdev_##level(dev, fmt, ##args);\t\t\\\n} while (0)\n\n#define netif_emerg(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(emerg, priv, type, dev, fmt, ##args)\n#define netif_alert(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(alert, priv, type, dev, fmt, ##args)\n#define netif_crit(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(crit, priv, type, dev, fmt, ##args)\n#define netif_err(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(err, priv, type, dev, fmt, ##args)\n#define netif_warn(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(warn, priv, type, dev, fmt, ##args)\n#define netif_notice(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(notice, priv, type, dev, fmt, ##args)\n#define netif_info(priv, type, dev, fmt, args...)\t\t\\\n\tnetif_level(info, priv, type, dev, fmt, ##args)\n\n#if defined(CONFIG_DYNAMIC_DEBUG)\n#define netif_dbg(priv, type, netdev, format, args...)\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (netif_msg_##type(priv))\t\t\t\t\\\n\t\tdynamic_netdev_dbg(netdev, format, ##args);\t\\\n} while (0)\n#elif defined(DEBUG)\n#define netif_dbg(priv, type, dev, format, args...)\t\t\\\n\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args)\n#else\n#define netif_dbg(priv, type, dev, format, args...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\t\\\n})\n#endif\n\n/* if @cond then downgrade to debug, else print at @level */\n#define netif_cond_dbg(priv, type, netdev, cond, level, fmt, args...)     \\\n\tdo {                                                              \\\n\t\tif (cond)                                                 \\\n\t\t\tnetif_dbg(priv, type, netdev, fmt, ##args);       \\\n\t\telse                                                      \\\n\t\t\tnetif_ ## level(priv, type, netdev, fmt, ##args); \\\n\t} while (0)\n\n#if defined(VERBOSE_DEBUG)\n#define netif_vdbg\tnetif_dbg\n#else\n#define netif_vdbg(priv, type, dev, format, args...)\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tif (0)\t\t\t\t\t\t\t\\\n\t\tnetif_printk(priv, type, KERN_DEBUG, dev, format, ##args); \\\n\t0;\t\t\t\t\t\t\t\\\n})\n#endif\n\n/*\n *\tThe list of packet types we will receive (as opposed to discard)\n *\tand the routines to invoke.\n *\n *\tWhy 16. Because with 16 the only overlap we get on a hash of the\n *\tlow nibble of the protocol value is RARP/SNAP/X.25.\n *\n *      NOTE:  That is no longer true with the addition of VLAN tags.  Not\n *             sure which should go first, but I bet it won't make much\n *             difference if we are running VLANs.  The good news is that\n *             this protocol won't be in the list unless compiled in, so\n *             the average user (w/out VLANs) will not be adversely affected.\n *             --BLG\n *\n *\t\t0800\tIP\n *\t\t8100    802.1Q VLAN\n *\t\t0001\t802.3\n *\t\t0002\tAX.25\n *\t\t0004\t802.2\n *\t\t8035\tRARP\n *\t\t0005\tSNAP\n *\t\t0805\tX.25\n *\t\t0806\tARP\n *\t\t8137\tIPX\n *\t\t0009\tLocaltalk\n *\t\t86DD\tIPv6\n */\n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\n#endif\t/* _LINUX_NETDEVICE_H */\n", "/*\n *      NET3    Protocol independent device support routines.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n *\tDerived from the non IP parts of dev.c 1.0.19\n *              Authors:\tRoss Biro\n *\t\t\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\t\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\n *\tAdditional Authors:\n *\t\tFlorian la Roche <rzsfl@rz.uni-sb.de>\n *\t\tAlan Cox <gw4pts@gw4pts.ampr.org>\n *\t\tDavid Hinds <dahinds@users.sourceforge.net>\n *\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n *\t\tAdam Sulmicki <adam@cfar.umd.edu>\n *              Pekka Riikonen <priikone@poesidon.pspt.fi>\n *\n *\tChanges:\n *              D.J. Barrow     :       Fixed bug where dev->refcnt gets set\n *                                      to 2 if register_netdev gets called\n *                                      before net_dev_init & also removed a\n *                                      few lines of code in the process.\n *\t\tAlan Cox\t:\tdevice private ioctl copies fields back.\n *\t\tAlan Cox\t:\tTransmit queue code does relevant\n *\t\t\t\t\tstunts to keep the queue safe.\n *\t\tAlan Cox\t:\tFixed double lock.\n *\t\tAlan Cox\t:\tFixed promisc NULL pointer trap\n *\t\t????????\t:\tSupport the full private ioctl range\n *\t\tAlan Cox\t:\tMoved ioctl permission check into\n *\t\t\t\t\tdrivers\n *\t\tTim Kordas\t:\tSIOCADDMULTI/SIOCDELMULTI\n *\t\tAlan Cox\t:\t100 backlog just doesn't cut it when\n *\t\t\t\t\tyou start doing multicast video 8)\n *\t\tAlan Cox\t:\tRewrote net_bh and list manager.\n *              Alan Cox        :       Fix ETH_P_ALL echoback lengths.\n *\t\tAlan Cox\t:\tTook out transmit every packet pass\n *\t\t\t\t\tSaved a few bytes in the ioctl handler\n *\t\tAlan Cox\t:\tNetwork driver sets packet type before\n *\t\t\t\t\tcalling netif_rx. Saves a function\n *\t\t\t\t\tcall a packet.\n *\t\tAlan Cox\t:\tHashed net_bh()\n *\t\tRichard Kooijman:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tWrong field in SIOCGIFDSTADDR\n *\t\tAlan Cox\t:\tDevice lock protection.\n *              Alan Cox        :       Fixed nasty side effect of device close\n *\t\t\t\t\tchanges.\n *\t\tRudi Cilibrasi\t:\tPass the right thing to\n *\t\t\t\t\tset_mac_address()\n *\t\tDave Miller\t:\t32bit quantity for the device lock to\n *\t\t\t\t\tmake it work out on a Sparc.\n *\t\tBjorn Ekwall\t:\tAdded KERNELD hack.\n *\t\tAlan Cox\t:\tCleaned up the backlog initialise.\n *\t\tCraig Metz\t:\tSIOCGIFCONF fix if space for under\n *\t\t\t\t\t1 device.\n *\t    Thomas Bogendoerfer :\tReturn ENODEV for dev_open, if there\n *\t\t\t\t\tis no device open function.\n *\t\tAndi Kleen\t:\tFix error reporting for SIOCGIFCONF\n *\t    Michael Chastain\t:\tFix signed/unsigned for SIOCGIFCONF\n *\t\tCyrus Durgin\t:\tCleaned for KMOD\n *\t\tAdam Sulmicki   :\tBug Fix : Network Device Unload\n *\t\t\t\t\tA network device unload needs to purge\n *\t\t\t\t\tthe backlog queue.\n *\tPaul Rusty Russell\t:\tSIOCSIFNAME\n *              Pekka Riikonen  :\tNetdev boot-time settings code\n *              Andrew Morton   :       Make unregister_netdevice wait\n *                                      indefinitely on dev->refcnt\n *              J Hadi Salim    :       - Backlog queue sampling\n *\t\t\t\t        - netif_rx() feedback\n */\n\n#include <linux/uaccess.h>\n#include <linux/bitops.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/hash.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/notifier.h>\n#include <linux/skbuff.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/busy_poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/stat.h>\n#include <net/dst.h>\n#include <net/dst_metadata.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n#include <net/checksum.h>\n#include <net/xfrm.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/netpoll.h>\n#include <linux/rcupdate.h>\n#include <linux/delay.h>\n#include <net/iw_handler.h>\n#include <asm/current.h>\n#include <linux/audit.h>\n#include <linux/dmaengine.h>\n#include <linux/err.h>\n#include <linux/ctype.h>\n#include <linux/if_arp.h>\n#include <linux/if_vlan.h>\n#include <linux/ip.h>\n#include <net/ip.h>\n#include <net/mpls.h>\n#include <linux/ipv6.h>\n#include <linux/in.h>\n#include <linux/jhash.h>\n#include <linux/random.h>\n#include <trace/events/napi.h>\n#include <trace/events/net.h>\n#include <trace/events/skb.h>\n#include <linux/pci.h>\n#include <linux/inetdevice.h>\n#include <linux/cpu_rmap.h>\n#include <linux/static_key.h>\n#include <linux/hashtable.h>\n#include <linux/vmalloc.h>\n#include <linux/if_macvlan.h>\n#include <linux/errqueue.h>\n#include <linux/hrtimer.h>\n#include <linux/netfilter_ingress.h>\n#include <linux/crash_dump.h>\n#include <linux/sctp.h>\n#include <net/udp_tunnel.h>\n\n#include \"net-sysfs.h\"\n\n/* Instead of increasing this, you should create a hash table. */\n#define MAX_GRO_SKBS 8\n\n/* This should be increased if a protocol with a bigger head is added. */\n#define GRO_MAX_HEAD (MAX_HEADER + 128)\n\nstatic DEFINE_SPINLOCK(ptype_lock);\nstatic DEFINE_SPINLOCK(offload_lock);\nstruct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\nstruct list_head ptype_all __read_mostly;\t/* Taps */\nstatic struct list_head offload_base __read_mostly;\n\nstatic int netif_rx_internal(struct sk_buff *skb);\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info);\nstatic struct napi_struct *napi_by_id(unsigned int napi_id);\n\n/*\n * The @dev_base_head list is protected by @dev_base_lock and the rtnl\n * semaphore.\n *\n * Pure readers hold dev_base_lock for reading, or rcu_read_lock()\n *\n * Writers must hold the rtnl semaphore while they loop through the\n * dev_base_head list, and hold dev_base_lock for writing when they do the\n * actual updates.  This allows pure readers to access the list even\n * while a writer is preparing to update it.\n *\n * To put it another way, dev_base_lock is held for writing only to\n * protect against pure readers; the rtnl semaphore provides the\n * protection against other writers.\n *\n * See, for example usages, register_netdevice() and\n * unregister_netdevice(), which must be called with the rtnl\n * semaphore held.\n */\nDEFINE_RWLOCK(dev_base_lock);\nEXPORT_SYMBOL(dev_base_lock);\n\n/* protects napi_hash addition/deletion and napi_gen_id */\nstatic DEFINE_SPINLOCK(napi_hash_lock);\n\nstatic unsigned int napi_gen_id = NR_CPUS;\nstatic DEFINE_READ_MOSTLY_HASHTABLE(napi_hash, 8);\n\nstatic seqcount_t devnet_rename_seq;\n\nstatic inline void dev_base_seq_inc(struct net *net)\n{\n\twhile (++net->dev_base_seq == 0)\n\t\t;\n}\n\nstatic inline struct hlist_head *dev_name_hash(struct net *net, const char *name)\n{\n\tunsigned int hash = full_name_hash(net, name, strnlen(name, IFNAMSIZ));\n\n\treturn &net->dev_name_head[hash_32(hash, NETDEV_HASHBITS)];\n}\n\nstatic inline struct hlist_head *dev_index_hash(struct net *net, int ifindex)\n{\n\treturn &net->dev_index_head[ifindex & (NETDEV_HASHENTRIES - 1)];\n}\n\nstatic inline void rps_lock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_lock(&sd->input_pkt_queue.lock);\n#endif\n}\n\nstatic inline void rps_unlock(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tspin_unlock(&sd->input_pkt_queue.lock);\n#endif\n}\n\n/* Device list insertion */\nstatic void list_netdevice(struct net_device *dev)\n{\n\tstruct net *net = dev_net(dev);\n\n\tASSERT_RTNL();\n\n\twrite_lock_bh(&dev_base_lock);\n\tlist_add_tail_rcu(&dev->dev_list, &net->dev_base_head);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_index_hash(net, dev->ifindex));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(net);\n}\n\n/* Device list removal\n * caller must respect a RCU grace period before freeing/reusing dev\n */\nstatic void unlist_netdevice(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\t/* Unlink dev from the device chain */\n\twrite_lock_bh(&dev_base_lock);\n\tlist_del_rcu(&dev->dev_list);\n\thlist_del_rcu(&dev->name_hlist);\n\thlist_del_rcu(&dev->index_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tdev_base_seq_inc(dev_net(dev));\n}\n\n/*\n *\tOur notifier list\n */\n\nstatic RAW_NOTIFIER_HEAD(netdev_chain);\n\n/*\n *\tDevice drivers call our routines to queue packets here. We empty the\n *\tqueue in the local softnet handler.\n */\n\nDEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\nEXPORT_PER_CPU_SYMBOL(softnet_data);\n\n#ifdef CONFIG_LOCKDEP\n/*\n * register_netdevice() inits txq->_xmit_lock and sets lockdep class\n * according to dev->type\n */\nstatic const unsigned short netdev_lock_type[] = {\n\t ARPHRD_NETROM, ARPHRD_ETHER, ARPHRD_EETHER, ARPHRD_AX25,\n\t ARPHRD_PRONET, ARPHRD_CHAOS, ARPHRD_IEEE802, ARPHRD_ARCNET,\n\t ARPHRD_APPLETLK, ARPHRD_DLCI, ARPHRD_ATM, ARPHRD_METRICOM,\n\t ARPHRD_IEEE1394, ARPHRD_EUI64, ARPHRD_INFINIBAND, ARPHRD_SLIP,\n\t ARPHRD_CSLIP, ARPHRD_SLIP6, ARPHRD_CSLIP6, ARPHRD_RSRVD,\n\t ARPHRD_ADAPT, ARPHRD_ROSE, ARPHRD_X25, ARPHRD_HWX25,\n\t ARPHRD_PPP, ARPHRD_CISCO, ARPHRD_LAPB, ARPHRD_DDCMP,\n\t ARPHRD_RAWHDLC, ARPHRD_TUNNEL, ARPHRD_TUNNEL6, ARPHRD_FRAD,\n\t ARPHRD_SKIP, ARPHRD_LOOPBACK, ARPHRD_LOCALTLK, ARPHRD_FDDI,\n\t ARPHRD_BIF, ARPHRD_SIT, ARPHRD_IPDDP, ARPHRD_IPGRE,\n\t ARPHRD_PIMREG, ARPHRD_HIPPI, ARPHRD_ASH, ARPHRD_ECONET,\n\t ARPHRD_IRDA, ARPHRD_FCPP, ARPHRD_FCAL, ARPHRD_FCPL,\n\t ARPHRD_FCFABRIC, ARPHRD_IEEE80211, ARPHRD_IEEE80211_PRISM,\n\t ARPHRD_IEEE80211_RADIOTAP, ARPHRD_PHONET, ARPHRD_PHONET_PIPE,\n\t ARPHRD_IEEE802154, ARPHRD_VOID, ARPHRD_NONE};\n\nstatic const char *const netdev_lock_name[] = {\n\t\"_xmit_NETROM\", \"_xmit_ETHER\", \"_xmit_EETHER\", \"_xmit_AX25\",\n\t\"_xmit_PRONET\", \"_xmit_CHAOS\", \"_xmit_IEEE802\", \"_xmit_ARCNET\",\n\t\"_xmit_APPLETLK\", \"_xmit_DLCI\", \"_xmit_ATM\", \"_xmit_METRICOM\",\n\t\"_xmit_IEEE1394\", \"_xmit_EUI64\", \"_xmit_INFINIBAND\", \"_xmit_SLIP\",\n\t\"_xmit_CSLIP\", \"_xmit_SLIP6\", \"_xmit_CSLIP6\", \"_xmit_RSRVD\",\n\t\"_xmit_ADAPT\", \"_xmit_ROSE\", \"_xmit_X25\", \"_xmit_HWX25\",\n\t\"_xmit_PPP\", \"_xmit_CISCO\", \"_xmit_LAPB\", \"_xmit_DDCMP\",\n\t\"_xmit_RAWHDLC\", \"_xmit_TUNNEL\", \"_xmit_TUNNEL6\", \"_xmit_FRAD\",\n\t\"_xmit_SKIP\", \"_xmit_LOOPBACK\", \"_xmit_LOCALTLK\", \"_xmit_FDDI\",\n\t\"_xmit_BIF\", \"_xmit_SIT\", \"_xmit_IPDDP\", \"_xmit_IPGRE\",\n\t\"_xmit_PIMREG\", \"_xmit_HIPPI\", \"_xmit_ASH\", \"_xmit_ECONET\",\n\t\"_xmit_IRDA\", \"_xmit_FCPP\", \"_xmit_FCAL\", \"_xmit_FCPL\",\n\t\"_xmit_FCFABRIC\", \"_xmit_IEEE80211\", \"_xmit_IEEE80211_PRISM\",\n\t\"_xmit_IEEE80211_RADIOTAP\", \"_xmit_PHONET\", \"_xmit_PHONET_PIPE\",\n\t\"_xmit_IEEE802154\", \"_xmit_VOID\", \"_xmit_NONE\"};\n\nstatic struct lock_class_key netdev_xmit_lock_key[ARRAY_SIZE(netdev_lock_type)];\nstatic struct lock_class_key netdev_addr_lock_key[ARRAY_SIZE(netdev_lock_type)];\n\nstatic inline unsigned short netdev_lock_pos(unsigned short dev_type)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(netdev_lock_type); i++)\n\t\tif (netdev_lock_type[i] == dev_type)\n\t\t\treturn i;\n\t/* the last key is used by default */\n\treturn ARRAY_SIZE(netdev_lock_type) - 1;\n}\n\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev_type);\n\tlockdep_set_class_and_name(lock, &netdev_xmit_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n\tint i;\n\n\ti = netdev_lock_pos(dev->type);\n\tlockdep_set_class_and_name(&dev->addr_list_lock,\n\t\t\t\t   &netdev_addr_lock_key[i],\n\t\t\t\t   netdev_lock_name[i]);\n}\n#else\nstatic inline void netdev_set_xmit_lockdep_class(spinlock_t *lock,\n\t\t\t\t\t\t unsigned short dev_type)\n{\n}\nstatic inline void netdev_set_addr_lockdep_class(struct net_device *dev)\n{\n}\n#endif\n\n/*******************************************************************************\n *\n *\t\tProtocol management and registration routines\n *\n *******************************************************************************/\n\n\n/*\n *\tAdd a protocol ID to the list. Now that the input handler is\n *\tsmarter we can dispense with all the messy stuff that used to be\n *\there.\n *\n *\tBEWARE!!! Protocol handlers, mangling input packets,\n *\tMUST BE last in hash buckets and checking protocol handlers\n *\tMUST start from promiscuous ptype_all chain in net_bh.\n *\tIt is true now, do not change it.\n *\tExplanation follows: if protocol handler, mangling packet, will\n *\tbe the first on list, it is not able to sense, that packet\n *\tis cloned and should be copied-on-write, so that it will\n *\tchange it and subsequent readers will get broken packet.\n *\t\t\t\t\t\t\t--ANK (980803)\n */\n\nstatic inline struct list_head *ptype_head(const struct packet_type *pt)\n{\n\tif (pt->type == htons(ETH_P_ALL))\n\t\treturn pt->dev ? &pt->dev->ptype_all : &ptype_all;\n\telse\n\t\treturn pt->dev ? &pt->dev->ptype_specific :\n\t\t\t\t &ptype_base[ntohs(pt->type) & PTYPE_HASH_MASK];\n}\n\n/**\n *\tdev_add_pack - add packet handler\n *\t@pt: packet type declaration\n *\n *\tAdd a protocol handler to the networking stack. The passed &packet_type\n *\tis linked into kernel lists and may not be freed until it has been\n *\tremoved from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new packet type (until the next received packet).\n */\n\nvoid dev_add_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\n\tspin_lock(&ptype_lock);\n\tlist_add_rcu(&pt->list, head);\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(dev_add_pack);\n\n/**\n *\t__dev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nvoid __dev_remove_pack(struct packet_type *pt)\n{\n\tstruct list_head *head = ptype_head(pt);\n\tstruct packet_type *pt1;\n\n\tspin_lock(&ptype_lock);\n\n\tlist_for_each_entry(pt1, head, list) {\n\t\tif (pt == pt1) {\n\t\t\tlist_del_rcu(&pt->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_pack: %p not found\\n\", pt);\nout:\n\tspin_unlock(&ptype_lock);\n}\nEXPORT_SYMBOL(__dev_remove_pack);\n\n/**\n *\tdev_remove_pack\t - remove packet handler\n *\t@pt: packet type declaration\n *\n *\tRemove a protocol handler that was previously added to the kernel\n *\tprotocol handlers by dev_add_pack(). The passed &packet_type is removed\n *\tfrom the kernel lists and can be freed or reused once this function\n *\treturns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_pack(struct packet_type *pt)\n{\n\t__dev_remove_pack(pt);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_pack);\n\n\n/**\n *\tdev_add_offload - register offload handlers\n *\t@po: protocol offload declaration\n *\n *\tAdd protocol offload handlers to the networking stack. The passed\n *\t&proto_offload is linked into kernel lists and may not be freed until\n *\tit has been removed from the kernel lists.\n *\n *\tThis call does not sleep therefore it can not\n *\tguarantee all CPU's that are in middle of receiving packets\n *\twill see the new offload handlers (until the next received packet).\n */\nvoid dev_add_offload(struct packet_offload *po)\n{\n\tstruct packet_offload *elem;\n\n\tspin_lock(&offload_lock);\n\tlist_for_each_entry(elem, &offload_base, list) {\n\t\tif (po->priority < elem->priority)\n\t\t\tbreak;\n\t}\n\tlist_add_rcu(&po->list, elem->list.prev);\n\tspin_unlock(&offload_lock);\n}\nEXPORT_SYMBOL(dev_add_offload);\n\n/**\n *\t__dev_remove_offload\t - remove offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a protocol offload handler that was previously added to the\n *\tkernel offload handlers by dev_add_offload(). The passed &offload_type\n *\tis removed from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *      The packet type might still be in use by receivers\n *\tand must not be freed until after all the CPU's have gone\n *\tthrough a quiescent state.\n */\nstatic void __dev_remove_offload(struct packet_offload *po)\n{\n\tstruct list_head *head = &offload_base;\n\tstruct packet_offload *po1;\n\n\tspin_lock(&offload_lock);\n\n\tlist_for_each_entry(po1, head, list) {\n\t\tif (po == po1) {\n\t\t\tlist_del_rcu(&po->list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"dev_remove_offload: %p not found\\n\", po);\nout:\n\tspin_unlock(&offload_lock);\n}\n\n/**\n *\tdev_remove_offload\t - remove packet offload handler\n *\t@po: packet offload declaration\n *\n *\tRemove a packet offload handler that was previously added to the kernel\n *\toffload handlers by dev_add_offload(). The passed &offload_type is\n *\tremoved from the kernel lists and can be freed or reused once this\n *\tfunction returns.\n *\n *\tThis call sleeps to guarantee that no CPU is looking at the packet\n *\ttype after return.\n */\nvoid dev_remove_offload(struct packet_offload *po)\n{\n\t__dev_remove_offload(po);\n\n\tsynchronize_net();\n}\nEXPORT_SYMBOL(dev_remove_offload);\n\n/******************************************************************************\n *\n *\t\t      Device Boot-time Settings Routines\n *\n ******************************************************************************/\n\n/* Boot time configuration table */\nstatic struct netdev_boot_setup dev_boot_setup[NETDEV_BOOT_SETUP_MAX];\n\n/**\n *\tnetdev_boot_setup_add\t- add new setup entry\n *\t@name: name of the device\n *\t@map: configured settings for the device\n *\n *\tAdds new setup entry to the dev_boot_setup list.  The function\n *\treturns 0 on error and 1 on success.  This is a generic routine to\n *\tall netdevices.\n */\nstatic int netdev_boot_setup_add(char *name, struct ifmap *map)\n{\n\tstruct netdev_boot_setup *s;\n\tint i;\n\n\ts = dev_boot_setup;\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] == '\\0' || s[i].name[0] == ' ') {\n\t\t\tmemset(s[i].name, 0, sizeof(s[i].name));\n\t\t\tstrlcpy(s[i].name, name, IFNAMSIZ);\n\t\t\tmemcpy(&s[i].map, map, sizeof(s[i].map));\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i >= NETDEV_BOOT_SETUP_MAX ? 0 : 1;\n}\n\n/**\n * netdev_boot_setup_check\t- check boot time settings\n * @dev: the netdevice\n *\n * Check boot time settings for the device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found, 1 if they are.\n */\nint netdev_boot_setup_check(struct net_device *dev)\n{\n\tstruct netdev_boot_setup *s = dev_boot_setup;\n\tint i;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++) {\n\t\tif (s[i].name[0] != '\\0' && s[i].name[0] != ' ' &&\n\t\t    !strcmp(dev->name, s[i].name)) {\n\t\t\tdev->irq = s[i].map.irq;\n\t\t\tdev->base_addr = s[i].map.base_addr;\n\t\t\tdev->mem_start = s[i].map.mem_start;\n\t\t\tdev->mem_end = s[i].map.mem_end;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_boot_setup_check);\n\n\n/**\n * netdev_boot_base\t- get address from boot time settings\n * @prefix: prefix for network device\n * @unit: id for network device\n *\n * Check boot time settings for the base address of device.\n * The found settings are set for the device to be used\n * later in the device probing.\n * Returns 0 if no settings found.\n */\nunsigned long netdev_boot_base(const char *prefix, int unit)\n{\n\tconst struct netdev_boot_setup *s = dev_boot_setup;\n\tchar name[IFNAMSIZ];\n\tint i;\n\n\tsprintf(name, \"%s%d\", prefix, unit);\n\n\t/*\n\t * If device already registered then return base of 1\n\t * to indicate not to probe for this interface\n\t */\n\tif (__dev_get_by_name(&init_net, name))\n\t\treturn 1;\n\n\tfor (i = 0; i < NETDEV_BOOT_SETUP_MAX; i++)\n\t\tif (!strcmp(name, s[i].name))\n\t\t\treturn s[i].map.base_addr;\n\treturn 0;\n}\n\n/*\n * Saves at boot time configured settings for any netdevice.\n */\nint __init netdev_boot_setup(char *str)\n{\n\tint ints[5];\n\tstruct ifmap map;\n\n\tstr = get_options(str, ARRAY_SIZE(ints), ints);\n\tif (!str || !*str)\n\t\treturn 0;\n\n\t/* Save settings */\n\tmemset(&map, 0, sizeof(map));\n\tif (ints[0] > 0)\n\t\tmap.irq = ints[1];\n\tif (ints[0] > 1)\n\t\tmap.base_addr = ints[2];\n\tif (ints[0] > 2)\n\t\tmap.mem_start = ints[3];\n\tif (ints[0] > 3)\n\t\tmap.mem_end = ints[4];\n\n\t/* Add new entry to the list */\n\treturn netdev_boot_setup_add(str, &map);\n}\n\n__setup(\"netdev=\", netdev_boot_setup);\n\n/*******************************************************************************\n *\n *\t\t\t    Device Interface Subroutines\n *\n *******************************************************************************/\n\n/**\n *\tdev_get_iflink\t- get 'iflink' value of a interface\n *\t@dev: targeted interface\n *\n *\tIndicates the ifindex the interface is linked to.\n *\tPhysical interfaces have the same 'ifindex' and 'iflink' values.\n */\n\nint dev_get_iflink(const struct net_device *dev)\n{\n\tif (dev->netdev_ops && dev->netdev_ops->ndo_get_iflink)\n\t\treturn dev->netdev_ops->ndo_get_iflink(dev);\n\n\treturn dev->ifindex;\n}\nEXPORT_SYMBOL(dev_get_iflink);\n\n/**\n *\tdev_fill_metadata_dst - Retrieve tunnel egress information.\n *\t@dev: targeted interface\n *\t@skb: The packet.\n *\n *\tFor better visibility of tunnel traffic OVS needs to retrieve\n *\tegress tunnel information for a packet. Following API allows\n *\tuser to get this info.\n */\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct ip_tunnel_info *info;\n\n\tif (!dev->netdev_ops  || !dev->netdev_ops->ndo_fill_metadata_dst)\n\t\treturn -EINVAL;\n\n\tinfo = skb_tunnel_info_unclone(skb);\n\tif (!info)\n\t\treturn -ENOMEM;\n\tif (unlikely(!(info->mode & IP_TUNNEL_INFO_TX)))\n\t\treturn -EINVAL;\n\n\treturn dev->netdev_ops->ndo_fill_metadata_dst(dev, skb);\n}\nEXPORT_SYMBOL_GPL(dev_fill_metadata_dst);\n\n/**\n *\t__dev_get_by_name\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. Must be called under RTNL semaphore\n *\tor @dev_base_lock. If the name is found a pointer to the device\n *\tis returned. If the name is not found then %NULL is returned. The\n *\treference counters are not incremented so the caller must be\n *\tcareful with locks.\n */\n\nstruct net_device *__dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_name);\n\n/**\n * dev_get_by_name_rcu\t- find a device by its name\n * @net: the applicable net namespace\n * @name: name to find\n *\n * Find an interface by name.\n * If the name is found a pointer to the device is returned.\n * If the name is not found then %NULL is returned.\n * The reference counters are not incremented so the caller must be\n * careful with locks. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_name_hash(net, name);\n\n\thlist_for_each_entry_rcu(dev, head, name_hlist)\n\t\tif (!strncmp(dev->name, name, IFNAMSIZ))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_name_rcu);\n\n/**\n *\tdev_get_by_name\t\t- find a device by its name\n *\t@net: the applicable net namespace\n *\t@name: name to find\n *\n *\tFind an interface by name. This can be called from any\n *\tcontext and does its own locking. The returned handle has\n *\tthe usage count incremented and the caller must use dev_put() to\n *\trelease it when it is no longer needed. %NULL is returned if no\n *\tmatching device is found.\n */\n\nstruct net_device *dev_get_by_name(struct net *net, const char *name)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(net, name);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_name);\n\n/**\n *\t__dev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold either the RTNL semaphore\n *\tor @dev_base_lock.\n */\n\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_get_by_index);\n\n/**\n *\tdev_get_by_index_rcu - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not\n *\thad its reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\tstruct hlist_head *head = dev_index_hash(net, ifindex);\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist)\n\t\tif (dev->ifindex == ifindex)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_get_by_index_rcu);\n\n\n/**\n *\tdev_get_by_index - find a device by its ifindex\n *\t@net: the applicable net namespace\n *\t@ifindex: index of device\n *\n *\tSearch for an interface by index. Returns NULL if the device\n *\tis not found or a pointer to the device. The device returned has\n *\thad a reference added and the pointer is safe until the user calls\n *\tdev_put to indicate they have finished with it.\n */\n\nstruct net_device *dev_get_by_index(struct net *net, int ifindex)\n{\n\tstruct net_device *dev;\n\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (dev)\n\t\tdev_hold(dev);\n\trcu_read_unlock();\n\treturn dev;\n}\nEXPORT_SYMBOL(dev_get_by_index);\n\n/**\n *\tdev_get_by_napi_id - find a device by napi_id\n *\t@napi_id: ID of the NAPI struct\n *\n *\tSearch for an interface by NAPI ID. Returns %NULL if the device\n *\tis not found or a pointer to the device. The device has not had\n *\tits reference counter increased so the caller must be careful\n *\tabout locking. The caller must hold RCU lock.\n */\n\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id)\n{\n\tstruct napi_struct *napi;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (napi_id < MIN_NAPI_ID)\n\t\treturn NULL;\n\n\tnapi = napi_by_id(napi_id);\n\n\treturn napi ? napi->dev : NULL;\n}\nEXPORT_SYMBOL(dev_get_by_napi_id);\n\n/**\n *\tnetdev_get_name - get a netdevice name, knowing its ifindex.\n *\t@net: network namespace\n *\t@name: a pointer to the buffer where the name will be stored.\n *\t@ifindex: the ifindex of the interface to get the name from.\n *\n *\tThe use of raw_seqcount_begin() and cond_resched() before\n *\tretrying is required as we want to give the writers a chance\n *\tto complete when CONFIG_PREEMPT is not set.\n */\nint netdev_get_name(struct net *net, char *name, int ifindex)\n{\n\tstruct net_device *dev;\n\tunsigned int seq;\n\nretry:\n\tseq = raw_seqcount_begin(&devnet_rename_seq);\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(net, ifindex);\n\tif (!dev) {\n\t\trcu_read_unlock();\n\t\treturn -ENODEV;\n\t}\n\n\tstrcpy(name, dev->name);\n\trcu_read_unlock();\n\tif (read_seqcount_retry(&devnet_rename_seq, seq)) {\n\t\tcond_resched();\n\t\tgoto retry;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tdev_getbyhwaddr_rcu - find a device by its hardware address\n *\t@net: the applicable net namespace\n *\t@type: media type of device\n *\t@ha: hardware address\n *\n *\tSearch for an interface by MAC address. Returns NULL if the device\n *\tis not found or a pointer to the device.\n *\tThe caller must hold RCU or RTNL.\n *\tThe returned device has not had its ref count increased\n *\tand the caller must therefore be careful about locking\n *\n */\n\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *ha)\n{\n\tstruct net_device *dev;\n\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type &&\n\t\t    !memcmp(dev->dev_addr, ha, dev->addr_len))\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dev_getbyhwaddr_rcu);\n\nstruct net_device *__dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tfor_each_netdev(net, dev)\n\t\tif (dev->type == type)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(__dev_getfirstbyhwtype);\n\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type)\n{\n\tstruct net_device *dev, *ret = NULL;\n\n\trcu_read_lock();\n\tfor_each_netdev_rcu(net, dev)\n\t\tif (dev->type == type) {\n\t\t\tdev_hold(dev);\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\trcu_read_unlock();\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_getfirstbyhwtype);\n\n/**\n *\t__dev_get_by_flags - find any device with given flags\n *\t@net: the applicable net namespace\n *\t@if_flags: IFF_* values\n *\t@mask: bitmask of bits in if_flags to check\n *\n *\tSearch for any interface with the given flags. Returns NULL if a device\n *\tis not found or a pointer to the device. Must be called inside\n *\trtnl_lock(), and result refcount is unchanged.\n */\n\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short if_flags,\n\t\t\t\t      unsigned short mask)\n{\n\tstruct net_device *dev, *ret;\n\n\tASSERT_RTNL();\n\n\tret = NULL;\n\tfor_each_netdev(net, dev) {\n\t\tif (((dev->flags ^ if_flags) & mask) == 0) {\n\t\t\tret = dev;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__dev_get_by_flags);\n\n/**\n *\tdev_valid_name - check if name is okay for network device\n *\t@name: name string\n *\n *\tNetwork device names need to be valid file names to\n *\tto allow sysfs to work.  We also disallow any kind of\n *\twhitespace.\n */\nbool dev_valid_name(const char *name)\n{\n\tif (*name == '\\0')\n\t\treturn false;\n\tif (strlen(name) >= IFNAMSIZ)\n\t\treturn false;\n\tif (!strcmp(name, \".\") || !strcmp(name, \"..\"))\n\t\treturn false;\n\n\twhile (*name) {\n\t\tif (*name == '/' || *name == ':' || isspace(*name))\n\t\t\treturn false;\n\t\tname++;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(dev_valid_name);\n\n/**\n *\t__dev_alloc_name - allocate a name for a device\n *\t@net: network namespace to allocate the device name in\n *\t@name: name format string\n *\t@buf:  scratch buffer and result name string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nstatic int __dev_alloc_name(struct net *net, const char *name, char *buf)\n{\n\tint i = 0;\n\tconst char *p;\n\tconst int max_netdevices = 8*PAGE_SIZE;\n\tunsigned long *inuse;\n\tstruct net_device *d;\n\n\tp = strnchr(name, IFNAMSIZ-1, '%');\n\tif (p) {\n\t\t/*\n\t\t * Verify the string as this thing may have come from\n\t\t * the user.  There must be either one \"%d\" and no other \"%\"\n\t\t * characters.\n\t\t */\n\t\tif (p[1] != 'd' || strchr(p + 2, '%'))\n\t\t\treturn -EINVAL;\n\n\t\t/* Use one page as a bit array of possible slots */\n\t\tinuse = (unsigned long *) get_zeroed_page(GFP_ATOMIC);\n\t\tif (!inuse)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_netdev(net, d) {\n\t\t\tif (!sscanf(d->name, name, &i))\n\t\t\t\tcontinue;\n\t\t\tif (i < 0 || i >= max_netdevices)\n\t\t\t\tcontinue;\n\n\t\t\t/*  avoid cases where sscanf is not exact inverse of printf */\n\t\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\t\t\tif (!strncmp(buf, d->name, IFNAMSIZ))\n\t\t\t\tset_bit(i, inuse);\n\t\t}\n\n\t\ti = find_first_zero_bit(inuse, max_netdevices);\n\t\tfree_page((unsigned long) inuse);\n\t}\n\n\tif (buf != name)\n\t\tsnprintf(buf, IFNAMSIZ, name, i);\n\tif (!__dev_get_by_name(net, buf))\n\t\treturn i;\n\n\t/* It is possible to run out of possible slots\n\t * when the name is long and there isn't enough space left\n\t * for the digits, or if all bits are used.\n\t */\n\treturn -ENFILE;\n}\n\n/**\n *\tdev_alloc_name - allocate a name for a device\n *\t@dev: device\n *\t@name: name format string\n *\n *\tPassed a format string - eg \"lt%d\" it will try and find a suitable\n *\tid. It scans list of devices to build up a free map, then chooses\n *\tthe first empty slot. The caller must hold the dev_base or rtnl lock\n *\twhile allocating the name and adding the device in order to avoid\n *\tduplicates.\n *\tLimited to bits_per_byte * page size devices (ie 32K on most platforms).\n *\tReturns the number of the unit assigned or a negative errno code.\n */\n\nint dev_alloc_name(struct net_device *dev, const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tstruct net *net;\n\tint ret;\n\n\tBUG_ON(!dev_net(dev));\n\tnet = dev_net(dev);\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_alloc_name);\n\nstatic int dev_alloc_name_ns(struct net *net,\n\t\t\t     struct net_device *dev,\n\t\t\t     const char *name)\n{\n\tchar buf[IFNAMSIZ];\n\tint ret;\n\n\tret = __dev_alloc_name(net, name, buf);\n\tif (ret >= 0)\n\t\tstrlcpy(dev->name, buf, IFNAMSIZ);\n\treturn ret;\n}\n\nint dev_get_valid_name(struct net *net, struct net_device *dev,\n\t\t       const char *name)\n{\n\tBUG_ON(!net);\n\n\tif (!dev_valid_name(name))\n\t\treturn -EINVAL;\n\n\tif (strchr(name, '%'))\n\t\treturn dev_alloc_name_ns(net, dev, name);\n\telse if (__dev_get_by_name(net, name))\n\t\treturn -EEXIST;\n\telse if (dev->name != name)\n\t\tstrlcpy(dev->name, name, IFNAMSIZ);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_get_valid_name);\n\n/**\n *\tdev_change_name - change name of a device\n *\t@dev: device\n *\t@newname: name (or format string) must be at least IFNAMSIZ\n *\n *\tChange name of a device, can pass format strings \"eth%d\".\n *\tfor wildcarding.\n */\nint dev_change_name(struct net_device *dev, const char *newname)\n{\n\tunsigned char old_assign_type;\n\tchar oldname[IFNAMSIZ];\n\tint err = 0;\n\tint ret;\n\tstruct net *net;\n\n\tASSERT_RTNL();\n\tBUG_ON(!dev_net(dev));\n\n\tnet = dev_net(dev);\n\tif (dev->flags & IFF_UP)\n\t\treturn -EBUSY;\n\n\twrite_seqcount_begin(&devnet_rename_seq);\n\n\tif (strncmp(newname, dev->name, IFNAMSIZ) == 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn 0;\n\t}\n\n\tmemcpy(oldname, dev->name, IFNAMSIZ);\n\n\terr = dev_get_valid_name(net, dev, newname);\n\tif (err < 0) {\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn err;\n\t}\n\n\tif (oldname[0] && !strchr(oldname, '%'))\n\t\tnetdev_info(dev, \"renamed from %s\\n\", oldname);\n\n\told_assign_type = dev->name_assign_type;\n\tdev->name_assign_type = NET_NAME_RENAMED;\n\nrollback:\n\tret = device_rename(&dev->dev, dev->name);\n\tif (ret) {\n\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\tdev->name_assign_type = old_assign_type;\n\t\twrite_seqcount_end(&devnet_rename_seq);\n\t\treturn ret;\n\t}\n\n\twrite_seqcount_end(&devnet_rename_seq);\n\n\tnetdev_adjacent_rename_links(dev, oldname);\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_del_rcu(&dev->name_hlist);\n\twrite_unlock_bh(&dev_base_lock);\n\n\tsynchronize_rcu();\n\n\twrite_lock_bh(&dev_base_lock);\n\thlist_add_head_rcu(&dev->name_hlist, dev_name_hash(net, dev->name));\n\twrite_unlock_bh(&dev_base_lock);\n\n\tret = call_netdevice_notifiers(NETDEV_CHANGENAME, dev);\n\tret = notifier_to_errno(ret);\n\n\tif (ret) {\n\t\t/* err >= 0 after dev_alloc_name() or stores the first errno */\n\t\tif (err >= 0) {\n\t\t\terr = ret;\n\t\t\twrite_seqcount_begin(&devnet_rename_seq);\n\t\t\tmemcpy(dev->name, oldname, IFNAMSIZ);\n\t\t\tmemcpy(oldname, newname, IFNAMSIZ);\n\t\t\tdev->name_assign_type = old_assign_type;\n\t\t\told_assign_type = NET_NAME_RENAMED;\n\t\t\tgoto rollback;\n\t\t} else {\n\t\t\tpr_err(\"%s: name change rollback failed: %d\\n\",\n\t\t\t       dev->name, ret);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n/**\n *\tdev_set_alias - change ifalias of a device\n *\t@dev: device\n *\t@alias: name up to IFALIASZ\n *\t@len: limit of bytes to copy from info\n *\n *\tSet ifalias for a device,\n */\nint dev_set_alias(struct net_device *dev, const char *alias, size_t len)\n{\n\tchar *new_ifalias;\n\n\tASSERT_RTNL();\n\n\tif (len >= IFALIASZ)\n\t\treturn -EINVAL;\n\n\tif (!len) {\n\t\tkfree(dev->ifalias);\n\t\tdev->ifalias = NULL;\n\t\treturn 0;\n\t}\n\n\tnew_ifalias = krealloc(dev->ifalias, len + 1, GFP_KERNEL);\n\tif (!new_ifalias)\n\t\treturn -ENOMEM;\n\tdev->ifalias = new_ifalias;\n\tmemcpy(dev->ifalias, alias, len);\n\tdev->ifalias[len] = 0;\n\n\treturn len;\n}\n\n\n/**\n *\tnetdev_features_change - device changes features\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed features.\n */\nvoid netdev_features_change(struct net_device *dev)\n{\n\tcall_netdevice_notifiers(NETDEV_FEAT_CHANGE, dev);\n}\nEXPORT_SYMBOL(netdev_features_change);\n\n/**\n *\tnetdev_state_change - device changes state\n *\t@dev: device to cause notification\n *\n *\tCalled to indicate a device has changed state. This function calls\n *\tthe notifier chains for netdev_chain and sends a NEWLINK message\n *\tto the routing socket.\n */\nvoid netdev_state_change(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = 0;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, 0, GFP_KERNEL);\n\t}\n}\nEXPORT_SYMBOL(netdev_state_change);\n\n/**\n * netdev_notify_peers - notify network peers about existence of @dev\n * @dev: network device\n *\n * Generate traffic such that interested network peers are aware of\n * @dev, such as by generating a gratuitous ARP. This may be used when\n * a device wants to inform the rest of the network about some sort of\n * reconfiguration such as a failover event or virtual machine\n * migration.\n */\nvoid netdev_notify_peers(struct net_device *dev)\n{\n\trtnl_lock();\n\tcall_netdevice_notifiers(NETDEV_NOTIFY_PEERS, dev);\n\tcall_netdevice_notifiers(NETDEV_RESEND_IGMP, dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(netdev_notify_peers);\n\nstatic int __dev_open(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\t/* Block netpoll from trying to do any rx path servicing.\n\t * If we don't do this there is a chance ndo_poll_controller\n\t * or ndo_poll may be running while we open the device\n\t */\n\tnetpoll_poll_disable(dev);\n\n\tret = call_netdevice_notifiers(NETDEV_PRE_UP, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\tif (ops->ndo_validate_addr)\n\t\tret = ops->ndo_validate_addr(dev);\n\n\tif (!ret && ops->ndo_open)\n\t\tret = ops->ndo_open(dev);\n\n\tnetpoll_poll_enable(dev);\n\n\tif (ret)\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\telse {\n\t\tdev->flags |= IFF_UP;\n\t\tdev_set_rx_mode(dev);\n\t\tdev_activate(dev);\n\t\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\t}\n\n\treturn ret;\n}\n\n/**\n *\tdev_open\t- prepare an interface for use.\n *\t@dev:\tdevice to open\n *\n *\tTakes a device from down to up state. The device's private open\n *\tfunction is invoked and then the multicast lists are loaded. Finally\n *\tthe device is moved into the up state and a %NETDEV_UP message is\n *\tsent to the netdev notifier chain.\n *\n *\tCalling this function on an active interface is a nop. On a failure\n *\ta negative errno code is returned.\n */\nint dev_open(struct net_device *dev)\n{\n\tint ret;\n\n\tif (dev->flags & IFF_UP)\n\t\treturn 0;\n\n\tret = __dev_open(dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_open);\n\nstatic void __dev_close_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tASSERT_RTNL();\n\tmight_sleep();\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\t/* Temporarily disable netpoll until the interface is down */\n\t\tnetpoll_poll_disable(dev);\n\n\t\tcall_netdevice_notifiers(NETDEV_GOING_DOWN, dev);\n\n\t\tclear_bit(__LINK_STATE_START, &dev->state);\n\n\t\t/* Synchronize to scheduled poll. We cannot touch poll list, it\n\t\t * can be even on different cpu. So just clear netif_running().\n\t\t *\n\t\t * dev->stop() will invoke napi_disable() on all of it's\n\t\t * napi_struct instances on this device.\n\t\t */\n\t\tsmp_mb__after_atomic(); /* Commit netif_running(). */\n\t}\n\n\tdev_deactivate_many(head);\n\n\tlist_for_each_entry(dev, head, close_list) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\t/*\n\t\t *\tCall the device specific close. This cannot fail.\n\t\t *\tOnly if device is UP\n\t\t *\n\t\t *\tWe allow it to be called even after a DETACH hot-plug\n\t\t *\tevent.\n\t\t */\n\t\tif (ops->ndo_stop)\n\t\t\tops->ndo_stop(dev);\n\n\t\tdev->flags &= ~IFF_UP;\n\t\tnetpoll_poll_enable(dev);\n\t}\n}\n\nstatic void __dev_close(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->close_list, &single);\n\t__dev_close_many(&single);\n\tlist_del(&single);\n}\n\nvoid dev_close_many(struct list_head *head, bool unlink)\n{\n\tstruct net_device *dev, *tmp;\n\n\t/* Remove the devices that don't need to be closed */\n\tlist_for_each_entry_safe(dev, tmp, head, close_list)\n\t\tif (!(dev->flags & IFF_UP))\n\t\t\tlist_del_init(&dev->close_list);\n\n\t__dev_close_many(head);\n\n\tlist_for_each_entry_safe(dev, tmp, head, close_list) {\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, IFF_UP|IFF_RUNNING, GFP_KERNEL);\n\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t\tif (unlink)\n\t\t\tlist_del_init(&dev->close_list);\n\t}\n}\nEXPORT_SYMBOL(dev_close_many);\n\n/**\n *\tdev_close - shutdown an interface.\n *\t@dev: device to shutdown\n *\n *\tThis function moves an active device into down state. A\n *\t%NETDEV_GOING_DOWN is sent to the netdev notifier chain. The device\n *\tis then deactivated and finally a %NETDEV_DOWN is sent to the notifier\n *\tchain.\n */\nvoid dev_close(struct net_device *dev)\n{\n\tif (dev->flags & IFF_UP) {\n\t\tLIST_HEAD(single);\n\n\t\tlist_add(&dev->close_list, &single);\n\t\tdev_close_many(&single, true);\n\t\tlist_del(&single);\n\t}\n}\nEXPORT_SYMBOL(dev_close);\n\n\n/**\n *\tdev_disable_lro - disable Large Receive Offload on a device\n *\t@dev: device\n *\n *\tDisable Large Receive Offload (LRO) on a net device.  Must be\n *\tcalled under RTNL.  This is needed if received packets may be\n *\tforwarded to another interface.\n */\nvoid dev_disable_lro(struct net_device *dev)\n{\n\tstruct net_device *lower_dev;\n\tstruct list_head *iter;\n\n\tdev->wanted_features &= ~NETIF_F_LRO;\n\tnetdev_update_features(dev);\n\n\tif (unlikely(dev->features & NETIF_F_LRO))\n\t\tnetdev_WARN(dev, \"failed to disable LRO!\\n\");\n\n\tnetdev_for_each_lower_dev(dev, lower_dev, iter)\n\t\tdev_disable_lro(lower_dev);\n}\nEXPORT_SYMBOL(dev_disable_lro);\n\nstatic int call_netdevice_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\t   struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\tnetdev_notifier_info_init(&info, dev);\n\treturn nb->notifier_call(nb, val, &info);\n}\n\nstatic int dev_boot_phase = 1;\n\n/**\n * register_netdevice_notifier - register a network notifier block\n * @nb: notifier\n *\n * Register a notifier to be called when network device events occur.\n * The notifier passed is linked into the kernel structures and must\n * not be reused until it has been unregistered. A negative errno code\n * is returned on a failure.\n *\n * When registered all registration and up events are replayed\n * to the new notifier to allow device to have a race free\n * view of the network device list.\n */\n\nint register_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net_device *last;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_register(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\tif (dev_boot_phase)\n\t\tgoto unlock;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\terr = call_netdevice_notifier(nb, NETDEV_REGISTER, dev);\n\t\t\terr = notifier_to_errno(err);\n\t\t\tif (err)\n\t\t\t\tgoto rollback;\n\n\t\t\tif (!(dev->flags & IFF_UP))\n\t\t\t\tcontinue;\n\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UP, dev);\n\t\t}\n\t}\n\nunlock:\n\trtnl_unlock();\n\treturn err;\n\nrollback:\n\tlast = dev;\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev == last)\n\t\t\t\tgoto outroll;\n\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\n\noutroll:\n\traw_notifier_chain_unregister(&netdev_chain, nb);\n\tgoto unlock;\n}\nEXPORT_SYMBOL(register_netdevice_notifier);\n\n/**\n * unregister_netdevice_notifier - unregister a network notifier block\n * @nb: notifier\n *\n * Unregister a notifier previously registered by\n * register_netdevice_notifier(). The notifier is unlinked into the\n * kernel structures and may then be reused. A negative errno code\n * is returned on a failure.\n *\n * After unregistering unregister and down device events are synthesized\n * for all devices on the device list to the removed notifier to remove\n * the need for special case cleanup code.\n */\n\nint unregister_netdevice_notifier(struct notifier_block *nb)\n{\n\tstruct net_device *dev;\n\tstruct net *net;\n\tint err;\n\n\trtnl_lock();\n\terr = raw_notifier_chain_unregister(&netdev_chain, nb);\n\tif (err)\n\t\tgoto unlock;\n\n\tfor_each_net(net) {\n\t\tfor_each_netdev(net, dev) {\n\t\t\tif (dev->flags & IFF_UP) {\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_GOING_DOWN,\n\t\t\t\t\t\t\tdev);\n\t\t\t\tcall_netdevice_notifier(nb, NETDEV_DOWN, dev);\n\t\t\t}\n\t\t\tcall_netdevice_notifier(nb, NETDEV_UNREGISTER, dev);\n\t\t}\n\t}\nunlock:\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(unregister_netdevice_notifier);\n\n/**\n *\tcall_netdevice_notifiers_info - call all network notifier blocks\n *\t@val: value passed unmodified to notifier function\n *\t@dev: net_device pointer passed unmodified to notifier function\n *\t@info: notifier information data\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nstatic int call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t\t struct net_device *dev,\n\t\t\t\t\t struct netdev_notifier_info *info)\n{\n\tASSERT_RTNL();\n\tnetdev_notifier_info_init(info, dev);\n\treturn raw_notifier_call_chain(&netdev_chain, val, info);\n}\n\n/**\n *\tcall_netdevice_notifiers - call all network notifier blocks\n *      @val: value passed unmodified to notifier function\n *      @dev: net_device pointer passed unmodified to notifier function\n *\n *\tCall all network notifier blocks.  Parameters and return value\n *\tare as for raw_notifier_call_chain().\n */\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev)\n{\n\tstruct netdev_notifier_info info;\n\n\treturn call_netdevice_notifiers_info(val, dev, &info);\n}\nEXPORT_SYMBOL(call_netdevice_notifiers);\n\n#ifdef CONFIG_NET_INGRESS\nstatic struct static_key ingress_needed __read_mostly;\n\nvoid net_inc_ingress_queue(void)\n{\n\tstatic_key_slow_inc(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_ingress_queue);\n\nvoid net_dec_ingress_queue(void)\n{\n\tstatic_key_slow_dec(&ingress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_ingress_queue);\n#endif\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct static_key egress_needed __read_mostly;\n\nvoid net_inc_egress_queue(void)\n{\n\tstatic_key_slow_inc(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_inc_egress_queue);\n\nvoid net_dec_egress_queue(void)\n{\n\tstatic_key_slow_dec(&egress_needed);\n}\nEXPORT_SYMBOL_GPL(net_dec_egress_queue);\n#endif\n\nstatic struct static_key netstamp_needed __read_mostly;\n#ifdef HAVE_JUMP_LABEL\nstatic atomic_t netstamp_needed_deferred;\nstatic atomic_t netstamp_wanted;\nstatic void netstamp_clear(struct work_struct *work)\n{\n\tint deferred = atomic_xchg(&netstamp_needed_deferred, 0);\n\tint wanted;\n\n\twanted = atomic_add_return(deferred, &netstamp_wanted);\n\tif (wanted > 0)\n\t\tstatic_key_enable(&netstamp_needed);\n\telse\n\t\tstatic_key_disable(&netstamp_needed);\n}\nstatic DECLARE_WORK(netstamp_work, netstamp_clear);\n#endif\n\nvoid net_enable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 0)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted + 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_inc(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_key_slow_inc(&netstamp_needed);\n#endif\n}\nEXPORT_SYMBOL(net_enable_timestamp);\n\nvoid net_disable_timestamp(void)\n{\n#ifdef HAVE_JUMP_LABEL\n\tint wanted;\n\n\twhile (1) {\n\t\twanted = atomic_read(&netstamp_wanted);\n\t\tif (wanted <= 1)\n\t\t\tbreak;\n\t\tif (atomic_cmpxchg(&netstamp_wanted, wanted, wanted - 1) == wanted)\n\t\t\treturn;\n\t}\n\tatomic_dec(&netstamp_needed_deferred);\n\tschedule_work(&netstamp_work);\n#else\n\tstatic_key_slow_dec(&netstamp_needed);\n#endif\n}\nEXPORT_SYMBOL(net_disable_timestamp);\n\nstatic inline void net_timestamp_set(struct sk_buff *skb)\n{\n\tskb->tstamp = 0;\n\tif (static_key_false(&netstamp_needed))\n\t\t__net_timestamp(skb);\n}\n\n#define net_timestamp_check(COND, SKB)\t\t\t\\\n\tif (static_key_false(&netstamp_needed)) {\t\t\\\n\t\tif ((COND) && !(SKB)->tstamp)\t\\\n\t\t\t__net_timestamp(SKB);\t\t\\\n\t}\t\t\t\t\t\t\\\n\nbool is_skb_forwardable(const struct net_device *dev, const struct sk_buff *skb)\n{\n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tlen = dev->mtu + dev->hard_header_len + VLAN_HLEN;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t/* if TSO is enabled, we don't care about the length as the packet\n\t * could be forwarded without being segmented before\n\t */\n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(is_skb_forwardable);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb);\n\n\tif (likely(!ret)) {\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tskb_postpull_rcsum(skb, eth_hdr(skb), ETH_HLEN);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__dev_forward_skb);\n\n/**\n * dev_forward_skb - loopback an skb to another netif\n *\n * @dev: destination network device\n * @skb: buffer to forward\n *\n * return values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped, but freed)\n *\n * dev_forward_skb can be used for injecting an skb from the\n * start_xmit function of one device into the receive queue\n * of another device.\n *\n * The receiving device may be in another namespace, so\n * we have to clear all information in the skb that could\n * impact namespace isolation.\n */\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn __dev_forward_skb(dev, skb) ?: netif_rx_internal(skb);\n}\nEXPORT_SYMBOL_GPL(dev_forward_skb);\n\nstatic inline int deliver_skb(struct sk_buff *skb,\n\t\t\t      struct packet_type *pt_prev,\n\t\t\t      struct net_device *orig_dev)\n{\n\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\treturn -ENOMEM;\n\trefcount_inc(&skb->users);\n\treturn pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n}\n\nstatic inline void deliver_ptype_list_skb(struct sk_buff *skb,\n\t\t\t\t\t  struct packet_type **pt,\n\t\t\t\t\t  struct net_device *orig_dev,\n\t\t\t\t\t  __be16 type,\n\t\t\t\t\t  struct list_head *ptype_list)\n{\n\tstruct packet_type *ptype, *pt_prev = *pt;\n\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\tif (ptype->type != type)\n\t\t\tcontinue;\n\t\tif (pt_prev)\n\t\t\tdeliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\t*pt = pt_prev;\n}\n\nstatic inline bool skb_loop_sk(struct packet_type *ptype, struct sk_buff *skb)\n{\n\tif (!ptype->af_packet_priv || !skb->sk)\n\t\treturn false;\n\n\tif (ptype->id_match)\n\t\treturn ptype->id_match(ptype, skb->sk);\n\telse if ((struct sock *)ptype->af_packet_priv == skb->sk)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n *\tSupport routine. Sends outgoing frames to any network\n *\ttaps currently in use.\n */\n\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct packet_type *ptype;\n\tstruct sk_buff *skb2 = NULL;\n\tstruct packet_type *pt_prev = NULL;\n\tstruct list_head *ptype_list = &ptype_all;\n\n\trcu_read_lock();\nagain:\n\tlist_for_each_entry_rcu(ptype, ptype_list, list) {\n\t\t/* Never send packets back to the socket\n\t\t * they originated from - MvS (miquels@drinkel.ow.org)\n\t\t */\n\t\tif (skb_loop_sk(ptype, skb))\n\t\t\tcontinue;\n\n\t\tif (pt_prev) {\n\t\t\tdeliver_skb(skb2, pt_prev, skb->dev);\n\t\t\tpt_prev = ptype;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* need to clone skb, done only once */\n\t\tskb2 = skb_clone(skb, GFP_ATOMIC);\n\t\tif (!skb2)\n\t\t\tgoto out_unlock;\n\n\t\tnet_timestamp_set(skb2);\n\n\t\t/* skb->nh should be correctly\n\t\t * set by sender, so that the second statement is\n\t\t * just protection against buggy protocols.\n\t\t */\n\t\tskb_reset_mac_header(skb2);\n\n\t\tif (skb_network_header(skb2) < skb2->data ||\n\t\t    skb_network_header(skb2) > skb_tail_pointer(skb2)) {\n\t\t\tnet_crit_ratelimited(\"protocol %04x is buggy, dev %s\\n\",\n\t\t\t\t\t     ntohs(skb2->protocol),\n\t\t\t\t\t     dev->name);\n\t\t\tskb_reset_network_header(skb2);\n\t\t}\n\n\t\tskb2->transport_header = skb2->network_header;\n\t\tskb2->pkt_type = PACKET_OUTGOING;\n\t\tpt_prev = ptype;\n\t}\n\n\tif (ptype_list == &ptype_all) {\n\t\tptype_list = &dev->ptype_all;\n\t\tgoto again;\n\t}\nout_unlock:\n\tif (pt_prev) {\n\t\tif (!skb_orphan_frags_rx(skb2, GFP_ATOMIC))\n\t\t\tpt_prev->func(skb2, skb->dev, pt_prev, skb->dev);\n\t\telse\n\t\t\tkfree_skb(skb2);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(dev_queue_xmit_nit);\n\n/**\n * netif_setup_tc - Handle tc mappings on real_num_tx_queues change\n * @dev: Network device\n * @txq: number of queues available\n *\n * If real_num_tx_queues is changed the tc mappings may no longer be\n * valid. To resolve this verify the tc mapping remains valid and if\n * not NULL the mapping. With no priorities mapping to this\n * offset/count pair it will no longer be used. In the worst case TC0\n * is invalid nothing can be done so disable priority mappings. If is\n * expected that drivers will fix this mapping if they can before\n * calling netif_set_real_num_tx_queues.\n */\nstatic void netif_setup_tc(struct net_device *dev, unsigned int txq)\n{\n\tint i;\n\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\n\t/* If TC0 is invalidated disable TC mapping */\n\tif (tc->offset + tc->count > txq) {\n\t\tpr_warn(\"Number of in use tx queues changed invalidating tc mappings. Priority traffic classification disabled!\\n\");\n\t\tdev->num_tc = 0;\n\t\treturn;\n\t}\n\n\t/* Invalidated prio to tc mappings set to TC0 */\n\tfor (i = 1; i < TC_BITMASK + 1; i++) {\n\t\tint q = netdev_get_prio_tc_map(dev, i);\n\n\t\ttc = &dev->tc_to_txq[q];\n\t\tif (tc->offset + tc->count > txq) {\n\t\t\tpr_warn(\"Number of in use tx queues changed. Priority %i to tc mapping %i is no longer valid. Setting map to 0\\n\",\n\t\t\t\ti, q);\n\t\t\tnetdev_set_prio_tc_map(dev, i, 0);\n\t\t}\n\t}\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq)\n{\n\tif (dev->num_tc) {\n\t\tstruct netdev_tc_txq *tc = &dev->tc_to_txq[0];\n\t\tint i;\n\n\t\tfor (i = 0; i < TC_MAX_QUEUE; i++, tc++) {\n\t\t\tif ((txq - tc->offset) < tc->count)\n\t\t\t\treturn i;\n\t\t}\n\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_XPS\nstatic DEFINE_MUTEX(xps_map_mutex);\n#define xmap_dereference(P)\t\t\\\n\trcu_dereference_protected((P), lockdep_is_held(&xps_map_mutex))\n\nstatic bool remove_xps_queue(struct xps_dev_maps *dev_maps,\n\t\t\t     int tci, u16 index)\n{\n\tstruct xps_map *map = NULL;\n\tint pos;\n\n\tif (dev_maps)\n\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\tif (!map)\n\t\treturn false;\n\n\tfor (pos = map->len; pos--;) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\n\t\tif (map->len > 1) {\n\t\t\tmap->queues[pos] = map->queues[--map->len];\n\t\t\tbreak;\n\t\t}\n\n\t\tRCU_INIT_POINTER(dev_maps->cpu_map[tci], NULL);\n\t\tkfree_rcu(map, rcu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool remove_xps_queue_cpu(struct net_device *dev,\n\t\t\t\t struct xps_dev_maps *dev_maps,\n\t\t\t\t int cpu, u16 offset, u16 count)\n{\n\tint num_tc = dev->num_tc ? : 1;\n\tbool active = false;\n\tint tci;\n\n\tfor (tci = cpu * num_tc; num_tc--; tci++) {\n\t\tint i, j;\n\n\t\tfor (i = count, j = offset; i--; j++) {\n\t\t\tif (!remove_xps_queue(dev_maps, cpu, j))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tactive |= i < 0;\n\t}\n\n\treturn active;\n}\n\nstatic void netif_reset_xps_queues(struct net_device *dev, u16 offset,\n\t\t\t\t   u16 count)\n{\n\tstruct xps_dev_maps *dev_maps;\n\tint cpu, i;\n\tbool active = false;\n\n\tmutex_lock(&xps_map_mutex);\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\tfor_each_possible_cpu(cpu)\n\t\tactive |= remove_xps_queue_cpu(dev, dev_maps, cpu,\n\t\t\t\t\t       offset, count);\n\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\n\tfor (i = offset + (count - 1); count--; i--)\n\t\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, i),\n\t\t\t\t\t     NUMA_NO_NODE);\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n}\n\nstatic void netif_reset_xps_queues_gt(struct net_device *dev, u16 index)\n{\n\tnetif_reset_xps_queues(dev, index, dev->num_tx_queues - index);\n}\n\nstatic struct xps_map *expand_xps_map(struct xps_map *map,\n\t\t\t\t      int cpu, u16 index)\n{\n\tstruct xps_map *new_map;\n\tint alloc_len = XPS_MIN_MAP_ALLOC;\n\tint i, pos;\n\n\tfor (pos = 0; map && pos < map->len; pos++) {\n\t\tif (map->queues[pos] != index)\n\t\t\tcontinue;\n\t\treturn map;\n\t}\n\n\t/* Need to add queue to this CPU's existing map */\n\tif (map) {\n\t\tif (pos < map->alloc_len)\n\t\t\treturn map;\n\n\t\talloc_len = map->alloc_len * 2;\n\t}\n\n\t/* Need to allocate new map to store queue on this CPU's map */\n\tnew_map = kzalloc_node(XPS_MAP_SIZE(alloc_len), GFP_KERNEL,\n\t\t\t       cpu_to_node(cpu));\n\tif (!new_map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < pos; i++)\n\t\tnew_map->queues[i] = map->queues[i];\n\tnew_map->alloc_len = alloc_len;\n\tnew_map->len = pos;\n\n\treturn new_map;\n}\n\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index)\n{\n\tstruct xps_dev_maps *dev_maps, *new_dev_maps = NULL;\n\tint i, cpu, tci, numa_node_id = -2;\n\tint maps_sz, num_tc = 1, tc = 0;\n\tstruct xps_map *map, *new_map;\n\tbool active = false;\n\n\tif (dev->num_tc) {\n\t\tnum_tc = dev->num_tc;\n\t\ttc = netdev_txq_to_tc(dev, index);\n\t\tif (tc < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmaps_sz = XPS_DEV_MAPS_SIZE(num_tc);\n\tif (maps_sz < L1_CACHE_BYTES)\n\t\tmaps_sz = L1_CACHE_BYTES;\n\n\tmutex_lock(&xps_map_mutex);\n\n\tdev_maps = xmap_dereference(dev->xps_maps);\n\n\t/* allocate memory for queue storage */\n\tfor_each_cpu_and(cpu, cpu_online_mask, mask) {\n\t\tif (!new_dev_maps)\n\t\t\tnew_dev_maps = kzalloc(maps_sz, GFP_KERNEL);\n\t\tif (!new_dev_maps) {\n\t\t\tmutex_unlock(&xps_map_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttci = cpu * num_tc + tc;\n\t\tmap = dev_maps ? xmap_dereference(dev_maps->cpu_map[tci]) :\n\t\t\t\t NULL;\n\n\t\tmap = expand_xps_map(map, cpu, index);\n\t\tif (!map)\n\t\t\tgoto error;\n\n\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t}\n\n\tif (!new_dev_maps)\n\t\tgoto out_no_new_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = tc, tci = cpu * num_tc; dev_maps && i--; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\n\t\t/* We need to explicitly update tci as prevous loop\n\t\t * could break out early if dev_maps is NULL.\n\t\t */\n\t\ttci = cpu * num_tc + tc;\n\n\t\tif (cpumask_test_cpu(cpu, mask) && cpu_online(cpu)) {\n\t\t\t/* add queue to CPU maps */\n\t\t\tint pos = 0;\n\n\t\t\tmap = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\twhile ((pos < map->len) && (map->queues[pos] != index))\n\t\t\t\tpos++;\n\n\t\t\tif (pos == map->len)\n\t\t\t\tmap->queues[map->len++] = index;\n#ifdef CONFIG_NUMA\n\t\t\tif (numa_node_id == -2)\n\t\t\t\tnuma_node_id = cpu_to_node(cpu);\n\t\t\telse if (numa_node_id != cpu_to_node(cpu))\n\t\t\t\tnuma_node_id = -1;\n#endif\n\t\t} else if (dev_maps) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\n\t\t/* copy maps belonging to foreign traffic classes */\n\t\tfor (i = num_tc - tc, tci++; dev_maps && --i; tci++) {\n\t\t\t/* fill in the new device map from the old device map */\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tRCU_INIT_POINTER(new_dev_maps->cpu_map[tci], map);\n\t\t}\n\t}\n\n\trcu_assign_pointer(dev->xps_maps, new_dev_maps);\n\n\t/* Cleanup old maps */\n\tif (!dev_maps)\n\t\tgoto out_no_old_maps;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = num_tc, tci = cpu * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\tmap = xmap_dereference(dev_maps->cpu_map[tci]);\n\t\t\tif (map && map != new_map)\n\t\t\t\tkfree_rcu(map, rcu);\n\t\t}\n\t}\n\n\tkfree_rcu(dev_maps, rcu);\n\nout_no_old_maps:\n\tdev_maps = new_dev_maps;\n\tactive = true;\n\nout_no_new_maps:\n\t/* update Tx queue numa node */\n\tnetdev_queue_numa_node_write(netdev_get_tx_queue(dev, index),\n\t\t\t\t     (numa_node_id >= 0) ? numa_node_id :\n\t\t\t\t     NUMA_NO_NODE);\n\n\tif (!dev_maps)\n\t\tgoto out_no_maps;\n\n\t/* removes queue from unused CPUs */\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = tc, tci = cpu * num_tc; i--; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tif (!cpumask_test_cpu(cpu, mask) || !cpu_online(cpu))\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t\tfor (i = num_tc - tc, tci++; --i; tci++)\n\t\t\tactive |= remove_xps_queue(dev_maps, tci, index);\n\t}\n\n\t/* free map if not active */\n\tif (!active) {\n\t\tRCU_INIT_POINTER(dev->xps_maps, NULL);\n\t\tkfree_rcu(dev_maps, rcu);\n\t}\n\nout_no_maps:\n\tmutex_unlock(&xps_map_mutex);\n\n\treturn 0;\nerror:\n\t/* remove any maps that we added */\n\tfor_each_possible_cpu(cpu) {\n\t\tfor (i = num_tc, tci = cpu * num_tc; i--; tci++) {\n\t\t\tnew_map = xmap_dereference(new_dev_maps->cpu_map[tci]);\n\t\t\tmap = dev_maps ?\n\t\t\t      xmap_dereference(dev_maps->cpu_map[tci]) :\n\t\t\t      NULL;\n\t\t\tif (new_map && new_map != map)\n\t\t\t\tkfree(new_map);\n\t\t}\n\t}\n\n\tmutex_unlock(&xps_map_mutex);\n\n\tkfree(new_dev_maps);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(netif_set_xps_queue);\n\n#endif\nvoid netdev_reset_tc(struct net_device *dev)\n{\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tdev->num_tc = 0;\n\tmemset(dev->tc_to_txq, 0, sizeof(dev->tc_to_txq));\n\tmemset(dev->prio_tc_map, 0, sizeof(dev->prio_tc_map));\n}\nEXPORT_SYMBOL(netdev_reset_tc);\n\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues(dev, offset, count);\n#endif\n\tdev->tc_to_txq[tc].count = count;\n\tdev->tc_to_txq[tc].offset = offset;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_tc_queue);\n\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc)\n{\n\tif (num_tc > TC_MAX_QUEUE)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_XPS\n\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\tdev->num_tc = num_tc;\n\treturn 0;\n}\nEXPORT_SYMBOL(netdev_set_num_tc);\n\n/*\n * Routine to help set real_num_tx_queues. To avoid skbs mapped to queues\n * greater then real_num_tx_queues stale skbs on the qdisc must be flushed.\n */\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq)\n{\n\tint rc;\n\n\tif (txq < 1 || txq > dev->num_tx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED ||\n\t    dev->reg_state == NETREG_UNREGISTERING) {\n\t\tASSERT_RTNL();\n\n\t\trc = netdev_queue_update_kobjects(dev, dev->real_num_tx_queues,\n\t\t\t\t\t\t  txq);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tif (dev->num_tc)\n\t\t\tnetif_setup_tc(dev, txq);\n\n\t\tif (txq < dev->real_num_tx_queues) {\n\t\t\tqdisc_reset_all_tx_gt(dev, txq);\n#ifdef CONFIG_XPS\n\t\t\tnetif_reset_xps_queues_gt(dev, txq);\n#endif\n\t\t}\n\t}\n\n\tdev->real_num_tx_queues = txq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_tx_queues);\n\n#ifdef CONFIG_SYSFS\n/**\n *\tnetif_set_real_num_rx_queues - set actual number of RX queues used\n *\t@dev: Network device\n *\t@rxq: Actual number of RX queues\n *\n *\tThis must be called either with the rtnl_lock held or before\n *\tregistration of the net device.  Returns 0 on success, or a\n *\tnegative error code.  If called before registration, it always\n *\tsucceeds.\n */\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq)\n{\n\tint rc;\n\n\tif (rxq < 1 || rxq > dev->num_rx_queues)\n\t\treturn -EINVAL;\n\n\tif (dev->reg_state == NETREG_REGISTERED) {\n\t\tASSERT_RTNL();\n\n\t\trc = net_rx_queue_update_kobjects(dev, dev->real_num_rx_queues,\n\t\t\t\t\t\t  rxq);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\tdev->real_num_rx_queues = rxq;\n\treturn 0;\n}\nEXPORT_SYMBOL(netif_set_real_num_rx_queues);\n#endif\n\n/**\n * netif_get_num_default_rss_queues - default number of RSS queues\n *\n * This routine should set an upper limit on the number of RSS queues\n * used by default by multiqueue devices.\n */\nint netif_get_num_default_rss_queues(void)\n{\n\treturn is_kdump_kernel() ?\n\t\t1 : min_t(int, DEFAULT_MAX_NUM_RSS_QUEUES, num_online_cpus());\n}\nEXPORT_SYMBOL(netif_get_num_default_rss_queues);\n\nstatic void __netif_reschedule(struct Qdisc *q)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsd = this_cpu_ptr(&softnet_data);\n\tq->next_sched = NULL;\n\t*sd->output_queue_tailp = q;\n\tsd->output_queue_tailp = &q->next_sched;\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\n\nvoid __netif_schedule(struct Qdisc *q)\n{\n\tif (!test_and_set_bit(__QDISC_STATE_SCHED, &q->state))\n\t\t__netif_reschedule(q);\n}\nEXPORT_SYMBOL(__netif_schedule);\n\nstruct dev_kfree_skb_cb {\n\tenum skb_free_reason reason;\n};\n\nstatic struct dev_kfree_skb_cb *get_kfree_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct dev_kfree_skb_cb *)skb->cb;\n}\n\nvoid netif_schedule_queue(struct netdev_queue *txq)\n{\n\trcu_read_lock();\n\tif (!(txq->state & QUEUE_STATE_ANY_XOFF)) {\n\t\tstruct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\t__netif_schedule(q);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(netif_schedule_queue);\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue)\n{\n\tif (test_and_clear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state)) {\n\t\tstruct Qdisc *q;\n\n\t\trcu_read_lock();\n\t\tq = rcu_dereference(dev_queue->qdisc);\n\t\t__netif_schedule(q);\n\t\trcu_read_unlock();\n\t}\n}\nEXPORT_SYMBOL(netif_tx_wake_queue);\n\nvoid __dev_kfree_skb_irq(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\tif (likely(refcount_read(&skb->users) == 1)) {\n\t\tsmp_rmb();\n\t\trefcount_set(&skb->users, 0);\n\t} else if (likely(!refcount_dec_and_test(&skb->users))) {\n\t\treturn;\n\t}\n\tget_kfree_skb_cb(skb)->reason = reason;\n\tlocal_irq_save(flags);\n\tskb->next = __this_cpu_read(softnet_data.completion_queue);\n\t__this_cpu_write(softnet_data.completion_queue, skb);\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_irq);\n\nvoid __dev_kfree_skb_any(struct sk_buff *skb, enum skb_free_reason reason)\n{\n\tif (in_irq() || irqs_disabled())\n\t\t__dev_kfree_skb_irq(skb, reason);\n\telse\n\t\tdev_kfree_skb(skb);\n}\nEXPORT_SYMBOL(__dev_kfree_skb_any);\n\n\n/**\n * netif_device_detach - mark device as removed\n * @dev: network device\n *\n * Mark device as removed from system and therefore no longer available.\n */\nvoid netif_device_detach(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_stop_all_queues(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_detach);\n\n/**\n * netif_device_attach - mark device as attached\n * @dev: network device\n *\n * Mark device as attached from system and restart if needed.\n */\nvoid netif_device_attach(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_PRESENT, &dev->state) &&\n\t    netif_running(dev)) {\n\t\tnetif_tx_wake_all_queues(dev);\n\t\t__netdev_watchdog_up(dev);\n\t}\n}\nEXPORT_SYMBOL(netif_device_attach);\n\n/*\n * Returns a Tx hash based on the given packet descriptor a Tx queues' number\n * to be used as a distribution range.\n */\nu16 __skb_tx_hash(const struct net_device *dev, struct sk_buff *skb,\n\t\t  unsigned int num_tx_queues)\n{\n\tu32 hash;\n\tu16 qoffset = 0;\n\tu16 qcount = num_tx_queues;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\thash = skb_get_rx_queue(skb);\n\t\twhile (unlikely(hash >= num_tx_queues))\n\t\t\thash -= num_tx_queues;\n\t\treturn hash;\n\t}\n\n\tif (dev->num_tc) {\n\t\tu8 tc = netdev_get_prio_tc_map(dev, skb->priority);\n\n\t\tqoffset = dev->tc_to_txq[tc].offset;\n\t\tqcount = dev->tc_to_txq[tc].count;\n\t}\n\n\treturn (u16) reciprocal_scale(skb_get_hash(skb), qcount) + qoffset;\n}\nEXPORT_SYMBOL(__skb_tx_hash);\n\nstatic void skb_warn_bad_offload(const struct sk_buff *skb)\n{\n\tstatic const netdev_features_t null_features;\n\tstruct net_device *dev = skb->dev;\n\tconst char *name = \"\";\n\n\tif (!net_ratelimit())\n\t\treturn;\n\n\tif (dev) {\n\t\tif (dev->dev.parent)\n\t\t\tname = dev_driver_string(dev->dev.parent);\n\t\telse\n\t\t\tname = netdev_name(dev);\n\t}\n\tWARN(1, \"%s: caps=(%pNF, %pNF) len=%d data_len=%d gso_size=%d \"\n\t     \"gso_type=%d ip_summed=%d\\n\",\n\t     name, dev ? &dev->features : &null_features,\n\t     skb->sk ? &skb->sk->sk_route_caps : &null_features,\n\t     skb->len, skb->data_len, skb_shinfo(skb)->gso_size,\n\t     skb_shinfo(skb)->gso_type, skb->ip_summed);\n}\n\n/*\n * Invalidate hardware checksum when packet is to be mangled, and\n * complete checksum manually on outgoing path.\n */\nint skb_checksum_help(struct sk_buff *skb)\n{\n\t__wsum csum;\n\tint ret = 0, offset;\n\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tgoto out_set_summed;\n\n\tif (unlikely(skb_shinfo(skb)->gso_size)) {\n\t\tskb_warn_bad_offload(skb);\n\t\treturn -EINVAL;\n\t}\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (skb_has_shared_frag(skb)) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\toffset = skb_checksum_start_offset(skb);\n\tBUG_ON(offset >= skb_headlen(skb));\n\tcsum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\toffset += skb->csum_offset;\n\tBUG_ON(offset + sizeof(__sum16) > skb_headlen(skb));\n\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__sum16))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum) ?: CSUM_MANGLED_0;\nout_set_summed:\n\tskb->ip_summed = CHECKSUM_NONE;\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL(skb_checksum_help);\n\nint skb_crc32c_csum_help(struct sk_buff *skb)\n{\n\t__le32 crc32c_csum;\n\tint ret = 0, offset, start;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\tgoto out;\n\n\tif (unlikely(skb_is_gso(skb)))\n\t\tgoto out;\n\n\t/* Before computing a checksum, we should make sure no frag could\n\t * be modified by an external entity : checksum could be wrong.\n\t */\n\tif (unlikely(skb_has_shared_frag(skb))) {\n\t\tret = __skb_linearize(skb);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tstart = skb_checksum_start_offset(skb);\n\toffset = start + offsetof(struct sctphdr, checksum);\n\tif (WARN_ON_ONCE(offset >= skb_headlen(skb))) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (skb_cloned(skb) &&\n\t    !skb_clone_writable(skb, offset + sizeof(__le32))) {\n\t\tret = pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\tcrc32c_csum = cpu_to_le32(~__skb_checksum(skb, start,\n\t\t\t\t\t\t  skb->len - start, ~(__u32)0,\n\t\t\t\t\t\t  crc32c_csum_stub));\n\t*(__le32 *)(skb->data + offset) = crc32c_csum;\n\tskb->ip_summed = CHECKSUM_NONE;\n\tskb->csum_not_inet = 0;\nout:\n\treturn ret;\n}\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth)\n{\n\t__be16 type = skb->protocol;\n\n\t/* Tunnel gso handlers can set protocol to ethernet. */\n\tif (type == htons(ETH_P_TEB)) {\n\t\tstruct ethhdr *eth;\n\n\t\tif (unlikely(!pskb_may_pull(skb, sizeof(struct ethhdr))))\n\t\t\treturn 0;\n\n\t\teth = (struct ethhdr *)skb_mac_header(skb);\n\t\ttype = eth->h_proto;\n\t}\n\n\treturn __vlan_get_protocol(skb, type, depth);\n}\n\n/**\n *\tskb_mac_gso_segment - mac layer segmentation handler.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n */\nstruct sk_buff *skb_mac_gso_segment(struct sk_buff *skb,\n\t\t\t\t    netdev_features_t features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EPROTONOSUPPORT);\n\tstruct packet_offload *ptype;\n\tint vlan_depth = skb->mac_len;\n\t__be16 type = skb_network_protocol(skb, &vlan_depth);\n\n\tif (unlikely(!type))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__skb_pull(skb, vlan_depth);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, &offload_base, list) {\n\t\tif (ptype->type == type && ptype->callbacks.gso_segment) {\n\t\t\tsegs = ptype->callbacks.gso_segment(skb, features);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t__skb_push(skb, skb->data - skb_mac_header(skb));\n\n\treturn segs;\n}\nEXPORT_SYMBOL(skb_mac_gso_segment);\n\n\n/* openvswitch calls this on rx path, so we need a different check.\n */\nstatic inline bool skb_needs_check(struct sk_buff *skb, bool tx_path)\n{\n\tif (tx_path)\n\t\treturn skb->ip_summed != CHECKSUM_PARTIAL;\n\n\treturn skb->ip_summed == CHECKSUM_NONE;\n}\n\n/**\n *\t__skb_gso_segment - Perform segmentation on skb.\n *\t@skb: buffer to segment\n *\t@features: features for the output path (see dev->features)\n *\t@tx_path: whether it is called in TX path\n *\n *\tThis function segments the given skb and returns a list of segments.\n *\n *\tIt may return NULL if the skb requires no segmentation.  This is\n *\tonly possible when GSO is used for verifying header integrity.\n *\n *\tSegmentation preserves SKB_SGO_CB_OFFSET bytes of previous skb cb.\n */\nstruct sk_buff *__skb_gso_segment(struct sk_buff *skb,\n\t\t\t\t  netdev_features_t features, bool tx_path)\n{\n\tstruct sk_buff *segs;\n\n\tif (unlikely(skb_needs_check(skb, tx_path))) {\n\t\tint err;\n\n\t\t/* We're going to init ->check field in TCP or UDP header */\n\t\terr = skb_cow_head(skb, 0);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\t/* Only report GSO partial support if it will enable us to\n\t * support segmentation on this frame without needing additional\n\t * work.\n\t */\n\tif (features & NETIF_F_GSO_PARTIAL) {\n\t\tnetdev_features_t partial_features = NETIF_F_GSO_ROBUST;\n\t\tstruct net_device *dev = skb->dev;\n\n\t\tpartial_features |= dev->features & dev->gso_partial_features;\n\t\tif (!skb_gso_ok(skb, features | partial_features))\n\t\t\tfeatures &= ~NETIF_F_GSO_PARTIAL;\n\t}\n\n\tBUILD_BUG_ON(SKB_SGO_CB_OFFSET +\n\t\t     sizeof(*SKB_GSO_CB(skb)) > sizeof(skb->cb));\n\n\tSKB_GSO_CB(skb)->mac_offset = skb_headroom(skb);\n\tSKB_GSO_CB(skb)->encap_level = 0;\n\n\tskb_reset_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tsegs = skb_mac_gso_segment(skb, features);\n\n\tif (unlikely(skb_needs_check(skb, tx_path)))\n\t\tskb_warn_bad_offload(skb);\n\n\treturn segs;\n}\nEXPORT_SYMBOL(__skb_gso_segment);\n\n/* Take action when hardware reception checksum errors are detected. */\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev)\n{\n\tif (net_ratelimit()) {\n\t\tpr_err(\"%s: hw csum failure\\n\", dev ? dev->name : \"<unknown>\");\n\t\tdump_stack();\n\t}\n}\nEXPORT_SYMBOL(netdev_rx_csum_fault);\n#endif\n\n/* Actually, we should eliminate this check as soon as we know, that:\n * 1. IOMMU is present and allows to map all the memory.\n * 2. No high memory really exists on this machine.\n */\n\nstatic int illegal_highdma(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_HIGHMEM\n\tint i;\n\n\tif (!(dev->features & NETIF_F_HIGHDMA)) {\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\t\tif (PageHighMem(skb_frag_page(frag)))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (PCI_DMA_BUS_IS_PHYS) {\n\t\tstruct device *pdev = dev->dev.parent;\n\n\t\tif (!pdev)\n\t\t\treturn 0;\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\t\tdma_addr_t addr = page_to_phys(skb_frag_page(frag));\n\n\t\t\tif (!pdev->dma_mask || addr + PAGE_SIZE - 1 > *pdev->dma_mask)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n#endif\n\treturn 0;\n}\n\n/* If MPLS offload request, verify we are testing hardware MPLS features\n * instead of standard features for the netdev.\n */\n#if IS_ENABLED(CONFIG_NET_MPLS_GSO)\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\tif (eth_p_mpls(type))\n\t\tfeatures &= skb->dev->mpls_features;\n\n\treturn features;\n}\n#else\nstatic netdev_features_t net_mpls_features(struct sk_buff *skb,\n\t\t\t\t\t   netdev_features_t features,\n\t\t\t\t\t   __be16 type)\n{\n\treturn features;\n}\n#endif\n\nstatic netdev_features_t harmonize_features(struct sk_buff *skb,\n\tnetdev_features_t features)\n{\n\tint tmp;\n\t__be16 type;\n\n\ttype = skb_network_protocol(skb, &tmp);\n\tfeatures = net_mpls_features(skb, features, type);\n\n\tif (skb->ip_summed != CHECKSUM_NONE &&\n\t    !can_checksum_protocol(features, type)) {\n\t\tfeatures &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);\n\t}\n\tif (illegal_highdma(skb->dev, skb))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\treturn features;\n}\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\treturn features;\n}\nEXPORT_SYMBOL(passthru_features_check);\n\nstatic netdev_features_t dflt_features_check(const struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\treturn vlan_features_check(skb, features);\n}\n\nstatic netdev_features_t gso_features_check(const struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t    netdev_features_t features)\n{\n\tu16 gso_segs = skb_shinfo(skb)->gso_segs;\n\n\tif (gso_segs > dev->gso_max_segs)\n\t\treturn features & ~NETIF_F_GSO_MASK;\n\n\t/* Support for GSO partial features requires software\n\t * intervention before we can actually process the packets\n\t * so we need to strip support for any partial features now\n\t * and we can pull them back in after we have partially\n\t * segmented the frame.\n\t */\n\tif (!(skb_shinfo(skb)->gso_type & SKB_GSO_PARTIAL))\n\t\tfeatures &= ~dev->gso_partial_features;\n\n\t/* Make sure to clear the IPv4 ID mangling feature if the\n\t * IPv4 header has the potential to be fragmented.\n\t */\n\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4) {\n\t\tstruct iphdr *iph = skb->encapsulation ?\n\t\t\t\t    inner_ip_hdr(skb) : ip_hdr(skb);\n\n\t\tif (!(iph->frag_off & htons(IP_DF)))\n\t\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\t}\n\n\treturn features;\n}\n\nnetdev_features_t netif_skb_features(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb->dev;\n\tnetdev_features_t features = dev->features;\n\n\tif (skb_is_gso(skb))\n\t\tfeatures = gso_features_check(skb, dev, features);\n\n\t/* If encapsulation offload request, verify we are testing\n\t * hardware encapsulation features instead of standard\n\t * features for the netdev\n\t */\n\tif (skb->encapsulation)\n\t\tfeatures &= dev->hw_enc_features;\n\n\tif (skb_vlan_tagged(skb))\n\t\tfeatures = netdev_intersect_features(features,\n\t\t\t\t\t\t     dev->vlan_features |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t\t\t     NETIF_F_HW_VLAN_STAG_TX);\n\n\tif (dev->netdev_ops->ndo_features_check)\n\t\tfeatures &= dev->netdev_ops->ndo_features_check(skb, dev,\n\t\t\t\t\t\t\t\tfeatures);\n\telse\n\t\tfeatures &= dflt_features_check(skb, dev, features);\n\n\treturn harmonize_features(skb, features);\n}\nEXPORT_SYMBOL(netif_skb_features);\n\nstatic int xmit_one(struct sk_buff *skb, struct net_device *dev,\n\t\t    struct netdev_queue *txq, bool more)\n{\n\tunsigned int len;\n\tint rc;\n\n\tif (!list_empty(&ptype_all) || !list_empty(&dev->ptype_all))\n\t\tdev_queue_xmit_nit(skb, dev);\n\n\tlen = skb->len;\n\ttrace_net_dev_start_xmit(skb, dev);\n\trc = netdev_start_xmit(skb, dev, txq, more);\n\ttrace_net_dev_xmit(skb, rc, dev, len);\n\n\treturn rc;\n}\n\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *first, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret)\n{\n\tstruct sk_buff *skb = first;\n\tint rc = NETDEV_TX_OK;\n\n\twhile (skb) {\n\t\tstruct sk_buff *next = skb->next;\n\n\t\tskb->next = NULL;\n\t\trc = xmit_one(skb, dev, txq, next != NULL);\n\t\tif (unlikely(!dev_xmit_complete(rc))) {\n\t\t\tskb->next = next;\n\t\t\tgoto out;\n\t\t}\n\n\t\tskb = next;\n\t\tif (netif_xmit_stopped(txq) && skb) {\n\t\t\trc = NETDEV_TX_BUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\t*ret = rc;\n\treturn skb;\n}\n\nstatic struct sk_buff *validate_xmit_vlan(struct sk_buff *skb,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (skb_vlan_tag_present(skb) &&\n\t    !vlan_hw_offload_capable(features, skb->vlan_proto))\n\t\tskb = __vlan_hwaccel_push_inside(skb);\n\treturn skb;\n}\n\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features)\n{\n\tif (unlikely(skb->csum_not_inet))\n\t\treturn !!(features & NETIF_F_SCTP_CRC) ? 0 :\n\t\t\tskb_crc32c_csum_help(skb);\n\n\treturn !!(features & NETIF_F_CSUM_MASK) ? 0 : skb_checksum_help(skb);\n}\nEXPORT_SYMBOL(skb_csum_hwoffload_help);\n\nstatic struct sk_buff *validate_xmit_skb(struct sk_buff *skb, struct net_device *dev)\n{\n\tnetdev_features_t features;\n\n\tfeatures = netif_skb_features(skb);\n\tskb = validate_xmit_vlan(skb, features);\n\tif (unlikely(!skb))\n\t\tgoto out_null;\n\n\tif (netif_needs_gso(skb, features)) {\n\t\tstruct sk_buff *segs;\n\n\t\tsegs = skb_gso_segment(skb, features);\n\t\tif (IS_ERR(segs)) {\n\t\t\tgoto out_kfree_skb;\n\t\t} else if (segs) {\n\t\t\tconsume_skb(skb);\n\t\t\tskb = segs;\n\t\t}\n\t} else {\n\t\tif (skb_needs_linearize(skb, features) &&\n\t\t    __skb_linearize(skb))\n\t\t\tgoto out_kfree_skb;\n\n\t\tif (validate_xmit_xfrm(skb, features))\n\t\t\tgoto out_kfree_skb;\n\n\t\t/* If packet is not checksummed and device does not\n\t\t * support checksumming for this protocol, complete\n\t\t * checksumming here.\n\t\t */\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (skb->encapsulation)\n\t\t\t\tskb_set_inner_transport_header(skb,\n\t\t\t\t\t\t\t       skb_checksum_start_offset(skb));\n\t\t\telse\n\t\t\t\tskb_set_transport_header(skb,\n\t\t\t\t\t\t\t skb_checksum_start_offset(skb));\n\t\t\tif (skb_csum_hwoffload_help(skb, features))\n\t\t\t\tgoto out_kfree_skb;\n\t\t}\n\t}\n\n\treturn skb;\n\nout_kfree_skb:\n\tkfree_skb(skb);\nout_null:\n\tatomic_long_inc(&dev->tx_dropped);\n\treturn NULL;\n}\n\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct sk_buff *next, *head = NULL, *tail;\n\n\tfor (; skb != NULL; skb = next) {\n\t\tnext = skb->next;\n\t\tskb->next = NULL;\n\n\t\t/* in case skb wont be segmented, point to itself */\n\t\tskb->prev = skb;\n\n\t\tskb = validate_xmit_skb(skb, dev);\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tif (!head)\n\t\t\thead = skb;\n\t\telse\n\t\t\ttail->next = skb;\n\t\t/* If skb was segmented, skb->prev points to\n\t\t * the last segment. If not, it still contains skb.\n\t\t */\n\t\ttail = skb->prev;\n\t}\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(validate_xmit_skb_list);\n\nstatic void qdisc_pkt_len_init(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\n\t/* To get more precise estimation of bytes sent on wire,\n\t * we add to pkt_len the headers size of all segments\n\t */\n\tif (shinfo->gso_size)  {\n\t\tunsigned int hdr_len;\n\t\tu16 gso_segs = shinfo->gso_segs;\n\n\t\t/* mac layer + network layer */\n\t\thdr_len = skb_transport_header(skb) - skb_mac_header(skb);\n\n\t\t/* + transport layer */\n\t\tif (likely(shinfo->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))\n\t\t\thdr_len += tcp_hdrlen(skb);\n\t\telse\n\t\t\thdr_len += sizeof(struct udphdr);\n\n\t\tif (shinfo->gso_type & SKB_GSO_DODGY)\n\t\t\tgso_segs = DIV_ROUND_UP(skb->len - hdr_len,\n\t\t\t\t\t\tshinfo->gso_size);\n\n\t\tqdisc_skb_cb(skb)->pkt_len += (gso_segs - 1) * hdr_len;\n\t}\n}\n\nstatic inline int __dev_xmit_skb(struct sk_buff *skb, struct Qdisc *q,\n\t\t\t\t struct net_device *dev,\n\t\t\t\t struct netdev_queue *txq)\n{\n\tspinlock_t *root_lock = qdisc_lock(q);\n\tstruct sk_buff *to_free = NULL;\n\tbool contended;\n\tint rc;\n\n\tqdisc_calculate_pkt_len(skb, q);\n\t/*\n\t * Heuristic to force contended enqueues to serialize on a\n\t * separate lock before trying to get qdisc main lock.\n\t * This permits qdisc->running owner to get the lock more\n\t * often and dequeue packets faster.\n\t */\n\tcontended = qdisc_is_running(q);\n\tif (unlikely(contended))\n\t\tspin_lock(&q->busylock);\n\n\tspin_lock(root_lock);\n\tif (unlikely(test_bit(__QDISC_STATE_DEACTIVATED, &q->state))) {\n\t\t__qdisc_drop(skb, &to_free);\n\t\trc = NET_XMIT_DROP;\n\t} else if ((q->flags & TCQ_F_CAN_BYPASS) && !qdisc_qlen(q) &&\n\t\t   qdisc_run_begin(q)) {\n\t\t/*\n\t\t * This is a work-conserving queue; there are no old skbs\n\t\t * waiting to be sent out; and the qdisc is not running -\n\t\t * xmit the skb directly.\n\t\t */\n\n\t\tqdisc_bstats_update(q, skb);\n\n\t\tif (sch_direct_xmit(skb, q, dev, txq, root_lock, true)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t} else\n\t\t\tqdisc_run_end(q);\n\n\t\trc = NET_XMIT_SUCCESS;\n\t} else {\n\t\trc = q->enqueue(skb, q, &to_free) & NET_XMIT_MASK;\n\t\tif (qdisc_run_begin(q)) {\n\t\t\tif (unlikely(contended)) {\n\t\t\t\tspin_unlock(&q->busylock);\n\t\t\t\tcontended = false;\n\t\t\t}\n\t\t\t__qdisc_run(q);\n\t\t}\n\t}\n\tspin_unlock(root_lock);\n\tif (unlikely(to_free))\n\t\tkfree_skb_list(to_free);\n\tif (unlikely(contended))\n\t\tspin_unlock(&q->busylock);\n\treturn rc;\n}\n\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\nstatic void skb_update_prio(struct sk_buff *skb)\n{\n\tstruct netprio_map *map = rcu_dereference_bh(skb->dev->priomap);\n\n\tif (!skb->priority && skb->sk && map) {\n\t\tunsigned int prioidx =\n\t\t\tsock_cgroup_prioidx(&skb->sk->sk_cgrp_data);\n\n\t\tif (prioidx < map->priomap_len)\n\t\t\tskb->priority = map->priomap[prioidx];\n\t}\n}\n#else\n#define skb_update_prio(skb)\n#endif\n\nDEFINE_PER_CPU(int, xmit_recursion);\nEXPORT_SYMBOL(xmit_recursion);\n\n/**\n *\tdev_loopback_xmit - loop back @skb\n *\t@net: network namespace this loopback is happening in\n *\t@sk:  sk needed to be a netfilter okfn\n *\t@skb: buffer to transmit\n */\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *skb)\n{\n\tskb_reset_mac_header(skb);\n\t__skb_pull(skb, skb_network_offset(skb));\n\tskb->pkt_type = PACKET_LOOPBACK;\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(skb));\n\tskb_dst_force(skb);\n\tnetif_rx_ni(skb);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_loopback_xmit);\n\n#ifdef CONFIG_NET_EGRESS\nstatic struct sk_buff *\nsch_handle_egress(struct sk_buff *skb, int *ret, struct net_device *dev)\n{\n\tstruct tcf_proto *cl = rcu_dereference_bh(dev->egress_cl_list);\n\tstruct tcf_result cl_res;\n\n\tif (!cl)\n\t\treturn skb;\n\n\t/* qdisc_skb_cb(skb)->pkt_len was already set by the caller. */\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tcf_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\t*ret = NET_XMIT_DROP;\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* No need to push/pop skb's mac_header here on egress! */\n\t\tskb_do_redirect(skb);\n\t\t*ret = NET_XMIT_SUCCESS;\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn skb;\n}\n#endif /* CONFIG_NET_EGRESS */\n\nstatic inline int get_xps_queue(struct net_device *dev, struct sk_buff *skb)\n{\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps *dev_maps;\n\tstruct xps_map *map;\n\tint queue_index = -1;\n\n\trcu_read_lock();\n\tdev_maps = rcu_dereference(dev->xps_maps);\n\tif (dev_maps) {\n\t\tunsigned int tci = skb->sender_cpu - 1;\n\n\t\tif (dev->num_tc) {\n\t\t\ttci *= dev->num_tc;\n\t\t\ttci += netdev_get_prio_tc_map(dev, skb->priority);\n\t\t}\n\n\t\tmap = rcu_dereference(dev_maps->cpu_map[tci]);\n\t\tif (map) {\n\t\t\tif (map->len == 1)\n\t\t\t\tqueue_index = map->queues[0];\n\t\t\telse\n\t\t\t\tqueue_index = map->queues[reciprocal_scale(skb_get_hash(skb),\n\t\t\t\t\t\t\t\t\t   map->len)];\n\t\t\tif (unlikely(queue_index >= dev->real_num_tx_queues))\n\t\t\t\tqueue_index = -1;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn queue_index;\n#else\n\treturn -1;\n#endif\n}\n\nstatic u16 __netdev_pick_tx(struct net_device *dev, struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tint queue_index = sk_tx_queue_get(sk);\n\n\tif (queue_index < 0 || skb->ooo_okay ||\n\t    queue_index >= dev->real_num_tx_queues) {\n\t\tint new_index = get_xps_queue(dev, skb);\n\n\t\tif (new_index < 0)\n\t\t\tnew_index = skb_tx_hash(dev, skb);\n\n\t\tif (queue_index != new_index && sk &&\n\t\t    sk_fullsock(sk) &&\n\t\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\t\tsk_tx_queue_set(sk, new_index);\n\n\t\tqueue_index = new_index;\n\t}\n\n\treturn queue_index;\n}\n\nstruct netdev_queue *netdev_pick_tx(struct net_device *dev,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    void *accel_priv)\n{\n\tint queue_index = 0;\n\n#ifdef CONFIG_XPS\n\tu32 sender_cpu = skb->sender_cpu - 1;\n\n\tif (sender_cpu >= (u32)NR_CPUS)\n\t\tskb->sender_cpu = raw_smp_processor_id() + 1;\n#endif\n\n\tif (dev->real_num_tx_queues != 1) {\n\t\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t\tif (ops->ndo_select_queue)\n\t\t\tqueue_index = ops->ndo_select_queue(dev, skb, accel_priv,\n\t\t\t\t\t\t\t    __netdev_pick_tx);\n\t\telse\n\t\t\tqueue_index = __netdev_pick_tx(dev, skb);\n\n\t\tif (!accel_priv)\n\t\t\tqueue_index = netdev_cap_txqueue(dev, queue_index);\n\t}\n\n\tskb_set_queue_mapping(skb, queue_index);\n\treturn netdev_get_tx_queue(dev, queue_index);\n}\n\n/**\n *\t__dev_queue_xmit - transmit a buffer\n *\t@skb: buffer to transmit\n *\t@accel_priv: private data used for L2 forwarding offload\n *\n *\tQueue a buffer for transmission to a network device. The caller must\n *\thave set the device and priority and built the buffer before calling\n *\tthis function. The function can be called from an interrupt.\n *\n *\tA negative errno code is returned on a failure. A success does not\n *\tguarantee the frame will be transmitted as it may be dropped due\n *\tto congestion or traffic shaping.\n *\n * -----------------------------------------------------------------------------------\n *      I notice this method can also return errors from the queue disciplines,\n *      including NET_XMIT_DROP, which is a positive value.  So, errors can also\n *      be positive.\n *\n *      Regardless of the return value, the skb is consumed, so it is currently\n *      difficult to retry a send to this method.  (You can bump the ref count\n *      before sending to hold a reference for retry if you are careful.)\n *\n *      When calling this method, interrupts MUST be enabled.  This is because\n *      the BH enable code must have IRQs enabled so that it will not deadlock.\n *          --BLG\n */\nstatic int __dev_queue_xmit(struct sk_buff *skb, void *accel_priv)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tstruct Qdisc *q;\n\tint rc = -ENOMEM;\n\n\tskb_reset_mac_header(skb);\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_SCHED_TSTAMP))\n\t\t__skb_tstamp_tx(skb, NULL, skb->sk, SCM_TSTAMP_SCHED);\n\n\t/* Disable soft irqs for various locks below. Also\n\t * stops preemption for RCU.\n\t */\n\trcu_read_lock_bh();\n\n\tskb_update_prio(skb);\n\n\tqdisc_pkt_len_init(skb);\n#ifdef CONFIG_NET_CLS_ACT\n\tskb->tc_at_ingress = 0;\n# ifdef CONFIG_NET_EGRESS\n\tif (static_key_false(&egress_needed)) {\n\t\tskb = sch_handle_egress(skb, &rc, dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\t}\n# endif\n#endif\n\t/* If device/qdisc don't need skb->dst, release it right now while\n\t * its hot in this cpu cache.\n\t */\n\tif (dev->priv_flags & IFF_XMIT_DST_RELEASE)\n\t\tskb_dst_drop(skb);\n\telse\n\t\tskb_dst_force(skb);\n\n\ttxq = netdev_pick_tx(dev, skb, accel_priv);\n\tq = rcu_dereference_bh(txq->qdisc);\n\n\ttrace_net_dev_queue(skb);\n\tif (q->enqueue) {\n\t\trc = __dev_xmit_skb(skb, q, dev, txq);\n\t\tgoto out;\n\t}\n\n\t/* The device has no queue. Common case for software devices:\n\t * loopback, all the sorts of tunnels...\n\n\t * Really, it is unlikely that netif_tx_lock protection is necessary\n\t * here.  (f.e. loopback and IP tunnels are clean ignoring statistics\n\t * counters.)\n\t * However, it is possible, that they rely on protection\n\t * made by us here.\n\n\t * Check this and shot the lock. It is not prone from deadlocks.\n\t *Either shot noqueue qdisc, it is even simpler 8)\n\t */\n\tif (dev->flags & IFF_UP) {\n\t\tint cpu = smp_processor_id(); /* ok because BHs are off */\n\n\t\tif (txq->xmit_lock_owner != cpu) {\n\t\t\tif (unlikely(__this_cpu_read(xmit_recursion) >\n\t\t\t\t     XMIT_RECURSION_LIMIT))\n\t\t\t\tgoto recursion_alert;\n\n\t\t\tskb = validate_xmit_skb(skb, dev);\n\t\t\tif (!skb)\n\t\t\t\tgoto out;\n\n\t\t\tHARD_TX_LOCK(dev, txq, cpu);\n\n\t\t\tif (!netif_xmit_stopped(txq)) {\n\t\t\t\t__this_cpu_inc(xmit_recursion);\n\t\t\t\tskb = dev_hard_start_xmit(skb, dev, txq, &rc);\n\t\t\t\t__this_cpu_dec(xmit_recursion);\n\t\t\t\tif (dev_xmit_complete(rc)) {\n\t\t\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tHARD_TX_UNLOCK(dev, txq);\n\t\t\tnet_crit_ratelimited(\"Virtual device %s asks to queue packet!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t} else {\n\t\t\t/* Recursion is detected! It is possible,\n\t\t\t * unfortunately\n\t\t\t */\nrecursion_alert:\n\t\t\tnet_crit_ratelimited(\"Dead loop on virtual device %s, fix it urgently!\\n\",\n\t\t\t\t\t     dev->name);\n\t\t}\n\t}\n\n\trc = -ENETDOWN;\n\trcu_read_unlock_bh();\n\n\tatomic_long_inc(&dev->tx_dropped);\n\tkfree_skb_list(skb);\n\treturn rc;\nout:\n\trcu_read_unlock_bh();\n\treturn rc;\n}\n\nint dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\nEXPORT_SYMBOL(dev_queue_xmit);\n\nint dev_queue_xmit_accel(struct sk_buff *skb, void *accel_priv)\n{\n\treturn __dev_queue_xmit(skb, accel_priv);\n}\nEXPORT_SYMBOL(dev_queue_xmit_accel);\n\n\n/*************************************************************************\n *\t\t\tReceiver routines\n *************************************************************************/\n\nint netdev_max_backlog __read_mostly = 1000;\nEXPORT_SYMBOL(netdev_max_backlog);\n\nint netdev_tstamp_prequeue __read_mostly = 1;\nint netdev_budget __read_mostly = 300;\nunsigned int __read_mostly netdev_budget_usecs = 2000;\nint weight_p __read_mostly = 64;           /* old backlog weight */\nint dev_weight_rx_bias __read_mostly = 1;  /* bias for backlog weight */\nint dev_weight_tx_bias __read_mostly = 1;  /* bias for output_queue quota */\nint dev_rx_weight __read_mostly = 64;\nint dev_tx_weight __read_mostly = 64;\n\n/* Called with irq disabled */\nstatic inline void ____napi_schedule(struct softnet_data *sd,\n\t\t\t\t     struct napi_struct *napi)\n{\n\tlist_add_tail(&napi->poll_list, &sd->poll_list);\n\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n}\n\n#ifdef CONFIG_RPS\n\n/* One global table that all flow-based protocols share. */\nstruct rps_sock_flow_table __rcu *rps_sock_flow_table __read_mostly;\nEXPORT_SYMBOL(rps_sock_flow_table);\nu32 rps_cpu_mask __read_mostly;\nEXPORT_SYMBOL(rps_cpu_mask);\n\nstruct static_key rps_needed __read_mostly;\nEXPORT_SYMBOL(rps_needed);\nstruct static_key rfs_needed __read_mostly;\nEXPORT_SYMBOL(rfs_needed);\n\nstatic struct rps_dev_flow *\nset_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t    struct rps_dev_flow *rflow, u16 next_cpu)\n{\n\tif (next_cpu < nr_cpu_ids) {\n#ifdef CONFIG_RFS_ACCEL\n\t\tstruct netdev_rx_queue *rxqueue;\n\t\tstruct rps_dev_flow_table *flow_table;\n\t\tstruct rps_dev_flow *old_rflow;\n\t\tu32 flow_id;\n\t\tu16 rxq_index;\n\t\tint rc;\n\n\t\t/* Should we steer this flow to a different hardware queue? */\n\t\tif (!skb_rx_queue_recorded(skb) || !dev->rx_cpu_rmap ||\n\t\t    !(dev->features & NETIF_F_NTUPLE))\n\t\t\tgoto out;\n\t\trxq_index = cpu_rmap_lookup_index(dev->rx_cpu_rmap, next_cpu);\n\t\tif (rxq_index == skb_get_rx_queue(skb))\n\t\t\tgoto out;\n\n\t\trxqueue = dev->_rx + rxq_index;\n\t\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\t\tif (!flow_table)\n\t\t\tgoto out;\n\t\tflow_id = skb_get_hash(skb) & flow_table->mask;\n\t\trc = dev->netdev_ops->ndo_rx_flow_steer(dev, skb,\n\t\t\t\t\t\t\trxq_index, flow_id);\n\t\tif (rc < 0)\n\t\t\tgoto out;\n\t\told_rflow = rflow;\n\t\trflow = &flow_table->flows[flow_id];\n\t\trflow->filter = rc;\n\t\tif (old_rflow->filter == rflow->filter)\n\t\t\told_rflow->filter = RPS_NO_FILTER;\n\tout:\n#endif\n\t\trflow->last_qtail =\n\t\t\tper_cpu(softnet_data, next_cpu).input_queue_head;\n\t}\n\n\trflow->cpu = next_cpu;\n\treturn rflow;\n}\n\n/*\n * get_rps_cpu is called from netif_receive_skb and returns the target\n * CPU from the RPS map of the receiving queue for a given skb.\n * rcu_read_lock must be held on entry.\n */\nstatic int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct rps_dev_flow **rflowp)\n{\n\tconst struct rps_sock_flow_table *sock_flow_table;\n\tstruct netdev_rx_queue *rxqueue = dev->_rx;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_map *map;\n\tint cpu = -1;\n\tu32 tcpu;\n\tu32 hash;\n\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 index = skb_get_rx_queue(skb);\n\n\t\tif (unlikely(index >= dev->real_num_rx_queues)) {\n\t\t\tWARN_ONCE(dev->real_num_rx_queues > 1,\n\t\t\t\t  \"%s received packet on queue %u, but number \"\n\t\t\t\t  \"of RX queues is %u\\n\",\n\t\t\t\t  dev->name, index, dev->real_num_rx_queues);\n\t\t\tgoto done;\n\t\t}\n\t\trxqueue += index;\n\t}\n\n\t/* Avoid computing hash if RFS/RPS is not active for this rxqueue */\n\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tmap = rcu_dereference(rxqueue->rps_map);\n\tif (!flow_table && !map)\n\t\tgoto done;\n\n\tskb_reset_network_header(skb);\n\thash = skb_get_hash(skb);\n\tif (!hash)\n\t\tgoto done;\n\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\tif (flow_table && sock_flow_table) {\n\t\tstruct rps_dev_flow *rflow;\n\t\tu32 next_cpu;\n\t\tu32 ident;\n\n\t\t/* First check into global flow table if there is a match */\n\t\tident = sock_flow_table->ents[hash & sock_flow_table->mask];\n\t\tif ((ident ^ hash) & ~rps_cpu_mask)\n\t\t\tgoto try_rps;\n\n\t\tnext_cpu = ident & rps_cpu_mask;\n\n\t\t/* OK, now we know there is a match,\n\t\t * we can look at the local (per receive queue) flow table\n\t\t */\n\t\trflow = &flow_table->flows[hash & flow_table->mask];\n\t\ttcpu = rflow->cpu;\n\n\t\t/*\n\t\t * If the desired CPU (where last recvmsg was done) is\n\t\t * different from current CPU (one in the rx-queue flow\n\t\t * table entry), switch if one of the following holds:\n\t\t *   - Current CPU is unset (>= nr_cpu_ids).\n\t\t *   - Current CPU is offline.\n\t\t *   - The current CPU's queue tail has advanced beyond the\n\t\t *     last packet that was enqueued using this table entry.\n\t\t *     This guarantees that all previous packets for the flow\n\t\t *     have been dequeued, thus preserving in order delivery.\n\t\t */\n\t\tif (unlikely(tcpu != next_cpu) &&\n\t\t    (tcpu >= nr_cpu_ids || !cpu_online(tcpu) ||\n\t\t     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -\n\t\t      rflow->last_qtail)) >= 0)) {\n\t\t\ttcpu = next_cpu;\n\t\t\trflow = set_rps_cpu(dev, skb, rflow, next_cpu);\n\t\t}\n\n\t\tif (tcpu < nr_cpu_ids && cpu_online(tcpu)) {\n\t\t\t*rflowp = rflow;\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ntry_rps:\n\n\tif (map) {\n\t\ttcpu = map->cpus[reciprocal_scale(hash, map->len)];\n\t\tif (cpu_online(tcpu)) {\n\t\t\tcpu = tcpu;\n\t\t\tgoto done;\n\t\t}\n\t}\n\ndone:\n\treturn cpu;\n}\n\n#ifdef CONFIG_RFS_ACCEL\n\n/**\n * rps_may_expire_flow - check whether an RFS hardware filter may be removed\n * @dev: Device on which the filter was set\n * @rxq_index: RX queue index\n * @flow_id: Flow ID passed to ndo_rx_flow_steer()\n * @filter_id: Filter ID returned by ndo_rx_flow_steer()\n *\n * Drivers that implement ndo_rx_flow_steer() should periodically call\n * this function for each installed filter and remove the filters for\n * which it returns %true.\n */\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index,\n\t\t\t u32 flow_id, u16 filter_id)\n{\n\tstruct netdev_rx_queue *rxqueue = dev->_rx + rxq_index;\n\tstruct rps_dev_flow_table *flow_table;\n\tstruct rps_dev_flow *rflow;\n\tbool expire = true;\n\tunsigned int cpu;\n\n\trcu_read_lock();\n\tflow_table = rcu_dereference(rxqueue->rps_flow_table);\n\tif (flow_table && flow_id <= flow_table->mask) {\n\t\trflow = &flow_table->flows[flow_id];\n\t\tcpu = ACCESS_ONCE(rflow->cpu);\n\t\tif (rflow->filter == filter_id && cpu < nr_cpu_ids &&\n\t\t    ((int)(per_cpu(softnet_data, cpu).input_queue_head -\n\t\t\t   rflow->last_qtail) <\n\t\t     (int)(10 * flow_table->mask)))\n\t\t\texpire = false;\n\t}\n\trcu_read_unlock();\n\treturn expire;\n}\nEXPORT_SYMBOL(rps_may_expire_flow);\n\n#endif /* CONFIG_RFS_ACCEL */\n\n/* Called from hardirq (IPI) context */\nstatic void rps_trigger_softirq(void *data)\n{\n\tstruct softnet_data *sd = data;\n\n\t____napi_schedule(sd, &sd->backlog);\n\tsd->received_rps++;\n}\n\n#endif /* CONFIG_RPS */\n\n/*\n * Check if this softnet_data structure is another cpu one\n * If yes, queue it to our IPI list and return 1\n * If no, return 0\n */\nstatic int rps_ipi_queued(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *mysd = this_cpu_ptr(&softnet_data);\n\n\tif (sd != mysd) {\n\t\tsd->rps_ipi_next = mysd->rps_ipi_list;\n\t\tmysd->rps_ipi_list = sd;\n\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\t\treturn 1;\n\t}\n#endif /* CONFIG_RPS */\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_FLOW_LIMIT\nint netdev_flow_limit_table_len __read_mostly = (1 << 12);\n#endif\n\nstatic bool skb_flow_limit(struct sk_buff *skb, unsigned int qlen)\n{\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit *fl;\n\tstruct softnet_data *sd;\n\tunsigned int old_flow, new_flow;\n\n\tif (qlen < (netdev_max_backlog >> 1))\n\t\treturn false;\n\n\tsd = this_cpu_ptr(&softnet_data);\n\n\trcu_read_lock();\n\tfl = rcu_dereference(sd->flow_limit);\n\tif (fl) {\n\t\tnew_flow = skb_get_hash(skb) & (fl->num_buckets - 1);\n\t\told_flow = fl->history[fl->history_head];\n\t\tfl->history[fl->history_head] = new_flow;\n\n\t\tfl->history_head++;\n\t\tfl->history_head &= FLOW_LIMIT_HISTORY - 1;\n\n\t\tif (likely(fl->buckets[old_flow]))\n\t\t\tfl->buckets[old_flow]--;\n\n\t\tif (++fl->buckets[new_flow] > (FLOW_LIMIT_HISTORY >> 1)) {\n\t\t\tfl->count++;\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n#endif\n\treturn false;\n}\n\n/*\n * enqueue_to_backlog is called to queue an skb to a per CPU backlog\n * queue (may be a remote CPU queue).\n */\nstatic int enqueue_to_backlog(struct sk_buff *skb, int cpu,\n\t\t\t      unsigned int *qtail)\n{\n\tstruct softnet_data *sd;\n\tunsigned long flags;\n\tunsigned int qlen;\n\n\tsd = &per_cpu(softnet_data, cpu);\n\n\tlocal_irq_save(flags);\n\n\trps_lock(sd);\n\tif (!netif_running(skb->dev))\n\t\tgoto drop;\n\tqlen = skb_queue_len(&sd->input_pkt_queue);\n\tif (qlen <= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {\n\t\tif (qlen) {\nenqueue:\n\t\t\t__skb_queue_tail(&sd->input_pkt_queue, skb);\n\t\t\tinput_queue_tail_incr_save(sd, qtail);\n\t\t\trps_unlock(sd);\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn NET_RX_SUCCESS;\n\t\t}\n\n\t\t/* Schedule NAPI for backlog device\n\t\t * We can use non atomic operation since we own the queue lock\n\t\t */\n\t\tif (!__test_and_set_bit(NAPI_STATE_SCHED, &sd->backlog.state)) {\n\t\t\tif (!rps_ipi_queued(sd))\n\t\t\t\t____napi_schedule(sd, &sd->backlog);\n\t\t}\n\t\tgoto enqueue;\n\t}\n\ndrop:\n\tsd->dropped++;\n\trps_unlock(sd);\n\n\tlocal_irq_restore(flags);\n\n\tatomic_long_inc(&skb->dev->rx_dropped);\n\tkfree_skb(skb);\n\treturn NET_RX_DROP;\n}\n\nstatic u32 netif_receive_generic_xdp(struct sk_buff *skb,\n\t\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tstruct xdp_buff xdp;\n\tu32 act = XDP_DROP;\n\tvoid *orig_data;\n\tint hlen, off;\n\tu32 mac_len;\n\n\t/* Reinjected packets coming from act_mirred or similar should\n\t * not get XDP generic processing.\n\t */\n\tif (skb_cloned(skb))\n\t\treturn XDP_PASS;\n\n\tif (skb_linearize(skb))\n\t\tgoto do_drop;\n\n\t/* The XDP program wants to see the packet starting at the MAC\n\t * header.\n\t */\n\tmac_len = skb->data - skb_mac_header(skb);\n\thlen = skb_headlen(skb) + mac_len;\n\txdp.data = skb->data - mac_len;\n\txdp.data_end = xdp.data + hlen;\n\txdp.data_hard_start = skb->data - skb_headroom(skb);\n\torig_data = xdp.data;\n\n\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\n\toff = xdp.data - orig_data;\n\tif (off > 0)\n\t\t__skb_pull(skb, off);\n\telse if (off < 0)\n\t\t__skb_push(skb, -off);\n\tskb->mac_header += off;\n\n\tswitch (act) {\n\tcase XDP_REDIRECT:\n\tcase XDP_TX:\n\t\t__skb_push(skb, mac_len);\n\t\t/* fall through */\n\tcase XDP_PASS:\n\t\tbreak;\n\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(act);\n\t\t/* fall through */\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(skb->dev, xdp_prog, act);\n\t\t/* fall through */\n\tcase XDP_DROP:\n\tdo_drop:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\n/* When doing generic XDP we have to bypass the qdisc layer and the\n * network taps in order to match in-driver-XDP behavior.\n */\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog)\n{\n\tstruct net_device *dev = skb->dev;\n\tstruct netdev_queue *txq;\n\tbool free_skb = true;\n\tint cpu, rc;\n\n\ttxq = netdev_pick_tx(dev, skb, NULL);\n\tcpu = smp_processor_id();\n\tHARD_TX_LOCK(dev, txq, cpu);\n\tif (!netif_xmit_stopped(txq)) {\n\t\trc = netdev_start_xmit(skb, dev, txq, 0);\n\t\tif (dev_xmit_complete(rc))\n\t\t\tfree_skb = false;\n\t}\n\tHARD_TX_UNLOCK(dev, txq);\n\tif (free_skb) {\n\t\ttrace_xdp_exception(dev, xdp_prog, XDP_TX);\n\t\tkfree_skb(skb);\n\t}\n}\nEXPORT_SYMBOL_GPL(generic_xdp_tx);\n\nstatic struct static_key generic_xdp_needed __read_mostly;\n\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb)\n{\n\tif (xdp_prog) {\n\t\tu32 act = netif_receive_generic_xdp(skb, xdp_prog);\n\t\tint err;\n\n\t\tif (act != XDP_PASS) {\n\t\t\tswitch (act) {\n\t\t\tcase XDP_REDIRECT:\n\t\t\t\terr = xdp_do_generic_redirect(skb->dev, skb,\n\t\t\t\t\t\t\t      xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_redir;\n\t\t\t/* fallthru to submit skb */\n\t\t\tcase XDP_TX:\n\t\t\t\tgeneric_xdp_tx(skb, xdp_prog);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn XDP_DROP;\n\t\t}\n\t}\n\treturn XDP_PASS;\nout_redir:\n\tkfree_skb(skb);\n\treturn XDP_DROP;\n}\nEXPORT_SYMBOL_GPL(do_xdp_generic);\n\nstatic int netif_rx_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_rx(skb);\n\n\tif (static_key_false(&generic_xdp_needed)) {\n\t\tint ret;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\t\tret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\n\t\t/* Consider XDP consuming the packet a success from\n\t\t * the netdev point of view we do not want to count\n\t\t * this as an error.\n\t\t */\n\t\tif (ret != XDP_PASS)\n\t\t\treturn NET_RX_SUCCESS;\n\t}\n\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\n\t\tcpu = get_rps_cpu(skb->dev, skb, &rflow);\n\t\tif (cpu < 0)\n\t\t\tcpu = smp_processor_id();\n\n\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\t} else\n#endif\n\t{\n\t\tunsigned int qtail;\n\n\t\tret = enqueue_to_backlog(skb, get_cpu(), &qtail);\n\t\tput_cpu();\n\t}\n\treturn ret;\n}\n\n/**\n *\tnetif_rx\t-\tpost buffer to the network code\n *\t@skb: buffer to post\n *\n *\tThis function receives a packet from a device driver and queues it for\n *\tthe upper (protocol) levels to process.  It always succeeds. The buffer\n *\tmay be dropped during processing for congestion control or by the\n *\tprotocol layers.\n *\n *\treturn values:\n *\tNET_RX_SUCCESS\t(no congestion)\n *\tNET_RX_DROP     (packet was dropped)\n *\n */\n\nint netif_rx(struct sk_buff *skb)\n{\n\ttrace_netif_rx_entry(skb);\n\n\treturn netif_rx_internal(skb);\n}\nEXPORT_SYMBOL(netif_rx);\n\nint netif_rx_ni(struct sk_buff *skb)\n{\n\tint err;\n\n\ttrace_netif_rx_ni_entry(skb);\n\n\tpreempt_disable();\n\terr = netif_rx_internal(skb);\n\tif (local_softirq_pending())\n\t\tdo_softirq();\n\tpreempt_enable();\n\n\treturn err;\n}\nEXPORT_SYMBOL(netif_rx_ni);\n\nstatic __latent_entropy void net_tx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\n\tif (sd->completion_queue) {\n\t\tstruct sk_buff *clist;\n\n\t\tlocal_irq_disable();\n\t\tclist = sd->completion_queue;\n\t\tsd->completion_queue = NULL;\n\t\tlocal_irq_enable();\n\n\t\twhile (clist) {\n\t\t\tstruct sk_buff *skb = clist;\n\n\t\t\tclist = clist->next;\n\n\t\t\tWARN_ON(refcount_read(&skb->users));\n\t\t\tif (likely(get_kfree_skb_cb(skb)->reason == SKB_REASON_CONSUMED))\n\t\t\t\ttrace_consume_skb(skb);\n\t\t\telse\n\t\t\t\ttrace_kfree_skb(skb, net_tx_action);\n\n\t\t\tif (skb->fclone != SKB_FCLONE_UNAVAILABLE)\n\t\t\t\t__kfree_skb(skb);\n\t\t\telse\n\t\t\t\t__kfree_skb_defer(skb);\n\t\t}\n\n\t\t__kfree_skb_flush();\n\t}\n\n\tif (sd->output_queue) {\n\t\tstruct Qdisc *head;\n\n\t\tlocal_irq_disable();\n\t\thead = sd->output_queue;\n\t\tsd->output_queue = NULL;\n\t\tsd->output_queue_tailp = &sd->output_queue;\n\t\tlocal_irq_enable();\n\n\t\twhile (head) {\n\t\t\tstruct Qdisc *q = head;\n\t\t\tspinlock_t *root_lock;\n\n\t\t\thead = head->next_sched;\n\n\t\t\troot_lock = qdisc_lock(q);\n\t\t\tspin_lock(root_lock);\n\t\t\t/* We need to make sure head->next_sched is read\n\t\t\t * before clearing __QDISC_STATE_SCHED\n\t\t\t */\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(__QDISC_STATE_SCHED, &q->state);\n\t\t\tqdisc_run(q);\n\t\t\tspin_unlock(root_lock);\n\t\t}\n\t}\n}\n\n#if IS_ENABLED(CONFIG_BRIDGE) && IS_ENABLED(CONFIG_ATM_LANE)\n/* This hook is defined here for ATM LANE */\nint (*br_fdb_test_addr_hook)(struct net_device *dev,\n\t\t\t     unsigned char *addr) __read_mostly;\nEXPORT_SYMBOL_GPL(br_fdb_test_addr_hook);\n#endif\n\nstatic inline struct sk_buff *\nsch_handle_ingress(struct sk_buff *skb, struct packet_type **pt_prev, int *ret,\n\t\t   struct net_device *orig_dev)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tstruct tcf_proto *cl = rcu_dereference_bh(skb->dev->ingress_cl_list);\n\tstruct tcf_result cl_res;\n\n\t/* If there's at least one ingress present somewhere (so\n\t * we get here via enabled static key), remaining devices\n\t * that are not configured with an ingress qdisc will bail\n\t * out here.\n\t */\n\tif (!cl)\n\t\treturn skb;\n\tif (*pt_prev) {\n\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t*pt_prev = NULL;\n\t}\n\n\tqdisc_skb_cb(skb)->pkt_len = skb->len;\n\tskb->tc_at_ingress = 1;\n\tqdisc_bstats_cpu_update(cl->q, skb);\n\n\tswitch (tcf_classify(skb, cl, &cl_res, false)) {\n\tcase TC_ACT_OK:\n\tcase TC_ACT_RECLASSIFY:\n\t\tskb->tc_index = TC_H_MIN(cl_res.classid);\n\t\tbreak;\n\tcase TC_ACT_SHOT:\n\t\tqdisc_qstats_cpu_drop(cl->q);\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_STOLEN:\n\tcase TC_ACT_QUEUED:\n\tcase TC_ACT_TRAP:\n\t\tconsume_skb(skb);\n\t\treturn NULL;\n\tcase TC_ACT_REDIRECT:\n\t\t/* skb_mac_header check was done by cls/act_bpf, so\n\t\t * we can safely push the L2 header back before\n\t\t * redirecting to another netdev\n\t\t */\n\t\t__skb_push(skb, skb->mac_len);\n\t\tskb_do_redirect(skb);\n\t\treturn NULL;\n\tdefault:\n\t\tbreak;\n\t}\n#endif /* CONFIG_NET_CLS_ACT */\n\treturn skb;\n}\n\n/**\n *\tnetdev_is_rx_handler_busy - check if receive handler is registered\n *\t@dev: device to check\n *\n *\tCheck if a receive handler is already registered for a given device.\n *\tReturn true if there one.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nbool netdev_is_rx_handler_busy(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\treturn dev && rtnl_dereference(dev->rx_handler);\n}\nEXPORT_SYMBOL_GPL(netdev_is_rx_handler_busy);\n\n/**\n *\tnetdev_rx_handler_register - register receive handler\n *\t@dev: device to register a handler for\n *\t@rx_handler: receive handler to register\n *\t@rx_handler_data: data pointer that is used by rx handler\n *\n *\tRegister a receive handler for a device. This handler will then be\n *\tcalled from __netif_receive_skb. A negative errno code is returned\n *\ton a failure.\n *\n *\tThe caller must hold the rtnl_mutex.\n *\n *\tFor a general description of rx_handler, see enum rx_handler_result.\n */\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data)\n{\n\tif (netdev_is_rx_handler_busy(dev))\n\t\treturn -EBUSY;\n\n\t/* Note: rx_handler_data must be set before rx_handler */\n\trcu_assign_pointer(dev->rx_handler_data, rx_handler_data);\n\trcu_assign_pointer(dev->rx_handler, rx_handler);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_register);\n\n/**\n *\tnetdev_rx_handler_unregister - unregister receive handler\n *\t@dev: device to unregister a handler from\n *\n *\tUnregister a receive handler from a device.\n *\n *\tThe caller must hold the rtnl_mutex.\n */\nvoid netdev_rx_handler_unregister(struct net_device *dev)\n{\n\n\tASSERT_RTNL();\n\tRCU_INIT_POINTER(dev->rx_handler, NULL);\n\t/* a reader seeing a non NULL rx_handler in a rcu_read_lock()\n\t * section has a guarantee to see a non NULL rx_handler_data\n\t * as well.\n\t */\n\tsynchronize_net();\n\tRCU_INIT_POINTER(dev->rx_handler_data, NULL);\n}\nEXPORT_SYMBOL_GPL(netdev_rx_handler_unregister);\n\n/*\n * Limit the use of PFMEMALLOC reserves to those protocols that implement\n * the special handling of PFMEMALLOC skbs.\n */\nstatic bool skb_pfmemalloc_protocol(struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_ARP):\n\tcase htons(ETH_P_IP):\n\tcase htons(ETH_P_IPV6):\n\tcase htons(ETH_P_8021Q):\n\tcase htons(ETH_P_8021AD):\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline int nf_ingress(struct sk_buff *skb, struct packet_type **pt_prev,\n\t\t\t     int *ret, struct net_device *orig_dev)\n{\n#ifdef CONFIG_NETFILTER_INGRESS\n\tif (nf_hook_ingress_active(skb)) {\n\t\tint ingress_retval;\n\n\t\tif (*pt_prev) {\n\t\t\t*ret = deliver_skb(skb, *pt_prev, orig_dev);\n\t\t\t*pt_prev = NULL;\n\t\t}\n\n\t\trcu_read_lock();\n\t\tingress_retval = nf_hook_ingress(skb);\n\t\trcu_read_unlock();\n\t\treturn ingress_retval;\n\t}\n#endif /* CONFIG_NETFILTER_INGRESS */\n\treturn 0;\n}\n\nstatic int __netif_receive_skb_core(struct sk_buff *skb, bool pfmemalloc)\n{\n\tstruct packet_type *ptype, *pt_prev;\n\trx_handler_func_t *rx_handler;\n\tstruct net_device *orig_dev;\n\tbool deliver_exact = false;\n\tint ret = NET_RX_DROP;\n\t__be16 type;\n\n\tnet_timestamp_check(!netdev_tstamp_prequeue, skb);\n\n\ttrace_netif_receive_skb(skb);\n\n\torig_dev = skb->dev;\n\n\tskb_reset_network_header(skb);\n\tif (!skb_transport_header_was_set(skb))\n\t\tskb_reset_transport_header(skb);\n\tskb_reset_mac_len(skb);\n\n\tpt_prev = NULL;\n\nanother_round:\n\tskb->skb_iif = skb->dev->ifindex;\n\n\t__this_cpu_inc(softnet_data.processed);\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD)) {\n\t\tskb = skb_vlan_untag(skb);\n\t\tif (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\tif (skb_skip_tc_classify(skb))\n\t\tgoto skip_classify;\n\n\tif (pfmemalloc)\n\t\tgoto skip_taps;\n\n\tlist_for_each_entry_rcu(ptype, &ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\n\tlist_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {\n\t\tif (pt_prev)\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\tpt_prev = ptype;\n\t}\n\nskip_taps:\n#ifdef CONFIG_NET_INGRESS\n\tif (static_key_false(&ingress_needed)) {\n\t\tskb = sch_handle_ingress(skb, &pt_prev, &ret, orig_dev);\n\t\tif (!skb)\n\t\t\tgoto out;\n\n\t\tif (nf_ingress(skb, &pt_prev, &ret, orig_dev) < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\tskb_reset_tc(skb);\nskip_classify:\n\tif (pfmemalloc && !skb_pfmemalloc_protocol(skb))\n\t\tgoto drop;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tif (vlan_do_receive(&skb))\n\t\t\tgoto another_round;\n\t\telse if (unlikely(!skb))\n\t\t\tgoto out;\n\t}\n\n\trx_handler = rcu_dereference(skb->dev->rx_handler);\n\tif (rx_handler) {\n\t\tif (pt_prev) {\n\t\t\tret = deliver_skb(skb, pt_prev, orig_dev);\n\t\t\tpt_prev = NULL;\n\t\t}\n\t\tswitch (rx_handler(&skb)) {\n\t\tcase RX_HANDLER_CONSUMED:\n\t\t\tret = NET_RX_SUCCESS;\n\t\t\tgoto out;\n\t\tcase RX_HANDLER_ANOTHER:\n\t\t\tgoto another_round;\n\t\tcase RX_HANDLER_EXACT:\n\t\t\tdeliver_exact = true;\n\t\tcase RX_HANDLER_PASS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (unlikely(skb_vlan_tag_present(skb))) {\n\t\tif (skb_vlan_tag_get_id(skb))\n\t\t\tskb->pkt_type = PACKET_OTHERHOST;\n\t\t/* Note: we might in the future use prio bits\n\t\t * and set skb->priority like in vlan_do_receive()\n\t\t * For the time being, just ignore Priority Code Point\n\t\t */\n\t\tskb->vlan_tci = 0;\n\t}\n\n\ttype = skb->protocol;\n\n\t/* deliver only exact match when indicated */\n\tif (likely(!deliver_exact)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &ptype_base[ntohs(type) &\n\t\t\t\t\t\t   PTYPE_HASH_MASK]);\n\t}\n\n\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t       &orig_dev->ptype_specific);\n\n\tif (unlikely(skb->dev != orig_dev)) {\n\t\tdeliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,\n\t\t\t\t       &skb->dev->ptype_specific);\n\t}\n\n\tif (pt_prev) {\n\t\tif (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))\n\t\t\tgoto drop;\n\t\telse\n\t\t\tret = pt_prev->func(skb, skb->dev, pt_prev, orig_dev);\n\t} else {\ndrop:\n\t\tif (!deliver_exact)\n\t\t\tatomic_long_inc(&skb->dev->rx_dropped);\n\t\telse\n\t\t\tatomic_long_inc(&skb->dev->rx_nohandler);\n\t\tkfree_skb(skb);\n\t\t/* Jamal, now you will not able to escape explaining\n\t\t * me how you were going to use this. :-)\n\t\t */\n\t\tret = NET_RX_DROP;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int __netif_receive_skb(struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb)) {\n\t\tunsigned int noreclaim_flag;\n\n\t\t/*\n\t\t * PFMEMALLOC skbs are special, they should\n\t\t * - be delivered to SOCK_MEMALLOC sockets only\n\t\t * - stay away from userspace\n\t\t * - have bounded memory usage\n\t\t *\n\t\t * Use PF_MEMALLOC as this saves us from propagating the allocation\n\t\t * context down to all allocation sites.\n\t\t */\n\t\tnoreclaim_flag = memalloc_noreclaim_save();\n\t\tret = __netif_receive_skb_core(skb, true);\n\t\tmemalloc_noreclaim_restore(noreclaim_flag);\n\t} else\n\t\tret = __netif_receive_skb_core(skb, false);\n\n\treturn ret;\n}\n\nstatic int generic_xdp_install(struct net_device *dev, struct netdev_xdp *xdp)\n{\n\tstruct bpf_prog *old = rtnl_dereference(dev->xdp_prog);\n\tstruct bpf_prog *new = xdp->prog;\n\tint ret = 0;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\trcu_assign_pointer(dev->xdp_prog, new);\n\t\tif (old)\n\t\t\tbpf_prog_put(old);\n\n\t\tif (old && !new) {\n\t\t\tstatic_key_slow_dec(&generic_xdp_needed);\n\t\t} else if (new && !old) {\n\t\t\tstatic_key_slow_inc(&generic_xdp_needed);\n\t\t\tdev_disable_lro(dev);\n\t\t}\n\t\tbreak;\n\n\tcase XDP_QUERY_PROG:\n\t\txdp->prog_attached = !!old;\n\t\txdp->prog_id = old ? old->aux->id : 0;\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int netif_receive_skb_internal(struct sk_buff *skb)\n{\n\tint ret;\n\n\tnet_timestamp_check(netdev_tstamp_prequeue, skb);\n\n\tif (skb_defer_rx_timestamp(skb))\n\t\treturn NET_RX_SUCCESS;\n\n\tif (static_key_false(&generic_xdp_needed)) {\n\t\tint ret;\n\n\t\tpreempt_disable();\n\t\trcu_read_lock();\n\t\tret = do_xdp_generic(rcu_dereference(skb->dev->xdp_prog), skb);\n\t\trcu_read_unlock();\n\t\tpreempt_enable();\n\n\t\tif (ret != XDP_PASS)\n\t\t\treturn NET_RX_DROP;\n\t}\n\n\trcu_read_lock();\n#ifdef CONFIG_RPS\n\tif (static_key_false(&rps_needed)) {\n\t\tstruct rps_dev_flow voidflow, *rflow = &voidflow;\n\t\tint cpu = get_rps_cpu(skb->dev, skb, &rflow);\n\n\t\tif (cpu >= 0) {\n\t\t\tret = enqueue_to_backlog(skb, cpu, &rflow->last_qtail);\n\t\t\trcu_read_unlock();\n\t\t\treturn ret;\n\t\t}\n\t}\n#endif\n\tret = __netif_receive_skb(skb);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n/**\n *\tnetif_receive_skb - process receive buffer from network\n *\t@skb: buffer to process\n *\n *\tnetif_receive_skb() is the main receive data processing function.\n *\tIt always succeeds. The buffer may be dropped during processing\n *\tfor congestion control or by the protocol layers.\n *\n *\tThis function may only be called from softirq context and interrupts\n *\tshould be enabled.\n *\n *\tReturn values (usually ignored):\n *\tNET_RX_SUCCESS: no congestion\n *\tNET_RX_DROP: packet was dropped\n */\nint netif_receive_skb(struct sk_buff *skb)\n{\n\ttrace_netif_receive_skb_entry(skb);\n\n\treturn netif_receive_skb_internal(skb);\n}\nEXPORT_SYMBOL(netif_receive_skb);\n\nDEFINE_PER_CPU(struct work_struct, flush_works);\n\n/* Network device is going away, flush any packets still pending */\nstatic void flush_backlog(struct work_struct *work)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct softnet_data *sd;\n\n\tlocal_bh_disable();\n\tsd = this_cpu_ptr(&softnet_data);\n\n\tlocal_irq_disable();\n\trps_lock(sd);\n\tskb_queue_walk_safe(&sd->input_pkt_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->input_pkt_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\trps_unlock(sd);\n\tlocal_irq_enable();\n\n\tskb_queue_walk_safe(&sd->process_queue, skb, tmp) {\n\t\tif (skb->dev->reg_state == NETREG_UNREGISTERING) {\n\t\t\t__skb_unlink(skb, &sd->process_queue);\n\t\t\tkfree_skb(skb);\n\t\t\tinput_queue_head_incr(sd);\n\t\t}\n\t}\n\tlocal_bh_enable();\n}\n\nstatic void flush_all_backlogs(void)\n{\n\tunsigned int cpu;\n\n\tget_online_cpus();\n\n\tfor_each_online_cpu(cpu)\n\t\tqueue_work_on(cpu, system_highpri_wq,\n\t\t\t      per_cpu_ptr(&flush_works, cpu));\n\n\tfor_each_online_cpu(cpu)\n\t\tflush_work(per_cpu_ptr(&flush_works, cpu));\n\n\tput_online_cpus();\n}\n\nstatic int napi_gro_complete(struct sk_buff *skb)\n{\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint err = -ENOENT;\n\n\tBUILD_BUG_ON(sizeof(struct napi_gro_cb) > sizeof(skb->cb));\n\n\tif (NAPI_GRO_CB(skb)->count == 1) {\n\t\tskb_shinfo(skb)->gso_size = 0;\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\n\t\terr = ptype->callbacks.gro_complete(skb, 0);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (err) {\n\t\tWARN_ON(&ptype->list == head);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_SUCCESS;\n\t}\n\nout:\n\treturn netif_receive_skb_internal(skb);\n}\n\n/* napi->gro_list contains packets ordered by age.\n * youngest packets at the head of it.\n * Complete skbs in reverse order to reduce latencies.\n */\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old)\n{\n\tstruct sk_buff *skb, *prev = NULL;\n\n\t/* scan list and build reverse chain */\n\tfor (skb = napi->gro_list; skb != NULL; skb = skb->next) {\n\t\tskb->prev = prev;\n\t\tprev = skb;\n\t}\n\n\tfor (skb = prev; skb; skb = prev) {\n\t\tskb->next = NULL;\n\n\t\tif (flush_old && NAPI_GRO_CB(skb)->age == jiffies)\n\t\t\treturn;\n\n\t\tprev = skb->prev;\n\t\tnapi_gro_complete(skb);\n\t\tnapi->gro_count--;\n\t}\n\n\tnapi->gro_list = NULL;\n}\nEXPORT_SYMBOL(napi_gro_flush);\n\nstatic void gro_list_prepare(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff *p;\n\tunsigned int maclen = skb->dev->hard_header_len;\n\tu32 hash = skb_get_hash_raw(skb);\n\n\tfor (p = napi->gro_list; p; p = p->next) {\n\t\tunsigned long diffs;\n\n\t\tNAPI_GRO_CB(p)->flush = 0;\n\n\t\tif (hash != skb_get_hash_raw(p)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdiffs = (unsigned long)p->dev ^ (unsigned long)skb->dev;\n\t\tdiffs |= p->vlan_tci ^ skb->vlan_tci;\n\t\tdiffs |= skb_metadata_dst_cmp(p, skb);\n\t\tif (maclen == ETH_HLEN)\n\t\t\tdiffs |= compare_ether_header(skb_mac_header(p),\n\t\t\t\t\t\t      skb_mac_header(skb));\n\t\telse if (!diffs)\n\t\t\tdiffs = memcmp(skb_mac_header(p),\n\t\t\t\t       skb_mac_header(skb),\n\t\t\t\t       maclen);\n\t\tNAPI_GRO_CB(p)->same_flow = !diffs;\n\t}\n}\n\nstatic void skb_gro_reset_offset(struct sk_buff *skb)\n{\n\tconst struct skb_shared_info *pinfo = skb_shinfo(skb);\n\tconst skb_frag_t *frag0 = &pinfo->frags[0];\n\n\tNAPI_GRO_CB(skb)->data_offset = 0;\n\tNAPI_GRO_CB(skb)->frag0 = NULL;\n\tNAPI_GRO_CB(skb)->frag0_len = 0;\n\n\tif (skb_mac_header(skb) == skb_tail_pointer(skb) &&\n\t    pinfo->nr_frags &&\n\t    !PageHighMem(skb_frag_page(frag0))) {\n\t\tNAPI_GRO_CB(skb)->frag0 = skb_frag_address(frag0);\n\t\tNAPI_GRO_CB(skb)->frag0_len = min_t(unsigned int,\n\t\t\t\t\t\t    skb_frag_size(frag0),\n\t\t\t\t\t\t    skb->end - skb->tail);\n\t}\n}\n\nstatic void gro_pull_from_frag0(struct sk_buff *skb, int grow)\n{\n\tstruct skb_shared_info *pinfo = skb_shinfo(skb);\n\n\tBUG_ON(skb->end - skb->tail < grow);\n\n\tmemcpy(skb_tail_pointer(skb), NAPI_GRO_CB(skb)->frag0, grow);\n\n\tskb->data_len -= grow;\n\tskb->tail += grow;\n\n\tpinfo->frags[0].page_offset += grow;\n\tskb_frag_size_sub(&pinfo->frags[0], grow);\n\n\tif (unlikely(!skb_frag_size(&pinfo->frags[0]))) {\n\t\tskb_frag_unref(skb, 0);\n\t\tmemmove(pinfo->frags, pinfo->frags + 1,\n\t\t\t--pinfo->nr_frags * sizeof(pinfo->frags[0]));\n\t}\n}\n\nstatic enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tstruct sk_buff **pp = NULL;\n\tstruct packet_offload *ptype;\n\t__be16 type = skb->protocol;\n\tstruct list_head *head = &offload_base;\n\tint same_flow;\n\tenum gro_result ret;\n\tint grow;\n\n\tif (netif_elide_gro(skb->dev))\n\t\tgoto normal;\n\n\tgro_list_prepare(napi, skb);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ptype, head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\n\t\tskb_set_network_header(skb, skb_gro_offset(skb));\n\t\tskb_reset_mac_len(skb);\n\t\tNAPI_GRO_CB(skb)->same_flow = 0;\n\t\tNAPI_GRO_CB(skb)->flush = skb_is_gso(skb) || skb_has_frag_list(skb);\n\t\tNAPI_GRO_CB(skb)->free = 0;\n\t\tNAPI_GRO_CB(skb)->encap_mark = 0;\n\t\tNAPI_GRO_CB(skb)->recursion_counter = 0;\n\t\tNAPI_GRO_CB(skb)->is_fou = 0;\n\t\tNAPI_GRO_CB(skb)->is_atomic = 1;\n\t\tNAPI_GRO_CB(skb)->gro_remcsum_start = 0;\n\n\t\t/* Setup for GRO checksum validation */\n\t\tswitch (skb->ip_summed) {\n\t\tcase CHECKSUM_COMPLETE:\n\t\t\tNAPI_GRO_CB(skb)->csum = skb->csum;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tbreak;\n\t\tcase CHECKSUM_UNNECESSARY:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = skb->csum_level + 1;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNAPI_GRO_CB(skb)->csum_cnt = 0;\n\t\t\tNAPI_GRO_CB(skb)->csum_valid = 0;\n\t\t}\n\n\t\tpp = ptype->callbacks.gro_receive(&napi->gro_list, skb);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\tif (&ptype->list == head)\n\t\tgoto normal;\n\n\tif (IS_ERR(pp) && PTR_ERR(pp) == -EINPROGRESS) {\n\t\tret = GRO_CONSUMED;\n\t\tgoto ok;\n\t}\n\n\tsame_flow = NAPI_GRO_CB(skb)->same_flow;\n\tret = NAPI_GRO_CB(skb)->free ? GRO_MERGED_FREE : GRO_MERGED;\n\n\tif (pp) {\n\t\tstruct sk_buff *nskb = *pp;\n\n\t\t*pp = nskb->next;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t\tnapi->gro_count--;\n\t}\n\n\tif (same_flow)\n\t\tgoto ok;\n\n\tif (NAPI_GRO_CB(skb)->flush)\n\t\tgoto normal;\n\n\tif (unlikely(napi->gro_count >= MAX_GRO_SKBS)) {\n\t\tstruct sk_buff *nskb = napi->gro_list;\n\n\t\t/* locate the end of the list to select the 'oldest' flow */\n\t\twhile (nskb->next) {\n\t\t\tpp = &nskb->next;\n\t\t\tnskb = *pp;\n\t\t}\n\t\t*pp = NULL;\n\t\tnskb->next = NULL;\n\t\tnapi_gro_complete(nskb);\n\t} else {\n\t\tnapi->gro_count++;\n\t}\n\tNAPI_GRO_CB(skb)->count = 1;\n\tNAPI_GRO_CB(skb)->age = jiffies;\n\tNAPI_GRO_CB(skb)->last = skb;\n\tskb_shinfo(skb)->gso_size = skb_gro_len(skb);\n\tskb->next = napi->gro_list;\n\tnapi->gro_list = skb;\n\tret = GRO_HELD;\n\npull:\n\tgrow = skb_gro_offset(skb) - skb_headlen(skb);\n\tif (grow > 0)\n\t\tgro_pull_from_frag0(skb, grow);\nok:\n\treturn ret;\n\nnormal:\n\tret = GRO_NORMAL;\n\tgoto pull;\n}\n\nstruct packet_offload *gro_find_receive_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_receive)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_receive_by_type);\n\nstruct packet_offload *gro_find_complete_by_type(__be16 type)\n{\n\tstruct list_head *offload_head = &offload_base;\n\tstruct packet_offload *ptype;\n\n\tlist_for_each_entry_rcu(ptype, offload_head, list) {\n\t\tif (ptype->type != type || !ptype->callbacks.gro_complete)\n\t\t\tcontinue;\n\t\treturn ptype;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(gro_find_complete_by_type);\n\nstatic void napi_skb_free_stolen_head(struct sk_buff *skb)\n{\n\tskb_dst_drop(skb);\n\tsecpath_reset(skb);\n\tkmem_cache_free(skbuff_head_cache, skb);\n}\n\nstatic gro_result_t napi_skb_finish(gro_result_t ret, struct sk_buff *skb)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\t\tif (netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\t__kfree_skb(skb);\n\t\tbreak;\n\n\tcase GRO_HELD:\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tskb_mark_napi_id(skb, napi);\n\ttrace_napi_gro_receive_entry(skb);\n\n\tskb_gro_reset_offset(skb);\n\n\treturn napi_skb_finish(dev_gro_receive(napi, skb), skb);\n}\nEXPORT_SYMBOL(napi_gro_receive);\n\nstatic void napi_reuse_skb(struct napi_struct *napi, struct sk_buff *skb)\n{\n\tif (unlikely(skb->pfmemalloc)) {\n\t\tconsume_skb(skb);\n\t\treturn;\n\t}\n\t__skb_pull(skb, skb_headlen(skb));\n\t/* restore the reserve we had after netdev_alloc_skb_ip_align() */\n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN - skb_headroom(skb));\n\tskb->vlan_tci = 0;\n\tskb->dev = napi->dev;\n\tskb->skb_iif = 0;\n\tskb->encapsulation = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n\tskb->truesize = SKB_TRUESIZE(skb_end_offset(skb));\n\tsecpath_reset(skb);\n\n\tnapi->skb = skb;\n}\n\nstruct sk_buff *napi_get_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\n\tif (!skb) {\n\t\tskb = napi_alloc_skb(napi, GRO_MAX_HEAD);\n\t\tif (skb) {\n\t\t\tnapi->skb = skb;\n\t\t\tskb_mark_napi_id(skb, napi);\n\t\t}\n\t}\n\treturn skb;\n}\nEXPORT_SYMBOL(napi_get_frags);\n\nstatic gro_result_t napi_frags_finish(struct napi_struct *napi,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      gro_result_t ret)\n{\n\tswitch (ret) {\n\tcase GRO_NORMAL:\n\tcase GRO_HELD:\n\t\t__skb_push(skb, ETH_HLEN);\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tif (ret == GRO_NORMAL && netif_receive_skb_internal(skb))\n\t\t\tret = GRO_DROP;\n\t\tbreak;\n\n\tcase GRO_DROP:\n\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED_FREE:\n\t\tif (NAPI_GRO_CB(skb)->free == NAPI_GRO_FREE_STOLEN_HEAD)\n\t\t\tnapi_skb_free_stolen_head(skb);\n\t\telse\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\tbreak;\n\n\tcase GRO_MERGED:\n\tcase GRO_CONSUMED:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n/* Upper GRO stack assumes network header starts at gro_offset=0\n * Drivers could call both napi_gro_frags() and napi_gro_receive()\n * We copy ethernet header into skb->data to have a common layout.\n */\nstatic struct sk_buff *napi_frags_skb(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi->skb;\n\tconst struct ethhdr *eth;\n\tunsigned int hlen = sizeof(*eth);\n\n\tnapi->skb = NULL;\n\n\tskb_reset_mac_header(skb);\n\tskb_gro_reset_offset(skb);\n\n\teth = skb_gro_header_fast(skb, 0);\n\tif (unlikely(skb_gro_header_hard(skb, hlen))) {\n\t\teth = skb_gro_header_slow(skb, hlen, 0);\n\t\tif (unlikely(!eth)) {\n\t\t\tnet_warn_ratelimited(\"%s: dropping impossible skb from %s\\n\",\n\t\t\t\t\t     __func__, napi->dev->name);\n\t\t\tnapi_reuse_skb(napi, skb);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tgro_pull_from_frag0(skb, hlen);\n\t\tNAPI_GRO_CB(skb)->frag0 += hlen;\n\t\tNAPI_GRO_CB(skb)->frag0_len -= hlen;\n\t}\n\t__skb_pull(skb, hlen);\n\n\t/*\n\t * This works because the only protocols we care about don't require\n\t * special handling.\n\t * We'll fix it up properly in napi_frags_finish()\n\t */\n\tskb->protocol = eth->h_proto;\n\n\treturn skb;\n}\n\ngro_result_t napi_gro_frags(struct napi_struct *napi)\n{\n\tstruct sk_buff *skb = napi_frags_skb(napi);\n\n\tif (!skb)\n\t\treturn GRO_DROP;\n\n\ttrace_napi_gro_frags_entry(skb);\n\n\treturn napi_frags_finish(napi, skb, dev_gro_receive(napi, skb));\n}\nEXPORT_SYMBOL(napi_gro_frags);\n\n/* Compute the checksum from gro_offset and return the folded value\n * after adding in any pseudo checksum.\n */\n__sum16 __skb_gro_checksum_complete(struct sk_buff *skb)\n{\n\t__wsum wsum;\n\t__sum16 sum;\n\n\twsum = skb_checksum(skb, skb_gro_offset(skb), skb_gro_len(skb), 0);\n\n\t/* NAPI_GRO_CB(skb)->csum holds pseudo checksum */\n\tsum = csum_fold(csum_add(NAPI_GRO_CB(skb)->csum, wsum));\n\tif (likely(!sum)) {\n\t\tif (unlikely(skb->ip_summed == CHECKSUM_COMPLETE) &&\n\t\t    !skb->csum_complete_sw)\n\t\t\tnetdev_rx_csum_fault(skb->dev);\n\t}\n\n\tNAPI_GRO_CB(skb)->csum = wsum;\n\tNAPI_GRO_CB(skb)->csum_valid = 1;\n\n\treturn sum;\n}\nEXPORT_SYMBOL(__skb_gro_checksum_complete);\n\nstatic void net_rps_send_ipi(struct softnet_data *remsd)\n{\n#ifdef CONFIG_RPS\n\twhile (remsd) {\n\t\tstruct softnet_data *next = remsd->rps_ipi_next;\n\n\t\tif (cpu_online(remsd->cpu))\n\t\t\tsmp_call_function_single_async(remsd->cpu, &remsd->csd);\n\t\tremsd = next;\n\t}\n#endif\n}\n\n/*\n * net_rps_action_and_irq_enable sends any pending IPI's for rps.\n * Note: called with local irq disabled, but exits with local irq enabled.\n */\nstatic void net_rps_action_and_irq_enable(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tstruct softnet_data *remsd = sd->rps_ipi_list;\n\n\tif (remsd) {\n\t\tsd->rps_ipi_list = NULL;\n\n\t\tlocal_irq_enable();\n\n\t\t/* Send pending IPI's to kick RPS processing on remote cpus. */\n\t\tnet_rps_send_ipi(remsd);\n\t} else\n#endif\n\t\tlocal_irq_enable();\n}\n\nstatic bool sd_has_rps_ipi_waiting(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\treturn sd->rps_ipi_list != NULL;\n#else\n\treturn false;\n#endif\n}\n\nstatic int process_backlog(struct napi_struct *napi, int quota)\n{\n\tstruct softnet_data *sd = container_of(napi, struct softnet_data, backlog);\n\tbool again = true;\n\tint work = 0;\n\n\t/* Check if we have pending ipi, its better to send them now,\n\t * not waiting net_rx_action() end.\n\t */\n\tif (sd_has_rps_ipi_waiting(sd)) {\n\t\tlocal_irq_disable();\n\t\tnet_rps_action_and_irq_enable(sd);\n\t}\n\n\tnapi->weight = dev_rx_weight;\n\twhile (again) {\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = __skb_dequeue(&sd->process_queue))) {\n\t\t\trcu_read_lock();\n\t\t\t__netif_receive_skb(skb);\n\t\t\trcu_read_unlock();\n\t\t\tinput_queue_head_incr(sd);\n\t\t\tif (++work >= quota)\n\t\t\t\treturn work;\n\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\trps_lock(sd);\n\t\tif (skb_queue_empty(&sd->input_pkt_queue)) {\n\t\t\t/*\n\t\t\t * Inline a custom version of __napi_complete().\n\t\t\t * only current cpu owns and manipulates this napi,\n\t\t\t * and NAPI_STATE_SCHED is the only possible flag set\n\t\t\t * on backlog.\n\t\t\t * We can use a plain write instead of clear_bit(),\n\t\t\t * and we dont need an smp_mb() memory barrier.\n\t\t\t */\n\t\t\tnapi->state = 0;\n\t\t\tagain = false;\n\t\t} else {\n\t\t\tskb_queue_splice_tail_init(&sd->input_pkt_queue,\n\t\t\t\t\t\t   &sd->process_queue);\n\t\t}\n\t\trps_unlock(sd);\n\t\tlocal_irq_enable();\n\t}\n\n\treturn work;\n}\n\n/**\n * __napi_schedule - schedule for receive\n * @n: entry to schedule\n *\n * The entry's receive function will be scheduled to run.\n * Consider using __napi_schedule_irqoff() if hard irqs are masked.\n */\nvoid __napi_schedule(struct napi_struct *n)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__napi_schedule);\n\n/**\n *\tnapi_schedule_prep - check if napi can be scheduled\n *\t@n: napi context\n *\n * Test if NAPI routine is already running, and if not mark\n * it as running.  This is used as a condition variable\n * insure only one NAPI poll instance runs.  We also make\n * sure there is no pending NAPI disable.\n */\nbool napi_schedule_prep(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\t\tif (unlikely(val & NAPIF_STATE_DISABLE))\n\t\t\treturn false;\n\t\tnew = val | NAPIF_STATE_SCHED;\n\n\t\t/* Sets STATE_MISSED bit if STATE_SCHED was already set\n\t\t * This was suggested by Alexander Duyck, as compiler\n\t\t * emits better code than :\n\t\t * if (val & NAPIF_STATE_SCHED)\n\t\t *     new |= NAPIF_STATE_MISSED;\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *\n\t\t\t\t\t\t   NAPIF_STATE_MISSED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\treturn !(val & NAPIF_STATE_SCHED);\n}\nEXPORT_SYMBOL(napi_schedule_prep);\n\n/**\n * __napi_schedule_irqoff - schedule for receive\n * @n: entry to schedule\n *\n * Variant of __napi_schedule() assuming hard irqs are masked\n */\nvoid __napi_schedule_irqoff(struct napi_struct *n)\n{\n\t____napi_schedule(this_cpu_ptr(&softnet_data), n);\n}\nEXPORT_SYMBOL(__napi_schedule_irqoff);\n\nbool napi_complete_done(struct napi_struct *n, int work_done)\n{\n\tunsigned long flags, val, new;\n\n\t/*\n\t * 1) Don't let napi dequeue from the cpu poll list\n\t *    just in case its running on a different cpu.\n\t * 2) If we are busy polling, do nothing here, we have\n\t *    the guarantee we will be called later.\n\t */\n\tif (unlikely(n->state & (NAPIF_STATE_NPSVC |\n\t\t\t\t NAPIF_STATE_IN_BUSY_POLL)))\n\t\treturn false;\n\n\tif (n->gro_list) {\n\t\tunsigned long timeout = 0;\n\n\t\tif (work_done)\n\t\t\ttimeout = n->dev->gro_flush_timeout;\n\n\t\tif (timeout)\n\t\t\thrtimer_start(&n->timer, ns_to_ktime(timeout),\n\t\t\t\t      HRTIMER_MODE_REL_PINNED);\n\t\telse\n\t\t\tnapi_gro_flush(n, false);\n\t}\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\t/* If n->poll_list is not empty, we need to mask irqs */\n\t\tlocal_irq_save(flags);\n\t\tlist_del_init(&n->poll_list);\n\t\tlocal_irq_restore(flags);\n\t}\n\n\tdo {\n\t\tval = READ_ONCE(n->state);\n\n\t\tWARN_ON_ONCE(!(val & NAPIF_STATE_SCHED));\n\n\t\tnew = val & ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED);\n\n\t\t/* If STATE_MISSED was set, leave STATE_SCHED set,\n\t\t * because we will call napi->poll() one more time.\n\t\t * This C code was suggested by Alexander Duyck to help gcc.\n\t\t */\n\t\tnew |= (val & NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *\n\t\t\t\t\t\t    NAPIF_STATE_SCHED;\n\t} while (cmpxchg(&n->state, val, new) != val);\n\n\tif (unlikely(val & NAPIF_STATE_MISSED)) {\n\t\t__napi_schedule(n);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL(napi_complete_done);\n\n/* must be called under rcu_read_lock(), as we dont take a reference */\nstatic struct napi_struct *napi_by_id(unsigned int napi_id)\n{\n\tunsigned int hash = napi_id % HASH_SIZE(napi_hash);\n\tstruct napi_struct *napi;\n\n\thlist_for_each_entry_rcu(napi, &napi_hash[hash], napi_hash_node)\n\t\tif (napi->napi_id == napi_id)\n\t\t\treturn napi;\n\n\treturn NULL;\n}\n\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\n#define BUSY_POLL_BUDGET 8\n\nstatic void busy_poll_stop(struct napi_struct *napi, void *have_poll_lock)\n{\n\tint rc;\n\n\t/* Busy polling means there is a high chance device driver hard irq\n\t * could not grab NAPI_STATE_SCHED, and that NAPI_STATE_MISSED was\n\t * set in napi_schedule_prep().\n\t * Since we are about to call napi->poll() once more, we can safely\n\t * clear NAPI_STATE_MISSED.\n\t *\n\t * Note: x86 could use a single \"lock and ...\" instruction\n\t * to perform these two clear_bit()\n\t */\n\tclear_bit(NAPI_STATE_MISSED, &napi->state);\n\tclear_bit(NAPI_STATE_IN_BUSY_POLL, &napi->state);\n\n\tlocal_bh_disable();\n\n\t/* All we really want here is to re-enable device interrupts.\n\t * Ideally, a new ndo_busy_poll_stop() could avoid another round.\n\t */\n\trc = napi->poll(napi, BUSY_POLL_BUDGET);\n\ttrace_napi_poll(napi, rc, BUSY_POLL_BUDGET);\n\tnetpoll_poll_unlock(have_poll_lock);\n\tif (rc == BUSY_POLL_BUDGET)\n\t\t__napi_schedule(napi);\n\tlocal_bh_enable();\n}\n\nvoid napi_busy_loop(unsigned int napi_id,\n\t\t    bool (*loop_end)(void *, unsigned long),\n\t\t    void *loop_end_arg)\n{\n\tunsigned long start_time = loop_end ? busy_loop_current_time() : 0;\n\tint (*napi_poll)(struct napi_struct *napi, int budget);\n\tvoid *have_poll_lock = NULL;\n\tstruct napi_struct *napi;\n\nrestart:\n\tnapi_poll = NULL;\n\n\trcu_read_lock();\n\n\tnapi = napi_by_id(napi_id);\n\tif (!napi)\n\t\tgoto out;\n\n\tpreempt_disable();\n\tfor (;;) {\n\t\tint work = 0;\n\n\t\tlocal_bh_disable();\n\t\tif (!napi_poll) {\n\t\t\tunsigned long val = READ_ONCE(napi->state);\n\n\t\t\t/* If multiple threads are competing for this napi,\n\t\t\t * we avoid dirtying napi->state as much as we can.\n\t\t\t */\n\t\t\tif (val & (NAPIF_STATE_DISABLE | NAPIF_STATE_SCHED |\n\t\t\t\t   NAPIF_STATE_IN_BUSY_POLL))\n\t\t\t\tgoto count;\n\t\t\tif (cmpxchg(&napi->state, val,\n\t\t\t\t    val | NAPIF_STATE_IN_BUSY_POLL |\n\t\t\t\t\t  NAPIF_STATE_SCHED) != val)\n\t\t\t\tgoto count;\n\t\t\thave_poll_lock = netpoll_poll_lock(napi);\n\t\t\tnapi_poll = napi->poll;\n\t\t}\n\t\twork = napi_poll(napi, BUSY_POLL_BUDGET);\n\t\ttrace_napi_poll(napi, work, BUSY_POLL_BUDGET);\ncount:\n\t\tif (work > 0)\n\t\t\t__NET_ADD_STATS(dev_net(napi->dev),\n\t\t\t\t\tLINUX_MIB_BUSYPOLLRXPACKETS, work);\n\t\tlocal_bh_enable();\n\n\t\tif (!loop_end || loop_end(loop_end_arg, start_time))\n\t\t\tbreak;\n\n\t\tif (unlikely(need_resched())) {\n\t\t\tif (napi_poll)\n\t\t\t\tbusy_poll_stop(napi, have_poll_lock);\n\t\t\tpreempt_enable();\n\t\t\trcu_read_unlock();\n\t\t\tcond_resched();\n\t\t\tif (loop_end(loop_end_arg, start_time))\n\t\t\t\treturn;\n\t\t\tgoto restart;\n\t\t}\n\t\tcpu_relax();\n\t}\n\tif (napi_poll)\n\t\tbusy_poll_stop(napi, have_poll_lock);\n\tpreempt_enable();\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(napi_busy_loop);\n\n#endif /* CONFIG_NET_RX_BUSY_POLL */\n\nstatic void napi_hash_add(struct napi_struct *napi)\n{\n\tif (test_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state) ||\n\t    test_and_set_bit(NAPI_STATE_HASHED, &napi->state))\n\t\treturn;\n\n\tspin_lock(&napi_hash_lock);\n\n\t/* 0..NR_CPUS range is reserved for sender_cpu use */\n\tdo {\n\t\tif (unlikely(++napi_gen_id < MIN_NAPI_ID))\n\t\t\tnapi_gen_id = MIN_NAPI_ID;\n\t} while (napi_by_id(napi_gen_id));\n\tnapi->napi_id = napi_gen_id;\n\n\thlist_add_head_rcu(&napi->napi_hash_node,\n\t\t\t   &napi_hash[napi->napi_id % HASH_SIZE(napi_hash)]);\n\n\tspin_unlock(&napi_hash_lock);\n}\n\n/* Warning : caller is responsible to make sure rcu grace period\n * is respected before freeing memory containing @napi\n */\nbool napi_hash_del(struct napi_struct *napi)\n{\n\tbool rcu_sync_needed = false;\n\n\tspin_lock(&napi_hash_lock);\n\n\tif (test_and_clear_bit(NAPI_STATE_HASHED, &napi->state)) {\n\t\trcu_sync_needed = true;\n\t\thlist_del_rcu(&napi->napi_hash_node);\n\t}\n\tspin_unlock(&napi_hash_lock);\n\treturn rcu_sync_needed;\n}\nEXPORT_SYMBOL_GPL(napi_hash_del);\n\nstatic enum hrtimer_restart napi_watchdog(struct hrtimer *timer)\n{\n\tstruct napi_struct *napi;\n\n\tnapi = container_of(timer, struct napi_struct, timer);\n\n\t/* Note : we use a relaxed variant of napi_schedule_prep() not setting\n\t * NAPI_STATE_MISSED, since we do not react to a device IRQ.\n\t */\n\tif (napi->gro_list && !napi_disable_pending(napi) &&\n\t    !test_and_set_bit(NAPI_STATE_SCHED, &napi->state))\n\t\t__napi_schedule_irqoff(napi);\n\n\treturn HRTIMER_NORESTART;\n}\n\nvoid netif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t\t    int (*poll)(struct napi_struct *, int), int weight)\n{\n\tINIT_LIST_HEAD(&napi->poll_list);\n\thrtimer_init(&napi->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);\n\tnapi->timer.function = napi_watchdog;\n\tnapi->gro_count = 0;\n\tnapi->gro_list = NULL;\n\tnapi->skb = NULL;\n\tnapi->poll = poll;\n\tif (weight > NAPI_POLL_WEIGHT)\n\t\tpr_err_once(\"netif_napi_add() called with weight %d on device %s\\n\",\n\t\t\t    weight, dev->name);\n\tnapi->weight = weight;\n\tlist_add(&napi->dev_list, &dev->napi_list);\n\tnapi->dev = dev;\n#ifdef CONFIG_NETPOLL\n\tnapi->poll_owner = -1;\n#endif\n\tset_bit(NAPI_STATE_SCHED, &napi->state);\n\tnapi_hash_add(napi);\n}\nEXPORT_SYMBOL(netif_napi_add);\n\nvoid napi_disable(struct napi_struct *n)\n{\n\tmight_sleep();\n\tset_bit(NAPI_STATE_DISABLE, &n->state);\n\n\twhile (test_and_set_bit(NAPI_STATE_SCHED, &n->state))\n\t\tmsleep(1);\n\twhile (test_and_set_bit(NAPI_STATE_NPSVC, &n->state))\n\t\tmsleep(1);\n\n\thrtimer_cancel(&n->timer);\n\n\tclear_bit(NAPI_STATE_DISABLE, &n->state);\n}\nEXPORT_SYMBOL(napi_disable);\n\n/* Must be called in process context */\nvoid netif_napi_del(struct napi_struct *napi)\n{\n\tmight_sleep();\n\tif (napi_hash_del(napi))\n\t\tsynchronize_net();\n\tlist_del_init(&napi->dev_list);\n\tnapi_free_frags(napi);\n\n\tkfree_skb_list(napi->gro_list);\n\tnapi->gro_list = NULL;\n\tnapi->gro_count = 0;\n}\nEXPORT_SYMBOL(netif_napi_del);\n\nstatic int napi_poll(struct napi_struct *n, struct list_head *repoll)\n{\n\tvoid *have;\n\tint work, weight;\n\n\tlist_del_init(&n->poll_list);\n\n\thave = netpoll_poll_lock(n);\n\n\tweight = n->weight;\n\n\t/* This NAPI_STATE_SCHED test is for avoiding a race\n\t * with netpoll's poll_napi().  Only the entity which\n\t * obtains the lock and sees NAPI_STATE_SCHED set will\n\t * actually make the ->poll() call.  Therefore we avoid\n\t * accidentally calling ->poll() when NAPI is not scheduled.\n\t */\n\twork = 0;\n\tif (test_bit(NAPI_STATE_SCHED, &n->state)) {\n\t\twork = n->poll(n, weight);\n\t\ttrace_napi_poll(n, work, weight);\n\t}\n\n\tWARN_ON_ONCE(work > weight);\n\n\tif (likely(work < weight))\n\t\tgoto out_unlock;\n\n\t/* Drivers must not modify the NAPI state if they\n\t * consume the entire weight.  In such cases this code\n\t * still \"owns\" the NAPI instance and therefore can\n\t * move the instance around on the list at-will.\n\t */\n\tif (unlikely(napi_disable_pending(n))) {\n\t\tnapi_complete(n);\n\t\tgoto out_unlock;\n\t}\n\n\tif (n->gro_list) {\n\t\t/* flush too old packets\n\t\t * If HZ < 1000, flush all packets.\n\t\t */\n\t\tnapi_gro_flush(n, HZ >= 1000);\n\t}\n\n\t/* Some drivers may have called napi_schedule\n\t * prior to exhausting their budget.\n\t */\n\tif (unlikely(!list_empty(&n->poll_list))) {\n\t\tpr_warn_once(\"%s: Budget exhausted after napi rescheduled\\n\",\n\t\t\t     n->dev ? n->dev->name : \"backlog\");\n\t\tgoto out_unlock;\n\t}\n\n\tlist_add_tail(&n->poll_list, repoll);\n\nout_unlock:\n\tnetpoll_poll_unlock(have);\n\n\treturn work;\n}\n\nstatic __latent_entropy void net_rx_action(struct softirq_action *h)\n{\n\tstruct softnet_data *sd = this_cpu_ptr(&softnet_data);\n\tunsigned long time_limit = jiffies +\n\t\tusecs_to_jiffies(netdev_budget_usecs);\n\tint budget = netdev_budget;\n\tLIST_HEAD(list);\n\tLIST_HEAD(repoll);\n\n\tlocal_irq_disable();\n\tlist_splice_init(&sd->poll_list, &list);\n\tlocal_irq_enable();\n\n\tfor (;;) {\n\t\tstruct napi_struct *n;\n\n\t\tif (list_empty(&list)) {\n\t\t\tif (!sd_has_rps_ipi_waiting(sd) && list_empty(&repoll))\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\n\t\tn = list_first_entry(&list, struct napi_struct, poll_list);\n\t\tbudget -= napi_poll(n, &repoll);\n\n\t\t/* If softirq window is exhausted then punt.\n\t\t * Allow this to run for 2 jiffies since which will allow\n\t\t * an average latency of 1.5/HZ.\n\t\t */\n\t\tif (unlikely(budget <= 0 ||\n\t\t\t     time_after_eq(jiffies, time_limit))) {\n\t\t\tsd->time_squeeze++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlocal_irq_disable();\n\n\tlist_splice_tail_init(&sd->poll_list, &list);\n\tlist_splice_tail(&repoll, &list);\n\tlist_splice(&list, &sd->poll_list);\n\tif (!list_empty(&sd->poll_list))\n\t\t__raise_softirq_irqoff(NET_RX_SOFTIRQ);\n\n\tnet_rps_action_and_irq_enable(sd);\nout:\n\t__kfree_skb_flush();\n}\n\nstruct netdev_adjacent {\n\tstruct net_device *dev;\n\n\t/* upper master flag, there can only be one master device per list */\n\tbool master;\n\n\t/* counter for the number of times this device was added to us */\n\tu16 ref_nr;\n\n\t/* private field for the users */\n\tvoid *private;\n\n\tstruct list_head list;\n\tstruct rcu_head rcu;\n};\n\nstatic struct netdev_adjacent *__netdev_find_adj(struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tlist_for_each_entry(adj, adj_list, list) {\n\t\tif (adj->dev == adj_dev)\n\t\t\treturn adj;\n\t}\n\treturn NULL;\n}\n\nstatic int __netdev_has_upper_dev(struct net_device *upper_dev, void *data)\n{\n\tstruct net_device *dev = data;\n\n\treturn upper_dev == dev;\n}\n\n/**\n * netdev_has_upper_dev - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks only immediate upper device,\n * not through a complete stack of devices. The caller must hold the RTNL lock.\n */\nbool netdev_has_upper_dev(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\tASSERT_RTNL();\n\n\treturn netdev_walk_all_upper_dev_rcu(dev, __netdev_has_upper_dev,\n\t\t\t\t\t     upper_dev);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev);\n\n/**\n * netdev_has_upper_dev_all - Check if device is linked to an upper device\n * @dev: device\n * @upper_dev: upper device to check\n *\n * Find out if a device is linked to specified upper device and return true\n * in case it is. Note that this checks the entire upper device chain.\n * The caller must hold rcu lock.\n */\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev)\n{\n\treturn !!netdev_walk_all_upper_dev_rcu(dev, __netdev_has_upper_dev,\n\t\t\t\t\t       upper_dev);\n}\nEXPORT_SYMBOL(netdev_has_upper_dev_all_rcu);\n\n/**\n * netdev_has_any_upper_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to an upper device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nbool netdev_has_any_upper_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.upper);\n}\nEXPORT_SYMBOL(netdev_has_any_upper_dev);\n\n/**\n * netdev_master_upper_dev_get - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RTNL lock.\n */\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tASSERT_RTNL();\n\n\tif (list_empty(&dev->adj_list.upper))\n\t\treturn NULL;\n\n\tupper = list_first_entry(&dev->adj_list.upper,\n\t\t\t\t struct netdev_adjacent, list);\n\tif (likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get);\n\n/**\n * netdev_has_any_lower_dev - Check if device is linked to some device\n * @dev: device\n *\n * Find out if a device is linked to a lower device and return true in case\n * it is. The caller must hold the RTNL lock.\n */\nstatic bool netdev_has_any_lower_dev(struct net_device *dev)\n{\n\tASSERT_RTNL();\n\n\treturn !list_empty(&dev->adj_list.lower);\n}\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tadj = list_entry(adj_list, struct netdev_adjacent, list);\n\n\treturn adj->private;\n}\nEXPORT_SYMBOL(netdev_adjacent_get_private);\n\n/**\n * netdev_upper_get_next_dev_rcu - Get the next dev from upper list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next device from the dev's upper list, starting from iter\n * position. The caller must hold RCU read lock.\n */\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\nEXPORT_SYMBOL(netdev_upper_get_next_dev_rcu);\n\nstatic struct net_device *netdev_next_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *upper;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !lockdep_rtnl_is_held());\n\n\tupper = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&upper->list == &dev->adj_list.upper)\n\t\treturn NULL;\n\n\t*iter = &upper->list;\n\n\treturn upper->dev;\n}\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data)\n{\n\tstruct net_device *udev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.upper,\n\t     udev = netdev_next_upper_dev_rcu(dev, &iter);\n\t     udev;\n\t     udev = netdev_next_upper_dev_rcu(dev, &iter)) {\n\t\t/* first is the upper device itself */\n\t\tret = fn(udev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its upper devices */\n\t\tret = netdev_walk_all_upper_dev_rcu(udev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_upper_dev_rcu);\n\n/**\n * netdev_lower_get_next_private - Get the next ->private from the\n *\t\t\t\t   lower neighbour list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold either hold the\n * RTNL lock or its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private);\n\n/**\n * netdev_lower_get_next_private_rcu - Get the next ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent->private from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_get_next_private_rcu);\n\n/**\n * netdev_lower_get_next - Get the next device from the lower neighbour\n *                         list\n * @dev: device\n * @iter: list_head ** of the current position\n *\n * Gets the next netdev_adjacent from the dev's lower neighbour\n * list, starting from iter position. The caller must hold RTNL lock or\n * its own locking that guarantees that the neighbour lower\n * list will remain unchanged.\n */\nvoid *netdev_lower_get_next(struct net_device *dev, struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry(*iter, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = lower->list.next;\n\n\treturn lower->dev;\n}\nEXPORT_SYMBOL(netdev_lower_get_next);\n\nstatic struct net_device *netdev_next_lower_dev(struct net_device *dev,\n\t\t\t\t\t\tstruct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry((*iter)->next, struct netdev_adjacent, list);\n\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *dev,\n\t\t\t\t\tvoid *data),\n\t\t\t      void *data)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = netdev_next_lower_dev(dev, &iter);\n\t     ldev;\n\t     ldev = netdev_next_lower_dev(dev, &iter)) {\n\t\t/* first is the lower device itself */\n\t\tret = fn(ldev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its lower devices */\n\t\tret = netdev_walk_all_lower_dev(ldev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev);\n\nstatic struct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t    struct list_head **iter)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_entry_rcu((*iter)->next, struct netdev_adjacent, list);\n\tif (&lower->list == &dev->adj_list.lower)\n\t\treturn NULL;\n\n\t*iter = &lower->list;\n\n\treturn lower->dev;\n}\n\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *dev,\n\t\t\t\t\t    void *data),\n\t\t\t\t  void *data)\n{\n\tstruct net_device *ldev;\n\tstruct list_head *iter;\n\tint ret;\n\n\tfor (iter = &dev->adj_list.lower,\n\t     ldev = netdev_next_lower_dev_rcu(dev, &iter);\n\t     ldev;\n\t     ldev = netdev_next_lower_dev_rcu(dev, &iter)) {\n\t\t/* first is the lower device itself */\n\t\tret = fn(ldev, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* then look at all of its lower devices */\n\t\tret = netdev_walk_all_lower_dev_rcu(ldev, fn, data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netdev_walk_all_lower_dev_rcu);\n\n/**\n * netdev_lower_get_first_private_rcu - Get the first ->private from the\n *\t\t\t\t       lower neighbour list, RCU\n *\t\t\t\t       variant\n * @dev: device\n *\n * Gets the first netdev_adjacent->private from the dev's lower neighbour\n * list. The caller must hold RCU read lock.\n */\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tlower = list_first_or_null_rcu(&dev->adj_list.lower,\n\t\t\tstruct netdev_adjacent, list);\n\tif (lower)\n\t\treturn lower->private;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_lower_get_first_private_rcu);\n\n/**\n * netdev_master_upper_dev_get_rcu - Get master upper device\n * @dev: device\n *\n * Find a master upper device and return pointer to it or NULL in case\n * it's not there. The caller must hold the RCU read lock.\n */\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev)\n{\n\tstruct netdev_adjacent *upper;\n\n\tupper = list_first_or_null_rcu(&dev->adj_list.upper,\n\t\t\t\t       struct netdev_adjacent, list);\n\tif (upper && likely(upper->master))\n\t\treturn upper->dev;\n\treturn NULL;\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_get_rcu);\n\nstatic int netdev_adjacent_sysfs_add(struct net_device *dev,\n\t\t\t      struct net_device *adj_dev,\n\t\t\t      struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", adj_dev->name);\n\treturn sysfs_create_link(&(dev->dev.kobj), &(adj_dev->dev.kobj),\n\t\t\t\t linkname);\n}\nstatic void netdev_adjacent_sysfs_del(struct net_device *dev,\n\t\t\t       char *name,\n\t\t\t       struct list_head *dev_list)\n{\n\tchar linkname[IFNAMSIZ+7];\n\n\tsprintf(linkname, dev_list == &dev->adj_list.upper ?\n\t\t\"upper_%s\" : \"lower_%s\", name);\n\tsysfs_remove_link(&(dev->dev.kobj), linkname);\n}\n\nstatic inline bool netdev_adjacent_is_neigh_list(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t\t struct list_head *dev_list)\n{\n\treturn (dev_list == &dev->adj_list.upper ||\n\t\tdev_list == &dev->adj_list.lower) &&\n\t\tnet_eq(dev_net(dev), dev_net(adj_dev));\n}\n\nstatic int __netdev_adjacent_dev_insert(struct net_device *dev,\n\t\t\t\t\tstruct net_device *adj_dev,\n\t\t\t\t\tstruct list_head *dev_list,\n\t\t\t\t\tvoid *private, bool master)\n{\n\tstruct netdev_adjacent *adj;\n\tint ret;\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (adj) {\n\t\tadj->ref_nr += 1;\n\t\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d\\n\",\n\t\t\t dev->name, adj_dev->name, adj->ref_nr);\n\n\t\treturn 0;\n\t}\n\n\tadj = kmalloc(sizeof(*adj), GFP_KERNEL);\n\tif (!adj)\n\t\treturn -ENOMEM;\n\n\tadj->dev = adj_dev;\n\tadj->master = master;\n\tadj->ref_nr = 1;\n\tadj->private = private;\n\tdev_hold(adj_dev);\n\n\tpr_debug(\"Insert adjacency: dev %s adj_dev %s adj->ref_nr %d; dev_hold on %s\\n\",\n\t\t dev->name, adj_dev->name, adj->ref_nr, adj_dev->name);\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list)) {\n\t\tret = netdev_adjacent_sysfs_add(dev, adj_dev, dev_list);\n\t\tif (ret)\n\t\t\tgoto free_adj;\n\t}\n\n\t/* Ensure that master link is always the first item in list. */\n\tif (master) {\n\t\tret = sysfs_create_link(&(dev->dev.kobj),\n\t\t\t\t\t&(adj_dev->dev.kobj), \"master\");\n\t\tif (ret)\n\t\t\tgoto remove_symlinks;\n\n\t\tlist_add_rcu(&adj->list, dev_list);\n\t} else {\n\t\tlist_add_tail_rcu(&adj->list, dev_list);\n\t}\n\n\treturn 0;\n\nremove_symlinks:\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\nfree_adj:\n\tkfree(adj);\n\tdev_put(adj_dev);\n\n\treturn ret;\n}\n\nstatic void __netdev_adjacent_dev_remove(struct net_device *dev,\n\t\t\t\t\t struct net_device *adj_dev,\n\t\t\t\t\t u16 ref_nr,\n\t\t\t\t\t struct list_head *dev_list)\n{\n\tstruct netdev_adjacent *adj;\n\n\tpr_debug(\"Remove adjacency: dev %s adj_dev %s ref_nr %d\\n\",\n\t\t dev->name, adj_dev->name, ref_nr);\n\n\tadj = __netdev_find_adj(adj_dev, dev_list);\n\n\tif (!adj) {\n\t\tpr_err(\"Adjacency does not exist for device %s from %s\\n\",\n\t\t       dev->name, adj_dev->name);\n\t\tWARN_ON(1);\n\t\treturn;\n\t}\n\n\tif (adj->ref_nr > ref_nr) {\n\t\tpr_debug(\"adjacency: %s to %s ref_nr - %d = %d\\n\",\n\t\t\t dev->name, adj_dev->name, ref_nr,\n\t\t\t adj->ref_nr - ref_nr);\n\t\tadj->ref_nr -= ref_nr;\n\t\treturn;\n\t}\n\n\tif (adj->master)\n\t\tsysfs_remove_link(&(dev->dev.kobj), \"master\");\n\n\tif (netdev_adjacent_is_neigh_list(dev, adj_dev, dev_list))\n\t\tnetdev_adjacent_sysfs_del(dev, adj_dev->name, dev_list);\n\n\tlist_del_rcu(&adj->list);\n\tpr_debug(\"adjacency: dev_put for %s, because link removed from %s to %s\\n\",\n\t\t adj_dev->name, dev->name, adj_dev->name);\n\tdev_put(adj_dev);\n\tkfree_rcu(adj, rcu);\n}\n\nstatic int __netdev_adjacent_dev_link_lists(struct net_device *dev,\n\t\t\t\t\t    struct net_device *upper_dev,\n\t\t\t\t\t    struct list_head *up_list,\n\t\t\t\t\t    struct list_head *down_list,\n\t\t\t\t\t    void *private, bool master)\n{\n\tint ret;\n\n\tret = __netdev_adjacent_dev_insert(dev, upper_dev, up_list,\n\t\t\t\t\t   private, master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_insert(upper_dev, dev, down_list,\n\t\t\t\t\t   private, false);\n\tif (ret) {\n\t\t__netdev_adjacent_dev_remove(dev, upper_dev, 1, up_list);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void __netdev_adjacent_dev_unlink_lists(struct net_device *dev,\n\t\t\t\t\t       struct net_device *upper_dev,\n\t\t\t\t\t       u16 ref_nr,\n\t\t\t\t\t       struct list_head *up_list,\n\t\t\t\t\t       struct list_head *down_list)\n{\n\t__netdev_adjacent_dev_remove(dev, upper_dev, ref_nr, up_list);\n\t__netdev_adjacent_dev_remove(upper_dev, dev, ref_nr, down_list);\n}\n\nstatic int __netdev_adjacent_dev_link_neighbour(struct net_device *dev,\n\t\t\t\t\t\tstruct net_device *upper_dev,\n\t\t\t\t\t\tvoid *private, bool master)\n{\n\treturn __netdev_adjacent_dev_link_lists(dev, upper_dev,\n\t\t\t\t\t\t&dev->adj_list.upper,\n\t\t\t\t\t\t&upper_dev->adj_list.lower,\n\t\t\t\t\t\tprivate, master);\n}\n\nstatic void __netdev_adjacent_dev_unlink_neighbour(struct net_device *dev,\n\t\t\t\t\t\t   struct net_device *upper_dev)\n{\n\t__netdev_adjacent_dev_unlink_lists(dev, upper_dev, 1,\n\t\t\t\t\t   &dev->adj_list.upper,\n\t\t\t\t\t   &upper_dev->adj_list.lower);\n}\n\nstatic int __netdev_upper_dev_link(struct net_device *dev,\n\t\t\t\t   struct net_device *upper_dev, bool master,\n\t\t\t\t   void *upper_priv, void *upper_info)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\tint ret = 0;\n\n\tASSERT_RTNL();\n\n\tif (dev == upper_dev)\n\t\treturn -EBUSY;\n\n\t/* To prevent loops, check if dev is not upper device to upper_dev. */\n\tif (netdev_has_upper_dev(upper_dev, dev))\n\t\treturn -EBUSY;\n\n\tif (netdev_has_upper_dev(dev, upper_dev))\n\t\treturn -EEXIST;\n\n\tif (master && netdev_master_upper_dev_get(dev))\n\t\treturn -EBUSY;\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = master;\n\tchangeupper_info.linking = true;\n\tchangeupper_info.upper_info = upper_info;\n\n\tret = call_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __netdev_adjacent_dev_link_neighbour(dev, upper_dev, upper_priv,\n\t\t\t\t\t\t   master);\n\tif (ret)\n\t\treturn ret;\n\n\tret = call_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t\t    &changeupper_info.info);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto rollback;\n\n\treturn 0;\n\nrollback:\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\treturn ret;\n}\n\n/**\n * netdev_upper_dev_link - Add a link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Adds a link to device which is upper to this one. The caller must hold\n * the RTNL lock. On a failure a negative errno code is returned.\n * On success the reference counts are adjusted and the function\n * returns zero.\n */\nint netdev_upper_dev_link(struct net_device *dev,\n\t\t\t  struct net_device *upper_dev)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, false, NULL, NULL);\n}\nEXPORT_SYMBOL(netdev_upper_dev_link);\n\n/**\n * netdev_master_upper_dev_link - Add a master link to the upper device\n * @dev: device\n * @upper_dev: new upper device\n * @upper_priv: upper device private\n * @upper_info: upper info to be passed down via notifier\n *\n * Adds a link to device which is upper to this one. In this case, only\n * one master upper device can be linked, although other non-master devices\n * might be linked as well. The caller must hold the RTNL lock.\n * On a failure a negative errno code is returned. On success the reference\n * counts are adjusted and the function returns zero.\n */\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info)\n{\n\treturn __netdev_upper_dev_link(dev, upper_dev, true,\n\t\t\t\t       upper_priv, upper_info);\n}\nEXPORT_SYMBOL(netdev_master_upper_dev_link);\n\n/**\n * netdev_upper_dev_unlink - Removes a link to upper device\n * @dev: device\n * @upper_dev: new upper device\n *\n * Removes a link to device which is upper to this one. The caller must hold\n * the RTNL lock.\n */\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev)\n{\n\tstruct netdev_notifier_changeupper_info changeupper_info;\n\n\tASSERT_RTNL();\n\n\tchangeupper_info.upper_dev = upper_dev;\n\tchangeupper_info.master = netdev_master_upper_dev_get(dev) == upper_dev;\n\tchangeupper_info.linking = false;\n\n\tcall_netdevice_notifiers_info(NETDEV_PRECHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n\n\t__netdev_adjacent_dev_unlink_neighbour(dev, upper_dev);\n\n\tcall_netdevice_notifiers_info(NETDEV_CHANGEUPPER, dev,\n\t\t\t\t      &changeupper_info.info);\n}\nEXPORT_SYMBOL(netdev_upper_dev_unlink);\n\n/**\n * netdev_bonding_info_change - Dispatch event about slave change\n * @dev: device\n * @bonding_info: info to dispatch\n *\n * Send NETDEV_BONDING_INFO to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info)\n{\n\tstruct netdev_notifier_bonding_info\tinfo;\n\n\tmemcpy(&info.bonding_info, bonding_info,\n\t       sizeof(struct netdev_bonding_info));\n\tcall_netdevice_notifiers_info(NETDEV_BONDING_INFO, dev,\n\t\t\t\t      &info.info);\n}\nEXPORT_SYMBOL(netdev_bonding_info_change);\n\nstatic void netdev_adjacent_add_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(dev, iter->dev,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nstatic void netdev_adjacent_del_links(struct net_device *dev)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.upper);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, dev->name,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_del(dev, iter->dev->name,\n\t\t\t\t\t  &dev->adj_list.lower);\n\t}\n}\n\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname)\n{\n\tstruct netdev_adjacent *iter;\n\n\tstruct net *net = dev_net(dev);\n\n\tlist_for_each_entry(iter, &dev->adj_list.upper, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.lower);\n\t}\n\n\tlist_for_each_entry(iter, &dev->adj_list.lower, list) {\n\t\tif (!net_eq(net, dev_net(iter->dev)))\n\t\t\tcontinue;\n\t\tnetdev_adjacent_sysfs_del(iter->dev, oldname,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t\tnetdev_adjacent_sysfs_add(iter->dev, dev,\n\t\t\t\t\t  &iter->dev->adj_list.upper);\n\t}\n}\n\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev)\n{\n\tstruct netdev_adjacent *lower;\n\n\tif (!lower_dev)\n\t\treturn NULL;\n\tlower = __netdev_find_adj(lower_dev, &dev->adj_list.lower);\n\tif (!lower)\n\t\treturn NULL;\n\n\treturn lower->private;\n}\nEXPORT_SYMBOL(netdev_lower_dev_get_private);\n\n\nint dev_get_nest_level(struct net_device *dev)\n{\n\tstruct net_device *lower = NULL;\n\tstruct list_head *iter;\n\tint max_nest = -1;\n\tint nest;\n\n\tASSERT_RTNL();\n\n\tnetdev_for_each_lower_dev(dev, lower, iter) {\n\t\tnest = dev_get_nest_level(lower);\n\t\tif (max_nest < nest)\n\t\t\tmax_nest = nest;\n\t}\n\n\treturn max_nest + 1;\n}\nEXPORT_SYMBOL(dev_get_nest_level);\n\n/**\n * netdev_lower_change - Dispatch event about lower device state change\n * @lower_dev: device\n * @lower_state_info: state to dispatch\n *\n * Send NETDEV_CHANGELOWERSTATE to netdev notifiers with info.\n * The caller must hold the RTNL lock.\n */\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info)\n{\n\tstruct netdev_notifier_changelowerstate_info changelowerstate_info;\n\n\tASSERT_RTNL();\n\tchangelowerstate_info.lower_state_info = lower_state_info;\n\tcall_netdevice_notifiers_info(NETDEV_CHANGELOWERSTATE, lower_dev,\n\t\t\t\t      &changelowerstate_info.info);\n}\nEXPORT_SYMBOL(netdev_lower_state_changed);\n\nstatic void dev_change_rx_flags(struct net_device *dev, int flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_rx_flags)\n\t\tops->ndo_change_rx_flags(dev, flags);\n}\n\nstatic int __dev_set_promiscuity(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_PROMISC;\n\tdev->promiscuity += inc;\n\tif (dev->promiscuity == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch promisc and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_PROMISC;\n\t\telse {\n\t\t\tdev->promiscuity -= inc;\n\t\t\tpr_warn(\"%s: promiscuity touches roof, set promiscuity failed. promiscuity feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags != old_flags) {\n\t\tpr_info(\"device %s %s promiscuous mode\\n\",\n\t\t\tdev->name,\n\t\t\tdev->flags & IFF_PROMISC ? \"entered\" : \"left\");\n\t\tif (audit_enabled) {\n\t\t\tcurrent_uid_gid(&uid, &gid);\n\t\t\taudit_log(current->audit_context, GFP_ATOMIC,\n\t\t\t\tAUDIT_ANOM_PROMISCUOUS,\n\t\t\t\t\"dev=%s prom=%d old_prom=%d auid=%u uid=%u gid=%u ses=%u\",\n\t\t\t\tdev->name, (dev->flags & IFF_PROMISC),\n\t\t\t\t(old_flags & IFF_PROMISC),\n\t\t\t\tfrom_kuid(&init_user_ns, audit_get_loginuid(current)),\n\t\t\t\tfrom_kuid(&init_user_ns, uid),\n\t\t\t\tfrom_kgid(&init_user_ns, gid),\n\t\t\t\taudit_get_sessionid(current));\n\t\t}\n\n\t\tdev_change_rx_flags(dev, IFF_PROMISC);\n\t}\n\tif (notify)\n\t\t__dev_notify_flags(dev, old_flags, IFF_PROMISC);\n\treturn 0;\n}\n\n/**\n *\tdev_set_promiscuity\t- update promiscuity count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove promiscuity from a device. While the count in the device\n *\tremains above zero the interface remains promiscuous. Once it hits zero\n *\tthe device reverts back to normal filtering operation. A negative inc\n *\tvalue is used to drop promiscuity on the device.\n *\tReturn 0 if successful or a negative errno code on error.\n */\nint dev_set_promiscuity(struct net_device *dev, int inc)\n{\n\tunsigned int old_flags = dev->flags;\n\tint err;\n\n\terr = __dev_set_promiscuity(dev, inc, true);\n\tif (err < 0)\n\t\treturn err;\n\tif (dev->flags != old_flags)\n\t\tdev_set_rx_mode(dev);\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_promiscuity);\n\nstatic int __dev_set_allmulti(struct net_device *dev, int inc, bool notify)\n{\n\tunsigned int old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tASSERT_RTNL();\n\n\tdev->flags |= IFF_ALLMULTI;\n\tdev->allmulti += inc;\n\tif (dev->allmulti == 0) {\n\t\t/*\n\t\t * Avoid overflow.\n\t\t * If inc causes overflow, untouch allmulti and return error.\n\t\t */\n\t\tif (inc < 0)\n\t\t\tdev->flags &= ~IFF_ALLMULTI;\n\t\telse {\n\t\t\tdev->allmulti -= inc;\n\t\t\tpr_warn(\"%s: allmulti touches roof, set allmulti failed. allmulti feature of device might be broken.\\n\",\n\t\t\t\tdev->name);\n\t\t\treturn -EOVERFLOW;\n\t\t}\n\t}\n\tif (dev->flags ^ old_flags) {\n\t\tdev_change_rx_flags(dev, IFF_ALLMULTI);\n\t\tdev_set_rx_mode(dev);\n\t\tif (notify)\n\t\t\t__dev_notify_flags(dev, old_flags,\n\t\t\t\t\t   dev->gflags ^ old_gflags);\n\t}\n\treturn 0;\n}\n\n/**\n *\tdev_set_allmulti\t- update allmulti count on a device\n *\t@dev: device\n *\t@inc: modifier\n *\n *\tAdd or remove reception of all multicast frames to a device. While the\n *\tcount in the device remains above zero the interface remains listening\n *\tto all interfaces. Once it hits zero the device reverts back to normal\n *\tfiltering operation. A negative @inc value is used to drop the counter\n *\twhen releasing a resource needing all multicasts.\n *\tReturn 0 if successful or a negative errno code on error.\n */\n\nint dev_set_allmulti(struct net_device *dev, int inc)\n{\n\treturn __dev_set_allmulti(dev, inc, true);\n}\nEXPORT_SYMBOL(dev_set_allmulti);\n\n/*\n *\tUpload unicast and multicast address lists to device and\n *\tconfigure RX filtering. When the device doesn't support unicast\n *\tfiltering it is put in promiscuous mode while unicast addresses\n *\tare present.\n */\nvoid __dev_set_rx_mode(struct net_device *dev)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\t/* dev_open will call this function so the list will stay sane. */\n\tif (!(dev->flags&IFF_UP))\n\t\treturn;\n\n\tif (!netif_device_present(dev))\n\t\treturn;\n\n\tif (!(dev->priv_flags & IFF_UNICAST_FLT)) {\n\t\t/* Unicast addresses changes may only happen under the rtnl,\n\t\t * therefore calling __dev_set_promiscuity here is safe.\n\t\t */\n\t\tif (!netdev_uc_empty(dev) && !dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, 1, false);\n\t\t\tdev->uc_promisc = true;\n\t\t} else if (netdev_uc_empty(dev) && dev->uc_promisc) {\n\t\t\t__dev_set_promiscuity(dev, -1, false);\n\t\t\tdev->uc_promisc = false;\n\t\t}\n\t}\n\n\tif (ops->ndo_set_rx_mode)\n\t\tops->ndo_set_rx_mode(dev);\n}\n\nvoid dev_set_rx_mode(struct net_device *dev)\n{\n\tnetif_addr_lock_bh(dev);\n\t__dev_set_rx_mode(dev);\n\tnetif_addr_unlock_bh(dev);\n}\n\n/**\n *\tdev_get_flags - get flags reported to userspace\n *\t@dev: device\n *\n *\tGet the combination of flag bits exported through APIs to userspace.\n */\nunsigned int dev_get_flags(const struct net_device *dev)\n{\n\tunsigned int flags;\n\n\tflags = (dev->flags & ~(IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI |\n\t\t\t\tIFF_RUNNING |\n\t\t\t\tIFF_LOWER_UP |\n\t\t\t\tIFF_DORMANT)) |\n\t\t(dev->gflags & (IFF_PROMISC |\n\t\t\t\tIFF_ALLMULTI));\n\n\tif (netif_running(dev)) {\n\t\tif (netif_oper_up(dev))\n\t\t\tflags |= IFF_RUNNING;\n\t\tif (netif_carrier_ok(dev))\n\t\t\tflags |= IFF_LOWER_UP;\n\t\tif (netif_dormant(dev))\n\t\t\tflags |= IFF_DORMANT;\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL(dev_get_flags);\n\nint __dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tunsigned int old_flags = dev->flags;\n\tint ret;\n\n\tASSERT_RTNL();\n\n\t/*\n\t *\tSet the flags on our device.\n\t */\n\n\tdev->flags = (flags & (IFF_DEBUG | IFF_NOTRAILERS | IFF_NOARP |\n\t\t\t       IFF_DYNAMIC | IFF_MULTICAST | IFF_PORTSEL |\n\t\t\t       IFF_AUTOMEDIA)) |\n\t\t     (dev->flags & (IFF_UP | IFF_VOLATILE | IFF_PROMISC |\n\t\t\t\t    IFF_ALLMULTI));\n\n\t/*\n\t *\tLoad in the correct multicast list now the flags have changed.\n\t */\n\n\tif ((old_flags ^ flags) & IFF_MULTICAST)\n\t\tdev_change_rx_flags(dev, IFF_MULTICAST);\n\n\tdev_set_rx_mode(dev);\n\n\t/*\n\t *\tHave we downed the interface. We handle IFF_UP ourselves\n\t *\taccording to user attempts to set it, rather than blindly\n\t *\tsetting it.\n\t */\n\n\tret = 0;\n\tif ((old_flags ^ flags) & IFF_UP) {\n\t\tif (old_flags & IFF_UP)\n\t\t\t__dev_close(dev);\n\t\telse\n\t\t\tret = __dev_open(dev);\n\t}\n\n\tif ((flags ^ dev->gflags) & IFF_PROMISC) {\n\t\tint inc = (flags & IFF_PROMISC) ? 1 : -1;\n\t\tunsigned int old_flags = dev->flags;\n\n\t\tdev->gflags ^= IFF_PROMISC;\n\n\t\tif (__dev_set_promiscuity(dev, inc, false) >= 0)\n\t\t\tif (dev->flags != old_flags)\n\t\t\t\tdev_set_rx_mode(dev);\n\t}\n\n\t/* NOTE: order of synchronization of IFF_PROMISC and IFF_ALLMULTI\n\t * is important. Some (broken) drivers set IFF_PROMISC, when\n\t * IFF_ALLMULTI is requested not asking us and not reporting.\n\t */\n\tif ((flags ^ dev->gflags) & IFF_ALLMULTI) {\n\t\tint inc = (flags & IFF_ALLMULTI) ? 1 : -1;\n\n\t\tdev->gflags ^= IFF_ALLMULTI;\n\t\t__dev_set_allmulti(dev, inc, false);\n\t}\n\n\treturn ret;\n}\n\nvoid __dev_notify_flags(struct net_device *dev, unsigned int old_flags,\n\t\t\tunsigned int gchanges)\n{\n\tunsigned int changes = dev->flags ^ old_flags;\n\n\tif (gchanges)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, gchanges, GFP_ATOMIC);\n\n\tif (changes & IFF_UP) {\n\t\tif (dev->flags & IFF_UP)\n\t\t\tcall_netdevice_notifiers(NETDEV_UP, dev);\n\t\telse\n\t\t\tcall_netdevice_notifiers(NETDEV_DOWN, dev);\n\t}\n\n\tif (dev->flags & IFF_UP &&\n\t    (changes & ~(IFF_UP | IFF_PROMISC | IFF_ALLMULTI | IFF_VOLATILE))) {\n\t\tstruct netdev_notifier_change_info change_info;\n\n\t\tchange_info.flags_changed = changes;\n\t\tcall_netdevice_notifiers_info(NETDEV_CHANGE, dev,\n\t\t\t\t\t      &change_info.info);\n\t}\n}\n\n/**\n *\tdev_change_flags - change device settings\n *\t@dev: device\n *\t@flags: device state flags\n *\n *\tChange settings on device based state flags. The flags are\n *\tin the userspace exported format.\n */\nint dev_change_flags(struct net_device *dev, unsigned int flags)\n{\n\tint ret;\n\tunsigned int changes, old_flags = dev->flags, old_gflags = dev->gflags;\n\n\tret = __dev_change_flags(dev, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tchanges = (old_flags ^ dev->flags) | (old_gflags ^ dev->gflags);\n\t__dev_notify_flags(dev, old_flags, changes);\n\treturn ret;\n}\nEXPORT_SYMBOL(dev_change_flags);\n\nint __dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_change_mtu)\n\t\treturn ops->ndo_change_mtu(dev, new_mtu);\n\n\tdev->mtu = new_mtu;\n\treturn 0;\n}\nEXPORT_SYMBOL(__dev_set_mtu);\n\n/**\n *\tdev_set_mtu - Change maximum transfer unit\n *\t@dev: device\n *\t@new_mtu: new transfer unit\n *\n *\tChange the maximum transfer size of the network device.\n */\nint dev_set_mtu(struct net_device *dev, int new_mtu)\n{\n\tint err, orig_mtu;\n\n\tif (new_mtu == dev->mtu)\n\t\treturn 0;\n\n\t/* MTU must be positive, and in range */\n\tif (new_mtu < 0 || new_mtu < dev->min_mtu) {\n\t\tnet_err_ratelimited(\"%s: Invalid MTU %d requested, hw min %d\\n\",\n\t\t\t\t    dev->name, new_mtu, dev->min_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tif (dev->max_mtu > 0 && new_mtu > dev->max_mtu) {\n\t\tnet_err_ratelimited(\"%s: Invalid MTU %d requested, hw max %d\\n\",\n\t\t\t\t    dev->name, new_mtu, dev->max_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\n\terr = call_netdevice_notifiers(NETDEV_PRECHANGEMTU, dev);\n\terr = notifier_to_errno(err);\n\tif (err)\n\t\treturn err;\n\n\torig_mtu = dev->mtu;\n\terr = __dev_set_mtu(dev, new_mtu);\n\n\tif (!err) {\n\t\terr = call_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\terr = notifier_to_errno(err);\n\t\tif (err) {\n\t\t\t/* setting mtu back and notifying everyone again,\n\t\t\t * so that they have a chance to revert changes.\n\t\t\t */\n\t\t\t__dev_set_mtu(dev, orig_mtu);\n\t\t\tcall_netdevice_notifiers(NETDEV_CHANGEMTU, dev);\n\t\t}\n\t}\n\treturn err;\n}\nEXPORT_SYMBOL(dev_set_mtu);\n\n/**\n *\tdev_set_group - Change group this device belongs to\n *\t@dev: device\n *\t@new_group: group this device should belong to\n */\nvoid dev_set_group(struct net_device *dev, int new_group)\n{\n\tdev->group = new_group;\n}\nEXPORT_SYMBOL(dev_set_group);\n\n/**\n *\tdev_set_mac_address - Change Media Access Control Address\n *\t@dev: device\n *\t@sa: new address\n *\n *\tChange the hardware (MAC) address of the device\n */\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tint err;\n\n\tif (!ops->ndo_set_mac_address)\n\t\treturn -EOPNOTSUPP;\n\tif (sa->sa_family != dev->type)\n\t\treturn -EINVAL;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\terr = ops->ndo_set_mac_address(dev, sa);\n\tif (err)\n\t\treturn err;\n\tdev->addr_assign_type = NET_ADDR_SET;\n\tcall_netdevice_notifiers(NETDEV_CHANGEADDR, dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(dev_set_mac_address);\n\n/**\n *\tdev_change_carrier - Change device carrier\n *\t@dev: device\n *\t@new_carrier: new value\n *\n *\tChange device carrier\n */\nint dev_change_carrier(struct net_device *dev, bool new_carrier)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_carrier)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_carrier(dev, new_carrier);\n}\nEXPORT_SYMBOL(dev_change_carrier);\n\n/**\n *\tdev_get_phys_port_id - Get device physical port ID\n *\t@dev: device\n *\t@ppid: port ID\n *\n *\tGet device physical port ID\n */\nint dev_get_phys_port_id(struct net_device *dev,\n\t\t\t struct netdev_phys_item_id *ppid)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_id)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_id(dev, ppid);\n}\nEXPORT_SYMBOL(dev_get_phys_port_id);\n\n/**\n *\tdev_get_phys_port_name - Get device physical port name\n *\t@dev: device\n *\t@name: port name\n *\t@len: limit of bytes to copy to name\n *\n *\tGet device physical port name\n */\nint dev_get_phys_port_name(struct net_device *dev,\n\t\t\t   char *name, size_t len)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_get_phys_port_name)\n\t\treturn -EOPNOTSUPP;\n\treturn ops->ndo_get_phys_port_name(dev, name, len);\n}\nEXPORT_SYMBOL(dev_get_phys_port_name);\n\n/**\n *\tdev_change_proto_down - update protocol port state information\n *\t@dev: device\n *\t@proto_down: new value\n *\n *\tThis info can be used by switch drivers to set the phys state of the\n *\tport.\n */\nint dev_change_proto_down(struct net_device *dev, bool proto_down)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (!ops->ndo_change_proto_down)\n\t\treturn -EOPNOTSUPP;\n\tif (!netif_device_present(dev))\n\t\treturn -ENODEV;\n\treturn ops->ndo_change_proto_down(dev, proto_down);\n}\nEXPORT_SYMBOL(dev_change_proto_down);\n\nu8 __dev_xdp_attached(struct net_device *dev, xdp_op_t xdp_op, u32 *prog_id)\n{\n\tstruct netdev_xdp xdp;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\txdp.command = XDP_QUERY_PROG;\n\n\t/* Query must always succeed. */\n\tWARN_ON(xdp_op(dev, &xdp) < 0);\n\tif (prog_id)\n\t\t*prog_id = xdp.prog_id;\n\n\treturn xdp.prog_attached;\n}\n\nstatic int dev_xdp_install(struct net_device *dev, xdp_op_t xdp_op,\n\t\t\t   struct netlink_ext_ack *extack, u32 flags,\n\t\t\t   struct bpf_prog *prog)\n{\n\tstruct netdev_xdp xdp;\n\n\tmemset(&xdp, 0, sizeof(xdp));\n\tif (flags & XDP_FLAGS_HW_MODE)\n\t\txdp.command = XDP_SETUP_PROG_HW;\n\telse\n\t\txdp.command = XDP_SETUP_PROG;\n\txdp.extack = extack;\n\txdp.flags = flags;\n\txdp.prog = prog;\n\n\treturn xdp_op(dev, &xdp);\n}\n\n/**\n *\tdev_change_xdp_fd - set or clear a bpf program for a device rx path\n *\t@dev: device\n *\t@extack: netlink extended ack\n *\t@fd: new program fd or negative value to clear\n *\t@flags: xdp-related flags\n *\n *\tSet or clear a bpf program for a device\n */\nint dev_change_xdp_fd(struct net_device *dev, struct netlink_ext_ack *extack,\n\t\t      int fd, u32 flags)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tstruct bpf_prog *prog = NULL;\n\txdp_op_t xdp_op, xdp_chk;\n\tint err;\n\n\tASSERT_RTNL();\n\n\txdp_op = xdp_chk = ops->ndo_xdp;\n\tif (!xdp_op && (flags & (XDP_FLAGS_DRV_MODE | XDP_FLAGS_HW_MODE)))\n\t\treturn -EOPNOTSUPP;\n\tif (!xdp_op || (flags & XDP_FLAGS_SKB_MODE))\n\t\txdp_op = generic_xdp_install;\n\tif (xdp_op == xdp_chk)\n\t\txdp_chk = generic_xdp_install;\n\n\tif (fd >= 0) {\n\t\tif (xdp_chk && __dev_xdp_attached(dev, xdp_chk, NULL))\n\t\t\treturn -EEXIST;\n\t\tif ((flags & XDP_FLAGS_UPDATE_IF_NOEXIST) &&\n\t\t    __dev_xdp_attached(dev, xdp_op, NULL))\n\t\t\treturn -EBUSY;\n\n\t\tprog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);\n\t\tif (IS_ERR(prog))\n\t\t\treturn PTR_ERR(prog);\n\t}\n\n\terr = dev_xdp_install(dev, xdp_op, extack, flags, prog);\n\tif (err < 0 && prog)\n\t\tbpf_prog_put(prog);\n\n\treturn err;\n}\n\n/**\n *\tdev_new_index\t-\tallocate an ifindex\n *\t@net: the applicable net namespace\n *\n *\tReturns a suitable unique value for a new device interface\n *\tnumber.  The caller must hold the rtnl semaphore or the\n *\tdev_base_lock to be sure it remains unique.\n */\nstatic int dev_new_index(struct net *net)\n{\n\tint ifindex = net->ifindex;\n\n\tfor (;;) {\n\t\tif (++ifindex <= 0)\n\t\t\tifindex = 1;\n\t\tif (!__dev_get_by_index(net, ifindex))\n\t\t\treturn net->ifindex = ifindex;\n\t}\n}\n\n/* Delayed registration/unregisteration */\nstatic LIST_HEAD(net_todo_list);\nDECLARE_WAIT_QUEUE_HEAD(netdev_unregistering_wq);\n\nstatic void net_set_todo(struct net_device *dev)\n{\n\tlist_add_tail(&dev->todo_list, &net_todo_list);\n\tdev_net(dev)->dev_unreg_count++;\n}\n\nstatic void rollback_registered_many(struct list_head *head)\n{\n\tstruct net_device *dev, *tmp;\n\tLIST_HEAD(close_head);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tlist_for_each_entry_safe(dev, tmp, head, unreg_list) {\n\t\t/* Some devices call without registering\n\t\t * for initialization unwind. Remove those\n\t\t * devices and proceed with the remaining.\n\t\t */\n\t\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\t\tpr_debug(\"unregister_netdevice: device %s/%p never was registered\\n\",\n\t\t\t\t dev->name, dev);\n\n\t\t\tWARN_ON(1);\n\t\t\tlist_del(&dev->unreg_list);\n\t\t\tcontinue;\n\t\t}\n\t\tdev->dismantle = true;\n\t\tBUG_ON(dev->reg_state != NETREG_REGISTERED);\n\t}\n\n\t/* If device is running, close it first. */\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tlist_add_tail(&dev->close_list, &close_head);\n\tdev_close_many(&close_head, true);\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\t/* And unlink it from device chain. */\n\t\tunlist_netdevice(dev);\n\n\t\tdev->reg_state = NETREG_UNREGISTERING;\n\t}\n\tflush_all_backlogs();\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list) {\n\t\tstruct sk_buff *skb = NULL;\n\n\t\t/* Shutdown queueing discipline. */\n\t\tdev_shutdown(dev);\n\n\n\t\t/* Notify protocols, that we are about to destroy\n\t\t * this device. They should clean all the things.\n\t\t */\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\tif (!dev->rtnl_link_ops ||\n\t\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\t\tskb = rtmsg_ifinfo_build_skb(RTM_DELLINK, dev, ~0U, 0,\n\t\t\t\t\t\t     GFP_KERNEL);\n\n\t\t/*\n\t\t *\tFlush the unicast and multicast chains\n\t\t */\n\t\tdev_uc_flush(dev);\n\t\tdev_mc_flush(dev);\n\n\t\tif (dev->netdev_ops->ndo_uninit)\n\t\t\tdev->netdev_ops->ndo_uninit(dev);\n\n\t\tif (skb)\n\t\t\trtmsg_ifinfo_send(skb, dev, GFP_KERNEL);\n\n\t\t/* Notifier chain MUST detach us all upper devices. */\n\t\tWARN_ON(netdev_has_any_upper_dev(dev));\n\t\tWARN_ON(netdev_has_any_lower_dev(dev));\n\n\t\t/* Remove entries from kobject tree */\n\t\tnetdev_unregister_kobject(dev);\n#ifdef CONFIG_XPS\n\t\t/* Remove XPS queueing entries */\n\t\tnetif_reset_xps_queues_gt(dev, 0);\n#endif\n\t}\n\n\tsynchronize_net();\n\n\tlist_for_each_entry(dev, head, unreg_list)\n\t\tdev_put(dev);\n}\n\nstatic void rollback_registered(struct net_device *dev)\n{\n\tLIST_HEAD(single);\n\n\tlist_add(&dev->unreg_list, &single);\n\trollback_registered_many(&single);\n\tlist_del(&single);\n}\n\nstatic netdev_features_t netdev_sync_upper_features(struct net_device *lower,\n\tstruct net_device *upper, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(upper->wanted_features & feature)\n\t\t    && (features & feature)) {\n\t\t\tnetdev_dbg(lower, \"Dropping feature %pNF, upper dev %s has it off.\\n\",\n\t\t\t\t   &feature, upper->name);\n\t\t\tfeatures &= ~feature;\n\t\t}\n\t}\n\n\treturn features;\n}\n\nstatic void netdev_sync_lower_features(struct net_device *upper,\n\tstruct net_device *lower, netdev_features_t features)\n{\n\tnetdev_features_t upper_disables = NETIF_F_UPPER_DISABLES;\n\tnetdev_features_t feature;\n\tint feature_bit;\n\n\tfor_each_netdev_feature(&upper_disables, feature_bit) {\n\t\tfeature = __NETIF_F_BIT(feature_bit);\n\t\tif (!(features & feature) && (lower->features & feature)) {\n\t\t\tnetdev_dbg(upper, \"Disabling feature %pNF on lower dev %s.\\n\",\n\t\t\t\t   &feature, lower->name);\n\t\t\tlower->wanted_features &= ~feature;\n\t\t\tnetdev_update_features(lower);\n\n\t\t\tif (unlikely(lower->features & feature))\n\t\t\t\tnetdev_WARN(upper, \"failed to disable %pNF on %s!\\n\",\n\t\t\t\t\t    &feature, lower->name);\n\t\t}\n\t}\n}\n\nstatic netdev_features_t netdev_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\t/* Fix illegal checksum combinations */\n\tif ((features & NETIF_F_HW_CSUM) &&\n\t    (features & (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM))) {\n\t\tnetdev_warn(dev, \"mixed HW and IP checksum settings.\\n\");\n\t\tfeatures &= ~(NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\t/* TSO requires that SG is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_ALL_TSO;\n\t}\n\n\tif ((features & NETIF_F_TSO) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t!(features & NETIF_F_IP_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO;\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\t}\n\n\tif ((features & NETIF_F_TSO6) && !(features & NETIF_F_HW_CSUM) &&\n\t\t\t\t\t !(features & NETIF_F_IPV6_CSUM)) {\n\t\tnetdev_dbg(dev, \"Dropping TSO6 features since no CSUM feature.\\n\");\n\t\tfeatures &= ~NETIF_F_TSO6;\n\t}\n\n\t/* TSO with IPv4 ID mangling requires IPv4 TSO be enabled */\n\tif ((features & NETIF_F_TSO_MANGLEID) && !(features & NETIF_F_TSO))\n\t\tfeatures &= ~NETIF_F_TSO_MANGLEID;\n\n\t/* TSO ECN requires that TSO is present as well. */\n\tif ((features & NETIF_F_ALL_TSO) == NETIF_F_TSO_ECN)\n\t\tfeatures &= ~NETIF_F_TSO_ECN;\n\n\t/* Software GSO depends on SG. */\n\tif ((features & NETIF_F_GSO) && !(features & NETIF_F_SG)) {\n\t\tnetdev_dbg(dev, \"Dropping NETIF_F_GSO since no SG feature.\\n\");\n\t\tfeatures &= ~NETIF_F_GSO;\n\t}\n\n\t/* GSO partial features require GSO partial be set */\n\tif ((features & dev->gso_partial_features) &&\n\t    !(features & NETIF_F_GSO_PARTIAL)) {\n\t\tnetdev_dbg(dev,\n\t\t\t   \"Dropping partially supported GSO features since no GSO partial.\\n\");\n\t\tfeatures &= ~dev->gso_partial_features;\n\t}\n\n\treturn features;\n}\n\nint __netdev_update_features(struct net_device *dev)\n{\n\tstruct net_device *upper, *lower;\n\tnetdev_features_t features;\n\tstruct list_head *iter;\n\tint err = -1;\n\n\tASSERT_RTNL();\n\n\tfeatures = netdev_get_wanted_features(dev);\n\n\tif (dev->netdev_ops->ndo_fix_features)\n\t\tfeatures = dev->netdev_ops->ndo_fix_features(dev, features);\n\n\t/* driver might be less strict about feature dependencies */\n\tfeatures = netdev_fix_features(dev, features);\n\n\t/* some features can't be enabled if they're off an an upper device */\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter)\n\t\tfeatures = netdev_sync_upper_features(dev, upper, features);\n\n\tif (dev->features == features)\n\t\tgoto sync_lower;\n\n\tnetdev_dbg(dev, \"Features changed: %pNF -> %pNF\\n\",\n\t\t&dev->features, &features);\n\n\tif (dev->netdev_ops->ndo_set_features)\n\t\terr = dev->netdev_ops->ndo_set_features(dev, features);\n\telse\n\t\terr = 0;\n\n\tif (unlikely(err < 0)) {\n\t\tnetdev_err(dev,\n\t\t\t\"set_features() failed (%d); wanted %pNF, left %pNF\\n\",\n\t\t\terr, &features, &dev->features);\n\t\t/* return non-0 since some features might have changed and\n\t\t * it's better to fire a spurious notification than miss it\n\t\t */\n\t\treturn -1;\n\t}\n\nsync_lower:\n\t/* some features must be disabled on lower devices when disabled\n\t * on an upper device (think: bonding master or bridge)\n\t */\n\tnetdev_for_each_lower_dev(dev, lower, iter)\n\t\tnetdev_sync_lower_features(dev, lower, features);\n\n\tif (!err) {\n\t\tnetdev_features_t diff = features ^ dev->features;\n\n\t\tif (diff & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t/* udp_tunnel_{get,drop}_rx_info both need\n\t\t\t * NETIF_F_RX_UDP_TUNNEL_PORT enabled on the\n\t\t\t * device, or they won't do anything.\n\t\t\t * Thus we need to update dev->features\n\t\t\t * *before* calling udp_tunnel_get_rx_info,\n\t\t\t * but *after* calling udp_tunnel_drop_rx_info.\n\t\t\t */\n\t\t\tif (features & NETIF_F_RX_UDP_TUNNEL_PORT) {\n\t\t\t\tdev->features = features;\n\t\t\t\tudp_tunnel_get_rx_info(dev);\n\t\t\t} else {\n\t\t\t\tudp_tunnel_drop_rx_info(dev);\n\t\t\t}\n\t\t}\n\n\t\tdev->features = features;\n\t}\n\n\treturn err < 0 ? 0 : 1;\n}\n\n/**\n *\tnetdev_update_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications if it\n *\thas changed. Should be called after driver or hardware dependent\n *\tconditions might have changed that influence the features.\n */\nvoid netdev_update_features(struct net_device *dev)\n{\n\tif (__netdev_update_features(dev))\n\t\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_update_features);\n\n/**\n *\tnetdev_change_features - recalculate device features\n *\t@dev: the device to check\n *\n *\tRecalculate dev->features set and send notifications even\n *\tif they have not changed. Should be called instead of\n *\tnetdev_update_features() if also dev->vlan_features might\n *\thave changed to allow the changes to be propagated to stacked\n *\tVLAN devices.\n */\nvoid netdev_change_features(struct net_device *dev)\n{\n\t__netdev_update_features(dev);\n\tnetdev_features_change(dev);\n}\nEXPORT_SYMBOL(netdev_change_features);\n\n/**\n *\tnetif_stacked_transfer_operstate -\ttransfer operstate\n *\t@rootdev: the root or lower level device to transfer state from\n *\t@dev: the device to transfer operstate to\n *\n *\tTransfer operational state from root to device. This is normally\n *\tcalled when a stacking relationship exists between the root\n *\tdevice and the device(a leaf device).\n */\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tif (rootdev->operstate == IF_OPER_DORMANT)\n\t\tnetif_dormant_on(dev);\n\telse\n\t\tnetif_dormant_off(dev);\n\n\tif (netif_carrier_ok(rootdev))\n\t\tnetif_carrier_on(dev);\n\telse\n\t\tnetif_carrier_off(dev);\n}\nEXPORT_SYMBOL(netif_stacked_transfer_operstate);\n\n#ifdef CONFIG_SYSFS\nstatic int netif_alloc_rx_queues(struct net_device *dev)\n{\n\tunsigned int i, count = dev->num_rx_queues;\n\tstruct netdev_rx_queue *rx;\n\tsize_t sz = count * sizeof(*rx);\n\n\tBUG_ON(count < 1);\n\n\trx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!rx)\n\t\treturn -ENOMEM;\n\n\tdev->_rx = rx;\n\n\tfor (i = 0; i < count; i++)\n\t\trx[i].dev = dev;\n\treturn 0;\n}\n#endif\n\nstatic void netdev_init_one_queue(struct net_device *dev,\n\t\t\t\t  struct netdev_queue *queue, void *_unused)\n{\n\t/* Initialize queue lock */\n\tspin_lock_init(&queue->_xmit_lock);\n\tnetdev_set_xmit_lockdep_class(&queue->_xmit_lock, dev->type);\n\tqueue->xmit_lock_owner = -1;\n\tnetdev_queue_numa_node_write(queue, NUMA_NO_NODE);\n\tqueue->dev = dev;\n#ifdef CONFIG_BQL\n\tdql_init(&queue->dql, HZ);\n#endif\n}\n\nstatic void netif_free_tx_queues(struct net_device *dev)\n{\n\tkvfree(dev->_tx);\n}\n\nstatic int netif_alloc_netdev_queues(struct net_device *dev)\n{\n\tunsigned int count = dev->num_tx_queues;\n\tstruct netdev_queue *tx;\n\tsize_t sz = count * sizeof(*tx);\n\n\tif (count < 1 || count > 0xffff)\n\t\treturn -EINVAL;\n\n\ttx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!tx)\n\t\treturn -ENOMEM;\n\n\tdev->_tx = tx;\n\n\tnetdev_for_each_tx_queue(dev, netdev_init_one_queue, NULL);\n\tspin_lock_init(&dev->tx_global_lock);\n\n\treturn 0;\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tnetif_tx_stop_queue(txq);\n\t}\n}\nEXPORT_SYMBOL(netif_tx_stop_all_queues);\n\n/**\n *\tregister_netdevice\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tCallers must hold the rtnl semaphore. You may want\n *\tregister_netdev() instead of this.\n *\n *\tBUGS:\n *\tThe locking appears insufficient to guarantee two parallel registers\n *\twill not get the same name.\n */\n\nint register_netdevice(struct net_device *dev)\n{\n\tint ret;\n\tstruct net *net = dev_net(dev);\n\n\tBUG_ON(dev_boot_phase);\n\tASSERT_RTNL();\n\n\tmight_sleep();\n\n\t/* When net_device's are persistent, this will be fatal. */\n\tBUG_ON(dev->reg_state != NETREG_UNINITIALIZED);\n\tBUG_ON(!net);\n\n\tspin_lock_init(&dev->addr_list_lock);\n\tnetdev_set_addr_lockdep_class(dev);\n\n\tret = dev_get_valid_name(net, dev, dev->name);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t/* Init, if this function is available */\n\tif (dev->netdev_ops->ndo_init) {\n\t\tret = dev->netdev_ops->ndo_init(dev);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (((dev->hw_features | dev->features) &\n\t     NETIF_F_HW_VLAN_CTAG_FILTER) &&\n\t    (!dev->netdev_ops->ndo_vlan_rx_add_vid ||\n\t     !dev->netdev_ops->ndo_vlan_rx_kill_vid)) {\n\t\tnetdev_WARN(dev, \"Buggy VLAN acceleration in driver!\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_uninit;\n\t}\n\n\tret = -EBUSY;\n\tif (!dev->ifindex)\n\t\tdev->ifindex = dev_new_index(net);\n\telse if (__dev_get_by_index(net, dev->ifindex))\n\t\tgoto err_uninit;\n\n\t/* Transfer changeable features to wanted_features and enable\n\t * software offloads (GSO and GRO).\n\t */\n\tdev->hw_features |= NETIF_F_SOFT_FEATURES;\n\tdev->features |= NETIF_F_SOFT_FEATURES;\n\n\tif (dev->netdev_ops->ndo_udp_tunnel_add) {\n\t\tdev->features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t\tdev->hw_features |= NETIF_F_RX_UDP_TUNNEL_PORT;\n\t}\n\n\tdev->wanted_features = dev->features & dev->hw_features;\n\n\tif (!(dev->flags & IFF_LOOPBACK))\n\t\tdev->hw_features |= NETIF_F_NOCACHE_COPY;\n\n\t/* If IPv4 TCP segmentation offload is supported we should also\n\t * allow the device to enable segmenting the frame with the option\n\t * of ignoring a static IP ID value.  This doesn't enable the\n\t * feature itself but allows the user to enable it later.\n\t */\n\tif (dev->hw_features & NETIF_F_TSO)\n\t\tdev->hw_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->vlan_features & NETIF_F_TSO)\n\t\tdev->vlan_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->mpls_features & NETIF_F_TSO)\n\t\tdev->mpls_features |= NETIF_F_TSO_MANGLEID;\n\tif (dev->hw_enc_features & NETIF_F_TSO)\n\t\tdev->hw_enc_features |= NETIF_F_TSO_MANGLEID;\n\n\t/* Make NETIF_F_HIGHDMA inheritable to VLAN devices.\n\t */\n\tdev->vlan_features |= NETIF_F_HIGHDMA;\n\n\t/* Make NETIF_F_SG inheritable to tunnel devices.\n\t */\n\tdev->hw_enc_features |= NETIF_F_SG | NETIF_F_GSO_PARTIAL;\n\n\t/* Make NETIF_F_SG inheritable to MPLS.\n\t */\n\tdev->mpls_features |= NETIF_F_SG;\n\n\tret = call_netdevice_notifiers(NETDEV_POST_INIT, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret)\n\t\tgoto err_uninit;\n\n\tret = netdev_register_kobject(dev);\n\tif (ret)\n\t\tgoto err_uninit;\n\tdev->reg_state = NETREG_REGISTERED;\n\n\t__netdev_update_features(dev);\n\n\t/*\n\t *\tDefault initial state at registry is that the\n\t *\tdevice is present.\n\t */\n\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\n\tlinkwatch_init_dev(dev);\n\n\tdev_init_scheduler(dev);\n\tdev_hold(dev);\n\tlist_netdevice(dev);\n\tadd_device_randomness(dev->dev_addr, dev->addr_len);\n\n\t/* If the device has permanent device address, driver should\n\t * set dev_addr and also addr_assign_type should be set to\n\t * NET_ADDR_PERM (default value).\n\t */\n\tif (dev->addr_assign_type == NET_ADDR_PERM)\n\t\tmemcpy(dev->perm_addr, dev->dev_addr, dev->addr_len);\n\n\t/* Notify protocols, that a new device appeared. */\n\tret = call_netdevice_notifiers(NETDEV_REGISTER, dev);\n\tret = notifier_to_errno(ret);\n\tif (ret) {\n\t\trollback_registered(dev);\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\t}\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\tif (!dev->rtnl_link_ops ||\n\t    dev->rtnl_link_state == RTNL_LINK_INITIALIZED)\n\t\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\nout:\n\treturn ret;\n\nerr_uninit:\n\tif (dev->netdev_ops->ndo_uninit)\n\t\tdev->netdev_ops->ndo_uninit(dev);\n\tif (dev->priv_destructor)\n\t\tdev->priv_destructor(dev);\n\tgoto out;\n}\nEXPORT_SYMBOL(register_netdevice);\n\n/**\n *\tinit_dummy_netdev\t- init a dummy network device for NAPI\n *\t@dev: device to init\n *\n *\tThis takes a network device structure and initialize the minimum\n *\tamount of fields so it can be used to schedule NAPI polls without\n *\tregistering a full blown interface. This is to be used by drivers\n *\tthat need to tie several hardware interfaces to a single NAPI\n *\tpoll scheduler due to HW limitations.\n */\nint init_dummy_netdev(struct net_device *dev)\n{\n\t/* Clear everything. Note we don't initialize spinlocks\n\t * are they aren't supposed to be taken by any of the\n\t * NAPI code and this dummy netdev is supposed to be\n\t * only ever used for NAPI polls\n\t */\n\tmemset(dev, 0, sizeof(struct net_device));\n\n\t/* make sure we BUG if trying to hit standard\n\t * register/unregister code path\n\t */\n\tdev->reg_state = NETREG_DUMMY;\n\n\t/* NAPI wants this */\n\tINIT_LIST_HEAD(&dev->napi_list);\n\n\t/* a dummy interface is started by default */\n\tset_bit(__LINK_STATE_PRESENT, &dev->state);\n\tset_bit(__LINK_STATE_START, &dev->state);\n\n\t/* Note : We dont allocate pcpu_refcnt for dummy devices,\n\t * because users of this 'device' dont need to change\n\t * its refcount.\n\t */\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(init_dummy_netdev);\n\n\n/**\n *\tregister_netdev\t- register a network device\n *\t@dev: device to register\n *\n *\tTake a completed network device structure and add it to the kernel\n *\tinterfaces. A %NETDEV_REGISTER message is sent to the netdev notifier\n *\tchain. 0 is returned on success. A negative errno code is returned\n *\ton a failure to set up the device, or if the name is a duplicate.\n *\n *\tThis is a wrapper around register_netdevice that takes the rtnl semaphore\n *\tand expands the device name if you passed a format string to\n *\talloc_netdev.\n */\nint register_netdev(struct net_device *dev)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = register_netdevice(dev);\n\trtnl_unlock();\n\treturn err;\n}\nEXPORT_SYMBOL(register_netdev);\n\nint netdev_refcnt_read(const struct net_device *dev)\n{\n\tint i, refcnt = 0;\n\n\tfor_each_possible_cpu(i)\n\t\trefcnt += *per_cpu_ptr(dev->pcpu_refcnt, i);\n\treturn refcnt;\n}\nEXPORT_SYMBOL(netdev_refcnt_read);\n\n/**\n * netdev_wait_allrefs - wait until all references are gone.\n * @dev: target net_device\n *\n * This is called when unregistering network devices.\n *\n * Any protocol or device that holds a reference should register\n * for netdevice notification, and cleanup and put back the\n * reference if they receive an UNREGISTER event.\n * We can get stuck here if buggy protocols don't correctly\n * call dev_put.\n */\nstatic void netdev_wait_allrefs(struct net_device *dev)\n{\n\tunsigned long rebroadcast_time, warning_time;\n\tint refcnt;\n\n\tlinkwatch_forget_dev(dev);\n\n\trebroadcast_time = warning_time = jiffies;\n\trefcnt = netdev_refcnt_read(dev);\n\n\twhile (refcnt != 0) {\n\t\tif (time_after(jiffies, rebroadcast_time + 1 * HZ)) {\n\t\t\trtnl_lock();\n\n\t\t\t/* Rebroadcast unregister notification */\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\n\t\t\t__rtnl_unlock();\n\t\t\trcu_barrier();\n\t\t\trtnl_lock();\n\n\t\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t\tif (test_bit(__LINK_STATE_LINKWATCH_PENDING,\n\t\t\t\t     &dev->state)) {\n\t\t\t\t/* We must not have linkwatch events\n\t\t\t\t * pending on unregister. If this\n\t\t\t\t * happens, we simply run the queue\n\t\t\t\t * unscheduled, resulting in a noop\n\t\t\t\t * for this device.\n\t\t\t\t */\n\t\t\t\tlinkwatch_run_queue();\n\t\t\t}\n\n\t\t\t__rtnl_unlock();\n\n\t\t\trebroadcast_time = jiffies;\n\t\t}\n\n\t\tmsleep(250);\n\n\t\trefcnt = netdev_refcnt_read(dev);\n\n\t\tif (time_after(jiffies, warning_time + 10 * HZ)) {\n\t\t\tpr_emerg(\"unregister_netdevice: waiting for %s to become free. Usage count = %d\\n\",\n\t\t\t\t dev->name, refcnt);\n\t\t\twarning_time = jiffies;\n\t\t}\n\t}\n}\n\n/* The sequence is:\n *\n *\trtnl_lock();\n *\t...\n *\tregister_netdevice(x1);\n *\tregister_netdevice(x2);\n *\t...\n *\tunregister_netdevice(y1);\n *\tunregister_netdevice(y2);\n *      ...\n *\trtnl_unlock();\n *\tfree_netdev(y1);\n *\tfree_netdev(y2);\n *\n * We are invoked by rtnl_unlock().\n * This allows us to deal with problems:\n * 1) We can delete sysfs objects which invoke hotplug\n *    without deadlocking with linkwatch via keventd.\n * 2) Since we run with the RTNL semaphore not held, we can sleep\n *    safely in order to wait for the netdev refcnt to drop to zero.\n *\n * We must not return until all unregister events added during\n * the interval the lock was held have been completed.\n */\nvoid netdev_run_todo(void)\n{\n\tstruct list_head list;\n\n\t/* Snapshot list, allow later requests */\n\tlist_replace_init(&net_todo_list, &list);\n\n\t__rtnl_unlock();\n\n\n\t/* Wait for rcu callbacks to finish before next phase */\n\tif (!list_empty(&list))\n\t\trcu_barrier();\n\n\twhile (!list_empty(&list)) {\n\t\tstruct net_device *dev\n\t\t\t= list_first_entry(&list, struct net_device, todo_list);\n\t\tlist_del(&dev->todo_list);\n\n\t\trtnl_lock();\n\t\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\t\t__rtnl_unlock();\n\n\t\tif (unlikely(dev->reg_state != NETREG_UNREGISTERING)) {\n\t\t\tpr_err(\"network todo '%s' but state %d\\n\",\n\t\t\t       dev->name, dev->reg_state);\n\t\t\tdump_stack();\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->reg_state = NETREG_UNREGISTERED;\n\n\t\tnetdev_wait_allrefs(dev);\n\n\t\t/* paranoia */\n\t\tBUG_ON(netdev_refcnt_read(dev));\n\t\tBUG_ON(!list_empty(&dev->ptype_all));\n\t\tBUG_ON(!list_empty(&dev->ptype_specific));\n\t\tWARN_ON(rcu_access_pointer(dev->ip_ptr));\n\t\tWARN_ON(rcu_access_pointer(dev->ip6_ptr));\n\t\tWARN_ON(dev->dn_ptr);\n\n\t\tif (dev->priv_destructor)\n\t\t\tdev->priv_destructor(dev);\n\t\tif (dev->needs_free_netdev)\n\t\t\tfree_netdev(dev);\n\n\t\t/* Report a network device has been unregistered */\n\t\trtnl_lock();\n\t\tdev_net(dev)->dev_unreg_count--;\n\t\t__rtnl_unlock();\n\t\twake_up(&netdev_unregistering_wq);\n\n\t\t/* Free network device */\n\t\tkobject_put(&dev->dev.kobj);\n\t}\n}\n\n/* Convert net_device_stats to rtnl_link_stats64. rtnl_link_stats64 has\n * all the same fields in the same order as net_device_stats, with only\n * the type differing, but rtnl_link_stats64 may have additional fields\n * at the end for newer counters.\n */\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats)\n{\n#if BITS_PER_LONG == 64\n\tBUILD_BUG_ON(sizeof(*stats64) < sizeof(*netdev_stats));\n\tmemcpy(stats64, netdev_stats, sizeof(*netdev_stats));\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + sizeof(*netdev_stats), 0,\n\t       sizeof(*stats64) - sizeof(*netdev_stats));\n#else\n\tsize_t i, n = sizeof(*netdev_stats) / sizeof(unsigned long);\n\tconst unsigned long *src = (const unsigned long *)netdev_stats;\n\tu64 *dst = (u64 *)stats64;\n\n\tBUILD_BUG_ON(n > sizeof(*stats64) / sizeof(u64));\n\tfor (i = 0; i < n; i++)\n\t\tdst[i] = src[i];\n\t/* zero out counters that only exist in rtnl_link_stats64 */\n\tmemset((char *)stats64 + n * sizeof(u64), 0,\n\t       sizeof(*stats64) - n * sizeof(u64));\n#endif\n}\nEXPORT_SYMBOL(netdev_stats_to_stats64);\n\n/**\n *\tdev_get_stats\t- get network device statistics\n *\t@dev: device to get statistics from\n *\t@storage: place to store stats\n *\n *\tGet network statistics from device. Return @storage.\n *\tThe device driver may provide its own method by setting\n *\tdev->netdev_ops->get_stats64 or dev->netdev_ops->get_stats;\n *\totherwise the internal statistics structure is used.\n */\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_stats64) {\n\t\tmemset(storage, 0, sizeof(*storage));\n\t\tops->ndo_get_stats64(dev, storage);\n\t} else if (ops->ndo_get_stats) {\n\t\tnetdev_stats_to_stats64(storage, ops->ndo_get_stats(dev));\n\t} else {\n\t\tnetdev_stats_to_stats64(storage, &dev->stats);\n\t}\n\tstorage->rx_dropped += (unsigned long)atomic_long_read(&dev->rx_dropped);\n\tstorage->tx_dropped += (unsigned long)atomic_long_read(&dev->tx_dropped);\n\tstorage->rx_nohandler += (unsigned long)atomic_long_read(&dev->rx_nohandler);\n\treturn storage;\n}\nEXPORT_SYMBOL(dev_get_stats);\n\nstruct netdev_queue *dev_ingress_queue_create(struct net_device *dev)\n{\n\tstruct netdev_queue *queue = dev_ingress_queue(dev);\n\n#ifdef CONFIG_NET_CLS_ACT\n\tif (queue)\n\t\treturn queue;\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\tnetdev_init_one_queue(dev, queue, NULL);\n\tRCU_INIT_POINTER(queue->qdisc, &noop_qdisc);\n\tqueue->qdisc_sleeping = &noop_qdisc;\n\trcu_assign_pointer(dev->ingress_queue, queue);\n#endif\n\treturn queue;\n}\n\nstatic const struct ethtool_ops default_ethtool_ops;\n\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops)\n{\n\tif (dev->ethtool_ops == &default_ethtool_ops)\n\t\tdev->ethtool_ops = ops;\n}\nEXPORT_SYMBOL_GPL(netdev_set_default_ethtool_ops);\n\nvoid netdev_freemem(struct net_device *dev)\n{\n\tchar *addr = (char *)dev - dev->padded;\n\n\tkvfree(addr);\n}\n\n/**\n * alloc_netdev_mqs - allocate network device\n * @sizeof_priv: size of private data to allocate space for\n * @name: device name format string\n * @name_assign_type: origin of device name\n * @setup: callback to initialize device\n * @txqs: the number of TX subqueues to allocate\n * @rxqs: the number of RX subqueues to allocate\n *\n * Allocates a struct net_device with private data area for driver use\n * and performs basic initialization.  Also allocates subqueue structs\n * for each queue on the device.\n */\nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\tunsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *),\n\t\tunsigned int txqs, unsigned int rxqs)\n{\n\tstruct net_device *dev;\n\tsize_t alloc_size;\n\tstruct net_device *p;\n\n\tBUG_ON(strlen(name) >= sizeof(dev->name));\n\n\tif (txqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero queues\\n\");\n\t\treturn NULL;\n\t}\n\n#ifdef CONFIG_SYSFS\n\tif (rxqs < 1) {\n\t\tpr_err(\"alloc_netdev: Unable to allocate device with zero RX queues\\n\");\n\t\treturn NULL;\n\t}\n#endif\n\n\talloc_size = sizeof(struct net_device);\n\tif (sizeof_priv) {\n\t\t/* ensure 32-byte alignment of private area */\n\t\talloc_size = ALIGN(alloc_size, NETDEV_ALIGN);\n\t\talloc_size += sizeof_priv;\n\t}\n\t/* ensure 32-byte alignment of whole construct */\n\talloc_size += NETDEV_ALIGN - 1;\n\n\tp = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!p)\n\t\treturn NULL;\n\n\tdev = PTR_ALIGN(p, NETDEV_ALIGN);\n\tdev->padded = (char *)dev - (char *)p;\n\n\tdev->pcpu_refcnt = alloc_percpu(int);\n\tif (!dev->pcpu_refcnt)\n\t\tgoto free_dev;\n\n\tif (dev_addr_init(dev))\n\t\tgoto free_pcpu;\n\n\tdev_mc_init(dev);\n\tdev_uc_init(dev);\n\n\tdev_net_set(dev, &init_net);\n\n\tdev->gso_max_size = GSO_MAX_SIZE;\n\tdev->gso_max_segs = GSO_MAX_SEGS;\n\n\tINIT_LIST_HEAD(&dev->napi_list);\n\tINIT_LIST_HEAD(&dev->unreg_list);\n\tINIT_LIST_HEAD(&dev->close_list);\n\tINIT_LIST_HEAD(&dev->link_watch_list);\n\tINIT_LIST_HEAD(&dev->adj_list.upper);\n\tINIT_LIST_HEAD(&dev->adj_list.lower);\n\tINIT_LIST_HEAD(&dev->ptype_all);\n\tINIT_LIST_HEAD(&dev->ptype_specific);\n#ifdef CONFIG_NET_SCHED\n\thash_init(dev->qdisc_hash);\n#endif\n\tdev->priv_flags = IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM;\n\tsetup(dev);\n\n\tif (!dev->tx_queue_len) {\n\t\tdev->priv_flags |= IFF_NO_QUEUE;\n\t\tdev->tx_queue_len = DEFAULT_TX_QUEUE_LEN;\n\t}\n\n\tdev->num_tx_queues = txqs;\n\tdev->real_num_tx_queues = txqs;\n\tif (netif_alloc_netdev_queues(dev))\n\t\tgoto free_all;\n\n#ifdef CONFIG_SYSFS\n\tdev->num_rx_queues = rxqs;\n\tdev->real_num_rx_queues = rxqs;\n\tif (netif_alloc_rx_queues(dev))\n\t\tgoto free_all;\n#endif\n\n\tstrcpy(dev->name, name);\n\tdev->name_assign_type = name_assign_type;\n\tdev->group = INIT_NETDEV_GROUP;\n\tif (!dev->ethtool_ops)\n\t\tdev->ethtool_ops = &default_ethtool_ops;\n\n\tnf_hook_ingress_init(dev);\n\n\treturn dev;\n\nfree_all:\n\tfree_netdev(dev);\n\treturn NULL;\n\nfree_pcpu:\n\tfree_percpu(dev->pcpu_refcnt);\nfree_dev:\n\tnetdev_freemem(dev);\n\treturn NULL;\n}\nEXPORT_SYMBOL(alloc_netdev_mqs);\n\n/**\n * free_netdev - free network device\n * @dev: device\n *\n * This function does the last stage of destroying an allocated device\n * interface. The reference to the device object is released. If this\n * is the last reference then it will be freed.Must be called in process\n * context.\n */\nvoid free_netdev(struct net_device *dev)\n{\n\tstruct napi_struct *p, *n;\n\tstruct bpf_prog *prog;\n\n\tmight_sleep();\n\tnetif_free_tx_queues(dev);\n#ifdef CONFIG_SYSFS\n\tkvfree(dev->_rx);\n#endif\n\n\tkfree(rcu_dereference_protected(dev->ingress_queue, 1));\n\n\t/* Flush device addresses */\n\tdev_addr_flush(dev);\n\n\tlist_for_each_entry_safe(p, n, &dev->napi_list, dev_list)\n\t\tnetif_napi_del(p);\n\n\tfree_percpu(dev->pcpu_refcnt);\n\tdev->pcpu_refcnt = NULL;\n\n\tprog = rcu_dereference_protected(dev->xdp_prog, 1);\n\tif (prog) {\n\t\tbpf_prog_put(prog);\n\t\tstatic_key_slow_dec(&generic_xdp_needed);\n\t}\n\n\t/*  Compatibility with error handling in drivers */\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\tnetdev_freemem(dev);\n\t\treturn;\n\t}\n\n\tBUG_ON(dev->reg_state != NETREG_UNREGISTERED);\n\tdev->reg_state = NETREG_RELEASED;\n\n\t/* will free via device release */\n\tput_device(&dev->dev);\n}\nEXPORT_SYMBOL(free_netdev);\n\n/**\n *\tsynchronize_net -  Synchronize with packet receive processing\n *\n *\tWait for packets currently being received to be done.\n *\tDoes not block later packets from starting.\n */\nvoid synchronize_net(void)\n{\n\tmight_sleep();\n\tif (rtnl_is_locked())\n\t\tsynchronize_rcu_expedited();\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL(synchronize_net);\n\n/**\n *\tunregister_netdevice_queue - remove device from the kernel\n *\t@dev: device\n *\t@head: list\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\tIf head not NULL, device is queued to be unregistered later.\n *\n *\tCallers must hold the rtnl semaphore.  You may want\n *\tunregister_netdev() instead of this.\n */\n\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head)\n{\n\tASSERT_RTNL();\n\n\tif (head) {\n\t\tlist_move_tail(&dev->unreg_list, head);\n\t} else {\n\t\trollback_registered(dev);\n\t\t/* Finish processing unregister after unlock */\n\t\tnet_set_todo(dev);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_queue);\n\n/**\n *\tunregister_netdevice_many - unregister many devices\n *\t@head: list of devices\n *\n *  Note: As most callers use a stack allocated list_head,\n *  we force a list_del() to make sure stack wont be corrupted later.\n */\nvoid unregister_netdevice_many(struct list_head *head)\n{\n\tstruct net_device *dev;\n\n\tif (!list_empty(head)) {\n\t\trollback_registered_many(head);\n\t\tlist_for_each_entry(dev, head, unreg_list)\n\t\t\tnet_set_todo(dev);\n\t\tlist_del(head);\n\t}\n}\nEXPORT_SYMBOL(unregister_netdevice_many);\n\n/**\n *\tunregister_netdev - remove device from the kernel\n *\t@dev: device\n *\n *\tThis function shuts down a device interface and removes it\n *\tfrom the kernel tables.\n *\n *\tThis is just a wrapper for unregister_netdevice that takes\n *\tthe rtnl semaphore.  In general you want to use this and not\n *\tunregister_netdevice.\n */\nvoid unregister_netdev(struct net_device *dev)\n{\n\trtnl_lock();\n\tunregister_netdevice(dev);\n\trtnl_unlock();\n}\nEXPORT_SYMBOL(unregister_netdev);\n\n/**\n *\tdev_change_net_namespace - move device to different nethost namespace\n *\t@dev: device\n *\t@net: network namespace\n *\t@pat: If not NULL name pattern to try if the current device name\n *\t      is already taken in the destination network namespace.\n *\n *\tThis function shuts down a device interface and moves it\n *\tto a new network namespace. On success 0 is returned, on\n *\ta failure a netagive errno code is returned.\n *\n *\tCallers must hold the rtnl semaphore.\n */\n\nint dev_change_net_namespace(struct net_device *dev, struct net *net, const char *pat)\n{\n\tint err;\n\n\tASSERT_RTNL();\n\n\t/* Don't allow namespace local devices to be moved. */\n\terr = -EINVAL;\n\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\tgoto out;\n\n\t/* Ensure the device has been registrered */\n\tif (dev->reg_state != NETREG_REGISTERED)\n\t\tgoto out;\n\n\t/* Get out if there is nothing todo */\n\terr = 0;\n\tif (net_eq(dev_net(dev), net))\n\t\tgoto out;\n\n\t/* Pick the destination device name, and ensure\n\t * we can use it in the destination network namespace.\n\t */\n\terr = -EEXIST;\n\tif (__dev_get_by_name(net, dev->name)) {\n\t\t/* We get here if we can't use the current device name */\n\t\tif (!pat)\n\t\t\tgoto out;\n\t\tif (dev_get_valid_name(net, dev, pat) < 0)\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * And now a mini version of register_netdevice unregister_netdevice.\n\t */\n\n\t/* If device is running close it first. */\n\tdev_close(dev);\n\n\t/* And unlink it from device chain */\n\terr = -ENODEV;\n\tunlist_netdevice(dev);\n\n\tsynchronize_net();\n\n\t/* Shutdown queueing discipline. */\n\tdev_shutdown(dev);\n\n\t/* Notify protocols, that we are about to destroy\n\t * this device. They should clean all the things.\n\t *\n\t * Note that dev->reg_state stays at NETREG_REGISTERED.\n\t * This is wanted because this way 8021q and macvlan know\n\t * the device is just moving and can keep their slaves up.\n\t */\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER, dev);\n\trcu_barrier();\n\tcall_netdevice_notifiers(NETDEV_UNREGISTER_FINAL, dev);\n\trtmsg_ifinfo(RTM_DELLINK, dev, ~0U, GFP_KERNEL);\n\n\t/*\n\t *\tFlush the unicast and multicast chains\n\t */\n\tdev_uc_flush(dev);\n\tdev_mc_flush(dev);\n\n\t/* Send a netdev-removed uevent to the old namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_REMOVE);\n\tnetdev_adjacent_del_links(dev);\n\n\t/* Actually switch the network namespace */\n\tdev_net_set(dev, net);\n\n\t/* If there is an ifindex conflict assign a new one */\n\tif (__dev_get_by_index(net, dev->ifindex))\n\t\tdev->ifindex = dev_new_index(net);\n\n\t/* Send a netdev-add uevent to the new namespace */\n\tkobject_uevent(&dev->dev.kobj, KOBJ_ADD);\n\tnetdev_adjacent_add_links(dev);\n\n\t/* Fixup kobjects */\n\terr = device_rename(&dev->dev, dev->name);\n\tWARN_ON(err);\n\n\t/* Add the device back in the hashes */\n\tlist_netdevice(dev);\n\n\t/* Notify protocols, that a new device appeared. */\n\tcall_netdevice_notifiers(NETDEV_REGISTER, dev);\n\n\t/*\n\t *\tPrevent userspace races by waiting until the network\n\t *\tdevice is fully setup before sending notifications.\n\t */\n\trtmsg_ifinfo(RTM_NEWLINK, dev, ~0U, GFP_KERNEL);\n\n\tsynchronize_net();\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(dev_change_net_namespace);\n\nstatic int dev_cpu_dead(unsigned int oldcpu)\n{\n\tstruct sk_buff **list_skb;\n\tstruct sk_buff *skb;\n\tunsigned int cpu;\n\tstruct softnet_data *sd, *oldsd, *remsd = NULL;\n\n\tlocal_irq_disable();\n\tcpu = smp_processor_id();\n\tsd = &per_cpu(softnet_data, cpu);\n\toldsd = &per_cpu(softnet_data, oldcpu);\n\n\t/* Find end of our completion_queue. */\n\tlist_skb = &sd->completion_queue;\n\twhile (*list_skb)\n\t\tlist_skb = &(*list_skb)->next;\n\t/* Append completion queue from offline CPU. */\n\t*list_skb = oldsd->completion_queue;\n\toldsd->completion_queue = NULL;\n\n\t/* Append output queue from offline CPU. */\n\tif (oldsd->output_queue) {\n\t\t*sd->output_queue_tailp = oldsd->output_queue;\n\t\tsd->output_queue_tailp = oldsd->output_queue_tailp;\n\t\toldsd->output_queue = NULL;\n\t\toldsd->output_queue_tailp = &oldsd->output_queue;\n\t}\n\t/* Append NAPI poll list from offline CPU, with one exception :\n\t * process_backlog() must be called by cpu owning percpu backlog.\n\t * We properly handle process_queue & input_pkt_queue later.\n\t */\n\twhile (!list_empty(&oldsd->poll_list)) {\n\t\tstruct napi_struct *napi = list_first_entry(&oldsd->poll_list,\n\t\t\t\t\t\t\t    struct napi_struct,\n\t\t\t\t\t\t\t    poll_list);\n\n\t\tlist_del_init(&napi->poll_list);\n\t\tif (napi->poll == process_backlog)\n\t\t\tnapi->state = 0;\n\t\telse\n\t\t\t____napi_schedule(sd, napi);\n\t}\n\n\traise_softirq_irqoff(NET_TX_SOFTIRQ);\n\tlocal_irq_enable();\n\n#ifdef CONFIG_RPS\n\tremsd = oldsd->rps_ipi_list;\n\toldsd->rps_ipi_list = NULL;\n#endif\n\t/* send out pending IPI's on offline CPU */\n\tnet_rps_send_ipi(remsd);\n\n\t/* Process offline CPU's input_pkt_queue */\n\twhile ((skb = __skb_dequeue(&oldsd->process_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\twhile ((skb = skb_dequeue(&oldsd->input_pkt_queue))) {\n\t\tnetif_rx_ni(skb);\n\t\tinput_queue_head_incr(oldsd);\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tnetdev_increment_features - increment feature set by one\n *\t@all: current feature set\n *\t@one: new feature set\n *\t@mask: mask feature set\n *\n *\tComputes a new feature set after adding a device with feature set\n *\t@one to the master device with current feature set @all.  Will not\n *\tenable anything that is off in @mask. Returns the new feature set.\n */\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask)\n{\n\tif (mask & NETIF_F_HW_CSUM)\n\t\tmask |= NETIF_F_CSUM_MASK;\n\tmask |= NETIF_F_VLAN_CHALLENGED;\n\n\tall |= one & (NETIF_F_ONE_FOR_ALL | NETIF_F_CSUM_MASK) & mask;\n\tall &= one | ~NETIF_F_ALL_FOR_ALL;\n\n\t/* If one device supports hw checksumming, set for all. */\n\tif (all & NETIF_F_HW_CSUM)\n\t\tall &= ~(NETIF_F_CSUM_MASK & ~NETIF_F_HW_CSUM);\n\n\treturn all;\n}\nEXPORT_SYMBOL(netdev_increment_features);\n\nstatic struct hlist_head * __net_init netdev_create_hash(void)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = kmalloc(sizeof(*hash) * NETDEV_HASHENTRIES, GFP_KERNEL);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < NETDEV_HASHENTRIES; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\n/* Initialize per network namespace state */\nstatic int __net_init netdev_init(struct net *net)\n{\n\tif (net != &init_net)\n\t\tINIT_LIST_HEAD(&net->dev_base_head);\n\n\tnet->dev_name_head = netdev_create_hash();\n\tif (net->dev_name_head == NULL)\n\t\tgoto err_name;\n\n\tnet->dev_index_head = netdev_create_hash();\n\tif (net->dev_index_head == NULL)\n\t\tgoto err_idx;\n\n\treturn 0;\n\nerr_idx:\n\tkfree(net->dev_name_head);\nerr_name:\n\treturn -ENOMEM;\n}\n\n/**\n *\tnetdev_drivername - network driver for the device\n *\t@dev: network device\n *\n *\tDetermine network driver for device.\n */\nconst char *netdev_drivername(const struct net_device *dev)\n{\n\tconst struct device_driver *driver;\n\tconst struct device *parent;\n\tconst char *empty = \"\";\n\n\tparent = dev->dev.parent;\n\tif (!parent)\n\t\treturn empty;\n\n\tdriver = parent->driver;\n\tif (driver && driver->name)\n\t\treturn driver->name;\n\treturn empty;\n}\n\nstatic void __netdev_printk(const char *level, const struct net_device *dev,\n\t\t\t    struct va_format *vaf)\n{\n\tif (dev && dev->dev.parent) {\n\t\tdev_printk_emit(level[1] - '0',\n\t\t\t\tdev->dev.parent,\n\t\t\t\t\"%s %s %s%s: %pV\",\n\t\t\t\tdev_driver_string(dev->dev.parent),\n\t\t\t\tdev_name(dev->dev.parent),\n\t\t\t\tnetdev_name(dev), netdev_reg_state(dev),\n\t\t\t\tvaf);\n\t} else if (dev) {\n\t\tprintk(\"%s%s%s: %pV\",\n\t\t       level, netdev_name(dev), netdev_reg_state(dev), vaf);\n\t} else {\n\t\tprintk(\"%s(NULL net_device): %pV\", level, vaf);\n\t}\n}\n\nvoid netdev_printk(const char *level, const struct net_device *dev,\n\t\t   const char *format, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, format);\n\n\tvaf.fmt = format;\n\tvaf.va = &args;\n\n\t__netdev_printk(level, dev, &vaf);\n\n\tva_end(args);\n}\nEXPORT_SYMBOL(netdev_printk);\n\n#define define_netdev_printk_level(func, level)\t\t\t\\\nvoid func(const struct net_device *dev, const char *fmt, ...)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstruct va_format vaf;\t\t\t\t\t\\\n\tva_list args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_start(args, fmt);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tvaf.fmt = fmt;\t\t\t\t\t\t\\\n\tvaf.va = &args;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__netdev_printk(level, dev, &vaf);\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tva_end(args);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(func);\n\ndefine_netdev_printk_level(netdev_emerg, KERN_EMERG);\ndefine_netdev_printk_level(netdev_alert, KERN_ALERT);\ndefine_netdev_printk_level(netdev_crit, KERN_CRIT);\ndefine_netdev_printk_level(netdev_err, KERN_ERR);\ndefine_netdev_printk_level(netdev_warn, KERN_WARNING);\ndefine_netdev_printk_level(netdev_notice, KERN_NOTICE);\ndefine_netdev_printk_level(netdev_info, KERN_INFO);\n\nstatic void __net_exit netdev_exit(struct net *net)\n{\n\tkfree(net->dev_name_head);\n\tkfree(net->dev_index_head);\n}\n\nstatic struct pernet_operations __net_initdata netdev_net_ops = {\n\t.init = netdev_init,\n\t.exit = netdev_exit,\n};\n\nstatic void __net_exit default_device_exit(struct net *net)\n{\n\tstruct net_device *dev, *aux;\n\t/*\n\t * Push all migratable network devices back to the\n\t * initial network namespace\n\t */\n\trtnl_lock();\n\tfor_each_netdev_safe(net, dev, aux) {\n\t\tint err;\n\t\tchar fb_name[IFNAMSIZ];\n\n\t\t/* Ignore unmoveable devices (i.e. loopback) */\n\t\tif (dev->features & NETIF_F_NETNS_LOCAL)\n\t\t\tcontinue;\n\n\t\t/* Leave virtual devices for the generic cleanup */\n\t\tif (dev->rtnl_link_ops)\n\t\t\tcontinue;\n\n\t\t/* Push remaining network devices to init_net */\n\t\tsnprintf(fb_name, IFNAMSIZ, \"dev%d\", dev->ifindex);\n\t\terr = dev_change_net_namespace(dev, &init_net, fb_name);\n\t\tif (err) {\n\t\t\tpr_emerg(\"%s: failed to move %s to init_net: %d\\n\",\n\t\t\t\t __func__, dev->name, err);\n\t\t\tBUG();\n\t\t}\n\t}\n\trtnl_unlock();\n}\n\nstatic void __net_exit rtnl_lock_unregistering(struct list_head *net_list)\n{\n\t/* Return with the rtnl_lock held when there are no network\n\t * devices unregistering in any network namespace in net_list.\n\t */\n\tstruct net *net;\n\tbool unregistering;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tadd_wait_queue(&netdev_unregistering_wq, &wait);\n\tfor (;;) {\n\t\tunregistering = false;\n\t\trtnl_lock();\n\t\tlist_for_each_entry(net, net_list, exit_list) {\n\t\t\tif (net->dev_unreg_count > 0) {\n\t\t\t\tunregistering = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!unregistering)\n\t\t\tbreak;\n\t\t__rtnl_unlock();\n\n\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);\n\t}\n\tremove_wait_queue(&netdev_unregistering_wq, &wait);\n}\n\nstatic void __net_exit default_device_exit_batch(struct list_head *net_list)\n{\n\t/* At exit all network devices most be removed from a network\n\t * namespace.  Do this in the reverse order of registration.\n\t * Do this across as many network namespaces as possible to\n\t * improve batching efficiency.\n\t */\n\tstruct net_device *dev;\n\tstruct net *net;\n\tLIST_HEAD(dev_kill_list);\n\n\t/* To prevent network device cleanup code from dereferencing\n\t * loopback devices or network devices that have been freed\n\t * wait here for all pending unregistrations to complete,\n\t * before unregistring the loopback device and allowing the\n\t * network namespace be freed.\n\t *\n\t * The netdev todo list containing all network devices\n\t * unregistrations that happen in default_device_exit_batch\n\t * will run in the rtnl_unlock() at the end of\n\t * default_device_exit_batch.\n\t */\n\trtnl_lock_unregistering(net_list);\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tfor_each_netdev_reverse(net, dev) {\n\t\t\tif (dev->rtnl_link_ops && dev->rtnl_link_ops->dellink)\n\t\t\t\tdev->rtnl_link_ops->dellink(dev, &dev_kill_list);\n\t\t\telse\n\t\t\t\tunregister_netdevice_queue(dev, &dev_kill_list);\n\t\t}\n\t}\n\tunregister_netdevice_many(&dev_kill_list);\n\trtnl_unlock();\n}\n\nstatic struct pernet_operations __net_initdata default_device_ops = {\n\t.exit = default_device_exit,\n\t.exit_batch = default_device_exit_batch,\n};\n\n/*\n *\tInitialize the DEV module. At boot time this walks the device list and\n *\tunhooks any devices that fail to initialise (normally hardware not\n *\tpresent) and leaves us with a valid list of present and active devices.\n *\n */\n\n/*\n *       This is called single threaded during boot, so no need\n *       to take the rtnl semaphore.\n */\nstatic int __init net_dev_init(void)\n{\n\tint i, rc = -ENOMEM;\n\n\tBUG_ON(!dev_boot_phase);\n\n\tif (dev_proc_init())\n\t\tgoto out;\n\n\tif (netdev_kobject_init())\n\t\tgoto out;\n\n\tINIT_LIST_HEAD(&ptype_all);\n\tfor (i = 0; i < PTYPE_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&ptype_base[i]);\n\n\tINIT_LIST_HEAD(&offload_base);\n\n\tif (register_pernet_subsys(&netdev_net_ops))\n\t\tgoto out;\n\n\t/*\n\t *\tInitialise the packet receive queues.\n\t */\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct work_struct *flush = per_cpu_ptr(&flush_works, i);\n\t\tstruct softnet_data *sd = &per_cpu(softnet_data, i);\n\n\t\tINIT_WORK(flush, flush_backlog);\n\n\t\tskb_queue_head_init(&sd->input_pkt_queue);\n\t\tskb_queue_head_init(&sd->process_queue);\n\t\tINIT_LIST_HEAD(&sd->poll_list);\n\t\tsd->output_queue_tailp = &sd->output_queue;\n#ifdef CONFIG_RPS\n\t\tsd->csd.func = rps_trigger_softirq;\n\t\tsd->csd.info = sd;\n\t\tsd->cpu = i;\n#endif\n\n\t\tsd->backlog.poll = process_backlog;\n\t\tsd->backlog.weight = weight_p;\n\t}\n\n\tdev_boot_phase = 0;\n\n\t/* The loopback device is special if any other network devices\n\t * is present in a network namespace the loopback device must\n\t * be present. Since we now dynamically allocate and free the\n\t * loopback device ensure this invariant is maintained by\n\t * keeping the loopback device as the first device on the\n\t * list of network devices.  Ensuring the loopback devices\n\t * is the first device that appears and the last network device\n\t * that disappears.\n\t */\n\tif (register_pernet_device(&loopback_net_ops))\n\t\tgoto out;\n\n\tif (register_pernet_device(&default_device_ops))\n\t\tgoto out;\n\n\topen_softirq(NET_TX_SOFTIRQ, net_tx_action);\n\topen_softirq(NET_RX_SOFTIRQ, net_rx_action);\n\n\trc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, \"net/dev:dead\",\n\t\t\t\t       NULL, dev_cpu_dead);\n\tWARN_ON(rc < 0);\n\trc = 0;\nout:\n\treturn rc;\n}\n\nsubsys_initcall(net_dev_init);\n"], "filenames": ["drivers/net/tun.c", "include/linux/netdevice.h", "net/core/dev.c"], "buggy_code_start_loc": [2029, 3696, 1150], "buggy_code_end_loc": [2029, 3696, 1167], "fixing_code_start_loc": [2030, 3697, 1150], "fixing_code_end_loc": [2033, 3700, 1168], "type": "CWE-476", "message": "In the tun subsystem in the Linux kernel before 4.13.14, dev_get_valid_name is not called before register_netdevice. This allows local users to cause a denial of service (NULL pointer dereference and panic) via an ioctl(TUNSETIFF) call with a dev name containing a / character. This is similar to CVE-2013-4343.", "other": {"cve": {"id": "CVE-2018-7191", "sourceIdentifier": "cve@mitre.org", "published": "2019-05-17T05:29:00.223", "lastModified": "2019-05-31T12:29:01.330", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In the tun subsystem in the Linux kernel before 4.13.14, dev_get_valid_name is not called before register_netdevice. This allows local users to cause a denial of service (NULL pointer dereference and panic) via an ioctl(TUNSETIFF) call with a dev name containing a / character. This is similar to CVE-2013-4343."}, {"lang": "es", "value": "En el subsistema tun en el kernel de Linux anterior a 4.13.14, dev_get_valid_name no es llamada antes de register_netdevice. Esto permite que los usuarios locales causen una denegaci\u00f3n de servicio (NULL pointer dereference and panic) por medio de una llamada ioctl (TUNSETIFF) con un nombre de desarrollo que contiene un car\u00e1cter /. Esto es similar a CVE-2013-4343."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.13.14", "matchCriteriaId": "B0163E34-8B5E-4DE0-91FD-A2950B9B85F7"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-05/msg00071.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00039.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00048.html", "source": "cve@mitre.org"}, {"url": "http://www.securityfocus.com/bid/108380", "source": "cve@mitre.org"}, {"url": "https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1743792", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1748846", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.13.14", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=0ad646c81b2182f7fa67ec0c8c825e0ee165696d", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5c25f65fd1e42685f7ccd80e0621829c105785d9", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/0ad646c81b2182f7fa67ec0c8c825e0ee165696d", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/5c25f65fd1e42685f7ccd80e0621829c105785d9", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0ad646c81b2182f7fa67ec0c8c825e0ee165696d"}}