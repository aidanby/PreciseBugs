{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0+\n/*\n * ipmi_si.c\n *\n * The interface to the IPMI driver for the system interfaces (KCS, SMIC,\n * BT).\n *\n * Author: MontaVista Software, Inc.\n *         Corey Minyard <minyard@mvista.com>\n *         source@mvista.com\n *\n * Copyright 2002 MontaVista Software Inc.\n * Copyright 2006 IBM Corp., Christian Krafft <krafft@de.ibm.com>\n */\n\n/*\n * This file holds the \"policy\" for the interface to the SMI state\n * machine.  It does the configuration, handles timers and interrupts,\n * and drives the real SMI state machine.\n */\n\n#define pr_fmt(fmt) \"ipmi_si: \" fmt\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/sched.h>\n#include <linux/seq_file.h>\n#include <linux/timer.h>\n#include <linux/errno.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/list.h>\n#include <linux/notifier.h>\n#include <linux/mutex.h>\n#include <linux/kthread.h>\n#include <asm/irq.h>\n#include <linux/interrupt.h>\n#include <linux/rcupdate.h>\n#include <linux/ipmi.h>\n#include <linux/ipmi_smi.h>\n#include \"ipmi_si.h\"\n#include <linux/string.h>\n#include <linux/ctype.h>\n\n/* Measure times between events in the driver. */\n#undef DEBUG_TIMING\n\n/* Call every 10 ms. */\n#define SI_TIMEOUT_TIME_USEC\t10000\n#define SI_USEC_PER_JIFFY\t(1000000/HZ)\n#define SI_TIMEOUT_JIFFIES\t(SI_TIMEOUT_TIME_USEC/SI_USEC_PER_JIFFY)\n#define SI_SHORT_TIMEOUT_USEC  250 /* .25ms when the SM request a\n\t\t\t\t      short timeout */\n\nenum si_intf_state {\n\tSI_NORMAL,\n\tSI_GETTING_FLAGS,\n\tSI_GETTING_EVENTS,\n\tSI_CLEARING_FLAGS,\n\tSI_GETTING_MESSAGES,\n\tSI_CHECKING_ENABLES,\n\tSI_SETTING_ENABLES\n\t/* FIXME - add watchdog stuff. */\n};\n\n/* Some BT-specific defines we need here. */\n#define IPMI_BT_INTMASK_REG\t\t2\n#define IPMI_BT_INTMASK_CLEAR_IRQ_BIT\t2\n#define IPMI_BT_INTMASK_ENABLE_IRQ_BIT\t1\n\nstatic const char * const si_to_str[] = { \"invalid\", \"kcs\", \"smic\", \"bt\" };\n\nstatic int initialized;\n\n/*\n * Indexes into stats[] in smi_info below.\n */\nenum si_stat_indexes {\n\t/*\n\t * Number of times the driver requested a timer while an operation\n\t * was in progress.\n\t */\n\tSI_STAT_short_timeouts = 0,\n\n\t/*\n\t * Number of times the driver requested a timer while nothing was in\n\t * progress.\n\t */\n\tSI_STAT_long_timeouts,\n\n\t/* Number of times the interface was idle while being polled. */\n\tSI_STAT_idles,\n\n\t/* Number of interrupts the driver handled. */\n\tSI_STAT_interrupts,\n\n\t/* Number of time the driver got an ATTN from the hardware. */\n\tSI_STAT_attentions,\n\n\t/* Number of times the driver requested flags from the hardware. */\n\tSI_STAT_flag_fetches,\n\n\t/* Number of times the hardware didn't follow the state machine. */\n\tSI_STAT_hosed_count,\n\n\t/* Number of completed messages. */\n\tSI_STAT_complete_transactions,\n\n\t/* Number of IPMI events received from the hardware. */\n\tSI_STAT_events,\n\n\t/* Number of watchdog pretimeouts. */\n\tSI_STAT_watchdog_pretimeouts,\n\n\t/* Number of asynchronous messages received. */\n\tSI_STAT_incoming_messages,\n\n\n\t/* This *must* remain last, add new values above this. */\n\tSI_NUM_STATS\n};\n\nstruct smi_info {\n\tint                    si_num;\n\tstruct ipmi_smi        *intf;\n\tstruct si_sm_data      *si_sm;\n\tconst struct si_sm_handlers *handlers;\n\tspinlock_t             si_lock;\n\tstruct ipmi_smi_msg    *waiting_msg;\n\tstruct ipmi_smi_msg    *curr_msg;\n\tenum si_intf_state     si_state;\n\n\t/*\n\t * Used to handle the various types of I/O that can occur with\n\t * IPMI\n\t */\n\tstruct si_sm_io io;\n\n\t/*\n\t * Per-OEM handler, called from handle_flags().  Returns 1\n\t * when handle_flags() needs to be re-run or 0 indicating it\n\t * set si_state itself.\n\t */\n\tint (*oem_data_avail_handler)(struct smi_info *smi_info);\n\n\t/*\n\t * Flags from the last GET_MSG_FLAGS command, used when an ATTN\n\t * is set to hold the flags until we are done handling everything\n\t * from the flags.\n\t */\n#define RECEIVE_MSG_AVAIL\t0x01\n#define EVENT_MSG_BUFFER_FULL\t0x02\n#define WDT_PRE_TIMEOUT_INT\t0x08\n#define OEM0_DATA_AVAIL     0x20\n#define OEM1_DATA_AVAIL     0x40\n#define OEM2_DATA_AVAIL     0x80\n#define OEM_DATA_AVAIL      (OEM0_DATA_AVAIL | \\\n\t\t\t     OEM1_DATA_AVAIL | \\\n\t\t\t     OEM2_DATA_AVAIL)\n\tunsigned char       msg_flags;\n\n\t/* Does the BMC have an event buffer? */\n\tbool\t\t    has_event_buffer;\n\n\t/*\n\t * If set to true, this will request events the next time the\n\t * state machine is idle.\n\t */\n\tatomic_t            req_events;\n\n\t/*\n\t * If true, run the state machine to completion on every send\n\t * call.  Generally used after a panic to make sure stuff goes\n\t * out.\n\t */\n\tbool                run_to_completion;\n\n\t/* The timer for this si. */\n\tstruct timer_list   si_timer;\n\n\t/* This flag is set, if the timer can be set */\n\tbool\t\t    timer_can_start;\n\n\t/* This flag is set, if the timer is running (timer_pending() isn't enough) */\n\tbool\t\t    timer_running;\n\n\t/* The time (in jiffies) the last timeout occurred at. */\n\tunsigned long       last_timeout_jiffies;\n\n\t/* Are we waiting for the events, pretimeouts, received msgs? */\n\tatomic_t            need_watch;\n\n\t/*\n\t * The driver will disable interrupts when it gets into a\n\t * situation where it cannot handle messages due to lack of\n\t * memory.  Once that situation clears up, it will re-enable\n\t * interrupts.\n\t */\n\tbool interrupt_disabled;\n\n\t/*\n\t * Does the BMC support events?\n\t */\n\tbool supports_event_msg_buff;\n\n\t/*\n\t * Can we disable interrupts the global enables receive irq\n\t * bit?  There are currently two forms of brokenness, some\n\t * systems cannot disable the bit (which is technically within\n\t * the spec but a bad idea) and some systems have the bit\n\t * forced to zero even though interrupts work (which is\n\t * clearly outside the spec).  The next bool tells which form\n\t * of brokenness is present.\n\t */\n\tbool cannot_disable_irq;\n\n\t/*\n\t * Some systems are broken and cannot set the irq enable\n\t * bit, even if they support interrupts.\n\t */\n\tbool irq_enable_broken;\n\n\t/*\n\t * Did we get an attention that we did not handle?\n\t */\n\tbool got_attn;\n\n\t/* From the get device id response... */\n\tstruct ipmi_device_id device_id;\n\n\t/* Default driver model device. */\n\tstruct platform_device *pdev;\n\n\t/* Have we added the device group to the device? */\n\tbool dev_group_added;\n\n\t/* Have we added the platform device? */\n\tbool pdev_registered;\n\n\t/* Counters and things for the proc filesystem. */\n\tatomic_t stats[SI_NUM_STATS];\n\n\tstruct task_struct *thread;\n\n\tstruct list_head link;\n};\n\n#define smi_inc_stat(smi, stat) \\\n\tatomic_inc(&(smi)->stats[SI_STAT_ ## stat])\n#define smi_get_stat(smi, stat) \\\n\t((unsigned int) atomic_read(&(smi)->stats[SI_STAT_ ## stat]))\n\n#define IPMI_MAX_INTFS 4\nstatic int force_kipmid[IPMI_MAX_INTFS];\nstatic int num_force_kipmid;\n\nstatic unsigned int kipmid_max_busy_us[IPMI_MAX_INTFS];\nstatic int num_max_busy_us;\n\nstatic bool unload_when_empty = true;\n\nstatic int try_smi_init(struct smi_info *smi);\nstatic void cleanup_one_si(struct smi_info *smi_info);\nstatic void cleanup_ipmi_si(void);\n\n#ifdef DEBUG_TIMING\nvoid debug_timestamp(char *msg)\n{\n\tstruct timespec64 t;\n\n\tktime_get_ts64(&t);\n\tpr_debug(\"**%s: %lld.%9.9ld\\n\", msg, (long long) t.tv_sec, t.tv_nsec);\n}\n#else\n#define debug_timestamp(x)\n#endif\n\nstatic ATOMIC_NOTIFIER_HEAD(xaction_notifier_list);\nstatic int register_xaction_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_register(&xaction_notifier_list, nb);\n}\n\nstatic void deliver_recv_msg(struct smi_info *smi_info,\n\t\t\t     struct ipmi_smi_msg *msg)\n{\n\t/* Deliver the message to the upper layer. */\n\tipmi_smi_msg_received(smi_info->intf, msg);\n}\n\nstatic void return_hosed_msg(struct smi_info *smi_info, int cCode)\n{\n\tstruct ipmi_smi_msg *msg = smi_info->curr_msg;\n\n\tif (cCode < 0 || cCode > IPMI_ERR_UNSPECIFIED)\n\t\tcCode = IPMI_ERR_UNSPECIFIED;\n\t/* else use it as is */\n\n\t/* Make it a response */\n\tmsg->rsp[0] = msg->data[0] | 4;\n\tmsg->rsp[1] = msg->data[1];\n\tmsg->rsp[2] = cCode;\n\tmsg->rsp_size = 3;\n\n\tsmi_info->curr_msg = NULL;\n\tdeliver_recv_msg(smi_info, msg);\n}\n\nstatic enum si_sm_result start_next_msg(struct smi_info *smi_info)\n{\n\tint              rv;\n\n\tif (!smi_info->waiting_msg) {\n\t\tsmi_info->curr_msg = NULL;\n\t\trv = SI_SM_IDLE;\n\t} else {\n\t\tint err;\n\n\t\tsmi_info->curr_msg = smi_info->waiting_msg;\n\t\tsmi_info->waiting_msg = NULL;\n\t\tdebug_timestamp(\"Start2\");\n\t\terr = atomic_notifier_call_chain(&xaction_notifier_list,\n\t\t\t\t0, smi_info);\n\t\tif (err & NOTIFY_STOP_MASK) {\n\t\t\trv = SI_SM_CALL_WITHOUT_DELAY;\n\t\t\tgoto out;\n\t\t}\n\t\terr = smi_info->handlers->start_transaction(\n\t\t\tsmi_info->si_sm,\n\t\t\tsmi_info->curr_msg->data,\n\t\t\tsmi_info->curr_msg->data_size);\n\t\tif (err)\n\t\t\treturn_hosed_msg(smi_info, err);\n\n\t\trv = SI_SM_CALL_WITHOUT_DELAY;\n\t}\nout:\n\treturn rv;\n}\n\nstatic void smi_mod_timer(struct smi_info *smi_info, unsigned long new_val)\n{\n\tif (!smi_info->timer_can_start)\n\t\treturn;\n\tsmi_info->last_timeout_jiffies = jiffies;\n\tmod_timer(&smi_info->si_timer, new_val);\n\tsmi_info->timer_running = true;\n}\n\n/*\n * Start a new message and (re)start the timer and thread.\n */\nstatic void start_new_msg(struct smi_info *smi_info, unsigned char *msg,\n\t\t\t  unsigned int size)\n{\n\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\tif (smi_info->thread)\n\t\twake_up_process(smi_info->thread);\n\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, size);\n}\n\nstatic void start_check_enables(struct smi_info *smi_info)\n{\n\tunsigned char msg[2];\n\n\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\n\tstart_new_msg(smi_info, msg, 2);\n\tsmi_info->si_state = SI_CHECKING_ENABLES;\n}\n\nstatic void start_clear_flags(struct smi_info *smi_info)\n{\n\tunsigned char msg[3];\n\n\t/* Make sure the watchdog pre-timeout flag is not set at startup. */\n\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tmsg[1] = IPMI_CLEAR_MSG_FLAGS_CMD;\n\tmsg[2] = WDT_PRE_TIMEOUT_INT;\n\n\tstart_new_msg(smi_info, msg, 3);\n\tsmi_info->si_state = SI_CLEARING_FLAGS;\n}\n\nstatic void start_getting_msg_queue(struct smi_info *smi_info)\n{\n\tsmi_info->curr_msg->data[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tsmi_info->curr_msg->data[1] = IPMI_GET_MSG_CMD;\n\tsmi_info->curr_msg->data_size = 2;\n\n\tstart_new_msg(smi_info, smi_info->curr_msg->data,\n\t\t      smi_info->curr_msg->data_size);\n\tsmi_info->si_state = SI_GETTING_MESSAGES;\n}\n\nstatic void start_getting_events(struct smi_info *smi_info)\n{\n\tsmi_info->curr_msg->data[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tsmi_info->curr_msg->data[1] = IPMI_READ_EVENT_MSG_BUFFER_CMD;\n\tsmi_info->curr_msg->data_size = 2;\n\n\tstart_new_msg(smi_info, smi_info->curr_msg->data,\n\t\t      smi_info->curr_msg->data_size);\n\tsmi_info->si_state = SI_GETTING_EVENTS;\n}\n\n/*\n * When we have a situtaion where we run out of memory and cannot\n * allocate messages, we just leave them in the BMC and run the system\n * polled until we can allocate some memory.  Once we have some\n * memory, we will re-enable the interrupt.\n *\n * Note that we cannot just use disable_irq(), since the interrupt may\n * be shared.\n */\nstatic inline bool disable_si_irq(struct smi_info *smi_info)\n{\n\tif ((smi_info->io.irq) && (!smi_info->interrupt_disabled)) {\n\t\tsmi_info->interrupt_disabled = true;\n\t\tstart_check_enables(smi_info);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool enable_si_irq(struct smi_info *smi_info)\n{\n\tif ((smi_info->io.irq) && (smi_info->interrupt_disabled)) {\n\t\tsmi_info->interrupt_disabled = false;\n\t\tstart_check_enables(smi_info);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * Allocate a message.  If unable to allocate, start the interrupt\n * disable process and return NULL.  If able to allocate but\n * interrupts are disabled, free the message and return NULL after\n * starting the interrupt enable process.\n */\nstatic struct ipmi_smi_msg *alloc_msg_handle_irq(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg;\n\n\tmsg = ipmi_alloc_smi_msg();\n\tif (!msg) {\n\t\tif (!disable_si_irq(smi_info))\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t} else if (enable_si_irq(smi_info)) {\n\t\tipmi_free_smi_msg(msg);\n\t\tmsg = NULL;\n\t}\n\treturn msg;\n}\n\nstatic void handle_flags(struct smi_info *smi_info)\n{\nretry:\n\tif (smi_info->msg_flags & WDT_PRE_TIMEOUT_INT) {\n\t\t/* Watchdog pre-timeout */\n\t\tsmi_inc_stat(smi_info, watchdog_pretimeouts);\n\n\t\tstart_clear_flags(smi_info);\n\t\tsmi_info->msg_flags &= ~WDT_PRE_TIMEOUT_INT;\n\t\tipmi_smi_watchdog_pretimeout(smi_info->intf);\n\t} else if (smi_info->msg_flags & RECEIVE_MSG_AVAIL) {\n\t\t/* Messages available. */\n\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\tif (!smi_info->curr_msg)\n\t\t\treturn;\n\n\t\tstart_getting_msg_queue(smi_info);\n\t} else if (smi_info->msg_flags & EVENT_MSG_BUFFER_FULL) {\n\t\t/* Events available. */\n\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\tif (!smi_info->curr_msg)\n\t\t\treturn;\n\n\t\tstart_getting_events(smi_info);\n\t} else if (smi_info->msg_flags & OEM_DATA_AVAIL &&\n\t\t   smi_info->oem_data_avail_handler) {\n\t\tif (smi_info->oem_data_avail_handler(smi_info))\n\t\t\tgoto retry;\n\t} else\n\t\tsmi_info->si_state = SI_NORMAL;\n}\n\n/*\n * Global enables we care about.\n */\n#define GLOBAL_ENABLES_MASK (IPMI_BMC_EVT_MSG_BUFF | IPMI_BMC_RCV_MSG_INTR | \\\n\t\t\t     IPMI_BMC_EVT_MSG_INTR)\n\nstatic u8 current_global_enables(struct smi_info *smi_info, u8 base,\n\t\t\t\t bool *irq_on)\n{\n\tu8 enables = 0;\n\n\tif (smi_info->supports_event_msg_buff)\n\t\tenables |= IPMI_BMC_EVT_MSG_BUFF;\n\n\tif (((smi_info->io.irq && !smi_info->interrupt_disabled) ||\n\t     smi_info->cannot_disable_irq) &&\n\t    !smi_info->irq_enable_broken)\n\t\tenables |= IPMI_BMC_RCV_MSG_INTR;\n\n\tif (smi_info->supports_event_msg_buff &&\n\t    smi_info->io.irq && !smi_info->interrupt_disabled &&\n\t    !smi_info->irq_enable_broken)\n\t\tenables |= IPMI_BMC_EVT_MSG_INTR;\n\n\t*irq_on = enables & (IPMI_BMC_EVT_MSG_INTR | IPMI_BMC_RCV_MSG_INTR);\n\n\treturn enables;\n}\n\nstatic void check_bt_irq(struct smi_info *smi_info, bool irq_on)\n{\n\tu8 irqstate = smi_info->io.inputb(&smi_info->io, IPMI_BT_INTMASK_REG);\n\n\tirqstate &= IPMI_BT_INTMASK_ENABLE_IRQ_BIT;\n\n\tif ((bool)irqstate == irq_on)\n\t\treturn;\n\n\tif (irq_on)\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG,\n\t\t\t\t     IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n\telse\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG, 0);\n}\n\nstatic void handle_transaction_done(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg;\n\n\tdebug_timestamp(\"Done\");\n\tswitch (smi_info->si_state) {\n\tcase SI_NORMAL:\n\t\tif (!smi_info->curr_msg)\n\t\t\tbreak;\n\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tdeliver_recv_msg(smi_info, msg);\n\t\tbreak;\n\n\tcase SI_GETTING_FLAGS:\n\t{\n\t\tunsigned char msg[4];\n\t\tunsigned int  len;\n\n\t\t/* We got the flags from the SMI, now handle them. */\n\t\tlen = smi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0) {\n\t\t\t/* Error fetching flags, just give up for now. */\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t} else if (len < 4) {\n\t\t\t/*\n\t\t\t * Hmm, no flags.  That's technically illegal, but\n\t\t\t * don't use uninitialized data.\n\t\t\t */\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t} else {\n\t\t\tsmi_info->msg_flags = msg[3];\n\t\t\thandle_flags(smi_info);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_CLEARING_FLAGS:\n\t{\n\t\tunsigned char msg[3];\n\n\t\t/* We cleared the flags. */\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 3);\n\t\tif (msg[2] != 0) {\n\t\t\t/* Error clearing flags */\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Error clearing flags: %2.2x\\n\", msg[2]);\n\t\t}\n\t\tsmi_info->si_state = SI_NORMAL;\n\t\tbreak;\n\t}\n\n\tcase SI_GETTING_EVENTS:\n\t{\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tif (msg->rsp[2] != 0) {\n\t\t\t/* Error getting event, probably done. */\n\t\t\tmsg->done(msg);\n\n\t\t\t/* Take off the event flag. */\n\t\t\tsmi_info->msg_flags &= ~EVENT_MSG_BUFFER_FULL;\n\t\t\thandle_flags(smi_info);\n\t\t} else {\n\t\t\tsmi_inc_stat(smi_info, events);\n\n\t\t\t/*\n\t\t\t * Do this before we deliver the message\n\t\t\t * because delivering the message releases the\n\t\t\t * lock and something else can mess with the\n\t\t\t * state.\n\t\t\t */\n\t\t\thandle_flags(smi_info);\n\n\t\t\tdeliver_recv_msg(smi_info, msg);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_GETTING_MESSAGES:\n\t{\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tif (msg->rsp[2] != 0) {\n\t\t\t/* Error getting event, probably done. */\n\t\t\tmsg->done(msg);\n\n\t\t\t/* Take off the msg flag. */\n\t\t\tsmi_info->msg_flags &= ~RECEIVE_MSG_AVAIL;\n\t\t\thandle_flags(smi_info);\n\t\t} else {\n\t\t\tsmi_inc_stat(smi_info, incoming_messages);\n\n\t\t\t/*\n\t\t\t * Do this before we deliver the message\n\t\t\t * because delivering the message releases the\n\t\t\t * lock and something else can mess with the\n\t\t\t * state.\n\t\t\t */\n\t\t\thandle_flags(smi_info);\n\n\t\t\tdeliver_recv_msg(smi_info, msg);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_CHECKING_ENABLES:\n\t{\n\t\tunsigned char msg[4];\n\t\tu8 enables;\n\t\tbool irq_on;\n\n\t\t/* We got the flags from the SMI, now handle them. */\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0) {\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Couldn't get irq info: %x.\\n\", msg[2]);\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Maybe ok, but ipmi might run very slowly.\\n\");\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\tbreak;\n\t\t}\n\t\tenables = current_global_enables(smi_info, 0, &irq_on);\n\t\tif (smi_info->io.si_type == SI_BT)\n\t\t\t/* BT has its own interrupt enable bit. */\n\t\t\tcheck_bt_irq(smi_info, irq_on);\n\t\tif (enables != (msg[3] & GLOBAL_ENABLES_MASK)) {\n\t\t\t/* Enables are not correct, fix them. */\n\t\t\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\t\t\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\t\t\tmsg[2] = enables | (msg[3] & ~GLOBAL_ENABLES_MASK);\n\t\t\tsmi_info->handlers->start_transaction(\n\t\t\t\tsmi_info->si_sm, msg, 3);\n\t\t\tsmi_info->si_state = SI_SETTING_ENABLES;\n\t\t} else if (smi_info->supports_event_msg_buff) {\n\t\t\tsmi_info->curr_msg = ipmi_alloc_smi_msg();\n\t\t\tif (!smi_info->curr_msg) {\n\t\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart_getting_events(smi_info);\n\t\t} else {\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_SETTING_ENABLES:\n\t{\n\t\tunsigned char msg[4];\n\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0)\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Could not set the global enables: 0x%x.\\n\",\n\t\t\t\t msg[2]);\n\n\t\tif (smi_info->supports_event_msg_buff) {\n\t\t\tsmi_info->curr_msg = ipmi_alloc_smi_msg();\n\t\t\tif (!smi_info->curr_msg) {\n\t\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart_getting_events(smi_info);\n\t\t} else {\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t}\n\t\tbreak;\n\t}\n\t}\n}\n\n/*\n * Called on timeouts and events.  Timeouts should pass the elapsed\n * time, interrupts should pass in zero.  Must be called with\n * si_lock held and interrupts disabled.\n */\nstatic enum si_sm_result smi_event_handler(struct smi_info *smi_info,\n\t\t\t\t\t   int time)\n{\n\tenum si_sm_result si_sm_result;\n\nrestart:\n\t/*\n\t * There used to be a loop here that waited a little while\n\t * (around 25us) before giving up.  That turned out to be\n\t * pointless, the minimum delays I was seeing were in the 300us\n\t * range, which is far too long to wait in an interrupt.  So\n\t * we just run until the state machine tells us something\n\t * happened or it needs a delay.\n\t */\n\tsi_sm_result = smi_info->handlers->event(smi_info->si_sm, time);\n\ttime = 0;\n\twhile (si_sm_result == SI_SM_CALL_WITHOUT_DELAY)\n\t\tsi_sm_result = smi_info->handlers->event(smi_info->si_sm, 0);\n\n\tif (si_sm_result == SI_SM_TRANSACTION_COMPLETE) {\n\t\tsmi_inc_stat(smi_info, complete_transactions);\n\n\t\thandle_transaction_done(smi_info);\n\t\tgoto restart;\n\t} else if (si_sm_result == SI_SM_HOSED) {\n\t\tsmi_inc_stat(smi_info, hosed_count);\n\n\t\t/*\n\t\t * Do the before return_hosed_msg, because that\n\t\t * releases the lock.\n\t\t */\n\t\tsmi_info->si_state = SI_NORMAL;\n\t\tif (smi_info->curr_msg != NULL) {\n\t\t\t/*\n\t\t\t * If we were handling a user message, format\n\t\t\t * a response to send to the upper layer to\n\t\t\t * tell it about the error.\n\t\t\t */\n\t\t\treturn_hosed_msg(smi_info, IPMI_ERR_UNSPECIFIED);\n\t\t}\n\t\tgoto restart;\n\t}\n\n\t/*\n\t * We prefer handling attn over new messages.  But don't do\n\t * this if there is not yet an upper layer to handle anything.\n\t */\n\tif (si_sm_result == SI_SM_ATTN || smi_info->got_attn) {\n\t\tunsigned char msg[2];\n\n\t\tif (smi_info->si_state != SI_NORMAL) {\n\t\t\t/*\n\t\t\t * We got an ATTN, but we are doing something else.\n\t\t\t * Handle the ATTN later.\n\t\t\t */\n\t\t\tsmi_info->got_attn = true;\n\t\t} else {\n\t\t\tsmi_info->got_attn = false;\n\t\t\tsmi_inc_stat(smi_info, attentions);\n\n\t\t\t/*\n\t\t\t * Got a attn, send down a get message flags to see\n\t\t\t * what's causing it.  It would be better to handle\n\t\t\t * this in the upper layer, but due to the way\n\t\t\t * interrupts work with the SMI, that's not really\n\t\t\t * possible.\n\t\t\t */\n\t\t\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\t\t\tmsg[1] = IPMI_GET_MSG_FLAGS_CMD;\n\n\t\t\tstart_new_msg(smi_info, msg, 2);\n\t\t\tsmi_info->si_state = SI_GETTING_FLAGS;\n\t\t\tgoto restart;\n\t\t}\n\t}\n\n\t/* If we are currently idle, try to start the next message. */\n\tif (si_sm_result == SI_SM_IDLE) {\n\t\tsmi_inc_stat(smi_info, idles);\n\n\t\tsi_sm_result = start_next_msg(smi_info);\n\t\tif (si_sm_result != SI_SM_IDLE)\n\t\t\tgoto restart;\n\t}\n\n\tif ((si_sm_result == SI_SM_IDLE)\n\t    && (atomic_read(&smi_info->req_events))) {\n\t\t/*\n\t\t * We are idle and the upper layer requested that I fetch\n\t\t * events, so do so.\n\t\t */\n\t\tatomic_set(&smi_info->req_events, 0);\n\n\t\t/*\n\t\t * Take this opportunity to check the interrupt and\n\t\t * message enable state for the BMC.  The BMC can be\n\t\t * asynchronously reset, and may thus get interrupts\n\t\t * disable and messages disabled.\n\t\t */\n\t\tif (smi_info->supports_event_msg_buff || smi_info->io.irq) {\n\t\t\tstart_check_enables(smi_info);\n\t\t} else {\n\t\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\t\tif (!smi_info->curr_msg)\n\t\t\t\tgoto out;\n\n\t\t\tstart_getting_events(smi_info);\n\t\t}\n\t\tgoto restart;\n\t}\n\n\tif (si_sm_result == SI_SM_IDLE && smi_info->timer_running) {\n\t\t/* Ok it if fails, the timer will just go off. */\n\t\tif (del_timer(&smi_info->si_timer))\n\t\t\tsmi_info->timer_running = false;\n\t}\n\nout:\n\treturn si_sm_result;\n}\n\nstatic void check_start_timer_thread(struct smi_info *smi_info)\n{\n\tif (smi_info->si_state == SI_NORMAL && smi_info->curr_msg == NULL) {\n\t\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t\tif (smi_info->thread)\n\t\t\twake_up_process(smi_info->thread);\n\n\t\tstart_next_msg(smi_info);\n\t\tsmi_event_handler(smi_info, 0);\n\t}\n}\n\nstatic void flush_messages(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\tenum si_sm_result result;\n\n\t/*\n\t * Currently, this function is called only in run-to-completion\n\t * mode.  This means we are single-threaded, no need for locks.\n\t */\n\tresult = smi_event_handler(smi_info, 0);\n\twhile (result != SI_SM_IDLE) {\n\t\tudelay(SI_SHORT_TIMEOUT_USEC);\n\t\tresult = smi_event_handler(smi_info, SI_SHORT_TIMEOUT_USEC);\n\t}\n}\n\nstatic void sender(void                *send_info,\n\t\t   struct ipmi_smi_msg *msg)\n{\n\tstruct smi_info   *smi_info = send_info;\n\tunsigned long     flags;\n\n\tdebug_timestamp(\"Enqueue\");\n\n\tif (smi_info->run_to_completion) {\n\t\t/*\n\t\t * If we are running to completion, start it.  Upper\n\t\t * layer will call flush_messages to clear it out.\n\t\t */\n\t\tsmi_info->waiting_msg = msg;\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\t/*\n\t * The following two lines don't need to be under the lock for\n\t * the lock's sake, but they do need SMP memory barriers to\n\t * avoid getting things out of order.  We are already claiming\n\t * the lock, anyway, so just do it under the lock to avoid the\n\t * ordering problem.\n\t */\n\tBUG_ON(smi_info->waiting_msg);\n\tsmi_info->waiting_msg = msg;\n\tcheck_start_timer_thread(smi_info);\n\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void set_run_to_completion(void *send_info, bool i_run_to_completion)\n{\n\tstruct smi_info   *smi_info = send_info;\n\n\tsmi_info->run_to_completion = i_run_to_completion;\n\tif (i_run_to_completion)\n\t\tflush_messages(smi_info);\n}\n\n/*\n * Use -1 in the nsec value of the busy waiting timespec to tell that\n * we are spinning in kipmid looking for something and not delaying\n * between checks\n */\nstatic inline void ipmi_si_set_not_busy(struct timespec64 *ts)\n{\n\tts->tv_nsec = -1;\n}\nstatic inline int ipmi_si_is_busy(struct timespec64 *ts)\n{\n\treturn ts->tv_nsec != -1;\n}\n\nstatic inline int ipmi_thread_busy_wait(enum si_sm_result smi_result,\n\t\t\t\t\tconst struct smi_info *smi_info,\n\t\t\t\t\tstruct timespec64 *busy_until)\n{\n\tunsigned int max_busy_us = 0;\n\n\tif (smi_info->si_num < num_max_busy_us)\n\t\tmax_busy_us = kipmid_max_busy_us[smi_info->si_num];\n\tif (max_busy_us == 0 || smi_result != SI_SM_CALL_WITH_DELAY)\n\t\tipmi_si_set_not_busy(busy_until);\n\telse if (!ipmi_si_is_busy(busy_until)) {\n\t\tktime_get_ts64(busy_until);\n\t\ttimespec64_add_ns(busy_until, max_busy_us*NSEC_PER_USEC);\n\t} else {\n\t\tstruct timespec64 now;\n\n\t\tktime_get_ts64(&now);\n\t\tif (unlikely(timespec64_compare(&now, busy_until) > 0)) {\n\t\t\tipmi_si_set_not_busy(busy_until);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 1;\n}\n\n\n/*\n * A busy-waiting loop for speeding up IPMI operation.\n *\n * Lousy hardware makes this hard.  This is only enabled for systems\n * that are not BT and do not have interrupts.  It starts spinning\n * when an operation is complete or until max_busy tells it to stop\n * (if that is enabled).  See the paragraph on kimid_max_busy_us in\n * Documentation/IPMI.txt for details.\n */\nstatic int ipmi_thread(void *data)\n{\n\tstruct smi_info *smi_info = data;\n\tunsigned long flags;\n\tenum si_sm_result smi_result;\n\tstruct timespec64 busy_until;\n\n\tipmi_si_set_not_busy(&busy_until);\n\tset_user_nice(current, MAX_NICE);\n\twhile (!kthread_should_stop()) {\n\t\tint busy_wait;\n\n\t\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\t\tsmi_result = smi_event_handler(smi_info, 0);\n\n\t\t/*\n\t\t * If the driver is doing something, there is a possible\n\t\t * race with the timer.  If the timer handler see idle,\n\t\t * and the thread here sees something else, the timer\n\t\t * handler won't restart the timer even though it is\n\t\t * required.  So start it here if necessary.\n\t\t */\n\t\tif (smi_result != SI_SM_IDLE && !smi_info->timer_running)\n\t\t\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n\t\tbusy_wait = ipmi_thread_busy_wait(smi_result, smi_info,\n\t\t\t\t\t\t  &busy_until);\n\t\tif (smi_result == SI_SM_CALL_WITHOUT_DELAY)\n\t\t\t; /* do nothing */\n\t\telse if (smi_result == SI_SM_CALL_WITH_DELAY && busy_wait)\n\t\t\tschedule();\n\t\telse if (smi_result == SI_SM_IDLE) {\n\t\t\tif (atomic_read(&smi_info->need_watch)) {\n\t\t\t\tschedule_timeout_interruptible(100);\n\t\t\t} else {\n\t\t\t\t/* Wait to be woken up when we are needed. */\n\t\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\t\t\tschedule();\n\t\t\t}\n\t\t} else\n\t\t\tschedule_timeout_interruptible(1);\n\t}\n\treturn 0;\n}\n\n\nstatic void poll(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\tunsigned long flags = 0;\n\tbool run_to_completion = smi_info->run_to_completion;\n\n\t/*\n\t * Make sure there is some delay in the poll loop so we can\n\t * drive time forward and timeout things.\n\t */\n\tudelay(10);\n\tif (!run_to_completion)\n\t\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\tsmi_event_handler(smi_info, 10);\n\tif (!run_to_completion)\n\t\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void request_events(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\n\tif (!smi_info->has_event_buffer)\n\t\treturn;\n\n\tatomic_set(&smi_info->req_events, 1);\n}\n\nstatic void set_need_watch(void *send_info, bool enable)\n{\n\tstruct smi_info *smi_info = send_info;\n\tunsigned long flags;\n\n\tatomic_set(&smi_info->need_watch, enable);\n\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\tcheck_start_timer_thread(smi_info);\n\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void smi_timeout(struct timer_list *t)\n{\n\tstruct smi_info   *smi_info = from_timer(smi_info, t, si_timer);\n\tenum si_sm_result smi_result;\n\tunsigned long     flags;\n\tunsigned long     jiffies_now;\n\tlong              time_diff;\n\tlong\t\t  timeout;\n\n\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\tdebug_timestamp(\"Timer\");\n\n\tjiffies_now = jiffies;\n\ttime_diff = (((long)jiffies_now - (long)smi_info->last_timeout_jiffies)\n\t\t     * SI_USEC_PER_JIFFY);\n\tsmi_result = smi_event_handler(smi_info, time_diff);\n\n\tif ((smi_info->io.irq) && (!smi_info->interrupt_disabled)) {\n\t\t/* Running with interrupts, only do long timeouts. */\n\t\ttimeout = jiffies + SI_TIMEOUT_JIFFIES;\n\t\tsmi_inc_stat(smi_info, long_timeouts);\n\t\tgoto do_mod_timer;\n\t}\n\n\t/*\n\t * If the state machine asks for a short delay, then shorten\n\t * the timer timeout.\n\t */\n\tif (smi_result == SI_SM_CALL_WITH_DELAY) {\n\t\tsmi_inc_stat(smi_info, short_timeouts);\n\t\ttimeout = jiffies + 1;\n\t} else {\n\t\tsmi_inc_stat(smi_info, long_timeouts);\n\t\ttimeout = jiffies + SI_TIMEOUT_JIFFIES;\n\t}\n\ndo_mod_timer:\n\tif (smi_result != SI_SM_IDLE)\n\t\tsmi_mod_timer(smi_info, timeout);\n\telse\n\t\tsmi_info->timer_running = false;\n\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n}\n\nirqreturn_t ipmi_si_irq_handler(int irq, void *data)\n{\n\tstruct smi_info *smi_info = data;\n\tunsigned long   flags;\n\n\tif (smi_info->io.si_type == SI_BT)\n\t\t/* We need to clear the IRQ flag for the BT interface. */\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG,\n\t\t\t\t     IPMI_BT_INTMASK_CLEAR_IRQ_BIT\n\t\t\t\t     | IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n\n\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\n\tsmi_inc_stat(smi_info, interrupts);\n\n\tdebug_timestamp(\"Interrupt\");\n\n\tsmi_event_handler(smi_info, 0);\n\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n\treturn IRQ_HANDLED;\n}\n\nstatic int smi_start_processing(void            *send_info,\n\t\t\t\tstruct ipmi_smi *intf)\n{\n\tstruct smi_info *new_smi = send_info;\n\tint             enable = 0;\n\n\tnew_smi->intf = intf;\n\n\t/* Set up the timer that drives the interface. */\n\ttimer_setup(&new_smi->si_timer, smi_timeout, 0);\n\tnew_smi->timer_can_start = true;\n\tsmi_mod_timer(new_smi, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t/* Try to claim any interrupts. */\n\tif (new_smi->io.irq_setup) {\n\t\tnew_smi->io.irq_handler_data = new_smi;\n\t\tnew_smi->io.irq_setup(&new_smi->io);\n\t}\n\n\t/*\n\t * Check if the user forcefully enabled the daemon.\n\t */\n\tif (new_smi->si_num < num_force_kipmid)\n\t\tenable = force_kipmid[new_smi->si_num];\n\t/*\n\t * The BT interface is efficient enough to not need a thread,\n\t * and there is no need for a thread if we have interrupts.\n\t */\n\telse if ((new_smi->io.si_type != SI_BT) && (!new_smi->io.irq))\n\t\tenable = 1;\n\n\tif (enable) {\n\t\tnew_smi->thread = kthread_run(ipmi_thread, new_smi,\n\t\t\t\t\t      \"kipmi%d\", new_smi->si_num);\n\t\tif (IS_ERR(new_smi->thread)) {\n\t\t\tdev_notice(new_smi->io.dev, \"Could not start\"\n\t\t\t\t   \" kernel thread due to error %ld, only using\"\n\t\t\t\t   \" timers to drive the interface\\n\",\n\t\t\t\t   PTR_ERR(new_smi->thread));\n\t\t\tnew_smi->thread = NULL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int get_smi_info(void *send_info, struct ipmi_smi_info *data)\n{\n\tstruct smi_info *smi = send_info;\n\n\tdata->addr_src = smi->io.addr_source;\n\tdata->dev = smi->io.dev;\n\tdata->addr_info = smi->io.addr_info;\n\tget_device(smi->io.dev);\n\n\treturn 0;\n}\n\nstatic void set_maintenance_mode(void *send_info, bool enable)\n{\n\tstruct smi_info   *smi_info = send_info;\n\n\tif (!enable)\n\t\tatomic_set(&smi_info->req_events, 0);\n}\n\nstatic void shutdown_smi(void *send_info);\nstatic const struct ipmi_smi_handlers handlers = {\n\t.owner                  = THIS_MODULE,\n\t.start_processing       = smi_start_processing,\n\t.shutdown               = shutdown_smi,\n\t.get_smi_info\t\t= get_smi_info,\n\t.sender\t\t\t= sender,\n\t.request_events\t\t= request_events,\n\t.set_need_watch\t\t= set_need_watch,\n\t.set_maintenance_mode   = set_maintenance_mode,\n\t.set_run_to_completion  = set_run_to_completion,\n\t.flush_messages\t\t= flush_messages,\n\t.poll\t\t\t= poll,\n};\n\nstatic LIST_HEAD(smi_infos);\nstatic DEFINE_MUTEX(smi_infos_lock);\nstatic int smi_num; /* Used to sequence the SMIs */\n\nstatic const char * const addr_space_to_str[] = { \"i/o\", \"mem\" };\n\nmodule_param_array(force_kipmid, int, &num_force_kipmid, 0);\nMODULE_PARM_DESC(force_kipmid, \"Force the kipmi daemon to be enabled (1) or\"\n\t\t \" disabled(0).  Normally the IPMI driver auto-detects\"\n\t\t \" this, but the value may be overridden by this parm.\");\nmodule_param(unload_when_empty, bool, 0);\nMODULE_PARM_DESC(unload_when_empty, \"Unload the module if no interfaces are\"\n\t\t \" specified or found, default is 1.  Setting to 0\"\n\t\t \" is useful for hot add of devices using hotmod.\");\nmodule_param_array(kipmid_max_busy_us, uint, &num_max_busy_us, 0644);\nMODULE_PARM_DESC(kipmid_max_busy_us,\n\t\t \"Max time (in microseconds) to busy-wait for IPMI data before\"\n\t\t \" sleeping. 0 (default) means to wait forever. Set to 100-500\"\n\t\t \" if kipmid is using up a lot of CPU time.\");\n\nvoid ipmi_irq_finish_setup(struct si_sm_io *io)\n{\n\tif (io->si_type == SI_BT)\n\t\t/* Enable the interrupt in the BT interface. */\n\t\tio->outputb(io, IPMI_BT_INTMASK_REG,\n\t\t\t    IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n}\n\nvoid ipmi_irq_start_cleanup(struct si_sm_io *io)\n{\n\tif (io->si_type == SI_BT)\n\t\t/* Disable the interrupt in the BT interface. */\n\t\tio->outputb(io, IPMI_BT_INTMASK_REG, 0);\n}\n\nstatic void std_irq_cleanup(struct si_sm_io *io)\n{\n\tipmi_irq_start_cleanup(io);\n\tfree_irq(io->irq, io->irq_handler_data);\n}\n\nint ipmi_std_irq_setup(struct si_sm_io *io)\n{\n\tint rv;\n\n\tif (!io->irq)\n\t\treturn 0;\n\n\trv = request_irq(io->irq,\n\t\t\t ipmi_si_irq_handler,\n\t\t\t IRQF_SHARED,\n\t\t\t DEVICE_NAME,\n\t\t\t io->irq_handler_data);\n\tif (rv) {\n\t\tdev_warn(io->dev, \"%s unable to claim interrupt %d,\"\n\t\t\t \" running polled\\n\",\n\t\t\t DEVICE_NAME, io->irq);\n\t\tio->irq = 0;\n\t} else {\n\t\tio->irq_cleanup = std_irq_cleanup;\n\t\tipmi_irq_finish_setup(io);\n\t\tdev_info(io->dev, \"Using irq %d\\n\", io->irq);\n\t}\n\n\treturn rv;\n}\n\nstatic int wait_for_msg_done(struct smi_info *smi_info)\n{\n\tenum si_sm_result     smi_result;\n\n\tsmi_result = smi_info->handlers->event(smi_info->si_sm, 0);\n\tfor (;;) {\n\t\tif (smi_result == SI_SM_CALL_WITH_DELAY ||\n\t\t    smi_result == SI_SM_CALL_WITH_TICK_DELAY) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tsmi_result = smi_info->handlers->event(\n\t\t\t\tsmi_info->si_sm, jiffies_to_usecs(1));\n\t\t} else if (smi_result == SI_SM_CALL_WITHOUT_DELAY) {\n\t\t\tsmi_result = smi_info->handlers->event(\n\t\t\t\tsmi_info->si_sm, 0);\n\t\t} else\n\t\t\tbreak;\n\t}\n\tif (smi_result == SI_SM_HOSED)\n\t\t/*\n\t\t * We couldn't get the state machine to run, so whatever's at\n\t\t * the port is probably not an IPMI SMI interface.\n\t\t */\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\nstatic int try_get_dev_id(struct smi_info *smi_info)\n{\n\tunsigned char         msg[2];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv = 0;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Do a Get Device ID command, since it comes back with some\n\t * useful info.\n\t */\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_DEVICE_ID_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv)\n\t\tgoto out;\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\t/* Check and record info from the get device id, in case we need it. */\n\trv = ipmi_demangle_device_id(resp[0] >> 2, resp[1],\n\t\t\tresp + 2, resp_len - 2, &smi_info->device_id);\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\nstatic int get_global_enables(struct smi_info *smi_info, u8 *enables)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Error getting response from get global enables command: %d\\n\",\n\t\t\t rv);\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 4 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_GET_BMC_GLOBAL_ENABLES_CMD   ||\n\t\t\tresp[2] != 0) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Invalid return from get global enables command: %ld %x %x %x\\n\",\n\t\t\t resp_len, resp[0], resp[1], resp[2]);\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t} else {\n\t\t*enables = resp[3];\n\t}\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n/*\n * Returns 1 if it gets an error from the command.\n */\nstatic int set_global_enables(struct smi_info *smi_info, u8 enables)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\tmsg[2] = enables;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 3);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Error getting response from set global enables command: %d\\n\",\n\t\t\t rv);\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 3 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_SET_BMC_GLOBAL_ENABLES_CMD) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Invalid return from set global enables command: %ld %x %x\\n\",\n\t\t\t resp_len, resp[0], resp[1]);\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[2] != 0)\n\t\trv = 1;\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n/*\n * Some BMCs do not support clearing the receive irq bit in the global\n * enables (even if they don't support interrupts on the BMC).  Check\n * for this and handle it properly.\n */\nstatic void check_clr_rcv_irq(struct smi_info *smi_info)\n{\n\tu8 enables = 0;\n\tint rv;\n\n\trv = get_global_enables(smi_info, &enables);\n\tif (!rv) {\n\t\tif ((enables & IPMI_BMC_RCV_MSG_INTR) == 0)\n\t\t\t/* Already clear, should work ok. */\n\t\t\treturn;\n\n\t\tenables &= ~IPMI_BMC_RCV_MSG_INTR;\n\t\trv = set_global_enables(smi_info, enables);\n\t}\n\n\tif (rv < 0) {\n\t\tdev_err(smi_info->io.dev,\n\t\t\t\"Cannot check clearing the rcv irq: %d\\n\", rv);\n\t\treturn;\n\t}\n\n\tif (rv) {\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * clearing the bit is not supported.\n\t\t */\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"The BMC does not support clearing the recv irq bit, compensating, but the BMC needs to be fixed.\\n\");\n\t\tsmi_info->cannot_disable_irq = true;\n\t}\n}\n\n/*\n * Some BMCs do not support setting the interrupt bits in the global\n * enables even if they support interrupts.  Clearly bad, but we can\n * compensate.\n */\nstatic void check_set_rcv_irq(struct smi_info *smi_info)\n{\n\tu8 enables = 0;\n\tint rv;\n\n\tif (!smi_info->io.irq)\n\t\treturn;\n\n\trv = get_global_enables(smi_info, &enables);\n\tif (!rv) {\n\t\tenables |= IPMI_BMC_RCV_MSG_INTR;\n\t\trv = set_global_enables(smi_info, enables);\n\t}\n\n\tif (rv < 0) {\n\t\tdev_err(smi_info->io.dev,\n\t\t\t\"Cannot check setting the rcv irq: %d\\n\", rv);\n\t\treturn;\n\t}\n\n\tif (rv) {\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * setting the bit is not supported.\n\t\t */\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"The BMC does not support setting the recv irq bit, compensating, but the BMC needs to be fixed.\\n\");\n\t\tsmi_info->cannot_disable_irq = true;\n\t\tsmi_info->irq_enable_broken = true;\n\t}\n}\n\nstatic int try_enable_event_buffer(struct smi_info *smi_info)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv = 0;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tpr_warn(\"Error getting response from get global enables command, the event buffer is not enabled\\n\");\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 4 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_GET_BMC_GLOBAL_ENABLES_CMD   ||\n\t\t\tresp[2] != 0) {\n\t\tpr_warn(\"Invalid return from get global enables command, cannot enable the event buffer\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[3] & IPMI_BMC_EVT_MSG_BUFF) {\n\t\t/* buffer is already enabled, nothing to do. */\n\t\tsmi_info->supports_event_msg_buff = true;\n\t\tgoto out;\n\t}\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\tmsg[2] = resp[3] | IPMI_BMC_EVT_MSG_BUFF;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 3);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tpr_warn(\"Error getting response from set global, enables command, the event buffer is not enabled\\n\");\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 3 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_SET_BMC_GLOBAL_ENABLES_CMD) {\n\t\tpr_warn(\"Invalid return from get global, enables command, not enable the event buffer\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[2] != 0)\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * that the event buffer is not supported.\n\t\t */\n\t\trv = -ENOENT;\n\telse\n\t\tsmi_info->supports_event_msg_buff = true;\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n#define IPMI_SI_ATTR(name) \\\nstatic ssize_t ipmi_##name##_show(struct device *dev,\t\t\t\\\n\t\t\t\t  struct device_attribute *attr,\t\\\n\t\t\t\t  char *buf)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn snprintf(buf, 10, \"%u\\n\", smi_get_stat(smi_info, name));\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic DEVICE_ATTR(name, S_IRUGO, ipmi_##name##_show, NULL)\n\nstatic ssize_t ipmi_type_show(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\n\treturn snprintf(buf, 10, \"%s\\n\", si_to_str[smi_info->io.si_type]);\n}\nstatic DEVICE_ATTR(type, S_IRUGO, ipmi_type_show, NULL);\n\nstatic ssize_t ipmi_interrupts_enabled_show(struct device *dev,\n\t\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t\t    char *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\tint enabled = smi_info->io.irq && !smi_info->interrupt_disabled;\n\n\treturn snprintf(buf, 10, \"%d\\n\", enabled);\n}\nstatic DEVICE_ATTR(interrupts_enabled, S_IRUGO,\n\t\t   ipmi_interrupts_enabled_show, NULL);\n\nIPMI_SI_ATTR(short_timeouts);\nIPMI_SI_ATTR(long_timeouts);\nIPMI_SI_ATTR(idles);\nIPMI_SI_ATTR(interrupts);\nIPMI_SI_ATTR(attentions);\nIPMI_SI_ATTR(flag_fetches);\nIPMI_SI_ATTR(hosed_count);\nIPMI_SI_ATTR(complete_transactions);\nIPMI_SI_ATTR(events);\nIPMI_SI_ATTR(watchdog_pretimeouts);\nIPMI_SI_ATTR(incoming_messages);\n\nstatic ssize_t ipmi_params_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tchar *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\n\treturn snprintf(buf, 200,\n\t\t\t\"%s,%s,0x%lx,rsp=%d,rsi=%d,rsh=%d,irq=%d,ipmb=%d\\n\",\n\t\t\tsi_to_str[smi_info->io.si_type],\n\t\t\taddr_space_to_str[smi_info->io.addr_type],\n\t\t\tsmi_info->io.addr_data,\n\t\t\tsmi_info->io.regspacing,\n\t\t\tsmi_info->io.regsize,\n\t\t\tsmi_info->io.regshift,\n\t\t\tsmi_info->io.irq,\n\t\t\tsmi_info->io.slave_addr);\n}\nstatic DEVICE_ATTR(params, S_IRUGO, ipmi_params_show, NULL);\n\nstatic struct attribute *ipmi_si_dev_attrs[] = {\n\t&dev_attr_type.attr,\n\t&dev_attr_interrupts_enabled.attr,\n\t&dev_attr_short_timeouts.attr,\n\t&dev_attr_long_timeouts.attr,\n\t&dev_attr_idles.attr,\n\t&dev_attr_interrupts.attr,\n\t&dev_attr_attentions.attr,\n\t&dev_attr_flag_fetches.attr,\n\t&dev_attr_hosed_count.attr,\n\t&dev_attr_complete_transactions.attr,\n\t&dev_attr_events.attr,\n\t&dev_attr_watchdog_pretimeouts.attr,\n\t&dev_attr_incoming_messages.attr,\n\t&dev_attr_params.attr,\n\tNULL\n};\n\nstatic const struct attribute_group ipmi_si_dev_attr_group = {\n\t.attrs\t\t= ipmi_si_dev_attrs,\n};\n\n/*\n * oem_data_avail_to_receive_msg_avail\n * @info - smi_info structure with msg_flags set\n *\n * Converts flags from OEM_DATA_AVAIL to RECEIVE_MSG_AVAIL\n * Returns 1 indicating need to re-run handle_flags().\n */\nstatic int oem_data_avail_to_receive_msg_avail(struct smi_info *smi_info)\n{\n\tsmi_info->msg_flags = ((smi_info->msg_flags & ~OEM_DATA_AVAIL) |\n\t\t\t       RECEIVE_MSG_AVAIL);\n\treturn 1;\n}\n\n/*\n * setup_dell_poweredge_oem_data_handler\n * @info - smi_info.device_id must be populated\n *\n * Systems that match, but have firmware version < 1.40 may assert\n * OEM0_DATA_AVAIL on their own, without being told via Set Flags that\n * it's safe to do so.  Such systems will de-assert OEM1_DATA_AVAIL\n * upon receipt of IPMI_GET_MSG_CMD, so we should treat these flags\n * as RECEIVE_MSG_AVAIL instead.\n *\n * As Dell has no plans to release IPMI 1.5 firmware that *ever*\n * assert the OEM[012] bits, and if it did, the driver would have to\n * change to handle that properly, we don't actually check for the\n * firmware version.\n * Device ID = 0x20                BMC on PowerEdge 8G servers\n * Device Revision = 0x80\n * Firmware Revision1 = 0x01       BMC version 1.40\n * Firmware Revision2 = 0x40       BCD encoded\n * IPMI Version = 0x51             IPMI 1.5\n * Manufacturer ID = A2 02 00      Dell IANA\n *\n * Additionally, PowerEdge systems with IPMI < 1.5 may also assert\n * OEM0_DATA_AVAIL and needs to be treated as RECEIVE_MSG_AVAIL.\n *\n */\n#define DELL_POWEREDGE_8G_BMC_DEVICE_ID  0x20\n#define DELL_POWEREDGE_8G_BMC_DEVICE_REV 0x80\n#define DELL_POWEREDGE_8G_BMC_IPMI_VERSION 0x51\n#define DELL_IANA_MFR_ID 0x0002a2\nstatic void setup_dell_poweredge_oem_data_handler(struct smi_info *smi_info)\n{\n\tstruct ipmi_device_id *id = &smi_info->device_id;\n\tif (id->manufacturer_id == DELL_IANA_MFR_ID) {\n\t\tif (id->device_id       == DELL_POWEREDGE_8G_BMC_DEVICE_ID  &&\n\t\t    id->device_revision == DELL_POWEREDGE_8G_BMC_DEVICE_REV &&\n\t\t    id->ipmi_version   == DELL_POWEREDGE_8G_BMC_IPMI_VERSION) {\n\t\t\tsmi_info->oem_data_avail_handler =\n\t\t\t\toem_data_avail_to_receive_msg_avail;\n\t\t} else if (ipmi_version_major(id) < 1 ||\n\t\t\t   (ipmi_version_major(id) == 1 &&\n\t\t\t    ipmi_version_minor(id) < 5)) {\n\t\t\tsmi_info->oem_data_avail_handler =\n\t\t\t\toem_data_avail_to_receive_msg_avail;\n\t\t}\n\t}\n}\n\n#define CANNOT_RETURN_REQUESTED_LENGTH 0xCA\nstatic void return_hosed_msg_badsize(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg = smi_info->curr_msg;\n\n\t/* Make it a response */\n\tmsg->rsp[0] = msg->data[0] | 4;\n\tmsg->rsp[1] = msg->data[1];\n\tmsg->rsp[2] = CANNOT_RETURN_REQUESTED_LENGTH;\n\tmsg->rsp_size = 3;\n\tsmi_info->curr_msg = NULL;\n\tdeliver_recv_msg(smi_info, msg);\n}\n\n/*\n * dell_poweredge_bt_xaction_handler\n * @info - smi_info.device_id must be populated\n *\n * Dell PowerEdge servers with the BT interface (x6xx and 1750) will\n * not respond to a Get SDR command if the length of the data\n * requested is exactly 0x3A, which leads to command timeouts and no\n * data returned.  This intercepts such commands, and causes userspace\n * callers to try again with a different-sized buffer, which succeeds.\n */\n\n#define STORAGE_NETFN 0x0A\n#define STORAGE_CMD_GET_SDR 0x23\nstatic int dell_poweredge_bt_xaction_handler(struct notifier_block *self,\n\t\t\t\t\t     unsigned long unused,\n\t\t\t\t\t     void *in)\n{\n\tstruct smi_info *smi_info = in;\n\tunsigned char *data = smi_info->curr_msg->data;\n\tunsigned int size   = smi_info->curr_msg->data_size;\n\tif (size >= 8 &&\n\t    (data[0]>>2) == STORAGE_NETFN &&\n\t    data[1] == STORAGE_CMD_GET_SDR &&\n\t    data[7] == 0x3A) {\n\t\treturn_hosed_msg_badsize(smi_info);\n\t\treturn NOTIFY_STOP;\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block dell_poweredge_bt_xaction_notifier = {\n\t.notifier_call\t= dell_poweredge_bt_xaction_handler,\n};\n\n/*\n * setup_dell_poweredge_bt_xaction_handler\n * @info - smi_info.device_id must be filled in already\n *\n * Fills in smi_info.device_id.start_transaction_pre_hook\n * when we know what function to use there.\n */\nstatic void\nsetup_dell_poweredge_bt_xaction_handler(struct smi_info *smi_info)\n{\n\tstruct ipmi_device_id *id = &smi_info->device_id;\n\tif (id->manufacturer_id == DELL_IANA_MFR_ID &&\n\t    smi_info->io.si_type == SI_BT)\n\t\tregister_xaction_notifier(&dell_poweredge_bt_xaction_notifier);\n}\n\n/*\n * setup_oem_data_handler\n * @info - smi_info.device_id must be filled in already\n *\n * Fills in smi_info.device_id.oem_data_available_handler\n * when we know what function to use there.\n */\n\nstatic void setup_oem_data_handler(struct smi_info *smi_info)\n{\n\tsetup_dell_poweredge_oem_data_handler(smi_info);\n}\n\nstatic void setup_xaction_handlers(struct smi_info *smi_info)\n{\n\tsetup_dell_poweredge_bt_xaction_handler(smi_info);\n}\n\nstatic void check_for_broken_irqs(struct smi_info *smi_info)\n{\n\tcheck_clr_rcv_irq(smi_info);\n\tcheck_set_rcv_irq(smi_info);\n}\n\nstatic inline void stop_timer_and_thread(struct smi_info *smi_info)\n{\n\tif (smi_info->thread != NULL) {\n\t\tkthread_stop(smi_info->thread);\n\t\tsmi_info->thread = NULL;\n\t}\n\n\tsmi_info->timer_can_start = false;\n\tif (smi_info->timer_running)\n\t\tdel_timer_sync(&smi_info->si_timer);\n}\n\nstatic struct smi_info *find_dup_si(struct smi_info *info)\n{\n\tstruct smi_info *e;\n\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (e->io.addr_type != info->io.addr_type)\n\t\t\tcontinue;\n\t\tif (e->io.addr_data == info->io.addr_data) {\n\t\t\t/*\n\t\t\t * This is a cheap hack, ACPI doesn't have a defined\n\t\t\t * slave address but SMBIOS does.  Pick it up from\n\t\t\t * any source that has it available.\n\t\t\t */\n\t\t\tif (info->io.slave_addr && !e->io.slave_addr)\n\t\t\t\te->io.slave_addr = info->io.slave_addr;\n\t\t\treturn e;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nint ipmi_si_add_smi(struct si_sm_io *io)\n{\n\tint rv = 0;\n\tstruct smi_info *new_smi, *dup;\n\n\tif (!io->io_setup) {\n\t\tif (io->addr_type == IPMI_IO_ADDR_SPACE) {\n\t\t\tio->io_setup = ipmi_si_port_setup;\n\t\t} else if (io->addr_type == IPMI_MEM_ADDR_SPACE) {\n\t\t\tio->io_setup = ipmi_si_mem_setup;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tnew_smi = kzalloc(sizeof(*new_smi), GFP_KERNEL);\n\tif (!new_smi)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&new_smi->si_lock);\n\n\tnew_smi->io = *io;\n\n\tmutex_lock(&smi_infos_lock);\n\tdup = find_dup_si(new_smi);\n\tif (dup) {\n\t\tif (new_smi->io.addr_source == SI_ACPI &&\n\t\t    dup->io.addr_source == SI_SMBIOS) {\n\t\t\t/* We prefer ACPI over SMBIOS. */\n\t\t\tdev_info(dup->io.dev,\n\t\t\t\t \"Removing SMBIOS-specified %s state machine in favor of ACPI\\n\",\n\t\t\t\t si_to_str[new_smi->io.si_type]);\n\t\t\tcleanup_one_si(dup);\n\t\t} else {\n\t\t\tdev_info(new_smi->io.dev,\n\t\t\t\t \"%s-specified %s state machine: duplicate\\n\",\n\t\t\t\t ipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\t\t\t si_to_str[new_smi->io.si_type]);\n\t\t\trv = -EBUSY;\n\t\t\tkfree(new_smi);\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\tpr_info(\"Adding %s-specified %s state machine\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type]);\n\n\tlist_add_tail(&new_smi->link, &smi_infos);\n\n\tif (initialized)\n\t\trv = try_smi_init(new_smi);\nout_err:\n\tmutex_unlock(&smi_infos_lock);\n\treturn rv;\n}\n\n/*\n * Try to start up an interface.  Must be called with smi_infos_lock\n * held, primarily to keep smi_num consistent, we only one to do these\n * one at a time.\n */\nstatic int try_smi_init(struct smi_info *new_smi)\n{\n\tint rv = 0;\n\tint i;\n\tchar *init_name = NULL;\n\n\tpr_info(\"Trying %s-specified %s state machine at %s address 0x%lx, slave address 0x%x, irq %d\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type],\n\t\taddr_space_to_str[new_smi->io.addr_type],\n\t\tnew_smi->io.addr_data,\n\t\tnew_smi->io.slave_addr, new_smi->io.irq);\n\n\tswitch (new_smi->io.si_type) {\n\tcase SI_KCS:\n\t\tnew_smi->handlers = &kcs_smi_handlers;\n\t\tbreak;\n\n\tcase SI_SMIC:\n\t\tnew_smi->handlers = &smic_smi_handlers;\n\t\tbreak;\n\n\tcase SI_BT:\n\t\tnew_smi->handlers = &bt_smi_handlers;\n\t\tbreak;\n\n\tdefault:\n\t\t/* No support for anything else yet. */\n\t\trv = -EIO;\n\t\tgoto out_err;\n\t}\n\n\tnew_smi->si_num = smi_num;\n\n\t/* Do this early so it's available for logs. */\n\tif (!new_smi->io.dev) {\n\t\tinit_name = kasprintf(GFP_KERNEL, \"ipmi_si.%d\",\n\t\t\t\t      new_smi->si_num);\n\n\t\t/*\n\t\t * If we don't already have a device from something\n\t\t * else (like PCI), then register a new one.\n\t\t */\n\t\tnew_smi->pdev = platform_device_alloc(\"ipmi_si\",\n\t\t\t\t\t\t      new_smi->si_num);\n\t\tif (!new_smi->pdev) {\n\t\t\tpr_err(\"Unable to allocate platform device\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->io.dev = &new_smi->pdev->dev;\n\t\tnew_smi->io.dev->driver = &ipmi_platform_driver.driver;\n\t\t/* Nulled by device_add() */\n\t\tnew_smi->io.dev->init_name = init_name;\n\t}\n\n\t/* Allocate the state machine's data and initialize it. */\n\tnew_smi->si_sm = kmalloc(new_smi->handlers->size(), GFP_KERNEL);\n\tif (!new_smi->si_sm) {\n\t\trv = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tnew_smi->io.io_size = new_smi->handlers->init_data(new_smi->si_sm,\n\t\t\t\t\t\t\t   &new_smi->io);\n\n\t/* Now that we know the I/O size, we can set up the I/O. */\n\trv = new_smi->io.io_setup(&new_smi->io);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev, \"Could not set up I/O space\\n\");\n\t\tgoto out_err;\n\t}\n\n\t/* Do low-level detection first. */\n\tif (new_smi->handlers->detect(new_smi->si_sm)) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Interface detection failed\\n\");\n\t\trv = -ENODEV;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Attempt a get device id command.  If it fails, we probably\n\t * don't have a BMC here.\n\t */\n\trv = try_get_dev_id(new_smi);\n\tif (rv) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t       \"There appears to be no BMC at this location\\n\");\n\t\tgoto out_err;\n\t}\n\n\tsetup_oem_data_handler(new_smi);\n\tsetup_xaction_handlers(new_smi);\n\tcheck_for_broken_irqs(new_smi);\n\n\tnew_smi->waiting_msg = NULL;\n\tnew_smi->curr_msg = NULL;\n\tatomic_set(&new_smi->req_events, 0);\n\tnew_smi->run_to_completion = false;\n\tfor (i = 0; i < SI_NUM_STATS; i++)\n\t\tatomic_set(&new_smi->stats[i], 0);\n\n\tnew_smi->interrupt_disabled = true;\n\tatomic_set(&new_smi->need_watch, 0);\n\n\trv = try_enable_event_buffer(new_smi);\n\tif (rv == 0)\n\t\tnew_smi->has_event_buffer = true;\n\n\t/*\n\t * Start clearing the flags before we enable interrupts or the\n\t * timer to avoid racing with the timer.\n\t */\n\tstart_clear_flags(new_smi);\n\n\t/*\n\t * IRQ is defined to be set when non-zero.  req_events will\n\t * cause a global flags check that will enable interrupts.\n\t */\n\tif (new_smi->io.irq) {\n\t\tnew_smi->interrupt_disabled = false;\n\t\tatomic_set(&new_smi->req_events, 1);\n\t}\n\n\tif (new_smi->pdev && !new_smi->pdev_registered) {\n\t\trv = platform_device_add(new_smi->pdev);\n\t\tif (rv) {\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Unable to register system interface device: %d\\n\",\n\t\t\t\trv);\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->pdev_registered = true;\n\t}\n\n\tdev_set_drvdata(new_smi->io.dev, new_smi);\n\trv = device_add_group(new_smi->io.dev, &ipmi_si_dev_attr_group);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to add device attributes: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\tnew_smi->dev_group_added = true;\n\n\trv = ipmi_register_smi(&handlers,\n\t\t\t       new_smi,\n\t\t\t       new_smi->io.dev,\n\t\t\t       new_smi->io.slave_addr);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to register device: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\n\t/* Don't increment till we know we have succeeded. */\n\tsmi_num++;\n\n\tdev_info(new_smi->io.dev, \"IPMI %s interface initialized\\n\",\n\t\t si_to_str[new_smi->io.si_type]);\n\n\tWARN_ON(new_smi->io.dev->init_name != NULL);\n\n out_err:\n\tkfree(init_name);\n\treturn rv;\n}\n\nstatic int init_ipmi_si(void)\n{\n\tstruct smi_info *e;\n\tenum ipmi_addr_src type = SI_INVALID;\n\n\tif (initialized)\n\t\treturn 0;\n\n\tpr_info(\"IPMI System Interface driver\\n\");\n\n\t/* If the user gave us a device, they presumably want us to use it */\n\tif (!ipmi_si_hardcode_find_bmc())\n\t\tgoto do_scan;\n\n\tipmi_si_platform_init();\n\n\tipmi_si_pci_init();\n\n\tipmi_si_parisc_init();\n\n\t/* We prefer devices with interrupts, but in the case of a machine\n\t   with multiple BMCs we assume that there will be several instances\n\t   of a given type so if we succeed in registering a type then also\n\t   try to register everything else of the same type */\ndo_scan:\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\t/* Try to register a device if it has an IRQ and we either\n\t\t   haven't successfully registered a device yet or this\n\t\t   device has the same type as one we successfully registered */\n\t\tif (e->io.irq && (!type || e->io.addr_source == type)) {\n\t\t\tif (!try_smi_init(e)) {\n\t\t\t\ttype = e->io.addr_source;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* type will only have been set if we successfully registered an si */\n\tif (type)\n\t\tgoto skip_fallback_noirq;\n\n\t/* Fall back to the preferred device */\n\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (!e->io.irq && (!type || e->io.addr_source == type)) {\n\t\t\tif (!try_smi_init(e)) {\n\t\t\t\ttype = e->io.addr_source;\n\t\t\t}\n\t\t}\n\t}\n\nskip_fallback_noirq:\n\tinitialized = 1;\n\tmutex_unlock(&smi_infos_lock);\n\n\tif (type)\n\t\treturn 0;\n\n\tmutex_lock(&smi_infos_lock);\n\tif (unload_when_empty && list_empty(&smi_infos)) {\n\t\tmutex_unlock(&smi_infos_lock);\n\t\tcleanup_ipmi_si();\n\t\tpr_warn(\"Unable to find any System Interface(s)\\n\");\n\t\treturn -ENODEV;\n\t} else {\n\t\tmutex_unlock(&smi_infos_lock);\n\t\treturn 0;\n\t}\n}\nmodule_init(init_ipmi_si);\n\nstatic void shutdown_smi(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\n\tif (smi_info->dev_group_added) {\n\t\tdevice_remove_group(smi_info->io.dev, &ipmi_si_dev_attr_group);\n\t\tsmi_info->dev_group_added = false;\n\t}\n\tif (smi_info->io.dev)\n\t\tdev_set_drvdata(smi_info->io.dev, NULL);\n\n\t/*\n\t * Make sure that interrupts, the timer and the thread are\n\t * stopped and will not run again.\n\t */\n\tsmi_info->interrupt_disabled = true;\n\tif (smi_info->io.irq_cleanup) {\n\t\tsmi_info->io.irq_cleanup(&smi_info->io);\n\t\tsmi_info->io.irq_cleanup = NULL;\n\t}\n\tstop_timer_and_thread(smi_info);\n\n\t/*\n\t * Wait until we know that we are out of any interrupt\n\t * handlers might have been running before we freed the\n\t * interrupt.\n\t */\n\tsynchronize_rcu();\n\n\t/*\n\t * Timeouts are stopped, now make sure the interrupts are off\n\t * in the BMC.  Note that timers and CPU interrupts are off,\n\t * so no need for locks.\n\t */\n\twhile (smi_info->curr_msg || (smi_info->si_state != SI_NORMAL)) {\n\t\tpoll(smi_info);\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\tif (smi_info->handlers)\n\t\tdisable_si_irq(smi_info);\n\twhile (smi_info->curr_msg || (smi_info->si_state != SI_NORMAL)) {\n\t\tpoll(smi_info);\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\tif (smi_info->handlers)\n\t\tsmi_info->handlers->cleanup(smi_info->si_sm);\n\n\tif (smi_info->io.addr_source_cleanup) {\n\t\tsmi_info->io.addr_source_cleanup(&smi_info->io);\n\t\tsmi_info->io.addr_source_cleanup = NULL;\n\t}\n\tif (smi_info->io.io_cleanup) {\n\t\tsmi_info->io.io_cleanup(&smi_info->io);\n\t\tsmi_info->io.io_cleanup = NULL;\n\t}\n\n\tkfree(smi_info->si_sm);\n\tsmi_info->si_sm = NULL;\n\n\tsmi_info->intf = NULL;\n}\n\n/*\n * Must be called with smi_infos_lock held, to serialize the\n * smi_info->intf check.\n */\nstatic void cleanup_one_si(struct smi_info *smi_info)\n{\n\tif (!smi_info)\n\t\treturn;\n\n\tlist_del(&smi_info->link);\n\n\tif (smi_info->intf)\n\t\tipmi_unregister_smi(smi_info->intf);\n\n\tif (smi_info->pdev) {\n\t\tif (smi_info->pdev_registered)\n\t\t\tplatform_device_unregister(smi_info->pdev);\n\t\telse\n\t\t\tplatform_device_put(smi_info->pdev);\n\t}\n\n\tkfree(smi_info);\n}\n\nint ipmi_si_remove_by_dev(struct device *dev)\n{\n\tstruct smi_info *e;\n\tint rv = -ENOENT;\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (e->io.dev == dev) {\n\t\t\tcleanup_one_si(e);\n\t\t\trv = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&smi_infos_lock);\n\n\treturn rv;\n}\n\nvoid ipmi_si_remove_by_data(int addr_space, enum si_type si_type,\n\t\t\t    unsigned long addr)\n{\n\t/* remove */\n\tstruct smi_info *e, *tmp_e;\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry_safe(e, tmp_e, &smi_infos, link) {\n\t\tif (e->io.addr_type != addr_space)\n\t\t\tcontinue;\n\t\tif (e->io.si_type != si_type)\n\t\t\tcontinue;\n\t\tif (e->io.addr_data == addr)\n\t\t\tcleanup_one_si(e);\n\t}\n\tmutex_unlock(&smi_infos_lock);\n}\n\nstatic void cleanup_ipmi_si(void)\n{\n\tstruct smi_info *e, *tmp_e;\n\n\tif (!initialized)\n\t\treturn;\n\n\tipmi_si_pci_shutdown();\n\n\tipmi_si_parisc_shutdown();\n\n\tipmi_si_platform_shutdown();\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry_safe(e, tmp_e, &smi_infos, link)\n\t\tcleanup_one_si(e);\n\tmutex_unlock(&smi_infos_lock);\n}\nmodule_exit(cleanup_ipmi_si);\n\nMODULE_ALIAS(\"platform:dmi-ipmi-si\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Corey Minyard <minyard@mvista.com>\");\nMODULE_DESCRIPTION(\"Interface to the IPMI driver for the KCS, SMIC, and BT\"\n\t\t   \" system interfaces.\");\n", "// SPDX-License-Identifier: GPL-2.0+\n\n#include <linux/io.h>\n#include \"ipmi_si.h\"\n\nstatic unsigned char intf_mem_inb(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn readb((io->addr)+(offset * io->regspacing));\n}\n\nstatic void intf_mem_outb(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twriteb(b, (io->addr)+(offset * io->regspacing));\n}\n\nstatic unsigned char intf_mem_inw(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn (readw((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void intf_mem_outw(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twriteb(b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n\nstatic unsigned char intf_mem_inl(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn (readl((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void intf_mem_outl(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twritel(b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n\n#ifdef readq\nstatic unsigned char mem_inq(const struct si_sm_io *io, unsigned int offset)\n{\n\treturn (readq((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void mem_outq(const struct si_sm_io *io, unsigned int offset,\n\t\t     unsigned char b)\n{\n\twriteq((u64)b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n#endif\n\nstatic void mem_region_cleanup(struct si_sm_io *io, int num)\n{\n\tunsigned long addr = io->addr_data;\n\tint idx;\n\n\tfor (idx = 0; idx < num; idx++)\n\t\trelease_mem_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize);\n}\n\nstatic void mem_cleanup(struct si_sm_io *io)\n{\n\tif (io->addr) {\n\t\tiounmap(io->addr);\n\t\tmem_region_cleanup(io, io->io_size);\n\t}\n}\n\nint ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = mem_cleanup;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n", "// SPDX-License-Identifier: GPL-2.0+\n\n#include <linux/io.h>\n#include \"ipmi_si.h\"\n\nstatic unsigned char port_inb(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn inb(addr + (offset * io->regspacing));\n}\n\nstatic void port_outb(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutb(b, addr + (offset * io->regspacing));\n}\n\nstatic unsigned char port_inw(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn (inw(addr + (offset * io->regspacing)) >> io->regshift) & 0xff;\n}\n\nstatic void port_outw(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutw(b << io->regshift, addr + (offset * io->regspacing));\n}\n\nstatic unsigned char port_inl(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn (inl(addr + (offset * io->regspacing)) >> io->regshift) & 0xff;\n}\n\nstatic void port_outl(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutl(b << io->regshift, addr+(offset * io->regspacing));\n}\n\nstatic void port_cleanup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (addr) {\n\t\tfor (idx = 0; idx < io->io_size; idx++)\n\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize);\n\t}\n}\n\nint ipmi_si_port_setup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\tio->io_cleanup = port_cleanup;\n\n\t/*\n\t * Figure out the actual inb/inw/inl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = port_inb;\n\t\tio->outputb = port_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = port_inw;\n\t\tio->outputb = port_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = port_inl;\n\t\tio->outputb = port_outl;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint I/O regions in their ACPI\n\t * tables.  This causes problems when trying to register the\n\t * entire I/O region.  Therefore we must register each I/O\n\t * port separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\twhile (idx--)\n\t\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t\t       io->regsize);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\treturn 0;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0+\n/*\n * ipmi_si.c\n *\n * The interface to the IPMI driver for the system interfaces (KCS, SMIC,\n * BT).\n *\n * Author: MontaVista Software, Inc.\n *         Corey Minyard <minyard@mvista.com>\n *         source@mvista.com\n *\n * Copyright 2002 MontaVista Software Inc.\n * Copyright 2006 IBM Corp., Christian Krafft <krafft@de.ibm.com>\n */\n\n/*\n * This file holds the \"policy\" for the interface to the SMI state\n * machine.  It does the configuration, handles timers and interrupts,\n * and drives the real SMI state machine.\n */\n\n#define pr_fmt(fmt) \"ipmi_si: \" fmt\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/sched.h>\n#include <linux/seq_file.h>\n#include <linux/timer.h>\n#include <linux/errno.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/list.h>\n#include <linux/notifier.h>\n#include <linux/mutex.h>\n#include <linux/kthread.h>\n#include <asm/irq.h>\n#include <linux/interrupt.h>\n#include <linux/rcupdate.h>\n#include <linux/ipmi.h>\n#include <linux/ipmi_smi.h>\n#include \"ipmi_si.h\"\n#include <linux/string.h>\n#include <linux/ctype.h>\n\n/* Measure times between events in the driver. */\n#undef DEBUG_TIMING\n\n/* Call every 10 ms. */\n#define SI_TIMEOUT_TIME_USEC\t10000\n#define SI_USEC_PER_JIFFY\t(1000000/HZ)\n#define SI_TIMEOUT_JIFFIES\t(SI_TIMEOUT_TIME_USEC/SI_USEC_PER_JIFFY)\n#define SI_SHORT_TIMEOUT_USEC  250 /* .25ms when the SM request a\n\t\t\t\t      short timeout */\n\nenum si_intf_state {\n\tSI_NORMAL,\n\tSI_GETTING_FLAGS,\n\tSI_GETTING_EVENTS,\n\tSI_CLEARING_FLAGS,\n\tSI_GETTING_MESSAGES,\n\tSI_CHECKING_ENABLES,\n\tSI_SETTING_ENABLES\n\t/* FIXME - add watchdog stuff. */\n};\n\n/* Some BT-specific defines we need here. */\n#define IPMI_BT_INTMASK_REG\t\t2\n#define IPMI_BT_INTMASK_CLEAR_IRQ_BIT\t2\n#define IPMI_BT_INTMASK_ENABLE_IRQ_BIT\t1\n\nstatic const char * const si_to_str[] = { \"invalid\", \"kcs\", \"smic\", \"bt\" };\n\nstatic int initialized;\n\n/*\n * Indexes into stats[] in smi_info below.\n */\nenum si_stat_indexes {\n\t/*\n\t * Number of times the driver requested a timer while an operation\n\t * was in progress.\n\t */\n\tSI_STAT_short_timeouts = 0,\n\n\t/*\n\t * Number of times the driver requested a timer while nothing was in\n\t * progress.\n\t */\n\tSI_STAT_long_timeouts,\n\n\t/* Number of times the interface was idle while being polled. */\n\tSI_STAT_idles,\n\n\t/* Number of interrupts the driver handled. */\n\tSI_STAT_interrupts,\n\n\t/* Number of time the driver got an ATTN from the hardware. */\n\tSI_STAT_attentions,\n\n\t/* Number of times the driver requested flags from the hardware. */\n\tSI_STAT_flag_fetches,\n\n\t/* Number of times the hardware didn't follow the state machine. */\n\tSI_STAT_hosed_count,\n\n\t/* Number of completed messages. */\n\tSI_STAT_complete_transactions,\n\n\t/* Number of IPMI events received from the hardware. */\n\tSI_STAT_events,\n\n\t/* Number of watchdog pretimeouts. */\n\tSI_STAT_watchdog_pretimeouts,\n\n\t/* Number of asynchronous messages received. */\n\tSI_STAT_incoming_messages,\n\n\n\t/* This *must* remain last, add new values above this. */\n\tSI_NUM_STATS\n};\n\nstruct smi_info {\n\tint                    si_num;\n\tstruct ipmi_smi        *intf;\n\tstruct si_sm_data      *si_sm;\n\tconst struct si_sm_handlers *handlers;\n\tspinlock_t             si_lock;\n\tstruct ipmi_smi_msg    *waiting_msg;\n\tstruct ipmi_smi_msg    *curr_msg;\n\tenum si_intf_state     si_state;\n\n\t/*\n\t * Used to handle the various types of I/O that can occur with\n\t * IPMI\n\t */\n\tstruct si_sm_io io;\n\n\t/*\n\t * Per-OEM handler, called from handle_flags().  Returns 1\n\t * when handle_flags() needs to be re-run or 0 indicating it\n\t * set si_state itself.\n\t */\n\tint (*oem_data_avail_handler)(struct smi_info *smi_info);\n\n\t/*\n\t * Flags from the last GET_MSG_FLAGS command, used when an ATTN\n\t * is set to hold the flags until we are done handling everything\n\t * from the flags.\n\t */\n#define RECEIVE_MSG_AVAIL\t0x01\n#define EVENT_MSG_BUFFER_FULL\t0x02\n#define WDT_PRE_TIMEOUT_INT\t0x08\n#define OEM0_DATA_AVAIL     0x20\n#define OEM1_DATA_AVAIL     0x40\n#define OEM2_DATA_AVAIL     0x80\n#define OEM_DATA_AVAIL      (OEM0_DATA_AVAIL | \\\n\t\t\t     OEM1_DATA_AVAIL | \\\n\t\t\t     OEM2_DATA_AVAIL)\n\tunsigned char       msg_flags;\n\n\t/* Does the BMC have an event buffer? */\n\tbool\t\t    has_event_buffer;\n\n\t/*\n\t * If set to true, this will request events the next time the\n\t * state machine is idle.\n\t */\n\tatomic_t            req_events;\n\n\t/*\n\t * If true, run the state machine to completion on every send\n\t * call.  Generally used after a panic to make sure stuff goes\n\t * out.\n\t */\n\tbool                run_to_completion;\n\n\t/* The timer for this si. */\n\tstruct timer_list   si_timer;\n\n\t/* This flag is set, if the timer can be set */\n\tbool\t\t    timer_can_start;\n\n\t/* This flag is set, if the timer is running (timer_pending() isn't enough) */\n\tbool\t\t    timer_running;\n\n\t/* The time (in jiffies) the last timeout occurred at. */\n\tunsigned long       last_timeout_jiffies;\n\n\t/* Are we waiting for the events, pretimeouts, received msgs? */\n\tatomic_t            need_watch;\n\n\t/*\n\t * The driver will disable interrupts when it gets into a\n\t * situation where it cannot handle messages due to lack of\n\t * memory.  Once that situation clears up, it will re-enable\n\t * interrupts.\n\t */\n\tbool interrupt_disabled;\n\n\t/*\n\t * Does the BMC support events?\n\t */\n\tbool supports_event_msg_buff;\n\n\t/*\n\t * Can we disable interrupts the global enables receive irq\n\t * bit?  There are currently two forms of brokenness, some\n\t * systems cannot disable the bit (which is technically within\n\t * the spec but a bad idea) and some systems have the bit\n\t * forced to zero even though interrupts work (which is\n\t * clearly outside the spec).  The next bool tells which form\n\t * of brokenness is present.\n\t */\n\tbool cannot_disable_irq;\n\n\t/*\n\t * Some systems are broken and cannot set the irq enable\n\t * bit, even if they support interrupts.\n\t */\n\tbool irq_enable_broken;\n\n\t/*\n\t * Did we get an attention that we did not handle?\n\t */\n\tbool got_attn;\n\n\t/* From the get device id response... */\n\tstruct ipmi_device_id device_id;\n\n\t/* Default driver model device. */\n\tstruct platform_device *pdev;\n\n\t/* Have we added the device group to the device? */\n\tbool dev_group_added;\n\n\t/* Have we added the platform device? */\n\tbool pdev_registered;\n\n\t/* Counters and things for the proc filesystem. */\n\tatomic_t stats[SI_NUM_STATS];\n\n\tstruct task_struct *thread;\n\n\tstruct list_head link;\n};\n\n#define smi_inc_stat(smi, stat) \\\n\tatomic_inc(&(smi)->stats[SI_STAT_ ## stat])\n#define smi_get_stat(smi, stat) \\\n\t((unsigned int) atomic_read(&(smi)->stats[SI_STAT_ ## stat]))\n\n#define IPMI_MAX_INTFS 4\nstatic int force_kipmid[IPMI_MAX_INTFS];\nstatic int num_force_kipmid;\n\nstatic unsigned int kipmid_max_busy_us[IPMI_MAX_INTFS];\nstatic int num_max_busy_us;\n\nstatic bool unload_when_empty = true;\n\nstatic int try_smi_init(struct smi_info *smi);\nstatic void cleanup_one_si(struct smi_info *smi_info);\nstatic void cleanup_ipmi_si(void);\n\n#ifdef DEBUG_TIMING\nvoid debug_timestamp(char *msg)\n{\n\tstruct timespec64 t;\n\n\tktime_get_ts64(&t);\n\tpr_debug(\"**%s: %lld.%9.9ld\\n\", msg, (long long) t.tv_sec, t.tv_nsec);\n}\n#else\n#define debug_timestamp(x)\n#endif\n\nstatic ATOMIC_NOTIFIER_HEAD(xaction_notifier_list);\nstatic int register_xaction_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_register(&xaction_notifier_list, nb);\n}\n\nstatic void deliver_recv_msg(struct smi_info *smi_info,\n\t\t\t     struct ipmi_smi_msg *msg)\n{\n\t/* Deliver the message to the upper layer. */\n\tipmi_smi_msg_received(smi_info->intf, msg);\n}\n\nstatic void return_hosed_msg(struct smi_info *smi_info, int cCode)\n{\n\tstruct ipmi_smi_msg *msg = smi_info->curr_msg;\n\n\tif (cCode < 0 || cCode > IPMI_ERR_UNSPECIFIED)\n\t\tcCode = IPMI_ERR_UNSPECIFIED;\n\t/* else use it as is */\n\n\t/* Make it a response */\n\tmsg->rsp[0] = msg->data[0] | 4;\n\tmsg->rsp[1] = msg->data[1];\n\tmsg->rsp[2] = cCode;\n\tmsg->rsp_size = 3;\n\n\tsmi_info->curr_msg = NULL;\n\tdeliver_recv_msg(smi_info, msg);\n}\n\nstatic enum si_sm_result start_next_msg(struct smi_info *smi_info)\n{\n\tint              rv;\n\n\tif (!smi_info->waiting_msg) {\n\t\tsmi_info->curr_msg = NULL;\n\t\trv = SI_SM_IDLE;\n\t} else {\n\t\tint err;\n\n\t\tsmi_info->curr_msg = smi_info->waiting_msg;\n\t\tsmi_info->waiting_msg = NULL;\n\t\tdebug_timestamp(\"Start2\");\n\t\terr = atomic_notifier_call_chain(&xaction_notifier_list,\n\t\t\t\t0, smi_info);\n\t\tif (err & NOTIFY_STOP_MASK) {\n\t\t\trv = SI_SM_CALL_WITHOUT_DELAY;\n\t\t\tgoto out;\n\t\t}\n\t\terr = smi_info->handlers->start_transaction(\n\t\t\tsmi_info->si_sm,\n\t\t\tsmi_info->curr_msg->data,\n\t\t\tsmi_info->curr_msg->data_size);\n\t\tif (err)\n\t\t\treturn_hosed_msg(smi_info, err);\n\n\t\trv = SI_SM_CALL_WITHOUT_DELAY;\n\t}\nout:\n\treturn rv;\n}\n\nstatic void smi_mod_timer(struct smi_info *smi_info, unsigned long new_val)\n{\n\tif (!smi_info->timer_can_start)\n\t\treturn;\n\tsmi_info->last_timeout_jiffies = jiffies;\n\tmod_timer(&smi_info->si_timer, new_val);\n\tsmi_info->timer_running = true;\n}\n\n/*\n * Start a new message and (re)start the timer and thread.\n */\nstatic void start_new_msg(struct smi_info *smi_info, unsigned char *msg,\n\t\t\t  unsigned int size)\n{\n\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\tif (smi_info->thread)\n\t\twake_up_process(smi_info->thread);\n\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, size);\n}\n\nstatic void start_check_enables(struct smi_info *smi_info)\n{\n\tunsigned char msg[2];\n\n\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\n\tstart_new_msg(smi_info, msg, 2);\n\tsmi_info->si_state = SI_CHECKING_ENABLES;\n}\n\nstatic void start_clear_flags(struct smi_info *smi_info)\n{\n\tunsigned char msg[3];\n\n\t/* Make sure the watchdog pre-timeout flag is not set at startup. */\n\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tmsg[1] = IPMI_CLEAR_MSG_FLAGS_CMD;\n\tmsg[2] = WDT_PRE_TIMEOUT_INT;\n\n\tstart_new_msg(smi_info, msg, 3);\n\tsmi_info->si_state = SI_CLEARING_FLAGS;\n}\n\nstatic void start_getting_msg_queue(struct smi_info *smi_info)\n{\n\tsmi_info->curr_msg->data[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tsmi_info->curr_msg->data[1] = IPMI_GET_MSG_CMD;\n\tsmi_info->curr_msg->data_size = 2;\n\n\tstart_new_msg(smi_info, smi_info->curr_msg->data,\n\t\t      smi_info->curr_msg->data_size);\n\tsmi_info->si_state = SI_GETTING_MESSAGES;\n}\n\nstatic void start_getting_events(struct smi_info *smi_info)\n{\n\tsmi_info->curr_msg->data[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\tsmi_info->curr_msg->data[1] = IPMI_READ_EVENT_MSG_BUFFER_CMD;\n\tsmi_info->curr_msg->data_size = 2;\n\n\tstart_new_msg(smi_info, smi_info->curr_msg->data,\n\t\t      smi_info->curr_msg->data_size);\n\tsmi_info->si_state = SI_GETTING_EVENTS;\n}\n\n/*\n * When we have a situtaion where we run out of memory and cannot\n * allocate messages, we just leave them in the BMC and run the system\n * polled until we can allocate some memory.  Once we have some\n * memory, we will re-enable the interrupt.\n *\n * Note that we cannot just use disable_irq(), since the interrupt may\n * be shared.\n */\nstatic inline bool disable_si_irq(struct smi_info *smi_info)\n{\n\tif ((smi_info->io.irq) && (!smi_info->interrupt_disabled)) {\n\t\tsmi_info->interrupt_disabled = true;\n\t\tstart_check_enables(smi_info);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool enable_si_irq(struct smi_info *smi_info)\n{\n\tif ((smi_info->io.irq) && (smi_info->interrupt_disabled)) {\n\t\tsmi_info->interrupt_disabled = false;\n\t\tstart_check_enables(smi_info);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n/*\n * Allocate a message.  If unable to allocate, start the interrupt\n * disable process and return NULL.  If able to allocate but\n * interrupts are disabled, free the message and return NULL after\n * starting the interrupt enable process.\n */\nstatic struct ipmi_smi_msg *alloc_msg_handle_irq(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg;\n\n\tmsg = ipmi_alloc_smi_msg();\n\tif (!msg) {\n\t\tif (!disable_si_irq(smi_info))\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t} else if (enable_si_irq(smi_info)) {\n\t\tipmi_free_smi_msg(msg);\n\t\tmsg = NULL;\n\t}\n\treturn msg;\n}\n\nstatic void handle_flags(struct smi_info *smi_info)\n{\nretry:\n\tif (smi_info->msg_flags & WDT_PRE_TIMEOUT_INT) {\n\t\t/* Watchdog pre-timeout */\n\t\tsmi_inc_stat(smi_info, watchdog_pretimeouts);\n\n\t\tstart_clear_flags(smi_info);\n\t\tsmi_info->msg_flags &= ~WDT_PRE_TIMEOUT_INT;\n\t\tipmi_smi_watchdog_pretimeout(smi_info->intf);\n\t} else if (smi_info->msg_flags & RECEIVE_MSG_AVAIL) {\n\t\t/* Messages available. */\n\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\tif (!smi_info->curr_msg)\n\t\t\treturn;\n\n\t\tstart_getting_msg_queue(smi_info);\n\t} else if (smi_info->msg_flags & EVENT_MSG_BUFFER_FULL) {\n\t\t/* Events available. */\n\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\tif (!smi_info->curr_msg)\n\t\t\treturn;\n\n\t\tstart_getting_events(smi_info);\n\t} else if (smi_info->msg_flags & OEM_DATA_AVAIL &&\n\t\t   smi_info->oem_data_avail_handler) {\n\t\tif (smi_info->oem_data_avail_handler(smi_info))\n\t\t\tgoto retry;\n\t} else\n\t\tsmi_info->si_state = SI_NORMAL;\n}\n\n/*\n * Global enables we care about.\n */\n#define GLOBAL_ENABLES_MASK (IPMI_BMC_EVT_MSG_BUFF | IPMI_BMC_RCV_MSG_INTR | \\\n\t\t\t     IPMI_BMC_EVT_MSG_INTR)\n\nstatic u8 current_global_enables(struct smi_info *smi_info, u8 base,\n\t\t\t\t bool *irq_on)\n{\n\tu8 enables = 0;\n\n\tif (smi_info->supports_event_msg_buff)\n\t\tenables |= IPMI_BMC_EVT_MSG_BUFF;\n\n\tif (((smi_info->io.irq && !smi_info->interrupt_disabled) ||\n\t     smi_info->cannot_disable_irq) &&\n\t    !smi_info->irq_enable_broken)\n\t\tenables |= IPMI_BMC_RCV_MSG_INTR;\n\n\tif (smi_info->supports_event_msg_buff &&\n\t    smi_info->io.irq && !smi_info->interrupt_disabled &&\n\t    !smi_info->irq_enable_broken)\n\t\tenables |= IPMI_BMC_EVT_MSG_INTR;\n\n\t*irq_on = enables & (IPMI_BMC_EVT_MSG_INTR | IPMI_BMC_RCV_MSG_INTR);\n\n\treturn enables;\n}\n\nstatic void check_bt_irq(struct smi_info *smi_info, bool irq_on)\n{\n\tu8 irqstate = smi_info->io.inputb(&smi_info->io, IPMI_BT_INTMASK_REG);\n\n\tirqstate &= IPMI_BT_INTMASK_ENABLE_IRQ_BIT;\n\n\tif ((bool)irqstate == irq_on)\n\t\treturn;\n\n\tif (irq_on)\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG,\n\t\t\t\t     IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n\telse\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG, 0);\n}\n\nstatic void handle_transaction_done(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg;\n\n\tdebug_timestamp(\"Done\");\n\tswitch (smi_info->si_state) {\n\tcase SI_NORMAL:\n\t\tif (!smi_info->curr_msg)\n\t\t\tbreak;\n\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tdeliver_recv_msg(smi_info, msg);\n\t\tbreak;\n\n\tcase SI_GETTING_FLAGS:\n\t{\n\t\tunsigned char msg[4];\n\t\tunsigned int  len;\n\n\t\t/* We got the flags from the SMI, now handle them. */\n\t\tlen = smi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0) {\n\t\t\t/* Error fetching flags, just give up for now. */\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t} else if (len < 4) {\n\t\t\t/*\n\t\t\t * Hmm, no flags.  That's technically illegal, but\n\t\t\t * don't use uninitialized data.\n\t\t\t */\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t} else {\n\t\t\tsmi_info->msg_flags = msg[3];\n\t\t\thandle_flags(smi_info);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_CLEARING_FLAGS:\n\t{\n\t\tunsigned char msg[3];\n\n\t\t/* We cleared the flags. */\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 3);\n\t\tif (msg[2] != 0) {\n\t\t\t/* Error clearing flags */\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Error clearing flags: %2.2x\\n\", msg[2]);\n\t\t}\n\t\tsmi_info->si_state = SI_NORMAL;\n\t\tbreak;\n\t}\n\n\tcase SI_GETTING_EVENTS:\n\t{\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tif (msg->rsp[2] != 0) {\n\t\t\t/* Error getting event, probably done. */\n\t\t\tmsg->done(msg);\n\n\t\t\t/* Take off the event flag. */\n\t\t\tsmi_info->msg_flags &= ~EVENT_MSG_BUFFER_FULL;\n\t\t\thandle_flags(smi_info);\n\t\t} else {\n\t\t\tsmi_inc_stat(smi_info, events);\n\n\t\t\t/*\n\t\t\t * Do this before we deliver the message\n\t\t\t * because delivering the message releases the\n\t\t\t * lock and something else can mess with the\n\t\t\t * state.\n\t\t\t */\n\t\t\thandle_flags(smi_info);\n\n\t\t\tdeliver_recv_msg(smi_info, msg);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_GETTING_MESSAGES:\n\t{\n\t\tsmi_info->curr_msg->rsp_size\n\t\t\t= smi_info->handlers->get_result(\n\t\t\t\tsmi_info->si_sm,\n\t\t\t\tsmi_info->curr_msg->rsp,\n\t\t\t\tIPMI_MAX_MSG_LENGTH);\n\n\t\t/*\n\t\t * Do this here becase deliver_recv_msg() releases the\n\t\t * lock, and a new message can be put in during the\n\t\t * time the lock is released.\n\t\t */\n\t\tmsg = smi_info->curr_msg;\n\t\tsmi_info->curr_msg = NULL;\n\t\tif (msg->rsp[2] != 0) {\n\t\t\t/* Error getting event, probably done. */\n\t\t\tmsg->done(msg);\n\n\t\t\t/* Take off the msg flag. */\n\t\t\tsmi_info->msg_flags &= ~RECEIVE_MSG_AVAIL;\n\t\t\thandle_flags(smi_info);\n\t\t} else {\n\t\t\tsmi_inc_stat(smi_info, incoming_messages);\n\n\t\t\t/*\n\t\t\t * Do this before we deliver the message\n\t\t\t * because delivering the message releases the\n\t\t\t * lock and something else can mess with the\n\t\t\t * state.\n\t\t\t */\n\t\t\thandle_flags(smi_info);\n\n\t\t\tdeliver_recv_msg(smi_info, msg);\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_CHECKING_ENABLES:\n\t{\n\t\tunsigned char msg[4];\n\t\tu8 enables;\n\t\tbool irq_on;\n\n\t\t/* We got the flags from the SMI, now handle them. */\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0) {\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Couldn't get irq info: %x.\\n\", msg[2]);\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Maybe ok, but ipmi might run very slowly.\\n\");\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\tbreak;\n\t\t}\n\t\tenables = current_global_enables(smi_info, 0, &irq_on);\n\t\tif (smi_info->io.si_type == SI_BT)\n\t\t\t/* BT has its own interrupt enable bit. */\n\t\t\tcheck_bt_irq(smi_info, irq_on);\n\t\tif (enables != (msg[3] & GLOBAL_ENABLES_MASK)) {\n\t\t\t/* Enables are not correct, fix them. */\n\t\t\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\t\t\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\t\t\tmsg[2] = enables | (msg[3] & ~GLOBAL_ENABLES_MASK);\n\t\t\tsmi_info->handlers->start_transaction(\n\t\t\t\tsmi_info->si_sm, msg, 3);\n\t\t\tsmi_info->si_state = SI_SETTING_ENABLES;\n\t\t} else if (smi_info->supports_event_msg_buff) {\n\t\t\tsmi_info->curr_msg = ipmi_alloc_smi_msg();\n\t\t\tif (!smi_info->curr_msg) {\n\t\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart_getting_events(smi_info);\n\t\t} else {\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t}\n\t\tbreak;\n\t}\n\n\tcase SI_SETTING_ENABLES:\n\t{\n\t\tunsigned char msg[4];\n\n\t\tsmi_info->handlers->get_result(smi_info->si_sm, msg, 4);\n\t\tif (msg[2] != 0)\n\t\t\tdev_warn(smi_info->io.dev,\n\t\t\t\t \"Could not set the global enables: 0x%x.\\n\",\n\t\t\t\t msg[2]);\n\n\t\tif (smi_info->supports_event_msg_buff) {\n\t\t\tsmi_info->curr_msg = ipmi_alloc_smi_msg();\n\t\t\tif (!smi_info->curr_msg) {\n\t\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart_getting_events(smi_info);\n\t\t} else {\n\t\t\tsmi_info->si_state = SI_NORMAL;\n\t\t}\n\t\tbreak;\n\t}\n\t}\n}\n\n/*\n * Called on timeouts and events.  Timeouts should pass the elapsed\n * time, interrupts should pass in zero.  Must be called with\n * si_lock held and interrupts disabled.\n */\nstatic enum si_sm_result smi_event_handler(struct smi_info *smi_info,\n\t\t\t\t\t   int time)\n{\n\tenum si_sm_result si_sm_result;\n\nrestart:\n\t/*\n\t * There used to be a loop here that waited a little while\n\t * (around 25us) before giving up.  That turned out to be\n\t * pointless, the minimum delays I was seeing were in the 300us\n\t * range, which is far too long to wait in an interrupt.  So\n\t * we just run until the state machine tells us something\n\t * happened or it needs a delay.\n\t */\n\tsi_sm_result = smi_info->handlers->event(smi_info->si_sm, time);\n\ttime = 0;\n\twhile (si_sm_result == SI_SM_CALL_WITHOUT_DELAY)\n\t\tsi_sm_result = smi_info->handlers->event(smi_info->si_sm, 0);\n\n\tif (si_sm_result == SI_SM_TRANSACTION_COMPLETE) {\n\t\tsmi_inc_stat(smi_info, complete_transactions);\n\n\t\thandle_transaction_done(smi_info);\n\t\tgoto restart;\n\t} else if (si_sm_result == SI_SM_HOSED) {\n\t\tsmi_inc_stat(smi_info, hosed_count);\n\n\t\t/*\n\t\t * Do the before return_hosed_msg, because that\n\t\t * releases the lock.\n\t\t */\n\t\tsmi_info->si_state = SI_NORMAL;\n\t\tif (smi_info->curr_msg != NULL) {\n\t\t\t/*\n\t\t\t * If we were handling a user message, format\n\t\t\t * a response to send to the upper layer to\n\t\t\t * tell it about the error.\n\t\t\t */\n\t\t\treturn_hosed_msg(smi_info, IPMI_ERR_UNSPECIFIED);\n\t\t}\n\t\tgoto restart;\n\t}\n\n\t/*\n\t * We prefer handling attn over new messages.  But don't do\n\t * this if there is not yet an upper layer to handle anything.\n\t */\n\tif (si_sm_result == SI_SM_ATTN || smi_info->got_attn) {\n\t\tunsigned char msg[2];\n\n\t\tif (smi_info->si_state != SI_NORMAL) {\n\t\t\t/*\n\t\t\t * We got an ATTN, but we are doing something else.\n\t\t\t * Handle the ATTN later.\n\t\t\t */\n\t\t\tsmi_info->got_attn = true;\n\t\t} else {\n\t\t\tsmi_info->got_attn = false;\n\t\t\tsmi_inc_stat(smi_info, attentions);\n\n\t\t\t/*\n\t\t\t * Got a attn, send down a get message flags to see\n\t\t\t * what's causing it.  It would be better to handle\n\t\t\t * this in the upper layer, but due to the way\n\t\t\t * interrupts work with the SMI, that's not really\n\t\t\t * possible.\n\t\t\t */\n\t\t\tmsg[0] = (IPMI_NETFN_APP_REQUEST << 2);\n\t\t\tmsg[1] = IPMI_GET_MSG_FLAGS_CMD;\n\n\t\t\tstart_new_msg(smi_info, msg, 2);\n\t\t\tsmi_info->si_state = SI_GETTING_FLAGS;\n\t\t\tgoto restart;\n\t\t}\n\t}\n\n\t/* If we are currently idle, try to start the next message. */\n\tif (si_sm_result == SI_SM_IDLE) {\n\t\tsmi_inc_stat(smi_info, idles);\n\n\t\tsi_sm_result = start_next_msg(smi_info);\n\t\tif (si_sm_result != SI_SM_IDLE)\n\t\t\tgoto restart;\n\t}\n\n\tif ((si_sm_result == SI_SM_IDLE)\n\t    && (atomic_read(&smi_info->req_events))) {\n\t\t/*\n\t\t * We are idle and the upper layer requested that I fetch\n\t\t * events, so do so.\n\t\t */\n\t\tatomic_set(&smi_info->req_events, 0);\n\n\t\t/*\n\t\t * Take this opportunity to check the interrupt and\n\t\t * message enable state for the BMC.  The BMC can be\n\t\t * asynchronously reset, and may thus get interrupts\n\t\t * disable and messages disabled.\n\t\t */\n\t\tif (smi_info->supports_event_msg_buff || smi_info->io.irq) {\n\t\t\tstart_check_enables(smi_info);\n\t\t} else {\n\t\t\tsmi_info->curr_msg = alloc_msg_handle_irq(smi_info);\n\t\t\tif (!smi_info->curr_msg)\n\t\t\t\tgoto out;\n\n\t\t\tstart_getting_events(smi_info);\n\t\t}\n\t\tgoto restart;\n\t}\n\n\tif (si_sm_result == SI_SM_IDLE && smi_info->timer_running) {\n\t\t/* Ok it if fails, the timer will just go off. */\n\t\tif (del_timer(&smi_info->si_timer))\n\t\t\tsmi_info->timer_running = false;\n\t}\n\nout:\n\treturn si_sm_result;\n}\n\nstatic void check_start_timer_thread(struct smi_info *smi_info)\n{\n\tif (smi_info->si_state == SI_NORMAL && smi_info->curr_msg == NULL) {\n\t\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t\tif (smi_info->thread)\n\t\t\twake_up_process(smi_info->thread);\n\n\t\tstart_next_msg(smi_info);\n\t\tsmi_event_handler(smi_info, 0);\n\t}\n}\n\nstatic void flush_messages(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\tenum si_sm_result result;\n\n\t/*\n\t * Currently, this function is called only in run-to-completion\n\t * mode.  This means we are single-threaded, no need for locks.\n\t */\n\tresult = smi_event_handler(smi_info, 0);\n\twhile (result != SI_SM_IDLE) {\n\t\tudelay(SI_SHORT_TIMEOUT_USEC);\n\t\tresult = smi_event_handler(smi_info, SI_SHORT_TIMEOUT_USEC);\n\t}\n}\n\nstatic void sender(void                *send_info,\n\t\t   struct ipmi_smi_msg *msg)\n{\n\tstruct smi_info   *smi_info = send_info;\n\tunsigned long     flags;\n\n\tdebug_timestamp(\"Enqueue\");\n\n\tif (smi_info->run_to_completion) {\n\t\t/*\n\t\t * If we are running to completion, start it.  Upper\n\t\t * layer will call flush_messages to clear it out.\n\t\t */\n\t\tsmi_info->waiting_msg = msg;\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\t/*\n\t * The following two lines don't need to be under the lock for\n\t * the lock's sake, but they do need SMP memory barriers to\n\t * avoid getting things out of order.  We are already claiming\n\t * the lock, anyway, so just do it under the lock to avoid the\n\t * ordering problem.\n\t */\n\tBUG_ON(smi_info->waiting_msg);\n\tsmi_info->waiting_msg = msg;\n\tcheck_start_timer_thread(smi_info);\n\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void set_run_to_completion(void *send_info, bool i_run_to_completion)\n{\n\tstruct smi_info   *smi_info = send_info;\n\n\tsmi_info->run_to_completion = i_run_to_completion;\n\tif (i_run_to_completion)\n\t\tflush_messages(smi_info);\n}\n\n/*\n * Use -1 in the nsec value of the busy waiting timespec to tell that\n * we are spinning in kipmid looking for something and not delaying\n * between checks\n */\nstatic inline void ipmi_si_set_not_busy(struct timespec64 *ts)\n{\n\tts->tv_nsec = -1;\n}\nstatic inline int ipmi_si_is_busy(struct timespec64 *ts)\n{\n\treturn ts->tv_nsec != -1;\n}\n\nstatic inline int ipmi_thread_busy_wait(enum si_sm_result smi_result,\n\t\t\t\t\tconst struct smi_info *smi_info,\n\t\t\t\t\tstruct timespec64 *busy_until)\n{\n\tunsigned int max_busy_us = 0;\n\n\tif (smi_info->si_num < num_max_busy_us)\n\t\tmax_busy_us = kipmid_max_busy_us[smi_info->si_num];\n\tif (max_busy_us == 0 || smi_result != SI_SM_CALL_WITH_DELAY)\n\t\tipmi_si_set_not_busy(busy_until);\n\telse if (!ipmi_si_is_busy(busy_until)) {\n\t\tktime_get_ts64(busy_until);\n\t\ttimespec64_add_ns(busy_until, max_busy_us*NSEC_PER_USEC);\n\t} else {\n\t\tstruct timespec64 now;\n\n\t\tktime_get_ts64(&now);\n\t\tif (unlikely(timespec64_compare(&now, busy_until) > 0)) {\n\t\t\tipmi_si_set_not_busy(busy_until);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn 1;\n}\n\n\n/*\n * A busy-waiting loop for speeding up IPMI operation.\n *\n * Lousy hardware makes this hard.  This is only enabled for systems\n * that are not BT and do not have interrupts.  It starts spinning\n * when an operation is complete or until max_busy tells it to stop\n * (if that is enabled).  See the paragraph on kimid_max_busy_us in\n * Documentation/IPMI.txt for details.\n */\nstatic int ipmi_thread(void *data)\n{\n\tstruct smi_info *smi_info = data;\n\tunsigned long flags;\n\tenum si_sm_result smi_result;\n\tstruct timespec64 busy_until;\n\n\tipmi_si_set_not_busy(&busy_until);\n\tset_user_nice(current, MAX_NICE);\n\twhile (!kthread_should_stop()) {\n\t\tint busy_wait;\n\n\t\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\t\tsmi_result = smi_event_handler(smi_info, 0);\n\n\t\t/*\n\t\t * If the driver is doing something, there is a possible\n\t\t * race with the timer.  If the timer handler see idle,\n\t\t * and the thread here sees something else, the timer\n\t\t * handler won't restart the timer even though it is\n\t\t * required.  So start it here if necessary.\n\t\t */\n\t\tif (smi_result != SI_SM_IDLE && !smi_info->timer_running)\n\t\t\tsmi_mod_timer(smi_info, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n\t\tbusy_wait = ipmi_thread_busy_wait(smi_result, smi_info,\n\t\t\t\t\t\t  &busy_until);\n\t\tif (smi_result == SI_SM_CALL_WITHOUT_DELAY)\n\t\t\t; /* do nothing */\n\t\telse if (smi_result == SI_SM_CALL_WITH_DELAY && busy_wait)\n\t\t\tschedule();\n\t\telse if (smi_result == SI_SM_IDLE) {\n\t\t\tif (atomic_read(&smi_info->need_watch)) {\n\t\t\t\tschedule_timeout_interruptible(100);\n\t\t\t} else {\n\t\t\t\t/* Wait to be woken up when we are needed. */\n\t\t\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\t\t\tschedule();\n\t\t\t}\n\t\t} else\n\t\t\tschedule_timeout_interruptible(1);\n\t}\n\treturn 0;\n}\n\n\nstatic void poll(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\tunsigned long flags = 0;\n\tbool run_to_completion = smi_info->run_to_completion;\n\n\t/*\n\t * Make sure there is some delay in the poll loop so we can\n\t * drive time forward and timeout things.\n\t */\n\tudelay(10);\n\tif (!run_to_completion)\n\t\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\tsmi_event_handler(smi_info, 10);\n\tif (!run_to_completion)\n\t\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void request_events(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\n\tif (!smi_info->has_event_buffer)\n\t\treturn;\n\n\tatomic_set(&smi_info->req_events, 1);\n}\n\nstatic void set_need_watch(void *send_info, bool enable)\n{\n\tstruct smi_info *smi_info = send_info;\n\tunsigned long flags;\n\n\tatomic_set(&smi_info->need_watch, enable);\n\tspin_lock_irqsave(&smi_info->si_lock, flags);\n\tcheck_start_timer_thread(smi_info);\n\tspin_unlock_irqrestore(&smi_info->si_lock, flags);\n}\n\nstatic void smi_timeout(struct timer_list *t)\n{\n\tstruct smi_info   *smi_info = from_timer(smi_info, t, si_timer);\n\tenum si_sm_result smi_result;\n\tunsigned long     flags;\n\tunsigned long     jiffies_now;\n\tlong              time_diff;\n\tlong\t\t  timeout;\n\n\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\tdebug_timestamp(\"Timer\");\n\n\tjiffies_now = jiffies;\n\ttime_diff = (((long)jiffies_now - (long)smi_info->last_timeout_jiffies)\n\t\t     * SI_USEC_PER_JIFFY);\n\tsmi_result = smi_event_handler(smi_info, time_diff);\n\n\tif ((smi_info->io.irq) && (!smi_info->interrupt_disabled)) {\n\t\t/* Running with interrupts, only do long timeouts. */\n\t\ttimeout = jiffies + SI_TIMEOUT_JIFFIES;\n\t\tsmi_inc_stat(smi_info, long_timeouts);\n\t\tgoto do_mod_timer;\n\t}\n\n\t/*\n\t * If the state machine asks for a short delay, then shorten\n\t * the timer timeout.\n\t */\n\tif (smi_result == SI_SM_CALL_WITH_DELAY) {\n\t\tsmi_inc_stat(smi_info, short_timeouts);\n\t\ttimeout = jiffies + 1;\n\t} else {\n\t\tsmi_inc_stat(smi_info, long_timeouts);\n\t\ttimeout = jiffies + SI_TIMEOUT_JIFFIES;\n\t}\n\ndo_mod_timer:\n\tif (smi_result != SI_SM_IDLE)\n\t\tsmi_mod_timer(smi_info, timeout);\n\telse\n\t\tsmi_info->timer_running = false;\n\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n}\n\nirqreturn_t ipmi_si_irq_handler(int irq, void *data)\n{\n\tstruct smi_info *smi_info = data;\n\tunsigned long   flags;\n\n\tif (smi_info->io.si_type == SI_BT)\n\t\t/* We need to clear the IRQ flag for the BT interface. */\n\t\tsmi_info->io.outputb(&smi_info->io, IPMI_BT_INTMASK_REG,\n\t\t\t\t     IPMI_BT_INTMASK_CLEAR_IRQ_BIT\n\t\t\t\t     | IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n\n\tspin_lock_irqsave(&(smi_info->si_lock), flags);\n\n\tsmi_inc_stat(smi_info, interrupts);\n\n\tdebug_timestamp(\"Interrupt\");\n\n\tsmi_event_handler(smi_info, 0);\n\tspin_unlock_irqrestore(&(smi_info->si_lock), flags);\n\treturn IRQ_HANDLED;\n}\n\nstatic int smi_start_processing(void            *send_info,\n\t\t\t\tstruct ipmi_smi *intf)\n{\n\tstruct smi_info *new_smi = send_info;\n\tint             enable = 0;\n\n\tnew_smi->intf = intf;\n\n\t/* Set up the timer that drives the interface. */\n\ttimer_setup(&new_smi->si_timer, smi_timeout, 0);\n\tnew_smi->timer_can_start = true;\n\tsmi_mod_timer(new_smi, jiffies + SI_TIMEOUT_JIFFIES);\n\n\t/* Try to claim any interrupts. */\n\tif (new_smi->io.irq_setup) {\n\t\tnew_smi->io.irq_handler_data = new_smi;\n\t\tnew_smi->io.irq_setup(&new_smi->io);\n\t}\n\n\t/*\n\t * Check if the user forcefully enabled the daemon.\n\t */\n\tif (new_smi->si_num < num_force_kipmid)\n\t\tenable = force_kipmid[new_smi->si_num];\n\t/*\n\t * The BT interface is efficient enough to not need a thread,\n\t * and there is no need for a thread if we have interrupts.\n\t */\n\telse if ((new_smi->io.si_type != SI_BT) && (!new_smi->io.irq))\n\t\tenable = 1;\n\n\tif (enable) {\n\t\tnew_smi->thread = kthread_run(ipmi_thread, new_smi,\n\t\t\t\t\t      \"kipmi%d\", new_smi->si_num);\n\t\tif (IS_ERR(new_smi->thread)) {\n\t\t\tdev_notice(new_smi->io.dev, \"Could not start\"\n\t\t\t\t   \" kernel thread due to error %ld, only using\"\n\t\t\t\t   \" timers to drive the interface\\n\",\n\t\t\t\t   PTR_ERR(new_smi->thread));\n\t\t\tnew_smi->thread = NULL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int get_smi_info(void *send_info, struct ipmi_smi_info *data)\n{\n\tstruct smi_info *smi = send_info;\n\n\tdata->addr_src = smi->io.addr_source;\n\tdata->dev = smi->io.dev;\n\tdata->addr_info = smi->io.addr_info;\n\tget_device(smi->io.dev);\n\n\treturn 0;\n}\n\nstatic void set_maintenance_mode(void *send_info, bool enable)\n{\n\tstruct smi_info   *smi_info = send_info;\n\n\tif (!enable)\n\t\tatomic_set(&smi_info->req_events, 0);\n}\n\nstatic void shutdown_smi(void *send_info);\nstatic const struct ipmi_smi_handlers handlers = {\n\t.owner                  = THIS_MODULE,\n\t.start_processing       = smi_start_processing,\n\t.shutdown               = shutdown_smi,\n\t.get_smi_info\t\t= get_smi_info,\n\t.sender\t\t\t= sender,\n\t.request_events\t\t= request_events,\n\t.set_need_watch\t\t= set_need_watch,\n\t.set_maintenance_mode   = set_maintenance_mode,\n\t.set_run_to_completion  = set_run_to_completion,\n\t.flush_messages\t\t= flush_messages,\n\t.poll\t\t\t= poll,\n};\n\nstatic LIST_HEAD(smi_infos);\nstatic DEFINE_MUTEX(smi_infos_lock);\nstatic int smi_num; /* Used to sequence the SMIs */\n\nstatic const char * const addr_space_to_str[] = { \"i/o\", \"mem\" };\n\nmodule_param_array(force_kipmid, int, &num_force_kipmid, 0);\nMODULE_PARM_DESC(force_kipmid, \"Force the kipmi daemon to be enabled (1) or\"\n\t\t \" disabled(0).  Normally the IPMI driver auto-detects\"\n\t\t \" this, but the value may be overridden by this parm.\");\nmodule_param(unload_when_empty, bool, 0);\nMODULE_PARM_DESC(unload_when_empty, \"Unload the module if no interfaces are\"\n\t\t \" specified or found, default is 1.  Setting to 0\"\n\t\t \" is useful for hot add of devices using hotmod.\");\nmodule_param_array(kipmid_max_busy_us, uint, &num_max_busy_us, 0644);\nMODULE_PARM_DESC(kipmid_max_busy_us,\n\t\t \"Max time (in microseconds) to busy-wait for IPMI data before\"\n\t\t \" sleeping. 0 (default) means to wait forever. Set to 100-500\"\n\t\t \" if kipmid is using up a lot of CPU time.\");\n\nvoid ipmi_irq_finish_setup(struct si_sm_io *io)\n{\n\tif (io->si_type == SI_BT)\n\t\t/* Enable the interrupt in the BT interface. */\n\t\tio->outputb(io, IPMI_BT_INTMASK_REG,\n\t\t\t    IPMI_BT_INTMASK_ENABLE_IRQ_BIT);\n}\n\nvoid ipmi_irq_start_cleanup(struct si_sm_io *io)\n{\n\tif (io->si_type == SI_BT)\n\t\t/* Disable the interrupt in the BT interface. */\n\t\tio->outputb(io, IPMI_BT_INTMASK_REG, 0);\n}\n\nstatic void std_irq_cleanup(struct si_sm_io *io)\n{\n\tipmi_irq_start_cleanup(io);\n\tfree_irq(io->irq, io->irq_handler_data);\n}\n\nint ipmi_std_irq_setup(struct si_sm_io *io)\n{\n\tint rv;\n\n\tif (!io->irq)\n\t\treturn 0;\n\n\trv = request_irq(io->irq,\n\t\t\t ipmi_si_irq_handler,\n\t\t\t IRQF_SHARED,\n\t\t\t DEVICE_NAME,\n\t\t\t io->irq_handler_data);\n\tif (rv) {\n\t\tdev_warn(io->dev, \"%s unable to claim interrupt %d,\"\n\t\t\t \" running polled\\n\",\n\t\t\t DEVICE_NAME, io->irq);\n\t\tio->irq = 0;\n\t} else {\n\t\tio->irq_cleanup = std_irq_cleanup;\n\t\tipmi_irq_finish_setup(io);\n\t\tdev_info(io->dev, \"Using irq %d\\n\", io->irq);\n\t}\n\n\treturn rv;\n}\n\nstatic int wait_for_msg_done(struct smi_info *smi_info)\n{\n\tenum si_sm_result     smi_result;\n\n\tsmi_result = smi_info->handlers->event(smi_info->si_sm, 0);\n\tfor (;;) {\n\t\tif (smi_result == SI_SM_CALL_WITH_DELAY ||\n\t\t    smi_result == SI_SM_CALL_WITH_TICK_DELAY) {\n\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\tsmi_result = smi_info->handlers->event(\n\t\t\t\tsmi_info->si_sm, jiffies_to_usecs(1));\n\t\t} else if (smi_result == SI_SM_CALL_WITHOUT_DELAY) {\n\t\t\tsmi_result = smi_info->handlers->event(\n\t\t\t\tsmi_info->si_sm, 0);\n\t\t} else\n\t\t\tbreak;\n\t}\n\tif (smi_result == SI_SM_HOSED)\n\t\t/*\n\t\t * We couldn't get the state machine to run, so whatever's at\n\t\t * the port is probably not an IPMI SMI interface.\n\t\t */\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\nstatic int try_get_dev_id(struct smi_info *smi_info)\n{\n\tunsigned char         msg[2];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv = 0;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Do a Get Device ID command, since it comes back with some\n\t * useful info.\n\t */\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_DEVICE_ID_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv)\n\t\tgoto out;\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\t/* Check and record info from the get device id, in case we need it. */\n\trv = ipmi_demangle_device_id(resp[0] >> 2, resp[1],\n\t\t\tresp + 2, resp_len - 2, &smi_info->device_id);\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\nstatic int get_global_enables(struct smi_info *smi_info, u8 *enables)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Error getting response from get global enables command: %d\\n\",\n\t\t\t rv);\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 4 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_GET_BMC_GLOBAL_ENABLES_CMD   ||\n\t\t\tresp[2] != 0) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Invalid return from get global enables command: %ld %x %x %x\\n\",\n\t\t\t resp_len, resp[0], resp[1], resp[2]);\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t} else {\n\t\t*enables = resp[3];\n\t}\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n/*\n * Returns 1 if it gets an error from the command.\n */\nstatic int set_global_enables(struct smi_info *smi_info, u8 enables)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\tmsg[2] = enables;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 3);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Error getting response from set global enables command: %d\\n\",\n\t\t\t rv);\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 3 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_SET_BMC_GLOBAL_ENABLES_CMD) {\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"Invalid return from set global enables command: %ld %x %x\\n\",\n\t\t\t resp_len, resp[0], resp[1]);\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[2] != 0)\n\t\trv = 1;\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n/*\n * Some BMCs do not support clearing the receive irq bit in the global\n * enables (even if they don't support interrupts on the BMC).  Check\n * for this and handle it properly.\n */\nstatic void check_clr_rcv_irq(struct smi_info *smi_info)\n{\n\tu8 enables = 0;\n\tint rv;\n\n\trv = get_global_enables(smi_info, &enables);\n\tif (!rv) {\n\t\tif ((enables & IPMI_BMC_RCV_MSG_INTR) == 0)\n\t\t\t/* Already clear, should work ok. */\n\t\t\treturn;\n\n\t\tenables &= ~IPMI_BMC_RCV_MSG_INTR;\n\t\trv = set_global_enables(smi_info, enables);\n\t}\n\n\tif (rv < 0) {\n\t\tdev_err(smi_info->io.dev,\n\t\t\t\"Cannot check clearing the rcv irq: %d\\n\", rv);\n\t\treturn;\n\t}\n\n\tif (rv) {\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * clearing the bit is not supported.\n\t\t */\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"The BMC does not support clearing the recv irq bit, compensating, but the BMC needs to be fixed.\\n\");\n\t\tsmi_info->cannot_disable_irq = true;\n\t}\n}\n\n/*\n * Some BMCs do not support setting the interrupt bits in the global\n * enables even if they support interrupts.  Clearly bad, but we can\n * compensate.\n */\nstatic void check_set_rcv_irq(struct smi_info *smi_info)\n{\n\tu8 enables = 0;\n\tint rv;\n\n\tif (!smi_info->io.irq)\n\t\treturn;\n\n\trv = get_global_enables(smi_info, &enables);\n\tif (!rv) {\n\t\tenables |= IPMI_BMC_RCV_MSG_INTR;\n\t\trv = set_global_enables(smi_info, enables);\n\t}\n\n\tif (rv < 0) {\n\t\tdev_err(smi_info->io.dev,\n\t\t\t\"Cannot check setting the rcv irq: %d\\n\", rv);\n\t\treturn;\n\t}\n\n\tif (rv) {\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * setting the bit is not supported.\n\t\t */\n\t\tdev_warn(smi_info->io.dev,\n\t\t\t \"The BMC does not support setting the recv irq bit, compensating, but the BMC needs to be fixed.\\n\");\n\t\tsmi_info->cannot_disable_irq = true;\n\t\tsmi_info->irq_enable_broken = true;\n\t}\n}\n\nstatic int try_enable_event_buffer(struct smi_info *smi_info)\n{\n\tunsigned char         msg[3];\n\tunsigned char         *resp;\n\tunsigned long         resp_len;\n\tint                   rv = 0;\n\n\tresp = kmalloc(IPMI_MAX_MSG_LENGTH, GFP_KERNEL);\n\tif (!resp)\n\t\treturn -ENOMEM;\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_GET_BMC_GLOBAL_ENABLES_CMD;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 2);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tpr_warn(\"Error getting response from get global enables command, the event buffer is not enabled\\n\");\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 4 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_GET_BMC_GLOBAL_ENABLES_CMD   ||\n\t\t\tresp[2] != 0) {\n\t\tpr_warn(\"Invalid return from get global enables command, cannot enable the event buffer\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[3] & IPMI_BMC_EVT_MSG_BUFF) {\n\t\t/* buffer is already enabled, nothing to do. */\n\t\tsmi_info->supports_event_msg_buff = true;\n\t\tgoto out;\n\t}\n\n\tmsg[0] = IPMI_NETFN_APP_REQUEST << 2;\n\tmsg[1] = IPMI_SET_BMC_GLOBAL_ENABLES_CMD;\n\tmsg[2] = resp[3] | IPMI_BMC_EVT_MSG_BUFF;\n\tsmi_info->handlers->start_transaction(smi_info->si_sm, msg, 3);\n\n\trv = wait_for_msg_done(smi_info);\n\tif (rv) {\n\t\tpr_warn(\"Error getting response from set global, enables command, the event buffer is not enabled\\n\");\n\t\tgoto out;\n\t}\n\n\tresp_len = smi_info->handlers->get_result(smi_info->si_sm,\n\t\t\t\t\t\t  resp, IPMI_MAX_MSG_LENGTH);\n\n\tif (resp_len < 3 ||\n\t\t\tresp[0] != (IPMI_NETFN_APP_REQUEST | 1) << 2 ||\n\t\t\tresp[1] != IPMI_SET_BMC_GLOBAL_ENABLES_CMD) {\n\t\tpr_warn(\"Invalid return from get global, enables command, not enable the event buffer\\n\");\n\t\trv = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (resp[2] != 0)\n\t\t/*\n\t\t * An error when setting the event buffer bit means\n\t\t * that the event buffer is not supported.\n\t\t */\n\t\trv = -ENOENT;\n\telse\n\t\tsmi_info->supports_event_msg_buff = true;\n\nout:\n\tkfree(resp);\n\treturn rv;\n}\n\n#define IPMI_SI_ATTR(name) \\\nstatic ssize_t ipmi_##name##_show(struct device *dev,\t\t\t\\\n\t\t\t\t  struct device_attribute *attr,\t\\\n\t\t\t\t  char *buf)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn snprintf(buf, 10, \"%u\\n\", smi_get_stat(smi_info, name));\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic DEVICE_ATTR(name, S_IRUGO, ipmi_##name##_show, NULL)\n\nstatic ssize_t ipmi_type_show(struct device *dev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\n\treturn snprintf(buf, 10, \"%s\\n\", si_to_str[smi_info->io.si_type]);\n}\nstatic DEVICE_ATTR(type, S_IRUGO, ipmi_type_show, NULL);\n\nstatic ssize_t ipmi_interrupts_enabled_show(struct device *dev,\n\t\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t\t    char *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\tint enabled = smi_info->io.irq && !smi_info->interrupt_disabled;\n\n\treturn snprintf(buf, 10, \"%d\\n\", enabled);\n}\nstatic DEVICE_ATTR(interrupts_enabled, S_IRUGO,\n\t\t   ipmi_interrupts_enabled_show, NULL);\n\nIPMI_SI_ATTR(short_timeouts);\nIPMI_SI_ATTR(long_timeouts);\nIPMI_SI_ATTR(idles);\nIPMI_SI_ATTR(interrupts);\nIPMI_SI_ATTR(attentions);\nIPMI_SI_ATTR(flag_fetches);\nIPMI_SI_ATTR(hosed_count);\nIPMI_SI_ATTR(complete_transactions);\nIPMI_SI_ATTR(events);\nIPMI_SI_ATTR(watchdog_pretimeouts);\nIPMI_SI_ATTR(incoming_messages);\n\nstatic ssize_t ipmi_params_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr,\n\t\t\t\tchar *buf)\n{\n\tstruct smi_info *smi_info = dev_get_drvdata(dev);\n\n\treturn snprintf(buf, 200,\n\t\t\t\"%s,%s,0x%lx,rsp=%d,rsi=%d,rsh=%d,irq=%d,ipmb=%d\\n\",\n\t\t\tsi_to_str[smi_info->io.si_type],\n\t\t\taddr_space_to_str[smi_info->io.addr_type],\n\t\t\tsmi_info->io.addr_data,\n\t\t\tsmi_info->io.regspacing,\n\t\t\tsmi_info->io.regsize,\n\t\t\tsmi_info->io.regshift,\n\t\t\tsmi_info->io.irq,\n\t\t\tsmi_info->io.slave_addr);\n}\nstatic DEVICE_ATTR(params, S_IRUGO, ipmi_params_show, NULL);\n\nstatic struct attribute *ipmi_si_dev_attrs[] = {\n\t&dev_attr_type.attr,\n\t&dev_attr_interrupts_enabled.attr,\n\t&dev_attr_short_timeouts.attr,\n\t&dev_attr_long_timeouts.attr,\n\t&dev_attr_idles.attr,\n\t&dev_attr_interrupts.attr,\n\t&dev_attr_attentions.attr,\n\t&dev_attr_flag_fetches.attr,\n\t&dev_attr_hosed_count.attr,\n\t&dev_attr_complete_transactions.attr,\n\t&dev_attr_events.attr,\n\t&dev_attr_watchdog_pretimeouts.attr,\n\t&dev_attr_incoming_messages.attr,\n\t&dev_attr_params.attr,\n\tNULL\n};\n\nstatic const struct attribute_group ipmi_si_dev_attr_group = {\n\t.attrs\t\t= ipmi_si_dev_attrs,\n};\n\n/*\n * oem_data_avail_to_receive_msg_avail\n * @info - smi_info structure with msg_flags set\n *\n * Converts flags from OEM_DATA_AVAIL to RECEIVE_MSG_AVAIL\n * Returns 1 indicating need to re-run handle_flags().\n */\nstatic int oem_data_avail_to_receive_msg_avail(struct smi_info *smi_info)\n{\n\tsmi_info->msg_flags = ((smi_info->msg_flags & ~OEM_DATA_AVAIL) |\n\t\t\t       RECEIVE_MSG_AVAIL);\n\treturn 1;\n}\n\n/*\n * setup_dell_poweredge_oem_data_handler\n * @info - smi_info.device_id must be populated\n *\n * Systems that match, but have firmware version < 1.40 may assert\n * OEM0_DATA_AVAIL on their own, without being told via Set Flags that\n * it's safe to do so.  Such systems will de-assert OEM1_DATA_AVAIL\n * upon receipt of IPMI_GET_MSG_CMD, so we should treat these flags\n * as RECEIVE_MSG_AVAIL instead.\n *\n * As Dell has no plans to release IPMI 1.5 firmware that *ever*\n * assert the OEM[012] bits, and if it did, the driver would have to\n * change to handle that properly, we don't actually check for the\n * firmware version.\n * Device ID = 0x20                BMC on PowerEdge 8G servers\n * Device Revision = 0x80\n * Firmware Revision1 = 0x01       BMC version 1.40\n * Firmware Revision2 = 0x40       BCD encoded\n * IPMI Version = 0x51             IPMI 1.5\n * Manufacturer ID = A2 02 00      Dell IANA\n *\n * Additionally, PowerEdge systems with IPMI < 1.5 may also assert\n * OEM0_DATA_AVAIL and needs to be treated as RECEIVE_MSG_AVAIL.\n *\n */\n#define DELL_POWEREDGE_8G_BMC_DEVICE_ID  0x20\n#define DELL_POWEREDGE_8G_BMC_DEVICE_REV 0x80\n#define DELL_POWEREDGE_8G_BMC_IPMI_VERSION 0x51\n#define DELL_IANA_MFR_ID 0x0002a2\nstatic void setup_dell_poweredge_oem_data_handler(struct smi_info *smi_info)\n{\n\tstruct ipmi_device_id *id = &smi_info->device_id;\n\tif (id->manufacturer_id == DELL_IANA_MFR_ID) {\n\t\tif (id->device_id       == DELL_POWEREDGE_8G_BMC_DEVICE_ID  &&\n\t\t    id->device_revision == DELL_POWEREDGE_8G_BMC_DEVICE_REV &&\n\t\t    id->ipmi_version   == DELL_POWEREDGE_8G_BMC_IPMI_VERSION) {\n\t\t\tsmi_info->oem_data_avail_handler =\n\t\t\t\toem_data_avail_to_receive_msg_avail;\n\t\t} else if (ipmi_version_major(id) < 1 ||\n\t\t\t   (ipmi_version_major(id) == 1 &&\n\t\t\t    ipmi_version_minor(id) < 5)) {\n\t\t\tsmi_info->oem_data_avail_handler =\n\t\t\t\toem_data_avail_to_receive_msg_avail;\n\t\t}\n\t}\n}\n\n#define CANNOT_RETURN_REQUESTED_LENGTH 0xCA\nstatic void return_hosed_msg_badsize(struct smi_info *smi_info)\n{\n\tstruct ipmi_smi_msg *msg = smi_info->curr_msg;\n\n\t/* Make it a response */\n\tmsg->rsp[0] = msg->data[0] | 4;\n\tmsg->rsp[1] = msg->data[1];\n\tmsg->rsp[2] = CANNOT_RETURN_REQUESTED_LENGTH;\n\tmsg->rsp_size = 3;\n\tsmi_info->curr_msg = NULL;\n\tdeliver_recv_msg(smi_info, msg);\n}\n\n/*\n * dell_poweredge_bt_xaction_handler\n * @info - smi_info.device_id must be populated\n *\n * Dell PowerEdge servers with the BT interface (x6xx and 1750) will\n * not respond to a Get SDR command if the length of the data\n * requested is exactly 0x3A, which leads to command timeouts and no\n * data returned.  This intercepts such commands, and causes userspace\n * callers to try again with a different-sized buffer, which succeeds.\n */\n\n#define STORAGE_NETFN 0x0A\n#define STORAGE_CMD_GET_SDR 0x23\nstatic int dell_poweredge_bt_xaction_handler(struct notifier_block *self,\n\t\t\t\t\t     unsigned long unused,\n\t\t\t\t\t     void *in)\n{\n\tstruct smi_info *smi_info = in;\n\tunsigned char *data = smi_info->curr_msg->data;\n\tunsigned int size   = smi_info->curr_msg->data_size;\n\tif (size >= 8 &&\n\t    (data[0]>>2) == STORAGE_NETFN &&\n\t    data[1] == STORAGE_CMD_GET_SDR &&\n\t    data[7] == 0x3A) {\n\t\treturn_hosed_msg_badsize(smi_info);\n\t\treturn NOTIFY_STOP;\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block dell_poweredge_bt_xaction_notifier = {\n\t.notifier_call\t= dell_poweredge_bt_xaction_handler,\n};\n\n/*\n * setup_dell_poweredge_bt_xaction_handler\n * @info - smi_info.device_id must be filled in already\n *\n * Fills in smi_info.device_id.start_transaction_pre_hook\n * when we know what function to use there.\n */\nstatic void\nsetup_dell_poweredge_bt_xaction_handler(struct smi_info *smi_info)\n{\n\tstruct ipmi_device_id *id = &smi_info->device_id;\n\tif (id->manufacturer_id == DELL_IANA_MFR_ID &&\n\t    smi_info->io.si_type == SI_BT)\n\t\tregister_xaction_notifier(&dell_poweredge_bt_xaction_notifier);\n}\n\n/*\n * setup_oem_data_handler\n * @info - smi_info.device_id must be filled in already\n *\n * Fills in smi_info.device_id.oem_data_available_handler\n * when we know what function to use there.\n */\n\nstatic void setup_oem_data_handler(struct smi_info *smi_info)\n{\n\tsetup_dell_poweredge_oem_data_handler(smi_info);\n}\n\nstatic void setup_xaction_handlers(struct smi_info *smi_info)\n{\n\tsetup_dell_poweredge_bt_xaction_handler(smi_info);\n}\n\nstatic void check_for_broken_irqs(struct smi_info *smi_info)\n{\n\tcheck_clr_rcv_irq(smi_info);\n\tcheck_set_rcv_irq(smi_info);\n}\n\nstatic inline void stop_timer_and_thread(struct smi_info *smi_info)\n{\n\tif (smi_info->thread != NULL) {\n\t\tkthread_stop(smi_info->thread);\n\t\tsmi_info->thread = NULL;\n\t}\n\n\tsmi_info->timer_can_start = false;\n\tif (smi_info->timer_running)\n\t\tdel_timer_sync(&smi_info->si_timer);\n}\n\nstatic struct smi_info *find_dup_si(struct smi_info *info)\n{\n\tstruct smi_info *e;\n\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (e->io.addr_type != info->io.addr_type)\n\t\t\tcontinue;\n\t\tif (e->io.addr_data == info->io.addr_data) {\n\t\t\t/*\n\t\t\t * This is a cheap hack, ACPI doesn't have a defined\n\t\t\t * slave address but SMBIOS does.  Pick it up from\n\t\t\t * any source that has it available.\n\t\t\t */\n\t\t\tif (info->io.slave_addr && !e->io.slave_addr)\n\t\t\t\te->io.slave_addr = info->io.slave_addr;\n\t\t\treturn e;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nint ipmi_si_add_smi(struct si_sm_io *io)\n{\n\tint rv = 0;\n\tstruct smi_info *new_smi, *dup;\n\n\tif (!io->io_setup) {\n\t\tif (io->addr_type == IPMI_IO_ADDR_SPACE) {\n\t\t\tio->io_setup = ipmi_si_port_setup;\n\t\t} else if (io->addr_type == IPMI_MEM_ADDR_SPACE) {\n\t\t\tio->io_setup = ipmi_si_mem_setup;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tnew_smi = kzalloc(sizeof(*new_smi), GFP_KERNEL);\n\tif (!new_smi)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&new_smi->si_lock);\n\n\tnew_smi->io = *io;\n\n\tmutex_lock(&smi_infos_lock);\n\tdup = find_dup_si(new_smi);\n\tif (dup) {\n\t\tif (new_smi->io.addr_source == SI_ACPI &&\n\t\t    dup->io.addr_source == SI_SMBIOS) {\n\t\t\t/* We prefer ACPI over SMBIOS. */\n\t\t\tdev_info(dup->io.dev,\n\t\t\t\t \"Removing SMBIOS-specified %s state machine in favor of ACPI\\n\",\n\t\t\t\t si_to_str[new_smi->io.si_type]);\n\t\t\tcleanup_one_si(dup);\n\t\t} else {\n\t\t\tdev_info(new_smi->io.dev,\n\t\t\t\t \"%s-specified %s state machine: duplicate\\n\",\n\t\t\t\t ipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\t\t\t si_to_str[new_smi->io.si_type]);\n\t\t\trv = -EBUSY;\n\t\t\tkfree(new_smi);\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\tpr_info(\"Adding %s-specified %s state machine\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type]);\n\n\tlist_add_tail(&new_smi->link, &smi_infos);\n\n\tif (initialized)\n\t\trv = try_smi_init(new_smi);\nout_err:\n\tmutex_unlock(&smi_infos_lock);\n\treturn rv;\n}\n\n/*\n * Try to start up an interface.  Must be called with smi_infos_lock\n * held, primarily to keep smi_num consistent, we only one to do these\n * one at a time.\n */\nstatic int try_smi_init(struct smi_info *new_smi)\n{\n\tint rv = 0;\n\tint i;\n\tchar *init_name = NULL;\n\n\tpr_info(\"Trying %s-specified %s state machine at %s address 0x%lx, slave address 0x%x, irq %d\\n\",\n\t\tipmi_addr_src_to_str(new_smi->io.addr_source),\n\t\tsi_to_str[new_smi->io.si_type],\n\t\taddr_space_to_str[new_smi->io.addr_type],\n\t\tnew_smi->io.addr_data,\n\t\tnew_smi->io.slave_addr, new_smi->io.irq);\n\n\tswitch (new_smi->io.si_type) {\n\tcase SI_KCS:\n\t\tnew_smi->handlers = &kcs_smi_handlers;\n\t\tbreak;\n\n\tcase SI_SMIC:\n\t\tnew_smi->handlers = &smic_smi_handlers;\n\t\tbreak;\n\n\tcase SI_BT:\n\t\tnew_smi->handlers = &bt_smi_handlers;\n\t\tbreak;\n\n\tdefault:\n\t\t/* No support for anything else yet. */\n\t\trv = -EIO;\n\t\tgoto out_err;\n\t}\n\n\tnew_smi->si_num = smi_num;\n\n\t/* Do this early so it's available for logs. */\n\tif (!new_smi->io.dev) {\n\t\tinit_name = kasprintf(GFP_KERNEL, \"ipmi_si.%d\",\n\t\t\t\t      new_smi->si_num);\n\n\t\t/*\n\t\t * If we don't already have a device from something\n\t\t * else (like PCI), then register a new one.\n\t\t */\n\t\tnew_smi->pdev = platform_device_alloc(\"ipmi_si\",\n\t\t\t\t\t\t      new_smi->si_num);\n\t\tif (!new_smi->pdev) {\n\t\t\tpr_err(\"Unable to allocate platform device\\n\");\n\t\t\trv = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->io.dev = &new_smi->pdev->dev;\n\t\tnew_smi->io.dev->driver = &ipmi_platform_driver.driver;\n\t\t/* Nulled by device_add() */\n\t\tnew_smi->io.dev->init_name = init_name;\n\t}\n\n\t/* Allocate the state machine's data and initialize it. */\n\tnew_smi->si_sm = kmalloc(new_smi->handlers->size(), GFP_KERNEL);\n\tif (!new_smi->si_sm) {\n\t\trv = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\tnew_smi->io.io_size = new_smi->handlers->init_data(new_smi->si_sm,\n\t\t\t\t\t\t\t   &new_smi->io);\n\n\t/* Now that we know the I/O size, we can set up the I/O. */\n\trv = new_smi->io.io_setup(&new_smi->io);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev, \"Could not set up I/O space\\n\");\n\t\tgoto out_err;\n\t}\n\n\t/* Do low-level detection first. */\n\tif (new_smi->handlers->detect(new_smi->si_sm)) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Interface detection failed\\n\");\n\t\trv = -ENODEV;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Attempt a get device id command.  If it fails, we probably\n\t * don't have a BMC here.\n\t */\n\trv = try_get_dev_id(new_smi);\n\tif (rv) {\n\t\tif (new_smi->io.addr_source)\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t       \"There appears to be no BMC at this location\\n\");\n\t\tgoto out_err;\n\t}\n\n\tsetup_oem_data_handler(new_smi);\n\tsetup_xaction_handlers(new_smi);\n\tcheck_for_broken_irqs(new_smi);\n\n\tnew_smi->waiting_msg = NULL;\n\tnew_smi->curr_msg = NULL;\n\tatomic_set(&new_smi->req_events, 0);\n\tnew_smi->run_to_completion = false;\n\tfor (i = 0; i < SI_NUM_STATS; i++)\n\t\tatomic_set(&new_smi->stats[i], 0);\n\n\tnew_smi->interrupt_disabled = true;\n\tatomic_set(&new_smi->need_watch, 0);\n\n\trv = try_enable_event_buffer(new_smi);\n\tif (rv == 0)\n\t\tnew_smi->has_event_buffer = true;\n\n\t/*\n\t * Start clearing the flags before we enable interrupts or the\n\t * timer to avoid racing with the timer.\n\t */\n\tstart_clear_flags(new_smi);\n\n\t/*\n\t * IRQ is defined to be set when non-zero.  req_events will\n\t * cause a global flags check that will enable interrupts.\n\t */\n\tif (new_smi->io.irq) {\n\t\tnew_smi->interrupt_disabled = false;\n\t\tatomic_set(&new_smi->req_events, 1);\n\t}\n\n\tif (new_smi->pdev && !new_smi->pdev_registered) {\n\t\trv = platform_device_add(new_smi->pdev);\n\t\tif (rv) {\n\t\t\tdev_err(new_smi->io.dev,\n\t\t\t\t\"Unable to register system interface device: %d\\n\",\n\t\t\t\trv);\n\t\t\tgoto out_err;\n\t\t}\n\t\tnew_smi->pdev_registered = true;\n\t}\n\n\tdev_set_drvdata(new_smi->io.dev, new_smi);\n\trv = device_add_group(new_smi->io.dev, &ipmi_si_dev_attr_group);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to add device attributes: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\tnew_smi->dev_group_added = true;\n\n\trv = ipmi_register_smi(&handlers,\n\t\t\t       new_smi,\n\t\t\t       new_smi->io.dev,\n\t\t\t       new_smi->io.slave_addr);\n\tif (rv) {\n\t\tdev_err(new_smi->io.dev,\n\t\t\t\"Unable to register device: error %d\\n\",\n\t\t\trv);\n\t\tgoto out_err;\n\t}\n\n\t/* Don't increment till we know we have succeeded. */\n\tsmi_num++;\n\n\tdev_info(new_smi->io.dev, \"IPMI %s interface initialized\\n\",\n\t\t si_to_str[new_smi->io.si_type]);\n\n\tWARN_ON(new_smi->io.dev->init_name != NULL);\n\n out_err:\n\tif (rv && new_smi->io.io_cleanup) {\n\t\tnew_smi->io.io_cleanup(&new_smi->io);\n\t\tnew_smi->io.io_cleanup = NULL;\n\t}\n\n\tkfree(init_name);\n\treturn rv;\n}\n\nstatic int init_ipmi_si(void)\n{\n\tstruct smi_info *e;\n\tenum ipmi_addr_src type = SI_INVALID;\n\n\tif (initialized)\n\t\treturn 0;\n\n\tpr_info(\"IPMI System Interface driver\\n\");\n\n\t/* If the user gave us a device, they presumably want us to use it */\n\tif (!ipmi_si_hardcode_find_bmc())\n\t\tgoto do_scan;\n\n\tipmi_si_platform_init();\n\n\tipmi_si_pci_init();\n\n\tipmi_si_parisc_init();\n\n\t/* We prefer devices with interrupts, but in the case of a machine\n\t   with multiple BMCs we assume that there will be several instances\n\t   of a given type so if we succeed in registering a type then also\n\t   try to register everything else of the same type */\ndo_scan:\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\t/* Try to register a device if it has an IRQ and we either\n\t\t   haven't successfully registered a device yet or this\n\t\t   device has the same type as one we successfully registered */\n\t\tif (e->io.irq && (!type || e->io.addr_source == type)) {\n\t\t\tif (!try_smi_init(e)) {\n\t\t\t\ttype = e->io.addr_source;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* type will only have been set if we successfully registered an si */\n\tif (type)\n\t\tgoto skip_fallback_noirq;\n\n\t/* Fall back to the preferred device */\n\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (!e->io.irq && (!type || e->io.addr_source == type)) {\n\t\t\tif (!try_smi_init(e)) {\n\t\t\t\ttype = e->io.addr_source;\n\t\t\t}\n\t\t}\n\t}\n\nskip_fallback_noirq:\n\tinitialized = 1;\n\tmutex_unlock(&smi_infos_lock);\n\n\tif (type)\n\t\treturn 0;\n\n\tmutex_lock(&smi_infos_lock);\n\tif (unload_when_empty && list_empty(&smi_infos)) {\n\t\tmutex_unlock(&smi_infos_lock);\n\t\tcleanup_ipmi_si();\n\t\tpr_warn(\"Unable to find any System Interface(s)\\n\");\n\t\treturn -ENODEV;\n\t} else {\n\t\tmutex_unlock(&smi_infos_lock);\n\t\treturn 0;\n\t}\n}\nmodule_init(init_ipmi_si);\n\nstatic void shutdown_smi(void *send_info)\n{\n\tstruct smi_info *smi_info = send_info;\n\n\tif (smi_info->dev_group_added) {\n\t\tdevice_remove_group(smi_info->io.dev, &ipmi_si_dev_attr_group);\n\t\tsmi_info->dev_group_added = false;\n\t}\n\tif (smi_info->io.dev)\n\t\tdev_set_drvdata(smi_info->io.dev, NULL);\n\n\t/*\n\t * Make sure that interrupts, the timer and the thread are\n\t * stopped and will not run again.\n\t */\n\tsmi_info->interrupt_disabled = true;\n\tif (smi_info->io.irq_cleanup) {\n\t\tsmi_info->io.irq_cleanup(&smi_info->io);\n\t\tsmi_info->io.irq_cleanup = NULL;\n\t}\n\tstop_timer_and_thread(smi_info);\n\n\t/*\n\t * Wait until we know that we are out of any interrupt\n\t * handlers might have been running before we freed the\n\t * interrupt.\n\t */\n\tsynchronize_rcu();\n\n\t/*\n\t * Timeouts are stopped, now make sure the interrupts are off\n\t * in the BMC.  Note that timers and CPU interrupts are off,\n\t * so no need for locks.\n\t */\n\twhile (smi_info->curr_msg || (smi_info->si_state != SI_NORMAL)) {\n\t\tpoll(smi_info);\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\tif (smi_info->handlers)\n\t\tdisable_si_irq(smi_info);\n\twhile (smi_info->curr_msg || (smi_info->si_state != SI_NORMAL)) {\n\t\tpoll(smi_info);\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\tif (smi_info->handlers)\n\t\tsmi_info->handlers->cleanup(smi_info->si_sm);\n\n\tif (smi_info->io.addr_source_cleanup) {\n\t\tsmi_info->io.addr_source_cleanup(&smi_info->io);\n\t\tsmi_info->io.addr_source_cleanup = NULL;\n\t}\n\tif (smi_info->io.io_cleanup) {\n\t\tsmi_info->io.io_cleanup(&smi_info->io);\n\t\tsmi_info->io.io_cleanup = NULL;\n\t}\n\n\tkfree(smi_info->si_sm);\n\tsmi_info->si_sm = NULL;\n\n\tsmi_info->intf = NULL;\n}\n\n/*\n * Must be called with smi_infos_lock held, to serialize the\n * smi_info->intf check.\n */\nstatic void cleanup_one_si(struct smi_info *smi_info)\n{\n\tif (!smi_info)\n\t\treturn;\n\n\tlist_del(&smi_info->link);\n\n\tif (smi_info->intf)\n\t\tipmi_unregister_smi(smi_info->intf);\n\n\tif (smi_info->pdev) {\n\t\tif (smi_info->pdev_registered)\n\t\t\tplatform_device_unregister(smi_info->pdev);\n\t\telse\n\t\t\tplatform_device_put(smi_info->pdev);\n\t}\n\n\tkfree(smi_info);\n}\n\nint ipmi_si_remove_by_dev(struct device *dev)\n{\n\tstruct smi_info *e;\n\tint rv = -ENOENT;\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry(e, &smi_infos, link) {\n\t\tif (e->io.dev == dev) {\n\t\t\tcleanup_one_si(e);\n\t\t\trv = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&smi_infos_lock);\n\n\treturn rv;\n}\n\nvoid ipmi_si_remove_by_data(int addr_space, enum si_type si_type,\n\t\t\t    unsigned long addr)\n{\n\t/* remove */\n\tstruct smi_info *e, *tmp_e;\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry_safe(e, tmp_e, &smi_infos, link) {\n\t\tif (e->io.addr_type != addr_space)\n\t\t\tcontinue;\n\t\tif (e->io.si_type != si_type)\n\t\t\tcontinue;\n\t\tif (e->io.addr_data == addr)\n\t\t\tcleanup_one_si(e);\n\t}\n\tmutex_unlock(&smi_infos_lock);\n}\n\nstatic void cleanup_ipmi_si(void)\n{\n\tstruct smi_info *e, *tmp_e;\n\n\tif (!initialized)\n\t\treturn;\n\n\tipmi_si_pci_shutdown();\n\n\tipmi_si_parisc_shutdown();\n\n\tipmi_si_platform_shutdown();\n\n\tmutex_lock(&smi_infos_lock);\n\tlist_for_each_entry_safe(e, tmp_e, &smi_infos, link)\n\t\tcleanup_one_si(e);\n\tmutex_unlock(&smi_infos_lock);\n}\nmodule_exit(cleanup_ipmi_si);\n\nMODULE_ALIAS(\"platform:dmi-ipmi-si\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Corey Minyard <minyard@mvista.com>\");\nMODULE_DESCRIPTION(\"Interface to the IPMI driver for the KCS, SMIC, and BT\"\n\t\t   \" system interfaces.\");\n", "// SPDX-License-Identifier: GPL-2.0+\n\n#include <linux/io.h>\n#include \"ipmi_si.h\"\n\nstatic unsigned char intf_mem_inb(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn readb((io->addr)+(offset * io->regspacing));\n}\n\nstatic void intf_mem_outb(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twriteb(b, (io->addr)+(offset * io->regspacing));\n}\n\nstatic unsigned char intf_mem_inw(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn (readw((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void intf_mem_outw(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twriteb(b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n\nstatic unsigned char intf_mem_inl(const struct si_sm_io *io,\n\t\t\t\t  unsigned int offset)\n{\n\treturn (readl((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void intf_mem_outl(const struct si_sm_io *io, unsigned int offset,\n\t\t\t  unsigned char b)\n{\n\twritel(b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n\n#ifdef readq\nstatic unsigned char mem_inq(const struct si_sm_io *io, unsigned int offset)\n{\n\treturn (readq((io->addr)+(offset * io->regspacing)) >> io->regshift)\n\t\t& 0xff;\n}\n\nstatic void mem_outq(const struct si_sm_io *io, unsigned int offset,\n\t\t     unsigned char b)\n{\n\twriteq((u64)b << io->regshift, (io->addr)+(offset * io->regspacing));\n}\n#endif\n\nstatic void mem_region_cleanup(struct si_sm_io *io, int num)\n{\n\tunsigned long addr = io->addr_data;\n\tint idx;\n\n\tfor (idx = 0; idx < num; idx++)\n\t\trelease_mem_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize);\n}\n\nstatic void mem_cleanup(struct si_sm_io *io)\n{\n\tif (io->addr) {\n\t\tiounmap(io->addr);\n\t\tmem_region_cleanup(io, io->io_size);\n\t}\n}\n\nint ipmi_si_mem_setup(struct si_sm_io *io)\n{\n\tunsigned long addr = io->addr_data;\n\tint           mapsize, idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual readb/readw/readl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = intf_mem_inb;\n\t\tio->outputb = intf_mem_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = intf_mem_inw;\n\t\tio->outputb = intf_mem_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = intf_mem_inl;\n\t\tio->outputb = intf_mem_outl;\n\t\tbreak;\n#ifdef readq\n\tcase 8:\n\t\tio->inputb = mem_inq;\n\t\tio->outputb = mem_outq;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint memory regions in their ACPI\n\t * tables.  This causes problems when trying to request the\n\t * entire region.  Therefore we must request each register\n\t * separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_mem_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\tmem_region_cleanup(io, idx);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t/*\n\t * Calculate the total amount of memory to claim.  This is an\n\t * unusual looking calculation, but it avoids claiming any\n\t * more memory than it has to.  It will claim everything\n\t * between the first address to the end of the last full\n\t * register.\n\t */\n\tmapsize = ((io->io_size * io->regspacing)\n\t\t   - (io->regspacing - io->regsize));\n\tio->addr = ioremap(addr, mapsize);\n\tif (io->addr == NULL) {\n\t\tmem_region_cleanup(io, io->io_size);\n\t\treturn -EIO;\n\t}\n\n\tio->io_cleanup = mem_cleanup;\n\n\treturn 0;\n}\n", "// SPDX-License-Identifier: GPL-2.0+\n\n#include <linux/io.h>\n#include \"ipmi_si.h\"\n\nstatic unsigned char port_inb(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn inb(addr + (offset * io->regspacing));\n}\n\nstatic void port_outb(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutb(b, addr + (offset * io->regspacing));\n}\n\nstatic unsigned char port_inw(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn (inw(addr + (offset * io->regspacing)) >> io->regshift) & 0xff;\n}\n\nstatic void port_outw(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutw(b << io->regshift, addr + (offset * io->regspacing));\n}\n\nstatic unsigned char port_inl(const struct si_sm_io *io, unsigned int offset)\n{\n\tunsigned int addr = io->addr_data;\n\n\treturn (inl(addr + (offset * io->regspacing)) >> io->regshift) & 0xff;\n}\n\nstatic void port_outl(const struct si_sm_io *io, unsigned int offset,\n\t\t      unsigned char b)\n{\n\tunsigned int addr = io->addr_data;\n\n\toutl(b << io->regshift, addr+(offset * io->regspacing));\n}\n\nstatic void port_cleanup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (addr) {\n\t\tfor (idx = 0; idx < io->io_size; idx++)\n\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t       io->regsize);\n\t}\n}\n\nint ipmi_si_port_setup(struct si_sm_io *io)\n{\n\tunsigned int addr = io->addr_data;\n\tint          idx;\n\n\tif (!addr)\n\t\treturn -ENODEV;\n\n\t/*\n\t * Figure out the actual inb/inw/inl/etc routine to use based\n\t * upon the register size.\n\t */\n\tswitch (io->regsize) {\n\tcase 1:\n\t\tio->inputb = port_inb;\n\t\tio->outputb = port_outb;\n\t\tbreak;\n\tcase 2:\n\t\tio->inputb = port_inw;\n\t\tio->outputb = port_outw;\n\t\tbreak;\n\tcase 4:\n\t\tio->inputb = port_inl;\n\t\tio->outputb = port_outl;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(io->dev, \"Invalid register size: %d\\n\",\n\t\t\t io->regsize);\n\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Some BIOSes reserve disjoint I/O regions in their ACPI\n\t * tables.  This causes problems when trying to register the\n\t * entire I/O region.  Therefore we must register each I/O\n\t * port separately.\n\t */\n\tfor (idx = 0; idx < io->io_size; idx++) {\n\t\tif (request_region(addr + idx * io->regspacing,\n\t\t\t\t   io->regsize, DEVICE_NAME) == NULL) {\n\t\t\t/* Undo allocations */\n\t\t\twhile (idx--)\n\t\t\t\trelease_region(addr + idx * io->regspacing,\n\t\t\t\t\t       io->regsize);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tio->io_cleanup = port_cleanup;\n\n\treturn 0;\n}\n"], "filenames": ["drivers/char/ipmi/ipmi_si_intf.c", "drivers/char/ipmi/ipmi_si_mem_io.c", "drivers/char/ipmi/ipmi_si_port_io.c"], "buggy_code_start_loc": [2087, 84, 71], "buggy_code_end_loc": [2087, 143, 111], "fixing_code_start_loc": [2088, 83, 70], "fixing_code_end_loc": [2093, 145, 113], "type": "CWE-416", "message": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c.", "other": {"cve": {"id": "CVE-2019-11811", "sourceIdentifier": "cve@mitre.org", "published": "2019-05-07T14:29:00.863", "lastModified": "2020-05-06T15:14:03.690", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel before 5.0.4. There is a use-after-free upon attempted read access to /proc/ioports after the ipmi_si module is removed, related to drivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, and drivers/char/ipmi/ipmi_si_port_io.c."}, {"lang": "es", "value": "Fue descubierto en un fallo en el kernel de Linux anterior a 5.0.4. Hay un uso despu\u00e9s de liberaci\u00f3n de memoria, una vez que intenta acceder a la lectura del modulo  proc/ioports after the ipmi_si es eliminado, relacionado adrivers/char/ipmi/ipmi_si_intf.c, drivers/char/ipmi/ipmi_si_mem_io.c, y drivers/char/ipmi/ipmi_si_port_io.c."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.0.4", "matchCriteriaId": "83D6D6D6-1A52-46B9-A1B0-1648F795A8BC"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_aus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "24E8D7FF-8269-4EDA-8DCB-7AF37673CD87"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_aus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D99A687E-EAE6-417E-A88E-D0082BC194CD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.4:*:*:*:*:*:*:*", "matchCriteriaId": "D5F7E11E-FB34-4467-8919-2B6BEAABF665"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server_tus:7.6:*:*:*:*:*:*:*", "matchCriteriaId": "B76AA310-FEC7-497F-AF04-C3EC1E76C4CC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-05/msg00071.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/108410", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1873", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1891", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1959", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:1971", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:4057", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:4058", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2020:0036", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.0.4", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=401e7e88d4ef80188ffa07095ac00456f901b8c4", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/401e7e88d4ef80188ffa07095ac00456f901b8c4", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20190719-0003/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://support.f5.com/csp/article/K01512680", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/401e7e88d4ef80188ffa07095ac00456f901b8c4"}}