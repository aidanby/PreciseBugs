{"buggy_code": ["/*\n * Linux driver for VMware's vmxnet3 ethernet NIC.\n *\n * Copyright (C) 2008-2021, VMware, Inc. All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; version 2 of the License and no later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or\n * NON INFRINGEMENT. See the GNU General Public License for more\n * details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n *\n * The full GNU General Public License is included in this distribution in\n * the file called \"COPYING\".\n *\n * Maintained by: pv-drivers@vmware.com\n *\n */\n\n#include <linux/module.h>\n#include <net/ip6_checksum.h>\n\n#include \"vmxnet3_int.h\"\n\nchar vmxnet3_driver_name[] = \"vmxnet3\";\n#define VMXNET3_DRIVER_DESC \"VMware vmxnet3 virtual NIC driver\"\n\n/*\n * PCI Device ID Table\n * Last entry must be all 0s\n */\nstatic const struct pci_device_id vmxnet3_pciid_table[] = {\n\t{PCI_VDEVICE(VMWARE, PCI_DEVICE_ID_VMWARE_VMXNET3)},\n\t{0}\n};\n\nMODULE_DEVICE_TABLE(pci, vmxnet3_pciid_table);\n\nstatic int enable_mq = 1;\n\nstatic void\nvmxnet3_write_mac_addr(struct vmxnet3_adapter *adapter, const u8 *mac);\n\n/*\n *    Enable/Disable the given intr\n */\nstatic void\nvmxnet3_enable_intr(struct vmxnet3_adapter *adapter, unsigned intr_idx)\n{\n\tVMXNET3_WRITE_BAR0_REG(adapter, VMXNET3_REG_IMR + intr_idx * 8, 0);\n}\n\n\nstatic void\nvmxnet3_disable_intr(struct vmxnet3_adapter *adapter, unsigned intr_idx)\n{\n\tVMXNET3_WRITE_BAR0_REG(adapter, VMXNET3_REG_IMR + intr_idx * 8, 1);\n}\n\n\n/*\n *    Enable/Disable all intrs used by the device\n */\nstatic void\nvmxnet3_enable_all_intrs(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\tvmxnet3_enable_intr(adapter, i);\n\tadapter->shared->devRead.intrConf.intrCtrl &=\n\t\t\t\t\tcpu_to_le32(~VMXNET3_IC_DISABLE_ALL);\n}\n\n\nstatic void\nvmxnet3_disable_all_intrs(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tadapter->shared->devRead.intrConf.intrCtrl |=\n\t\t\t\t\tcpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\tvmxnet3_disable_intr(adapter, i);\n}\n\n\nstatic void\nvmxnet3_ack_events(struct vmxnet3_adapter *adapter, u32 events)\n{\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_ECR, events);\n}\n\n\nstatic bool\nvmxnet3_tq_stopped(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\treturn tq->stopped;\n}\n\n\nstatic void\nvmxnet3_tq_start(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = false;\n\tnetif_start_subqueue(adapter->netdev, tq - adapter->tx_queue);\n}\n\n\nstatic void\nvmxnet3_tq_wake(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = false;\n\tnetif_wake_subqueue(adapter->netdev, (tq - adapter->tx_queue));\n}\n\n\nstatic void\nvmxnet3_tq_stop(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = true;\n\ttq->num_stop++;\n\tnetif_stop_subqueue(adapter->netdev, (tq - adapter->tx_queue));\n}\n\n\n/*\n * Check the link state. This may start or stop the tx queue.\n */\nstatic void\nvmxnet3_check_link(struct vmxnet3_adapter *adapter, bool affectTxQueue)\n{\n\tu32 ret;\n\tint i;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD, VMXNET3_CMD_GET_LINK);\n\tret = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tadapter->link_speed = ret >> 16;\n\tif (ret & 1) { /* Link is up. */\n\t\tnetdev_info(adapter->netdev, \"NIC Link is Up %d Mbps\\n\",\n\t\t\t    adapter->link_speed);\n\t\tnetif_carrier_on(adapter->netdev);\n\n\t\tif (affectTxQueue) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tvmxnet3_tq_start(&adapter->tx_queue[i],\n\t\t\t\t\t\t adapter);\n\t\t}\n\t} else {\n\t\tnetdev_info(adapter->netdev, \"NIC Link is Down\\n\");\n\t\tnetif_carrier_off(adapter->netdev);\n\n\t\tif (affectTxQueue) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tvmxnet3_tq_stop(&adapter->tx_queue[i], adapter);\n\t\t}\n\t}\n}\n\nstatic void\nvmxnet3_process_events(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tunsigned long flags;\n\tu32 events = le32_to_cpu(adapter->shared->ecr);\n\tif (!events)\n\t\treturn;\n\n\tvmxnet3_ack_events(adapter, events);\n\n\t/* Check if link state has changed */\n\tif (events & VMXNET3_ECR_LINK)\n\t\tvmxnet3_check_link(adapter, true);\n\n\t/* Check if there is an error on xmit/recv queues */\n\tif (events & (VMXNET3_ECR_TQERR | VMXNET3_ECR_RQERR)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_QUEUE_STATUS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tif (adapter->tqd_start[i].status.stopped)\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"%s: tq[%d] error 0x%x\\n\",\n\t\t\t\t\tadapter->netdev->name, i, le32_to_cpu(\n\t\t\t\t\tadapter->tqd_start[i].status.error));\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tif (adapter->rqd_start[i].status.stopped)\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"%s: rq[%d] error 0x%x\\n\",\n\t\t\t\t\tadapter->netdev->name, i,\n\t\t\t\t\tadapter->rqd_start[i].status.error);\n\n\t\tschedule_work(&adapter->work);\n\t}\n}\n\n#ifdef __BIG_ENDIAN_BITFIELD\n/*\n * The device expects the bitfields in shared structures to be written in\n * little endian. When CPU is big endian, the following routines are used to\n * correctly read and write into ABI.\n * The general technique used here is : double word bitfields are defined in\n * opposite order for big endian architecture. Then before reading them in\n * driver the complete double word is translated using le32_to_cpu. Similarly\n * After the driver writes into bitfields, cpu_to_le32 is used to translate the\n * double words into required format.\n * In order to avoid touching bits in shared structure more than once, temporary\n * descriptors are used. These are passed as srcDesc to following functions.\n */\nstatic void vmxnet3_RxDescToCPU(const struct Vmxnet3_RxDesc *srcDesc,\n\t\t\t\tstruct Vmxnet3_RxDesc *dstDesc)\n{\n\tu32 *src = (u32 *)srcDesc + 2;\n\tu32 *dst = (u32 *)dstDesc + 2;\n\tdstDesc->addr = le64_to_cpu(srcDesc->addr);\n\t*dst = le32_to_cpu(*src);\n\tdstDesc->ext1 = le32_to_cpu(srcDesc->ext1);\n}\n\nstatic void vmxnet3_TxDescToLe(const struct Vmxnet3_TxDesc *srcDesc,\n\t\t\t       struct Vmxnet3_TxDesc *dstDesc)\n{\n\tint i;\n\tu32 *src = (u32 *)(srcDesc + 1);\n\tu32 *dst = (u32 *)(dstDesc + 1);\n\n\t/* Working backwards so that the gen bit is set at the end. */\n\tfor (i = 2; i > 0; i--) {\n\t\tsrc--;\n\t\tdst--;\n\t\t*dst = cpu_to_le32(*src);\n\t}\n}\n\n\nstatic void vmxnet3_RxCompToCPU(const struct Vmxnet3_RxCompDesc *srcDesc,\n\t\t\t\tstruct Vmxnet3_RxCompDesc *dstDesc)\n{\n\tint i = 0;\n\tu32 *src = (u32 *)srcDesc;\n\tu32 *dst = (u32 *)dstDesc;\n\tfor (i = 0; i < sizeof(struct Vmxnet3_RxCompDesc) / sizeof(u32); i++) {\n\t\t*dst = le32_to_cpu(*src);\n\t\tsrc++;\n\t\tdst++;\n\t}\n}\n\n\n/* Used to read bitfield values from double words. */\nstatic u32 get_bitfield32(const __le32 *bitfield, u32 pos, u32 size)\n{\n\tu32 temp = le32_to_cpu(*bitfield);\n\tu32 mask = ((1 << size) - 1) << pos;\n\ttemp &= mask;\n\ttemp >>= pos;\n\treturn temp;\n}\n\n\n\n#endif  /* __BIG_ENDIAN_BITFIELD */\n\n#ifdef __BIG_ENDIAN_BITFIELD\n\n#   define VMXNET3_TXDESC_GET_GEN(txdesc) get_bitfield32(((const __le32 *) \\\n\t\t\ttxdesc) + VMXNET3_TXD_GEN_DWORD_SHIFT, \\\n\t\t\tVMXNET3_TXD_GEN_SHIFT, VMXNET3_TXD_GEN_SIZE)\n#   define VMXNET3_TXDESC_GET_EOP(txdesc) get_bitfield32(((const __le32 *) \\\n\t\t\ttxdesc) + VMXNET3_TXD_EOP_DWORD_SHIFT, \\\n\t\t\tVMXNET3_TXD_EOP_SHIFT, VMXNET3_TXD_EOP_SIZE)\n#   define VMXNET3_TCD_GET_GEN(tcd) get_bitfield32(((const __le32 *)tcd) + \\\n\t\t\tVMXNET3_TCD_GEN_DWORD_SHIFT, VMXNET3_TCD_GEN_SHIFT, \\\n\t\t\tVMXNET3_TCD_GEN_SIZE)\n#   define VMXNET3_TCD_GET_TXIDX(tcd) get_bitfield32((const __le32 *)tcd, \\\n\t\t\tVMXNET3_TCD_TXIDX_SHIFT, VMXNET3_TCD_TXIDX_SIZE)\n#   define vmxnet3_getRxComp(dstrcd, rcd, tmp) do { \\\n\t\t\t(dstrcd) = (tmp); \\\n\t\t\tvmxnet3_RxCompToCPU((rcd), (tmp)); \\\n\t\t} while (0)\n#   define vmxnet3_getRxDesc(dstrxd, rxd, tmp) do { \\\n\t\t\t(dstrxd) = (tmp); \\\n\t\t\tvmxnet3_RxDescToCPU((rxd), (tmp)); \\\n\t\t} while (0)\n\n#else\n\n#   define VMXNET3_TXDESC_GET_GEN(txdesc) ((txdesc)->gen)\n#   define VMXNET3_TXDESC_GET_EOP(txdesc) ((txdesc)->eop)\n#   define VMXNET3_TCD_GET_GEN(tcd) ((tcd)->gen)\n#   define VMXNET3_TCD_GET_TXIDX(tcd) ((tcd)->txdIdx)\n#   define vmxnet3_getRxComp(dstrcd, rcd, tmp) (dstrcd) = (rcd)\n#   define vmxnet3_getRxDesc(dstrxd, rxd, tmp) (dstrxd) = (rxd)\n\n#endif /* __BIG_ENDIAN_BITFIELD  */\n\n\nstatic void\nvmxnet3_unmap_tx_buf(struct vmxnet3_tx_buf_info *tbi,\n\t\t     struct pci_dev *pdev)\n{\n\tif (tbi->map_type == VMXNET3_MAP_SINGLE)\n\t\tdma_unmap_single(&pdev->dev, tbi->dma_addr, tbi->len,\n\t\t\t\t DMA_TO_DEVICE);\n\telse if (tbi->map_type == VMXNET3_MAP_PAGE)\n\t\tdma_unmap_page(&pdev->dev, tbi->dma_addr, tbi->len,\n\t\t\t       DMA_TO_DEVICE);\n\telse\n\t\tBUG_ON(tbi->map_type != VMXNET3_MAP_NONE);\n\n\ttbi->map_type = VMXNET3_MAP_NONE; /* to help debugging */\n}\n\n\nstatic int\nvmxnet3_unmap_pkt(u32 eop_idx, struct vmxnet3_tx_queue *tq,\n\t\t  struct pci_dev *pdev,\tstruct vmxnet3_adapter *adapter)\n{\n\tstruct sk_buff *skb;\n\tint entries = 0;\n\n\t/* no out of order completion */\n\tBUG_ON(tq->buf_info[eop_idx].sop_idx != tq->tx_ring.next2comp);\n\tBUG_ON(VMXNET3_TXDESC_GET_EOP(&(tq->tx_ring.base[eop_idx].txd)) != 1);\n\n\tskb = tq->buf_info[eop_idx].skb;\n\tBUG_ON(skb == NULL);\n\ttq->buf_info[eop_idx].skb = NULL;\n\n\tVMXNET3_INC_RING_IDX_ONLY(eop_idx, tq->tx_ring.size);\n\n\twhile (tq->tx_ring.next2comp != eop_idx) {\n\t\tvmxnet3_unmap_tx_buf(tq->buf_info + tq->tx_ring.next2comp,\n\t\t\t\t     pdev);\n\n\t\t/* update next2comp w/o tx_lock. Since we are marking more,\n\t\t * instead of less, tx ring entries avail, the worst case is\n\t\t * that the tx routine incorrectly re-queues a pkt due to\n\t\t * insufficient tx ring entries.\n\t\t */\n\t\tvmxnet3_cmd_ring_adv_next2comp(&tq->tx_ring);\n\t\tentries++;\n\t}\n\n\tdev_kfree_skb_any(skb);\n\treturn entries;\n}\n\n\nstatic int\nvmxnet3_tq_tx_complete(struct vmxnet3_tx_queue *tq,\n\t\t\tstruct vmxnet3_adapter *adapter)\n{\n\tint completed = 0;\n\tunion Vmxnet3_GenericDesc *gdesc;\n\n\tgdesc = tq->comp_ring.base + tq->comp_ring.next2proc;\n\twhile (VMXNET3_TCD_GET_GEN(&gdesc->tcd) == tq->comp_ring.gen) {\n\t\t/* Prevent any &gdesc->tcd field from being (speculatively)\n\t\t * read before (&gdesc->tcd)->gen is read.\n\t\t */\n\t\tdma_rmb();\n\n\t\tcompleted += vmxnet3_unmap_pkt(VMXNET3_TCD_GET_TXIDX(\n\t\t\t\t\t       &gdesc->tcd), tq, adapter->pdev,\n\t\t\t\t\t       adapter);\n\n\t\tvmxnet3_comp_ring_adv_next2proc(&tq->comp_ring);\n\t\tgdesc = tq->comp_ring.base + tq->comp_ring.next2proc;\n\t}\n\n\tif (completed) {\n\t\tspin_lock(&tq->tx_lock);\n\t\tif (unlikely(vmxnet3_tq_stopped(tq, adapter) &&\n\t\t\t     vmxnet3_cmd_ring_desc_avail(&tq->tx_ring) >\n\t\t\t     VMXNET3_WAKE_QUEUE_THRESHOLD(tq) &&\n\t\t\t     netif_carrier_ok(adapter->netdev))) {\n\t\t\tvmxnet3_tq_wake(tq, adapter);\n\t\t}\n\t\tspin_unlock(&tq->tx_lock);\n\t}\n\treturn completed;\n}\n\n\nstatic void\nvmxnet3_tq_cleanup(struct vmxnet3_tx_queue *tq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\twhile (tq->tx_ring.next2comp != tq->tx_ring.next2fill) {\n\t\tstruct vmxnet3_tx_buf_info *tbi;\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2comp;\n\n\t\tvmxnet3_unmap_tx_buf(tbi, adapter->pdev);\n\t\tif (tbi->skb) {\n\t\t\tdev_kfree_skb_any(tbi->skb);\n\t\t\ttbi->skb = NULL;\n\t\t}\n\t\tvmxnet3_cmd_ring_adv_next2comp(&tq->tx_ring);\n\t}\n\n\t/* sanity check, verify all buffers are indeed unmapped and freed */\n\tfor (i = 0; i < tq->tx_ring.size; i++) {\n\t\tBUG_ON(tq->buf_info[i].skb != NULL ||\n\t\t       tq->buf_info[i].map_type != VMXNET3_MAP_NONE);\n\t}\n\n\ttq->tx_ring.gen = VMXNET3_INIT_GEN;\n\ttq->tx_ring.next2fill = tq->tx_ring.next2comp = 0;\n\n\ttq->comp_ring.gen = VMXNET3_INIT_GEN;\n\ttq->comp_ring.next2proc = 0;\n}\n\n\nstatic void\nvmxnet3_tq_destroy(struct vmxnet3_tx_queue *tq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tif (tq->tx_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, tq->tx_ring.size *\n\t\t\t\t  sizeof(struct Vmxnet3_TxDesc),\n\t\t\t\t  tq->tx_ring.base, tq->tx_ring.basePA);\n\t\ttq->tx_ring.base = NULL;\n\t}\n\tif (tq->data_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  tq->data_ring.size * tq->txdata_desc_size,\n\t\t\t\t  tq->data_ring.base, tq->data_ring.basePA);\n\t\ttq->data_ring.base = NULL;\n\t}\n\tif (tq->comp_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, tq->comp_ring.size *\n\t\t\t\t  sizeof(struct Vmxnet3_TxCompDesc),\n\t\t\t\t  tq->comp_ring.base, tq->comp_ring.basePA);\n\t\ttq->comp_ring.base = NULL;\n\t}\n\tkfree(tq->buf_info);\n\ttq->buf_info = NULL;\n}\n\n\n/* Destroy all tx queues */\nvoid\nvmxnet3_tq_destroy_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_destroy(&adapter->tx_queue[i], adapter);\n}\n\n\nstatic void\nvmxnet3_tq_init(struct vmxnet3_tx_queue *tq,\n\t\tstruct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\t/* reset the tx ring contents to 0 and reset the tx ring states */\n\tmemset(tq->tx_ring.base, 0, tq->tx_ring.size *\n\t       sizeof(struct Vmxnet3_TxDesc));\n\ttq->tx_ring.next2fill = tq->tx_ring.next2comp = 0;\n\ttq->tx_ring.gen = VMXNET3_INIT_GEN;\n\n\tmemset(tq->data_ring.base, 0,\n\t       tq->data_ring.size * tq->txdata_desc_size);\n\n\t/* reset the tx comp ring contents to 0 and reset comp ring states */\n\tmemset(tq->comp_ring.base, 0, tq->comp_ring.size *\n\t       sizeof(struct Vmxnet3_TxCompDesc));\n\ttq->comp_ring.next2proc = 0;\n\ttq->comp_ring.gen = VMXNET3_INIT_GEN;\n\n\t/* reset the bookkeeping data */\n\tmemset(tq->buf_info, 0, sizeof(tq->buf_info[0]) * tq->tx_ring.size);\n\tfor (i = 0; i < tq->tx_ring.size; i++)\n\t\ttq->buf_info[i].map_type = VMXNET3_MAP_NONE;\n\n\t/* stats are not reset */\n}\n\n\nstatic int\nvmxnet3_tq_create(struct vmxnet3_tx_queue *tq,\n\t\t  struct vmxnet3_adapter *adapter)\n{\n\tBUG_ON(tq->tx_ring.base || tq->data_ring.base ||\n\t       tq->comp_ring.base || tq->buf_info);\n\n\ttq->tx_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->tx_ring.size * sizeof(struct Vmxnet3_TxDesc),\n\t\t\t&tq->tx_ring.basePA, GFP_KERNEL);\n\tif (!tq->tx_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->data_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->data_ring.size * tq->txdata_desc_size,\n\t\t\t&tq->data_ring.basePA, GFP_KERNEL);\n\tif (!tq->data_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx data ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->comp_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->comp_ring.size * sizeof(struct Vmxnet3_TxCompDesc),\n\t\t\t&tq->comp_ring.basePA, GFP_KERNEL);\n\tif (!tq->comp_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx comp ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->buf_info = kcalloc_node(tq->tx_ring.size, sizeof(tq->buf_info[0]),\n\t\t\t\t    GFP_KERNEL,\n\t\t\t\t    dev_to_node(&adapter->pdev->dev));\n\tif (!tq->buf_info)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tvmxnet3_tq_destroy(tq, adapter);\n\treturn -ENOMEM;\n}\n\nstatic void\nvmxnet3_tq_cleanup_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_cleanup(&adapter->tx_queue[i], adapter);\n}\n\n/*\n *    starting from ring->next2fill, allocate rx buffers for the given ring\n *    of the rx queue and update the rx desc. stop after @num_to_alloc buffers\n *    are allocated or allocation fails\n */\n\nstatic int\nvmxnet3_rq_alloc_rx_buf(struct vmxnet3_rx_queue *rq, u32 ring_idx,\n\t\t\tint num_to_alloc, struct vmxnet3_adapter *adapter)\n{\n\tint num_allocated = 0;\n\tstruct vmxnet3_rx_buf_info *rbi_base = rq->buf_info[ring_idx];\n\tstruct vmxnet3_cmd_ring *ring = &rq->rx_ring[ring_idx];\n\tu32 val;\n\n\twhile (num_allocated <= num_to_alloc) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tunion Vmxnet3_GenericDesc *gd;\n\n\t\trbi = rbi_base + ring->next2fill;\n\t\tgd = ring->base + ring->next2fill;\n\n\t\tif (rbi->buf_type == VMXNET3_RX_BUF_SKB) {\n\t\t\tif (rbi->skb == NULL) {\n\t\t\t\trbi->skb = __netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t\t       rbi->len,\n\t\t\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (unlikely(rbi->skb == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\trbi->dma_addr = dma_map_single(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->skb->data, rbi->len,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tdev_kfree_skb_any(rbi->skb);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffer skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_HEAD << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t} else {\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE ||\n\t\t\t       rbi->len  != PAGE_SIZE);\n\n\t\t\tif (rbi->page == NULL) {\n\t\t\t\trbi->page = alloc_page(GFP_ATOMIC);\n\t\t\t\tif (unlikely(rbi->page == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trbi->dma_addr = dma_map_page(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->page, 0, PAGE_SIZE,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tput_page(rbi->page);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffers skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_BODY << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t}\n\n\t\tgd->rxd.addr = cpu_to_le64(rbi->dma_addr);\n\t\tgd->dword[2] = cpu_to_le32((!ring->gen << VMXNET3_RXD_GEN_SHIFT)\n\t\t\t\t\t   | val | rbi->len);\n\n\t\t/* Fill the last buffer but dont mark it ready, or else the\n\t\t * device will think that the queue is full */\n\t\tif (num_allocated == num_to_alloc)\n\t\t\tbreak;\n\n\t\tgd->dword[2] |= cpu_to_le32(ring->gen << VMXNET3_RXD_GEN_SHIFT);\n\t\tnum_allocated++;\n\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t}\n\n\tnetdev_dbg(adapter->netdev,\n\t\t\"alloc_rx_buf: %d allocated, next2fill %u, next2comp %u\\n\",\n\t\tnum_allocated, ring->next2fill, ring->next2comp);\n\n\t/* so that the device can distinguish a full ring and an empty ring */\n\tBUG_ON(num_allocated != 0 && ring->next2fill == ring->next2comp);\n\n\treturn num_allocated;\n}\n\n\nstatic void\nvmxnet3_append_frag(struct sk_buff *skb, struct Vmxnet3_RxCompDesc *rcd,\n\t\t    struct vmxnet3_rx_buf_info *rbi)\n{\n\tskb_frag_t *frag = skb_shinfo(skb)->frags + skb_shinfo(skb)->nr_frags;\n\n\tBUG_ON(skb_shinfo(skb)->nr_frags >= MAX_SKB_FRAGS);\n\n\t__skb_frag_set_page(frag, rbi->page);\n\tskb_frag_off_set(frag, 0);\n\tskb_frag_size_set(frag, rcd->len);\n\tskb->data_len += rcd->len;\n\tskb->truesize += PAGE_SIZE;\n\tskb_shinfo(skb)->nr_frags++;\n}\n\n\nstatic int\nvmxnet3_map_pkt(struct sk_buff *skb, struct vmxnet3_tx_ctx *ctx,\n\t\tstruct vmxnet3_tx_queue *tq, struct pci_dev *pdev,\n\t\tstruct vmxnet3_adapter *adapter)\n{\n\tu32 dw2, len;\n\tunsigned long buf_offset;\n\tint i;\n\tunion Vmxnet3_GenericDesc *gdesc;\n\tstruct vmxnet3_tx_buf_info *tbi = NULL;\n\n\tBUG_ON(ctx->copy_size > skb_headlen(skb));\n\n\t/* use the previous gen bit for the SOP desc */\n\tdw2 = (tq->tx_ring.gen ^ 0x1) << VMXNET3_TXD_GEN_SHIFT;\n\n\tctx->sop_txd = tq->tx_ring.base + tq->tx_ring.next2fill;\n\tgdesc = ctx->sop_txd; /* both loops below can be skipped */\n\n\t/* no need to map the buffer if headers are copied */\n\tif (ctx->copy_size) {\n\t\tctx->sop_txd->txd.addr = cpu_to_le64(tq->data_ring.basePA +\n\t\t\t\t\ttq->tx_ring.next2fill *\n\t\t\t\t\ttq->txdata_desc_size);\n\t\tctx->sop_txd->dword[2] = cpu_to_le32(dw2 | ctx->copy_size);\n\t\tctx->sop_txd->dword[3] = 0;\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\ttbi->map_type = VMXNET3_MAP_NONE;\n\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"txd[%u]: 0x%Lx 0x%x 0x%x\\n\",\n\t\t\ttq->tx_ring.next2fill,\n\t\t\tle64_to_cpu(ctx->sop_txd->txd.addr),\n\t\t\tctx->sop_txd->dword[2], ctx->sop_txd->dword[3]);\n\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\n\t\t/* use the right gen for non-SOP desc */\n\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\t}\n\n\t/* linear part can use multiple tx desc if it's big */\n\tlen = skb_headlen(skb) - ctx->copy_size;\n\tbuf_offset = ctx->copy_size;\n\twhile (len) {\n\t\tu32 buf_size;\n\n\t\tif (len < VMXNET3_MAX_TX_BUF_SIZE) {\n\t\t\tbuf_size = len;\n\t\t\tdw2 |= len;\n\t\t} else {\n\t\t\tbuf_size = VMXNET3_MAX_TX_BUF_SIZE;\n\t\t\t/* spec says that for TxDesc.len, 0 == 2^14 */\n\t\t}\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\ttbi->map_type = VMXNET3_MAP_SINGLE;\n\t\ttbi->dma_addr = dma_map_single(&adapter->pdev->dev,\n\t\t\t\tskb->data + buf_offset, buf_size,\n\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&adapter->pdev->dev, tbi->dma_addr))\n\t\t\treturn -EFAULT;\n\n\t\ttbi->len = buf_size;\n\n\t\tgdesc = tq->tx_ring.base + tq->tx_ring.next2fill;\n\t\tBUG_ON(gdesc->txd.gen == tq->tx_ring.gen);\n\n\t\tgdesc->txd.addr = cpu_to_le64(tbi->dma_addr);\n\t\tgdesc->dword[2] = cpu_to_le32(dw2);\n\t\tgdesc->dword[3] = 0;\n\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"txd[%u]: 0x%Lx 0x%x 0x%x\\n\",\n\t\t\ttq->tx_ring.next2fill, le64_to_cpu(gdesc->txd.addr),\n\t\t\tle32_to_cpu(gdesc->dword[2]), gdesc->dword[3]);\n\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\n\t\tlen -= buf_size;\n\t\tbuf_offset += buf_size;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 buf_size;\n\n\t\tbuf_offset = 0;\n\t\tlen = skb_frag_size(frag);\n\t\twhile (len) {\n\t\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\t\tif (len < VMXNET3_MAX_TX_BUF_SIZE) {\n\t\t\t\tbuf_size = len;\n\t\t\t\tdw2 |= len;\n\t\t\t} else {\n\t\t\t\tbuf_size = VMXNET3_MAX_TX_BUF_SIZE;\n\t\t\t\t/* spec says that for TxDesc.len, 0 == 2^14 */\n\t\t\t}\n\t\t\ttbi->map_type = VMXNET3_MAP_PAGE;\n\t\t\ttbi->dma_addr = skb_frag_dma_map(&adapter->pdev->dev, frag,\n\t\t\t\t\t\t\t buf_offset, buf_size,\n\t\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&adapter->pdev->dev, tbi->dma_addr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\ttbi->len = buf_size;\n\n\t\t\tgdesc = tq->tx_ring.base + tq->tx_ring.next2fill;\n\t\t\tBUG_ON(gdesc->txd.gen == tq->tx_ring.gen);\n\n\t\t\tgdesc->txd.addr = cpu_to_le64(tbi->dma_addr);\n\t\t\tgdesc->dword[2] = cpu_to_le32(dw2);\n\t\t\tgdesc->dword[3] = 0;\n\n\t\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\t\"txd[%u]: 0x%llx %u %u\\n\",\n\t\t\t\ttq->tx_ring.next2fill, le64_to_cpu(gdesc->txd.addr),\n\t\t\t\tle32_to_cpu(gdesc->dword[2]), gdesc->dword[3]);\n\t\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\t\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\n\t\t\tlen -= buf_size;\n\t\t\tbuf_offset += buf_size;\n\t\t}\n\t}\n\n\tctx->eop_txd = gdesc;\n\n\t/* set the last buf_info for the pkt */\n\ttbi->skb = skb;\n\ttbi->sop_idx = ctx->sop_txd - tq->tx_ring.base;\n\n\treturn 0;\n}\n\n\n/* Init all tx queues */\nstatic void\nvmxnet3_tq_init_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_init(&adapter->tx_queue[i], adapter);\n}\n\n\n/*\n *    parse relevant protocol headers:\n *      For a tso pkt, relevant headers are L2/3/4 including options\n *      For a pkt requesting csum offloading, they are L2/3 and may include L4\n *      if it's a TCP/UDP pkt\n *\n * Returns:\n *    -1:  error happens during parsing\n *     0:  protocol headers parsed, but too big to be copied\n *     1:  protocol headers parsed and copied\n *\n * Other effects:\n *    1. related *ctx fields are updated.\n *    2. ctx->copy_size is # of bytes copied\n *    3. the portion to be copied is guaranteed to be in the linear part\n *\n */\nstatic int\nvmxnet3_parse_hdr(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\t  struct vmxnet3_tx_ctx *ctx,\n\t\t  struct vmxnet3_adapter *adapter)\n{\n\tu8 protocol = 0;\n\n\tif (ctx->mss) {\t/* TSO */\n\t\tif (VMXNET3_VERSION_GE_4(adapter) && skb->encapsulation) {\n\t\t\tctx->l4_offset = skb_inner_transport_offset(skb);\n\t\t\tctx->l4_hdr_size = inner_tcp_hdrlen(skb);\n\t\t\tctx->copy_size = ctx->l4_offset + ctx->l4_hdr_size;\n\t\t} else {\n\t\t\tctx->l4_offset = skb_transport_offset(skb);\n\t\t\tctx->l4_hdr_size = tcp_hdrlen(skb);\n\t\t\tctx->copy_size = ctx->l4_offset + ctx->l4_hdr_size;\n\t\t}\n\t} else {\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t/* For encap packets, skb_checksum_start_offset refers\n\t\t\t * to inner L4 offset. Thus, below works for encap as\n\t\t\t * well as non-encap case\n\t\t\t */\n\t\t\tctx->l4_offset = skb_checksum_start_offset(skb);\n\n\t\t\tif (VMXNET3_VERSION_GE_4(adapter) &&\n\t\t\t    skb->encapsulation) {\n\t\t\t\tstruct iphdr *iph = inner_ip_hdr(skb);\n\n\t\t\t\tif (iph->version == 4) {\n\t\t\t\t\tprotocol = iph->protocol;\n\t\t\t\t} else {\n\t\t\t\t\tconst struct ipv6hdr *ipv6h;\n\n\t\t\t\t\tipv6h = inner_ipv6_hdr(skb);\n\t\t\t\t\tprotocol = ipv6h->nexthdr;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ctx->ipv4) {\n\t\t\t\t\tconst struct iphdr *iph = ip_hdr(skb);\n\n\t\t\t\t\tprotocol = iph->protocol;\n\t\t\t\t} else if (ctx->ipv6) {\n\t\t\t\t\tconst struct ipv6hdr *ipv6h;\n\n\t\t\t\t\tipv6h = ipv6_hdr(skb);\n\t\t\t\t\tprotocol = ipv6h->nexthdr;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tswitch (protocol) {\n\t\t\tcase IPPROTO_TCP:\n\t\t\t\tctx->l4_hdr_size = skb->encapsulation ? inner_tcp_hdrlen(skb) :\n\t\t\t\t\t\t   tcp_hdrlen(skb);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_UDP:\n\t\t\t\tctx->l4_hdr_size = sizeof(struct udphdr);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tctx->l4_hdr_size = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tctx->copy_size = min(ctx->l4_offset +\n\t\t\t\t\t ctx->l4_hdr_size, skb->len);\n\t\t} else {\n\t\t\tctx->l4_offset = 0;\n\t\t\tctx->l4_hdr_size = 0;\n\t\t\t/* copy as much as allowed */\n\t\t\tctx->copy_size = min_t(unsigned int,\n\t\t\t\t\t       tq->txdata_desc_size,\n\t\t\t\t\t       skb_headlen(skb));\n\t\t}\n\n\t\tif (skb->len <= VMXNET3_HDR_COPY_SIZE)\n\t\t\tctx->copy_size = skb->len;\n\n\t\t/* make sure headers are accessible directly */\n\t\tif (unlikely(!pskb_may_pull(skb, ctx->copy_size)))\n\t\t\tgoto err;\n\t}\n\n\tif (unlikely(ctx->copy_size > tq->txdata_desc_size)) {\n\t\ttq->stats.oversized_hdr++;\n\t\tctx->copy_size = 0;\n\t\treturn 0;\n\t}\n\n\treturn 1;\nerr:\n\treturn -1;\n}\n\n/*\n *    copy relevant protocol headers to the transmit ring:\n *      For a tso pkt, relevant headers are L2/3/4 including options\n *      For a pkt requesting csum offloading, they are L2/3 and may include L4\n *      if it's a TCP/UDP pkt\n *\n *\n *    Note that this requires that vmxnet3_parse_hdr be called first to set the\n *      appropriate bits in ctx first\n */\nstatic void\nvmxnet3_copy_hdr(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\t struct vmxnet3_tx_ctx *ctx,\n\t\t struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_TxDataDesc *tdd;\n\n\ttdd = (struct Vmxnet3_TxDataDesc *)((u8 *)tq->data_ring.base +\n\t\t\t\t\t    tq->tx_ring.next2fill *\n\t\t\t\t\t    tq->txdata_desc_size);\n\n\tmemcpy(tdd->data, skb->data, ctx->copy_size);\n\tnetdev_dbg(adapter->netdev,\n\t\t\"copy %u bytes to dataRing[%u]\\n\",\n\t\tctx->copy_size, tq->tx_ring.next2fill);\n}\n\n\nstatic void\nvmxnet3_prepare_inner_tso(struct sk_buff *skb,\n\t\t\t  struct vmxnet3_tx_ctx *ctx)\n{\n\tstruct tcphdr *tcph = inner_tcp_hdr(skb);\n\tstruct iphdr *iph = inner_ip_hdr(skb);\n\n\tif (iph->version == 4) {\n\t\tiph->check = 0;\n\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t} else {\n\t\tstruct ipv6hdr *iph = inner_ipv6_hdr(skb);\n\n\t\ttcph->check = ~csum_ipv6_magic(&iph->saddr, &iph->daddr, 0,\n\t\t\t\t\t       IPPROTO_TCP, 0);\n\t}\n}\n\nstatic void\nvmxnet3_prepare_tso(struct sk_buff *skb,\n\t\t    struct vmxnet3_tx_ctx *ctx)\n{\n\tstruct tcphdr *tcph = tcp_hdr(skb);\n\n\tif (ctx->ipv4) {\n\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\tiph->check = 0;\n\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t} else if (ctx->ipv6) {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t}\n}\n\nstatic int txd_estimate(const struct sk_buff *skb)\n{\n\tint count = VMXNET3_TXD_NEEDED(skb_headlen(skb)) + 1;\n\tint i;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tcount += VMXNET3_TXD_NEEDED(skb_frag_size(frag));\n\t}\n\treturn count;\n}\n\n/*\n * Transmits a pkt thru a given tq\n * Returns:\n *    NETDEV_TX_OK:      descriptors are setup successfully\n *    NETDEV_TX_OK:      error occurred, the pkt is dropped\n *    NETDEV_TX_BUSY:    tx ring is full, queue is stopped\n *\n * Side-effects:\n *    1. tx ring may be changed\n *    2. tq stats may be updated accordingly\n *    3. shared->txNumDeferred may be updated\n */\n\nstatic int\nvmxnet3_tq_xmit(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\tstruct vmxnet3_adapter *adapter, struct net_device *netdev)\n{\n\tint ret;\n\tu32 count;\n\tint num_pkts;\n\tint tx_num_deferred;\n\tunsigned long flags;\n\tstruct vmxnet3_tx_ctx ctx;\n\tunion Vmxnet3_GenericDesc *gdesc;\n#ifdef __BIG_ENDIAN_BITFIELD\n\t/* Use temporary descriptor to avoid touching bits multiple times */\n\tunion Vmxnet3_GenericDesc tempTxDesc;\n#endif\n\n\tcount = txd_estimate(skb);\n\n\tctx.ipv4 = (vlan_get_protocol(skb) == cpu_to_be16(ETH_P_IP));\n\tctx.ipv6 = (vlan_get_protocol(skb) == cpu_to_be16(ETH_P_IPV6));\n\n\tctx.mss = skb_shinfo(skb)->gso_size;\n\tif (ctx.mss) {\n\t\tif (skb_header_cloned(skb)) {\n\t\t\tif (unlikely(pskb_expand_head(skb, 0, 0,\n\t\t\t\t\t\t      GFP_ATOMIC) != 0)) {\n\t\t\t\ttq->stats.drop_tso++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t\ttq->stats.copy_skb_header++;\n\t\t}\n\t\tif (skb->encapsulation) {\n\t\t\tvmxnet3_prepare_inner_tso(skb, &ctx);\n\t\t} else {\n\t\t\tvmxnet3_prepare_tso(skb, &ctx);\n\t\t}\n\t} else {\n\t\tif (unlikely(count > VMXNET3_MAX_TXD_PER_PKT)) {\n\n\t\t\t/* non-tso pkts must not use more than\n\t\t\t * VMXNET3_MAX_TXD_PER_PKT entries\n\t\t\t */\n\t\t\tif (skb_linearize(skb) != 0) {\n\t\t\t\ttq->stats.drop_too_many_frags++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t\ttq->stats.linearized++;\n\n\t\t\t/* recalculate the # of descriptors to use */\n\t\t\tcount = VMXNET3_TXD_NEEDED(skb_headlen(skb)) + 1;\n\t\t}\n\t}\n\n\tret = vmxnet3_parse_hdr(skb, tq, &ctx, adapter);\n\tif (ret >= 0) {\n\t\tBUG_ON(ret <= 0 && ctx.copy_size != 0);\n\t\t/* hdrs parsed, check against other limits */\n\t\tif (ctx.mss) {\n\t\t\tif (unlikely(ctx.l4_offset + ctx.l4_hdr_size >\n\t\t\t\t     VMXNET3_MAX_TX_BUF_SIZE)) {\n\t\t\t\ttq->stats.drop_oversized_hdr++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t} else {\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t\tif (unlikely(ctx.l4_offset +\n\t\t\t\t\t     skb->csum_offset >\n\t\t\t\t\t     VMXNET3_MAX_CSUM_OFFSET)) {\n\t\t\t\t\ttq->stats.drop_oversized_hdr++;\n\t\t\t\t\tgoto drop_pkt;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttq->stats.drop_hdr_inspect_err++;\n\t\tgoto drop_pkt;\n\t}\n\n\tspin_lock_irqsave(&tq->tx_lock, flags);\n\n\tif (count > vmxnet3_cmd_ring_desc_avail(&tq->tx_ring)) {\n\t\ttq->stats.tx_ring_full++;\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"tx queue stopped on %s, next2comp %u\"\n\t\t\t\" next2fill %u\\n\", adapter->netdev->name,\n\t\t\ttq->tx_ring.next2comp, tq->tx_ring.next2fill);\n\n\t\tvmxnet3_tq_stop(tq, adapter);\n\t\tspin_unlock_irqrestore(&tq->tx_lock, flags);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\n\tvmxnet3_copy_hdr(skb, tq, &ctx, adapter);\n\n\t/* fill tx descs related to addr & len */\n\tif (vmxnet3_map_pkt(skb, &ctx, tq, adapter->pdev, adapter))\n\t\tgoto unlock_drop_pkt;\n\n\t/* setup the EOP desc */\n\tctx.eop_txd->dword[3] = cpu_to_le32(VMXNET3_TXD_CQ | VMXNET3_TXD_EOP);\n\n\t/* setup the SOP desc */\n#ifdef __BIG_ENDIAN_BITFIELD\n\tgdesc = &tempTxDesc;\n\tgdesc->dword[2] = ctx.sop_txd->dword[2];\n\tgdesc->dword[3] = ctx.sop_txd->dword[3];\n#else\n\tgdesc = ctx.sop_txd;\n#endif\n\ttx_num_deferred = le32_to_cpu(tq->shared->txNumDeferred);\n\tif (ctx.mss) {\n\t\tif (VMXNET3_VERSION_GE_4(adapter) && skb->encapsulation) {\n\t\t\tgdesc->txd.hlen = ctx.l4_offset + ctx.l4_hdr_size;\n\t\t\tgdesc->txd.om = VMXNET3_OM_ENCAP;\n\t\t\tgdesc->txd.msscof = ctx.mss;\n\n\t\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)\n\t\t\t\tgdesc->txd.oco = 1;\n\t\t} else {\n\t\t\tgdesc->txd.hlen = ctx.l4_offset + ctx.l4_hdr_size;\n\t\t\tgdesc->txd.om = VMXNET3_OM_TSO;\n\t\t\tgdesc->txd.msscof = ctx.mss;\n\t\t}\n\t\tnum_pkts = (skb->len - gdesc->txd.hlen + ctx.mss - 1) / ctx.mss;\n\t} else {\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (VMXNET3_VERSION_GE_4(adapter) &&\n\t\t\t    skb->encapsulation) {\n\t\t\t\tgdesc->txd.hlen = ctx.l4_offset +\n\t\t\t\t\t\t  ctx.l4_hdr_size;\n\t\t\t\tgdesc->txd.om = VMXNET3_OM_ENCAP;\n\t\t\t\tgdesc->txd.msscof = 0;\t\t/* Reserved */\n\t\t\t} else {\n\t\t\t\tgdesc->txd.hlen = ctx.l4_offset;\n\t\t\t\tgdesc->txd.om = VMXNET3_OM_CSUM;\n\t\t\t\tgdesc->txd.msscof = ctx.l4_offset +\n\t\t\t\t\t\t    skb->csum_offset;\n\t\t\t}\n\t\t} else {\n\t\t\tgdesc->txd.om = 0;\n\t\t\tgdesc->txd.msscof = 0;\n\t\t}\n\t\tnum_pkts = 1;\n\t}\n\tle32_add_cpu(&tq->shared->txNumDeferred, num_pkts);\n\ttx_num_deferred += num_pkts;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tgdesc->txd.ti = 1;\n\t\tgdesc->txd.tci = skb_vlan_tag_get(skb);\n\t}\n\n\t/* Ensure that the write to (&gdesc->txd)->gen will be observed after\n\t * all other writes to &gdesc->txd.\n\t */\n\tdma_wmb();\n\n\t/* finally flips the GEN bit of the SOP desc. */\n\tgdesc->dword[2] = cpu_to_le32(le32_to_cpu(gdesc->dword[2]) ^\n\t\t\t\t\t\t  VMXNET3_TXD_GEN);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t/* Finished updating in bitfields of Tx Desc, so write them in original\n\t * place.\n\t */\n\tvmxnet3_TxDescToLe((struct Vmxnet3_TxDesc *)gdesc,\n\t\t\t   (struct Vmxnet3_TxDesc *)ctx.sop_txd);\n\tgdesc = ctx.sop_txd;\n#endif\n\tnetdev_dbg(adapter->netdev,\n\t\t\"txd[%u]: SOP 0x%Lx 0x%x 0x%x\\n\",\n\t\t(u32)(ctx.sop_txd -\n\t\ttq->tx_ring.base), le64_to_cpu(gdesc->txd.addr),\n\t\tle32_to_cpu(gdesc->dword[2]), le32_to_cpu(gdesc->dword[3]));\n\n\tspin_unlock_irqrestore(&tq->tx_lock, flags);\n\n\tif (tx_num_deferred >= le32_to_cpu(tq->shared->txThreshold)) {\n\t\ttq->shared->txNumDeferred = 0;\n\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\t       VMXNET3_REG_TXPROD + tq->qid * 8,\n\t\t\t\t       tq->tx_ring.next2fill);\n\t}\n\n\treturn NETDEV_TX_OK;\n\nunlock_drop_pkt:\n\tspin_unlock_irqrestore(&tq->tx_lock, flags);\ndrop_pkt:\n\ttq->stats.drop_total++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n\nstatic netdev_tx_t\nvmxnet3_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tBUG_ON(skb->queue_mapping > adapter->num_tx_queues);\n\treturn vmxnet3_tq_xmit(skb,\n\t\t\t       &adapter->tx_queue[skb->queue_mapping],\n\t\t\t       adapter, netdev);\n}\n\n\nstatic void\nvmxnet3_rx_csum(struct vmxnet3_adapter *adapter,\n\t\tstruct sk_buff *skb,\n\t\tunion Vmxnet3_GenericDesc *gdesc)\n{\n\tif (!gdesc->rcd.cnc && adapter->netdev->features & NETIF_F_RXCSUM) {\n\t\tif (gdesc->rcd.v4 &&\n\t\t    (le32_to_cpu(gdesc->dword[3]) &\n\t\t     VMXNET3_RCD_CSUM_OK) == VMXNET3_RCD_CSUM_OK) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tWARN_ON_ONCE(!(gdesc->rcd.tcp || gdesc->rcd.udp) &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t\tWARN_ON_ONCE(gdesc->rcd.frg &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t} else if (gdesc->rcd.v6 && (le32_to_cpu(gdesc->dword[3]) &\n\t\t\t\t\t     (1 << VMXNET3_RCD_TUC_SHIFT))) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tWARN_ON_ONCE(!(gdesc->rcd.tcp || gdesc->rcd.udp) &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t\tWARN_ON_ONCE(gdesc->rcd.frg &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t} else {\n\t\t\tif (gdesc->rcd.csum) {\n\t\t\t\tskb->csum = htons(gdesc->rcd.csum);\n\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\t} else {\n\t\t\t\tskb_checksum_none_assert(skb);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tskb_checksum_none_assert(skb);\n\t}\n}\n\n\nstatic void\nvmxnet3_rx_error(struct vmxnet3_rx_queue *rq, struct Vmxnet3_RxCompDesc *rcd,\n\t\t struct vmxnet3_rx_ctx *ctx,  struct vmxnet3_adapter *adapter)\n{\n\trq->stats.drop_err++;\n\tif (!rcd->fcs)\n\t\trq->stats.drop_fcs++;\n\n\trq->stats.drop_total++;\n\n\t/*\n\t * We do not unmap and chain the rx buffer to the skb.\n\t * We basically pretend this buffer is not used and will be recycled\n\t * by vmxnet3_rq_alloc_rx_buf()\n\t */\n\n\t/*\n\t * ctx->skb may be NULL if this is the first and the only one\n\t * desc for the pkt\n\t */\n\tif (ctx->skb)\n\t\tdev_kfree_skb_irq(ctx->skb);\n\n\tctx->skb = NULL;\n}\n\n\nstatic u32\nvmxnet3_get_hdr_len(struct vmxnet3_adapter *adapter, struct sk_buff *skb,\n\t\t    union Vmxnet3_GenericDesc *gdesc)\n{\n\tu32 hlen, maplen;\n\tunion {\n\t\tvoid *ptr;\n\t\tstruct ethhdr *eth;\n\t\tstruct vlan_ethhdr *veth;\n\t\tstruct iphdr *ipv4;\n\t\tstruct ipv6hdr *ipv6;\n\t\tstruct tcphdr *tcp;\n\t} hdr;\n\tBUG_ON(gdesc->rcd.tcp == 0);\n\n\tmaplen = skb_headlen(skb);\n\tif (unlikely(sizeof(struct iphdr) + sizeof(struct tcphdr) > maplen))\n\t\treturn 0;\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD))\n\t\thlen = sizeof(struct vlan_ethhdr);\n\telse\n\t\thlen = sizeof(struct ethhdr);\n\n\thdr.eth = eth_hdr(skb);\n\tif (gdesc->rcd.v4) {\n\t\tBUG_ON(hdr.eth->h_proto != htons(ETH_P_IP) &&\n\t\t       hdr.veth->h_vlan_encapsulated_proto != htons(ETH_P_IP));\n\t\thdr.ptr += hlen;\n\t\tBUG_ON(hdr.ipv4->protocol != IPPROTO_TCP);\n\t\thlen = hdr.ipv4->ihl << 2;\n\t\thdr.ptr += hdr.ipv4->ihl << 2;\n\t} else if (gdesc->rcd.v6) {\n\t\tBUG_ON(hdr.eth->h_proto != htons(ETH_P_IPV6) &&\n\t\t       hdr.veth->h_vlan_encapsulated_proto != htons(ETH_P_IPV6));\n\t\thdr.ptr += hlen;\n\t\t/* Use an estimated value, since we also need to handle\n\t\t * TSO case.\n\t\t */\n\t\tif (hdr.ipv6->nexthdr != IPPROTO_TCP)\n\t\t\treturn sizeof(struct ipv6hdr) + sizeof(struct tcphdr);\n\t\thlen = sizeof(struct ipv6hdr);\n\t\thdr.ptr += sizeof(struct ipv6hdr);\n\t} else {\n\t\t/* Non-IP pkt, dont estimate header length */\n\t\treturn 0;\n\t}\n\n\tif (hlen + sizeof(struct tcphdr) > maplen)\n\t\treturn 0;\n\n\treturn (hlen + (hdr.tcp->doff << 2));\n}\n\nstatic int\nvmxnet3_rq_rx_complete(struct vmxnet3_rx_queue *rq,\n\t\t       struct vmxnet3_adapter *adapter, int quota)\n{\n\tstatic const u32 rxprod_reg[2] = {\n\t\tVMXNET3_REG_RXPROD, VMXNET3_REG_RXPROD2\n\t};\n\tu32 num_pkts = 0;\n\tbool skip_page_frags = false;\n\tstruct Vmxnet3_RxCompDesc *rcd;\n\tstruct vmxnet3_rx_ctx *ctx = &rq->rx_ctx;\n\tu16 segCnt = 0, mss = 0;\n#ifdef __BIG_ENDIAN_BITFIELD\n\tstruct Vmxnet3_RxDesc rxCmdDesc;\n\tstruct Vmxnet3_RxCompDesc rxComp;\n#endif\n\tvmxnet3_getRxComp(rcd, &rq->comp_ring.base[rq->comp_ring.next2proc].rcd,\n\t\t\t  &rxComp);\n\twhile (rcd->gen == rq->comp_ring.gen) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tstruct sk_buff *skb, *new_skb = NULL;\n\t\tstruct page *new_page = NULL;\n\t\tdma_addr_t new_dma_addr;\n\t\tint num_to_alloc;\n\t\tstruct Vmxnet3_RxDesc *rxd;\n\t\tu32 idx, ring_idx;\n\t\tstruct vmxnet3_cmd_ring\t*ring = NULL;\n\t\tif (num_pkts >= quota) {\n\t\t\t/* we may stop even before we see the EOP desc of\n\t\t\t * the current pkt\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Prevent any rcd field from being (speculatively) read before\n\t\t * rcd->gen is read.\n\t\t */\n\t\tdma_rmb();\n\n\t\tBUG_ON(rcd->rqID != rq->qid && rcd->rqID != rq->qid2 &&\n\t\t       rcd->rqID != rq->dataRingQid);\n\t\tidx = rcd->rxdIdx;\n\t\tring_idx = VMXNET3_GET_RING_IDX(adapter, rcd->rqID);\n\t\tring = rq->rx_ring + ring_idx;\n\t\tvmxnet3_getRxDesc(rxd, &rq->rx_ring[ring_idx].base[idx].rxd,\n\t\t\t\t  &rxCmdDesc);\n\t\trbi = rq->buf_info[ring_idx] + idx;\n\n\t\tBUG_ON(rxd->addr != rbi->dma_addr ||\n\t\t       rxd->len != rbi->len);\n\n\t\tif (unlikely(rcd->eop && rcd->err)) {\n\t\t\tvmxnet3_rx_error(rq, rcd, ctx, adapter);\n\t\t\tgoto rcd_done;\n\t\t}\n\n\t\tif (rcd->sop) { /* first buf of the pkt */\n\t\t\tbool rxDataRingUsed;\n\t\t\tu16 len;\n\n\t\t\tBUG_ON(rxd->btype != VMXNET3_RXD_BTYPE_HEAD ||\n\t\t\t       (rcd->rqID != rq->qid &&\n\t\t\t\trcd->rqID != rq->dataRingQid));\n\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_SKB);\n\t\t\tBUG_ON(ctx->skb != NULL || rbi->skb == NULL);\n\n\t\t\tif (unlikely(rcd->len == 0)) {\n\t\t\t\t/* Pretend the rx buffer is skipped. */\n\t\t\t\tBUG_ON(!(rcd->sop && rcd->eop));\n\t\t\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\t\t\"rxRing[%u][%u] 0 length\\n\",\n\t\t\t\t\tring_idx, idx);\n\t\t\t\tgoto rcd_done;\n\t\t\t}\n\n\t\t\tskip_page_frags = false;\n\t\t\tctx->skb = rbi->skb;\n\n\t\t\trxDataRingUsed =\n\t\t\t\tVMXNET3_RX_DATA_RING(adapter, rcd->rqID);\n\t\t\tlen = rxDataRingUsed ? rcd->len : rbi->len;\n\t\t\tnew_skb = netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t    len);\n\t\t\tif (new_skb == NULL) {\n\t\t\t\t/* Skb allocation failed, do not handover this\n\t\t\t\t * skb to stack. Reuse it. Drop the existing pkt\n\t\t\t\t */\n\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\tctx->skb = NULL;\n\t\t\t\trq->stats.drop_total++;\n\t\t\t\tskip_page_frags = true;\n\t\t\t\tgoto rcd_done;\n\t\t\t}\n\n\t\t\tif (rxDataRingUsed) {\n\t\t\t\tsize_t sz;\n\n\t\t\t\tBUG_ON(rcd->len > rq->data_ring.desc_size);\n\n\t\t\t\tctx->skb = new_skb;\n\t\t\t\tsz = rcd->rxdIdx * rq->data_ring.desc_size;\n\t\t\t\tmemcpy(new_skb->data,\n\t\t\t\t       &rq->data_ring.base[sz], rcd->len);\n\t\t\t} else {\n\t\t\t\tctx->skb = rbi->skb;\n\n\t\t\t\tnew_dma_addr =\n\t\t\t\t\tdma_map_single(&adapter->pdev->dev,\n\t\t\t\t\t\t       new_skb->data, rbi->len,\n\t\t\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      new_dma_addr)) {\n\t\t\t\t\tdev_kfree_skb(new_skb);\n\t\t\t\t\t/* Skb allocation failed, do not\n\t\t\t\t\t * handover this skb to stack. Reuse\n\t\t\t\t\t * it. Drop the existing pkt.\n\t\t\t\t\t */\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\trq->stats.drop_total++;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\n\t\t\t\tdma_unmap_single(&adapter->pdev->dev,\n\t\t\t\t\t\t rbi->dma_addr,\n\t\t\t\t\t\t rbi->len,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t\t\t/* Immediate refill */\n\t\t\t\trbi->skb = new_skb;\n\t\t\t\trbi->dma_addr = new_dma_addr;\n\t\t\t\trxd->addr = cpu_to_le64(rbi->dma_addr);\n\t\t\t\trxd->len = rbi->len;\n\t\t\t}\n\n#ifdef VMXNET3_RSS\n\t\t\tif (rcd->rssType != VMXNET3_RCD_RSS_TYPE_NONE &&\n\t\t\t    (adapter->netdev->features & NETIF_F_RXHASH)) {\n\t\t\t\tenum pkt_hash_types hash_type;\n\n\t\t\t\tswitch (rcd->rssType) {\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_IPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_IPV6:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\t\t\tbreak;\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_TCPIPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_TCPIPV6:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_UDPIPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_UDPIPV6:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L4;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tskb_set_hash(ctx->skb,\n\t\t\t\t\t     le32_to_cpu(rcd->rssHash),\n\t\t\t\t\t     hash_type);\n\t\t\t}\n#endif\n\t\t\tskb_put(ctx->skb, rcd->len);\n\n\t\t\tif (VMXNET3_VERSION_GE_2(adapter) &&\n\t\t\t    rcd->type == VMXNET3_CDTYPE_RXCOMP_LRO) {\n\t\t\t\tstruct Vmxnet3_RxCompDescExt *rcdlro;\n\t\t\t\trcdlro = (struct Vmxnet3_RxCompDescExt *)rcd;\n\n\t\t\t\tsegCnt = rcdlro->segCnt;\n\t\t\t\tWARN_ON_ONCE(segCnt == 0);\n\t\t\t\tmss = rcdlro->mss;\n\t\t\t\tif (unlikely(segCnt <= 1))\n\t\t\t\t\tsegCnt = 0;\n\t\t\t} else {\n\t\t\t\tsegCnt = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(ctx->skb == NULL && !skip_page_frags);\n\n\t\t\t/* non SOP buffer must be type 1 in most cases */\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE);\n\t\t\tBUG_ON(rxd->btype != VMXNET3_RXD_BTYPE_BODY);\n\n\t\t\t/* If an sop buffer was dropped, skip all\n\t\t\t * following non-sop fragments. They will be reused.\n\t\t\t */\n\t\t\tif (skip_page_frags)\n\t\t\t\tgoto rcd_done;\n\n\t\t\tif (rcd->len) {\n\t\t\t\tnew_page = alloc_page(GFP_ATOMIC);\n\t\t\t\t/* Replacement page frag could not be allocated.\n\t\t\t\t * Reuse this page. Drop the pkt and free the\n\t\t\t\t * skb which contained this page as a frag. Skip\n\t\t\t\t * processing all the following non-sop frags.\n\t\t\t\t */\n\t\t\t\tif (unlikely(!new_page)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tdev_kfree_skb(ctx->skb);\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\t\t\t\tnew_dma_addr = dma_map_page(&adapter->pdev->dev,\n\t\t\t\t\t\t\t    new_page,\n\t\t\t\t\t\t\t    0, PAGE_SIZE,\n\t\t\t\t\t\t\t    DMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      new_dma_addr)) {\n\t\t\t\t\tput_page(new_page);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tdev_kfree_skb(ctx->skb);\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\n\t\t\t\tdma_unmap_page(&adapter->pdev->dev,\n\t\t\t\t\t       rbi->dma_addr, rbi->len,\n\t\t\t\t\t       DMA_FROM_DEVICE);\n\n\t\t\t\tvmxnet3_append_frag(ctx->skb, rcd, rbi);\n\n\t\t\t\t/* Immediate refill */\n\t\t\t\trbi->page = new_page;\n\t\t\t\trbi->dma_addr = new_dma_addr;\n\t\t\t\trxd->addr = cpu_to_le64(rbi->dma_addr);\n\t\t\t\trxd->len = rbi->len;\n\t\t\t}\n\t\t}\n\n\n\t\tskb = ctx->skb;\n\t\tif (rcd->eop) {\n\t\t\tu32 mtu = adapter->netdev->mtu;\n\t\t\tskb->len += skb->data_len;\n\n\t\t\tvmxnet3_rx_csum(adapter, skb,\n\t\t\t\t\t(union Vmxnet3_GenericDesc *)rcd);\n\t\t\tskb->protocol = eth_type_trans(skb, adapter->netdev);\n\t\t\tif (!rcd->tcp ||\n\t\t\t    !(adapter->netdev->features & NETIF_F_LRO))\n\t\t\t\tgoto not_lro;\n\n\t\t\tif (segCnt != 0 && mss != 0) {\n\t\t\t\tskb_shinfo(skb)->gso_type = rcd->v4 ?\n\t\t\t\t\tSKB_GSO_TCPV4 : SKB_GSO_TCPV6;\n\t\t\t\tskb_shinfo(skb)->gso_size = mss;\n\t\t\t\tskb_shinfo(skb)->gso_segs = segCnt;\n\t\t\t} else if (segCnt != 0 || skb->len > mtu) {\n\t\t\t\tu32 hlen;\n\n\t\t\t\thlen = vmxnet3_get_hdr_len(adapter, skb,\n\t\t\t\t\t(union Vmxnet3_GenericDesc *)rcd);\n\t\t\t\tif (hlen == 0)\n\t\t\t\t\tgoto not_lro;\n\n\t\t\t\tskb_shinfo(skb)->gso_type =\n\t\t\t\t\trcd->v4 ? SKB_GSO_TCPV4 : SKB_GSO_TCPV6;\n\t\t\t\tif (segCnt != 0) {\n\t\t\t\t\tskb_shinfo(skb)->gso_segs = segCnt;\n\t\t\t\t\tskb_shinfo(skb)->gso_size =\n\t\t\t\t\t\tDIV_ROUND_UP(skb->len -\n\t\t\t\t\t\t\thlen, segCnt);\n\t\t\t\t} else {\n\t\t\t\t\tskb_shinfo(skb)->gso_size = mtu - hlen;\n\t\t\t\t}\n\t\t\t}\nnot_lro:\n\t\t\tif (unlikely(rcd->ts))\n\t\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), rcd->tci);\n\n\t\t\tif (adapter->netdev->features & NETIF_F_LRO)\n\t\t\t\tnetif_receive_skb(skb);\n\t\t\telse\n\t\t\t\tnapi_gro_receive(&rq->napi, skb);\n\n\t\t\tctx->skb = NULL;\n\t\t\tnum_pkts++;\n\t\t}\n\nrcd_done:\n\t\t/* device may have skipped some rx descs */\n\t\tring->next2comp = idx;\n\t\tnum_to_alloc = vmxnet3_cmd_ring_desc_avail(ring);\n\t\tring = rq->rx_ring + ring_idx;\n\n\t\t/* Ensure that the writes to rxd->gen bits will be observed\n\t\t * after all other writes to rxd objects.\n\t\t */\n\t\tdma_wmb();\n\n\t\twhile (num_to_alloc) {\n\t\t\tvmxnet3_getRxDesc(rxd, &ring->base[ring->next2fill].rxd,\n\t\t\t\t\t  &rxCmdDesc);\n\t\t\tBUG_ON(!rxd->addr);\n\n\t\t\t/* Recv desc is ready to be used by the device */\n\t\t\trxd->gen = ring->gen;\n\t\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t\t\tnum_to_alloc--;\n\t\t}\n\n\t\t/* if needed, update the register */\n\t\tif (unlikely(rq->shared->updateRxProd)) {\n\t\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\t\t       rxprod_reg[ring_idx] + rq->qid * 8,\n\t\t\t\t\t       ring->next2fill);\n\t\t}\n\n\t\tvmxnet3_comp_ring_adv_next2proc(&rq->comp_ring);\n\t\tvmxnet3_getRxComp(rcd,\n\t\t\t\t  &rq->comp_ring.base[rq->comp_ring.next2proc].rcd, &rxComp);\n\t}\n\n\treturn num_pkts;\n}\n\n\nstatic void\nvmxnet3_rq_cleanup(struct vmxnet3_rx_queue *rq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tu32 i, ring_idx;\n\tstruct Vmxnet3_RxDesc *rxd;\n\n\tfor (ring_idx = 0; ring_idx < 2; ring_idx++) {\n\t\tfor (i = 0; i < rq->rx_ring[ring_idx].size; i++) {\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t\tstruct Vmxnet3_RxDesc rxDesc;\n#endif\n\t\t\tvmxnet3_getRxDesc(rxd,\n\t\t\t\t&rq->rx_ring[ring_idx].base[i].rxd, &rxDesc);\n\n\t\t\tif (rxd->btype == VMXNET3_RXD_BTYPE_HEAD &&\n\t\t\t\t\trq->buf_info[ring_idx][i].skb) {\n\t\t\t\tdma_unmap_single(&adapter->pdev->dev, rxd->addr,\n\t\t\t\t\t\t rxd->len, DMA_FROM_DEVICE);\n\t\t\t\tdev_kfree_skb(rq->buf_info[ring_idx][i].skb);\n\t\t\t\trq->buf_info[ring_idx][i].skb = NULL;\n\t\t\t} else if (rxd->btype == VMXNET3_RXD_BTYPE_BODY &&\n\t\t\t\t\trq->buf_info[ring_idx][i].page) {\n\t\t\t\tdma_unmap_page(&adapter->pdev->dev, rxd->addr,\n\t\t\t\t\t       rxd->len, DMA_FROM_DEVICE);\n\t\t\t\tput_page(rq->buf_info[ring_idx][i].page);\n\t\t\t\trq->buf_info[ring_idx][i].page = NULL;\n\t\t\t}\n\t\t}\n\n\t\trq->rx_ring[ring_idx].gen = VMXNET3_INIT_GEN;\n\t\trq->rx_ring[ring_idx].next2fill =\n\t\t\t\t\trq->rx_ring[ring_idx].next2comp = 0;\n\t}\n\n\trq->comp_ring.gen = VMXNET3_INIT_GEN;\n\trq->comp_ring.next2proc = 0;\n}\n\n\nstatic void\nvmxnet3_rq_cleanup_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tvmxnet3_rq_cleanup(&adapter->rx_queue[i], adapter);\n}\n\n\nstatic void vmxnet3_rq_destroy(struct vmxnet3_rx_queue *rq,\n\t\t\t       struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tint j;\n\n\t/* all rx buffers must have already been freed */\n\tfor (i = 0; i < 2; i++) {\n\t\tif (rq->buf_info[i]) {\n\t\t\tfor (j = 0; j < rq->rx_ring[i].size; j++)\n\t\t\t\tBUG_ON(rq->buf_info[i][j].page != NULL);\n\t\t}\n\t}\n\n\n\tfor (i = 0; i < 2; i++) {\n\t\tif (rq->rx_ring[i].base) {\n\t\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t\t  rq->rx_ring[i].size\n\t\t\t\t\t  * sizeof(struct Vmxnet3_RxDesc),\n\t\t\t\t\t  rq->rx_ring[i].base,\n\t\t\t\t\t  rq->rx_ring[i].basePA);\n\t\t\trq->rx_ring[i].base = NULL;\n\t\t}\n\t}\n\n\tif (rq->data_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  rq->rx_ring[0].size * rq->data_ring.desc_size,\n\t\t\t\t  rq->data_ring.base, rq->data_ring.basePA);\n\t\trq->data_ring.base = NULL;\n\t}\n\n\tif (rq->comp_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, rq->comp_ring.size\n\t\t\t\t  * sizeof(struct Vmxnet3_RxCompDesc),\n\t\t\t\t  rq->comp_ring.base, rq->comp_ring.basePA);\n\t\trq->comp_ring.base = NULL;\n\t}\n\n\tkfree(rq->buf_info[0]);\n\trq->buf_info[0] = NULL;\n\trq->buf_info[1] = NULL;\n}\n\nstatic void\nvmxnet3_rq_destroy_all_rxdataring(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\n\t\tif (rq->data_ring.base) {\n\t\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t\t  (rq->rx_ring[0].size *\n\t\t\t\t\t  rq->data_ring.desc_size),\n\t\t\t\t\t  rq->data_ring.base,\n\t\t\t\t\t  rq->data_ring.basePA);\n\t\t\trq->data_ring.base = NULL;\n\t\t\trq->data_ring.desc_size = 0;\n\t\t}\n\t}\n}\n\nstatic int\nvmxnet3_rq_init(struct vmxnet3_rx_queue *rq,\n\t\tstruct vmxnet3_adapter  *adapter)\n{\n\tint i;\n\n\t/* initialize buf_info */\n\tfor (i = 0; i < rq->rx_ring[0].size; i++) {\n\n\t\t/* 1st buf for a pkt is skbuff */\n\t\tif (i % adapter->rx_buf_per_pkt == 0) {\n\t\t\trq->buf_info[0][i].buf_type = VMXNET3_RX_BUF_SKB;\n\t\t\trq->buf_info[0][i].len = adapter->skb_buf_size;\n\t\t} else { /* subsequent bufs for a pkt is frag */\n\t\t\trq->buf_info[0][i].buf_type = VMXNET3_RX_BUF_PAGE;\n\t\t\trq->buf_info[0][i].len = PAGE_SIZE;\n\t\t}\n\t}\n\tfor (i = 0; i < rq->rx_ring[1].size; i++) {\n\t\trq->buf_info[1][i].buf_type = VMXNET3_RX_BUF_PAGE;\n\t\trq->buf_info[1][i].len = PAGE_SIZE;\n\t}\n\n\t/* reset internal state and allocate buffers for both rings */\n\tfor (i = 0; i < 2; i++) {\n\t\trq->rx_ring[i].next2fill = rq->rx_ring[i].next2comp = 0;\n\n\t\tmemset(rq->rx_ring[i].base, 0, rq->rx_ring[i].size *\n\t\t       sizeof(struct Vmxnet3_RxDesc));\n\t\trq->rx_ring[i].gen = VMXNET3_INIT_GEN;\n\t}\n\tif (vmxnet3_rq_alloc_rx_buf(rq, 0, rq->rx_ring[0].size - 1,\n\t\t\t\t    adapter) == 0) {\n\t\t/* at least has 1 rx buffer for the 1st ring */\n\t\treturn -ENOMEM;\n\t}\n\tvmxnet3_rq_alloc_rx_buf(rq, 1, rq->rx_ring[1].size - 1, adapter);\n\n\t/* reset the comp ring */\n\trq->comp_ring.next2proc = 0;\n\tmemset(rq->comp_ring.base, 0, rq->comp_ring.size *\n\t       sizeof(struct Vmxnet3_RxCompDesc));\n\trq->comp_ring.gen = VMXNET3_INIT_GEN;\n\n\t/* reset rxctx */\n\trq->rx_ctx.skb = NULL;\n\n\t/* stats are not reset */\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_rq_init_all(struct vmxnet3_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = vmxnet3_rq_init(&adapter->rx_queue[i], adapter);\n\t\tif (unlikely(err)) {\n\t\t\tdev_err(&adapter->netdev->dev, \"%s: failed to \"\n\t\t\t\t\"initialize rx queue%i\\n\",\n\t\t\t\tadapter->netdev->name, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n\n}\n\n\nstatic int\nvmxnet3_rq_create(struct vmxnet3_rx_queue *rq, struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tsize_t sz;\n\tstruct vmxnet3_rx_buf_info *bi;\n\n\tfor (i = 0; i < 2; i++) {\n\n\t\tsz = rq->rx_ring[i].size * sizeof(struct Vmxnet3_RxDesc);\n\t\trq->rx_ring[i].base = dma_alloc_coherent(\n\t\t\t\t\t\t&adapter->pdev->dev, sz,\n\t\t\t\t\t\t&rq->rx_ring[i].basePA,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!rq->rx_ring[i].base) {\n\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t   \"failed to allocate rx ring %d\\n\", i);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tif ((adapter->rxdataring_enabled) && (rq->data_ring.desc_size != 0)) {\n\t\tsz = rq->rx_ring[0].size * rq->data_ring.desc_size;\n\t\trq->data_ring.base =\n\t\t\tdma_alloc_coherent(&adapter->pdev->dev, sz,\n\t\t\t\t\t   &rq->data_ring.basePA,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!rq->data_ring.base) {\n\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t   \"rx data ring will be disabled\\n\");\n\t\t\tadapter->rxdataring_enabled = false;\n\t\t}\n\t} else {\n\t\trq->data_ring.base = NULL;\n\t\trq->data_ring.desc_size = 0;\n\t}\n\n\tsz = rq->comp_ring.size * sizeof(struct Vmxnet3_RxCompDesc);\n\trq->comp_ring.base = dma_alloc_coherent(&adapter->pdev->dev, sz,\n\t\t\t\t\t\t&rq->comp_ring.basePA,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!rq->comp_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate rx comp ring\\n\");\n\t\tgoto err;\n\t}\n\n\tbi = kcalloc_node(rq->rx_ring[0].size + rq->rx_ring[1].size,\n\t\t\t  sizeof(rq->buf_info[0][0]), GFP_KERNEL,\n\t\t\t  dev_to_node(&adapter->pdev->dev));\n\tif (!bi)\n\t\tgoto err;\n\n\trq->buf_info[0] = bi;\n\trq->buf_info[1] = bi + rq->rx_ring[0].size;\n\n\treturn 0;\n\nerr:\n\tvmxnet3_rq_destroy(rq, adapter);\n\treturn -ENOMEM;\n}\n\n\nstatic int\nvmxnet3_rq_create_all(struct vmxnet3_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tadapter->rxdataring_enabled = VMXNET3_VERSION_GE_3(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = vmxnet3_rq_create(&adapter->rx_queue[i], adapter);\n\t\tif (unlikely(err)) {\n\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\"%s: failed to create rx queue%i\\n\",\n\t\t\t\tadapter->netdev->name, i);\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tif (!adapter->rxdataring_enabled)\n\t\tvmxnet3_rq_destroy_all_rxdataring(adapter);\n\n\treturn err;\nerr_out:\n\tvmxnet3_rq_destroy_all(adapter);\n\treturn err;\n\n}\n\n/* Multiple queue aware polling function for tx and rx */\n\nstatic int\nvmxnet3_do_poll(struct vmxnet3_adapter *adapter, int budget)\n{\n\tint rcd_done = 0, i;\n\tif (unlikely(adapter->shared->ecr))\n\t\tvmxnet3_process_events(adapter);\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_tx_complete(&adapter->tx_queue[i], adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\trcd_done += vmxnet3_rq_rx_complete(&adapter->rx_queue[i],\n\t\t\t\t\t\t   adapter, budget);\n\treturn rcd_done;\n}\n\n\nstatic int\nvmxnet3_poll(struct napi_struct *napi, int budget)\n{\n\tstruct vmxnet3_rx_queue *rx_queue = container_of(napi,\n\t\t\t\t\t  struct vmxnet3_rx_queue, napi);\n\tint rxd_done;\n\n\trxd_done = vmxnet3_do_poll(rx_queue->adapter, budget);\n\n\tif (rxd_done < budget) {\n\t\tnapi_complete_done(napi, rxd_done);\n\t\tvmxnet3_enable_all_intrs(rx_queue->adapter);\n\t}\n\treturn rxd_done;\n}\n\n/*\n * NAPI polling function for MSI-X mode with multiple Rx queues\n * Returns the # of the NAPI credit consumed (# of rx descriptors processed)\n */\n\nstatic int\nvmxnet3_poll_rx_only(struct napi_struct *napi, int budget)\n{\n\tstruct vmxnet3_rx_queue *rq = container_of(napi,\n\t\t\t\t\t\tstruct vmxnet3_rx_queue, napi);\n\tstruct vmxnet3_adapter *adapter = rq->adapter;\n\tint rxd_done;\n\n\t/* When sharing interrupt with corresponding tx queue, process\n\t * tx completions in that queue as well\n\t */\n\tif (adapter->share_intr == VMXNET3_INTR_BUDDYSHARE) {\n\t\tstruct vmxnet3_tx_queue *tq =\n\t\t\t\t&adapter->tx_queue[rq - adapter->rx_queue];\n\t\tvmxnet3_tq_tx_complete(tq, adapter);\n\t}\n\n\trxd_done = vmxnet3_rq_rx_complete(rq, adapter, budget);\n\n\tif (rxd_done < budget) {\n\t\tnapi_complete_done(napi, rxd_done);\n\t\tvmxnet3_enable_intr(adapter, rq->comp_ring.intr_idx);\n\t}\n\treturn rxd_done;\n}\n\n\n#ifdef CONFIG_PCI_MSI\n\n/*\n * Handle completion interrupts on tx queues\n * Returns whether or not the intr is handled\n */\n\nstatic irqreturn_t\nvmxnet3_msix_tx(int irq, void *data)\n{\n\tstruct vmxnet3_tx_queue *tq = data;\n\tstruct vmxnet3_adapter *adapter = tq->adapter;\n\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, tq->comp_ring.intr_idx);\n\n\t/* Handle the case where only one irq is allocate for all tx queues */\n\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE) {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tstruct vmxnet3_tx_queue *txq = &adapter->tx_queue[i];\n\t\t\tvmxnet3_tq_tx_complete(txq, adapter);\n\t\t}\n\t} else {\n\t\tvmxnet3_tq_tx_complete(tq, adapter);\n\t}\n\tvmxnet3_enable_intr(adapter, tq->comp_ring.intr_idx);\n\n\treturn IRQ_HANDLED;\n}\n\n\n/*\n * Handle completion interrupts on rx queues. Returns whether or not the\n * intr is handled\n */\n\nstatic irqreturn_t\nvmxnet3_msix_rx(int irq, void *data)\n{\n\tstruct vmxnet3_rx_queue *rq = data;\n\tstruct vmxnet3_adapter *adapter = rq->adapter;\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, rq->comp_ring.intr_idx);\n\tnapi_schedule(&rq->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/*\n *----------------------------------------------------------------------------\n *\n * vmxnet3_msix_event --\n *\n *    vmxnet3 msix event intr handler\n *\n * Result:\n *    whether or not the intr is handled\n *\n *----------------------------------------------------------------------------\n */\n\nstatic irqreturn_t\nvmxnet3_msix_event(int irq, void *data)\n{\n\tstruct net_device *dev = data;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(dev);\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, adapter->intr.event_intr_idx);\n\n\tif (adapter->shared->ecr)\n\t\tvmxnet3_process_events(adapter);\n\n\tvmxnet3_enable_intr(adapter, adapter->intr.event_intr_idx);\n\n\treturn IRQ_HANDLED;\n}\n\n#endif /* CONFIG_PCI_MSI  */\n\n\n/* Interrupt handler for vmxnet3  */\nstatic irqreturn_t\nvmxnet3_intr(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(dev);\n\n\tif (adapter->intr.type == VMXNET3_IT_INTX) {\n\t\tu32 icr = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_ICR);\n\t\tif (unlikely(icr == 0))\n\t\t\t/* not ours */\n\t\t\treturn IRQ_NONE;\n\t}\n\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_all_intrs(adapter);\n\n\tnapi_schedule(&adapter->rx_queue[0].napi);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\n/* netpoll callback. */\nstatic void\nvmxnet3_netpoll(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tswitch (adapter->intr.type) {\n#ifdef CONFIG_PCI_MSI\n\tcase VMXNET3_IT_MSIX: {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tvmxnet3_msix_rx(0, &adapter->rx_queue[i]);\n\t\tbreak;\n\t}\n#endif\n\tcase VMXNET3_IT_MSI:\n\tdefault:\n\t\tvmxnet3_intr(0, adapter->netdev);\n\t\tbreak;\n\t}\n\n}\n#endif\t/* CONFIG_NET_POLL_CONTROLLER */\n\nstatic int\nvmxnet3_request_irqs(struct vmxnet3_adapter *adapter)\n{\n\tstruct vmxnet3_intr *intr = &adapter->intr;\n\tint err = 0, i;\n\tint vector = 0;\n\n#ifdef CONFIG_PCI_MSI\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE) {\n\t\t\t\tsprintf(adapter->tx_queue[i].name, \"%s-tx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\t\terr = request_irq(\n\t\t\t\t\t      intr->msix_entries[vector].vector,\n\t\t\t\t\t      vmxnet3_msix_tx, 0,\n\t\t\t\t\t      adapter->tx_queue[i].name,\n\t\t\t\t\t      &adapter->tx_queue[i]);\n\t\t\t} else {\n\t\t\t\tsprintf(adapter->tx_queue[i].name, \"%s-rxtx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\t}\n\t\t\tif (err) {\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"Failed to request irq for MSIX, %s, \"\n\t\t\t\t\t\"error %d\\n\",\n\t\t\t\t\tadapter->tx_queue[i].name, err);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Handle the case where only 1 MSIx was allocated for\n\t\t\t * all tx queues */\n\t\t\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE) {\n\t\t\t\tfor (; i < adapter->num_tx_queues; i++)\n\t\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx\n\t\t\t\t\t\t\t\t= vector;\n\t\t\t\tvector++;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx\n\t\t\t\t\t\t\t\t= vector++;\n\t\t\t}\n\t\t}\n\t\tif (adapter->share_intr == VMXNET3_INTR_BUDDYSHARE)\n\t\t\tvector = 0;\n\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE)\n\t\t\t\tsprintf(adapter->rx_queue[i].name, \"%s-rx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\telse\n\t\t\t\tsprintf(adapter->rx_queue[i].name, \"%s-rxtx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\terr = request_irq(intr->msix_entries[vector].vector,\n\t\t\t\t\t  vmxnet3_msix_rx, 0,\n\t\t\t\t\t  adapter->rx_queue[i].name,\n\t\t\t\t\t  &(adapter->rx_queue[i]));\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Failed to request irq for MSIX, \"\n\t\t\t\t\t   \"%s, error %d\\n\",\n\t\t\t\t\t   adapter->rx_queue[i].name, err);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tadapter->rx_queue[i].comp_ring.intr_idx = vector++;\n\t\t}\n\n\t\tsprintf(intr->event_msi_vector_name, \"%s-event-%d\",\n\t\t\tadapter->netdev->name, vector);\n\t\terr = request_irq(intr->msix_entries[vector].vector,\n\t\t\t\t  vmxnet3_msix_event, 0,\n\t\t\t\t  intr->event_msi_vector_name, adapter->netdev);\n\t\tintr->event_intr_idx = vector;\n\n\t} else if (intr->type == VMXNET3_IT_MSI) {\n\t\tadapter->num_rx_queues = 1;\n\t\terr = request_irq(adapter->pdev->irq, vmxnet3_intr, 0,\n\t\t\t\t  adapter->netdev->name, adapter->netdev);\n\t} else {\n#endif\n\t\tadapter->num_rx_queues = 1;\n\t\terr = request_irq(adapter->pdev->irq, vmxnet3_intr,\n\t\t\t\t  IRQF_SHARED, adapter->netdev->name,\n\t\t\t\t  adapter->netdev);\n#ifdef CONFIG_PCI_MSI\n\t}\n#endif\n\tintr->num_intrs = vector + 1;\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to request irq (intr type:%d), error %d\\n\",\n\t\t\t   intr->type, err);\n\t} else {\n\t\t/* Number of rx queues will not change after this */\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\t\t\trq->qid = i;\n\t\t\trq->qid2 = i + adapter->num_rx_queues;\n\t\t\trq->dataRingQid = i + 2 * adapter->num_rx_queues;\n\t\t}\n\n\t\t/* init our intr settings */\n\t\tfor (i = 0; i < intr->num_intrs; i++)\n\t\t\tintr->mod_levels[i] = UPT1_IML_ADAPTIVE;\n\t\tif (adapter->intr.type != VMXNET3_IT_MSIX) {\n\t\t\tadapter->intr.event_intr_idx = 0;\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx = 0;\n\t\t\tadapter->rx_queue[0].comp_ring.intr_idx = 0;\n\t\t}\n\n\t\tnetdev_info(adapter->netdev,\n\t\t\t    \"intr type %u, mode %u, %u vectors allocated\\n\",\n\t\t\t    intr->type, intr->mask_mode, intr->num_intrs);\n\t}\n\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_free_irqs(struct vmxnet3_adapter *adapter)\n{\n\tstruct vmxnet3_intr *intr = &adapter->intr;\n\tBUG_ON(intr->type == VMXNET3_IT_AUTO || intr->num_intrs <= 0);\n\n\tswitch (intr->type) {\n#ifdef CONFIG_PCI_MSI\n\tcase VMXNET3_IT_MSIX:\n\t{\n\t\tint i, vector = 0;\n\n\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\t\tfree_irq(intr->msix_entries[vector++].vector,\n\t\t\t\t\t &(adapter->tx_queue[i]));\n\t\t\t\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tfree_irq(intr->msix_entries[vector++].vector,\n\t\t\t\t &(adapter->rx_queue[i]));\n\t\t}\n\n\t\tfree_irq(intr->msix_entries[vector].vector,\n\t\t\t adapter->netdev);\n\t\tBUG_ON(vector >= intr->num_intrs);\n\t\tbreak;\n\t}\n#endif\n\tcase VMXNET3_IT_MSI:\n\t\tfree_irq(adapter->pdev->irq, adapter->netdev);\n\t\tbreak;\n\tcase VMXNET3_IT_INTX:\n\t\tfree_irq(adapter->pdev->irq, adapter->netdev);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n\nstatic void\nvmxnet3_restore_vlan(struct vmxnet3_adapter *adapter)\n{\n\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\tu16 vid;\n\n\t/* allow untagged pkts */\n\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, 0);\n\n\tfor_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)\n\t\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, vid);\n}\n\n\nstatic int\nvmxnet3_vlan_rx_add_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!(netdev->flags & IFF_PROMISC)) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tunsigned long flags;\n\n\t\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, vid);\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t}\n\n\tset_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!(netdev->flags & IFF_PROMISC)) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tunsigned long flags;\n\n\t\tVMXNET3_CLEAR_VFTABLE_ENTRY(vfTable, vid);\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t}\n\n\tclear_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\n\nstatic u8 *\nvmxnet3_copy_mc(struct net_device *netdev)\n{\n\tu8 *buf = NULL;\n\tu32 sz = netdev_mc_count(netdev) * ETH_ALEN;\n\n\t/* struct Vmxnet3_RxFilterConf.mfTableLen is u16. */\n\tif (sz <= 0xffff) {\n\t\t/* We may be called with BH disabled */\n\t\tbuf = kmalloc(sz, GFP_ATOMIC);\n\t\tif (buf) {\n\t\t\tstruct netdev_hw_addr *ha;\n\t\t\tint i = 0;\n\n\t\t\tnetdev_for_each_mc_addr(ha, netdev)\n\t\t\t\tmemcpy(buf + i++ * ETH_ALEN, ha->addr,\n\t\t\t\t       ETH_ALEN);\n\t\t}\n\t}\n\treturn buf;\n}\n\n\nstatic void\nvmxnet3_set_mc(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tunsigned long flags;\n\tstruct Vmxnet3_RxFilterConf *rxConf =\n\t\t\t\t\t&adapter->shared->devRead.rxFilterConf;\n\tu8 *new_table = NULL;\n\tdma_addr_t new_table_pa = 0;\n\tbool new_table_pa_valid = false;\n\tu32 new_mode = VMXNET3_RXM_UCAST;\n\n\tif (netdev->flags & IFF_PROMISC) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tmemset(vfTable, 0, VMXNET3_VFT_SIZE * sizeof(*vfTable));\n\n\t\tnew_mode |= VMXNET3_RXM_PROMISC;\n\t} else {\n\t\tvmxnet3_restore_vlan(adapter);\n\t}\n\n\tif (netdev->flags & IFF_BROADCAST)\n\t\tnew_mode |= VMXNET3_RXM_BCAST;\n\n\tif (netdev->flags & IFF_ALLMULTI)\n\t\tnew_mode |= VMXNET3_RXM_ALL_MULTI;\n\telse\n\t\tif (!netdev_mc_empty(netdev)) {\n\t\t\tnew_table = vmxnet3_copy_mc(netdev);\n\t\t\tif (new_table) {\n\t\t\t\tsize_t sz = netdev_mc_count(netdev) * ETH_ALEN;\n\n\t\t\t\trxConf->mfTableLen = cpu_to_le16(sz);\n\t\t\t\tnew_table_pa = dma_map_single(\n\t\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\t\tnew_table,\n\t\t\t\t\t\t\tsz,\n\t\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t\tif (!dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t       new_table_pa)) {\n\t\t\t\t\tnew_mode |= VMXNET3_RXM_MCAST;\n\t\t\t\t\tnew_table_pa_valid = true;\n\t\t\t\t\trxConf->mfTablePA = cpu_to_le64(\n\t\t\t\t\t\t\t\tnew_table_pa);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!new_table_pa_valid) {\n\t\t\t\tnetdev_info(netdev,\n\t\t\t\t\t    \"failed to copy mcast list, setting ALL_MULTI\\n\");\n\t\t\t\tnew_mode |= VMXNET3_RXM_ALL_MULTI;\n\t\t\t}\n\t\t}\n\n\tif (!(new_mode & VMXNET3_RXM_MCAST)) {\n\t\trxConf->mfTableLen = 0;\n\t\trxConf->mfTablePA = 0;\n\t}\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tif (new_mode != rxConf->rxMode) {\n\t\trxConf->rxMode = cpu_to_le32(new_mode);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_RX_MODE);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t}\n\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_UPDATE_MAC_FILTERS);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tif (new_table_pa_valid)\n\t\tdma_unmap_single(&adapter->pdev->dev, new_table_pa,\n\t\t\t\t rxConf->mfTableLen, DMA_TO_DEVICE);\n\tkfree(new_table);\n}\n\nvoid\nvmxnet3_rq_destroy_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tvmxnet3_rq_destroy(&adapter->rx_queue[i], adapter);\n}\n\n\n/*\n *   Set up driver_shared based on settings in adapter.\n */\n\nstatic void\nvmxnet3_setup_driver_shared(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tstruct Vmxnet3_DSDevRead *devRead = &shared->devRead;\n\tstruct Vmxnet3_DSDevReadExt *devReadExt = &shared->devReadExt;\n\tstruct Vmxnet3_TxQueueConf *tqc;\n\tstruct Vmxnet3_RxQueueConf *rqc;\n\tint i;\n\n\tmemset(shared, 0, sizeof(*shared));\n\n\t/* driver settings */\n\tshared->magic = cpu_to_le32(VMXNET3_REV1_MAGIC);\n\tdevRead->misc.driverInfo.version = cpu_to_le32(\n\t\t\t\t\t\tVMXNET3_DRIVER_VERSION_NUM);\n\tdevRead->misc.driverInfo.gos.gosBits = (sizeof(void *) == 4 ?\n\t\t\t\tVMXNET3_GOS_BITS_32 : VMXNET3_GOS_BITS_64);\n\tdevRead->misc.driverInfo.gos.gosType = VMXNET3_GOS_TYPE_LINUX;\n\t*((u32 *)&devRead->misc.driverInfo.gos) = cpu_to_le32(\n\t\t\t\t*((u32 *)&devRead->misc.driverInfo.gos));\n\tdevRead->misc.driverInfo.vmxnet3RevSpt = cpu_to_le32(1);\n\tdevRead->misc.driverInfo.uptVerSpt = cpu_to_le32(1);\n\n\tdevRead->misc.ddPA = cpu_to_le64(adapter->adapter_pa);\n\tdevRead->misc.ddLen = cpu_to_le32(sizeof(struct vmxnet3_adapter));\n\n\t/* set up feature flags */\n\tif (adapter->netdev->features & NETIF_F_RXCSUM)\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXCSUM;\n\n\tif (adapter->netdev->features & NETIF_F_LRO) {\n\t\tdevRead->misc.uptFeatures |= UPT1_F_LRO;\n\t\tdevRead->misc.maxNumRxSG = cpu_to_le16(1 + MAX_SKB_FRAGS);\n\t}\n\tif (adapter->netdev->features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXVLAN;\n\n\tif (adapter->netdev->features & (NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t NETIF_F_GSO_UDP_TUNNEL_CSUM))\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXINNEROFLD;\n\n\tdevRead->misc.mtu = cpu_to_le32(adapter->netdev->mtu);\n\tdevRead->misc.queueDescPA = cpu_to_le64(adapter->queue_desc_pa);\n\tdevRead->misc.queueDescLen = cpu_to_le32(\n\t\tadapter->num_tx_queues * sizeof(struct Vmxnet3_TxQueueDesc) +\n\t\tadapter->num_rx_queues * sizeof(struct Vmxnet3_RxQueueDesc));\n\n\t/* tx queue settings */\n\tdevRead->misc.numTxQueues =  adapter->num_tx_queues;\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct vmxnet3_tx_queue\t*tq = &adapter->tx_queue[i];\n\t\tBUG_ON(adapter->tx_queue[i].tx_ring.base == NULL);\n\t\ttqc = &adapter->tqd_start[i].conf;\n\t\ttqc->txRingBasePA   = cpu_to_le64(tq->tx_ring.basePA);\n\t\ttqc->dataRingBasePA = cpu_to_le64(tq->data_ring.basePA);\n\t\ttqc->compRingBasePA = cpu_to_le64(tq->comp_ring.basePA);\n\t\ttqc->ddPA           = cpu_to_le64(~0ULL);\n\t\ttqc->txRingSize     = cpu_to_le32(tq->tx_ring.size);\n\t\ttqc->dataRingSize   = cpu_to_le32(tq->data_ring.size);\n\t\ttqc->txDataRingDescSize = cpu_to_le32(tq->txdata_desc_size);\n\t\ttqc->compRingSize   = cpu_to_le32(tq->comp_ring.size);\n\t\ttqc->ddLen          = cpu_to_le32(0);\n\t\ttqc->intrIdx        = tq->comp_ring.intr_idx;\n\t}\n\n\t/* rx queue settings */\n\tdevRead->misc.numRxQueues = adapter->num_rx_queues;\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue\t*rq = &adapter->rx_queue[i];\n\t\trqc = &adapter->rqd_start[i].conf;\n\t\trqc->rxRingBasePA[0] = cpu_to_le64(rq->rx_ring[0].basePA);\n\t\trqc->rxRingBasePA[1] = cpu_to_le64(rq->rx_ring[1].basePA);\n\t\trqc->compRingBasePA  = cpu_to_le64(rq->comp_ring.basePA);\n\t\trqc->ddPA            = cpu_to_le64(~0ULL);\n\t\trqc->rxRingSize[0]   = cpu_to_le32(rq->rx_ring[0].size);\n\t\trqc->rxRingSize[1]   = cpu_to_le32(rq->rx_ring[1].size);\n\t\trqc->compRingSize    = cpu_to_le32(rq->comp_ring.size);\n\t\trqc->ddLen           = cpu_to_le32(0);\n\t\trqc->intrIdx         = rq->comp_ring.intr_idx;\n\t\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\t\trqc->rxDataRingBasePA =\n\t\t\t\tcpu_to_le64(rq->data_ring.basePA);\n\t\t\trqc->rxDataRingDescSize =\n\t\t\t\tcpu_to_le16(rq->data_ring.desc_size);\n\t\t}\n\t}\n\n#ifdef VMXNET3_RSS\n\tmemset(adapter->rss_conf, 0, sizeof(*adapter->rss_conf));\n\n\tif (adapter->rss) {\n\t\tstruct UPT1_RSSConf *rssConf = adapter->rss_conf;\n\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RSS;\n\t\tdevRead->misc.numRxQueues = adapter->num_rx_queues;\n\t\trssConf->hashType = UPT1_RSS_HASH_TYPE_TCP_IPV4 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_IPV4 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_TCP_IPV6 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_IPV6;\n\t\trssConf->hashFunc = UPT1_RSS_HASH_FUNC_TOEPLITZ;\n\t\trssConf->hashKeySize = UPT1_RSS_MAX_KEY_SIZE;\n\t\trssConf->indTableSize = VMXNET3_RSS_IND_TABLE_SIZE;\n\t\tnetdev_rss_key_fill(rssConf->hashKey, sizeof(rssConf->hashKey));\n\n\t\tfor (i = 0; i < rssConf->indTableSize; i++)\n\t\t\trssConf->indTable[i] = ethtool_rxfh_indir_default(\n\t\t\t\ti, adapter->num_rx_queues);\n\n\t\tdevRead->rssConfDesc.confVer = 1;\n\t\tdevRead->rssConfDesc.confLen = cpu_to_le32(sizeof(*rssConf));\n\t\tdevRead->rssConfDesc.confPA =\n\t\t\tcpu_to_le64(adapter->rss_conf_pa);\n\t}\n\n#endif /* VMXNET3_RSS */\n\n\t/* intr settings */\n\tif (!VMXNET3_VERSION_GE_6(adapter) ||\n\t    !adapter->queuesExtEnabled) {\n\t\tdevRead->intrConf.autoMask = adapter->intr.mask_mode ==\n\t\t\t\t\t     VMXNET3_IMM_AUTO;\n\t\tdevRead->intrConf.numIntrs = adapter->intr.num_intrs;\n\t\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\t\tdevRead->intrConf.modLevels[i] = adapter->intr.mod_levels[i];\n\n\t\tdevRead->intrConf.eventIntrIdx = adapter->intr.event_intr_idx;\n\t\tdevRead->intrConf.intrCtrl |= cpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\t} else {\n\t\tdevReadExt->intrConfExt.autoMask = adapter->intr.mask_mode ==\n\t\t\t\t\t\t   VMXNET3_IMM_AUTO;\n\t\tdevReadExt->intrConfExt.numIntrs = adapter->intr.num_intrs;\n\t\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\t\tdevReadExt->intrConfExt.modLevels[i] = adapter->intr.mod_levels[i];\n\n\t\tdevReadExt->intrConfExt.eventIntrIdx = adapter->intr.event_intr_idx;\n\t\tdevReadExt->intrConfExt.intrCtrl |= cpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\t}\n\n\t/* rx filter settings */\n\tdevRead->rxFilterConf.rxMode = 0;\n\tvmxnet3_restore_vlan(adapter);\n\tvmxnet3_write_mac_addr(adapter, adapter->netdev->dev_addr);\n\n\t/* the rest are already zeroed */\n}\n\nstatic void\nvmxnet3_init_coalesce(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tunion Vmxnet3_CmdInfo *cmdInfo = &shared->cu.cmdInfo;\n\tunsigned long flags;\n\n\tif (!VMXNET3_VERSION_GE_3(adapter))\n\t\treturn;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tcmdInfo->varConf.confVer = 1;\n\tcmdInfo->varConf.confLen =\n\t\tcpu_to_le32(sizeof(*adapter->coal_conf));\n\tcmdInfo->varConf.confPA  = cpu_to_le64(adapter->coal_conf_pa);\n\n\tif (adapter->default_coal_mode) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_COALESCE);\n\t} else {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_SET_COALESCE);\n\t}\n\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\nstatic void\nvmxnet3_init_rssfields(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tunion Vmxnet3_CmdInfo *cmdInfo = &shared->cu.cmdInfo;\n\tunsigned long flags;\n\n\tif (!VMXNET3_VERSION_GE_4(adapter))\n\t\treturn;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\n\tif (adapter->default_rss_fields) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_RSS_FIELDS);\n\t\tadapter->rss_fields =\n\t\t\tVMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t} else {\n\t\tcmdInfo->setRssFields = adapter->rss_fields;\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_SET_RSS_FIELDS);\n\t\t/* Not all requested RSS may get applied, so get and\n\t\t * cache what was actually applied.\n\t\t */\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_RSS_FIELDS);\n\t\tadapter->rss_fields =\n\t\t\tVMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t}\n\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\nint\nvmxnet3_activate_dev(struct vmxnet3_adapter *adapter)\n{\n\tint err, i;\n\tu32 ret;\n\tunsigned long flags;\n\n\tnetdev_dbg(adapter->netdev, \"%s: skb_buf_size %d, rx_buf_per_pkt %d,\"\n\t\t\" ring sizes %u %u %u\\n\", adapter->netdev->name,\n\t\tadapter->skb_buf_size, adapter->rx_buf_per_pkt,\n\t\tadapter->tx_queue[0].tx_ring.size,\n\t\tadapter->rx_queue[0].rx_ring[0].size,\n\t\tadapter->rx_queue[0].rx_ring[1].size);\n\n\tvmxnet3_tq_init_all(adapter);\n\terr = vmxnet3_rq_init_all(adapter);\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to init rx queue error %d\\n\", err);\n\t\tgoto rq_err;\n\t}\n\n\terr = vmxnet3_request_irqs(adapter);\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to setup irq for error %d\\n\", err);\n\t\tgoto irq_err;\n\t}\n\n\tvmxnet3_setup_driver_shared(adapter);\n\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAL, VMXNET3_GET_ADDR_LO(\n\t\t\t       adapter->shared_pa));\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAH, VMXNET3_GET_ADDR_HI(\n\t\t\t       adapter->shared_pa));\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_ACTIVATE_DEV);\n\tret = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tif (ret != 0) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to activate dev: error %u\\n\", ret);\n\t\terr = -EINVAL;\n\t\tgoto activate_err;\n\t}\n\n\tvmxnet3_init_coalesce(adapter);\n\tvmxnet3_init_rssfields(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\tVMXNET3_REG_RXPROD + i * VMXNET3_REG_ALIGN,\n\t\t\t\tadapter->rx_queue[i].rx_ring[0].next2fill);\n\t\tVMXNET3_WRITE_BAR0_REG(adapter, (VMXNET3_REG_RXPROD2 +\n\t\t\t\t(i * VMXNET3_REG_ALIGN)),\n\t\t\t\tadapter->rx_queue[i].rx_ring[1].next2fill);\n\t}\n\n\t/* Apply the rx filter settins last. */\n\tvmxnet3_set_mc(adapter->netdev);\n\n\t/*\n\t * Check link state when first activating device. It will start the\n\t * tx queue if the link is up.\n\t */\n\tvmxnet3_check_link(adapter, true);\n\tnetif_tx_wake_all_queues(adapter->netdev);\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_enable(&adapter->rx_queue[i].napi);\n\tvmxnet3_enable_all_intrs(adapter);\n\tclear_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\treturn 0;\n\nactivate_err:\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAL, 0);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAH, 0);\n\tvmxnet3_free_irqs(adapter);\nirq_err:\nrq_err:\n\t/* free up buffers we allocated */\n\tvmxnet3_rq_cleanup_all(adapter);\n\treturn err;\n}\n\n\nvoid\nvmxnet3_reset_dev(struct vmxnet3_adapter *adapter)\n{\n\tunsigned long flags;\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD, VMXNET3_CMD_RESET_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\n\nint\nvmxnet3_quiesce_dev(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tunsigned long flags;\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state))\n\t\treturn 0;\n\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_disable_all_intrs(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_disable(&adapter->rx_queue[i].napi);\n\tnetif_tx_disable(adapter->netdev);\n\tadapter->link_speed = 0;\n\tnetif_carrier_off(adapter->netdev);\n\n\tvmxnet3_tq_cleanup_all(adapter);\n\tvmxnet3_rq_cleanup_all(adapter);\n\tvmxnet3_free_irqs(adapter);\n\treturn 0;\n}\n\n\nstatic void\nvmxnet3_write_mac_addr(struct vmxnet3_adapter *adapter, const u8 *mac)\n{\n\tu32 tmp;\n\n\ttmp = *(u32 *)mac;\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_MACL, tmp);\n\n\ttmp = (mac[5] << 8) | mac[4];\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_MACH, tmp);\n}\n\n\nstatic int\nvmxnet3_set_mac_addr(struct net_device *netdev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tdev_addr_set(netdev, addr->sa_data);\n\tvmxnet3_write_mac_addr(adapter, addr->sa_data);\n\n\treturn 0;\n}\n\n\n/* ==================== initialization and cleanup routines ============ */\n\nstatic int\nvmxnet3_alloc_pci_resources(struct vmxnet3_adapter *adapter)\n{\n\tint err;\n\tunsigned long mmio_start, mmio_len;\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to enable adapter: error %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = pci_request_selected_regions(pdev, (1 << 2) - 1,\n\t\t\t\t\t   vmxnet3_driver_name);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Failed to request region for adapter: error %d\\n\", err);\n\t\tgoto err_enable_device;\n\t}\n\n\tpci_set_master(pdev);\n\n\tmmio_start = pci_resource_start(pdev, 0);\n\tmmio_len = pci_resource_len(pdev, 0);\n\tadapter->hw_addr0 = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw_addr0) {\n\t\tdev_err(&pdev->dev, \"Failed to map bar0\\n\");\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tmmio_start = pci_resource_start(pdev, 1);\n\tmmio_len = pci_resource_len(pdev, 1);\n\tadapter->hw_addr1 = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw_addr1) {\n\t\tdev_err(&pdev->dev, \"Failed to map bar1\\n\");\n\t\terr = -EIO;\n\t\tgoto err_bar1;\n\t}\n\treturn 0;\n\nerr_bar1:\n\tiounmap(adapter->hw_addr0);\nerr_ioremap:\n\tpci_release_selected_regions(pdev, (1 << 2) - 1);\nerr_enable_device:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_free_pci_resources(struct vmxnet3_adapter *adapter)\n{\n\tBUG_ON(!adapter->pdev);\n\n\tiounmap(adapter->hw_addr0);\n\tiounmap(adapter->hw_addr1);\n\tpci_release_selected_regions(adapter->pdev, (1 << 2) - 1);\n\tpci_disable_device(adapter->pdev);\n}\n\n\nstatic void\nvmxnet3_adjust_rx_ring_size(struct vmxnet3_adapter *adapter)\n{\n\tsize_t sz, i, ring0_size, ring1_size, comp_size;\n\tif (adapter->netdev->mtu <= VMXNET3_MAX_SKB_BUF_SIZE -\n\t\t\t\t    VMXNET3_MAX_ETH_HDR_SIZE) {\n\t\tadapter->skb_buf_size = adapter->netdev->mtu +\n\t\t\t\t\tVMXNET3_MAX_ETH_HDR_SIZE;\n\t\tif (adapter->skb_buf_size < VMXNET3_MIN_T0_BUF_SIZE)\n\t\t\tadapter->skb_buf_size = VMXNET3_MIN_T0_BUF_SIZE;\n\n\t\tadapter->rx_buf_per_pkt = 1;\n\t} else {\n\t\tadapter->skb_buf_size = VMXNET3_MAX_SKB_BUF_SIZE;\n\t\tsz = adapter->netdev->mtu - VMXNET3_MAX_SKB_BUF_SIZE +\n\t\t\t\t\t    VMXNET3_MAX_ETH_HDR_SIZE;\n\t\tadapter->rx_buf_per_pkt = 1 + (sz + PAGE_SIZE - 1) / PAGE_SIZE;\n\t}\n\n\t/*\n\t * for simplicity, force the ring0 size to be a multiple of\n\t * rx_buf_per_pkt * VMXNET3_RING_SIZE_ALIGN\n\t */\n\tsz = adapter->rx_buf_per_pkt * VMXNET3_RING_SIZE_ALIGN;\n\tring0_size = adapter->rx_queue[0].rx_ring[0].size;\n\tring0_size = (ring0_size + sz - 1) / sz * sz;\n\tring0_size = min_t(u32, ring0_size, VMXNET3_RX_RING_MAX_SIZE /\n\t\t\t   sz * sz);\n\tring1_size = adapter->rx_queue[0].rx_ring[1].size;\n\tring1_size = (ring1_size + sz - 1) / sz * sz;\n\tring1_size = min_t(u32, ring1_size, VMXNET3_RX_RING2_MAX_SIZE /\n\t\t\t   sz * sz);\n\tcomp_size = ring0_size + ring1_size;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue\t*rq = &adapter->rx_queue[i];\n\n\t\trq->rx_ring[0].size = ring0_size;\n\t\trq->rx_ring[1].size = ring1_size;\n\t\trq->comp_ring.size = comp_size;\n\t}\n}\n\n\nint\nvmxnet3_create_queues(struct vmxnet3_adapter *adapter, u32 tx_ring_size,\n\t\t      u32 rx_ring_size, u32 rx_ring2_size,\n\t\t      u16 txdata_desc_size, u16 rxdata_desc_size)\n{\n\tint err = 0, i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct vmxnet3_tx_queue\t*tq = &adapter->tx_queue[i];\n\t\ttq->tx_ring.size   = tx_ring_size;\n\t\ttq->data_ring.size = tx_ring_size;\n\t\ttq->comp_ring.size = tx_ring_size;\n\t\ttq->txdata_desc_size = txdata_desc_size;\n\t\ttq->shared = &adapter->tqd_start[i].ctrl;\n\t\ttq->stopped = true;\n\t\ttq->adapter = adapter;\n\t\ttq->qid = i;\n\t\terr = vmxnet3_tq_create(tq, adapter);\n\t\t/*\n\t\t * Too late to change num_tx_queues. We cannot do away with\n\t\t * lesser number of queues than what we asked for\n\t\t */\n\t\tif (err)\n\t\t\tgoto queue_err;\n\t}\n\n\tadapter->rx_queue[0].rx_ring[0].size = rx_ring_size;\n\tadapter->rx_queue[0].rx_ring[1].size = rx_ring2_size;\n\tvmxnet3_adjust_rx_ring_size(adapter);\n\n\tadapter->rxdataring_enabled = VMXNET3_VERSION_GE_3(adapter);\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\t\t/* qid and qid2 for rx queues will be assigned later when num\n\t\t * of rx queues is finalized after allocating intrs */\n\t\trq->shared = &adapter->rqd_start[i].ctrl;\n\t\trq->adapter = adapter;\n\t\trq->data_ring.desc_size = rxdata_desc_size;\n\t\terr = vmxnet3_rq_create(rq, adapter);\n\t\tif (err) {\n\t\t\tif (i == 0) {\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Could not allocate any rx queues. \"\n\t\t\t\t\t   \"Aborting.\\n\");\n\t\t\t\tgoto queue_err;\n\t\t\t} else {\n\t\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\t    \"Number of rx queues changed \"\n\t\t\t\t\t    \"to : %d.\\n\", i);\n\t\t\t\tadapter->num_rx_queues = i;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!adapter->rxdataring_enabled)\n\t\tvmxnet3_rq_destroy_all_rxdataring(adapter);\n\n\treturn err;\nqueue_err:\n\tvmxnet3_tq_destroy_all(adapter);\n\treturn err;\n}\n\nstatic int\nvmxnet3_open(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter;\n\tint err, i;\n\n\tadapter = netdev_priv(netdev);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tspin_lock_init(&adapter->tx_queue[i].tx_lock);\n\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tunsigned long flags;\n\t\tu16 txdata_desc_size;\n\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_TXDATA_DESC_SIZE);\n\t\ttxdata_desc_size = VMXNET3_READ_BAR1_REG(adapter,\n\t\t\t\t\t\t\t VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\t\tif ((txdata_desc_size < VMXNET3_TXDATA_DESC_MIN_SIZE) ||\n\t\t    (txdata_desc_size > VMXNET3_TXDATA_DESC_MAX_SIZE) ||\n\t\t    (txdata_desc_size & VMXNET3_TXDATA_DESC_SIZE_MASK)) {\n\t\t\tadapter->txdata_desc_size =\n\t\t\t\tsizeof(struct Vmxnet3_TxDataDesc);\n\t\t} else {\n\t\t\tadapter->txdata_desc_size = txdata_desc_size;\n\t\t}\n\t} else {\n\t\tadapter->txdata_desc_size = sizeof(struct Vmxnet3_TxDataDesc);\n\t}\n\n\terr = vmxnet3_create_queues(adapter,\n\t\t\t\t    adapter->tx_ring_size,\n\t\t\t\t    adapter->rx_ring_size,\n\t\t\t\t    adapter->rx_ring2_size,\n\t\t\t\t    adapter->txdata_desc_size,\n\t\t\t\t    adapter->rxdata_desc_size);\n\tif (err)\n\t\tgoto queue_err;\n\n\terr = vmxnet3_activate_dev(adapter);\n\tif (err)\n\t\tgoto activate_err;\n\n\treturn 0;\n\nactivate_err:\n\tvmxnet3_rq_destroy_all(adapter);\n\tvmxnet3_tq_destroy_all(adapter);\nqueue_err:\n\treturn err;\n}\n\n\nstatic int\nvmxnet3_close(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\t/*\n\t * Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tvmxnet3_quiesce_dev(adapter);\n\n\tvmxnet3_rq_destroy_all(adapter);\n\tvmxnet3_tq_destroy_all(adapter);\n\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\n\n\treturn 0;\n}\n\n\nvoid\nvmxnet3_force_close(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\t/*\n\t * we must clear VMXNET3_STATE_BIT_RESETTING, otherwise\n\t * vmxnet3_close() will deadlock.\n\t */\n\tBUG_ON(test_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state));\n\n\t/* we need to enable NAPI, otherwise dev_close will deadlock */\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_enable(&adapter->rx_queue[i].napi);\n\t/*\n\t * Need to clear the quiesce bit to ensure that vmxnet3_close\n\t * can quiesce the device properly\n\t */\n\tclear_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\tdev_close(adapter->netdev);\n}\n\n\nstatic int\nvmxnet3_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tint err = 0;\n\n\tnetdev->mtu = new_mtu;\n\n\t/*\n\t * Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tif (netif_running(netdev)) {\n\t\tvmxnet3_quiesce_dev(adapter);\n\t\tvmxnet3_reset_dev(adapter);\n\n\t\t/* we need to re-create the rx queue based on the new mtu */\n\t\tvmxnet3_rq_destroy_all(adapter);\n\t\tvmxnet3_adjust_rx_ring_size(adapter);\n\t\terr = vmxnet3_rq_create_all(adapter);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"failed to re-create rx queues, \"\n\t\t\t\t   \" error %d. Closing it.\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = vmxnet3_activate_dev(adapter);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"failed to re-activate, error %d. \"\n\t\t\t\t   \"Closing it\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\tif (err)\n\t\tvmxnet3_force_close(adapter);\n\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_declare_features(struct vmxnet3_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\n\tnetdev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM |\n\t\tNETIF_F_HW_CSUM | NETIF_F_HW_VLAN_CTAG_TX |\n\t\tNETIF_F_HW_VLAN_CTAG_RX | NETIF_F_TSO | NETIF_F_TSO6 |\n\t\tNETIF_F_LRO | NETIF_F_HIGHDMA;\n\n\tif (VMXNET3_VERSION_GE_4(adapter)) {\n\t\tnetdev->hw_features |= NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\tNETIF_F_GSO_UDP_TUNNEL_CSUM;\n\n\t\tnetdev->hw_enc_features = NETIF_F_SG | NETIF_F_RXCSUM |\n\t\t\tNETIF_F_HW_CSUM | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\tNETIF_F_HW_VLAN_CTAG_RX | NETIF_F_TSO | NETIF_F_TSO6 |\n\t\t\tNETIF_F_LRO | NETIF_F_GSO_UDP_TUNNEL |\n\t\t\tNETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t}\n\n\tnetdev->vlan_features = netdev->hw_features &\n\t\t\t\t~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t  NETIF_F_HW_VLAN_CTAG_RX);\n\tnetdev->features = netdev->hw_features | NETIF_F_HW_VLAN_CTAG_FILTER;\n}\n\n\nstatic void\nvmxnet3_read_mac_addr(struct vmxnet3_adapter *adapter, u8 *mac)\n{\n\tu32 tmp;\n\n\ttmp = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_MACL);\n\t*(u32 *)mac = tmp;\n\n\ttmp = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_MACH);\n\tmac[4] = tmp & 0xff;\n\tmac[5] = (tmp >> 8) & 0xff;\n}\n\n#ifdef CONFIG_PCI_MSI\n\n/*\n * Enable MSIx vectors.\n * Returns :\n *\tVMXNET3_LINUX_MIN_MSIX_VECT when only minimum number of vectors required\n *\t were enabled.\n *\tnumber of vectors which were enabled otherwise (this number is greater\n *\t than VMXNET3_LINUX_MIN_MSIX_VECT)\n */\n\nstatic int\nvmxnet3_acquire_msix_vectors(struct vmxnet3_adapter *adapter, int nvec)\n{\n\tint ret = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t\tadapter->intr.msix_entries, nvec, nvec);\n\n\tif (ret == -ENOSPC && nvec > VMXNET3_LINUX_MIN_MSIX_VECT) {\n\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\"Failed to enable %d MSI-X, trying %d\\n\",\n\t\t\tnvec, VMXNET3_LINUX_MIN_MSIX_VECT);\n\n\t\tret = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t\t    adapter->intr.msix_entries,\n\t\t\t\t\t    VMXNET3_LINUX_MIN_MSIX_VECT,\n\t\t\t\t\t    VMXNET3_LINUX_MIN_MSIX_VECT);\n\t}\n\n\tif (ret < 0) {\n\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\"Failed to enable MSI-X, error: %d\\n\", ret);\n\t}\n\n\treturn ret;\n}\n\n\n#endif /* CONFIG_PCI_MSI */\n\nstatic void\nvmxnet3_alloc_intr_resources(struct vmxnet3_adapter *adapter)\n{\n\tu32 cfg;\n\tunsigned long flags;\n\n\t/* intr settings */\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_GET_CONF_INTR);\n\tcfg = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tadapter->intr.type = cfg & 0x3;\n\tadapter->intr.mask_mode = (cfg >> 2) & 0x3;\n\n\tif (adapter->intr.type == VMXNET3_IT_AUTO) {\n\t\tadapter->intr.type = VMXNET3_IT_MSIX;\n\t}\n\n#ifdef CONFIG_PCI_MSI\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tint i, nvec, nvec_allocated;\n\n\t\tnvec  = adapter->share_intr == VMXNET3_INTR_TXSHARE ?\n\t\t\t1 : adapter->num_tx_queues;\n\t\tnvec += adapter->share_intr == VMXNET3_INTR_BUDDYSHARE ?\n\t\t\t0 : adapter->num_rx_queues;\n\t\tnvec += 1;\t/* for link event */\n\t\tnvec = nvec > VMXNET3_LINUX_MIN_MSIX_VECT ?\n\t\t       nvec : VMXNET3_LINUX_MIN_MSIX_VECT;\n\n\t\tfor (i = 0; i < nvec; i++)\n\t\t\tadapter->intr.msix_entries[i].entry = i;\n\n\t\tnvec_allocated = vmxnet3_acquire_msix_vectors(adapter, nvec);\n\t\tif (nvec_allocated < 0)\n\t\t\tgoto msix_err;\n\n\t\t/* If we cannot allocate one MSIx vector per queue\n\t\t * then limit the number of rx queues to 1\n\t\t */\n\t\tif (nvec_allocated == VMXNET3_LINUX_MIN_MSIX_VECT &&\n\t\t    nvec != VMXNET3_LINUX_MIN_MSIX_VECT) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE\n\t\t\t    || adapter->num_rx_queues != 1) {\n\t\t\t\tadapter->share_intr = VMXNET3_INTR_TXSHARE;\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Number of rx queues : 1\\n\");\n\t\t\t\tadapter->num_rx_queues = 1;\n\t\t\t}\n\t\t}\n\n\t\tadapter->intr.num_intrs = nvec_allocated;\n\t\treturn;\n\nmsix_err:\n\t\t/* If we cannot allocate MSIx vectors use only one rx queue */\n\t\tdev_info(&adapter->pdev->dev,\n\t\t\t \"Failed to enable MSI-X, error %d. \"\n\t\t\t \"Limiting #rx queues to 1, try MSI.\\n\", nvec_allocated);\n\n\t\tadapter->intr.type = VMXNET3_IT_MSI;\n\t}\n\n\tif (adapter->intr.type == VMXNET3_IT_MSI) {\n\t\tif (!pci_enable_msi(adapter->pdev)) {\n\t\t\tadapter->num_rx_queues = 1;\n\t\t\tadapter->intr.num_intrs = 1;\n\t\t\treturn;\n\t\t}\n\t}\n#endif /* CONFIG_PCI_MSI */\n\n\tadapter->num_rx_queues = 1;\n\tdev_info(&adapter->netdev->dev,\n\t\t \"Using INTx interrupt, #Rx queues: 1.\\n\");\n\tadapter->intr.type = VMXNET3_IT_INTX;\n\n\t/* INT-X related setting */\n\tadapter->intr.num_intrs = 1;\n}\n\n\nstatic void\nvmxnet3_free_intr_resources(struct vmxnet3_adapter *adapter)\n{\n\tif (adapter->intr.type == VMXNET3_IT_MSIX)\n\t\tpci_disable_msix(adapter->pdev);\n\telse if (adapter->intr.type == VMXNET3_IT_MSI)\n\t\tpci_disable_msi(adapter->pdev);\n\telse\n\t\tBUG_ON(adapter->intr.type != VMXNET3_IT_INTX);\n}\n\n\nstatic void\nvmxnet3_tx_timeout(struct net_device *netdev, unsigned int txqueue)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tadapter->tx_timeout_count++;\n\n\tnetdev_err(adapter->netdev, \"tx hang\\n\");\n\tschedule_work(&adapter->work);\n}\n\n\nstatic void\nvmxnet3_reset_work(struct work_struct *data)\n{\n\tstruct vmxnet3_adapter *adapter;\n\n\tadapter = container_of(data, struct vmxnet3_adapter, work);\n\n\t/* if another thread is resetting the device, no need to proceed */\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\treturn;\n\n\t/* if the device is closed, we must leave it alone */\n\trtnl_lock();\n\tif (netif_running(adapter->netdev)) {\n\t\tnetdev_notice(adapter->netdev, \"resetting\\n\");\n\t\tvmxnet3_quiesce_dev(adapter);\n\t\tvmxnet3_reset_dev(adapter);\n\t\tvmxnet3_activate_dev(adapter);\n\t} else {\n\t\tnetdev_info(adapter->netdev, \"already closed\\n\");\n\t}\n\trtnl_unlock();\n\n\tnetif_wake_queue(adapter->netdev);\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n}\n\n\nstatic int\nvmxnet3_probe_device(struct pci_dev *pdev,\n\t\t     const struct pci_device_id *id)\n{\n\tstatic const struct net_device_ops vmxnet3_netdev_ops = {\n\t\t.ndo_open = vmxnet3_open,\n\t\t.ndo_stop = vmxnet3_close,\n\t\t.ndo_start_xmit = vmxnet3_xmit_frame,\n\t\t.ndo_set_mac_address = vmxnet3_set_mac_addr,\n\t\t.ndo_change_mtu = vmxnet3_change_mtu,\n\t\t.ndo_fix_features = vmxnet3_fix_features,\n\t\t.ndo_set_features = vmxnet3_set_features,\n\t\t.ndo_features_check = vmxnet3_features_check,\n\t\t.ndo_get_stats64 = vmxnet3_get_stats64,\n\t\t.ndo_tx_timeout = vmxnet3_tx_timeout,\n\t\t.ndo_set_rx_mode = vmxnet3_set_mc,\n\t\t.ndo_vlan_rx_add_vid = vmxnet3_vlan_rx_add_vid,\n\t\t.ndo_vlan_rx_kill_vid = vmxnet3_vlan_rx_kill_vid,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t\t.ndo_poll_controller = vmxnet3_netpoll,\n#endif\n\t};\n\tint err;\n\tu32 ver;\n\tstruct net_device *netdev;\n\tstruct vmxnet3_adapter *adapter;\n\tu8 mac[ETH_ALEN];\n\tint size;\n\tint num_tx_queues;\n\tint num_rx_queues;\n\tint queues;\n\tunsigned long flags;\n\n\tif (!pci_msi_enabled())\n\t\tenable_mq = 0;\n\n#ifdef VMXNET3_RSS\n\tif (enable_mq)\n\t\tnum_rx_queues = min(VMXNET3_DEVICE_MAX_RX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n#endif\n\t\tnum_rx_queues = 1;\n\n\tif (enable_mq)\n\t\tnum_tx_queues = min(VMXNET3_DEVICE_MAX_TX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n\t\tnum_tx_queues = 1;\n\n\tnetdev = alloc_etherdev_mq(sizeof(struct vmxnet3_adapter),\n\t\t\t\t   max(num_tx_queues, num_rx_queues));\n\tif (!netdev)\n\t\treturn -ENOMEM;\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\n\tadapter->tx_ring_size = VMXNET3_DEF_TX_RING_SIZE;\n\tadapter->rx_ring_size = VMXNET3_DEF_RX_RING_SIZE;\n\tadapter->rx_ring2_size = VMXNET3_DEF_RX_RING2_SIZE;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"dma_set_mask failed\\n\");\n\t\tgoto err_set_mask;\n\t}\n\n\tspin_lock_init(&adapter->cmd_lock);\n\tadapter->adapter_pa = dma_map_single(&adapter->pdev->dev, adapter,\n\t\t\t\t\t     sizeof(struct vmxnet3_adapter),\n\t\t\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(&adapter->pdev->dev, adapter->adapter_pa)) {\n\t\tdev_err(&pdev->dev, \"Failed to map dma\\n\");\n\t\terr = -EFAULT;\n\t\tgoto err_set_mask;\n\t}\n\tadapter->shared = dma_alloc_coherent(\n\t\t\t\t&adapter->pdev->dev,\n\t\t\t\tsizeof(struct Vmxnet3_DriverShared),\n\t\t\t\t&adapter->shared_pa, GFP_KERNEL);\n\tif (!adapter->shared) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate memory\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_shared;\n\t}\n\n\terr = vmxnet3_alloc_pci_resources(adapter);\n\tif (err < 0)\n\t\tgoto err_alloc_pci;\n\n\tver = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_VRRS);\n\tif (ver & (1 << VMXNET3_REV_6)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_6);\n\t\tadapter->version = VMXNET3_REV_6 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_5)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_5);\n\t\tadapter->version = VMXNET3_REV_5 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_4)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_4);\n\t\tadapter->version = VMXNET3_REV_4 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_3)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_3);\n\t\tadapter->version = VMXNET3_REV_3 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_2)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_2);\n\t\tadapter->version = VMXNET3_REV_2 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_1)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_1);\n\t\tadapter->version = VMXNET3_REV_1 + 1;\n\t} else {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Incompatible h/w version (0x%x) for adapter\\n\", ver);\n\t\terr = -EBUSY;\n\t\tgoto err_ver;\n\t}\n\tdev_dbg(&pdev->dev, \"Using device version %d\\n\", adapter->version);\n\n\tver = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_UVRS);\n\tif (ver & 1) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_UVRS, 1);\n\t} else {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Incompatible upt version (0x%x) for adapter\\n\", ver);\n\t\terr = -EBUSY;\n\t\tgoto err_ver;\n\t}\n\n\tif (VMXNET3_VERSION_GE_6(adapter)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_MAX_QUEUES_CONF);\n\t\tqueues = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t\tif (queues > 0) {\n\t\t\tadapter->num_rx_queues = min(num_rx_queues, ((queues >> 8) & 0xff));\n\t\t\tadapter->num_tx_queues = min(num_tx_queues, (queues & 0xff));\n\t\t} else {\n\t\t\tadapter->num_rx_queues = min(num_rx_queues,\n\t\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\t\tadapter->num_tx_queues = min(num_tx_queues,\n\t\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_TX_QUEUES);\n\t\t}\n\t\tif (adapter->num_rx_queues > VMXNET3_MAX_RX_QUEUES ||\n\t\t    adapter->num_tx_queues > VMXNET3_MAX_TX_QUEUES) {\n\t\t\tadapter->queuesExtEnabled = true;\n\t\t} else {\n\t\t\tadapter->queuesExtEnabled = false;\n\t\t}\n\t} else {\n\t\tadapter->queuesExtEnabled = false;\n\t\tnum_rx_queues = rounddown_pow_of_two(num_rx_queues);\n\t\tnum_tx_queues = rounddown_pow_of_two(num_tx_queues);\n\t\tadapter->num_rx_queues = min(num_rx_queues,\n\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\tadapter->num_tx_queues = min(num_tx_queues,\n\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_TX_QUEUES);\n\t}\n\tdev_info(&pdev->dev,\n\t\t \"# of Tx queues : %d, # of Rx queues : %d\\n\",\n\t\t adapter->num_tx_queues, adapter->num_rx_queues);\n\n\tadapter->rx_buf_per_pkt = 1;\n\n\tsize = sizeof(struct Vmxnet3_TxQueueDesc) * adapter->num_tx_queues;\n\tsize += sizeof(struct Vmxnet3_RxQueueDesc) * adapter->num_rx_queues;\n\tadapter->tqd_start = dma_alloc_coherent(&adapter->pdev->dev, size,\n\t\t\t\t\t\t&adapter->queue_desc_pa,\n\t\t\t\t\t\tGFP_KERNEL);\n\n\tif (!adapter->tqd_start) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate memory\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_ver;\n\t}\n\tadapter->rqd_start = (struct Vmxnet3_RxQueueDesc *)(adapter->tqd_start +\n\t\t\t\t\t\t\t    adapter->num_tx_queues);\n\n\tadapter->pm_conf = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t      sizeof(struct Vmxnet3_PMConf),\n\t\t\t\t\t      &adapter->pm_conf_pa,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (adapter->pm_conf == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_pm;\n\t}\n\n#ifdef VMXNET3_RSS\n\n\tadapter->rss_conf = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t       sizeof(struct UPT1_RSSConf),\n\t\t\t\t\t       &adapter->rss_conf_pa,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (adapter->rss_conf == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_rss;\n\t}\n#endif /* VMXNET3_RSS */\n\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tadapter->coal_conf =\n\t\t\tdma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t   sizeof(struct Vmxnet3_CoalesceScheme)\n\t\t\t\t\t   ,\n\t\t\t\t\t   &adapter->coal_conf_pa,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!adapter->coal_conf) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_coal_conf;\n\t\t}\n\t\tadapter->coal_conf->coalMode = VMXNET3_COALESCE_DISABLED;\n\t\tadapter->default_coal_mode = true;\n\t}\n\n\tif (VMXNET3_VERSION_GE_4(adapter)) {\n\t\tadapter->default_rss_fields = true;\n\t\tadapter->rss_fields = VMXNET3_RSS_FIELDS_DEFAULT;\n\t}\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\tvmxnet3_declare_features(adapter);\n\n\tadapter->rxdata_desc_size = VMXNET3_VERSION_GE_3(adapter) ?\n\t\tVMXNET3_DEF_RXDATA_DESC_SIZE : 0;\n\n\tif (adapter->num_tx_queues == adapter->num_rx_queues)\n\t\tadapter->share_intr = VMXNET3_INTR_BUDDYSHARE;\n\telse\n\t\tadapter->share_intr = VMXNET3_INTR_DONTSHARE;\n\n\tvmxnet3_alloc_intr_resources(adapter);\n\n#ifdef VMXNET3_RSS\n\tif (adapter->num_rx_queues > 1 &&\n\t    adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tadapter->rss = true;\n\t\tnetdev->hw_features |= NETIF_F_RXHASH;\n\t\tnetdev->features |= NETIF_F_RXHASH;\n\t\tdev_dbg(&pdev->dev, \"RSS is enabled.\\n\");\n\t} else {\n\t\tadapter->rss = false;\n\t}\n#endif\n\n\tvmxnet3_read_mac_addr(adapter, mac);\n\tdev_addr_set(netdev, mac);\n\n\tnetdev->netdev_ops = &vmxnet3_netdev_ops;\n\tvmxnet3_set_ethtool_ops(netdev);\n\tnetdev->watchdog_timeo = 5 * HZ;\n\n\t/* MTU range: 60 - 9190 */\n\tnetdev->min_mtu = VMXNET3_MIN_MTU;\n\tif (VMXNET3_VERSION_GE_6(adapter))\n\t\tnetdev->max_mtu = VMXNET3_V6_MAX_MTU;\n\telse\n\t\tnetdev->max_mtu = VMXNET3_MAX_MTU;\n\n\tINIT_WORK(&adapter->work, vmxnet3_reset_work);\n\tset_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tnetif_napi_add(adapter->netdev,\n\t\t\t\t       &adapter->rx_queue[i].napi,\n\t\t\t\t       vmxnet3_poll_rx_only, 64);\n\t\t}\n\t} else {\n\t\tnetif_napi_add(adapter->netdev, &adapter->rx_queue[0].napi,\n\t\t\t       vmxnet3_poll, 64);\n\t}\n\n\tnetif_set_real_num_tx_queues(adapter->netdev, adapter->num_tx_queues);\n\tnetif_set_real_num_rx_queues(adapter->netdev, adapter->num_rx_queues);\n\n\tnetif_carrier_off(netdev);\n\terr = register_netdev(netdev);\n\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to register adapter\\n\");\n\t\tgoto err_register;\n\t}\n\n\tvmxnet3_check_link(adapter, false);\n\treturn 0;\n\nerr_register:\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  sizeof(struct Vmxnet3_CoalesceScheme),\n\t\t\t\t  adapter->coal_conf, adapter->coal_conf_pa);\n\t}\n\tvmxnet3_free_intr_resources(adapter);\nerr_coal_conf:\n#ifdef VMXNET3_RSS\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct UPT1_RSSConf),\n\t\t\t  adapter->rss_conf, adapter->rss_conf_pa);\nerr_alloc_rss:\n#endif\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct Vmxnet3_PMConf),\n\t\t\t  adapter->pm_conf, adapter->pm_conf_pa);\nerr_alloc_pm:\n\tdma_free_coherent(&adapter->pdev->dev, size, adapter->tqd_start,\n\t\t\t  adapter->queue_desc_pa);\nerr_ver:\n\tvmxnet3_free_pci_resources(adapter);\nerr_alloc_pci:\n\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t  sizeof(struct Vmxnet3_DriverShared),\n\t\t\t  adapter->shared, adapter->shared_pa);\nerr_alloc_shared:\n\tdma_unmap_single(&adapter->pdev->dev, adapter->adapter_pa,\n\t\t\t sizeof(struct vmxnet3_adapter), DMA_TO_DEVICE);\nerr_set_mask:\n\tfree_netdev(netdev);\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_remove_device(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tint size = 0;\n\tint num_rx_queues, rx_queues;\n\tunsigned long flags;\n\n#ifdef VMXNET3_RSS\n\tif (enable_mq)\n\t\tnum_rx_queues = min(VMXNET3_DEVICE_MAX_RX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n#endif\n\t\tnum_rx_queues = 1;\n\tif (!VMXNET3_VERSION_GE_6(adapter)) {\n\t\tnum_rx_queues = rounddown_pow_of_two(num_rx_queues);\n\t}\n\tif (VMXNET3_VERSION_GE_6(adapter)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_MAX_QUEUES_CONF);\n\t\trx_queues = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t\tif (rx_queues > 0)\n\t\t\trx_queues = (rx_queues >> 8) & 0xff;\n\t\telse\n\t\t\trx_queues = min(num_rx_queues, VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\tnum_rx_queues = min(num_rx_queues, rx_queues);\n\t} else {\n\t\tnum_rx_queues = min(num_rx_queues,\n\t\t\t\t    VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t}\n\n\tcancel_work_sync(&adapter->work);\n\n\tunregister_netdev(netdev);\n\n\tvmxnet3_free_intr_resources(adapter);\n\tvmxnet3_free_pci_resources(adapter);\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  sizeof(struct Vmxnet3_CoalesceScheme),\n\t\t\t\t  adapter->coal_conf, adapter->coal_conf_pa);\n\t}\n#ifdef VMXNET3_RSS\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct UPT1_RSSConf),\n\t\t\t  adapter->rss_conf, adapter->rss_conf_pa);\n#endif\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct Vmxnet3_PMConf),\n\t\t\t  adapter->pm_conf, adapter->pm_conf_pa);\n\n\tsize = sizeof(struct Vmxnet3_TxQueueDesc) * adapter->num_tx_queues;\n\tsize += sizeof(struct Vmxnet3_RxQueueDesc) * num_rx_queues;\n\tdma_free_coherent(&adapter->pdev->dev, size, adapter->tqd_start,\n\t\t\t  adapter->queue_desc_pa);\n\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t  sizeof(struct Vmxnet3_DriverShared),\n\t\t\t  adapter->shared, adapter->shared_pa);\n\tdma_unmap_single(&adapter->pdev->dev, adapter->adapter_pa,\n\t\t\t sizeof(struct vmxnet3_adapter), DMA_TO_DEVICE);\n\tfree_netdev(netdev);\n}\n\nstatic void vmxnet3_shutdown_device(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tunsigned long flags;\n\n\t/* Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_QUIESCED,\n\t\t\t     &adapter->state)) {\n\t\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\t\treturn;\n\t}\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_disable_all_intrs(adapter);\n\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n}\n\n\n#ifdef CONFIG_PM\n\nstatic int\nvmxnet3_suspend(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tstruct Vmxnet3_PMConf *pmConf;\n\tstruct ethhdr *ehdr;\n\tstruct arphdr *ahdr;\n\tu8 *arpreq;\n\tstruct in_device *in_dev;\n\tstruct in_ifaddr *ifa;\n\tunsigned long flags;\n\tint i = 0;\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_disable(&adapter->rx_queue[i].napi);\n\n\tvmxnet3_disable_all_intrs(adapter);\n\tvmxnet3_free_irqs(adapter);\n\tvmxnet3_free_intr_resources(adapter);\n\n\tnetif_device_detach(netdev);\n\n\t/* Create wake-up filters. */\n\tpmConf = adapter->pm_conf;\n\tmemset(pmConf, 0, sizeof(*pmConf));\n\n\tif (adapter->wol & WAKE_UCAST) {\n\t\tpmConf->filters[i].patternSize = ETH_ALEN;\n\t\tpmConf->filters[i].maskSize = 1;\n\t\tmemcpy(pmConf->filters[i].pattern, netdev->dev_addr, ETH_ALEN);\n\t\tpmConf->filters[i].mask[0] = 0x3F; /* LSB ETH_ALEN bits */\n\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_FILTER;\n\t\ti++;\n\t}\n\n\tif (adapter->wol & WAKE_ARP) {\n\t\trcu_read_lock();\n\n\t\tin_dev = __in_dev_get_rcu(netdev);\n\t\tif (!in_dev) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto skip_arp;\n\t\t}\n\n\t\tifa = rcu_dereference(in_dev->ifa_list);\n\t\tif (!ifa) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto skip_arp;\n\t\t}\n\n\t\tpmConf->filters[i].patternSize = ETH_HLEN + /* Ethernet header*/\n\t\t\tsizeof(struct arphdr) +\t\t/* ARP header */\n\t\t\t2 * ETH_ALEN +\t\t/* 2 Ethernet addresses*/\n\t\t\t2 * sizeof(u32);\t/*2 IPv4 addresses */\n\t\tpmConf->filters[i].maskSize =\n\t\t\t(pmConf->filters[i].patternSize - 1) / 8 + 1;\n\n\t\t/* ETH_P_ARP in Ethernet header. */\n\t\tehdr = (struct ethhdr *)pmConf->filters[i].pattern;\n\t\tehdr->h_proto = htons(ETH_P_ARP);\n\n\t\t/* ARPOP_REQUEST in ARP header. */\n\t\tahdr = (struct arphdr *)&pmConf->filters[i].pattern[ETH_HLEN];\n\t\tahdr->ar_op = htons(ARPOP_REQUEST);\n\t\tarpreq = (u8 *)(ahdr + 1);\n\n\t\t/* The Unicast IPv4 address in 'tip' field. */\n\t\tarpreq += 2 * ETH_ALEN + sizeof(u32);\n\t\t*(__be32 *)arpreq = ifa->ifa_address;\n\n\t\trcu_read_unlock();\n\n\t\t/* The mask for the relevant bits. */\n\t\tpmConf->filters[i].mask[0] = 0x00;\n\t\tpmConf->filters[i].mask[1] = 0x30; /* ETH_P_ARP */\n\t\tpmConf->filters[i].mask[2] = 0x30; /* ARPOP_REQUEST */\n\t\tpmConf->filters[i].mask[3] = 0x00;\n\t\tpmConf->filters[i].mask[4] = 0xC0; /* IPv4 TIP */\n\t\tpmConf->filters[i].mask[5] = 0x03; /* IPv4 TIP */\n\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_FILTER;\n\t\ti++;\n\t}\n\nskip_arp:\n\tif (adapter->wol & WAKE_MAGIC)\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_MAGIC;\n\n\tpmConf->numFilters = i;\n\n\tadapter->shared->devRead.pmConfDesc.confVer = cpu_to_le32(1);\n\tadapter->shared->devRead.pmConfDesc.confLen = cpu_to_le32(sizeof(\n\t\t\t\t\t\t\t\t  *pmConf));\n\tadapter->shared->devRead.pmConfDesc.confPA =\n\t\tcpu_to_le64(adapter->pm_conf_pa);\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_UPDATE_PMCFG);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tpci_save_state(pdev);\n\tpci_enable_wake(pdev, pci_choose_state(pdev, PMSG_SUSPEND),\n\t\t\tadapter->wol);\n\tpci_disable_device(pdev);\n\tpci_set_power_state(pdev, pci_choose_state(pdev, PMSG_SUSPEND));\n\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_resume(struct device *device)\n{\n\tint err;\n\tunsigned long flags;\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\tpci_restore_state(pdev);\n\terr = pci_enable_device_mem(pdev);\n\tif (err != 0)\n\t\treturn err;\n\n\tpci_enable_wake(pdev, PCI_D0, 0);\n\n\tvmxnet3_alloc_intr_resources(adapter);\n\n\t/* During hibernate and suspend, device has to be reinitialized as the\n\t * device state need not be preserved.\n\t */\n\n\t/* Need not check adapter state as other reset tasks cannot run during\n\t * device resume.\n\t */\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_tq_cleanup_all(adapter);\n\tvmxnet3_rq_cleanup_all(adapter);\n\n\tvmxnet3_reset_dev(adapter);\n\terr = vmxnet3_activate_dev(adapter);\n\tif (err != 0) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"failed to re-activate on resume, error: %d\", err);\n\t\tvmxnet3_force_close(adapter);\n\t\treturn err;\n\t}\n\tnetif_device_attach(netdev);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops vmxnet3_pm_ops = {\n\t.suspend = vmxnet3_suspend,\n\t.resume = vmxnet3_resume,\n\t.freeze = vmxnet3_suspend,\n\t.restore = vmxnet3_resume,\n};\n#endif\n\nstatic struct pci_driver vmxnet3_driver = {\n\t.name\t\t= vmxnet3_driver_name,\n\t.id_table\t= vmxnet3_pciid_table,\n\t.probe\t\t= vmxnet3_probe_device,\n\t.remove\t\t= vmxnet3_remove_device,\n\t.shutdown\t= vmxnet3_shutdown_device,\n#ifdef CONFIG_PM\n\t.driver.pm\t= &vmxnet3_pm_ops,\n#endif\n};\n\n\nstatic int __init\nvmxnet3_init_module(void)\n{\n\tpr_info(\"%s - version %s\\n\", VMXNET3_DRIVER_DESC,\n\t\tVMXNET3_DRIVER_VERSION_REPORT);\n\treturn pci_register_driver(&vmxnet3_driver);\n}\n\nmodule_init(vmxnet3_init_module);\n\n\nstatic void\nvmxnet3_exit_module(void)\n{\n\tpci_unregister_driver(&vmxnet3_driver);\n}\n\nmodule_exit(vmxnet3_exit_module);\n\nMODULE_AUTHOR(\"VMware, Inc.\");\nMODULE_DESCRIPTION(VMXNET3_DRIVER_DESC);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_VERSION(VMXNET3_DRIVER_VERSION_STRING);\n"], "fixing_code": ["/*\n * Linux driver for VMware's vmxnet3 ethernet NIC.\n *\n * Copyright (C) 2008-2021, VMware, Inc. All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; version 2 of the License and no later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or\n * NON INFRINGEMENT. See the GNU General Public License for more\n * details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n *\n * The full GNU General Public License is included in this distribution in\n * the file called \"COPYING\".\n *\n * Maintained by: pv-drivers@vmware.com\n *\n */\n\n#include <linux/module.h>\n#include <net/ip6_checksum.h>\n\n#include \"vmxnet3_int.h\"\n\nchar vmxnet3_driver_name[] = \"vmxnet3\";\n#define VMXNET3_DRIVER_DESC \"VMware vmxnet3 virtual NIC driver\"\n\n/*\n * PCI Device ID Table\n * Last entry must be all 0s\n */\nstatic const struct pci_device_id vmxnet3_pciid_table[] = {\n\t{PCI_VDEVICE(VMWARE, PCI_DEVICE_ID_VMWARE_VMXNET3)},\n\t{0}\n};\n\nMODULE_DEVICE_TABLE(pci, vmxnet3_pciid_table);\n\nstatic int enable_mq = 1;\n\nstatic void\nvmxnet3_write_mac_addr(struct vmxnet3_adapter *adapter, const u8 *mac);\n\n/*\n *    Enable/Disable the given intr\n */\nstatic void\nvmxnet3_enable_intr(struct vmxnet3_adapter *adapter, unsigned intr_idx)\n{\n\tVMXNET3_WRITE_BAR0_REG(adapter, VMXNET3_REG_IMR + intr_idx * 8, 0);\n}\n\n\nstatic void\nvmxnet3_disable_intr(struct vmxnet3_adapter *adapter, unsigned intr_idx)\n{\n\tVMXNET3_WRITE_BAR0_REG(adapter, VMXNET3_REG_IMR + intr_idx * 8, 1);\n}\n\n\n/*\n *    Enable/Disable all intrs used by the device\n */\nstatic void\nvmxnet3_enable_all_intrs(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\tvmxnet3_enable_intr(adapter, i);\n\tadapter->shared->devRead.intrConf.intrCtrl &=\n\t\t\t\t\tcpu_to_le32(~VMXNET3_IC_DISABLE_ALL);\n}\n\n\nstatic void\nvmxnet3_disable_all_intrs(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tadapter->shared->devRead.intrConf.intrCtrl |=\n\t\t\t\t\tcpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\tvmxnet3_disable_intr(adapter, i);\n}\n\n\nstatic void\nvmxnet3_ack_events(struct vmxnet3_adapter *adapter, u32 events)\n{\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_ECR, events);\n}\n\n\nstatic bool\nvmxnet3_tq_stopped(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\treturn tq->stopped;\n}\n\n\nstatic void\nvmxnet3_tq_start(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = false;\n\tnetif_start_subqueue(adapter->netdev, tq - adapter->tx_queue);\n}\n\n\nstatic void\nvmxnet3_tq_wake(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = false;\n\tnetif_wake_subqueue(adapter->netdev, (tq - adapter->tx_queue));\n}\n\n\nstatic void\nvmxnet3_tq_stop(struct vmxnet3_tx_queue *tq, struct vmxnet3_adapter *adapter)\n{\n\ttq->stopped = true;\n\ttq->num_stop++;\n\tnetif_stop_subqueue(adapter->netdev, (tq - adapter->tx_queue));\n}\n\n\n/*\n * Check the link state. This may start or stop the tx queue.\n */\nstatic void\nvmxnet3_check_link(struct vmxnet3_adapter *adapter, bool affectTxQueue)\n{\n\tu32 ret;\n\tint i;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD, VMXNET3_CMD_GET_LINK);\n\tret = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tadapter->link_speed = ret >> 16;\n\tif (ret & 1) { /* Link is up. */\n\t\tnetdev_info(adapter->netdev, \"NIC Link is Up %d Mbps\\n\",\n\t\t\t    adapter->link_speed);\n\t\tnetif_carrier_on(adapter->netdev);\n\n\t\tif (affectTxQueue) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tvmxnet3_tq_start(&adapter->tx_queue[i],\n\t\t\t\t\t\t adapter);\n\t\t}\n\t} else {\n\t\tnetdev_info(adapter->netdev, \"NIC Link is Down\\n\");\n\t\tnetif_carrier_off(adapter->netdev);\n\n\t\tif (affectTxQueue) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tvmxnet3_tq_stop(&adapter->tx_queue[i], adapter);\n\t\t}\n\t}\n}\n\nstatic void\nvmxnet3_process_events(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tunsigned long flags;\n\tu32 events = le32_to_cpu(adapter->shared->ecr);\n\tif (!events)\n\t\treturn;\n\n\tvmxnet3_ack_events(adapter, events);\n\n\t/* Check if link state has changed */\n\tif (events & VMXNET3_ECR_LINK)\n\t\tvmxnet3_check_link(adapter, true);\n\n\t/* Check if there is an error on xmit/recv queues */\n\tif (events & (VMXNET3_ECR_TQERR | VMXNET3_ECR_RQERR)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_QUEUE_STATUS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\tif (adapter->tqd_start[i].status.stopped)\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"%s: tq[%d] error 0x%x\\n\",\n\t\t\t\t\tadapter->netdev->name, i, le32_to_cpu(\n\t\t\t\t\tadapter->tqd_start[i].status.error));\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tif (adapter->rqd_start[i].status.stopped)\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"%s: rq[%d] error 0x%x\\n\",\n\t\t\t\t\tadapter->netdev->name, i,\n\t\t\t\t\tadapter->rqd_start[i].status.error);\n\n\t\tschedule_work(&adapter->work);\n\t}\n}\n\n#ifdef __BIG_ENDIAN_BITFIELD\n/*\n * The device expects the bitfields in shared structures to be written in\n * little endian. When CPU is big endian, the following routines are used to\n * correctly read and write into ABI.\n * The general technique used here is : double word bitfields are defined in\n * opposite order for big endian architecture. Then before reading them in\n * driver the complete double word is translated using le32_to_cpu. Similarly\n * After the driver writes into bitfields, cpu_to_le32 is used to translate the\n * double words into required format.\n * In order to avoid touching bits in shared structure more than once, temporary\n * descriptors are used. These are passed as srcDesc to following functions.\n */\nstatic void vmxnet3_RxDescToCPU(const struct Vmxnet3_RxDesc *srcDesc,\n\t\t\t\tstruct Vmxnet3_RxDesc *dstDesc)\n{\n\tu32 *src = (u32 *)srcDesc + 2;\n\tu32 *dst = (u32 *)dstDesc + 2;\n\tdstDesc->addr = le64_to_cpu(srcDesc->addr);\n\t*dst = le32_to_cpu(*src);\n\tdstDesc->ext1 = le32_to_cpu(srcDesc->ext1);\n}\n\nstatic void vmxnet3_TxDescToLe(const struct Vmxnet3_TxDesc *srcDesc,\n\t\t\t       struct Vmxnet3_TxDesc *dstDesc)\n{\n\tint i;\n\tu32 *src = (u32 *)(srcDesc + 1);\n\tu32 *dst = (u32 *)(dstDesc + 1);\n\n\t/* Working backwards so that the gen bit is set at the end. */\n\tfor (i = 2; i > 0; i--) {\n\t\tsrc--;\n\t\tdst--;\n\t\t*dst = cpu_to_le32(*src);\n\t}\n}\n\n\nstatic void vmxnet3_RxCompToCPU(const struct Vmxnet3_RxCompDesc *srcDesc,\n\t\t\t\tstruct Vmxnet3_RxCompDesc *dstDesc)\n{\n\tint i = 0;\n\tu32 *src = (u32 *)srcDesc;\n\tu32 *dst = (u32 *)dstDesc;\n\tfor (i = 0; i < sizeof(struct Vmxnet3_RxCompDesc) / sizeof(u32); i++) {\n\t\t*dst = le32_to_cpu(*src);\n\t\tsrc++;\n\t\tdst++;\n\t}\n}\n\n\n/* Used to read bitfield values from double words. */\nstatic u32 get_bitfield32(const __le32 *bitfield, u32 pos, u32 size)\n{\n\tu32 temp = le32_to_cpu(*bitfield);\n\tu32 mask = ((1 << size) - 1) << pos;\n\ttemp &= mask;\n\ttemp >>= pos;\n\treturn temp;\n}\n\n\n\n#endif  /* __BIG_ENDIAN_BITFIELD */\n\n#ifdef __BIG_ENDIAN_BITFIELD\n\n#   define VMXNET3_TXDESC_GET_GEN(txdesc) get_bitfield32(((const __le32 *) \\\n\t\t\ttxdesc) + VMXNET3_TXD_GEN_DWORD_SHIFT, \\\n\t\t\tVMXNET3_TXD_GEN_SHIFT, VMXNET3_TXD_GEN_SIZE)\n#   define VMXNET3_TXDESC_GET_EOP(txdesc) get_bitfield32(((const __le32 *) \\\n\t\t\ttxdesc) + VMXNET3_TXD_EOP_DWORD_SHIFT, \\\n\t\t\tVMXNET3_TXD_EOP_SHIFT, VMXNET3_TXD_EOP_SIZE)\n#   define VMXNET3_TCD_GET_GEN(tcd) get_bitfield32(((const __le32 *)tcd) + \\\n\t\t\tVMXNET3_TCD_GEN_DWORD_SHIFT, VMXNET3_TCD_GEN_SHIFT, \\\n\t\t\tVMXNET3_TCD_GEN_SIZE)\n#   define VMXNET3_TCD_GET_TXIDX(tcd) get_bitfield32((const __le32 *)tcd, \\\n\t\t\tVMXNET3_TCD_TXIDX_SHIFT, VMXNET3_TCD_TXIDX_SIZE)\n#   define vmxnet3_getRxComp(dstrcd, rcd, tmp) do { \\\n\t\t\t(dstrcd) = (tmp); \\\n\t\t\tvmxnet3_RxCompToCPU((rcd), (tmp)); \\\n\t\t} while (0)\n#   define vmxnet3_getRxDesc(dstrxd, rxd, tmp) do { \\\n\t\t\t(dstrxd) = (tmp); \\\n\t\t\tvmxnet3_RxDescToCPU((rxd), (tmp)); \\\n\t\t} while (0)\n\n#else\n\n#   define VMXNET3_TXDESC_GET_GEN(txdesc) ((txdesc)->gen)\n#   define VMXNET3_TXDESC_GET_EOP(txdesc) ((txdesc)->eop)\n#   define VMXNET3_TCD_GET_GEN(tcd) ((tcd)->gen)\n#   define VMXNET3_TCD_GET_TXIDX(tcd) ((tcd)->txdIdx)\n#   define vmxnet3_getRxComp(dstrcd, rcd, tmp) (dstrcd) = (rcd)\n#   define vmxnet3_getRxDesc(dstrxd, rxd, tmp) (dstrxd) = (rxd)\n\n#endif /* __BIG_ENDIAN_BITFIELD  */\n\n\nstatic void\nvmxnet3_unmap_tx_buf(struct vmxnet3_tx_buf_info *tbi,\n\t\t     struct pci_dev *pdev)\n{\n\tif (tbi->map_type == VMXNET3_MAP_SINGLE)\n\t\tdma_unmap_single(&pdev->dev, tbi->dma_addr, tbi->len,\n\t\t\t\t DMA_TO_DEVICE);\n\telse if (tbi->map_type == VMXNET3_MAP_PAGE)\n\t\tdma_unmap_page(&pdev->dev, tbi->dma_addr, tbi->len,\n\t\t\t       DMA_TO_DEVICE);\n\telse\n\t\tBUG_ON(tbi->map_type != VMXNET3_MAP_NONE);\n\n\ttbi->map_type = VMXNET3_MAP_NONE; /* to help debugging */\n}\n\n\nstatic int\nvmxnet3_unmap_pkt(u32 eop_idx, struct vmxnet3_tx_queue *tq,\n\t\t  struct pci_dev *pdev,\tstruct vmxnet3_adapter *adapter)\n{\n\tstruct sk_buff *skb;\n\tint entries = 0;\n\n\t/* no out of order completion */\n\tBUG_ON(tq->buf_info[eop_idx].sop_idx != tq->tx_ring.next2comp);\n\tBUG_ON(VMXNET3_TXDESC_GET_EOP(&(tq->tx_ring.base[eop_idx].txd)) != 1);\n\n\tskb = tq->buf_info[eop_idx].skb;\n\tBUG_ON(skb == NULL);\n\ttq->buf_info[eop_idx].skb = NULL;\n\n\tVMXNET3_INC_RING_IDX_ONLY(eop_idx, tq->tx_ring.size);\n\n\twhile (tq->tx_ring.next2comp != eop_idx) {\n\t\tvmxnet3_unmap_tx_buf(tq->buf_info + tq->tx_ring.next2comp,\n\t\t\t\t     pdev);\n\n\t\t/* update next2comp w/o tx_lock. Since we are marking more,\n\t\t * instead of less, tx ring entries avail, the worst case is\n\t\t * that the tx routine incorrectly re-queues a pkt due to\n\t\t * insufficient tx ring entries.\n\t\t */\n\t\tvmxnet3_cmd_ring_adv_next2comp(&tq->tx_ring);\n\t\tentries++;\n\t}\n\n\tdev_kfree_skb_any(skb);\n\treturn entries;\n}\n\n\nstatic int\nvmxnet3_tq_tx_complete(struct vmxnet3_tx_queue *tq,\n\t\t\tstruct vmxnet3_adapter *adapter)\n{\n\tint completed = 0;\n\tunion Vmxnet3_GenericDesc *gdesc;\n\n\tgdesc = tq->comp_ring.base + tq->comp_ring.next2proc;\n\twhile (VMXNET3_TCD_GET_GEN(&gdesc->tcd) == tq->comp_ring.gen) {\n\t\t/* Prevent any &gdesc->tcd field from being (speculatively)\n\t\t * read before (&gdesc->tcd)->gen is read.\n\t\t */\n\t\tdma_rmb();\n\n\t\tcompleted += vmxnet3_unmap_pkt(VMXNET3_TCD_GET_TXIDX(\n\t\t\t\t\t       &gdesc->tcd), tq, adapter->pdev,\n\t\t\t\t\t       adapter);\n\n\t\tvmxnet3_comp_ring_adv_next2proc(&tq->comp_ring);\n\t\tgdesc = tq->comp_ring.base + tq->comp_ring.next2proc;\n\t}\n\n\tif (completed) {\n\t\tspin_lock(&tq->tx_lock);\n\t\tif (unlikely(vmxnet3_tq_stopped(tq, adapter) &&\n\t\t\t     vmxnet3_cmd_ring_desc_avail(&tq->tx_ring) >\n\t\t\t     VMXNET3_WAKE_QUEUE_THRESHOLD(tq) &&\n\t\t\t     netif_carrier_ok(adapter->netdev))) {\n\t\t\tvmxnet3_tq_wake(tq, adapter);\n\t\t}\n\t\tspin_unlock(&tq->tx_lock);\n\t}\n\treturn completed;\n}\n\n\nstatic void\nvmxnet3_tq_cleanup(struct vmxnet3_tx_queue *tq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\twhile (tq->tx_ring.next2comp != tq->tx_ring.next2fill) {\n\t\tstruct vmxnet3_tx_buf_info *tbi;\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2comp;\n\n\t\tvmxnet3_unmap_tx_buf(tbi, adapter->pdev);\n\t\tif (tbi->skb) {\n\t\t\tdev_kfree_skb_any(tbi->skb);\n\t\t\ttbi->skb = NULL;\n\t\t}\n\t\tvmxnet3_cmd_ring_adv_next2comp(&tq->tx_ring);\n\t}\n\n\t/* sanity check, verify all buffers are indeed unmapped and freed */\n\tfor (i = 0; i < tq->tx_ring.size; i++) {\n\t\tBUG_ON(tq->buf_info[i].skb != NULL ||\n\t\t       tq->buf_info[i].map_type != VMXNET3_MAP_NONE);\n\t}\n\n\ttq->tx_ring.gen = VMXNET3_INIT_GEN;\n\ttq->tx_ring.next2fill = tq->tx_ring.next2comp = 0;\n\n\ttq->comp_ring.gen = VMXNET3_INIT_GEN;\n\ttq->comp_ring.next2proc = 0;\n}\n\n\nstatic void\nvmxnet3_tq_destroy(struct vmxnet3_tx_queue *tq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tif (tq->tx_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, tq->tx_ring.size *\n\t\t\t\t  sizeof(struct Vmxnet3_TxDesc),\n\t\t\t\t  tq->tx_ring.base, tq->tx_ring.basePA);\n\t\ttq->tx_ring.base = NULL;\n\t}\n\tif (tq->data_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  tq->data_ring.size * tq->txdata_desc_size,\n\t\t\t\t  tq->data_ring.base, tq->data_ring.basePA);\n\t\ttq->data_ring.base = NULL;\n\t}\n\tif (tq->comp_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, tq->comp_ring.size *\n\t\t\t\t  sizeof(struct Vmxnet3_TxCompDesc),\n\t\t\t\t  tq->comp_ring.base, tq->comp_ring.basePA);\n\t\ttq->comp_ring.base = NULL;\n\t}\n\tkfree(tq->buf_info);\n\ttq->buf_info = NULL;\n}\n\n\n/* Destroy all tx queues */\nvoid\nvmxnet3_tq_destroy_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_destroy(&adapter->tx_queue[i], adapter);\n}\n\n\nstatic void\nvmxnet3_tq_init(struct vmxnet3_tx_queue *tq,\n\t\tstruct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\t/* reset the tx ring contents to 0 and reset the tx ring states */\n\tmemset(tq->tx_ring.base, 0, tq->tx_ring.size *\n\t       sizeof(struct Vmxnet3_TxDesc));\n\ttq->tx_ring.next2fill = tq->tx_ring.next2comp = 0;\n\ttq->tx_ring.gen = VMXNET3_INIT_GEN;\n\n\tmemset(tq->data_ring.base, 0,\n\t       tq->data_ring.size * tq->txdata_desc_size);\n\n\t/* reset the tx comp ring contents to 0 and reset comp ring states */\n\tmemset(tq->comp_ring.base, 0, tq->comp_ring.size *\n\t       sizeof(struct Vmxnet3_TxCompDesc));\n\ttq->comp_ring.next2proc = 0;\n\ttq->comp_ring.gen = VMXNET3_INIT_GEN;\n\n\t/* reset the bookkeeping data */\n\tmemset(tq->buf_info, 0, sizeof(tq->buf_info[0]) * tq->tx_ring.size);\n\tfor (i = 0; i < tq->tx_ring.size; i++)\n\t\ttq->buf_info[i].map_type = VMXNET3_MAP_NONE;\n\n\t/* stats are not reset */\n}\n\n\nstatic int\nvmxnet3_tq_create(struct vmxnet3_tx_queue *tq,\n\t\t  struct vmxnet3_adapter *adapter)\n{\n\tBUG_ON(tq->tx_ring.base || tq->data_ring.base ||\n\t       tq->comp_ring.base || tq->buf_info);\n\n\ttq->tx_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->tx_ring.size * sizeof(struct Vmxnet3_TxDesc),\n\t\t\t&tq->tx_ring.basePA, GFP_KERNEL);\n\tif (!tq->tx_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->data_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->data_ring.size * tq->txdata_desc_size,\n\t\t\t&tq->data_ring.basePA, GFP_KERNEL);\n\tif (!tq->data_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx data ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->comp_ring.base = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\ttq->comp_ring.size * sizeof(struct Vmxnet3_TxCompDesc),\n\t\t\t&tq->comp_ring.basePA, GFP_KERNEL);\n\tif (!tq->comp_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate tx comp ring\\n\");\n\t\tgoto err;\n\t}\n\n\ttq->buf_info = kcalloc_node(tq->tx_ring.size, sizeof(tq->buf_info[0]),\n\t\t\t\t    GFP_KERNEL,\n\t\t\t\t    dev_to_node(&adapter->pdev->dev));\n\tif (!tq->buf_info)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tvmxnet3_tq_destroy(tq, adapter);\n\treturn -ENOMEM;\n}\n\nstatic void\nvmxnet3_tq_cleanup_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_cleanup(&adapter->tx_queue[i], adapter);\n}\n\n/*\n *    starting from ring->next2fill, allocate rx buffers for the given ring\n *    of the rx queue and update the rx desc. stop after @num_to_alloc buffers\n *    are allocated or allocation fails\n */\n\nstatic int\nvmxnet3_rq_alloc_rx_buf(struct vmxnet3_rx_queue *rq, u32 ring_idx,\n\t\t\tint num_to_alloc, struct vmxnet3_adapter *adapter)\n{\n\tint num_allocated = 0;\n\tstruct vmxnet3_rx_buf_info *rbi_base = rq->buf_info[ring_idx];\n\tstruct vmxnet3_cmd_ring *ring = &rq->rx_ring[ring_idx];\n\tu32 val;\n\n\twhile (num_allocated <= num_to_alloc) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tunion Vmxnet3_GenericDesc *gd;\n\n\t\trbi = rbi_base + ring->next2fill;\n\t\tgd = ring->base + ring->next2fill;\n\n\t\tif (rbi->buf_type == VMXNET3_RX_BUF_SKB) {\n\t\t\tif (rbi->skb == NULL) {\n\t\t\t\trbi->skb = __netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t\t       rbi->len,\n\t\t\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\t\tif (unlikely(rbi->skb == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\trbi->dma_addr = dma_map_single(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->skb->data, rbi->len,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tdev_kfree_skb_any(rbi->skb);\n\t\t\t\t\trbi->skb = NULL;\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffer skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_HEAD << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t} else {\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE ||\n\t\t\t       rbi->len  != PAGE_SIZE);\n\n\t\t\tif (rbi->page == NULL) {\n\t\t\t\trbi->page = alloc_page(GFP_ATOMIC);\n\t\t\t\tif (unlikely(rbi->page == NULL)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\trbi->dma_addr = dma_map_page(\n\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\trbi->page, 0, PAGE_SIZE,\n\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      rbi->dma_addr)) {\n\t\t\t\t\tput_page(rbi->page);\n\t\t\t\t\trbi->page = NULL;\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* rx buffers skipped by the device */\n\t\t\t}\n\t\t\tval = VMXNET3_RXD_BTYPE_BODY << VMXNET3_RXD_BTYPE_SHIFT;\n\t\t}\n\n\t\tgd->rxd.addr = cpu_to_le64(rbi->dma_addr);\n\t\tgd->dword[2] = cpu_to_le32((!ring->gen << VMXNET3_RXD_GEN_SHIFT)\n\t\t\t\t\t   | val | rbi->len);\n\n\t\t/* Fill the last buffer but dont mark it ready, or else the\n\t\t * device will think that the queue is full */\n\t\tif (num_allocated == num_to_alloc)\n\t\t\tbreak;\n\n\t\tgd->dword[2] |= cpu_to_le32(ring->gen << VMXNET3_RXD_GEN_SHIFT);\n\t\tnum_allocated++;\n\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t}\n\n\tnetdev_dbg(adapter->netdev,\n\t\t\"alloc_rx_buf: %d allocated, next2fill %u, next2comp %u\\n\",\n\t\tnum_allocated, ring->next2fill, ring->next2comp);\n\n\t/* so that the device can distinguish a full ring and an empty ring */\n\tBUG_ON(num_allocated != 0 && ring->next2fill == ring->next2comp);\n\n\treturn num_allocated;\n}\n\n\nstatic void\nvmxnet3_append_frag(struct sk_buff *skb, struct Vmxnet3_RxCompDesc *rcd,\n\t\t    struct vmxnet3_rx_buf_info *rbi)\n{\n\tskb_frag_t *frag = skb_shinfo(skb)->frags + skb_shinfo(skb)->nr_frags;\n\n\tBUG_ON(skb_shinfo(skb)->nr_frags >= MAX_SKB_FRAGS);\n\n\t__skb_frag_set_page(frag, rbi->page);\n\tskb_frag_off_set(frag, 0);\n\tskb_frag_size_set(frag, rcd->len);\n\tskb->data_len += rcd->len;\n\tskb->truesize += PAGE_SIZE;\n\tskb_shinfo(skb)->nr_frags++;\n}\n\n\nstatic int\nvmxnet3_map_pkt(struct sk_buff *skb, struct vmxnet3_tx_ctx *ctx,\n\t\tstruct vmxnet3_tx_queue *tq, struct pci_dev *pdev,\n\t\tstruct vmxnet3_adapter *adapter)\n{\n\tu32 dw2, len;\n\tunsigned long buf_offset;\n\tint i;\n\tunion Vmxnet3_GenericDesc *gdesc;\n\tstruct vmxnet3_tx_buf_info *tbi = NULL;\n\n\tBUG_ON(ctx->copy_size > skb_headlen(skb));\n\n\t/* use the previous gen bit for the SOP desc */\n\tdw2 = (tq->tx_ring.gen ^ 0x1) << VMXNET3_TXD_GEN_SHIFT;\n\n\tctx->sop_txd = tq->tx_ring.base + tq->tx_ring.next2fill;\n\tgdesc = ctx->sop_txd; /* both loops below can be skipped */\n\n\t/* no need to map the buffer if headers are copied */\n\tif (ctx->copy_size) {\n\t\tctx->sop_txd->txd.addr = cpu_to_le64(tq->data_ring.basePA +\n\t\t\t\t\ttq->tx_ring.next2fill *\n\t\t\t\t\ttq->txdata_desc_size);\n\t\tctx->sop_txd->dword[2] = cpu_to_le32(dw2 | ctx->copy_size);\n\t\tctx->sop_txd->dword[3] = 0;\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\ttbi->map_type = VMXNET3_MAP_NONE;\n\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"txd[%u]: 0x%Lx 0x%x 0x%x\\n\",\n\t\t\ttq->tx_ring.next2fill,\n\t\t\tle64_to_cpu(ctx->sop_txd->txd.addr),\n\t\t\tctx->sop_txd->dword[2], ctx->sop_txd->dword[3]);\n\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\n\t\t/* use the right gen for non-SOP desc */\n\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\t}\n\n\t/* linear part can use multiple tx desc if it's big */\n\tlen = skb_headlen(skb) - ctx->copy_size;\n\tbuf_offset = ctx->copy_size;\n\twhile (len) {\n\t\tu32 buf_size;\n\n\t\tif (len < VMXNET3_MAX_TX_BUF_SIZE) {\n\t\t\tbuf_size = len;\n\t\t\tdw2 |= len;\n\t\t} else {\n\t\t\tbuf_size = VMXNET3_MAX_TX_BUF_SIZE;\n\t\t\t/* spec says that for TxDesc.len, 0 == 2^14 */\n\t\t}\n\n\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\ttbi->map_type = VMXNET3_MAP_SINGLE;\n\t\ttbi->dma_addr = dma_map_single(&adapter->pdev->dev,\n\t\t\t\tskb->data + buf_offset, buf_size,\n\t\t\t\tDMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&adapter->pdev->dev, tbi->dma_addr))\n\t\t\treturn -EFAULT;\n\n\t\ttbi->len = buf_size;\n\n\t\tgdesc = tq->tx_ring.base + tq->tx_ring.next2fill;\n\t\tBUG_ON(gdesc->txd.gen == tq->tx_ring.gen);\n\n\t\tgdesc->txd.addr = cpu_to_le64(tbi->dma_addr);\n\t\tgdesc->dword[2] = cpu_to_le32(dw2);\n\t\tgdesc->dword[3] = 0;\n\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"txd[%u]: 0x%Lx 0x%x 0x%x\\n\",\n\t\t\ttq->tx_ring.next2fill, le64_to_cpu(gdesc->txd.addr),\n\t\t\tle32_to_cpu(gdesc->dword[2]), gdesc->dword[3]);\n\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\n\t\tlen -= buf_size;\n\t\tbuf_offset += buf_size;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32 buf_size;\n\n\t\tbuf_offset = 0;\n\t\tlen = skb_frag_size(frag);\n\t\twhile (len) {\n\t\t\ttbi = tq->buf_info + tq->tx_ring.next2fill;\n\t\t\tif (len < VMXNET3_MAX_TX_BUF_SIZE) {\n\t\t\t\tbuf_size = len;\n\t\t\t\tdw2 |= len;\n\t\t\t} else {\n\t\t\t\tbuf_size = VMXNET3_MAX_TX_BUF_SIZE;\n\t\t\t\t/* spec says that for TxDesc.len, 0 == 2^14 */\n\t\t\t}\n\t\t\ttbi->map_type = VMXNET3_MAP_PAGE;\n\t\t\ttbi->dma_addr = skb_frag_dma_map(&adapter->pdev->dev, frag,\n\t\t\t\t\t\t\t buf_offset, buf_size,\n\t\t\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&adapter->pdev->dev, tbi->dma_addr))\n\t\t\t\treturn -EFAULT;\n\n\t\t\ttbi->len = buf_size;\n\n\t\t\tgdesc = tq->tx_ring.base + tq->tx_ring.next2fill;\n\t\t\tBUG_ON(gdesc->txd.gen == tq->tx_ring.gen);\n\n\t\t\tgdesc->txd.addr = cpu_to_le64(tbi->dma_addr);\n\t\t\tgdesc->dword[2] = cpu_to_le32(dw2);\n\t\t\tgdesc->dword[3] = 0;\n\n\t\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\t\"txd[%u]: 0x%llx %u %u\\n\",\n\t\t\t\ttq->tx_ring.next2fill, le64_to_cpu(gdesc->txd.addr),\n\t\t\t\tle32_to_cpu(gdesc->dword[2]), gdesc->dword[3]);\n\t\t\tvmxnet3_cmd_ring_adv_next2fill(&tq->tx_ring);\n\t\t\tdw2 = tq->tx_ring.gen << VMXNET3_TXD_GEN_SHIFT;\n\n\t\t\tlen -= buf_size;\n\t\t\tbuf_offset += buf_size;\n\t\t}\n\t}\n\n\tctx->eop_txd = gdesc;\n\n\t/* set the last buf_info for the pkt */\n\ttbi->skb = skb;\n\ttbi->sop_idx = ctx->sop_txd - tq->tx_ring.base;\n\n\treturn 0;\n}\n\n\n/* Init all tx queues */\nstatic void\nvmxnet3_tq_init_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_init(&adapter->tx_queue[i], adapter);\n}\n\n\n/*\n *    parse relevant protocol headers:\n *      For a tso pkt, relevant headers are L2/3/4 including options\n *      For a pkt requesting csum offloading, they are L2/3 and may include L4\n *      if it's a TCP/UDP pkt\n *\n * Returns:\n *    -1:  error happens during parsing\n *     0:  protocol headers parsed, but too big to be copied\n *     1:  protocol headers parsed and copied\n *\n * Other effects:\n *    1. related *ctx fields are updated.\n *    2. ctx->copy_size is # of bytes copied\n *    3. the portion to be copied is guaranteed to be in the linear part\n *\n */\nstatic int\nvmxnet3_parse_hdr(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\t  struct vmxnet3_tx_ctx *ctx,\n\t\t  struct vmxnet3_adapter *adapter)\n{\n\tu8 protocol = 0;\n\n\tif (ctx->mss) {\t/* TSO */\n\t\tif (VMXNET3_VERSION_GE_4(adapter) && skb->encapsulation) {\n\t\t\tctx->l4_offset = skb_inner_transport_offset(skb);\n\t\t\tctx->l4_hdr_size = inner_tcp_hdrlen(skb);\n\t\t\tctx->copy_size = ctx->l4_offset + ctx->l4_hdr_size;\n\t\t} else {\n\t\t\tctx->l4_offset = skb_transport_offset(skb);\n\t\t\tctx->l4_hdr_size = tcp_hdrlen(skb);\n\t\t\tctx->copy_size = ctx->l4_offset + ctx->l4_hdr_size;\n\t\t}\n\t} else {\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t/* For encap packets, skb_checksum_start_offset refers\n\t\t\t * to inner L4 offset. Thus, below works for encap as\n\t\t\t * well as non-encap case\n\t\t\t */\n\t\t\tctx->l4_offset = skb_checksum_start_offset(skb);\n\n\t\t\tif (VMXNET3_VERSION_GE_4(adapter) &&\n\t\t\t    skb->encapsulation) {\n\t\t\t\tstruct iphdr *iph = inner_ip_hdr(skb);\n\n\t\t\t\tif (iph->version == 4) {\n\t\t\t\t\tprotocol = iph->protocol;\n\t\t\t\t} else {\n\t\t\t\t\tconst struct ipv6hdr *ipv6h;\n\n\t\t\t\t\tipv6h = inner_ipv6_hdr(skb);\n\t\t\t\t\tprotocol = ipv6h->nexthdr;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (ctx->ipv4) {\n\t\t\t\t\tconst struct iphdr *iph = ip_hdr(skb);\n\n\t\t\t\t\tprotocol = iph->protocol;\n\t\t\t\t} else if (ctx->ipv6) {\n\t\t\t\t\tconst struct ipv6hdr *ipv6h;\n\n\t\t\t\t\tipv6h = ipv6_hdr(skb);\n\t\t\t\t\tprotocol = ipv6h->nexthdr;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tswitch (protocol) {\n\t\t\tcase IPPROTO_TCP:\n\t\t\t\tctx->l4_hdr_size = skb->encapsulation ? inner_tcp_hdrlen(skb) :\n\t\t\t\t\t\t   tcp_hdrlen(skb);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_UDP:\n\t\t\t\tctx->l4_hdr_size = sizeof(struct udphdr);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tctx->l4_hdr_size = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tctx->copy_size = min(ctx->l4_offset +\n\t\t\t\t\t ctx->l4_hdr_size, skb->len);\n\t\t} else {\n\t\t\tctx->l4_offset = 0;\n\t\t\tctx->l4_hdr_size = 0;\n\t\t\t/* copy as much as allowed */\n\t\t\tctx->copy_size = min_t(unsigned int,\n\t\t\t\t\t       tq->txdata_desc_size,\n\t\t\t\t\t       skb_headlen(skb));\n\t\t}\n\n\t\tif (skb->len <= VMXNET3_HDR_COPY_SIZE)\n\t\t\tctx->copy_size = skb->len;\n\n\t\t/* make sure headers are accessible directly */\n\t\tif (unlikely(!pskb_may_pull(skb, ctx->copy_size)))\n\t\t\tgoto err;\n\t}\n\n\tif (unlikely(ctx->copy_size > tq->txdata_desc_size)) {\n\t\ttq->stats.oversized_hdr++;\n\t\tctx->copy_size = 0;\n\t\treturn 0;\n\t}\n\n\treturn 1;\nerr:\n\treturn -1;\n}\n\n/*\n *    copy relevant protocol headers to the transmit ring:\n *      For a tso pkt, relevant headers are L2/3/4 including options\n *      For a pkt requesting csum offloading, they are L2/3 and may include L4\n *      if it's a TCP/UDP pkt\n *\n *\n *    Note that this requires that vmxnet3_parse_hdr be called first to set the\n *      appropriate bits in ctx first\n */\nstatic void\nvmxnet3_copy_hdr(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\t struct vmxnet3_tx_ctx *ctx,\n\t\t struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_TxDataDesc *tdd;\n\n\ttdd = (struct Vmxnet3_TxDataDesc *)((u8 *)tq->data_ring.base +\n\t\t\t\t\t    tq->tx_ring.next2fill *\n\t\t\t\t\t    tq->txdata_desc_size);\n\n\tmemcpy(tdd->data, skb->data, ctx->copy_size);\n\tnetdev_dbg(adapter->netdev,\n\t\t\"copy %u bytes to dataRing[%u]\\n\",\n\t\tctx->copy_size, tq->tx_ring.next2fill);\n}\n\n\nstatic void\nvmxnet3_prepare_inner_tso(struct sk_buff *skb,\n\t\t\t  struct vmxnet3_tx_ctx *ctx)\n{\n\tstruct tcphdr *tcph = inner_tcp_hdr(skb);\n\tstruct iphdr *iph = inner_ip_hdr(skb);\n\n\tif (iph->version == 4) {\n\t\tiph->check = 0;\n\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t} else {\n\t\tstruct ipv6hdr *iph = inner_ipv6_hdr(skb);\n\n\t\ttcph->check = ~csum_ipv6_magic(&iph->saddr, &iph->daddr, 0,\n\t\t\t\t\t       IPPROTO_TCP, 0);\n\t}\n}\n\nstatic void\nvmxnet3_prepare_tso(struct sk_buff *skb,\n\t\t    struct vmxnet3_tx_ctx *ctx)\n{\n\tstruct tcphdr *tcph = tcp_hdr(skb);\n\n\tif (ctx->ipv4) {\n\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\tiph->check = 0;\n\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t} else if (ctx->ipv6) {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t}\n}\n\nstatic int txd_estimate(const struct sk_buff *skb)\n{\n\tint count = VMXNET3_TXD_NEEDED(skb_headlen(skb)) + 1;\n\tint i;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\n\t\tcount += VMXNET3_TXD_NEEDED(skb_frag_size(frag));\n\t}\n\treturn count;\n}\n\n/*\n * Transmits a pkt thru a given tq\n * Returns:\n *    NETDEV_TX_OK:      descriptors are setup successfully\n *    NETDEV_TX_OK:      error occurred, the pkt is dropped\n *    NETDEV_TX_BUSY:    tx ring is full, queue is stopped\n *\n * Side-effects:\n *    1. tx ring may be changed\n *    2. tq stats may be updated accordingly\n *    3. shared->txNumDeferred may be updated\n */\n\nstatic int\nvmxnet3_tq_xmit(struct sk_buff *skb, struct vmxnet3_tx_queue *tq,\n\t\tstruct vmxnet3_adapter *adapter, struct net_device *netdev)\n{\n\tint ret;\n\tu32 count;\n\tint num_pkts;\n\tint tx_num_deferred;\n\tunsigned long flags;\n\tstruct vmxnet3_tx_ctx ctx;\n\tunion Vmxnet3_GenericDesc *gdesc;\n#ifdef __BIG_ENDIAN_BITFIELD\n\t/* Use temporary descriptor to avoid touching bits multiple times */\n\tunion Vmxnet3_GenericDesc tempTxDesc;\n#endif\n\n\tcount = txd_estimate(skb);\n\n\tctx.ipv4 = (vlan_get_protocol(skb) == cpu_to_be16(ETH_P_IP));\n\tctx.ipv6 = (vlan_get_protocol(skb) == cpu_to_be16(ETH_P_IPV6));\n\n\tctx.mss = skb_shinfo(skb)->gso_size;\n\tif (ctx.mss) {\n\t\tif (skb_header_cloned(skb)) {\n\t\t\tif (unlikely(pskb_expand_head(skb, 0, 0,\n\t\t\t\t\t\t      GFP_ATOMIC) != 0)) {\n\t\t\t\ttq->stats.drop_tso++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t\ttq->stats.copy_skb_header++;\n\t\t}\n\t\tif (skb->encapsulation) {\n\t\t\tvmxnet3_prepare_inner_tso(skb, &ctx);\n\t\t} else {\n\t\t\tvmxnet3_prepare_tso(skb, &ctx);\n\t\t}\n\t} else {\n\t\tif (unlikely(count > VMXNET3_MAX_TXD_PER_PKT)) {\n\n\t\t\t/* non-tso pkts must not use more than\n\t\t\t * VMXNET3_MAX_TXD_PER_PKT entries\n\t\t\t */\n\t\t\tif (skb_linearize(skb) != 0) {\n\t\t\t\ttq->stats.drop_too_many_frags++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t\ttq->stats.linearized++;\n\n\t\t\t/* recalculate the # of descriptors to use */\n\t\t\tcount = VMXNET3_TXD_NEEDED(skb_headlen(skb)) + 1;\n\t\t}\n\t}\n\n\tret = vmxnet3_parse_hdr(skb, tq, &ctx, adapter);\n\tif (ret >= 0) {\n\t\tBUG_ON(ret <= 0 && ctx.copy_size != 0);\n\t\t/* hdrs parsed, check against other limits */\n\t\tif (ctx.mss) {\n\t\t\tif (unlikely(ctx.l4_offset + ctx.l4_hdr_size >\n\t\t\t\t     VMXNET3_MAX_TX_BUF_SIZE)) {\n\t\t\t\ttq->stats.drop_oversized_hdr++;\n\t\t\t\tgoto drop_pkt;\n\t\t\t}\n\t\t} else {\n\t\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t\tif (unlikely(ctx.l4_offset +\n\t\t\t\t\t     skb->csum_offset >\n\t\t\t\t\t     VMXNET3_MAX_CSUM_OFFSET)) {\n\t\t\t\t\ttq->stats.drop_oversized_hdr++;\n\t\t\t\t\tgoto drop_pkt;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\ttq->stats.drop_hdr_inspect_err++;\n\t\tgoto drop_pkt;\n\t}\n\n\tspin_lock_irqsave(&tq->tx_lock, flags);\n\n\tif (count > vmxnet3_cmd_ring_desc_avail(&tq->tx_ring)) {\n\t\ttq->stats.tx_ring_full++;\n\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\"tx queue stopped on %s, next2comp %u\"\n\t\t\t\" next2fill %u\\n\", adapter->netdev->name,\n\t\t\ttq->tx_ring.next2comp, tq->tx_ring.next2fill);\n\n\t\tvmxnet3_tq_stop(tq, adapter);\n\t\tspin_unlock_irqrestore(&tq->tx_lock, flags);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\n\tvmxnet3_copy_hdr(skb, tq, &ctx, adapter);\n\n\t/* fill tx descs related to addr & len */\n\tif (vmxnet3_map_pkt(skb, &ctx, tq, adapter->pdev, adapter))\n\t\tgoto unlock_drop_pkt;\n\n\t/* setup the EOP desc */\n\tctx.eop_txd->dword[3] = cpu_to_le32(VMXNET3_TXD_CQ | VMXNET3_TXD_EOP);\n\n\t/* setup the SOP desc */\n#ifdef __BIG_ENDIAN_BITFIELD\n\tgdesc = &tempTxDesc;\n\tgdesc->dword[2] = ctx.sop_txd->dword[2];\n\tgdesc->dword[3] = ctx.sop_txd->dword[3];\n#else\n\tgdesc = ctx.sop_txd;\n#endif\n\ttx_num_deferred = le32_to_cpu(tq->shared->txNumDeferred);\n\tif (ctx.mss) {\n\t\tif (VMXNET3_VERSION_GE_4(adapter) && skb->encapsulation) {\n\t\t\tgdesc->txd.hlen = ctx.l4_offset + ctx.l4_hdr_size;\n\t\t\tgdesc->txd.om = VMXNET3_OM_ENCAP;\n\t\t\tgdesc->txd.msscof = ctx.mss;\n\n\t\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_TUNNEL_CSUM)\n\t\t\t\tgdesc->txd.oco = 1;\n\t\t} else {\n\t\t\tgdesc->txd.hlen = ctx.l4_offset + ctx.l4_hdr_size;\n\t\t\tgdesc->txd.om = VMXNET3_OM_TSO;\n\t\t\tgdesc->txd.msscof = ctx.mss;\n\t\t}\n\t\tnum_pkts = (skb->len - gdesc->txd.hlen + ctx.mss - 1) / ctx.mss;\n\t} else {\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tif (VMXNET3_VERSION_GE_4(adapter) &&\n\t\t\t    skb->encapsulation) {\n\t\t\t\tgdesc->txd.hlen = ctx.l4_offset +\n\t\t\t\t\t\t  ctx.l4_hdr_size;\n\t\t\t\tgdesc->txd.om = VMXNET3_OM_ENCAP;\n\t\t\t\tgdesc->txd.msscof = 0;\t\t/* Reserved */\n\t\t\t} else {\n\t\t\t\tgdesc->txd.hlen = ctx.l4_offset;\n\t\t\t\tgdesc->txd.om = VMXNET3_OM_CSUM;\n\t\t\t\tgdesc->txd.msscof = ctx.l4_offset +\n\t\t\t\t\t\t    skb->csum_offset;\n\t\t\t}\n\t\t} else {\n\t\t\tgdesc->txd.om = 0;\n\t\t\tgdesc->txd.msscof = 0;\n\t\t}\n\t\tnum_pkts = 1;\n\t}\n\tle32_add_cpu(&tq->shared->txNumDeferred, num_pkts);\n\ttx_num_deferred += num_pkts;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tgdesc->txd.ti = 1;\n\t\tgdesc->txd.tci = skb_vlan_tag_get(skb);\n\t}\n\n\t/* Ensure that the write to (&gdesc->txd)->gen will be observed after\n\t * all other writes to &gdesc->txd.\n\t */\n\tdma_wmb();\n\n\t/* finally flips the GEN bit of the SOP desc. */\n\tgdesc->dword[2] = cpu_to_le32(le32_to_cpu(gdesc->dword[2]) ^\n\t\t\t\t\t\t  VMXNET3_TXD_GEN);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t/* Finished updating in bitfields of Tx Desc, so write them in original\n\t * place.\n\t */\n\tvmxnet3_TxDescToLe((struct Vmxnet3_TxDesc *)gdesc,\n\t\t\t   (struct Vmxnet3_TxDesc *)ctx.sop_txd);\n\tgdesc = ctx.sop_txd;\n#endif\n\tnetdev_dbg(adapter->netdev,\n\t\t\"txd[%u]: SOP 0x%Lx 0x%x 0x%x\\n\",\n\t\t(u32)(ctx.sop_txd -\n\t\ttq->tx_ring.base), le64_to_cpu(gdesc->txd.addr),\n\t\tle32_to_cpu(gdesc->dword[2]), le32_to_cpu(gdesc->dword[3]));\n\n\tspin_unlock_irqrestore(&tq->tx_lock, flags);\n\n\tif (tx_num_deferred >= le32_to_cpu(tq->shared->txThreshold)) {\n\t\ttq->shared->txNumDeferred = 0;\n\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\t       VMXNET3_REG_TXPROD + tq->qid * 8,\n\t\t\t\t       tq->tx_ring.next2fill);\n\t}\n\n\treturn NETDEV_TX_OK;\n\nunlock_drop_pkt:\n\tspin_unlock_irqrestore(&tq->tx_lock, flags);\ndrop_pkt:\n\ttq->stats.drop_total++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\n\nstatic netdev_tx_t\nvmxnet3_xmit_frame(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tBUG_ON(skb->queue_mapping > adapter->num_tx_queues);\n\treturn vmxnet3_tq_xmit(skb,\n\t\t\t       &adapter->tx_queue[skb->queue_mapping],\n\t\t\t       adapter, netdev);\n}\n\n\nstatic void\nvmxnet3_rx_csum(struct vmxnet3_adapter *adapter,\n\t\tstruct sk_buff *skb,\n\t\tunion Vmxnet3_GenericDesc *gdesc)\n{\n\tif (!gdesc->rcd.cnc && adapter->netdev->features & NETIF_F_RXCSUM) {\n\t\tif (gdesc->rcd.v4 &&\n\t\t    (le32_to_cpu(gdesc->dword[3]) &\n\t\t     VMXNET3_RCD_CSUM_OK) == VMXNET3_RCD_CSUM_OK) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tWARN_ON_ONCE(!(gdesc->rcd.tcp || gdesc->rcd.udp) &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t\tWARN_ON_ONCE(gdesc->rcd.frg &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t} else if (gdesc->rcd.v6 && (le32_to_cpu(gdesc->dword[3]) &\n\t\t\t\t\t     (1 << VMXNET3_RCD_TUC_SHIFT))) {\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\tWARN_ON_ONCE(!(gdesc->rcd.tcp || gdesc->rcd.udp) &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t\tWARN_ON_ONCE(gdesc->rcd.frg &&\n\t\t\t\t     !(le32_to_cpu(gdesc->dword[0]) &\n\t\t\t\t     (1UL << VMXNET3_RCD_HDR_INNER_SHIFT)));\n\t\t} else {\n\t\t\tif (gdesc->rcd.csum) {\n\t\t\t\tskb->csum = htons(gdesc->rcd.csum);\n\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\t\t} else {\n\t\t\t\tskb_checksum_none_assert(skb);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tskb_checksum_none_assert(skb);\n\t}\n}\n\n\nstatic void\nvmxnet3_rx_error(struct vmxnet3_rx_queue *rq, struct Vmxnet3_RxCompDesc *rcd,\n\t\t struct vmxnet3_rx_ctx *ctx,  struct vmxnet3_adapter *adapter)\n{\n\trq->stats.drop_err++;\n\tif (!rcd->fcs)\n\t\trq->stats.drop_fcs++;\n\n\trq->stats.drop_total++;\n\n\t/*\n\t * We do not unmap and chain the rx buffer to the skb.\n\t * We basically pretend this buffer is not used and will be recycled\n\t * by vmxnet3_rq_alloc_rx_buf()\n\t */\n\n\t/*\n\t * ctx->skb may be NULL if this is the first and the only one\n\t * desc for the pkt\n\t */\n\tif (ctx->skb)\n\t\tdev_kfree_skb_irq(ctx->skb);\n\n\tctx->skb = NULL;\n}\n\n\nstatic u32\nvmxnet3_get_hdr_len(struct vmxnet3_adapter *adapter, struct sk_buff *skb,\n\t\t    union Vmxnet3_GenericDesc *gdesc)\n{\n\tu32 hlen, maplen;\n\tunion {\n\t\tvoid *ptr;\n\t\tstruct ethhdr *eth;\n\t\tstruct vlan_ethhdr *veth;\n\t\tstruct iphdr *ipv4;\n\t\tstruct ipv6hdr *ipv6;\n\t\tstruct tcphdr *tcp;\n\t} hdr;\n\tBUG_ON(gdesc->rcd.tcp == 0);\n\n\tmaplen = skb_headlen(skb);\n\tif (unlikely(sizeof(struct iphdr) + sizeof(struct tcphdr) > maplen))\n\t\treturn 0;\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_8021Q) ||\n\t    skb->protocol == cpu_to_be16(ETH_P_8021AD))\n\t\thlen = sizeof(struct vlan_ethhdr);\n\telse\n\t\thlen = sizeof(struct ethhdr);\n\n\thdr.eth = eth_hdr(skb);\n\tif (gdesc->rcd.v4) {\n\t\tBUG_ON(hdr.eth->h_proto != htons(ETH_P_IP) &&\n\t\t       hdr.veth->h_vlan_encapsulated_proto != htons(ETH_P_IP));\n\t\thdr.ptr += hlen;\n\t\tBUG_ON(hdr.ipv4->protocol != IPPROTO_TCP);\n\t\thlen = hdr.ipv4->ihl << 2;\n\t\thdr.ptr += hdr.ipv4->ihl << 2;\n\t} else if (gdesc->rcd.v6) {\n\t\tBUG_ON(hdr.eth->h_proto != htons(ETH_P_IPV6) &&\n\t\t       hdr.veth->h_vlan_encapsulated_proto != htons(ETH_P_IPV6));\n\t\thdr.ptr += hlen;\n\t\t/* Use an estimated value, since we also need to handle\n\t\t * TSO case.\n\t\t */\n\t\tif (hdr.ipv6->nexthdr != IPPROTO_TCP)\n\t\t\treturn sizeof(struct ipv6hdr) + sizeof(struct tcphdr);\n\t\thlen = sizeof(struct ipv6hdr);\n\t\thdr.ptr += sizeof(struct ipv6hdr);\n\t} else {\n\t\t/* Non-IP pkt, dont estimate header length */\n\t\treturn 0;\n\t}\n\n\tif (hlen + sizeof(struct tcphdr) > maplen)\n\t\treturn 0;\n\n\treturn (hlen + (hdr.tcp->doff << 2));\n}\n\nstatic int\nvmxnet3_rq_rx_complete(struct vmxnet3_rx_queue *rq,\n\t\t       struct vmxnet3_adapter *adapter, int quota)\n{\n\tstatic const u32 rxprod_reg[2] = {\n\t\tVMXNET3_REG_RXPROD, VMXNET3_REG_RXPROD2\n\t};\n\tu32 num_pkts = 0;\n\tbool skip_page_frags = false;\n\tstruct Vmxnet3_RxCompDesc *rcd;\n\tstruct vmxnet3_rx_ctx *ctx = &rq->rx_ctx;\n\tu16 segCnt = 0, mss = 0;\n#ifdef __BIG_ENDIAN_BITFIELD\n\tstruct Vmxnet3_RxDesc rxCmdDesc;\n\tstruct Vmxnet3_RxCompDesc rxComp;\n#endif\n\tvmxnet3_getRxComp(rcd, &rq->comp_ring.base[rq->comp_ring.next2proc].rcd,\n\t\t\t  &rxComp);\n\twhile (rcd->gen == rq->comp_ring.gen) {\n\t\tstruct vmxnet3_rx_buf_info *rbi;\n\t\tstruct sk_buff *skb, *new_skb = NULL;\n\t\tstruct page *new_page = NULL;\n\t\tdma_addr_t new_dma_addr;\n\t\tint num_to_alloc;\n\t\tstruct Vmxnet3_RxDesc *rxd;\n\t\tu32 idx, ring_idx;\n\t\tstruct vmxnet3_cmd_ring\t*ring = NULL;\n\t\tif (num_pkts >= quota) {\n\t\t\t/* we may stop even before we see the EOP desc of\n\t\t\t * the current pkt\n\t\t\t */\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Prevent any rcd field from being (speculatively) read before\n\t\t * rcd->gen is read.\n\t\t */\n\t\tdma_rmb();\n\n\t\tBUG_ON(rcd->rqID != rq->qid && rcd->rqID != rq->qid2 &&\n\t\t       rcd->rqID != rq->dataRingQid);\n\t\tidx = rcd->rxdIdx;\n\t\tring_idx = VMXNET3_GET_RING_IDX(adapter, rcd->rqID);\n\t\tring = rq->rx_ring + ring_idx;\n\t\tvmxnet3_getRxDesc(rxd, &rq->rx_ring[ring_idx].base[idx].rxd,\n\t\t\t\t  &rxCmdDesc);\n\t\trbi = rq->buf_info[ring_idx] + idx;\n\n\t\tBUG_ON(rxd->addr != rbi->dma_addr ||\n\t\t       rxd->len != rbi->len);\n\n\t\tif (unlikely(rcd->eop && rcd->err)) {\n\t\t\tvmxnet3_rx_error(rq, rcd, ctx, adapter);\n\t\t\tgoto rcd_done;\n\t\t}\n\n\t\tif (rcd->sop) { /* first buf of the pkt */\n\t\t\tbool rxDataRingUsed;\n\t\t\tu16 len;\n\n\t\t\tBUG_ON(rxd->btype != VMXNET3_RXD_BTYPE_HEAD ||\n\t\t\t       (rcd->rqID != rq->qid &&\n\t\t\t\trcd->rqID != rq->dataRingQid));\n\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_SKB);\n\t\t\tBUG_ON(ctx->skb != NULL || rbi->skb == NULL);\n\n\t\t\tif (unlikely(rcd->len == 0)) {\n\t\t\t\t/* Pretend the rx buffer is skipped. */\n\t\t\t\tBUG_ON(!(rcd->sop && rcd->eop));\n\t\t\t\tnetdev_dbg(adapter->netdev,\n\t\t\t\t\t\"rxRing[%u][%u] 0 length\\n\",\n\t\t\t\t\tring_idx, idx);\n\t\t\t\tgoto rcd_done;\n\t\t\t}\n\n\t\t\tskip_page_frags = false;\n\t\t\tctx->skb = rbi->skb;\n\n\t\t\trxDataRingUsed =\n\t\t\t\tVMXNET3_RX_DATA_RING(adapter, rcd->rqID);\n\t\t\tlen = rxDataRingUsed ? rcd->len : rbi->len;\n\t\t\tnew_skb = netdev_alloc_skb_ip_align(adapter->netdev,\n\t\t\t\t\t\t\t    len);\n\t\t\tif (new_skb == NULL) {\n\t\t\t\t/* Skb allocation failed, do not handover this\n\t\t\t\t * skb to stack. Reuse it. Drop the existing pkt\n\t\t\t\t */\n\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\tctx->skb = NULL;\n\t\t\t\trq->stats.drop_total++;\n\t\t\t\tskip_page_frags = true;\n\t\t\t\tgoto rcd_done;\n\t\t\t}\n\n\t\t\tif (rxDataRingUsed) {\n\t\t\t\tsize_t sz;\n\n\t\t\t\tBUG_ON(rcd->len > rq->data_ring.desc_size);\n\n\t\t\t\tctx->skb = new_skb;\n\t\t\t\tsz = rcd->rxdIdx * rq->data_ring.desc_size;\n\t\t\t\tmemcpy(new_skb->data,\n\t\t\t\t       &rq->data_ring.base[sz], rcd->len);\n\t\t\t} else {\n\t\t\t\tctx->skb = rbi->skb;\n\n\t\t\t\tnew_dma_addr =\n\t\t\t\t\tdma_map_single(&adapter->pdev->dev,\n\t\t\t\t\t\t       new_skb->data, rbi->len,\n\t\t\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      new_dma_addr)) {\n\t\t\t\t\tdev_kfree_skb(new_skb);\n\t\t\t\t\t/* Skb allocation failed, do not\n\t\t\t\t\t * handover this skb to stack. Reuse\n\t\t\t\t\t * it. Drop the existing pkt.\n\t\t\t\t\t */\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\trq->stats.drop_total++;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\n\t\t\t\tdma_unmap_single(&adapter->pdev->dev,\n\t\t\t\t\t\t rbi->dma_addr,\n\t\t\t\t\t\t rbi->len,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\t\t\t/* Immediate refill */\n\t\t\t\trbi->skb = new_skb;\n\t\t\t\trbi->dma_addr = new_dma_addr;\n\t\t\t\trxd->addr = cpu_to_le64(rbi->dma_addr);\n\t\t\t\trxd->len = rbi->len;\n\t\t\t}\n\n#ifdef VMXNET3_RSS\n\t\t\tif (rcd->rssType != VMXNET3_RCD_RSS_TYPE_NONE &&\n\t\t\t    (adapter->netdev->features & NETIF_F_RXHASH)) {\n\t\t\t\tenum pkt_hash_types hash_type;\n\n\t\t\t\tswitch (rcd->rssType) {\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_IPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_IPV6:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\t\t\tbreak;\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_TCPIPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_TCPIPV6:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_UDPIPV4:\n\t\t\t\tcase VMXNET3_RCD_RSS_TYPE_UDPIPV6:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L4;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\thash_type = PKT_HASH_TYPE_L3;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tskb_set_hash(ctx->skb,\n\t\t\t\t\t     le32_to_cpu(rcd->rssHash),\n\t\t\t\t\t     hash_type);\n\t\t\t}\n#endif\n\t\t\tskb_put(ctx->skb, rcd->len);\n\n\t\t\tif (VMXNET3_VERSION_GE_2(adapter) &&\n\t\t\t    rcd->type == VMXNET3_CDTYPE_RXCOMP_LRO) {\n\t\t\t\tstruct Vmxnet3_RxCompDescExt *rcdlro;\n\t\t\t\trcdlro = (struct Vmxnet3_RxCompDescExt *)rcd;\n\n\t\t\t\tsegCnt = rcdlro->segCnt;\n\t\t\t\tWARN_ON_ONCE(segCnt == 0);\n\t\t\t\tmss = rcdlro->mss;\n\t\t\t\tif (unlikely(segCnt <= 1))\n\t\t\t\t\tsegCnt = 0;\n\t\t\t} else {\n\t\t\t\tsegCnt = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tBUG_ON(ctx->skb == NULL && !skip_page_frags);\n\n\t\t\t/* non SOP buffer must be type 1 in most cases */\n\t\t\tBUG_ON(rbi->buf_type != VMXNET3_RX_BUF_PAGE);\n\t\t\tBUG_ON(rxd->btype != VMXNET3_RXD_BTYPE_BODY);\n\n\t\t\t/* If an sop buffer was dropped, skip all\n\t\t\t * following non-sop fragments. They will be reused.\n\t\t\t */\n\t\t\tif (skip_page_frags)\n\t\t\t\tgoto rcd_done;\n\n\t\t\tif (rcd->len) {\n\t\t\t\tnew_page = alloc_page(GFP_ATOMIC);\n\t\t\t\t/* Replacement page frag could not be allocated.\n\t\t\t\t * Reuse this page. Drop the pkt and free the\n\t\t\t\t * skb which contained this page as a frag. Skip\n\t\t\t\t * processing all the following non-sop frags.\n\t\t\t\t */\n\t\t\t\tif (unlikely(!new_page)) {\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tdev_kfree_skb(ctx->skb);\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\t\t\t\tnew_dma_addr = dma_map_page(&adapter->pdev->dev,\n\t\t\t\t\t\t\t    new_page,\n\t\t\t\t\t\t\t    0, PAGE_SIZE,\n\t\t\t\t\t\t\t    DMA_FROM_DEVICE);\n\t\t\t\tif (dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t      new_dma_addr)) {\n\t\t\t\t\tput_page(new_page);\n\t\t\t\t\trq->stats.rx_buf_alloc_failure++;\n\t\t\t\t\tdev_kfree_skb(ctx->skb);\n\t\t\t\t\tctx->skb = NULL;\n\t\t\t\t\tskip_page_frags = true;\n\t\t\t\t\tgoto rcd_done;\n\t\t\t\t}\n\n\t\t\t\tdma_unmap_page(&adapter->pdev->dev,\n\t\t\t\t\t       rbi->dma_addr, rbi->len,\n\t\t\t\t\t       DMA_FROM_DEVICE);\n\n\t\t\t\tvmxnet3_append_frag(ctx->skb, rcd, rbi);\n\n\t\t\t\t/* Immediate refill */\n\t\t\t\trbi->page = new_page;\n\t\t\t\trbi->dma_addr = new_dma_addr;\n\t\t\t\trxd->addr = cpu_to_le64(rbi->dma_addr);\n\t\t\t\trxd->len = rbi->len;\n\t\t\t}\n\t\t}\n\n\n\t\tskb = ctx->skb;\n\t\tif (rcd->eop) {\n\t\t\tu32 mtu = adapter->netdev->mtu;\n\t\t\tskb->len += skb->data_len;\n\n\t\t\tvmxnet3_rx_csum(adapter, skb,\n\t\t\t\t\t(union Vmxnet3_GenericDesc *)rcd);\n\t\t\tskb->protocol = eth_type_trans(skb, adapter->netdev);\n\t\t\tif (!rcd->tcp ||\n\t\t\t    !(adapter->netdev->features & NETIF_F_LRO))\n\t\t\t\tgoto not_lro;\n\n\t\t\tif (segCnt != 0 && mss != 0) {\n\t\t\t\tskb_shinfo(skb)->gso_type = rcd->v4 ?\n\t\t\t\t\tSKB_GSO_TCPV4 : SKB_GSO_TCPV6;\n\t\t\t\tskb_shinfo(skb)->gso_size = mss;\n\t\t\t\tskb_shinfo(skb)->gso_segs = segCnt;\n\t\t\t} else if (segCnt != 0 || skb->len > mtu) {\n\t\t\t\tu32 hlen;\n\n\t\t\t\thlen = vmxnet3_get_hdr_len(adapter, skb,\n\t\t\t\t\t(union Vmxnet3_GenericDesc *)rcd);\n\t\t\t\tif (hlen == 0)\n\t\t\t\t\tgoto not_lro;\n\n\t\t\t\tskb_shinfo(skb)->gso_type =\n\t\t\t\t\trcd->v4 ? SKB_GSO_TCPV4 : SKB_GSO_TCPV6;\n\t\t\t\tif (segCnt != 0) {\n\t\t\t\t\tskb_shinfo(skb)->gso_segs = segCnt;\n\t\t\t\t\tskb_shinfo(skb)->gso_size =\n\t\t\t\t\t\tDIV_ROUND_UP(skb->len -\n\t\t\t\t\t\t\thlen, segCnt);\n\t\t\t\t} else {\n\t\t\t\t\tskb_shinfo(skb)->gso_size = mtu - hlen;\n\t\t\t\t}\n\t\t\t}\nnot_lro:\n\t\t\tif (unlikely(rcd->ts))\n\t\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), rcd->tci);\n\n\t\t\tif (adapter->netdev->features & NETIF_F_LRO)\n\t\t\t\tnetif_receive_skb(skb);\n\t\t\telse\n\t\t\t\tnapi_gro_receive(&rq->napi, skb);\n\n\t\t\tctx->skb = NULL;\n\t\t\tnum_pkts++;\n\t\t}\n\nrcd_done:\n\t\t/* device may have skipped some rx descs */\n\t\tring->next2comp = idx;\n\t\tnum_to_alloc = vmxnet3_cmd_ring_desc_avail(ring);\n\t\tring = rq->rx_ring + ring_idx;\n\n\t\t/* Ensure that the writes to rxd->gen bits will be observed\n\t\t * after all other writes to rxd objects.\n\t\t */\n\t\tdma_wmb();\n\n\t\twhile (num_to_alloc) {\n\t\t\tvmxnet3_getRxDesc(rxd, &ring->base[ring->next2fill].rxd,\n\t\t\t\t\t  &rxCmdDesc);\n\t\t\tBUG_ON(!rxd->addr);\n\n\t\t\t/* Recv desc is ready to be used by the device */\n\t\t\trxd->gen = ring->gen;\n\t\t\tvmxnet3_cmd_ring_adv_next2fill(ring);\n\t\t\tnum_to_alloc--;\n\t\t}\n\n\t\t/* if needed, update the register */\n\t\tif (unlikely(rq->shared->updateRxProd)) {\n\t\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\t\t       rxprod_reg[ring_idx] + rq->qid * 8,\n\t\t\t\t\t       ring->next2fill);\n\t\t}\n\n\t\tvmxnet3_comp_ring_adv_next2proc(&rq->comp_ring);\n\t\tvmxnet3_getRxComp(rcd,\n\t\t\t\t  &rq->comp_ring.base[rq->comp_ring.next2proc].rcd, &rxComp);\n\t}\n\n\treturn num_pkts;\n}\n\n\nstatic void\nvmxnet3_rq_cleanup(struct vmxnet3_rx_queue *rq,\n\t\t   struct vmxnet3_adapter *adapter)\n{\n\tu32 i, ring_idx;\n\tstruct Vmxnet3_RxDesc *rxd;\n\n\tfor (ring_idx = 0; ring_idx < 2; ring_idx++) {\n\t\tfor (i = 0; i < rq->rx_ring[ring_idx].size; i++) {\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t\tstruct Vmxnet3_RxDesc rxDesc;\n#endif\n\t\t\tvmxnet3_getRxDesc(rxd,\n\t\t\t\t&rq->rx_ring[ring_idx].base[i].rxd, &rxDesc);\n\n\t\t\tif (rxd->btype == VMXNET3_RXD_BTYPE_HEAD &&\n\t\t\t\t\trq->buf_info[ring_idx][i].skb) {\n\t\t\t\tdma_unmap_single(&adapter->pdev->dev, rxd->addr,\n\t\t\t\t\t\t rxd->len, DMA_FROM_DEVICE);\n\t\t\t\tdev_kfree_skb(rq->buf_info[ring_idx][i].skb);\n\t\t\t\trq->buf_info[ring_idx][i].skb = NULL;\n\t\t\t} else if (rxd->btype == VMXNET3_RXD_BTYPE_BODY &&\n\t\t\t\t\trq->buf_info[ring_idx][i].page) {\n\t\t\t\tdma_unmap_page(&adapter->pdev->dev, rxd->addr,\n\t\t\t\t\t       rxd->len, DMA_FROM_DEVICE);\n\t\t\t\tput_page(rq->buf_info[ring_idx][i].page);\n\t\t\t\trq->buf_info[ring_idx][i].page = NULL;\n\t\t\t}\n\t\t}\n\n\t\trq->rx_ring[ring_idx].gen = VMXNET3_INIT_GEN;\n\t\trq->rx_ring[ring_idx].next2fill =\n\t\t\t\t\trq->rx_ring[ring_idx].next2comp = 0;\n\t}\n\n\trq->comp_ring.gen = VMXNET3_INIT_GEN;\n\trq->comp_ring.next2proc = 0;\n}\n\n\nstatic void\nvmxnet3_rq_cleanup_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tvmxnet3_rq_cleanup(&adapter->rx_queue[i], adapter);\n}\n\n\nstatic void vmxnet3_rq_destroy(struct vmxnet3_rx_queue *rq,\n\t\t\t       struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tint j;\n\n\t/* all rx buffers must have already been freed */\n\tfor (i = 0; i < 2; i++) {\n\t\tif (rq->buf_info[i]) {\n\t\t\tfor (j = 0; j < rq->rx_ring[i].size; j++)\n\t\t\t\tBUG_ON(rq->buf_info[i][j].page != NULL);\n\t\t}\n\t}\n\n\n\tfor (i = 0; i < 2; i++) {\n\t\tif (rq->rx_ring[i].base) {\n\t\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t\t  rq->rx_ring[i].size\n\t\t\t\t\t  * sizeof(struct Vmxnet3_RxDesc),\n\t\t\t\t\t  rq->rx_ring[i].base,\n\t\t\t\t\t  rq->rx_ring[i].basePA);\n\t\t\trq->rx_ring[i].base = NULL;\n\t\t}\n\t}\n\n\tif (rq->data_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  rq->rx_ring[0].size * rq->data_ring.desc_size,\n\t\t\t\t  rq->data_ring.base, rq->data_ring.basePA);\n\t\trq->data_ring.base = NULL;\n\t}\n\n\tif (rq->comp_ring.base) {\n\t\tdma_free_coherent(&adapter->pdev->dev, rq->comp_ring.size\n\t\t\t\t  * sizeof(struct Vmxnet3_RxCompDesc),\n\t\t\t\t  rq->comp_ring.base, rq->comp_ring.basePA);\n\t\trq->comp_ring.base = NULL;\n\t}\n\n\tkfree(rq->buf_info[0]);\n\trq->buf_info[0] = NULL;\n\trq->buf_info[1] = NULL;\n}\n\nstatic void\nvmxnet3_rq_destroy_all_rxdataring(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\n\t\tif (rq->data_ring.base) {\n\t\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t\t  (rq->rx_ring[0].size *\n\t\t\t\t\t  rq->data_ring.desc_size),\n\t\t\t\t\t  rq->data_ring.base,\n\t\t\t\t\t  rq->data_ring.basePA);\n\t\t\trq->data_ring.base = NULL;\n\t\t\trq->data_ring.desc_size = 0;\n\t\t}\n\t}\n}\n\nstatic int\nvmxnet3_rq_init(struct vmxnet3_rx_queue *rq,\n\t\tstruct vmxnet3_adapter  *adapter)\n{\n\tint i;\n\n\t/* initialize buf_info */\n\tfor (i = 0; i < rq->rx_ring[0].size; i++) {\n\n\t\t/* 1st buf for a pkt is skbuff */\n\t\tif (i % adapter->rx_buf_per_pkt == 0) {\n\t\t\trq->buf_info[0][i].buf_type = VMXNET3_RX_BUF_SKB;\n\t\t\trq->buf_info[0][i].len = adapter->skb_buf_size;\n\t\t} else { /* subsequent bufs for a pkt is frag */\n\t\t\trq->buf_info[0][i].buf_type = VMXNET3_RX_BUF_PAGE;\n\t\t\trq->buf_info[0][i].len = PAGE_SIZE;\n\t\t}\n\t}\n\tfor (i = 0; i < rq->rx_ring[1].size; i++) {\n\t\trq->buf_info[1][i].buf_type = VMXNET3_RX_BUF_PAGE;\n\t\trq->buf_info[1][i].len = PAGE_SIZE;\n\t}\n\n\t/* reset internal state and allocate buffers for both rings */\n\tfor (i = 0; i < 2; i++) {\n\t\trq->rx_ring[i].next2fill = rq->rx_ring[i].next2comp = 0;\n\n\t\tmemset(rq->rx_ring[i].base, 0, rq->rx_ring[i].size *\n\t\t       sizeof(struct Vmxnet3_RxDesc));\n\t\trq->rx_ring[i].gen = VMXNET3_INIT_GEN;\n\t}\n\tif (vmxnet3_rq_alloc_rx_buf(rq, 0, rq->rx_ring[0].size - 1,\n\t\t\t\t    adapter) == 0) {\n\t\t/* at least has 1 rx buffer for the 1st ring */\n\t\treturn -ENOMEM;\n\t}\n\tvmxnet3_rq_alloc_rx_buf(rq, 1, rq->rx_ring[1].size - 1, adapter);\n\n\t/* reset the comp ring */\n\trq->comp_ring.next2proc = 0;\n\tmemset(rq->comp_ring.base, 0, rq->comp_ring.size *\n\t       sizeof(struct Vmxnet3_RxCompDesc));\n\trq->comp_ring.gen = VMXNET3_INIT_GEN;\n\n\t/* reset rxctx */\n\trq->rx_ctx.skb = NULL;\n\n\t/* stats are not reset */\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_rq_init_all(struct vmxnet3_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = vmxnet3_rq_init(&adapter->rx_queue[i], adapter);\n\t\tif (unlikely(err)) {\n\t\t\tdev_err(&adapter->netdev->dev, \"%s: failed to \"\n\t\t\t\t\"initialize rx queue%i\\n\",\n\t\t\t\tadapter->netdev->name, i);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n\n}\n\n\nstatic int\nvmxnet3_rq_create(struct vmxnet3_rx_queue *rq, struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tsize_t sz;\n\tstruct vmxnet3_rx_buf_info *bi;\n\n\tfor (i = 0; i < 2; i++) {\n\n\t\tsz = rq->rx_ring[i].size * sizeof(struct Vmxnet3_RxDesc);\n\t\trq->rx_ring[i].base = dma_alloc_coherent(\n\t\t\t\t\t\t&adapter->pdev->dev, sz,\n\t\t\t\t\t\t&rq->rx_ring[i].basePA,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!rq->rx_ring[i].base) {\n\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t   \"failed to allocate rx ring %d\\n\", i);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tif ((adapter->rxdataring_enabled) && (rq->data_ring.desc_size != 0)) {\n\t\tsz = rq->rx_ring[0].size * rq->data_ring.desc_size;\n\t\trq->data_ring.base =\n\t\t\tdma_alloc_coherent(&adapter->pdev->dev, sz,\n\t\t\t\t\t   &rq->data_ring.basePA,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!rq->data_ring.base) {\n\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t   \"rx data ring will be disabled\\n\");\n\t\t\tadapter->rxdataring_enabled = false;\n\t\t}\n\t} else {\n\t\trq->data_ring.base = NULL;\n\t\trq->data_ring.desc_size = 0;\n\t}\n\n\tsz = rq->comp_ring.size * sizeof(struct Vmxnet3_RxCompDesc);\n\trq->comp_ring.base = dma_alloc_coherent(&adapter->pdev->dev, sz,\n\t\t\t\t\t\t&rq->comp_ring.basePA,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!rq->comp_ring.base) {\n\t\tnetdev_err(adapter->netdev, \"failed to allocate rx comp ring\\n\");\n\t\tgoto err;\n\t}\n\n\tbi = kcalloc_node(rq->rx_ring[0].size + rq->rx_ring[1].size,\n\t\t\t  sizeof(rq->buf_info[0][0]), GFP_KERNEL,\n\t\t\t  dev_to_node(&adapter->pdev->dev));\n\tif (!bi)\n\t\tgoto err;\n\n\trq->buf_info[0] = bi;\n\trq->buf_info[1] = bi + rq->rx_ring[0].size;\n\n\treturn 0;\n\nerr:\n\tvmxnet3_rq_destroy(rq, adapter);\n\treturn -ENOMEM;\n}\n\n\nstatic int\nvmxnet3_rq_create_all(struct vmxnet3_adapter *adapter)\n{\n\tint i, err = 0;\n\n\tadapter->rxdataring_enabled = VMXNET3_VERSION_GE_3(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\terr = vmxnet3_rq_create(&adapter->rx_queue[i], adapter);\n\t\tif (unlikely(err)) {\n\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\"%s: failed to create rx queue%i\\n\",\n\t\t\t\tadapter->netdev->name, i);\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tif (!adapter->rxdataring_enabled)\n\t\tvmxnet3_rq_destroy_all_rxdataring(adapter);\n\n\treturn err;\nerr_out:\n\tvmxnet3_rq_destroy_all(adapter);\n\treturn err;\n\n}\n\n/* Multiple queue aware polling function for tx and rx */\n\nstatic int\nvmxnet3_do_poll(struct vmxnet3_adapter *adapter, int budget)\n{\n\tint rcd_done = 0, i;\n\tif (unlikely(adapter->shared->ecr))\n\t\tvmxnet3_process_events(adapter);\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tvmxnet3_tq_tx_complete(&adapter->tx_queue[i], adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\trcd_done += vmxnet3_rq_rx_complete(&adapter->rx_queue[i],\n\t\t\t\t\t\t   adapter, budget);\n\treturn rcd_done;\n}\n\n\nstatic int\nvmxnet3_poll(struct napi_struct *napi, int budget)\n{\n\tstruct vmxnet3_rx_queue *rx_queue = container_of(napi,\n\t\t\t\t\t  struct vmxnet3_rx_queue, napi);\n\tint rxd_done;\n\n\trxd_done = vmxnet3_do_poll(rx_queue->adapter, budget);\n\n\tif (rxd_done < budget) {\n\t\tnapi_complete_done(napi, rxd_done);\n\t\tvmxnet3_enable_all_intrs(rx_queue->adapter);\n\t}\n\treturn rxd_done;\n}\n\n/*\n * NAPI polling function for MSI-X mode with multiple Rx queues\n * Returns the # of the NAPI credit consumed (# of rx descriptors processed)\n */\n\nstatic int\nvmxnet3_poll_rx_only(struct napi_struct *napi, int budget)\n{\n\tstruct vmxnet3_rx_queue *rq = container_of(napi,\n\t\t\t\t\t\tstruct vmxnet3_rx_queue, napi);\n\tstruct vmxnet3_adapter *adapter = rq->adapter;\n\tint rxd_done;\n\n\t/* When sharing interrupt with corresponding tx queue, process\n\t * tx completions in that queue as well\n\t */\n\tif (adapter->share_intr == VMXNET3_INTR_BUDDYSHARE) {\n\t\tstruct vmxnet3_tx_queue *tq =\n\t\t\t\t&adapter->tx_queue[rq - adapter->rx_queue];\n\t\tvmxnet3_tq_tx_complete(tq, adapter);\n\t}\n\n\trxd_done = vmxnet3_rq_rx_complete(rq, adapter, budget);\n\n\tif (rxd_done < budget) {\n\t\tnapi_complete_done(napi, rxd_done);\n\t\tvmxnet3_enable_intr(adapter, rq->comp_ring.intr_idx);\n\t}\n\treturn rxd_done;\n}\n\n\n#ifdef CONFIG_PCI_MSI\n\n/*\n * Handle completion interrupts on tx queues\n * Returns whether or not the intr is handled\n */\n\nstatic irqreturn_t\nvmxnet3_msix_tx(int irq, void *data)\n{\n\tstruct vmxnet3_tx_queue *tq = data;\n\tstruct vmxnet3_adapter *adapter = tq->adapter;\n\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, tq->comp_ring.intr_idx);\n\n\t/* Handle the case where only one irq is allocate for all tx queues */\n\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE) {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tstruct vmxnet3_tx_queue *txq = &adapter->tx_queue[i];\n\t\t\tvmxnet3_tq_tx_complete(txq, adapter);\n\t\t}\n\t} else {\n\t\tvmxnet3_tq_tx_complete(tq, adapter);\n\t}\n\tvmxnet3_enable_intr(adapter, tq->comp_ring.intr_idx);\n\n\treturn IRQ_HANDLED;\n}\n\n\n/*\n * Handle completion interrupts on rx queues. Returns whether or not the\n * intr is handled\n */\n\nstatic irqreturn_t\nvmxnet3_msix_rx(int irq, void *data)\n{\n\tstruct vmxnet3_rx_queue *rq = data;\n\tstruct vmxnet3_adapter *adapter = rq->adapter;\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, rq->comp_ring.intr_idx);\n\tnapi_schedule(&rq->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/*\n *----------------------------------------------------------------------------\n *\n * vmxnet3_msix_event --\n *\n *    vmxnet3 msix event intr handler\n *\n * Result:\n *    whether or not the intr is handled\n *\n *----------------------------------------------------------------------------\n */\n\nstatic irqreturn_t\nvmxnet3_msix_event(int irq, void *data)\n{\n\tstruct net_device *dev = data;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(dev);\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_intr(adapter, adapter->intr.event_intr_idx);\n\n\tif (adapter->shared->ecr)\n\t\tvmxnet3_process_events(adapter);\n\n\tvmxnet3_enable_intr(adapter, adapter->intr.event_intr_idx);\n\n\treturn IRQ_HANDLED;\n}\n\n#endif /* CONFIG_PCI_MSI  */\n\n\n/* Interrupt handler for vmxnet3  */\nstatic irqreturn_t\nvmxnet3_intr(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(dev);\n\n\tif (adapter->intr.type == VMXNET3_IT_INTX) {\n\t\tu32 icr = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_ICR);\n\t\tif (unlikely(icr == 0))\n\t\t\t/* not ours */\n\t\t\treturn IRQ_NONE;\n\t}\n\n\n\t/* disable intr if needed */\n\tif (adapter->intr.mask_mode == VMXNET3_IMM_ACTIVE)\n\t\tvmxnet3_disable_all_intrs(adapter);\n\n\tnapi_schedule(&adapter->rx_queue[0].napi);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\n/* netpoll callback. */\nstatic void\nvmxnet3_netpoll(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tswitch (adapter->intr.type) {\n#ifdef CONFIG_PCI_MSI\n\tcase VMXNET3_IT_MSIX: {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\t\tvmxnet3_msix_rx(0, &adapter->rx_queue[i]);\n\t\tbreak;\n\t}\n#endif\n\tcase VMXNET3_IT_MSI:\n\tdefault:\n\t\tvmxnet3_intr(0, adapter->netdev);\n\t\tbreak;\n\t}\n\n}\n#endif\t/* CONFIG_NET_POLL_CONTROLLER */\n\nstatic int\nvmxnet3_request_irqs(struct vmxnet3_adapter *adapter)\n{\n\tstruct vmxnet3_intr *intr = &adapter->intr;\n\tint err = 0, i;\n\tint vector = 0;\n\n#ifdef CONFIG_PCI_MSI\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE) {\n\t\t\t\tsprintf(adapter->tx_queue[i].name, \"%s-tx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\t\terr = request_irq(\n\t\t\t\t\t      intr->msix_entries[vector].vector,\n\t\t\t\t\t      vmxnet3_msix_tx, 0,\n\t\t\t\t\t      adapter->tx_queue[i].name,\n\t\t\t\t\t      &adapter->tx_queue[i]);\n\t\t\t} else {\n\t\t\t\tsprintf(adapter->tx_queue[i].name, \"%s-rxtx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\t}\n\t\t\tif (err) {\n\t\t\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\t\t\"Failed to request irq for MSIX, %s, \"\n\t\t\t\t\t\"error %d\\n\",\n\t\t\t\t\tadapter->tx_queue[i].name, err);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* Handle the case where only 1 MSIx was allocated for\n\t\t\t * all tx queues */\n\t\t\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE) {\n\t\t\t\tfor (; i < adapter->num_tx_queues; i++)\n\t\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx\n\t\t\t\t\t\t\t\t= vector;\n\t\t\t\tvector++;\n\t\t\t\tbreak;\n\t\t\t} else {\n\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx\n\t\t\t\t\t\t\t\t= vector++;\n\t\t\t}\n\t\t}\n\t\tif (adapter->share_intr == VMXNET3_INTR_BUDDYSHARE)\n\t\t\tvector = 0;\n\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE)\n\t\t\t\tsprintf(adapter->rx_queue[i].name, \"%s-rx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\telse\n\t\t\t\tsprintf(adapter->rx_queue[i].name, \"%s-rxtx-%d\",\n\t\t\t\t\tadapter->netdev->name, vector);\n\t\t\terr = request_irq(intr->msix_entries[vector].vector,\n\t\t\t\t\t  vmxnet3_msix_rx, 0,\n\t\t\t\t\t  adapter->rx_queue[i].name,\n\t\t\t\t\t  &(adapter->rx_queue[i]));\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Failed to request irq for MSIX, \"\n\t\t\t\t\t   \"%s, error %d\\n\",\n\t\t\t\t\t   adapter->rx_queue[i].name, err);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tadapter->rx_queue[i].comp_ring.intr_idx = vector++;\n\t\t}\n\n\t\tsprintf(intr->event_msi_vector_name, \"%s-event-%d\",\n\t\t\tadapter->netdev->name, vector);\n\t\terr = request_irq(intr->msix_entries[vector].vector,\n\t\t\t\t  vmxnet3_msix_event, 0,\n\t\t\t\t  intr->event_msi_vector_name, adapter->netdev);\n\t\tintr->event_intr_idx = vector;\n\n\t} else if (intr->type == VMXNET3_IT_MSI) {\n\t\tadapter->num_rx_queues = 1;\n\t\terr = request_irq(adapter->pdev->irq, vmxnet3_intr, 0,\n\t\t\t\t  adapter->netdev->name, adapter->netdev);\n\t} else {\n#endif\n\t\tadapter->num_rx_queues = 1;\n\t\terr = request_irq(adapter->pdev->irq, vmxnet3_intr,\n\t\t\t\t  IRQF_SHARED, adapter->netdev->name,\n\t\t\t\t  adapter->netdev);\n#ifdef CONFIG_PCI_MSI\n\t}\n#endif\n\tintr->num_intrs = vector + 1;\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to request irq (intr type:%d), error %d\\n\",\n\t\t\t   intr->type, err);\n\t} else {\n\t\t/* Number of rx queues will not change after this */\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\t\t\trq->qid = i;\n\t\t\trq->qid2 = i + adapter->num_rx_queues;\n\t\t\trq->dataRingQid = i + 2 * adapter->num_rx_queues;\n\t\t}\n\n\t\t/* init our intr settings */\n\t\tfor (i = 0; i < intr->num_intrs; i++)\n\t\t\tintr->mod_levels[i] = UPT1_IML_ADAPTIVE;\n\t\tif (adapter->intr.type != VMXNET3_IT_MSIX) {\n\t\t\tadapter->intr.event_intr_idx = 0;\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\t\t\tadapter->tx_queue[i].comp_ring.intr_idx = 0;\n\t\t\tadapter->rx_queue[0].comp_ring.intr_idx = 0;\n\t\t}\n\n\t\tnetdev_info(adapter->netdev,\n\t\t\t    \"intr type %u, mode %u, %u vectors allocated\\n\",\n\t\t\t    intr->type, intr->mask_mode, intr->num_intrs);\n\t}\n\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_free_irqs(struct vmxnet3_adapter *adapter)\n{\n\tstruct vmxnet3_intr *intr = &adapter->intr;\n\tBUG_ON(intr->type == VMXNET3_IT_AUTO || intr->num_intrs <= 0);\n\n\tswitch (intr->type) {\n#ifdef CONFIG_PCI_MSI\n\tcase VMXNET3_IT_MSIX:\n\t{\n\t\tint i, vector = 0;\n\n\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE) {\n\t\t\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\t\t\tfree_irq(intr->msix_entries[vector++].vector,\n\t\t\t\t\t &(adapter->tx_queue[i]));\n\t\t\t\tif (adapter->share_intr == VMXNET3_INTR_TXSHARE)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tfree_irq(intr->msix_entries[vector++].vector,\n\t\t\t\t &(adapter->rx_queue[i]));\n\t\t}\n\n\t\tfree_irq(intr->msix_entries[vector].vector,\n\t\t\t adapter->netdev);\n\t\tBUG_ON(vector >= intr->num_intrs);\n\t\tbreak;\n\t}\n#endif\n\tcase VMXNET3_IT_MSI:\n\t\tfree_irq(adapter->pdev->irq, adapter->netdev);\n\t\tbreak;\n\tcase VMXNET3_IT_INTX:\n\t\tfree_irq(adapter->pdev->irq, adapter->netdev);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n\nstatic void\nvmxnet3_restore_vlan(struct vmxnet3_adapter *adapter)\n{\n\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\tu16 vid;\n\n\t/* allow untagged pkts */\n\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, 0);\n\n\tfor_each_set_bit(vid, adapter->active_vlans, VLAN_N_VID)\n\t\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, vid);\n}\n\n\nstatic int\nvmxnet3_vlan_rx_add_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!(netdev->flags & IFF_PROMISC)) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tunsigned long flags;\n\n\t\tVMXNET3_SET_VFTABLE_ENTRY(vfTable, vid);\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t}\n\n\tset_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!(netdev->flags & IFF_PROMISC)) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tunsigned long flags;\n\n\t\tVMXNET3_CLEAR_VFTABLE_ENTRY(vfTable, vid);\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t}\n\n\tclear_bit(vid, adapter->active_vlans);\n\n\treturn 0;\n}\n\n\nstatic u8 *\nvmxnet3_copy_mc(struct net_device *netdev)\n{\n\tu8 *buf = NULL;\n\tu32 sz = netdev_mc_count(netdev) * ETH_ALEN;\n\n\t/* struct Vmxnet3_RxFilterConf.mfTableLen is u16. */\n\tif (sz <= 0xffff) {\n\t\t/* We may be called with BH disabled */\n\t\tbuf = kmalloc(sz, GFP_ATOMIC);\n\t\tif (buf) {\n\t\t\tstruct netdev_hw_addr *ha;\n\t\t\tint i = 0;\n\n\t\t\tnetdev_for_each_mc_addr(ha, netdev)\n\t\t\t\tmemcpy(buf + i++ * ETH_ALEN, ha->addr,\n\t\t\t\t       ETH_ALEN);\n\t\t}\n\t}\n\treturn buf;\n}\n\n\nstatic void\nvmxnet3_set_mc(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tunsigned long flags;\n\tstruct Vmxnet3_RxFilterConf *rxConf =\n\t\t\t\t\t&adapter->shared->devRead.rxFilterConf;\n\tu8 *new_table = NULL;\n\tdma_addr_t new_table_pa = 0;\n\tbool new_table_pa_valid = false;\n\tu32 new_mode = VMXNET3_RXM_UCAST;\n\n\tif (netdev->flags & IFF_PROMISC) {\n\t\tu32 *vfTable = adapter->shared->devRead.rxFilterConf.vfTable;\n\t\tmemset(vfTable, 0, VMXNET3_VFT_SIZE * sizeof(*vfTable));\n\n\t\tnew_mode |= VMXNET3_RXM_PROMISC;\n\t} else {\n\t\tvmxnet3_restore_vlan(adapter);\n\t}\n\n\tif (netdev->flags & IFF_BROADCAST)\n\t\tnew_mode |= VMXNET3_RXM_BCAST;\n\n\tif (netdev->flags & IFF_ALLMULTI)\n\t\tnew_mode |= VMXNET3_RXM_ALL_MULTI;\n\telse\n\t\tif (!netdev_mc_empty(netdev)) {\n\t\t\tnew_table = vmxnet3_copy_mc(netdev);\n\t\t\tif (new_table) {\n\t\t\t\tsize_t sz = netdev_mc_count(netdev) * ETH_ALEN;\n\n\t\t\t\trxConf->mfTableLen = cpu_to_le16(sz);\n\t\t\t\tnew_table_pa = dma_map_single(\n\t\t\t\t\t\t\t&adapter->pdev->dev,\n\t\t\t\t\t\t\tnew_table,\n\t\t\t\t\t\t\tsz,\n\t\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t\tif (!dma_mapping_error(&adapter->pdev->dev,\n\t\t\t\t\t\t       new_table_pa)) {\n\t\t\t\t\tnew_mode |= VMXNET3_RXM_MCAST;\n\t\t\t\t\tnew_table_pa_valid = true;\n\t\t\t\t\trxConf->mfTablePA = cpu_to_le64(\n\t\t\t\t\t\t\t\tnew_table_pa);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!new_table_pa_valid) {\n\t\t\t\tnetdev_info(netdev,\n\t\t\t\t\t    \"failed to copy mcast list, setting ALL_MULTI\\n\");\n\t\t\t\tnew_mode |= VMXNET3_RXM_ALL_MULTI;\n\t\t\t}\n\t\t}\n\n\tif (!(new_mode & VMXNET3_RXM_MCAST)) {\n\t\trxConf->mfTableLen = 0;\n\t\trxConf->mfTablePA = 0;\n\t}\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tif (new_mode != rxConf->rxMode) {\n\t\trxConf->rxMode = cpu_to_le32(new_mode);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_RX_MODE);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_UPDATE_VLAN_FILTERS);\n\t}\n\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_UPDATE_MAC_FILTERS);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tif (new_table_pa_valid)\n\t\tdma_unmap_single(&adapter->pdev->dev, new_table_pa,\n\t\t\t\t rxConf->mfTableLen, DMA_TO_DEVICE);\n\tkfree(new_table);\n}\n\nvoid\nvmxnet3_rq_destroy_all(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tvmxnet3_rq_destroy(&adapter->rx_queue[i], adapter);\n}\n\n\n/*\n *   Set up driver_shared based on settings in adapter.\n */\n\nstatic void\nvmxnet3_setup_driver_shared(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tstruct Vmxnet3_DSDevRead *devRead = &shared->devRead;\n\tstruct Vmxnet3_DSDevReadExt *devReadExt = &shared->devReadExt;\n\tstruct Vmxnet3_TxQueueConf *tqc;\n\tstruct Vmxnet3_RxQueueConf *rqc;\n\tint i;\n\n\tmemset(shared, 0, sizeof(*shared));\n\n\t/* driver settings */\n\tshared->magic = cpu_to_le32(VMXNET3_REV1_MAGIC);\n\tdevRead->misc.driverInfo.version = cpu_to_le32(\n\t\t\t\t\t\tVMXNET3_DRIVER_VERSION_NUM);\n\tdevRead->misc.driverInfo.gos.gosBits = (sizeof(void *) == 4 ?\n\t\t\t\tVMXNET3_GOS_BITS_32 : VMXNET3_GOS_BITS_64);\n\tdevRead->misc.driverInfo.gos.gosType = VMXNET3_GOS_TYPE_LINUX;\n\t*((u32 *)&devRead->misc.driverInfo.gos) = cpu_to_le32(\n\t\t\t\t*((u32 *)&devRead->misc.driverInfo.gos));\n\tdevRead->misc.driverInfo.vmxnet3RevSpt = cpu_to_le32(1);\n\tdevRead->misc.driverInfo.uptVerSpt = cpu_to_le32(1);\n\n\tdevRead->misc.ddPA = cpu_to_le64(adapter->adapter_pa);\n\tdevRead->misc.ddLen = cpu_to_le32(sizeof(struct vmxnet3_adapter));\n\n\t/* set up feature flags */\n\tif (adapter->netdev->features & NETIF_F_RXCSUM)\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXCSUM;\n\n\tif (adapter->netdev->features & NETIF_F_LRO) {\n\t\tdevRead->misc.uptFeatures |= UPT1_F_LRO;\n\t\tdevRead->misc.maxNumRxSG = cpu_to_le16(1 + MAX_SKB_FRAGS);\n\t}\n\tif (adapter->netdev->features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXVLAN;\n\n\tif (adapter->netdev->features & (NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\t\t NETIF_F_GSO_UDP_TUNNEL_CSUM))\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RXINNEROFLD;\n\n\tdevRead->misc.mtu = cpu_to_le32(adapter->netdev->mtu);\n\tdevRead->misc.queueDescPA = cpu_to_le64(adapter->queue_desc_pa);\n\tdevRead->misc.queueDescLen = cpu_to_le32(\n\t\tadapter->num_tx_queues * sizeof(struct Vmxnet3_TxQueueDesc) +\n\t\tadapter->num_rx_queues * sizeof(struct Vmxnet3_RxQueueDesc));\n\n\t/* tx queue settings */\n\tdevRead->misc.numTxQueues =  adapter->num_tx_queues;\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct vmxnet3_tx_queue\t*tq = &adapter->tx_queue[i];\n\t\tBUG_ON(adapter->tx_queue[i].tx_ring.base == NULL);\n\t\ttqc = &adapter->tqd_start[i].conf;\n\t\ttqc->txRingBasePA   = cpu_to_le64(tq->tx_ring.basePA);\n\t\ttqc->dataRingBasePA = cpu_to_le64(tq->data_ring.basePA);\n\t\ttqc->compRingBasePA = cpu_to_le64(tq->comp_ring.basePA);\n\t\ttqc->ddPA           = cpu_to_le64(~0ULL);\n\t\ttqc->txRingSize     = cpu_to_le32(tq->tx_ring.size);\n\t\ttqc->dataRingSize   = cpu_to_le32(tq->data_ring.size);\n\t\ttqc->txDataRingDescSize = cpu_to_le32(tq->txdata_desc_size);\n\t\ttqc->compRingSize   = cpu_to_le32(tq->comp_ring.size);\n\t\ttqc->ddLen          = cpu_to_le32(0);\n\t\ttqc->intrIdx        = tq->comp_ring.intr_idx;\n\t}\n\n\t/* rx queue settings */\n\tdevRead->misc.numRxQueues = adapter->num_rx_queues;\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue\t*rq = &adapter->rx_queue[i];\n\t\trqc = &adapter->rqd_start[i].conf;\n\t\trqc->rxRingBasePA[0] = cpu_to_le64(rq->rx_ring[0].basePA);\n\t\trqc->rxRingBasePA[1] = cpu_to_le64(rq->rx_ring[1].basePA);\n\t\trqc->compRingBasePA  = cpu_to_le64(rq->comp_ring.basePA);\n\t\trqc->ddPA            = cpu_to_le64(~0ULL);\n\t\trqc->rxRingSize[0]   = cpu_to_le32(rq->rx_ring[0].size);\n\t\trqc->rxRingSize[1]   = cpu_to_le32(rq->rx_ring[1].size);\n\t\trqc->compRingSize    = cpu_to_le32(rq->comp_ring.size);\n\t\trqc->ddLen           = cpu_to_le32(0);\n\t\trqc->intrIdx         = rq->comp_ring.intr_idx;\n\t\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\t\trqc->rxDataRingBasePA =\n\t\t\t\tcpu_to_le64(rq->data_ring.basePA);\n\t\t\trqc->rxDataRingDescSize =\n\t\t\t\tcpu_to_le16(rq->data_ring.desc_size);\n\t\t}\n\t}\n\n#ifdef VMXNET3_RSS\n\tmemset(adapter->rss_conf, 0, sizeof(*adapter->rss_conf));\n\n\tif (adapter->rss) {\n\t\tstruct UPT1_RSSConf *rssConf = adapter->rss_conf;\n\n\t\tdevRead->misc.uptFeatures |= UPT1_F_RSS;\n\t\tdevRead->misc.numRxQueues = adapter->num_rx_queues;\n\t\trssConf->hashType = UPT1_RSS_HASH_TYPE_TCP_IPV4 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_IPV4 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_TCP_IPV6 |\n\t\t\t\t    UPT1_RSS_HASH_TYPE_IPV6;\n\t\trssConf->hashFunc = UPT1_RSS_HASH_FUNC_TOEPLITZ;\n\t\trssConf->hashKeySize = UPT1_RSS_MAX_KEY_SIZE;\n\t\trssConf->indTableSize = VMXNET3_RSS_IND_TABLE_SIZE;\n\t\tnetdev_rss_key_fill(rssConf->hashKey, sizeof(rssConf->hashKey));\n\n\t\tfor (i = 0; i < rssConf->indTableSize; i++)\n\t\t\trssConf->indTable[i] = ethtool_rxfh_indir_default(\n\t\t\t\ti, adapter->num_rx_queues);\n\n\t\tdevRead->rssConfDesc.confVer = 1;\n\t\tdevRead->rssConfDesc.confLen = cpu_to_le32(sizeof(*rssConf));\n\t\tdevRead->rssConfDesc.confPA =\n\t\t\tcpu_to_le64(adapter->rss_conf_pa);\n\t}\n\n#endif /* VMXNET3_RSS */\n\n\t/* intr settings */\n\tif (!VMXNET3_VERSION_GE_6(adapter) ||\n\t    !adapter->queuesExtEnabled) {\n\t\tdevRead->intrConf.autoMask = adapter->intr.mask_mode ==\n\t\t\t\t\t     VMXNET3_IMM_AUTO;\n\t\tdevRead->intrConf.numIntrs = adapter->intr.num_intrs;\n\t\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\t\tdevRead->intrConf.modLevels[i] = adapter->intr.mod_levels[i];\n\n\t\tdevRead->intrConf.eventIntrIdx = adapter->intr.event_intr_idx;\n\t\tdevRead->intrConf.intrCtrl |= cpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\t} else {\n\t\tdevReadExt->intrConfExt.autoMask = adapter->intr.mask_mode ==\n\t\t\t\t\t\t   VMXNET3_IMM_AUTO;\n\t\tdevReadExt->intrConfExt.numIntrs = adapter->intr.num_intrs;\n\t\tfor (i = 0; i < adapter->intr.num_intrs; i++)\n\t\t\tdevReadExt->intrConfExt.modLevels[i] = adapter->intr.mod_levels[i];\n\n\t\tdevReadExt->intrConfExt.eventIntrIdx = adapter->intr.event_intr_idx;\n\t\tdevReadExt->intrConfExt.intrCtrl |= cpu_to_le32(VMXNET3_IC_DISABLE_ALL);\n\t}\n\n\t/* rx filter settings */\n\tdevRead->rxFilterConf.rxMode = 0;\n\tvmxnet3_restore_vlan(adapter);\n\tvmxnet3_write_mac_addr(adapter, adapter->netdev->dev_addr);\n\n\t/* the rest are already zeroed */\n}\n\nstatic void\nvmxnet3_init_coalesce(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tunion Vmxnet3_CmdInfo *cmdInfo = &shared->cu.cmdInfo;\n\tunsigned long flags;\n\n\tif (!VMXNET3_VERSION_GE_3(adapter))\n\t\treturn;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tcmdInfo->varConf.confVer = 1;\n\tcmdInfo->varConf.confLen =\n\t\tcpu_to_le32(sizeof(*adapter->coal_conf));\n\tcmdInfo->varConf.confPA  = cpu_to_le64(adapter->coal_conf_pa);\n\n\tif (adapter->default_coal_mode) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_COALESCE);\n\t} else {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_SET_COALESCE);\n\t}\n\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\nstatic void\nvmxnet3_init_rssfields(struct vmxnet3_adapter *adapter)\n{\n\tstruct Vmxnet3_DriverShared *shared = adapter->shared;\n\tunion Vmxnet3_CmdInfo *cmdInfo = &shared->cu.cmdInfo;\n\tunsigned long flags;\n\n\tif (!VMXNET3_VERSION_GE_4(adapter))\n\t\treturn;\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\n\tif (adapter->default_rss_fields) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_RSS_FIELDS);\n\t\tadapter->rss_fields =\n\t\t\tVMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t} else {\n\t\tcmdInfo->setRssFields = adapter->rss_fields;\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_SET_RSS_FIELDS);\n\t\t/* Not all requested RSS may get applied, so get and\n\t\t * cache what was actually applied.\n\t\t */\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_RSS_FIELDS);\n\t\tadapter->rss_fields =\n\t\t\tVMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t}\n\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\nint\nvmxnet3_activate_dev(struct vmxnet3_adapter *adapter)\n{\n\tint err, i;\n\tu32 ret;\n\tunsigned long flags;\n\n\tnetdev_dbg(adapter->netdev, \"%s: skb_buf_size %d, rx_buf_per_pkt %d,\"\n\t\t\" ring sizes %u %u %u\\n\", adapter->netdev->name,\n\t\tadapter->skb_buf_size, adapter->rx_buf_per_pkt,\n\t\tadapter->tx_queue[0].tx_ring.size,\n\t\tadapter->rx_queue[0].rx_ring[0].size,\n\t\tadapter->rx_queue[0].rx_ring[1].size);\n\n\tvmxnet3_tq_init_all(adapter);\n\terr = vmxnet3_rq_init_all(adapter);\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to init rx queue error %d\\n\", err);\n\t\tgoto rq_err;\n\t}\n\n\terr = vmxnet3_request_irqs(adapter);\n\tif (err) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to setup irq for error %d\\n\", err);\n\t\tgoto irq_err;\n\t}\n\n\tvmxnet3_setup_driver_shared(adapter);\n\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAL, VMXNET3_GET_ADDR_LO(\n\t\t\t       adapter->shared_pa));\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAH, VMXNET3_GET_ADDR_HI(\n\t\t\t       adapter->shared_pa));\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_ACTIVATE_DEV);\n\tret = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tif (ret != 0) {\n\t\tnetdev_err(adapter->netdev,\n\t\t\t   \"Failed to activate dev: error %u\\n\", ret);\n\t\terr = -EINVAL;\n\t\tgoto activate_err;\n\t}\n\n\tvmxnet3_init_coalesce(adapter);\n\tvmxnet3_init_rssfields(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tVMXNET3_WRITE_BAR0_REG(adapter,\n\t\t\t\tVMXNET3_REG_RXPROD + i * VMXNET3_REG_ALIGN,\n\t\t\t\tadapter->rx_queue[i].rx_ring[0].next2fill);\n\t\tVMXNET3_WRITE_BAR0_REG(adapter, (VMXNET3_REG_RXPROD2 +\n\t\t\t\t(i * VMXNET3_REG_ALIGN)),\n\t\t\t\tadapter->rx_queue[i].rx_ring[1].next2fill);\n\t}\n\n\t/* Apply the rx filter settins last. */\n\tvmxnet3_set_mc(adapter->netdev);\n\n\t/*\n\t * Check link state when first activating device. It will start the\n\t * tx queue if the link is up.\n\t */\n\tvmxnet3_check_link(adapter, true);\n\tnetif_tx_wake_all_queues(adapter->netdev);\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_enable(&adapter->rx_queue[i].napi);\n\tvmxnet3_enable_all_intrs(adapter);\n\tclear_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\treturn 0;\n\nactivate_err:\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAL, 0);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_DSAH, 0);\n\tvmxnet3_free_irqs(adapter);\nirq_err:\nrq_err:\n\t/* free up buffers we allocated */\n\tvmxnet3_rq_cleanup_all(adapter);\n\treturn err;\n}\n\n\nvoid\nvmxnet3_reset_dev(struct vmxnet3_adapter *adapter)\n{\n\tunsigned long flags;\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD, VMXNET3_CMD_RESET_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n}\n\n\nint\nvmxnet3_quiesce_dev(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\tunsigned long flags;\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state))\n\t\treturn 0;\n\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_disable_all_intrs(adapter);\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_disable(&adapter->rx_queue[i].napi);\n\tnetif_tx_disable(adapter->netdev);\n\tadapter->link_speed = 0;\n\tnetif_carrier_off(adapter->netdev);\n\n\tvmxnet3_tq_cleanup_all(adapter);\n\tvmxnet3_rq_cleanup_all(adapter);\n\tvmxnet3_free_irqs(adapter);\n\treturn 0;\n}\n\n\nstatic void\nvmxnet3_write_mac_addr(struct vmxnet3_adapter *adapter, const u8 *mac)\n{\n\tu32 tmp;\n\n\ttmp = *(u32 *)mac;\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_MACL, tmp);\n\n\ttmp = (mac[5] << 8) | mac[4];\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_MACH, tmp);\n}\n\n\nstatic int\nvmxnet3_set_mac_addr(struct net_device *netdev, void *p)\n{\n\tstruct sockaddr *addr = p;\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tdev_addr_set(netdev, addr->sa_data);\n\tvmxnet3_write_mac_addr(adapter, addr->sa_data);\n\n\treturn 0;\n}\n\n\n/* ==================== initialization and cleanup routines ============ */\n\nstatic int\nvmxnet3_alloc_pci_resources(struct vmxnet3_adapter *adapter)\n{\n\tint err;\n\tunsigned long mmio_start, mmio_len;\n\tstruct pci_dev *pdev = adapter->pdev;\n\n\terr = pci_enable_device(pdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to enable adapter: error %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = pci_request_selected_regions(pdev, (1 << 2) - 1,\n\t\t\t\t\t   vmxnet3_driver_name);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Failed to request region for adapter: error %d\\n\", err);\n\t\tgoto err_enable_device;\n\t}\n\n\tpci_set_master(pdev);\n\n\tmmio_start = pci_resource_start(pdev, 0);\n\tmmio_len = pci_resource_len(pdev, 0);\n\tadapter->hw_addr0 = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw_addr0) {\n\t\tdev_err(&pdev->dev, \"Failed to map bar0\\n\");\n\t\terr = -EIO;\n\t\tgoto err_ioremap;\n\t}\n\n\tmmio_start = pci_resource_start(pdev, 1);\n\tmmio_len = pci_resource_len(pdev, 1);\n\tadapter->hw_addr1 = ioremap(mmio_start, mmio_len);\n\tif (!adapter->hw_addr1) {\n\t\tdev_err(&pdev->dev, \"Failed to map bar1\\n\");\n\t\terr = -EIO;\n\t\tgoto err_bar1;\n\t}\n\treturn 0;\n\nerr_bar1:\n\tiounmap(adapter->hw_addr0);\nerr_ioremap:\n\tpci_release_selected_regions(pdev, (1 << 2) - 1);\nerr_enable_device:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_free_pci_resources(struct vmxnet3_adapter *adapter)\n{\n\tBUG_ON(!adapter->pdev);\n\n\tiounmap(adapter->hw_addr0);\n\tiounmap(adapter->hw_addr1);\n\tpci_release_selected_regions(adapter->pdev, (1 << 2) - 1);\n\tpci_disable_device(adapter->pdev);\n}\n\n\nstatic void\nvmxnet3_adjust_rx_ring_size(struct vmxnet3_adapter *adapter)\n{\n\tsize_t sz, i, ring0_size, ring1_size, comp_size;\n\tif (adapter->netdev->mtu <= VMXNET3_MAX_SKB_BUF_SIZE -\n\t\t\t\t    VMXNET3_MAX_ETH_HDR_SIZE) {\n\t\tadapter->skb_buf_size = adapter->netdev->mtu +\n\t\t\t\t\tVMXNET3_MAX_ETH_HDR_SIZE;\n\t\tif (adapter->skb_buf_size < VMXNET3_MIN_T0_BUF_SIZE)\n\t\t\tadapter->skb_buf_size = VMXNET3_MIN_T0_BUF_SIZE;\n\n\t\tadapter->rx_buf_per_pkt = 1;\n\t} else {\n\t\tadapter->skb_buf_size = VMXNET3_MAX_SKB_BUF_SIZE;\n\t\tsz = adapter->netdev->mtu - VMXNET3_MAX_SKB_BUF_SIZE +\n\t\t\t\t\t    VMXNET3_MAX_ETH_HDR_SIZE;\n\t\tadapter->rx_buf_per_pkt = 1 + (sz + PAGE_SIZE - 1) / PAGE_SIZE;\n\t}\n\n\t/*\n\t * for simplicity, force the ring0 size to be a multiple of\n\t * rx_buf_per_pkt * VMXNET3_RING_SIZE_ALIGN\n\t */\n\tsz = adapter->rx_buf_per_pkt * VMXNET3_RING_SIZE_ALIGN;\n\tring0_size = adapter->rx_queue[0].rx_ring[0].size;\n\tring0_size = (ring0_size + sz - 1) / sz * sz;\n\tring0_size = min_t(u32, ring0_size, VMXNET3_RX_RING_MAX_SIZE /\n\t\t\t   sz * sz);\n\tring1_size = adapter->rx_queue[0].rx_ring[1].size;\n\tring1_size = (ring1_size + sz - 1) / sz * sz;\n\tring1_size = min_t(u32, ring1_size, VMXNET3_RX_RING2_MAX_SIZE /\n\t\t\t   sz * sz);\n\tcomp_size = ring0_size + ring1_size;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue\t*rq = &adapter->rx_queue[i];\n\n\t\trq->rx_ring[0].size = ring0_size;\n\t\trq->rx_ring[1].size = ring1_size;\n\t\trq->comp_ring.size = comp_size;\n\t}\n}\n\n\nint\nvmxnet3_create_queues(struct vmxnet3_adapter *adapter, u32 tx_ring_size,\n\t\t      u32 rx_ring_size, u32 rx_ring2_size,\n\t\t      u16 txdata_desc_size, u16 rxdata_desc_size)\n{\n\tint err = 0, i;\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++) {\n\t\tstruct vmxnet3_tx_queue\t*tq = &adapter->tx_queue[i];\n\t\ttq->tx_ring.size   = tx_ring_size;\n\t\ttq->data_ring.size = tx_ring_size;\n\t\ttq->comp_ring.size = tx_ring_size;\n\t\ttq->txdata_desc_size = txdata_desc_size;\n\t\ttq->shared = &adapter->tqd_start[i].ctrl;\n\t\ttq->stopped = true;\n\t\ttq->adapter = adapter;\n\t\ttq->qid = i;\n\t\terr = vmxnet3_tq_create(tq, adapter);\n\t\t/*\n\t\t * Too late to change num_tx_queues. We cannot do away with\n\t\t * lesser number of queues than what we asked for\n\t\t */\n\t\tif (err)\n\t\t\tgoto queue_err;\n\t}\n\n\tadapter->rx_queue[0].rx_ring[0].size = rx_ring_size;\n\tadapter->rx_queue[0].rx_ring[1].size = rx_ring2_size;\n\tvmxnet3_adjust_rx_ring_size(adapter);\n\n\tadapter->rxdataring_enabled = VMXNET3_VERSION_GE_3(adapter);\n\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\tstruct vmxnet3_rx_queue *rq = &adapter->rx_queue[i];\n\t\t/* qid and qid2 for rx queues will be assigned later when num\n\t\t * of rx queues is finalized after allocating intrs */\n\t\trq->shared = &adapter->rqd_start[i].ctrl;\n\t\trq->adapter = adapter;\n\t\trq->data_ring.desc_size = rxdata_desc_size;\n\t\terr = vmxnet3_rq_create(rq, adapter);\n\t\tif (err) {\n\t\t\tif (i == 0) {\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Could not allocate any rx queues. \"\n\t\t\t\t\t   \"Aborting.\\n\");\n\t\t\t\tgoto queue_err;\n\t\t\t} else {\n\t\t\t\tnetdev_info(adapter->netdev,\n\t\t\t\t\t    \"Number of rx queues changed \"\n\t\t\t\t\t    \"to : %d.\\n\", i);\n\t\t\t\tadapter->num_rx_queues = i;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!adapter->rxdataring_enabled)\n\t\tvmxnet3_rq_destroy_all_rxdataring(adapter);\n\n\treturn err;\nqueue_err:\n\tvmxnet3_tq_destroy_all(adapter);\n\treturn err;\n}\n\nstatic int\nvmxnet3_open(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter;\n\tint err, i;\n\n\tadapter = netdev_priv(netdev);\n\n\tfor (i = 0; i < adapter->num_tx_queues; i++)\n\t\tspin_lock_init(&adapter->tx_queue[i].tx_lock);\n\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tunsigned long flags;\n\t\tu16 txdata_desc_size;\n\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_TXDATA_DESC_SIZE);\n\t\ttxdata_desc_size = VMXNET3_READ_BAR1_REG(adapter,\n\t\t\t\t\t\t\t VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\t\tif ((txdata_desc_size < VMXNET3_TXDATA_DESC_MIN_SIZE) ||\n\t\t    (txdata_desc_size > VMXNET3_TXDATA_DESC_MAX_SIZE) ||\n\t\t    (txdata_desc_size & VMXNET3_TXDATA_DESC_SIZE_MASK)) {\n\t\t\tadapter->txdata_desc_size =\n\t\t\t\tsizeof(struct Vmxnet3_TxDataDesc);\n\t\t} else {\n\t\t\tadapter->txdata_desc_size = txdata_desc_size;\n\t\t}\n\t} else {\n\t\tadapter->txdata_desc_size = sizeof(struct Vmxnet3_TxDataDesc);\n\t}\n\n\terr = vmxnet3_create_queues(adapter,\n\t\t\t\t    adapter->tx_ring_size,\n\t\t\t\t    adapter->rx_ring_size,\n\t\t\t\t    adapter->rx_ring2_size,\n\t\t\t\t    adapter->txdata_desc_size,\n\t\t\t\t    adapter->rxdata_desc_size);\n\tif (err)\n\t\tgoto queue_err;\n\n\terr = vmxnet3_activate_dev(adapter);\n\tif (err)\n\t\tgoto activate_err;\n\n\treturn 0;\n\nactivate_err:\n\tvmxnet3_rq_destroy_all(adapter);\n\tvmxnet3_tq_destroy_all(adapter);\nqueue_err:\n\treturn err;\n}\n\n\nstatic int\nvmxnet3_close(struct net_device *netdev)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\t/*\n\t * Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tvmxnet3_quiesce_dev(adapter);\n\n\tvmxnet3_rq_destroy_all(adapter);\n\tvmxnet3_tq_destroy_all(adapter);\n\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\n\n\treturn 0;\n}\n\n\nvoid\nvmxnet3_force_close(struct vmxnet3_adapter *adapter)\n{\n\tint i;\n\n\t/*\n\t * we must clear VMXNET3_STATE_BIT_RESETTING, otherwise\n\t * vmxnet3_close() will deadlock.\n\t */\n\tBUG_ON(test_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state));\n\n\t/* we need to enable NAPI, otherwise dev_close will deadlock */\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_enable(&adapter->rx_queue[i].napi);\n\t/*\n\t * Need to clear the quiesce bit to ensure that vmxnet3_close\n\t * can quiesce the device properly\n\t */\n\tclear_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\tdev_close(adapter->netdev);\n}\n\n\nstatic int\nvmxnet3_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tint err = 0;\n\n\tnetdev->mtu = new_mtu;\n\n\t/*\n\t * Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tif (netif_running(netdev)) {\n\t\tvmxnet3_quiesce_dev(adapter);\n\t\tvmxnet3_reset_dev(adapter);\n\n\t\t/* we need to re-create the rx queue based on the new mtu */\n\t\tvmxnet3_rq_destroy_all(adapter);\n\t\tvmxnet3_adjust_rx_ring_size(adapter);\n\t\terr = vmxnet3_rq_create_all(adapter);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"failed to re-create rx queues, \"\n\t\t\t\t   \" error %d. Closing it.\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = vmxnet3_activate_dev(adapter);\n\t\tif (err) {\n\t\t\tnetdev_err(netdev,\n\t\t\t\t   \"failed to re-activate, error %d. \"\n\t\t\t\t   \"Closing it\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\tif (err)\n\t\tvmxnet3_force_close(adapter);\n\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_declare_features(struct vmxnet3_adapter *adapter)\n{\n\tstruct net_device *netdev = adapter->netdev;\n\n\tnetdev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM |\n\t\tNETIF_F_HW_CSUM | NETIF_F_HW_VLAN_CTAG_TX |\n\t\tNETIF_F_HW_VLAN_CTAG_RX | NETIF_F_TSO | NETIF_F_TSO6 |\n\t\tNETIF_F_LRO | NETIF_F_HIGHDMA;\n\n\tif (VMXNET3_VERSION_GE_4(adapter)) {\n\t\tnetdev->hw_features |= NETIF_F_GSO_UDP_TUNNEL |\n\t\t\t\tNETIF_F_GSO_UDP_TUNNEL_CSUM;\n\n\t\tnetdev->hw_enc_features = NETIF_F_SG | NETIF_F_RXCSUM |\n\t\t\tNETIF_F_HW_CSUM | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\tNETIF_F_HW_VLAN_CTAG_RX | NETIF_F_TSO | NETIF_F_TSO6 |\n\t\t\tNETIF_F_LRO | NETIF_F_GSO_UDP_TUNNEL |\n\t\t\tNETIF_F_GSO_UDP_TUNNEL_CSUM;\n\t}\n\n\tnetdev->vlan_features = netdev->hw_features &\n\t\t\t\t~(NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t\t  NETIF_F_HW_VLAN_CTAG_RX);\n\tnetdev->features = netdev->hw_features | NETIF_F_HW_VLAN_CTAG_FILTER;\n}\n\n\nstatic void\nvmxnet3_read_mac_addr(struct vmxnet3_adapter *adapter, u8 *mac)\n{\n\tu32 tmp;\n\n\ttmp = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_MACL);\n\t*(u32 *)mac = tmp;\n\n\ttmp = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_MACH);\n\tmac[4] = tmp & 0xff;\n\tmac[5] = (tmp >> 8) & 0xff;\n}\n\n#ifdef CONFIG_PCI_MSI\n\n/*\n * Enable MSIx vectors.\n * Returns :\n *\tVMXNET3_LINUX_MIN_MSIX_VECT when only minimum number of vectors required\n *\t were enabled.\n *\tnumber of vectors which were enabled otherwise (this number is greater\n *\t than VMXNET3_LINUX_MIN_MSIX_VECT)\n */\n\nstatic int\nvmxnet3_acquire_msix_vectors(struct vmxnet3_adapter *adapter, int nvec)\n{\n\tint ret = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t\tadapter->intr.msix_entries, nvec, nvec);\n\n\tif (ret == -ENOSPC && nvec > VMXNET3_LINUX_MIN_MSIX_VECT) {\n\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\"Failed to enable %d MSI-X, trying %d\\n\",\n\t\t\tnvec, VMXNET3_LINUX_MIN_MSIX_VECT);\n\n\t\tret = pci_enable_msix_range(adapter->pdev,\n\t\t\t\t\t    adapter->intr.msix_entries,\n\t\t\t\t\t    VMXNET3_LINUX_MIN_MSIX_VECT,\n\t\t\t\t\t    VMXNET3_LINUX_MIN_MSIX_VECT);\n\t}\n\n\tif (ret < 0) {\n\t\tdev_err(&adapter->netdev->dev,\n\t\t\t\"Failed to enable MSI-X, error: %d\\n\", ret);\n\t}\n\n\treturn ret;\n}\n\n\n#endif /* CONFIG_PCI_MSI */\n\nstatic void\nvmxnet3_alloc_intr_resources(struct vmxnet3_adapter *adapter)\n{\n\tu32 cfg;\n\tunsigned long flags;\n\n\t/* intr settings */\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_GET_CONF_INTR);\n\tcfg = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tadapter->intr.type = cfg & 0x3;\n\tadapter->intr.mask_mode = (cfg >> 2) & 0x3;\n\n\tif (adapter->intr.type == VMXNET3_IT_AUTO) {\n\t\tadapter->intr.type = VMXNET3_IT_MSIX;\n\t}\n\n#ifdef CONFIG_PCI_MSI\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tint i, nvec, nvec_allocated;\n\n\t\tnvec  = adapter->share_intr == VMXNET3_INTR_TXSHARE ?\n\t\t\t1 : adapter->num_tx_queues;\n\t\tnvec += adapter->share_intr == VMXNET3_INTR_BUDDYSHARE ?\n\t\t\t0 : adapter->num_rx_queues;\n\t\tnvec += 1;\t/* for link event */\n\t\tnvec = nvec > VMXNET3_LINUX_MIN_MSIX_VECT ?\n\t\t       nvec : VMXNET3_LINUX_MIN_MSIX_VECT;\n\n\t\tfor (i = 0; i < nvec; i++)\n\t\t\tadapter->intr.msix_entries[i].entry = i;\n\n\t\tnvec_allocated = vmxnet3_acquire_msix_vectors(adapter, nvec);\n\t\tif (nvec_allocated < 0)\n\t\t\tgoto msix_err;\n\n\t\t/* If we cannot allocate one MSIx vector per queue\n\t\t * then limit the number of rx queues to 1\n\t\t */\n\t\tif (nvec_allocated == VMXNET3_LINUX_MIN_MSIX_VECT &&\n\t\t    nvec != VMXNET3_LINUX_MIN_MSIX_VECT) {\n\t\t\tif (adapter->share_intr != VMXNET3_INTR_BUDDYSHARE\n\t\t\t    || adapter->num_rx_queues != 1) {\n\t\t\t\tadapter->share_intr = VMXNET3_INTR_TXSHARE;\n\t\t\t\tnetdev_err(adapter->netdev,\n\t\t\t\t\t   \"Number of rx queues : 1\\n\");\n\t\t\t\tadapter->num_rx_queues = 1;\n\t\t\t}\n\t\t}\n\n\t\tadapter->intr.num_intrs = nvec_allocated;\n\t\treturn;\n\nmsix_err:\n\t\t/* If we cannot allocate MSIx vectors use only one rx queue */\n\t\tdev_info(&adapter->pdev->dev,\n\t\t\t \"Failed to enable MSI-X, error %d. \"\n\t\t\t \"Limiting #rx queues to 1, try MSI.\\n\", nvec_allocated);\n\n\t\tadapter->intr.type = VMXNET3_IT_MSI;\n\t}\n\n\tif (adapter->intr.type == VMXNET3_IT_MSI) {\n\t\tif (!pci_enable_msi(adapter->pdev)) {\n\t\t\tadapter->num_rx_queues = 1;\n\t\t\tadapter->intr.num_intrs = 1;\n\t\t\treturn;\n\t\t}\n\t}\n#endif /* CONFIG_PCI_MSI */\n\n\tadapter->num_rx_queues = 1;\n\tdev_info(&adapter->netdev->dev,\n\t\t \"Using INTx interrupt, #Rx queues: 1.\\n\");\n\tadapter->intr.type = VMXNET3_IT_INTX;\n\n\t/* INT-X related setting */\n\tadapter->intr.num_intrs = 1;\n}\n\n\nstatic void\nvmxnet3_free_intr_resources(struct vmxnet3_adapter *adapter)\n{\n\tif (adapter->intr.type == VMXNET3_IT_MSIX)\n\t\tpci_disable_msix(adapter->pdev);\n\telse if (adapter->intr.type == VMXNET3_IT_MSI)\n\t\tpci_disable_msi(adapter->pdev);\n\telse\n\t\tBUG_ON(adapter->intr.type != VMXNET3_IT_INTX);\n}\n\n\nstatic void\nvmxnet3_tx_timeout(struct net_device *netdev, unsigned int txqueue)\n{\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tadapter->tx_timeout_count++;\n\n\tnetdev_err(adapter->netdev, \"tx hang\\n\");\n\tschedule_work(&adapter->work);\n}\n\n\nstatic void\nvmxnet3_reset_work(struct work_struct *data)\n{\n\tstruct vmxnet3_adapter *adapter;\n\n\tadapter = container_of(data, struct vmxnet3_adapter, work);\n\n\t/* if another thread is resetting the device, no need to proceed */\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\treturn;\n\n\t/* if the device is closed, we must leave it alone */\n\trtnl_lock();\n\tif (netif_running(adapter->netdev)) {\n\t\tnetdev_notice(adapter->netdev, \"resetting\\n\");\n\t\tvmxnet3_quiesce_dev(adapter);\n\t\tvmxnet3_reset_dev(adapter);\n\t\tvmxnet3_activate_dev(adapter);\n\t} else {\n\t\tnetdev_info(adapter->netdev, \"already closed\\n\");\n\t}\n\trtnl_unlock();\n\n\tnetif_wake_queue(adapter->netdev);\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n}\n\n\nstatic int\nvmxnet3_probe_device(struct pci_dev *pdev,\n\t\t     const struct pci_device_id *id)\n{\n\tstatic const struct net_device_ops vmxnet3_netdev_ops = {\n\t\t.ndo_open = vmxnet3_open,\n\t\t.ndo_stop = vmxnet3_close,\n\t\t.ndo_start_xmit = vmxnet3_xmit_frame,\n\t\t.ndo_set_mac_address = vmxnet3_set_mac_addr,\n\t\t.ndo_change_mtu = vmxnet3_change_mtu,\n\t\t.ndo_fix_features = vmxnet3_fix_features,\n\t\t.ndo_set_features = vmxnet3_set_features,\n\t\t.ndo_features_check = vmxnet3_features_check,\n\t\t.ndo_get_stats64 = vmxnet3_get_stats64,\n\t\t.ndo_tx_timeout = vmxnet3_tx_timeout,\n\t\t.ndo_set_rx_mode = vmxnet3_set_mc,\n\t\t.ndo_vlan_rx_add_vid = vmxnet3_vlan_rx_add_vid,\n\t\t.ndo_vlan_rx_kill_vid = vmxnet3_vlan_rx_kill_vid,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t\t.ndo_poll_controller = vmxnet3_netpoll,\n#endif\n\t};\n\tint err;\n\tu32 ver;\n\tstruct net_device *netdev;\n\tstruct vmxnet3_adapter *adapter;\n\tu8 mac[ETH_ALEN];\n\tint size;\n\tint num_tx_queues;\n\tint num_rx_queues;\n\tint queues;\n\tunsigned long flags;\n\n\tif (!pci_msi_enabled())\n\t\tenable_mq = 0;\n\n#ifdef VMXNET3_RSS\n\tif (enable_mq)\n\t\tnum_rx_queues = min(VMXNET3_DEVICE_MAX_RX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n#endif\n\t\tnum_rx_queues = 1;\n\n\tif (enable_mq)\n\t\tnum_tx_queues = min(VMXNET3_DEVICE_MAX_TX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n\t\tnum_tx_queues = 1;\n\n\tnetdev = alloc_etherdev_mq(sizeof(struct vmxnet3_adapter),\n\t\t\t\t   max(num_tx_queues, num_rx_queues));\n\tif (!netdev)\n\t\treturn -ENOMEM;\n\n\tpci_set_drvdata(pdev, netdev);\n\tadapter = netdev_priv(netdev);\n\tadapter->netdev = netdev;\n\tadapter->pdev = pdev;\n\n\tadapter->tx_ring_size = VMXNET3_DEF_TX_RING_SIZE;\n\tadapter->rx_ring_size = VMXNET3_DEF_RX_RING_SIZE;\n\tadapter->rx_ring2_size = VMXNET3_DEF_RX_RING2_SIZE;\n\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"dma_set_mask failed\\n\");\n\t\tgoto err_set_mask;\n\t}\n\n\tspin_lock_init(&adapter->cmd_lock);\n\tadapter->adapter_pa = dma_map_single(&adapter->pdev->dev, adapter,\n\t\t\t\t\t     sizeof(struct vmxnet3_adapter),\n\t\t\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(&adapter->pdev->dev, adapter->adapter_pa)) {\n\t\tdev_err(&pdev->dev, \"Failed to map dma\\n\");\n\t\terr = -EFAULT;\n\t\tgoto err_set_mask;\n\t}\n\tadapter->shared = dma_alloc_coherent(\n\t\t\t\t&adapter->pdev->dev,\n\t\t\t\tsizeof(struct Vmxnet3_DriverShared),\n\t\t\t\t&adapter->shared_pa, GFP_KERNEL);\n\tif (!adapter->shared) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate memory\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_shared;\n\t}\n\n\terr = vmxnet3_alloc_pci_resources(adapter);\n\tif (err < 0)\n\t\tgoto err_alloc_pci;\n\n\tver = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_VRRS);\n\tif (ver & (1 << VMXNET3_REV_6)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_6);\n\t\tadapter->version = VMXNET3_REV_6 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_5)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_5);\n\t\tadapter->version = VMXNET3_REV_5 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_4)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_4);\n\t\tadapter->version = VMXNET3_REV_4 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_3)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_3);\n\t\tadapter->version = VMXNET3_REV_3 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_2)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_2);\n\t\tadapter->version = VMXNET3_REV_2 + 1;\n\t} else if (ver & (1 << VMXNET3_REV_1)) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter,\n\t\t\t\t       VMXNET3_REG_VRRS,\n\t\t\t\t       1 << VMXNET3_REV_1);\n\t\tadapter->version = VMXNET3_REV_1 + 1;\n\t} else {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Incompatible h/w version (0x%x) for adapter\\n\", ver);\n\t\terr = -EBUSY;\n\t\tgoto err_ver;\n\t}\n\tdev_dbg(&pdev->dev, \"Using device version %d\\n\", adapter->version);\n\n\tver = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_UVRS);\n\tif (ver & 1) {\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_UVRS, 1);\n\t} else {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Incompatible upt version (0x%x) for adapter\\n\", ver);\n\t\terr = -EBUSY;\n\t\tgoto err_ver;\n\t}\n\n\tif (VMXNET3_VERSION_GE_6(adapter)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_MAX_QUEUES_CONF);\n\t\tqueues = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t\tif (queues > 0) {\n\t\t\tadapter->num_rx_queues = min(num_rx_queues, ((queues >> 8) & 0xff));\n\t\t\tadapter->num_tx_queues = min(num_tx_queues, (queues & 0xff));\n\t\t} else {\n\t\t\tadapter->num_rx_queues = min(num_rx_queues,\n\t\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\t\tadapter->num_tx_queues = min(num_tx_queues,\n\t\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_TX_QUEUES);\n\t\t}\n\t\tif (adapter->num_rx_queues > VMXNET3_MAX_RX_QUEUES ||\n\t\t    adapter->num_tx_queues > VMXNET3_MAX_TX_QUEUES) {\n\t\t\tadapter->queuesExtEnabled = true;\n\t\t} else {\n\t\t\tadapter->queuesExtEnabled = false;\n\t\t}\n\t} else {\n\t\tadapter->queuesExtEnabled = false;\n\t\tnum_rx_queues = rounddown_pow_of_two(num_rx_queues);\n\t\tnum_tx_queues = rounddown_pow_of_two(num_tx_queues);\n\t\tadapter->num_rx_queues = min(num_rx_queues,\n\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\tadapter->num_tx_queues = min(num_tx_queues,\n\t\t\t\t\t     VMXNET3_DEVICE_DEFAULT_TX_QUEUES);\n\t}\n\tdev_info(&pdev->dev,\n\t\t \"# of Tx queues : %d, # of Rx queues : %d\\n\",\n\t\t adapter->num_tx_queues, adapter->num_rx_queues);\n\n\tadapter->rx_buf_per_pkt = 1;\n\n\tsize = sizeof(struct Vmxnet3_TxQueueDesc) * adapter->num_tx_queues;\n\tsize += sizeof(struct Vmxnet3_RxQueueDesc) * adapter->num_rx_queues;\n\tadapter->tqd_start = dma_alloc_coherent(&adapter->pdev->dev, size,\n\t\t\t\t\t\t&adapter->queue_desc_pa,\n\t\t\t\t\t\tGFP_KERNEL);\n\n\tif (!adapter->tqd_start) {\n\t\tdev_err(&pdev->dev, \"Failed to allocate memory\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto err_ver;\n\t}\n\tadapter->rqd_start = (struct Vmxnet3_RxQueueDesc *)(adapter->tqd_start +\n\t\t\t\t\t\t\t    adapter->num_tx_queues);\n\n\tadapter->pm_conf = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t      sizeof(struct Vmxnet3_PMConf),\n\t\t\t\t\t      &adapter->pm_conf_pa,\n\t\t\t\t\t      GFP_KERNEL);\n\tif (adapter->pm_conf == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_pm;\n\t}\n\n#ifdef VMXNET3_RSS\n\n\tadapter->rss_conf = dma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t       sizeof(struct UPT1_RSSConf),\n\t\t\t\t\t       &adapter->rss_conf_pa,\n\t\t\t\t\t       GFP_KERNEL);\n\tif (adapter->rss_conf == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto err_alloc_rss;\n\t}\n#endif /* VMXNET3_RSS */\n\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tadapter->coal_conf =\n\t\t\tdma_alloc_coherent(&adapter->pdev->dev,\n\t\t\t\t\t   sizeof(struct Vmxnet3_CoalesceScheme)\n\t\t\t\t\t   ,\n\t\t\t\t\t   &adapter->coal_conf_pa,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!adapter->coal_conf) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_coal_conf;\n\t\t}\n\t\tadapter->coal_conf->coalMode = VMXNET3_COALESCE_DISABLED;\n\t\tadapter->default_coal_mode = true;\n\t}\n\n\tif (VMXNET3_VERSION_GE_4(adapter)) {\n\t\tadapter->default_rss_fields = true;\n\t\tadapter->rss_fields = VMXNET3_RSS_FIELDS_DEFAULT;\n\t}\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\tvmxnet3_declare_features(adapter);\n\n\tadapter->rxdata_desc_size = VMXNET3_VERSION_GE_3(adapter) ?\n\t\tVMXNET3_DEF_RXDATA_DESC_SIZE : 0;\n\n\tif (adapter->num_tx_queues == adapter->num_rx_queues)\n\t\tadapter->share_intr = VMXNET3_INTR_BUDDYSHARE;\n\telse\n\t\tadapter->share_intr = VMXNET3_INTR_DONTSHARE;\n\n\tvmxnet3_alloc_intr_resources(adapter);\n\n#ifdef VMXNET3_RSS\n\tif (adapter->num_rx_queues > 1 &&\n\t    adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tadapter->rss = true;\n\t\tnetdev->hw_features |= NETIF_F_RXHASH;\n\t\tnetdev->features |= NETIF_F_RXHASH;\n\t\tdev_dbg(&pdev->dev, \"RSS is enabled.\\n\");\n\t} else {\n\t\tadapter->rss = false;\n\t}\n#endif\n\n\tvmxnet3_read_mac_addr(adapter, mac);\n\tdev_addr_set(netdev, mac);\n\n\tnetdev->netdev_ops = &vmxnet3_netdev_ops;\n\tvmxnet3_set_ethtool_ops(netdev);\n\tnetdev->watchdog_timeo = 5 * HZ;\n\n\t/* MTU range: 60 - 9190 */\n\tnetdev->min_mtu = VMXNET3_MIN_MTU;\n\tif (VMXNET3_VERSION_GE_6(adapter))\n\t\tnetdev->max_mtu = VMXNET3_V6_MAX_MTU;\n\telse\n\t\tnetdev->max_mtu = VMXNET3_MAX_MTU;\n\n\tINIT_WORK(&adapter->work, vmxnet3_reset_work);\n\tset_bit(VMXNET3_STATE_BIT_QUIESCED, &adapter->state);\n\n\tif (adapter->intr.type == VMXNET3_IT_MSIX) {\n\t\tint i;\n\t\tfor (i = 0; i < adapter->num_rx_queues; i++) {\n\t\t\tnetif_napi_add(adapter->netdev,\n\t\t\t\t       &adapter->rx_queue[i].napi,\n\t\t\t\t       vmxnet3_poll_rx_only, 64);\n\t\t}\n\t} else {\n\t\tnetif_napi_add(adapter->netdev, &adapter->rx_queue[0].napi,\n\t\t\t       vmxnet3_poll, 64);\n\t}\n\n\tnetif_set_real_num_tx_queues(adapter->netdev, adapter->num_tx_queues);\n\tnetif_set_real_num_rx_queues(adapter->netdev, adapter->num_rx_queues);\n\n\tnetif_carrier_off(netdev);\n\terr = register_netdev(netdev);\n\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to register adapter\\n\");\n\t\tgoto err_register;\n\t}\n\n\tvmxnet3_check_link(adapter, false);\n\treturn 0;\n\nerr_register:\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  sizeof(struct Vmxnet3_CoalesceScheme),\n\t\t\t\t  adapter->coal_conf, adapter->coal_conf_pa);\n\t}\n\tvmxnet3_free_intr_resources(adapter);\nerr_coal_conf:\n#ifdef VMXNET3_RSS\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct UPT1_RSSConf),\n\t\t\t  adapter->rss_conf, adapter->rss_conf_pa);\nerr_alloc_rss:\n#endif\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct Vmxnet3_PMConf),\n\t\t\t  adapter->pm_conf, adapter->pm_conf_pa);\nerr_alloc_pm:\n\tdma_free_coherent(&adapter->pdev->dev, size, adapter->tqd_start,\n\t\t\t  adapter->queue_desc_pa);\nerr_ver:\n\tvmxnet3_free_pci_resources(adapter);\nerr_alloc_pci:\n\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t  sizeof(struct Vmxnet3_DriverShared),\n\t\t\t  adapter->shared, adapter->shared_pa);\nerr_alloc_shared:\n\tdma_unmap_single(&adapter->pdev->dev, adapter->adapter_pa,\n\t\t\t sizeof(struct vmxnet3_adapter), DMA_TO_DEVICE);\nerr_set_mask:\n\tfree_netdev(netdev);\n\treturn err;\n}\n\n\nstatic void\nvmxnet3_remove_device(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tint size = 0;\n\tint num_rx_queues, rx_queues;\n\tunsigned long flags;\n\n#ifdef VMXNET3_RSS\n\tif (enable_mq)\n\t\tnum_rx_queues = min(VMXNET3_DEVICE_MAX_RX_QUEUES,\n\t\t\t\t    (int)num_online_cpus());\n\telse\n#endif\n\t\tnum_rx_queues = 1;\n\tif (!VMXNET3_VERSION_GE_6(adapter)) {\n\t\tnum_rx_queues = rounddown_pow_of_two(num_rx_queues);\n\t}\n\tif (VMXNET3_VERSION_GE_6(adapter)) {\n\t\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\t\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t\t       VMXNET3_CMD_GET_MAX_QUEUES_CONF);\n\t\trx_queues = VMXNET3_READ_BAR1_REG(adapter, VMXNET3_REG_CMD);\n\t\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\t\tif (rx_queues > 0)\n\t\t\trx_queues = (rx_queues >> 8) & 0xff;\n\t\telse\n\t\t\trx_queues = min(num_rx_queues, VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t\tnum_rx_queues = min(num_rx_queues, rx_queues);\n\t} else {\n\t\tnum_rx_queues = min(num_rx_queues,\n\t\t\t\t    VMXNET3_DEVICE_DEFAULT_RX_QUEUES);\n\t}\n\n\tcancel_work_sync(&adapter->work);\n\n\tunregister_netdev(netdev);\n\n\tvmxnet3_free_intr_resources(adapter);\n\tvmxnet3_free_pci_resources(adapter);\n\tif (VMXNET3_VERSION_GE_3(adapter)) {\n\t\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t\t  sizeof(struct Vmxnet3_CoalesceScheme),\n\t\t\t\t  adapter->coal_conf, adapter->coal_conf_pa);\n\t}\n#ifdef VMXNET3_RSS\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct UPT1_RSSConf),\n\t\t\t  adapter->rss_conf, adapter->rss_conf_pa);\n#endif\n\tdma_free_coherent(&adapter->pdev->dev, sizeof(struct Vmxnet3_PMConf),\n\t\t\t  adapter->pm_conf, adapter->pm_conf_pa);\n\n\tsize = sizeof(struct Vmxnet3_TxQueueDesc) * adapter->num_tx_queues;\n\tsize += sizeof(struct Vmxnet3_RxQueueDesc) * num_rx_queues;\n\tdma_free_coherent(&adapter->pdev->dev, size, adapter->tqd_start,\n\t\t\t  adapter->queue_desc_pa);\n\tdma_free_coherent(&adapter->pdev->dev,\n\t\t\t  sizeof(struct Vmxnet3_DriverShared),\n\t\t\t  adapter->shared, adapter->shared_pa);\n\tdma_unmap_single(&adapter->pdev->dev, adapter->adapter_pa,\n\t\t\t sizeof(struct vmxnet3_adapter), DMA_TO_DEVICE);\n\tfree_netdev(netdev);\n}\n\nstatic void vmxnet3_shutdown_device(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tunsigned long flags;\n\n\t/* Reset_work may be in the middle of resetting the device, wait for its\n\t * completion.\n\t */\n\twhile (test_and_set_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state))\n\t\tusleep_range(1000, 2000);\n\n\tif (test_and_set_bit(VMXNET3_STATE_BIT_QUIESCED,\n\t\t\t     &adapter->state)) {\n\t\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n\t\treturn;\n\t}\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_disable_all_intrs(adapter);\n\n\tclear_bit(VMXNET3_STATE_BIT_RESETTING, &adapter->state);\n}\n\n\n#ifdef CONFIG_PM\n\nstatic int\nvmxnet3_suspend(struct device *device)\n{\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\tstruct Vmxnet3_PMConf *pmConf;\n\tstruct ethhdr *ehdr;\n\tstruct arphdr *ahdr;\n\tu8 *arpreq;\n\tstruct in_device *in_dev;\n\tstruct in_ifaddr *ifa;\n\tunsigned long flags;\n\tint i = 0;\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tfor (i = 0; i < adapter->num_rx_queues; i++)\n\t\tnapi_disable(&adapter->rx_queue[i].napi);\n\n\tvmxnet3_disable_all_intrs(adapter);\n\tvmxnet3_free_irqs(adapter);\n\tvmxnet3_free_intr_resources(adapter);\n\n\tnetif_device_detach(netdev);\n\n\t/* Create wake-up filters. */\n\tpmConf = adapter->pm_conf;\n\tmemset(pmConf, 0, sizeof(*pmConf));\n\n\tif (adapter->wol & WAKE_UCAST) {\n\t\tpmConf->filters[i].patternSize = ETH_ALEN;\n\t\tpmConf->filters[i].maskSize = 1;\n\t\tmemcpy(pmConf->filters[i].pattern, netdev->dev_addr, ETH_ALEN);\n\t\tpmConf->filters[i].mask[0] = 0x3F; /* LSB ETH_ALEN bits */\n\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_FILTER;\n\t\ti++;\n\t}\n\n\tif (adapter->wol & WAKE_ARP) {\n\t\trcu_read_lock();\n\n\t\tin_dev = __in_dev_get_rcu(netdev);\n\t\tif (!in_dev) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto skip_arp;\n\t\t}\n\n\t\tifa = rcu_dereference(in_dev->ifa_list);\n\t\tif (!ifa) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto skip_arp;\n\t\t}\n\n\t\tpmConf->filters[i].patternSize = ETH_HLEN + /* Ethernet header*/\n\t\t\tsizeof(struct arphdr) +\t\t/* ARP header */\n\t\t\t2 * ETH_ALEN +\t\t/* 2 Ethernet addresses*/\n\t\t\t2 * sizeof(u32);\t/*2 IPv4 addresses */\n\t\tpmConf->filters[i].maskSize =\n\t\t\t(pmConf->filters[i].patternSize - 1) / 8 + 1;\n\n\t\t/* ETH_P_ARP in Ethernet header. */\n\t\tehdr = (struct ethhdr *)pmConf->filters[i].pattern;\n\t\tehdr->h_proto = htons(ETH_P_ARP);\n\n\t\t/* ARPOP_REQUEST in ARP header. */\n\t\tahdr = (struct arphdr *)&pmConf->filters[i].pattern[ETH_HLEN];\n\t\tahdr->ar_op = htons(ARPOP_REQUEST);\n\t\tarpreq = (u8 *)(ahdr + 1);\n\n\t\t/* The Unicast IPv4 address in 'tip' field. */\n\t\tarpreq += 2 * ETH_ALEN + sizeof(u32);\n\t\t*(__be32 *)arpreq = ifa->ifa_address;\n\n\t\trcu_read_unlock();\n\n\t\t/* The mask for the relevant bits. */\n\t\tpmConf->filters[i].mask[0] = 0x00;\n\t\tpmConf->filters[i].mask[1] = 0x30; /* ETH_P_ARP */\n\t\tpmConf->filters[i].mask[2] = 0x30; /* ARPOP_REQUEST */\n\t\tpmConf->filters[i].mask[3] = 0x00;\n\t\tpmConf->filters[i].mask[4] = 0xC0; /* IPv4 TIP */\n\t\tpmConf->filters[i].mask[5] = 0x03; /* IPv4 TIP */\n\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_FILTER;\n\t\ti++;\n\t}\n\nskip_arp:\n\tif (adapter->wol & WAKE_MAGIC)\n\t\tpmConf->wakeUpEvents |= VMXNET3_PM_WAKEUP_MAGIC;\n\n\tpmConf->numFilters = i;\n\n\tadapter->shared->devRead.pmConfDesc.confVer = cpu_to_le32(1);\n\tadapter->shared->devRead.pmConfDesc.confLen = cpu_to_le32(sizeof(\n\t\t\t\t\t\t\t\t  *pmConf));\n\tadapter->shared->devRead.pmConfDesc.confPA =\n\t\tcpu_to_le64(adapter->pm_conf_pa);\n\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_UPDATE_PMCFG);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\n\tpci_save_state(pdev);\n\tpci_enable_wake(pdev, pci_choose_state(pdev, PMSG_SUSPEND),\n\t\t\tadapter->wol);\n\tpci_disable_device(pdev);\n\tpci_set_power_state(pdev, pci_choose_state(pdev, PMSG_SUSPEND));\n\n\treturn 0;\n}\n\n\nstatic int\nvmxnet3_resume(struct device *device)\n{\n\tint err;\n\tunsigned long flags;\n\tstruct pci_dev *pdev = to_pci_dev(device);\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct vmxnet3_adapter *adapter = netdev_priv(netdev);\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\tpci_restore_state(pdev);\n\terr = pci_enable_device_mem(pdev);\n\tif (err != 0)\n\t\treturn err;\n\n\tpci_enable_wake(pdev, PCI_D0, 0);\n\n\tvmxnet3_alloc_intr_resources(adapter);\n\n\t/* During hibernate and suspend, device has to be reinitialized as the\n\t * device state need not be preserved.\n\t */\n\n\t/* Need not check adapter state as other reset tasks cannot run during\n\t * device resume.\n\t */\n\tspin_lock_irqsave(&adapter->cmd_lock, flags);\n\tVMXNET3_WRITE_BAR1_REG(adapter, VMXNET3_REG_CMD,\n\t\t\t       VMXNET3_CMD_QUIESCE_DEV);\n\tspin_unlock_irqrestore(&adapter->cmd_lock, flags);\n\tvmxnet3_tq_cleanup_all(adapter);\n\tvmxnet3_rq_cleanup_all(adapter);\n\n\tvmxnet3_reset_dev(adapter);\n\terr = vmxnet3_activate_dev(adapter);\n\tif (err != 0) {\n\t\tnetdev_err(netdev,\n\t\t\t   \"failed to re-activate on resume, error: %d\", err);\n\t\tvmxnet3_force_close(adapter);\n\t\treturn err;\n\t}\n\tnetif_device_attach(netdev);\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops vmxnet3_pm_ops = {\n\t.suspend = vmxnet3_suspend,\n\t.resume = vmxnet3_resume,\n\t.freeze = vmxnet3_suspend,\n\t.restore = vmxnet3_resume,\n};\n#endif\n\nstatic struct pci_driver vmxnet3_driver = {\n\t.name\t\t= vmxnet3_driver_name,\n\t.id_table\t= vmxnet3_pciid_table,\n\t.probe\t\t= vmxnet3_probe_device,\n\t.remove\t\t= vmxnet3_remove_device,\n\t.shutdown\t= vmxnet3_shutdown_device,\n#ifdef CONFIG_PM\n\t.driver.pm\t= &vmxnet3_pm_ops,\n#endif\n};\n\n\nstatic int __init\nvmxnet3_init_module(void)\n{\n\tpr_info(\"%s - version %s\\n\", VMXNET3_DRIVER_DESC,\n\t\tVMXNET3_DRIVER_VERSION_REPORT);\n\treturn pci_register_driver(&vmxnet3_driver);\n}\n\nmodule_init(vmxnet3_init_module);\n\n\nstatic void\nvmxnet3_exit_module(void)\n{\n\tpci_unregister_driver(&vmxnet3_driver);\n}\n\nmodule_exit(vmxnet3_exit_module);\n\nMODULE_AUTHOR(\"VMware, Inc.\");\nMODULE_DESCRIPTION(VMXNET3_DRIVER_DESC);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_VERSION(VMXNET3_DRIVER_VERSION_STRING);\n"], "filenames": ["drivers/net/vmxnet3/vmxnet3_drv.c"], "buggy_code_start_loc": [591], "buggy_code_end_loc": [615], "fixing_code_start_loc": [592], "fixing_code_end_loc": [618], "type": "CWE-416", "message": "A use-after-free flaw was found in vmxnet3_rq_alloc_rx_buf in drivers/net/vmxnet3/vmxnet3_drv.c in VMware's vmxnet3 ethernet NIC driver in the Linux Kernel. This issue could allow a local attacker to crash the system due to a double-free while cleaning up vmxnet3_rq_cleanup_all, which could also lead to a kernel information leak problem.", "other": {"cve": {"id": "CVE-2023-4387", "sourceIdentifier": "secalert@redhat.com", "published": "2023-08-16T19:15:10.087", "lastModified": "2024-01-12T17:43:56.697", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A use-after-free flaw was found in vmxnet3_rq_alloc_rx_buf in drivers/net/vmxnet3/vmxnet3_drv.c in VMware's vmxnet3 ethernet NIC driver in the Linux Kernel. This issue could allow a local attacker to crash the system due to a double-free while cleaning up vmxnet3_rq_cleanup_all, which could also lead to a kernel information leak problem."}, {"lang": "es", "value": "Se encontr\u00f3 una falla de use-after-free en vmxnet3_rq_alloc_rx_buf en drivers/net/vmxnet3/vmxnet3_drv.c en el controlador NIC Ethernet vmxnet3 de VMware en el kernel de Linux. Este problema podr\u00eda permitir que un atacante local bloquee el sistema debido a una doble liberaci\u00f3n mientras se limpia vmxnet3_rq_cleanup_all, lo que tambi\u00e9n podr\u00eda provocar un problema de fuga de informaci\u00f3n del kernel."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}, {"source": "secalert@redhat.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.16.60", "versionEndExcluding": "3.17", "matchCriteriaId": "00B213F5-FFE1-4935-A397-495350473D00"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.4", "versionEndExcluding": "4.9.316", "matchCriteriaId": "4A90441C-F154-45D1-A968-9673EEC0C058"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.14.281", "matchCriteriaId": "EBB1A3B4-E46A-4454-A428-85CC0AC925F6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.19.245", "matchCriteriaId": "239757EB-B2DF-4DD4-8EEE-97141186DA12"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.4.196", "matchCriteriaId": "87FC1554-2185-4ED6-BF1C-293AA14FFC32"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5", "versionEndExcluding": "5.10.118", "matchCriteriaId": "CA790029-5DF7-42D7-962E-C810540457A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.11", "versionEndExcluding": "5.15.42", "matchCriteriaId": "555641B6-5319-4C13-9CC9-50B1CCF9E816"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.16", "versionEndExcluding": "5.17.10", "matchCriteriaId": "6D0772F5-6B38-4D6C-B29E-A04E7CC5CB9F"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "142AD0DD-4CF3-4D74-9442-459CE3347E3A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "F4CFF558-3C47-480D-A2F0-BABF26042943"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "7F6FB57C-2BC7-487C-96DD-132683AEB35D"}]}]}], "references": [{"url": "https://access.redhat.com/security/cve/CVE-2023-4387", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2219270", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/9e7fef9521e73ca8afd7da9e58c14654b02dfad8", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9e7fef9521e73ca8afd7da9e58c14654b02dfad8"}}