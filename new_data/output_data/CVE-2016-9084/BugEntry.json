{"buggy_code": ["/*\n * Copyright (C) 2012 Red Hat, Inc.  All rights reserved.\n *     Author: Alex Williamson <alex.williamson@redhat.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * Derived from original vfio:\n * Copyright 2010 Cisco Systems, Inc.  All rights reserved.\n * Author: Tom Lyon, pugs@cisco.com\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/device.h>\n#include <linux/eventfd.h>\n#include <linux/file.h>\n#include <linux/interrupt.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/notifier.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/vfio.h>\n#include <linux/vgaarb.h>\n\n#include \"vfio_pci_private.h\"\n\n#define DRIVER_VERSION  \"0.2\"\n#define DRIVER_AUTHOR   \"Alex Williamson <alex.williamson@redhat.com>\"\n#define DRIVER_DESC     \"VFIO PCI - User Level meta-driver\"\n\nstatic char ids[1024] __initdata;\nmodule_param_string(ids, ids, sizeof(ids), 0);\nMODULE_PARM_DESC(ids, \"Initial PCI IDs to add to the vfio driver, format is \\\"vendor:device[:subvendor[:subdevice[:class[:class_mask]]]]\\\" and multiple comma separated entries can be specified\");\n\nstatic bool nointxmask;\nmodule_param_named(nointxmask, nointxmask, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(nointxmask,\n\t\t  \"Disable support for PCI 2.3 style INTx masking.  If this resolves problems for specific devices, report lspci -vvvxxx to linux-pci@vger.kernel.org so the device can be fixed automatically via the broken_intx_masking flag.\");\n\n#ifdef CONFIG_VFIO_PCI_VGA\nstatic bool disable_vga;\nmodule_param(disable_vga, bool, S_IRUGO);\nMODULE_PARM_DESC(disable_vga, \"Disable VGA resource access through vfio-pci\");\n#endif\n\nstatic bool disable_idle_d3;\nmodule_param(disable_idle_d3, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(disable_idle_d3,\n\t\t \"Disable using the PCI D3 low power state for idle, unused devices\");\n\nstatic DEFINE_MUTEX(driver_lock);\n\nstatic inline bool vfio_vga_disabled(void)\n{\n#ifdef CONFIG_VFIO_PCI_VGA\n\treturn disable_vga;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * Our VGA arbiter participation is limited since we don't know anything\n * about the device itself.  However, if the device is the only VGA device\n * downstream of a bridge and VFIO VGA support is disabled, then we can\n * safely return legacy VGA IO and memory as not decoded since the user\n * has no way to get to it and routing can be disabled externally at the\n * bridge.\n */\nstatic unsigned int vfio_pci_set_vga_decode(void *opaque, bool single_vga)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\tstruct pci_dev *tmp = NULL, *pdev = vdev->pdev;\n\tunsigned char max_busnr;\n\tunsigned int decodes;\n\n\tif (single_vga || !vfio_vga_disabled() || pci_is_root_bus(pdev->bus))\n\t\treturn VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t       VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\n\tmax_busnr = pci_bus_max_busnr(pdev->bus);\n\tdecodes = VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;\n\n\twhile ((tmp = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, tmp)) != NULL) {\n\t\tif (tmp == pdev ||\n\t\t    pci_domain_nr(tmp->bus) != pci_domain_nr(pdev->bus) ||\n\t\t    pci_is_root_bus(tmp->bus))\n\t\t\tcontinue;\n\n\t\tif (tmp->bus->number >= pdev->bus->number &&\n\t\t    tmp->bus->number <= max_busnr) {\n\t\t\tpci_dev_put(tmp);\n\t\t\tdecodes |= VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn decodes;\n}\n\nstatic inline bool vfio_pci_is_vga(struct pci_dev *pdev)\n{\n\treturn (pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA;\n}\n\nstatic void vfio_pci_probe_mmaps(struct vfio_pci_device *vdev)\n{\n\tstruct resource *res;\n\tint bar;\n\tstruct vfio_pci_dummy_resource *dummy_res;\n\n\tINIT_LIST_HEAD(&vdev->dummy_resources_list);\n\n\tfor (bar = PCI_STD_RESOURCES; bar <= PCI_STD_RESOURCE_END; bar++) {\n\t\tres = vdev->pdev->resource + bar;\n\n\t\tif (!IS_ENABLED(CONFIG_VFIO_PCI_MMAP))\n\t\t\tgoto no_mmap;\n\n\t\tif (!(res->flags & IORESOURCE_MEM))\n\t\t\tgoto no_mmap;\n\n\t\t/*\n\t\t * The PCI core shouldn't set up a resource with a\n\t\t * type but zero size. But there may be bugs that\n\t\t * cause us to do that.\n\t\t */\n\t\tif (!resource_size(res))\n\t\t\tgoto no_mmap;\n\n\t\tif (resource_size(res) >= PAGE_SIZE) {\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!(res->start & ~PAGE_MASK)) {\n\t\t\t/*\n\t\t\t * Add a dummy resource to reserve the remainder\n\t\t\t * of the exclusive page in case that hot-add\n\t\t\t * device's bar is assigned into it.\n\t\t\t */\n\t\t\tdummy_res = kzalloc(sizeof(*dummy_res), GFP_KERNEL);\n\t\t\tif (dummy_res == NULL)\n\t\t\t\tgoto no_mmap;\n\n\t\t\tdummy_res->resource.name = \"vfio sub-page reserved\";\n\t\t\tdummy_res->resource.start = res->end + 1;\n\t\t\tdummy_res->resource.end = res->start + PAGE_SIZE - 1;\n\t\t\tdummy_res->resource.flags = res->flags;\n\t\t\tif (request_resource(res->parent,\n\t\t\t\t\t\t&dummy_res->resource)) {\n\t\t\t\tkfree(dummy_res);\n\t\t\t\tgoto no_mmap;\n\t\t\t}\n\t\t\tdummy_res->index = bar;\n\t\t\tlist_add(&dummy_res->res_next,\n\t\t\t\t\t&vdev->dummy_resources_list);\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * Here we don't handle the case when the BAR is not page\n\t\t * aligned because we can't expect the BAR will be\n\t\t * assigned into the same location in a page in guest\n\t\t * when we passthrough the BAR. And it's hard to access\n\t\t * this BAR in userspace because we have no way to get\n\t\t * the BAR's location in a page.\n\t\t */\nno_mmap:\n\t\tvdev->bar_mmap_supported[bar] = false;\n\t}\n}\n\nstatic void vfio_pci_try_bus_reset(struct vfio_pci_device *vdev);\nstatic void vfio_pci_disable(struct vfio_pci_device *vdev);\n\n/*\n * INTx masking requires the ability to disable INTx signaling via PCI_COMMAND\n * _and_ the ability detect when the device is asserting INTx via PCI_STATUS.\n * If a device implements the former but not the latter we would typically\n * expect broken_intx_masking be set and require an exclusive interrupt.\n * However since we do have control of the device's ability to assert INTx,\n * we can instead pretend that the device does not implement INTx, virtualizing\n * the pin register to report zero and maintaining DisINTx set on the host.\n */\nstatic bool vfio_pci_nointx(struct pci_dev *pdev)\n{\n\tswitch (pdev->vendor) {\n\tcase PCI_VENDOR_ID_INTEL:\n\t\tswitch (pdev->device) {\n\t\t/* All i40e (XL710/X710) 10/20/40GbE NICs */\n\t\tcase 0x1572:\n\t\tcase 0x1574:\n\t\tcase 0x1580 ... 0x1581:\n\t\tcase 0x1583 ... 0x1589:\n\t\tcase 0x37d0 ... 0x37d2:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int vfio_pci_enable(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint ret;\n\tu16 cmd;\n\tu8 msix_pos;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\n\t/* Don't allow our initial saved state to include busmaster */\n\tpci_clear_master(pdev);\n\n\tret = pci_enable_device(pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tvdev->reset_works = (pci_reset_function(pdev) == 0);\n\tpci_save_state(pdev);\n\tvdev->pci_saved_state = pci_store_saved_state(pdev);\n\tif (!vdev->pci_saved_state)\n\t\tpr_debug(\"%s: Couldn't store %s saved state\\n\",\n\t\t\t __func__, dev_name(&pdev->dev));\n\n\tif (likely(!nointxmask)) {\n\t\tif (vfio_pci_nointx(pdev)) {\n\t\t\tdev_info(&pdev->dev, \"Masking broken INTx support\\n\");\n\t\t\tvdev->nointx = true;\n\t\t\tpci_intx(pdev, 0);\n\t\t} else\n\t\t\tvdev->pci_2_3 = pci_intx_mask_supported(pdev);\n\t}\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &cmd);\n\tif (vdev->pci_2_3 && (cmd & PCI_COMMAND_INTX_DISABLE)) {\n\t\tcmd &= ~PCI_COMMAND_INTX_DISABLE;\n\t\tpci_write_config_word(pdev, PCI_COMMAND, cmd);\n\t}\n\n\tret = vfio_config_init(vdev);\n\tif (ret) {\n\t\tkfree(vdev->pci_saved_state);\n\t\tvdev->pci_saved_state = NULL;\n\t\tpci_disable_device(pdev);\n\t\treturn ret;\n\t}\n\n\tmsix_pos = pdev->msix_cap;\n\tif (msix_pos) {\n\t\tu16 flags;\n\t\tu32 table;\n\n\t\tpci_read_config_word(pdev, msix_pos + PCI_MSIX_FLAGS, &flags);\n\t\tpci_read_config_dword(pdev, msix_pos + PCI_MSIX_TABLE, &table);\n\n\t\tvdev->msix_bar = table & PCI_MSIX_TABLE_BIR;\n\t\tvdev->msix_offset = table & PCI_MSIX_TABLE_OFFSET;\n\t\tvdev->msix_size = ((flags & PCI_MSIX_FLAGS_QSIZE) + 1) * 16;\n\t} else\n\t\tvdev->msix_bar = 0xFF;\n\n\tif (!vfio_vga_disabled() && vfio_pci_is_vga(pdev))\n\t\tvdev->has_vga = true;\n\n\n\tif (vfio_pci_is_vga(pdev) &&\n\t    pdev->vendor == PCI_VENDOR_ID_INTEL &&\n\t    IS_ENABLED(CONFIG_VFIO_PCI_IGD)) {\n\t\tret = vfio_pci_igd_init(vdev);\n\t\tif (ret) {\n\t\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t\t \"Failed to setup Intel IGD regions\\n\");\n\t\t\tvfio_pci_disable(vdev);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tvfio_pci_probe_mmaps(vdev);\n\n\treturn 0;\n}\n\nstatic void vfio_pci_disable(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct vfio_pci_dummy_resource *dummy_res, *tmp;\n\tint i, bar;\n\n\t/* Stop the device from further DMA */\n\tpci_clear_master(pdev);\n\n\tvfio_pci_set_irqs_ioctl(vdev, VFIO_IRQ_SET_DATA_NONE |\n\t\t\t\tVFIO_IRQ_SET_ACTION_TRIGGER,\n\t\t\t\tvdev->irq_type, 0, 0, NULL);\n\n\tvdev->virq_disabled = false;\n\n\tfor (i = 0; i < vdev->num_regions; i++)\n\t\tvdev->region[i].ops->release(vdev, &vdev->region[i]);\n\n\tvdev->num_regions = 0;\n\tkfree(vdev->region);\n\tvdev->region = NULL; /* don't krealloc a freed pointer */\n\n\tvfio_config_free(vdev);\n\n\tfor (bar = PCI_STD_RESOURCES; bar <= PCI_STD_RESOURCE_END; bar++) {\n\t\tif (!vdev->barmap[bar])\n\t\t\tcontinue;\n\t\tpci_iounmap(pdev, vdev->barmap[bar]);\n\t\tpci_release_selected_regions(pdev, 1 << bar);\n\t\tvdev->barmap[bar] = NULL;\n\t}\n\n\tlist_for_each_entry_safe(dummy_res, tmp,\n\t\t\t\t &vdev->dummy_resources_list, res_next) {\n\t\tlist_del(&dummy_res->res_next);\n\t\trelease_resource(&dummy_res->resource);\n\t\tkfree(dummy_res);\n\t}\n\n\tvdev->needs_reset = true;\n\n\t/*\n\t * If we have saved state, restore it.  If we can reset the device,\n\t * even better.  Resetting with current state seems better than\n\t * nothing, but saving and restoring current state without reset\n\t * is just busy work.\n\t */\n\tif (pci_load_and_free_saved_state(pdev, &vdev->pci_saved_state)) {\n\t\tpr_info(\"%s: Couldn't reload %s saved state\\n\",\n\t\t\t__func__, dev_name(&pdev->dev));\n\n\t\tif (!vdev->reset_works)\n\t\t\tgoto out;\n\n\t\tpci_save_state(pdev);\n\t}\n\n\t/*\n\t * Disable INTx and MSI, presumably to avoid spurious interrupts\n\t * during reset.  Stolen from pci_reset_function()\n\t */\n\tpci_write_config_word(pdev, PCI_COMMAND, PCI_COMMAND_INTX_DISABLE);\n\n\t/*\n\t * Try to reset the device.  The success of this is dependent on\n\t * being able to lock the device, which is not always possible.\n\t */\n\tif (vdev->reset_works && !pci_try_reset_function(pdev))\n\t\tvdev->needs_reset = false;\n\n\tpci_restore_state(pdev);\nout:\n\tpci_disable_device(pdev);\n\n\tvfio_pci_try_bus_reset(vdev);\n\n\tif (!disable_idle_d3)\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n}\n\nstatic void vfio_pci_release(void *device_data)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tmutex_lock(&driver_lock);\n\n\tif (!(--vdev->refcnt)) {\n\t\tvfio_spapr_pci_eeh_release(vdev->pdev);\n\t\tvfio_pci_disable(vdev);\n\t}\n\n\tmutex_unlock(&driver_lock);\n\n\tmodule_put(THIS_MODULE);\n}\n\nstatic int vfio_pci_open(void *device_data)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tint ret = 0;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\tmutex_lock(&driver_lock);\n\n\tif (!vdev->refcnt) {\n\t\tret = vfio_pci_enable(vdev);\n\t\tif (ret)\n\t\t\tgoto error;\n\n\t\tvfio_spapr_pci_eeh_open(vdev->pdev);\n\t}\n\tvdev->refcnt++;\nerror:\n\tmutex_unlock(&driver_lock);\n\tif (ret)\n\t\tmodule_put(THIS_MODULE);\n\treturn ret;\n}\n\nstatic int vfio_pci_get_irq_count(struct vfio_pci_device *vdev, int irq_type)\n{\n\tif (irq_type == VFIO_PCI_INTX_IRQ_INDEX) {\n\t\tu8 pin;\n\t\tpci_read_config_byte(vdev->pdev, PCI_INTERRUPT_PIN, &pin);\n\t\tif (IS_ENABLED(CONFIG_VFIO_PCI_INTX) && !vdev->nointx && pin)\n\t\t\treturn 1;\n\n\t} else if (irq_type == VFIO_PCI_MSI_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msi_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSI_FLAGS, &flags);\n\t\t\treturn 1 << ((flags & PCI_MSI_FLAGS_QMASK) >> 1);\n\t\t}\n\t} else if (irq_type == VFIO_PCI_MSIX_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msix_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSIX_FLAGS, &flags);\n\n\t\t\treturn (flags & PCI_MSIX_FLAGS_QSIZE) + 1;\n\t\t}\n\t} else if (irq_type == VFIO_PCI_ERR_IRQ_INDEX) {\n\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\treturn 1;\n\t} else if (irq_type == VFIO_PCI_REQ_IRQ_INDEX) {\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_count_devs(struct pci_dev *pdev, void *data)\n{\n\t(*(int *)data)++;\n\treturn 0;\n}\n\nstruct vfio_pci_fill_info {\n\tint max;\n\tint cur;\n\tstruct vfio_pci_dependent_device *devices;\n};\n\nstatic int vfio_pci_fill_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_fill_info *fill = data;\n\tstruct iommu_group *iommu_group;\n\n\tif (fill->cur == fill->max)\n\t\treturn -EAGAIN; /* Something changed, try again */\n\n\tiommu_group = iommu_group_get(&pdev->dev);\n\tif (!iommu_group)\n\t\treturn -EPERM; /* Cannot reset non-isolated devices */\n\n\tfill->devices[fill->cur].group_id = iommu_group_id(iommu_group);\n\tfill->devices[fill->cur].segment = pci_domain_nr(pdev->bus);\n\tfill->devices[fill->cur].bus = pdev->bus->number;\n\tfill->devices[fill->cur].devfn = pdev->devfn;\n\tfill->cur++;\n\tiommu_group_put(iommu_group);\n\treturn 0;\n}\n\nstruct vfio_pci_group_entry {\n\tstruct vfio_group *group;\n\tint id;\n};\n\nstruct vfio_pci_group_info {\n\tint count;\n\tstruct vfio_pci_group_entry *groups;\n};\n\nstatic int vfio_pci_validate_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_group_info *info = data;\n\tstruct iommu_group *group;\n\tint id, i;\n\n\tgroup = iommu_group_get(&pdev->dev);\n\tif (!group)\n\t\treturn -EPERM;\n\n\tid = iommu_group_id(group);\n\n\tfor (i = 0; i < info->count; i++)\n\t\tif (info->groups[i].id == id)\n\t\t\tbreak;\n\n\tiommu_group_put(group);\n\n\treturn (i == info->count) ? -EINVAL : 0;\n}\n\nstatic bool vfio_pci_dev_below_slot(struct pci_dev *pdev, struct pci_slot *slot)\n{\n\tfor (; pdev; pdev = pdev->bus->self)\n\t\tif (pdev->bus == slot->bus)\n\t\t\treturn (pdev->slot == slot);\n\treturn false;\n}\n\nstruct vfio_pci_walk_info {\n\tint (*fn)(struct pci_dev *, void *data);\n\tvoid *data;\n\tstruct pci_dev *pdev;\n\tbool slot;\n\tint ret;\n};\n\nstatic int vfio_pci_walk_wrapper(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_walk_info *walk = data;\n\n\tif (!walk->slot || vfio_pci_dev_below_slot(pdev, walk->pdev->slot))\n\t\twalk->ret = walk->fn(pdev, walk->data);\n\n\treturn walk->ret;\n}\n\nstatic int vfio_pci_for_each_slot_or_bus(struct pci_dev *pdev,\n\t\t\t\t\t int (*fn)(struct pci_dev *,\n\t\t\t\t\t\t   void *data), void *data,\n\t\t\t\t\t bool slot)\n{\n\tstruct vfio_pci_walk_info walk = {\n\t\t.fn = fn, .data = data, .pdev = pdev, .slot = slot, .ret = 0,\n\t};\n\n\tpci_walk_bus(pdev->bus, vfio_pci_walk_wrapper, &walk);\n\n\treturn walk.ret;\n}\n\nstatic int msix_sparse_mmap_cap(struct vfio_pci_device *vdev,\n\t\t\t\tstruct vfio_info_cap *caps)\n{\n\tstruct vfio_info_cap_header *header;\n\tstruct vfio_region_info_cap_sparse_mmap *sparse;\n\tsize_t end, size;\n\tint nr_areas = 2, i = 0;\n\n\tend = pci_resource_len(vdev->pdev, vdev->msix_bar);\n\n\t/* If MSI-X table is aligned to the start or end, only one area */\n\tif (((vdev->msix_offset & PAGE_MASK) == 0) ||\n\t    (PAGE_ALIGN(vdev->msix_offset + vdev->msix_size) >= end))\n\t\tnr_areas = 1;\n\n\tsize = sizeof(*sparse) + (nr_areas * sizeof(*sparse->areas));\n\n\theader = vfio_info_cap_add(caps, size,\n\t\t\t\t   VFIO_REGION_INFO_CAP_SPARSE_MMAP, 1);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tsparse = container_of(header,\n\t\t\t      struct vfio_region_info_cap_sparse_mmap, header);\n\tsparse->nr_areas = nr_areas;\n\n\tif (vdev->msix_offset & PAGE_MASK) {\n\t\tsparse->areas[i].offset = 0;\n\t\tsparse->areas[i].size = vdev->msix_offset & PAGE_MASK;\n\t\ti++;\n\t}\n\n\tif (PAGE_ALIGN(vdev->msix_offset + vdev->msix_size) < end) {\n\t\tsparse->areas[i].offset = PAGE_ALIGN(vdev->msix_offset +\n\t\t\t\t\t\t     vdev->msix_size);\n\t\tsparse->areas[i].size = end - sparse->areas[i].offset;\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nstatic int region_type_cap(struct vfio_pci_device *vdev,\n\t\t\t   struct vfio_info_cap *caps,\n\t\t\t   unsigned int type, unsigned int subtype)\n{\n\tstruct vfio_info_cap_header *header;\n\tstruct vfio_region_info_cap_type *cap;\n\n\theader = vfio_info_cap_add(caps, sizeof(*cap),\n\t\t\t\t   VFIO_REGION_INFO_CAP_TYPE, 1);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tcap = container_of(header, struct vfio_region_info_cap_type, header);\n\tcap->type = type;\n\tcap->subtype = subtype;\n\n\treturn 0;\n}\n\nint vfio_pci_register_dev_region(struct vfio_pci_device *vdev,\n\t\t\t\t unsigned int type, unsigned int subtype,\n\t\t\t\t const struct vfio_pci_regops *ops,\n\t\t\t\t size_t size, u32 flags, void *data)\n{\n\tstruct vfio_pci_region *region;\n\n\tregion = krealloc(vdev->region,\n\t\t\t  (vdev->num_regions + 1) * sizeof(*region),\n\t\t\t  GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tvdev->region = region;\n\tvdev->region[vdev->num_regions].type = type;\n\tvdev->region[vdev->num_regions].subtype = subtype;\n\tvdev->region[vdev->num_regions].ops = ops;\n\tvdev->region[vdev->num_regions].size = size;\n\tvdev->region[vdev->num_regions].flags = flags;\n\tvdev->region[vdev->num_regions].data = data;\n\n\tvdev->num_regions++;\n\n\treturn 0;\n}\n\nstatic long vfio_pci_ioctl(void *device_data,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tunsigned long minsz;\n\n\tif (cmd == VFIO_DEVICE_GET_INFO) {\n\t\tstruct vfio_device_info info;\n\n\t\tminsz = offsetofend(struct vfio_device_info, num_irqs);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tinfo.flags = VFIO_DEVICE_FLAGS_PCI;\n\n\t\tif (vdev->reset_works)\n\t\t\tinfo.flags |= VFIO_DEVICE_FLAGS_RESET;\n\n\t\tinfo.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;\n\t\tinfo.num_irqs = VFIO_PCI_NUM_IRQS;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_GET_REGION_INFO) {\n\t\tstruct pci_dev *pdev = vdev->pdev;\n\t\tstruct vfio_region_info info;\n\t\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\t\tint i, ret;\n\n\t\tminsz = offsetofend(struct vfio_region_info, offset);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = pdev->cfg_size;\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tbreak;\n\t\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\t\tif (!info.size) {\n\t\t\t\tinfo.flags = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tif (vdev->bar_mmap_supported[info.index]) {\n\t\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_MMAP;\n\t\t\t\tif (info.index == vdev->msix_bar) {\n\t\t\t\t\tret = msix_sparse_mmap_cap(vdev, &caps);\n\t\t\t\t\tif (ret)\n\t\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\t{\n\t\t\tvoid __iomem *io;\n\t\t\tsize_t size;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.flags = 0;\n\n\t\t\t/* Report the BAR size, not the ROM size */\n\t\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\t\tif (!info.size) {\n\t\t\t\t/* Shadow ROMs appear as PCI option ROMs */\n\t\t\t\tif (pdev->resource[PCI_ROM_RESOURCE].flags &\n\t\t\t\t\t\t\tIORESOURCE_ROM_SHADOW)\n\t\t\t\t\tinfo.size = 0x20000;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Is it really there? */\n\t\t\tio = pci_map_rom(pdev, &size);\n\t\t\tif (!io || !size) {\n\t\t\t\tinfo.size = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpci_unmap_rom(pdev, io);\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ;\n\t\t\tbreak;\n\t\t}\n\t\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\t\tif (!vdev->has_vga)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = 0xc0000;\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (info.index >=\n\t\t\t    VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ti = info.index - VFIO_PCI_NUM_REGIONS;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = vdev->region[i].size;\n\t\t\tinfo.flags = vdev->region[i].flags;\n\n\t\t\tret = region_type_cap(vdev, &caps,\n\t\t\t\t\t      vdev->region[i].type,\n\t\t\t\t\t      vdev->region[i].subtype);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (caps.size) {\n\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_CAPS;\n\t\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t\t\tinfo.cap_offset = 0;\n\t\t\t} else {\n\t\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\t\tif (copy_to_user((void __user *)arg +\n\t\t\t\t\t\t  sizeof(info), caps.buf,\n\t\t\t\t\t\t  caps.size)) {\n\t\t\t\t\tkfree(caps.buf);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tinfo.cap_offset = sizeof(info);\n\t\t\t}\n\n\t\t\tkfree(caps.buf);\n\t\t}\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_GET_IRQ_INFO) {\n\t\tstruct vfio_irq_info info;\n\n\t\tminsz = offsetofend(struct vfio_irq_info, count);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_INTX_IRQ_INDEX ... VFIO_PCI_MSIX_IRQ_INDEX:\n\t\tcase VFIO_PCI_REQ_IRQ_INDEX:\n\t\t\tbreak;\n\t\tcase VFIO_PCI_ERR_IRQ_INDEX:\n\t\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\t\tbreak;\n\t\t/* pass thru to return error */\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinfo.flags = VFIO_IRQ_INFO_EVENTFD;\n\n\t\tinfo.count = vfio_pci_get_irq_count(vdev, info.index);\n\n\t\tif (info.index == VFIO_PCI_INTX_IRQ_INDEX)\n\t\t\tinfo.flags |= (VFIO_IRQ_INFO_MASKABLE |\n\t\t\t\t       VFIO_IRQ_INFO_AUTOMASKED);\n\t\telse\n\t\t\tinfo.flags |= VFIO_IRQ_INFO_NORESIZE;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_SET_IRQS) {\n\t\tstruct vfio_irq_set hdr;\n\t\tu8 *data = NULL;\n\t\tint ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_irq_set, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||\n\t\t    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |\n\t\t\t\t  VFIO_IRQ_SET_ACTION_TYPE_MASK))\n\t\t\treturn -EINVAL;\n\n\t\tif (!(hdr.flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\t\tsize_t size;\n\t\t\tint max = vfio_pci_get_irq_count(vdev, hdr.index);\n\n\t\t\tif (hdr.flags & VFIO_IRQ_SET_DATA_BOOL)\n\t\t\t\tsize = sizeof(uint8_t);\n\t\t\telse if (hdr.flags & VFIO_IRQ_SET_DATA_EVENTFD)\n\t\t\t\tsize = sizeof(int32_t);\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (hdr.argsz - minsz < hdr.count * size ||\n\t\t\t    hdr.start >= max || hdr.start + hdr.count > max)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdata = memdup_user((void __user *)(arg + minsz),\n\t\t\t\t\t   hdr.count * size);\n\t\t\tif (IS_ERR(data))\n\t\t\t\treturn PTR_ERR(data);\n\t\t}\n\n\t\tmutex_lock(&vdev->igate);\n\n\t\tret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index,\n\t\t\t\t\t      hdr.start, hdr.count, data);\n\n\t\tmutex_unlock(&vdev->igate);\n\t\tkfree(data);\n\n\t\treturn ret;\n\n\t} else if (cmd == VFIO_DEVICE_RESET) {\n\t\treturn vdev->reset_works ?\n\t\t\tpci_try_reset_function(vdev->pdev) : -EINVAL;\n\n\t} else if (cmd == VFIO_DEVICE_GET_PCI_HOT_RESET_INFO) {\n\t\tstruct vfio_pci_hot_reset_info hdr;\n\t\tstruct vfio_pci_fill_info fill = { 0 };\n\t\tstruct vfio_pci_dependent_device *devices = NULL;\n\t\tbool slot = false;\n\t\tint ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_pci_hot_reset_info, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\thdr.flags = 0;\n\n\t\t/* Can we do a slot or bus reset or neither? */\n\t\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\t\tslot = true;\n\t\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\t\treturn -ENODEV;\n\n\t\t/* How many devices are affected? */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_count_devs,\n\t\t\t\t\t\t    &fill.max, slot);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tWARN_ON(!fill.max); /* Should always be at least one */\n\n\t\t/*\n\t\t * If there's enough space, fill it now, otherwise return\n\t\t * -ENOSPC and the number of devices affected.\n\t\t */\n\t\tif (hdr.argsz < sizeof(hdr) + (fill.max * sizeof(*devices))) {\n\t\t\tret = -ENOSPC;\n\t\t\thdr.count = fill.max;\n\t\t\tgoto reset_info_exit;\n\t\t}\n\n\t\tdevices = kcalloc(fill.max, sizeof(*devices), GFP_KERNEL);\n\t\tif (!devices)\n\t\t\treturn -ENOMEM;\n\n\t\tfill.devices = devices;\n\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_fill_devs,\n\t\t\t\t\t\t    &fill, slot);\n\n\t\t/*\n\t\t * If a device was removed between counting and filling,\n\t\t * we may come up short of fill.max.  If a device was\n\t\t * added, we'll have a return of -EAGAIN above.\n\t\t */\n\t\tif (!ret)\n\t\t\thdr.count = fill.cur;\n\nreset_info_exit:\n\t\tif (copy_to_user((void __user *)arg, &hdr, minsz))\n\t\t\tret = -EFAULT;\n\n\t\tif (!ret) {\n\t\t\tif (copy_to_user((void __user *)(arg + minsz), devices,\n\t\t\t\t\t hdr.count * sizeof(*devices)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\n\t\tkfree(devices);\n\t\treturn ret;\n\n\t} else if (cmd == VFIO_DEVICE_PCI_HOT_RESET) {\n\t\tstruct vfio_pci_hot_reset hdr;\n\t\tint32_t *group_fds;\n\t\tstruct vfio_pci_group_entry *groups;\n\t\tstruct vfio_pci_group_info info;\n\t\tbool slot = false;\n\t\tint i, count = 0, ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_pci_hot_reset, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz || hdr.flags)\n\t\t\treturn -EINVAL;\n\n\t\t/* Can we do a slot or bus reset or neither? */\n\t\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\t\tslot = true;\n\t\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\t\treturn -ENODEV;\n\n\t\t/*\n\t\t * We can't let userspace give us an arbitrarily large\n\t\t * buffer to copy, so verify how many we think there\n\t\t * could be.  Note groups can have multiple devices so\n\t\t * one group per device is the max.\n\t\t */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_count_devs,\n\t\t\t\t\t\t    &count, slot);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* Somewhere between 1 and count is OK */\n\t\tif (!hdr.count || hdr.count > count)\n\t\t\treturn -EINVAL;\n\n\t\tgroup_fds = kcalloc(hdr.count, sizeof(*group_fds), GFP_KERNEL);\n\t\tgroups = kcalloc(hdr.count, sizeof(*groups), GFP_KERNEL);\n\t\tif (!group_fds || !groups) {\n\t\t\tkfree(group_fds);\n\t\t\tkfree(groups);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (copy_from_user(group_fds, (void __user *)(arg + minsz),\n\t\t\t\t   hdr.count * sizeof(*group_fds))) {\n\t\t\tkfree(group_fds);\n\t\t\tkfree(groups);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t/*\n\t\t * For each group_fd, get the group through the vfio external\n\t\t * user interface and store the group and iommu ID.  This\n\t\t * ensures the group is held across the reset.\n\t\t */\n\t\tfor (i = 0; i < hdr.count; i++) {\n\t\t\tstruct vfio_group *group;\n\t\t\tstruct fd f = fdget(group_fds[i]);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgroup = vfio_group_get_external_user(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(group)) {\n\t\t\t\tret = PTR_ERR(group);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgroups[i].group = group;\n\t\t\tgroups[i].id = vfio_external_user_iommu_id(group);\n\t\t}\n\n\t\tkfree(group_fds);\n\n\t\t/* release reference to groups on error */\n\t\tif (ret)\n\t\t\tgoto hot_reset_release;\n\n\t\tinfo.count = hdr.count;\n\t\tinfo.groups = groups;\n\n\t\t/*\n\t\t * Test whether all the affected devices are contained\n\t\t * by the set of groups provided by the user.\n\t\t */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_validate_devs,\n\t\t\t\t\t\t    &info, slot);\n\t\tif (!ret)\n\t\t\t/* User has access, do the reset */\n\t\t\tret = slot ? pci_try_reset_slot(vdev->pdev->slot) :\n\t\t\t\t     pci_try_reset_bus(vdev->pdev->bus);\n\nhot_reset_release:\n\t\tfor (i--; i >= 0; i--)\n\t\t\tvfio_group_put_external_user(groups[i].group);\n\n\t\tkfree(groups);\n\t\treturn ret;\n\t}\n\n\treturn -ENOTTY;\n}\n\nstatic ssize_t vfio_pci_rw(void *device_data, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos, bool iswrite)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tif (index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\treturn -EINVAL;\n\n\tswitch (index) {\n\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\treturn vfio_pci_config_rw(vdev, buf, count, ppos, iswrite);\n\n\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\tif (iswrite)\n\t\t\treturn -EINVAL;\n\t\treturn vfio_pci_bar_rw(vdev, buf, count, ppos, false);\n\n\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\treturn vfio_pci_bar_rw(vdev, buf, count, ppos, iswrite);\n\n\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\treturn vfio_pci_vga_rw(vdev, buf, count, ppos, iswrite);\n\tdefault:\n\t\tindex -= VFIO_PCI_NUM_REGIONS;\n\t\treturn vdev->region[index].ops->rw(vdev, buf,\n\t\t\t\t\t\t   count, ppos, iswrite);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic ssize_t vfio_pci_read(void *device_data, char __user *buf,\n\t\t\t     size_t count, loff_t *ppos)\n{\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(device_data, buf, count, ppos, false);\n}\n\nstatic ssize_t vfio_pci_write(void *device_data, const char __user *buf,\n\t\t\t      size_t count, loff_t *ppos)\n{\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(device_data, (char __user *)buf, count, ppos, true);\n}\n\nstatic int vfio_pci_mmap(void *device_data, struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned int index;\n\tu64 phys_len, req_len, pgoff, req_start;\n\tint ret;\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\tif (!vdev->bar_mmap_supported[index])\n\t\treturn -EINVAL;\n\n\tphys_len = PAGE_ALIGN(pci_resource_len(pdev, index));\n\treq_len = vma->vm_end - vma->vm_start;\n\tpgoff = vma->vm_pgoff &\n\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\treq_start = pgoff << PAGE_SHIFT;\n\n\tif (req_start + req_len > phys_len)\n\t\treturn -EINVAL;\n\n\tif (index == vdev->msix_bar) {\n\t\t/*\n\t\t * Disallow mmaps overlapping the MSI-X table; users don't\n\t\t * get to touch this directly.  We could find somewhere\n\t\t * else to map the overlap, but page granularity is only\n\t\t * a recommendation, not a requirement, so the user needs\n\t\t * to know which bits are real.  Requiring them to mmap\n\t\t * around the table makes that clear.\n\t\t */\n\n\t\t/* If neither entirely above nor below, then it overlaps */\n\t\tif (!(req_start >= vdev->msix_offset + vdev->msix_size ||\n\t\t      req_start + req_len <= vdev->msix_offset))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Even though we don't make use of the barmap for the mmap,\n\t * we need to request the region and the barmap tracks that.\n\t */\n\tif (!vdev->barmap[index]) {\n\t\tret = pci_request_selected_regions(pdev,\n\t\t\t\t\t\t   1 << index, \"vfio-pci\");\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvdev->barmap[index] = pci_iomap(pdev, index, 0);\n\t}\n\n\tvma->vm_private_data = vdev;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tvma->vm_pgoff = (pci_resource_start(pdev, index) >> PAGE_SHIFT) + pgoff;\n\n\treturn remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t       req_len, vma->vm_page_prot);\n}\n\nstatic void vfio_pci_request(void *device_data, unsigned int count)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->req_trigger) {\n\t\tif (!(count % 10))\n\t\t\tdev_notice_ratelimited(&vdev->pdev->dev,\n\t\t\t\t\"Relaying device request to user (#%u)\\n\",\n\t\t\t\tcount);\n\t\teventfd_signal(vdev->req_trigger, 1);\n\t} else if (count == 0) {\n\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t\"No device request channel registered, blocked until released by user\\n\");\n\t}\n\n\tmutex_unlock(&vdev->igate);\n}\n\nstatic const struct vfio_device_ops vfio_pci_ops = {\n\t.name\t\t= \"vfio-pci\",\n\t.open\t\t= vfio_pci_open,\n\t.release\t= vfio_pci_release,\n\t.ioctl\t\t= vfio_pci_ioctl,\n\t.read\t\t= vfio_pci_read,\n\t.write\t\t= vfio_pci_write,\n\t.mmap\t\t= vfio_pci_mmap,\n\t.request\t= vfio_pci_request,\n};\n\nstatic int vfio_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct vfio_pci_device *vdev;\n\tstruct iommu_group *group;\n\tint ret;\n\n\tif (pdev->hdr_type != PCI_HEADER_TYPE_NORMAL)\n\t\treturn -EINVAL;\n\n\tgroup = vfio_iommu_group_get(&pdev->dev);\n\tif (!group)\n\t\treturn -EINVAL;\n\n\tvdev = kzalloc(sizeof(*vdev), GFP_KERNEL);\n\tif (!vdev) {\n\t\tvfio_iommu_group_put(group, &pdev->dev);\n\t\treturn -ENOMEM;\n\t}\n\n\tvdev->pdev = pdev;\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tmutex_init(&vdev->igate);\n\tspin_lock_init(&vdev->irqlock);\n\n\tret = vfio_add_group_dev(&pdev->dev, &vfio_pci_ops, vdev);\n\tif (ret) {\n\t\tvfio_iommu_group_put(group, &pdev->dev);\n\t\tkfree(vdev);\n\t\treturn ret;\n\t}\n\n\tif (vfio_pci_is_vga(pdev)) {\n\t\tvga_client_register(pdev, vdev, NULL, vfio_pci_set_vga_decode);\n\t\tvga_set_legacy_decoding(pdev,\n\t\t\t\t\tvfio_pci_set_vga_decode(vdev, false));\n\t}\n\n\tif (!disable_idle_d3) {\n\t\t/*\n\t\t * pci-core sets the device power state to an unknown value at\n\t\t * bootup and after being removed from a driver.  The only\n\t\t * transition it allows from this unknown state is to D0, which\n\t\t * typically happens when a driver calls pci_enable_device().\n\t\t * We're not ready to enable the device yet, but we do want to\n\t\t * be able to get to D3.  Therefore first do a D0 transition\n\t\t * before going to D3.\n\t\t */\n\t\tpci_set_power_state(pdev, PCI_D0);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n\n\treturn ret;\n}\n\nstatic void vfio_pci_remove(struct pci_dev *pdev)\n{\n\tstruct vfio_pci_device *vdev;\n\n\tvdev = vfio_del_group_dev(&pdev->dev);\n\tif (!vdev)\n\t\treturn;\n\n\tvfio_iommu_group_put(pdev->dev.iommu_group, &pdev->dev);\n\tkfree(vdev->region);\n\tkfree(vdev);\n\n\tif (vfio_pci_is_vga(pdev)) {\n\t\tvga_client_register(pdev, NULL, NULL, NULL);\n\t\tvga_set_legacy_decoding(pdev,\n\t\t\t\tVGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t\t\tVGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM);\n\t}\n\n\tif (!disable_idle_d3)\n\t\tpci_set_power_state(pdev, PCI_D0);\n}\n\nstatic pci_ers_result_t vfio_pci_aer_err_detected(struct pci_dev *pdev,\n\t\t\t\t\t\t  pci_channel_state_t state)\n{\n\tstruct vfio_pci_device *vdev;\n\tstruct vfio_device *device;\n\n\tdevice = vfio_device_get_from_dev(&pdev->dev);\n\tif (device == NULL)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tvdev = vfio_device_data(device);\n\tif (vdev == NULL) {\n\t\tvfio_device_put(device);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->err_trigger)\n\t\teventfd_signal(vdev->err_trigger, 1);\n\n\tmutex_unlock(&vdev->igate);\n\n\tvfio_device_put(device);\n\n\treturn PCI_ERS_RESULT_CAN_RECOVER;\n}\n\nstatic const struct pci_error_handlers vfio_err_handlers = {\n\t.error_detected = vfio_pci_aer_err_detected,\n};\n\nstatic struct pci_driver vfio_pci_driver = {\n\t.name\t\t= \"vfio-pci\",\n\t.id_table\t= NULL, /* only dynamic ids */\n\t.probe\t\t= vfio_pci_probe,\n\t.remove\t\t= vfio_pci_remove,\n\t.err_handler\t= &vfio_err_handlers,\n};\n\nstruct vfio_devices {\n\tstruct vfio_device **devices;\n\tint cur_index;\n\tint max_index;\n};\n\nstatic int vfio_pci_get_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_devices *devs = data;\n\tstruct vfio_device *device;\n\n\tif (devs->cur_index == devs->max_index)\n\t\treturn -ENOSPC;\n\n\tdevice = vfio_device_get_from_dev(&pdev->dev);\n\tif (!device)\n\t\treturn -EINVAL;\n\n\tif (pci_dev_driver(pdev) != &vfio_pci_driver) {\n\t\tvfio_device_put(device);\n\t\treturn -EBUSY;\n\t}\n\n\tdevs->devices[devs->cur_index++] = device;\n\treturn 0;\n}\n\n/*\n * Attempt to do a bus/slot reset if there are devices affected by a reset for\n * this device that are needs_reset and all of the affected devices are unused\n * (!refcnt).  Callers are required to hold driver_lock when calling this to\n * prevent device opens and concurrent bus reset attempts.  We prevent device\n * unbinds by acquiring and holding a reference to the vfio_device.\n *\n * NB: vfio-core considers a group to be viable even if some devices are\n * bound to drivers like pci-stub or pcieport.  Here we require all devices\n * to be bound to vfio_pci since that's the only way we can be sure they\n * stay put.\n */\nstatic void vfio_pci_try_bus_reset(struct vfio_pci_device *vdev)\n{\n\tstruct vfio_devices devs = { .cur_index = 0 };\n\tint i = 0, ret = -EINVAL;\n\tbool needs_reset = false, slot = false;\n\tstruct vfio_pci_device *tmp;\n\n\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\tslot = true;\n\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\treturn;\n\n\tif (vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_count_devs,\n\t\t\t\t\t  &i, slot) || !i)\n\t\treturn;\n\n\tdevs.max_index = i;\n\tdevs.devices = kcalloc(i, sizeof(struct vfio_device *), GFP_KERNEL);\n\tif (!devs.devices)\n\t\treturn;\n\n\tif (vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t  vfio_pci_get_devs, &devs, slot))\n\t\tgoto put_devs;\n\n\tfor (i = 0; i < devs.cur_index; i++) {\n\t\ttmp = vfio_device_data(devs.devices[i]);\n\t\tif (tmp->needs_reset)\n\t\t\tneeds_reset = true;\n\t\tif (tmp->refcnt)\n\t\t\tgoto put_devs;\n\t}\n\n\tif (needs_reset)\n\t\tret = slot ? pci_try_reset_slot(vdev->pdev->slot) :\n\t\t\t     pci_try_reset_bus(vdev->pdev->bus);\n\nput_devs:\n\tfor (i = 0; i < devs.cur_index; i++) {\n\t\ttmp = vfio_device_data(devs.devices[i]);\n\t\tif (!ret)\n\t\t\ttmp->needs_reset = false;\n\n\t\tif (!tmp->refcnt && !disable_idle_d3)\n\t\t\tpci_set_power_state(tmp->pdev, PCI_D3hot);\n\n\t\tvfio_device_put(devs.devices[i]);\n\t}\n\n\tkfree(devs.devices);\n}\n\nstatic void __exit vfio_pci_cleanup(void)\n{\n\tpci_unregister_driver(&vfio_pci_driver);\n\tvfio_pci_uninit_perm_bits();\n}\n\nstatic void __init vfio_pci_fill_ids(void)\n{\n\tchar *p, *id;\n\tint rc;\n\n\t/* no ids passed actually */\n\tif (ids[0] == '\\0')\n\t\treturn;\n\n\t/* add ids specified in the module parameter */\n\tp = ids;\n\twhile ((id = strsep(&p, \",\"))) {\n\t\tunsigned int vendor, device, subvendor = PCI_ANY_ID,\n\t\t\tsubdevice = PCI_ANY_ID, class = 0, class_mask = 0;\n\t\tint fields;\n\n\t\tif (!strlen(id))\n\t\t\tcontinue;\n\n\t\tfields = sscanf(id, \"%x:%x:%x:%x:%x:%x\",\n\t\t\t\t&vendor, &device, &subvendor, &subdevice,\n\t\t\t\t&class, &class_mask);\n\n\t\tif (fields < 2) {\n\t\t\tpr_warn(\"invalid id string \\\"%s\\\"\\n\", id);\n\t\t\tcontinue;\n\t\t}\n\n\t\trc = pci_add_dynid(&vfio_pci_driver, vendor, device,\n\t\t\t\t   subvendor, subdevice, class, class_mask, 0);\n\t\tif (rc)\n\t\t\tpr_warn(\"failed to add dynamic id [%04hx:%04hx[%04hx:%04hx]] class %#08x/%08x (%d)\\n\",\n\t\t\t\tvendor, device, subvendor, subdevice,\n\t\t\t\tclass, class_mask, rc);\n\t\telse\n\t\t\tpr_info(\"add [%04hx:%04hx[%04hx:%04hx]] class %#08x/%08x\\n\",\n\t\t\t\tvendor, device, subvendor, subdevice,\n\t\t\t\tclass, class_mask);\n\t}\n}\n\nstatic int __init vfio_pci_init(void)\n{\n\tint ret;\n\n\t/* Allocate shared config space permision data used by all devices */\n\tret = vfio_pci_init_perm_bits();\n\tif (ret)\n\t\treturn ret;\n\n\t/* Register and scan for devices */\n\tret = pci_register_driver(&vfio_pci_driver);\n\tif (ret)\n\t\tgoto out_driver;\n\n\tvfio_pci_fill_ids();\n\n\treturn 0;\n\nout_driver:\n\tvfio_pci_uninit_perm_bits();\n\treturn ret;\n}\n\nmodule_init(vfio_pci_init);\nmodule_exit(vfio_pci_cleanup);\n\nMODULE_VERSION(DRIVER_VERSION);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(DRIVER_AUTHOR);\nMODULE_DESCRIPTION(DRIVER_DESC);\n", "/*\n * VFIO PCI interrupt handling\n *\n * Copyright (C) 2012 Red Hat, Inc.  All rights reserved.\n *     Author: Alex Williamson <alex.williamson@redhat.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * Derived from original vfio:\n * Copyright 2010 Cisco Systems, Inc.  All rights reserved.\n * Author: Tom Lyon, pugs@cisco.com\n */\n\n#include <linux/device.h>\n#include <linux/interrupt.h>\n#include <linux/eventfd.h>\n#include <linux/msi.h>\n#include <linux/pci.h>\n#include <linux/file.h>\n#include <linux/vfio.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n\n#include \"vfio_pci_private.h\"\n\n/*\n * INTx\n */\nstatic void vfio_send_intx_eventfd(void *opaque, void *unused)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\n\tif (likely(is_intx(vdev) && !vdev->virq_disabled))\n\t\teventfd_signal(vdev->ctx[0].trigger, 1);\n}\n\nvoid vfio_pci_intx_mask(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\t/*\n\t * Masking can come from interrupt, ioctl, or config space\n\t * via INTx disable.  The latter means this can get called\n\t * even when not using intx delivery.  In this case, just\n\t * try to have the physical bit follow the virtual bit.\n\t */\n\tif (unlikely(!is_intx(vdev))) {\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 0);\n\t} else if (!vdev->ctx[0].masked) {\n\t\t/*\n\t\t * Can't use check_and_mask here because we always want to\n\t\t * mask, not just when something is pending.\n\t\t */\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 0);\n\t\telse\n\t\t\tdisable_irq_nosync(pdev->irq);\n\n\t\tvdev->ctx[0].masked = true;\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n}\n\n/*\n * If this is triggered by an eventfd, we can't call eventfd_signal\n * or else we'll deadlock on the eventfd wait queue.  Return >0 when\n * a signal is necessary, which can then be handled via a work queue\n * or directly depending on the caller.\n */\nstatic int vfio_pci_intx_unmask_handler(void *opaque, void *unused)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\t/*\n\t * Unmasking comes from ioctl or config, so again, have the\n\t * physical bit follow the virtual even when not using INTx.\n\t */\n\tif (unlikely(!is_intx(vdev))) {\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 1);\n\t} else if (vdev->ctx[0].masked && !vdev->virq_disabled) {\n\t\t/*\n\t\t * A pending interrupt here would immediately trigger,\n\t\t * but we can avoid that overhead by just re-sending\n\t\t * the interrupt to the user.\n\t\t */\n\t\tif (vdev->pci_2_3) {\n\t\t\tif (!pci_check_and_unmask_intx(pdev))\n\t\t\t\tret = 1;\n\t\t} else\n\t\t\tenable_irq(pdev->irq);\n\n\t\tvdev->ctx[0].masked = (ret > 0);\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\treturn ret;\n}\n\nvoid vfio_pci_intx_unmask(struct vfio_pci_device *vdev)\n{\n\tif (vfio_pci_intx_unmask_handler(vdev, NULL) > 0)\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n}\n\nstatic irqreturn_t vfio_intx_handler(int irq, void *dev_id)\n{\n\tstruct vfio_pci_device *vdev = dev_id;\n\tunsigned long flags;\n\tint ret = IRQ_NONE;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\tif (!vdev->pci_2_3) {\n\t\tdisable_irq_nosync(vdev->pdev->irq);\n\t\tvdev->ctx[0].masked = true;\n\t\tret = IRQ_HANDLED;\n\t} else if (!vdev->ctx[0].masked &&  /* may be shared */\n\t\t   pci_check_and_mask_intx(vdev->pdev)) {\n\t\tvdev->ctx[0].masked = true;\n\t\tret = IRQ_HANDLED;\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\tif (ret == IRQ_HANDLED)\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n\n\treturn ret;\n}\n\nstatic int vfio_intx_enable(struct vfio_pci_device *vdev)\n{\n\tif (!is_irq_none(vdev))\n\t\treturn -EINVAL;\n\n\tif (!vdev->pdev->irq)\n\t\treturn -ENODEV;\n\n\tvdev->ctx = kzalloc(sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);\n\tif (!vdev->ctx)\n\t\treturn -ENOMEM;\n\n\tvdev->num_ctx = 1;\n\n\t/*\n\t * If the virtual interrupt is masked, restore it.  Devices\n\t * supporting DisINTx can be masked at the hardware level\n\t * here, non-PCI-2.3 devices will have to wait until the\n\t * interrupt is enabled.\n\t */\n\tvdev->ctx[0].masked = vdev->virq_disabled;\n\tif (vdev->pci_2_3)\n\t\tpci_intx(vdev->pdev, !vdev->ctx[0].masked);\n\n\tvdev->irq_type = VFIO_PCI_INTX_IRQ_INDEX;\n\n\treturn 0;\n}\n\nstatic int vfio_intx_set_signal(struct vfio_pci_device *vdev, int fd)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long irqflags = IRQF_SHARED;\n\tstruct eventfd_ctx *trigger;\n\tunsigned long flags;\n\tint ret;\n\n\tif (vdev->ctx[0].trigger) {\n\t\tfree_irq(pdev->irq, vdev);\n\t\tkfree(vdev->ctx[0].name);\n\t\teventfd_ctx_put(vdev->ctx[0].trigger);\n\t\tvdev->ctx[0].trigger = NULL;\n\t}\n\n\tif (fd < 0) /* Disable only */\n\t\treturn 0;\n\n\tvdev->ctx[0].name = kasprintf(GFP_KERNEL, \"vfio-intx(%s)\",\n\t\t\t\t      pci_name(pdev));\n\tif (!vdev->ctx[0].name)\n\t\treturn -ENOMEM;\n\n\ttrigger = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(trigger)) {\n\t\tkfree(vdev->ctx[0].name);\n\t\treturn PTR_ERR(trigger);\n\t}\n\n\tvdev->ctx[0].trigger = trigger;\n\n\tif (!vdev->pci_2_3)\n\t\tirqflags = 0;\n\n\tret = request_irq(pdev->irq, vfio_intx_handler,\n\t\t\t  irqflags, vdev->ctx[0].name, vdev);\n\tif (ret) {\n\t\tvdev->ctx[0].trigger = NULL;\n\t\tkfree(vdev->ctx[0].name);\n\t\teventfd_ctx_put(trigger);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * INTx disable will stick across the new irq setup,\n\t * disable_irq won't.\n\t */\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\tif (!vdev->pci_2_3 && vdev->ctx[0].masked)\n\t\tdisable_irq_nosync(pdev->irq);\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\treturn 0;\n}\n\nstatic void vfio_intx_disable(struct vfio_pci_device *vdev)\n{\n\tvfio_virqfd_disable(&vdev->ctx[0].unmask);\n\tvfio_virqfd_disable(&vdev->ctx[0].mask);\n\tvfio_intx_set_signal(vdev, -1);\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tvdev->num_ctx = 0;\n\tkfree(vdev->ctx);\n}\n\n/*\n * MSI/MSI-X\n */\nstatic irqreturn_t vfio_msihandler(int irq, void *arg)\n{\n\tstruct eventfd_ctx *trigger = arg;\n\n\teventfd_signal(trigger, 1);\n\treturn IRQ_HANDLED;\n}\n\nstatic int vfio_msi_enable(struct vfio_pci_device *vdev, int nvec, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned int flag = msix ? PCI_IRQ_MSIX : PCI_IRQ_MSI;\n\tint ret;\n\n\tif (!is_irq_none(vdev))\n\t\treturn -EINVAL;\n\n\tvdev->ctx = kzalloc(nvec * sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);\n\tif (!vdev->ctx)\n\t\treturn -ENOMEM;\n\n\t/* return the number of supported vectors if we can't get all: */\n\tret = pci_alloc_irq_vectors(pdev, 1, nvec, flag);\n\tif (ret < nvec) {\n\t\tif (ret > 0)\n\t\t\tpci_free_irq_vectors(pdev);\n\t\tkfree(vdev->ctx);\n\t\treturn ret;\n\t}\n\n\tvdev->num_ctx = nvec;\n\tvdev->irq_type = msix ? VFIO_PCI_MSIX_IRQ_INDEX :\n\t\t\t\tVFIO_PCI_MSI_IRQ_INDEX;\n\n\tif (!msix) {\n\t\t/*\n\t\t * Compute the virtual hardware field for max msi vectors -\n\t\t * it is the log base 2 of the number of vectors.\n\t\t */\n\t\tvdev->msi_qmax = fls(nvec * 2 - 1) - 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,\n\t\t\t\t      int vector, int fd, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct eventfd_ctx *trigger;\n\tint irq, ret;\n\n\tif (vector < 0 || vector >= vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tirq = pci_irq_vector(pdev, vector);\n\n\tif (vdev->ctx[vector].trigger) {\n\t\tfree_irq(irq, vdev->ctx[vector].trigger);\n\t\tirq_bypass_unregister_producer(&vdev->ctx[vector].producer);\n\t\tkfree(vdev->ctx[vector].name);\n\t\teventfd_ctx_put(vdev->ctx[vector].trigger);\n\t\tvdev->ctx[vector].trigger = NULL;\n\t}\n\n\tif (fd < 0)\n\t\treturn 0;\n\n\tvdev->ctx[vector].name = kasprintf(GFP_KERNEL, \"vfio-msi%s[%d](%s)\",\n\t\t\t\t\t   msix ? \"x\" : \"\", vector,\n\t\t\t\t\t   pci_name(pdev));\n\tif (!vdev->ctx[vector].name)\n\t\treturn -ENOMEM;\n\n\ttrigger = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(trigger)) {\n\t\tkfree(vdev->ctx[vector].name);\n\t\treturn PTR_ERR(trigger);\n\t}\n\n\t/*\n\t * The MSIx vector table resides in device memory which may be cleared\n\t * via backdoor resets. We don't allow direct access to the vector\n\t * table so even if a userspace driver attempts to save/restore around\n\t * such a reset it would be unsuccessful. To avoid this, restore the\n\t * cached value of the message prior to enabling.\n\t */\n\tif (msix) {\n\t\tstruct msi_msg msg;\n\n\t\tget_cached_msi_msg(irq, &msg);\n\t\tpci_write_msi_msg(irq, &msg);\n\t}\n\n\tret = request_irq(irq, vfio_msihandler, 0,\n\t\t\t  vdev->ctx[vector].name, trigger);\n\tif (ret) {\n\t\tkfree(vdev->ctx[vector].name);\n\t\teventfd_ctx_put(trigger);\n\t\treturn ret;\n\t}\n\n\tvdev->ctx[vector].producer.token = trigger;\n\tvdev->ctx[vector].producer.irq = irq;\n\tret = irq_bypass_register_producer(&vdev->ctx[vector].producer);\n\tif (unlikely(ret))\n\t\tdev_info(&pdev->dev,\n\t\t\"irq bypass producer (token %p) registration fails: %d\\n\",\n\t\tvdev->ctx[vector].producer.token, ret);\n\n\tvdev->ctx[vector].trigger = trigger;\n\n\treturn 0;\n}\n\nstatic int vfio_msi_set_block(struct vfio_pci_device *vdev, unsigned start,\n\t\t\t      unsigned count, int32_t *fds, bool msix)\n{\n\tint i, j, ret = 0;\n\n\tif (start >= vdev->num_ctx || start + count > vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tfor (i = 0, j = start; i < count && !ret; i++, j++) {\n\t\tint fd = fds ? fds[i] : -1;\n\t\tret = vfio_msi_set_vector_signal(vdev, j, fd, msix);\n\t}\n\n\tif (ret) {\n\t\tfor (--j; j >= (int)start; j--)\n\t\t\tvfio_msi_set_vector_signal(vdev, j, -1, msix);\n\t}\n\n\treturn ret;\n}\n\nstatic void vfio_msi_disable(struct vfio_pci_device *vdev, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint i;\n\n\tfor (i = 0; i < vdev->num_ctx; i++) {\n\t\tvfio_virqfd_disable(&vdev->ctx[i].unmask);\n\t\tvfio_virqfd_disable(&vdev->ctx[i].mask);\n\t}\n\n\tvfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);\n\n\tpci_free_irq_vectors(pdev);\n\n\t/*\n\t * Both disable paths above use pci_intx_for_msi() to clear DisINTx\n\t * via their shutdown paths.  Restore for NoINTx devices.\n\t */\n\tif (vdev->nointx)\n\t\tpci_intx(pdev, 0);\n\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tvdev->num_ctx = 0;\n\tkfree(vdev->ctx);\n}\n\n/*\n * IOCTL support\n */\nstatic int vfio_pci_set_intx_unmask(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (!is_intx(vdev) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_pci_intx_unmask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t unmask = *(uint8_t *)data;\n\t\tif (unmask)\n\t\t\tvfio_pci_intx_unmask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd = *(int32_t *)data;\n\t\tif (fd >= 0)\n\t\t\treturn vfio_virqfd_enable((void *) vdev,\n\t\t\t\t\t\t  vfio_pci_intx_unmask_handler,\n\t\t\t\t\t\t  vfio_send_intx_eventfd, NULL,\n\t\t\t\t\t\t  &vdev->ctx[0].unmask, fd);\n\n\t\tvfio_virqfd_disable(&vdev->ctx[0].unmask);\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_set_intx_mask(struct vfio_pci_device *vdev,\n\t\t\t\t  unsigned index, unsigned start,\n\t\t\t\t  unsigned count, uint32_t flags, void *data)\n{\n\tif (!is_intx(vdev) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_pci_intx_mask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t mask = *(uint8_t *)data;\n\t\tif (mask)\n\t\t\tvfio_pci_intx_mask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\treturn -ENOTTY; /* XXX implement me */\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_set_intx_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t     unsigned index, unsigned start,\n\t\t\t\t     unsigned count, uint32_t flags, void *data)\n{\n\tif (is_intx(vdev) && !count && (flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\tvfio_intx_disable(vdev);\n\t\treturn 0;\n\t}\n\n\tif (!(is_intx(vdev) || is_irq_none(vdev)) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd = *(int32_t *)data;\n\t\tint ret;\n\n\t\tif (is_intx(vdev))\n\t\t\treturn vfio_intx_set_signal(vdev, fd);\n\n\t\tret = vfio_intx_enable(vdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vfio_intx_set_signal(vdev, fd);\n\t\tif (ret)\n\t\t\tvfio_intx_disable(vdev);\n\n\t\treturn ret;\n\t}\n\n\tif (!is_intx(vdev))\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t trigger = *(uint8_t *)data;\n\t\tif (trigger)\n\t\t\tvfio_send_intx_eventfd(vdev, NULL);\n\t}\n\treturn 0;\n}\n\nstatic int vfio_pci_set_msi_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tint i;\n\tbool msix = (index == VFIO_PCI_MSIX_IRQ_INDEX) ? true : false;\n\n\tif (irq_is(vdev, index) && !count && (flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\tvfio_msi_disable(vdev, msix);\n\t\treturn 0;\n\t}\n\n\tif (!(irq_is(vdev, index) || is_irq_none(vdev)))\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t *fds = data;\n\t\tint ret;\n\n\t\tif (vdev->irq_type == index)\n\t\t\treturn vfio_msi_set_block(vdev, start, count,\n\t\t\t\t\t\t  fds, msix);\n\n\t\tret = vfio_msi_enable(vdev, start + count, msix);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vfio_msi_set_block(vdev, start, count, fds, msix);\n\t\tif (ret)\n\t\t\tvfio_msi_disable(vdev, msix);\n\n\t\treturn ret;\n\t}\n\n\tif (!irq_is(vdev, index) || start + count > vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tfor (i = start; i < start + count; i++) {\n\t\tif (!vdev->ctx[i].trigger)\n\t\t\tcontinue;\n\t\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\t\teventfd_signal(vdev->ctx[i].trigger, 1);\n\t\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\t\tuint8_t *bools = data;\n\t\t\tif (bools[i - start])\n\t\t\t\teventfd_signal(vdev->ctx[i].trigger, 1);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int vfio_pci_set_ctx_trigger_single(struct eventfd_ctx **ctx,\n\t\t\t\t\t   unsigned int count, uint32_t flags,\n\t\t\t\t\t   void *data)\n{\n\t/* DATA_NONE/DATA_BOOL enables loopback testing */\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tif (*ctx) {\n\t\t\tif (count) {\n\t\t\t\teventfd_signal(*ctx, 1);\n\t\t\t} else {\n\t\t\t\teventfd_ctx_put(*ctx);\n\t\t\t\t*ctx = NULL;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t trigger;\n\n\t\tif (!count)\n\t\t\treturn -EINVAL;\n\n\t\ttrigger = *(uint8_t *)data;\n\t\tif (trigger && *ctx)\n\t\t\teventfd_signal(*ctx, 1);\n\n\t\treturn 0;\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd;\n\n\t\tif (!count)\n\t\t\treturn -EINVAL;\n\n\t\tfd = *(int32_t *)data;\n\t\tif (fd == -1) {\n\t\t\tif (*ctx)\n\t\t\t\teventfd_ctx_put(*ctx);\n\t\t\t*ctx = NULL;\n\t\t} else if (fd >= 0) {\n\t\t\tstruct eventfd_ctx *efdctx;\n\n\t\t\tefdctx = eventfd_ctx_fdget(fd);\n\t\t\tif (IS_ERR(efdctx))\n\t\t\t\treturn PTR_ERR(efdctx);\n\n\t\t\tif (*ctx)\n\t\t\t\teventfd_ctx_put(*ctx);\n\n\t\t\t*ctx = efdctx;\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int vfio_pci_set_err_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (index != VFIO_PCI_ERR_IRQ_INDEX || start != 0 || count > 1)\n\t\treturn -EINVAL;\n\n\treturn vfio_pci_set_ctx_trigger_single(&vdev->err_trigger,\n\t\t\t\t\t       count, flags, data);\n}\n\nstatic int vfio_pci_set_req_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (index != VFIO_PCI_REQ_IRQ_INDEX || start != 0 || count > 1)\n\t\treturn -EINVAL;\n\n\treturn vfio_pci_set_ctx_trigger_single(&vdev->req_trigger,\n\t\t\t\t\t       count, flags, data);\n}\n\nint vfio_pci_set_irqs_ioctl(struct vfio_pci_device *vdev, uint32_t flags,\n\t\t\t    unsigned index, unsigned start, unsigned count,\n\t\t\t    void *data)\n{\n\tint (*func)(struct vfio_pci_device *vdev, unsigned index,\n\t\t    unsigned start, unsigned count, uint32_t flags,\n\t\t    void *data) = NULL;\n\n\tswitch (index) {\n\tcase VFIO_PCI_INTX_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\t\tfunc = vfio_pci_set_intx_mask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\tfunc = vfio_pci_set_intx_unmask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_intx_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_MSI_IRQ_INDEX:\n\tcase VFIO_PCI_MSIX_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\t/* XXX Need masking support exported */\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_msi_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_ERR_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\t\tfunc = vfio_pci_set_err_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_REQ_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_req_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (!func)\n\t\treturn -ENOTTY;\n\n\treturn func(vdev, index, start, count, flags, data);\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2012 Red Hat, Inc.  All rights reserved.\n *     Author: Alex Williamson <alex.williamson@redhat.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * Derived from original vfio:\n * Copyright 2010 Cisco Systems, Inc.  All rights reserved.\n * Author: Tom Lyon, pugs@cisco.com\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/device.h>\n#include <linux/eventfd.h>\n#include <linux/file.h>\n#include <linux/interrupt.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/notifier.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/vfio.h>\n#include <linux/vgaarb.h>\n\n#include \"vfio_pci_private.h\"\n\n#define DRIVER_VERSION  \"0.2\"\n#define DRIVER_AUTHOR   \"Alex Williamson <alex.williamson@redhat.com>\"\n#define DRIVER_DESC     \"VFIO PCI - User Level meta-driver\"\n\nstatic char ids[1024] __initdata;\nmodule_param_string(ids, ids, sizeof(ids), 0);\nMODULE_PARM_DESC(ids, \"Initial PCI IDs to add to the vfio driver, format is \\\"vendor:device[:subvendor[:subdevice[:class[:class_mask]]]]\\\" and multiple comma separated entries can be specified\");\n\nstatic bool nointxmask;\nmodule_param_named(nointxmask, nointxmask, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(nointxmask,\n\t\t  \"Disable support for PCI 2.3 style INTx masking.  If this resolves problems for specific devices, report lspci -vvvxxx to linux-pci@vger.kernel.org so the device can be fixed automatically via the broken_intx_masking flag.\");\n\n#ifdef CONFIG_VFIO_PCI_VGA\nstatic bool disable_vga;\nmodule_param(disable_vga, bool, S_IRUGO);\nMODULE_PARM_DESC(disable_vga, \"Disable VGA resource access through vfio-pci\");\n#endif\n\nstatic bool disable_idle_d3;\nmodule_param(disable_idle_d3, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(disable_idle_d3,\n\t\t \"Disable using the PCI D3 low power state for idle, unused devices\");\n\nstatic DEFINE_MUTEX(driver_lock);\n\nstatic inline bool vfio_vga_disabled(void)\n{\n#ifdef CONFIG_VFIO_PCI_VGA\n\treturn disable_vga;\n#else\n\treturn true;\n#endif\n}\n\n/*\n * Our VGA arbiter participation is limited since we don't know anything\n * about the device itself.  However, if the device is the only VGA device\n * downstream of a bridge and VFIO VGA support is disabled, then we can\n * safely return legacy VGA IO and memory as not decoded since the user\n * has no way to get to it and routing can be disabled externally at the\n * bridge.\n */\nstatic unsigned int vfio_pci_set_vga_decode(void *opaque, bool single_vga)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\tstruct pci_dev *tmp = NULL, *pdev = vdev->pdev;\n\tunsigned char max_busnr;\n\tunsigned int decodes;\n\n\tif (single_vga || !vfio_vga_disabled() || pci_is_root_bus(pdev->bus))\n\t\treturn VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t       VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\n\tmax_busnr = pci_bus_max_busnr(pdev->bus);\n\tdecodes = VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;\n\n\twhile ((tmp = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, tmp)) != NULL) {\n\t\tif (tmp == pdev ||\n\t\t    pci_domain_nr(tmp->bus) != pci_domain_nr(pdev->bus) ||\n\t\t    pci_is_root_bus(tmp->bus))\n\t\t\tcontinue;\n\n\t\tif (tmp->bus->number >= pdev->bus->number &&\n\t\t    tmp->bus->number <= max_busnr) {\n\t\t\tpci_dev_put(tmp);\n\t\t\tdecodes |= VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn decodes;\n}\n\nstatic inline bool vfio_pci_is_vga(struct pci_dev *pdev)\n{\n\treturn (pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA;\n}\n\nstatic void vfio_pci_probe_mmaps(struct vfio_pci_device *vdev)\n{\n\tstruct resource *res;\n\tint bar;\n\tstruct vfio_pci_dummy_resource *dummy_res;\n\n\tINIT_LIST_HEAD(&vdev->dummy_resources_list);\n\n\tfor (bar = PCI_STD_RESOURCES; bar <= PCI_STD_RESOURCE_END; bar++) {\n\t\tres = vdev->pdev->resource + bar;\n\n\t\tif (!IS_ENABLED(CONFIG_VFIO_PCI_MMAP))\n\t\t\tgoto no_mmap;\n\n\t\tif (!(res->flags & IORESOURCE_MEM))\n\t\t\tgoto no_mmap;\n\n\t\t/*\n\t\t * The PCI core shouldn't set up a resource with a\n\t\t * type but zero size. But there may be bugs that\n\t\t * cause us to do that.\n\t\t */\n\t\tif (!resource_size(res))\n\t\t\tgoto no_mmap;\n\n\t\tif (resource_size(res) >= PAGE_SIZE) {\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!(res->start & ~PAGE_MASK)) {\n\t\t\t/*\n\t\t\t * Add a dummy resource to reserve the remainder\n\t\t\t * of the exclusive page in case that hot-add\n\t\t\t * device's bar is assigned into it.\n\t\t\t */\n\t\t\tdummy_res = kzalloc(sizeof(*dummy_res), GFP_KERNEL);\n\t\t\tif (dummy_res == NULL)\n\t\t\t\tgoto no_mmap;\n\n\t\t\tdummy_res->resource.name = \"vfio sub-page reserved\";\n\t\t\tdummy_res->resource.start = res->end + 1;\n\t\t\tdummy_res->resource.end = res->start + PAGE_SIZE - 1;\n\t\t\tdummy_res->resource.flags = res->flags;\n\t\t\tif (request_resource(res->parent,\n\t\t\t\t\t\t&dummy_res->resource)) {\n\t\t\t\tkfree(dummy_res);\n\t\t\t\tgoto no_mmap;\n\t\t\t}\n\t\t\tdummy_res->index = bar;\n\t\t\tlist_add(&dummy_res->res_next,\n\t\t\t\t\t&vdev->dummy_resources_list);\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * Here we don't handle the case when the BAR is not page\n\t\t * aligned because we can't expect the BAR will be\n\t\t * assigned into the same location in a page in guest\n\t\t * when we passthrough the BAR. And it's hard to access\n\t\t * this BAR in userspace because we have no way to get\n\t\t * the BAR's location in a page.\n\t\t */\nno_mmap:\n\t\tvdev->bar_mmap_supported[bar] = false;\n\t}\n}\n\nstatic void vfio_pci_try_bus_reset(struct vfio_pci_device *vdev);\nstatic void vfio_pci_disable(struct vfio_pci_device *vdev);\n\n/*\n * INTx masking requires the ability to disable INTx signaling via PCI_COMMAND\n * _and_ the ability detect when the device is asserting INTx via PCI_STATUS.\n * If a device implements the former but not the latter we would typically\n * expect broken_intx_masking be set and require an exclusive interrupt.\n * However since we do have control of the device's ability to assert INTx,\n * we can instead pretend that the device does not implement INTx, virtualizing\n * the pin register to report zero and maintaining DisINTx set on the host.\n */\nstatic bool vfio_pci_nointx(struct pci_dev *pdev)\n{\n\tswitch (pdev->vendor) {\n\tcase PCI_VENDOR_ID_INTEL:\n\t\tswitch (pdev->device) {\n\t\t/* All i40e (XL710/X710) 10/20/40GbE NICs */\n\t\tcase 0x1572:\n\t\tcase 0x1574:\n\t\tcase 0x1580 ... 0x1581:\n\t\tcase 0x1583 ... 0x1589:\n\t\tcase 0x37d0 ... 0x37d2:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int vfio_pci_enable(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint ret;\n\tu16 cmd;\n\tu8 msix_pos;\n\n\tpci_set_power_state(pdev, PCI_D0);\n\n\t/* Don't allow our initial saved state to include busmaster */\n\tpci_clear_master(pdev);\n\n\tret = pci_enable_device(pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tvdev->reset_works = (pci_reset_function(pdev) == 0);\n\tpci_save_state(pdev);\n\tvdev->pci_saved_state = pci_store_saved_state(pdev);\n\tif (!vdev->pci_saved_state)\n\t\tpr_debug(\"%s: Couldn't store %s saved state\\n\",\n\t\t\t __func__, dev_name(&pdev->dev));\n\n\tif (likely(!nointxmask)) {\n\t\tif (vfio_pci_nointx(pdev)) {\n\t\t\tdev_info(&pdev->dev, \"Masking broken INTx support\\n\");\n\t\t\tvdev->nointx = true;\n\t\t\tpci_intx(pdev, 0);\n\t\t} else\n\t\t\tvdev->pci_2_3 = pci_intx_mask_supported(pdev);\n\t}\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &cmd);\n\tif (vdev->pci_2_3 && (cmd & PCI_COMMAND_INTX_DISABLE)) {\n\t\tcmd &= ~PCI_COMMAND_INTX_DISABLE;\n\t\tpci_write_config_word(pdev, PCI_COMMAND, cmd);\n\t}\n\n\tret = vfio_config_init(vdev);\n\tif (ret) {\n\t\tkfree(vdev->pci_saved_state);\n\t\tvdev->pci_saved_state = NULL;\n\t\tpci_disable_device(pdev);\n\t\treturn ret;\n\t}\n\n\tmsix_pos = pdev->msix_cap;\n\tif (msix_pos) {\n\t\tu16 flags;\n\t\tu32 table;\n\n\t\tpci_read_config_word(pdev, msix_pos + PCI_MSIX_FLAGS, &flags);\n\t\tpci_read_config_dword(pdev, msix_pos + PCI_MSIX_TABLE, &table);\n\n\t\tvdev->msix_bar = table & PCI_MSIX_TABLE_BIR;\n\t\tvdev->msix_offset = table & PCI_MSIX_TABLE_OFFSET;\n\t\tvdev->msix_size = ((flags & PCI_MSIX_FLAGS_QSIZE) + 1) * 16;\n\t} else\n\t\tvdev->msix_bar = 0xFF;\n\n\tif (!vfio_vga_disabled() && vfio_pci_is_vga(pdev))\n\t\tvdev->has_vga = true;\n\n\n\tif (vfio_pci_is_vga(pdev) &&\n\t    pdev->vendor == PCI_VENDOR_ID_INTEL &&\n\t    IS_ENABLED(CONFIG_VFIO_PCI_IGD)) {\n\t\tret = vfio_pci_igd_init(vdev);\n\t\tif (ret) {\n\t\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t\t \"Failed to setup Intel IGD regions\\n\");\n\t\t\tvfio_pci_disable(vdev);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tvfio_pci_probe_mmaps(vdev);\n\n\treturn 0;\n}\n\nstatic void vfio_pci_disable(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct vfio_pci_dummy_resource *dummy_res, *tmp;\n\tint i, bar;\n\n\t/* Stop the device from further DMA */\n\tpci_clear_master(pdev);\n\n\tvfio_pci_set_irqs_ioctl(vdev, VFIO_IRQ_SET_DATA_NONE |\n\t\t\t\tVFIO_IRQ_SET_ACTION_TRIGGER,\n\t\t\t\tvdev->irq_type, 0, 0, NULL);\n\n\tvdev->virq_disabled = false;\n\n\tfor (i = 0; i < vdev->num_regions; i++)\n\t\tvdev->region[i].ops->release(vdev, &vdev->region[i]);\n\n\tvdev->num_regions = 0;\n\tkfree(vdev->region);\n\tvdev->region = NULL; /* don't krealloc a freed pointer */\n\n\tvfio_config_free(vdev);\n\n\tfor (bar = PCI_STD_RESOURCES; bar <= PCI_STD_RESOURCE_END; bar++) {\n\t\tif (!vdev->barmap[bar])\n\t\t\tcontinue;\n\t\tpci_iounmap(pdev, vdev->barmap[bar]);\n\t\tpci_release_selected_regions(pdev, 1 << bar);\n\t\tvdev->barmap[bar] = NULL;\n\t}\n\n\tlist_for_each_entry_safe(dummy_res, tmp,\n\t\t\t\t &vdev->dummy_resources_list, res_next) {\n\t\tlist_del(&dummy_res->res_next);\n\t\trelease_resource(&dummy_res->resource);\n\t\tkfree(dummy_res);\n\t}\n\n\tvdev->needs_reset = true;\n\n\t/*\n\t * If we have saved state, restore it.  If we can reset the device,\n\t * even better.  Resetting with current state seems better than\n\t * nothing, but saving and restoring current state without reset\n\t * is just busy work.\n\t */\n\tif (pci_load_and_free_saved_state(pdev, &vdev->pci_saved_state)) {\n\t\tpr_info(\"%s: Couldn't reload %s saved state\\n\",\n\t\t\t__func__, dev_name(&pdev->dev));\n\n\t\tif (!vdev->reset_works)\n\t\t\tgoto out;\n\n\t\tpci_save_state(pdev);\n\t}\n\n\t/*\n\t * Disable INTx and MSI, presumably to avoid spurious interrupts\n\t * during reset.  Stolen from pci_reset_function()\n\t */\n\tpci_write_config_word(pdev, PCI_COMMAND, PCI_COMMAND_INTX_DISABLE);\n\n\t/*\n\t * Try to reset the device.  The success of this is dependent on\n\t * being able to lock the device, which is not always possible.\n\t */\n\tif (vdev->reset_works && !pci_try_reset_function(pdev))\n\t\tvdev->needs_reset = false;\n\n\tpci_restore_state(pdev);\nout:\n\tpci_disable_device(pdev);\n\n\tvfio_pci_try_bus_reset(vdev);\n\n\tif (!disable_idle_d3)\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n}\n\nstatic void vfio_pci_release(void *device_data)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tmutex_lock(&driver_lock);\n\n\tif (!(--vdev->refcnt)) {\n\t\tvfio_spapr_pci_eeh_release(vdev->pdev);\n\t\tvfio_pci_disable(vdev);\n\t}\n\n\tmutex_unlock(&driver_lock);\n\n\tmodule_put(THIS_MODULE);\n}\n\nstatic int vfio_pci_open(void *device_data)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tint ret = 0;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\treturn -ENODEV;\n\n\tmutex_lock(&driver_lock);\n\n\tif (!vdev->refcnt) {\n\t\tret = vfio_pci_enable(vdev);\n\t\tif (ret)\n\t\t\tgoto error;\n\n\t\tvfio_spapr_pci_eeh_open(vdev->pdev);\n\t}\n\tvdev->refcnt++;\nerror:\n\tmutex_unlock(&driver_lock);\n\tif (ret)\n\t\tmodule_put(THIS_MODULE);\n\treturn ret;\n}\n\nstatic int vfio_pci_get_irq_count(struct vfio_pci_device *vdev, int irq_type)\n{\n\tif (irq_type == VFIO_PCI_INTX_IRQ_INDEX) {\n\t\tu8 pin;\n\t\tpci_read_config_byte(vdev->pdev, PCI_INTERRUPT_PIN, &pin);\n\t\tif (IS_ENABLED(CONFIG_VFIO_PCI_INTX) && !vdev->nointx && pin)\n\t\t\treturn 1;\n\n\t} else if (irq_type == VFIO_PCI_MSI_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msi_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSI_FLAGS, &flags);\n\t\t\treturn 1 << ((flags & PCI_MSI_FLAGS_QMASK) >> 1);\n\t\t}\n\t} else if (irq_type == VFIO_PCI_MSIX_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msix_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSIX_FLAGS, &flags);\n\n\t\t\treturn (flags & PCI_MSIX_FLAGS_QSIZE) + 1;\n\t\t}\n\t} else if (irq_type == VFIO_PCI_ERR_IRQ_INDEX) {\n\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\treturn 1;\n\t} else if (irq_type == VFIO_PCI_REQ_IRQ_INDEX) {\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_count_devs(struct pci_dev *pdev, void *data)\n{\n\t(*(int *)data)++;\n\treturn 0;\n}\n\nstruct vfio_pci_fill_info {\n\tint max;\n\tint cur;\n\tstruct vfio_pci_dependent_device *devices;\n};\n\nstatic int vfio_pci_fill_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_fill_info *fill = data;\n\tstruct iommu_group *iommu_group;\n\n\tif (fill->cur == fill->max)\n\t\treturn -EAGAIN; /* Something changed, try again */\n\n\tiommu_group = iommu_group_get(&pdev->dev);\n\tif (!iommu_group)\n\t\treturn -EPERM; /* Cannot reset non-isolated devices */\n\n\tfill->devices[fill->cur].group_id = iommu_group_id(iommu_group);\n\tfill->devices[fill->cur].segment = pci_domain_nr(pdev->bus);\n\tfill->devices[fill->cur].bus = pdev->bus->number;\n\tfill->devices[fill->cur].devfn = pdev->devfn;\n\tfill->cur++;\n\tiommu_group_put(iommu_group);\n\treturn 0;\n}\n\nstruct vfio_pci_group_entry {\n\tstruct vfio_group *group;\n\tint id;\n};\n\nstruct vfio_pci_group_info {\n\tint count;\n\tstruct vfio_pci_group_entry *groups;\n};\n\nstatic int vfio_pci_validate_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_group_info *info = data;\n\tstruct iommu_group *group;\n\tint id, i;\n\n\tgroup = iommu_group_get(&pdev->dev);\n\tif (!group)\n\t\treturn -EPERM;\n\n\tid = iommu_group_id(group);\n\n\tfor (i = 0; i < info->count; i++)\n\t\tif (info->groups[i].id == id)\n\t\t\tbreak;\n\n\tiommu_group_put(group);\n\n\treturn (i == info->count) ? -EINVAL : 0;\n}\n\nstatic bool vfio_pci_dev_below_slot(struct pci_dev *pdev, struct pci_slot *slot)\n{\n\tfor (; pdev; pdev = pdev->bus->self)\n\t\tif (pdev->bus == slot->bus)\n\t\t\treturn (pdev->slot == slot);\n\treturn false;\n}\n\nstruct vfio_pci_walk_info {\n\tint (*fn)(struct pci_dev *, void *data);\n\tvoid *data;\n\tstruct pci_dev *pdev;\n\tbool slot;\n\tint ret;\n};\n\nstatic int vfio_pci_walk_wrapper(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_walk_info *walk = data;\n\n\tif (!walk->slot || vfio_pci_dev_below_slot(pdev, walk->pdev->slot))\n\t\twalk->ret = walk->fn(pdev, walk->data);\n\n\treturn walk->ret;\n}\n\nstatic int vfio_pci_for_each_slot_or_bus(struct pci_dev *pdev,\n\t\t\t\t\t int (*fn)(struct pci_dev *,\n\t\t\t\t\t\t   void *data), void *data,\n\t\t\t\t\t bool slot)\n{\n\tstruct vfio_pci_walk_info walk = {\n\t\t.fn = fn, .data = data, .pdev = pdev, .slot = slot, .ret = 0,\n\t};\n\n\tpci_walk_bus(pdev->bus, vfio_pci_walk_wrapper, &walk);\n\n\treturn walk.ret;\n}\n\nstatic int msix_sparse_mmap_cap(struct vfio_pci_device *vdev,\n\t\t\t\tstruct vfio_info_cap *caps)\n{\n\tstruct vfio_info_cap_header *header;\n\tstruct vfio_region_info_cap_sparse_mmap *sparse;\n\tsize_t end, size;\n\tint nr_areas = 2, i = 0;\n\n\tend = pci_resource_len(vdev->pdev, vdev->msix_bar);\n\n\t/* If MSI-X table is aligned to the start or end, only one area */\n\tif (((vdev->msix_offset & PAGE_MASK) == 0) ||\n\t    (PAGE_ALIGN(vdev->msix_offset + vdev->msix_size) >= end))\n\t\tnr_areas = 1;\n\n\tsize = sizeof(*sparse) + (nr_areas * sizeof(*sparse->areas));\n\n\theader = vfio_info_cap_add(caps, size,\n\t\t\t\t   VFIO_REGION_INFO_CAP_SPARSE_MMAP, 1);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tsparse = container_of(header,\n\t\t\t      struct vfio_region_info_cap_sparse_mmap, header);\n\tsparse->nr_areas = nr_areas;\n\n\tif (vdev->msix_offset & PAGE_MASK) {\n\t\tsparse->areas[i].offset = 0;\n\t\tsparse->areas[i].size = vdev->msix_offset & PAGE_MASK;\n\t\ti++;\n\t}\n\n\tif (PAGE_ALIGN(vdev->msix_offset + vdev->msix_size) < end) {\n\t\tsparse->areas[i].offset = PAGE_ALIGN(vdev->msix_offset +\n\t\t\t\t\t\t     vdev->msix_size);\n\t\tsparse->areas[i].size = end - sparse->areas[i].offset;\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nstatic int region_type_cap(struct vfio_pci_device *vdev,\n\t\t\t   struct vfio_info_cap *caps,\n\t\t\t   unsigned int type, unsigned int subtype)\n{\n\tstruct vfio_info_cap_header *header;\n\tstruct vfio_region_info_cap_type *cap;\n\n\theader = vfio_info_cap_add(caps, sizeof(*cap),\n\t\t\t\t   VFIO_REGION_INFO_CAP_TYPE, 1);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tcap = container_of(header, struct vfio_region_info_cap_type, header);\n\tcap->type = type;\n\tcap->subtype = subtype;\n\n\treturn 0;\n}\n\nint vfio_pci_register_dev_region(struct vfio_pci_device *vdev,\n\t\t\t\t unsigned int type, unsigned int subtype,\n\t\t\t\t const struct vfio_pci_regops *ops,\n\t\t\t\t size_t size, u32 flags, void *data)\n{\n\tstruct vfio_pci_region *region;\n\n\tregion = krealloc(vdev->region,\n\t\t\t  (vdev->num_regions + 1) * sizeof(*region),\n\t\t\t  GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tvdev->region = region;\n\tvdev->region[vdev->num_regions].type = type;\n\tvdev->region[vdev->num_regions].subtype = subtype;\n\tvdev->region[vdev->num_regions].ops = ops;\n\tvdev->region[vdev->num_regions].size = size;\n\tvdev->region[vdev->num_regions].flags = flags;\n\tvdev->region[vdev->num_regions].data = data;\n\n\tvdev->num_regions++;\n\n\treturn 0;\n}\n\nstatic long vfio_pci_ioctl(void *device_data,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tunsigned long minsz;\n\n\tif (cmd == VFIO_DEVICE_GET_INFO) {\n\t\tstruct vfio_device_info info;\n\n\t\tminsz = offsetofend(struct vfio_device_info, num_irqs);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tinfo.flags = VFIO_DEVICE_FLAGS_PCI;\n\n\t\tif (vdev->reset_works)\n\t\t\tinfo.flags |= VFIO_DEVICE_FLAGS_RESET;\n\n\t\tinfo.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;\n\t\tinfo.num_irqs = VFIO_PCI_NUM_IRQS;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_GET_REGION_INFO) {\n\t\tstruct pci_dev *pdev = vdev->pdev;\n\t\tstruct vfio_region_info info;\n\t\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\t\tint i, ret;\n\n\t\tminsz = offsetofend(struct vfio_region_info, offset);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = pdev->cfg_size;\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tbreak;\n\t\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\t\tif (!info.size) {\n\t\t\t\tinfo.flags = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tif (vdev->bar_mmap_supported[info.index]) {\n\t\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_MMAP;\n\t\t\t\tif (info.index == vdev->msix_bar) {\n\t\t\t\t\tret = msix_sparse_mmap_cap(vdev, &caps);\n\t\t\t\t\tif (ret)\n\t\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\t{\n\t\t\tvoid __iomem *io;\n\t\t\tsize_t size;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.flags = 0;\n\n\t\t\t/* Report the BAR size, not the ROM size */\n\t\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\t\tif (!info.size) {\n\t\t\t\t/* Shadow ROMs appear as PCI option ROMs */\n\t\t\t\tif (pdev->resource[PCI_ROM_RESOURCE].flags &\n\t\t\t\t\t\t\tIORESOURCE_ROM_SHADOW)\n\t\t\t\t\tinfo.size = 0x20000;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Is it really there? */\n\t\t\tio = pci_map_rom(pdev, &size);\n\t\t\tif (!io || !size) {\n\t\t\t\tinfo.size = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpci_unmap_rom(pdev, io);\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ;\n\t\t\tbreak;\n\t\t}\n\t\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\t\tif (!vdev->has_vga)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = 0xc0000;\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (info.index >=\n\t\t\t    VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ti = info.index - VFIO_PCI_NUM_REGIONS;\n\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = vdev->region[i].size;\n\t\t\tinfo.flags = vdev->region[i].flags;\n\n\t\t\tret = region_type_cap(vdev, &caps,\n\t\t\t\t\t      vdev->region[i].type,\n\t\t\t\t\t      vdev->region[i].subtype);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (caps.size) {\n\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_CAPS;\n\t\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t\t\tinfo.cap_offset = 0;\n\t\t\t} else {\n\t\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\t\tif (copy_to_user((void __user *)arg +\n\t\t\t\t\t\t  sizeof(info), caps.buf,\n\t\t\t\t\t\t  caps.size)) {\n\t\t\t\t\tkfree(caps.buf);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tinfo.cap_offset = sizeof(info);\n\t\t\t}\n\n\t\t\tkfree(caps.buf);\n\t\t}\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_GET_IRQ_INFO) {\n\t\tstruct vfio_irq_info info;\n\n\t\tminsz = offsetofend(struct vfio_irq_info, count);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_INTX_IRQ_INDEX ... VFIO_PCI_MSIX_IRQ_INDEX:\n\t\tcase VFIO_PCI_REQ_IRQ_INDEX:\n\t\t\tbreak;\n\t\tcase VFIO_PCI_ERR_IRQ_INDEX:\n\t\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\t\tbreak;\n\t\t/* pass thru to return error */\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinfo.flags = VFIO_IRQ_INFO_EVENTFD;\n\n\t\tinfo.count = vfio_pci_get_irq_count(vdev, info.index);\n\n\t\tif (info.index == VFIO_PCI_INTX_IRQ_INDEX)\n\t\t\tinfo.flags |= (VFIO_IRQ_INFO_MASKABLE |\n\t\t\t\t       VFIO_IRQ_INFO_AUTOMASKED);\n\t\telse\n\t\t\tinfo.flags |= VFIO_IRQ_INFO_NORESIZE;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_SET_IRQS) {\n\t\tstruct vfio_irq_set hdr;\n\t\tsize_t size;\n\t\tu8 *data = NULL;\n\t\tint max, ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_irq_set, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||\n\t\t    hdr.count >= (U32_MAX - hdr.start) ||\n\t\t    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |\n\t\t\t\t  VFIO_IRQ_SET_ACTION_TYPE_MASK))\n\t\t\treturn -EINVAL;\n\n\t\tmax = vfio_pci_get_irq_count(vdev, hdr.index);\n\t\tif (hdr.start >= max || hdr.start + hdr.count > max)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (hdr.flags & VFIO_IRQ_SET_DATA_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_DATA_NONE:\n\t\t\tsize = 0;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_DATA_BOOL:\n\t\t\tsize = sizeof(uint8_t);\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_DATA_EVENTFD:\n\t\t\tsize = sizeof(int32_t);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (size) {\n\t\t\tif (hdr.argsz - minsz < hdr.count * size)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdata = memdup_user((void __user *)(arg + minsz),\n\t\t\t\t\t   hdr.count * size);\n\t\t\tif (IS_ERR(data))\n\t\t\t\treturn PTR_ERR(data);\n\t\t}\n\n\t\tmutex_lock(&vdev->igate);\n\n\t\tret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index,\n\t\t\t\t\t      hdr.start, hdr.count, data);\n\n\t\tmutex_unlock(&vdev->igate);\n\t\tkfree(data);\n\n\t\treturn ret;\n\n\t} else if (cmd == VFIO_DEVICE_RESET) {\n\t\treturn vdev->reset_works ?\n\t\t\tpci_try_reset_function(vdev->pdev) : -EINVAL;\n\n\t} else if (cmd == VFIO_DEVICE_GET_PCI_HOT_RESET_INFO) {\n\t\tstruct vfio_pci_hot_reset_info hdr;\n\t\tstruct vfio_pci_fill_info fill = { 0 };\n\t\tstruct vfio_pci_dependent_device *devices = NULL;\n\t\tbool slot = false;\n\t\tint ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_pci_hot_reset_info, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\thdr.flags = 0;\n\n\t\t/* Can we do a slot or bus reset or neither? */\n\t\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\t\tslot = true;\n\t\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\t\treturn -ENODEV;\n\n\t\t/* How many devices are affected? */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_count_devs,\n\t\t\t\t\t\t    &fill.max, slot);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tWARN_ON(!fill.max); /* Should always be at least one */\n\n\t\t/*\n\t\t * If there's enough space, fill it now, otherwise return\n\t\t * -ENOSPC and the number of devices affected.\n\t\t */\n\t\tif (hdr.argsz < sizeof(hdr) + (fill.max * sizeof(*devices))) {\n\t\t\tret = -ENOSPC;\n\t\t\thdr.count = fill.max;\n\t\t\tgoto reset_info_exit;\n\t\t}\n\n\t\tdevices = kcalloc(fill.max, sizeof(*devices), GFP_KERNEL);\n\t\tif (!devices)\n\t\t\treturn -ENOMEM;\n\n\t\tfill.devices = devices;\n\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_fill_devs,\n\t\t\t\t\t\t    &fill, slot);\n\n\t\t/*\n\t\t * If a device was removed between counting and filling,\n\t\t * we may come up short of fill.max.  If a device was\n\t\t * added, we'll have a return of -EAGAIN above.\n\t\t */\n\t\tif (!ret)\n\t\t\thdr.count = fill.cur;\n\nreset_info_exit:\n\t\tif (copy_to_user((void __user *)arg, &hdr, minsz))\n\t\t\tret = -EFAULT;\n\n\t\tif (!ret) {\n\t\t\tif (copy_to_user((void __user *)(arg + minsz), devices,\n\t\t\t\t\t hdr.count * sizeof(*devices)))\n\t\t\t\tret = -EFAULT;\n\t\t}\n\n\t\tkfree(devices);\n\t\treturn ret;\n\n\t} else if (cmd == VFIO_DEVICE_PCI_HOT_RESET) {\n\t\tstruct vfio_pci_hot_reset hdr;\n\t\tint32_t *group_fds;\n\t\tstruct vfio_pci_group_entry *groups;\n\t\tstruct vfio_pci_group_info info;\n\t\tbool slot = false;\n\t\tint i, count = 0, ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_pci_hot_reset, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (hdr.argsz < minsz || hdr.flags)\n\t\t\treturn -EINVAL;\n\n\t\t/* Can we do a slot or bus reset or neither? */\n\t\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\t\tslot = true;\n\t\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\t\treturn -ENODEV;\n\n\t\t/*\n\t\t * We can't let userspace give us an arbitrarily large\n\t\t * buffer to copy, so verify how many we think there\n\t\t * could be.  Note groups can have multiple devices so\n\t\t * one group per device is the max.\n\t\t */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_count_devs,\n\t\t\t\t\t\t    &count, slot);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t/* Somewhere between 1 and count is OK */\n\t\tif (!hdr.count || hdr.count > count)\n\t\t\treturn -EINVAL;\n\n\t\tgroup_fds = kcalloc(hdr.count, sizeof(*group_fds), GFP_KERNEL);\n\t\tgroups = kcalloc(hdr.count, sizeof(*groups), GFP_KERNEL);\n\t\tif (!group_fds || !groups) {\n\t\t\tkfree(group_fds);\n\t\t\tkfree(groups);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (copy_from_user(group_fds, (void __user *)(arg + minsz),\n\t\t\t\t   hdr.count * sizeof(*group_fds))) {\n\t\t\tkfree(group_fds);\n\t\t\tkfree(groups);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t/*\n\t\t * For each group_fd, get the group through the vfio external\n\t\t * user interface and store the group and iommu ID.  This\n\t\t * ensures the group is held across the reset.\n\t\t */\n\t\tfor (i = 0; i < hdr.count; i++) {\n\t\t\tstruct vfio_group *group;\n\t\t\tstruct fd f = fdget(group_fds[i]);\n\t\t\tif (!f.file) {\n\t\t\t\tret = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgroup = vfio_group_get_external_user(f.file);\n\t\t\tfdput(f);\n\t\t\tif (IS_ERR(group)) {\n\t\t\t\tret = PTR_ERR(group);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgroups[i].group = group;\n\t\t\tgroups[i].id = vfio_external_user_iommu_id(group);\n\t\t}\n\n\t\tkfree(group_fds);\n\n\t\t/* release reference to groups on error */\n\t\tif (ret)\n\t\t\tgoto hot_reset_release;\n\n\t\tinfo.count = hdr.count;\n\t\tinfo.groups = groups;\n\n\t\t/*\n\t\t * Test whether all the affected devices are contained\n\t\t * by the set of groups provided by the user.\n\t\t */\n\t\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t\t    vfio_pci_validate_devs,\n\t\t\t\t\t\t    &info, slot);\n\t\tif (!ret)\n\t\t\t/* User has access, do the reset */\n\t\t\tret = slot ? pci_try_reset_slot(vdev->pdev->slot) :\n\t\t\t\t     pci_try_reset_bus(vdev->pdev->bus);\n\nhot_reset_release:\n\t\tfor (i--; i >= 0; i--)\n\t\t\tvfio_group_put_external_user(groups[i].group);\n\n\t\tkfree(groups);\n\t\treturn ret;\n\t}\n\n\treturn -ENOTTY;\n}\n\nstatic ssize_t vfio_pci_rw(void *device_data, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos, bool iswrite)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tif (index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\treturn -EINVAL;\n\n\tswitch (index) {\n\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\treturn vfio_pci_config_rw(vdev, buf, count, ppos, iswrite);\n\n\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\tif (iswrite)\n\t\t\treturn -EINVAL;\n\t\treturn vfio_pci_bar_rw(vdev, buf, count, ppos, false);\n\n\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\treturn vfio_pci_bar_rw(vdev, buf, count, ppos, iswrite);\n\n\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\treturn vfio_pci_vga_rw(vdev, buf, count, ppos, iswrite);\n\tdefault:\n\t\tindex -= VFIO_PCI_NUM_REGIONS;\n\t\treturn vdev->region[index].ops->rw(vdev, buf,\n\t\t\t\t\t\t   count, ppos, iswrite);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic ssize_t vfio_pci_read(void *device_data, char __user *buf,\n\t\t\t     size_t count, loff_t *ppos)\n{\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(device_data, buf, count, ppos, false);\n}\n\nstatic ssize_t vfio_pci_write(void *device_data, const char __user *buf,\n\t\t\t      size_t count, loff_t *ppos)\n{\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(device_data, (char __user *)buf, count, ppos, true);\n}\n\nstatic int vfio_pci_mmap(void *device_data, struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned int index;\n\tu64 phys_len, req_len, pgoff, req_start;\n\tint ret;\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\tif (!vdev->bar_mmap_supported[index])\n\t\treturn -EINVAL;\n\n\tphys_len = PAGE_ALIGN(pci_resource_len(pdev, index));\n\treq_len = vma->vm_end - vma->vm_start;\n\tpgoff = vma->vm_pgoff &\n\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\treq_start = pgoff << PAGE_SHIFT;\n\n\tif (req_start + req_len > phys_len)\n\t\treturn -EINVAL;\n\n\tif (index == vdev->msix_bar) {\n\t\t/*\n\t\t * Disallow mmaps overlapping the MSI-X table; users don't\n\t\t * get to touch this directly.  We could find somewhere\n\t\t * else to map the overlap, but page granularity is only\n\t\t * a recommendation, not a requirement, so the user needs\n\t\t * to know which bits are real.  Requiring them to mmap\n\t\t * around the table makes that clear.\n\t\t */\n\n\t\t/* If neither entirely above nor below, then it overlaps */\n\t\tif (!(req_start >= vdev->msix_offset + vdev->msix_size ||\n\t\t      req_start + req_len <= vdev->msix_offset))\n\t\t\treturn -EINVAL;\n\t}\n\n\t/*\n\t * Even though we don't make use of the barmap for the mmap,\n\t * we need to request the region and the barmap tracks that.\n\t */\n\tif (!vdev->barmap[index]) {\n\t\tret = pci_request_selected_regions(pdev,\n\t\t\t\t\t\t   1 << index, \"vfio-pci\");\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvdev->barmap[index] = pci_iomap(pdev, index, 0);\n\t}\n\n\tvma->vm_private_data = vdev;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tvma->vm_pgoff = (pci_resource_start(pdev, index) >> PAGE_SHIFT) + pgoff;\n\n\treturn remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t       req_len, vma->vm_page_prot);\n}\n\nstatic void vfio_pci_request(void *device_data, unsigned int count)\n{\n\tstruct vfio_pci_device *vdev = device_data;\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->req_trigger) {\n\t\tif (!(count % 10))\n\t\t\tdev_notice_ratelimited(&vdev->pdev->dev,\n\t\t\t\t\"Relaying device request to user (#%u)\\n\",\n\t\t\t\tcount);\n\t\teventfd_signal(vdev->req_trigger, 1);\n\t} else if (count == 0) {\n\t\tdev_warn(&vdev->pdev->dev,\n\t\t\t\"No device request channel registered, blocked until released by user\\n\");\n\t}\n\n\tmutex_unlock(&vdev->igate);\n}\n\nstatic const struct vfio_device_ops vfio_pci_ops = {\n\t.name\t\t= \"vfio-pci\",\n\t.open\t\t= vfio_pci_open,\n\t.release\t= vfio_pci_release,\n\t.ioctl\t\t= vfio_pci_ioctl,\n\t.read\t\t= vfio_pci_read,\n\t.write\t\t= vfio_pci_write,\n\t.mmap\t\t= vfio_pci_mmap,\n\t.request\t= vfio_pci_request,\n};\n\nstatic int vfio_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct vfio_pci_device *vdev;\n\tstruct iommu_group *group;\n\tint ret;\n\n\tif (pdev->hdr_type != PCI_HEADER_TYPE_NORMAL)\n\t\treturn -EINVAL;\n\n\tgroup = vfio_iommu_group_get(&pdev->dev);\n\tif (!group)\n\t\treturn -EINVAL;\n\n\tvdev = kzalloc(sizeof(*vdev), GFP_KERNEL);\n\tif (!vdev) {\n\t\tvfio_iommu_group_put(group, &pdev->dev);\n\t\treturn -ENOMEM;\n\t}\n\n\tvdev->pdev = pdev;\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tmutex_init(&vdev->igate);\n\tspin_lock_init(&vdev->irqlock);\n\n\tret = vfio_add_group_dev(&pdev->dev, &vfio_pci_ops, vdev);\n\tif (ret) {\n\t\tvfio_iommu_group_put(group, &pdev->dev);\n\t\tkfree(vdev);\n\t\treturn ret;\n\t}\n\n\tif (vfio_pci_is_vga(pdev)) {\n\t\tvga_client_register(pdev, vdev, NULL, vfio_pci_set_vga_decode);\n\t\tvga_set_legacy_decoding(pdev,\n\t\t\t\t\tvfio_pci_set_vga_decode(vdev, false));\n\t}\n\n\tif (!disable_idle_d3) {\n\t\t/*\n\t\t * pci-core sets the device power state to an unknown value at\n\t\t * bootup and after being removed from a driver.  The only\n\t\t * transition it allows from this unknown state is to D0, which\n\t\t * typically happens when a driver calls pci_enable_device().\n\t\t * We're not ready to enable the device yet, but we do want to\n\t\t * be able to get to D3.  Therefore first do a D0 transition\n\t\t * before going to D3.\n\t\t */\n\t\tpci_set_power_state(pdev, PCI_D0);\n\t\tpci_set_power_state(pdev, PCI_D3hot);\n\t}\n\n\treturn ret;\n}\n\nstatic void vfio_pci_remove(struct pci_dev *pdev)\n{\n\tstruct vfio_pci_device *vdev;\n\n\tvdev = vfio_del_group_dev(&pdev->dev);\n\tif (!vdev)\n\t\treturn;\n\n\tvfio_iommu_group_put(pdev->dev.iommu_group, &pdev->dev);\n\tkfree(vdev->region);\n\tkfree(vdev);\n\n\tif (vfio_pci_is_vga(pdev)) {\n\t\tvga_client_register(pdev, NULL, NULL, NULL);\n\t\tvga_set_legacy_decoding(pdev,\n\t\t\t\tVGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t\t\tVGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM);\n\t}\n\n\tif (!disable_idle_d3)\n\t\tpci_set_power_state(pdev, PCI_D0);\n}\n\nstatic pci_ers_result_t vfio_pci_aer_err_detected(struct pci_dev *pdev,\n\t\t\t\t\t\t  pci_channel_state_t state)\n{\n\tstruct vfio_pci_device *vdev;\n\tstruct vfio_device *device;\n\n\tdevice = vfio_device_get_from_dev(&pdev->dev);\n\tif (device == NULL)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\n\tvdev = vfio_device_data(device);\n\tif (vdev == NULL) {\n\t\tvfio_device_put(device);\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->err_trigger)\n\t\teventfd_signal(vdev->err_trigger, 1);\n\n\tmutex_unlock(&vdev->igate);\n\n\tvfio_device_put(device);\n\n\treturn PCI_ERS_RESULT_CAN_RECOVER;\n}\n\nstatic const struct pci_error_handlers vfio_err_handlers = {\n\t.error_detected = vfio_pci_aer_err_detected,\n};\n\nstatic struct pci_driver vfio_pci_driver = {\n\t.name\t\t= \"vfio-pci\",\n\t.id_table\t= NULL, /* only dynamic ids */\n\t.probe\t\t= vfio_pci_probe,\n\t.remove\t\t= vfio_pci_remove,\n\t.err_handler\t= &vfio_err_handlers,\n};\n\nstruct vfio_devices {\n\tstruct vfio_device **devices;\n\tint cur_index;\n\tint max_index;\n};\n\nstatic int vfio_pci_get_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_devices *devs = data;\n\tstruct vfio_device *device;\n\n\tif (devs->cur_index == devs->max_index)\n\t\treturn -ENOSPC;\n\n\tdevice = vfio_device_get_from_dev(&pdev->dev);\n\tif (!device)\n\t\treturn -EINVAL;\n\n\tif (pci_dev_driver(pdev) != &vfio_pci_driver) {\n\t\tvfio_device_put(device);\n\t\treturn -EBUSY;\n\t}\n\n\tdevs->devices[devs->cur_index++] = device;\n\treturn 0;\n}\n\n/*\n * Attempt to do a bus/slot reset if there are devices affected by a reset for\n * this device that are needs_reset and all of the affected devices are unused\n * (!refcnt).  Callers are required to hold driver_lock when calling this to\n * prevent device opens and concurrent bus reset attempts.  We prevent device\n * unbinds by acquiring and holding a reference to the vfio_device.\n *\n * NB: vfio-core considers a group to be viable even if some devices are\n * bound to drivers like pci-stub or pcieport.  Here we require all devices\n * to be bound to vfio_pci since that's the only way we can be sure they\n * stay put.\n */\nstatic void vfio_pci_try_bus_reset(struct vfio_pci_device *vdev)\n{\n\tstruct vfio_devices devs = { .cur_index = 0 };\n\tint i = 0, ret = -EINVAL;\n\tbool needs_reset = false, slot = false;\n\tstruct vfio_pci_device *tmp;\n\n\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\tslot = true;\n\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\treturn;\n\n\tif (vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_count_devs,\n\t\t\t\t\t  &i, slot) || !i)\n\t\treturn;\n\n\tdevs.max_index = i;\n\tdevs.devices = kcalloc(i, sizeof(struct vfio_device *), GFP_KERNEL);\n\tif (!devs.devices)\n\t\treturn;\n\n\tif (vfio_pci_for_each_slot_or_bus(vdev->pdev,\n\t\t\t\t\t  vfio_pci_get_devs, &devs, slot))\n\t\tgoto put_devs;\n\n\tfor (i = 0; i < devs.cur_index; i++) {\n\t\ttmp = vfio_device_data(devs.devices[i]);\n\t\tif (tmp->needs_reset)\n\t\t\tneeds_reset = true;\n\t\tif (tmp->refcnt)\n\t\t\tgoto put_devs;\n\t}\n\n\tif (needs_reset)\n\t\tret = slot ? pci_try_reset_slot(vdev->pdev->slot) :\n\t\t\t     pci_try_reset_bus(vdev->pdev->bus);\n\nput_devs:\n\tfor (i = 0; i < devs.cur_index; i++) {\n\t\ttmp = vfio_device_data(devs.devices[i]);\n\t\tif (!ret)\n\t\t\ttmp->needs_reset = false;\n\n\t\tif (!tmp->refcnt && !disable_idle_d3)\n\t\t\tpci_set_power_state(tmp->pdev, PCI_D3hot);\n\n\t\tvfio_device_put(devs.devices[i]);\n\t}\n\n\tkfree(devs.devices);\n}\n\nstatic void __exit vfio_pci_cleanup(void)\n{\n\tpci_unregister_driver(&vfio_pci_driver);\n\tvfio_pci_uninit_perm_bits();\n}\n\nstatic void __init vfio_pci_fill_ids(void)\n{\n\tchar *p, *id;\n\tint rc;\n\n\t/* no ids passed actually */\n\tif (ids[0] == '\\0')\n\t\treturn;\n\n\t/* add ids specified in the module parameter */\n\tp = ids;\n\twhile ((id = strsep(&p, \",\"))) {\n\t\tunsigned int vendor, device, subvendor = PCI_ANY_ID,\n\t\t\tsubdevice = PCI_ANY_ID, class = 0, class_mask = 0;\n\t\tint fields;\n\n\t\tif (!strlen(id))\n\t\t\tcontinue;\n\n\t\tfields = sscanf(id, \"%x:%x:%x:%x:%x:%x\",\n\t\t\t\t&vendor, &device, &subvendor, &subdevice,\n\t\t\t\t&class, &class_mask);\n\n\t\tif (fields < 2) {\n\t\t\tpr_warn(\"invalid id string \\\"%s\\\"\\n\", id);\n\t\t\tcontinue;\n\t\t}\n\n\t\trc = pci_add_dynid(&vfio_pci_driver, vendor, device,\n\t\t\t\t   subvendor, subdevice, class, class_mask, 0);\n\t\tif (rc)\n\t\t\tpr_warn(\"failed to add dynamic id [%04hx:%04hx[%04hx:%04hx]] class %#08x/%08x (%d)\\n\",\n\t\t\t\tvendor, device, subvendor, subdevice,\n\t\t\t\tclass, class_mask, rc);\n\t\telse\n\t\t\tpr_info(\"add [%04hx:%04hx[%04hx:%04hx]] class %#08x/%08x\\n\",\n\t\t\t\tvendor, device, subvendor, subdevice,\n\t\t\t\tclass, class_mask);\n\t}\n}\n\nstatic int __init vfio_pci_init(void)\n{\n\tint ret;\n\n\t/* Allocate shared config space permision data used by all devices */\n\tret = vfio_pci_init_perm_bits();\n\tif (ret)\n\t\treturn ret;\n\n\t/* Register and scan for devices */\n\tret = pci_register_driver(&vfio_pci_driver);\n\tif (ret)\n\t\tgoto out_driver;\n\n\tvfio_pci_fill_ids();\n\n\treturn 0;\n\nout_driver:\n\tvfio_pci_uninit_perm_bits();\n\treturn ret;\n}\n\nmodule_init(vfio_pci_init);\nmodule_exit(vfio_pci_cleanup);\n\nMODULE_VERSION(DRIVER_VERSION);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(DRIVER_AUTHOR);\nMODULE_DESCRIPTION(DRIVER_DESC);\n", "/*\n * VFIO PCI interrupt handling\n *\n * Copyright (C) 2012 Red Hat, Inc.  All rights reserved.\n *     Author: Alex Williamson <alex.williamson@redhat.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * Derived from original vfio:\n * Copyright 2010 Cisco Systems, Inc.  All rights reserved.\n * Author: Tom Lyon, pugs@cisco.com\n */\n\n#include <linux/device.h>\n#include <linux/interrupt.h>\n#include <linux/eventfd.h>\n#include <linux/msi.h>\n#include <linux/pci.h>\n#include <linux/file.h>\n#include <linux/vfio.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n\n#include \"vfio_pci_private.h\"\n\n/*\n * INTx\n */\nstatic void vfio_send_intx_eventfd(void *opaque, void *unused)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\n\tif (likely(is_intx(vdev) && !vdev->virq_disabled))\n\t\teventfd_signal(vdev->ctx[0].trigger, 1);\n}\n\nvoid vfio_pci_intx_mask(struct vfio_pci_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\t/*\n\t * Masking can come from interrupt, ioctl, or config space\n\t * via INTx disable.  The latter means this can get called\n\t * even when not using intx delivery.  In this case, just\n\t * try to have the physical bit follow the virtual bit.\n\t */\n\tif (unlikely(!is_intx(vdev))) {\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 0);\n\t} else if (!vdev->ctx[0].masked) {\n\t\t/*\n\t\t * Can't use check_and_mask here because we always want to\n\t\t * mask, not just when something is pending.\n\t\t */\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 0);\n\t\telse\n\t\t\tdisable_irq_nosync(pdev->irq);\n\n\t\tvdev->ctx[0].masked = true;\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n}\n\n/*\n * If this is triggered by an eventfd, we can't call eventfd_signal\n * or else we'll deadlock on the eventfd wait queue.  Return >0 when\n * a signal is necessary, which can then be handled via a work queue\n * or directly depending on the caller.\n */\nstatic int vfio_pci_intx_unmask_handler(void *opaque, void *unused)\n{\n\tstruct vfio_pci_device *vdev = opaque;\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\t/*\n\t * Unmasking comes from ioctl or config, so again, have the\n\t * physical bit follow the virtual even when not using INTx.\n\t */\n\tif (unlikely(!is_intx(vdev))) {\n\t\tif (vdev->pci_2_3)\n\t\t\tpci_intx(pdev, 1);\n\t} else if (vdev->ctx[0].masked && !vdev->virq_disabled) {\n\t\t/*\n\t\t * A pending interrupt here would immediately trigger,\n\t\t * but we can avoid that overhead by just re-sending\n\t\t * the interrupt to the user.\n\t\t */\n\t\tif (vdev->pci_2_3) {\n\t\t\tif (!pci_check_and_unmask_intx(pdev))\n\t\t\t\tret = 1;\n\t\t} else\n\t\t\tenable_irq(pdev->irq);\n\n\t\tvdev->ctx[0].masked = (ret > 0);\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\treturn ret;\n}\n\nvoid vfio_pci_intx_unmask(struct vfio_pci_device *vdev)\n{\n\tif (vfio_pci_intx_unmask_handler(vdev, NULL) > 0)\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n}\n\nstatic irqreturn_t vfio_intx_handler(int irq, void *dev_id)\n{\n\tstruct vfio_pci_device *vdev = dev_id;\n\tunsigned long flags;\n\tint ret = IRQ_NONE;\n\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\n\tif (!vdev->pci_2_3) {\n\t\tdisable_irq_nosync(vdev->pdev->irq);\n\t\tvdev->ctx[0].masked = true;\n\t\tret = IRQ_HANDLED;\n\t} else if (!vdev->ctx[0].masked &&  /* may be shared */\n\t\t   pci_check_and_mask_intx(vdev->pdev)) {\n\t\tvdev->ctx[0].masked = true;\n\t\tret = IRQ_HANDLED;\n\t}\n\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\tif (ret == IRQ_HANDLED)\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n\n\treturn ret;\n}\n\nstatic int vfio_intx_enable(struct vfio_pci_device *vdev)\n{\n\tif (!is_irq_none(vdev))\n\t\treturn -EINVAL;\n\n\tif (!vdev->pdev->irq)\n\t\treturn -ENODEV;\n\n\tvdev->ctx = kzalloc(sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);\n\tif (!vdev->ctx)\n\t\treturn -ENOMEM;\n\n\tvdev->num_ctx = 1;\n\n\t/*\n\t * If the virtual interrupt is masked, restore it.  Devices\n\t * supporting DisINTx can be masked at the hardware level\n\t * here, non-PCI-2.3 devices will have to wait until the\n\t * interrupt is enabled.\n\t */\n\tvdev->ctx[0].masked = vdev->virq_disabled;\n\tif (vdev->pci_2_3)\n\t\tpci_intx(vdev->pdev, !vdev->ctx[0].masked);\n\n\tvdev->irq_type = VFIO_PCI_INTX_IRQ_INDEX;\n\n\treturn 0;\n}\n\nstatic int vfio_intx_set_signal(struct vfio_pci_device *vdev, int fd)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned long irqflags = IRQF_SHARED;\n\tstruct eventfd_ctx *trigger;\n\tunsigned long flags;\n\tint ret;\n\n\tif (vdev->ctx[0].trigger) {\n\t\tfree_irq(pdev->irq, vdev);\n\t\tkfree(vdev->ctx[0].name);\n\t\teventfd_ctx_put(vdev->ctx[0].trigger);\n\t\tvdev->ctx[0].trigger = NULL;\n\t}\n\n\tif (fd < 0) /* Disable only */\n\t\treturn 0;\n\n\tvdev->ctx[0].name = kasprintf(GFP_KERNEL, \"vfio-intx(%s)\",\n\t\t\t\t      pci_name(pdev));\n\tif (!vdev->ctx[0].name)\n\t\treturn -ENOMEM;\n\n\ttrigger = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(trigger)) {\n\t\tkfree(vdev->ctx[0].name);\n\t\treturn PTR_ERR(trigger);\n\t}\n\n\tvdev->ctx[0].trigger = trigger;\n\n\tif (!vdev->pci_2_3)\n\t\tirqflags = 0;\n\n\tret = request_irq(pdev->irq, vfio_intx_handler,\n\t\t\t  irqflags, vdev->ctx[0].name, vdev);\n\tif (ret) {\n\t\tvdev->ctx[0].trigger = NULL;\n\t\tkfree(vdev->ctx[0].name);\n\t\teventfd_ctx_put(trigger);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * INTx disable will stick across the new irq setup,\n\t * disable_irq won't.\n\t */\n\tspin_lock_irqsave(&vdev->irqlock, flags);\n\tif (!vdev->pci_2_3 && vdev->ctx[0].masked)\n\t\tdisable_irq_nosync(pdev->irq);\n\tspin_unlock_irqrestore(&vdev->irqlock, flags);\n\n\treturn 0;\n}\n\nstatic void vfio_intx_disable(struct vfio_pci_device *vdev)\n{\n\tvfio_virqfd_disable(&vdev->ctx[0].unmask);\n\tvfio_virqfd_disable(&vdev->ctx[0].mask);\n\tvfio_intx_set_signal(vdev, -1);\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tvdev->num_ctx = 0;\n\tkfree(vdev->ctx);\n}\n\n/*\n * MSI/MSI-X\n */\nstatic irqreturn_t vfio_msihandler(int irq, void *arg)\n{\n\tstruct eventfd_ctx *trigger = arg;\n\n\teventfd_signal(trigger, 1);\n\treturn IRQ_HANDLED;\n}\n\nstatic int vfio_msi_enable(struct vfio_pci_device *vdev, int nvec, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned int flag = msix ? PCI_IRQ_MSIX : PCI_IRQ_MSI;\n\tint ret;\n\n\tif (!is_irq_none(vdev))\n\t\treturn -EINVAL;\n\n\tvdev->ctx = kcalloc(nvec, sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);\n\tif (!vdev->ctx)\n\t\treturn -ENOMEM;\n\n\t/* return the number of supported vectors if we can't get all: */\n\tret = pci_alloc_irq_vectors(pdev, 1, nvec, flag);\n\tif (ret < nvec) {\n\t\tif (ret > 0)\n\t\t\tpci_free_irq_vectors(pdev);\n\t\tkfree(vdev->ctx);\n\t\treturn ret;\n\t}\n\n\tvdev->num_ctx = nvec;\n\tvdev->irq_type = msix ? VFIO_PCI_MSIX_IRQ_INDEX :\n\t\t\t\tVFIO_PCI_MSI_IRQ_INDEX;\n\n\tif (!msix) {\n\t\t/*\n\t\t * Compute the virtual hardware field for max msi vectors -\n\t\t * it is the log base 2 of the number of vectors.\n\t\t */\n\t\tvdev->msi_qmax = fls(nvec * 2 - 1) - 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_msi_set_vector_signal(struct vfio_pci_device *vdev,\n\t\t\t\t      int vector, int fd, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct eventfd_ctx *trigger;\n\tint irq, ret;\n\n\tif (vector < 0 || vector >= vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tirq = pci_irq_vector(pdev, vector);\n\n\tif (vdev->ctx[vector].trigger) {\n\t\tfree_irq(irq, vdev->ctx[vector].trigger);\n\t\tirq_bypass_unregister_producer(&vdev->ctx[vector].producer);\n\t\tkfree(vdev->ctx[vector].name);\n\t\teventfd_ctx_put(vdev->ctx[vector].trigger);\n\t\tvdev->ctx[vector].trigger = NULL;\n\t}\n\n\tif (fd < 0)\n\t\treturn 0;\n\n\tvdev->ctx[vector].name = kasprintf(GFP_KERNEL, \"vfio-msi%s[%d](%s)\",\n\t\t\t\t\t   msix ? \"x\" : \"\", vector,\n\t\t\t\t\t   pci_name(pdev));\n\tif (!vdev->ctx[vector].name)\n\t\treturn -ENOMEM;\n\n\ttrigger = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(trigger)) {\n\t\tkfree(vdev->ctx[vector].name);\n\t\treturn PTR_ERR(trigger);\n\t}\n\n\t/*\n\t * The MSIx vector table resides in device memory which may be cleared\n\t * via backdoor resets. We don't allow direct access to the vector\n\t * table so even if a userspace driver attempts to save/restore around\n\t * such a reset it would be unsuccessful. To avoid this, restore the\n\t * cached value of the message prior to enabling.\n\t */\n\tif (msix) {\n\t\tstruct msi_msg msg;\n\n\t\tget_cached_msi_msg(irq, &msg);\n\t\tpci_write_msi_msg(irq, &msg);\n\t}\n\n\tret = request_irq(irq, vfio_msihandler, 0,\n\t\t\t  vdev->ctx[vector].name, trigger);\n\tif (ret) {\n\t\tkfree(vdev->ctx[vector].name);\n\t\teventfd_ctx_put(trigger);\n\t\treturn ret;\n\t}\n\n\tvdev->ctx[vector].producer.token = trigger;\n\tvdev->ctx[vector].producer.irq = irq;\n\tret = irq_bypass_register_producer(&vdev->ctx[vector].producer);\n\tif (unlikely(ret))\n\t\tdev_info(&pdev->dev,\n\t\t\"irq bypass producer (token %p) registration fails: %d\\n\",\n\t\tvdev->ctx[vector].producer.token, ret);\n\n\tvdev->ctx[vector].trigger = trigger;\n\n\treturn 0;\n}\n\nstatic int vfio_msi_set_block(struct vfio_pci_device *vdev, unsigned start,\n\t\t\t      unsigned count, int32_t *fds, bool msix)\n{\n\tint i, j, ret = 0;\n\n\tif (start >= vdev->num_ctx || start + count > vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tfor (i = 0, j = start; i < count && !ret; i++, j++) {\n\t\tint fd = fds ? fds[i] : -1;\n\t\tret = vfio_msi_set_vector_signal(vdev, j, fd, msix);\n\t}\n\n\tif (ret) {\n\t\tfor (--j; j >= (int)start; j--)\n\t\t\tvfio_msi_set_vector_signal(vdev, j, -1, msix);\n\t}\n\n\treturn ret;\n}\n\nstatic void vfio_msi_disable(struct vfio_pci_device *vdev, bool msix)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint i;\n\n\tfor (i = 0; i < vdev->num_ctx; i++) {\n\t\tvfio_virqfd_disable(&vdev->ctx[i].unmask);\n\t\tvfio_virqfd_disable(&vdev->ctx[i].mask);\n\t}\n\n\tvfio_msi_set_block(vdev, 0, vdev->num_ctx, NULL, msix);\n\n\tpci_free_irq_vectors(pdev);\n\n\t/*\n\t * Both disable paths above use pci_intx_for_msi() to clear DisINTx\n\t * via their shutdown paths.  Restore for NoINTx devices.\n\t */\n\tif (vdev->nointx)\n\t\tpci_intx(pdev, 0);\n\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tvdev->num_ctx = 0;\n\tkfree(vdev->ctx);\n}\n\n/*\n * IOCTL support\n */\nstatic int vfio_pci_set_intx_unmask(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (!is_intx(vdev) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_pci_intx_unmask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t unmask = *(uint8_t *)data;\n\t\tif (unmask)\n\t\t\tvfio_pci_intx_unmask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd = *(int32_t *)data;\n\t\tif (fd >= 0)\n\t\t\treturn vfio_virqfd_enable((void *) vdev,\n\t\t\t\t\t\t  vfio_pci_intx_unmask_handler,\n\t\t\t\t\t\t  vfio_send_intx_eventfd, NULL,\n\t\t\t\t\t\t  &vdev->ctx[0].unmask, fd);\n\n\t\tvfio_virqfd_disable(&vdev->ctx[0].unmask);\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_set_intx_mask(struct vfio_pci_device *vdev,\n\t\t\t\t  unsigned index, unsigned start,\n\t\t\t\t  unsigned count, uint32_t flags, void *data)\n{\n\tif (!is_intx(vdev) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_pci_intx_mask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t mask = *(uint8_t *)data;\n\t\tif (mask)\n\t\t\tvfio_pci_intx_mask(vdev);\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\treturn -ENOTTY; /* XXX implement me */\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_set_intx_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t     unsigned index, unsigned start,\n\t\t\t\t     unsigned count, uint32_t flags, void *data)\n{\n\tif (is_intx(vdev) && !count && (flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\tvfio_intx_disable(vdev);\n\t\treturn 0;\n\t}\n\n\tif (!(is_intx(vdev) || is_irq_none(vdev)) || start != 0 || count != 1)\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd = *(int32_t *)data;\n\t\tint ret;\n\n\t\tif (is_intx(vdev))\n\t\t\treturn vfio_intx_set_signal(vdev, fd);\n\n\t\tret = vfio_intx_enable(vdev);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vfio_intx_set_signal(vdev, fd);\n\t\tif (ret)\n\t\t\tvfio_intx_disable(vdev);\n\n\t\treturn ret;\n\t}\n\n\tif (!is_intx(vdev))\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tvfio_send_intx_eventfd(vdev, NULL);\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t trigger = *(uint8_t *)data;\n\t\tif (trigger)\n\t\t\tvfio_send_intx_eventfd(vdev, NULL);\n\t}\n\treturn 0;\n}\n\nstatic int vfio_pci_set_msi_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tint i;\n\tbool msix = (index == VFIO_PCI_MSIX_IRQ_INDEX) ? true : false;\n\n\tif (irq_is(vdev, index) && !count && (flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\tvfio_msi_disable(vdev, msix);\n\t\treturn 0;\n\t}\n\n\tif (!(irq_is(vdev, index) || is_irq_none(vdev)))\n\t\treturn -EINVAL;\n\n\tif (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t *fds = data;\n\t\tint ret;\n\n\t\tif (vdev->irq_type == index)\n\t\t\treturn vfio_msi_set_block(vdev, start, count,\n\t\t\t\t\t\t  fds, msix);\n\n\t\tret = vfio_msi_enable(vdev, start + count, msix);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = vfio_msi_set_block(vdev, start, count, fds, msix);\n\t\tif (ret)\n\t\t\tvfio_msi_disable(vdev, msix);\n\n\t\treturn ret;\n\t}\n\n\tif (!irq_is(vdev, index) || start + count > vdev->num_ctx)\n\t\treturn -EINVAL;\n\n\tfor (i = start; i < start + count; i++) {\n\t\tif (!vdev->ctx[i].trigger)\n\t\t\tcontinue;\n\t\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\t\teventfd_signal(vdev->ctx[i].trigger, 1);\n\t\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\t\tuint8_t *bools = data;\n\t\t\tif (bools[i - start])\n\t\t\t\teventfd_signal(vdev->ctx[i].trigger, 1);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int vfio_pci_set_ctx_trigger_single(struct eventfd_ctx **ctx,\n\t\t\t\t\t   unsigned int count, uint32_t flags,\n\t\t\t\t\t   void *data)\n{\n\t/* DATA_NONE/DATA_BOOL enables loopback testing */\n\tif (flags & VFIO_IRQ_SET_DATA_NONE) {\n\t\tif (*ctx) {\n\t\t\tif (count) {\n\t\t\t\teventfd_signal(*ctx, 1);\n\t\t\t} else {\n\t\t\t\teventfd_ctx_put(*ctx);\n\t\t\t\t*ctx = NULL;\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t} else if (flags & VFIO_IRQ_SET_DATA_BOOL) {\n\t\tuint8_t trigger;\n\n\t\tif (!count)\n\t\t\treturn -EINVAL;\n\n\t\ttrigger = *(uint8_t *)data;\n\t\tif (trigger && *ctx)\n\t\t\teventfd_signal(*ctx, 1);\n\n\t\treturn 0;\n\t} else if (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint32_t fd;\n\n\t\tif (!count)\n\t\t\treturn -EINVAL;\n\n\t\tfd = *(int32_t *)data;\n\t\tif (fd == -1) {\n\t\t\tif (*ctx)\n\t\t\t\teventfd_ctx_put(*ctx);\n\t\t\t*ctx = NULL;\n\t\t} else if (fd >= 0) {\n\t\t\tstruct eventfd_ctx *efdctx;\n\n\t\t\tefdctx = eventfd_ctx_fdget(fd);\n\t\t\tif (IS_ERR(efdctx))\n\t\t\t\treturn PTR_ERR(efdctx);\n\n\t\t\tif (*ctx)\n\t\t\t\teventfd_ctx_put(*ctx);\n\n\t\t\t*ctx = efdctx;\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int vfio_pci_set_err_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (index != VFIO_PCI_ERR_IRQ_INDEX || start != 0 || count > 1)\n\t\treturn -EINVAL;\n\n\treturn vfio_pci_set_ctx_trigger_single(&vdev->err_trigger,\n\t\t\t\t\t       count, flags, data);\n}\n\nstatic int vfio_pci_set_req_trigger(struct vfio_pci_device *vdev,\n\t\t\t\t    unsigned index, unsigned start,\n\t\t\t\t    unsigned count, uint32_t flags, void *data)\n{\n\tif (index != VFIO_PCI_REQ_IRQ_INDEX || start != 0 || count > 1)\n\t\treturn -EINVAL;\n\n\treturn vfio_pci_set_ctx_trigger_single(&vdev->req_trigger,\n\t\t\t\t\t       count, flags, data);\n}\n\nint vfio_pci_set_irqs_ioctl(struct vfio_pci_device *vdev, uint32_t flags,\n\t\t\t    unsigned index, unsigned start, unsigned count,\n\t\t\t    void *data)\n{\n\tint (*func)(struct vfio_pci_device *vdev, unsigned index,\n\t\t    unsigned start, unsigned count, uint32_t flags,\n\t\t    void *data) = NULL;\n\n\tswitch (index) {\n\tcase VFIO_PCI_INTX_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\t\tfunc = vfio_pci_set_intx_mask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\tfunc = vfio_pci_set_intx_unmask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_intx_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_MSI_IRQ_INDEX:\n\tcase VFIO_PCI_MSIX_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\t/* XXX Need masking support exported */\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_msi_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_ERR_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\t\tfunc = vfio_pci_set_err_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_REQ_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = vfio_pci_set_req_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (!func)\n\t\treturn -ENOTTY;\n\n\treturn func(vdev, index, start, count, flags, data);\n}\n"], "filenames": ["drivers/vfio/pci/vfio_pci.c", "drivers/vfio/pci/vfio_pci_intrs.c"], "buggy_code_start_loc": [831, 259], "buggy_code_end_loc": [858, 260], "fixing_code_start_loc": [832, 259], "fixing_code_end_loc": [867, 260], "type": "CWE-190", "message": "drivers/vfio/pci/vfio_pci_intrs.c in the Linux kernel through 4.8.11 misuses the kzalloc function, which allows local users to cause a denial of service (integer overflow) or have unspecified other impact by leveraging access to a vfio PCI device file.", "other": {"cve": {"id": "CVE-2016-9084", "sourceIdentifier": "cve@mitre.org", "published": "2016-11-28T03:59:12.127", "lastModified": "2018-01-05T02:31:20.543", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "drivers/vfio/pci/vfio_pci_intrs.c in the Linux kernel through 4.8.11 misuses the kzalloc function, which allows local users to cause a denial of service (integer overflow) or have unspecified other impact by leveraging access to a vfio PCI device file."}, {"lang": "es", "value": "drivers/vfio/pci/vfio_pci_intrs.c en el kernel Linux hasta la versi\u00f3n 4.8.11 usa de forma incorrecta la funci\u00f3n kzalloc, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (desbordamiento de entero) o tener otro posible impacto no especificado aprovechando el acceso al archivo de dispositivo vfio PCI."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.8.11", "matchCriteriaId": "00B2C5A0-53C6-4A5A-A79A-9A213FA01EE5"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=05692d7005a364add85c6e25a6c4447ce08f913a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0386.html", "source": "cve@mitre.org"}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0387.html", "source": "cve@mitre.org"}, {"url": "http://www.openwall.com/lists/oss-security/2016/10/26/11", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/93930", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1389259", "source": "cve@mitre.org", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/05692d7005a364add85c6e25a6c4447ce08f913a", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://patchwork.kernel.org/patch/9373631/", "source": "cve@mitre.org", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/05692d7005a364add85c6e25a6c4447ce08f913a"}}