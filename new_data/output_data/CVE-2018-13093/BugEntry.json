{"buggy_code": ["/*\n * Copyright (c) 2000-2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it would be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write the Free Software Foundation,\n * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n */\n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_sb.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_error.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_trans_priv.h\"\n#include \"xfs_inode_item.h\"\n#include \"xfs_quota.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_icache.h\"\n#include \"xfs_bmap_util.h\"\n#include \"xfs_dquot_item.h\"\n#include \"xfs_dquot.h\"\n#include \"xfs_reflink.h\"\n\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/iversion.h>\n\n/*\n * Allocate and initialise an xfs_inode.\n */\nstruct xfs_inode *\nxfs_inode_alloc(\n\tstruct xfs_mount\t*mp,\n\txfs_ino_t\t\tino)\n{\n\tstruct xfs_inode\t*ip;\n\n\t/*\n\t * if this didn't occur in transactions, we could use\n\t * KM_MAYFAIL and return NULL here on ENOMEM. Set the\n\t * code up to do this anyway.\n\t */\n\tip = kmem_zone_alloc(xfs_inode_zone, KM_SLEEP);\n\tif (!ip)\n\t\treturn NULL;\n\tif (inode_init_always(mp->m_super, VFS_I(ip))) {\n\t\tkmem_zone_free(xfs_inode_zone, ip);\n\t\treturn NULL;\n\t}\n\n\t/* VFS doesn't initialise i_mode! */\n\tVFS_I(ip)->i_mode = 0;\n\n\tXFS_STATS_INC(mp, vn_active);\n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tASSERT(!xfs_isiflocked(ip));\n\tASSERT(ip->i_ino == 0);\n\n\t/* initialise the xfs inode */\n\tip->i_ino = ino;\n\tip->i_mount = mp;\n\tmemset(&ip->i_imap, 0, sizeof(struct xfs_imap));\n\tip->i_afp = NULL;\n\tip->i_cowfp = NULL;\n\tip->i_cnextents = 0;\n\tip->i_cformat = XFS_DINODE_FMT_EXTENTS;\n\tmemset(&ip->i_df, 0, sizeof(xfs_ifork_t));\n\tip->i_flags = 0;\n\tip->i_delayed_blks = 0;\n\tmemset(&ip->i_d, 0, sizeof(ip->i_d));\n\n\treturn ip;\n}\n\nSTATIC void\nxfs_inode_free_callback(\n\tstruct rcu_head\t\t*head)\n{\n\tstruct inode\t\t*inode = container_of(head, struct inode, i_rcu);\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\n\tswitch (VFS_I(ip)->i_mode & S_IFMT) {\n\tcase S_IFREG:\n\tcase S_IFDIR:\n\tcase S_IFLNK:\n\t\txfs_idestroy_fork(ip, XFS_DATA_FORK);\n\t\tbreak;\n\t}\n\n\tif (ip->i_afp)\n\t\txfs_idestroy_fork(ip, XFS_ATTR_FORK);\n\tif (ip->i_cowfp)\n\t\txfs_idestroy_fork(ip, XFS_COW_FORK);\n\n\tif (ip->i_itemp) {\n\t\tASSERT(!(ip->i_itemp->ili_item.li_flags & XFS_LI_IN_AIL));\n\t\txfs_inode_item_destroy(ip);\n\t\tip->i_itemp = NULL;\n\t}\n\n\tkmem_zone_free(xfs_inode_zone, ip);\n}\n\nstatic void\n__xfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\t/* asserts to verify all state is correct here */\n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tXFS_STATS_DEC(ip->i_mount, vn_active);\n\n\tcall_rcu(&VFS_I(ip)->i_rcu, xfs_inode_free_callback);\n}\n\nvoid\nxfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\tASSERT(!xfs_isiflocked(ip));\n\n\t/*\n\t * Because we use RCU freeing we need to ensure the inode always\n\t * appears to be reclaimed with an invalid inode number when in the\n\t * free state. The ip->i_flags_lock provides the barrier against lookup\n\t * races.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t__xfs_inode_free(ip);\n}\n\n/*\n * Queue a new inode reclaim pass if there are reclaimable inodes and there\n * isn't a reclaim pass already in progress. By default it runs every 5s based\n * on the xfs periodic sync default of 30s. Perhaps this should have it's own\n * tunable, but that can be done if this method proves to be ineffective or too\n * aggressive.\n */\nstatic void\nxfs_reclaim_work_queue(\n\tstruct xfs_mount        *mp)\n{\n\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_RECLAIM_TAG)) {\n\t\tqueue_delayed_work(mp->m_reclaim_workqueue, &mp->m_reclaim_work,\n\t\t\tmsecs_to_jiffies(xfs_syncd_centisecs / 6 * 10));\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * This is a fast pass over the inode cache to try to get reclaim moving on as\n * many inodes as possible in a short period of time. It kicks itself every few\n * seconds, as well as being kicked by the inode cache shrinker when memory\n * goes low. It scans as quickly as possible avoiding locked inodes or those\n * already being flushed, and once done schedules a future pass.\n */\nvoid\nxfs_reclaim_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct xfs_mount, m_reclaim_work);\n\n\txfs_reclaim_inodes(mp, SYNC_TRYLOCK);\n\txfs_reclaim_work_queue(mp);\n}\n\nstatic void\nxfs_perag_set_reclaim_tag(\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\tif (pag->pag_ici_reclaimable++)\n\t\treturn;\n\n\t/* propagate the reclaim tag up into the perag radix tree */\n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_set(&mp->m_perag_tree, pag->pag_agno,\n\t\t\t   XFS_ICI_RECLAIM_TAG);\n\tspin_unlock(&mp->m_perag_lock);\n\n\t/* schedule periodic background inode reclaim */\n\txfs_reclaim_work_queue(mp);\n\n\ttrace_xfs_perag_set_reclaim(mp, pag->pag_agno, -1, _RET_IP_);\n}\n\nstatic void\nxfs_perag_clear_reclaim_tag(\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\tif (--pag->pag_ici_reclaimable)\n\t\treturn;\n\n\t/* clear the reclaim tag from the perag radix tree */\n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_clear(&mp->m_perag_tree, pag->pag_agno,\n\t\t\t     XFS_ICI_RECLAIM_TAG);\n\tspin_unlock(&mp->m_perag_lock);\n\ttrace_xfs_perag_clear_reclaim(mp, pag->pag_agno, -1, _RET_IP_);\n}\n\n\n/*\n * We set the inode flag atomically with the radix tree tag.\n * Once we get tag lookups on the radix tree, this inode flag\n * can go away.\n */\nvoid\nxfs_inode_set_reclaim_tag(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_perag\t*pag;\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\tspin_lock(&ip->i_flags_lock);\n\n\tradix_tree_tag_set(&pag->pag_ici_root, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\t   XFS_ICI_RECLAIM_TAG);\n\txfs_perag_set_reclaim_tag(pag);\n\t__xfs_iflags_set(ip, XFS_IRECLAIMABLE);\n\n\tspin_unlock(&ip->i_flags_lock);\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nSTATIC void\nxfs_inode_clear_reclaim_tag(\n\tstruct xfs_perag\t*pag,\n\txfs_ino_t\t\tino)\n{\n\tradix_tree_tag_clear(&pag->pag_ici_root,\n\t\t\t     XFS_INO_TO_AGINO(pag->pag_mount, ino),\n\t\t\t     XFS_ICI_RECLAIM_TAG);\n\txfs_perag_clear_reclaim_tag(pag);\n}\n\nstatic void\nxfs_inew_wait(\n\tstruct xfs_inode\t*ip)\n{\n\twait_queue_head_t *wq = bit_waitqueue(&ip->i_flags, __XFS_INEW_BIT);\n\tDEFINE_WAIT_BIT(wait, &ip->i_flags, __XFS_INEW_BIT);\n\n\tdo {\n\t\tprepare_to_wait(wq, &wait.wq_entry, TASK_UNINTERRUPTIBLE);\n\t\tif (!xfs_iflags_test(ip, XFS_INEW))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (true);\n\tfinish_wait(wq, &wait.wq_entry);\n}\n\n/*\n * When we recycle a reclaimable inode, we need to re-initialise the VFS inode\n * part of the structure. This is made more complex by the fact we store\n * information about the on-disk values in the VFS inode and so we can't just\n * overwrite the values unconditionally. Hence we save the parameters we\n * need to retain across reinitialisation, and rewrite them into the VFS inode\n * after reinitialisation even if it fails.\n */\nstatic int\nxfs_reinit_inode(\n\tstruct xfs_mount\t*mp,\n\tstruct inode\t\t*inode)\n{\n\tint\t\terror;\n\tuint32_t\tnlink = inode->i_nlink;\n\tuint32_t\tgeneration = inode->i_generation;\n\tuint64_t\tversion = inode_peek_iversion(inode);\n\tumode_t\t\tmode = inode->i_mode;\n\tdev_t\t\tdev = inode->i_rdev;\n\n\terror = inode_init_always(mp->m_super, inode);\n\n\tset_nlink(inode, nlink);\n\tinode->i_generation = generation;\n\tinode_set_iversion_queried(inode, version);\n\tinode->i_mode = mode;\n\tinode->i_rdev = dev;\n\treturn error;\n}\n\n/*\n * Check the validity of the inode we just found it the cache\n */\nstatic int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If lookup is racing with unlink return an error immediately.\n\t */\n\tif (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {\n\t\terror = -ENOENT;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}\n\n\nstatic int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * If we are allocating a new inode, then check what was returned is\n\t * actually a free, empty inode. If we are not allocating an inode,\n\t * the check we didn't find a free inode.\n\t */\n\tif (flags & XFS_IGET_CREATE) {\n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx not marked free on disk\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t\tif (ip->i_d.di_nblocks != 0) {\n\t\t\txfs_warn(mp,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tino);\n\t\t\terror = -EFSCORRUPTED;\n\t\t\tgoto out_destroy;\n\t\t}\n\t} else if (VFS_I(ip)->i_mode == 0) {\n\t\terror = -ENOENT;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}\n\n/*\n * Look up an inode by number in the given file system.\n * The inode is looked up in the cache held in each AG.\n * If the inode is found in the cache, initialise the vfs inode\n * if necessary.\n *\n * If it is not in core, read it in from the file system's device,\n * add it to the cache and initialise the vfs inode.\n *\n * The inode is locked according to the value of the lock_flags parameter.\n * This flag parameter indicates how and if the inode's IO lock and inode lock\n * should be taken.\n *\n * mp -- the mount point structure for the current file system.  It points\n *       to the inode hash table.\n * tp -- a pointer to the current transaction if there is one.  This is\n *       simply passed through to the xfs_iread() call.\n * ino -- the number of the inode desired.  This is the unique identifier\n *        within the file system for the inode being requested.\n * lock_flags -- flags indicating how to lock the inode.  See the comment\n *\t\t for xfs_ilock() for a list of valid values.\n */\nint\nxfs_iget(\n\txfs_mount_t\t*mp,\n\txfs_trans_t\t*tp,\n\txfs_ino_t\tino,\n\tuint\t\tflags,\n\tuint\t\tlock_flags,\n\txfs_inode_t\t**ipp)\n{\n\txfs_inode_t\t*ip;\n\tint\t\terror;\n\txfs_perag_t\t*pag;\n\txfs_agino_t\tagino;\n\n\t/*\n\t * xfs_reclaim_inode() uses the ILOCK to ensure an inode\n\t * doesn't get freed while it's being referenced during a\n\t * radix tree traversal here.  It assumes this function\n\t * aqcuires only the ILOCK (and therefore it has no need to\n\t * involve the IOLOCK in this synchronization).\n\t */\n\tASSERT((lock_flags & (XFS_IOLOCK_EXCL | XFS_IOLOCK_SHARED)) == 0);\n\n\t/* reject inode numbers outside existing AGs */\n\tif (!ino || XFS_INO_TO_AGNO(mp, ino) >= mp->m_sb.sb_agcount)\n\t\treturn -EINVAL;\n\n\tXFS_STATS_INC(mp, xs_ig_attempts);\n\n\t/* get the perag structure and ensure that it's inode capable */\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ino));\n\tagino = XFS_INO_TO_AGINO(mp, ino);\n\nagain:\n\terror = 0;\n\trcu_read_lock();\n\tip = radix_tree_lookup(&pag->pag_ici_root, agino);\n\n\tif (ip) {\n\t\terror = xfs_iget_cache_hit(pag, ip, ino, flags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t} else {\n\t\trcu_read_unlock();\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -ENODATA;\n\t\t\tgoto out_error_or_again;\n\t\t}\n\t\tXFS_STATS_INC(mp, xs_ig_missed);\n\n\t\terror = xfs_iget_cache_miss(mp, pag, tp, ino, &ip,\n\t\t\t\t\t\t\tflags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t}\n\txfs_perag_put(pag);\n\n\t*ipp = ip;\n\n\t/*\n\t * If we have a real type for an on-disk inode, we can setup the inode\n\t * now.\t If it's a new inode being created, xfs_ialloc will handle it.\n\t */\n\tif (xfs_iflags_test(ip, XFS_INEW) && VFS_I(ip)->i_mode != 0)\n\t\txfs_setup_existing_inode(ip);\n\treturn 0;\n\nout_error_or_again:\n\tif (!(flags & XFS_IGET_INCORE) && error == -EAGAIN) {\n\t\tdelay(1);\n\t\tgoto again;\n\t}\n\txfs_perag_put(pag);\n\treturn error;\n}\n\n/*\n * \"Is this a cached inode that's also allocated?\"\n *\n * Look up an inode by number in the given file system.  If the inode is\n * in cache and isn't in purgatory, return 1 if the inode is allocated\n * and 0 if it is not.  For all other cases (not in cache, being torn\n * down, etc.), return a negative error code.\n *\n * The caller has to prevent inode allocation and freeing activity,\n * presumably by locking the AGI buffer.   This is to ensure that an\n * inode cannot transition from allocated to freed until the caller is\n * ready to allow that.  If the inode is in an intermediate state (new,\n * reclaimable, or being reclaimed), -EAGAIN will be returned; if the\n * inode is not in the cache, -ENOENT will be returned.  The caller must\n * deal with these scenarios appropriately.\n *\n * This is a specialized use case for the online scrubber; if you're\n * reading this, you probably want xfs_iget.\n */\nint\nxfs_icache_inode_is_allocated(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_trans\t*tp,\n\txfs_ino_t\t\tino,\n\tbool\t\t\t*inuse)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\n\terror = xfs_iget(mp, tp, ino, XFS_IGET_INCORE, 0, &ip);\n\tif (error)\n\t\treturn error;\n\n\t*inuse = !!(VFS_I(ip)->i_mode);\n\tIRELE(ip);\n\treturn 0;\n}\n\n/*\n * The inode lookup is done in batches to keep the amount of lock traffic and\n * radix tree lookups to a minimum. The batch size is a trade off between\n * lookup reduction and stack usage. This is in the reclaim path, so we can't\n * be too greedy.\n */\n#define XFS_LOOKUP_BATCH\t32\n\nSTATIC int\nxfs_inode_ag_walk_grab(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tbool\t\t\tnewinos = !!(flags & XFS_AGITER_INEW_WAIT);\n\n\tASSERT(rcu_read_lock_held());\n\n\t/*\n\t * check for stale RCU freed inode\n\t *\n\t * If the inode has been reallocated, it doesn't matter if it's not in\n\t * the AG we are walking - we are walking for writeback, so if it\n\t * passes all the \"valid inode\" checks and is dirty, then we'll write\n\t * it back anyway.  If it has been reallocated and still being\n\t * initialised, the XFS_INEW check below will catch it.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (!ip->i_ino)\n\t\tgoto out_unlock_noent;\n\n\t/* avoid new or reclaimable inodes. Leave for reclaim code to flush */\n\tif ((!newinos && __xfs_iflags_test(ip, XFS_INEW)) ||\n\t    __xfs_iflags_test(ip, XFS_IRECLAIMABLE | XFS_IRECLAIM))\n\t\tgoto out_unlock_noent;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t/* nothing to sync during shutdown */\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\treturn -EFSCORRUPTED;\n\n\t/* If we can't grab the inode, it must on it's way to reclaim. */\n\tif (!igrab(inode))\n\t\treturn -ENOENT;\n\n\t/* inode is valid */\n\treturn 0;\n\nout_unlock_noent:\n\tspin_unlock(&ip->i_flags_lock);\n\treturn -ENOENT;\n}\n\nSTATIC int\nxfs_inode_ag_walk(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\ttag,\n\tint\t\t\titer_flags)\n{\n\tuint32_t\t\tfirst_index;\n\tint\t\t\tlast_error = 0;\n\tint\t\t\tskipped;\n\tint\t\t\tdone;\n\tint\t\t\tnr_found;\n\nrestart:\n\tdone = 0;\n\tskipped = 0;\n\tfirst_index = 0;\n\tnr_found = 0;\n\tdo {\n\t\tstruct xfs_inode *batch[XFS_LOOKUP_BATCH];\n\t\tint\t\terror = 0;\n\t\tint\t\ti;\n\n\t\trcu_read_lock();\n\n\t\tif (tag == -1)\n\t\t\tnr_found = radix_tree_gang_lookup(&pag->pag_ici_root,\n\t\t\t\t\t(void **)batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH);\n\t\telse\n\t\t\tnr_found = radix_tree_gang_lookup_tag(\n\t\t\t\t\t&pag->pag_ici_root,\n\t\t\t\t\t(void **) batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH, tag);\n\n\t\tif (!nr_found) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Grab the inodes before we drop the lock. if we found\n\t\t * nothing, nr == 0 and the loop will be skipped.\n\t\t */\n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tstruct xfs_inode *ip = batch[i];\n\n\t\t\tif (done || xfs_inode_ag_walk_grab(ip, iter_flags))\n\t\t\t\tbatch[i] = NULL;\n\n\t\t\t/*\n\t\t\t * Update the index for the next lookup. Catch\n\t\t\t * overflows into the next AG range which can occur if\n\t\t\t * we have inodes in the last block of the AG and we\n\t\t\t * are currently pointing to the last inode.\n\t\t\t *\n\t\t\t * Because we may see inodes that are from the wrong AG\n\t\t\t * due to RCU freeing and reallocation, only update the\n\t\t\t * index if it lies in this AG. It was a race that lead\n\t\t\t * us to see this inode, so another lookup from the\n\t\t\t * same index will not find it again.\n\t\t\t */\n\t\t\tif (XFS_INO_TO_AGNO(mp, ip->i_ino) != pag->pag_agno)\n\t\t\t\tcontinue;\n\t\t\tfirst_index = XFS_INO_TO_AGINO(mp, ip->i_ino + 1);\n\t\t\tif (first_index < XFS_INO_TO_AGINO(mp, ip->i_ino))\n\t\t\t\tdone = 1;\n\t\t}\n\n\t\t/* unlock now we've grabbed the inodes. */\n\t\trcu_read_unlock();\n\n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tif (!batch[i])\n\t\t\t\tcontinue;\n\t\t\tif ((iter_flags & XFS_AGITER_INEW_WAIT) &&\n\t\t\t    xfs_iflags_test(batch[i], XFS_INEW))\n\t\t\t\txfs_inew_wait(batch[i]);\n\t\t\terror = execute(batch[i], flags, args);\n\t\t\tIRELE(batch[i]);\n\t\t\tif (error == -EAGAIN) {\n\t\t\t\tskipped++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (error && last_error != -EFSCORRUPTED)\n\t\t\t\tlast_error = error;\n\t\t}\n\n\t\t/* bail out if the filesystem is corrupted.  */\n\t\tif (error == -EFSCORRUPTED)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\n\t} while (nr_found && !done);\n\n\tif (skipped) {\n\t\tdelay(1);\n\t\tgoto restart;\n\t}\n\treturn last_error;\n}\n\n/*\n * Background scanning to trim post-EOF preallocated space. This is queued\n * based on the 'speculative_prealloc_lifetime' tunable (5m by default).\n */\nvoid\nxfs_queue_eofblocks(\n\tstruct xfs_mount *mp)\n{\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_EOFBLOCKS_TAG))\n\t\tqueue_delayed_work(mp->m_eofblocks_workqueue,\n\t\t\t\t   &mp->m_eofblocks_work,\n\t\t\t\t   msecs_to_jiffies(xfs_eofb_secs * 1000));\n\trcu_read_unlock();\n}\n\nvoid\nxfs_eofblocks_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\tstruct xfs_mount, m_eofblocks_work);\n\txfs_icache_free_eofblocks(mp, NULL);\n\txfs_queue_eofblocks(mp);\n}\n\n/*\n * Background scanning to trim preallocated CoW space. This is queued\n * based on the 'speculative_cow_prealloc_lifetime' tunable (5m by default).\n * (We'll just piggyback on the post-EOF prealloc space workqueue.)\n */\nvoid\nxfs_queue_cowblocks(\n\tstruct xfs_mount *mp)\n{\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_COWBLOCKS_TAG))\n\t\tqueue_delayed_work(mp->m_eofblocks_workqueue,\n\t\t\t\t   &mp->m_cowblocks_work,\n\t\t\t\t   msecs_to_jiffies(xfs_cowb_secs * 1000));\n\trcu_read_unlock();\n}\n\nvoid\nxfs_cowblocks_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\tstruct xfs_mount, m_cowblocks_work);\n\txfs_icache_free_cowblocks(mp, NULL);\n\txfs_queue_cowblocks(mp);\n}\n\nint\nxfs_inode_ag_iterator_flags(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\titer_flags)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\n\tag = 0;\n\twhile ((pag = xfs_perag_get(mp, ag))) {\n\t\tag = pag->pag_agno + 1;\n\t\terror = xfs_inode_ag_walk(mp, pag, execute, flags, args, -1,\n\t\t\t\t\t  iter_flags);\n\t\txfs_perag_put(pag);\n\t\tif (error) {\n\t\t\tlast_error = error;\n\t\t\tif (error == -EFSCORRUPTED)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn last_error;\n}\n\nint\nxfs_inode_ag_iterator(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\treturn xfs_inode_ag_iterator_flags(mp, execute, flags, args, 0);\n}\n\nint\nxfs_inode_ag_iterator_tag(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\ttag)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\n\tag = 0;\n\twhile ((pag = xfs_perag_get_tag(mp, ag, tag))) {\n\t\tag = pag->pag_agno + 1;\n\t\terror = xfs_inode_ag_walk(mp, pag, execute, flags, args, tag,\n\t\t\t\t\t  0);\n\t\txfs_perag_put(pag);\n\t\tif (error) {\n\t\t\tlast_error = error;\n\t\t\tif (error == -EFSCORRUPTED)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn last_error;\n}\n\n/*\n * Grab the inode for reclaim exclusively.\n * Return 0 if we grabbed it, non-zero otherwise.\n */\nSTATIC int\nxfs_reclaim_inode_grab(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tASSERT(rcu_read_lock_held());\n\n\t/* quick check for stale RCU freed inode */\n\tif (!ip->i_ino)\n\t\treturn 1;\n\n\t/*\n\t * If we are asked for non-blocking operation, do unlocked checks to\n\t * see if the inode already is being flushed or in reclaim to avoid\n\t * lock traffic.\n\t */\n\tif ((flags & SYNC_TRYLOCK) &&\n\t    __xfs_iflags_test(ip, XFS_IFLOCK | XFS_IRECLAIM))\n\t\treturn 1;\n\n\t/*\n\t * The radix tree lock here protects a thread in xfs_iget from racing\n\t * with us starting reclaim on the inode.  Once we have the\n\t * XFS_IRECLAIM flag set it will not touch us.\n\t *\n\t * Due to RCU lookup, we may find inodes that have been freed and only\n\t * have XFS_IRECLAIM set.  Indeed, we may see reallocated inodes that\n\t * aren't candidates for reclaim at all, so we must check the\n\t * XFS_IRECLAIMABLE is set first before proceeding to reclaim.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (!__xfs_iflags_test(ip, XFS_IRECLAIMABLE) ||\n\t    __xfs_iflags_test(ip, XFS_IRECLAIM)) {\n\t\t/* not a reclaim candidate. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\treturn 1;\n\t}\n\t__xfs_iflags_set(ip, XFS_IRECLAIM);\n\tspin_unlock(&ip->i_flags_lock);\n\treturn 0;\n}\n\n/*\n * Inodes in different states need to be treated differently. The following\n * table lists the inode states and the reclaim actions necessary:\n *\n *\tinode state\t     iflush ret\t\trequired action\n *      ---------------      ----------         ---------------\n *\tbad\t\t\t-\t\treclaim\n *\tshutdown\t\tEIO\t\tunpin and reclaim\n *\tclean, unpinned\t\t0\t\treclaim\n *\tstale, unpinned\t\t0\t\treclaim\n *\tclean, pinned(*)\t0\t\trequeue\n *\tstale, pinned\t\tEAGAIN\t\trequeue\n *\tdirty, async\t\t-\t\trequeue\n *\tdirty, sync\t\t0\t\treclaim\n *\n * (*) dgc: I don't think the clean, pinned state is possible but it gets\n * handled anyway given the order of checks implemented.\n *\n * Also, because we get the flush lock first, we know that any inode that has\n * been flushed delwri has had the flush completed by the time we check that\n * the inode is clean.\n *\n * Note that because the inode is flushed delayed write by AIL pushing, the\n * flush lock may already be held here and waiting on it can result in very\n * long latencies.  Hence for sync reclaims, where we wait on the flush lock,\n * the caller should push the AIL first before trying to reclaim inodes to\n * minimise the amount of time spent waiting.  For background relaim, we only\n * bother to reclaim clean inodes anyway.\n *\n * Hence the order of actions after gaining the locks should be:\n *\tbad\t\t=> reclaim\n *\tshutdown\t=> unpin and reclaim\n *\tpinned, async\t=> requeue\n *\tpinned, sync\t=> unpin\n *\tstale\t\t=> reclaim\n *\tclean\t\t=> reclaim\n *\tdirty, async\t=> requeue\n *\tdirty, sync\t=> flush, wait and reclaim\n */\nSTATIC int\nxfs_reclaim_inode(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_perag\t*pag,\n\tint\t\t\tsync_mode)\n{\n\tstruct xfs_buf\t\t*bp = NULL;\n\txfs_ino_t\t\tino = ip->i_ino; /* for radix_tree_delete */\n\tint\t\t\terror;\n\nrestart:\n\terror = 0;\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tif (!xfs_iflock_nowait(ip)) {\n\t\tif (!(sync_mode & SYNC_WAIT))\n\t\t\tgoto out;\n\t\txfs_iflock(ip);\n\t}\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount)) {\n\t\txfs_iunpin_wait(ip);\n\t\t/* xfs_iflush_abort() drops the flush lock */\n\t\txfs_iflush_abort(ip, false);\n\t\tgoto reclaim;\n\t}\n\tif (xfs_ipincount(ip)) {\n\t\tif (!(sync_mode & SYNC_WAIT))\n\t\t\tgoto out_ifunlock;\n\t\txfs_iunpin_wait(ip);\n\t}\n\tif (xfs_iflags_test(ip, XFS_ISTALE) || xfs_inode_clean(ip)) {\n\t\txfs_ifunlock(ip);\n\t\tgoto reclaim;\n\t}\n\n\t/*\n\t * Never flush out dirty data during non-blocking reclaim, as it would\n\t * just contend with AIL pushing trying to do the same job.\n\t */\n\tif (!(sync_mode & SYNC_WAIT))\n\t\tgoto out_ifunlock;\n\n\t/*\n\t * Now we have an inode that needs flushing.\n\t *\n\t * Note that xfs_iflush will never block on the inode buffer lock, as\n\t * xfs_ifree_cluster() can lock the inode buffer before it locks the\n\t * ip->i_lock, and we are doing the exact opposite here.  As a result,\n\t * doing a blocking xfs_imap_to_bp() to get the cluster buffer would\n\t * result in an ABBA deadlock with xfs_ifree_cluster().\n\t *\n\t * As xfs_ifree_cluser() must gather all inodes that are active in the\n\t * cache to mark them stale, if we hit this case we don't actually want\n\t * to do IO here - we want the inode marked stale so we can simply\n\t * reclaim it.  Hence if we get an EAGAIN error here,  just unlock the\n\t * inode, back off and try again.  Hopefully the next pass through will\n\t * see the stale flag set on the inode.\n\t */\n\terror = xfs_iflush(ip, &bp);\n\tif (error == -EAGAIN) {\n\t\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t\t/* backoff longer than in xfs_ifree_cluster */\n\t\tdelay(2);\n\t\tgoto restart;\n\t}\n\n\tif (!error) {\n\t\terror = xfs_bwrite(bp);\n\t\txfs_buf_relse(bp);\n\t}\n\nreclaim:\n\tASSERT(!xfs_isiflocked(ip));\n\n\t/*\n\t * Because we use RCU freeing we need to ensure the inode always appears\n\t * to be reclaimed with an invalid inode number when in the free state.\n\t * We do this as early as possible under the ILOCK so that\n\t * xfs_iflush_cluster() and xfs_ifree_cluster() can be guaranteed to\n\t * detect races with us here. By doing this, we guarantee that once\n\t * xfs_iflush_cluster() or xfs_ifree_cluster() has locked XFS_ILOCK that\n\t * it will see either a valid inode that will serialise correctly, or it\n\t * will see an invalid inode that it can skip.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\tXFS_STATS_INC(ip->i_mount, xs_ig_reclaims);\n\t/*\n\t * Remove the inode from the per-AG radix tree.\n\t *\n\t * Because radix_tree_delete won't complain even if the item was never\n\t * added to the tree assert that it's been there before to catch\n\t * problems with the inode life time early on.\n\t */\n\tspin_lock(&pag->pag_ici_lock);\n\tif (!radix_tree_delete(&pag->pag_ici_root,\n\t\t\t\tXFS_INO_TO_AGINO(ip->i_mount, ino)))\n\t\tASSERT(0);\n\txfs_perag_clear_reclaim_tag(pag);\n\tspin_unlock(&pag->pag_ici_lock);\n\n\t/*\n\t * Here we do an (almost) spurious inode lock in order to coordinate\n\t * with inode cache radix tree lookups.  This is because the lookup\n\t * can reference the inodes in the cache without taking references.\n\t *\n\t * We make that OK here by ensuring that we wait until the inode is\n\t * unlocked after the lookup before we go ahead and free it.\n\t */\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\txfs_qm_dqdetach(ip);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\t__xfs_inode_free(ip);\n\treturn error;\n\nout_ifunlock:\n\txfs_ifunlock(ip);\nout:\n\txfs_iflags_clear(ip, XFS_IRECLAIM);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t/*\n\t * We could return -EAGAIN here to make reclaim rescan the inode tree in\n\t * a short while. However, this just burns CPU time scanning the tree\n\t * waiting for IO to complete and the reclaim work never goes back to\n\t * the idle state. Instead, return 0 to let the next scheduled\n\t * background reclaim attempt to reclaim the inode again.\n\t */\n\treturn 0;\n}\n\n/*\n * Walk the AGs and reclaim the inodes in them. Even if the filesystem is\n * corrupted, we still want to try to reclaim all the inodes. If we don't,\n * then a shut down during filesystem unmount reclaim walk leak all the\n * unreclaimed inodes.\n */\nSTATIC int\nxfs_reclaim_inodes_ag(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tflags,\n\tint\t\t\t*nr_to_scan)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\tint\t\t\ttrylock = flags & SYNC_TRYLOCK;\n\tint\t\t\tskipped;\n\nrestart:\n\tag = 0;\n\tskipped = 0;\n\twhile ((pag = xfs_perag_get_tag(mp, ag, XFS_ICI_RECLAIM_TAG))) {\n\t\tunsigned long\tfirst_index = 0;\n\t\tint\t\tdone = 0;\n\t\tint\t\tnr_found = 0;\n\n\t\tag = pag->pag_agno + 1;\n\n\t\tif (trylock) {\n\t\t\tif (!mutex_trylock(&pag->pag_ici_reclaim_lock)) {\n\t\t\t\tskipped++;\n\t\t\t\txfs_perag_put(pag);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfirst_index = pag->pag_ici_reclaim_cursor;\n\t\t} else\n\t\t\tmutex_lock(&pag->pag_ici_reclaim_lock);\n\n\t\tdo {\n\t\t\tstruct xfs_inode *batch[XFS_LOOKUP_BATCH];\n\t\t\tint\ti;\n\n\t\t\trcu_read_lock();\n\t\t\tnr_found = radix_tree_gang_lookup_tag(\n\t\t\t\t\t&pag->pag_ici_root,\n\t\t\t\t\t(void **)batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH,\n\t\t\t\t\tXFS_ICI_RECLAIM_TAG);\n\t\t\tif (!nr_found) {\n\t\t\t\tdone = 1;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Grab the inodes before we drop the lock. if we found\n\t\t\t * nothing, nr == 0 and the loop will be skipped.\n\t\t\t */\n\t\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\t\tstruct xfs_inode *ip = batch[i];\n\n\t\t\t\tif (done || xfs_reclaim_inode_grab(ip, flags))\n\t\t\t\t\tbatch[i] = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * Update the index for the next lookup. Catch\n\t\t\t\t * overflows into the next AG range which can\n\t\t\t\t * occur if we have inodes in the last block of\n\t\t\t\t * the AG and we are currently pointing to the\n\t\t\t\t * last inode.\n\t\t\t\t *\n\t\t\t\t * Because we may see inodes that are from the\n\t\t\t\t * wrong AG due to RCU freeing and\n\t\t\t\t * reallocation, only update the index if it\n\t\t\t\t * lies in this AG. It was a race that lead us\n\t\t\t\t * to see this inode, so another lookup from\n\t\t\t\t * the same index will not find it again.\n\t\t\t\t */\n\t\t\t\tif (XFS_INO_TO_AGNO(mp, ip->i_ino) !=\n\t\t\t\t\t\t\t\tpag->pag_agno)\n\t\t\t\t\tcontinue;\n\t\t\t\tfirst_index = XFS_INO_TO_AGINO(mp, ip->i_ino + 1);\n\t\t\t\tif (first_index < XFS_INO_TO_AGINO(mp, ip->i_ino))\n\t\t\t\t\tdone = 1;\n\t\t\t}\n\n\t\t\t/* unlock now we've grabbed the inodes. */\n\t\t\trcu_read_unlock();\n\n\t\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\t\tif (!batch[i])\n\t\t\t\t\tcontinue;\n\t\t\t\terror = xfs_reclaim_inode(batch[i], pag, flags);\n\t\t\t\tif (error && last_error != -EFSCORRUPTED)\n\t\t\t\t\tlast_error = error;\n\t\t\t}\n\n\t\t\t*nr_to_scan -= XFS_LOOKUP_BATCH;\n\n\t\t\tcond_resched();\n\n\t\t} while (nr_found && !done && *nr_to_scan > 0);\n\n\t\tif (trylock && !done)\n\t\t\tpag->pag_ici_reclaim_cursor = first_index;\n\t\telse\n\t\t\tpag->pag_ici_reclaim_cursor = 0;\n\t\tmutex_unlock(&pag->pag_ici_reclaim_lock);\n\t\txfs_perag_put(pag);\n\t}\n\n\t/*\n\t * if we skipped any AG, and we still have scan count remaining, do\n\t * another pass this time using blocking reclaim semantics (i.e\n\t * waiting on the reclaim locks and ignoring the reclaim cursors). This\n\t * ensure that when we get more reclaimers than AGs we block rather\n\t * than spin trying to execute reclaim.\n\t */\n\tif (skipped && (flags & SYNC_WAIT) && *nr_to_scan > 0) {\n\t\ttrylock = 0;\n\t\tgoto restart;\n\t}\n\treturn last_error;\n}\n\nint\nxfs_reclaim_inodes(\n\txfs_mount_t\t*mp,\n\tint\t\tmode)\n{\n\tint\t\tnr_to_scan = INT_MAX;\n\n\treturn xfs_reclaim_inodes_ag(mp, mode, &nr_to_scan);\n}\n\n/*\n * Scan a certain number of inodes for reclaim.\n *\n * When called we make sure that there is a background (fast) inode reclaim in\n * progress, while we will throttle the speed of reclaim via doing synchronous\n * reclaim of inodes. That means if we come across dirty inodes, we wait for\n * them to be cleaned, which we hope will not be very long due to the\n * background walker having already kicked the IO off on those dirty inodes.\n */\nlong\nxfs_reclaim_inodes_nr(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tnr_to_scan)\n{\n\t/* kick background reclaimer and push the AIL */\n\txfs_reclaim_work_queue(mp);\n\txfs_ail_push_all(mp->m_ail);\n\n\treturn xfs_reclaim_inodes_ag(mp, SYNC_TRYLOCK | SYNC_WAIT, &nr_to_scan);\n}\n\n/*\n * Return the number of reclaimable inodes in the filesystem for\n * the shrinker to determine how much to reclaim.\n */\nint\nxfs_reclaim_inodes_count(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tag = 0;\n\tint\t\t\treclaimable = 0;\n\n\twhile ((pag = xfs_perag_get_tag(mp, ag, XFS_ICI_RECLAIM_TAG))) {\n\t\tag = pag->pag_agno + 1;\n\t\treclaimable += pag->pag_ici_reclaimable;\n\t\txfs_perag_put(pag);\n\t}\n\treturn reclaimable;\n}\n\nSTATIC int\nxfs_inode_match_id(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_UID) &&\n\t    !uid_eq(VFS_I(ip)->i_uid, eofb->eof_uid))\n\t\treturn 0;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_GID) &&\n\t    !gid_eq(VFS_I(ip)->i_gid, eofb->eof_gid))\n\t\treturn 0;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_PRID) &&\n\t    xfs_get_projid(ip) != eofb->eof_prid)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * A union-based inode filtering algorithm. Process the inode if any of the\n * criteria match. This is for global/internal scans only.\n */\nSTATIC int\nxfs_inode_match_id_union(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_UID) &&\n\t    uid_eq(VFS_I(ip)->i_uid, eofb->eof_uid))\n\t\treturn 1;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_GID) &&\n\t    gid_eq(VFS_I(ip)->i_gid, eofb->eof_gid))\n\t\treturn 1;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_PRID) &&\n\t    xfs_get_projid(ip) == eofb->eof_prid)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nSTATIC int\nxfs_inode_free_eofblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\tint ret = 0;\n\tstruct xfs_eofblocks *eofb = args;\n\tint match;\n\n\tif (!xfs_can_free_eofblocks(ip, false)) {\n\t\t/* inode could be preallocated or append-only */\n\t\ttrace_xfs_inode_free_eofblocks_invalid(ip);\n\t\txfs_inode_clear_eofblocks_tag(ip);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the mapping is dirty the operation can block and wait for some\n\t * time. Unless we are waiting, skip it.\n\t */\n\tif (!(flags & SYNC_WAIT) &&\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\tif (eofb) {\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_UNION)\n\t\t\tmatch = xfs_inode_match_id_union(ip, eofb);\n\t\telse\n\t\t\tmatch = xfs_inode_match_id(ip, eofb);\n\t\tif (!match)\n\t\t\treturn 0;\n\n\t\t/* skip the inode if the file size is too small */\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_MINFILESIZE &&\n\t\t    XFS_ISIZE(ip) < eofb->eof_min_file_size)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * If the caller is waiting, return -EAGAIN to keep the background\n\t * scanner moving and revisit the inode in a subsequent pass.\n\t */\n\tif (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {\n\t\tif (flags & SYNC_WAIT)\n\t\t\tret = -EAGAIN;\n\t\treturn ret;\n\t}\n\tret = xfs_free_eofblocks(ip);\n\txfs_iunlock(ip, XFS_IOLOCK_EXCL);\n\n\treturn ret;\n}\n\nstatic int\n__xfs_icache_free_eofblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\ttag)\n{\n\tint flags = SYNC_TRYLOCK;\n\n\tif (eofb && (eofb->eof_flags & XFS_EOF_FLAGS_SYNC))\n\t\tflags = SYNC_WAIT;\n\n\treturn xfs_inode_ag_iterator_tag(mp, execute, flags,\n\t\t\t\t\t eofb, tag);\n}\n\nint\nxfs_icache_free_eofblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\treturn __xfs_icache_free_eofblocks(mp, eofb, xfs_inode_free_eofblocks,\n\t\t\tXFS_ICI_EOFBLOCKS_TAG);\n}\n\n/*\n * Run eofblocks scans on the quotas applicable to the inode. For inodes with\n * multiple quotas, we don't know exactly which quota caused an allocation\n * failure. We make a best effort by including each quota under low free space\n * conditions (less than 1% free space) in the scan.\n */\nstatic int\n__xfs_inode_free_quota_eofblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\t(*execute)(struct xfs_mount *mp,\n\t\t\t\t\t   struct xfs_eofblocks\t*eofb))\n{\n\tint scan = 0;\n\tstruct xfs_eofblocks eofb = {0};\n\tstruct xfs_dquot *dq;\n\n\t/*\n\t * Run a sync scan to increase effectiveness and use the union filter to\n\t * cover all applicable quotas in a single scan.\n\t */\n\teofb.eof_flags = XFS_EOF_FLAGS_UNION|XFS_EOF_FLAGS_SYNC;\n\n\tif (XFS_IS_UQUOTA_ENFORCED(ip->i_mount)) {\n\t\tdq = xfs_inode_dquot(ip, XFS_DQ_USER);\n\t\tif (dq && xfs_dquot_lowsp(dq)) {\n\t\t\teofb.eof_uid = VFS_I(ip)->i_uid;\n\t\t\teofb.eof_flags |= XFS_EOF_FLAGS_UID;\n\t\t\tscan = 1;\n\t\t}\n\t}\n\n\tif (XFS_IS_GQUOTA_ENFORCED(ip->i_mount)) {\n\t\tdq = xfs_inode_dquot(ip, XFS_DQ_GROUP);\n\t\tif (dq && xfs_dquot_lowsp(dq)) {\n\t\t\teofb.eof_gid = VFS_I(ip)->i_gid;\n\t\t\teofb.eof_flags |= XFS_EOF_FLAGS_GID;\n\t\t\tscan = 1;\n\t\t}\n\t}\n\n\tif (scan)\n\t\texecute(ip->i_mount, &eofb);\n\n\treturn scan;\n}\n\nint\nxfs_inode_free_quota_eofblocks(\n\tstruct xfs_inode *ip)\n{\n\treturn __xfs_inode_free_quota_eofblocks(ip, xfs_icache_free_eofblocks);\n}\n\nstatic inline unsigned long\nxfs_iflag_for_tag(\n\tint\t\ttag)\n{\n\tswitch (tag) {\n\tcase XFS_ICI_EOFBLOCKS_TAG:\n\t\treturn XFS_IEOFBLOCKS;\n\tcase XFS_ICI_COWBLOCKS_TAG:\n\t\treturn XFS_ICOWBLOCKS;\n\tdefault:\n\t\tASSERT(0);\n\t\treturn 0;\n\t}\n}\n\nstatic void\n__xfs_inode_set_blocks_tag(\n\txfs_inode_t\t*ip,\n\tvoid\t\t(*execute)(struct xfs_mount *mp),\n\tvoid\t\t(*set_tp)(struct xfs_mount *mp, xfs_agnumber_t agno,\n\t\t\t\t  int error, unsigned long caller_ip),\n\tint\t\ttag)\n{\n\tstruct xfs_mount *mp = ip->i_mount;\n\tstruct xfs_perag *pag;\n\tint tagged;\n\n\t/*\n\t * Don't bother locking the AG and looking up in the radix trees\n\t * if we already know that we have the tag set.\n\t */\n\tif (ip->i_flags & xfs_iflag_for_tag(tag))\n\t\treturn;\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags |= xfs_iflag_for_tag(tag);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\ttagged = radix_tree_tagged(&pag->pag_ici_root, tag);\n\tradix_tree_tag_set(&pag->pag_ici_root,\n\t\t\t   XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino), tag);\n\tif (!tagged) {\n\t\t/* propagate the eofblocks tag up into the perag radix tree */\n\t\tspin_lock(&ip->i_mount->m_perag_lock);\n\t\tradix_tree_tag_set(&ip->i_mount->m_perag_tree,\n\t\t\t\t   XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino),\n\t\t\t\t   tag);\n\t\tspin_unlock(&ip->i_mount->m_perag_lock);\n\n\t\t/* kick off background trimming */\n\t\texecute(ip->i_mount);\n\n\t\tset_tp(ip->i_mount, pag->pag_agno, -1, _RET_IP_);\n\t}\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_set_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_eofblocks_tag(ip);\n\treturn __xfs_inode_set_blocks_tag(ip, xfs_queue_eofblocks,\n\t\t\ttrace_xfs_perag_set_eofblocks,\n\t\t\tXFS_ICI_EOFBLOCKS_TAG);\n}\n\nstatic void\n__xfs_inode_clear_blocks_tag(\n\txfs_inode_t\t*ip,\n\tvoid\t\t(*clear_tp)(struct xfs_mount *mp, xfs_agnumber_t agno,\n\t\t\t\t    int error, unsigned long caller_ip),\n\tint\t\ttag)\n{\n\tstruct xfs_mount *mp = ip->i_mount;\n\tstruct xfs_perag *pag;\n\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags &= ~xfs_iflag_for_tag(tag);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\tradix_tree_tag_clear(&pag->pag_ici_root,\n\t\t\t     XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino), tag);\n\tif (!radix_tree_tagged(&pag->pag_ici_root, tag)) {\n\t\t/* clear the eofblocks tag from the perag radix tree */\n\t\tspin_lock(&ip->i_mount->m_perag_lock);\n\t\tradix_tree_tag_clear(&ip->i_mount->m_perag_tree,\n\t\t\t\t     XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino),\n\t\t\t\t     tag);\n\t\tspin_unlock(&ip->i_mount->m_perag_lock);\n\t\tclear_tp(ip->i_mount, pag->pag_agno, -1, _RET_IP_);\n\t}\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_clear_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_eofblocks_tag(ip);\n\treturn __xfs_inode_clear_blocks_tag(ip,\n\t\t\ttrace_xfs_perag_clear_eofblocks, XFS_ICI_EOFBLOCKS_TAG);\n}\n\n/*\n * Set ourselves up to free CoW blocks from this file.  If it's already clean\n * then we can bail out quickly, but otherwise we must back off if the file\n * is undergoing some kind of write.\n */\nstatic bool\nxfs_prep_free_cowblocks(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_ifork\t*ifp)\n{\n\t/*\n\t * Just clear the tag if we have an empty cow fork or none at all. It's\n\t * possible the inode was fully unshared since it was originally tagged.\n\t */\n\tif (!xfs_is_reflink_inode(ip) || !ifp->if_bytes) {\n\t\ttrace_xfs_inode_free_cowblocks_invalid(ip);\n\t\txfs_inode_clear_cowblocks_tag(ip);\n\t\treturn false;\n\t}\n\n\t/*\n\t * If the mapping is dirty or under writeback we cannot touch the\n\t * CoW fork.  Leave it alone if we're in the midst of a directio.\n\t */\n\tif ((VFS_I(ip)->i_state & I_DIRTY_PAGES) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_WRITEBACK) ||\n\t    atomic_read(&VFS_I(ip)->i_dio_count))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Automatic CoW Reservation Freeing\n *\n * These functions automatically garbage collect leftover CoW reservations\n * that were made on behalf of a cowextsize hint when we start to run out\n * of quota or when the reservations sit around for too long.  If the file\n * has dirty pages or is undergoing writeback, its CoW reservations will\n * be retained.\n *\n * The actual garbage collection piggybacks off the same code that runs\n * the speculative EOF preallocation garbage collector.\n */\nSTATIC int\nxfs_inode_free_cowblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\tstruct xfs_eofblocks\t*eofb = args;\n\tstruct xfs_ifork\t*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);\n\tint\t\t\tmatch;\n\tint\t\t\tret = 0;\n\n\tif (!xfs_prep_free_cowblocks(ip, ifp))\n\t\treturn 0;\n\n\tif (eofb) {\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_UNION)\n\t\t\tmatch = xfs_inode_match_id_union(ip, eofb);\n\t\telse\n\t\t\tmatch = xfs_inode_match_id(ip, eofb);\n\t\tif (!match)\n\t\t\treturn 0;\n\n\t\t/* skip the inode if the file size is too small */\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_MINFILESIZE &&\n\t\t    XFS_ISIZE(ip) < eofb->eof_min_file_size)\n\t\t\treturn 0;\n\t}\n\n\t/* Free the CoW blocks */\n\txfs_ilock(ip, XFS_IOLOCK_EXCL);\n\txfs_ilock(ip, XFS_MMAPLOCK_EXCL);\n\n\t/*\n\t * Check again, nobody else should be able to dirty blocks or change\n\t * the reflink iflag now that we have the first two locks held.\n\t */\n\tif (xfs_prep_free_cowblocks(ip, ifp))\n\t\tret = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF, false);\n\n\txfs_iunlock(ip, XFS_MMAPLOCK_EXCL);\n\txfs_iunlock(ip, XFS_IOLOCK_EXCL);\n\n\treturn ret;\n}\n\nint\nxfs_icache_free_cowblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\treturn __xfs_icache_free_eofblocks(mp, eofb, xfs_inode_free_cowblocks,\n\t\t\tXFS_ICI_COWBLOCKS_TAG);\n}\n\nint\nxfs_inode_free_quota_cowblocks(\n\tstruct xfs_inode *ip)\n{\n\treturn __xfs_inode_free_quota_eofblocks(ip, xfs_icache_free_cowblocks);\n}\n\nvoid\nxfs_inode_set_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_cowblocks_tag(ip);\n\treturn __xfs_inode_set_blocks_tag(ip, xfs_queue_cowblocks,\n\t\t\ttrace_xfs_perag_set_cowblocks,\n\t\t\tXFS_ICI_COWBLOCKS_TAG);\n}\n\nvoid\nxfs_inode_clear_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_cowblocks_tag(ip);\n\treturn __xfs_inode_clear_blocks_tag(ip,\n\t\t\ttrace_xfs_perag_clear_cowblocks, XFS_ICI_COWBLOCKS_TAG);\n}\n"], "fixing_code": ["/*\n * Copyright (c) 2000-2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it would be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write the Free Software Foundation,\n * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n */\n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_sb.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_error.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_trans_priv.h\"\n#include \"xfs_inode_item.h\"\n#include \"xfs_quota.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_icache.h\"\n#include \"xfs_bmap_util.h\"\n#include \"xfs_dquot_item.h\"\n#include \"xfs_dquot.h\"\n#include \"xfs_reflink.h\"\n\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/iversion.h>\n\n/*\n * Allocate and initialise an xfs_inode.\n */\nstruct xfs_inode *\nxfs_inode_alloc(\n\tstruct xfs_mount\t*mp,\n\txfs_ino_t\t\tino)\n{\n\tstruct xfs_inode\t*ip;\n\n\t/*\n\t * if this didn't occur in transactions, we could use\n\t * KM_MAYFAIL and return NULL here on ENOMEM. Set the\n\t * code up to do this anyway.\n\t */\n\tip = kmem_zone_alloc(xfs_inode_zone, KM_SLEEP);\n\tif (!ip)\n\t\treturn NULL;\n\tif (inode_init_always(mp->m_super, VFS_I(ip))) {\n\t\tkmem_zone_free(xfs_inode_zone, ip);\n\t\treturn NULL;\n\t}\n\n\t/* VFS doesn't initialise i_mode! */\n\tVFS_I(ip)->i_mode = 0;\n\n\tXFS_STATS_INC(mp, vn_active);\n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tASSERT(!xfs_isiflocked(ip));\n\tASSERT(ip->i_ino == 0);\n\n\t/* initialise the xfs inode */\n\tip->i_ino = ino;\n\tip->i_mount = mp;\n\tmemset(&ip->i_imap, 0, sizeof(struct xfs_imap));\n\tip->i_afp = NULL;\n\tip->i_cowfp = NULL;\n\tip->i_cnextents = 0;\n\tip->i_cformat = XFS_DINODE_FMT_EXTENTS;\n\tmemset(&ip->i_df, 0, sizeof(xfs_ifork_t));\n\tip->i_flags = 0;\n\tip->i_delayed_blks = 0;\n\tmemset(&ip->i_d, 0, sizeof(ip->i_d));\n\n\treturn ip;\n}\n\nSTATIC void\nxfs_inode_free_callback(\n\tstruct rcu_head\t\t*head)\n{\n\tstruct inode\t\t*inode = container_of(head, struct inode, i_rcu);\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\n\tswitch (VFS_I(ip)->i_mode & S_IFMT) {\n\tcase S_IFREG:\n\tcase S_IFDIR:\n\tcase S_IFLNK:\n\t\txfs_idestroy_fork(ip, XFS_DATA_FORK);\n\t\tbreak;\n\t}\n\n\tif (ip->i_afp)\n\t\txfs_idestroy_fork(ip, XFS_ATTR_FORK);\n\tif (ip->i_cowfp)\n\t\txfs_idestroy_fork(ip, XFS_COW_FORK);\n\n\tif (ip->i_itemp) {\n\t\tASSERT(!(ip->i_itemp->ili_item.li_flags & XFS_LI_IN_AIL));\n\t\txfs_inode_item_destroy(ip);\n\t\tip->i_itemp = NULL;\n\t}\n\n\tkmem_zone_free(xfs_inode_zone, ip);\n}\n\nstatic void\n__xfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\t/* asserts to verify all state is correct here */\n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tXFS_STATS_DEC(ip->i_mount, vn_active);\n\n\tcall_rcu(&VFS_I(ip)->i_rcu, xfs_inode_free_callback);\n}\n\nvoid\nxfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\tASSERT(!xfs_isiflocked(ip));\n\n\t/*\n\t * Because we use RCU freeing we need to ensure the inode always\n\t * appears to be reclaimed with an invalid inode number when in the\n\t * free state. The ip->i_flags_lock provides the barrier against lookup\n\t * races.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t__xfs_inode_free(ip);\n}\n\n/*\n * Queue a new inode reclaim pass if there are reclaimable inodes and there\n * isn't a reclaim pass already in progress. By default it runs every 5s based\n * on the xfs periodic sync default of 30s. Perhaps this should have it's own\n * tunable, but that can be done if this method proves to be ineffective or too\n * aggressive.\n */\nstatic void\nxfs_reclaim_work_queue(\n\tstruct xfs_mount        *mp)\n{\n\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_RECLAIM_TAG)) {\n\t\tqueue_delayed_work(mp->m_reclaim_workqueue, &mp->m_reclaim_work,\n\t\t\tmsecs_to_jiffies(xfs_syncd_centisecs / 6 * 10));\n\t}\n\trcu_read_unlock();\n}\n\n/*\n * This is a fast pass over the inode cache to try to get reclaim moving on as\n * many inodes as possible in a short period of time. It kicks itself every few\n * seconds, as well as being kicked by the inode cache shrinker when memory\n * goes low. It scans as quickly as possible avoiding locked inodes or those\n * already being flushed, and once done schedules a future pass.\n */\nvoid\nxfs_reclaim_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct xfs_mount, m_reclaim_work);\n\n\txfs_reclaim_inodes(mp, SYNC_TRYLOCK);\n\txfs_reclaim_work_queue(mp);\n}\n\nstatic void\nxfs_perag_set_reclaim_tag(\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\tif (pag->pag_ici_reclaimable++)\n\t\treturn;\n\n\t/* propagate the reclaim tag up into the perag radix tree */\n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_set(&mp->m_perag_tree, pag->pag_agno,\n\t\t\t   XFS_ICI_RECLAIM_TAG);\n\tspin_unlock(&mp->m_perag_lock);\n\n\t/* schedule periodic background inode reclaim */\n\txfs_reclaim_work_queue(mp);\n\n\ttrace_xfs_perag_set_reclaim(mp, pag->pag_agno, -1, _RET_IP_);\n}\n\nstatic void\nxfs_perag_clear_reclaim_tag(\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\tif (--pag->pag_ici_reclaimable)\n\t\treturn;\n\n\t/* clear the reclaim tag from the perag radix tree */\n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_clear(&mp->m_perag_tree, pag->pag_agno,\n\t\t\t     XFS_ICI_RECLAIM_TAG);\n\tspin_unlock(&mp->m_perag_lock);\n\ttrace_xfs_perag_clear_reclaim(mp, pag->pag_agno, -1, _RET_IP_);\n}\n\n\n/*\n * We set the inode flag atomically with the radix tree tag.\n * Once we get tag lookups on the radix tree, this inode flag\n * can go away.\n */\nvoid\nxfs_inode_set_reclaim_tag(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_perag\t*pag;\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\tspin_lock(&ip->i_flags_lock);\n\n\tradix_tree_tag_set(&pag->pag_ici_root, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\t   XFS_ICI_RECLAIM_TAG);\n\txfs_perag_set_reclaim_tag(pag);\n\t__xfs_iflags_set(ip, XFS_IRECLAIMABLE);\n\n\tspin_unlock(&ip->i_flags_lock);\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nSTATIC void\nxfs_inode_clear_reclaim_tag(\n\tstruct xfs_perag\t*pag,\n\txfs_ino_t\t\tino)\n{\n\tradix_tree_tag_clear(&pag->pag_ici_root,\n\t\t\t     XFS_INO_TO_AGINO(pag->pag_mount, ino),\n\t\t\t     XFS_ICI_RECLAIM_TAG);\n\txfs_perag_clear_reclaim_tag(pag);\n}\n\nstatic void\nxfs_inew_wait(\n\tstruct xfs_inode\t*ip)\n{\n\twait_queue_head_t *wq = bit_waitqueue(&ip->i_flags, __XFS_INEW_BIT);\n\tDEFINE_WAIT_BIT(wait, &ip->i_flags, __XFS_INEW_BIT);\n\n\tdo {\n\t\tprepare_to_wait(wq, &wait.wq_entry, TASK_UNINTERRUPTIBLE);\n\t\tif (!xfs_iflags_test(ip, XFS_INEW))\n\t\t\tbreak;\n\t\tschedule();\n\t} while (true);\n\tfinish_wait(wq, &wait.wq_entry);\n}\n\n/*\n * When we recycle a reclaimable inode, we need to re-initialise the VFS inode\n * part of the structure. This is made more complex by the fact we store\n * information about the on-disk values in the VFS inode and so we can't just\n * overwrite the values unconditionally. Hence we save the parameters we\n * need to retain across reinitialisation, and rewrite them into the VFS inode\n * after reinitialisation even if it fails.\n */\nstatic int\nxfs_reinit_inode(\n\tstruct xfs_mount\t*mp,\n\tstruct inode\t\t*inode)\n{\n\tint\t\terror;\n\tuint32_t\tnlink = inode->i_nlink;\n\tuint32_t\tgeneration = inode->i_generation;\n\tuint64_t\tversion = inode_peek_iversion(inode);\n\tumode_t\t\tmode = inode->i_mode;\n\tdev_t\t\tdev = inode->i_rdev;\n\n\terror = inode_init_always(mp->m_super, inode);\n\n\tset_nlink(inode, nlink);\n\tinode->i_generation = generation;\n\tinode_set_iversion_queried(inode, version);\n\tinode->i_mode = mode;\n\tinode->i_rdev = dev;\n\treturn error;\n}\n\n/*\n * If we are allocating a new inode, then check what was returned is\n * actually a free, empty inode. If we are not allocating an inode,\n * then check we didn't find a free inode.\n *\n * Returns:\n *\t0\t\tif the inode free state matches the lookup context\n *\t-ENOENT\t\tif the inode is free and we are not allocating\n *\t-EFSCORRUPTED\tif there is any state mismatch at all\n */\nstatic int\nxfs_iget_check_free_state(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tif (flags & XFS_IGET_CREATE) {\n\t\t/* should be a free inode */\n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(ip->i_mount,\n\"Corruption detected! Free inode 0x%llx not marked free! (mode 0x%x)\",\n\t\t\t\tip->i_ino, VFS_I(ip)->i_mode);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tif (ip->i_d.di_nblocks != 0) {\n\t\t\txfs_warn(ip->i_mount,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tip->i_ino);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t/* should be an allocated inode */\n\tif (VFS_I(ip)->i_mode == 0)\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\n/*\n * Check the validity of the inode we just found it the cache\n */\nstatic int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t/*\n\t * check for re-use of an inode within an RCU grace period due to the\n\t * radix tree nodes not being updated yet. We monitor for this by\n\t * setting the inode number to zero before freeing the inode structure.\n\t * If the inode has been reallocated and set up, then the inode number\n\t * will not match, so check for that, too.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\n\t/*\n\t * If we are racing with another cache hit that is currently\n\t * instantiating this inode or currently recycling it out of\n\t * reclaimabe state, wait for the initialisation to complete\n\t * before continuing.\n\t *\n\t * XXX(hch): eventually we should do something equivalent to\n\t *\t     wait_on_inode to wait for these flags to be cleared\n\t *\t     instead of polling for it.\n\t */\n\tif (ip->i_flags & (XFS_INEW|XFS_IRECLAIM)) {\n\t\ttrace_xfs_iget_skip(ip);\n\t\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\t\terror = -EAGAIN;\n\t\tgoto out_error;\n\t}\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_error;\n\n\t/*\n\t * If IRECLAIMABLE is set, we've torn down the VFS inode already.\n\t * Need to carefully get it back into useable state.\n\t */\n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\ttrace_xfs_iget_reclaim(ip);\n\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/*\n\t\t * We need to set XFS_IRECLAIM to prevent xfs_reclaim_inode\n\t\t * from stomping over us while we recycle the inode.  We can't\n\t\t * clear the radix tree reclaimable tag yet as it requires\n\t\t * pag_ici_lock to be held exclusive.\n\t\t */\n\t\tip->i_flags |= XFS_IRECLAIM;\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\terror = xfs_reinit_inode(mp, inode);\n\t\tif (error) {\n\t\t\tbool wake;\n\t\t\t/*\n\t\t\t * Re-initializing the inode failed, and we are in deep\n\t\t\t * trouble.  Try to re-add it to the reclaim list.\n\t\t\t */\n\t\t\trcu_read_lock();\n\t\t\tspin_lock(&ip->i_flags_lock);\n\t\t\twake = !!__xfs_iflags_test(ip, XFS_INEW);\n\t\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\t\tif (wake)\n\t\t\t\twake_up_bit(&ip->i_flags, __XFS_INEW_BIT);\n\t\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\t\ttrace_xfs_iget_reclaim_fail(ip);\n\t\t\tgoto out_error;\n\t\t}\n\n\t\tspin_lock(&pag->pag_ici_lock);\n\t\tspin_lock(&ip->i_flags_lock);\n\n\t\t/*\n\t\t * Clear the per-lifetime state in the inode as we are now\n\t\t * effectively a new inode and need to return to the initial\n\t\t * state before reuse occurs.\n\t\t */\n\t\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\t\tip->i_flags |= XFS_INEW;\n\t\txfs_inode_clear_reclaim_tag(pag, ip->i_ino);\n\t\tinode->i_state = I_NEW;\n\n\t\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\t\tinit_rwsem(&inode->i_rwsem);\n\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\tspin_unlock(&pag->pag_ici_lock);\n\t} else {\n\t\t/* If the VFS inode is being torn down, pause and try again. */\n\t\tif (!igrab(inode)) {\n\t\t\ttrace_xfs_iget_skip(ip);\n\t\t\terror = -EAGAIN;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\t/* We've got a live one. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE | XFS_IDONTCACHE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n}\n\n\nstatic int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_iread(mp, tp, ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\tif (!xfs_inode_verify_forks(ip)) {\n\t\terror = -EFSCORRUPTED;\n\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\n\t/*\n\t * Check the inode free state is valid. This also detects lookup\n\t * racing with unlinks.\n\t */\n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t/*\n\t * Preload the radix tree so we can insert safely under the\n\t * write spinlock. Note that we cannot sleep inside the preload\n\t * region. Since we can be called from transaction context, don't\n\t * recurse into the file system.\n\t */\n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t/*\n\t * Because the inode hasn't been added to the radix-tree yet it can't\n\t * be found by another thread, so we can do the non-sleeping lock here.\n\t */\n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t/*\n\t * These values must be set before inserting the inode into the radix\n\t * tree as the moment it is inserted a concurrent lookup (allowed by the\n\t * RCU locking mechanism) can find it and that lookup must see that this\n\t * is an inode currently under construction (i.e. that XFS_INEW is set).\n\t * The ip->i_flags_lock that protects the XFS_INEW flag forms the\n\t * memory barrier that ensures this detection works correctly at lookup\n\t * time.\n\t */\n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\tiflags |= XFS_IDONTCACHE;\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t/* insert the new inode */\n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}\n\n/*\n * Look up an inode by number in the given file system.\n * The inode is looked up in the cache held in each AG.\n * If the inode is found in the cache, initialise the vfs inode\n * if necessary.\n *\n * If it is not in core, read it in from the file system's device,\n * add it to the cache and initialise the vfs inode.\n *\n * The inode is locked according to the value of the lock_flags parameter.\n * This flag parameter indicates how and if the inode's IO lock and inode lock\n * should be taken.\n *\n * mp -- the mount point structure for the current file system.  It points\n *       to the inode hash table.\n * tp -- a pointer to the current transaction if there is one.  This is\n *       simply passed through to the xfs_iread() call.\n * ino -- the number of the inode desired.  This is the unique identifier\n *        within the file system for the inode being requested.\n * lock_flags -- flags indicating how to lock the inode.  See the comment\n *\t\t for xfs_ilock() for a list of valid values.\n */\nint\nxfs_iget(\n\txfs_mount_t\t*mp,\n\txfs_trans_t\t*tp,\n\txfs_ino_t\tino,\n\tuint\t\tflags,\n\tuint\t\tlock_flags,\n\txfs_inode_t\t**ipp)\n{\n\txfs_inode_t\t*ip;\n\tint\t\terror;\n\txfs_perag_t\t*pag;\n\txfs_agino_t\tagino;\n\n\t/*\n\t * xfs_reclaim_inode() uses the ILOCK to ensure an inode\n\t * doesn't get freed while it's being referenced during a\n\t * radix tree traversal here.  It assumes this function\n\t * aqcuires only the ILOCK (and therefore it has no need to\n\t * involve the IOLOCK in this synchronization).\n\t */\n\tASSERT((lock_flags & (XFS_IOLOCK_EXCL | XFS_IOLOCK_SHARED)) == 0);\n\n\t/* reject inode numbers outside existing AGs */\n\tif (!ino || XFS_INO_TO_AGNO(mp, ino) >= mp->m_sb.sb_agcount)\n\t\treturn -EINVAL;\n\n\tXFS_STATS_INC(mp, xs_ig_attempts);\n\n\t/* get the perag structure and ensure that it's inode capable */\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ino));\n\tagino = XFS_INO_TO_AGINO(mp, ino);\n\nagain:\n\terror = 0;\n\trcu_read_lock();\n\tip = radix_tree_lookup(&pag->pag_ici_root, agino);\n\n\tif (ip) {\n\t\terror = xfs_iget_cache_hit(pag, ip, ino, flags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t} else {\n\t\trcu_read_unlock();\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -ENODATA;\n\t\t\tgoto out_error_or_again;\n\t\t}\n\t\tXFS_STATS_INC(mp, xs_ig_missed);\n\n\t\terror = xfs_iget_cache_miss(mp, pag, tp, ino, &ip,\n\t\t\t\t\t\t\tflags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t}\n\txfs_perag_put(pag);\n\n\t*ipp = ip;\n\n\t/*\n\t * If we have a real type for an on-disk inode, we can setup the inode\n\t * now.\t If it's a new inode being created, xfs_ialloc will handle it.\n\t */\n\tif (xfs_iflags_test(ip, XFS_INEW) && VFS_I(ip)->i_mode != 0)\n\t\txfs_setup_existing_inode(ip);\n\treturn 0;\n\nout_error_or_again:\n\tif (!(flags & XFS_IGET_INCORE) && error == -EAGAIN) {\n\t\tdelay(1);\n\t\tgoto again;\n\t}\n\txfs_perag_put(pag);\n\treturn error;\n}\n\n/*\n * \"Is this a cached inode that's also allocated?\"\n *\n * Look up an inode by number in the given file system.  If the inode is\n * in cache and isn't in purgatory, return 1 if the inode is allocated\n * and 0 if it is not.  For all other cases (not in cache, being torn\n * down, etc.), return a negative error code.\n *\n * The caller has to prevent inode allocation and freeing activity,\n * presumably by locking the AGI buffer.   This is to ensure that an\n * inode cannot transition from allocated to freed until the caller is\n * ready to allow that.  If the inode is in an intermediate state (new,\n * reclaimable, or being reclaimed), -EAGAIN will be returned; if the\n * inode is not in the cache, -ENOENT will be returned.  The caller must\n * deal with these scenarios appropriately.\n *\n * This is a specialized use case for the online scrubber; if you're\n * reading this, you probably want xfs_iget.\n */\nint\nxfs_icache_inode_is_allocated(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_trans\t*tp,\n\txfs_ino_t\t\tino,\n\tbool\t\t\t*inuse)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\n\terror = xfs_iget(mp, tp, ino, XFS_IGET_INCORE, 0, &ip);\n\tif (error)\n\t\treturn error;\n\n\t*inuse = !!(VFS_I(ip)->i_mode);\n\tIRELE(ip);\n\treturn 0;\n}\n\n/*\n * The inode lookup is done in batches to keep the amount of lock traffic and\n * radix tree lookups to a minimum. The batch size is a trade off between\n * lookup reduction and stack usage. This is in the reclaim path, so we can't\n * be too greedy.\n */\n#define XFS_LOOKUP_BATCH\t32\n\nSTATIC int\nxfs_inode_ag_walk_grab(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tbool\t\t\tnewinos = !!(flags & XFS_AGITER_INEW_WAIT);\n\n\tASSERT(rcu_read_lock_held());\n\n\t/*\n\t * check for stale RCU freed inode\n\t *\n\t * If the inode has been reallocated, it doesn't matter if it's not in\n\t * the AG we are walking - we are walking for writeback, so if it\n\t * passes all the \"valid inode\" checks and is dirty, then we'll write\n\t * it back anyway.  If it has been reallocated and still being\n\t * initialised, the XFS_INEW check below will catch it.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (!ip->i_ino)\n\t\tgoto out_unlock_noent;\n\n\t/* avoid new or reclaimable inodes. Leave for reclaim code to flush */\n\tif ((!newinos && __xfs_iflags_test(ip, XFS_INEW)) ||\n\t    __xfs_iflags_test(ip, XFS_IRECLAIMABLE | XFS_IRECLAIM))\n\t\tgoto out_unlock_noent;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t/* nothing to sync during shutdown */\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\treturn -EFSCORRUPTED;\n\n\t/* If we can't grab the inode, it must on it's way to reclaim. */\n\tif (!igrab(inode))\n\t\treturn -ENOENT;\n\n\t/* inode is valid */\n\treturn 0;\n\nout_unlock_noent:\n\tspin_unlock(&ip->i_flags_lock);\n\treturn -ENOENT;\n}\n\nSTATIC int\nxfs_inode_ag_walk(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\ttag,\n\tint\t\t\titer_flags)\n{\n\tuint32_t\t\tfirst_index;\n\tint\t\t\tlast_error = 0;\n\tint\t\t\tskipped;\n\tint\t\t\tdone;\n\tint\t\t\tnr_found;\n\nrestart:\n\tdone = 0;\n\tskipped = 0;\n\tfirst_index = 0;\n\tnr_found = 0;\n\tdo {\n\t\tstruct xfs_inode *batch[XFS_LOOKUP_BATCH];\n\t\tint\t\terror = 0;\n\t\tint\t\ti;\n\n\t\trcu_read_lock();\n\n\t\tif (tag == -1)\n\t\t\tnr_found = radix_tree_gang_lookup(&pag->pag_ici_root,\n\t\t\t\t\t(void **)batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH);\n\t\telse\n\t\t\tnr_found = radix_tree_gang_lookup_tag(\n\t\t\t\t\t&pag->pag_ici_root,\n\t\t\t\t\t(void **) batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH, tag);\n\n\t\tif (!nr_found) {\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Grab the inodes before we drop the lock. if we found\n\t\t * nothing, nr == 0 and the loop will be skipped.\n\t\t */\n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tstruct xfs_inode *ip = batch[i];\n\n\t\t\tif (done || xfs_inode_ag_walk_grab(ip, iter_flags))\n\t\t\t\tbatch[i] = NULL;\n\n\t\t\t/*\n\t\t\t * Update the index for the next lookup. Catch\n\t\t\t * overflows into the next AG range which can occur if\n\t\t\t * we have inodes in the last block of the AG and we\n\t\t\t * are currently pointing to the last inode.\n\t\t\t *\n\t\t\t * Because we may see inodes that are from the wrong AG\n\t\t\t * due to RCU freeing and reallocation, only update the\n\t\t\t * index if it lies in this AG. It was a race that lead\n\t\t\t * us to see this inode, so another lookup from the\n\t\t\t * same index will not find it again.\n\t\t\t */\n\t\t\tif (XFS_INO_TO_AGNO(mp, ip->i_ino) != pag->pag_agno)\n\t\t\t\tcontinue;\n\t\t\tfirst_index = XFS_INO_TO_AGINO(mp, ip->i_ino + 1);\n\t\t\tif (first_index < XFS_INO_TO_AGINO(mp, ip->i_ino))\n\t\t\t\tdone = 1;\n\t\t}\n\n\t\t/* unlock now we've grabbed the inodes. */\n\t\trcu_read_unlock();\n\n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tif (!batch[i])\n\t\t\t\tcontinue;\n\t\t\tif ((iter_flags & XFS_AGITER_INEW_WAIT) &&\n\t\t\t    xfs_iflags_test(batch[i], XFS_INEW))\n\t\t\t\txfs_inew_wait(batch[i]);\n\t\t\terror = execute(batch[i], flags, args);\n\t\t\tIRELE(batch[i]);\n\t\t\tif (error == -EAGAIN) {\n\t\t\t\tskipped++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (error && last_error != -EFSCORRUPTED)\n\t\t\t\tlast_error = error;\n\t\t}\n\n\t\t/* bail out if the filesystem is corrupted.  */\n\t\tif (error == -EFSCORRUPTED)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\n\t} while (nr_found && !done);\n\n\tif (skipped) {\n\t\tdelay(1);\n\t\tgoto restart;\n\t}\n\treturn last_error;\n}\n\n/*\n * Background scanning to trim post-EOF preallocated space. This is queued\n * based on the 'speculative_prealloc_lifetime' tunable (5m by default).\n */\nvoid\nxfs_queue_eofblocks(\n\tstruct xfs_mount *mp)\n{\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_EOFBLOCKS_TAG))\n\t\tqueue_delayed_work(mp->m_eofblocks_workqueue,\n\t\t\t\t   &mp->m_eofblocks_work,\n\t\t\t\t   msecs_to_jiffies(xfs_eofb_secs * 1000));\n\trcu_read_unlock();\n}\n\nvoid\nxfs_eofblocks_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\tstruct xfs_mount, m_eofblocks_work);\n\txfs_icache_free_eofblocks(mp, NULL);\n\txfs_queue_eofblocks(mp);\n}\n\n/*\n * Background scanning to trim preallocated CoW space. This is queued\n * based on the 'speculative_cow_prealloc_lifetime' tunable (5m by default).\n * (We'll just piggyback on the post-EOF prealloc space workqueue.)\n */\nvoid\nxfs_queue_cowblocks(\n\tstruct xfs_mount *mp)\n{\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_COWBLOCKS_TAG))\n\t\tqueue_delayed_work(mp->m_eofblocks_workqueue,\n\t\t\t\t   &mp->m_cowblocks_work,\n\t\t\t\t   msecs_to_jiffies(xfs_cowb_secs * 1000));\n\trcu_read_unlock();\n}\n\nvoid\nxfs_cowblocks_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\tstruct xfs_mount, m_cowblocks_work);\n\txfs_icache_free_cowblocks(mp, NULL);\n\txfs_queue_cowblocks(mp);\n}\n\nint\nxfs_inode_ag_iterator_flags(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\titer_flags)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\n\tag = 0;\n\twhile ((pag = xfs_perag_get(mp, ag))) {\n\t\tag = pag->pag_agno + 1;\n\t\terror = xfs_inode_ag_walk(mp, pag, execute, flags, args, -1,\n\t\t\t\t\t  iter_flags);\n\t\txfs_perag_put(pag);\n\t\tif (error) {\n\t\t\tlast_error = error;\n\t\t\tif (error == -EFSCORRUPTED)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn last_error;\n}\n\nint\nxfs_inode_ag_iterator(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\treturn xfs_inode_ag_iterator_flags(mp, execute, flags, args, 0);\n}\n\nint\nxfs_inode_ag_iterator_tag(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\tflags,\n\tvoid\t\t\t*args,\n\tint\t\t\ttag)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\n\tag = 0;\n\twhile ((pag = xfs_perag_get_tag(mp, ag, tag))) {\n\t\tag = pag->pag_agno + 1;\n\t\terror = xfs_inode_ag_walk(mp, pag, execute, flags, args, tag,\n\t\t\t\t\t  0);\n\t\txfs_perag_put(pag);\n\t\tif (error) {\n\t\t\tlast_error = error;\n\t\t\tif (error == -EFSCORRUPTED)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn last_error;\n}\n\n/*\n * Grab the inode for reclaim exclusively.\n * Return 0 if we grabbed it, non-zero otherwise.\n */\nSTATIC int\nxfs_reclaim_inode_grab(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tASSERT(rcu_read_lock_held());\n\n\t/* quick check for stale RCU freed inode */\n\tif (!ip->i_ino)\n\t\treturn 1;\n\n\t/*\n\t * If we are asked for non-blocking operation, do unlocked checks to\n\t * see if the inode already is being flushed or in reclaim to avoid\n\t * lock traffic.\n\t */\n\tif ((flags & SYNC_TRYLOCK) &&\n\t    __xfs_iflags_test(ip, XFS_IFLOCK | XFS_IRECLAIM))\n\t\treturn 1;\n\n\t/*\n\t * The radix tree lock here protects a thread in xfs_iget from racing\n\t * with us starting reclaim on the inode.  Once we have the\n\t * XFS_IRECLAIM flag set it will not touch us.\n\t *\n\t * Due to RCU lookup, we may find inodes that have been freed and only\n\t * have XFS_IRECLAIM set.  Indeed, we may see reallocated inodes that\n\t * aren't candidates for reclaim at all, so we must check the\n\t * XFS_IRECLAIMABLE is set first before proceeding to reclaim.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (!__xfs_iflags_test(ip, XFS_IRECLAIMABLE) ||\n\t    __xfs_iflags_test(ip, XFS_IRECLAIM)) {\n\t\t/* not a reclaim candidate. */\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\treturn 1;\n\t}\n\t__xfs_iflags_set(ip, XFS_IRECLAIM);\n\tspin_unlock(&ip->i_flags_lock);\n\treturn 0;\n}\n\n/*\n * Inodes in different states need to be treated differently. The following\n * table lists the inode states and the reclaim actions necessary:\n *\n *\tinode state\t     iflush ret\t\trequired action\n *      ---------------      ----------         ---------------\n *\tbad\t\t\t-\t\treclaim\n *\tshutdown\t\tEIO\t\tunpin and reclaim\n *\tclean, unpinned\t\t0\t\treclaim\n *\tstale, unpinned\t\t0\t\treclaim\n *\tclean, pinned(*)\t0\t\trequeue\n *\tstale, pinned\t\tEAGAIN\t\trequeue\n *\tdirty, async\t\t-\t\trequeue\n *\tdirty, sync\t\t0\t\treclaim\n *\n * (*) dgc: I don't think the clean, pinned state is possible but it gets\n * handled anyway given the order of checks implemented.\n *\n * Also, because we get the flush lock first, we know that any inode that has\n * been flushed delwri has had the flush completed by the time we check that\n * the inode is clean.\n *\n * Note that because the inode is flushed delayed write by AIL pushing, the\n * flush lock may already be held here and waiting on it can result in very\n * long latencies.  Hence for sync reclaims, where we wait on the flush lock,\n * the caller should push the AIL first before trying to reclaim inodes to\n * minimise the amount of time spent waiting.  For background relaim, we only\n * bother to reclaim clean inodes anyway.\n *\n * Hence the order of actions after gaining the locks should be:\n *\tbad\t\t=> reclaim\n *\tshutdown\t=> unpin and reclaim\n *\tpinned, async\t=> requeue\n *\tpinned, sync\t=> unpin\n *\tstale\t\t=> reclaim\n *\tclean\t\t=> reclaim\n *\tdirty, async\t=> requeue\n *\tdirty, sync\t=> flush, wait and reclaim\n */\nSTATIC int\nxfs_reclaim_inode(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_perag\t*pag,\n\tint\t\t\tsync_mode)\n{\n\tstruct xfs_buf\t\t*bp = NULL;\n\txfs_ino_t\t\tino = ip->i_ino; /* for radix_tree_delete */\n\tint\t\t\terror;\n\nrestart:\n\terror = 0;\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tif (!xfs_iflock_nowait(ip)) {\n\t\tif (!(sync_mode & SYNC_WAIT))\n\t\t\tgoto out;\n\t\txfs_iflock(ip);\n\t}\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount)) {\n\t\txfs_iunpin_wait(ip);\n\t\t/* xfs_iflush_abort() drops the flush lock */\n\t\txfs_iflush_abort(ip, false);\n\t\tgoto reclaim;\n\t}\n\tif (xfs_ipincount(ip)) {\n\t\tif (!(sync_mode & SYNC_WAIT))\n\t\t\tgoto out_ifunlock;\n\t\txfs_iunpin_wait(ip);\n\t}\n\tif (xfs_iflags_test(ip, XFS_ISTALE) || xfs_inode_clean(ip)) {\n\t\txfs_ifunlock(ip);\n\t\tgoto reclaim;\n\t}\n\n\t/*\n\t * Never flush out dirty data during non-blocking reclaim, as it would\n\t * just contend with AIL pushing trying to do the same job.\n\t */\n\tif (!(sync_mode & SYNC_WAIT))\n\t\tgoto out_ifunlock;\n\n\t/*\n\t * Now we have an inode that needs flushing.\n\t *\n\t * Note that xfs_iflush will never block on the inode buffer lock, as\n\t * xfs_ifree_cluster() can lock the inode buffer before it locks the\n\t * ip->i_lock, and we are doing the exact opposite here.  As a result,\n\t * doing a blocking xfs_imap_to_bp() to get the cluster buffer would\n\t * result in an ABBA deadlock with xfs_ifree_cluster().\n\t *\n\t * As xfs_ifree_cluser() must gather all inodes that are active in the\n\t * cache to mark them stale, if we hit this case we don't actually want\n\t * to do IO here - we want the inode marked stale so we can simply\n\t * reclaim it.  Hence if we get an EAGAIN error here,  just unlock the\n\t * inode, back off and try again.  Hopefully the next pass through will\n\t * see the stale flag set on the inode.\n\t */\n\terror = xfs_iflush(ip, &bp);\n\tif (error == -EAGAIN) {\n\t\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t\t/* backoff longer than in xfs_ifree_cluster */\n\t\tdelay(2);\n\t\tgoto restart;\n\t}\n\n\tif (!error) {\n\t\terror = xfs_bwrite(bp);\n\t\txfs_buf_relse(bp);\n\t}\n\nreclaim:\n\tASSERT(!xfs_isiflocked(ip));\n\n\t/*\n\t * Because we use RCU freeing we need to ensure the inode always appears\n\t * to be reclaimed with an invalid inode number when in the free state.\n\t * We do this as early as possible under the ILOCK so that\n\t * xfs_iflush_cluster() and xfs_ifree_cluster() can be guaranteed to\n\t * detect races with us here. By doing this, we guarantee that once\n\t * xfs_iflush_cluster() or xfs_ifree_cluster() has locked XFS_ILOCK that\n\t * it will see either a valid inode that will serialise correctly, or it\n\t * will see an invalid inode that it can skip.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\tXFS_STATS_INC(ip->i_mount, xs_ig_reclaims);\n\t/*\n\t * Remove the inode from the per-AG radix tree.\n\t *\n\t * Because radix_tree_delete won't complain even if the item was never\n\t * added to the tree assert that it's been there before to catch\n\t * problems with the inode life time early on.\n\t */\n\tspin_lock(&pag->pag_ici_lock);\n\tif (!radix_tree_delete(&pag->pag_ici_root,\n\t\t\t\tXFS_INO_TO_AGINO(ip->i_mount, ino)))\n\t\tASSERT(0);\n\txfs_perag_clear_reclaim_tag(pag);\n\tspin_unlock(&pag->pag_ici_lock);\n\n\t/*\n\t * Here we do an (almost) spurious inode lock in order to coordinate\n\t * with inode cache radix tree lookups.  This is because the lookup\n\t * can reference the inodes in the cache without taking references.\n\t *\n\t * We make that OK here by ensuring that we wait until the inode is\n\t * unlocked after the lookup before we go ahead and free it.\n\t */\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\txfs_qm_dqdetach(ip);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\t__xfs_inode_free(ip);\n\treturn error;\n\nout_ifunlock:\n\txfs_ifunlock(ip);\nout:\n\txfs_iflags_clear(ip, XFS_IRECLAIM);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t/*\n\t * We could return -EAGAIN here to make reclaim rescan the inode tree in\n\t * a short while. However, this just burns CPU time scanning the tree\n\t * waiting for IO to complete and the reclaim work never goes back to\n\t * the idle state. Instead, return 0 to let the next scheduled\n\t * background reclaim attempt to reclaim the inode again.\n\t */\n\treturn 0;\n}\n\n/*\n * Walk the AGs and reclaim the inodes in them. Even if the filesystem is\n * corrupted, we still want to try to reclaim all the inodes. If we don't,\n * then a shut down during filesystem unmount reclaim walk leak all the\n * unreclaimed inodes.\n */\nSTATIC int\nxfs_reclaim_inodes_ag(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tflags,\n\tint\t\t\t*nr_to_scan)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tag;\n\tint\t\t\ttrylock = flags & SYNC_TRYLOCK;\n\tint\t\t\tskipped;\n\nrestart:\n\tag = 0;\n\tskipped = 0;\n\twhile ((pag = xfs_perag_get_tag(mp, ag, XFS_ICI_RECLAIM_TAG))) {\n\t\tunsigned long\tfirst_index = 0;\n\t\tint\t\tdone = 0;\n\t\tint\t\tnr_found = 0;\n\n\t\tag = pag->pag_agno + 1;\n\n\t\tif (trylock) {\n\t\t\tif (!mutex_trylock(&pag->pag_ici_reclaim_lock)) {\n\t\t\t\tskipped++;\n\t\t\t\txfs_perag_put(pag);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfirst_index = pag->pag_ici_reclaim_cursor;\n\t\t} else\n\t\t\tmutex_lock(&pag->pag_ici_reclaim_lock);\n\n\t\tdo {\n\t\t\tstruct xfs_inode *batch[XFS_LOOKUP_BATCH];\n\t\t\tint\ti;\n\n\t\t\trcu_read_lock();\n\t\t\tnr_found = radix_tree_gang_lookup_tag(\n\t\t\t\t\t&pag->pag_ici_root,\n\t\t\t\t\t(void **)batch, first_index,\n\t\t\t\t\tXFS_LOOKUP_BATCH,\n\t\t\t\t\tXFS_ICI_RECLAIM_TAG);\n\t\t\tif (!nr_found) {\n\t\t\t\tdone = 1;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Grab the inodes before we drop the lock. if we found\n\t\t\t * nothing, nr == 0 and the loop will be skipped.\n\t\t\t */\n\t\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\t\tstruct xfs_inode *ip = batch[i];\n\n\t\t\t\tif (done || xfs_reclaim_inode_grab(ip, flags))\n\t\t\t\t\tbatch[i] = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * Update the index for the next lookup. Catch\n\t\t\t\t * overflows into the next AG range which can\n\t\t\t\t * occur if we have inodes in the last block of\n\t\t\t\t * the AG and we are currently pointing to the\n\t\t\t\t * last inode.\n\t\t\t\t *\n\t\t\t\t * Because we may see inodes that are from the\n\t\t\t\t * wrong AG due to RCU freeing and\n\t\t\t\t * reallocation, only update the index if it\n\t\t\t\t * lies in this AG. It was a race that lead us\n\t\t\t\t * to see this inode, so another lookup from\n\t\t\t\t * the same index will not find it again.\n\t\t\t\t */\n\t\t\t\tif (XFS_INO_TO_AGNO(mp, ip->i_ino) !=\n\t\t\t\t\t\t\t\tpag->pag_agno)\n\t\t\t\t\tcontinue;\n\t\t\t\tfirst_index = XFS_INO_TO_AGINO(mp, ip->i_ino + 1);\n\t\t\t\tif (first_index < XFS_INO_TO_AGINO(mp, ip->i_ino))\n\t\t\t\t\tdone = 1;\n\t\t\t}\n\n\t\t\t/* unlock now we've grabbed the inodes. */\n\t\t\trcu_read_unlock();\n\n\t\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\t\tif (!batch[i])\n\t\t\t\t\tcontinue;\n\t\t\t\terror = xfs_reclaim_inode(batch[i], pag, flags);\n\t\t\t\tif (error && last_error != -EFSCORRUPTED)\n\t\t\t\t\tlast_error = error;\n\t\t\t}\n\n\t\t\t*nr_to_scan -= XFS_LOOKUP_BATCH;\n\n\t\t\tcond_resched();\n\n\t\t} while (nr_found && !done && *nr_to_scan > 0);\n\n\t\tif (trylock && !done)\n\t\t\tpag->pag_ici_reclaim_cursor = first_index;\n\t\telse\n\t\t\tpag->pag_ici_reclaim_cursor = 0;\n\t\tmutex_unlock(&pag->pag_ici_reclaim_lock);\n\t\txfs_perag_put(pag);\n\t}\n\n\t/*\n\t * if we skipped any AG, and we still have scan count remaining, do\n\t * another pass this time using blocking reclaim semantics (i.e\n\t * waiting on the reclaim locks and ignoring the reclaim cursors). This\n\t * ensure that when we get more reclaimers than AGs we block rather\n\t * than spin trying to execute reclaim.\n\t */\n\tif (skipped && (flags & SYNC_WAIT) && *nr_to_scan > 0) {\n\t\ttrylock = 0;\n\t\tgoto restart;\n\t}\n\treturn last_error;\n}\n\nint\nxfs_reclaim_inodes(\n\txfs_mount_t\t*mp,\n\tint\t\tmode)\n{\n\tint\t\tnr_to_scan = INT_MAX;\n\n\treturn xfs_reclaim_inodes_ag(mp, mode, &nr_to_scan);\n}\n\n/*\n * Scan a certain number of inodes for reclaim.\n *\n * When called we make sure that there is a background (fast) inode reclaim in\n * progress, while we will throttle the speed of reclaim via doing synchronous\n * reclaim of inodes. That means if we come across dirty inodes, we wait for\n * them to be cleaned, which we hope will not be very long due to the\n * background walker having already kicked the IO off on those dirty inodes.\n */\nlong\nxfs_reclaim_inodes_nr(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tnr_to_scan)\n{\n\t/* kick background reclaimer and push the AIL */\n\txfs_reclaim_work_queue(mp);\n\txfs_ail_push_all(mp->m_ail);\n\n\treturn xfs_reclaim_inodes_ag(mp, SYNC_TRYLOCK | SYNC_WAIT, &nr_to_scan);\n}\n\n/*\n * Return the number of reclaimable inodes in the filesystem for\n * the shrinker to determine how much to reclaim.\n */\nint\nxfs_reclaim_inodes_count(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tag = 0;\n\tint\t\t\treclaimable = 0;\n\n\twhile ((pag = xfs_perag_get_tag(mp, ag, XFS_ICI_RECLAIM_TAG))) {\n\t\tag = pag->pag_agno + 1;\n\t\treclaimable += pag->pag_ici_reclaimable;\n\t\txfs_perag_put(pag);\n\t}\n\treturn reclaimable;\n}\n\nSTATIC int\nxfs_inode_match_id(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_UID) &&\n\t    !uid_eq(VFS_I(ip)->i_uid, eofb->eof_uid))\n\t\treturn 0;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_GID) &&\n\t    !gid_eq(VFS_I(ip)->i_gid, eofb->eof_gid))\n\t\treturn 0;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_PRID) &&\n\t    xfs_get_projid(ip) != eofb->eof_prid)\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * A union-based inode filtering algorithm. Process the inode if any of the\n * criteria match. This is for global/internal scans only.\n */\nSTATIC int\nxfs_inode_match_id_union(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_UID) &&\n\t    uid_eq(VFS_I(ip)->i_uid, eofb->eof_uid))\n\t\treturn 1;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_GID) &&\n\t    gid_eq(VFS_I(ip)->i_gid, eofb->eof_gid))\n\t\treturn 1;\n\n\tif ((eofb->eof_flags & XFS_EOF_FLAGS_PRID) &&\n\t    xfs_get_projid(ip) == eofb->eof_prid)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nSTATIC int\nxfs_inode_free_eofblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\tint ret = 0;\n\tstruct xfs_eofblocks *eofb = args;\n\tint match;\n\n\tif (!xfs_can_free_eofblocks(ip, false)) {\n\t\t/* inode could be preallocated or append-only */\n\t\ttrace_xfs_inode_free_eofblocks_invalid(ip);\n\t\txfs_inode_clear_eofblocks_tag(ip);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the mapping is dirty the operation can block and wait for some\n\t * time. Unless we are waiting, skip it.\n\t */\n\tif (!(flags & SYNC_WAIT) &&\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\tif (eofb) {\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_UNION)\n\t\t\tmatch = xfs_inode_match_id_union(ip, eofb);\n\t\telse\n\t\t\tmatch = xfs_inode_match_id(ip, eofb);\n\t\tif (!match)\n\t\t\treturn 0;\n\n\t\t/* skip the inode if the file size is too small */\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_MINFILESIZE &&\n\t\t    XFS_ISIZE(ip) < eofb->eof_min_file_size)\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t * If the caller is waiting, return -EAGAIN to keep the background\n\t * scanner moving and revisit the inode in a subsequent pass.\n\t */\n\tif (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {\n\t\tif (flags & SYNC_WAIT)\n\t\t\tret = -EAGAIN;\n\t\treturn ret;\n\t}\n\tret = xfs_free_eofblocks(ip);\n\txfs_iunlock(ip, XFS_IOLOCK_EXCL);\n\n\treturn ret;\n}\n\nstatic int\n__xfs_icache_free_eofblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb,\n\tint\t\t\t(*execute)(struct xfs_inode *ip, int flags,\n\t\t\t\t\t   void *args),\n\tint\t\t\ttag)\n{\n\tint flags = SYNC_TRYLOCK;\n\n\tif (eofb && (eofb->eof_flags & XFS_EOF_FLAGS_SYNC))\n\t\tflags = SYNC_WAIT;\n\n\treturn xfs_inode_ag_iterator_tag(mp, execute, flags,\n\t\t\t\t\t eofb, tag);\n}\n\nint\nxfs_icache_free_eofblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\treturn __xfs_icache_free_eofblocks(mp, eofb, xfs_inode_free_eofblocks,\n\t\t\tXFS_ICI_EOFBLOCKS_TAG);\n}\n\n/*\n * Run eofblocks scans on the quotas applicable to the inode. For inodes with\n * multiple quotas, we don't know exactly which quota caused an allocation\n * failure. We make a best effort by including each quota under low free space\n * conditions (less than 1% free space) in the scan.\n */\nstatic int\n__xfs_inode_free_quota_eofblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\t(*execute)(struct xfs_mount *mp,\n\t\t\t\t\t   struct xfs_eofblocks\t*eofb))\n{\n\tint scan = 0;\n\tstruct xfs_eofblocks eofb = {0};\n\tstruct xfs_dquot *dq;\n\n\t/*\n\t * Run a sync scan to increase effectiveness and use the union filter to\n\t * cover all applicable quotas in a single scan.\n\t */\n\teofb.eof_flags = XFS_EOF_FLAGS_UNION|XFS_EOF_FLAGS_SYNC;\n\n\tif (XFS_IS_UQUOTA_ENFORCED(ip->i_mount)) {\n\t\tdq = xfs_inode_dquot(ip, XFS_DQ_USER);\n\t\tif (dq && xfs_dquot_lowsp(dq)) {\n\t\t\teofb.eof_uid = VFS_I(ip)->i_uid;\n\t\t\teofb.eof_flags |= XFS_EOF_FLAGS_UID;\n\t\t\tscan = 1;\n\t\t}\n\t}\n\n\tif (XFS_IS_GQUOTA_ENFORCED(ip->i_mount)) {\n\t\tdq = xfs_inode_dquot(ip, XFS_DQ_GROUP);\n\t\tif (dq && xfs_dquot_lowsp(dq)) {\n\t\t\teofb.eof_gid = VFS_I(ip)->i_gid;\n\t\t\teofb.eof_flags |= XFS_EOF_FLAGS_GID;\n\t\t\tscan = 1;\n\t\t}\n\t}\n\n\tif (scan)\n\t\texecute(ip->i_mount, &eofb);\n\n\treturn scan;\n}\n\nint\nxfs_inode_free_quota_eofblocks(\n\tstruct xfs_inode *ip)\n{\n\treturn __xfs_inode_free_quota_eofblocks(ip, xfs_icache_free_eofblocks);\n}\n\nstatic inline unsigned long\nxfs_iflag_for_tag(\n\tint\t\ttag)\n{\n\tswitch (tag) {\n\tcase XFS_ICI_EOFBLOCKS_TAG:\n\t\treturn XFS_IEOFBLOCKS;\n\tcase XFS_ICI_COWBLOCKS_TAG:\n\t\treturn XFS_ICOWBLOCKS;\n\tdefault:\n\t\tASSERT(0);\n\t\treturn 0;\n\t}\n}\n\nstatic void\n__xfs_inode_set_blocks_tag(\n\txfs_inode_t\t*ip,\n\tvoid\t\t(*execute)(struct xfs_mount *mp),\n\tvoid\t\t(*set_tp)(struct xfs_mount *mp, xfs_agnumber_t agno,\n\t\t\t\t  int error, unsigned long caller_ip),\n\tint\t\ttag)\n{\n\tstruct xfs_mount *mp = ip->i_mount;\n\tstruct xfs_perag *pag;\n\tint tagged;\n\n\t/*\n\t * Don't bother locking the AG and looking up in the radix trees\n\t * if we already know that we have the tag set.\n\t */\n\tif (ip->i_flags & xfs_iflag_for_tag(tag))\n\t\treturn;\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags |= xfs_iflag_for_tag(tag);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\ttagged = radix_tree_tagged(&pag->pag_ici_root, tag);\n\tradix_tree_tag_set(&pag->pag_ici_root,\n\t\t\t   XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino), tag);\n\tif (!tagged) {\n\t\t/* propagate the eofblocks tag up into the perag radix tree */\n\t\tspin_lock(&ip->i_mount->m_perag_lock);\n\t\tradix_tree_tag_set(&ip->i_mount->m_perag_tree,\n\t\t\t\t   XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino),\n\t\t\t\t   tag);\n\t\tspin_unlock(&ip->i_mount->m_perag_lock);\n\n\t\t/* kick off background trimming */\n\t\texecute(ip->i_mount);\n\n\t\tset_tp(ip->i_mount, pag->pag_agno, -1, _RET_IP_);\n\t}\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_set_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_eofblocks_tag(ip);\n\treturn __xfs_inode_set_blocks_tag(ip, xfs_queue_eofblocks,\n\t\t\ttrace_xfs_perag_set_eofblocks,\n\t\t\tXFS_ICI_EOFBLOCKS_TAG);\n}\n\nstatic void\n__xfs_inode_clear_blocks_tag(\n\txfs_inode_t\t*ip,\n\tvoid\t\t(*clear_tp)(struct xfs_mount *mp, xfs_agnumber_t agno,\n\t\t\t\t    int error, unsigned long caller_ip),\n\tint\t\ttag)\n{\n\tstruct xfs_mount *mp = ip->i_mount;\n\tstruct xfs_perag *pag;\n\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags &= ~xfs_iflag_for_tag(tag);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\tradix_tree_tag_clear(&pag->pag_ici_root,\n\t\t\t     XFS_INO_TO_AGINO(ip->i_mount, ip->i_ino), tag);\n\tif (!radix_tree_tagged(&pag->pag_ici_root, tag)) {\n\t\t/* clear the eofblocks tag from the perag radix tree */\n\t\tspin_lock(&ip->i_mount->m_perag_lock);\n\t\tradix_tree_tag_clear(&ip->i_mount->m_perag_tree,\n\t\t\t\t     XFS_INO_TO_AGNO(ip->i_mount, ip->i_ino),\n\t\t\t\t     tag);\n\t\tspin_unlock(&ip->i_mount->m_perag_lock);\n\t\tclear_tp(ip->i_mount, pag->pag_agno, -1, _RET_IP_);\n\t}\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_clear_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_eofblocks_tag(ip);\n\treturn __xfs_inode_clear_blocks_tag(ip,\n\t\t\ttrace_xfs_perag_clear_eofblocks, XFS_ICI_EOFBLOCKS_TAG);\n}\n\n/*\n * Set ourselves up to free CoW blocks from this file.  If it's already clean\n * then we can bail out quickly, but otherwise we must back off if the file\n * is undergoing some kind of write.\n */\nstatic bool\nxfs_prep_free_cowblocks(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_ifork\t*ifp)\n{\n\t/*\n\t * Just clear the tag if we have an empty cow fork or none at all. It's\n\t * possible the inode was fully unshared since it was originally tagged.\n\t */\n\tif (!xfs_is_reflink_inode(ip) || !ifp->if_bytes) {\n\t\ttrace_xfs_inode_free_cowblocks_invalid(ip);\n\t\txfs_inode_clear_cowblocks_tag(ip);\n\t\treturn false;\n\t}\n\n\t/*\n\t * If the mapping is dirty or under writeback we cannot touch the\n\t * CoW fork.  Leave it alone if we're in the midst of a directio.\n\t */\n\tif ((VFS_I(ip)->i_state & I_DIRTY_PAGES) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_WRITEBACK) ||\n\t    atomic_read(&VFS_I(ip)->i_dio_count))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Automatic CoW Reservation Freeing\n *\n * These functions automatically garbage collect leftover CoW reservations\n * that were made on behalf of a cowextsize hint when we start to run out\n * of quota or when the reservations sit around for too long.  If the file\n * has dirty pages or is undergoing writeback, its CoW reservations will\n * be retained.\n *\n * The actual garbage collection piggybacks off the same code that runs\n * the speculative EOF preallocation garbage collector.\n */\nSTATIC int\nxfs_inode_free_cowblocks(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags,\n\tvoid\t\t\t*args)\n{\n\tstruct xfs_eofblocks\t*eofb = args;\n\tstruct xfs_ifork\t*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);\n\tint\t\t\tmatch;\n\tint\t\t\tret = 0;\n\n\tif (!xfs_prep_free_cowblocks(ip, ifp))\n\t\treturn 0;\n\n\tif (eofb) {\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_UNION)\n\t\t\tmatch = xfs_inode_match_id_union(ip, eofb);\n\t\telse\n\t\t\tmatch = xfs_inode_match_id(ip, eofb);\n\t\tif (!match)\n\t\t\treturn 0;\n\n\t\t/* skip the inode if the file size is too small */\n\t\tif (eofb->eof_flags & XFS_EOF_FLAGS_MINFILESIZE &&\n\t\t    XFS_ISIZE(ip) < eofb->eof_min_file_size)\n\t\t\treturn 0;\n\t}\n\n\t/* Free the CoW blocks */\n\txfs_ilock(ip, XFS_IOLOCK_EXCL);\n\txfs_ilock(ip, XFS_MMAPLOCK_EXCL);\n\n\t/*\n\t * Check again, nobody else should be able to dirty blocks or change\n\t * the reflink iflag now that we have the first two locks held.\n\t */\n\tif (xfs_prep_free_cowblocks(ip, ifp))\n\t\tret = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF, false);\n\n\txfs_iunlock(ip, XFS_MMAPLOCK_EXCL);\n\txfs_iunlock(ip, XFS_IOLOCK_EXCL);\n\n\treturn ret;\n}\n\nint\nxfs_icache_free_cowblocks(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_eofblocks\t*eofb)\n{\n\treturn __xfs_icache_free_eofblocks(mp, eofb, xfs_inode_free_cowblocks,\n\t\t\tXFS_ICI_COWBLOCKS_TAG);\n}\n\nint\nxfs_inode_free_quota_cowblocks(\n\tstruct xfs_inode *ip)\n{\n\treturn __xfs_inode_free_quota_eofblocks(ip, xfs_icache_free_cowblocks);\n}\n\nvoid\nxfs_inode_set_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_cowblocks_tag(ip);\n\treturn __xfs_inode_set_blocks_tag(ip, xfs_queue_cowblocks,\n\t\t\ttrace_xfs_perag_set_cowblocks,\n\t\t\tXFS_ICI_COWBLOCKS_TAG);\n}\n\nvoid\nxfs_inode_clear_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_cowblocks_tag(ip);\n\treturn __xfs_inode_clear_blocks_tag(ip,\n\t\t\ttrace_xfs_perag_clear_cowblocks, XFS_ICI_COWBLOCKS_TAG);\n}\n"], "filenames": ["fs/xfs/xfs_icache.c"], "buggy_code_start_loc": [311], "buggy_code_end_loc": [511], "fixing_code_start_loc": [312], "fixing_code_end_loc": [533], "type": "CWE-476", "message": "An issue was discovered in fs/xfs/xfs_icache.c in the Linux kernel through 4.17.3. There is a NULL pointer dereference and panic in lookup_slow() on a NULL inode->i_ops pointer when doing pathwalks on a corrupted xfs image. This occurs because of a lack of proper validation that cached inodes are free during allocation.", "other": {"cve": {"id": "CVE-2018-13093", "sourceIdentifier": "cve@mitre.org", "published": "2018-07-03T10:29:00.223", "lastModified": "2019-08-06T17:15:21.303", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "An issue was discovered in fs/xfs/xfs_icache.c in the Linux kernel through 4.17.3. There is a NULL pointer dereference and panic in lookup_slow() on a NULL inode->i_ops pointer when doing pathwalks on a corrupted xfs image. This occurs because of a lack of proper validation that cached inodes are free during allocation."}, {"lang": "es", "value": "Se ha descubierto un problema en fs/xfs/xfs_icache.c en el kernel de Linux hasta la versi\u00f3n 4.17.3. Existe una desreferencia de puntero NULL y p\u00e1nico en lookup_slow() en un puntero NULL inode->i_ops cuando se navega por rutas en una imagen xfs corrupta. Esto ocurre a causa de la ausencia de una validaci\u00f3n correcta de que los inodes en cach\u00e9 est\u00e9n libres durante la asignaci\u00f3n."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.17.3", "matchCriteriaId": "C6ECF5D0-AFCC-470A-9D01-4A372C09556D"}]}]}], "references": [{"url": "https://access.redhat.com/errata/RHSA-2019:2029", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2019:2043", "source": "cve@mitre.org"}, {"url": "https://bugzilla.kernel.org/show_bug.cgi?id=199367", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://git.kernel.org/pub/scm/fs/xfs/xfs-linux.git/commit/?h=for-next&id=afca6c5b2595fc44383919fba740c194b0b76aff", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/afca6c5b2595fc44383919fba740c194b0b76aff", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2020/03/msg00001.html", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4094-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/afca6c5b2595fc44383919fba740c194b0b76aff"}}