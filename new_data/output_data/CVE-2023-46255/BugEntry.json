{"buggy_code": ["package common\n\nimport (\n\t\"fmt\"\n\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\n\tv1 \"github.com/authzed/authzed-go/proto/authzed/api/v1\"\n\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n\t\"github.com/authzed/spicedb/pkg/spiceerrors\"\n\t\"github.com/authzed/spicedb/pkg/tuple\"\n)\n\n// SerializationError is returned when there's been a serialization\n// error while performing a datastore operation\ntype SerializationError struct {\n\terror\n}\n\nfunc (err SerializationError) GRPCStatus() *status.Status {\n\treturn spiceerrors.WithCodeAndDetails(\n\t\terr,\n\t\tcodes.Aborted,\n\t\tspiceerrors.ForReason(\n\t\t\tv1.ErrorReason_ERROR_REASON_SERIALIZATION_FAILURE,\n\t\t\tmap[string]string{},\n\t\t),\n\t)\n}\n\nfunc (err SerializationError) Unwrap() error {\n\treturn err.error\n}\n\n// NewSerializationError creates a new SerializationError\nfunc NewSerializationError(err error) error {\n\treturn SerializationError{err}\n}\n\n// CreateRelationshipExistsError is an error returned when attempting to CREATE an already-existing\n// relationship.\ntype CreateRelationshipExistsError struct {\n\terror\n\n\t// Relationship is the relationship that caused the error. May be nil, depending on the datastore.\n\tRelationship *core.RelationTuple\n}\n\n// GRPCStatus implements retrieving the gRPC status for the error.\nfunc (err CreateRelationshipExistsError) GRPCStatus() *status.Status {\n\tif err.Relationship == nil {\n\t\treturn spiceerrors.WithCodeAndDetails(\n\t\t\terr,\n\t\t\tcodes.AlreadyExists,\n\t\t\tspiceerrors.ForReason(\n\t\t\t\tv1.ErrorReason_ERROR_REASON_ATTEMPT_TO_RECREATE_RELATIONSHIP,\n\t\t\t\tmap[string]string{},\n\t\t\t),\n\t\t)\n\t}\n\n\trelationship := tuple.ToRelationship(err.Relationship)\n\treturn spiceerrors.WithCodeAndDetails(\n\t\terr,\n\t\tcodes.AlreadyExists,\n\t\tspiceerrors.ForReason(\n\t\t\tv1.ErrorReason_ERROR_REASON_ATTEMPT_TO_RECREATE_RELATIONSHIP,\n\t\t\tmap[string]string{\n\t\t\t\t\"relationship\":       tuple.StringRelationshipWithoutCaveat(relationship),\n\t\t\t\t\"resource_type\":      relationship.Resource.ObjectType,\n\t\t\t\t\"resource_object_id\": relationship.Resource.ObjectId,\n\t\t\t\t\"resource_relation\":  relationship.Relation,\n\t\t\t\t\"subject_type\":       relationship.Subject.Object.ObjectType,\n\t\t\t\t\"subject_object_id\":  relationship.Subject.Object.ObjectId,\n\t\t\t\t\"subject_relation\":   relationship.Subject.OptionalRelation,\n\t\t\t},\n\t\t),\n\t)\n}\n\n// NewCreateRelationshipExistsError creates a new CreateRelationshipExistsError.\nfunc NewCreateRelationshipExistsError(relationship *core.RelationTuple) error {\n\tmsg := \"could not CREATE one or more relationships, as they already existed. If this is persistent, please switch to TOUCH operations or specify a precondition\"\n\tif relationship != nil {\n\t\tmsg = fmt.Sprintf(\"could not CREATE relationship `%s`, as it already existed. If this is persistent, please switch to TOUCH operations or specify a precondition\", tuple.StringWithoutCaveat(relationship))\n\t}\n\n\treturn CreateRelationshipExistsError{\n\t\tfmt.Errorf(msg),\n\t\trelationship,\n\t}\n}\n", "package crdb\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/IBM/pgxpoolprometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/shopspring/decimal\"\n\t\"go.opentelemetry.io/otel\"\n\t\"golang.org/x/sync/errgroup\"\n\t\"resenje.org/singleflight\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/crdb/migrations\"\n\t\"github.com/authzed/spicedb/internal/datastore/crdb/pool\"\n\tpgxcommon \"github.com/authzed/spicedb/internal/datastore/postgres/common\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nvar (\n\tpsql = sq.StatementBuilder.PlaceholderFormat(sq.Dollar)\n\n\tgcTTLRegex = regexp.MustCompile(`gc\\.ttlseconds\\s*=\\s*([1-9][0-9]+)`)\n\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/crdb\")\n)\n\nconst (\n\tEngine            = \"cockroachdb\"\n\ttableNamespace    = \"namespace_config\"\n\ttableTuple        = \"relation_tuple\"\n\ttableTransactions = \"transactions\"\n\ttableCaveat       = \"caveat\"\n\n\tcolNamespace         = \"namespace\"\n\tcolConfig            = \"serialized_config\"\n\tcolTimestamp         = \"timestamp\"\n\tcolTransactionKey    = \"key\"\n\tcolObjectID          = \"object_id\"\n\tcolRelation          = \"relation\"\n\tcolUsersetNamespace  = \"userset_namespace\"\n\tcolUsersetObjectID   = \"userset_object_id\"\n\tcolUsersetRelation   = \"userset_relation\"\n\tcolCaveatName        = \"name\"\n\tcolCaveatDefinition  = \"definition\"\n\tcolCaveatContextName = \"caveat_name\"\n\tcolCaveatContext     = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore: %w\"\n\terrRevision            = \"unable to find revision: %w\"\n\n\tquerySelectNow      = \"SELECT cluster_logical_timestamp()\"\n\tqueryShowZoneConfig = \"SHOW ZONE CONFIGURATION FOR RANGE default;\"\n\n\tlivingTupleConstraint = \"pk_relation_tuple\"\n)\n\nfunc newCRDBDatastore(url string, options ...Option) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\treadPoolConfig, err := pgxpool.ParseConfig(url)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\tconfig.readPoolOpts.ConfigurePgx(readPoolConfig)\n\n\twritePoolConfig, err := pgxpool.ParseConfig(url)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\tconfig.writePoolOpts.ConfigurePgx(writePoolConfig)\n\n\tinitCtx, initCancel := context.WithTimeout(context.Background(), 5*time.Minute)\n\tdefer initCancel()\n\n\thealthChecker, err := pool.NewNodeHealthChecker(url)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\t// The initPool is a 1-connection pool that is only used for setup tasks.\n\t// The actual pools are not given the initCtx, since cancellation can\n\t// interfere with pool setup.\n\tinitPoolConfig := readPoolConfig.Copy()\n\tinitPoolConfig.MinConns = 1\n\tinitPool, err := pool.NewRetryPool(initCtx, \"init\", initPoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\tdefer initPool.Close()\n\n\tvar version crdbVersion\n\tif err := queryServerVersion(initCtx, initPool, &version); err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tchangefeedQuery := queryChangefeed\n\tif version.Major < 22 {\n\t\tlog.Info().Object(\"version\", version).Msg(\"using changefeed query for CRDB version < 22\")\n\t\tchangefeedQuery = queryChangefeedPreV22\n\t}\n\n\tclusterTTLNanos, err := readClusterTTLNanos(initCtx, initPool)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to read cluster gc window: %w\", err)\n\t}\n\n\tgcWindowNanos := config.gcWindow.Nanoseconds()\n\tif clusterTTLNanos < gcWindowNanos {\n\t\tlog.Warn().\n\t\t\tInt64(\"cockroach_cluster_gc_window_nanos\", clusterTTLNanos).\n\t\t\tInt64(\"spicedb_gc_window_nanos\", gcWindowNanos).\n\t\t\tMsg(\"configured CockroachDB cluster gc window is less than configured SpiceDB gc window, falling back to CRDB value - see https://spicedb.dev/d/crdb-gc-window-warning\")\n\t\tconfig.gcWindow = time.Duration(clusterTTLNanos) * time.Nanosecond\n\t}\n\n\tkeySetInit := newKeySet\n\tvar keyer overlapKeyer\n\tswitch config.overlapStrategy {\n\tcase overlapStrategyStatic:\n\t\tif len(config.overlapKey) == 0 {\n\t\t\treturn nil, fmt.Errorf(\n\t\t\t\terrUnableToInstantiate,\n\t\t\t\tfmt.Errorf(\"static tx overlap strategy specified without an overlap key\"),\n\t\t\t)\n\t\t}\n\t\tkeyer = appendStaticKey(config.overlapKey)\n\tcase overlapStrategyPrefix:\n\t\tkeyer = prefixKeyer\n\tcase overlapStrategyRequest:\n\t\t// overlap keys are computed over requests and not data\n\t\tkeyer = noOverlapKeyer\n\t\tkeySetInit = overlapKeysFromContext\n\tcase overlapStrategyInsecure:\n\t\tlog.Warn().Str(\"strategy\", overlapStrategyInsecure).\n\t\t\tMsg(\"running in this mode is only safe when replicas == nodes\")\n\t\tkeyer = noOverlapKeyer\n\t}\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tds := &crdbDatastore{\n\t\tRemoteClockRevisions: revisions.NewRemoteClockRevisions(\n\t\t\tconfig.gcWindow,\n\t\t\tmaxRevisionStaleness,\n\t\t\tconfig.followerReadDelay,\n\t\t\tconfig.revisionQuantization,\n\t\t),\n\t\tDecimalDecoder:       revision.DecimalDecoder{},\n\t\tdburl:                url,\n\t\twatchBufferLength:    config.watchBufferLength,\n\t\twriteOverlapKeyer:    keyer,\n\t\toverlapKeyInit:       keySetInit,\n\t\tdisableStats:         config.disableStats,\n\t\tbeginChangefeedQuery: changefeedQuery,\n\t}\n\tds.RemoteClockRevisions.SetNowFunc(ds.headRevisionInternal)\n\n\t// this ctx and cancel is tied to the lifetime of the datastore\n\tds.ctx, ds.cancel = context.WithCancel(context.Background())\n\tds.writePool, err = pool.NewRetryPool(ds.ctx, \"write\", writePoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\tds.cancel()\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\tds.readPool, err = pool.NewRetryPool(ds.ctx, \"read\", readPoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\tds.cancel()\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tif config.enablePrometheusStats {\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(ds.writePool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"write\",\n\t\t})); err != nil {\n\t\t\tds.cancel()\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(ds.readPool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"read\",\n\t\t})); err != nil {\n\t\t\tds.cancel()\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t}\n\n\t// TODO: this (and the GC startup that it's based on for mysql/pg) should\n\t// be removed and have the lifetimes tied to server start/stop.\n\n\t// Start goroutines for pruning\n\tif config.enableConnectionBalancing {\n\t\tlog.Ctx(initCtx).Info().Msg(\"starting cockroach connection balancer\")\n\t\tds.pruneGroup, ds.ctx = errgroup.WithContext(ds.ctx)\n\t\twritePoolBalancer := pool.NewNodeConnectionBalancer(ds.writePool, healthChecker, 5*time.Second)\n\t\treadPoolBalancer := pool.NewNodeConnectionBalancer(ds.readPool, healthChecker, 5*time.Second)\n\t\tds.pruneGroup.Go(func() error {\n\t\t\twritePoolBalancer.Prune(ds.ctx)\n\t\t\treturn nil\n\t\t})\n\t\tds.pruneGroup.Go(func() error {\n\t\t\treadPoolBalancer.Prune(ds.ctx)\n\t\t\treturn nil\n\t\t})\n\t\tds.pruneGroup.Go(func() error {\n\t\t\thealthChecker.Poll(ds.ctx, 5*time.Second)\n\t\t\treturn nil\n\t\t})\n\t}\n\n\treturn ds, nil\n}\n\n// NewCRDBDatastore initializes a SpiceDB datastore that uses a CockroachDB\n// database while leveraging its AOST functionality.\nfunc NewCRDBDatastore(url string, options ...Option) (datastore.Datastore, error) {\n\tds, err := newCRDBDatastore(url, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\ntype crdbDatastore struct {\n\t*revisions.RemoteClockRevisions\n\trevision.DecimalDecoder\n\n\tdburl               string\n\treadPool, writePool *pool.RetryPool\n\twatchBufferLength   uint16\n\twriteOverlapKeyer   overlapKeyer\n\toverlapKeyInit      func(ctx context.Context) keySet\n\tdisableStats        bool\n\n\tbeginChangefeedQuery string\n\n\tfeatureGroup singleflight.Group[string, *datastore.Features]\n\n\tpruneGroup *errgroup.Group\n\tctx        context.Context\n\tcancel     context.CancelFunc\n}\n\nfunc (cds *crdbDatastore) SnapshotReader(rev datastore.Revision) datastore.Reader {\n\texecutor := common.QueryExecutor{\n\t\tExecutor: pgxcommon.NewPGXExecutor(cds.readPool),\n\t}\n\n\tfromBuilder := func(query sq.SelectBuilder, fromStr string) sq.SelectBuilder {\n\t\treturn query.From(fromStr + \" AS OF SYSTEM TIME \" + rev.String())\n\t}\n\n\treturn &crdbReader{cds.readPool, executor, noOverlapKeyer, nil, fromBuilder}\n}\n\nfunc (cds *crdbDatastore) ReadWriteTx(\n\tctx context.Context,\n\tf datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tvar commitTimestamp revision.Decimal\n\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\tif config.DisableRetries {\n\t\tctx = context.WithValue(ctx, pool.CtxDisableRetries, true)\n\t}\n\n\terr := cds.writePool.BeginFunc(ctx, func(tx pgx.Tx) error {\n\t\tquerier := pgxcommon.QuerierFuncsFor(tx)\n\t\texecutor := common.QueryExecutor{\n\t\t\tExecutor: pgxcommon.NewPGXExecutor(querier),\n\t\t}\n\n\t\trwt := &crdbReadWriteTXN{\n\t\t\t&crdbReader{\n\t\t\t\tquerier,\n\t\t\t\texecutor,\n\t\t\t\tcds.writeOverlapKeyer,\n\t\t\t\tcds.overlapKeyInit(ctx),\n\t\t\t\tfunc(query sq.SelectBuilder, fromStr string) sq.SelectBuilder {\n\t\t\t\t\treturn query.From(fromStr)\n\t\t\t\t},\n\t\t\t},\n\t\t\ttx,\n\t\t\t0,\n\t\t}\n\n\t\tif err := f(ctx, rwt); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Touching the transaction key happens last so that the \"write intent\" for\n\t\t// the transaction as a whole lands in a range for the affected tuples.\n\t\tfor k := range rwt.overlapKeySet {\n\t\t\tif _, err := tx.Exec(ctx, queryTouchTransaction, k); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error writing overlapping keys: %w\", err)\n\t\t\t}\n\t\t}\n\n\t\tif cds.disableStats {\n\t\t\tvar err error\n\t\t\tcommitTimestamp, err = readCRDBNow(ctx, querier)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error getting commit timestamp: %w\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\tvar err error\n\t\tcommitTimestamp, err = updateCounter(ctx, tx, rwt.relCountChange)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error updating relationship counter: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn datastore.NoRevision, err\n\t}\n\n\treturn commitTimestamp, nil\n}\n\nfunc (cds *crdbDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.CRDBMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for cockroach: %w\", err)\n\t}\n\n\tcurrentRevision, err := migrations.NewCRDBDriver(cds.dburl)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tdefer currentRevision.Close(ctx)\n\n\tversion, err := currentRevision.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tif version != headMigration {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: fmt.Sprintf(\n\t\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\t\tversion,\n\t\t\t\theadMigration,\n\t\t\t),\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\treadMin := cds.readPool.MinConns()\n\tif readMin > 0 {\n\t\treadMin--\n\t}\n\twriteMin := cds.writePool.MinConns()\n\tif writeMin > 0 {\n\t\twriteMin--\n\t}\n\twriteTotal := uint32(cds.writePool.Stat().TotalConns())\n\treadTotal := uint32(cds.readPool.Stat().TotalConns())\n\tif writeTotal < writeMin || readTotal < readMin {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: fmt.Sprintf(\n\t\t\t\t\"spicedb does not have the required minimum connection count to the datastore. Read: %d/%d, Write: %d/%d\",\n\t\t\t\treadTotal,\n\t\t\t\treadMin,\n\t\t\t\twriteTotal,\n\t\t\t\twriteMin,\n\t\t\t),\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\treturn datastore.ReadyState{IsReady: true}, nil\n}\n\nfunc (cds *crdbDatastore) Close() error {\n\tcds.cancel()\n\tcds.readPool.Close()\n\tcds.writePool.Close()\n\treturn nil\n}\n\nfunc (cds *crdbDatastore) HeadRevision(ctx context.Context) (datastore.Revision, error) {\n\treturn cds.headRevisionInternal(ctx)\n}\n\nfunc (cds *crdbDatastore) headRevisionInternal(ctx context.Context) (revision.Decimal, error) {\n\tvar hlcNow revision.Decimal\n\n\tvar fnErr error\n\thlcNow, fnErr = readCRDBNow(ctx, cds.readPool)\n\tif fnErr != nil {\n\t\treturn revision.NoRevision, fmt.Errorf(errRevision, fnErr)\n\t}\n\n\treturn hlcNow, fnErr\n}\n\nfunc (cds *crdbDatastore) Features(ctx context.Context) (*datastore.Features, error) {\n\tfeatures, _, err := cds.featureGroup.Do(ctx, \"\", func(ictx context.Context) (*datastore.Features, error) {\n\t\treturn cds.features(ictx)\n\t})\n\treturn features, err\n}\n\nfunc (cds *crdbDatastore) features(ctx context.Context) (*datastore.Features, error) {\n\tvar features datastore.Features\n\n\thead, err := cds.HeadRevision(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// streams don't return at all if they succeed, so the only way to know\n\t// it was created successfully is to wait a bit and then cancel\n\tstreamCtx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\ttime.AfterFunc(1*time.Second, cancel)\n\n\t_ = cds.writePool.ExecFunc(streamCtx, func(ctx context.Context, tag pgconn.CommandTag, err error) error {\n\t\tif err != nil && errors.Is(err, context.Canceled) {\n\t\t\tfeatures.Watch.Enabled = true\n\t\t\tfeatures.Watch.Reason = \"\"\n\t\t} else if err != nil {\n\t\t\tfeatures.Watch.Enabled = false\n\t\t\tfeatures.Watch.Reason = fmt.Sprintf(\"Range feeds must be enabled in CockroachDB and the user must have permission to create them in order to enable the Watch API: %s\", err.Error())\n\t\t}\n\t\treturn nil\n\t}, fmt.Sprintf(cds.beginChangefeedQuery, tableTuple, head))\n\n\t<-streamCtx.Done()\n\n\treturn &features, nil\n}\n\nfunc readCRDBNow(ctx context.Context, reader pgxcommon.DBFuncQuerier) (revision.Decimal, error) {\n\tctx, span := tracer.Start(ctx, \"readCRDBNow\")\n\tdefer span.End()\n\n\tvar hlcNow decimal.Decimal\n\tif err := reader.QueryRowFunc(ctx, func(ctx context.Context, row pgx.Row) error {\n\t\treturn row.Scan(&hlcNow)\n\t}, querySelectNow); err != nil {\n\t\treturn revision.NoRevision, fmt.Errorf(\"unable to read timestamp: %w\", err)\n\t}\n\n\treturn revision.NewFromDecimal(hlcNow), nil\n}\n\nfunc readClusterTTLNanos(ctx context.Context, conn pgxcommon.DBFuncQuerier) (int64, error) {\n\tvar target, configSQL string\n\n\tif err := conn.QueryRowFunc(ctx, func(ctx context.Context, row pgx.Row) error {\n\t\treturn row.Scan(&target, &configSQL)\n\t}, queryShowZoneConfig); err != nil {\n\t\treturn 0, err\n\t}\n\n\tgroups := gcTTLRegex.FindStringSubmatch(configSQL)\n\tif groups == nil || len(groups) != 2 {\n\t\treturn 0, fmt.Errorf(\"CRDB zone config unexpected format\")\n\t}\n\n\tgcSeconds, err := strconv.ParseInt(groups[1], 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn gcSeconds * 1_000_000_000, nil\n}\n\nfunc revisionFromTimestamp(t time.Time) revision.Decimal {\n\treturn revision.NewFromDecimal(decimal.NewFromInt(t.UnixNano()))\n}\n", "package mysql\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\t\"time\"\n\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/dlmiddlecote/sqlstats\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opentelemetry.io/otel\"\n\t\"go.opentelemetry.io/otel/attribute\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"golang.org/x/sync/errgroup\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/mysql/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n)\n\nconst (\n\tEngine = \"mysql\"\n\n\tcolID               = \"id\"\n\tcolTimestamp        = \"timestamp\"\n\tcolNamespace        = \"namespace\"\n\tcolConfig           = \"serialized_config\"\n\tcolCreatedTxn       = \"created_transaction\"\n\tcolDeletedTxn       = \"deleted_transaction\"\n\tcolObjectID         = \"object_id\"\n\tcolRelation         = \"relation\"\n\tcolUsersetNamespace = \"userset_namespace\"\n\tcolUsersetObjectID  = \"userset_object_id\"\n\tcolUsersetRelation  = \"userset_relation\"\n\tcolName             = \"name\"\n\tcolCaveatDefinition = \"definition\"\n\tcolCaveatName       = \"caveat_name\"\n\tcolCaveatContext    = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore: %w\"\n\tliveDeletedTxnID       = uint64(math.MaxInt64)\n\tbatchDeleteSize        = 1000\n\tnoLastInsertID         = 0\n\tseedingTimeout         = 10 * time.Second\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_lock_wait_timeout\n\terrMysqlLockWaitTimeout = 1205\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_lock_deadlock\n\terrMysqlDeadlock = 1213\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_dup_entry\n\terrMysqlDuplicateEntry = 1062\n)\n\nvar (\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/mysql\")\n\n\t// Unless specified otherwise, Go's MySQL driver will assume\n\t// the server sends datetime in UTC,\n\t// see https://github.com/go-sql-driver/mysql#loc. This parameter\n\t// is unrelated to the session's timezone.\n\t// If the server's global timezone is set to something other than UTC,\n\t// the driver will incorrectly convert SELECT NOW(), because\n\t// the default session timezone is the one specified by the server.\n\tgetNow = sb.Select(\"UTC_TIMESTAMP(6)\")\n\n\tsb = sq.StatementBuilder.PlaceholderFormat(sq.Question)\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\ntype sqlFilter interface {\n\tToSql() (string, []interface{}, error)\n}\n\n// NewMySQLDatastore creates a new mysql.Datastore value configured with the MySQL instance\n// specified in through the URI parameter. Supports customization via the various options available\n// in this package.\n//\n// URI: [scheme://][user[:[password]]@]host[:port][/schema][?attribute1=value1&attribute2=value2...\n// See https://dev.mysql.com/doc/refman/8.0/en/connecting-using-uri-or-key-value-pairs.html\nfunc NewMySQLDatastore(uri string, options ...Option) (datastore.Datastore, error) {\n\tds, err := newMySQLDatastore(uri, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\nfunc newMySQLDatastore(uri string, options ...Option) (*Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tparsedURI, err := mysql.ParseDSN(uri)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: could not parse connection URI `%s`: %w\", uri, err)\n\t}\n\n\tif !parsedURI.ParseTime {\n\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: connection URI for MySQL datastore must include `parseTime=true` as a query parameter. See https://spicedb.dev/d/parse-time-mysql for more details. Found: `%s`\", uri)\n\t}\n\n\tconnector, err := mysql.MySQLDriver{}.OpenConnector(uri)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: failed to create connector: %w\", err)\n\t}\n\n\tif config.lockWaitTimeoutSeconds != nil {\n\t\tlog.Info().Uint8(\"timeout\", *config.lockWaitTimeoutSeconds).Msg(\"overriding innodb_lock_wait_timeout\")\n\t\tconnector, err = addSessionVariables(connector, map[string]string{\n\t\t\t\"innodb_lock_wait_timeout\": strconv.FormatUint(uint64(*config.lockWaitTimeoutSeconds), 10),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: failed to add session variables to connector: %w\", err)\n\t\t}\n\t}\n\n\tvar db *sql.DB\n\tif config.enablePrometheusStats {\n\t\tconnector, err = instrumentConnector(connector)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: unable to instrument connector: %w\", err)\n\t\t}\n\n\t\tdb = sql.OpenDB(connector)\n\t\tcollector := sqlstats.NewStatsCollector(\"spicedb\", db)\n\t\tif err := prometheus.Register(collector); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\n\t\tif err := common.RegisterGCMetrics(); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t} else {\n\t\tdb = sql.OpenDB(connector)\n\t}\n\n\tdb.SetConnMaxLifetime(config.connMaxLifetime)\n\tdb.SetConnMaxIdleTime(config.connMaxIdleTime)\n\tdb.SetMaxOpenConns(config.maxOpenConns)\n\tdb.SetMaxIdleConns(config.maxOpenConns)\n\n\tdriver := migrations.NewMySQLDriverFromDB(db, config.tablePrefix)\n\tqueryBuilder := NewQueryBuilder(driver)\n\n\tcreateTxn, _, err := sb.Insert(driver.RelationTupleTransaction()).Values().ToSql()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: %w\", err)\n\t}\n\n\t// used for seeding the initial relation_tuple_transaction. using INSERT IGNORE on a known\n\t// ID value makes this idempotent (i.e. safe to execute concurrently).\n\tcreateBaseTxn := fmt.Sprintf(\"INSERT IGNORE INTO %s (id, timestamp) VALUES (1, FROM_UNIXTIME(1))\", driver.RelationTupleTransaction())\n\n\tgcCtx, cancelGc := context.WithCancel(context.Background())\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tquantizationPeriodNanos := config.revisionQuantization.Nanoseconds()\n\tif quantizationPeriodNanos < 1 {\n\t\tquantizationPeriodNanos = 1\n\t}\n\n\trevisionQuery := fmt.Sprintf(\n\t\tquerySelectRevision,\n\t\tcolID,\n\t\tdriver.RelationTupleTransaction(),\n\t\tcolTimestamp,\n\t\tquantizationPeriodNanos,\n\t)\n\n\tvalidTransactionQuery := fmt.Sprintf(\n\t\tqueryValidTransaction,\n\t\tcolID,\n\t\tdriver.RelationTupleTransaction(),\n\t\tcolTimestamp,\n\t\t-1*config.gcWindow.Seconds(),\n\t)\n\n\tstore := &Datastore{\n\t\tdb:                     db,\n\t\tdriver:                 driver,\n\t\turl:                    uri,\n\t\trevisionQuantization:   config.revisionQuantization,\n\t\tgcWindow:               config.gcWindow,\n\t\tgcInterval:             config.gcInterval,\n\t\tgcTimeout:              config.gcMaxOperationTime,\n\t\tgcCtx:                  gcCtx,\n\t\tcancelGc:               cancelGc,\n\t\twatchBufferLength:      config.watchBufferLength,\n\t\toptimizedRevisionQuery: revisionQuery,\n\t\tvalidTransactionQuery:  validTransactionQuery,\n\t\tcreateTxn:              createTxn,\n\t\tcreateBaseTxn:          createBaseTxn,\n\t\tQueryBuilder:           queryBuilder,\n\t\treadTxOptions:          &sql.TxOptions{Isolation: sql.LevelSerializable, ReadOnly: true},\n\t\tmaxRetries:             config.maxRetries,\n\t\tanalyzeBeforeStats:     config.analyzeBeforeStats,\n\t\tCachedOptimizedRevisions: revisions.NewCachedOptimizedRevisions(\n\t\t\tmaxRevisionStaleness,\n\t\t),\n\t}\n\n\tstore.SetOptimizedRevisionFunc(store.optimizedRevisionFunc)\n\n\tctx, cancel := context.WithTimeout(context.Background(), seedingTimeout)\n\tdefer cancel()\n\terr = store.seedDatabase(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Start a goroutine for garbage collection.\n\tif store.gcInterval > 0*time.Minute && config.gcEnabled {\n\t\tstore.gcGroup, store.gcCtx = errgroup.WithContext(store.gcCtx)\n\t\tstore.gcGroup.Go(func() error {\n\t\t\treturn common.StartGarbageCollector(\n\t\t\t\tstore.gcCtx,\n\t\t\t\tstore,\n\t\t\t\tstore.gcInterval,\n\t\t\t\tstore.gcWindow,\n\t\t\t\tstore.gcTimeout,\n\t\t\t)\n\t\t})\n\t} else {\n\t\tlog.Warn().Msg(\"datastore background garbage collection disabled\")\n\t}\n\n\treturn store, nil\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc (mds *Datastore) SnapshotReader(revisionRaw datastore.Revision) datastore.Reader {\n\trev := revisionRaw.(revision.Decimal)\n\n\tcreateTxFunc := func(ctx context.Context) (*sql.Tx, txCleanupFunc, error) {\n\t\ttx, err := mds.db.BeginTx(ctx, mds.readTxOptions)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn tx, tx.Rollback, nil\n\t}\n\n\texecutor := common.QueryExecutor{\n\t\tExecutor: newMySQLExecutor(mds.db),\n\t}\n\n\treturn &mysqlReader{\n\t\tmds.QueryBuilder,\n\t\tcreateTxFunc,\n\t\texecutor,\n\t\tbuildLivingObjectFilterForRevision(rev),\n\t}\n}\n\nfunc noCleanup() error { return nil }\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\n// ReadWriteTx starts a read/write transaction, which will be committed if no error is\n// returned and rolled back if an error is returned.\nfunc (mds *Datastore) ReadWriteTx(\n\tctx context.Context,\n\tfn datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tvar err error\n\tfor i := uint8(0); i <= mds.maxRetries; i++ {\n\t\tvar newTxnID uint64\n\t\tif err = migrations.BeginTxFunc(ctx, mds.db, &sql.TxOptions{Isolation: sql.LevelSerializable}, func(tx *sql.Tx) error {\n\t\t\tnewTxnID, err = mds.createNewTransaction(ctx, tx)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to create new txn ID: %w\", err)\n\t\t\t}\n\n\t\t\tlongLivedTx := func(context.Context) (*sql.Tx, txCleanupFunc, error) {\n\t\t\t\treturn tx, noCleanup, nil\n\t\t\t}\n\n\t\t\texecutor := common.QueryExecutor{\n\t\t\t\tExecutor: newMySQLExecutor(tx),\n\t\t\t}\n\n\t\t\trwt := &mysqlReadWriteTXN{\n\t\t\t\t&mysqlReader{\n\t\t\t\t\tmds.QueryBuilder,\n\t\t\t\t\tlongLivedTx,\n\t\t\t\t\texecutor,\n\t\t\t\t\tcurrentlyLivingObjects,\n\t\t\t\t},\n\t\t\t\tmds.driver.RelationTuple(),\n\t\t\t\ttx,\n\t\t\t\tnewTxnID,\n\t\t\t}\n\n\t\t\treturn fn(ctx, rwt)\n\t\t}); err != nil {\n\t\t\tif !config.DisableRetries && isErrorRetryable(err) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\treturn datastore.NoRevision, err\n\t\t}\n\n\t\treturn revisionFromTransaction(newTxnID), nil\n\t}\n\tif !config.DisableRetries {\n\t\terr = fmt.Errorf(\"max retries exceeded: %w\", err)\n\t}\n\treturn datastore.NoRevision, err\n}\n\nfunc isErrorRetryable(err error) bool {\n\tvar mysqlerr *mysql.MySQLError\n\tif !errors.As(err, &mysqlerr) {\n\t\tlog.Debug().Err(err).Msg(\"couldn't determine a sqlstate error code\")\n\t\treturn false\n\t}\n\n\treturn mysqlerr.Number == errMysqlDeadlock || mysqlerr.Number == errMysqlLockWaitTimeout\n}\n\ntype querier interface {\n\tQueryContext(context.Context, string, ...interface{}) (*sql.Rows, error)\n}\n\nfunc newMySQLExecutor(tx querier) common.ExecuteQueryFunc {\n\t// This implementation does not create a transaction because it's redundant for single statements, and it avoids\n\t// the network overhead and reduce contention on the connection pool. From MySQL docs:\n\t//\n\t// https://dev.mysql.com/doc/refman/5.7/en/commit.html\n\t// \"By default, MySQL runs with autocommit mode enabled. This means that, when not otherwise inside a transaction,\n\t// each statement is atomic, as if it were surrounded by START TRANSACTION and COMMIT.\"\n\t//\n\t// https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html\n\t// \"Consistent read is the default mode in which InnoDB processes SELECT statements in READ COMMITTED and\n\t// REPEATABLE READ isolation levels. A consistent read does not set any locks on the tables it accesses,\n\t// and therefore other sessions are free to modify those tables at the same time a consistent read\n\t// is being performed on the table.\"\n\t//\n\t// Prepared statements are also not used given they perform poorly on environments where connections have\n\t// short lifetime (e.g. to gracefully handle load-balancer connection drain)\n\treturn func(ctx context.Context, sqlQuery string, args []interface{}) ([]*core.RelationTuple, error) {\n\t\tspan := trace.SpanFromContext(ctx)\n\n\t\trows, err := tx.QueryContext(ctx, sqlQuery, args...)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t}\n\t\tdefer common.LogOnError(ctx, rows.Close)\n\n\t\tspan.AddEvent(\"Query issued to database\")\n\n\t\tvar tuples []*core.RelationTuple\n\t\tfor rows.Next() {\n\t\t\tnextTuple := &core.RelationTuple{\n\t\t\t\tResourceAndRelation: &core.ObjectAndRelation{},\n\t\t\t\tSubject:             &core.ObjectAndRelation{},\n\t\t\t}\n\n\t\t\tvar caveatName string\n\t\t\tvar caveatContext caveatContextWrapper\n\t\t\terr := rows.Scan(\n\t\t\t\t&nextTuple.ResourceAndRelation.Namespace,\n\t\t\t\t&nextTuple.ResourceAndRelation.ObjectId,\n\t\t\t\t&nextTuple.ResourceAndRelation.Relation,\n\t\t\t\t&nextTuple.Subject.Namespace,\n\t\t\t\t&nextTuple.Subject.ObjectId,\n\t\t\t\t&nextTuple.Subject.Relation,\n\t\t\t\t&caveatName,\n\t\t\t\t&caveatContext,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t\t}\n\n\t\t\tnextTuple.Caveat, err = common.ContextualizedCaveatFrom(caveatName, caveatContext)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t\t}\n\n\t\t\ttuples = append(tuples, nextTuple)\n\t\t}\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t}\n\t\tspan.AddEvent(\"Tuples loaded\", trace.WithAttributes(attribute.Int(\"tupleCount\", len(tuples))))\n\t\treturn tuples, nil\n\t}\n}\n\n// Datastore is a MySQL-based implementation of the datastore.Datastore interface\ntype Datastore struct {\n\tdb                 *sql.DB\n\tdriver             *migrations.MySQLDriver\n\treadTxOptions      *sql.TxOptions\n\turl                string\n\tanalyzeBeforeStats bool\n\n\trevisionQuantization time.Duration\n\tgcWindow             time.Duration\n\tgcInterval           time.Duration\n\tgcTimeout            time.Duration\n\twatchBufferLength    uint16\n\tmaxRetries           uint8\n\n\toptimizedRevisionQuery string\n\tvalidTransactionQuery  string\n\n\tgcGroup  *errgroup.Group\n\tgcCtx    context.Context\n\tcancelGc context.CancelFunc\n\n\tcreateTxn     string\n\tcreateBaseTxn string\n\n\t*QueryBuilder\n\t*revisions.CachedOptimizedRevisions\n\trevision.DecimalDecoder\n}\n\n// Close closes the data store.\nfunc (mds *Datastore) Close() error {\n\t// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\n\tmds.cancelGc()\n\tif mds.gcGroup != nil {\n\t\tif err := mds.gcGroup.Wait(); err != nil && !errors.Is(err, context.Canceled) {\n\t\t\tlog.Error().Err(err).Msg(\"error waiting for garbage collector to shutdown\")\n\t\t}\n\t}\n\treturn mds.db.Close()\n}\n\n// ReadyState returns whether the datastore is ready to accept data. Datastores that require\n// database schema creation will return false until the migrations have been run to create\n// the necessary tables.\n//\n// fundamentally different from PSQL implementation:\n//   - checking if the current migration version is compatible is implemented with IsHeadCompatible\n//   - Database seeding is handled here, so that we can decouple schema migration from data migration\n//     and support skeema-based migrations.\nfunc (mds *Datastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\tif err := mds.db.PingContext(ctx); err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tcurrentMigrationRevision, err := mds.driver.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tcompatible, err := migrations.Manager.IsHeadCompatible(currentMigrationRevision)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tif !compatible {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: \"datastore is not at a currently compatible revision\",\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\tisSeeded, err := mds.isSeeded(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tif !isSeeded {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: \"datastore is not properly seeded\",\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: \"\",\n\t\tIsReady: true,\n\t}, nil\n}\n\nfunc (mds *Datastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: true}}, nil\n}\n\n// isSeeded determines if the backing database has been seeded\nfunc (mds *Datastore) isSeeded(ctx context.Context) (bool, error) {\n\theadRevision, err := mds.HeadRevision(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif headRevision == datastore.NoRevision {\n\t\treturn false, nil\n\t}\n\n\t_, err = mds.getUniqueID(ctx)\n\tif err != nil {\n\t\treturn false, nil\n\t}\n\n\treturn true, nil\n}\n\n// seedDatabase initializes the first transaction revision if necessary.\nfunc (mds *Datastore) seedDatabase(ctx context.Context) error {\n\tctx, span := tracer.Start(ctx, \"seedDatabase\")\n\tdefer span.End()\n\n\tisSeeded, err := mds.isSeeded(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif isSeeded {\n\t\treturn nil\n\t}\n\n\t// this seeds the transaction table with the first transaction, in a way that is idempotent\n\treturn migrations.BeginTxFunc(ctx, mds.db, nil, func(tx *sql.Tx) error {\n\t\t// idempotent INSERT IGNORE transaction id=1. safe to be executed concurrently.\n\t\tresult, err := tx.ExecContext(ctx, mds.createBaseTxn)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: %w\", err)\n\t\t}\n\n\t\tlastInsertID, err := result.LastInsertId()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to get last inserted id: %w\", err)\n\t\t}\n\n\t\tif lastInsertID != noLastInsertID {\n\t\t\t// If there was no error and `lastInsertID` is 0, the insert was ignored. This indicates the transaction\n\t\t\t// was already seeded by another processes (i.e. race condition).\n\t\t\tlog.Ctx(ctx).Info().Int64(\"headRevision\", lastInsertID).Msg(\"seeded base datastore headRevision\")\n\t\t}\n\n\t\tuuidSQL, uuidArgs, err := sb.\n\t\t\tInsert(mds.driver.Metadata()).\n\t\t\tOptions(\"IGNORE\").\n\t\t\tColumns(metadataIDColumn, metadataUniqueIDColumn).\n\t\t\tValues(0, uuid.NewString()).\n\t\t\tToSql()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to prepare SQL: %w\", err)\n\t\t}\n\n\t\tinsertUniqueResult, err := tx.ExecContext(ctx, uuidSQL, uuidArgs...)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to insert unique ID: %w\", err)\n\t\t}\n\n\t\tlastInsertID, err = insertUniqueResult.LastInsertId()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to get last inserted unique id: %w\", err)\n\t\t}\n\n\t\tif lastInsertID != noLastInsertID {\n\t\t\t// If there was no error and `lastInsertID` is 0, the insert was ignored. This indicates the transaction\n\t\t\t// was already seeded by another processes (i.e. race condition).\n\t\t\tlog.Ctx(ctx).Info().Int64(\"headRevision\", lastInsertID).Msg(\"seeded base datastore unique ID\")\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc buildLivingObjectFilterForRevision(revision revision.Decimal) queryFilterer {\n\treturn func(original sq.SelectBuilder) sq.SelectBuilder {\n\t\treturn original.Where(sq.LtOrEq{colCreatedTxn: transactionFromRevision(revision)}).\n\t\t\tWhere(sq.Or{\n\t\t\t\tsq.Eq{colDeletedTxn: liveDeletedTxnID},\n\t\t\t\tsq.Gt{colDeletedTxn: revision},\n\t\t\t})\n\t}\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc currentlyLivingObjects(original sq.SelectBuilder) sq.SelectBuilder {\n\treturn original.Where(sq.Eq{colDeletedTxn: liveDeletedTxnID})\n}\n", "package postgres\n\nimport (\n\t\"context\"\n\tdbsql \"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/IBM/pgxpoolprometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/jackc/pgx/v5/stdlib\"\n\t\"github.com/ngrok/sqlmw\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opentelemetry.io/otel\"\n\t\"golang.org/x/sync/errgroup\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\tpgxcommon \"github.com/authzed/spicedb/internal/datastore/postgres/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/postgres/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nconst (\n\tEngine           = \"postgres\"\n\ttableNamespace   = \"namespace_config\"\n\ttableTransaction = \"relation_tuple_transaction\"\n\ttableTuple       = \"relation_tuple\"\n\ttableCaveat      = \"caveat\"\n\n\tcolXID               = \"xid\"\n\tcolTimestamp         = \"timestamp\"\n\tcolNamespace         = \"namespace\"\n\tcolConfig            = \"serialized_config\"\n\tcolCreatedXid        = \"created_xid\"\n\tcolDeletedXid        = \"deleted_xid\"\n\tcolSnapshot          = \"snapshot\"\n\tcolObjectID          = \"object_id\"\n\tcolRelation          = \"relation\"\n\tcolUsersetNamespace  = \"userset_namespace\"\n\tcolUsersetObjectID   = \"userset_object_id\"\n\tcolUsersetRelation   = \"userset_relation\"\n\tcolCaveatName        = \"name\"\n\tcolCaveatDefinition  = \"definition\"\n\tcolCaveatContextName = \"caveat_name\"\n\tcolCaveatContext     = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore: %w\"\n\n\t// The parameters to this format string are:\n\t// 1: the created_xid or deleted_xid column name\n\t//\n\t// The placeholders are the snapshot and the expected boolean value respectively.\n\tsnapshotAlive = \"pg_visible_in_snapshot(%[1]s, ?) = ?\"\n\n\t// This is the largest positive integer possible in postgresql\n\tliveDeletedTxnID = uint64(9223372036854775807)\n\n\ttracingDriverName = \"postgres-tracing\"\n\n\tgcBatchDeleteSize = 1000\n\n\tlivingTupleConstraint = \"uq_relation_tuple_living_xid\"\n)\n\nfunc init() {\n\tdbsql.Register(tracingDriverName, sqlmw.Driver(stdlib.GetDefaultDriver(), new(traceInterceptor)))\n}\n\nvar (\n\tpsql = sq.StatementBuilder.PlaceholderFormat(sq.Dollar)\n\n\tgetRevision = psql.\n\t\t\tSelect(colXID, colSnapshot).\n\t\t\tFrom(tableTransaction).\n\t\t\tOrderByClause(fmt.Sprintf(\"%s DESC\", colXID)).\n\t\t\tLimit(1)\n\n\tcreateTxn = fmt.Sprintf(\n\t\t\"INSERT INTO %s DEFAULT VALUES RETURNING %s, %s\",\n\t\ttableTransaction,\n\t\tcolXID,\n\t\tcolSnapshot,\n\t)\n\n\tgetNow = psql.Select(\"NOW()\")\n\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/postgres\")\n)\n\ntype sqlFilter interface {\n\tToSql() (string, []interface{}, error)\n}\n\n// NewPostgresDatastore initializes a SpiceDB datastore that uses a PostgreSQL\n// database by leveraging manual book-keeping to implement revisioning.\n//\n// This datastore is also tested to be compatible with CockroachDB.\nfunc NewPostgresDatastore(\n\turl string,\n\toptions ...Option,\n) (datastore.Datastore, error) {\n\tds, err := newPostgresDatastore(url, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\nfunc newPostgresDatastore(\n\tpgURL string,\n\toptions ...Option,\n) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\t// Parse the DB URI into configuration.\n\tparsedConfig, err := pgxpool.ParseConfig(pgURL)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\t// Setup the default custom plan setting, if applicable.\n\tpgConfig, err := defaultCustomPlan(parsedConfig)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\t// Setup the config for each of the read and write pools.\n\treadPoolConfig := pgConfig.Copy()\n\tconfig.readPoolOpts.ConfigurePgx(readPoolConfig)\n\n\treadPoolConfig.AfterConnect = func(ctx context.Context, conn *pgx.Conn) error {\n\t\tRegisterTypes(conn.TypeMap())\n\t\treturn nil\n\t}\n\n\twritePoolConfig := pgConfig.Copy()\n\tconfig.writePoolOpts.ConfigurePgx(writePoolConfig)\n\n\twritePoolConfig.AfterConnect = func(ctx context.Context, conn *pgx.Conn) error {\n\t\tRegisterTypes(conn.TypeMap())\n\t\treturn nil\n\t}\n\n\tif config.migrationPhase != \"\" {\n\t\tlog.Info().\n\t\t\tStr(\"phase\", config.migrationPhase).\n\t\t\tMsg(\"postgres configured to use intermediate migration phase\")\n\t}\n\n\tinitializationContext, cancelInit := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancelInit()\n\n\treadPool, err := pgxpool.NewWithConfig(initializationContext, readPoolConfig)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\twritePool, err := pgxpool.NewWithConfig(initializationContext, writePoolConfig)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\t// Verify that the server supports commit timestamps\n\tvar trackTSOn string\n\tif err := readPool.\n\t\tQueryRow(initializationContext, \"SHOW track_commit_timestamp;\").\n\t\tScan(&trackTSOn); err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\twatchEnabled := trackTSOn == \"on\"\n\tif !watchEnabled {\n\t\tlog.Warn().Msg(\"watch API disabled, postgres must be run with track_commit_timestamp=on\")\n\t}\n\n\tif config.enablePrometheusStats {\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(readPool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"read\",\n\t\t})); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(writePool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"write\",\n\t\t})); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t\tif err := common.RegisterGCMetrics(); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t}\n\n\tgcCtx, cancelGc := context.WithCancel(context.Background())\n\n\tquantizationPeriodNanos := config.revisionQuantization.Nanoseconds()\n\tif quantizationPeriodNanos < 1 {\n\t\tquantizationPeriodNanos = 1\n\t}\n\trevisionQuery := fmt.Sprintf(\n\t\tquerySelectRevision,\n\t\tcolXID,\n\t\ttableTransaction,\n\t\tcolTimestamp,\n\t\tquantizationPeriodNanos,\n\t\tcolSnapshot,\n\t)\n\n\tvalidTransactionQuery := fmt.Sprintf(\n\t\tqueryValidTransaction,\n\t\tcolXID,\n\t\ttableTransaction,\n\t\tcolTimestamp,\n\t\tconfig.gcWindow.Seconds(),\n\t\tcolSnapshot,\n\t)\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tdatastore := &pgDatastore{\n\t\tCachedOptimizedRevisions: revisions.NewCachedOptimizedRevisions(\n\t\t\tmaxRevisionStaleness,\n\t\t),\n\t\tdburl:                   pgURL,\n\t\treadPool:                pgxcommon.MustNewInterceptorPooler(readPool, config.queryInterceptor),\n\t\twritePool:               pgxcommon.MustNewInterceptorPooler(writePool, config.queryInterceptor),\n\t\twatchBufferLength:       config.watchBufferLength,\n\t\toptimizedRevisionQuery:  revisionQuery,\n\t\tvalidTransactionQuery:   validTransactionQuery,\n\t\tgcWindow:                config.gcWindow,\n\t\tgcInterval:              config.gcInterval,\n\t\tgcTimeout:               config.gcMaxOperationTime,\n\t\tanalyzeBeforeStatistics: config.analyzeBeforeStatistics,\n\t\twatchEnabled:            watchEnabled,\n\t\tgcCtx:                   gcCtx,\n\t\tcancelGc:                cancelGc,\n\t\treadTxOptions:           pgx.TxOptions{IsoLevel: pgx.RepeatableRead, AccessMode: pgx.ReadOnly},\n\t\tmaxRetries:              config.maxRetries,\n\t}\n\n\tdatastore.SetOptimizedRevisionFunc(datastore.optimizedRevisionFunc)\n\n\t// Start a goroutine for garbage collection.\n\tif datastore.gcInterval > 0*time.Minute && config.gcEnabled {\n\t\tdatastore.gcGroup, datastore.gcCtx = errgroup.WithContext(datastore.gcCtx)\n\t\tdatastore.gcGroup.Go(func() error {\n\t\t\treturn common.StartGarbageCollector(\n\t\t\t\tdatastore.gcCtx,\n\t\t\t\tdatastore,\n\t\t\t\tdatastore.gcInterval,\n\t\t\t\tdatastore.gcWindow,\n\t\t\t\tdatastore.gcTimeout,\n\t\t\t)\n\t\t})\n\t} else {\n\t\tlog.Warn().Msg(\"datastore background garbage collection disabled\")\n\t}\n\n\treturn datastore, nil\n}\n\ntype pgDatastore struct {\n\t*revisions.CachedOptimizedRevisions\n\n\tdburl                   string\n\treadPool, writePool     pgxcommon.ConnPooler\n\twatchBufferLength       uint16\n\toptimizedRevisionQuery  string\n\tvalidTransactionQuery   string\n\tgcWindow                time.Duration\n\tgcInterval              time.Duration\n\tgcTimeout               time.Duration\n\tanalyzeBeforeStatistics bool\n\treadTxOptions           pgx.TxOptions\n\tmaxRetries              uint8\n\twatchEnabled            bool\n\n\tgcGroup  *errgroup.Group\n\tgcCtx    context.Context\n\tcancelGc context.CancelFunc\n}\n\nfunc (pgd *pgDatastore) SnapshotReader(revRaw datastore.Revision) datastore.Reader {\n\trev := revRaw.(postgresRevision)\n\n\tqueryFuncs := pgxcommon.QuerierFuncsFor(pgd.readPool)\n\texecutor := common.QueryExecutor{\n\t\tExecutor: pgxcommon.NewPGXExecutor(queryFuncs),\n\t}\n\n\treturn &pgReader{\n\t\tqueryFuncs,\n\t\texecutor,\n\t\tbuildLivingObjectFilterForRevision(rev),\n\t}\n}\n\n// ReadWriteTx starts a read/write transaction, which will be committed if no error is\n// returned and rolled back if an error is returned.\nfunc (pgd *pgDatastore) ReadWriteTx(\n\tctx context.Context,\n\tfn datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tvar err error\n\tfor i := uint8(0); i <= pgd.maxRetries; i++ {\n\t\tvar newXID xid8\n\t\tvar newSnapshot pgSnapshot\n\t\terr = wrapError(pgx.BeginTxFunc(ctx, pgd.writePool, pgx.TxOptions{IsoLevel: pgx.Serializable}, func(tx pgx.Tx) error {\n\t\t\tvar err error\n\t\t\tnewXID, newSnapshot, err = createNewTransaction(ctx, tx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tqueryFuncs := pgxcommon.QuerierFuncsFor(pgd.readPool)\n\t\t\texecutor := common.QueryExecutor{\n\t\t\t\tExecutor: pgxcommon.NewPGXExecutor(queryFuncs),\n\t\t\t}\n\n\t\t\trwt := &pgReadWriteTXN{\n\t\t\t\t&pgReader{\n\t\t\t\t\tqueryFuncs,\n\t\t\t\t\texecutor,\n\t\t\t\t\tcurrentlyLivingObjects,\n\t\t\t\t},\n\t\t\t\ttx,\n\t\t\t\tnewXID,\n\t\t\t}\n\n\t\t\treturn fn(ctx, rwt)\n\t\t}))\n\n\t\tif err != nil {\n\t\t\tif !config.DisableRetries && errorRetryable(err) {\n\t\t\t\tpgxcommon.SleepOnErr(ctx, err, i)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\treturn datastore.NoRevision, err\n\t\t}\n\n\t\tif i > 0 {\n\t\t\tlog.Debug().Uint8(\"retries\", i).Msg(\"transaction succeeded after retry\")\n\t\t}\n\n\t\treturn postgresRevision{newSnapshot.markComplete(newXID.Uint64)}, nil\n\t}\n\n\tif !config.DisableRetries {\n\t\terr = fmt.Errorf(\"max retries exceeded: %w\", err)\n\t}\n\n\treturn datastore.NoRevision, err\n}\n\nfunc wrapError(err error) error {\n\tif pgxcommon.IsSerializationError(err) {\n\t\treturn common.NewSerializationError(err)\n\t}\n\n\t// hack: pgx asyncClose usually happens after cancellation,\n\t// but the reason for it being closed is not propagated\n\t// and all we get is attempting to perform an operation\n\t// on cancelled connection. This keeps the same error,\n\t// but wrapped along a cancellation so that:\n\t// - pgx logger does not log it\n\t// - response is sent as canceled back to the client\n\tif err != nil && err.Error() == \"conn closed\" {\n\t\treturn errors.Join(err, context.Canceled)\n\t}\n\n\treturn err\n}\n\nfunc (pgd *pgDatastore) Close() error {\n\tpgd.cancelGc()\n\n\tif pgd.gcGroup != nil {\n\t\terr := pgd.gcGroup.Wait()\n\t\tlog.Warn().Err(err).Msg(\"completed shutdown of postgres datastore\")\n\t}\n\n\tpgd.readPool.Close()\n\tpgd.writePool.Close()\n\treturn nil\n}\n\nfunc errorRetryable(err error) bool {\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\treturn false\n\t}\n\n\tif pgconn.SafeToRetry(err) {\n\t\treturn true\n\t}\n\n\tif pgxcommon.IsSerializationError(err) {\n\t\treturn true\n\t}\n\n\tlog.Warn().Err(err).Msg(\"unable to determine if pgx error is retryable\")\n\treturn false\n}\n\nfunc (pgd *pgDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.DatabaseMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for postgres: %w\", err)\n\t}\n\n\tpgDriver, err := migrations.NewAlembicPostgresDriver(ctx, pgd.dburl)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tdefer pgDriver.Close(ctx)\n\n\tversion, err := pgDriver.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tif version == headMigration {\n\t\treturn datastore.ReadyState{IsReady: true}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: fmt.Sprintf(\n\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\tversion,\n\t\t\theadMigration,\n\t\t),\n\t\tIsReady: false,\n\t}, nil\n}\n\nfunc (pgd *pgDatastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: pgd.watchEnabled}}, nil\n}\n\nfunc buildLivingObjectFilterForRevision(revision postgresRevision) queryFilterer {\n\tcreatedBeforeTXN := sq.Expr(fmt.Sprintf(\n\t\tsnapshotAlive,\n\t\tcolCreatedXid,\n\t), revision.snapshot, true)\n\n\tdeletedAfterTXN := sq.Expr(fmt.Sprintf(\n\t\tsnapshotAlive,\n\t\tcolDeletedXid,\n\t), revision.snapshot, false)\n\n\treturn func(original sq.SelectBuilder) sq.SelectBuilder {\n\t\treturn original.Where(createdBeforeTXN).Where(deletedAfterTXN)\n\t}\n}\n\nfunc currentlyLivingObjects(original sq.SelectBuilder) sq.SelectBuilder {\n\treturn original.Where(sq.Eq{colDeletedXid: liveDeletedTxnID})\n}\n\n// defaultCustomPlan parses a Postgres URI and determines if a plan_cache_mode\n// has been specified. If not, it defaults to \"force_custom_plan\".\n// This works around a bug impacting performance documented here:\n// https://spicedb.dev/d/force-custom-plan.\nfunc defaultCustomPlan(poolConfig *pgxpool.Config) (*pgxpool.Config, error) {\n\tif existing, ok := poolConfig.ConnConfig.Config.RuntimeParams[\"plan_cache_mode\"]; ok {\n\t\tlog.Info().\n\t\t\tStr(\"plan_cache_mode\", existing).\n\t\t\tMsg(\"found plan_cache_mode in DB URI; leaving as-is\")\n\t\treturn poolConfig, nil\n\t}\n\n\tpoolConfig.ConnConfig.Config.RuntimeParams[\"plan_cache_mode\"] = \"force_custom_plan\"\n\tlog.Warn().\n\t\tStr(\"details-url\", \"https://spicedb.dev/d/force-custom-plan\").\n\t\tStr(\"plan_cache_mode\", \"force_custom_plan\").\n\t\tMsg(\"defaulting value in Postgres DB URI\")\n\n\treturn poolConfig, nil\n}\n\nvar _ datastore.Datastore = &pgDatastore{}\n", "package spanner\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"cloud.google.com/go/spanner\"\n\tocprom \"contrib.go.opencensus.io/exporter/prometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opencensus.io/plugin/ocgrpc\"\n\t\"go.opencensus.io/stats/view\"\n\t\"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\"\n\t\"go.opentelemetry.io/otel\"\n\t\"go.opentelemetry.io/otel/attribute\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"google.golang.org/api/option\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/spanner/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nconst (\n\tEngine = \"spanner\"\n\n\terrUnableToInstantiate = \"unable to instantiate spanner client: %w\"\n\n\terrRevision = \"unable to load revision: %w\"\n\n\terrUnableToWriteRelationships    = \"unable to write relationships: %w\"\n\terrUnableToBulkLoadRelationships = \"unable to bulk load relationships: %w\"\n\terrUnableToDeleteRelationships   = \"unable to delete relationships: %w\"\n\n\terrUnableToWriteConfig    = \"unable to write namespace config: %w\"\n\terrUnableToReadConfig     = \"unable to read namespace config: %w\"\n\terrUnableToDeleteConfig   = \"unable to delete namespace config: %w\"\n\terrUnableToListNamespaces = \"unable to list namespaces: %w\"\n\n\terrUnableToReadCaveat   = \"unable to read caveat: %w\"\n\terrUnableToWriteCaveat  = \"unable to write caveat: %w\"\n\terrUnableToListCaveats  = \"unable to list caveats: %w\"\n\terrUnableToDeleteCaveat = \"unable to delete caveat: %w\"\n\n\t// See https://cloud.google.com/spanner/docs/change-streams#data-retention\n\t// See https://github.com/authzed/spicedb/issues/1457\n\tdefaultChangeStreamRetention = 24 * time.Hour\n)\n\nvar (\n\tsql    = sq.StatementBuilder.PlaceholderFormat(sq.AtP)\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/spanner\")\n\n\talreadyExistsRegex = regexp.MustCompile(`^Table relation_tuple: Row {String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\)} already exists.$`)\n)\n\ntype spannerDatastore struct {\n\t*revisions.RemoteClockRevisions\n\trevision.DecimalDecoder\n\n\tclient   *spanner.Client\n\tconfig   spannerOptions\n\tdatabase string\n}\n\n// NewSpannerDatastore returns a datastore backed by cloud spanner\nfunc NewSpannerDatastore(database string, opts ...Option) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(opts)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tif len(config.emulatorHost) > 0 {\n\t\tif err := os.Setenv(\"SPANNER_EMULATOR_HOST\", config.emulatorHost); err != nil {\n\t\t\tlog.Error().Err(err).Msg(\"failed to set SPANNER_EMULATOR_HOST env variable\")\n\t\t}\n\t}\n\tif len(os.Getenv(\"SPANNER_EMULATOR_HOST\")) > 0 {\n\t\tlog.Info().Str(\"spanner-emulator-host\", os.Getenv(\"SPANNER_EMULATOR_HOST\")).Msg(\"running against spanner emulator\")\n\t}\n\n\terr = spanner.EnableStatViews()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner session metrics: %w\", err)\n\t}\n\terr = spanner.EnableGfeLatencyAndHeaderMissingCountViews()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner GFE metrics: %w\", err)\n\t}\n\n\t// Register Spanner client gRPC metrics (include round-trip latency, received/sent bytes...)\n\tif err := view.Register(ocgrpc.DefaultClientViews...); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable gRPC metrics for Spanner client: %w\", err)\n\t}\n\n\t_, err = ocprom.NewExporter(ocprom.Options{\n\t\tNamespace:  \"spicedb\",\n\t\tRegisterer: prometheus.DefaultRegisterer,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner GFE latency stats: %w\", err)\n\t}\n\n\tcfg := spanner.DefaultSessionPoolConfig\n\tcfg.MinOpened = config.minSessions\n\tcfg.MaxOpened = config.maxSessions\n\tclient, err := spanner.NewClientWithConfig(context.Background(), database,\n\t\tspanner.ClientConfig{SessionPoolConfig: cfg},\n\t\toption.WithCredentialsFile(config.credentialsFilePath),\n\t\toption.WithGRPCConnectionPool(max(config.readMaxOpen, config.writeMaxOpen)),\n\t\toption.WithGRPCDialOption(\n\t\t\tgrpc.WithStatsHandler(otelgrpc.NewClientHandler()),\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tds := spannerDatastore{\n\t\tRemoteClockRevisions: revisions.NewRemoteClockRevisions(\n\t\t\tdefaultChangeStreamRetention,\n\t\t\tmaxRevisionStaleness,\n\t\t\tconfig.followerReadDelay,\n\t\t\tconfig.revisionQuantization,\n\t\t),\n\t\tclient:   client,\n\t\tconfig:   config,\n\t\tdatabase: database,\n\t}\n\tds.RemoteClockRevisions.SetNowFunc(ds.headRevisionInternal)\n\n\treturn ds, nil\n}\n\ntype traceableRTX struct {\n\tdelegate readTX\n}\n\nfunc (t *traceableRTX) ReadRow(ctx context.Context, table string, key spanner.Key, columns []string) (*spanner.Row, error) {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.ReadRow\"),\n\t\tattribute.String(\"table\", table),\n\t\tattribute.String(\"key\", key.String()),\n\t\tattribute.StringSlice(\"columns\", columns))\n\n\treturn t.delegate.ReadRow(ctx, table, key, columns)\n}\n\nfunc (t *traceableRTX) Read(ctx context.Context, table string, keys spanner.KeySet, columns []string) *spanner.RowIterator {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.Read\"),\n\t\tattribute.String(\"table\", table),\n\t\tattribute.StringSlice(\"columns\", columns))\n\n\treturn t.delegate.Read(ctx, table, keys, columns)\n}\n\nfunc (t *traceableRTX) Query(ctx context.Context, statement spanner.Statement) *spanner.RowIterator {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.Query\"),\n\t\tattribute.String(\"statement\", statement.SQL))\n\n\treturn t.delegate.Query(ctx, statement)\n}\n\nfunc (sd spannerDatastore) SnapshotReader(revisionRaw datastore.Revision) datastore.Reader {\n\tr := revisionRaw.(revision.Decimal)\n\n\ttxSource := func() readTX {\n\t\treturn &traceableRTX{delegate: sd.client.Single().WithTimestampBound(spanner.ReadTimestamp(timestampFromRevision(r)))}\n\t}\n\texecutor := common.QueryExecutor{Executor: queryExecutor(txSource)}\n\treturn spannerReader{executor, txSource}\n}\n\nfunc (sd spannerDatastore) ReadWriteTx(ctx context.Context, fn datastore.TxUserFunc, opts ...options.RWTOptionsOption) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tctx, span := tracer.Start(ctx, \"ReadWriteTx\")\n\tdefer span.End()\n\n\tctx, cancel := context.WithCancel(ctx)\n\tts, err := sd.client.ReadWriteTransaction(ctx, func(ctx context.Context, spannerRWT *spanner.ReadWriteTransaction) error {\n\t\ttxSource := func() readTX {\n\t\t\treturn &traceableRTX{delegate: spannerRWT}\n\t\t}\n\n\t\texecutor := common.QueryExecutor{Executor: queryExecutor(txSource)}\n\t\trwt := spannerReadWriteTXN{\n\t\t\tspannerReader{executor, txSource},\n\t\t\tspannerRWT,\n\t\t\tsd.config.disableStats,\n\t\t}\n\t\terr := func() error {\n\t\t\tinnerCtx, innerSpan := tracer.Start(ctx, \"TxUserFunc\")\n\t\t\tdefer innerSpan.End()\n\n\t\t\treturn fn(innerCtx, rwt)\n\t\t}()\n\t\tif err != nil {\n\t\t\tif config.DisableRetries {\n\t\t\t\tdefer cancel()\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\tif cerr := convertToWriteConstraintError(err); cerr != nil {\n\t\t\treturn datastore.NoRevision, cerr\n\t\t}\n\t\treturn datastore.NoRevision, err\n\t}\n\n\treturn revisionFromTimestamp(ts), nil\n}\n\nfunc (sd spannerDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.SpannerMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for spanner: %w\", err)\n\t}\n\n\tchecker := migrations.NewSpannerVersionChecker(sd.client)\n\tversion, err := checker.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\t// TODO(jschorr): Remove register-tuple-change-stream once the multi-phase is done.\n\tif version == headMigration || version == \"register-tuple-change-stream\" {\n\t\treturn datastore.ReadyState{IsReady: true}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: fmt.Sprintf(\n\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\tversion,\n\t\t\theadMigration,\n\t\t),\n\t\tIsReady: false,\n\t}, nil\n}\n\nfunc (sd spannerDatastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: true}}, nil\n}\n\nfunc (sd spannerDatastore) Close() error {\n\tsd.client.Close()\n\treturn nil\n}\n\nfunc statementFromSQL(sql string, args []any) spanner.Statement {\n\tparams := make(map[string]any, len(args))\n\tfor index, arg := range args {\n\t\tparams[\"p\"+strconv.Itoa(index+1)] = arg\n\t}\n\n\treturn spanner.Statement{\n\t\tSQL:    sql,\n\t\tParams: params,\n\t}\n}\n\nfunc convertToWriteConstraintError(err error) error {\n\tif spanner.ErrCode(err) == codes.AlreadyExists {\n\t\tdescription := spanner.ErrDesc(err)\n\t\tfound := alreadyExistsRegex.FindStringSubmatch(description)\n\t\tif found != nil {\n\t\t\treturn common.NewCreateRelationshipExistsError(&core.RelationTuple{\n\t\t\t\tResourceAndRelation: &core.ObjectAndRelation{\n\t\t\t\t\tNamespace: found[1],\n\t\t\t\t\tObjectId:  found[2],\n\t\t\t\t\tRelation:  found[3],\n\t\t\t\t},\n\t\t\t\tSubject: &core.ObjectAndRelation{\n\t\t\t\t\tNamespace: found[4],\n\t\t\t\t\tObjectId:  found[5],\n\t\t\t\t\tRelation:  found[6],\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\n\t\treturn common.NewCreateRelationshipExistsError(nil)\n\t}\n\treturn nil\n}\n", "package datastore\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/authzed/spicedb/internal/logging\"\n)\n\nfunc TestError(_ *testing.T) {\n\tlogging.Info().Err(ErrNamespaceNotFound{\n\t\terror:         fmt.Errorf(\"test\"),\n\t\tnamespaceName: \"test/test\",\n\t},\n\t).Msg(\"test\")\n}\n"], "fixing_code": ["package common\n\nimport (\n\t\"fmt\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/status\"\n\n\tv1 \"github.com/authzed/authzed-go/proto/authzed/api/v1\"\n\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n\t\"github.com/authzed/spicedb/pkg/spiceerrors\"\n\t\"github.com/authzed/spicedb/pkg/tuple\"\n)\n\n// SerializationError is returned when there's been a serialization\n// error while performing a datastore operation\ntype SerializationError struct {\n\terror\n}\n\nfunc (err SerializationError) GRPCStatus() *status.Status {\n\treturn spiceerrors.WithCodeAndDetails(\n\t\terr,\n\t\tcodes.Aborted,\n\t\tspiceerrors.ForReason(\n\t\t\tv1.ErrorReason_ERROR_REASON_SERIALIZATION_FAILURE,\n\t\t\tmap[string]string{},\n\t\t),\n\t)\n}\n\nfunc (err SerializationError) Unwrap() error {\n\treturn err.error\n}\n\n// NewSerializationError creates a new SerializationError\nfunc NewSerializationError(err error) error {\n\treturn SerializationError{err}\n}\n\n// CreateRelationshipExistsError is an error returned when attempting to CREATE an already-existing\n// relationship.\ntype CreateRelationshipExistsError struct {\n\terror\n\n\t// Relationship is the relationship that caused the error. May be nil, depending on the datastore.\n\tRelationship *core.RelationTuple\n}\n\n// GRPCStatus implements retrieving the gRPC status for the error.\nfunc (err CreateRelationshipExistsError) GRPCStatus() *status.Status {\n\tif err.Relationship == nil {\n\t\treturn spiceerrors.WithCodeAndDetails(\n\t\t\terr,\n\t\t\tcodes.AlreadyExists,\n\t\t\tspiceerrors.ForReason(\n\t\t\t\tv1.ErrorReason_ERROR_REASON_ATTEMPT_TO_RECREATE_RELATIONSHIP,\n\t\t\t\tmap[string]string{},\n\t\t\t),\n\t\t)\n\t}\n\n\trelationship := tuple.ToRelationship(err.Relationship)\n\treturn spiceerrors.WithCodeAndDetails(\n\t\terr,\n\t\tcodes.AlreadyExists,\n\t\tspiceerrors.ForReason(\n\t\t\tv1.ErrorReason_ERROR_REASON_ATTEMPT_TO_RECREATE_RELATIONSHIP,\n\t\t\tmap[string]string{\n\t\t\t\t\"relationship\":       tuple.StringRelationshipWithoutCaveat(relationship),\n\t\t\t\t\"resource_type\":      relationship.Resource.ObjectType,\n\t\t\t\t\"resource_object_id\": relationship.Resource.ObjectId,\n\t\t\t\t\"resource_relation\":  relationship.Relation,\n\t\t\t\t\"subject_type\":       relationship.Subject.Object.ObjectType,\n\t\t\t\t\"subject_object_id\":  relationship.Subject.Object.ObjectId,\n\t\t\t\t\"subject_relation\":   relationship.Subject.OptionalRelation,\n\t\t\t},\n\t\t),\n\t)\n}\n\n// NewCreateRelationshipExistsError creates a new CreateRelationshipExistsError.\nfunc NewCreateRelationshipExistsError(relationship *core.RelationTuple) error {\n\tmsg := \"could not CREATE one or more relationships, as they already existed. If this is persistent, please switch to TOUCH operations or specify a precondition\"\n\tif relationship != nil {\n\t\tmsg = fmt.Sprintf(\"could not CREATE relationship `%s`, as it already existed. If this is persistent, please switch to TOUCH operations or specify a precondition\", tuple.StringWithoutCaveat(relationship))\n\t}\n\n\treturn CreateRelationshipExistsError{\n\t\tfmt.Errorf(msg),\n\t\trelationship,\n\t}\n}\n\nvar (\n\tportMatchRegex  = regexp.MustCompile(\"invalid port \\\\\\\"(.+)\\\\\\\" after host\")\n\tparseMatchRegex = regexp.MustCompile(\"parse \\\\\\\"(.+)\\\\\\\":\")\n)\n\n// RedactAndLogSensitiveConnString elides the given error, logging it only at trace\n// level (after being redacted).\nfunc RedactAndLogSensitiveConnString(baseErr string, err error, pgURL string) error {\n\t// See: https://github.com/jackc/pgx/issues/1271\n\tfiltered := err.Error()\n\tfiltered = strings.ReplaceAll(filtered, pgURL, \"(redacted)\")\n\tfiltered = portMatchRegex.ReplaceAllString(filtered, \"(redacted)\")\n\tfiltered = parseMatchRegex.ReplaceAllString(filtered, \"(redacted)\")\n\tlog.Trace().Msg(baseErr + \": \" + filtered)\n\treturn fmt.Errorf(\"%s. To view details of this error (that may contain sensitive information), please run with --log-level=trace\", baseErr)\n}\n", "package crdb\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/IBM/pgxpoolprometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/shopspring/decimal\"\n\t\"go.opentelemetry.io/otel\"\n\t\"golang.org/x/sync/errgroup\"\n\t\"resenje.org/singleflight\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/crdb/migrations\"\n\t\"github.com/authzed/spicedb/internal/datastore/crdb/pool\"\n\tpgxcommon \"github.com/authzed/spicedb/internal/datastore/postgres/common\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nvar (\n\tpsql = sq.StatementBuilder.PlaceholderFormat(sq.Dollar)\n\n\tgcTTLRegex = regexp.MustCompile(`gc\\.ttlseconds\\s*=\\s*([1-9][0-9]+)`)\n\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/crdb\")\n)\n\nconst (\n\tEngine            = \"cockroachdb\"\n\ttableNamespace    = \"namespace_config\"\n\ttableTuple        = \"relation_tuple\"\n\ttableTransactions = \"transactions\"\n\ttableCaveat       = \"caveat\"\n\n\tcolNamespace         = \"namespace\"\n\tcolConfig            = \"serialized_config\"\n\tcolTimestamp         = \"timestamp\"\n\tcolTransactionKey    = \"key\"\n\tcolObjectID          = \"object_id\"\n\tcolRelation          = \"relation\"\n\tcolUsersetNamespace  = \"userset_namespace\"\n\tcolUsersetObjectID   = \"userset_object_id\"\n\tcolUsersetRelation   = \"userset_relation\"\n\tcolCaveatName        = \"name\"\n\tcolCaveatDefinition  = \"definition\"\n\tcolCaveatContextName = \"caveat_name\"\n\tcolCaveatContext     = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore\"\n\terrRevision            = \"unable to find revision: %w\"\n\n\tquerySelectNow      = \"SELECT cluster_logical_timestamp()\"\n\tqueryShowZoneConfig = \"SHOW ZONE CONFIGURATION FOR RANGE default;\"\n\n\tlivingTupleConstraint = \"pk_relation_tuple\"\n)\n\nfunc newCRDBDatastore(url string, options ...Option) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\n\treadPoolConfig, err := pgxpool.ParseConfig(url)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\tconfig.readPoolOpts.ConfigurePgx(readPoolConfig)\n\n\twritePoolConfig, err := pgxpool.ParseConfig(url)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\tconfig.writePoolOpts.ConfigurePgx(writePoolConfig)\n\n\tinitCtx, initCancel := context.WithTimeout(context.Background(), 5*time.Minute)\n\tdefer initCancel()\n\n\thealthChecker, err := pool.NewNodeHealthChecker(url)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\n\t// The initPool is a 1-connection pool that is only used for setup tasks.\n\t// The actual pools are not given the initCtx, since cancellation can\n\t// interfere with pool setup.\n\tinitPoolConfig := readPoolConfig.Copy()\n\tinitPoolConfig.MinConns = 1\n\tinitPool, err := pool.NewRetryPool(initCtx, \"init\", initPoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\tdefer initPool.Close()\n\n\tvar version crdbVersion\n\tif err := queryServerVersion(initCtx, initPool, &version); err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\n\tchangefeedQuery := queryChangefeed\n\tif version.Major < 22 {\n\t\tlog.Info().Object(\"version\", version).Msg(\"using changefeed query for CRDB version < 22\")\n\t\tchangefeedQuery = queryChangefeedPreV22\n\t}\n\n\tclusterTTLNanos, err := readClusterTTLNanos(initCtx, initPool)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"unable to read cluster gc window: %w\", err)\n\t}\n\n\tgcWindowNanos := config.gcWindow.Nanoseconds()\n\tif clusterTTLNanos < gcWindowNanos {\n\t\tlog.Warn().\n\t\t\tInt64(\"cockroach_cluster_gc_window_nanos\", clusterTTLNanos).\n\t\t\tInt64(\"spicedb_gc_window_nanos\", gcWindowNanos).\n\t\t\tMsg(\"configured CockroachDB cluster gc window is less than configured SpiceDB gc window, falling back to CRDB value - see https://spicedb.dev/d/crdb-gc-window-warning\")\n\t\tconfig.gcWindow = time.Duration(clusterTTLNanos) * time.Nanosecond\n\t}\n\n\tkeySetInit := newKeySet\n\tvar keyer overlapKeyer\n\tswitch config.overlapStrategy {\n\tcase overlapStrategyStatic:\n\t\tif len(config.overlapKey) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"static tx overlap strategy specified without an overlap key\")\n\t\t}\n\t\tkeyer = appendStaticKey(config.overlapKey)\n\tcase overlapStrategyPrefix:\n\t\tkeyer = prefixKeyer\n\tcase overlapStrategyRequest:\n\t\t// overlap keys are computed over requests and not data\n\t\tkeyer = noOverlapKeyer\n\t\tkeySetInit = overlapKeysFromContext\n\tcase overlapStrategyInsecure:\n\t\tlog.Warn().Str(\"strategy\", overlapStrategyInsecure).\n\t\t\tMsg(\"running in this mode is only safe when replicas == nodes\")\n\t\tkeyer = noOverlapKeyer\n\t}\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tds := &crdbDatastore{\n\t\tRemoteClockRevisions: revisions.NewRemoteClockRevisions(\n\t\t\tconfig.gcWindow,\n\t\t\tmaxRevisionStaleness,\n\t\t\tconfig.followerReadDelay,\n\t\t\tconfig.revisionQuantization,\n\t\t),\n\t\tDecimalDecoder:       revision.DecimalDecoder{},\n\t\tdburl:                url,\n\t\twatchBufferLength:    config.watchBufferLength,\n\t\twriteOverlapKeyer:    keyer,\n\t\toverlapKeyInit:       keySetInit,\n\t\tdisableStats:         config.disableStats,\n\t\tbeginChangefeedQuery: changefeedQuery,\n\t}\n\tds.RemoteClockRevisions.SetNowFunc(ds.headRevisionInternal)\n\n\t// this ctx and cancel is tied to the lifetime of the datastore\n\tds.ctx, ds.cancel = context.WithCancel(context.Background())\n\tds.writePool, err = pool.NewRetryPool(ds.ctx, \"write\", writePoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\tds.cancel()\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\tds.readPool, err = pool.NewRetryPool(ds.ctx, \"read\", readPoolConfig, healthChecker, config.maxRetries, config.connectRate)\n\tif err != nil {\n\t\tds.cancel()\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, url)\n\t}\n\n\tif config.enablePrometheusStats {\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(ds.writePool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"write\",\n\t\t})); err != nil {\n\t\t\tds.cancel()\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(ds.readPool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"read\",\n\t\t})); err != nil {\n\t\t\tds.cancel()\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\t// TODO: this (and the GC startup that it's based on for mysql/pg) should\n\t// be removed and have the lifetimes tied to server start/stop.\n\n\t// Start goroutines for pruning\n\tif config.enableConnectionBalancing {\n\t\tlog.Ctx(initCtx).Info().Msg(\"starting cockroach connection balancer\")\n\t\tds.pruneGroup, ds.ctx = errgroup.WithContext(ds.ctx)\n\t\twritePoolBalancer := pool.NewNodeConnectionBalancer(ds.writePool, healthChecker, 5*time.Second)\n\t\treadPoolBalancer := pool.NewNodeConnectionBalancer(ds.readPool, healthChecker, 5*time.Second)\n\t\tds.pruneGroup.Go(func() error {\n\t\t\twritePoolBalancer.Prune(ds.ctx)\n\t\t\treturn nil\n\t\t})\n\t\tds.pruneGroup.Go(func() error {\n\t\t\treadPoolBalancer.Prune(ds.ctx)\n\t\t\treturn nil\n\t\t})\n\t\tds.pruneGroup.Go(func() error {\n\t\t\thealthChecker.Poll(ds.ctx, 5*time.Second)\n\t\t\treturn nil\n\t\t})\n\t}\n\n\treturn ds, nil\n}\n\n// NewCRDBDatastore initializes a SpiceDB datastore that uses a CockroachDB\n// database while leveraging its AOST functionality.\nfunc NewCRDBDatastore(url string, options ...Option) (datastore.Datastore, error) {\n\tds, err := newCRDBDatastore(url, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\ntype crdbDatastore struct {\n\t*revisions.RemoteClockRevisions\n\trevision.DecimalDecoder\n\n\tdburl               string\n\treadPool, writePool *pool.RetryPool\n\twatchBufferLength   uint16\n\twriteOverlapKeyer   overlapKeyer\n\toverlapKeyInit      func(ctx context.Context) keySet\n\tdisableStats        bool\n\n\tbeginChangefeedQuery string\n\n\tfeatureGroup singleflight.Group[string, *datastore.Features]\n\n\tpruneGroup *errgroup.Group\n\tctx        context.Context\n\tcancel     context.CancelFunc\n}\n\nfunc (cds *crdbDatastore) SnapshotReader(rev datastore.Revision) datastore.Reader {\n\texecutor := common.QueryExecutor{\n\t\tExecutor: pgxcommon.NewPGXExecutor(cds.readPool),\n\t}\n\n\tfromBuilder := func(query sq.SelectBuilder, fromStr string) sq.SelectBuilder {\n\t\treturn query.From(fromStr + \" AS OF SYSTEM TIME \" + rev.String())\n\t}\n\n\treturn &crdbReader{cds.readPool, executor, noOverlapKeyer, nil, fromBuilder}\n}\n\nfunc (cds *crdbDatastore) ReadWriteTx(\n\tctx context.Context,\n\tf datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tvar commitTimestamp revision.Decimal\n\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\tif config.DisableRetries {\n\t\tctx = context.WithValue(ctx, pool.CtxDisableRetries, true)\n\t}\n\n\terr := cds.writePool.BeginFunc(ctx, func(tx pgx.Tx) error {\n\t\tquerier := pgxcommon.QuerierFuncsFor(tx)\n\t\texecutor := common.QueryExecutor{\n\t\t\tExecutor: pgxcommon.NewPGXExecutor(querier),\n\t\t}\n\n\t\trwt := &crdbReadWriteTXN{\n\t\t\t&crdbReader{\n\t\t\t\tquerier,\n\t\t\t\texecutor,\n\t\t\t\tcds.writeOverlapKeyer,\n\t\t\t\tcds.overlapKeyInit(ctx),\n\t\t\t\tfunc(query sq.SelectBuilder, fromStr string) sq.SelectBuilder {\n\t\t\t\t\treturn query.From(fromStr)\n\t\t\t\t},\n\t\t\t},\n\t\t\ttx,\n\t\t\t0,\n\t\t}\n\n\t\tif err := f(ctx, rwt); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Touching the transaction key happens last so that the \"write intent\" for\n\t\t// the transaction as a whole lands in a range for the affected tuples.\n\t\tfor k := range rwt.overlapKeySet {\n\t\t\tif _, err := tx.Exec(ctx, queryTouchTransaction, k); err != nil {\n\t\t\t\treturn fmt.Errorf(\"error writing overlapping keys: %w\", err)\n\t\t\t}\n\t\t}\n\n\t\tif cds.disableStats {\n\t\t\tvar err error\n\t\t\tcommitTimestamp, err = readCRDBNow(ctx, querier)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"error getting commit timestamp: %w\", err)\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\n\t\tvar err error\n\t\tcommitTimestamp, err = updateCounter(ctx, tx, rwt.relCountChange)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error updating relationship counter: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn datastore.NoRevision, err\n\t}\n\n\treturn commitTimestamp, nil\n}\n\nfunc (cds *crdbDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.CRDBMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for cockroach: %w\", err)\n\t}\n\n\tcurrentRevision, err := migrations.NewCRDBDriver(cds.dburl)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tdefer currentRevision.Close(ctx)\n\n\tversion, err := currentRevision.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tif version != headMigration {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: fmt.Sprintf(\n\t\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\t\tversion,\n\t\t\t\theadMigration,\n\t\t\t),\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\treadMin := cds.readPool.MinConns()\n\tif readMin > 0 {\n\t\treadMin--\n\t}\n\twriteMin := cds.writePool.MinConns()\n\tif writeMin > 0 {\n\t\twriteMin--\n\t}\n\twriteTotal := uint32(cds.writePool.Stat().TotalConns())\n\treadTotal := uint32(cds.readPool.Stat().TotalConns())\n\tif writeTotal < writeMin || readTotal < readMin {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: fmt.Sprintf(\n\t\t\t\t\"spicedb does not have the required minimum connection count to the datastore. Read: %d/%d, Write: %d/%d\",\n\t\t\t\treadTotal,\n\t\t\t\treadMin,\n\t\t\t\twriteTotal,\n\t\t\t\twriteMin,\n\t\t\t),\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\treturn datastore.ReadyState{IsReady: true}, nil\n}\n\nfunc (cds *crdbDatastore) Close() error {\n\tcds.cancel()\n\tcds.readPool.Close()\n\tcds.writePool.Close()\n\treturn nil\n}\n\nfunc (cds *crdbDatastore) HeadRevision(ctx context.Context) (datastore.Revision, error) {\n\treturn cds.headRevisionInternal(ctx)\n}\n\nfunc (cds *crdbDatastore) headRevisionInternal(ctx context.Context) (revision.Decimal, error) {\n\tvar hlcNow revision.Decimal\n\n\tvar fnErr error\n\thlcNow, fnErr = readCRDBNow(ctx, cds.readPool)\n\tif fnErr != nil {\n\t\treturn revision.NoRevision, fmt.Errorf(errRevision, fnErr)\n\t}\n\n\treturn hlcNow, fnErr\n}\n\nfunc (cds *crdbDatastore) Features(ctx context.Context) (*datastore.Features, error) {\n\tfeatures, _, err := cds.featureGroup.Do(ctx, \"\", func(ictx context.Context) (*datastore.Features, error) {\n\t\treturn cds.features(ictx)\n\t})\n\treturn features, err\n}\n\nfunc (cds *crdbDatastore) features(ctx context.Context) (*datastore.Features, error) {\n\tvar features datastore.Features\n\n\thead, err := cds.HeadRevision(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// streams don't return at all if they succeed, so the only way to know\n\t// it was created successfully is to wait a bit and then cancel\n\tstreamCtx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\ttime.AfterFunc(1*time.Second, cancel)\n\n\t_ = cds.writePool.ExecFunc(streamCtx, func(ctx context.Context, tag pgconn.CommandTag, err error) error {\n\t\tif err != nil && errors.Is(err, context.Canceled) {\n\t\t\tfeatures.Watch.Enabled = true\n\t\t\tfeatures.Watch.Reason = \"\"\n\t\t} else if err != nil {\n\t\t\tfeatures.Watch.Enabled = false\n\t\t\tfeatures.Watch.Reason = fmt.Sprintf(\"Range feeds must be enabled in CockroachDB and the user must have permission to create them in order to enable the Watch API: %s\", err.Error())\n\t\t}\n\t\treturn nil\n\t}, fmt.Sprintf(cds.beginChangefeedQuery, tableTuple, head))\n\n\t<-streamCtx.Done()\n\n\treturn &features, nil\n}\n\nfunc readCRDBNow(ctx context.Context, reader pgxcommon.DBFuncQuerier) (revision.Decimal, error) {\n\tctx, span := tracer.Start(ctx, \"readCRDBNow\")\n\tdefer span.End()\n\n\tvar hlcNow decimal.Decimal\n\tif err := reader.QueryRowFunc(ctx, func(ctx context.Context, row pgx.Row) error {\n\t\treturn row.Scan(&hlcNow)\n\t}, querySelectNow); err != nil {\n\t\treturn revision.NoRevision, fmt.Errorf(\"unable to read timestamp: %w\", err)\n\t}\n\n\treturn revision.NewFromDecimal(hlcNow), nil\n}\n\nfunc readClusterTTLNanos(ctx context.Context, conn pgxcommon.DBFuncQuerier) (int64, error) {\n\tvar target, configSQL string\n\n\tif err := conn.QueryRowFunc(ctx, func(ctx context.Context, row pgx.Row) error {\n\t\treturn row.Scan(&target, &configSQL)\n\t}, queryShowZoneConfig); err != nil {\n\t\treturn 0, err\n\t}\n\n\tgroups := gcTTLRegex.FindStringSubmatch(configSQL)\n\tif groups == nil || len(groups) != 2 {\n\t\treturn 0, fmt.Errorf(\"CRDB zone config unexpected format\")\n\t}\n\n\tgcSeconds, err := strconv.ParseInt(groups[1], 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn gcSeconds * 1_000_000_000, nil\n}\n\nfunc revisionFromTimestamp(t time.Time) revision.Decimal {\n\treturn revision.NewFromDecimal(decimal.NewFromInt(t.UnixNano()))\n}\n", "package mysql\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"strconv\"\n\t\"time\"\n\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/dlmiddlecote/sqlstats\"\n\t\"github.com/go-sql-driver/mysql\"\n\t\"github.com/google/uuid\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opentelemetry.io/otel\"\n\t\"go.opentelemetry.io/otel/attribute\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"golang.org/x/sync/errgroup\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/mysql/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n)\n\nconst (\n\tEngine = \"mysql\"\n\n\tcolID               = \"id\"\n\tcolTimestamp        = \"timestamp\"\n\tcolNamespace        = \"namespace\"\n\tcolConfig           = \"serialized_config\"\n\tcolCreatedTxn       = \"created_transaction\"\n\tcolDeletedTxn       = \"deleted_transaction\"\n\tcolObjectID         = \"object_id\"\n\tcolRelation         = \"relation\"\n\tcolUsersetNamespace = \"userset_namespace\"\n\tcolUsersetObjectID  = \"userset_object_id\"\n\tcolUsersetRelation  = \"userset_relation\"\n\tcolName             = \"name\"\n\tcolCaveatDefinition = \"definition\"\n\tcolCaveatName       = \"caveat_name\"\n\tcolCaveatContext    = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore: %w\"\n\tliveDeletedTxnID       = uint64(math.MaxInt64)\n\tbatchDeleteSize        = 1000\n\tnoLastInsertID         = 0\n\tseedingTimeout         = 10 * time.Second\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_lock_wait_timeout\n\terrMysqlLockWaitTimeout = 1205\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_lock_deadlock\n\terrMysqlDeadlock = 1213\n\n\t// https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_dup_entry\n\terrMysqlDuplicateEntry = 1062\n)\n\nvar (\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/mysql\")\n\n\t// Unless specified otherwise, Go's MySQL driver will assume\n\t// the server sends datetime in UTC,\n\t// see https://github.com/go-sql-driver/mysql#loc. This parameter\n\t// is unrelated to the session's timezone.\n\t// If the server's global timezone is set to something other than UTC,\n\t// the driver will incorrectly convert SELECT NOW(), because\n\t// the default session timezone is the one specified by the server.\n\tgetNow = sb.Select(\"UTC_TIMESTAMP(6)\")\n\n\tsb = sq.StatementBuilder.PlaceholderFormat(sq.Question)\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\ntype sqlFilter interface {\n\tToSql() (string, []interface{}, error)\n}\n\n// NewMySQLDatastore creates a new mysql.Datastore value configured with the MySQL instance\n// specified in through the URI parameter. Supports customization via the various options available\n// in this package.\n//\n// URI: [scheme://][user[:[password]]@]host[:port][/schema][?attribute1=value1&attribute2=value2...\n// See https://dev.mysql.com/doc/refman/8.0/en/connecting-using-uri-or-key-value-pairs.html\nfunc NewMySQLDatastore(uri string, options ...Option) (datastore.Datastore, error) {\n\tds, err := newMySQLDatastore(uri, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\nfunc newMySQLDatastore(uri string, options ...Option) (*Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t}\n\n\tparsedURI, err := mysql.ParseDSN(uri)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(\"NewMySQLDatastore: could not parse connection URI\", err, uri)\n\t}\n\n\tif !parsedURI.ParseTime {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(\"NewMySQLDatastore: connection URI for MySQL datastore must include `parseTime=true` as a query parameter. See https://spicedb.dev/d/parse-time-mysql for more details.\", err, uri)\n\t}\n\n\tconnector, err := mysql.MySQLDriver{}.OpenConnector(uri)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(\"NewMySQLDatastore: failed to create connector\", err, uri)\n\t}\n\n\tif config.lockWaitTimeoutSeconds != nil {\n\t\tlog.Info().Uint8(\"timeout\", *config.lockWaitTimeoutSeconds).Msg(\"overriding innodb_lock_wait_timeout\")\n\t\tconnector, err = addSessionVariables(connector, map[string]string{\n\t\t\t\"innodb_lock_wait_timeout\": strconv.FormatUint(uint64(*config.lockWaitTimeoutSeconds), 10),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, common.RedactAndLogSensitiveConnString(\"NewMySQLDatastore: failed to add session variables to connector\", err, uri)\n\t\t}\n\t}\n\n\tvar db *sql.DB\n\tif config.enablePrometheusStats {\n\t\tconnector, err = instrumentConnector(connector)\n\t\tif err != nil {\n\t\t\treturn nil, common.RedactAndLogSensitiveConnString(\"NewMySQLDatastore: unable to instrument connector\", err, uri)\n\t\t}\n\n\t\tdb = sql.OpenDB(connector)\n\t\tcollector := sqlstats.NewStatsCollector(\"spicedb\", db)\n\t\tif err := prometheus.Register(collector); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\n\t\tif err := common.RegisterGCMetrics(); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToInstantiate, err)\n\t\t}\n\t} else {\n\t\tdb = sql.OpenDB(connector)\n\t}\n\n\tdb.SetConnMaxLifetime(config.connMaxLifetime)\n\tdb.SetConnMaxIdleTime(config.connMaxIdleTime)\n\tdb.SetMaxOpenConns(config.maxOpenConns)\n\tdb.SetMaxIdleConns(config.maxOpenConns)\n\n\tdriver := migrations.NewMySQLDriverFromDB(db, config.tablePrefix)\n\tqueryBuilder := NewQueryBuilder(driver)\n\n\tcreateTxn, _, err := sb.Insert(driver.RelationTupleTransaction()).Values().ToSql()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"NewMySQLDatastore: %w\", err)\n\t}\n\n\t// used for seeding the initial relation_tuple_transaction. using INSERT IGNORE on a known\n\t// ID value makes this idempotent (i.e. safe to execute concurrently).\n\tcreateBaseTxn := fmt.Sprintf(\"INSERT IGNORE INTO %s (id, timestamp) VALUES (1, FROM_UNIXTIME(1))\", driver.RelationTupleTransaction())\n\n\tgcCtx, cancelGc := context.WithCancel(context.Background())\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tquantizationPeriodNanos := config.revisionQuantization.Nanoseconds()\n\tif quantizationPeriodNanos < 1 {\n\t\tquantizationPeriodNanos = 1\n\t}\n\n\trevisionQuery := fmt.Sprintf(\n\t\tquerySelectRevision,\n\t\tcolID,\n\t\tdriver.RelationTupleTransaction(),\n\t\tcolTimestamp,\n\t\tquantizationPeriodNanos,\n\t)\n\n\tvalidTransactionQuery := fmt.Sprintf(\n\t\tqueryValidTransaction,\n\t\tcolID,\n\t\tdriver.RelationTupleTransaction(),\n\t\tcolTimestamp,\n\t\t-1*config.gcWindow.Seconds(),\n\t)\n\n\tstore := &Datastore{\n\t\tdb:                     db,\n\t\tdriver:                 driver,\n\t\turl:                    uri,\n\t\trevisionQuantization:   config.revisionQuantization,\n\t\tgcWindow:               config.gcWindow,\n\t\tgcInterval:             config.gcInterval,\n\t\tgcTimeout:              config.gcMaxOperationTime,\n\t\tgcCtx:                  gcCtx,\n\t\tcancelGc:               cancelGc,\n\t\twatchBufferLength:      config.watchBufferLength,\n\t\toptimizedRevisionQuery: revisionQuery,\n\t\tvalidTransactionQuery:  validTransactionQuery,\n\t\tcreateTxn:              createTxn,\n\t\tcreateBaseTxn:          createBaseTxn,\n\t\tQueryBuilder:           queryBuilder,\n\t\treadTxOptions:          &sql.TxOptions{Isolation: sql.LevelSerializable, ReadOnly: true},\n\t\tmaxRetries:             config.maxRetries,\n\t\tanalyzeBeforeStats:     config.analyzeBeforeStats,\n\t\tCachedOptimizedRevisions: revisions.NewCachedOptimizedRevisions(\n\t\t\tmaxRevisionStaleness,\n\t\t),\n\t}\n\n\tstore.SetOptimizedRevisionFunc(store.optimizedRevisionFunc)\n\n\tctx, cancel := context.WithTimeout(context.Background(), seedingTimeout)\n\tdefer cancel()\n\terr = store.seedDatabase(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Start a goroutine for garbage collection.\n\tif store.gcInterval > 0*time.Minute && config.gcEnabled {\n\t\tstore.gcGroup, store.gcCtx = errgroup.WithContext(store.gcCtx)\n\t\tstore.gcGroup.Go(func() error {\n\t\t\treturn common.StartGarbageCollector(\n\t\t\t\tstore.gcCtx,\n\t\t\t\tstore,\n\t\t\t\tstore.gcInterval,\n\t\t\t\tstore.gcWindow,\n\t\t\t\tstore.gcTimeout,\n\t\t\t)\n\t\t})\n\t} else {\n\t\tlog.Warn().Msg(\"datastore background garbage collection disabled\")\n\t}\n\n\treturn store, nil\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc (mds *Datastore) SnapshotReader(revisionRaw datastore.Revision) datastore.Reader {\n\trev := revisionRaw.(revision.Decimal)\n\n\tcreateTxFunc := func(ctx context.Context) (*sql.Tx, txCleanupFunc, error) {\n\t\ttx, err := mds.db.BeginTx(ctx, mds.readTxOptions)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn tx, tx.Rollback, nil\n\t}\n\n\texecutor := common.QueryExecutor{\n\t\tExecutor: newMySQLExecutor(mds.db),\n\t}\n\n\treturn &mysqlReader{\n\t\tmds.QueryBuilder,\n\t\tcreateTxFunc,\n\t\texecutor,\n\t\tbuildLivingObjectFilterForRevision(rev),\n\t}\n}\n\nfunc noCleanup() error { return nil }\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\n// ReadWriteTx starts a read/write transaction, which will be committed if no error is\n// returned and rolled back if an error is returned.\nfunc (mds *Datastore) ReadWriteTx(\n\tctx context.Context,\n\tfn datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tvar err error\n\tfor i := uint8(0); i <= mds.maxRetries; i++ {\n\t\tvar newTxnID uint64\n\t\tif err = migrations.BeginTxFunc(ctx, mds.db, &sql.TxOptions{Isolation: sql.LevelSerializable}, func(tx *sql.Tx) error {\n\t\t\tnewTxnID, err = mds.createNewTransaction(ctx, tx)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to create new txn ID: %w\", err)\n\t\t\t}\n\n\t\t\tlongLivedTx := func(context.Context) (*sql.Tx, txCleanupFunc, error) {\n\t\t\t\treturn tx, noCleanup, nil\n\t\t\t}\n\n\t\t\texecutor := common.QueryExecutor{\n\t\t\t\tExecutor: newMySQLExecutor(tx),\n\t\t\t}\n\n\t\t\trwt := &mysqlReadWriteTXN{\n\t\t\t\t&mysqlReader{\n\t\t\t\t\tmds.QueryBuilder,\n\t\t\t\t\tlongLivedTx,\n\t\t\t\t\texecutor,\n\t\t\t\t\tcurrentlyLivingObjects,\n\t\t\t\t},\n\t\t\t\tmds.driver.RelationTuple(),\n\t\t\t\ttx,\n\t\t\t\tnewTxnID,\n\t\t\t}\n\n\t\t\treturn fn(ctx, rwt)\n\t\t}); err != nil {\n\t\t\tif !config.DisableRetries && isErrorRetryable(err) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\treturn datastore.NoRevision, err\n\t\t}\n\n\t\treturn revisionFromTransaction(newTxnID), nil\n\t}\n\tif !config.DisableRetries {\n\t\terr = fmt.Errorf(\"max retries exceeded: %w\", err)\n\t}\n\treturn datastore.NoRevision, err\n}\n\nfunc isErrorRetryable(err error) bool {\n\tvar mysqlerr *mysql.MySQLError\n\tif !errors.As(err, &mysqlerr) {\n\t\tlog.Debug().Err(err).Msg(\"couldn't determine a sqlstate error code\")\n\t\treturn false\n\t}\n\n\treturn mysqlerr.Number == errMysqlDeadlock || mysqlerr.Number == errMysqlLockWaitTimeout\n}\n\ntype querier interface {\n\tQueryContext(context.Context, string, ...interface{}) (*sql.Rows, error)\n}\n\nfunc newMySQLExecutor(tx querier) common.ExecuteQueryFunc {\n\t// This implementation does not create a transaction because it's redundant for single statements, and it avoids\n\t// the network overhead and reduce contention on the connection pool. From MySQL docs:\n\t//\n\t// https://dev.mysql.com/doc/refman/5.7/en/commit.html\n\t// \"By default, MySQL runs with autocommit mode enabled. This means that, when not otherwise inside a transaction,\n\t// each statement is atomic, as if it were surrounded by START TRANSACTION and COMMIT.\"\n\t//\n\t// https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html\n\t// \"Consistent read is the default mode in which InnoDB processes SELECT statements in READ COMMITTED and\n\t// REPEATABLE READ isolation levels. A consistent read does not set any locks on the tables it accesses,\n\t// and therefore other sessions are free to modify those tables at the same time a consistent read\n\t// is being performed on the table.\"\n\t//\n\t// Prepared statements are also not used given they perform poorly on environments where connections have\n\t// short lifetime (e.g. to gracefully handle load-balancer connection drain)\n\treturn func(ctx context.Context, sqlQuery string, args []interface{}) ([]*core.RelationTuple, error) {\n\t\tspan := trace.SpanFromContext(ctx)\n\n\t\trows, err := tx.QueryContext(ctx, sqlQuery, args...)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t}\n\t\tdefer common.LogOnError(ctx, rows.Close)\n\n\t\tspan.AddEvent(\"Query issued to database\")\n\n\t\tvar tuples []*core.RelationTuple\n\t\tfor rows.Next() {\n\t\t\tnextTuple := &core.RelationTuple{\n\t\t\t\tResourceAndRelation: &core.ObjectAndRelation{},\n\t\t\t\tSubject:             &core.ObjectAndRelation{},\n\t\t\t}\n\n\t\t\tvar caveatName string\n\t\t\tvar caveatContext caveatContextWrapper\n\t\t\terr := rows.Scan(\n\t\t\t\t&nextTuple.ResourceAndRelation.Namespace,\n\t\t\t\t&nextTuple.ResourceAndRelation.ObjectId,\n\t\t\t\t&nextTuple.ResourceAndRelation.Relation,\n\t\t\t\t&nextTuple.Subject.Namespace,\n\t\t\t\t&nextTuple.Subject.ObjectId,\n\t\t\t\t&nextTuple.Subject.Relation,\n\t\t\t\t&caveatName,\n\t\t\t\t&caveatContext,\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t\t}\n\n\t\t\tnextTuple.Caveat, err = common.ContextualizedCaveatFrom(caveatName, caveatContext)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t\t}\n\n\t\t\ttuples = append(tuples, nextTuple)\n\t\t}\n\t\tif err := rows.Err(); err != nil {\n\t\t\treturn nil, fmt.Errorf(errUnableToQueryTuples, err)\n\t\t}\n\t\tspan.AddEvent(\"Tuples loaded\", trace.WithAttributes(attribute.Int(\"tupleCount\", len(tuples))))\n\t\treturn tuples, nil\n\t}\n}\n\n// Datastore is a MySQL-based implementation of the datastore.Datastore interface\ntype Datastore struct {\n\tdb                 *sql.DB\n\tdriver             *migrations.MySQLDriver\n\treadTxOptions      *sql.TxOptions\n\turl                string\n\tanalyzeBeforeStats bool\n\n\trevisionQuantization time.Duration\n\tgcWindow             time.Duration\n\tgcInterval           time.Duration\n\tgcTimeout            time.Duration\n\twatchBufferLength    uint16\n\tmaxRetries           uint8\n\n\toptimizedRevisionQuery string\n\tvalidTransactionQuery  string\n\n\tgcGroup  *errgroup.Group\n\tgcCtx    context.Context\n\tcancelGc context.CancelFunc\n\n\tcreateTxn     string\n\tcreateBaseTxn string\n\n\t*QueryBuilder\n\t*revisions.CachedOptimizedRevisions\n\trevision.DecimalDecoder\n}\n\n// Close closes the data store.\nfunc (mds *Datastore) Close() error {\n\t// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\n\tmds.cancelGc()\n\tif mds.gcGroup != nil {\n\t\tif err := mds.gcGroup.Wait(); err != nil && !errors.Is(err, context.Canceled) {\n\t\t\tlog.Error().Err(err).Msg(\"error waiting for garbage collector to shutdown\")\n\t\t}\n\t}\n\treturn mds.db.Close()\n}\n\n// ReadyState returns whether the datastore is ready to accept data. Datastores that require\n// database schema creation will return false until the migrations have been run to create\n// the necessary tables.\n//\n// fundamentally different from PSQL implementation:\n//   - checking if the current migration version is compatible is implemented with IsHeadCompatible\n//   - Database seeding is handled here, so that we can decouple schema migration from data migration\n//     and support skeema-based migrations.\nfunc (mds *Datastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\tif err := mds.db.PingContext(ctx); err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tcurrentMigrationRevision, err := mds.driver.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tcompatible, err := migrations.Manager.IsHeadCompatible(currentMigrationRevision)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tif !compatible {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: \"datastore is not at a currently compatible revision\",\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\tisSeeded, err := mds.isSeeded(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tif !isSeeded {\n\t\treturn datastore.ReadyState{\n\t\t\tMessage: \"datastore is not properly seeded\",\n\t\t\tIsReady: false,\n\t\t}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: \"\",\n\t\tIsReady: true,\n\t}, nil\n}\n\nfunc (mds *Datastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: true}}, nil\n}\n\n// isSeeded determines if the backing database has been seeded\nfunc (mds *Datastore) isSeeded(ctx context.Context) (bool, error) {\n\theadRevision, err := mds.HeadRevision(ctx)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif headRevision == datastore.NoRevision {\n\t\treturn false, nil\n\t}\n\n\t_, err = mds.getUniqueID(ctx)\n\tif err != nil {\n\t\treturn false, nil\n\t}\n\n\treturn true, nil\n}\n\n// seedDatabase initializes the first transaction revision if necessary.\nfunc (mds *Datastore) seedDatabase(ctx context.Context) error {\n\tctx, span := tracer.Start(ctx, \"seedDatabase\")\n\tdefer span.End()\n\n\tisSeeded, err := mds.isSeeded(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif isSeeded {\n\t\treturn nil\n\t}\n\n\t// this seeds the transaction table with the first transaction, in a way that is idempotent\n\treturn migrations.BeginTxFunc(ctx, mds.db, nil, func(tx *sql.Tx) error {\n\t\t// idempotent INSERT IGNORE transaction id=1. safe to be executed concurrently.\n\t\tresult, err := tx.ExecContext(ctx, mds.createBaseTxn)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: %w\", err)\n\t\t}\n\n\t\tlastInsertID, err := result.LastInsertId()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to get last inserted id: %w\", err)\n\t\t}\n\n\t\tif lastInsertID != noLastInsertID {\n\t\t\t// If there was no error and `lastInsertID` is 0, the insert was ignored. This indicates the transaction\n\t\t\t// was already seeded by another processes (i.e. race condition).\n\t\t\tlog.Ctx(ctx).Info().Int64(\"headRevision\", lastInsertID).Msg(\"seeded base datastore headRevision\")\n\t\t}\n\n\t\tuuidSQL, uuidArgs, err := sb.\n\t\t\tInsert(mds.driver.Metadata()).\n\t\t\tOptions(\"IGNORE\").\n\t\t\tColumns(metadataIDColumn, metadataUniqueIDColumn).\n\t\t\tValues(0, uuid.NewString()).\n\t\t\tToSql()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to prepare SQL: %w\", err)\n\t\t}\n\n\t\tinsertUniqueResult, err := tx.ExecContext(ctx, uuidSQL, uuidArgs...)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to insert unique ID: %w\", err)\n\t\t}\n\n\t\tlastInsertID, err = insertUniqueResult.LastInsertId()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"seedDatabase: failed to get last inserted unique id: %w\", err)\n\t\t}\n\n\t\tif lastInsertID != noLastInsertID {\n\t\t\t// If there was no error and `lastInsertID` is 0, the insert was ignored. This indicates the transaction\n\t\t\t// was already seeded by another processes (i.e. race condition).\n\t\t\tlog.Ctx(ctx).Info().Int64(\"headRevision\", lastInsertID).Msg(\"seeded base datastore unique ID\")\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc buildLivingObjectFilterForRevision(revision revision.Decimal) queryFilterer {\n\treturn func(original sq.SelectBuilder) sq.SelectBuilder {\n\t\treturn original.Where(sq.LtOrEq{colCreatedTxn: transactionFromRevision(revision)}).\n\t\t\tWhere(sq.Or{\n\t\t\t\tsq.Eq{colDeletedTxn: liveDeletedTxnID},\n\t\t\t\tsq.Gt{colDeletedTxn: revision},\n\t\t\t})\n\t}\n}\n\n// TODO (@vroldanbet) dupe from postgres datastore - need to refactor\nfunc currentlyLivingObjects(original sq.SelectBuilder) sq.SelectBuilder {\n\treturn original.Where(sq.Eq{colDeletedTxn: liveDeletedTxnID})\n}\n", "package postgres\n\nimport (\n\t\"context\"\n\tdbsql \"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/IBM/pgxpoolprometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/jackc/pgx/v5\"\n\t\"github.com/jackc/pgx/v5/pgconn\"\n\t\"github.com/jackc/pgx/v5/pgxpool\"\n\t\"github.com/jackc/pgx/v5/stdlib\"\n\t\"github.com/ngrok/sqlmw\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opentelemetry.io/otel\"\n\t\"golang.org/x/sync/errgroup\"\n\n\tdatastoreinternal \"github.com/authzed/spicedb/internal/datastore\"\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\tpgxcommon \"github.com/authzed/spicedb/internal/datastore/postgres/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/postgres/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nconst (\n\tEngine           = \"postgres\"\n\ttableNamespace   = \"namespace_config\"\n\ttableTransaction = \"relation_tuple_transaction\"\n\ttableTuple       = \"relation_tuple\"\n\ttableCaveat      = \"caveat\"\n\n\tcolXID               = \"xid\"\n\tcolTimestamp         = \"timestamp\"\n\tcolNamespace         = \"namespace\"\n\tcolConfig            = \"serialized_config\"\n\tcolCreatedXid        = \"created_xid\"\n\tcolDeletedXid        = \"deleted_xid\"\n\tcolSnapshot          = \"snapshot\"\n\tcolObjectID          = \"object_id\"\n\tcolRelation          = \"relation\"\n\tcolUsersetNamespace  = \"userset_namespace\"\n\tcolUsersetObjectID   = \"userset_object_id\"\n\tcolUsersetRelation   = \"userset_relation\"\n\tcolCaveatName        = \"name\"\n\tcolCaveatDefinition  = \"definition\"\n\tcolCaveatContextName = \"caveat_name\"\n\tcolCaveatContext     = \"caveat_context\"\n\n\terrUnableToInstantiate = \"unable to instantiate datastore\"\n\n\t// The parameters to this format string are:\n\t// 1: the created_xid or deleted_xid column name\n\t//\n\t// The placeholders are the snapshot and the expected boolean value respectively.\n\tsnapshotAlive = \"pg_visible_in_snapshot(%[1]s, ?) = ?\"\n\n\t// This is the largest positive integer possible in postgresql\n\tliveDeletedTxnID = uint64(9223372036854775807)\n\n\ttracingDriverName = \"postgres-tracing\"\n\n\tgcBatchDeleteSize = 1000\n\n\tlivingTupleConstraint = \"uq_relation_tuple_living_xid\"\n)\n\nfunc init() {\n\tdbsql.Register(tracingDriverName, sqlmw.Driver(stdlib.GetDefaultDriver(), new(traceInterceptor)))\n}\n\nvar (\n\tpsql = sq.StatementBuilder.PlaceholderFormat(sq.Dollar)\n\n\tgetRevision = psql.\n\t\t\tSelect(colXID, colSnapshot).\n\t\t\tFrom(tableTransaction).\n\t\t\tOrderByClause(fmt.Sprintf(\"%s DESC\", colXID)).\n\t\t\tLimit(1)\n\n\tcreateTxn = fmt.Sprintf(\n\t\t\"INSERT INTO %s DEFAULT VALUES RETURNING %s, %s\",\n\t\ttableTransaction,\n\t\tcolXID,\n\t\tcolSnapshot,\n\t)\n\n\tgetNow = psql.Select(\"NOW()\")\n\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/postgres\")\n)\n\ntype sqlFilter interface {\n\tToSql() (string, []interface{}, error)\n}\n\n// NewPostgresDatastore initializes a SpiceDB datastore that uses a PostgreSQL\n// database by leveraging manual book-keeping to implement revisioning.\n//\n// This datastore is also tested to be compatible with CockroachDB.\nfunc NewPostgresDatastore(\n\turl string,\n\toptions ...Option,\n) (datastore.Datastore, error) {\n\tds, err := newPostgresDatastore(url, options...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn datastoreinternal.NewSeparatingContextDatastoreProxy(ds), nil\n}\n\nfunc newPostgresDatastore(\n\tpgURL string,\n\toptions ...Option,\n) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(options)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, pgURL)\n\t}\n\n\t// Parse the DB URI into configuration.\n\tparsedConfig, err := pgxpool.ParseConfig(pgURL)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, pgURL)\n\t}\n\n\t// Setup the default custom plan setting, if applicable.\n\tpgConfig, err := defaultCustomPlan(parsedConfig)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, pgURL)\n\t}\n\n\t// Setup the config for each of the read and write pools.\n\treadPoolConfig := pgConfig.Copy()\n\tconfig.readPoolOpts.ConfigurePgx(readPoolConfig)\n\n\treadPoolConfig.AfterConnect = func(ctx context.Context, conn *pgx.Conn) error {\n\t\tRegisterTypes(conn.TypeMap())\n\t\treturn nil\n\t}\n\n\twritePoolConfig := pgConfig.Copy()\n\tconfig.writePoolOpts.ConfigurePgx(writePoolConfig)\n\n\twritePoolConfig.AfterConnect = func(ctx context.Context, conn *pgx.Conn) error {\n\t\tRegisterTypes(conn.TypeMap())\n\t\treturn nil\n\t}\n\n\tif config.migrationPhase != \"\" {\n\t\tlog.Info().\n\t\t\tStr(\"phase\", config.migrationPhase).\n\t\t\tMsg(\"postgres configured to use intermediate migration phase\")\n\t}\n\n\tinitializationContext, cancelInit := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancelInit()\n\n\treadPool, err := pgxpool.NewWithConfig(initializationContext, readPoolConfig)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, pgURL)\n\t}\n\n\twritePool, err := pgxpool.NewWithConfig(initializationContext, writePoolConfig)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, pgURL)\n\t}\n\n\t// Verify that the server supports commit timestamps\n\tvar trackTSOn string\n\tif err := readPool.\n\t\tQueryRow(initializationContext, \"SHOW track_commit_timestamp;\").\n\t\tScan(&trackTSOn); err != nil {\n\t\treturn nil, err\n\t}\n\n\twatchEnabled := trackTSOn == \"on\"\n\tif !watchEnabled {\n\t\tlog.Warn().Msg(\"watch API disabled, postgres must be run with track_commit_timestamp=on\")\n\t}\n\n\tif config.enablePrometheusStats {\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(readPool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"read\",\n\t\t})); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := prometheus.Register(pgxpoolprometheus.NewCollector(writePool, map[string]string{\n\t\t\t\"db_name\":    \"spicedb\",\n\t\t\t\"pool_usage\": \"write\",\n\t\t})); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := common.RegisterGCMetrics(); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tgcCtx, cancelGc := context.WithCancel(context.Background())\n\n\tquantizationPeriodNanos := config.revisionQuantization.Nanoseconds()\n\tif quantizationPeriodNanos < 1 {\n\t\tquantizationPeriodNanos = 1\n\t}\n\trevisionQuery := fmt.Sprintf(\n\t\tquerySelectRevision,\n\t\tcolXID,\n\t\ttableTransaction,\n\t\tcolTimestamp,\n\t\tquantizationPeriodNanos,\n\t\tcolSnapshot,\n\t)\n\n\tvalidTransactionQuery := fmt.Sprintf(\n\t\tqueryValidTransaction,\n\t\tcolXID,\n\t\ttableTransaction,\n\t\tcolTimestamp,\n\t\tconfig.gcWindow.Seconds(),\n\t\tcolSnapshot,\n\t)\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tdatastore := &pgDatastore{\n\t\tCachedOptimizedRevisions: revisions.NewCachedOptimizedRevisions(\n\t\t\tmaxRevisionStaleness,\n\t\t),\n\t\tdburl:                   pgURL,\n\t\treadPool:                pgxcommon.MustNewInterceptorPooler(readPool, config.queryInterceptor),\n\t\twritePool:               pgxcommon.MustNewInterceptorPooler(writePool, config.queryInterceptor),\n\t\twatchBufferLength:       config.watchBufferLength,\n\t\toptimizedRevisionQuery:  revisionQuery,\n\t\tvalidTransactionQuery:   validTransactionQuery,\n\t\tgcWindow:                config.gcWindow,\n\t\tgcInterval:              config.gcInterval,\n\t\tgcTimeout:               config.gcMaxOperationTime,\n\t\tanalyzeBeforeStatistics: config.analyzeBeforeStatistics,\n\t\twatchEnabled:            watchEnabled,\n\t\tgcCtx:                   gcCtx,\n\t\tcancelGc:                cancelGc,\n\t\treadTxOptions:           pgx.TxOptions{IsoLevel: pgx.RepeatableRead, AccessMode: pgx.ReadOnly},\n\t\tmaxRetries:              config.maxRetries,\n\t}\n\n\tdatastore.SetOptimizedRevisionFunc(datastore.optimizedRevisionFunc)\n\n\t// Start a goroutine for garbage collection.\n\tif datastore.gcInterval > 0*time.Minute && config.gcEnabled {\n\t\tdatastore.gcGroup, datastore.gcCtx = errgroup.WithContext(datastore.gcCtx)\n\t\tdatastore.gcGroup.Go(func() error {\n\t\t\treturn common.StartGarbageCollector(\n\t\t\t\tdatastore.gcCtx,\n\t\t\t\tdatastore,\n\t\t\t\tdatastore.gcInterval,\n\t\t\t\tdatastore.gcWindow,\n\t\t\t\tdatastore.gcTimeout,\n\t\t\t)\n\t\t})\n\t} else {\n\t\tlog.Warn().Msg(\"datastore background garbage collection disabled\")\n\t}\n\n\treturn datastore, nil\n}\n\ntype pgDatastore struct {\n\t*revisions.CachedOptimizedRevisions\n\n\tdburl                   string\n\treadPool, writePool     pgxcommon.ConnPooler\n\twatchBufferLength       uint16\n\toptimizedRevisionQuery  string\n\tvalidTransactionQuery   string\n\tgcWindow                time.Duration\n\tgcInterval              time.Duration\n\tgcTimeout               time.Duration\n\tanalyzeBeforeStatistics bool\n\treadTxOptions           pgx.TxOptions\n\tmaxRetries              uint8\n\twatchEnabled            bool\n\n\tgcGroup  *errgroup.Group\n\tgcCtx    context.Context\n\tcancelGc context.CancelFunc\n}\n\nfunc (pgd *pgDatastore) SnapshotReader(revRaw datastore.Revision) datastore.Reader {\n\trev := revRaw.(postgresRevision)\n\n\tqueryFuncs := pgxcommon.QuerierFuncsFor(pgd.readPool)\n\texecutor := common.QueryExecutor{\n\t\tExecutor: pgxcommon.NewPGXExecutor(queryFuncs),\n\t}\n\n\treturn &pgReader{\n\t\tqueryFuncs,\n\t\texecutor,\n\t\tbuildLivingObjectFilterForRevision(rev),\n\t}\n}\n\n// ReadWriteTx starts a read/write transaction, which will be committed if no error is\n// returned and rolled back if an error is returned.\nfunc (pgd *pgDatastore) ReadWriteTx(\n\tctx context.Context,\n\tfn datastore.TxUserFunc,\n\topts ...options.RWTOptionsOption,\n) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tvar err error\n\tfor i := uint8(0); i <= pgd.maxRetries; i++ {\n\t\tvar newXID xid8\n\t\tvar newSnapshot pgSnapshot\n\t\terr = wrapError(pgx.BeginTxFunc(ctx, pgd.writePool, pgx.TxOptions{IsoLevel: pgx.Serializable}, func(tx pgx.Tx) error {\n\t\t\tvar err error\n\t\t\tnewXID, newSnapshot, err = createNewTransaction(ctx, tx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tqueryFuncs := pgxcommon.QuerierFuncsFor(pgd.readPool)\n\t\t\texecutor := common.QueryExecutor{\n\t\t\t\tExecutor: pgxcommon.NewPGXExecutor(queryFuncs),\n\t\t\t}\n\n\t\t\trwt := &pgReadWriteTXN{\n\t\t\t\t&pgReader{\n\t\t\t\t\tqueryFuncs,\n\t\t\t\t\texecutor,\n\t\t\t\t\tcurrentlyLivingObjects,\n\t\t\t\t},\n\t\t\t\ttx,\n\t\t\t\tnewXID,\n\t\t\t}\n\n\t\t\treturn fn(ctx, rwt)\n\t\t}))\n\n\t\tif err != nil {\n\t\t\tif !config.DisableRetries && errorRetryable(err) {\n\t\t\t\tpgxcommon.SleepOnErr(ctx, err, i)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\treturn datastore.NoRevision, err\n\t\t}\n\n\t\tif i > 0 {\n\t\t\tlog.Debug().Uint8(\"retries\", i).Msg(\"transaction succeeded after retry\")\n\t\t}\n\n\t\treturn postgresRevision{newSnapshot.markComplete(newXID.Uint64)}, nil\n\t}\n\n\tif !config.DisableRetries {\n\t\terr = fmt.Errorf(\"max retries exceeded: %w\", err)\n\t}\n\n\treturn datastore.NoRevision, err\n}\n\nfunc wrapError(err error) error {\n\tif pgxcommon.IsSerializationError(err) {\n\t\treturn common.NewSerializationError(err)\n\t}\n\n\t// hack: pgx asyncClose usually happens after cancellation,\n\t// but the reason for it being closed is not propagated\n\t// and all we get is attempting to perform an operation\n\t// on cancelled connection. This keeps the same error,\n\t// but wrapped along a cancellation so that:\n\t// - pgx logger does not log it\n\t// - response is sent as canceled back to the client\n\tif err != nil && err.Error() == \"conn closed\" {\n\t\treturn errors.Join(err, context.Canceled)\n\t}\n\n\treturn err\n}\n\nfunc (pgd *pgDatastore) Close() error {\n\tpgd.cancelGc()\n\n\tif pgd.gcGroup != nil {\n\t\terr := pgd.gcGroup.Wait()\n\t\tlog.Warn().Err(err).Msg(\"completed shutdown of postgres datastore\")\n\t}\n\n\tpgd.readPool.Close()\n\tpgd.writePool.Close()\n\treturn nil\n}\n\nfunc errorRetryable(err error) bool {\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\treturn false\n\t}\n\n\tif pgconn.SafeToRetry(err) {\n\t\treturn true\n\t}\n\n\tif pgxcommon.IsSerializationError(err) {\n\t\treturn true\n\t}\n\n\tlog.Warn().Err(err).Msg(\"unable to determine if pgx error is retryable\")\n\treturn false\n}\n\nfunc (pgd *pgDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.DatabaseMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for postgres: %w\", err)\n\t}\n\n\tpgDriver, err := migrations.NewAlembicPostgresDriver(ctx, pgd.dburl)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\tdefer pgDriver.Close(ctx)\n\n\tversion, err := pgDriver.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\tif version == headMigration {\n\t\treturn datastore.ReadyState{IsReady: true}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: fmt.Sprintf(\n\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\tversion,\n\t\t\theadMigration,\n\t\t),\n\t\tIsReady: false,\n\t}, nil\n}\n\nfunc (pgd *pgDatastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: pgd.watchEnabled}}, nil\n}\n\nfunc buildLivingObjectFilterForRevision(revision postgresRevision) queryFilterer {\n\tcreatedBeforeTXN := sq.Expr(fmt.Sprintf(\n\t\tsnapshotAlive,\n\t\tcolCreatedXid,\n\t), revision.snapshot, true)\n\n\tdeletedAfterTXN := sq.Expr(fmt.Sprintf(\n\t\tsnapshotAlive,\n\t\tcolDeletedXid,\n\t), revision.snapshot, false)\n\n\treturn func(original sq.SelectBuilder) sq.SelectBuilder {\n\t\treturn original.Where(createdBeforeTXN).Where(deletedAfterTXN)\n\t}\n}\n\nfunc currentlyLivingObjects(original sq.SelectBuilder) sq.SelectBuilder {\n\treturn original.Where(sq.Eq{colDeletedXid: liveDeletedTxnID})\n}\n\n// defaultCustomPlan parses a Postgres URI and determines if a plan_cache_mode\n// has been specified. If not, it defaults to \"force_custom_plan\".\n// This works around a bug impacting performance documented here:\n// https://spicedb.dev/d/force-custom-plan.\nfunc defaultCustomPlan(poolConfig *pgxpool.Config) (*pgxpool.Config, error) {\n\tif existing, ok := poolConfig.ConnConfig.Config.RuntimeParams[\"plan_cache_mode\"]; ok {\n\t\tlog.Info().\n\t\t\tStr(\"plan_cache_mode\", existing).\n\t\t\tMsg(\"found plan_cache_mode in DB URI; leaving as-is\")\n\t\treturn poolConfig, nil\n\t}\n\n\tpoolConfig.ConnConfig.Config.RuntimeParams[\"plan_cache_mode\"] = \"force_custom_plan\"\n\tlog.Warn().\n\t\tStr(\"details-url\", \"https://spicedb.dev/d/force-custom-plan\").\n\t\tStr(\"plan_cache_mode\", \"force_custom_plan\").\n\t\tMsg(\"defaulting value in Postgres DB URI\")\n\n\treturn poolConfig, nil\n}\n\nvar _ datastore.Datastore = &pgDatastore{}\n", "package spanner\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"cloud.google.com/go/spanner\"\n\tocprom \"contrib.go.opencensus.io/exporter/prometheus\"\n\tsq \"github.com/Masterminds/squirrel\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"go.opencensus.io/plugin/ocgrpc\"\n\t\"go.opencensus.io/stats/view\"\n\t\"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\"\n\t\"go.opentelemetry.io/otel\"\n\t\"go.opentelemetry.io/otel/attribute\"\n\t\"go.opentelemetry.io/otel/trace\"\n\t\"google.golang.org/api/option\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\n\t\"github.com/authzed/spicedb/internal/datastore/common\"\n\t\"github.com/authzed/spicedb/internal/datastore/common/revisions\"\n\t\"github.com/authzed/spicedb/internal/datastore/spanner/migrations\"\n\tlog \"github.com/authzed/spicedb/internal/logging\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n\t\"github.com/authzed/spicedb/pkg/datastore/options\"\n\t\"github.com/authzed/spicedb/pkg/datastore/revision\"\n\tcore \"github.com/authzed/spicedb/pkg/proto/core/v1\"\n)\n\nfunc init() {\n\tdatastore.Engines = append(datastore.Engines, Engine)\n}\n\nconst (\n\tEngine = \"spanner\"\n\n\terrUnableToInstantiate = \"unable to instantiate spanner client\"\n\n\terrRevision = \"unable to load revision: %w\"\n\n\terrUnableToWriteRelationships    = \"unable to write relationships: %w\"\n\terrUnableToBulkLoadRelationships = \"unable to bulk load relationships: %w\"\n\terrUnableToDeleteRelationships   = \"unable to delete relationships: %w\"\n\n\terrUnableToWriteConfig    = \"unable to write namespace config: %w\"\n\terrUnableToReadConfig     = \"unable to read namespace config: %w\"\n\terrUnableToDeleteConfig   = \"unable to delete namespace config: %w\"\n\terrUnableToListNamespaces = \"unable to list namespaces: %w\"\n\n\terrUnableToReadCaveat   = \"unable to read caveat: %w\"\n\terrUnableToWriteCaveat  = \"unable to write caveat: %w\"\n\terrUnableToListCaveats  = \"unable to list caveats: %w\"\n\terrUnableToDeleteCaveat = \"unable to delete caveat: %w\"\n\n\t// See https://cloud.google.com/spanner/docs/change-streams#data-retention\n\t// See https://github.com/authzed/spicedb/issues/1457\n\tdefaultChangeStreamRetention = 24 * time.Hour\n)\n\nvar (\n\tsql    = sq.StatementBuilder.PlaceholderFormat(sq.AtP)\n\ttracer = otel.Tracer(\"spicedb/internal/datastore/spanner\")\n\n\talreadyExistsRegex = regexp.MustCompile(`^Table relation_tuple: Row {String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\), String\\(\"([^\\\"]+)\"\\)} already exists.$`)\n)\n\ntype spannerDatastore struct {\n\t*revisions.RemoteClockRevisions\n\trevision.DecimalDecoder\n\n\tclient   *spanner.Client\n\tconfig   spannerOptions\n\tdatabase string\n}\n\n// NewSpannerDatastore returns a datastore backed by cloud spanner\nfunc NewSpannerDatastore(database string, opts ...Option) (datastore.Datastore, error) {\n\tconfig, err := generateConfig(opts)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, database)\n\t}\n\n\tif len(config.emulatorHost) > 0 {\n\t\tif err := os.Setenv(\"SPANNER_EMULATOR_HOST\", config.emulatorHost); err != nil {\n\t\t\tlog.Error().Err(err).Msg(\"failed to set SPANNER_EMULATOR_HOST env variable\")\n\t\t}\n\t}\n\tif len(os.Getenv(\"SPANNER_EMULATOR_HOST\")) > 0 {\n\t\tlog.Info().Str(\"spanner-emulator-host\", os.Getenv(\"SPANNER_EMULATOR_HOST\")).Msg(\"running against spanner emulator\")\n\t}\n\n\terr = spanner.EnableStatViews()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner session metrics: %w\", err)\n\t}\n\terr = spanner.EnableGfeLatencyAndHeaderMissingCountViews()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner GFE metrics: %w\", err)\n\t}\n\n\t// Register Spanner client gRPC metrics (include round-trip latency, received/sent bytes...)\n\tif err := view.Register(ocgrpc.DefaultClientViews...); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable gRPC metrics for Spanner client: %w\", err)\n\t}\n\n\t_, err = ocprom.NewExporter(ocprom.Options{\n\t\tNamespace:  \"spicedb\",\n\t\tRegisterer: prometheus.DefaultRegisterer,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to enable spanner GFE latency stats: %w\", err)\n\t}\n\n\tcfg := spanner.DefaultSessionPoolConfig\n\tcfg.MinOpened = config.minSessions\n\tcfg.MaxOpened = config.maxSessions\n\tclient, err := spanner.NewClientWithConfig(context.Background(), database,\n\t\tspanner.ClientConfig{SessionPoolConfig: cfg},\n\t\toption.WithCredentialsFile(config.credentialsFilePath),\n\t\toption.WithGRPCConnectionPool(max(config.readMaxOpen, config.writeMaxOpen)),\n\t\toption.WithGRPCDialOption(\n\t\t\tgrpc.WithStatsHandler(otelgrpc.NewClientHandler()),\n\t\t),\n\t)\n\tif err != nil {\n\t\treturn nil, common.RedactAndLogSensitiveConnString(errUnableToInstantiate, err, database)\n\t}\n\n\tmaxRevisionStaleness := time.Duration(float64(config.revisionQuantization.Nanoseconds())*\n\t\tconfig.maxRevisionStalenessPercent) * time.Nanosecond\n\n\tds := spannerDatastore{\n\t\tRemoteClockRevisions: revisions.NewRemoteClockRevisions(\n\t\t\tdefaultChangeStreamRetention,\n\t\t\tmaxRevisionStaleness,\n\t\t\tconfig.followerReadDelay,\n\t\t\tconfig.revisionQuantization,\n\t\t),\n\t\tclient:   client,\n\t\tconfig:   config,\n\t\tdatabase: database,\n\t}\n\tds.RemoteClockRevisions.SetNowFunc(ds.headRevisionInternal)\n\n\treturn ds, nil\n}\n\ntype traceableRTX struct {\n\tdelegate readTX\n}\n\nfunc (t *traceableRTX) ReadRow(ctx context.Context, table string, key spanner.Key, columns []string) (*spanner.Row, error) {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.ReadRow\"),\n\t\tattribute.String(\"table\", table),\n\t\tattribute.String(\"key\", key.String()),\n\t\tattribute.StringSlice(\"columns\", columns))\n\n\treturn t.delegate.ReadRow(ctx, table, key, columns)\n}\n\nfunc (t *traceableRTX) Read(ctx context.Context, table string, keys spanner.KeySet, columns []string) *spanner.RowIterator {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.Read\"),\n\t\tattribute.String(\"table\", table),\n\t\tattribute.StringSlice(\"columns\", columns))\n\n\treturn t.delegate.Read(ctx, table, keys, columns)\n}\n\nfunc (t *traceableRTX) Query(ctx context.Context, statement spanner.Statement) *spanner.RowIterator {\n\ttrace.SpanFromContext(ctx).SetAttributes(\n\t\tattribute.String(\"spannerAPI\", \"ReadOnlyTransaction.Query\"),\n\t\tattribute.String(\"statement\", statement.SQL))\n\n\treturn t.delegate.Query(ctx, statement)\n}\n\nfunc (sd spannerDatastore) SnapshotReader(revisionRaw datastore.Revision) datastore.Reader {\n\tr := revisionRaw.(revision.Decimal)\n\n\ttxSource := func() readTX {\n\t\treturn &traceableRTX{delegate: sd.client.Single().WithTimestampBound(spanner.ReadTimestamp(timestampFromRevision(r)))}\n\t}\n\texecutor := common.QueryExecutor{Executor: queryExecutor(txSource)}\n\treturn spannerReader{executor, txSource}\n}\n\nfunc (sd spannerDatastore) ReadWriteTx(ctx context.Context, fn datastore.TxUserFunc, opts ...options.RWTOptionsOption) (datastore.Revision, error) {\n\tconfig := options.NewRWTOptionsWithOptions(opts...)\n\n\tctx, span := tracer.Start(ctx, \"ReadWriteTx\")\n\tdefer span.End()\n\n\tctx, cancel := context.WithCancel(ctx)\n\tts, err := sd.client.ReadWriteTransaction(ctx, func(ctx context.Context, spannerRWT *spanner.ReadWriteTransaction) error {\n\t\ttxSource := func() readTX {\n\t\t\treturn &traceableRTX{delegate: spannerRWT}\n\t\t}\n\n\t\texecutor := common.QueryExecutor{Executor: queryExecutor(txSource)}\n\t\trwt := spannerReadWriteTXN{\n\t\t\tspannerReader{executor, txSource},\n\t\t\tspannerRWT,\n\t\t\tsd.config.disableStats,\n\t\t}\n\t\terr := func() error {\n\t\t\tinnerCtx, innerSpan := tracer.Start(ctx, \"TxUserFunc\")\n\t\t\tdefer innerSpan.End()\n\n\t\t\treturn fn(innerCtx, rwt)\n\t\t}()\n\t\tif err != nil {\n\t\t\tif config.DisableRetries {\n\t\t\t\tdefer cancel()\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\tif cerr := convertToWriteConstraintError(err); cerr != nil {\n\t\t\treturn datastore.NoRevision, cerr\n\t\t}\n\t\treturn datastore.NoRevision, err\n\t}\n\n\treturn revisionFromTimestamp(ts), nil\n}\n\nfunc (sd spannerDatastore) ReadyState(ctx context.Context) (datastore.ReadyState, error) {\n\theadMigration, err := migrations.SpannerMigrations.HeadRevision()\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, fmt.Errorf(\"invalid head migration found for spanner: %w\", err)\n\t}\n\n\tchecker := migrations.NewSpannerVersionChecker(sd.client)\n\tversion, err := checker.Version(ctx)\n\tif err != nil {\n\t\treturn datastore.ReadyState{}, err\n\t}\n\n\t// TODO(jschorr): Remove register-tuple-change-stream once the multi-phase is done.\n\tif version == headMigration || version == \"register-tuple-change-stream\" {\n\t\treturn datastore.ReadyState{IsReady: true}, nil\n\t}\n\n\treturn datastore.ReadyState{\n\t\tMessage: fmt.Sprintf(\n\t\t\t\"datastore is not migrated: currently at revision `%s`, but requires `%s`. Please run `spicedb migrate`.\",\n\t\t\tversion,\n\t\t\theadMigration,\n\t\t),\n\t\tIsReady: false,\n\t}, nil\n}\n\nfunc (sd spannerDatastore) Features(_ context.Context) (*datastore.Features, error) {\n\treturn &datastore.Features{Watch: datastore.Feature{Enabled: true}}, nil\n}\n\nfunc (sd spannerDatastore) Close() error {\n\tsd.client.Close()\n\treturn nil\n}\n\nfunc statementFromSQL(sql string, args []any) spanner.Statement {\n\tparams := make(map[string]any, len(args))\n\tfor index, arg := range args {\n\t\tparams[\"p\"+strconv.Itoa(index+1)] = arg\n\t}\n\n\treturn spanner.Statement{\n\t\tSQL:    sql,\n\t\tParams: params,\n\t}\n}\n\nfunc convertToWriteConstraintError(err error) error {\n\tif spanner.ErrCode(err) == codes.AlreadyExists {\n\t\tdescription := spanner.ErrDesc(err)\n\t\tfound := alreadyExistsRegex.FindStringSubmatch(description)\n\t\tif found != nil {\n\t\t\treturn common.NewCreateRelationshipExistsError(&core.RelationTuple{\n\t\t\t\tResourceAndRelation: &core.ObjectAndRelation{\n\t\t\t\t\tNamespace: found[1],\n\t\t\t\t\tObjectId:  found[2],\n\t\t\t\t\tRelation:  found[3],\n\t\t\t\t},\n\t\t\t\tSubject: &core.ObjectAndRelation{\n\t\t\t\t\tNamespace: found[4],\n\t\t\t\t\tObjectId:  found[5],\n\t\t\t\t\tRelation:  found[6],\n\t\t\t\t},\n\t\t\t})\n\t\t}\n\n\t\treturn common.NewCreateRelationshipExistsError(nil)\n\t}\n\treturn nil\n}\n", "package datastore_test\n\nimport (\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/authzed/spicedb/internal/datastore/crdb\"\n\t\"github.com/authzed/spicedb/internal/datastore/mysql\"\n\t\"github.com/authzed/spicedb/internal/datastore/postgres\"\n\t\"github.com/authzed/spicedb/internal/datastore/spanner\"\n\t\"github.com/authzed/spicedb/pkg/datastore\"\n)\n\nfunc createEngine(engineID string, uri string) error {\n\tswitch engineID {\n\tcase \"postgres\":\n\t\t_, err := postgres.NewPostgresDatastore(uri)\n\t\treturn err\n\n\tcase \"mysql\":\n\t\t_, err := mysql.NewMySQLDatastore(uri)\n\t\treturn err\n\n\tcase \"spanner\":\n\t\t_, err := spanner.NewSpannerDatastore(uri)\n\t\treturn err\n\n\tcase \"cockroachdb\":\n\t\t_, err := crdb.NewCRDBDatastore(uri)\n\t\treturn err\n\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"missing create implementation for engine %s\", engineID))\n\t}\n}\n\nfunc TestDatastoreURIErrors(t *testing.T) {\n\ttcs := map[string]string{\n\t\t\"some-wrong-uri\":                                  \"wrong\",\n\t\t\"postgres://foo:bar:baz@someurl\":                  \"bar\",\n\t\t\"postgres://spicedb:somepassword\":                 \"somepassword\",\n\t\t\"postgres://spicedb:somepassword#@foo\":            \"somepassword\",\n\t\t\"username=foo password=somepassword dsn=whatever\": \"somepassword\",\n\t}\n\n\tfor _, engineID := range datastore.Engines {\n\t\tt.Run(engineID, func(t *testing.T) {\n\t\t\tfor tc, check := range tcs {\n\t\t\t\tt.Run(tc, func(t *testing.T) {\n\t\t\t\t\terr := createEngine(engineID, tc)\n\t\t\t\t\trequire.Error(t, err)\n\t\t\t\t\trequire.NotContains(t, err.Error(), check)\n\t\t\t\t})\n\t\t\t}\n\t\t})\n\t}\n}\n"], "filenames": ["internal/datastore/common/errors.go", "internal/datastore/crdb/crdb.go", "internal/datastore/mysql/datastore.go", "internal/datastore/postgres/postgres.go", "internal/datastore/spanner/spanner.go", "pkg/datastore/errors_test.go"], "buggy_code_start_loc": [4, 67, 114, 59, 42, 1], "buggy_code_end_loc": [94, 209, 141, 207, 132, 16], "fixing_code_start_loc": [5, 67, 114, 59, 42, 1], "fixing_code_end_loc": [115, 206, 141, 207, 132, 60], "type": "CWE-532", "message": "SpiceDB is an open source, Google Zanzibar-inspired database for creating and managing security-critical application permissions. Prior to version 1.27.0-rc1, when the provided datastore URI is malformed (e.g. by having a password which contains `:`) the full URI (including the provided password) is printed, so that the password is shown in the logs. Version 1.27.0-rc1 patches this issue.", "other": {"cve": {"id": "CVE-2023-46255", "sourceIdentifier": "security-advisories@github.com", "published": "2023-10-31T16:15:10.007", "lastModified": "2023-11-08T17:52:06.617", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "SpiceDB is an open source, Google Zanzibar-inspired database for creating and managing security-critical application permissions. Prior to version 1.27.0-rc1, when the provided datastore URI is malformed (e.g. by having a password which contains `:`) the full URI (including the provided password) is printed, so that the password is shown in the logs. Version 1.27.0-rc1 patches this issue."}, {"lang": "es", "value": "SpiceDB es una base de datos de c\u00f3digo abierto inspirada en Google Zanz\u00edbar para crear y administrar permisos de aplicaciones cr\u00edticas para la seguridad. Antes de la versi\u00f3n 1.27.0-rc1, cuando el URI del almac\u00e9n de datos proporcionado tiene un formato incorrecto (por ejemplo, al tener una contrase\u00f1a que contiene `:`), se imprime el URI completo (incluida la contrase\u00f1a proporcionada), de modo que la contrase\u00f1a se muestra en los registros. La versi\u00f3n 1.27.0-rc1 soluciona este problema."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:H/UI:R/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.6, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-532"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:authzed:spicedb:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.27.0", "matchCriteriaId": "1339CD3F-78E6-4CCC-B453-9ED4AC5C8F6E"}]}]}], "references": [{"url": "https://github.com/authzed/spicedb/commit/ae50421b80f895e4c98d999b18e06b6f1e6f1cf8", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/authzed/spicedb/security/advisories/GHSA-jg7w-cxjv-98c2", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/authzed/spicedb/commit/ae50421b80f895e4c98d999b18e06b6f1e6f1cf8"}}