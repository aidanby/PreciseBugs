{"buggy_code": ["// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/signal\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\tshimlog \"github.com/containerd/containerd/runtime/v1\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\tshimapi \"github.com/containerd/containerd/runtime/v1/shim/v1\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/containerd/typeurl\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\tdebugFlag            bool\n\tnamespaceFlag        string\n\tsocketFlag           string\n\taddressFlag          string\n\tworkdirFlag          string\n\truntimeRootFlag      string\n\tcriuFlag             string\n\tsystemdCgroupFlag    bool\n\tcontainerdBinaryFlag string\n\n\tbufPool = sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\treturn bytes.NewBuffer(nil)\n\t\t},\n\t}\n)\n\nfunc init() {\n\tflag.BoolVar(&debugFlag, \"debug\", false, \"enable debug output in logs\")\n\tflag.StringVar(&namespaceFlag, \"namespace\", \"\", \"namespace that owns the shim\")\n\tflag.StringVar(&socketFlag, \"socket\", \"\", \"abstract socket path to serve\")\n\tflag.StringVar(&addressFlag, \"address\", \"\", \"grpc address back to main containerd\")\n\tflag.StringVar(&workdirFlag, \"workdir\", \"\", \"path used to storge large temporary data\")\n\tflag.StringVar(&runtimeRootFlag, \"runtime-root\", process.RuncRoot, \"root directory for the runtime\")\n\tflag.StringVar(&criuFlag, \"criu\", \"\", \"path to criu binary\")\n\tflag.BoolVar(&systemdCgroupFlag, \"systemd-cgroup\", false, \"set runtime to use systemd-cgroup\")\n\t// currently, the `containerd publish` utility is embedded in the daemon binary.\n\t// The daemon invokes `containerd-shim -containerd-binary ...` with its own os.Executable() path.\n\tflag.StringVar(&containerdBinaryFlag, \"containerd-binary\", \"containerd\", \"path to containerd binary (used for `containerd publish`)\")\n\tflag.Parse()\n}\n\nfunc main() {\n\tdebug.SetGCPercent(40)\n\tgo func() {\n\t\tfor range time.Tick(30 * time.Second) {\n\t\t\tdebug.FreeOSMemory()\n\t\t}\n\t}()\n\n\tif debugFlag {\n\t\tlogrus.SetLevel(logrus.DebugLevel)\n\t}\n\n\tif os.Getenv(\"GOMAXPROCS\") == \"\" {\n\t\t// If GOMAXPROCS hasn't been set, we default to a value of 2 to reduce\n\t\t// the number of Go stacks present in the shim.\n\t\truntime.GOMAXPROCS(2)\n\t}\n\n\tstdout, stderr, err := openStdioKeepAlivePipes(workdirFlag)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"containerd-shim: %s\\n\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer func() {\n\t\tstdout.Close()\n\t\tstderr.Close()\n\t}()\n\n\t// redirect the following output into fifo to make sure that containerd\n\t// still can read the log after restart\n\tlogrus.SetOutput(stdout)\n\n\tif err := executeShim(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"containerd-shim: %s\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n\n// If containerd server process dies, we need the shim to keep stdout/err reader\n// FDs so that Linux does not SIGPIPE the shim process if it tries to use its end of\n// these pipes.\nfunc openStdioKeepAlivePipes(dir string) (io.ReadWriteCloser, io.ReadWriteCloser, error) {\n\tbackground := context.Background()\n\tkeepStdoutAlive, err := shimlog.OpenShimStdoutLog(background, dir)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tkeepStderrAlive, err := shimlog.OpenShimStderrLog(background, dir)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn keepStdoutAlive, keepStderrAlive, nil\n}\n\nfunc executeShim() error {\n\t// start handling signals as soon as possible so that things are properly reaped\n\t// or if runtime exits before we hit the handler\n\tsignals, err := setupSignals()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdump := make(chan os.Signal, 32)\n\tsignal.Notify(dump, syscall.SIGUSR1)\n\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn err\n\t}\n\tserver, err := newServer()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed creating server\")\n\t}\n\tsv, err := shim.NewService(\n\t\tshim.Config{\n\t\t\tPath:          path,\n\t\t\tNamespace:     namespaceFlag,\n\t\t\tWorkDir:       workdirFlag,\n\t\t\tCriu:          criuFlag,\n\t\t\tSystemdCgroup: systemdCgroupFlag,\n\t\t\tRuntimeRoot:   runtimeRootFlag,\n\t\t},\n\t\t&remoteEventsPublisher{address: addressFlag},\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.Debug(\"registering ttrpc server\")\n\tshimapi.RegisterShimService(server, sv)\n\n\tsocket := socketFlag\n\tif err := serve(context.Background(), server, socket); err != nil {\n\t\treturn err\n\t}\n\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\"pid\":       os.Getpid(),\n\t\t\"path\":      path,\n\t\t\"namespace\": namespaceFlag,\n\t})\n\tgo func() {\n\t\tfor range dump {\n\t\t\tdumpStacks(logger)\n\t\t}\n\t}()\n\treturn handleSignals(logger, signals, server, sv)\n}\n\n// serve serves the ttrpc API over a unix socket at the provided path\n// this function does not block\nfunc serve(ctx context.Context, server *ttrpc.Server, path string) error {\n\tvar (\n\t\tl   net.Listener\n\t\terr error\n\t)\n\tif path == \"\" {\n\t\tf := os.NewFile(3, \"socket\")\n\t\tl, err = net.FileListener(f)\n\t\tf.Close()\n\t\tpath = \"[inherited from parent]\"\n\t} else {\n\t\tif len(path) > 106 {\n\t\t\treturn errors.Errorf(\"%q: unix socket path too long (> 106)\", path)\n\t\t}\n\t\tl, err = net.Listen(\"unix\", \"\\x00\"+path)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.WithField(\"socket\", path).Debug(\"serving api on unix socket\")\n\tgo func() {\n\t\tdefer l.Close()\n\t\tif err := server.Serve(ctx, l); err != nil &&\n\t\t\t!strings.Contains(err.Error(), \"use of closed network connection\") {\n\t\t\tlogrus.WithError(err).Fatal(\"containerd-shim: ttrpc server failure\")\n\t\t}\n\t}()\n\treturn nil\n}\n\nfunc handleSignals(logger *logrus.Entry, signals chan os.Signal, server *ttrpc.Server, sv *shim.Service) error {\n\tvar (\n\t\ttermOnce sync.Once\n\t\tdone     = make(chan struct{})\n\t)\n\n\tfor {\n\t\tselect {\n\t\tcase <-done:\n\t\t\treturn nil\n\t\tcase s := <-signals:\n\t\t\tswitch s {\n\t\t\tcase unix.SIGCHLD:\n\t\t\t\tif err := reaper.Reap(); err != nil {\n\t\t\t\t\tlogger.WithError(err).Error(\"reap exit status\")\n\t\t\t\t}\n\t\t\tcase unix.SIGTERM, unix.SIGINT:\n\t\t\t\tgo termOnce.Do(func() {\n\t\t\t\t\tctx := context.TODO()\n\t\t\t\t\tif err := server.Shutdown(ctx); err != nil {\n\t\t\t\t\t\tlogger.WithError(err).Error(\"failed to shutdown server\")\n\t\t\t\t\t}\n\t\t\t\t\t// Ensure our child is dead if any\n\t\t\t\t\tsv.Kill(ctx, &shimapi.KillRequest{\n\t\t\t\t\t\tSignal: uint32(syscall.SIGKILL),\n\t\t\t\t\t\tAll:    true,\n\t\t\t\t\t})\n\t\t\t\t\tsv.Delete(context.Background(), &ptypes.Empty{})\n\t\t\t\t\tclose(done)\n\t\t\t\t})\n\t\t\tcase unix.SIGPIPE:\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc dumpStacks(logger *logrus.Entry) {\n\tvar (\n\t\tbuf       []byte\n\t\tstackSize int\n\t)\n\tbufferLen := 16384\n\tfor stackSize == len(buf) {\n\t\tbuf = make([]byte, bufferLen)\n\t\tstackSize = runtime.Stack(buf, true)\n\t\tbufferLen *= 2\n\t}\n\tbuf = buf[:stackSize]\n\tlogger.Infof(\"=== BEGIN goroutine stack dump ===\\n%s\\n=== END goroutine stack dump ===\", buf)\n}\n\ntype remoteEventsPublisher struct {\n\taddress string\n}\n\nfunc (l *remoteEventsPublisher) Publish(ctx context.Context, topic string, event events.Event) error {\n\tns, _ := namespaces.Namespace(ctx)\n\tencoded, err := typeurl.MarshalAny(event)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdata, err := encoded.Marshal()\n\tif err != nil {\n\t\treturn err\n\t}\n\tcmd := exec.CommandContext(ctx, containerdBinaryFlag, \"--address\", l.address, \"publish\", \"--topic\", topic, \"--namespace\", ns)\n\tcmd.Stdin = bytes.NewReader(data)\n\tb := bufPool.Get().(*bytes.Buffer)\n\tdefer bufPool.Put(b)\n\tcmd.Stdout = b\n\tcmd.Stderr = b\n\tc, err := reaper.Default.Start(cmd)\n\tif err != nil {\n\t\treturn err\n\t}\n\tstatus, err := reaper.Default.Wait(cmd, c)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"failed to publish event: %s\", b.String())\n\t}\n\tif status != 0 {\n\t\treturn errors.Errorf(\"failed to publish event: %s\", b.String())\n\t}\n\treturn nil\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\tgocontext \"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"path/filepath\"\n\n\t\"github.com/containerd/console\"\n\t\"github.com/containerd/containerd/cmd/ctr/commands\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\t\"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/containerd/typeurl\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/opencontainers/runtime-spec/specs-go\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/urfave/cli\"\n)\n\nvar fifoFlags = []cli.Flag{\n\tcli.StringFlag{\n\t\tName:  \"stdin\",\n\t\tUsage: \"specify the path to the stdin fifo\",\n\t},\n\tcli.StringFlag{\n\t\tName:  \"stdout\",\n\t\tUsage: \"specify the path to the stdout fifo\",\n\t},\n\tcli.StringFlag{\n\t\tName:  \"stderr\",\n\t\tUsage: \"specify the path to the stderr fifo\",\n\t},\n\tcli.BoolFlag{\n\t\tName:  \"tty,t\",\n\t\tUsage: \"enable tty support\",\n\t},\n}\n\n// Command is the cli command for interacting with a task\nvar Command = cli.Command{\n\tName:  \"shim\",\n\tUsage: \"interact with a shim directly\",\n\tFlags: []cli.Flag{\n\t\tcli.StringFlag{\n\t\t\tName:  \"id\",\n\t\t\tUsage: \"container id\",\n\t\t},\n\t},\n\tSubcommands: []cli.Command{\n\t\tdeleteCommand,\n\t\texecCommand,\n\t\tstartCommand,\n\t\tstateCommand,\n\t},\n}\n\nvar startCommand = cli.Command{\n\tName:  \"start\",\n\tUsage: \"start a container with a task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = service.Start(gocontext.Background(), &task.StartRequest{\n\t\t\tID: context.Args().First(),\n\t\t})\n\t\treturn err\n\t},\n}\n\nvar deleteCommand = cli.Command{\n\tName:  \"delete\",\n\tUsage: \"delete a container with a task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.Delete(gocontext.Background(), &task.DeleteRequest{\n\t\t\tID: context.Args().First(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfmt.Printf(\"container deleted and returned exit status %d\\n\", r.ExitStatus)\n\t\treturn nil\n\t},\n}\n\nvar stateCommand = cli.Command{\n\tName:  \"state\",\n\tUsage: \"get the state of all the processes of the task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.State(gocontext.Background(), &task.StateRequest{\n\t\t\tID: context.GlobalString(\"id\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcommands.PrintAsJSON(r)\n\t\treturn nil\n\t},\n}\n\nvar execCommand = cli.Command{\n\tName:  \"exec\",\n\tUsage: \"exec a new process in the task's container\",\n\tFlags: append(fifoFlags,\n\t\tcli.BoolFlag{\n\t\t\tName:  \"attach,a\",\n\t\t\tUsage: \"stay attached to the container and open the fifos\",\n\t\t},\n\t\tcli.StringSliceFlag{\n\t\t\tName:  \"env,e\",\n\t\t\tUsage: \"add environment vars\",\n\t\t\tValue: &cli.StringSlice{},\n\t\t},\n\t\tcli.StringFlag{\n\t\t\tName:  \"cwd\",\n\t\t\tUsage: \"current working directory\",\n\t\t},\n\t\tcli.StringFlag{\n\t\t\tName:  \"spec\",\n\t\t\tUsage: \"runtime spec\",\n\t\t},\n\t),\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar (\n\t\t\tid  = context.Args().First()\n\t\t\tctx = gocontext.Background()\n\t\t)\n\n\t\tif id == \"\" {\n\t\t\treturn errors.New(\"exec id must be provided\")\n\t\t}\n\n\t\ttty := context.Bool(\"tty\")\n\t\twg, err := prepareStdio(context.String(\"stdin\"), context.String(\"stdout\"), context.String(\"stderr\"), tty)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// read spec file and extract Any object\n\t\tspec, err := ioutil.ReadFile(context.String(\"spec\"))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\turl, err := typeurl.TypeURL(specs.Process{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trq := &task.ExecProcessRequest{\n\t\t\tID: id,\n\t\t\tSpec: &ptypes.Any{\n\t\t\t\tTypeUrl: url,\n\t\t\t\tValue:   spec,\n\t\t\t},\n\t\t\tStdin:    context.String(\"stdin\"),\n\t\t\tStdout:   context.String(\"stdout\"),\n\t\t\tStderr:   context.String(\"stderr\"),\n\t\t\tTerminal: tty,\n\t\t}\n\t\tif _, err := service.Exec(ctx, rq); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.Start(ctx, &task.StartRequest{\n\t\t\tID: id,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfmt.Printf(\"exec running with pid %d\\n\", r.Pid)\n\t\tif context.Bool(\"attach\") {\n\t\t\tlogrus.Info(\"attaching\")\n\t\t\tif tty {\n\t\t\t\tcurrent := console.Current()\n\t\t\t\tdefer current.Reset()\n\t\t\t\tif err := current.SetRaw(); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tsize, err := current.Size()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif _, err := service.ResizePty(ctx, &task.ResizePtyRequest{\n\t\t\t\t\tID:     id,\n\t\t\t\t\tWidth:  uint32(size.Width),\n\t\t\t\t\tHeight: uint32(size.Height),\n\t\t\t\t}); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\twg.Wait()\n\t\t}\n\t\treturn nil\n\t},\n}\n\nfunc getTaskService(context *cli.Context) (task.TaskService, error) {\n\tid := context.GlobalString(\"id\")\n\tif id == \"\" {\n\t\treturn nil, fmt.Errorf(\"container id must be specified\")\n\t}\n\tns := context.GlobalString(\"namespace\")\n\n\t// /containerd-shim/ns/id/shim.sock is the old way to generate shim socket,\n\t// compatible it\n\ts1 := filepath.Join(string(filepath.Separator), \"containerd-shim\", ns, id, \"shim.sock\")\n\t// this should not error, ctr always get a default ns\n\tctx := namespaces.WithNamespace(gocontext.Background(), ns)\n\ts2, _ := shim.SocketAddress(ctx, id)\n\n\tfor _, socket := range []string{s1, s2} {\n\t\tconn, err := net.Dial(\"unix\", \"\\x00\"+socket)\n\t\tif err == nil {\n\t\t\tclient := ttrpc.NewClient(conn)\n\n\t\t\t// TODO(stevvooe): This actually leaks the connection. We were leaking it\n\t\t\t// before, so may not be a huge deal.\n\n\t\t\treturn task.NewTaskClient(client), nil\n\t\t}\n\t}\n\n\treturn nil, fmt.Errorf(\"fail to connect to container %s's shim\", id)\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage linux\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/containerd/containerd/events/exchange\"\n\t\"github.com/containerd/containerd/runtime/linux/runctypes\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\t\"github.com/containerd/containerd/runtime/v1/shim/client\"\n\t\"github.com/pkg/errors\"\n)\n\n// loadBundle loads an existing bundle from disk\nfunc loadBundle(id, path, workdir string) *bundle {\n\treturn &bundle{\n\t\tid:      id,\n\t\tpath:    path,\n\t\tworkDir: workdir,\n\t}\n}\n\n// newBundle creates a new bundle on disk at the provided path for the given id\nfunc newBundle(id, path, workDir string, spec []byte) (b *bundle, err error) {\n\tif err := os.MkdirAll(path, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tpath = filepath.Join(path, id)\n\tif err := os.Mkdir(path, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(path)\n\t\t}\n\t}()\n\tworkDir = filepath.Join(workDir, id)\n\tif err := os.MkdirAll(workDir, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(workDir)\n\t\t}\n\t}()\n\trootfs := filepath.Join(path, \"rootfs\")\n\tif err := os.MkdirAll(rootfs, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\terr = ioutil.WriteFile(filepath.Join(path, configFilename), spec, 0666)\n\treturn &bundle{\n\t\tid:      id,\n\t\tpath:    path,\n\t\tworkDir: workDir,\n\t}, err\n}\n\ntype bundle struct {\n\tid      string\n\tpath    string\n\tworkDir string\n}\n\n// ShimOpt specifies shim options for initialization and connection\ntype ShimOpt func(*bundle, string, *runctypes.RuncOptions) (shim.Config, client.Opt)\n\n// ShimRemote is a ShimOpt for connecting and starting a remote shim\nfunc ShimRemote(c *Config, daemonAddress, cgroup string, exitHandler func()) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\tconfig := b.shimConfig(ns, c, ropts)\n\t\treturn config,\n\t\t\tclient.WithStart(c.Shim, b.shimAddress(ns), daemonAddress, cgroup, c.ShimDebug, exitHandler)\n\t}\n}\n\n// ShimLocal is a ShimOpt for using an in process shim implementation\nfunc ShimLocal(c *Config, exchange *exchange.Exchange) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\treturn b.shimConfig(ns, c, ropts), client.WithLocal(exchange)\n\t}\n}\n\n// ShimConnect is a ShimOpt for connecting to an existing remote shim\nfunc ShimConnect(c *Config, onClose func()) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\treturn b.shimConfig(ns, c, ropts), client.WithConnect(b.decideShimAddress(ns), onClose)\n\t}\n}\n\n// NewShimClient connects to the shim managing the bundle and tasks creating it if needed\nfunc (b *bundle) NewShimClient(ctx context.Context, namespace string, getClientOpts ShimOpt, runcOpts *runctypes.RuncOptions) (*client.Client, error) {\n\tcfg, opt := getClientOpts(b, namespace, runcOpts)\n\treturn client.New(ctx, cfg, opt)\n}\n\n// Delete deletes the bundle from disk\nfunc (b *bundle) Delete() error {\n\terr := atomicDelete(b.path)\n\tif err == nil {\n\t\treturn atomicDelete(b.workDir)\n\t}\n\t// error removing the bundle path; still attempt removing work dir\n\terr2 := atomicDelete(b.workDir)\n\tif err2 == nil {\n\t\treturn err\n\t}\n\treturn errors.Wrapf(err, \"Failed to remove both bundle and workdir locations: %v\", err2)\n}\n\nfunc (b *bundle) legacyShimAddress(namespace string) string {\n\treturn filepath.Join(string(filepath.Separator), \"containerd-shim\", namespace, b.id, \"shim.sock\")\n}\n\nfunc (b *bundle) shimAddress(namespace string) string {\n\td := sha256.Sum256([]byte(filepath.Join(namespace, b.id)))\n\treturn filepath.Join(string(filepath.Separator), \"containerd-shim\", fmt.Sprintf(\"%x.sock\", d))\n}\n\nfunc (b *bundle) loadAddress() (string, error) {\n\taddressPath := filepath.Join(b.path, \"address\")\n\tdata, err := ioutil.ReadFile(addressPath)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(data), nil\n}\n\nfunc (b *bundle) decideShimAddress(namespace string) string {\n\taddress, err := b.loadAddress()\n\tif err != nil {\n\t\treturn b.legacyShimAddress(namespace)\n\t}\n\treturn address\n}\n\nfunc (b *bundle) shimConfig(namespace string, c *Config, runcOptions *runctypes.RuncOptions) shim.Config {\n\tvar (\n\t\tcriuPath      string\n\t\truntimeRoot   = c.RuntimeRoot\n\t\tsystemdCgroup bool\n\t)\n\tif runcOptions != nil {\n\t\tcriuPath = runcOptions.CriuPath\n\t\tsystemdCgroup = runcOptions.SystemdCgroup\n\t\tif runcOptions.RuntimeRoot != \"\" {\n\t\t\truntimeRoot = runcOptions.RuntimeRoot\n\t\t}\n\t}\n\treturn shim.Config{\n\t\tPath:          b.path,\n\t\tWorkDir:       b.workDir,\n\t\tNamespace:     namespace,\n\t\tCriu:          criuPath,\n\t\tRuntimeRoot:   runtimeRoot,\n\t\tSystemdCgroup: systemdCgroup,\n\t}\n}\n\n// atomicDelete renames the path to a hidden file before removal\nfunc atomicDelete(path string) error {\n\t// create a hidden dir for an atomic removal\n\tatomicPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tif err := os.Rename(path, atomicPath); err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\treturn os.RemoveAll(atomicPath)\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"golang.org/x/sys/unix\"\n\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\tv1 \"github.com/containerd/containerd/runtime/v1\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\tshimapi \"github.com/containerd/containerd/runtime/v1/shim/v1\"\n\t\"github.com/containerd/containerd/sys\"\n\tptypes \"github.com/gogo/protobuf/types\"\n)\n\nvar empty = &ptypes.Empty{}\n\n// Opt is an option for a shim client configuration\ntype Opt func(context.Context, shim.Config) (shimapi.ShimService, io.Closer, error)\n\n// WithStart executes a new shim process\nfunc WithStart(binary, address, daemonAddress, cgroup string, debug bool, exitHandler func()) Opt {\n\treturn func(ctx context.Context, config shim.Config) (_ shimapi.ShimService, _ io.Closer, err error) {\n\t\tsocket, err := newSocket(address)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tdefer socket.Close()\n\t\tf, err := socket.File()\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to get fd for socket %s\", address)\n\t\t}\n\t\tdefer f.Close()\n\n\t\tstdoutCopy := ioutil.Discard\n\t\tstderrCopy := ioutil.Discard\n\t\tstdoutLog, err := v1.OpenShimStdoutLog(ctx, config.WorkDir)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to create stdout log\")\n\t\t}\n\n\t\tstderrLog, err := v1.OpenShimStderrLog(ctx, config.WorkDir)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to create stderr log\")\n\t\t}\n\t\tif debug {\n\t\t\tstdoutCopy = os.Stdout\n\t\t\tstderrCopy = os.Stderr\n\t\t}\n\n\t\tgo io.Copy(stdoutCopy, stdoutLog)\n\t\tgo io.Copy(stderrCopy, stderrLog)\n\n\t\tcmd, err := newCommand(binary, daemonAddress, debug, config, f, stdoutLog, stderrLog)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif err := cmd.Start(); err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to start shim\")\n\t\t}\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tcmd.Process.Kill()\n\t\t\t}\n\t\t}()\n\t\tgo func() {\n\t\t\tcmd.Wait()\n\t\t\texitHandler()\n\t\t\tif stdoutLog != nil {\n\t\t\t\tstdoutLog.Close()\n\t\t\t}\n\t\t\tif stderrLog != nil {\n\t\t\t\tstderrLog.Close()\n\t\t\t}\n\t\t}()\n\t\tlog.G(ctx).WithFields(logrus.Fields{\n\t\t\t\"pid\":     cmd.Process.Pid,\n\t\t\t\"address\": address,\n\t\t\t\"debug\":   debug,\n\t\t}).Infof(\"shim %s started\", binary)\n\n\t\tif err := writeFile(filepath.Join(config.Path, \"address\"), address); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif err := writeFile(filepath.Join(config.Path, \"shim.pid\"), strconv.Itoa(cmd.Process.Pid)); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\t// set shim in cgroup if it is provided\n\t\tif cgroup != \"\" {\n\t\t\tif err := setCgroup(cgroup, cmd); err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t\tlog.G(ctx).WithFields(logrus.Fields{\n\t\t\t\t\"pid\":     cmd.Process.Pid,\n\t\t\t\t\"address\": address,\n\t\t\t}).Infof(\"shim placed in cgroup %s\", cgroup)\n\t\t}\n\t\tif err = setupOOMScore(cmd.Process.Pid); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tc, clo, err := WithConnect(address, func() {})(ctx, config)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"failed to connect\")\n\t\t}\n\t\treturn c, clo, nil\n\t}\n}\n\n// setupOOMScore gets containerd's oom score and adds +1 to it\n// to ensure a shim has a lower* score than the daemons\nfunc setupOOMScore(shimPid int) error {\n\tpid := os.Getpid()\n\tscore, err := sys.GetOOMScoreAdj(pid)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"get daemon OOM score\")\n\t}\n\tshimScore := score + 1\n\tif err := sys.SetOOMScore(shimPid, shimScore); err != nil {\n\t\treturn errors.Wrap(err, \"set shim OOM score\")\n\t}\n\treturn nil\n}\n\nfunc newCommand(binary, daemonAddress string, debug bool, config shim.Config, socket *os.File, stdout, stderr io.Writer) (*exec.Cmd, error) {\n\tselfExe, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", config.Namespace,\n\t\t\"-workdir\", config.WorkDir,\n\t\t\"-address\", daemonAddress,\n\t\t\"-containerd-binary\", selfExe,\n\t}\n\n\tif config.Criu != \"\" {\n\t\targs = append(args, \"-criu-path\", config.Criu)\n\t}\n\tif config.RuntimeRoot != \"\" {\n\t\targs = append(args, \"-runtime-root\", config.RuntimeRoot)\n\t}\n\tif config.SystemdCgroup {\n\t\targs = append(args, \"-systemd-cgroup\")\n\t}\n\tif debug {\n\t\targs = append(args, \"-debug\")\n\t}\n\n\tcmd := exec.Command(binary, args...)\n\tcmd.Dir = config.Path\n\t// make sure the shim can be re-parented to system init\n\t// and is cloned in a new mount namespace because the overlay/filesystems\n\t// will be mounted by the shim\n\tcmd.SysProcAttr = getSysProcAttr()\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, socket)\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=2\")\n\tcmd.Stdout = stdout\n\tcmd.Stderr = stderr\n\treturn cmd, nil\n}\n\n// writeFile writes a address file atomically\nfunc writeFile(path, address string) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = f.WriteString(address)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\nfunc newSocket(address string) (*net.UnixListener, error) {\n\tif len(address) > 106 {\n\t\treturn nil, errors.Errorf(\"%q: unix socket path too long (> 106)\", address)\n\t}\n\tl, err := net.Listen(\"unix\", \"\\x00\"+address)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to listen to abstract unix socket %q\", address)\n\t}\n\n\treturn l.(*net.UnixListener), nil\n}\n\nfunc connect(address string, d func(string, time.Duration) (net.Conn, error)) (net.Conn, error) {\n\treturn d(address, 100*time.Second)\n}\n\nfunc annonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\taddress = strings.TrimPrefix(address, \"unix://\")\n\treturn dialer.Dialer(\"\\x00\"+address, timeout)\n}\n\n// WithConnect connects to an existing shim\nfunc WithConnect(address string, onClose func()) Opt {\n\treturn func(ctx context.Context, config shim.Config) (shimapi.ShimService, io.Closer, error) {\n\t\tconn, err := connect(address, annonDialer)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tclient := ttrpc.NewClient(conn, ttrpc.WithOnClose(onClose))\n\t\treturn shimapi.NewShimClient(client), conn, nil\n\t}\n}\n\n// WithLocal uses an in process shim\nfunc WithLocal(publisher events.Publisher) func(context.Context, shim.Config) (shimapi.ShimService, io.Closer, error) {\n\treturn func(ctx context.Context, config shim.Config) (shimapi.ShimService, io.Closer, error) {\n\t\tservice, err := shim.NewService(config, publisher)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn shim.NewLocal(service), nil, nil\n\t}\n}\n\n// New returns a new shim client\nfunc New(ctx context.Context, config shim.Config, opt Opt) (*Client, error) {\n\ts, c, err := opt(ctx, config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Client{\n\t\tShimService: s,\n\t\tc:           c,\n\t\texitCh:      make(chan struct{}),\n\t}, nil\n}\n\n// Client is a shim client containing the connection to a shim\ntype Client struct {\n\tshimapi.ShimService\n\n\tc        io.Closer\n\texitCh   chan struct{}\n\texitOnce sync.Once\n}\n\n// IsAlive returns true if the shim can be contacted.\n// NOTE: a negative answer doesn't mean that the process is gone.\nfunc (c *Client) IsAlive(ctx context.Context) (bool, error) {\n\t_, err := c.ShimInfo(ctx, empty)\n\tif err != nil {\n\t\t// TODO(stevvooe): There are some error conditions that need to be\n\t\t// handle with unix sockets existence to give the right answer here.\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\n// StopShim signals the shim to exit and wait for the process to disappear\nfunc (c *Client) StopShim(ctx context.Context) error {\n\treturn c.signalShim(ctx, unix.SIGTERM)\n}\n\n// KillShim kills the shim forcefully and wait for the process to disappear\nfunc (c *Client) KillShim(ctx context.Context) error {\n\treturn c.signalShim(ctx, unix.SIGKILL)\n}\n\n// Close the client connection\nfunc (c *Client) Close() error {\n\tif c.c == nil {\n\t\treturn nil\n\t}\n\treturn c.c.Close()\n}\n\nfunc (c *Client) signalShim(ctx context.Context, sig syscall.Signal) error {\n\tinfo, err := c.ShimInfo(ctx, empty)\n\tif err != nil {\n\t\treturn err\n\t}\n\tpid := int(info.ShimPid)\n\t// make sure we don't kill ourselves if we are running a local shim\n\tif os.Getpid() == pid {\n\t\treturn nil\n\t}\n\tif err := unix.Kill(pid, sig); err != nil && err != unix.ESRCH {\n\t\treturn err\n\t}\n\t// wait for shim to die after being signaled\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\tcase <-c.waitForExit(ctx, pid):\n\t\treturn nil\n\t}\n}\n\nfunc (c *Client) waitForExit(ctx context.Context, pid int) <-chan struct{} {\n\tgo c.exitOnce.Do(func() {\n\t\tdefer close(c.exitCh)\n\n\t\tticker := time.NewTicker(10 * time.Millisecond)\n\t\tdefer ticker.Stop()\n\n\t\tfor {\n\t\t\t// use kill(pid, 0) here because the shim could have been reparented\n\t\t\t// and we are no longer able to waitpid(pid, ...) on the shim\n\t\t\tif err := unix.Kill(pid, 0); err == unix.ESRCH {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.G(ctx).WithField(\"pid\", pid).Warn(\"timed out while waiting for shim to exit\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\treturn c.exitCh\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage v1\n\nimport (\n\t\"context\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\teventstypes \"github.com/containerd/containerd/api/events\"\n\t\"github.com/containerd/containerd/api/types/task\"\n\t\"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/mount\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/oom\"\n\toomv1 \"github.com/containerd/containerd/pkg/oom/v1\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\t\"github.com/containerd/containerd/pkg/stdio\"\n\t\"github.com/containerd/containerd/runtime/v2/runc\"\n\t\"github.com/containerd/containerd/runtime/v2/runc/options\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\ttaskAPI \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\truncC \"github.com/containerd/go-runc\"\n\t\"github.com/containerd/typeurl\"\n\t\"github.com/gogo/protobuf/proto\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t_     = (taskAPI.TaskService)(&service{})\n\tempty = &ptypes.Empty{}\n)\n\n// New returns a new shim service that can be used via GRPC\nfunc New(ctx context.Context, id string, publisher shim.Publisher, shutdown func()) (shim.Shim, error) {\n\tep, err := oomv1.New(publisher)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tgo ep.Run(ctx)\n\ts := &service{\n\t\tid:      id,\n\t\tcontext: ctx,\n\t\tevents:  make(chan interface{}, 128),\n\t\tec:      reaper.Default.Subscribe(),\n\t\tep:      ep,\n\t\tcancel:  shutdown,\n\t}\n\tgo s.processExits()\n\truncC.Monitor = reaper.Default\n\tif err := s.initPlatform(); err != nil {\n\t\tshutdown()\n\t\treturn nil, errors.Wrap(err, \"failed to initialized platform behavior\")\n\t}\n\tgo s.forward(ctx, publisher)\n\treturn s, nil\n}\n\n// service is the shim implementation of a remote shim over GRPC\ntype service struct {\n\tmu          sync.Mutex\n\teventSendMu sync.Mutex\n\n\tcontext  context.Context\n\tevents   chan interface{}\n\tplatform stdio.Platform\n\tec       chan runcC.Exit\n\tep       oom.Watcher\n\n\tid        string\n\tcontainer *runc.Container\n\n\tcancel func()\n}\n\nfunc newCommand(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-id\", id,\n\t\t\"-address\", containerdAddress,\n\t}\n\tcmd := exec.Command(self, args...)\n\tcmd.Dir = cwd\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=2\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n\treturn cmd, nil\n}\n\nfunc (s *service) StartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (string, error) {\n\tcmd, err := newCommand(ctx, id, containerdBinary, containerdAddress, containerdTTRPCAddress)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\taddress, err := shim.SocketAddress(ctx, id)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tsocket, err := shim.NewSocket(address)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer socket.Close()\n\tf, err := socket.File()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer f.Close()\n\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, f)\n\n\tif err := cmd.Start(); err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tcmd.Process.Kill()\n\t\t}\n\t}()\n\t// make sure to wait after start\n\tgo cmd.Wait()\n\tif err := shim.WritePidFile(\"shim.pid\", cmd.Process.Pid); err != nil {\n\t\treturn \"\", err\n\t}\n\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\treturn \"\", err\n\t}\n\tif data, err := ioutil.ReadAll(os.Stdin); err == nil {\n\t\tif len(data) > 0 {\n\t\t\tvar any ptypes.Any\n\t\t\tif err := proto.Unmarshal(data, &any); err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tv, err := typeurl.UnmarshalAny(&any)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tif opts, ok := v.(*options.Options); ok {\n\t\t\t\tif opts.ShimCgroup != \"\" {\n\t\t\t\t\tcg, err := cgroups.Load(cgroups.V1, cgroups.StaticPath(opts.ShimCgroup))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t}\n\t\t\t\t\tif err := cg.Add(cgroups.Process{\n\t\t\t\t\t\tPid: cmd.Process.Pid,\n\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif err := shim.AdjustOOMScore(cmd.Process.Pid); err != nil {\n\t\treturn \"\", errors.Wrap(err, \"failed to adjust OOM score for shim\")\n\t}\n\treturn address, nil\n}\n\nfunc (s *service) Cleanup(ctx context.Context) (*taskAPI.DeleteResponse, error) {\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\truntime, err := runc.ReadRuntime(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\topts, err := runc.ReadOptions(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\troot := process.RuncRoot\n\tif opts != nil && opts.Root != \"\" {\n\t\troot = opts.Root\n\t}\n\n\tr := process.NewRunc(root, path, ns, runtime, \"\", false)\n\tif err := r.Delete(ctx, s.id, &runcC.DeleteOpts{\n\t\tForce: true,\n\t}); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to remove runc container\")\n\t}\n\tif err := mount.UnmountAll(filepath.Join(path, \"rootfs\"), 0); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to cleanup rootfs mount\")\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitedAt:   time.Now(),\n\t\tExitStatus: 128 + uint32(unix.SIGKILL),\n\t}, nil\n}\n\n// Create a new initial process and container with the underlying OCI runtime\nfunc (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tcontainer, err := runc.NewContainer(ctx, s.platform, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.container = container\n\n\ts.send(&eventstypes.TaskCreate{\n\t\tContainerID: r.ID,\n\t\tBundle:      r.Bundle,\n\t\tRootfs:      r.Rootfs,\n\t\tIO: &eventstypes.TaskIO{\n\t\t\tStdin:    r.Stdin,\n\t\t\tStdout:   r.Stdout,\n\t\t\tStderr:   r.Stderr,\n\t\t\tTerminal: r.Terminal,\n\t\t},\n\t\tCheckpoint: r.Checkpoint,\n\t\tPid:        uint32(container.Pid()),\n\t})\n\n\treturn &taskAPI.CreateTaskResponse{\n\t\tPid: uint32(container.Pid()),\n\t}, nil\n}\n\n// Start a process\nfunc (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// hold the send lock so that the start events are sent before any exit events in the error case\n\ts.eventSendMu.Lock()\n\tp, err := container.Start(ctx, r)\n\tif err != nil {\n\t\ts.eventSendMu.Unlock()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tswitch r.ExecID {\n\tcase \"\":\n\t\tif cg, ok := container.Cgroup().(cgroups.Cgroup); ok {\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.WithError(errdefs.ErrNotImplemented).Error(\"add cg to OOM monitor\")\n\t\t}\n\t\ts.send(&eventstypes.TaskStart{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\tdefault:\n\t\ts.send(&eventstypes.TaskExecStarted{\n\t\t\tContainerID: container.ID,\n\t\t\tExecID:      r.ExecID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\t}\n\ts.eventSendMu.Unlock()\n\treturn &taskAPI.StartResponse{\n\t\tPid: uint32(p.Pid()),\n\t}, nil\n}\n\n// Delete the initial process and container\nfunc (s *service) Delete(ctx context.Context, r *taskAPI.DeleteRequest) (*taskAPI.DeleteResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Delete(ctx, r)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\t// if we deleted our init task, close the platform and send the task delete event\n\tif r.ExecID == \"\" {\n\t\tif s.platform != nil {\n\t\t\ts.platform.Close()\n\t\t}\n\t\ts.send(&eventstypes.TaskDelete{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t\tExitStatus:  uint32(p.ExitStatus()),\n\t\t\tExitedAt:    p.ExitedAt(),\n\t\t})\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t\tPid:        uint32(p.Pid()),\n\t}, nil\n}\n\n// Exec an additional process inside the container\nfunc (s *service) Exec(ctx context.Context, r *taskAPI.ExecProcessRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tok, cancel := container.ReserveProcess(r.ExecID)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrAlreadyExists, \"id %s\", r.ExecID)\n\t}\n\tprocess, err := container.Exec(ctx, r)\n\tif err != nil {\n\t\tcancel()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\ts.send(&eventstypes.TaskExecAdded{\n\t\tContainerID: s.container.ID,\n\t\tExecID:      process.ID(),\n\t})\n\treturn empty, nil\n}\n\n// ResizePty of a process\nfunc (s *service) ResizePty(ctx context.Context, r *taskAPI.ResizePtyRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.ResizePty(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// State returns runtime state information for a process\nfunc (s *service) State(ctx context.Context, r *taskAPI.StateRequest) (*taskAPI.StateResponse, error) {\n\tp, err := s.getProcess(r.ExecID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tst, err := p.Status(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstatus := task.StatusUnknown\n\tswitch st {\n\tcase \"created\":\n\t\tstatus = task.StatusCreated\n\tcase \"running\":\n\t\tstatus = task.StatusRunning\n\tcase \"stopped\":\n\t\tstatus = task.StatusStopped\n\tcase \"paused\":\n\t\tstatus = task.StatusPaused\n\tcase \"pausing\":\n\t\tstatus = task.StatusPausing\n\t}\n\tsio := p.Stdio()\n\treturn &taskAPI.StateResponse{\n\t\tID:         p.ID(),\n\t\tBundle:     s.container.Bundle,\n\t\tPid:        uint32(p.Pid()),\n\t\tStatus:     status,\n\t\tStdin:      sio.Stdin,\n\t\tStdout:     sio.Stdout,\n\t\tStderr:     sio.Stderr,\n\t\tTerminal:   sio.Terminal,\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Pause the container\nfunc (s *service) Pause(ctx context.Context, r *taskAPI.PauseRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Pause(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskPaused{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Resume the container\nfunc (s *service) Resume(ctx context.Context, r *taskAPI.ResumeRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Resume(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskResumed{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Kill a process with the provided signal\nfunc (s *service) Kill(ctx context.Context, r *taskAPI.KillRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Kill(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Pids returns all pids inside the container\nfunc (s *service) Pids(ctx context.Context, r *taskAPI.PidsRequest) (*taskAPI.PidsResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids, err := s.getContainerPids(ctx, r.ID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tvar processes []*task.ProcessInfo\n\tfor _, pid := range pids {\n\t\tpInfo := task.ProcessInfo{\n\t\t\tPid: pid,\n\t\t}\n\t\tfor _, p := range container.ExecdProcesses() {\n\t\t\tif p.Pid() == int(pid) {\n\t\t\t\td := &options.ProcessDetails{\n\t\t\t\t\tExecID: p.ID(),\n\t\t\t\t}\n\t\t\t\ta, err := typeurl.MarshalAny(d)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrapf(err, \"failed to marshal process %d info\", pid)\n\t\t\t\t}\n\t\t\t\tpInfo.Info = a\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tprocesses = append(processes, &pInfo)\n\t}\n\treturn &taskAPI.PidsResponse{\n\t\tProcesses: processes,\n\t}, nil\n}\n\n// CloseIO of a process\nfunc (s *service) CloseIO(ctx context.Context, r *taskAPI.CloseIORequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.CloseIO(ctx, r); err != nil {\n\t\treturn nil, err\n\t}\n\treturn empty, nil\n}\n\n// Checkpoint the container\nfunc (s *service) Checkpoint(ctx context.Context, r *taskAPI.CheckpointTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Checkpoint(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Update a running container\nfunc (s *service) Update(ctx context.Context, r *taskAPI.UpdateTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Update(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Wait for a process to exit\nfunc (s *service) Wait(ctx context.Context, r *taskAPI.WaitRequest) (*taskAPI.WaitResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tp.Wait()\n\n\treturn &taskAPI.WaitResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Connect returns shim information such as the shim's pid\nfunc (s *service) Connect(ctx context.Context, r *taskAPI.ConnectRequest) (*taskAPI.ConnectResponse, error) {\n\tvar pid int\n\tif s.container != nil {\n\t\tpid = s.container.Pid()\n\t}\n\treturn &taskAPI.ConnectResponse{\n\t\tShimPid: uint32(os.Getpid()),\n\t\tTaskPid: uint32(pid),\n\t}, nil\n}\n\nfunc (s *service) Shutdown(ctx context.Context, r *taskAPI.ShutdownRequest) (*ptypes.Empty, error) {\n\ts.cancel()\n\tclose(s.events)\n\treturn empty, nil\n}\n\nfunc (s *service) Stats(ctx context.Context, r *taskAPI.StatsRequest) (*taskAPI.StatsResponse, error) {\n\tcgx := s.container.Cgroup()\n\tif cgx == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tcg, ok := cgx.(cgroups.Cgroup)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotImplemented, \"cgroup v2 not implemented for Stats\")\n\t}\n\tif cg == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tstats, err := cg.Stat(cgroups.IgnoreNotExist)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdata, err := typeurl.MarshalAny(stats)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &taskAPI.StatsResponse{\n\t\tStats: data,\n\t}, nil\n}\n\nfunc (s *service) processExits() {\n\tfor e := range s.ec {\n\t\ts.checkProcesses(e)\n\t}\n}\n\nfunc (s *service) send(evt interface{}) {\n\ts.events <- evt\n}\n\nfunc (s *service) sendL(evt interface{}) {\n\ts.eventSendMu.Lock()\n\ts.events <- evt\n\ts.eventSendMu.Unlock()\n}\n\nfunc (s *service) checkProcesses(e runcC.Exit) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn\n\t}\n\n\tfor _, p := range container.All() {\n\t\tif p.Pid() == e.Pid {\n\t\t\tif runc.ShouldKillAllOnExit(s.context, container.Bundle) {\n\t\t\t\tif ip, ok := p.(*process.Init); ok {\n\t\t\t\t\t// Ensure all children are killed\n\t\t\t\t\tif err := ip.KillAll(s.context); err != nil {\n\t\t\t\t\t\tlogrus.WithError(err).WithField(\"id\", ip.ID()).\n\t\t\t\t\t\t\tError(\"failed to kill init's children\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.SetExited(e.Status)\n\t\t\ts.sendL(&eventstypes.TaskExit{\n\t\t\t\tContainerID: container.ID,\n\t\t\t\tID:          p.ID(),\n\t\t\t\tPid:         uint32(e.Pid),\n\t\t\t\tExitStatus:  uint32(e.Status),\n\t\t\t\tExitedAt:    p.ExitedAt(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s *service) getContainerPids(ctx context.Context, id string) ([]uint32, error) {\n\tp, err := s.container.Process(\"\")\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tps, err := p.(*process.Init).Runtime().Ps(ctx, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids := make([]uint32, 0, len(ps))\n\tfor _, pid := range ps {\n\t\tpids = append(pids, uint32(pid))\n\t}\n\treturn pids, nil\n}\n\nfunc (s *service) forward(ctx context.Context, publisher shim.Publisher) {\n\tns, _ := namespaces.Namespace(ctx)\n\tctx = namespaces.WithNamespace(context.Background(), ns)\n\tfor e := range s.events {\n\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\terr := publisher.Publish(ctx, runc.GetTopic(e), e)\n\t\tcancel()\n\t\tif err != nil {\n\t\t\tlogrus.WithError(err).Error(\"post event\")\n\t\t}\n\t}\n\tpublisher.Close()\n}\n\nfunc (s *service) getContainer() (*runc.Container, error) {\n\ts.mu.Lock()\n\tcontainer := s.container\n\ts.mu.Unlock()\n\tif container == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"container not created\")\n\t}\n\treturn container, nil\n}\n\nfunc (s *service) getProcess(execID string) (process.Process, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(execID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn p, nil\n}\n\n// initialize a single epoll fd to manage our consoles. `initPlatform` should\n// only be called once.\nfunc (s *service) initPlatform() error {\n\tif s.platform != nil {\n\t\treturn nil\n\t}\n\tp, err := runc.NewPlatform()\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.platform = p\n\treturn nil\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage v2\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\tcgroupsv2 \"github.com/containerd/cgroups/v2\"\n\teventstypes \"github.com/containerd/containerd/api/events\"\n\t\"github.com/containerd/containerd/api/types/task\"\n\t\"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/mount\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/oom\"\n\toomv1 \"github.com/containerd/containerd/pkg/oom/v1\"\n\toomv2 \"github.com/containerd/containerd/pkg/oom/v2\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\t\"github.com/containerd/containerd/pkg/stdio\"\n\t\"github.com/containerd/containerd/runtime/v2/runc\"\n\t\"github.com/containerd/containerd/runtime/v2/runc/options\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\ttaskAPI \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\truncC \"github.com/containerd/go-runc\"\n\t\"github.com/containerd/typeurl\"\n\t\"github.com/gogo/protobuf/proto\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t_     = (taskAPI.TaskService)(&service{})\n\tempty = &ptypes.Empty{}\n)\n\n// group labels specifies how the shim groups services.\n// currently supports a runc.v2 specific .group label and the\n// standard k8s pod label.  Order matters in this list\nvar groupLabels = []string{\n\t\"io.containerd.runc.v2.group\",\n\t\"io.kubernetes.cri.sandbox-id\",\n}\n\ntype spec struct {\n\tAnnotations map[string]string `json:\"annotations,omitempty\"`\n}\n\n// New returns a new shim service that can be used via GRPC\nfunc New(ctx context.Context, id string, publisher shim.Publisher, shutdown func()) (shim.Shim, error) {\n\tvar (\n\t\tep  oom.Watcher\n\t\terr error\n\t)\n\tif cgroups.Mode() == cgroups.Unified {\n\t\tep, err = oomv2.New(publisher)\n\t} else {\n\t\tep, err = oomv1.New(publisher)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tgo ep.Run(ctx)\n\ts := &service{\n\t\tid:         id,\n\t\tcontext:    ctx,\n\t\tevents:     make(chan interface{}, 128),\n\t\tec:         reaper.Default.Subscribe(),\n\t\tep:         ep,\n\t\tcancel:     shutdown,\n\t\tcontainers: make(map[string]*runc.Container),\n\t}\n\tgo s.processExits()\n\truncC.Monitor = reaper.Default\n\tif err := s.initPlatform(); err != nil {\n\t\tshutdown()\n\t\treturn nil, errors.Wrap(err, \"failed to initialized platform behavior\")\n\t}\n\tgo s.forward(ctx, publisher)\n\treturn s, nil\n}\n\n// service is the shim implementation of a remote shim over GRPC\ntype service struct {\n\tmu          sync.Mutex\n\teventSendMu sync.Mutex\n\n\tcontext  context.Context\n\tevents   chan interface{}\n\tplatform stdio.Platform\n\tec       chan runcC.Exit\n\tep       oom.Watcher\n\n\t// id only used in cleanup case\n\tid string\n\n\tcontainers map[string]*runc.Container\n\n\tcancel func()\n}\n\nfunc newCommand(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-id\", id,\n\t\t\"-address\", containerdAddress,\n\t}\n\tcmd := exec.Command(self, args...)\n\tcmd.Dir = cwd\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=4\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n\treturn cmd, nil\n}\n\nfunc readSpec() (*spec, error) {\n\tf, err := os.Open(\"config.json\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\tvar s spec\n\tif err := json.NewDecoder(f).Decode(&s); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &s, nil\n}\n\nfunc (s *service) StartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (_ string, retErr error) {\n\tcmd, err := newCommand(ctx, id, containerdBinary, containerdAddress, containerdTTRPCAddress)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tgrouping := id\n\tspec, err := readSpec()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tfor _, group := range groupLabels {\n\t\tif groupID, ok := spec.Annotations[group]; ok {\n\t\t\tgrouping = groupID\n\t\t\tbreak\n\t\t}\n\t}\n\taddress, err := shim.SocketAddress(ctx, grouping)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tsocket, err := shim.NewSocket(address)\n\tif err != nil {\n\t\tif strings.Contains(err.Error(), \"address already in use\") {\n\t\t\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\treturn address, nil\n\t\t}\n\t\treturn \"\", err\n\t}\n\tdefer socket.Close()\n\tf, err := socket.File()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer f.Close()\n\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, f)\n\n\tif err := cmd.Start(); err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tcmd.Process.Kill()\n\t\t}\n\t}()\n\t// make sure to wait after start\n\tgo cmd.Wait()\n\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\treturn \"\", err\n\t}\n\tif data, err := ioutil.ReadAll(os.Stdin); err == nil {\n\t\tif len(data) > 0 {\n\t\t\tvar any ptypes.Any\n\t\t\tif err := proto.Unmarshal(data, &any); err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tv, err := typeurl.UnmarshalAny(&any)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tif opts, ok := v.(*options.Options); ok {\n\t\t\t\tif opts.ShimCgroup != \"\" {\n\t\t\t\t\tif cgroups.Mode() == cgroups.Unified {\n\t\t\t\t\t\tif err := cgroupsv2.VerifyGroupPath(opts.ShimCgroup); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to verify cgroup path %q\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcg, err := cgroupsv2.LoadManager(\"/sys/fs/cgroup\", opts.ShimCgroup)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := cg.AddProc(uint64(cmd.Process.Pid)); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tcg, err := cgroups.Load(cgroups.V1, cgroups.StaticPath(opts.ShimCgroup))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := cg.Add(cgroups.Process{\n\t\t\t\t\t\t\tPid: cmd.Process.Pid,\n\t\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif err := shim.AdjustOOMScore(cmd.Process.Pid); err != nil {\n\t\treturn \"\", errors.Wrap(err, \"failed to adjust OOM score for shim\")\n\t}\n\treturn address, nil\n}\n\nfunc (s *service) Cleanup(ctx context.Context) (*taskAPI.DeleteResponse, error) {\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpath := filepath.Join(filepath.Dir(cwd), s.id)\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\truntime, err := runc.ReadRuntime(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\topts, err := runc.ReadOptions(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\troot := process.RuncRoot\n\tif opts != nil && opts.Root != \"\" {\n\t\troot = opts.Root\n\t}\n\n\tr := process.NewRunc(root, path, ns, runtime, \"\", false)\n\tif err := r.Delete(ctx, s.id, &runcC.DeleteOpts{\n\t\tForce: true,\n\t}); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to remove runc container\")\n\t}\n\tif err := mount.UnmountAll(filepath.Join(path, \"rootfs\"), 0); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to cleanup rootfs mount\")\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitedAt:   time.Now(),\n\t\tExitStatus: 128 + uint32(unix.SIGKILL),\n\t}, nil\n}\n\n// Create a new initial process and container with the underlying OCI runtime\nfunc (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tcontainer, err := runc.NewContainer(ctx, s.platform, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.containers[r.ID] = container\n\n\ts.send(&eventstypes.TaskCreate{\n\t\tContainerID: r.ID,\n\t\tBundle:      r.Bundle,\n\t\tRootfs:      r.Rootfs,\n\t\tIO: &eventstypes.TaskIO{\n\t\t\tStdin:    r.Stdin,\n\t\t\tStdout:   r.Stdout,\n\t\t\tStderr:   r.Stderr,\n\t\t\tTerminal: r.Terminal,\n\t\t},\n\t\tCheckpoint: r.Checkpoint,\n\t\tPid:        uint32(container.Pid()),\n\t})\n\n\treturn &taskAPI.CreateTaskResponse{\n\t\tPid: uint32(container.Pid()),\n\t}, nil\n}\n\n// Start a process\nfunc (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// hold the send lock so that the start events are sent before any exit events in the error case\n\ts.eventSendMu.Lock()\n\tp, err := container.Start(ctx, r)\n\tif err != nil {\n\t\ts.eventSendMu.Unlock()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\tswitch r.ExecID {\n\tcase \"\":\n\t\tswitch cg := container.Cgroup().(type) {\n\t\tcase cgroups.Cgroup:\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\tcase *cgroupsv2.Manager:\n\t\t\tallControllers, err := cg.RootControllers()\n\t\t\tif err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"failed to get root controllers\")\n\t\t\t} else {\n\t\t\t\tif err := cg.ToggleControllers(allControllers, cgroupsv2.Enable); err != nil {\n\t\t\t\t\tif sys.RunningInUserNS() {\n\t\t\t\t\t\tlogrus.WithError(err).Debugf(\"failed to enable controllers (%v)\", allControllers)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogrus.WithError(err).Errorf(\"failed to enable controllers (%v)\", allControllers)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\t}\n\n\t\ts.send(&eventstypes.TaskStart{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\tdefault:\n\t\ts.send(&eventstypes.TaskExecStarted{\n\t\t\tContainerID: container.ID,\n\t\t\tExecID:      r.ExecID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\t}\n\ts.eventSendMu.Unlock()\n\treturn &taskAPI.StartResponse{\n\t\tPid: uint32(p.Pid()),\n\t}, nil\n}\n\n// Delete the initial process and container\nfunc (s *service) Delete(ctx context.Context, r *taskAPI.DeleteRequest) (*taskAPI.DeleteResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Delete(ctx, r)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\t// if we deleted an init task, send the task delete event\n\tif r.ExecID == \"\" {\n\t\ts.mu.Lock()\n\t\tdelete(s.containers, r.ID)\n\t\ts.mu.Unlock()\n\t\ts.send(&eventstypes.TaskDelete{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t\tExitStatus:  uint32(p.ExitStatus()),\n\t\t\tExitedAt:    p.ExitedAt(),\n\t\t})\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t\tPid:        uint32(p.Pid()),\n\t}, nil\n}\n\n// Exec an additional process inside the container\nfunc (s *service) Exec(ctx context.Context, r *taskAPI.ExecProcessRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tok, cancel := container.ReserveProcess(r.ExecID)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrAlreadyExists, \"id %s\", r.ExecID)\n\t}\n\tprocess, err := container.Exec(ctx, r)\n\tif err != nil {\n\t\tcancel()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\ts.send(&eventstypes.TaskExecAdded{\n\t\tContainerID: container.ID,\n\t\tExecID:      process.ID(),\n\t})\n\treturn empty, nil\n}\n\n// ResizePty of a process\nfunc (s *service) ResizePty(ctx context.Context, r *taskAPI.ResizePtyRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.ResizePty(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// State returns runtime state information for a process\nfunc (s *service) State(ctx context.Context, r *taskAPI.StateRequest) (*taskAPI.StateResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tst, err := p.Status(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstatus := task.StatusUnknown\n\tswitch st {\n\tcase \"created\":\n\t\tstatus = task.StatusCreated\n\tcase \"running\":\n\t\tstatus = task.StatusRunning\n\tcase \"stopped\":\n\t\tstatus = task.StatusStopped\n\tcase \"paused\":\n\t\tstatus = task.StatusPaused\n\tcase \"pausing\":\n\t\tstatus = task.StatusPausing\n\t}\n\tsio := p.Stdio()\n\treturn &taskAPI.StateResponse{\n\t\tID:         p.ID(),\n\t\tBundle:     container.Bundle,\n\t\tPid:        uint32(p.Pid()),\n\t\tStatus:     status,\n\t\tStdin:      sio.Stdin,\n\t\tStdout:     sio.Stdout,\n\t\tStderr:     sio.Stderr,\n\t\tTerminal:   sio.Terminal,\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Pause the container\nfunc (s *service) Pause(ctx context.Context, r *taskAPI.PauseRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Pause(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskPaused{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Resume the container\nfunc (s *service) Resume(ctx context.Context, r *taskAPI.ResumeRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Resume(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskResumed{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Kill a process with the provided signal\nfunc (s *service) Kill(ctx context.Context, r *taskAPI.KillRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Kill(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Pids returns all pids inside the container\nfunc (s *service) Pids(ctx context.Context, r *taskAPI.PidsRequest) (*taskAPI.PidsResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids, err := s.getContainerPids(ctx, r.ID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tvar processes []*task.ProcessInfo\n\tfor _, pid := range pids {\n\t\tpInfo := task.ProcessInfo{\n\t\t\tPid: pid,\n\t\t}\n\t\tfor _, p := range container.ExecdProcesses() {\n\t\t\tif p.Pid() == int(pid) {\n\t\t\t\td := &options.ProcessDetails{\n\t\t\t\t\tExecID: p.ID(),\n\t\t\t\t}\n\t\t\t\ta, err := typeurl.MarshalAny(d)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrapf(err, \"failed to marshal process %d info\", pid)\n\t\t\t\t}\n\t\t\t\tpInfo.Info = a\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tprocesses = append(processes, &pInfo)\n\t}\n\treturn &taskAPI.PidsResponse{\n\t\tProcesses: processes,\n\t}, nil\n}\n\n// CloseIO of a process\nfunc (s *service) CloseIO(ctx context.Context, r *taskAPI.CloseIORequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.CloseIO(ctx, r); err != nil {\n\t\treturn nil, err\n\t}\n\treturn empty, nil\n}\n\n// Checkpoint the container\nfunc (s *service) Checkpoint(ctx context.Context, r *taskAPI.CheckpointTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Checkpoint(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Update a running container\nfunc (s *service) Update(ctx context.Context, r *taskAPI.UpdateTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Update(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Wait for a process to exit\nfunc (s *service) Wait(ctx context.Context, r *taskAPI.WaitRequest) (*taskAPI.WaitResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tp.Wait()\n\n\treturn &taskAPI.WaitResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Connect returns shim information such as the shim's pid\nfunc (s *service) Connect(ctx context.Context, r *taskAPI.ConnectRequest) (*taskAPI.ConnectResponse, error) {\n\tvar pid int\n\tif container, err := s.getContainer(r.ID); err == nil {\n\t\tpid = container.Pid()\n\t}\n\treturn &taskAPI.ConnectResponse{\n\t\tShimPid: uint32(os.Getpid()),\n\t\tTaskPid: uint32(pid),\n\t}, nil\n}\n\nfunc (s *service) Shutdown(ctx context.Context, r *taskAPI.ShutdownRequest) (*ptypes.Empty, error) {\n\ts.mu.Lock()\n\t// return out if the shim is still servicing containers\n\tif len(s.containers) > 0 {\n\t\ts.mu.Unlock()\n\t\treturn empty, nil\n\t}\n\ts.cancel()\n\tclose(s.events)\n\n\tif s.platform != nil {\n\t\ts.platform.Close()\n\t}\n\n\treturn empty, nil\n}\n\nfunc (s *service) Stats(ctx context.Context, r *taskAPI.StatsRequest) (*taskAPI.StatsResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcgx := container.Cgroup()\n\tif cgx == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tvar statsx interface{}\n\tswitch cg := cgx.(type) {\n\tcase cgroups.Cgroup:\n\t\tstats, err := cg.Stat(cgroups.IgnoreNotExist)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatsx = stats\n\tcase *cgroupsv2.Manager:\n\t\tstats, err := cg.Stat()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatsx = stats\n\tdefault:\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotImplemented, \"unsupported cgroup type %T\", cg)\n\t}\n\tdata, err := typeurl.MarshalAny(statsx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &taskAPI.StatsResponse{\n\t\tStats: data,\n\t}, nil\n}\n\nfunc (s *service) processExits() {\n\tfor e := range s.ec {\n\t\ts.checkProcesses(e)\n\t}\n}\n\nfunc (s *service) send(evt interface{}) {\n\ts.events <- evt\n}\n\nfunc (s *service) sendL(evt interface{}) {\n\ts.eventSendMu.Lock()\n\ts.events <- evt\n\ts.eventSendMu.Unlock()\n}\n\nfunc (s *service) checkProcesses(e runcC.Exit) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tfor _, container := range s.containers {\n\t\tif !container.HasPid(e.Pid) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, p := range container.All() {\n\t\t\tif p.Pid() != e.Pid {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif ip, ok := p.(*process.Init); ok {\n\t\t\t\t// Ensure all children are killed\n\t\t\t\tif runc.ShouldKillAllOnExit(s.context, container.Bundle) {\n\t\t\t\t\tif err := ip.KillAll(s.context); err != nil {\n\t\t\t\t\t\tlogrus.WithError(err).WithField(\"id\", ip.ID()).\n\t\t\t\t\t\t\tError(\"failed to kill init's children\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tp.SetExited(e.Status)\n\t\t\ts.sendL(&eventstypes.TaskExit{\n\t\t\t\tContainerID: container.ID,\n\t\t\t\tID:          p.ID(),\n\t\t\t\tPid:         uint32(e.Pid),\n\t\t\t\tExitStatus:  uint32(e.Status),\n\t\t\t\tExitedAt:    p.ExitedAt(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\treturn\n\t}\n}\n\nfunc (s *service) getContainerPids(ctx context.Context, id string) ([]uint32, error) {\n\tcontainer, err := s.getContainer(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(\"\")\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tps, err := p.(*process.Init).Runtime().Ps(ctx, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids := make([]uint32, 0, len(ps))\n\tfor _, pid := range ps {\n\t\tpids = append(pids, uint32(pid))\n\t}\n\treturn pids, nil\n}\n\nfunc (s *service) forward(ctx context.Context, publisher shim.Publisher) {\n\tns, _ := namespaces.Namespace(ctx)\n\tctx = namespaces.WithNamespace(context.Background(), ns)\n\tfor e := range s.events {\n\t\terr := publisher.Publish(ctx, runc.GetTopic(e), e)\n\t\tif err != nil {\n\t\t\tlogrus.WithError(err).Error(\"post event\")\n\t\t}\n\t}\n\tpublisher.Close()\n}\n\nfunc (s *service) getContainer(id string) (*runc.Container, error) {\n\ts.mu.Lock()\n\tcontainer := s.containers[id]\n\ts.mu.Unlock()\n\tif container == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"container not created\")\n\t}\n\treturn container, nil\n}\n\n// initialize a single epoll fd to manage our consoles. `initPlatform` should\n// only be called once.\nfunc (s *service) initPlatform() error {\n\tif s.platform != nil {\n\t\treturn nil\n\t}\n\tp, err := runc.NewPlatform()\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.platform = p\n\treturn nil\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/namespaces\"\n\tshimapi \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/version\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Client for a shim server\ntype Client struct {\n\tservice shimapi.TaskService\n\tcontext context.Context\n\tsignals chan os.Signal\n}\n\n// Publisher for events\ntype Publisher interface {\n\tevents.Publisher\n\tio.Closer\n}\n\n// Init func for the creation of a shim server\ntype Init func(context.Context, string, Publisher, func()) (Shim, error)\n\n// Shim server interface\ntype Shim interface {\n\tshimapi.TaskService\n\tCleanup(ctx context.Context) (*shimapi.DeleteResponse, error)\n\tStartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (string, error)\n}\n\n// OptsKey is the context key for the Opts value.\ntype OptsKey struct{}\n\n// Opts are context options associated with the shim invocation.\ntype Opts struct {\n\tBundlePath string\n\tDebug      bool\n}\n\n// BinaryOpts allows the configuration of a shims binary setup\ntype BinaryOpts func(*Config)\n\n// Config of shim binary options provided by shim implementations\ntype Config struct {\n\t// NoSubreaper disables setting the shim as a child subreaper\n\tNoSubreaper bool\n\t// NoReaper disables the shim binary from reaping any child process implicitly\n\tNoReaper bool\n\t// NoSetupLogger disables automatic configuration of logrus to use the shim FIFO\n\tNoSetupLogger bool\n}\n\nvar (\n\tdebugFlag            bool\n\tversionFlag          bool\n\tidFlag               string\n\tnamespaceFlag        string\n\tsocketFlag           string\n\tbundlePath           string\n\taddressFlag          string\n\tcontainerdBinaryFlag string\n\taction               string\n)\n\nconst (\n\tttrpcAddressEnv = \"TTRPC_ADDRESS\"\n)\n\nfunc parseFlags() {\n\tflag.BoolVar(&debugFlag, \"debug\", false, \"enable debug output in logs\")\n\tflag.BoolVar(&versionFlag, \"v\", false, \"show the shim version and exit\")\n\tflag.StringVar(&namespaceFlag, \"namespace\", \"\", \"namespace that owns the shim\")\n\tflag.StringVar(&idFlag, \"id\", \"\", \"id of the task\")\n\tflag.StringVar(&socketFlag, \"socket\", \"\", \"abstract socket path to serve\")\n\tflag.StringVar(&bundlePath, \"bundle\", \"\", \"path to the bundle if not workdir\")\n\n\tflag.StringVar(&addressFlag, \"address\", \"\", \"grpc address back to main containerd\")\n\tflag.StringVar(&containerdBinaryFlag, \"publish-binary\", \"containerd\", \"path to publish binary (used for publishing events)\")\n\n\tflag.Parse()\n\taction = flag.Arg(0)\n}\n\nfunc setRuntime() {\n\tdebug.SetGCPercent(40)\n\tgo func() {\n\t\tfor range time.Tick(30 * time.Second) {\n\t\t\tdebug.FreeOSMemory()\n\t\t}\n\t}()\n\tif os.Getenv(\"GOMAXPROCS\") == \"\" {\n\t\t// If GOMAXPROCS hasn't been set, we default to a value of 2 to reduce\n\t\t// the number of Go stacks present in the shim.\n\t\truntime.GOMAXPROCS(2)\n\t}\n}\n\nfunc setLogger(ctx context.Context, id string) error {\n\tlogrus.SetFormatter(&logrus.TextFormatter{\n\t\tTimestampFormat: log.RFC3339NanoFixed,\n\t\tFullTimestamp:   true,\n\t})\n\tif debugFlag {\n\t\tlogrus.SetLevel(logrus.DebugLevel)\n\t}\n\tf, err := openLog(ctx, id)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.SetOutput(f)\n\treturn nil\n}\n\n// Run initializes and runs a shim server\nfunc Run(id string, initFunc Init, opts ...BinaryOpts) {\n\tvar config Config\n\tfor _, o := range opts {\n\t\to(&config)\n\t}\n\tif err := run(id, initFunc, config); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%s: %s\\n\", id, err)\n\t\tos.Exit(1)\n\t}\n}\n\nfunc run(id string, initFunc Init, config Config) error {\n\tparseFlags()\n\tif versionFlag {\n\t\tfmt.Printf(\"%s:\\n\", os.Args[0])\n\t\tfmt.Println(\"  Version: \", version.Version)\n\t\tfmt.Println(\"  Revision:\", version.Revision)\n\t\tfmt.Println(\"  Go version:\", version.GoVersion)\n\t\tfmt.Println(\"\")\n\t\treturn nil\n\t}\n\n\tsetRuntime()\n\n\tsignals, err := setupSignals(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !config.NoSubreaper {\n\t\tif err := subreaper(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tttrpcAddress := os.Getenv(ttrpcAddressEnv)\n\n\tpublisher, err := NewPublisher(ttrpcAddress)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer publisher.Close()\n\n\tif namespaceFlag == \"\" {\n\t\treturn fmt.Errorf(\"shim namespace cannot be empty\")\n\t}\n\tctx := namespaces.WithNamespace(context.Background(), namespaceFlag)\n\tctx = context.WithValue(ctx, OptsKey{}, Opts{BundlePath: bundlePath, Debug: debugFlag})\n\tctx = log.WithLogger(ctx, log.G(ctx).WithField(\"runtime\", id))\n\tctx, cancel := context.WithCancel(ctx)\n\n\tservice, err := initFunc(ctx, idFlag, publisher, cancel)\n\tif err != nil {\n\t\treturn err\n\t}\n\tswitch action {\n\tcase \"delete\":\n\t\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\t\"pid\":       os.Getpid(),\n\t\t\t\"namespace\": namespaceFlag,\n\t\t})\n\t\tgo handleSignals(ctx, logger, signals)\n\t\tresponse, err := service.Cleanup(ctx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdata, err := proto.Marshal(response)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := os.Stdout.Write(data); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tcase \"start\":\n\t\taddress, err := service.StartShim(ctx, idFlag, containerdBinaryFlag, addressFlag, ttrpcAddress)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := os.Stdout.WriteString(address); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tdefault:\n\t\tif !config.NoSetupLogger {\n\t\t\tif err := setLogger(ctx, idFlag); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tclient := NewShimClient(ctx, service, signals)\n\t\tif err := client.Serve(); err != nil {\n\t\t\tif err != context.Canceled {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tselect {\n\t\tcase <-publisher.Done():\n\t\t\treturn nil\n\t\tcase <-time.After(5 * time.Second):\n\t\t\treturn errors.New(\"publisher not closed\")\n\t\t}\n\t}\n}\n\n// NewShimClient creates a new shim server client\nfunc NewShimClient(ctx context.Context, svc shimapi.TaskService, signals chan os.Signal) *Client {\n\ts := &Client{\n\t\tservice: svc,\n\t\tcontext: ctx,\n\t\tsignals: signals,\n\t}\n\treturn s\n}\n\n// Serve the shim server\nfunc (s *Client) Serve() error {\n\tdump := make(chan os.Signal, 32)\n\tsetupDumpStacks(dump)\n\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn err\n\t}\n\tserver, err := newServer()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed creating server\")\n\t}\n\n\tlogrus.Debug(\"registering ttrpc server\")\n\tshimapi.RegisterTaskService(server, s.service)\n\n\tif err := serve(s.context, server, socketFlag); err != nil {\n\t\treturn err\n\t}\n\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\"pid\":       os.Getpid(),\n\t\t\"path\":      path,\n\t\t\"namespace\": namespaceFlag,\n\t})\n\tgo func() {\n\t\tfor range dump {\n\t\t\tdumpStacks(logger)\n\t\t}\n\t}()\n\treturn handleSignals(s.context, logger, s.signals)\n}\n\n// serve serves the ttrpc API over a unix socket at the provided path\n// this function does not block\nfunc serve(ctx context.Context, server *ttrpc.Server, path string) error {\n\tl, err := serveListener(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tgo func() {\n\t\tdefer l.Close()\n\t\tif err := server.Serve(ctx, l); err != nil &&\n\t\t\t!strings.Contains(err.Error(), \"use of closed network connection\") {\n\t\t\tlogrus.WithError(err).Fatal(\"containerd-shim: ttrpc server failure\")\n\t\t}\n\t}()\n\treturn nil\n}\n\nfunc dumpStacks(logger *logrus.Entry) {\n\tvar (\n\t\tbuf       []byte\n\t\tstackSize int\n\t)\n\tbufferLen := 16384\n\tfor stackSize == len(buf) {\n\t\tbuf = make([]byte, bufferLen)\n\t\tstackSize = runtime.Stack(buf, true)\n\t\tbufferLen *= 2\n\t}\n\tbuf = buf[:stackSize]\n\tlogger.Infof(\"=== BEGIN goroutine stack dump ===\\n%s\\n=== END goroutine stack dump ===\", buf)\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\n\t\"github.com/containerd/containerd/sys/reaper\"\n\t\"github.com/containerd/fifo\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\n// setupSignals creates a new signal handler for all signals and sets the shim as a\n// sub-reaper so that the container processes are reparented\nfunc setupSignals(config Config) (chan os.Signal, error) {\n\tsignals := make(chan os.Signal, 32)\n\tsmp := []os.Signal{unix.SIGTERM, unix.SIGINT, unix.SIGPIPE}\n\tif !config.NoReaper {\n\t\tsmp = append(smp, unix.SIGCHLD)\n\t}\n\tsignal.Notify(signals, smp...)\n\treturn signals, nil\n}\n\nfunc setupDumpStacks(dump chan<- os.Signal) {\n\tsignal.Notify(dump, syscall.SIGUSR1)\n}\n\nfunc serveListener(path string) (net.Listener, error) {\n\tvar (\n\t\tl   net.Listener\n\t\terr error\n\t)\n\tif path == \"\" {\n\t\tl, err = net.FileListener(os.NewFile(3, \"socket\"))\n\t\tpath = \"[inherited from parent]\"\n\t} else {\n\t\tif len(path) > 106 {\n\t\t\treturn nil, errors.Errorf(\"%q: unix socket path too long (> 106)\", path)\n\t\t}\n\t\tl, err = net.Listen(\"unix\", \"\\x00\"+path)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlogrus.WithField(\"socket\", path).Debug(\"serving api on abstract socket\")\n\treturn l, nil\n}\n\nfunc handleSignals(ctx context.Context, logger *logrus.Entry, signals chan os.Signal) error {\n\tlogger.Info(\"starting signal loop\")\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase s := <-signals:\n\t\t\tswitch s {\n\t\t\tcase unix.SIGCHLD:\n\t\t\t\tif err := reaper.Reap(); err != nil {\n\t\t\t\t\tlogger.WithError(err).Error(\"reap exit status\")\n\t\t\t\t}\n\t\t\tcase unix.SIGPIPE:\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc openLog(ctx context.Context, _ string) (io.Writer, error) {\n\treturn fifo.OpenFifoDup2(ctx, \"log\", unix.O_WRONLY, 0700, int(os.Stderr.Fd()))\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n)\n\nvar runtimePaths sync.Map\n\n// Command returns the shim command with the provided args and configuration\nfunc Command(ctx context.Context, runtime, containerdAddress, containerdTTRPCAddress, path string, opts *types.Any, cmdArgs ...string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-address\", containerdAddress,\n\t\t\"-publish-binary\", self,\n\t}\n\targs = append(args, cmdArgs...)\n\tname := BinaryName(runtime)\n\tif name == \"\" {\n\t\treturn nil, fmt.Errorf(\"invalid runtime name %s, correct runtime name should format like io.containerd.runc.v1\", runtime)\n\t}\n\n\tvar cmdPath string\n\tcmdPathI, cmdPathFound := runtimePaths.Load(name)\n\tif cmdPathFound {\n\t\tcmdPath = cmdPathI.(string)\n\t} else {\n\t\tvar lerr error\n\t\tif cmdPath, lerr = exec.LookPath(name); lerr != nil {\n\t\t\tif eerr, ok := lerr.(*exec.Error); ok {\n\t\t\t\tif eerr.Err == exec.ErrNotFound {\n\t\t\t\t\t// LookPath only finds current directory matches based on\n\t\t\t\t\t// the callers current directory but the caller is not\n\t\t\t\t\t// likely in the same directory as the containerd\n\t\t\t\t\t// executables. Instead match the calling binaries path\n\t\t\t\t\t// (containerd) and see if they are side by side. If so\n\t\t\t\t\t// execute the shim found there.\n\t\t\t\t\ttestPath := filepath.Join(filepath.Dir(self), name)\n\t\t\t\t\tif _, serr := os.Stat(testPath); serr == nil {\n\t\t\t\t\t\tcmdPath = testPath\n\t\t\t\t\t}\n\t\t\t\t\tif cmdPath == \"\" {\n\t\t\t\t\t\treturn nil, errors.Wrapf(os.ErrNotExist, \"runtime %q binary not installed %q\", runtime, name)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcmdPath, err = filepath.Abs(cmdPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif cmdPathI, cmdPathFound = runtimePaths.LoadOrStore(name, cmdPath); cmdPathFound {\n\t\t\t// We didn't store cmdPath we loaded an already cached value. Use it.\n\t\t\tcmdPath = cmdPathI.(string)\n\t\t}\n\t}\n\n\tcmd := exec.Command(cmdPath, args...)\n\tcmd.Dir = path\n\tcmd.Env = append(\n\t\tos.Environ(),\n\t\t\"GOMAXPROCS=2\",\n\t\tfmt.Sprintf(\"%s=%s\", ttrpcAddressEnv, containerdTTRPCAddress),\n\t)\n\tcmd.SysProcAttr = getSysProcAttr()\n\tif opts != nil {\n\t\td, err := proto.Marshal(opts)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcmd.Stdin = bytes.NewReader(d)\n\t}\n\treturn cmd, nil\n}\n\n// BinaryName returns the shim binary name from the runtime name,\n// empty string returns means runtime name is invalid\nfunc BinaryName(runtime string) string {\n\t// runtime name should format like $prefix.name.version\n\tparts := strings.Split(runtime, \".\")\n\tif len(parts) < 2 {\n\t\treturn \"\"\n\t}\n\n\treturn fmt.Sprintf(shimBinaryFormat, parts[len(parts)-2], parts[len(parts)-1])\n}\n\n// Connect to the provided address\nfunc Connect(address string, d func(string, time.Duration) (net.Conn, error)) (net.Conn, error) {\n\treturn d(address, 100*time.Second)\n}\n\n// WritePidFile writes a pid file atomically\nfunc WritePidFile(path string, pid int) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = fmt.Fprintf(f, \"%d\", pid)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\n// WriteAddress writes a address file atomically\nfunc WriteAddress(path, address string) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = f.WriteString(address)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\n// ErrNoAddress is returned when the address file has no content\nvar ErrNoAddress = errors.New(\"no shim address\")\n\n// ReadAddress returns the shim's abstract socket address from the path\nfunc ReadAddress(path string) (string, error) {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdata, err := ioutil.ReadFile(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif len(data) == 0 {\n\t\treturn \"\", ErrNoAddress\n\t}\n\treturn string(data), nil\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/pkg/errors\"\n)\n\nconst shimBinaryFormat = \"containerd-shim-%s-%s\"\n\nfunc getSysProcAttr() *syscall.SysProcAttr {\n\treturn &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n}\n\n// SetScore sets the oom score for a process\nfunc SetScore(pid int) error {\n\treturn sys.SetOOMScore(pid, sys.OOMScoreMaxKillable)\n}\n\n// AdjustOOMScore sets the OOM score for the process to the parents OOM score +1\n// to ensure that they parent has a lower* score than the shim\nfunc AdjustOOMScore(pid int) error {\n\tparent := os.Getppid()\n\tscore, err := sys.GetOOMScoreAdj(parent)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"get parent OOM score\")\n\t}\n\tshimScore := score + 1\n\tif err := sys.SetOOMScore(pid, shimScore); err != nil {\n\t\treturn errors.Wrap(err, \"set shim OOM score\")\n\t}\n\treturn nil\n}\n\n// SocketAddress returns an abstract socket address\nfunc SocketAddress(ctx context.Context, id string) (string, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\td := sha256.Sum256([]byte(filepath.Join(ns, id)))\n\treturn filepath.Join(string(filepath.Separator), \"containerd-shim\", fmt.Sprintf(\"%x.sock\", d)), nil\n}\n\n// AnonDialer returns a dialer for an abstract socket\nfunc AnonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\taddress = strings.TrimPrefix(address, \"unix://\")\n\treturn dialer.Dialer(\"\\x00\"+address, timeout)\n}\n\nfunc AnonReconnectDialer(address string, timeout time.Duration) (net.Conn, error) {\n\treturn AnonDialer(address, timeout)\n}\n\n// NewSocket returns a new socket\nfunc NewSocket(address string) (*net.UnixListener, error) {\n\tif len(address) > 106 {\n\t\treturn nil, errors.Errorf(\"%q: unix socket path too long (> 106)\", address)\n\t}\n\tl, err := net.Listen(\"unix\", \"\\x00\"+address)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to listen to abstract unix socket %q\", address)\n\t}\n\treturn l.(*net.UnixListener), nil\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"os\"\n\t\"syscall\"\n\t\"time\"\n\n\twinio \"github.com/Microsoft/go-winio\"\n\t\"github.com/pkg/errors\"\n)\n\nconst shimBinaryFormat = \"containerd-shim-%s-%s.exe\"\n\nfunc getSysProcAttr() *syscall.SysProcAttr {\n\treturn nil\n}\n\n// AnonReconnectDialer returns a dialer for an existing npipe on containerd reconnection\nfunc AnonReconnectDialer(address string, timeout time.Duration) (net.Conn, error) {\n\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n\tdefer cancel()\n\n\tc, err := winio.DialPipeContext(ctx, address)\n\tif os.IsNotExist(err) {\n\t\treturn nil, errors.Wrap(os.ErrNotExist, \"npipe not found on reconnect\")\n\t} else if err == context.DeadlineExceeded {\n\t\treturn nil, errors.Wrapf(err, \"timed out waiting for npipe %s\", address)\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\treturn c, nil\n}\n\n// AnonDialer returns a dialer for a npipe\nfunc AnonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n\tdefer cancel()\n\n\t// If there is nobody serving the pipe we limit the timeout for this case to\n\t// 5 seconds because any shim that would serve this endpoint should serve it\n\t// within 5 seconds.\n\tserveTimer := time.NewTimer(5 * time.Second)\n\tdefer serveTimer.Stop()\n\tfor {\n\t\tc, err := winio.DialPipeContext(ctx, address)\n\t\tif err != nil {\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\tselect {\n\t\t\t\tcase <-serveTimer.C:\n\t\t\t\t\treturn nil, errors.Wrap(os.ErrNotExist, \"pipe not found before timeout\")\n\t\t\t\tdefault:\n\t\t\t\t\t// Wait 10ms for the shim to serve and try again.\n\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if err == context.DeadlineExceeded {\n\t\t\t\treturn nil, errors.Wrapf(err, \"timed out waiting for npipe %s\", address)\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\treturn c, nil\n\t}\n}\n"], "fixing_code": ["// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage main\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/signal\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\tshimlog \"github.com/containerd/containerd/runtime/v1\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\tshimapi \"github.com/containerd/containerd/runtime/v1/shim/v1\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/containerd/typeurl\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\tdebugFlag            bool\n\tnamespaceFlag        string\n\tsocketFlag           string\n\taddressFlag          string\n\tworkdirFlag          string\n\truntimeRootFlag      string\n\tcriuFlag             string\n\tsystemdCgroupFlag    bool\n\tcontainerdBinaryFlag string\n\n\tbufPool = sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\treturn bytes.NewBuffer(nil)\n\t\t},\n\t}\n)\n\nfunc init() {\n\tflag.BoolVar(&debugFlag, \"debug\", false, \"enable debug output in logs\")\n\tflag.StringVar(&namespaceFlag, \"namespace\", \"\", \"namespace that owns the shim\")\n\tflag.StringVar(&socketFlag, \"socket\", \"\", \"socket path to serve\")\n\tflag.StringVar(&addressFlag, \"address\", \"\", \"grpc address back to main containerd\")\n\tflag.StringVar(&workdirFlag, \"workdir\", \"\", \"path used to storge large temporary data\")\n\tflag.StringVar(&runtimeRootFlag, \"runtime-root\", process.RuncRoot, \"root directory for the runtime\")\n\tflag.StringVar(&criuFlag, \"criu\", \"\", \"path to criu binary\")\n\tflag.BoolVar(&systemdCgroupFlag, \"systemd-cgroup\", false, \"set runtime to use systemd-cgroup\")\n\t// currently, the `containerd publish` utility is embedded in the daemon binary.\n\t// The daemon invokes `containerd-shim -containerd-binary ...` with its own os.Executable() path.\n\tflag.StringVar(&containerdBinaryFlag, \"containerd-binary\", \"containerd\", \"path to containerd binary (used for `containerd publish`)\")\n\tflag.Parse()\n}\n\nfunc main() {\n\tdebug.SetGCPercent(40)\n\tgo func() {\n\t\tfor range time.Tick(30 * time.Second) {\n\t\t\tdebug.FreeOSMemory()\n\t\t}\n\t}()\n\n\tif debugFlag {\n\t\tlogrus.SetLevel(logrus.DebugLevel)\n\t}\n\n\tif os.Getenv(\"GOMAXPROCS\") == \"\" {\n\t\t// If GOMAXPROCS hasn't been set, we default to a value of 2 to reduce\n\t\t// the number of Go stacks present in the shim.\n\t\truntime.GOMAXPROCS(2)\n\t}\n\n\tstdout, stderr, err := openStdioKeepAlivePipes(workdirFlag)\n\tif err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"containerd-shim: %s\\n\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer func() {\n\t\tstdout.Close()\n\t\tstderr.Close()\n\t}()\n\n\t// redirect the following output into fifo to make sure that containerd\n\t// still can read the log after restart\n\tlogrus.SetOutput(stdout)\n\n\tif err := executeShim(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"containerd-shim: %s\\n\", err)\n\t\tos.Exit(1)\n\t}\n}\n\n// If containerd server process dies, we need the shim to keep stdout/err reader\n// FDs so that Linux does not SIGPIPE the shim process if it tries to use its end of\n// these pipes.\nfunc openStdioKeepAlivePipes(dir string) (io.ReadWriteCloser, io.ReadWriteCloser, error) {\n\tbackground := context.Background()\n\tkeepStdoutAlive, err := shimlog.OpenShimStdoutLog(background, dir)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tkeepStderrAlive, err := shimlog.OpenShimStderrLog(background, dir)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn keepStdoutAlive, keepStderrAlive, nil\n}\n\nfunc executeShim() error {\n\t// start handling signals as soon as possible so that things are properly reaped\n\t// or if runtime exits before we hit the handler\n\tsignals, err := setupSignals()\n\tif err != nil {\n\t\treturn err\n\t}\n\tdump := make(chan os.Signal, 32)\n\tsignal.Notify(dump, syscall.SIGUSR1)\n\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn err\n\t}\n\tserver, err := newServer()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed creating server\")\n\t}\n\tsv, err := shim.NewService(\n\t\tshim.Config{\n\t\t\tPath:          path,\n\t\t\tNamespace:     namespaceFlag,\n\t\t\tWorkDir:       workdirFlag,\n\t\t\tCriu:          criuFlag,\n\t\t\tSystemdCgroup: systemdCgroupFlag,\n\t\t\tRuntimeRoot:   runtimeRootFlag,\n\t\t},\n\t\t&remoteEventsPublisher{address: addressFlag},\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.Debug(\"registering ttrpc server\")\n\tshimapi.RegisterShimService(server, sv)\n\n\tsocket := socketFlag\n\tif err := serve(context.Background(), server, socket); err != nil {\n\t\treturn err\n\t}\n\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\"pid\":       os.Getpid(),\n\t\t\"path\":      path,\n\t\t\"namespace\": namespaceFlag,\n\t})\n\tgo func() {\n\t\tfor range dump {\n\t\t\tdumpStacks(logger)\n\t\t}\n\t}()\n\treturn handleSignals(logger, signals, server, sv)\n}\n\n// serve serves the ttrpc API over a unix socket at the provided path\n// this function does not block\nfunc serve(ctx context.Context, server *ttrpc.Server, path string) error {\n\tvar (\n\t\tl   net.Listener\n\t\terr error\n\t)\n\tif path == \"\" {\n\t\tf := os.NewFile(3, \"socket\")\n\t\tl, err = net.FileListener(f)\n\t\tf.Close()\n\t\tpath = \"[inherited from parent]\"\n\t} else {\n\t\tconst (\n\t\t\tabstractSocketPrefix = \"\\x00\"\n\t\t\tsocketPathLimit      = 106\n\t\t)\n\t\tp := strings.TrimPrefix(path, \"unix://\")\n\t\tif len(p) == len(path) {\n\t\t\tp = abstractSocketPrefix + p\n\t\t}\n\t\tif len(p) > socketPathLimit {\n\t\t\treturn errors.Errorf(\"%q: unix socket path too long (> %d)\", p, socketPathLimit)\n\t\t}\n\t\tl, err = net.Listen(\"unix\", p)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.WithField(\"socket\", path).Debug(\"serving api on unix socket\")\n\tgo func() {\n\t\tdefer l.Close()\n\t\tif err := server.Serve(ctx, l); err != nil &&\n\t\t\t!strings.Contains(err.Error(), \"use of closed network connection\") {\n\t\t\tlogrus.WithError(err).Fatal(\"containerd-shim: ttrpc server failure\")\n\t\t}\n\t}()\n\treturn nil\n}\n\nfunc handleSignals(logger *logrus.Entry, signals chan os.Signal, server *ttrpc.Server, sv *shim.Service) error {\n\tvar (\n\t\ttermOnce sync.Once\n\t\tdone     = make(chan struct{})\n\t)\n\n\tfor {\n\t\tselect {\n\t\tcase <-done:\n\t\t\treturn nil\n\t\tcase s := <-signals:\n\t\t\tswitch s {\n\t\t\tcase unix.SIGCHLD:\n\t\t\t\tif err := reaper.Reap(); err != nil {\n\t\t\t\t\tlogger.WithError(err).Error(\"reap exit status\")\n\t\t\t\t}\n\t\t\tcase unix.SIGTERM, unix.SIGINT:\n\t\t\t\tgo termOnce.Do(func() {\n\t\t\t\t\tctx := context.TODO()\n\t\t\t\t\tif err := server.Shutdown(ctx); err != nil {\n\t\t\t\t\t\tlogger.WithError(err).Error(\"failed to shutdown server\")\n\t\t\t\t\t}\n\t\t\t\t\t// Ensure our child is dead if any\n\t\t\t\t\tsv.Kill(ctx, &shimapi.KillRequest{\n\t\t\t\t\t\tSignal: uint32(syscall.SIGKILL),\n\t\t\t\t\t\tAll:    true,\n\t\t\t\t\t})\n\t\t\t\t\tsv.Delete(context.Background(), &ptypes.Empty{})\n\t\t\t\t\tclose(done)\n\t\t\t\t})\n\t\t\tcase unix.SIGPIPE:\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc dumpStacks(logger *logrus.Entry) {\n\tvar (\n\t\tbuf       []byte\n\t\tstackSize int\n\t)\n\tbufferLen := 16384\n\tfor stackSize == len(buf) {\n\t\tbuf = make([]byte, bufferLen)\n\t\tstackSize = runtime.Stack(buf, true)\n\t\tbufferLen *= 2\n\t}\n\tbuf = buf[:stackSize]\n\tlogger.Infof(\"=== BEGIN goroutine stack dump ===\\n%s\\n=== END goroutine stack dump ===\", buf)\n}\n\ntype remoteEventsPublisher struct {\n\taddress string\n}\n\nfunc (l *remoteEventsPublisher) Publish(ctx context.Context, topic string, event events.Event) error {\n\tns, _ := namespaces.Namespace(ctx)\n\tencoded, err := typeurl.MarshalAny(event)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdata, err := encoded.Marshal()\n\tif err != nil {\n\t\treturn err\n\t}\n\tcmd := exec.CommandContext(ctx, containerdBinaryFlag, \"--address\", l.address, \"publish\", \"--topic\", topic, \"--namespace\", ns)\n\tcmd.Stdin = bytes.NewReader(data)\n\tb := bufPool.Get().(*bytes.Buffer)\n\tdefer bufPool.Put(b)\n\tcmd.Stdout = b\n\tcmd.Stderr = b\n\tc, err := reaper.Default.Start(cmd)\n\tif err != nil {\n\t\treturn err\n\t}\n\tstatus, err := reaper.Default.Wait(cmd, c)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"failed to publish event: %s\", b.String())\n\t}\n\tif status != 0 {\n\t\treturn errors.Errorf(\"failed to publish event: %s\", b.String())\n\t}\n\treturn nil\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\tgocontext \"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/containerd/console\"\n\t\"github.com/containerd/containerd/cmd/ctr/commands\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\t\"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/containerd/typeurl\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/opencontainers/runtime-spec/specs-go\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"github.com/urfave/cli\"\n)\n\nvar fifoFlags = []cli.Flag{\n\tcli.StringFlag{\n\t\tName:  \"stdin\",\n\t\tUsage: \"specify the path to the stdin fifo\",\n\t},\n\tcli.StringFlag{\n\t\tName:  \"stdout\",\n\t\tUsage: \"specify the path to the stdout fifo\",\n\t},\n\tcli.StringFlag{\n\t\tName:  \"stderr\",\n\t\tUsage: \"specify the path to the stderr fifo\",\n\t},\n\tcli.BoolFlag{\n\t\tName:  \"tty,t\",\n\t\tUsage: \"enable tty support\",\n\t},\n}\n\n// Command is the cli command for interacting with a task\nvar Command = cli.Command{\n\tName:  \"shim\",\n\tUsage: \"interact with a shim directly\",\n\tFlags: []cli.Flag{\n\t\tcli.StringFlag{\n\t\t\tName:  \"id\",\n\t\t\tUsage: \"container id\",\n\t\t},\n\t},\n\tSubcommands: []cli.Command{\n\t\tdeleteCommand,\n\t\texecCommand,\n\t\tstartCommand,\n\t\tstateCommand,\n\t},\n}\n\nvar startCommand = cli.Command{\n\tName:  \"start\",\n\tUsage: \"start a container with a task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = service.Start(gocontext.Background(), &task.StartRequest{\n\t\t\tID: context.Args().First(),\n\t\t})\n\t\treturn err\n\t},\n}\n\nvar deleteCommand = cli.Command{\n\tName:  \"delete\",\n\tUsage: \"delete a container with a task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.Delete(gocontext.Background(), &task.DeleteRequest{\n\t\t\tID: context.Args().First(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfmt.Printf(\"container deleted and returned exit status %d\\n\", r.ExitStatus)\n\t\treturn nil\n\t},\n}\n\nvar stateCommand = cli.Command{\n\tName:  \"state\",\n\tUsage: \"get the state of all the processes of the task\",\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.State(gocontext.Background(), &task.StateRequest{\n\t\t\tID: context.GlobalString(\"id\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcommands.PrintAsJSON(r)\n\t\treturn nil\n\t},\n}\n\nvar execCommand = cli.Command{\n\tName:  \"exec\",\n\tUsage: \"exec a new process in the task's container\",\n\tFlags: append(fifoFlags,\n\t\tcli.BoolFlag{\n\t\t\tName:  \"attach,a\",\n\t\t\tUsage: \"stay attached to the container and open the fifos\",\n\t\t},\n\t\tcli.StringSliceFlag{\n\t\t\tName:  \"env,e\",\n\t\t\tUsage: \"add environment vars\",\n\t\t\tValue: &cli.StringSlice{},\n\t\t},\n\t\tcli.StringFlag{\n\t\t\tName:  \"cwd\",\n\t\t\tUsage: \"current working directory\",\n\t\t},\n\t\tcli.StringFlag{\n\t\t\tName:  \"spec\",\n\t\t\tUsage: \"runtime spec\",\n\t\t},\n\t),\n\tAction: func(context *cli.Context) error {\n\t\tservice, err := getTaskService(context)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar (\n\t\t\tid  = context.Args().First()\n\t\t\tctx = gocontext.Background()\n\t\t)\n\n\t\tif id == \"\" {\n\t\t\treturn errors.New(\"exec id must be provided\")\n\t\t}\n\n\t\ttty := context.Bool(\"tty\")\n\t\twg, err := prepareStdio(context.String(\"stdin\"), context.String(\"stdout\"), context.String(\"stderr\"), tty)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// read spec file and extract Any object\n\t\tspec, err := ioutil.ReadFile(context.String(\"spec\"))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\turl, err := typeurl.TypeURL(specs.Process{})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\trq := &task.ExecProcessRequest{\n\t\t\tID: id,\n\t\t\tSpec: &ptypes.Any{\n\t\t\t\tTypeUrl: url,\n\t\t\t\tValue:   spec,\n\t\t\t},\n\t\t\tStdin:    context.String(\"stdin\"),\n\t\t\tStdout:   context.String(\"stdout\"),\n\t\t\tStderr:   context.String(\"stderr\"),\n\t\t\tTerminal: tty,\n\t\t}\n\t\tif _, err := service.Exec(ctx, rq); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tr, err := service.Start(ctx, &task.StartRequest{\n\t\t\tID: id,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfmt.Printf(\"exec running with pid %d\\n\", r.Pid)\n\t\tif context.Bool(\"attach\") {\n\t\t\tlogrus.Info(\"attaching\")\n\t\t\tif tty {\n\t\t\t\tcurrent := console.Current()\n\t\t\t\tdefer current.Reset()\n\t\t\t\tif err := current.SetRaw(); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tsize, err := current.Size()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif _, err := service.ResizePty(ctx, &task.ResizePtyRequest{\n\t\t\t\t\tID:     id,\n\t\t\t\t\tWidth:  uint32(size.Width),\n\t\t\t\t\tHeight: uint32(size.Height),\n\t\t\t\t}); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\twg.Wait()\n\t\t}\n\t\treturn nil\n\t},\n}\n\nfunc getTaskService(context *cli.Context) (task.TaskService, error) {\n\tid := context.GlobalString(\"id\")\n\tif id == \"\" {\n\t\treturn nil, fmt.Errorf(\"container id must be specified\")\n\t}\n\tns := context.GlobalString(\"namespace\")\n\n\t// /containerd-shim/ns/id/shim.sock is the old way to generate shim socket,\n\t// compatible it\n\ts1 := filepath.Join(string(filepath.Separator), \"containerd-shim\", ns, id, \"shim.sock\")\n\t// this should not error, ctr always get a default ns\n\tctx := namespaces.WithNamespace(gocontext.Background(), ns)\n\ts2, _ := shim.SocketAddress(ctx, context.GlobalString(\"address\"), id)\n\ts2 = strings.TrimPrefix(s2, \"unix://\")\n\n\tfor _, socket := range []string{s2, \"\\x00\" + s1} {\n\t\tconn, err := net.Dial(\"unix\", socket)\n\t\tif err == nil {\n\t\t\tclient := ttrpc.NewClient(conn)\n\n\t\t\t// TODO(stevvooe): This actually leaks the connection. We were leaking it\n\t\t\t// before, so may not be a huge deal.\n\n\t\t\treturn task.NewTaskClient(client), nil\n\t\t}\n\t}\n\n\treturn nil, fmt.Errorf(\"fail to connect to container %s's shim\", id)\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage linux\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"path/filepath\"\n\n\t\"github.com/containerd/containerd/events/exchange\"\n\t\"github.com/containerd/containerd/runtime/linux/runctypes\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\t\"github.com/containerd/containerd/runtime/v1/shim/client\"\n\t\"github.com/pkg/errors\"\n)\n\n// loadBundle loads an existing bundle from disk\nfunc loadBundle(id, path, workdir string) *bundle {\n\treturn &bundle{\n\t\tid:      id,\n\t\tpath:    path,\n\t\tworkDir: workdir,\n\t}\n}\n\n// newBundle creates a new bundle on disk at the provided path for the given id\nfunc newBundle(id, path, workDir string, spec []byte) (b *bundle, err error) {\n\tif err := os.MkdirAll(path, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tpath = filepath.Join(path, id)\n\tif err := os.Mkdir(path, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(path)\n\t\t}\n\t}()\n\tworkDir = filepath.Join(workDir, id)\n\tif err := os.MkdirAll(workDir, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(workDir)\n\t\t}\n\t}()\n\trootfs := filepath.Join(path, \"rootfs\")\n\tif err := os.MkdirAll(rootfs, 0711); err != nil {\n\t\treturn nil, err\n\t}\n\terr = ioutil.WriteFile(filepath.Join(path, configFilename), spec, 0666)\n\treturn &bundle{\n\t\tid:      id,\n\t\tpath:    path,\n\t\tworkDir: workDir,\n\t}, err\n}\n\ntype bundle struct {\n\tid      string\n\tpath    string\n\tworkDir string\n}\n\n// ShimOpt specifies shim options for initialization and connection\ntype ShimOpt func(*bundle, string, *runctypes.RuncOptions) (shim.Config, client.Opt)\n\n// ShimRemote is a ShimOpt for connecting and starting a remote shim\nfunc ShimRemote(c *Config, daemonAddress, cgroup string, exitHandler func()) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\tconfig := b.shimConfig(ns, c, ropts)\n\t\treturn config,\n\t\t\tclient.WithStart(c.Shim, b.shimAddress(ns, daemonAddress), daemonAddress, cgroup, c.ShimDebug, exitHandler)\n\t}\n}\n\n// ShimLocal is a ShimOpt for using an in process shim implementation\nfunc ShimLocal(c *Config, exchange *exchange.Exchange) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\treturn b.shimConfig(ns, c, ropts), client.WithLocal(exchange)\n\t}\n}\n\n// ShimConnect is a ShimOpt for connecting to an existing remote shim\nfunc ShimConnect(c *Config, onClose func()) ShimOpt {\n\treturn func(b *bundle, ns string, ropts *runctypes.RuncOptions) (shim.Config, client.Opt) {\n\t\treturn b.shimConfig(ns, c, ropts), client.WithConnect(b.decideShimAddress(ns), onClose)\n\t}\n}\n\n// NewShimClient connects to the shim managing the bundle and tasks creating it if needed\nfunc (b *bundle) NewShimClient(ctx context.Context, namespace string, getClientOpts ShimOpt, runcOpts *runctypes.RuncOptions) (*client.Client, error) {\n\tcfg, opt := getClientOpts(b, namespace, runcOpts)\n\treturn client.New(ctx, cfg, opt)\n}\n\n// Delete deletes the bundle from disk\nfunc (b *bundle) Delete() error {\n\taddress, _ := b.loadAddress()\n\tif address != \"\" {\n\t\t// we don't care about errors here\n\t\tclient.RemoveSocket(address)\n\t}\n\terr := atomicDelete(b.path)\n\tif err == nil {\n\t\treturn atomicDelete(b.workDir)\n\t}\n\t// error removing the bundle path; still attempt removing work dir\n\terr2 := atomicDelete(b.workDir)\n\tif err2 == nil {\n\t\treturn err\n\t}\n\treturn errors.Wrapf(err, \"Failed to remove both bundle and workdir locations: %v\", err2)\n}\n\nfunc (b *bundle) legacyShimAddress(namespace string) string {\n\treturn filepath.Join(string(filepath.Separator), \"containerd-shim\", namespace, b.id, \"shim.sock\")\n}\n\nconst socketRoot = \"/run/containerd\"\n\nfunc (b *bundle) shimAddress(namespace, socketPath string) string {\n\td := sha256.Sum256([]byte(filepath.Join(socketPath, namespace, b.id)))\n\treturn fmt.Sprintf(\"unix://%s/%x\", filepath.Join(socketRoot, \"s\"), d)\n}\n\nfunc (b *bundle) loadAddress() (string, error) {\n\taddressPath := filepath.Join(b.path, \"address\")\n\tdata, err := ioutil.ReadFile(addressPath)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(data), nil\n}\n\nfunc (b *bundle) decideShimAddress(namespace string) string {\n\taddress, err := b.loadAddress()\n\tif err != nil {\n\t\treturn b.legacyShimAddress(namespace)\n\t}\n\treturn address\n}\n\nfunc (b *bundle) shimConfig(namespace string, c *Config, runcOptions *runctypes.RuncOptions) shim.Config {\n\tvar (\n\t\tcriuPath      string\n\t\truntimeRoot   = c.RuntimeRoot\n\t\tsystemdCgroup bool\n\t)\n\tif runcOptions != nil {\n\t\tcriuPath = runcOptions.CriuPath\n\t\tsystemdCgroup = runcOptions.SystemdCgroup\n\t\tif runcOptions.RuntimeRoot != \"\" {\n\t\t\truntimeRoot = runcOptions.RuntimeRoot\n\t\t}\n\t}\n\treturn shim.Config{\n\t\tPath:          b.path,\n\t\tWorkDir:       b.workDir,\n\t\tNamespace:     namespace,\n\t\tCriu:          criuPath,\n\t\tRuntimeRoot:   runtimeRoot,\n\t\tSystemdCgroup: systemdCgroup,\n\t}\n}\n\n// atomicDelete renames the path to a hidden file before removal\nfunc atomicDelete(path string) error {\n\t// create a hidden dir for an atomic removal\n\tatomicPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tif err := os.Rename(path, atomicPath); err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\treturn os.RemoveAll(atomicPath)\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"golang.org/x/sys/unix\"\n\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\tv1 \"github.com/containerd/containerd/runtime/v1\"\n\t\"github.com/containerd/containerd/runtime/v1/shim\"\n\tshimapi \"github.com/containerd/containerd/runtime/v1/shim/v1\"\n\t\"github.com/containerd/containerd/sys\"\n\tptypes \"github.com/gogo/protobuf/types\"\n)\n\nvar empty = &ptypes.Empty{}\n\n// Opt is an option for a shim client configuration\ntype Opt func(context.Context, shim.Config) (shimapi.ShimService, io.Closer, error)\n\n// WithStart executes a new shim process\nfunc WithStart(binary, address, daemonAddress, cgroup string, debug bool, exitHandler func()) Opt {\n\treturn func(ctx context.Context, config shim.Config) (_ shimapi.ShimService, _ io.Closer, err error) {\n\t\tsocket, err := newSocket(address)\n\t\tif err != nil {\n\t\t\tif !eaddrinuse(err) {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t\tif err := RemoveSocket(address); err != nil {\n\t\t\t\treturn nil, nil, errors.Wrap(err, \"remove already used socket\")\n\t\t\t}\n\t\t\tif socket, err = newSocket(address); err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t}\n\n\t\tf, err := socket.File()\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to get fd for socket %s\", address)\n\t\t}\n\t\tdefer f.Close()\n\n\t\tstdoutCopy := ioutil.Discard\n\t\tstderrCopy := ioutil.Discard\n\t\tstdoutLog, err := v1.OpenShimStdoutLog(ctx, config.WorkDir)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to create stdout log\")\n\t\t}\n\n\t\tstderrLog, err := v1.OpenShimStderrLog(ctx, config.WorkDir)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to create stderr log\")\n\t\t}\n\t\tif debug {\n\t\t\tstdoutCopy = os.Stdout\n\t\t\tstderrCopy = os.Stderr\n\t\t}\n\n\t\tgo io.Copy(stdoutCopy, stdoutLog)\n\t\tgo io.Copy(stderrCopy, stderrLog)\n\n\t\tcmd, err := newCommand(binary, daemonAddress, debug, config, f, stdoutLog, stderrLog)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif err := cmd.Start(); err != nil {\n\t\t\treturn nil, nil, errors.Wrapf(err, \"failed to start shim\")\n\t\t}\n\t\tdefer func() {\n\t\t\tif err != nil {\n\t\t\t\tcmd.Process.Kill()\n\t\t\t}\n\t\t}()\n\t\tgo func() {\n\t\t\tcmd.Wait()\n\t\t\texitHandler()\n\t\t\tif stdoutLog != nil {\n\t\t\t\tstdoutLog.Close()\n\t\t\t}\n\t\t\tif stderrLog != nil {\n\t\t\t\tstderrLog.Close()\n\t\t\t}\n\t\t\tsocket.Close()\n\t\t\tRemoveSocket(address)\n\t\t}()\n\t\tlog.G(ctx).WithFields(logrus.Fields{\n\t\t\t\"pid\":     cmd.Process.Pid,\n\t\t\t\"address\": address,\n\t\t\t\"debug\":   debug,\n\t\t}).Infof(\"shim %s started\", binary)\n\n\t\tif err := writeFile(filepath.Join(config.Path, \"address\"), address); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tif err := writeFile(filepath.Join(config.Path, \"shim.pid\"), strconv.Itoa(cmd.Process.Pid)); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\t// set shim in cgroup if it is provided\n\t\tif cgroup != \"\" {\n\t\t\tif err := setCgroup(cgroup, cmd); err != nil {\n\t\t\t\treturn nil, nil, err\n\t\t\t}\n\t\t\tlog.G(ctx).WithFields(logrus.Fields{\n\t\t\t\t\"pid\":     cmd.Process.Pid,\n\t\t\t\t\"address\": address,\n\t\t\t}).Infof(\"shim placed in cgroup %s\", cgroup)\n\t\t}\n\t\tif err = setupOOMScore(cmd.Process.Pid); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tc, clo, err := WithConnect(address, func() {})(ctx, config)\n\t\tif err != nil {\n\t\t\treturn nil, nil, errors.Wrap(err, \"failed to connect\")\n\t\t}\n\t\treturn c, clo, nil\n\t}\n}\n\nfunc eaddrinuse(err error) bool {\n\tcause := errors.Cause(err)\n\tnetErr, ok := cause.(*net.OpError)\n\tif !ok {\n\t\treturn false\n\t}\n\tif netErr.Op != \"listen\" {\n\t\treturn false\n\t}\n\tsyscallErr, ok := netErr.Err.(*os.SyscallError)\n\tif !ok {\n\t\treturn false\n\t}\n\terrno, ok := syscallErr.Err.(syscall.Errno)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn errno == syscall.EADDRINUSE\n}\n\n// setupOOMScore gets containerd's oom score and adds +1 to it\n// to ensure a shim has a lower* score than the daemons\nfunc setupOOMScore(shimPid int) error {\n\tpid := os.Getpid()\n\tscore, err := sys.GetOOMScoreAdj(pid)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"get daemon OOM score\")\n\t}\n\tshimScore := score + 1\n\tif err := sys.SetOOMScore(shimPid, shimScore); err != nil {\n\t\treturn errors.Wrap(err, \"set shim OOM score\")\n\t}\n\treturn nil\n}\n\nfunc newCommand(binary, daemonAddress string, debug bool, config shim.Config, socket *os.File, stdout, stderr io.Writer) (*exec.Cmd, error) {\n\tselfExe, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", config.Namespace,\n\t\t\"-workdir\", config.WorkDir,\n\t\t\"-address\", daemonAddress,\n\t\t\"-containerd-binary\", selfExe,\n\t}\n\n\tif config.Criu != \"\" {\n\t\targs = append(args, \"-criu-path\", config.Criu)\n\t}\n\tif config.RuntimeRoot != \"\" {\n\t\targs = append(args, \"-runtime-root\", config.RuntimeRoot)\n\t}\n\tif config.SystemdCgroup {\n\t\targs = append(args, \"-systemd-cgroup\")\n\t}\n\tif debug {\n\t\targs = append(args, \"-debug\")\n\t}\n\n\tcmd := exec.Command(binary, args...)\n\tcmd.Dir = config.Path\n\t// make sure the shim can be re-parented to system init\n\t// and is cloned in a new mount namespace because the overlay/filesystems\n\t// will be mounted by the shim\n\tcmd.SysProcAttr = getSysProcAttr()\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, socket)\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=2\")\n\tcmd.Stdout = stdout\n\tcmd.Stderr = stderr\n\treturn cmd, nil\n}\n\n// writeFile writes a address file atomically\nfunc writeFile(path, address string) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = f.WriteString(address)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\nconst (\n\tabstractSocketPrefix = \"\\x00\"\n\tsocketPathLimit      = 106\n)\n\ntype socket string\n\nfunc (s socket) isAbstract() bool {\n\treturn !strings.HasPrefix(string(s), \"unix://\")\n}\n\nfunc (s socket) path() string {\n\tpath := strings.TrimPrefix(string(s), \"unix://\")\n\t// if there was no trim performed, we assume an abstract socket\n\tif len(path) == len(s) {\n\t\tpath = abstractSocketPrefix + path\n\t}\n\treturn path\n}\n\nfunc newSocket(address string) (*net.UnixListener, error) {\n\tif len(address) > socketPathLimit {\n\t\treturn nil, errors.Errorf(\"%q: unix socket path too long (> %d)\", address, socketPathLimit)\n\t}\n\tvar (\n\t\tsock = socket(address)\n\t\tpath = sock.path()\n\t)\n\tif !sock.isAbstract() {\n\t\tif err := os.MkdirAll(filepath.Dir(path), 0600); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"%s\", path)\n\t\t}\n\t}\n\tl, err := net.Listen(\"unix\", path)\n\tif err != nil {\n\t\treturn nil, errors.Wrapf(err, \"failed to listen to unix socket %q (abstract: %t)\", address, sock.isAbstract())\n\t}\n\tif err := os.Chmod(path, 0600); err != nil {\n\t\tl.Close()\n\t\treturn nil, err\n\t}\n\n\treturn l.(*net.UnixListener), nil\n}\n\n// RemoveSocket removes the socket at the specified address if\n// it exists on the filesystem\nfunc RemoveSocket(address string) error {\n\tsock := socket(address)\n\tif !sock.isAbstract() {\n\t\treturn os.Remove(sock.path())\n\t}\n\treturn nil\n}\n\nfunc connect(address string, d func(string, time.Duration) (net.Conn, error)) (net.Conn, error) {\n\treturn d(address, 100*time.Second)\n}\n\nfunc anonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\treturn dialer.Dialer(socket(address).path(), timeout)\n}\n\n// WithConnect connects to an existing shim\nfunc WithConnect(address string, onClose func()) Opt {\n\treturn func(ctx context.Context, config shim.Config) (shimapi.ShimService, io.Closer, error) {\n\t\tconn, err := connect(address, anonDialer)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tclient := ttrpc.NewClient(conn, ttrpc.WithOnClose(onClose))\n\t\treturn shimapi.NewShimClient(client), conn, nil\n\t}\n}\n\n// WithLocal uses an in process shim\nfunc WithLocal(publisher events.Publisher) func(context.Context, shim.Config) (shimapi.ShimService, io.Closer, error) {\n\treturn func(ctx context.Context, config shim.Config) (shimapi.ShimService, io.Closer, error) {\n\t\tservice, err := shim.NewService(config, publisher)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\treturn shim.NewLocal(service), nil, nil\n\t}\n}\n\n// New returns a new shim client\nfunc New(ctx context.Context, config shim.Config, opt Opt) (*Client, error) {\n\ts, c, err := opt(ctx, config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Client{\n\t\tShimService: s,\n\t\tc:           c,\n\t\texitCh:      make(chan struct{}),\n\t}, nil\n}\n\n// Client is a shim client containing the connection to a shim\ntype Client struct {\n\tshimapi.ShimService\n\n\tc        io.Closer\n\texitCh   chan struct{}\n\texitOnce sync.Once\n}\n\n// IsAlive returns true if the shim can be contacted.\n// NOTE: a negative answer doesn't mean that the process is gone.\nfunc (c *Client) IsAlive(ctx context.Context) (bool, error) {\n\t_, err := c.ShimInfo(ctx, empty)\n\tif err != nil {\n\t\t// TODO(stevvooe): There are some error conditions that need to be\n\t\t// handle with unix sockets existence to give the right answer here.\n\t\treturn false, err\n\t}\n\treturn true, nil\n}\n\n// StopShim signals the shim to exit and wait for the process to disappear\nfunc (c *Client) StopShim(ctx context.Context) error {\n\treturn c.signalShim(ctx, unix.SIGTERM)\n}\n\n// KillShim kills the shim forcefully and wait for the process to disappear\nfunc (c *Client) KillShim(ctx context.Context) error {\n\treturn c.signalShim(ctx, unix.SIGKILL)\n}\n\n// Close the client connection\nfunc (c *Client) Close() error {\n\tif c.c == nil {\n\t\treturn nil\n\t}\n\treturn c.c.Close()\n}\n\nfunc (c *Client) signalShim(ctx context.Context, sig syscall.Signal) error {\n\tinfo, err := c.ShimInfo(ctx, empty)\n\tif err != nil {\n\t\treturn err\n\t}\n\tpid := int(info.ShimPid)\n\t// make sure we don't kill ourselves if we are running a local shim\n\tif os.Getpid() == pid {\n\t\treturn nil\n\t}\n\tif err := unix.Kill(pid, sig); err != nil && err != unix.ESRCH {\n\t\treturn err\n\t}\n\t// wait for shim to die after being signaled\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn ctx.Err()\n\tcase <-c.waitForExit(ctx, pid):\n\t\treturn nil\n\t}\n}\n\nfunc (c *Client) waitForExit(ctx context.Context, pid int) <-chan struct{} {\n\tgo c.exitOnce.Do(func() {\n\t\tdefer close(c.exitCh)\n\n\t\tticker := time.NewTicker(10 * time.Millisecond)\n\t\tdefer ticker.Stop()\n\n\t\tfor {\n\t\t\t// use kill(pid, 0) here because the shim could have been reparented\n\t\t\t// and we are no longer able to waitpid(pid, ...) on the shim\n\t\t\tif err := unix.Kill(pid, 0); err == unix.ESRCH {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.G(ctx).WithField(\"pid\", pid).Warn(\"timed out while waiting for shim to exit\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\treturn c.exitCh\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage v1\n\nimport (\n\t\"context\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\teventstypes \"github.com/containerd/containerd/api/events\"\n\t\"github.com/containerd/containerd/api/types/task\"\n\t\"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/mount\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/oom\"\n\toomv1 \"github.com/containerd/containerd/pkg/oom/v1\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\t\"github.com/containerd/containerd/pkg/stdio\"\n\t\"github.com/containerd/containerd/runtime/v2/runc\"\n\t\"github.com/containerd/containerd/runtime/v2/runc/options\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\ttaskAPI \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\truncC \"github.com/containerd/go-runc\"\n\t\"github.com/containerd/typeurl\"\n\t\"github.com/gogo/protobuf/proto\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t_     = (taskAPI.TaskService)(&service{})\n\tempty = &ptypes.Empty{}\n)\n\n// New returns a new shim service that can be used via GRPC\nfunc New(ctx context.Context, id string, publisher shim.Publisher, shutdown func()) (shim.Shim, error) {\n\tep, err := oomv1.New(publisher)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tgo ep.Run(ctx)\n\ts := &service{\n\t\tid:      id,\n\t\tcontext: ctx,\n\t\tevents:  make(chan interface{}, 128),\n\t\tec:      reaper.Default.Subscribe(),\n\t\tep:      ep,\n\t\tcancel:  shutdown,\n\t}\n\tgo s.processExits()\n\truncC.Monitor = reaper.Default\n\tif err := s.initPlatform(); err != nil {\n\t\tshutdown()\n\t\treturn nil, errors.Wrap(err, \"failed to initialized platform behavior\")\n\t}\n\tgo s.forward(ctx, publisher)\n\treturn s, nil\n}\n\n// service is the shim implementation of a remote shim over GRPC\ntype service struct {\n\tmu          sync.Mutex\n\teventSendMu sync.Mutex\n\n\tcontext  context.Context\n\tevents   chan interface{}\n\tplatform stdio.Platform\n\tec       chan runcC.Exit\n\tep       oom.Watcher\n\n\tid        string\n\tcontainer *runc.Container\n\n\tcancel func()\n}\n\nfunc newCommand(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-id\", id,\n\t\t\"-address\", containerdAddress,\n\t}\n\tcmd := exec.Command(self, args...)\n\tcmd.Dir = cwd\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=2\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n\treturn cmd, nil\n}\n\nfunc (s *service) StartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (string, error) {\n\tcmd, err := newCommand(ctx, id, containerdBinary, containerdAddress, containerdTTRPCAddress)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\taddress, err := shim.SocketAddress(ctx, containerdAddress, id)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tsocket, err := shim.NewSocket(address)\n\tif err != nil {\n\t\tif !shim.SocketEaddrinuse(err) {\n\t\t\treturn \"\", err\n\t\t}\n\t\tif err := shim.RemoveSocket(address); err != nil {\n\t\t\treturn \"\", errors.Wrap(err, \"remove already used socket\")\n\t\t}\n\t\tif socket, err = shim.NewSocket(address); err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\tf, err := socket.File()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, f)\n\n\tif err := cmd.Start(); err != nil {\n\t\treturn \"\", err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = shim.RemoveSocket(address)\n\t\t\tcmd.Process.Kill()\n\t\t}\n\t}()\n\t// make sure to wait after start\n\tgo cmd.Wait()\n\tif err := shim.WritePidFile(\"shim.pid\", cmd.Process.Pid); err != nil {\n\t\treturn \"\", err\n\t}\n\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\treturn \"\", err\n\t}\n\tif data, err := ioutil.ReadAll(os.Stdin); err == nil {\n\t\tif len(data) > 0 {\n\t\t\tvar any ptypes.Any\n\t\t\tif err := proto.Unmarshal(data, &any); err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tv, err := typeurl.UnmarshalAny(&any)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tif opts, ok := v.(*options.Options); ok {\n\t\t\t\tif opts.ShimCgroup != \"\" {\n\t\t\t\t\tcg, err := cgroups.Load(cgroups.V1, cgroups.StaticPath(opts.ShimCgroup))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t}\n\t\t\t\t\tif err := cg.Add(cgroups.Process{\n\t\t\t\t\t\tPid: cmd.Process.Pid,\n\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif err := shim.AdjustOOMScore(cmd.Process.Pid); err != nil {\n\t\treturn \"\", errors.Wrap(err, \"failed to adjust OOM score for shim\")\n\t}\n\treturn address, nil\n}\n\nfunc (s *service) Cleanup(ctx context.Context) (*taskAPI.DeleteResponse, error) {\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\truntime, err := runc.ReadRuntime(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\topts, err := runc.ReadOptions(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\troot := process.RuncRoot\n\tif opts != nil && opts.Root != \"\" {\n\t\troot = opts.Root\n\t}\n\n\tr := process.NewRunc(root, path, ns, runtime, \"\", false)\n\tif err := r.Delete(ctx, s.id, &runcC.DeleteOpts{\n\t\tForce: true,\n\t}); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to remove runc container\")\n\t}\n\tif err := mount.UnmountAll(filepath.Join(path, \"rootfs\"), 0); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to cleanup rootfs mount\")\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitedAt:   time.Now(),\n\t\tExitStatus: 128 + uint32(unix.SIGKILL),\n\t}, nil\n}\n\n// Create a new initial process and container with the underlying OCI runtime\nfunc (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tcontainer, err := runc.NewContainer(ctx, s.platform, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.container = container\n\n\ts.send(&eventstypes.TaskCreate{\n\t\tContainerID: r.ID,\n\t\tBundle:      r.Bundle,\n\t\tRootfs:      r.Rootfs,\n\t\tIO: &eventstypes.TaskIO{\n\t\t\tStdin:    r.Stdin,\n\t\t\tStdout:   r.Stdout,\n\t\t\tStderr:   r.Stderr,\n\t\t\tTerminal: r.Terminal,\n\t\t},\n\t\tCheckpoint: r.Checkpoint,\n\t\tPid:        uint32(container.Pid()),\n\t})\n\n\treturn &taskAPI.CreateTaskResponse{\n\t\tPid: uint32(container.Pid()),\n\t}, nil\n}\n\n// Start a process\nfunc (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// hold the send lock so that the start events are sent before any exit events in the error case\n\ts.eventSendMu.Lock()\n\tp, err := container.Start(ctx, r)\n\tif err != nil {\n\t\ts.eventSendMu.Unlock()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tswitch r.ExecID {\n\tcase \"\":\n\t\tif cg, ok := container.Cgroup().(cgroups.Cgroup); ok {\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\t} else {\n\t\t\tlogrus.WithError(errdefs.ErrNotImplemented).Error(\"add cg to OOM monitor\")\n\t\t}\n\t\ts.send(&eventstypes.TaskStart{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\tdefault:\n\t\ts.send(&eventstypes.TaskExecStarted{\n\t\t\tContainerID: container.ID,\n\t\t\tExecID:      r.ExecID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\t}\n\ts.eventSendMu.Unlock()\n\treturn &taskAPI.StartResponse{\n\t\tPid: uint32(p.Pid()),\n\t}, nil\n}\n\n// Delete the initial process and container\nfunc (s *service) Delete(ctx context.Context, r *taskAPI.DeleteRequest) (*taskAPI.DeleteResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Delete(ctx, r)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\t// if we deleted our init task, close the platform and send the task delete event\n\tif r.ExecID == \"\" {\n\t\tif s.platform != nil {\n\t\t\ts.platform.Close()\n\t\t}\n\t\ts.send(&eventstypes.TaskDelete{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t\tExitStatus:  uint32(p.ExitStatus()),\n\t\t\tExitedAt:    p.ExitedAt(),\n\t\t})\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t\tPid:        uint32(p.Pid()),\n\t}, nil\n}\n\n// Exec an additional process inside the container\nfunc (s *service) Exec(ctx context.Context, r *taskAPI.ExecProcessRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tok, cancel := container.ReserveProcess(r.ExecID)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrAlreadyExists, \"id %s\", r.ExecID)\n\t}\n\tprocess, err := container.Exec(ctx, r)\n\tif err != nil {\n\t\tcancel()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\ts.send(&eventstypes.TaskExecAdded{\n\t\tContainerID: s.container.ID,\n\t\tExecID:      process.ID(),\n\t})\n\treturn empty, nil\n}\n\n// ResizePty of a process\nfunc (s *service) ResizePty(ctx context.Context, r *taskAPI.ResizePtyRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.ResizePty(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// State returns runtime state information for a process\nfunc (s *service) State(ctx context.Context, r *taskAPI.StateRequest) (*taskAPI.StateResponse, error) {\n\tp, err := s.getProcess(r.ExecID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tst, err := p.Status(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstatus := task.StatusUnknown\n\tswitch st {\n\tcase \"created\":\n\t\tstatus = task.StatusCreated\n\tcase \"running\":\n\t\tstatus = task.StatusRunning\n\tcase \"stopped\":\n\t\tstatus = task.StatusStopped\n\tcase \"paused\":\n\t\tstatus = task.StatusPaused\n\tcase \"pausing\":\n\t\tstatus = task.StatusPausing\n\t}\n\tsio := p.Stdio()\n\treturn &taskAPI.StateResponse{\n\t\tID:         p.ID(),\n\t\tBundle:     s.container.Bundle,\n\t\tPid:        uint32(p.Pid()),\n\t\tStatus:     status,\n\t\tStdin:      sio.Stdin,\n\t\tStdout:     sio.Stdout,\n\t\tStderr:     sio.Stderr,\n\t\tTerminal:   sio.Terminal,\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Pause the container\nfunc (s *service) Pause(ctx context.Context, r *taskAPI.PauseRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Pause(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskPaused{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Resume the container\nfunc (s *service) Resume(ctx context.Context, r *taskAPI.ResumeRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Resume(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskResumed{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Kill a process with the provided signal\nfunc (s *service) Kill(ctx context.Context, r *taskAPI.KillRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Kill(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Pids returns all pids inside the container\nfunc (s *service) Pids(ctx context.Context, r *taskAPI.PidsRequest) (*taskAPI.PidsResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids, err := s.getContainerPids(ctx, r.ID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tvar processes []*task.ProcessInfo\n\tfor _, pid := range pids {\n\t\tpInfo := task.ProcessInfo{\n\t\t\tPid: pid,\n\t\t}\n\t\tfor _, p := range container.ExecdProcesses() {\n\t\t\tif p.Pid() == int(pid) {\n\t\t\t\td := &options.ProcessDetails{\n\t\t\t\t\tExecID: p.ID(),\n\t\t\t\t}\n\t\t\t\ta, err := typeurl.MarshalAny(d)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrapf(err, \"failed to marshal process %d info\", pid)\n\t\t\t\t}\n\t\t\t\tpInfo.Info = a\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tprocesses = append(processes, &pInfo)\n\t}\n\treturn &taskAPI.PidsResponse{\n\t\tProcesses: processes,\n\t}, nil\n}\n\n// CloseIO of a process\nfunc (s *service) CloseIO(ctx context.Context, r *taskAPI.CloseIORequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.CloseIO(ctx, r); err != nil {\n\t\treturn nil, err\n\t}\n\treturn empty, nil\n}\n\n// Checkpoint the container\nfunc (s *service) Checkpoint(ctx context.Context, r *taskAPI.CheckpointTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Checkpoint(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Update a running container\nfunc (s *service) Update(ctx context.Context, r *taskAPI.UpdateTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Update(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Wait for a process to exit\nfunc (s *service) Wait(ctx context.Context, r *taskAPI.WaitRequest) (*taskAPI.WaitResponse, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tp.Wait()\n\n\treturn &taskAPI.WaitResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Connect returns shim information such as the shim's pid\nfunc (s *service) Connect(ctx context.Context, r *taskAPI.ConnectRequest) (*taskAPI.ConnectResponse, error) {\n\tvar pid int\n\tif s.container != nil {\n\t\tpid = s.container.Pid()\n\t}\n\treturn &taskAPI.ConnectResponse{\n\t\tShimPid: uint32(os.Getpid()),\n\t\tTaskPid: uint32(pid),\n\t}, nil\n}\n\nfunc (s *service) Shutdown(ctx context.Context, r *taskAPI.ShutdownRequest) (*ptypes.Empty, error) {\n\ts.cancel()\n\tclose(s.events)\n\tif address, err := shim.ReadAddress(\"address\"); err == nil {\n\t\t_ = shim.RemoveSocket(address)\n\t}\n\treturn empty, nil\n}\n\nfunc (s *service) Stats(ctx context.Context, r *taskAPI.StatsRequest) (*taskAPI.StatsResponse, error) {\n\tcgx := s.container.Cgroup()\n\tif cgx == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tcg, ok := cgx.(cgroups.Cgroup)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotImplemented, \"cgroup v2 not implemented for Stats\")\n\t}\n\tif cg == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tstats, err := cg.Stat(cgroups.IgnoreNotExist)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdata, err := typeurl.MarshalAny(stats)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &taskAPI.StatsResponse{\n\t\tStats: data,\n\t}, nil\n}\n\nfunc (s *service) processExits() {\n\tfor e := range s.ec {\n\t\ts.checkProcesses(e)\n\t}\n}\n\nfunc (s *service) send(evt interface{}) {\n\ts.events <- evt\n}\n\nfunc (s *service) sendL(evt interface{}) {\n\ts.eventSendMu.Lock()\n\ts.events <- evt\n\ts.eventSendMu.Unlock()\n}\n\nfunc (s *service) checkProcesses(e runcC.Exit) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn\n\t}\n\n\tfor _, p := range container.All() {\n\t\tif p.Pid() == e.Pid {\n\t\t\tif runc.ShouldKillAllOnExit(s.context, container.Bundle) {\n\t\t\t\tif ip, ok := p.(*process.Init); ok {\n\t\t\t\t\t// Ensure all children are killed\n\t\t\t\t\tif err := ip.KillAll(s.context); err != nil {\n\t\t\t\t\t\tlogrus.WithError(err).WithField(\"id\", ip.ID()).\n\t\t\t\t\t\t\tError(\"failed to kill init's children\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.SetExited(e.Status)\n\t\t\ts.sendL(&eventstypes.TaskExit{\n\t\t\t\tContainerID: container.ID,\n\t\t\t\tID:          p.ID(),\n\t\t\t\tPid:         uint32(e.Pid),\n\t\t\t\tExitStatus:  uint32(e.Status),\n\t\t\t\tExitedAt:    p.ExitedAt(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (s *service) getContainerPids(ctx context.Context, id string) ([]uint32, error) {\n\tp, err := s.container.Process(\"\")\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tps, err := p.(*process.Init).Runtime().Ps(ctx, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids := make([]uint32, 0, len(ps))\n\tfor _, pid := range ps {\n\t\tpids = append(pids, uint32(pid))\n\t}\n\treturn pids, nil\n}\n\nfunc (s *service) forward(ctx context.Context, publisher shim.Publisher) {\n\tns, _ := namespaces.Namespace(ctx)\n\tctx = namespaces.WithNamespace(context.Background(), ns)\n\tfor e := range s.events {\n\t\tctx, cancel := context.WithTimeout(ctx, 5*time.Second)\n\t\terr := publisher.Publish(ctx, runc.GetTopic(e), e)\n\t\tcancel()\n\t\tif err != nil {\n\t\t\tlogrus.WithError(err).Error(\"post event\")\n\t\t}\n\t}\n\tpublisher.Close()\n}\n\nfunc (s *service) getContainer() (*runc.Container, error) {\n\ts.mu.Lock()\n\tcontainer := s.container\n\ts.mu.Unlock()\n\tif container == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"container not created\")\n\t}\n\treturn container, nil\n}\n\nfunc (s *service) getProcess(execID string) (process.Process, error) {\n\tcontainer, err := s.getContainer()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(execID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn p, nil\n}\n\n// initialize a single epoll fd to manage our consoles. `initPlatform` should\n// only be called once.\nfunc (s *service) initPlatform() error {\n\tif s.platform != nil {\n\t\treturn nil\n\t}\n\tp, err := runc.NewPlatform()\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.platform = p\n\treturn nil\n}\n", "// +build linux\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage v2\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"sync\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/cgroups\"\n\tcgroupsv2 \"github.com/containerd/cgroups/v2\"\n\teventstypes \"github.com/containerd/containerd/api/events\"\n\t\"github.com/containerd/containerd/api/types/task\"\n\t\"github.com/containerd/containerd/errdefs\"\n\t\"github.com/containerd/containerd/mount\"\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/oom\"\n\toomv1 \"github.com/containerd/containerd/pkg/oom/v1\"\n\toomv2 \"github.com/containerd/containerd/pkg/oom/v2\"\n\t\"github.com/containerd/containerd/pkg/process\"\n\t\"github.com/containerd/containerd/pkg/stdio\"\n\t\"github.com/containerd/containerd/runtime/v2/runc\"\n\t\"github.com/containerd/containerd/runtime/v2/runc/options\"\n\t\"github.com/containerd/containerd/runtime/v2/shim\"\n\ttaskAPI \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/containerd/containerd/sys/reaper\"\n\truncC \"github.com/containerd/go-runc\"\n\t\"github.com/containerd/typeurl\"\n\t\"github.com/gogo/protobuf/proto\"\n\tptypes \"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\nvar (\n\t_     = (taskAPI.TaskService)(&service{})\n\tempty = &ptypes.Empty{}\n)\n\n// group labels specifies how the shim groups services.\n// currently supports a runc.v2 specific .group label and the\n// standard k8s pod label.  Order matters in this list\nvar groupLabels = []string{\n\t\"io.containerd.runc.v2.group\",\n\t\"io.kubernetes.cri.sandbox-id\",\n}\n\ntype spec struct {\n\tAnnotations map[string]string `json:\"annotations,omitempty\"`\n}\n\n// New returns a new shim service that can be used via GRPC\nfunc New(ctx context.Context, id string, publisher shim.Publisher, shutdown func()) (shim.Shim, error) {\n\tvar (\n\t\tep  oom.Watcher\n\t\terr error\n\t)\n\tif cgroups.Mode() == cgroups.Unified {\n\t\tep, err = oomv2.New(publisher)\n\t} else {\n\t\tep, err = oomv1.New(publisher)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tgo ep.Run(ctx)\n\ts := &service{\n\t\tid:         id,\n\t\tcontext:    ctx,\n\t\tevents:     make(chan interface{}, 128),\n\t\tec:         reaper.Default.Subscribe(),\n\t\tep:         ep,\n\t\tcancel:     shutdown,\n\t\tcontainers: make(map[string]*runc.Container),\n\t}\n\tgo s.processExits()\n\truncC.Monitor = reaper.Default\n\tif err := s.initPlatform(); err != nil {\n\t\tshutdown()\n\t\treturn nil, errors.Wrap(err, \"failed to initialized platform behavior\")\n\t}\n\tgo s.forward(ctx, publisher)\n\n\tif address, err := shim.ReadAddress(\"address\"); err == nil {\n\t\ts.shimAddress = address\n\t}\n\treturn s, nil\n}\n\n// service is the shim implementation of a remote shim over GRPC\ntype service struct {\n\tmu          sync.Mutex\n\teventSendMu sync.Mutex\n\n\tcontext  context.Context\n\tevents   chan interface{}\n\tplatform stdio.Platform\n\tec       chan runcC.Exit\n\tep       oom.Watcher\n\n\t// id only used in cleanup case\n\tid string\n\n\tcontainers map[string]*runc.Container\n\n\tshimAddress string\n\tcancel      func()\n}\n\nfunc newCommand(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-id\", id,\n\t\t\"-address\", containerdAddress,\n\t}\n\tcmd := exec.Command(self, args...)\n\tcmd.Dir = cwd\n\tcmd.Env = append(os.Environ(), \"GOMAXPROCS=4\")\n\tcmd.SysProcAttr = &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n\treturn cmd, nil\n}\n\nfunc readSpec() (*spec, error) {\n\tf, err := os.Open(\"config.json\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\tvar s spec\n\tif err := json.NewDecoder(f).Decode(&s); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &s, nil\n}\n\nfunc (s *service) StartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (_ string, retErr error) {\n\tcmd, err := newCommand(ctx, id, containerdBinary, containerdAddress, containerdTTRPCAddress)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tgrouping := id\n\tspec, err := readSpec()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tfor _, group := range groupLabels {\n\t\tif groupID, ok := spec.Annotations[group]; ok {\n\t\t\tgrouping = groupID\n\t\t\tbreak\n\t\t}\n\t}\n\taddress, err := shim.SocketAddress(ctx, containerdAddress, grouping)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tsocket, err := shim.NewSocket(address)\n\tif err != nil {\n\t\t// the only time where this would happen is if there is a bug and the socket\n\t\t// was not cleaned up in the cleanup method of the shim or we are using the\n\t\t// grouping functionality where the new process should be run with the same\n\t\t// shim as an existing container\n\t\tif !shim.SocketEaddrinuse(err) {\n\t\t\treturn \"\", errors.Wrap(err, \"create new shim socket\")\n\t\t}\n\t\tif shim.CanConnect(address) {\n\t\t\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\t\t\treturn \"\", errors.Wrap(err, \"write existing socket for shim\")\n\t\t\t}\n\t\t\treturn address, nil\n\t\t}\n\t\tif err := shim.RemoveSocket(address); err != nil {\n\t\t\treturn \"\", errors.Wrap(err, \"remove pre-existing socket\")\n\t\t}\n\t\tif socket, err = shim.NewSocket(address); err != nil {\n\t\t\treturn \"\", errors.Wrap(err, \"try create new shim socket 2x\")\n\t\t}\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tsocket.Close()\n\t\t\t_ = shim.RemoveSocket(address)\n\t\t}\n\t}()\n\tf, err := socket.File()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tcmd.ExtraFiles = append(cmd.ExtraFiles, f)\n\n\tif err := cmd.Start(); err != nil {\n\t\tf.Close()\n\t\treturn \"\", err\n\t}\n\tdefer func() {\n\t\tif retErr != nil {\n\t\t\tcmd.Process.Kill()\n\t\t}\n\t}()\n\t// make sure to wait after start\n\tgo cmd.Wait()\n\tif err := shim.WriteAddress(\"address\", address); err != nil {\n\t\treturn \"\", err\n\t}\n\tif data, err := ioutil.ReadAll(os.Stdin); err == nil {\n\t\tif len(data) > 0 {\n\t\t\tvar any ptypes.Any\n\t\t\tif err := proto.Unmarshal(data, &any); err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tv, err := typeurl.UnmarshalAny(&any)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t\tif opts, ok := v.(*options.Options); ok {\n\t\t\t\tif opts.ShimCgroup != \"\" {\n\t\t\t\t\tif cgroups.Mode() == cgroups.Unified {\n\t\t\t\t\t\tif err := cgroupsv2.VerifyGroupPath(opts.ShimCgroup); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to verify cgroup path %q\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcg, err := cgroupsv2.LoadManager(\"/sys/fs/cgroup\", opts.ShimCgroup)\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := cg.AddProc(uint64(cmd.Process.Pid)); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\tcg, err := cgroups.Load(cgroups.V1, cgroups.StaticPath(opts.ShimCgroup))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to load cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif err := cg.Add(cgroups.Process{\n\t\t\t\t\t\t\tPid: cmd.Process.Pid,\n\t\t\t\t\t\t}); err != nil {\n\t\t\t\t\t\t\treturn \"\", errors.Wrapf(err, \"failed to join cgroup %s\", opts.ShimCgroup)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif err := shim.AdjustOOMScore(cmd.Process.Pid); err != nil {\n\t\treturn \"\", errors.Wrap(err, \"failed to adjust OOM score for shim\")\n\t}\n\treturn address, nil\n}\n\nfunc (s *service) Cleanup(ctx context.Context) (*taskAPI.DeleteResponse, error) {\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpath := filepath.Join(filepath.Dir(cwd), s.id)\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\truntime, err := runc.ReadRuntime(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\topts, err := runc.ReadOptions(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\troot := process.RuncRoot\n\tif opts != nil && opts.Root != \"\" {\n\t\troot = opts.Root\n\t}\n\n\tr := process.NewRunc(root, path, ns, runtime, \"\", false)\n\tif err := r.Delete(ctx, s.id, &runcC.DeleteOpts{\n\t\tForce: true,\n\t}); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to remove runc container\")\n\t}\n\tif err := mount.UnmountAll(filepath.Join(path, \"rootfs\"), 0); err != nil {\n\t\tlogrus.WithError(err).Warn(\"failed to cleanup rootfs mount\")\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitedAt:   time.Now(),\n\t\tExitStatus: 128 + uint32(unix.SIGKILL),\n\t}, nil\n}\n\n// Create a new initial process and container with the underlying OCI runtime\nfunc (s *service) Create(ctx context.Context, r *taskAPI.CreateTaskRequest) (_ *taskAPI.CreateTaskResponse, err error) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tcontainer, err := runc.NewContainer(ctx, s.platform, r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\ts.containers[r.ID] = container\n\n\ts.send(&eventstypes.TaskCreate{\n\t\tContainerID: r.ID,\n\t\tBundle:      r.Bundle,\n\t\tRootfs:      r.Rootfs,\n\t\tIO: &eventstypes.TaskIO{\n\t\t\tStdin:    r.Stdin,\n\t\t\tStdout:   r.Stdout,\n\t\t\tStderr:   r.Stderr,\n\t\t\tTerminal: r.Terminal,\n\t\t},\n\t\tCheckpoint: r.Checkpoint,\n\t\tPid:        uint32(container.Pid()),\n\t})\n\n\treturn &taskAPI.CreateTaskResponse{\n\t\tPid: uint32(container.Pid()),\n\t}, nil\n}\n\n// Start a process\nfunc (s *service) Start(ctx context.Context, r *taskAPI.StartRequest) (*taskAPI.StartResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// hold the send lock so that the start events are sent before any exit events in the error case\n\ts.eventSendMu.Lock()\n\tp, err := container.Start(ctx, r)\n\tif err != nil {\n\t\ts.eventSendMu.Unlock()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\tswitch r.ExecID {\n\tcase \"\":\n\t\tswitch cg := container.Cgroup().(type) {\n\t\tcase cgroups.Cgroup:\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\tcase *cgroupsv2.Manager:\n\t\t\tallControllers, err := cg.RootControllers()\n\t\t\tif err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"failed to get root controllers\")\n\t\t\t} else {\n\t\t\t\tif err := cg.ToggleControllers(allControllers, cgroupsv2.Enable); err != nil {\n\t\t\t\t\tif sys.RunningInUserNS() {\n\t\t\t\t\t\tlogrus.WithError(err).Debugf(\"failed to enable controllers (%v)\", allControllers)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogrus.WithError(err).Errorf(\"failed to enable controllers (%v)\", allControllers)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := s.ep.Add(container.ID, cg); err != nil {\n\t\t\t\tlogrus.WithError(err).Error(\"add cg to OOM monitor\")\n\t\t\t}\n\t\t}\n\n\t\ts.send(&eventstypes.TaskStart{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\tdefault:\n\t\ts.send(&eventstypes.TaskExecStarted{\n\t\t\tContainerID: container.ID,\n\t\t\tExecID:      r.ExecID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t})\n\t}\n\ts.eventSendMu.Unlock()\n\treturn &taskAPI.StartResponse{\n\t\tPid: uint32(p.Pid()),\n\t}, nil\n}\n\n// Delete the initial process and container\nfunc (s *service) Delete(ctx context.Context, r *taskAPI.DeleteRequest) (*taskAPI.DeleteResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Delete(ctx, r)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\t// if we deleted an init task, send the task delete event\n\tif r.ExecID == \"\" {\n\t\ts.mu.Lock()\n\t\tdelete(s.containers, r.ID)\n\t\ts.mu.Unlock()\n\t\ts.send(&eventstypes.TaskDelete{\n\t\t\tContainerID: container.ID,\n\t\t\tPid:         uint32(p.Pid()),\n\t\t\tExitStatus:  uint32(p.ExitStatus()),\n\t\t\tExitedAt:    p.ExitedAt(),\n\t\t})\n\t}\n\treturn &taskAPI.DeleteResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t\tPid:        uint32(p.Pid()),\n\t}, nil\n}\n\n// Exec an additional process inside the container\nfunc (s *service) Exec(ctx context.Context, r *taskAPI.ExecProcessRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tok, cancel := container.ReserveProcess(r.ExecID)\n\tif !ok {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrAlreadyExists, \"id %s\", r.ExecID)\n\t}\n\tprocess, err := container.Exec(ctx, r)\n\tif err != nil {\n\t\tcancel()\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\n\ts.send(&eventstypes.TaskExecAdded{\n\t\tContainerID: container.ID,\n\t\tExecID:      process.ID(),\n\t})\n\treturn empty, nil\n}\n\n// ResizePty of a process\nfunc (s *service) ResizePty(ctx context.Context, r *taskAPI.ResizePtyRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.ResizePty(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// State returns runtime state information for a process\nfunc (s *service) State(ctx context.Context, r *taskAPI.StateRequest) (*taskAPI.StateResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tst, err := p.Status(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstatus := task.StatusUnknown\n\tswitch st {\n\tcase \"created\":\n\t\tstatus = task.StatusCreated\n\tcase \"running\":\n\t\tstatus = task.StatusRunning\n\tcase \"stopped\":\n\t\tstatus = task.StatusStopped\n\tcase \"paused\":\n\t\tstatus = task.StatusPaused\n\tcase \"pausing\":\n\t\tstatus = task.StatusPausing\n\t}\n\tsio := p.Stdio()\n\treturn &taskAPI.StateResponse{\n\t\tID:         p.ID(),\n\t\tBundle:     container.Bundle,\n\t\tPid:        uint32(p.Pid()),\n\t\tStatus:     status,\n\t\tStdin:      sio.Stdin,\n\t\tStdout:     sio.Stdout,\n\t\tStderr:     sio.Stderr,\n\t\tTerminal:   sio.Terminal,\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Pause the container\nfunc (s *service) Pause(ctx context.Context, r *taskAPI.PauseRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Pause(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskPaused{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Resume the container\nfunc (s *service) Resume(ctx context.Context, r *taskAPI.ResumeRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Resume(ctx); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\ts.send(&eventstypes.TaskResumed{\n\t\tContainerID: container.ID,\n\t})\n\treturn empty, nil\n}\n\n// Kill a process with the provided signal\nfunc (s *service) Kill(ctx context.Context, r *taskAPI.KillRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Kill(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Pids returns all pids inside the container\nfunc (s *service) Pids(ctx context.Context, r *taskAPI.PidsRequest) (*taskAPI.PidsResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids, err := s.getContainerPids(ctx, r.ID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tvar processes []*task.ProcessInfo\n\tfor _, pid := range pids {\n\t\tpInfo := task.ProcessInfo{\n\t\t\tPid: pid,\n\t\t}\n\t\tfor _, p := range container.ExecdProcesses() {\n\t\t\tif p.Pid() == int(pid) {\n\t\t\t\td := &options.ProcessDetails{\n\t\t\t\t\tExecID: p.ID(),\n\t\t\t\t}\n\t\t\t\ta, err := typeurl.MarshalAny(d)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, errors.Wrapf(err, \"failed to marshal process %d info\", pid)\n\t\t\t\t}\n\t\t\t\tpInfo.Info = a\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tprocesses = append(processes, &pInfo)\n\t}\n\treturn &taskAPI.PidsResponse{\n\t\tProcesses: processes,\n\t}, nil\n}\n\n// CloseIO of a process\nfunc (s *service) CloseIO(ctx context.Context, r *taskAPI.CloseIORequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.CloseIO(ctx, r); err != nil {\n\t\treturn nil, err\n\t}\n\treturn empty, nil\n}\n\n// Checkpoint the container\nfunc (s *service) Checkpoint(ctx context.Context, r *taskAPI.CheckpointTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Checkpoint(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Update a running container\nfunc (s *service) Update(ctx context.Context, r *taskAPI.UpdateTaskRequest) (*ptypes.Empty, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := container.Update(ctx, r); err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\treturn empty, nil\n}\n\n// Wait for a process to exit\nfunc (s *service) Wait(ctx context.Context, r *taskAPI.WaitRequest) (*taskAPI.WaitResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(r.ExecID)\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tp.Wait()\n\n\treturn &taskAPI.WaitResponse{\n\t\tExitStatus: uint32(p.ExitStatus()),\n\t\tExitedAt:   p.ExitedAt(),\n\t}, nil\n}\n\n// Connect returns shim information such as the shim's pid\nfunc (s *service) Connect(ctx context.Context, r *taskAPI.ConnectRequest) (*taskAPI.ConnectResponse, error) {\n\tvar pid int\n\tif container, err := s.getContainer(r.ID); err == nil {\n\t\tpid = container.Pid()\n\t}\n\treturn &taskAPI.ConnectResponse{\n\t\tShimPid: uint32(os.Getpid()),\n\t\tTaskPid: uint32(pid),\n\t}, nil\n}\n\nfunc (s *service) Shutdown(ctx context.Context, r *taskAPI.ShutdownRequest) (*ptypes.Empty, error) {\n\ts.mu.Lock()\n\t// return out if the shim is still servicing containers\n\tif len(s.containers) > 0 {\n\t\ts.mu.Unlock()\n\t\treturn empty, nil\n\t}\n\ts.cancel()\n\tclose(s.events)\n\n\tif s.platform != nil {\n\t\ts.platform.Close()\n\t}\n\tif s.shimAddress != \"\" {\n\t\t_ = shim.RemoveSocket(s.shimAddress)\n\t}\n\treturn empty, nil\n}\n\nfunc (s *service) Stats(ctx context.Context, r *taskAPI.StatsRequest) (*taskAPI.StatsResponse, error) {\n\tcontainer, err := s.getContainer(r.ID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcgx := container.Cgroup()\n\tif cgx == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"cgroup does not exist\")\n\t}\n\tvar statsx interface{}\n\tswitch cg := cgx.(type) {\n\tcase cgroups.Cgroup:\n\t\tstats, err := cg.Stat(cgroups.IgnoreNotExist)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatsx = stats\n\tcase *cgroupsv2.Manager:\n\t\tstats, err := cg.Stat()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatsx = stats\n\tdefault:\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotImplemented, \"unsupported cgroup type %T\", cg)\n\t}\n\tdata, err := typeurl.MarshalAny(statsx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &taskAPI.StatsResponse{\n\t\tStats: data,\n\t}, nil\n}\n\nfunc (s *service) processExits() {\n\tfor e := range s.ec {\n\t\ts.checkProcesses(e)\n\t}\n}\n\nfunc (s *service) send(evt interface{}) {\n\ts.events <- evt\n}\n\nfunc (s *service) sendL(evt interface{}) {\n\ts.eventSendMu.Lock()\n\ts.events <- evt\n\ts.eventSendMu.Unlock()\n}\n\nfunc (s *service) checkProcesses(e runcC.Exit) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tfor _, container := range s.containers {\n\t\tif !container.HasPid(e.Pid) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, p := range container.All() {\n\t\t\tif p.Pid() != e.Pid {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif ip, ok := p.(*process.Init); ok {\n\t\t\t\t// Ensure all children are killed\n\t\t\t\tif runc.ShouldKillAllOnExit(s.context, container.Bundle) {\n\t\t\t\t\tif err := ip.KillAll(s.context); err != nil {\n\t\t\t\t\t\tlogrus.WithError(err).WithField(\"id\", ip.ID()).\n\t\t\t\t\t\t\tError(\"failed to kill init's children\")\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tp.SetExited(e.Status)\n\t\t\ts.sendL(&eventstypes.TaskExit{\n\t\t\t\tContainerID: container.ID,\n\t\t\t\tID:          p.ID(),\n\t\t\t\tPid:         uint32(e.Pid),\n\t\t\t\tExitStatus:  uint32(e.Status),\n\t\t\t\tExitedAt:    p.ExitedAt(),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\t\treturn\n\t}\n}\n\nfunc (s *service) getContainerPids(ctx context.Context, id string) ([]uint32, error) {\n\tcontainer, err := s.getContainer(id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp, err := container.Process(\"\")\n\tif err != nil {\n\t\treturn nil, errdefs.ToGRPC(err)\n\t}\n\tps, err := p.(*process.Init).Runtime().Ps(ctx, id)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpids := make([]uint32, 0, len(ps))\n\tfor _, pid := range ps {\n\t\tpids = append(pids, uint32(pid))\n\t}\n\treturn pids, nil\n}\n\nfunc (s *service) forward(ctx context.Context, publisher shim.Publisher) {\n\tns, _ := namespaces.Namespace(ctx)\n\tctx = namespaces.WithNamespace(context.Background(), ns)\n\tfor e := range s.events {\n\t\terr := publisher.Publish(ctx, runc.GetTopic(e), e)\n\t\tif err != nil {\n\t\t\tlogrus.WithError(err).Error(\"post event\")\n\t\t}\n\t}\n\tpublisher.Close()\n}\n\nfunc (s *service) getContainer(id string) (*runc.Container, error) {\n\ts.mu.Lock()\n\tcontainer := s.containers[id]\n\ts.mu.Unlock()\n\tif container == nil {\n\t\treturn nil, errdefs.ToGRPCf(errdefs.ErrNotFound, \"container not created\")\n\t}\n\treturn container, nil\n}\n\n// initialize a single epoll fd to manage our consoles. `initPlatform` should\n// only be called once.\nfunc (s *service) initPlatform() error {\n\tif s.platform != nil {\n\t\treturn nil\n\t}\n\tp, err := runc.NewPlatform()\n\tif err != nil {\n\t\treturn err\n\t}\n\ts.platform = p\n\treturn nil\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/events\"\n\t\"github.com/containerd/containerd/log\"\n\t\"github.com/containerd/containerd/namespaces\"\n\tshimapi \"github.com/containerd/containerd/runtime/v2/task\"\n\t\"github.com/containerd/containerd/version\"\n\t\"github.com/containerd/ttrpc\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n)\n\n// Client for a shim server\ntype Client struct {\n\tservice shimapi.TaskService\n\tcontext context.Context\n\tsignals chan os.Signal\n}\n\n// Publisher for events\ntype Publisher interface {\n\tevents.Publisher\n\tio.Closer\n}\n\n// Init func for the creation of a shim server\ntype Init func(context.Context, string, Publisher, func()) (Shim, error)\n\n// Shim server interface\ntype Shim interface {\n\tshimapi.TaskService\n\tCleanup(ctx context.Context) (*shimapi.DeleteResponse, error)\n\tStartShim(ctx context.Context, id, containerdBinary, containerdAddress, containerdTTRPCAddress string) (string, error)\n}\n\n// OptsKey is the context key for the Opts value.\ntype OptsKey struct{}\n\n// Opts are context options associated with the shim invocation.\ntype Opts struct {\n\tBundlePath string\n\tDebug      bool\n}\n\n// BinaryOpts allows the configuration of a shims binary setup\ntype BinaryOpts func(*Config)\n\n// Config of shim binary options provided by shim implementations\ntype Config struct {\n\t// NoSubreaper disables setting the shim as a child subreaper\n\tNoSubreaper bool\n\t// NoReaper disables the shim binary from reaping any child process implicitly\n\tNoReaper bool\n\t// NoSetupLogger disables automatic configuration of logrus to use the shim FIFO\n\tNoSetupLogger bool\n}\n\nvar (\n\tdebugFlag            bool\n\tversionFlag          bool\n\tidFlag               string\n\tnamespaceFlag        string\n\tsocketFlag           string\n\tbundlePath           string\n\taddressFlag          string\n\tcontainerdBinaryFlag string\n\taction               string\n)\n\nconst (\n\tttrpcAddressEnv = \"TTRPC_ADDRESS\"\n)\n\nfunc parseFlags() {\n\tflag.BoolVar(&debugFlag, \"debug\", false, \"enable debug output in logs\")\n\tflag.BoolVar(&versionFlag, \"v\", false, \"show the shim version and exit\")\n\tflag.StringVar(&namespaceFlag, \"namespace\", \"\", \"namespace that owns the shim\")\n\tflag.StringVar(&idFlag, \"id\", \"\", \"id of the task\")\n\tflag.StringVar(&socketFlag, \"socket\", \"\", \"socket path to serve\")\n\tflag.StringVar(&bundlePath, \"bundle\", \"\", \"path to the bundle if not workdir\")\n\n\tflag.StringVar(&addressFlag, \"address\", \"\", \"grpc address back to main containerd\")\n\tflag.StringVar(&containerdBinaryFlag, \"publish-binary\", \"containerd\", \"path to publish binary (used for publishing events)\")\n\n\tflag.Parse()\n\taction = flag.Arg(0)\n}\n\nfunc setRuntime() {\n\tdebug.SetGCPercent(40)\n\tgo func() {\n\t\tfor range time.Tick(30 * time.Second) {\n\t\t\tdebug.FreeOSMemory()\n\t\t}\n\t}()\n\tif os.Getenv(\"GOMAXPROCS\") == \"\" {\n\t\t// If GOMAXPROCS hasn't been set, we default to a value of 2 to reduce\n\t\t// the number of Go stacks present in the shim.\n\t\truntime.GOMAXPROCS(2)\n\t}\n}\n\nfunc setLogger(ctx context.Context, id string) error {\n\tlogrus.SetFormatter(&logrus.TextFormatter{\n\t\tTimestampFormat: log.RFC3339NanoFixed,\n\t\tFullTimestamp:   true,\n\t})\n\tif debugFlag {\n\t\tlogrus.SetLevel(logrus.DebugLevel)\n\t}\n\tf, err := openLog(ctx, id)\n\tif err != nil {\n\t\treturn err\n\t}\n\tlogrus.SetOutput(f)\n\treturn nil\n}\n\n// Run initializes and runs a shim server\nfunc Run(id string, initFunc Init, opts ...BinaryOpts) {\n\tvar config Config\n\tfor _, o := range opts {\n\t\to(&config)\n\t}\n\tif err := run(id, initFunc, config); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"%s: %s\\n\", id, err)\n\t\tos.Exit(1)\n\t}\n}\n\nfunc run(id string, initFunc Init, config Config) error {\n\tparseFlags()\n\tif versionFlag {\n\t\tfmt.Printf(\"%s:\\n\", os.Args[0])\n\t\tfmt.Println(\"  Version: \", version.Version)\n\t\tfmt.Println(\"  Revision:\", version.Revision)\n\t\tfmt.Println(\"  Go version:\", version.GoVersion)\n\t\tfmt.Println(\"\")\n\t\treturn nil\n\t}\n\n\tsetRuntime()\n\n\tsignals, err := setupSignals(config)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !config.NoSubreaper {\n\t\tif err := subreaper(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tttrpcAddress := os.Getenv(ttrpcAddressEnv)\n\n\tpublisher, err := NewPublisher(ttrpcAddress)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer publisher.Close()\n\n\tif namespaceFlag == \"\" {\n\t\treturn fmt.Errorf(\"shim namespace cannot be empty\")\n\t}\n\tctx := namespaces.WithNamespace(context.Background(), namespaceFlag)\n\tctx = context.WithValue(ctx, OptsKey{}, Opts{BundlePath: bundlePath, Debug: debugFlag})\n\tctx = log.WithLogger(ctx, log.G(ctx).WithField(\"runtime\", id))\n\tctx, cancel := context.WithCancel(ctx)\n\tservice, err := initFunc(ctx, idFlag, publisher, cancel)\n\tif err != nil {\n\t\treturn err\n\t}\n\tswitch action {\n\tcase \"delete\":\n\t\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\t\"pid\":       os.Getpid(),\n\t\t\t\"namespace\": namespaceFlag,\n\t\t})\n\t\tgo handleSignals(ctx, logger, signals)\n\t\tresponse, err := service.Cleanup(ctx)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdata, err := proto.Marshal(response)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := os.Stdout.Write(data); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tcase \"start\":\n\t\taddress, err := service.StartShim(ctx, idFlag, containerdBinaryFlag, addressFlag, ttrpcAddress)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := os.Stdout.WriteString(address); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tdefault:\n\t\tif !config.NoSetupLogger {\n\t\t\tif err := setLogger(ctx, idFlag); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tclient := NewShimClient(ctx, service, signals)\n\t\tif err := client.Serve(); err != nil {\n\t\t\tif err != context.Canceled {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tselect {\n\t\tcase <-publisher.Done():\n\t\t\treturn nil\n\t\tcase <-time.After(5 * time.Second):\n\t\t\treturn errors.New(\"publisher not closed\")\n\t\t}\n\t}\n}\n\n// NewShimClient creates a new shim server client\nfunc NewShimClient(ctx context.Context, svc shimapi.TaskService, signals chan os.Signal) *Client {\n\ts := &Client{\n\t\tservice: svc,\n\t\tcontext: ctx,\n\t\tsignals: signals,\n\t}\n\treturn s\n}\n\n// Serve the shim server\nfunc (s *Client) Serve() error {\n\tdump := make(chan os.Signal, 32)\n\tsetupDumpStacks(dump)\n\n\tpath, err := os.Getwd()\n\tif err != nil {\n\t\treturn err\n\t}\n\tserver, err := newServer()\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed creating server\")\n\t}\n\n\tlogrus.Debug(\"registering ttrpc server\")\n\tshimapi.RegisterTaskService(server, s.service)\n\n\tif err := serve(s.context, server, socketFlag); err != nil {\n\t\treturn err\n\t}\n\tlogger := logrus.WithFields(logrus.Fields{\n\t\t\"pid\":       os.Getpid(),\n\t\t\"path\":      path,\n\t\t\"namespace\": namespaceFlag,\n\t})\n\tgo func() {\n\t\tfor range dump {\n\t\t\tdumpStacks(logger)\n\t\t}\n\t}()\n\treturn handleSignals(s.context, logger, s.signals)\n}\n\n// serve serves the ttrpc API over a unix socket at the provided path\n// this function does not block\nfunc serve(ctx context.Context, server *ttrpc.Server, path string) error {\n\tl, err := serveListener(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tgo func() {\n\t\tif err := server.Serve(ctx, l); err != nil &&\n\t\t\t!strings.Contains(err.Error(), \"use of closed network connection\") {\n\t\t\tlogrus.WithError(err).Fatal(\"containerd-shim: ttrpc server failure\")\n\t\t}\n\t\tl.Close()\n\t\tif address, err := ReadAddress(\"address\"); err == nil {\n\t\t\t_ = RemoveSocket(address)\n\t\t}\n\n\t}()\n\treturn nil\n}\n\nfunc dumpStacks(logger *logrus.Entry) {\n\tvar (\n\t\tbuf       []byte\n\t\tstackSize int\n\t)\n\tbufferLen := 16384\n\tfor stackSize == len(buf) {\n\t\tbuf = make([]byte, bufferLen)\n\t\tstackSize = runtime.Stack(buf, true)\n\t\tbufferLen *= 2\n\t}\n\tbuf = buf[:stackSize]\n\tlogger.Infof(\"=== BEGIN goroutine stack dump ===\\n%s\\n=== END goroutine stack dump ===\", buf)\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"net\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\n\t\"github.com/containerd/containerd/sys/reaper\"\n\t\"github.com/containerd/fifo\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/sirupsen/logrus\"\n\t\"golang.org/x/sys/unix\"\n)\n\n// setupSignals creates a new signal handler for all signals and sets the shim as a\n// sub-reaper so that the container processes are reparented\nfunc setupSignals(config Config) (chan os.Signal, error) {\n\tsignals := make(chan os.Signal, 32)\n\tsmp := []os.Signal{unix.SIGTERM, unix.SIGINT, unix.SIGPIPE}\n\tif !config.NoReaper {\n\t\tsmp = append(smp, unix.SIGCHLD)\n\t}\n\tsignal.Notify(signals, smp...)\n\treturn signals, nil\n}\n\nfunc setupDumpStacks(dump chan<- os.Signal) {\n\tsignal.Notify(dump, syscall.SIGUSR1)\n}\n\nfunc serveListener(path string) (net.Listener, error) {\n\tvar (\n\t\tl   net.Listener\n\t\terr error\n\t)\n\tif path == \"\" {\n\t\tl, err = net.FileListener(os.NewFile(3, \"socket\"))\n\t\tpath = \"[inherited from parent]\"\n\t} else {\n\t\tif len(path) > socketPathLimit {\n\t\t\treturn nil, errors.Errorf(\"%q: unix socket path too long (> %d)\", path, socketPathLimit)\n\t\t}\n\t\tl, err = net.Listen(\"unix\", path)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tlogrus.WithField(\"socket\", path).Debug(\"serving api on socket\")\n\treturn l, nil\n}\n\nfunc handleSignals(ctx context.Context, logger *logrus.Entry, signals chan os.Signal) error {\n\tlogger.Info(\"starting signal loop\")\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase s := <-signals:\n\t\t\tswitch s {\n\t\t\tcase unix.SIGCHLD:\n\t\t\t\tif err := reaper.Reap(); err != nil {\n\t\t\t\t\tlogger.WithError(err).Error(\"reap exit status\")\n\t\t\t\t}\n\t\t\tcase unix.SIGPIPE:\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc openLog(ctx context.Context, _ string) (io.Writer, error) {\n\treturn fifo.OpenFifoDup2(ctx, \"log\", unix.O_WRONLY, 0700, int(os.Stderr.Fd()))\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"net\"\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/gogo/protobuf/proto\"\n\t\"github.com/gogo/protobuf/types\"\n\t\"github.com/pkg/errors\"\n)\n\nvar runtimePaths sync.Map\n\n// Command returns the shim command with the provided args and configuration\nfunc Command(ctx context.Context, runtime, containerdAddress, containerdTTRPCAddress, path string, opts *types.Any, cmdArgs ...string) (*exec.Cmd, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tself, err := os.Executable()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\targs := []string{\n\t\t\"-namespace\", ns,\n\t\t\"-address\", containerdAddress,\n\t\t\"-publish-binary\", self,\n\t}\n\targs = append(args, cmdArgs...)\n\tname := BinaryName(runtime)\n\tif name == \"\" {\n\t\treturn nil, fmt.Errorf(\"invalid runtime name %s, correct runtime name should format like io.containerd.runc.v1\", runtime)\n\t}\n\n\tvar cmdPath string\n\tcmdPathI, cmdPathFound := runtimePaths.Load(name)\n\tif cmdPathFound {\n\t\tcmdPath = cmdPathI.(string)\n\t} else {\n\t\tvar lerr error\n\t\tif cmdPath, lerr = exec.LookPath(name); lerr != nil {\n\t\t\tif eerr, ok := lerr.(*exec.Error); ok {\n\t\t\t\tif eerr.Err == exec.ErrNotFound {\n\t\t\t\t\t// LookPath only finds current directory matches based on\n\t\t\t\t\t// the callers current directory but the caller is not\n\t\t\t\t\t// likely in the same directory as the containerd\n\t\t\t\t\t// executables. Instead match the calling binaries path\n\t\t\t\t\t// (containerd) and see if they are side by side. If so\n\t\t\t\t\t// execute the shim found there.\n\t\t\t\t\ttestPath := filepath.Join(filepath.Dir(self), name)\n\t\t\t\t\tif _, serr := os.Stat(testPath); serr == nil {\n\t\t\t\t\t\tcmdPath = testPath\n\t\t\t\t\t}\n\t\t\t\t\tif cmdPath == \"\" {\n\t\t\t\t\t\treturn nil, errors.Wrapf(os.ErrNotExist, \"runtime %q binary not installed %q\", runtime, name)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcmdPath, err = filepath.Abs(cmdPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif cmdPathI, cmdPathFound = runtimePaths.LoadOrStore(name, cmdPath); cmdPathFound {\n\t\t\t// We didn't store cmdPath we loaded an already cached value. Use it.\n\t\t\tcmdPath = cmdPathI.(string)\n\t\t}\n\t}\n\n\tcmd := exec.Command(cmdPath, args...)\n\tcmd.Dir = path\n\tcmd.Env = append(\n\t\tos.Environ(),\n\t\t\"GOMAXPROCS=2\",\n\t\tfmt.Sprintf(\"%s=%s\", ttrpcAddressEnv, containerdTTRPCAddress),\n\t)\n\tcmd.SysProcAttr = getSysProcAttr()\n\tif opts != nil {\n\t\td, err := proto.Marshal(opts)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcmd.Stdin = bytes.NewReader(d)\n\t}\n\treturn cmd, nil\n}\n\n// BinaryName returns the shim binary name from the runtime name,\n// empty string returns means runtime name is invalid\nfunc BinaryName(runtime string) string {\n\t// runtime name should format like $prefix.name.version\n\tparts := strings.Split(runtime, \".\")\n\tif len(parts) < 2 {\n\t\treturn \"\"\n\t}\n\n\treturn fmt.Sprintf(shimBinaryFormat, parts[len(parts)-2], parts[len(parts)-1])\n}\n\n// Connect to the provided address\nfunc Connect(address string, d func(string, time.Duration) (net.Conn, error)) (net.Conn, error) {\n\treturn d(address, 100*time.Second)\n}\n\n// WritePidFile writes a pid file atomically\nfunc WritePidFile(path string, pid int) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = fmt.Fprintf(f, \"%d\", pid)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\n// WriteAddress writes a address file atomically\nfunc WriteAddress(path, address string) error {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttempPath := filepath.Join(filepath.Dir(path), fmt.Sprintf(\".%s\", filepath.Base(path)))\n\tf, err := os.OpenFile(tempPath, os.O_RDWR|os.O_CREATE|os.O_EXCL|os.O_SYNC, 0666)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = f.WriteString(address)\n\tf.Close()\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn os.Rename(tempPath, path)\n}\n\n// ErrNoAddress is returned when the address file has no content\nvar ErrNoAddress = errors.New(\"no shim address\")\n\n// ReadAddress returns the shim's socket address from the path\nfunc ReadAddress(path string) (string, error) {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tdata, err := ioutil.ReadFile(path)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tif len(data) == 0 {\n\t\treturn \"\", ErrNoAddress\n\t}\n\treturn string(data), nil\n}\n", "// +build !windows\n\n/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/containerd/containerd/namespaces\"\n\t\"github.com/containerd/containerd/pkg/dialer\"\n\t\"github.com/containerd/containerd/sys\"\n\t\"github.com/pkg/errors\"\n)\n\nconst (\n\tshimBinaryFormat = \"containerd-shim-%s-%s\"\n\tsocketPathLimit  = 106\n)\n\nfunc getSysProcAttr() *syscall.SysProcAttr {\n\treturn &syscall.SysProcAttr{\n\t\tSetpgid: true,\n\t}\n}\n\n// SetScore sets the oom score for a process\nfunc SetScore(pid int) error {\n\treturn sys.SetOOMScore(pid, sys.OOMScoreMaxKillable)\n}\n\n// AdjustOOMScore sets the OOM score for the process to the parents OOM score +1\n// to ensure that they parent has a lower* score than the shim\nfunc AdjustOOMScore(pid int) error {\n\tparent := os.Getppid()\n\tscore, err := sys.GetOOMScoreAdj(parent)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"get parent OOM score\")\n\t}\n\tshimScore := score + 1\n\tif err := sys.SetOOMScore(pid, shimScore); err != nil {\n\t\treturn errors.Wrap(err, \"set shim OOM score\")\n\t}\n\treturn nil\n}\n\nconst socketRoot = \"/run/containerd\"\n\n// SocketAddress returns a socket address\nfunc SocketAddress(ctx context.Context, socketPath, id string) (string, error) {\n\tns, err := namespaces.NamespaceRequired(ctx)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\td := sha256.Sum256([]byte(filepath.Join(socketPath, ns, id)))\n\treturn fmt.Sprintf(\"unix://%s/%x\", filepath.Join(socketRoot, \"s\"), d), nil\n}\n\n// AnonDialer returns a dialer for a socket\nfunc AnonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\treturn dialer.Dialer(socket(address).path(), timeout)\n}\n\nfunc AnonReconnectDialer(address string, timeout time.Duration) (net.Conn, error) {\n\treturn AnonDialer(address, timeout)\n}\n\n// NewSocket returns a new socket\nfunc NewSocket(address string) (*net.UnixListener, error) {\n\tvar (\n\t\tsock = socket(address)\n\t\tpath = sock.path()\n\t)\n\tif !sock.isAbstract() {\n\t\tif err := os.MkdirAll(filepath.Dir(path), 0600); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"%s\", path)\n\t\t}\n\t}\n\tl, err := net.Listen(\"unix\", path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := os.Chmod(path, 0600); err != nil {\n\t\tos.Remove(sock.path())\n\t\tl.Close()\n\t\treturn nil, err\n\t}\n\treturn l.(*net.UnixListener), nil\n}\n\nconst abstractSocketPrefix = \"\\x00\"\n\ntype socket string\n\nfunc (s socket) isAbstract() bool {\n\treturn !strings.HasPrefix(string(s), \"unix://\")\n}\n\nfunc (s socket) path() string {\n\tpath := strings.TrimPrefix(string(s), \"unix://\")\n\t// if there was no trim performed, we assume an abstract socket\n\tif len(path) == len(s) {\n\t\tpath = abstractSocketPrefix + path\n\t}\n\treturn path\n}\n\n// RemoveSocket removes the socket at the specified address if\n// it exists on the filesystem\nfunc RemoveSocket(address string) error {\n\tsock := socket(address)\n\tif !sock.isAbstract() {\n\t\treturn os.Remove(sock.path())\n\t}\n\treturn nil\n}\n\n// SocketEaddrinuse returns true if the provided error is caused by the\n// EADDRINUSE error number\nfunc SocketEaddrinuse(err error) bool {\n\tnetErr, ok := err.(*net.OpError)\n\tif !ok {\n\t\treturn false\n\t}\n\tif netErr.Op != \"listen\" {\n\t\treturn false\n\t}\n\tsyscallErr, ok := netErr.Err.(*os.SyscallError)\n\tif !ok {\n\t\treturn false\n\t}\n\terrno, ok := syscallErr.Err.(syscall.Errno)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn errno == syscall.EADDRINUSE\n}\n\n// CanConnect returns true if the socket provided at the address\n// is accepting new connections\nfunc CanConnect(address string) bool {\n\tconn, err := AnonDialer(address, 100*time.Millisecond)\n\tif err != nil {\n\t\treturn false\n\t}\n\tconn.Close()\n\treturn true\n}\n", "/*\n   Copyright The containerd Authors.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n*/\n\npackage shim\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"os\"\n\t\"syscall\"\n\t\"time\"\n\n\twinio \"github.com/Microsoft/go-winio\"\n\t\"github.com/pkg/errors\"\n)\n\nconst shimBinaryFormat = \"containerd-shim-%s-%s.exe\"\n\nfunc getSysProcAttr() *syscall.SysProcAttr {\n\treturn nil\n}\n\n// AnonReconnectDialer returns a dialer for an existing npipe on containerd reconnection\nfunc AnonReconnectDialer(address string, timeout time.Duration) (net.Conn, error) {\n\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n\tdefer cancel()\n\n\tc, err := winio.DialPipeContext(ctx, address)\n\tif os.IsNotExist(err) {\n\t\treturn nil, errors.Wrap(os.ErrNotExist, \"npipe not found on reconnect\")\n\t} else if err == context.DeadlineExceeded {\n\t\treturn nil, errors.Wrapf(err, \"timed out waiting for npipe %s\", address)\n\t} else if err != nil {\n\t\treturn nil, err\n\t}\n\treturn c, nil\n}\n\n// AnonDialer returns a dialer for a npipe\nfunc AnonDialer(address string, timeout time.Duration) (net.Conn, error) {\n\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n\tdefer cancel()\n\n\t// If there is nobody serving the pipe we limit the timeout for this case to\n\t// 5 seconds because any shim that would serve this endpoint should serve it\n\t// within 5 seconds.\n\tserveTimer := time.NewTimer(5 * time.Second)\n\tdefer serveTimer.Stop()\n\tfor {\n\t\tc, err := winio.DialPipeContext(ctx, address)\n\t\tif err != nil {\n\t\t\tif os.IsNotExist(err) {\n\t\t\t\tselect {\n\t\t\t\tcase <-serveTimer.C:\n\t\t\t\t\treturn nil, errors.Wrap(os.ErrNotExist, \"pipe not found before timeout\")\n\t\t\t\tdefault:\n\t\t\t\t\t// Wait 10ms for the shim to serve and try again.\n\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if err == context.DeadlineExceeded {\n\t\t\t\treturn nil, errors.Wrapf(err, \"timed out waiting for npipe %s\", address)\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\treturn c, nil\n\t}\n}\n\n// RemoveSocket removes the socket at the specified address if\n// it exists on the filesystem\nfunc RemoveSocket(address string) error {\n\treturn nil\n}\n"], "filenames": ["cmd/containerd-shim/main_unix.go", "cmd/ctr/commands/shim/shim.go", "runtime/v1/linux/bundle.go", "runtime/v1/shim/client/client.go", "runtime/v2/runc/v1/service.go", "runtime/v2/runc/v2/service.go", "runtime/v2/shim/shim.go", "runtime/v2/shim/shim_unix.go", "runtime/v2/shim/util.go", "runtime/v2/shim/util_unix.go", "runtime/v2/shim/util_windows.go"], "buggy_code_start_loc": [74, 26, 94, 62, 134, 28, 107, 61, 172, 38, 81], "buggy_code_end_loc": [209, 247, 139, 242, 553, 656, 307, 70, 173, 96, 81], "fixing_code_start_loc": [74, 27, 94, 62, 134, 27, 107, 61, 172, 38, 82], "fixing_code_end_loc": [217, 249, 146, 314, 564, 679, 311, 70, 173, 171, 88], "type": "CWE-669", "message": "containerd is an industry-standard container runtime and is available as a daemon for Linux and Windows. In containerd before versions 1.3.9 and 1.4.3, the containerd-shim API is improperly exposed to host network containers. Access controls for the shim\u2019s API socket verified that the connecting process had an effective UID of 0, but did not otherwise restrict access to the abstract Unix domain socket. This would allow malicious containers running in the same network namespace as the shim, with an effective UID of 0 but otherwise reduced privileges, to cause new processes to be run with elevated privileges. This vulnerability has been fixed in containerd 1.3.9 and 1.4.3. Users should update to these versions as soon as they are released. It should be noted that containers started with an old version of containerd-shim should be stopped and restarted, as running containers will continue to be vulnerable even after an upgrade. If you are not providing the ability for untrusted users to start containers in the same network namespace as the shim (typically the \"host\" network namespace, for example with docker run --net=host or hostNetwork: true in a Kubernetes pod) and run with an effective UID of 0, you are not vulnerable to this issue. If you are running containers with a vulnerable configuration, you can deny access to all abstract sockets with AppArmor by adding a line similar to deny unix addr=@**, to your policy. It is best practice to run containers with a reduced set of privileges, with a non-zero UID, and with isolated namespaces. The containerd maintainers strongly advise against sharing namespaces with the host. Reducing the set of isolation mechanisms used for a container necessarily increases that container's privilege, regardless of what container runtime is used for running that container.", "other": {"cve": {"id": "CVE-2020-15257", "sourceIdentifier": "security-advisories@github.com", "published": "2020-12-01T03:15:11.257", "lastModified": "2022-01-01T18:11:31.360", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "containerd is an industry-standard container runtime and is available as a daemon for Linux and Windows. In containerd before versions 1.3.9 and 1.4.3, the containerd-shim API is improperly exposed to host network containers. Access controls for the shim\u2019s API socket verified that the connecting process had an effective UID of 0, but did not otherwise restrict access to the abstract Unix domain socket. This would allow malicious containers running in the same network namespace as the shim, with an effective UID of 0 but otherwise reduced privileges, to cause new processes to be run with elevated privileges. This vulnerability has been fixed in containerd 1.3.9 and 1.4.3. Users should update to these versions as soon as they are released. It should be noted that containers started with an old version of containerd-shim should be stopped and restarted, as running containers will continue to be vulnerable even after an upgrade. If you are not providing the ability for untrusted users to start containers in the same network namespace as the shim (typically the \"host\" network namespace, for example with docker run --net=host or hostNetwork: true in a Kubernetes pod) and run with an effective UID of 0, you are not vulnerable to this issue. If you are running containers with a vulnerable configuration, you can deny access to all abstract sockets with AppArmor by adding a line similar to deny unix addr=@**, to your policy. It is best practice to run containers with a reduced set of privileges, with a non-zero UID, and with isolated namespaces. The containerd maintainers strongly advise against sharing namespaces with the host. Reducing the set of isolation mechanisms used for a container necessarily increases that container's privilege, regardless of what container runtime is used for running that container."}, {"lang": "es", "value": "containerd es un tiempo de ejecuci\u00f3n de contenedor est\u00e1ndar de la industria y est\u00e1 disponible como demonio para Linux y Windows. En containerd anterior a las versiones 1.3.9 y 1.4.3, la API containerd-shim est\u00e1 expuesta inapropiadamente a los contenedores de red del host. Los controles de acceso para el socket de la API de shim verificaron que el proceso de conexi\u00f3n tuviera un UID efectivo de 0, pero no restringieron de otra manera el acceso al socket de dominio Unix abstracto. Esto permitir\u00eda que los contenedores maliciosos se ejecuten en el mismo espacio de nombres de red que el shim, con un UID efectivo de 0 pero con privilegios reducidos, para causar que nuevos procesos se ejecuten con privilegios elevados. Esta vulnerabilidad se ha corregido en containerd versiones 1.3.9 y 1.4.3. Los usuarios deben actualizar a estas versiones tan pronto como se publiquen. Cabe se\u00f1alar que los contenedores iniciados con una versi\u00f3n anterior de containerd-shim deben detenerse y reiniciarse, ya que los contenedores en ejecuci\u00f3n seguir\u00e1n siendo vulnerables inclusive despu\u00e9s de una actualizaci\u00f3n. Si no proporciona la capacidad para que los usuarios que no son de confianza inicien contenedores en el mismo espacio de nombres de red que el shim (normalmente el espacio de nombres de red \"host\", por ejemplo, con docker run --net=host o hostNetwork: true en un pod de Kubernetes) y ejecutar con un UID efectivo de 0, no es vulnerable a este problema. Si est\u00e1 ejecutando contenedores con una configuraci\u00f3n vulnerable, puede denegar el acceso a todos los sockets abstractos con AppArmor agregando una l\u00ednea similar a denegar unix addr=@**, para su pol\u00edtica. Es una buena pr\u00e1ctica ejecutar contenedores con un conjunto reducido de privilegios, con un UID distinto de cero y con espacios de nombres aislados. Los encargados de mantenimiento de contenedores no aconsejan compartir espacios de nombres con el host. Reducir el conjunto de mecanismos de aislamiento usados para un contenedor necesariamente aumenta el privilegio de ese contenedor, independientemente del tiempo de ejecuci\u00f3n del contenedor que se use para ejecutar ese contenedor"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:L/I:L/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 5.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 2.7}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:L/I:L/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 5.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 2.7}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-669"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-669"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:linuxfoundation:containerd:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.3.9", "matchCriteriaId": "AE0C7E47-205B-4949-88A5-E5885F9F3C8B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:linuxfoundation:containerd:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.4.0", "versionEndExcluding": "1.4.3", "matchCriteriaId": "043B8600-6802-4946-89C1-A3CC3FC50112"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:33:*:*:*:*:*:*:*", "matchCriteriaId": "E460AA51-FCDA-46B9-AE97-E6676AA5E194"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}], "references": [{"url": "https://github.com/containerd/containerd/commit/4a4bb851f5da563ff6e68a83dc837c7699c469ad", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/containerd/containerd/releases/tag/v1.4.3", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/containerd/containerd/security/advisories/GHSA-36xw-fx78-c5r4", "source": "security-advisories@github.com", "tags": ["Mitigation", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/LNKXLOLZWO5FMAPX63ZL7JNKTNNT5NQD/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/202105-33", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2021/dsa-4865", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/containerd/containerd/commit/4a4bb851f5da563ff6e68a83dc837c7699c469ad"}}