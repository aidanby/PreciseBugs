{"buggy_code": ["//! Implements a barebones client to the Google Cloud TTS service\n\nuse anyhow::{anyhow, bail, Context, Error as AnyError};\nuse bytes::Bytes;\nuse serde::Deserialize;\n\nuse core::iter;\n\n/// Path to the file that holds the Google Cloud API key\nconst API_KEY_FILE: &str = \"gcp_api.key\";\n\n//const GCP_API_BASE: &str = \"https://texttospeech.googleapis.com/v1\";\nconst GCP_TTS_API: &str = \"https://texttospeech.googleapis.com/v1beta1/text:synthesize\";\n\n// See https://cloud.google.com/text-to-speech/quotas\nconst MAX_CHARS_PER_REQUEST: usize = 5000;\n\n#[derive(Deserialize)]\nstruct AudioResponse<'a> {\n    #[serde(borrow, rename = \"audioContent\")]\n    audio_content: &'a str,\n}\n\n/// Denotes the voice quality options with Google Cloud\n#[derive(Copy, Clone, Eq, PartialEq)]\npub(crate) enum VoiceQuality {\n    /// The standard quality\n    Standard,\n    /// Represents Neural2 or Wavenet if the Neural2 version of whatever the client wants is not\n    /// available\n    High,\n}\n\n/// Voices are only classified as high or low pitch\n#[derive(Copy, Clone, Eq, PartialEq)]\npub(crate) enum VoiceType {\n    HighPitch,\n    LowPitch,\n}\n\n/// The description of a Google Cloud TTS reading voice\n#[derive(Clone, Copy)]\npub(crate) struct GcpVoice {\n    /// The unique voice identifier\n    pub(crate) id: &'static str,\n    // The English description of this voice. E.g., \"Portuguese (Brazil)\"\n    pub(crate) english_desc: &'static str,\n    /// The type of voice this is (high/low)\n    pub(crate) ty: VoiceType,\n}\n\n#[derive(Clone, Debug)]\npub(crate) struct TtsRequest {\n    /// The contents of the request\n    pub text: String,\n    /// The Google TTS voice to use\n    pub voice_name: &'static str,\n}\n\nimpl TtsRequest {\n    fn into_json(&self) -> serde_json::Value {\n        serde_json::json!({\n            \"input\": {\n                \"text\": self.text\n            },\n            \"voice\":{\n                \"languageCode\":\"en-US\",\n                \"name\": self.voice_name,\n            },\n            \"audioConfig\":{\n                \"audioEncoding\": \"MP3_64_KBPS\",\n                \"sampleRateHertz\": 48000\n            }\n        })\n    }\n}\n\npub(crate) fn get_api_key() -> Result<String, AnyError> {\n    std::fs::read_to_string(API_KEY_FILE).map_err(|e| {\n        anyhow!(\n            \"Could not open API key file {API_KEY_FILE}. \\\n            Read the README for info about how to get an API key. {:?}\",\n            e,\n        )\n    })\n}\n\n/// Speaks text string of length at most MAX_CHARS_PER_REQUEST. Returns an error if length exceeds,\n/// or an error occurs in the Google Cloud API call.\npub(crate) async fn tts_single(api_key: &str, req: &TtsRequest) -> Result<Bytes, AnyError> {\n    let payload = req.into_json();\n\n    // The Google API has a hard upper limit on characters per request. The text breaking before\n    // this point should ensure this limit is never exceeded\n    if req.text.len() > MAX_CHARS_PER_REQUEST {\n        bail!(\"TTS request is too long\");\n    }\n\n    // Do the HTTP request\n    let client = reqwest::Client::new();\n    let url = reqwest::Url::parse_with_params(GCP_TTS_API, &[(\"key\", api_key)])?;\n    let res = client\n        .post(url)\n        .json(&payload)\n        .send()\n        .await\n        .with_context(|| \"Couldn't make TTS request\")?\n        .error_for_status()\n        .with_context(|| \"TTS request failed\")?;\n\n    // The resulting JSON response has our MP3 data\n    let res_bytes = res.bytes().await?;\n    let audio_response: AudioResponse = serde_json::from_slice(&res_bytes)?;\n    let audio_blob = Bytes::from(base64::decode(audio_response.audio_content)?);\n\n    Ok(audio_blob)\n}\n\n/// Speaks text string. Returns an error if an error occurs in the Google Cloud API call.\npub(crate) async fn tts(\n    api_key: &str,\n    TtsRequest { text, voice_name }: TtsRequest,\n) -> Result<Bytes, AnyError> {\n    let api_key_iter = core::iter::repeat(api_key);\n\n    // Break up the TTS tasks into smaller ones of at most MAX_CHARS_PER_REQUEST\n    let tts_tasks = break_english_text(&text, MAX_CHARS_PER_REQUEST)?\n        .into_iter()\n        .zip(api_key_iter)\n        .map(|(slice, api_key)| {\n            let slice = slice.to_string();\n            let api_key = api_key.to_string();\n            async move {\n                let slice_req = TtsRequest {\n                    text: slice,\n                    voice_name,\n                };\n                tts_single(&api_key, &slice_req).await\n            }\n        });\n\n    // Do the tasks in parallel. If one task fails, try_join_all will cancel the rest of them\n    // immediately. This prevents us from wasting API calls.\n    let mp3_blobs = futures::future::try_join_all(tts_tasks).await?;\n    // Concat the resulting MP3 blobs. Fun fact: the concatenation of MP3 files is itself a valid\n    // MP3 file.\n    let final_mp3: Bytes = mp3_blobs.concat().into();\n\n    Ok(final_mp3)\n}\n\n// Helper function that finds the next index i of the delimiter in the text such that txt[0, i]\n// is below the chunk limit. If no such i is found, then the first occurance of the delimiter\n// is returned (and text[0, i] is too big). If no delimiter occurs at all, txt.len() is\n// returned.\nfn next_break(text: &str, delim: char, max_chunk_size: usize) -> usize {\n    // Keep track of the last break that keeps us under the chunk size limit\n    let mut last_candidate_break = None;\n\n    // Find all the delimiters\n    let delim_indices = text.match_indices(delim).map(|(i, _)| i);\n\n    // Make the last breakpoint the EOF, otherwise we'd just be counting span size between\n    // delims (leaving out the span between the final delim and EOF)\n    let eof = text.len();\n\n    for cur_break in delim_indices.chain(iter::once(eof)) {\n        if cur_break != eof {\n            assert_eq!(text.split_at(cur_break).1.chars().next().unwrap(), delim);\n        }\n        if cur_break <= max_chunk_size {\n            last_candidate_break = Some(cur_break);\n        } else {\n            // The current break puts us over the chunk limit. Split at the last break if\n            // possible.\n            return last_candidate_break.unwrap_or(cur_break);\n        }\n    }\n\n    // If we made it to the end, then the text is now small enough to be a chunk in itself, and\n    // we're done.\n    return eof;\n}\n\n/// Attempts to break the given text into chunks of size at most `max_chunk_size`, making chunks as\n/// large as possible. The only allowed break point is `delim` characters. This is best-effort,\n/// meaning that there may be chunks returned which exceed `max_chunk_size`.\npub(crate) fn break_greedily_at_delim(\n    mut text: &str,\n    delim: char,\n    max_chunk_size: usize,\n) -> Vec<&str> {\n    let mut chunks = Vec::new();\n\n    while !text.is_empty() {\n        // While the text is not fully chunked, find the next break and break it up.\n        let b = next_break(text, delim, max_chunk_size);\n        let (chunk, mut rest) = text.split_at(b);\n\n        // Strip the leading punctuation off of the remainder of the text\n        if rest.chars().nth(0) == Some(delim) {\n            rest = &rest[1..];\n        }\n\n        // Save the chunk and truncate the text\n        chunks.push(chunk);\n        text = rest;\n    }\n\n    chunks\n}\n\n/// Attempts to break the given text into chunks of size at most `max_chunk_size`, making chunks as\n/// large as possible. The delimiters given are used in decreasing order of preference. If the text\n/// can be broken at just newlines, for example, that'd be preferred. But if there's a very large\n/// paragraph, then we have to break at periods. And if there's a very long sentence, we have to\n/// break on commas, etc.\nfn break_greedily_at_delims<'a>(\n    text: &'a str,\n    max_chunk_size: usize,\n    delims: &[char],\n) -> Result<Vec<&'a str>, AnyError> {\n    // Break the text into the f irst delim first\n    let mut chunks = break_greedily_at_delim(text, delims[0], max_chunk_size);\n\n    // If there are chunks greater than max_chunk_size, break them up. Use every delimiter if need\n    // be.\n    for &delim in &delims[1..] {\n        chunks = chunks\n            .into_iter()\n            .flat_map(|chunk| {\n                if chunk.len() > max_chunk_size {\n                    break_greedily_at_delim(&chunk, delim, max_chunk_size)\n                } else {\n                    vec![chunk]\n                }\n            })\n            .collect();\n    }\n\n    // Now check that everything was broken into sufficiently small pieces\n    for chunk in &chunks {\n        if chunk.len() > max_chunk_size {\n            bail!(\"Couldn't break text chunk {:?}\", chunk);\n        }\n    }\n\n    Ok(chunks)\n}\n\n/// Breaks English text into bounded-size chunks by newlines, then (if necessary) colons,\n/// then periods, then commas.\nfn break_english_text(text: &str, max_chunk_size: usize) -> Result<Vec<&str>, AnyError> {\n    break_greedily_at_delims(text, max_chunk_size, &['\\n', ':', '.', ','])\n}\n\n#[test]\nfn text_breaking() {\n    let text = \"\\\n        Mr James Duffy lived in Chapelizod because he wished to live as far as possible from the \\\n        city of which he was a citizen and because he found all the other suburbs of Dublin mean, \\\n        modern and pretentious.\n        He lived in an old sombre house and from his windows he could look into the disused \\\n        distillery or upwards along the shallow river on which Dublin is built. The lofty walls \\\n        of his uncarpeted room were free from pictures. He had himself bought every article of \\\n        furniture in the room: a black iron bedstead, an iron washstand, four cane chairs, a \\\n        clothes-rack, a coal-scuttle, a fender and irons and a square table on which lay a double \\\n        desk. A bookcase had been made in an alcove by means of shelves of white wood. The bed \\\n        was clothed with white bedclothes and a black and scarlet rug covered the foot. A little \\\n        hand-mirror hung above the washstand and during the day a white-shaded lamp stood as the \\\n        sole ornament of the mantelpiece. The books on the white wooden shelves were arranged \\\n        from below upwards according to bulk. A complete Wordsworth stood at one end of the \\\n        lowest shelf and a copy of the Maynooth Catechism, sewn into the cloth cover of a \\\n        notebook, stood at one end of the top shelf. Writing materials were always on the desk. \\\n        In the desk lay a manuscript translation of Hauptmann\u2019s Michael Kramer, the stage \\\n        directions of which were written in purple ink, and a little sheaf of papers held \\\n        together by a brass pin. In these sheets a sentence was inscribed from time to time and, \\\n        in an ironical moment, the headline of an advertisement for Bile Beans had been pasted on \\\n        to the first sheet. On lifting the lid of the desk a faint fragrance escaped\u2014the \\\n        fragrance of new cedarwood pencils or of a bottle of gum or of an overripe apple which \\\n        might have been left there and forgotten.\\\n    \";\n    let chunk_size = 220;\n    let chunks = break_english_text(text, chunk_size).unwrap();\n\n    // Make sure the chunk sizes add up the the total text size, minus the delimiters which were\n    // deleted (there's chunks.len() - 1 deleted delimiters).\n    assert_eq!(\n        chunks.iter().map(|c| c.len()).sum::<usize>(),\n        text.len() - (chunks.len() - 1)\n    );\n\n    // Make sure the chunks are nontrivial and don't exceed the chunk_size\n    for chunk in chunks {\n        assert!(chunk.len() > 1);\n        assert!(chunk.len() <= chunk_size);\n    }\n\n    //\n    // Test for a short text sample\n    //\n\n    let text = \"\\\n        Mr James Duffy lived in Chapelizod because he wished to live as far as possible from the \\\n        city of which he was a citizen and because he found all the other suburbs of Dublin mean, \\\n        modern and pretentious.\\\n    \";\n    let chunks = break_english_text(text, chunk_size).unwrap();\n    // Make sure there's just one chunk and it's the length of the whole text\n    assert_eq!(chunks.len(), 1);\n    assert_eq!(chunks[0].len(), text.len());\n}\n"], "fixing_code": ["//! Implements a barebones client to the Google Cloud TTS service\n\nuse anyhow::{anyhow, bail, Context, Error as AnyError};\nuse bytes::Bytes;\nuse serde::Deserialize;\n\nuse core::iter;\n\n/// Path to the file that holds the Google Cloud API key\nconst API_KEY_FILE: &str = \"gcp_api.key\";\n\n//const GCP_API_BASE: &str = \"https://texttospeech.googleapis.com/v1\";\nconst GCP_TTS_API: &str = \"https://texttospeech.googleapis.com/v1beta1/text:synthesize\";\n\n// See https://cloud.google.com/text-to-speech/quotas\nconst MAX_CHARS_PER_REQUEST: usize = 5000;\n\n#[derive(Deserialize)]\nstruct AudioResponse<'a> {\n    #[serde(borrow, rename = \"audioContent\")]\n    audio_content: &'a str,\n}\n\n/// Denotes the voice quality options with Google Cloud\n#[derive(Copy, Clone, Eq, PartialEq)]\npub(crate) enum VoiceQuality {\n    /// The standard quality\n    Standard,\n    /// Represents Neural2 or Wavenet if the Neural2 version of whatever the client wants is not\n    /// available\n    High,\n}\n\n/// Voices are only classified as high or low pitch\n#[derive(Copy, Clone, Eq, PartialEq)]\npub(crate) enum VoiceType {\n    HighPitch,\n    LowPitch,\n}\n\n/// The description of a Google Cloud TTS reading voice\n#[derive(Clone, Copy)]\npub(crate) struct GcpVoice {\n    /// The unique voice identifier\n    pub(crate) id: &'static str,\n    // The English description of this voice. E.g., \"Portuguese (Brazil)\"\n    pub(crate) english_desc: &'static str,\n    /// The type of voice this is (high/low)\n    pub(crate) ty: VoiceType,\n}\n\n#[derive(Clone, Debug)]\npub(crate) struct TtsRequest {\n    /// The contents of the request\n    pub text: String,\n    /// The Google TTS voice to use\n    pub voice_name: &'static str,\n}\n\nimpl TtsRequest {\n    fn into_json(&self) -> serde_json::Value {\n        serde_json::json!({\n            \"input\": {\n                \"text\": self.text\n            },\n            \"voice\":{\n                \"languageCode\":\"en-US\",\n                \"name\": self.voice_name,\n            },\n            \"audioConfig\":{\n                \"audioEncoding\": \"MP3_64_KBPS\",\n                \"sampleRateHertz\": 48000\n            }\n        })\n    }\n}\n\npub(crate) fn get_api_key() -> Result<String, AnyError> {\n    std::fs::read_to_string(API_KEY_FILE).map_err(|e| {\n        anyhow!(\n            \"Could not open API key file {API_KEY_FILE}. \\\n            Read the README for info about how to get an API key. {:?}\",\n            e,\n        )\n    })\n}\n\n/// Redacts the API key out of the TTS error message\nfn redact_error(mut e: reqwest::Error) -> reqwest::Error {\n    // The API key is specified by `?key=...`. Overwriting the query section of the URL will make\n    // this go away\n    e.url_mut().map(|u| u.set_query(Some(\"key=REDACTED\")));\n    e\n}\n\n/// Speaks text string of length at most MAX_CHARS_PER_REQUEST. Returns an error if length exceeds,\n/// or an error occurs in the Google Cloud API call.\npub(crate) async fn tts_single(api_key: &str, req: &TtsRequest) -> Result<Bytes, AnyError> {\n    let payload = req.into_json();\n\n    // The Google API has a hard upper limit on characters per request. The text breaking before\n    // this point should ensure this limit is never exceeded\n    if req.text.len() > MAX_CHARS_PER_REQUEST {\n        bail!(\"TTS request is too long\");\n    }\n\n    // Do the HTTP request\n    let client = reqwest::Client::new();\n    let url = reqwest::Url::parse_with_params(GCP_TTS_API, &[(\"key\", api_key)])?;\n    let res = client\n        .post(url)\n        .json(&payload)\n        .send()\n        .await\n        .with_context(|| \"Couldn't make TTS request\")?\n        .error_for_status()\n        .map_err(redact_error)\n        .with_context(|| \"TTS request failed\")?;\n\n    // The resulting JSON response has our MP3 data\n    let res_bytes = res.bytes().await?;\n    let audio_response: AudioResponse = serde_json::from_slice(&res_bytes)?;\n    let audio_blob = Bytes::from(base64::decode(audio_response.audio_content)?);\n\n    Ok(audio_blob)\n}\n\n/// Speaks text string. Returns an error if an error occurs in the Google Cloud API call.\npub(crate) async fn tts(\n    api_key: &str,\n    TtsRequest { text, voice_name }: TtsRequest,\n) -> Result<Bytes, AnyError> {\n    let api_key_iter = core::iter::repeat(api_key);\n\n    // Break up the TTS tasks into smaller ones of at most MAX_CHARS_PER_REQUEST\n    let tts_tasks = break_english_text(&text, MAX_CHARS_PER_REQUEST)?\n        .into_iter()\n        .zip(api_key_iter)\n        .map(|(slice, api_key)| {\n            let slice = slice.to_string();\n            let api_key = api_key.to_string();\n            async move {\n                let slice_req = TtsRequest {\n                    text: slice,\n                    voice_name,\n                };\n                tts_single(&api_key, &slice_req).await\n            }\n        });\n\n    // Do the tasks in parallel. If one task fails, try_join_all will cancel the rest of them\n    // immediately. This prevents us from wasting API calls.\n    let mp3_blobs = futures::future::try_join_all(tts_tasks).await?;\n    // Concat the resulting MP3 blobs. Fun fact: the concatenation of MP3 files is itself a valid\n    // MP3 file.\n    let final_mp3: Bytes = mp3_blobs.concat().into();\n\n    Ok(final_mp3)\n}\n\n// Helper function that finds the next index i of the delimiter in the text such that txt[0, i]\n// is below the chunk limit. If no such i is found, then the first occurance of the delimiter\n// is returned (and text[0, i] is too big). If no delimiter occurs at all, txt.len() is\n// returned.\nfn next_break(text: &str, delim: char, max_chunk_size: usize) -> usize {\n    // Keep track of the last break that keeps us under the chunk size limit\n    let mut last_candidate_break = None;\n\n    // Find all the delimiters\n    let delim_indices = text.match_indices(delim).map(|(i, _)| i);\n\n    // Make the last breakpoint the EOF, otherwise we'd just be counting span size between\n    // delims (leaving out the span between the final delim and EOF)\n    let eof = text.len();\n\n    for cur_break in delim_indices.chain(iter::once(eof)) {\n        if cur_break != eof {\n            assert_eq!(text.split_at(cur_break).1.chars().next().unwrap(), delim);\n        }\n        if cur_break <= max_chunk_size {\n            last_candidate_break = Some(cur_break);\n        } else {\n            // The current break puts us over the chunk limit. Split at the last break if\n            // possible.\n            return last_candidate_break.unwrap_or(cur_break);\n        }\n    }\n\n    // If we made it to the end, then the text is now small enough to be a chunk in itself, and\n    // we're done.\n    return eof;\n}\n\n/// Attempts to break the given text into chunks of size at most `max_chunk_size`, making chunks as\n/// large as possible. The only allowed break point is `delim` characters. This is best-effort,\n/// meaning that there may be chunks returned which exceed `max_chunk_size`.\npub(crate) fn break_greedily_at_delim(\n    mut text: &str,\n    delim: char,\n    max_chunk_size: usize,\n) -> Vec<&str> {\n    let mut chunks = Vec::new();\n\n    while !text.is_empty() {\n        // While the text is not fully chunked, find the next break and break it up.\n        let b = next_break(text, delim, max_chunk_size);\n        let (chunk, mut rest) = text.split_at(b);\n\n        // Strip the leading punctuation off of the remainder of the text\n        if rest.chars().nth(0) == Some(delim) {\n            rest = &rest[1..];\n        }\n\n        // Save the chunk and truncate the text\n        chunks.push(chunk);\n        text = rest;\n    }\n\n    chunks\n}\n\n/// Attempts to break the given text into chunks of size at most `max_chunk_size`, making chunks as\n/// large as possible. The delimiters given are used in decreasing order of preference. If the text\n/// can be broken at just newlines, for example, that'd be preferred. But if there's a very large\n/// paragraph, then we have to break at periods. And if there's a very long sentence, we have to\n/// break on commas, etc.\nfn break_greedily_at_delims<'a>(\n    text: &'a str,\n    max_chunk_size: usize,\n    delims: &[char],\n) -> Result<Vec<&'a str>, AnyError> {\n    // Break the text into the f irst delim first\n    let mut chunks = break_greedily_at_delim(text, delims[0], max_chunk_size);\n\n    // If there are chunks greater than max_chunk_size, break them up. Use every delimiter if need\n    // be.\n    for &delim in &delims[1..] {\n        chunks = chunks\n            .into_iter()\n            .flat_map(|chunk| {\n                if chunk.len() > max_chunk_size {\n                    break_greedily_at_delim(&chunk, delim, max_chunk_size)\n                } else {\n                    vec![chunk]\n                }\n            })\n            .collect();\n    }\n\n    // Now check that everything was broken into sufficiently small pieces\n    for chunk in &chunks {\n        if chunk.len() > max_chunk_size {\n            bail!(\"Couldn't break text chunk {:?}\", chunk);\n        }\n    }\n\n    Ok(chunks)\n}\n\n/// Breaks English text into bounded-size chunks by newlines, then (if necessary) colons,\n/// then periods, then commas.\nfn break_english_text(text: &str, max_chunk_size: usize) -> Result<Vec<&str>, AnyError> {\n    break_greedily_at_delims(text, max_chunk_size, &['\\n', ':', '.', ','])\n}\n\n#[test]\nfn text_breaking() {\n    let text = \"\\\n        Mr James Duffy lived in Chapelizod because he wished to live as far as possible from the \\\n        city of which he was a citizen and because he found all the other suburbs of Dublin mean, \\\n        modern and pretentious.\n        He lived in an old sombre house and from his windows he could look into the disused \\\n        distillery or upwards along the shallow river on which Dublin is built. The lofty walls \\\n        of his uncarpeted room were free from pictures. He had himself bought every article of \\\n        furniture in the room: a black iron bedstead, an iron washstand, four cane chairs, a \\\n        clothes-rack, a coal-scuttle, a fender and irons and a square table on which lay a double \\\n        desk. A bookcase had been made in an alcove by means of shelves of white wood. The bed \\\n        was clothed with white bedclothes and a black and scarlet rug covered the foot. A little \\\n        hand-mirror hung above the washstand and during the day a white-shaded lamp stood as the \\\n        sole ornament of the mantelpiece. The books on the white wooden shelves were arranged \\\n        from below upwards according to bulk. A complete Wordsworth stood at one end of the \\\n        lowest shelf and a copy of the Maynooth Catechism, sewn into the cloth cover of a \\\n        notebook, stood at one end of the top shelf. Writing materials were always on the desk. \\\n        In the desk lay a manuscript translation of Hauptmann\u2019s Michael Kramer, the stage \\\n        directions of which were written in purple ink, and a little sheaf of papers held \\\n        together by a brass pin. In these sheets a sentence was inscribed from time to time and, \\\n        in an ironical moment, the headline of an advertisement for Bile Beans had been pasted on \\\n        to the first sheet. On lifting the lid of the desk a faint fragrance escaped\u2014the \\\n        fragrance of new cedarwood pencils or of a bottle of gum or of an overripe apple which \\\n        might have been left there and forgotten.\\\n    \";\n    let chunk_size = 220;\n    let chunks = break_english_text(text, chunk_size).unwrap();\n\n    // Make sure the chunk sizes add up the the total text size, minus the delimiters which were\n    // deleted (there's chunks.len() - 1 deleted delimiters).\n    assert_eq!(\n        chunks.iter().map(|c| c.len()).sum::<usize>(),\n        text.len() - (chunks.len() - 1)\n    );\n\n    // Make sure the chunks are nontrivial and don't exceed the chunk_size\n    for chunk in chunks {\n        assert!(chunk.len() > 1);\n        assert!(chunk.len() <= chunk_size);\n    }\n\n    //\n    // Test for a short text sample\n    //\n\n    let text = \"\\\n        Mr James Duffy lived in Chapelizod because he wished to live as far as possible from the \\\n        city of which he was a citizen and because he found all the other suburbs of Dublin mean, \\\n        modern and pretentious.\\\n    \";\n    let chunks = break_english_text(text, chunk_size).unwrap();\n    // Make sure there's just one chunk and it's the length of the whole text\n    assert_eq!(chunks.len(), 1);\n    assert_eq!(chunks[0].len(), text.len());\n}\n"], "filenames": ["server/src/tts.rs"], "buggy_code_start_loc": [87], "buggy_code_end_loc": [108], "fixing_code_start_loc": [88], "fixing_code_end_loc": [118], "type": "CWE-209", "message": "ReadtoMyShoe, a web app that lets users upload articles and listen to them later, generates an error message containing sensitive information prior to commit 8533b01. If an error occurs when adding an article, the website shows the user an error message. If the error originates from the Google Cloud TTS request, then it will include the full URL of the request. The request URL contains the Google Cloud API key. This has been patched in commit 8533b01. Upgrading should be accompanied by deleting the current GCP API key and issuing a new one. There are no known workarounds.", "other": {"cve": {"id": "CVE-2023-27587", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-13T22:15:12.483", "lastModified": "2023-03-17T16:26:13.203", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "ReadtoMyShoe, a web app that lets users upload articles and listen to them later, generates an error message containing sensitive information prior to commit 8533b01. If an error occurs when adding an article, the website shows the user an error message. If the error originates from the Google Cloud TTS request, then it will include the full URL of the request. The request URL contains the Google Cloud API key. This has been patched in commit 8533b01. Upgrading should be accompanied by deleting the current GCP API key and issuing a new one. There are no known workarounds."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.4, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 4.0}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-209"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:readtomyshoe_project:readtomyshoe:*:*:*:*:*:*:*:*", "versionEndExcluding": "2023-03-13", "matchCriteriaId": "4D49C440-C74D-454C-92D7-F0D8C2B203AB"}]}]}], "references": [{"url": "https://github.com/rozbb/readtomyshoe/commit/8533b01c818939a0fa919c7244d8dbf5daf032af", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/rozbb/readtomyshoe/security/advisories/GHSA-23g5-r34j-mr8g", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/rozbb/readtomyshoe/commit/8533b01c818939a0fa919c7244d8dbf5daf032af"}}