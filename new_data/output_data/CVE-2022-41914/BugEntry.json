{"buggy_code": ["import cProfile\nimport logging\nimport time\nimport traceback\nfrom typing import (\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    MutableMapping,\n    Optional,\n    Tuple,\n)\n\nfrom django.conf import settings\nfrom django.conf.urls.i18n import is_language_prefix_patterns_used\nfrom django.db import connection\nfrom django.http import HttpRequest, HttpResponse, HttpResponseRedirect, StreamingHttpResponse\nfrom django.http.response import HttpResponseBase\nfrom django.middleware.common import CommonMiddleware\nfrom django.middleware.locale import LocaleMiddleware as DjangoLocaleMiddleware\nfrom django.shortcuts import render\nfrom django.utils import translation\nfrom django.utils.cache import patch_vary_headers\nfrom django.utils.deprecation import MiddlewareMixin\nfrom django.utils.log import log_response\nfrom django.utils.translation import gettext as _\nfrom django.views.csrf import csrf_failure as html_csrf_failure\nfrom django_scim.middleware import SCIMAuthCheckMiddleware\nfrom django_scim.settings import scim_settings\nfrom sentry_sdk import capture_exception\nfrom sentry_sdk.integrations.logging import ignore_logger\n\nfrom zerver.lib.cache import get_remote_cache_requests, get_remote_cache_time\nfrom zerver.lib.db import reset_queries\nfrom zerver.lib.debug import maybe_tracemalloc_listen\nfrom zerver.lib.exceptions import ErrorCode, JsonableError, MissingAuthenticationError\nfrom zerver.lib.html_to_text import get_content_description\nfrom zerver.lib.markdown import get_markdown_requests, get_markdown_time\nfrom zerver.lib.rate_limiter import RateLimitResult\nfrom zerver.lib.request import RequestNotes, set_request, unset_request\nfrom zerver.lib.response import json_response, json_response_from_error, json_unauthorized\nfrom zerver.lib.subdomains import get_subdomain\nfrom zerver.lib.types import ViewFuncT\nfrom zerver.lib.user_agent import parse_user_agent\nfrom zerver.lib.utils import statsd\nfrom zerver.models import Realm, SCIMClient, flush_per_request_caches, get_realm\n\nlogger = logging.getLogger(\"zulip.requests\")\nslow_query_logger = logging.getLogger(\"zulip.slow_queries\")\n\n\ndef record_request_stop_data(log_data: MutableMapping[str, Any]) -> None:\n    log_data[\"time_stopped\"] = time.time()\n    log_data[\"remote_cache_time_stopped\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_stopped\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_stopped\"] = get_markdown_time()\n    log_data[\"markdown_requests_stopped\"] = get_markdown_requests()\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].disable()\n\n\ndef async_request_timer_stop(request: HttpRequest) -> None:\n    log_data = RequestNotes.get_notes(request).log_data\n    assert log_data is not None\n    record_request_stop_data(log_data)\n\n\ndef record_request_restart_data(log_data: MutableMapping[str, Any]) -> None:\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].enable()\n    log_data[\"time_restarted\"] = time.time()\n    log_data[\"remote_cache_time_restarted\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_restarted\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_restarted\"] = get_markdown_time()\n    log_data[\"markdown_requests_restarted\"] = get_markdown_requests()\n\n\ndef async_request_timer_restart(request: HttpRequest) -> None:\n    log_data = RequestNotes.get_notes(request).log_data\n    assert log_data is not None\n    if \"time_restarted\" in log_data:\n        # Don't destroy data when being called from\n        # finish_current_handler\n        return\n    record_request_restart_data(log_data)\n\n\ndef record_request_start_data(log_data: MutableMapping[str, Any]) -> None:\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"] = cProfile.Profile()\n        log_data[\"prof\"].enable()\n\n    reset_queries()\n    log_data[\"time_started\"] = time.time()\n    log_data[\"remote_cache_time_start\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_start\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_start\"] = get_markdown_time()\n    log_data[\"markdown_requests_start\"] = get_markdown_requests()\n\n\ndef timedelta_ms(timedelta: float) -> float:\n    return timedelta * 1000\n\n\ndef format_timedelta(timedelta: float) -> str:\n    if timedelta >= 1:\n        return f\"{timedelta:.1f}s\"\n    return f\"{timedelta_ms(timedelta):.0f}ms\"\n\n\ndef is_slow_query(time_delta: float, path: str) -> bool:\n    if time_delta < 1.2:\n        return False\n    is_exempt = (\n        path in [\"/activity\", \"/json/report/error\", \"/api/v1/deployments/report_error\"]\n        or path.startswith(\"/realm_activity/\")\n        or path.startswith(\"/user_activity/\")\n    )\n    if is_exempt:\n        return time_delta >= 5\n    if \"webathena_kerberos\" in path:\n        return time_delta >= 10\n    return True\n\n\nstatsd_blacklisted_requests = [\n    \"do_confirm\",\n    \"signup_send_confirm\",\n    \"new_realm_send_confirm\",\n    \"eventslast_event_id\",\n    \"webreq.content\",\n    \"avatar\",\n    \"user_uploads\",\n    \"password.reset\",\n    \"static\",\n    \"json.bots\",\n    \"json.users\",\n    \"json.streams\",\n    \"accounts.unsubscribe\",\n    \"apple-touch-icon\",\n    \"emoji\",\n    \"json.bots\",\n    \"upload_file\",\n    \"realm_activity\",\n    \"user_activity\",\n]\n\n\ndef write_log_line(\n    log_data: MutableMapping[str, Any],\n    path: str,\n    method: str,\n    remote_ip: str,\n    requestor_for_logs: str,\n    client_name: str,\n    client_version: Optional[str] = None,\n    status_code: int = 200,\n    error_content: Optional[AnyStr] = None,\n    error_content_iter: Optional[Iterable[AnyStr]] = None,\n) -> None:\n    assert error_content is None or error_content_iter is None\n    if error_content is not None:\n        error_content_iter = (error_content,)\n\n    if settings.STATSD_HOST != \"\":\n        # For statsd timer name\n        if path == \"/\":\n            statsd_path = \"webreq\"\n        else:\n            statsd_path = \"webreq.{}\".format(path[1:].replace(\"/\", \".\"))\n            # Remove non-ascii chars from path (there should be none; if there are, it's\n            # because someone manually entered a nonexistent path), as UTF-8 chars make\n            # statsd sad when it sends the key name over the socket\n            statsd_path = statsd_path.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n        # TODO: This could probably be optimized to use a regular expression rather than a loop.\n        suppress_statsd = any(\n            blacklisted in statsd_path for blacklisted in statsd_blacklisted_requests\n        )\n    else:\n        suppress_statsd = True\n        statsd_path = \"\"\n\n    time_delta = -1\n    # A time duration of -1 means the StartLogRequests middleware\n    # didn't run for some reason\n    optional_orig_delta = \"\"\n    if \"time_started\" in log_data:\n        time_delta = time.time() - log_data[\"time_started\"]\n    if \"time_stopped\" in log_data:\n        orig_time_delta = time_delta\n        time_delta = (log_data[\"time_stopped\"] - log_data[\"time_started\"]) + (\n            time.time() - log_data[\"time_restarted\"]\n        )\n        optional_orig_delta = f\" (lp: {format_timedelta(orig_time_delta)})\"\n    remote_cache_output = \"\"\n    if \"remote_cache_time_start\" in log_data:\n        remote_cache_time_delta = get_remote_cache_time() - log_data[\"remote_cache_time_start\"]\n        remote_cache_count_delta = (\n            get_remote_cache_requests() - log_data[\"remote_cache_requests_start\"]\n        )\n        if \"remote_cache_requests_stopped\" in log_data:\n            # (now - restarted) + (stopped - start) = (now - start) + (stopped - restarted)\n            remote_cache_time_delta += (\n                log_data[\"remote_cache_time_stopped\"] - log_data[\"remote_cache_time_restarted\"]\n            )\n            remote_cache_count_delta += (\n                log_data[\"remote_cache_requests_stopped\"]\n                - log_data[\"remote_cache_requests_restarted\"]\n            )\n\n        if remote_cache_time_delta > 0.005:\n            remote_cache_output = (\n                f\" (mem: {format_timedelta(remote_cache_time_delta)}/{remote_cache_count_delta})\"\n            )\n\n        if not suppress_statsd:\n            statsd.timing(f\"{statsd_path}.remote_cache.time\", timedelta_ms(remote_cache_time_delta))\n            statsd.incr(f\"{statsd_path}.remote_cache.querycount\", remote_cache_count_delta)\n\n    startup_output = \"\"\n    if \"startup_time_delta\" in log_data and log_data[\"startup_time_delta\"] > 0.005:\n        startup_output = \" (+start: {})\".format(format_timedelta(log_data[\"startup_time_delta\"]))\n\n    markdown_output = \"\"\n    if \"markdown_time_start\" in log_data:\n        markdown_time_delta = get_markdown_time() - log_data[\"markdown_time_start\"]\n        markdown_count_delta = get_markdown_requests() - log_data[\"markdown_requests_start\"]\n        if \"markdown_requests_stopped\" in log_data:\n            # (now - restarted) + (stopped - start) = (now - start) + (stopped - restarted)\n            markdown_time_delta += (\n                log_data[\"markdown_time_stopped\"] - log_data[\"markdown_time_restarted\"]\n            )\n            markdown_count_delta += (\n                log_data[\"markdown_requests_stopped\"] - log_data[\"markdown_requests_restarted\"]\n            )\n\n        if markdown_time_delta > 0.005:\n            markdown_output = (\n                f\" (md: {format_timedelta(markdown_time_delta)}/{markdown_count_delta})\"\n            )\n\n            if not suppress_statsd:\n                statsd.timing(f\"{statsd_path}.markdown.time\", timedelta_ms(markdown_time_delta))\n                statsd.incr(f\"{statsd_path}.markdown.count\", markdown_count_delta)\n\n    # Get the amount of time spent doing database queries\n    db_time_output = \"\"\n    queries = connection.connection.queries if connection.connection is not None else []\n    if len(queries) > 0:\n        query_time = sum(float(query.get(\"time\", 0)) for query in queries)\n        db_time_output = f\" (db: {format_timedelta(query_time)}/{len(queries)}q)\"\n\n        if not suppress_statsd:\n            # Log ms, db ms, and num queries to statsd\n            statsd.timing(f\"{statsd_path}.dbtime\", timedelta_ms(query_time))\n            statsd.incr(f\"{statsd_path}.dbq\", len(queries))\n            statsd.timing(f\"{statsd_path}.total\", timedelta_ms(time_delta))\n\n    if \"extra\" in log_data:\n        extra_request_data = \" {}\".format(log_data[\"extra\"])\n    else:\n        extra_request_data = \"\"\n    if client_version is None:\n        logger_client = f\"({requestor_for_logs} via {client_name})\"\n    else:\n        logger_client = f\"({requestor_for_logs} via {client_name}/{client_version})\"\n    logger_timing = f\"{format_timedelta(time_delta):>5}{optional_orig_delta}{remote_cache_output}{markdown_output}{db_time_output}{startup_output} {path}\"\n    logger_line = f\"{remote_ip:<15} {method:<7} {status_code:3} {logger_timing}{extra_request_data} {logger_client}\"\n    if status_code in [200, 304] and method == \"GET\" and path.startswith(\"/static\"):\n        logger.debug(logger_line)\n    else:\n        logger.info(logger_line)\n\n    if is_slow_query(time_delta, path):\n        slow_query_logger.info(logger_line)\n\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].disable()\n        profile_path = \"/tmp/profile.data.{}.{}\".format(path.split(\"/\")[-1], int(time_delta * 1000))\n        log_data[\"prof\"].dump_stats(profile_path)\n\n    # Log some additional data whenever we return certain 40x errors\n    if 400 <= status_code < 500 and status_code not in [401, 404, 405]:\n        assert error_content_iter is not None\n        error_content_list = list(error_content_iter)\n        if not error_content_list:\n            error_data = \"\"\n        elif isinstance(error_content_list[0], str):\n            error_data = \"\".join(error_content_list)\n        elif isinstance(error_content_list[0], bytes):\n            error_data = repr(b\"\".join(error_content_list))\n        if len(error_data) > 200:\n            error_data = \"[content more than 200 characters]\"\n        logger.info(\"status=%3d, data=%s, uid=%s\", status_code, error_data, requestor_for_logs)\n\n\nclass RequestContext(MiddlewareMixin):\n    def __call__(self, request: HttpRequest) -> HttpResponse:\n        set_request(request)\n        try:\n            return self.get_response(request)\n        finally:\n            unset_request()\n\n\ndef parse_client(request: HttpRequest) -> Tuple[str, Optional[str]]:\n    # If the API request specified a client in the request content,\n    # that has priority.  Otherwise, extract the client from the\n    # User-Agent.\n    if \"client\" in request.GET:  # nocoverage\n        return request.GET[\"client\"], None\n    if \"client\" in request.POST:\n        return request.POST[\"client\"], None\n    if \"HTTP_USER_AGENT\" in request.META:\n        user_agent: Optional[Dict[str, str]] = parse_user_agent(request.META[\"HTTP_USER_AGENT\"])\n    else:\n        user_agent = None\n    if user_agent is None:\n        # In the future, we will require setting USER_AGENT, but for\n        # now we just want to tag these requests so we can review them\n        # in logs and figure out the extent of the problem\n        return \"Unspecified\", None\n\n    client_name = user_agent[\"name\"]\n    if client_name.startswith(\"Zulip\"):\n        return client_name, user_agent.get(\"version\")\n\n    # We could show browser versions in logs, and it'd probably be a\n    # good idea, but the current parsing will just get you Mozilla/5.0.\n    #\n    # Fixing this probably means using a third-party library, and\n    # making sure it's fast enough that we're happy to do it even on\n    # hot-path cases.\n    return client_name, None\n\n\nclass LogRequests(MiddlewareMixin):\n    # We primarily are doing logging using the process_view hook, but\n    # for some views, process_view isn't run, so we call the start\n    # method here too\n    def process_request(self, request: HttpRequest) -> None:\n        maybe_tracemalloc_listen()\n        request_notes = RequestNotes.get_notes(request)\n\n        if request_notes.log_data is not None:\n            # Sanity check to ensure this is being called from the\n            # Tornado code path that returns responses asynchronously.\n            assert request_notes.saved_response is not None\n\n            # Avoid re-initializing request_notes.log_data if it's already there.\n            return\n\n        request_notes.client_name, request_notes.client_version = parse_client(request)\n        request_notes.log_data = {}\n        record_request_start_data(request_notes.log_data)\n\n    def process_view(\n        self,\n        request: HttpRequest,\n        view_func: ViewFuncT,\n        args: List[str],\n        kwargs: Dict[str, Any],\n    ) -> None:\n        request_notes = RequestNotes.get_notes(request)\n        if request_notes.saved_response is not None:\n            # The below logging adjustments are unnecessary (because\n            # we've already imported everything) and incorrect\n            # (because they'll overwrite data from pre-long-poll\n            # request processing) when returning a saved response.\n            return\n\n        # process_request was already run; we save the initialization\n        # time (i.e. the time between receiving the request and\n        # figuring out which view function to call, which is primarily\n        # importing modules on the first start)\n        assert request_notes.log_data is not None\n        request_notes.log_data[\"startup_time_delta\"] = (\n            time.time() - request_notes.log_data[\"time_started\"]\n        )\n        # And then completely reset our tracking to only cover work\n        # done as part of this request\n        record_request_start_data(request_notes.log_data)\n\n    def process_response(\n        self, request: HttpRequest, response: HttpResponseBase\n    ) -> HttpResponseBase:\n        if getattr(response, \"asynchronous\", False):\n            # This special Tornado \"asynchronous\" response is\n            # discarded after going through this code path as Tornado\n            # intends to block, so we stop here to avoid unnecessary work.\n            return response\n\n        remote_ip = request.META[\"REMOTE_ADDR\"]\n\n        # Get the requestor's identifier and client, if available.\n        request_notes = RequestNotes.get_notes(request)\n        requestor_for_logs = request_notes.requestor_for_logs\n        if requestor_for_logs is None:\n            # Note that request.user is a Union[RemoteZulipServer, UserProfile, AnonymousUser],\n            # if it is present.\n            if hasattr(request, \"user\") and hasattr(request.user, \"format_requestor_for_logs\"):\n                requestor_for_logs = request.user.format_requestor_for_logs()\n            else:\n                requestor_for_logs = \"unauth@{}\".format(get_subdomain(request) or \"root\")\n\n        if response.streaming:\n            assert isinstance(response, StreamingHttpResponse)\n            content_iter: Optional[Iterator[bytes]] = response.streaming_content\n            content = None\n        else:\n            content = response.content\n            content_iter = None\n\n        assert request_notes.client_name is not None and request_notes.log_data is not None\n        write_log_line(\n            request_notes.log_data,\n            request.path,\n            request.method,\n            remote_ip,\n            requestor_for_logs,\n            request_notes.client_name,\n            client_version=request_notes.client_version,\n            status_code=response.status_code,\n            error_content=content,\n            error_content_iter=content_iter,\n        )\n        return response\n\n\nclass JsonErrorHandler(MiddlewareMixin):\n    def __init__(self, get_response: Callable[[HttpRequest], HttpResponse]) -> None:\n        super().__init__(get_response)\n        ignore_logger(\"zerver.middleware.json_error_handler\")\n\n    def process_exception(\n        self, request: HttpRequest, exception: Exception\n    ) -> Optional[HttpResponse]:\n        if isinstance(exception, MissingAuthenticationError):\n            if \"text/html\" in request.META.get(\"HTTP_ACCEPT\", \"\"):\n                # If this looks like a request from a top-level page in a\n                # browser, send the user to the login page.\n                #\n                # TODO: The next part is a bit questionable; it will\n                # execute the likely intent for intentionally visiting\n                # an API endpoint without authentication in a browser,\n                # but that's an unlikely to be done intentionally often.\n                return HttpResponseRedirect(f\"{settings.HOME_NOT_LOGGED_IN}?next={request.path}\")\n            if request.path.startswith(\"/api\"):\n                # For API routes, ask for HTTP basic auth (email:apiKey).\n                return json_unauthorized()\n            else:\n                # For /json routes, ask for session authentication.\n                return json_unauthorized(www_authenticate=\"session\")\n\n        if isinstance(exception, JsonableError):\n            response = json_response_from_error(exception)\n            if response.status_code >= 500:\n                # Here we use Django's log_response the way Django uses\n                # it normally to log error responses. However, we make the small\n                # modification of including the traceback to make the log message\n                # more helpful. log_response takes care of knowing not to duplicate\n                # the logging, so Django won't generate a second log message.\n                log_response(\n                    \"%s: %s\",\n                    response.reason_phrase,\n                    request.path,\n                    response=response,\n                    request=request,\n                    exc_info=True,\n                )\n            return response\n        if RequestNotes.get_notes(request).error_format == \"JSON\" and not settings.TEST_SUITE:\n            capture_exception(exception)\n            json_error_logger = logging.getLogger(\"zerver.middleware.json_error_handler\")\n            json_error_logger.error(traceback.format_exc(), extra=dict(request=request))\n            return json_response(res_type=\"error\", msg=_(\"Internal server error\"), status=500)\n        return None\n\n\nclass TagRequests(MiddlewareMixin):\n    def process_view(\n        self, request: HttpRequest, view_func: ViewFuncT, args: List[str], kwargs: Dict[str, Any]\n    ) -> None:\n        self.process_request(request)\n\n    def process_request(self, request: HttpRequest) -> None:\n        if request.path.startswith(\"/api/\") or request.path.startswith(\"/json/\"):\n            RequestNotes.get_notes(request).error_format = \"JSON\"\n        else:\n            RequestNotes.get_notes(request).error_format = \"HTML\"\n\n\nclass CsrfFailureError(JsonableError):\n    http_status_code = 403\n    code = ErrorCode.CSRF_FAILED\n    data_fields = [\"reason\"]\n\n    def __init__(self, reason: str) -> None:\n        self.reason: str = reason\n\n    @staticmethod\n    def msg_format() -> str:\n        return _(\"CSRF error: {reason}\")\n\n\ndef csrf_failure(request: HttpRequest, reason: str = \"\") -> HttpResponse:\n    if RequestNotes.get_notes(request).error_format == \"JSON\":\n        return json_response_from_error(CsrfFailureError(reason))\n    else:\n        return html_csrf_failure(request, reason)\n\n\nclass LocaleMiddleware(DjangoLocaleMiddleware):\n    def process_response(\n        self, request: HttpRequest, response: HttpResponseBase\n    ) -> HttpResponseBase:\n\n        # This is the same as the default LocaleMiddleware, minus the\n        # logic that redirects 404's that lack a prefixed language in\n        # the path into having a language.  See\n        # https://code.djangoproject.com/ticket/32005\n        language = translation.get_language()\n        language_from_path = translation.get_language_from_path(request.path_info)\n        urlconf = getattr(request, \"urlconf\", settings.ROOT_URLCONF)\n        i18n_patterns_used, _ = is_language_prefix_patterns_used(urlconf)\n        if not (i18n_patterns_used and language_from_path):\n            patch_vary_headers(response, (\"Accept-Language\",))\n        assert language is not None\n        response.setdefault(\"Content-Language\", language)\n\n        # An additional responsibility of our override of this middleware is to save the user's language\n        # preference in a cookie. That determination is made by code handling the request\n        # and saved in the set_language flag so that it can be used here.\n        set_language = RequestNotes.get_notes(request).set_language\n        if set_language is not None:\n            response.set_cookie(settings.LANGUAGE_COOKIE_NAME, set_language)\n\n        return response\n\n\nclass RateLimitMiddleware(MiddlewareMixin):\n    def set_response_headers(\n        self, response: HttpResponse, rate_limit_results: List[RateLimitResult]\n    ) -> None:\n        # The limit on the action that was requested is the minimum of the limits that get applied:\n        limit = min(result.entity.max_api_calls() for result in rate_limit_results)\n        response[\"X-RateLimit-Limit\"] = str(limit)\n        # Same principle applies to remaining API calls:\n        remaining_api_calls = min(result.remaining for result in rate_limit_results)\n        response[\"X-RateLimit-Remaining\"] = str(remaining_api_calls)\n\n        # The full reset time is the maximum of the reset times for the limits that get applied:\n        reset_time = time.time() + max(result.secs_to_freedom for result in rate_limit_results)\n        response[\"X-RateLimit-Reset\"] = str(int(reset_time))\n\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:\n        if not settings.RATE_LIMITING:\n            return response\n\n        # Add X-RateLimit-*** headers\n        ratelimits_applied = RequestNotes.get_notes(request).ratelimits_applied\n        if len(ratelimits_applied) > 0:\n            self.set_response_headers(response, ratelimits_applied)\n\n        return response\n\n\nclass FlushDisplayRecipientCache(MiddlewareMixin):\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:\n        # We flush the per-request caches after every request, so they\n        # are not shared at all between requests.\n        flush_per_request_caches()\n        return response\n\n\nclass HostDomainMiddleware(MiddlewareMixin):\n    def process_request(self, request: HttpRequest) -> Optional[HttpResponse]:\n        # Match against ALLOWED_HOSTS, which is rather permissive;\n        # failure will raise DisallowedHost, which is a 400.\n        request.get_host()\n\n        # This check is important to avoid doing the extra work of\n        # `get_realm` (which does a database query that could be\n        # problematic for Tornado).  Also the error page below is only\n        # appropriate for a page visited in a browser, not the API.\n        #\n        # API authentication will end up checking for an invalid\n        # realm, and throw a JSON-format error if appropriate.\n        if request.path.startswith((\"/static/\", \"/api/\", \"/json/\")):\n            return None\n\n        subdomain = get_subdomain(request)\n        if subdomain == settings.SOCIAL_AUTH_SUBDOMAIN:\n            # Realms are not supposed to exist on SOCIAL_AUTH_SUBDOMAIN.\n            return None\n\n        request_notes = RequestNotes.get_notes(request)\n        try:\n            request_notes.realm = get_realm(subdomain)\n            request_notes.has_fetched_realm = True\n        except Realm.DoesNotExist:\n            if subdomain == Realm.SUBDOMAIN_FOR_ROOT_DOMAIN:\n                # The root domain is used for creating new\n                # organizations even if it does not host a realm.\n                return None\n\n            return render(request, \"zerver/invalid_realm.html\", status=404)\n\n        return None\n\n\nclass SetRemoteAddrFromRealIpHeader(MiddlewareMixin):\n    \"\"\"Middleware that sets REMOTE_ADDR based on the X-Real-Ip header.\n\n    This middleware is similar to Django's old\n    SetRemoteAddrFromForwardedFor middleware.  We use X-Real-Ip, and\n    not X-Forwarded-For, because the latter is a list of proxies, some\n    number of which are trusted by us, and some of which could be\n    arbitrarily set by the user.  nginx has already parsed which are\n    which, and has set X-Real-Ip to the first one, going right to\n    left, which is untrusted.\n\n    Since we are always deployed behind nginx, we can trust the\n    X-Real-Ip which is so set.  In development, we fall back to the\n    REMOTE_ADDR supplied by the server.\n\n    \"\"\"\n\n    def process_request(self, request: HttpRequest) -> None:\n        try:\n            real_ip = request.META[\"HTTP_X_REAL_IP\"]\n        except KeyError:\n            return None\n        else:\n            request.META[\"REMOTE_ADDR\"] = real_ip\n\n\ndef alter_content(request: HttpRequest, content: bytes) -> bytes:\n    first_paragraph_text = get_content_description(content, request)\n    placeholder_open_graph_description = RequestNotes.get_notes(\n        request\n    ).placeholder_open_graph_description\n    assert placeholder_open_graph_description is not None\n    return content.replace(\n        placeholder_open_graph_description.encode(),\n        first_paragraph_text.encode(),\n    )\n\n\nclass FinalizeOpenGraphDescription(MiddlewareMixin):\n    def process_response(\n        self, request: HttpRequest, response: StreamingHttpResponse\n    ) -> StreamingHttpResponse:\n\n        if RequestNotes.get_notes(request).placeholder_open_graph_description is not None:\n            assert not response.streaming\n            response.content = alter_content(request, response.content)\n        return response\n\n\nclass ZulipCommonMiddleware(CommonMiddleware):\n    \"\"\"\n    Patched version of CommonMiddleware to disable the APPEND_SLASH\n    redirect behavior inside Tornado.\n\n    While this has some correctness benefit in encouraging clients\n    to implement the API correctly, this also saves about 600us in\n    the runtime of every GET /events query, as the APPEND_SLASH\n    route resolution logic is surprisingly expensive.\n\n    TODO: We should probably extend this behavior to apply to all of\n    our API routes.  The APPEND_SLASH behavior is really only useful\n    for non-API endpoints things like /login.  But doing that\n    transition will require more careful testing.\n    \"\"\"\n\n    def should_redirect_with_slash(self, request: HttpRequest) -> bool:\n        if settings.RUNNING_INSIDE_TORNADO:\n            return False\n        return super().should_redirect_with_slash(request)\n\n\ndef validate_scim_bearer_token(request: HttpRequest) -> Optional[SCIMClient]:\n    \"\"\"\n    This function verifies the request is allowed to make SCIM requests on this subdomain,\n    by checking the provided bearer token and ensuring it matches a scim client configured\n    for this subdomain in settings.SCIM_CONFIG.\n    If successful, returns the corresponding SCIMClient object. Returns None otherwise.\n    \"\"\"\n\n    subdomain = get_subdomain(request)\n    scim_config_dict = settings.SCIM_CONFIG.get(subdomain)\n    if not scim_config_dict:\n        return None\n\n    valid_bearer_token = scim_config_dict.get(\"bearer_token\")\n    scim_client_name = scim_config_dict.get(\"scim_client_name\")\n    # We really don't want a misconfiguration where this is unset,\n    # allowing free access to the SCIM API:\n    assert valid_bearer_token\n    assert scim_client_name\n\n    if request.headers.get(\"Authorization\") != f\"Bearer {valid_bearer_token}\":\n        return None\n\n    request_notes = RequestNotes.get_notes(request)\n    assert request_notes.realm\n\n    # While API authentication code paths are sufficiently high\n    # traffic that we prefer to use a cache, SCIM is much lower\n    # traffic, and doing a database query is plenty fast.\n    return SCIMClient.objects.get(realm=request_notes.realm, name=scim_client_name)\n\n\nclass ZulipSCIMAuthCheckMiddleware(SCIMAuthCheckMiddleware):\n    \"\"\"\n    Overridden version of middleware implemented in django-scim2\n    (https://github.com/15five/django-scim2/blob/master/src/django_scim/middleware.py)\n    to also handle authenticating the client.\n    \"\"\"\n\n    def process_request(self, request: HttpRequest) -> Optional[HttpResponse]:\n        # This determines whether this is a SCIM request based on the request's path\n        # and if it is, logs request information, including the body, as well as the response\n        # for debugging purposes to the `django_scim.middleware` logger, at DEBUG level.\n        # We keep those logs in /var/log/zulip/scim.log\n        if self.should_log_request(request):\n            self.log_request(request)\n\n        # Here we verify the request is indeed to a SCIM endpoint. That's ensured\n        # by comparing the path with self.reverse_url, which is the root SCIM path /scim/b2/.\n        # Of course we don't want to proceed with authenticating the request for SCIM\n        # if a non-SCIM endpoint is being queried.\n        if not request.path.startswith(self.reverse_url):\n            return None\n\n        scim_client = validate_scim_bearer_token(request)\n        if not scim_client:\n            response = HttpResponse(status=401)\n            response[\"WWW-Authenticate\"] = scim_settings.WWW_AUTHENTICATE_HEADER\n            return response\n\n        # The client has been successfully authenticated for SCIM on this subdomain,\n        # so we can assign the corresponding SCIMClient object to request.user - which\n        # will allow this request to pass request.user.is_authenticated checks from now on,\n        # to be served by the relevant views implemented in django-scim2.\n        request.user = scim_client\n        return None\n"], "fixing_code": ["import cProfile\nimport logging\nimport time\nimport traceback\nfrom typing import (\n    Any,\n    AnyStr,\n    Callable,\n    Dict,\n    Iterable,\n    Iterator,\n    List,\n    MutableMapping,\n    Optional,\n    Tuple,\n)\n\nfrom django.conf import settings\nfrom django.conf.urls.i18n import is_language_prefix_patterns_used\nfrom django.db import connection\nfrom django.http import HttpRequest, HttpResponse, HttpResponseRedirect, StreamingHttpResponse\nfrom django.http.response import HttpResponseBase\nfrom django.middleware.common import CommonMiddleware\nfrom django.middleware.locale import LocaleMiddleware as DjangoLocaleMiddleware\nfrom django.shortcuts import render\nfrom django.utils import translation\nfrom django.utils.cache import patch_vary_headers\nfrom django.utils.crypto import constant_time_compare\nfrom django.utils.deprecation import MiddlewareMixin\nfrom django.utils.log import log_response\nfrom django.utils.translation import gettext as _\nfrom django.views.csrf import csrf_failure as html_csrf_failure\nfrom django_scim.middleware import SCIMAuthCheckMiddleware\nfrom django_scim.settings import scim_settings\nfrom sentry_sdk import capture_exception\nfrom sentry_sdk.integrations.logging import ignore_logger\n\nfrom zerver.lib.cache import get_remote_cache_requests, get_remote_cache_time\nfrom zerver.lib.db import reset_queries\nfrom zerver.lib.debug import maybe_tracemalloc_listen\nfrom zerver.lib.exceptions import ErrorCode, JsonableError, MissingAuthenticationError\nfrom zerver.lib.html_to_text import get_content_description\nfrom zerver.lib.markdown import get_markdown_requests, get_markdown_time\nfrom zerver.lib.rate_limiter import RateLimitResult\nfrom zerver.lib.request import RequestNotes, set_request, unset_request\nfrom zerver.lib.response import json_response, json_response_from_error, json_unauthorized\nfrom zerver.lib.subdomains import get_subdomain\nfrom zerver.lib.types import ViewFuncT\nfrom zerver.lib.user_agent import parse_user_agent\nfrom zerver.lib.utils import statsd\nfrom zerver.models import Realm, SCIMClient, flush_per_request_caches, get_realm\n\nlogger = logging.getLogger(\"zulip.requests\")\nslow_query_logger = logging.getLogger(\"zulip.slow_queries\")\n\n\ndef record_request_stop_data(log_data: MutableMapping[str, Any]) -> None:\n    log_data[\"time_stopped\"] = time.time()\n    log_data[\"remote_cache_time_stopped\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_stopped\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_stopped\"] = get_markdown_time()\n    log_data[\"markdown_requests_stopped\"] = get_markdown_requests()\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].disable()\n\n\ndef async_request_timer_stop(request: HttpRequest) -> None:\n    log_data = RequestNotes.get_notes(request).log_data\n    assert log_data is not None\n    record_request_stop_data(log_data)\n\n\ndef record_request_restart_data(log_data: MutableMapping[str, Any]) -> None:\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].enable()\n    log_data[\"time_restarted\"] = time.time()\n    log_data[\"remote_cache_time_restarted\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_restarted\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_restarted\"] = get_markdown_time()\n    log_data[\"markdown_requests_restarted\"] = get_markdown_requests()\n\n\ndef async_request_timer_restart(request: HttpRequest) -> None:\n    log_data = RequestNotes.get_notes(request).log_data\n    assert log_data is not None\n    if \"time_restarted\" in log_data:\n        # Don't destroy data when being called from\n        # finish_current_handler\n        return\n    record_request_restart_data(log_data)\n\n\ndef record_request_start_data(log_data: MutableMapping[str, Any]) -> None:\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"] = cProfile.Profile()\n        log_data[\"prof\"].enable()\n\n    reset_queries()\n    log_data[\"time_started\"] = time.time()\n    log_data[\"remote_cache_time_start\"] = get_remote_cache_time()\n    log_data[\"remote_cache_requests_start\"] = get_remote_cache_requests()\n    log_data[\"markdown_time_start\"] = get_markdown_time()\n    log_data[\"markdown_requests_start\"] = get_markdown_requests()\n\n\ndef timedelta_ms(timedelta: float) -> float:\n    return timedelta * 1000\n\n\ndef format_timedelta(timedelta: float) -> str:\n    if timedelta >= 1:\n        return f\"{timedelta:.1f}s\"\n    return f\"{timedelta_ms(timedelta):.0f}ms\"\n\n\ndef is_slow_query(time_delta: float, path: str) -> bool:\n    if time_delta < 1.2:\n        return False\n    is_exempt = (\n        path in [\"/activity\", \"/json/report/error\", \"/api/v1/deployments/report_error\"]\n        or path.startswith(\"/realm_activity/\")\n        or path.startswith(\"/user_activity/\")\n    )\n    if is_exempt:\n        return time_delta >= 5\n    if \"webathena_kerberos\" in path:\n        return time_delta >= 10\n    return True\n\n\nstatsd_blacklisted_requests = [\n    \"do_confirm\",\n    \"signup_send_confirm\",\n    \"new_realm_send_confirm\",\n    \"eventslast_event_id\",\n    \"webreq.content\",\n    \"avatar\",\n    \"user_uploads\",\n    \"password.reset\",\n    \"static\",\n    \"json.bots\",\n    \"json.users\",\n    \"json.streams\",\n    \"accounts.unsubscribe\",\n    \"apple-touch-icon\",\n    \"emoji\",\n    \"json.bots\",\n    \"upload_file\",\n    \"realm_activity\",\n    \"user_activity\",\n]\n\n\ndef write_log_line(\n    log_data: MutableMapping[str, Any],\n    path: str,\n    method: str,\n    remote_ip: str,\n    requestor_for_logs: str,\n    client_name: str,\n    client_version: Optional[str] = None,\n    status_code: int = 200,\n    error_content: Optional[AnyStr] = None,\n    error_content_iter: Optional[Iterable[AnyStr]] = None,\n) -> None:\n    assert error_content is None or error_content_iter is None\n    if error_content is not None:\n        error_content_iter = (error_content,)\n\n    if settings.STATSD_HOST != \"\":\n        # For statsd timer name\n        if path == \"/\":\n            statsd_path = \"webreq\"\n        else:\n            statsd_path = \"webreq.{}\".format(path[1:].replace(\"/\", \".\"))\n            # Remove non-ascii chars from path (there should be none; if there are, it's\n            # because someone manually entered a nonexistent path), as UTF-8 chars make\n            # statsd sad when it sends the key name over the socket\n            statsd_path = statsd_path.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n        # TODO: This could probably be optimized to use a regular expression rather than a loop.\n        suppress_statsd = any(\n            blacklisted in statsd_path for blacklisted in statsd_blacklisted_requests\n        )\n    else:\n        suppress_statsd = True\n        statsd_path = \"\"\n\n    time_delta = -1\n    # A time duration of -1 means the StartLogRequests middleware\n    # didn't run for some reason\n    optional_orig_delta = \"\"\n    if \"time_started\" in log_data:\n        time_delta = time.time() - log_data[\"time_started\"]\n    if \"time_stopped\" in log_data:\n        orig_time_delta = time_delta\n        time_delta = (log_data[\"time_stopped\"] - log_data[\"time_started\"]) + (\n            time.time() - log_data[\"time_restarted\"]\n        )\n        optional_orig_delta = f\" (lp: {format_timedelta(orig_time_delta)})\"\n    remote_cache_output = \"\"\n    if \"remote_cache_time_start\" in log_data:\n        remote_cache_time_delta = get_remote_cache_time() - log_data[\"remote_cache_time_start\"]\n        remote_cache_count_delta = (\n            get_remote_cache_requests() - log_data[\"remote_cache_requests_start\"]\n        )\n        if \"remote_cache_requests_stopped\" in log_data:\n            # (now - restarted) + (stopped - start) = (now - start) + (stopped - restarted)\n            remote_cache_time_delta += (\n                log_data[\"remote_cache_time_stopped\"] - log_data[\"remote_cache_time_restarted\"]\n            )\n            remote_cache_count_delta += (\n                log_data[\"remote_cache_requests_stopped\"]\n                - log_data[\"remote_cache_requests_restarted\"]\n            )\n\n        if remote_cache_time_delta > 0.005:\n            remote_cache_output = (\n                f\" (mem: {format_timedelta(remote_cache_time_delta)}/{remote_cache_count_delta})\"\n            )\n\n        if not suppress_statsd:\n            statsd.timing(f\"{statsd_path}.remote_cache.time\", timedelta_ms(remote_cache_time_delta))\n            statsd.incr(f\"{statsd_path}.remote_cache.querycount\", remote_cache_count_delta)\n\n    startup_output = \"\"\n    if \"startup_time_delta\" in log_data and log_data[\"startup_time_delta\"] > 0.005:\n        startup_output = \" (+start: {})\".format(format_timedelta(log_data[\"startup_time_delta\"]))\n\n    markdown_output = \"\"\n    if \"markdown_time_start\" in log_data:\n        markdown_time_delta = get_markdown_time() - log_data[\"markdown_time_start\"]\n        markdown_count_delta = get_markdown_requests() - log_data[\"markdown_requests_start\"]\n        if \"markdown_requests_stopped\" in log_data:\n            # (now - restarted) + (stopped - start) = (now - start) + (stopped - restarted)\n            markdown_time_delta += (\n                log_data[\"markdown_time_stopped\"] - log_data[\"markdown_time_restarted\"]\n            )\n            markdown_count_delta += (\n                log_data[\"markdown_requests_stopped\"] - log_data[\"markdown_requests_restarted\"]\n            )\n\n        if markdown_time_delta > 0.005:\n            markdown_output = (\n                f\" (md: {format_timedelta(markdown_time_delta)}/{markdown_count_delta})\"\n            )\n\n            if not suppress_statsd:\n                statsd.timing(f\"{statsd_path}.markdown.time\", timedelta_ms(markdown_time_delta))\n                statsd.incr(f\"{statsd_path}.markdown.count\", markdown_count_delta)\n\n    # Get the amount of time spent doing database queries\n    db_time_output = \"\"\n    queries = connection.connection.queries if connection.connection is not None else []\n    if len(queries) > 0:\n        query_time = sum(float(query.get(\"time\", 0)) for query in queries)\n        db_time_output = f\" (db: {format_timedelta(query_time)}/{len(queries)}q)\"\n\n        if not suppress_statsd:\n            # Log ms, db ms, and num queries to statsd\n            statsd.timing(f\"{statsd_path}.dbtime\", timedelta_ms(query_time))\n            statsd.incr(f\"{statsd_path}.dbq\", len(queries))\n            statsd.timing(f\"{statsd_path}.total\", timedelta_ms(time_delta))\n\n    if \"extra\" in log_data:\n        extra_request_data = \" {}\".format(log_data[\"extra\"])\n    else:\n        extra_request_data = \"\"\n    if client_version is None:\n        logger_client = f\"({requestor_for_logs} via {client_name})\"\n    else:\n        logger_client = f\"({requestor_for_logs} via {client_name}/{client_version})\"\n    logger_timing = f\"{format_timedelta(time_delta):>5}{optional_orig_delta}{remote_cache_output}{markdown_output}{db_time_output}{startup_output} {path}\"\n    logger_line = f\"{remote_ip:<15} {method:<7} {status_code:3} {logger_timing}{extra_request_data} {logger_client}\"\n    if status_code in [200, 304] and method == \"GET\" and path.startswith(\"/static\"):\n        logger.debug(logger_line)\n    else:\n        logger.info(logger_line)\n\n    if is_slow_query(time_delta, path):\n        slow_query_logger.info(logger_line)\n\n    if settings.PROFILE_ALL_REQUESTS:\n        log_data[\"prof\"].disable()\n        profile_path = \"/tmp/profile.data.{}.{}\".format(path.split(\"/\")[-1], int(time_delta * 1000))\n        log_data[\"prof\"].dump_stats(profile_path)\n\n    # Log some additional data whenever we return certain 40x errors\n    if 400 <= status_code < 500 and status_code not in [401, 404, 405]:\n        assert error_content_iter is not None\n        error_content_list = list(error_content_iter)\n        if not error_content_list:\n            error_data = \"\"\n        elif isinstance(error_content_list[0], str):\n            error_data = \"\".join(error_content_list)\n        elif isinstance(error_content_list[0], bytes):\n            error_data = repr(b\"\".join(error_content_list))\n        if len(error_data) > 200:\n            error_data = \"[content more than 200 characters]\"\n        logger.info(\"status=%3d, data=%s, uid=%s\", status_code, error_data, requestor_for_logs)\n\n\nclass RequestContext(MiddlewareMixin):\n    def __call__(self, request: HttpRequest) -> HttpResponse:\n        set_request(request)\n        try:\n            return self.get_response(request)\n        finally:\n            unset_request()\n\n\ndef parse_client(request: HttpRequest) -> Tuple[str, Optional[str]]:\n    # If the API request specified a client in the request content,\n    # that has priority.  Otherwise, extract the client from the\n    # User-Agent.\n    if \"client\" in request.GET:  # nocoverage\n        return request.GET[\"client\"], None\n    if \"client\" in request.POST:\n        return request.POST[\"client\"], None\n    if \"HTTP_USER_AGENT\" in request.META:\n        user_agent: Optional[Dict[str, str]] = parse_user_agent(request.META[\"HTTP_USER_AGENT\"])\n    else:\n        user_agent = None\n    if user_agent is None:\n        # In the future, we will require setting USER_AGENT, but for\n        # now we just want to tag these requests so we can review them\n        # in logs and figure out the extent of the problem\n        return \"Unspecified\", None\n\n    client_name = user_agent[\"name\"]\n    if client_name.startswith(\"Zulip\"):\n        return client_name, user_agent.get(\"version\")\n\n    # We could show browser versions in logs, and it'd probably be a\n    # good idea, but the current parsing will just get you Mozilla/5.0.\n    #\n    # Fixing this probably means using a third-party library, and\n    # making sure it's fast enough that we're happy to do it even on\n    # hot-path cases.\n    return client_name, None\n\n\nclass LogRequests(MiddlewareMixin):\n    # We primarily are doing logging using the process_view hook, but\n    # for some views, process_view isn't run, so we call the start\n    # method here too\n    def process_request(self, request: HttpRequest) -> None:\n        maybe_tracemalloc_listen()\n        request_notes = RequestNotes.get_notes(request)\n\n        if request_notes.log_data is not None:\n            # Sanity check to ensure this is being called from the\n            # Tornado code path that returns responses asynchronously.\n            assert request_notes.saved_response is not None\n\n            # Avoid re-initializing request_notes.log_data if it's already there.\n            return\n\n        request_notes.client_name, request_notes.client_version = parse_client(request)\n        request_notes.log_data = {}\n        record_request_start_data(request_notes.log_data)\n\n    def process_view(\n        self,\n        request: HttpRequest,\n        view_func: ViewFuncT,\n        args: List[str],\n        kwargs: Dict[str, Any],\n    ) -> None:\n        request_notes = RequestNotes.get_notes(request)\n        if request_notes.saved_response is not None:\n            # The below logging adjustments are unnecessary (because\n            # we've already imported everything) and incorrect\n            # (because they'll overwrite data from pre-long-poll\n            # request processing) when returning a saved response.\n            return\n\n        # process_request was already run; we save the initialization\n        # time (i.e. the time between receiving the request and\n        # figuring out which view function to call, which is primarily\n        # importing modules on the first start)\n        assert request_notes.log_data is not None\n        request_notes.log_data[\"startup_time_delta\"] = (\n            time.time() - request_notes.log_data[\"time_started\"]\n        )\n        # And then completely reset our tracking to only cover work\n        # done as part of this request\n        record_request_start_data(request_notes.log_data)\n\n    def process_response(\n        self, request: HttpRequest, response: HttpResponseBase\n    ) -> HttpResponseBase:\n        if getattr(response, \"asynchronous\", False):\n            # This special Tornado \"asynchronous\" response is\n            # discarded after going through this code path as Tornado\n            # intends to block, so we stop here to avoid unnecessary work.\n            return response\n\n        remote_ip = request.META[\"REMOTE_ADDR\"]\n\n        # Get the requestor's identifier and client, if available.\n        request_notes = RequestNotes.get_notes(request)\n        requestor_for_logs = request_notes.requestor_for_logs\n        if requestor_for_logs is None:\n            # Note that request.user is a Union[RemoteZulipServer, UserProfile, AnonymousUser],\n            # if it is present.\n            if hasattr(request, \"user\") and hasattr(request.user, \"format_requestor_for_logs\"):\n                requestor_for_logs = request.user.format_requestor_for_logs()\n            else:\n                requestor_for_logs = \"unauth@{}\".format(get_subdomain(request) or \"root\")\n\n        if response.streaming:\n            assert isinstance(response, StreamingHttpResponse)\n            content_iter: Optional[Iterator[bytes]] = response.streaming_content\n            content = None\n        else:\n            content = response.content\n            content_iter = None\n\n        assert request_notes.client_name is not None and request_notes.log_data is not None\n        write_log_line(\n            request_notes.log_data,\n            request.path,\n            request.method,\n            remote_ip,\n            requestor_for_logs,\n            request_notes.client_name,\n            client_version=request_notes.client_version,\n            status_code=response.status_code,\n            error_content=content,\n            error_content_iter=content_iter,\n        )\n        return response\n\n\nclass JsonErrorHandler(MiddlewareMixin):\n    def __init__(self, get_response: Callable[[HttpRequest], HttpResponse]) -> None:\n        super().__init__(get_response)\n        ignore_logger(\"zerver.middleware.json_error_handler\")\n\n    def process_exception(\n        self, request: HttpRequest, exception: Exception\n    ) -> Optional[HttpResponse]:\n        if isinstance(exception, MissingAuthenticationError):\n            if \"text/html\" in request.META.get(\"HTTP_ACCEPT\", \"\"):\n                # If this looks like a request from a top-level page in a\n                # browser, send the user to the login page.\n                #\n                # TODO: The next part is a bit questionable; it will\n                # execute the likely intent for intentionally visiting\n                # an API endpoint without authentication in a browser,\n                # but that's an unlikely to be done intentionally often.\n                return HttpResponseRedirect(f\"{settings.HOME_NOT_LOGGED_IN}?next={request.path}\")\n            if request.path.startswith(\"/api\"):\n                # For API routes, ask for HTTP basic auth (email:apiKey).\n                return json_unauthorized()\n            else:\n                # For /json routes, ask for session authentication.\n                return json_unauthorized(www_authenticate=\"session\")\n\n        if isinstance(exception, JsonableError):\n            response = json_response_from_error(exception)\n            if response.status_code >= 500:\n                # Here we use Django's log_response the way Django uses\n                # it normally to log error responses. However, we make the small\n                # modification of including the traceback to make the log message\n                # more helpful. log_response takes care of knowing not to duplicate\n                # the logging, so Django won't generate a second log message.\n                log_response(\n                    \"%s: %s\",\n                    response.reason_phrase,\n                    request.path,\n                    response=response,\n                    request=request,\n                    exc_info=True,\n                )\n            return response\n        if RequestNotes.get_notes(request).error_format == \"JSON\" and not settings.TEST_SUITE:\n            capture_exception(exception)\n            json_error_logger = logging.getLogger(\"zerver.middleware.json_error_handler\")\n            json_error_logger.error(traceback.format_exc(), extra=dict(request=request))\n            return json_response(res_type=\"error\", msg=_(\"Internal server error\"), status=500)\n        return None\n\n\nclass TagRequests(MiddlewareMixin):\n    def process_view(\n        self, request: HttpRequest, view_func: ViewFuncT, args: List[str], kwargs: Dict[str, Any]\n    ) -> None:\n        self.process_request(request)\n\n    def process_request(self, request: HttpRequest) -> None:\n        if request.path.startswith(\"/api/\") or request.path.startswith(\"/json/\"):\n            RequestNotes.get_notes(request).error_format = \"JSON\"\n        else:\n            RequestNotes.get_notes(request).error_format = \"HTML\"\n\n\nclass CsrfFailureError(JsonableError):\n    http_status_code = 403\n    code = ErrorCode.CSRF_FAILED\n    data_fields = [\"reason\"]\n\n    def __init__(self, reason: str) -> None:\n        self.reason: str = reason\n\n    @staticmethod\n    def msg_format() -> str:\n        return _(\"CSRF error: {reason}\")\n\n\ndef csrf_failure(request: HttpRequest, reason: str = \"\") -> HttpResponse:\n    if RequestNotes.get_notes(request).error_format == \"JSON\":\n        return json_response_from_error(CsrfFailureError(reason))\n    else:\n        return html_csrf_failure(request, reason)\n\n\nclass LocaleMiddleware(DjangoLocaleMiddleware):\n    def process_response(\n        self, request: HttpRequest, response: HttpResponseBase\n    ) -> HttpResponseBase:\n\n        # This is the same as the default LocaleMiddleware, minus the\n        # logic that redirects 404's that lack a prefixed language in\n        # the path into having a language.  See\n        # https://code.djangoproject.com/ticket/32005\n        language = translation.get_language()\n        language_from_path = translation.get_language_from_path(request.path_info)\n        urlconf = getattr(request, \"urlconf\", settings.ROOT_URLCONF)\n        i18n_patterns_used, _ = is_language_prefix_patterns_used(urlconf)\n        if not (i18n_patterns_used and language_from_path):\n            patch_vary_headers(response, (\"Accept-Language\",))\n        assert language is not None\n        response.setdefault(\"Content-Language\", language)\n\n        # An additional responsibility of our override of this middleware is to save the user's language\n        # preference in a cookie. That determination is made by code handling the request\n        # and saved in the set_language flag so that it can be used here.\n        set_language = RequestNotes.get_notes(request).set_language\n        if set_language is not None:\n            response.set_cookie(settings.LANGUAGE_COOKIE_NAME, set_language)\n\n        return response\n\n\nclass RateLimitMiddleware(MiddlewareMixin):\n    def set_response_headers(\n        self, response: HttpResponse, rate_limit_results: List[RateLimitResult]\n    ) -> None:\n        # The limit on the action that was requested is the minimum of the limits that get applied:\n        limit = min(result.entity.max_api_calls() for result in rate_limit_results)\n        response[\"X-RateLimit-Limit\"] = str(limit)\n        # Same principle applies to remaining API calls:\n        remaining_api_calls = min(result.remaining for result in rate_limit_results)\n        response[\"X-RateLimit-Remaining\"] = str(remaining_api_calls)\n\n        # The full reset time is the maximum of the reset times for the limits that get applied:\n        reset_time = time.time() + max(result.secs_to_freedom for result in rate_limit_results)\n        response[\"X-RateLimit-Reset\"] = str(int(reset_time))\n\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:\n        if not settings.RATE_LIMITING:\n            return response\n\n        # Add X-RateLimit-*** headers\n        ratelimits_applied = RequestNotes.get_notes(request).ratelimits_applied\n        if len(ratelimits_applied) > 0:\n            self.set_response_headers(response, ratelimits_applied)\n\n        return response\n\n\nclass FlushDisplayRecipientCache(MiddlewareMixin):\n    def process_response(self, request: HttpRequest, response: HttpResponse) -> HttpResponse:\n        # We flush the per-request caches after every request, so they\n        # are not shared at all between requests.\n        flush_per_request_caches()\n        return response\n\n\nclass HostDomainMiddleware(MiddlewareMixin):\n    def process_request(self, request: HttpRequest) -> Optional[HttpResponse]:\n        # Match against ALLOWED_HOSTS, which is rather permissive;\n        # failure will raise DisallowedHost, which is a 400.\n        request.get_host()\n\n        # This check is important to avoid doing the extra work of\n        # `get_realm` (which does a database query that could be\n        # problematic for Tornado).  Also the error page below is only\n        # appropriate for a page visited in a browser, not the API.\n        #\n        # API authentication will end up checking for an invalid\n        # realm, and throw a JSON-format error if appropriate.\n        if request.path.startswith((\"/static/\", \"/api/\", \"/json/\")):\n            return None\n\n        subdomain = get_subdomain(request)\n        if subdomain == settings.SOCIAL_AUTH_SUBDOMAIN:\n            # Realms are not supposed to exist on SOCIAL_AUTH_SUBDOMAIN.\n            return None\n\n        request_notes = RequestNotes.get_notes(request)\n        try:\n            request_notes.realm = get_realm(subdomain)\n            request_notes.has_fetched_realm = True\n        except Realm.DoesNotExist:\n            if subdomain == Realm.SUBDOMAIN_FOR_ROOT_DOMAIN:\n                # The root domain is used for creating new\n                # organizations even if it does not host a realm.\n                return None\n\n            return render(request, \"zerver/invalid_realm.html\", status=404)\n\n        return None\n\n\nclass SetRemoteAddrFromRealIpHeader(MiddlewareMixin):\n    \"\"\"Middleware that sets REMOTE_ADDR based on the X-Real-Ip header.\n\n    This middleware is similar to Django's old\n    SetRemoteAddrFromForwardedFor middleware.  We use X-Real-Ip, and\n    not X-Forwarded-For, because the latter is a list of proxies, some\n    number of which are trusted by us, and some of which could be\n    arbitrarily set by the user.  nginx has already parsed which are\n    which, and has set X-Real-Ip to the first one, going right to\n    left, which is untrusted.\n\n    Since we are always deployed behind nginx, we can trust the\n    X-Real-Ip which is so set.  In development, we fall back to the\n    REMOTE_ADDR supplied by the server.\n\n    \"\"\"\n\n    def process_request(self, request: HttpRequest) -> None:\n        try:\n            real_ip = request.META[\"HTTP_X_REAL_IP\"]\n        except KeyError:\n            return None\n        else:\n            request.META[\"REMOTE_ADDR\"] = real_ip\n\n\ndef alter_content(request: HttpRequest, content: bytes) -> bytes:\n    first_paragraph_text = get_content_description(content, request)\n    placeholder_open_graph_description = RequestNotes.get_notes(\n        request\n    ).placeholder_open_graph_description\n    assert placeholder_open_graph_description is not None\n    return content.replace(\n        placeholder_open_graph_description.encode(),\n        first_paragraph_text.encode(),\n    )\n\n\nclass FinalizeOpenGraphDescription(MiddlewareMixin):\n    def process_response(\n        self, request: HttpRequest, response: StreamingHttpResponse\n    ) -> StreamingHttpResponse:\n\n        if RequestNotes.get_notes(request).placeholder_open_graph_description is not None:\n            assert not response.streaming\n            response.content = alter_content(request, response.content)\n        return response\n\n\nclass ZulipCommonMiddleware(CommonMiddleware):\n    \"\"\"\n    Patched version of CommonMiddleware to disable the APPEND_SLASH\n    redirect behavior inside Tornado.\n\n    While this has some correctness benefit in encouraging clients\n    to implement the API correctly, this also saves about 600us in\n    the runtime of every GET /events query, as the APPEND_SLASH\n    route resolution logic is surprisingly expensive.\n\n    TODO: We should probably extend this behavior to apply to all of\n    our API routes.  The APPEND_SLASH behavior is really only useful\n    for non-API endpoints things like /login.  But doing that\n    transition will require more careful testing.\n    \"\"\"\n\n    def should_redirect_with_slash(self, request: HttpRequest) -> bool:\n        if settings.RUNNING_INSIDE_TORNADO:\n            return False\n        return super().should_redirect_with_slash(request)\n\n\ndef validate_scim_bearer_token(request: HttpRequest) -> Optional[SCIMClient]:\n    \"\"\"\n    This function verifies the request is allowed to make SCIM requests on this subdomain,\n    by checking the provided bearer token and ensuring it matches a scim client configured\n    for this subdomain in settings.SCIM_CONFIG.\n    If successful, returns the corresponding SCIMClient object. Returns None otherwise.\n    \"\"\"\n\n    subdomain = get_subdomain(request)\n    scim_config_dict = settings.SCIM_CONFIG.get(subdomain)\n    if not scim_config_dict:\n        return None\n\n    valid_bearer_token = scim_config_dict.get(\"bearer_token\")\n    scim_client_name = scim_config_dict.get(\"scim_client_name\")\n    # We really don't want a misconfiguration where this is unset,\n    # allowing free access to the SCIM API:\n    assert valid_bearer_token\n    assert scim_client_name\n\n    authorization = request.headers.get(\"Authorization\")\n    if authorization is None or not constant_time_compare(\n        authorization, f\"Bearer {valid_bearer_token}\"\n    ):\n        return None\n\n    request_notes = RequestNotes.get_notes(request)\n    assert request_notes.realm\n\n    # While API authentication code paths are sufficiently high\n    # traffic that we prefer to use a cache, SCIM is much lower\n    # traffic, and doing a database query is plenty fast.\n    return SCIMClient.objects.get(realm=request_notes.realm, name=scim_client_name)\n\n\nclass ZulipSCIMAuthCheckMiddleware(SCIMAuthCheckMiddleware):\n    \"\"\"\n    Overridden version of middleware implemented in django-scim2\n    (https://github.com/15five/django-scim2/blob/master/src/django_scim/middleware.py)\n    to also handle authenticating the client.\n    \"\"\"\n\n    def process_request(self, request: HttpRequest) -> Optional[HttpResponse]:\n        # This determines whether this is a SCIM request based on the request's path\n        # and if it is, logs request information, including the body, as well as the response\n        # for debugging purposes to the `django_scim.middleware` logger, at DEBUG level.\n        # We keep those logs in /var/log/zulip/scim.log\n        if self.should_log_request(request):\n            self.log_request(request)\n\n        # Here we verify the request is indeed to a SCIM endpoint. That's ensured\n        # by comparing the path with self.reverse_url, which is the root SCIM path /scim/b2/.\n        # Of course we don't want to proceed with authenticating the request for SCIM\n        # if a non-SCIM endpoint is being queried.\n        if not request.path.startswith(self.reverse_url):\n            return None\n\n        scim_client = validate_scim_bearer_token(request)\n        if not scim_client:\n            response = HttpResponse(status=401)\n            response[\"WWW-Authenticate\"] = scim_settings.WWW_AUTHENTICATE_HEADER\n            return response\n\n        # The client has been successfully authenticated for SCIM on this subdomain,\n        # so we can assign the corresponding SCIMClient object to request.user - which\n        # will allow this request to pass request.user.is_authenticated checks from now on,\n        # to be served by the relevant views implemented in django-scim2.\n        request.user = scim_client\n        return None\n"], "filenames": ["zerver/middleware.py"], "buggy_code_start_loc": [27], "buggy_code_end_loc": [708], "fixing_code_start_loc": [28], "fixing_code_end_loc": [712], "type": "CWE-203", "message": "Zulip is an open-source team collaboration tool. For organizations with System for Cross-domain Identity Management(SCIM) account management enabled, Zulip Server 5.0 through 5.6 checked the SCIM bearer token using a comparator that did not run in constant time. Therefore, it might theoretically be possible for an attacker to infer the value of the token by performing a sophisticated timing analysis on a large number of failing requests. If successful, this would allow the attacker to impersonate the SCIM client for its abilities to read and update user accounts in the Zulip organization. Organizations where SCIM account management has not been enabled are not affected.", "other": {"cve": {"id": "CVE-2022-41914", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-16T20:15:10.580", "lastModified": "2022-11-21T20:22:33.803", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Zulip is an open-source team collaboration tool. For organizations with System for Cross-domain Identity Management(SCIM) account management enabled, Zulip Server 5.0 through 5.6 checked the SCIM bearer token using a comparator that did not run in constant time. Therefore, it might theoretically be possible for an attacker to infer the value of the token by performing a sophisticated timing analysis on a large number of failing requests. If successful, this would allow the attacker to impersonate the SCIM client for its abilities to read and update user accounts in the Zulip organization. Organizations where SCIM account management has not been enabled are not affected."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 3.7, "baseSeverity": "LOW"}, "exploitabilityScore": 2.2, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 3.7, "baseSeverity": "LOW"}, "exploitabilityScore": 2.2, "impactScore": 1.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-203"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:zulip:zulip_server:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.0", "versionEndExcluding": "5.7", "matchCriteriaId": "4FC3AF9F-EF20-4737-B7CC-0D2F0C69781E"}]}]}], "references": [{"url": "https://github.com/zulip/zulip/commit/59edbfa4113d140d3e20126bc65f4d67b2a8ffe5", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/zulip/zulip/security/advisories/GHSA-q5gx-377v-w76f", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/zulip/zulip/commit/59edbfa4113d140d3e20126bc65f4d67b2a8ffe5"}}