{"buggy_code": ["/*\n * Copyright (C) 2012,2013 - ARM Ltd\n * Author: Marc Zyngier <marc.zyngier@arm.com>\n *\n * Derived from arch/arm/kvm/coproc.c:\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Authors: Rusty Russell <rusty@rustcorp.com.au>\n *          Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/bsearch.h>\n#include <linux/kvm_host.h>\n#include <linux/mm.h>\n#include <linux/uaccess.h>\n\n#include <asm/cacheflush.h>\n#include <asm/cputype.h>\n#include <asm/debug-monitors.h>\n#include <asm/esr.h>\n#include <asm/kvm_arm.h>\n#include <asm/kvm_asm.h>\n#include <asm/kvm_coproc.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_host.h>\n#include <asm/kvm_mmu.h>\n#include <asm/perf_event.h>\n#include <asm/sysreg.h>\n\n#include <trace/events/kvm.h>\n\n#include \"sys_regs.h\"\n\n#include \"trace.h\"\n\n/*\n * All of this file is extremly similar to the ARM coproc.c, but the\n * types are different. My gut feeling is that it should be pretty\n * easy to merge, but that would be an ABI breakage -- again. VFP\n * would also need to be abstracted.\n *\n * For AArch32, we only take care of what is being trapped. Anything\n * that has to do with init and userspace access has to go via the\n * 64bit interface.\n */\n\n/* 3 bits per cache level, as per CLIDR, but non-existent caches always 0 */\nstatic u32 cache_levels;\n\n/* CSSELR values; used to index KVM_REG_ARM_DEMUX_ID_CCSIDR */\n#define CSSELR_MAX 12\n\n/* Which cache CCSIDR represents depends on CSSELR value. */\nstatic u32 get_ccsidr(u32 csselr)\n{\n\tu32 ccsidr;\n\n\t/* Make sure noone else changes CSSELR during this! */\n\tlocal_irq_disable();\n\twrite_sysreg(csselr, csselr_el1);\n\tisb();\n\tccsidr = read_sysreg(ccsidr_el1);\n\tlocal_irq_enable();\n\n\treturn ccsidr;\n}\n\n/*\n * See note at ARMv7 ARM B1.14.4 (TL;DR: S/W ops are not easily virtualized).\n */\nstatic bool access_dcsw(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (!p->is_write)\n\t\treturn read_from_write_only(vcpu, p);\n\n\tkvm_set_way_flush(vcpu);\n\treturn true;\n}\n\n/*\n * Generic accessor for VM registers. Only called as long as HCR_TVM\n * is set. If the guest enables the MMU, we stop trapping the VM\n * sys_regs and leave it in complete control of the caches.\n */\nstatic bool access_vm_reg(struct kvm_vcpu *vcpu,\n\t\t\t  struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tbool was_enabled = vcpu_has_cache_enabled(vcpu);\n\n\tBUG_ON(!p->is_write);\n\n\tif (!p->is_aarch32) {\n\t\tvcpu_sys_reg(vcpu, r->reg) = p->regval;\n\t} else {\n\t\tif (!p->is_32bit)\n\t\t\tvcpu_cp15_64_high(vcpu, r->reg) = upper_32_bits(p->regval);\n\t\tvcpu_cp15_64_low(vcpu, r->reg) = lower_32_bits(p->regval);\n\t}\n\n\tkvm_toggle_cache(vcpu, was_enabled);\n\treturn true;\n}\n\n/*\n * Trap handler for the GICv3 SGI generation system register.\n * Forward the request to the VGIC emulation.\n * The cp15_64 code makes sure this automatically works\n * for both AArch64 and AArch32 accesses.\n */\nstatic bool access_gic_sgi(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (!p->is_write)\n\t\treturn read_from_write_only(vcpu, p);\n\n\tvgic_v3_dispatch_sgi(vcpu, p->regval);\n\n\treturn true;\n}\n\nstatic bool access_gic_sre(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write)\n\t\treturn ignore_write(vcpu, p);\n\n\tp->regval = vcpu->arch.vgic_cpu.vgic_v3.vgic_sre;\n\treturn true;\n}\n\nstatic bool trap_raz_wi(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (p->is_write)\n\t\treturn ignore_write(vcpu, p);\n\telse\n\t\treturn read_zero(vcpu, p);\n}\n\nstatic bool trap_oslsr_el1(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tp->regval = (1 << 3);\n\t\treturn true;\n\t}\n}\n\nstatic bool trap_dbgauthstatus_el1(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct sys_reg_params *p,\n\t\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tp->regval = read_sysreg(dbgauthstatus_el1);\n\t\treturn true;\n\t}\n}\n\n/*\n * We want to avoid world-switching all the DBG registers all the\n * time:\n * \n * - If we've touched any debug register, it is likely that we're\n *   going to touch more of them. It then makes sense to disable the\n *   traps and start doing the save/restore dance\n * - If debug is active (DBG_MDSCR_KDE or DBG_MDSCR_MDE set), it is\n *   then mandatory to save/restore the registers, as the guest\n *   depends on them.\n * \n * For this, we use a DIRTY bit, indicating the guest has modified the\n * debug registers, used as follow:\n *\n * On guest entry:\n * - If the dirty bit is set (because we're coming back from trapping),\n *   disable the traps, save host registers, restore guest registers.\n * - If debug is actively in use (DBG_MDSCR_KDE or DBG_MDSCR_MDE set),\n *   set the dirty bit, disable the traps, save host registers,\n *   restore guest registers.\n * - Otherwise, enable the traps\n *\n * On guest exit:\n * - If the dirty bit is set, save guest registers, restore host\n *   registers and clear the dirty bit. This ensure that the host can\n *   now use the debug registers.\n */\nstatic bool trap_debug_regs(struct kvm_vcpu *vcpu,\n\t\t\t    struct sys_reg_params *p,\n\t\t\t    const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\tvcpu_sys_reg(vcpu, r->reg) = p->regval;\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, r->reg);\n\t}\n\n\ttrace_trap_reg(__func__, r->reg, p->is_write, p->regval);\n\n\treturn true;\n}\n\n/*\n * reg_to_dbg/dbg_to_reg\n *\n * A 32 bit write to a debug register leave top bits alone\n * A 32 bit read from a debug register only returns the bottom bits\n *\n * All writes will set the KVM_ARM64_DEBUG_DIRTY flag to ensure the\n * hyp.S code switches between host and guest values in future.\n */\nstatic void reg_to_dbg(struct kvm_vcpu *vcpu,\n\t\t       struct sys_reg_params *p,\n\t\t       u64 *dbg_reg)\n{\n\tu64 val = p->regval;\n\n\tif (p->is_32bit) {\n\t\tval &= 0xffffffffUL;\n\t\tval |= ((*dbg_reg >> 32) << 32);\n\t}\n\n\t*dbg_reg = val;\n\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n}\n\nstatic void dbg_to_reg(struct kvm_vcpu *vcpu,\n\t\t       struct sys_reg_params *p,\n\t\t       u64 *dbg_reg)\n{\n\tp->regval = *dbg_reg;\n\tif (p->is_32bit)\n\t\tp->regval &= 0xffffffffUL;\n}\n\nstatic bool trap_bvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_bvr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg] = rd->val;\n}\n\nstatic bool trap_bcr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int get_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_bcr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg] = rd->val;\n}\n\nstatic bool trap_wvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write,\n\t\tvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg]);\n\n\treturn true;\n}\n\nstatic int set_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_wvr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg] = rd->val;\n}\n\nstatic bool trap_wcr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_wcr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg] = rd->val;\n}\n\nstatic void reset_amair_el1(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tvcpu_sys_reg(vcpu, AMAIR_EL1) = read_sysreg(amair_el1);\n}\n\nstatic void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tu64 mpidr;\n\n\t/*\n\t * Map the vcpu_id into the first three affinity level fields of\n\t * the MPIDR. We limit the number of VCPUs in level 0 due to a\n\t * limitation to 16 CPUs in that level in the ICC_SGIxR registers\n\t * of the GICv3 to be able to address each CPU directly when\n\t * sending IPIs.\n\t */\n\tmpidr = (vcpu->vcpu_id & 0x0f) << MPIDR_LEVEL_SHIFT(0);\n\tmpidr |= ((vcpu->vcpu_id >> 4) & 0xff) << MPIDR_LEVEL_SHIFT(1);\n\tmpidr |= ((vcpu->vcpu_id >> 12) & 0xff) << MPIDR_LEVEL_SHIFT(2);\n\tvcpu_sys_reg(vcpu, MPIDR_EL1) = (1ULL << 31) | mpidr;\n}\n\nstatic void reset_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tu64 pmcr, val;\n\n\tpmcr = read_sysreg(pmcr_el0);\n\t/*\n\t * Writable bits of PMCR_EL0 (ARMV8_PMU_PMCR_MASK) are reset to UNKNOWN\n\t * except PMCR.E resetting to zero.\n\t */\n\tval = ((pmcr & ~ARMV8_PMU_PMCR_MASK)\n\t       | (ARMV8_PMU_PMCR_MASK & 0xdecafbad)) & (~ARMV8_PMU_PMCR_E);\n\tvcpu_sys_reg(vcpu, PMCR_EL0) = val;\n}\n\nstatic bool pmu_access_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & ARMV8_PMU_USERENR_EN) || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_write_swinc_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_SW | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_access_cycle_counter_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_CR | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_access_event_counter_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tu64 val;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\t/* Only update writeable bits of PMCR */\n\t\tval = vcpu_sys_reg(vcpu, PMCR_EL0);\n\t\tval &= ~ARMV8_PMU_PMCR_MASK;\n\t\tval |= p->regval & ARMV8_PMU_PMCR_MASK;\n\t\tvcpu_sys_reg(vcpu, PMCR_EL0) = val;\n\t\tkvm_pmu_handle_pmcr(vcpu, val);\n\t} else {\n\t\t/* PMCR.P & PMCR.C are RAZ */\n\t\tval = vcpu_sys_reg(vcpu, PMCR_EL0)\n\t\t      & ~(ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C);\n\t\tp->regval = val;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write)\n\t\tvcpu_sys_reg(vcpu, PMSELR_EL0) = p->regval;\n\telse\n\t\t/* return PMSELR.SEL field */\n\t\tp->regval = vcpu_sys_reg(vcpu, PMSELR_EL0)\n\t\t\t    & ARMV8_PMU_COUNTER_MASK;\n\n\treturn true;\n}\n\nstatic bool access_pmceid(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tu64 pmceid;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tBUG_ON(p->is_write);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (!(p->Op2 & 1))\n\t\tpmceid = read_sysreg(pmceid0_el0);\n\telse\n\t\tpmceid = read_sysreg(pmceid1_el0);\n\n\tp->regval = pmceid;\n\n\treturn true;\n}\n\nstatic bool pmu_counter_idx_valid(struct kvm_vcpu *vcpu, u64 idx)\n{\n\tu64 pmcr, val;\n\n\tpmcr = vcpu_sys_reg(vcpu, PMCR_EL0);\n\tval = (pmcr >> ARMV8_PMU_PMCR_N_SHIFT) & ARMV8_PMU_PMCR_N_MASK;\n\tif (idx >= val && idx != ARMV8_PMU_CYCLE_IDX)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool access_pmu_evcntr(struct kvm_vcpu *vcpu,\n\t\t\t      struct sys_reg_params *p,\n\t\t\t      const struct sys_reg_desc *r)\n{\n\tu64 idx;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (r->CRn == 9 && r->CRm == 13) {\n\t\tif (r->Op2 == 2) {\n\t\t\t/* PMXEVCNTR_EL0 */\n\t\t\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\t\t\treturn false;\n\n\t\t\tidx = vcpu_sys_reg(vcpu, PMSELR_EL0)\n\t\t\t      & ARMV8_PMU_COUNTER_MASK;\n\t\t} else if (r->Op2 == 0) {\n\t\t\t/* PMCCNTR_EL0 */\n\t\t\tif (pmu_access_cycle_counter_el0_disabled(vcpu))\n\t\t\t\treturn false;\n\n\t\t\tidx = ARMV8_PMU_CYCLE_IDX;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t} else if (r->CRn == 14 && (r->CRm & 12) == 8) {\n\t\t/* PMEVCNTRn_EL0 */\n\t\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\t\treturn false;\n\n\t\tidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\n\t} else {\n\t\tBUG();\n\t}\n\n\tif (!pmu_counter_idx_valid(vcpu, idx))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tif (pmu_access_el0_disabled(vcpu))\n\t\t\treturn false;\n\n\t\tkvm_pmu_set_counter_value(vcpu, idx, p->regval);\n\t} else {\n\t\tp->regval = kvm_pmu_get_counter_value(vcpu, idx);\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t       const struct sys_reg_desc *r)\n{\n\tu64 idx, reg;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (r->CRn == 9 && r->CRm == 13 && r->Op2 == 1) {\n\t\t/* PMXEVTYPER_EL0 */\n\t\tidx = vcpu_sys_reg(vcpu, PMSELR_EL0) & ARMV8_PMU_COUNTER_MASK;\n\t\treg = PMEVTYPER0_EL0 + idx;\n\t} else if (r->CRn == 14 && (r->CRm & 12) == 12) {\n\t\tidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\n\t\tif (idx == ARMV8_PMU_CYCLE_IDX)\n\t\t\treg = PMCCFILTR_EL0;\n\t\telse\n\t\t\t/* PMEVTYPERn_EL0 */\n\t\t\treg = PMEVTYPER0_EL0 + idx;\n\t} else {\n\t\tBUG();\n\t}\n\n\tif (!pmu_counter_idx_valid(vcpu, idx))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tkvm_pmu_set_counter_event_type(vcpu, p->regval, idx);\n\t\tvcpu_sys_reg(vcpu, reg) = p->regval & ARMV8_PMU_EVTYPE_MASK;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, reg) & ARMV8_PMU_EVTYPE_MASK;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmcnten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 val, mask;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tmask = kvm_pmu_valid_counter_mask(vcpu);\n\tif (p->is_write) {\n\t\tval = p->regval & mask;\n\t\tif (r->Op2 & 0x1) {\n\t\t\t/* accessing PMCNTENSET_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMCNTENSET_EL0) |= val;\n\t\t\tkvm_pmu_enable_counter(vcpu, val);\n\t\t} else {\n\t\t\t/* accessing PMCNTENCLR_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMCNTENSET_EL0) &= ~val;\n\t\t\tkvm_pmu_disable_counter(vcpu, val);\n\t\t}\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMCNTENSET_EL0) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pminten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 mask = kvm_pmu_valid_counter_mask(vcpu);\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (!vcpu_mode_priv(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tu64 val = p->regval & mask;\n\n\t\tif (r->Op2 & 0x1)\n\t\t\t/* accessing PMINTENSET_EL1 */\n\t\t\tvcpu_sys_reg(vcpu, PMINTENSET_EL1) |= val;\n\t\telse\n\t\t\t/* accessing PMINTENCLR_EL1 */\n\t\t\tvcpu_sys_reg(vcpu, PMINTENSET_EL1) &= ~val;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMINTENSET_EL1) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmovs(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t const struct sys_reg_desc *r)\n{\n\tu64 mask = kvm_pmu_valid_counter_mask(vcpu);\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tif (r->CRm & 0x2)\n\t\t\t/* accessing PMOVSSET_EL0 */\n\t\t\tkvm_pmu_overflow_set(vcpu, p->regval & mask);\n\t\telse\n\t\t\t/* accessing PMOVSCLR_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMOVSSET_EL0) &= ~(p->regval & mask);\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMOVSSET_EL0) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmswinc(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 mask;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_write_swinc_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tmask = kvm_pmu_valid_counter_mask(vcpu);\n\t\tkvm_pmu_software_increment(vcpu, p->regval & mask);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t     const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (p->is_write) {\n\t\tif (!vcpu_mode_priv(vcpu))\n\t\t\treturn false;\n\n\t\tvcpu_sys_reg(vcpu, PMUSERENR_EL0) = p->regval\n\t\t\t\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMUSERENR_EL0)\n\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t}\n\n\treturn true;\n}\n\n/* Silly macro to expand the DBG{BCR,BVR,WVR,WCR}n_EL1 registers in one go */\n#define DBG_BCR_BVR_WCR_WVR_EL1(n)\t\t\t\t\t\\\n\t/* DBGBVRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b100),\t\\\n\t  trap_bvr, reset_bvr, n, 0, get_bvr, set_bvr },\t\t\\\n\t/* DBGBCRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b101),\t\\\n\t  trap_bcr, reset_bcr, n, 0, get_bcr, set_bcr },\t\t\\\n\t/* DBGWVRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b110),\t\\\n\t  trap_wvr, reset_wvr, n, 0,  get_wvr, set_wvr },\t\t\\\n\t/* DBGWCRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b111),\t\\\n\t  trap_wcr, reset_wcr, n, 0,  get_wcr, set_wcr }\n\n/* Macro to expand the PMEVCNTRn_EL0 register */\n#define PMU_PMEVCNTR_EL0(n)\t\t\t\t\t\t\\\n\t/* PMEVCNTRn_EL0 */\t\t\t\t\t\t\\\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110),\t\t\t\t\\\n\t  CRm((0b1000 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evcntr, reset_unknown, (PMEVCNTR0_EL0 + n), }\n\n/* Macro to expand the PMEVTYPERn_EL0 register */\n#define PMU_PMEVTYPER_EL0(n)\t\t\t\t\t\t\\\n\t/* PMEVTYPERn_EL0 */\t\t\t\t\t\t\\\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110),\t\t\t\t\\\n\t  CRm((0b1100 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evtyper, reset_unknown, (PMEVTYPER0_EL0 + n), }\n\n/*\n * Architected system registers.\n * Important: Must be sorted ascending by Op0, Op1, CRn, CRm, Op2\n *\n * Debug handling: We do trap most, if not all debug related system\n * registers. The implementation is good enough to ensure that a guest\n * can use these with minimal performance degradation. The drawback is\n * that we don't implement any of the external debug, none of the\n * OSlock protocol. This should be revisited if we ever encounter a\n * more demanding guest...\n */\nstatic const struct sys_reg_desc sys_reg_descs[] = {\n\t/* DC ISW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b0110), Op2(0b010),\n\t  access_dcsw },\n\t/* DC CSW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b1010), Op2(0b010),\n\t  access_dcsw },\n\t/* DC CISW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b1110), Op2(0b010),\n\t  access_dcsw },\n\n\tDBG_BCR_BVR_WCR_WVR_EL1(0),\n\tDBG_BCR_BVR_WCR_WVR_EL1(1),\n\t/* MDCCINT_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b000),\n\t  trap_debug_regs, reset_val, MDCCINT_EL1, 0 },\n\t/* MDSCR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b010),\n\t  trap_debug_regs, reset_val, MDSCR_EL1, 0 },\n\tDBG_BCR_BVR_WCR_WVR_EL1(2),\n\tDBG_BCR_BVR_WCR_WVR_EL1(3),\n\tDBG_BCR_BVR_WCR_WVR_EL1(4),\n\tDBG_BCR_BVR_WCR_WVR_EL1(5),\n\tDBG_BCR_BVR_WCR_WVR_EL1(6),\n\tDBG_BCR_BVR_WCR_WVR_EL1(7),\n\tDBG_BCR_BVR_WCR_WVR_EL1(8),\n\tDBG_BCR_BVR_WCR_WVR_EL1(9),\n\tDBG_BCR_BVR_WCR_WVR_EL1(10),\n\tDBG_BCR_BVR_WCR_WVR_EL1(11),\n\tDBG_BCR_BVR_WCR_WVR_EL1(12),\n\tDBG_BCR_BVR_WCR_WVR_EL1(13),\n\tDBG_BCR_BVR_WCR_WVR_EL1(14),\n\tDBG_BCR_BVR_WCR_WVR_EL1(15),\n\n\t/* MDRAR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b000),\n\t  trap_raz_wi },\n\t/* OSLAR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b100),\n\t  trap_raz_wi },\n\t/* OSLSR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0001), Op2(0b100),\n\t  trap_oslsr_el1 },\n\t/* OSDLR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0011), Op2(0b100),\n\t  trap_raz_wi },\n\t/* DBGPRCR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0100), Op2(0b100),\n\t  trap_raz_wi },\n\t/* DBGCLAIMSET_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1000), Op2(0b110),\n\t  trap_raz_wi },\n\t/* DBGCLAIMCLR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1001), Op2(0b110),\n\t  trap_raz_wi },\n\t/* DBGAUTHSTATUS_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1110), Op2(0b110),\n\t  trap_dbgauthstatus_el1 },\n\n\t/* MDCCSR_EL1 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0001), Op2(0b000),\n\t  trap_raz_wi },\n\t/* DBGDTR_EL0 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0100), Op2(0b000),\n\t  trap_raz_wi },\n\t/* DBGDTR[TR]X_EL0 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0101), Op2(0b000),\n\t  trap_raz_wi },\n\n\t/* DBGVCR32_EL2 */\n\t{ Op0(0b10), Op1(0b100), CRn(0b0000), CRm(0b0111), Op2(0b000),\n\t  NULL, reset_val, DBGVCR32_EL2, 0 },\n\n\t/* MPIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b101),\n\t  NULL, reset_mpidr, MPIDR_EL1 },\n\t/* SCTLR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_val, SCTLR_EL1, 0x00C50078 },\n\t/* CPACR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b010),\n\t  NULL, reset_val, CPACR_EL1, 0 },\n\t/* TTBR0_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_unknown, TTBR0_EL1 },\n\t/* TTBR1_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b001),\n\t  access_vm_reg, reset_unknown, TTBR1_EL1 },\n\t/* TCR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b010),\n\t  access_vm_reg, reset_val, TCR_EL1, 0 },\n\n\t/* AFSR0_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0001), Op2(0b000),\n\t  access_vm_reg, reset_unknown, AFSR0_EL1 },\n\t/* AFSR1_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0001), Op2(0b001),\n\t  access_vm_reg, reset_unknown, AFSR1_EL1 },\n\t/* ESR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0010), Op2(0b000),\n\t  access_vm_reg, reset_unknown, ESR_EL1 },\n\t/* FAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0110), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_unknown, FAR_EL1 },\n\t/* PAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0111), CRm(0b0100), Op2(0b000),\n\t  NULL, reset_unknown, PAR_EL1 },\n\n\t/* PMINTENSET_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1001), CRm(0b1110), Op2(0b001),\n\t  access_pminten, reset_unknown, PMINTENSET_EL1 },\n\t/* PMINTENCLR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1001), CRm(0b1110), Op2(0b010),\n\t  access_pminten, NULL, PMINTENSET_EL1 },\n\n\t/* MAIR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1010), CRm(0b0010), Op2(0b000),\n\t  access_vm_reg, reset_unknown, MAIR_EL1 },\n\t/* AMAIR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1010), CRm(0b0011), Op2(0b000),\n\t  access_vm_reg, reset_amair_el1, AMAIR_EL1 },\n\n\t/* VBAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_val, VBAR_EL1, 0 },\n\n\t/* ICC_SGI1R_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b1011), Op2(0b101),\n\t  access_gic_sgi },\n\t/* ICC_SRE_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b1100), Op2(0b101),\n\t  access_gic_sre },\n\n\t/* CONTEXTIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1101), CRm(0b0000), Op2(0b001),\n\t  access_vm_reg, reset_val, CONTEXTIDR_EL1, 0 },\n\t/* TPIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1101), CRm(0b0000), Op2(0b100),\n\t  NULL, reset_unknown, TPIDR_EL1 },\n\n\t/* CNTKCTL_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1110), CRm(0b0001), Op2(0b000),\n\t  NULL, reset_val, CNTKCTL_EL1, 0},\n\n\t/* CSSELR_EL1 */\n\t{ Op0(0b11), Op1(0b010), CRn(0b0000), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_unknown, CSSELR_EL1 },\n\n\t/* PMCR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b000),\n\t  access_pmcr, reset_pmcr, },\n\t/* PMCNTENSET_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b001),\n\t  access_pmcnten, reset_unknown, PMCNTENSET_EL0 },\n\t/* PMCNTENCLR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b010),\n\t  access_pmcnten, NULL, PMCNTENSET_EL0 },\n\t/* PMOVSCLR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b011),\n\t  access_pmovs, NULL, PMOVSSET_EL0 },\n\t/* PMSWINC_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b100),\n\t  access_pmswinc, reset_unknown, PMSWINC_EL0 },\n\t/* PMSELR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b101),\n\t  access_pmselr, reset_unknown, PMSELR_EL0 },\n\t/* PMCEID0_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b110),\n\t  access_pmceid },\n\t/* PMCEID1_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b111),\n\t  access_pmceid },\n\t/* PMCCNTR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b000),\n\t  access_pmu_evcntr, reset_unknown, PMCCNTR_EL0 },\n\t/* PMXEVTYPER_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b001),\n\t  access_pmu_evtyper },\n\t/* PMXEVCNTR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b010),\n\t  access_pmu_evcntr },\n\t/* PMUSERENR_EL0\n\t * This register resets as unknown in 64bit mode while it resets as zero\n\t * in 32bit mode. Here we choose to reset it as zero for consistency.\n\t */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1110), Op2(0b000),\n\t  access_pmuserenr, reset_val, PMUSERENR_EL0, 0 },\n\t/* PMOVSSET_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1110), Op2(0b011),\n\t  access_pmovs, reset_unknown, PMOVSSET_EL0 },\n\n\t/* TPIDR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1101), CRm(0b0000), Op2(0b010),\n\t  NULL, reset_unknown, TPIDR_EL0 },\n\t/* TPIDRRO_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1101), CRm(0b0000), Op2(0b011),\n\t  NULL, reset_unknown, TPIDRRO_EL0 },\n\n\t/* PMEVCNTRn_EL0 */\n\tPMU_PMEVCNTR_EL0(0),\n\tPMU_PMEVCNTR_EL0(1),\n\tPMU_PMEVCNTR_EL0(2),\n\tPMU_PMEVCNTR_EL0(3),\n\tPMU_PMEVCNTR_EL0(4),\n\tPMU_PMEVCNTR_EL0(5),\n\tPMU_PMEVCNTR_EL0(6),\n\tPMU_PMEVCNTR_EL0(7),\n\tPMU_PMEVCNTR_EL0(8),\n\tPMU_PMEVCNTR_EL0(9),\n\tPMU_PMEVCNTR_EL0(10),\n\tPMU_PMEVCNTR_EL0(11),\n\tPMU_PMEVCNTR_EL0(12),\n\tPMU_PMEVCNTR_EL0(13),\n\tPMU_PMEVCNTR_EL0(14),\n\tPMU_PMEVCNTR_EL0(15),\n\tPMU_PMEVCNTR_EL0(16),\n\tPMU_PMEVCNTR_EL0(17),\n\tPMU_PMEVCNTR_EL0(18),\n\tPMU_PMEVCNTR_EL0(19),\n\tPMU_PMEVCNTR_EL0(20),\n\tPMU_PMEVCNTR_EL0(21),\n\tPMU_PMEVCNTR_EL0(22),\n\tPMU_PMEVCNTR_EL0(23),\n\tPMU_PMEVCNTR_EL0(24),\n\tPMU_PMEVCNTR_EL0(25),\n\tPMU_PMEVCNTR_EL0(26),\n\tPMU_PMEVCNTR_EL0(27),\n\tPMU_PMEVCNTR_EL0(28),\n\tPMU_PMEVCNTR_EL0(29),\n\tPMU_PMEVCNTR_EL0(30),\n\t/* PMEVTYPERn_EL0 */\n\tPMU_PMEVTYPER_EL0(0),\n\tPMU_PMEVTYPER_EL0(1),\n\tPMU_PMEVTYPER_EL0(2),\n\tPMU_PMEVTYPER_EL0(3),\n\tPMU_PMEVTYPER_EL0(4),\n\tPMU_PMEVTYPER_EL0(5),\n\tPMU_PMEVTYPER_EL0(6),\n\tPMU_PMEVTYPER_EL0(7),\n\tPMU_PMEVTYPER_EL0(8),\n\tPMU_PMEVTYPER_EL0(9),\n\tPMU_PMEVTYPER_EL0(10),\n\tPMU_PMEVTYPER_EL0(11),\n\tPMU_PMEVTYPER_EL0(12),\n\tPMU_PMEVTYPER_EL0(13),\n\tPMU_PMEVTYPER_EL0(14),\n\tPMU_PMEVTYPER_EL0(15),\n\tPMU_PMEVTYPER_EL0(16),\n\tPMU_PMEVTYPER_EL0(17),\n\tPMU_PMEVTYPER_EL0(18),\n\tPMU_PMEVTYPER_EL0(19),\n\tPMU_PMEVTYPER_EL0(20),\n\tPMU_PMEVTYPER_EL0(21),\n\tPMU_PMEVTYPER_EL0(22),\n\tPMU_PMEVTYPER_EL0(23),\n\tPMU_PMEVTYPER_EL0(24),\n\tPMU_PMEVTYPER_EL0(25),\n\tPMU_PMEVTYPER_EL0(26),\n\tPMU_PMEVTYPER_EL0(27),\n\tPMU_PMEVTYPER_EL0(28),\n\tPMU_PMEVTYPER_EL0(29),\n\tPMU_PMEVTYPER_EL0(30),\n\t/* PMCCFILTR_EL0\n\t * This register resets as unknown in 64bit mode while it resets as zero\n\t * in 32bit mode. Here we choose to reset it as zero for consistency.\n\t */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110), CRm(0b1111), Op2(0b111),\n\t  access_pmu_evtyper, reset_val, PMCCFILTR_EL0, 0 },\n\n\t/* DACR32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0011), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_unknown, DACR32_EL2 },\n\t/* IFSR32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0101), CRm(0b0000), Op2(0b001),\n\t  NULL, reset_unknown, IFSR32_EL2 },\n\t/* FPEXC32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0101), CRm(0b0011), Op2(0b000),\n\t  NULL, reset_val, FPEXC32_EL2, 0x70 },\n};\n\nstatic bool trap_dbgidr(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tu64 dfr = read_system_reg(SYS_ID_AA64DFR0_EL1);\n\t\tu64 pfr = read_system_reg(SYS_ID_AA64PFR0_EL1);\n\t\tu32 el3 = !!cpuid_feature_extract_unsigned_field(pfr, ID_AA64PFR0_EL3_SHIFT);\n\n\t\tp->regval = ((((dfr >> ID_AA64DFR0_WRPS_SHIFT) & 0xf) << 28) |\n\t\t\t     (((dfr >> ID_AA64DFR0_BRPS_SHIFT) & 0xf) << 24) |\n\t\t\t     (((dfr >> ID_AA64DFR0_CTX_CMPS_SHIFT) & 0xf) << 20)\n\t\t\t     | (6 << 16) | (el3 << 14) | (el3 << 12));\n\t\treturn true;\n\t}\n}\n\nstatic bool trap_debug32(struct kvm_vcpu *vcpu,\n\t\t\t struct sys_reg_params *p,\n\t\t\t const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\tvcpu_cp14(vcpu, r->reg) = p->regval;\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = vcpu_cp14(vcpu, r->reg);\n\t}\n\n\treturn true;\n}\n\n/* AArch32 debug register mappings\n *\n * AArch32 DBGBVRn is mapped to DBGBVRn_EL1[31:0]\n * AArch32 DBGBXVRn is mapped to DBGBVRn_EL1[63:32]\n *\n * All control registers and watchpoint value registers are mapped to\n * the lower 32 bits of their AArch64 equivalents. We share the trap\n * handlers with the above AArch64 code which checks what mode the\n * system is in.\n */\n\nstatic bool trap_xvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (p->is_write) {\n\t\tu64 val = *dbg_reg;\n\n\t\tval &= 0xffffffffUL;\n\t\tval |= p->regval << 32;\n\t\t*dbg_reg = val;\n\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = *dbg_reg >> 32;\n\t}\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\n#define DBG_BCR_BVR_WCR_WVR(n)\t\t\t\t\t\t\\\n\t/* DBGBVRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 4), trap_bvr, NULL, n }, \t\\\n\t/* DBGBCRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 5), trap_bcr, NULL, n },\t\\\n\t/* DBGWVRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 6), trap_wvr, NULL, n },\t\\\n\t/* DBGWCRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 7), trap_wcr, NULL, n }\n\n#define DBGBXVR(n)\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 1), CRm((n)), Op2( 1), trap_xvr, NULL, n }\n\n/*\n * Trapped cp14 registers. We generally ignore most of the external\n * debug, on the principle that they don't really make sense to a\n * guest. Revisit this one day, would this principle change.\n */\nstatic const struct sys_reg_desc cp14_regs[] = {\n\t/* DBGIDR */\n\t{ Op1( 0), CRn( 0), CRm( 0), Op2( 0), trap_dbgidr },\n\t/* DBGDTRRXext */\n\t{ Op1( 0), CRn( 0), CRm( 0), Op2( 2), trap_raz_wi },\n\n\tDBG_BCR_BVR_WCR_WVR(0),\n\t/* DBGDSCRint */\n\t{ Op1( 0), CRn( 0), CRm( 1), Op2( 0), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(1),\n\t/* DBGDCCINT */\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 0), trap_debug32 },\n\t/* DBGDSCRext */\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 2), trap_debug32 },\n\tDBG_BCR_BVR_WCR_WVR(2),\n\t/* DBGDTR[RT]Xint */\n\t{ Op1( 0), CRn( 0), CRm( 3), Op2( 0), trap_raz_wi },\n\t/* DBGDTR[RT]Xext */\n\t{ Op1( 0), CRn( 0), CRm( 3), Op2( 2), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(3),\n\tDBG_BCR_BVR_WCR_WVR(4),\n\tDBG_BCR_BVR_WCR_WVR(5),\n\t/* DBGWFAR */\n\t{ Op1( 0), CRn( 0), CRm( 6), Op2( 0), trap_raz_wi },\n\t/* DBGOSECCR */\n\t{ Op1( 0), CRn( 0), CRm( 6), Op2( 2), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(6),\n\t/* DBGVCR */\n\t{ Op1( 0), CRn( 0), CRm( 7), Op2( 0), trap_debug32 },\n\tDBG_BCR_BVR_WCR_WVR(7),\n\tDBG_BCR_BVR_WCR_WVR(8),\n\tDBG_BCR_BVR_WCR_WVR(9),\n\tDBG_BCR_BVR_WCR_WVR(10),\n\tDBG_BCR_BVR_WCR_WVR(11),\n\tDBG_BCR_BVR_WCR_WVR(12),\n\tDBG_BCR_BVR_WCR_WVR(13),\n\tDBG_BCR_BVR_WCR_WVR(14),\n\tDBG_BCR_BVR_WCR_WVR(15),\n\n\t/* DBGDRAR (32bit) */\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 0), trap_raz_wi },\n\n\tDBGBXVR(0),\n\t/* DBGOSLAR */\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 4), trap_raz_wi },\n\tDBGBXVR(1),\n\t/* DBGOSLSR */\n\t{ Op1( 0), CRn( 1), CRm( 1), Op2( 4), trap_oslsr_el1 },\n\tDBGBXVR(2),\n\tDBGBXVR(3),\n\t/* DBGOSDLR */\n\t{ Op1( 0), CRn( 1), CRm( 3), Op2( 4), trap_raz_wi },\n\tDBGBXVR(4),\n\t/* DBGPRCR */\n\t{ Op1( 0), CRn( 1), CRm( 4), Op2( 4), trap_raz_wi },\n\tDBGBXVR(5),\n\tDBGBXVR(6),\n\tDBGBXVR(7),\n\tDBGBXVR(8),\n\tDBGBXVR(9),\n\tDBGBXVR(10),\n\tDBGBXVR(11),\n\tDBGBXVR(12),\n\tDBGBXVR(13),\n\tDBGBXVR(14),\n\tDBGBXVR(15),\n\n\t/* DBGDSAR (32bit) */\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 0), trap_raz_wi },\n\n\t/* DBGDEVID2 */\n\t{ Op1( 0), CRn( 7), CRm( 0), Op2( 7), trap_raz_wi },\n\t/* DBGDEVID1 */\n\t{ Op1( 0), CRn( 7), CRm( 1), Op2( 7), trap_raz_wi },\n\t/* DBGDEVID */\n\t{ Op1( 0), CRn( 7), CRm( 2), Op2( 7), trap_raz_wi },\n\t/* DBGCLAIMSET */\n\t{ Op1( 0), CRn( 7), CRm( 8), Op2( 6), trap_raz_wi },\n\t/* DBGCLAIMCLR */\n\t{ Op1( 0), CRn( 7), CRm( 9), Op2( 6), trap_raz_wi },\n\t/* DBGAUTHSTATUS */\n\t{ Op1( 0), CRn( 7), CRm(14), Op2( 6), trap_dbgauthstatus_el1 },\n};\n\n/* Trapped cp14 64bit registers */\nstatic const struct sys_reg_desc cp14_64_regs[] = {\n\t/* DBGDRAR (64bit) */\n\t{ Op1( 0), CRm( 1), .access = trap_raz_wi },\n\n\t/* DBGDSAR (64bit) */\n\t{ Op1( 0), CRm( 2), .access = trap_raz_wi },\n};\n\n/* Macro to expand the PMEVCNTRn register */\n#define PMU_PMEVCNTR(n)\t\t\t\t\t\t\t\\\n\t/* PMEVCNTRn */\t\t\t\t\t\t\t\\\n\t{ Op1(0), CRn(0b1110),\t\t\t\t\t\t\\\n\t  CRm((0b1000 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evcntr }\n\n/* Macro to expand the PMEVTYPERn register */\n#define PMU_PMEVTYPER(n)\t\t\t\t\t\t\\\n\t/* PMEVTYPERn */\t\t\t\t\t\t\\\n\t{ Op1(0), CRn(0b1110),\t\t\t\t\t\t\\\n\t  CRm((0b1100 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evtyper }\n\n/*\n * Trapped cp15 registers. TTBR0/TTBR1 get a double encoding,\n * depending on the way they are accessed (as a 32bit or a 64bit\n * register).\n */\nstatic const struct sys_reg_desc cp15_regs[] = {\n\t{ Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },\n\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 0), access_vm_reg, NULL, c1_SCTLR },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 0), access_vm_reg, NULL, c2_TTBR0 },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 1), access_vm_reg, NULL, c2_TTBR1 },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 2), access_vm_reg, NULL, c2_TTBCR },\n\t{ Op1( 0), CRn( 3), CRm( 0), Op2( 0), access_vm_reg, NULL, c3_DACR },\n\t{ Op1( 0), CRn( 5), CRm( 0), Op2( 0), access_vm_reg, NULL, c5_DFSR },\n\t{ Op1( 0), CRn( 5), CRm( 0), Op2( 1), access_vm_reg, NULL, c5_IFSR },\n\t{ Op1( 0), CRn( 5), CRm( 1), Op2( 0), access_vm_reg, NULL, c5_ADFSR },\n\t{ Op1( 0), CRn( 5), CRm( 1), Op2( 1), access_vm_reg, NULL, c5_AIFSR },\n\t{ Op1( 0), CRn( 6), CRm( 0), Op2( 0), access_vm_reg, NULL, c6_DFAR },\n\t{ Op1( 0), CRn( 6), CRm( 0), Op2( 2), access_vm_reg, NULL, c6_IFAR },\n\n\t/*\n\t * DC{C,I,CI}SW operations:\n\t */\n\t{ Op1( 0), CRn( 7), CRm( 6), Op2( 2), access_dcsw },\n\t{ Op1( 0), CRn( 7), CRm(10), Op2( 2), access_dcsw },\n\t{ Op1( 0), CRn( 7), CRm(14), Op2( 2), access_dcsw },\n\n\t/* PMU */\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 0), access_pmcr },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 1), access_pmcnten },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 2), access_pmcnten },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 3), access_pmovs },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 4), access_pmswinc },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 5), access_pmselr },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 6), access_pmceid },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 7), access_pmceid },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 0), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 1), access_pmu_evtyper },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 2), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 0), access_pmuserenr },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 1), access_pminten },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 2), access_pminten },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 3), access_pmovs },\n\n\t{ Op1( 0), CRn(10), CRm( 2), Op2( 0), access_vm_reg, NULL, c10_PRRR },\n\t{ Op1( 0), CRn(10), CRm( 2), Op2( 1), access_vm_reg, NULL, c10_NMRR },\n\t{ Op1( 0), CRn(10), CRm( 3), Op2( 0), access_vm_reg, NULL, c10_AMAIR0 },\n\t{ Op1( 0), CRn(10), CRm( 3), Op2( 1), access_vm_reg, NULL, c10_AMAIR1 },\n\n\t/* ICC_SRE */\n\t{ Op1( 0), CRn(12), CRm(12), Op2( 5), access_gic_sre },\n\n\t{ Op1( 0), CRn(13), CRm( 0), Op2( 1), access_vm_reg, NULL, c13_CID },\n\n\t/* PMEVCNTRn */\n\tPMU_PMEVCNTR(0),\n\tPMU_PMEVCNTR(1),\n\tPMU_PMEVCNTR(2),\n\tPMU_PMEVCNTR(3),\n\tPMU_PMEVCNTR(4),\n\tPMU_PMEVCNTR(5),\n\tPMU_PMEVCNTR(6),\n\tPMU_PMEVCNTR(7),\n\tPMU_PMEVCNTR(8),\n\tPMU_PMEVCNTR(9),\n\tPMU_PMEVCNTR(10),\n\tPMU_PMEVCNTR(11),\n\tPMU_PMEVCNTR(12),\n\tPMU_PMEVCNTR(13),\n\tPMU_PMEVCNTR(14),\n\tPMU_PMEVCNTR(15),\n\tPMU_PMEVCNTR(16),\n\tPMU_PMEVCNTR(17),\n\tPMU_PMEVCNTR(18),\n\tPMU_PMEVCNTR(19),\n\tPMU_PMEVCNTR(20),\n\tPMU_PMEVCNTR(21),\n\tPMU_PMEVCNTR(22),\n\tPMU_PMEVCNTR(23),\n\tPMU_PMEVCNTR(24),\n\tPMU_PMEVCNTR(25),\n\tPMU_PMEVCNTR(26),\n\tPMU_PMEVCNTR(27),\n\tPMU_PMEVCNTR(28),\n\tPMU_PMEVCNTR(29),\n\tPMU_PMEVCNTR(30),\n\t/* PMEVTYPERn */\n\tPMU_PMEVTYPER(0),\n\tPMU_PMEVTYPER(1),\n\tPMU_PMEVTYPER(2),\n\tPMU_PMEVTYPER(3),\n\tPMU_PMEVTYPER(4),\n\tPMU_PMEVTYPER(5),\n\tPMU_PMEVTYPER(6),\n\tPMU_PMEVTYPER(7),\n\tPMU_PMEVTYPER(8),\n\tPMU_PMEVTYPER(9),\n\tPMU_PMEVTYPER(10),\n\tPMU_PMEVTYPER(11),\n\tPMU_PMEVTYPER(12),\n\tPMU_PMEVTYPER(13),\n\tPMU_PMEVTYPER(14),\n\tPMU_PMEVTYPER(15),\n\tPMU_PMEVTYPER(16),\n\tPMU_PMEVTYPER(17),\n\tPMU_PMEVTYPER(18),\n\tPMU_PMEVTYPER(19),\n\tPMU_PMEVTYPER(20),\n\tPMU_PMEVTYPER(21),\n\tPMU_PMEVTYPER(22),\n\tPMU_PMEVTYPER(23),\n\tPMU_PMEVTYPER(24),\n\tPMU_PMEVTYPER(25),\n\tPMU_PMEVTYPER(26),\n\tPMU_PMEVTYPER(27),\n\tPMU_PMEVTYPER(28),\n\tPMU_PMEVTYPER(29),\n\tPMU_PMEVTYPER(30),\n\t/* PMCCFILTR */\n\t{ Op1(0), CRn(14), CRm(15), Op2(7), access_pmu_evtyper },\n};\n\nstatic const struct sys_reg_desc cp15_64_regs[] = {\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 0), access_vm_reg, NULL, c2_TTBR0 },\n\t{ Op1( 0), CRn( 0), CRm( 9), Op2( 0), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },\n\t{ Op1( 1), CRn( 0), CRm( 2), Op2( 0), access_vm_reg, NULL, c2_TTBR1 },\n};\n\n/* Target specific emulation tables */\nstatic struct kvm_sys_reg_target_table *target_tables[KVM_ARM_NUM_TARGETS];\n\nvoid kvm_register_target_sys_reg_table(unsigned int target,\n\t\t\t\t       struct kvm_sys_reg_target_table *table)\n{\n\ttarget_tables[target] = table;\n}\n\n/* Get specific register table for this target. */\nstatic const struct sys_reg_desc *get_target_table(unsigned target,\n\t\t\t\t\t\t   bool mode_is_64,\n\t\t\t\t\t\t   size_t *num)\n{\n\tstruct kvm_sys_reg_target_table *table;\n\n\ttable = target_tables[target];\n\tif (mode_is_64) {\n\t\t*num = table->table64.num;\n\t\treturn table->table64.table;\n\t} else {\n\t\t*num = table->table32.num;\n\t\treturn table->table32.table;\n\t}\n}\n\n#define reg_to_match_value(x)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tunsigned long val;\t\t\t\t\t\\\n\t\tval  = (x)->Op0 << 14;\t\t\t\t\t\\\n\t\tval |= (x)->Op1 << 11;\t\t\t\t\t\\\n\t\tval |= (x)->CRn << 7;\t\t\t\t\t\\\n\t\tval |= (x)->CRm << 3;\t\t\t\t\t\\\n\t\tval |= (x)->Op2;\t\t\t\t\t\\\n\t\tval;\t\t\t\t\t\t\t\\\n\t })\n\nstatic int match_sys_reg(const void *key, const void *elt)\n{\n\tconst unsigned long pval = (unsigned long)key;\n\tconst struct sys_reg_desc *r = elt;\n\n\treturn pval - reg_to_match_value(r);\n}\n\nstatic const struct sys_reg_desc *find_reg(const struct sys_reg_params *params,\n\t\t\t\t\t const struct sys_reg_desc table[],\n\t\t\t\t\t unsigned int num)\n{\n\tunsigned long pval = reg_to_match_value(params);\n\n\treturn bsearch((void *)pval, table, num, sizeof(table[0]), match_sys_reg);\n}\n\nint kvm_handle_cp14_load_store(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tkvm_inject_undefined(vcpu);\n\treturn 1;\n}\n\n/*\n * emulate_cp --  tries to match a sys_reg access in a handling table, and\n *                call the corresponding trap handler.\n *\n * @params: pointer to the descriptor of the access\n * @table: array of trap descriptors\n * @num: size of the trap descriptor array\n *\n * Return 0 if the access has been handled, and -1 if not.\n */\nstatic int emulate_cp(struct kvm_vcpu *vcpu,\n\t\t      struct sys_reg_params *params,\n\t\t      const struct sys_reg_desc *table,\n\t\t      size_t num)\n{\n\tconst struct sys_reg_desc *r;\n\n\tif (!table)\n\t\treturn -1;\t/* Not handled */\n\n\tr = find_reg(params, table, num);\n\n\tif (r) {\n\t\t/*\n\t\t * Not having an accessor means that we have\n\t\t * configured a trap that we don't know how to\n\t\t * handle. This certainly qualifies as a gross bug\n\t\t * that should be fixed right away.\n\t\t */\n\t\tBUG_ON(!r->access);\n\n\t\tif (likely(r->access(vcpu, params, r))) {\n\t\t\t/* Skip instruction, since it was emulated */\n\t\t\tkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\n\t\t\t/* Handled */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* Not handled */\n\treturn -1;\n}\n\nstatic void unhandled_cp_access(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct sys_reg_params *params)\n{\n\tu8 hsr_ec = kvm_vcpu_trap_get_class(vcpu);\n\tint cp = -1;\n\n\tswitch(hsr_ec) {\n\tcase ESR_ELx_EC_CP15_32:\n\tcase ESR_ELx_EC_CP15_64:\n\t\tcp = 15;\n\t\tbreak;\n\tcase ESR_ELx_EC_CP14_MR:\n\tcase ESR_ELx_EC_CP14_64:\n\t\tcp = 14;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t}\n\n\tkvm_err(\"Unsupported guest CP%d access at: %08lx\\n\",\n\t\tcp, *vcpu_pc(vcpu));\n\tprint_sys_reg_instr(params);\n\tkvm_inject_undefined(vcpu);\n}\n\n/**\n * kvm_handle_cp_64 -- handles a mrrc/mcrr trap on a guest CP14/CP15 access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nstatic int kvm_handle_cp_64(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt = (hsr >> 5) & 0xf;\n\tint Rt2 = (hsr >> 10) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = false;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.is_write = ((hsr & 1) == 0);\n\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 16) & 0xf;\n\tparams.Op2 = 0;\n\tparams.CRn = 0;\n\n\t/*\n\t * Make a 64-bit value out of Rt and Rt2. As we use the same trap\n\t * backends between AArch32 and AArch64, we get away with it.\n\t */\n\tif (params.is_write) {\n\t\tparams.regval = vcpu_get_reg(vcpu, Rt) & 0xffffffff;\n\t\tparams.regval |= vcpu_get_reg(vcpu, Rt2) << 32;\n\t}\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific))\n\t\tgoto out;\n\tif (!emulate_cp(vcpu, &params, global, nr_global))\n\t\tgoto out;\n\n\tunhandled_cp_access(vcpu, &params);\n\nout:\n\t/* Split up the value between registers for the read side */\n\tif (!params.is_write) {\n\t\tvcpu_set_reg(vcpu, Rt, lower_32_bits(params.regval));\n\t\tvcpu_set_reg(vcpu, Rt2, upper_32_bits(params.regval));\n\t}\n\n\treturn 1;\n}\n\n/**\n * kvm_handle_cp_32 -- handles a mrc/mcr trap on a guest CP14/CP15 access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nstatic int kvm_handle_cp_32(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt  = (hsr >> 5) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = true;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.regval = vcpu_get_reg(vcpu, Rt);\n\tparams.is_write = ((hsr & 1) == 0);\n\tparams.CRn = (hsr >> 10) & 0xf;\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 14) & 0x7;\n\tparams.Op2 = (hsr >> 17) & 0x7;\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific) ||\n\t    !emulate_cp(vcpu, &params, global, nr_global)) {\n\t\tif (!params.is_write)\n\t\t\tvcpu_set_reg(vcpu, Rt, params.regval);\n\t\treturn 1;\n\t}\n\n\tunhandled_cp_access(vcpu, &params);\n\treturn 1;\n}\n\nint kvm_handle_cp15_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tconst struct sys_reg_desc *target_specific;\n\tsize_t num;\n\n\ttarget_specific = get_target_table(vcpu->arch.target, false, &num);\n\treturn kvm_handle_cp_64(vcpu,\n\t\t\t\tcp15_64_regs, ARRAY_SIZE(cp15_64_regs),\n\t\t\t\ttarget_specific, num);\n}\n\nint kvm_handle_cp15_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tconst struct sys_reg_desc *target_specific;\n\tsize_t num;\n\n\ttarget_specific = get_target_table(vcpu->arch.target, false, &num);\n\treturn kvm_handle_cp_32(vcpu,\n\t\t\t\tcp15_regs, ARRAY_SIZE(cp15_regs),\n\t\t\t\ttarget_specific, num);\n}\n\nint kvm_handle_cp14_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\treturn kvm_handle_cp_64(vcpu,\n\t\t\t\tcp14_64_regs, ARRAY_SIZE(cp14_64_regs),\n\t\t\t\tNULL, 0);\n}\n\nint kvm_handle_cp14_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\treturn kvm_handle_cp_32(vcpu,\n\t\t\t\tcp14_regs, ARRAY_SIZE(cp14_regs),\n\t\t\t\tNULL, 0);\n}\n\nstatic int emulate_sys_reg(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *params)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table, *r;\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\n\t/* Search target-specific then generic table. */\n\tr = find_reg(params, table, num);\n\tif (!r)\n\t\tr = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\tif (likely(r)) {\n\t\t/*\n\t\t * Not having an accessor means that we have\n\t\t * configured a trap that we don't know how to\n\t\t * handle. This certainly qualifies as a gross bug\n\t\t * that should be fixed right away.\n\t\t */\n\t\tBUG_ON(!r->access);\n\n\t\tif (likely(r->access(vcpu, params, r))) {\n\t\t\t/* Skip instruction, since it was emulated */\n\t\t\tkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\n\t\t\treturn 1;\n\t\t}\n\t\t/* If access function fails, it should complain. */\n\t} else {\n\t\tkvm_err(\"Unsupported guest sys_reg access at: %lx\\n\",\n\t\t\t*vcpu_pc(vcpu));\n\t\tprint_sys_reg_instr(params);\n\t}\n\tkvm_inject_undefined(vcpu);\n\treturn 1;\n}\n\nstatic void reset_sys_reg_descs(struct kvm_vcpu *vcpu,\n\t\t\t      const struct sys_reg_desc *table, size_t num)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < num; i++)\n\t\tif (table[i].reset)\n\t\t\ttable[i].reset(vcpu, &table[i]);\n}\n\n/**\n * kvm_handle_sys_reg -- handles a mrs/msr trap on a guest sys_reg access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nint kvm_handle_sys_reg(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tstruct sys_reg_params params;\n\tunsigned long esr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt = (esr >> 5) & 0x1f;\n\tint ret;\n\n\ttrace_kvm_handle_sys_reg(esr);\n\n\tparams.is_aarch32 = false;\n\tparams.is_32bit = false;\n\tparams.Op0 = (esr >> 20) & 3;\n\tparams.Op1 = (esr >> 14) & 0x7;\n\tparams.CRn = (esr >> 10) & 0xf;\n\tparams.CRm = (esr >> 1) & 0xf;\n\tparams.Op2 = (esr >> 17) & 0x7;\n\tparams.regval = vcpu_get_reg(vcpu, Rt);\n\tparams.is_write = !(esr & 1);\n\n\tret = emulate_sys_reg(vcpu, &params);\n\n\tif (!params.is_write)\n\t\tvcpu_set_reg(vcpu, Rt, params.regval);\n\treturn ret;\n}\n\n/******************************************************************************\n * Userspace API\n *****************************************************************************/\n\nstatic bool index_to_params(u64 id, struct sys_reg_params *params)\n{\n\tswitch (id & KVM_REG_SIZE_MASK) {\n\tcase KVM_REG_SIZE_U64:\n\t\t/* Any unused index bits means it's not valid. */\n\t\tif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\n\t\t\t      | KVM_REG_ARM_COPROC_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP0_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP1_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRN_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRM_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP2_MASK))\n\t\t\treturn false;\n\t\tparams->Op0 = ((id & KVM_REG_ARM64_SYSREG_OP0_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP0_SHIFT);\n\t\tparams->Op1 = ((id & KVM_REG_ARM64_SYSREG_OP1_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP1_SHIFT);\n\t\tparams->CRn = ((id & KVM_REG_ARM64_SYSREG_CRN_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRN_SHIFT);\n\t\tparams->CRm = ((id & KVM_REG_ARM64_SYSREG_CRM_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRM_SHIFT);\n\t\tparams->Op2 = ((id & KVM_REG_ARM64_SYSREG_OP2_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP2_SHIFT);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Decode an index value, and find the sys_reg_desc entry. */\nstatic const struct sys_reg_desc *index_to_sys_reg_desc(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t    u64 id)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table, *r;\n\tstruct sys_reg_params params;\n\n\t/* We only do sys_reg for now. */\n\tif ((id & KVM_REG_ARM_COPROC_MASK) != KVM_REG_ARM64_SYSREG)\n\t\treturn NULL;\n\n\tif (!index_to_params(id, &params))\n\t\treturn NULL;\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\tr = find_reg(&params, table, num);\n\tif (!r)\n\t\tr = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\t/* Not saved in the sys_reg array? */\n\tif (r && !r->reg)\n\t\tr = NULL;\n\n\treturn r;\n}\n\n/*\n * These are the invariant sys_reg registers: we let the guest see the\n * host versions of these, so they're part of the guest state.\n *\n * A future CPU may provide a mechanism to present different values to\n * the guest, or a future kvm may trap them.\n */\n\n#define FUNCTION_INVARIANT(reg)\t\t\t\t\t\t\\\n\tstatic void get_##reg(struct kvm_vcpu *v,\t\t\t\\\n\t\t\t      const struct sys_reg_desc *r)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t((struct sys_reg_desc *)r)->val = read_sysreg(reg);\t\\\n\t}\n\nFUNCTION_INVARIANT(midr_el1)\nFUNCTION_INVARIANT(ctr_el0)\nFUNCTION_INVARIANT(revidr_el1)\nFUNCTION_INVARIANT(id_pfr0_el1)\nFUNCTION_INVARIANT(id_pfr1_el1)\nFUNCTION_INVARIANT(id_dfr0_el1)\nFUNCTION_INVARIANT(id_afr0_el1)\nFUNCTION_INVARIANT(id_mmfr0_el1)\nFUNCTION_INVARIANT(id_mmfr1_el1)\nFUNCTION_INVARIANT(id_mmfr2_el1)\nFUNCTION_INVARIANT(id_mmfr3_el1)\nFUNCTION_INVARIANT(id_isar0_el1)\nFUNCTION_INVARIANT(id_isar1_el1)\nFUNCTION_INVARIANT(id_isar2_el1)\nFUNCTION_INVARIANT(id_isar3_el1)\nFUNCTION_INVARIANT(id_isar4_el1)\nFUNCTION_INVARIANT(id_isar5_el1)\nFUNCTION_INVARIANT(clidr_el1)\nFUNCTION_INVARIANT(aidr_el1)\n\n/* ->val is filled in by kvm_sys_reg_table_init() */\nstatic struct sys_reg_desc invariant_sys_regs[] = {\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b000),\n\t  NULL, get_midr_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b110),\n\t  NULL, get_revidr_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b000),\n\t  NULL, get_id_pfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b001),\n\t  NULL, get_id_pfr1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b010),\n\t  NULL, get_id_dfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b011),\n\t  NULL, get_id_afr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b100),\n\t  NULL, get_id_mmfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b101),\n\t  NULL, get_id_mmfr1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b110),\n\t  NULL, get_id_mmfr2_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b111),\n\t  NULL, get_id_mmfr3_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b000),\n\t  NULL, get_id_isar0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b001),\n\t  NULL, get_id_isar1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b010),\n\t  NULL, get_id_isar2_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b011),\n\t  NULL, get_id_isar3_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b100),\n\t  NULL, get_id_isar4_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b101),\n\t  NULL, get_id_isar5_el1 },\n\t{ Op0(0b11), Op1(0b001), CRn(0b0000), CRm(0b0000), Op2(0b001),\n\t  NULL, get_clidr_el1 },\n\t{ Op0(0b11), Op1(0b001), CRn(0b0000), CRm(0b0000), Op2(0b111),\n\t  NULL, get_aidr_el1 },\n\t{ Op0(0b11), Op1(0b011), CRn(0b0000), CRm(0b0000), Op2(0b001),\n\t  NULL, get_ctr_el0 },\n};\n\nstatic int reg_from_user(u64 *val, const void __user *uaddr, u64 id)\n{\n\tif (copy_from_user(val, uaddr, KVM_REG_SIZE(id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int reg_to_user(void __user *uaddr, const u64 *val, u64 id)\n{\n\tif (copy_to_user(uaddr, val, KVM_REG_SIZE(id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_invariant_sys_reg(u64 id, void __user *uaddr)\n{\n\tstruct sys_reg_params params;\n\tconst struct sys_reg_desc *r;\n\n\tif (!index_to_params(id, &params))\n\t\treturn -ENOENT;\n\n\tr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\n\tif (!r)\n\t\treturn -ENOENT;\n\n\treturn reg_to_user(uaddr, &r->val, id);\n}\n\nstatic int set_invariant_sys_reg(u64 id, void __user *uaddr)\n{\n\tstruct sys_reg_params params;\n\tconst struct sys_reg_desc *r;\n\tint err;\n\tu64 val = 0; /* Make sure high bits are 0 for 32-bit regs */\n\n\tif (!index_to_params(id, &params))\n\t\treturn -ENOENT;\n\tr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\n\tif (!r)\n\t\treturn -ENOENT;\n\n\terr = reg_from_user(&val, uaddr, id);\n\tif (err)\n\t\treturn err;\n\n\t/* This is what we mean by invariant: you can't change it. */\n\tif (r->val != val)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic bool is_valid_cache(u32 val)\n{\n\tu32 level, ctype;\n\n\tif (val >= CSSELR_MAX)\n\t\treturn false;\n\n\t/* Bottom bit is Instruction or Data bit.  Next 3 bits are level. */\n\tlevel = (val >> 1);\n\tctype = (cache_levels >> (level * 3)) & 7;\n\n\tswitch (ctype) {\n\tcase 0: /* No cache */\n\t\treturn false;\n\tcase 1: /* Instruction cache only */\n\t\treturn (val & 1);\n\tcase 2: /* Data cache only */\n\tcase 4: /* Unified cache */\n\t\treturn !(val & 1);\n\tcase 3: /* Separate instruction and data caches */\n\t\treturn true;\n\tdefault: /* Reserved: we can't know instruction or data. */\n\t\treturn false;\n\t}\n}\n\nstatic int demux_c15_get(u64 id, void __user *uaddr)\n{\n\tu32 val;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\treturn put_user(get_ccsidr(val), uval);\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n\nstatic int demux_c15_set(u64 id, void __user *uaddr)\n{\n\tu32 val, newval;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\tif (get_user(newval, uval))\n\t\t\treturn -EFAULT;\n\n\t\t/* This is also invariant: you can't change it. */\n\t\tif (newval != get_ccsidr(val))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n\nint kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_get(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn get_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->get_user)\n\t\treturn (r->get_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_to_user(uaddr, &vcpu_sys_reg(vcpu, r->reg), reg->id);\n}\n\nint kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_set(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn set_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->set_user)\n\t\treturn (r->set_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_from_user(&vcpu_sys_reg(vcpu, r->reg), uaddr, reg->id);\n}\n\nstatic unsigned int num_demux_regs(void)\n{\n\tunsigned int i, count = 0;\n\n\tfor (i = 0; i < CSSELR_MAX; i++)\n\t\tif (is_valid_cache(i))\n\t\t\tcount++;\n\n\treturn count;\n}\n\nstatic int write_demux_regids(u64 __user *uindices)\n{\n\tu64 val = KVM_REG_ARM64 | KVM_REG_SIZE_U32 | KVM_REG_ARM_DEMUX;\n\tunsigned int i;\n\n\tval |= KVM_REG_ARM_DEMUX_ID_CCSIDR;\n\tfor (i = 0; i < CSSELR_MAX; i++) {\n\t\tif (!is_valid_cache(i))\n\t\t\tcontinue;\n\t\tif (put_user(val | i, uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\treturn 0;\n}\n\nstatic u64 sys_reg_to_index(const struct sys_reg_desc *reg)\n{\n\treturn (KVM_REG_ARM64 | KVM_REG_SIZE_U64 |\n\t\tKVM_REG_ARM64_SYSREG |\n\t\t(reg->Op0 << KVM_REG_ARM64_SYSREG_OP0_SHIFT) |\n\t\t(reg->Op1 << KVM_REG_ARM64_SYSREG_OP1_SHIFT) |\n\t\t(reg->CRn << KVM_REG_ARM64_SYSREG_CRN_SHIFT) |\n\t\t(reg->CRm << KVM_REG_ARM64_SYSREG_CRM_SHIFT) |\n\t\t(reg->Op2 << KVM_REG_ARM64_SYSREG_OP2_SHIFT));\n}\n\nstatic bool copy_reg_to_user(const struct sys_reg_desc *reg, u64 __user **uind)\n{\n\tif (!*uind)\n\t\treturn true;\n\n\tif (put_user(sys_reg_to_index(reg), *uind))\n\t\treturn false;\n\n\t(*uind)++;\n\treturn true;\n}\n\n/* Assumed ordered tables, see kvm_sys_reg_table_init. */\nstatic int walk_sys_regs(struct kvm_vcpu *vcpu, u64 __user *uind)\n{\n\tconst struct sys_reg_desc *i1, *i2, *end1, *end2;\n\tunsigned int total = 0;\n\tsize_t num;\n\n\t/* We check for duplicates here, to allow arch-specific overrides. */\n\ti1 = get_target_table(vcpu->arch.target, true, &num);\n\tend1 = i1 + num;\n\ti2 = sys_reg_descs;\n\tend2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);\n\n\tBUG_ON(i1 == end1 || i2 == end2);\n\n\t/* Walk carefully, as both tables may refer to the same register. */\n\twhile (i1 || i2) {\n\t\tint cmp = cmp_sys_reg(i1, i2);\n\t\t/* target-specific overrides generic entry. */\n\t\tif (cmp <= 0) {\n\t\t\t/* Ignore registers we trap but don't save. */\n\t\t\tif (i1->reg) {\n\t\t\t\tif (!copy_reg_to_user(i1, &uind))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t} else {\n\t\t\t/* Ignore registers we trap but don't save. */\n\t\t\tif (i2->reg) {\n\t\t\t\tif (!copy_reg_to_user(i2, &uind))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t}\n\n\t\tif (cmp <= 0 && ++i1 == end1)\n\t\t\ti1 = NULL;\n\t\tif (cmp >= 0 && ++i2 == end2)\n\t\t\ti2 = NULL;\n\t}\n\treturn total;\n}\n\nunsigned long kvm_arm_num_sys_reg_descs(struct kvm_vcpu *vcpu)\n{\n\treturn ARRAY_SIZE(invariant_sys_regs)\n\t\t+ num_demux_regs()\n\t\t+ walk_sys_regs(vcpu, (u64 __user *)NULL);\n}\n\nint kvm_arm_copy_sys_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tunsigned int i;\n\tint err;\n\n\t/* Then give them all the invariant registers' indices. */\n\tfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++) {\n\t\tif (put_user(sys_reg_to_index(&invariant_sys_regs[i]), uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\n\terr = walk_sys_regs(vcpu, uindices);\n\tif (err < 0)\n\t\treturn err;\n\tuindices += err;\n\n\treturn write_demux_regids(uindices);\n}\n\nstatic int check_sysreg_table(const struct sys_reg_desc *table, unsigned int n)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < n; i++) {\n\t\tif (cmp_sys_reg(&table[i-1], &table[i]) >= 0) {\n\t\t\tkvm_err(\"sys_reg table %p out of order (%d)\\n\", table, i - 1);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nvoid kvm_sys_reg_table_init(void)\n{\n\tunsigned int i;\n\tstruct sys_reg_desc clidr;\n\n\t/* Make sure tables are unique and in order. */\n\tBUG_ON(check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs)));\n\tBUG_ON(check_sysreg_table(cp14_regs, ARRAY_SIZE(cp14_regs)));\n\tBUG_ON(check_sysreg_table(cp14_64_regs, ARRAY_SIZE(cp14_64_regs)));\n\tBUG_ON(check_sysreg_table(cp15_regs, ARRAY_SIZE(cp15_regs)));\n\tBUG_ON(check_sysreg_table(cp15_64_regs, ARRAY_SIZE(cp15_64_regs)));\n\tBUG_ON(check_sysreg_table(invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs)));\n\n\t/* We abuse the reset function to overwrite the table itself. */\n\tfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++)\n\t\tinvariant_sys_regs[i].reset(NULL, &invariant_sys_regs[i]);\n\n\t/*\n\t * CLIDR format is awkward, so clean it up.  See ARM B4.1.20:\n\t *\n\t *   If software reads the Cache Type fields from Ctype1\n\t *   upwards, once it has seen a value of 0b000, no caches\n\t *   exist at further-out levels of the hierarchy. So, for\n\t *   example, if Ctype3 is the first Cache Type field with a\n\t *   value of 0b000, the values of Ctype4 to Ctype7 must be\n\t *   ignored.\n\t */\n\tget_clidr_el1(NULL, &clidr); /* Ugly... */\n\tcache_levels = clidr.val;\n\tfor (i = 0; i < 7; i++)\n\t\tif (((cache_levels >> (i*3)) & 7) == 0)\n\t\t\tbreak;\n\t/* Clear all higher bits. */\n\tcache_levels &= (1 << (i*3))-1;\n}\n\n/**\n * kvm_reset_sys_regs - sets system registers to reset value\n * @vcpu: The VCPU pointer\n *\n * This function finds the right table above and sets the registers on the\n * virtual CPU struct to their architecturally defined reset values.\n */\nvoid kvm_reset_sys_regs(struct kvm_vcpu *vcpu)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table;\n\n\t/* Catch someone adding a register without putting in reset entry. */\n\tmemset(&vcpu->arch.ctxt.sys_regs, 0x42, sizeof(vcpu->arch.ctxt.sys_regs));\n\n\t/* Generic chip reset first (so target could override). */\n\treset_sys_reg_descs(vcpu, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\treset_sys_reg_descs(vcpu, table, num);\n\n\tfor (num = 1; num < NR_SYS_REGS; num++)\n\t\tif (vcpu_sys_reg(vcpu, num) == 0x4242424242424242)\n\t\t\tpanic(\"Didn't reset vcpu_sys_reg(%zi)\", num);\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2012,2013 - ARM Ltd\n * Author: Marc Zyngier <marc.zyngier@arm.com>\n *\n * Derived from arch/arm/kvm/coproc.c:\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Authors: Rusty Russell <rusty@rustcorp.com.au>\n *          Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License, version 2, as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/bsearch.h>\n#include <linux/kvm_host.h>\n#include <linux/mm.h>\n#include <linux/uaccess.h>\n\n#include <asm/cacheflush.h>\n#include <asm/cputype.h>\n#include <asm/debug-monitors.h>\n#include <asm/esr.h>\n#include <asm/kvm_arm.h>\n#include <asm/kvm_asm.h>\n#include <asm/kvm_coproc.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_host.h>\n#include <asm/kvm_mmu.h>\n#include <asm/perf_event.h>\n#include <asm/sysreg.h>\n\n#include <trace/events/kvm.h>\n\n#include \"sys_regs.h\"\n\n#include \"trace.h\"\n\n/*\n * All of this file is extremly similar to the ARM coproc.c, but the\n * types are different. My gut feeling is that it should be pretty\n * easy to merge, but that would be an ABI breakage -- again. VFP\n * would also need to be abstracted.\n *\n * For AArch32, we only take care of what is being trapped. Anything\n * that has to do with init and userspace access has to go via the\n * 64bit interface.\n */\n\n/* 3 bits per cache level, as per CLIDR, but non-existent caches always 0 */\nstatic u32 cache_levels;\n\n/* CSSELR values; used to index KVM_REG_ARM_DEMUX_ID_CCSIDR */\n#define CSSELR_MAX 12\n\n/* Which cache CCSIDR represents depends on CSSELR value. */\nstatic u32 get_ccsidr(u32 csselr)\n{\n\tu32 ccsidr;\n\n\t/* Make sure noone else changes CSSELR during this! */\n\tlocal_irq_disable();\n\twrite_sysreg(csselr, csselr_el1);\n\tisb();\n\tccsidr = read_sysreg(ccsidr_el1);\n\tlocal_irq_enable();\n\n\treturn ccsidr;\n}\n\n/*\n * See note at ARMv7 ARM B1.14.4 (TL;DR: S/W ops are not easily virtualized).\n */\nstatic bool access_dcsw(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (!p->is_write)\n\t\treturn read_from_write_only(vcpu, p);\n\n\tkvm_set_way_flush(vcpu);\n\treturn true;\n}\n\n/*\n * Generic accessor for VM registers. Only called as long as HCR_TVM\n * is set. If the guest enables the MMU, we stop trapping the VM\n * sys_regs and leave it in complete control of the caches.\n */\nstatic bool access_vm_reg(struct kvm_vcpu *vcpu,\n\t\t\t  struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tbool was_enabled = vcpu_has_cache_enabled(vcpu);\n\n\tBUG_ON(!p->is_write);\n\n\tif (!p->is_aarch32) {\n\t\tvcpu_sys_reg(vcpu, r->reg) = p->regval;\n\t} else {\n\t\tif (!p->is_32bit)\n\t\t\tvcpu_cp15_64_high(vcpu, r->reg) = upper_32_bits(p->regval);\n\t\tvcpu_cp15_64_low(vcpu, r->reg) = lower_32_bits(p->regval);\n\t}\n\n\tkvm_toggle_cache(vcpu, was_enabled);\n\treturn true;\n}\n\n/*\n * Trap handler for the GICv3 SGI generation system register.\n * Forward the request to the VGIC emulation.\n * The cp15_64 code makes sure this automatically works\n * for both AArch64 and AArch32 accesses.\n */\nstatic bool access_gic_sgi(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (!p->is_write)\n\t\treturn read_from_write_only(vcpu, p);\n\n\tvgic_v3_dispatch_sgi(vcpu, p->regval);\n\n\treturn true;\n}\n\nstatic bool access_gic_sre(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write)\n\t\treturn ignore_write(vcpu, p);\n\n\tp->regval = vcpu->arch.vgic_cpu.vgic_v3.vgic_sre;\n\treturn true;\n}\n\nstatic bool trap_raz_wi(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (p->is_write)\n\t\treturn ignore_write(vcpu, p);\n\telse\n\t\treturn read_zero(vcpu, p);\n}\n\nstatic bool trap_oslsr_el1(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tp->regval = (1 << 3);\n\t\treturn true;\n\t}\n}\n\nstatic bool trap_dbgauthstatus_el1(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct sys_reg_params *p,\n\t\t\t\t   const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tp->regval = read_sysreg(dbgauthstatus_el1);\n\t\treturn true;\n\t}\n}\n\n/*\n * We want to avoid world-switching all the DBG registers all the\n * time:\n * \n * - If we've touched any debug register, it is likely that we're\n *   going to touch more of them. It then makes sense to disable the\n *   traps and start doing the save/restore dance\n * - If debug is active (DBG_MDSCR_KDE or DBG_MDSCR_MDE set), it is\n *   then mandatory to save/restore the registers, as the guest\n *   depends on them.\n * \n * For this, we use a DIRTY bit, indicating the guest has modified the\n * debug registers, used as follow:\n *\n * On guest entry:\n * - If the dirty bit is set (because we're coming back from trapping),\n *   disable the traps, save host registers, restore guest registers.\n * - If debug is actively in use (DBG_MDSCR_KDE or DBG_MDSCR_MDE set),\n *   set the dirty bit, disable the traps, save host registers,\n *   restore guest registers.\n * - Otherwise, enable the traps\n *\n * On guest exit:\n * - If the dirty bit is set, save guest registers, restore host\n *   registers and clear the dirty bit. This ensure that the host can\n *   now use the debug registers.\n */\nstatic bool trap_debug_regs(struct kvm_vcpu *vcpu,\n\t\t\t    struct sys_reg_params *p,\n\t\t\t    const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\tvcpu_sys_reg(vcpu, r->reg) = p->regval;\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, r->reg);\n\t}\n\n\ttrace_trap_reg(__func__, r->reg, p->is_write, p->regval);\n\n\treturn true;\n}\n\n/*\n * reg_to_dbg/dbg_to_reg\n *\n * A 32 bit write to a debug register leave top bits alone\n * A 32 bit read from a debug register only returns the bottom bits\n *\n * All writes will set the KVM_ARM64_DEBUG_DIRTY flag to ensure the\n * hyp.S code switches between host and guest values in future.\n */\nstatic void reg_to_dbg(struct kvm_vcpu *vcpu,\n\t\t       struct sys_reg_params *p,\n\t\t       u64 *dbg_reg)\n{\n\tu64 val = p->regval;\n\n\tif (p->is_32bit) {\n\t\tval &= 0xffffffffUL;\n\t\tval |= ((*dbg_reg >> 32) << 32);\n\t}\n\n\t*dbg_reg = val;\n\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n}\n\nstatic void dbg_to_reg(struct kvm_vcpu *vcpu,\n\t\t       struct sys_reg_params *p,\n\t\t       u64 *dbg_reg)\n{\n\tp->regval = *dbg_reg;\n\tif (p->is_32bit)\n\t\tp->regval &= 0xffffffffUL;\n}\n\nstatic bool trap_bvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_bvr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg] = rd->val;\n}\n\nstatic bool trap_bcr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int get_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_bcr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg] = rd->val;\n}\n\nstatic bool trap_wvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write,\n\t\tvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg]);\n\n\treturn true;\n}\n\nstatic int set_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_wvr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg] = rd->val;\n}\n\nstatic bool trap_wcr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (p->is_write)\n\t\treg_to_dbg(vcpu, p, dbg_reg);\n\telse\n\t\tdbg_to_reg(vcpu, p, dbg_reg);\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\nstatic int set_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\t\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (copy_from_user(r, uaddr, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,\n\tconst struct kvm_one_reg *reg, void __user *uaddr)\n{\n\t__u64 *r = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];\n\n\tif (copy_to_user(uaddr, r, KVM_REG_SIZE(reg->id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic void reset_wcr(struct kvm_vcpu *vcpu,\n\t\t      const struct sys_reg_desc *rd)\n{\n\tvcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg] = rd->val;\n}\n\nstatic void reset_amair_el1(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tvcpu_sys_reg(vcpu, AMAIR_EL1) = read_sysreg(amair_el1);\n}\n\nstatic void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tu64 mpidr;\n\n\t/*\n\t * Map the vcpu_id into the first three affinity level fields of\n\t * the MPIDR. We limit the number of VCPUs in level 0 due to a\n\t * limitation to 16 CPUs in that level in the ICC_SGIxR registers\n\t * of the GICv3 to be able to address each CPU directly when\n\t * sending IPIs.\n\t */\n\tmpidr = (vcpu->vcpu_id & 0x0f) << MPIDR_LEVEL_SHIFT(0);\n\tmpidr |= ((vcpu->vcpu_id >> 4) & 0xff) << MPIDR_LEVEL_SHIFT(1);\n\tmpidr |= ((vcpu->vcpu_id >> 12) & 0xff) << MPIDR_LEVEL_SHIFT(2);\n\tvcpu_sys_reg(vcpu, MPIDR_EL1) = (1ULL << 31) | mpidr;\n}\n\nstatic void reset_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)\n{\n\tu64 pmcr, val;\n\n\tpmcr = read_sysreg(pmcr_el0);\n\t/*\n\t * Writable bits of PMCR_EL0 (ARMV8_PMU_PMCR_MASK) are reset to UNKNOWN\n\t * except PMCR.E resetting to zero.\n\t */\n\tval = ((pmcr & ~ARMV8_PMU_PMCR_MASK)\n\t       | (ARMV8_PMU_PMCR_MASK & 0xdecafbad)) & (~ARMV8_PMU_PMCR_E);\n\tvcpu_sys_reg(vcpu, PMCR_EL0) = val;\n}\n\nstatic bool pmu_access_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & ARMV8_PMU_USERENR_EN) || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_write_swinc_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_SW | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_access_cycle_counter_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_CR | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool pmu_access_event_counter_el0_disabled(struct kvm_vcpu *vcpu)\n{\n\tu64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);\n\n\treturn !((reg & (ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_EN))\n\t\t || vcpu_mode_priv(vcpu));\n}\n\nstatic bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tu64 val;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\t/* Only update writeable bits of PMCR */\n\t\tval = vcpu_sys_reg(vcpu, PMCR_EL0);\n\t\tval &= ~ARMV8_PMU_PMCR_MASK;\n\t\tval |= p->regval & ARMV8_PMU_PMCR_MASK;\n\t\tvcpu_sys_reg(vcpu, PMCR_EL0) = val;\n\t\tkvm_pmu_handle_pmcr(vcpu, val);\n\t} else {\n\t\t/* PMCR.P & PMCR.C are RAZ */\n\t\tval = vcpu_sys_reg(vcpu, PMCR_EL0)\n\t\t      & ~(ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C);\n\t\tp->regval = val;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write)\n\t\tvcpu_sys_reg(vcpu, PMSELR_EL0) = p->regval;\n\telse\n\t\t/* return PMSELR.SEL field */\n\t\tp->regval = vcpu_sys_reg(vcpu, PMSELR_EL0)\n\t\t\t    & ARMV8_PMU_COUNTER_MASK;\n\n\treturn true;\n}\n\nstatic bool access_pmceid(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t  const struct sys_reg_desc *r)\n{\n\tu64 pmceid;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tBUG_ON(p->is_write);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (!(p->Op2 & 1))\n\t\tpmceid = read_sysreg(pmceid0_el0);\n\telse\n\t\tpmceid = read_sysreg(pmceid1_el0);\n\n\tp->regval = pmceid;\n\n\treturn true;\n}\n\nstatic bool pmu_counter_idx_valid(struct kvm_vcpu *vcpu, u64 idx)\n{\n\tu64 pmcr, val;\n\n\tpmcr = vcpu_sys_reg(vcpu, PMCR_EL0);\n\tval = (pmcr >> ARMV8_PMU_PMCR_N_SHIFT) & ARMV8_PMU_PMCR_N_MASK;\n\tif (idx >= val && idx != ARMV8_PMU_CYCLE_IDX)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool access_pmu_evcntr(struct kvm_vcpu *vcpu,\n\t\t\t      struct sys_reg_params *p,\n\t\t\t      const struct sys_reg_desc *r)\n{\n\tu64 idx;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (r->CRn == 9 && r->CRm == 13) {\n\t\tif (r->Op2 == 2) {\n\t\t\t/* PMXEVCNTR_EL0 */\n\t\t\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\t\t\treturn false;\n\n\t\t\tidx = vcpu_sys_reg(vcpu, PMSELR_EL0)\n\t\t\t      & ARMV8_PMU_COUNTER_MASK;\n\t\t} else if (r->Op2 == 0) {\n\t\t\t/* PMCCNTR_EL0 */\n\t\t\tif (pmu_access_cycle_counter_el0_disabled(vcpu))\n\t\t\t\treturn false;\n\n\t\t\tidx = ARMV8_PMU_CYCLE_IDX;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t} else if (r->CRn == 0 && r->CRm == 9) {\n\t\t/* PMCCNTR */\n\t\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\t\treturn false;\n\n\t\tidx = ARMV8_PMU_CYCLE_IDX;\n\t} else if (r->CRn == 14 && (r->CRm & 12) == 8) {\n\t\t/* PMEVCNTRn_EL0 */\n\t\tif (pmu_access_event_counter_el0_disabled(vcpu))\n\t\t\treturn false;\n\n\t\tidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\n\t} else {\n\t\treturn false;\n\t}\n\n\tif (!pmu_counter_idx_valid(vcpu, idx))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tif (pmu_access_el0_disabled(vcpu))\n\t\t\treturn false;\n\n\t\tkvm_pmu_set_counter_value(vcpu, idx, p->regval);\n\t} else {\n\t\tp->regval = kvm_pmu_get_counter_value(vcpu, idx);\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t       const struct sys_reg_desc *r)\n{\n\tu64 idx, reg;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (r->CRn == 9 && r->CRm == 13 && r->Op2 == 1) {\n\t\t/* PMXEVTYPER_EL0 */\n\t\tidx = vcpu_sys_reg(vcpu, PMSELR_EL0) & ARMV8_PMU_COUNTER_MASK;\n\t\treg = PMEVTYPER0_EL0 + idx;\n\t} else if (r->CRn == 14 && (r->CRm & 12) == 12) {\n\t\tidx = ((r->CRm & 3) << 3) | (r->Op2 & 7);\n\t\tif (idx == ARMV8_PMU_CYCLE_IDX)\n\t\t\treg = PMCCFILTR_EL0;\n\t\telse\n\t\t\t/* PMEVTYPERn_EL0 */\n\t\t\treg = PMEVTYPER0_EL0 + idx;\n\t} else {\n\t\tBUG();\n\t}\n\n\tif (!pmu_counter_idx_valid(vcpu, idx))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tkvm_pmu_set_counter_event_type(vcpu, p->regval, idx);\n\t\tvcpu_sys_reg(vcpu, reg) = p->regval & ARMV8_PMU_EVTYPE_MASK;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, reg) & ARMV8_PMU_EVTYPE_MASK;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmcnten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 val, mask;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tmask = kvm_pmu_valid_counter_mask(vcpu);\n\tif (p->is_write) {\n\t\tval = p->regval & mask;\n\t\tif (r->Op2 & 0x1) {\n\t\t\t/* accessing PMCNTENSET_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMCNTENSET_EL0) |= val;\n\t\t\tkvm_pmu_enable_counter(vcpu, val);\n\t\t} else {\n\t\t\t/* accessing PMCNTENCLR_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMCNTENSET_EL0) &= ~val;\n\t\t\tkvm_pmu_disable_counter(vcpu, val);\n\t\t}\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMCNTENSET_EL0) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pminten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 mask = kvm_pmu_valid_counter_mask(vcpu);\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (!vcpu_mode_priv(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tu64 val = p->regval & mask;\n\n\t\tif (r->Op2 & 0x1)\n\t\t\t/* accessing PMINTENSET_EL1 */\n\t\t\tvcpu_sys_reg(vcpu, PMINTENSET_EL1) |= val;\n\t\telse\n\t\t\t/* accessing PMINTENCLR_EL1 */\n\t\t\tvcpu_sys_reg(vcpu, PMINTENSET_EL1) &= ~val;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMINTENSET_EL1) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmovs(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t const struct sys_reg_desc *r)\n{\n\tu64 mask = kvm_pmu_valid_counter_mask(vcpu);\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_access_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tif (r->CRm & 0x2)\n\t\t\t/* accessing PMOVSSET_EL0 */\n\t\t\tkvm_pmu_overflow_set(vcpu, p->regval & mask);\n\t\telse\n\t\t\t/* accessing PMOVSCLR_EL0 */\n\t\t\tvcpu_sys_reg(vcpu, PMOVSSET_EL0) &= ~(p->regval & mask);\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMOVSSET_EL0) & mask;\n\t}\n\n\treturn true;\n}\n\nstatic bool access_pmswinc(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t   const struct sys_reg_desc *r)\n{\n\tu64 mask;\n\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (pmu_write_swinc_el0_disabled(vcpu))\n\t\treturn false;\n\n\tif (p->is_write) {\n\t\tmask = kvm_pmu_valid_counter_mask(vcpu);\n\t\tkvm_pmu_software_increment(vcpu, p->regval & mask);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,\n\t\t\t     const struct sys_reg_desc *r)\n{\n\tif (!kvm_arm_pmu_v3_ready(vcpu))\n\t\treturn trap_raz_wi(vcpu, p, r);\n\n\tif (p->is_write) {\n\t\tif (!vcpu_mode_priv(vcpu))\n\t\t\treturn false;\n\n\t\tvcpu_sys_reg(vcpu, PMUSERENR_EL0) = p->regval\n\t\t\t\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t} else {\n\t\tp->regval = vcpu_sys_reg(vcpu, PMUSERENR_EL0)\n\t\t\t    & ARMV8_PMU_USERENR_MASK;\n\t}\n\n\treturn true;\n}\n\n/* Silly macro to expand the DBG{BCR,BVR,WVR,WCR}n_EL1 registers in one go */\n#define DBG_BCR_BVR_WCR_WVR_EL1(n)\t\t\t\t\t\\\n\t/* DBGBVRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b100),\t\\\n\t  trap_bvr, reset_bvr, n, 0, get_bvr, set_bvr },\t\t\\\n\t/* DBGBCRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b101),\t\\\n\t  trap_bcr, reset_bcr, n, 0, get_bcr, set_bcr },\t\t\\\n\t/* DBGWVRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b110),\t\\\n\t  trap_wvr, reset_wvr, n, 0,  get_wvr, set_wvr },\t\t\\\n\t/* DBGWCRn_EL1 */\t\t\t\t\t\t\\\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm((n)), Op2(0b111),\t\\\n\t  trap_wcr, reset_wcr, n, 0,  get_wcr, set_wcr }\n\n/* Macro to expand the PMEVCNTRn_EL0 register */\n#define PMU_PMEVCNTR_EL0(n)\t\t\t\t\t\t\\\n\t/* PMEVCNTRn_EL0 */\t\t\t\t\t\t\\\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110),\t\t\t\t\\\n\t  CRm((0b1000 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evcntr, reset_unknown, (PMEVCNTR0_EL0 + n), }\n\n/* Macro to expand the PMEVTYPERn_EL0 register */\n#define PMU_PMEVTYPER_EL0(n)\t\t\t\t\t\t\\\n\t/* PMEVTYPERn_EL0 */\t\t\t\t\t\t\\\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110),\t\t\t\t\\\n\t  CRm((0b1100 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evtyper, reset_unknown, (PMEVTYPER0_EL0 + n), }\n\n/*\n * Architected system registers.\n * Important: Must be sorted ascending by Op0, Op1, CRn, CRm, Op2\n *\n * Debug handling: We do trap most, if not all debug related system\n * registers. The implementation is good enough to ensure that a guest\n * can use these with minimal performance degradation. The drawback is\n * that we don't implement any of the external debug, none of the\n * OSlock protocol. This should be revisited if we ever encounter a\n * more demanding guest...\n */\nstatic const struct sys_reg_desc sys_reg_descs[] = {\n\t/* DC ISW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b0110), Op2(0b010),\n\t  access_dcsw },\n\t/* DC CSW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b1010), Op2(0b010),\n\t  access_dcsw },\n\t/* DC CISW */\n\t{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b1110), Op2(0b010),\n\t  access_dcsw },\n\n\tDBG_BCR_BVR_WCR_WVR_EL1(0),\n\tDBG_BCR_BVR_WCR_WVR_EL1(1),\n\t/* MDCCINT_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b000),\n\t  trap_debug_regs, reset_val, MDCCINT_EL1, 0 },\n\t/* MDSCR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b010),\n\t  trap_debug_regs, reset_val, MDSCR_EL1, 0 },\n\tDBG_BCR_BVR_WCR_WVR_EL1(2),\n\tDBG_BCR_BVR_WCR_WVR_EL1(3),\n\tDBG_BCR_BVR_WCR_WVR_EL1(4),\n\tDBG_BCR_BVR_WCR_WVR_EL1(5),\n\tDBG_BCR_BVR_WCR_WVR_EL1(6),\n\tDBG_BCR_BVR_WCR_WVR_EL1(7),\n\tDBG_BCR_BVR_WCR_WVR_EL1(8),\n\tDBG_BCR_BVR_WCR_WVR_EL1(9),\n\tDBG_BCR_BVR_WCR_WVR_EL1(10),\n\tDBG_BCR_BVR_WCR_WVR_EL1(11),\n\tDBG_BCR_BVR_WCR_WVR_EL1(12),\n\tDBG_BCR_BVR_WCR_WVR_EL1(13),\n\tDBG_BCR_BVR_WCR_WVR_EL1(14),\n\tDBG_BCR_BVR_WCR_WVR_EL1(15),\n\n\t/* MDRAR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b000),\n\t  trap_raz_wi },\n\t/* OSLAR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b100),\n\t  trap_raz_wi },\n\t/* OSLSR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0001), Op2(0b100),\n\t  trap_oslsr_el1 },\n\t/* OSDLR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0011), Op2(0b100),\n\t  trap_raz_wi },\n\t/* DBGPRCR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0001), CRm(0b0100), Op2(0b100),\n\t  trap_raz_wi },\n\t/* DBGCLAIMSET_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1000), Op2(0b110),\n\t  trap_raz_wi },\n\t/* DBGCLAIMCLR_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1001), Op2(0b110),\n\t  trap_raz_wi },\n\t/* DBGAUTHSTATUS_EL1 */\n\t{ Op0(0b10), Op1(0b000), CRn(0b0111), CRm(0b1110), Op2(0b110),\n\t  trap_dbgauthstatus_el1 },\n\n\t/* MDCCSR_EL1 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0001), Op2(0b000),\n\t  trap_raz_wi },\n\t/* DBGDTR_EL0 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0100), Op2(0b000),\n\t  trap_raz_wi },\n\t/* DBGDTR[TR]X_EL0 */\n\t{ Op0(0b10), Op1(0b011), CRn(0b0000), CRm(0b0101), Op2(0b000),\n\t  trap_raz_wi },\n\n\t/* DBGVCR32_EL2 */\n\t{ Op0(0b10), Op1(0b100), CRn(0b0000), CRm(0b0111), Op2(0b000),\n\t  NULL, reset_val, DBGVCR32_EL2, 0 },\n\n\t/* MPIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b101),\n\t  NULL, reset_mpidr, MPIDR_EL1 },\n\t/* SCTLR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_val, SCTLR_EL1, 0x00C50078 },\n\t/* CPACR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0001), CRm(0b0000), Op2(0b010),\n\t  NULL, reset_val, CPACR_EL1, 0 },\n\t/* TTBR0_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_unknown, TTBR0_EL1 },\n\t/* TTBR1_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b001),\n\t  access_vm_reg, reset_unknown, TTBR1_EL1 },\n\t/* TCR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0010), CRm(0b0000), Op2(0b010),\n\t  access_vm_reg, reset_val, TCR_EL1, 0 },\n\n\t/* AFSR0_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0001), Op2(0b000),\n\t  access_vm_reg, reset_unknown, AFSR0_EL1 },\n\t/* AFSR1_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0001), Op2(0b001),\n\t  access_vm_reg, reset_unknown, AFSR1_EL1 },\n\t/* ESR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0101), CRm(0b0010), Op2(0b000),\n\t  access_vm_reg, reset_unknown, ESR_EL1 },\n\t/* FAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0110), CRm(0b0000), Op2(0b000),\n\t  access_vm_reg, reset_unknown, FAR_EL1 },\n\t/* PAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b0111), CRm(0b0100), Op2(0b000),\n\t  NULL, reset_unknown, PAR_EL1 },\n\n\t/* PMINTENSET_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1001), CRm(0b1110), Op2(0b001),\n\t  access_pminten, reset_unknown, PMINTENSET_EL1 },\n\t/* PMINTENCLR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1001), CRm(0b1110), Op2(0b010),\n\t  access_pminten, NULL, PMINTENSET_EL1 },\n\n\t/* MAIR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1010), CRm(0b0010), Op2(0b000),\n\t  access_vm_reg, reset_unknown, MAIR_EL1 },\n\t/* AMAIR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1010), CRm(0b0011), Op2(0b000),\n\t  access_vm_reg, reset_amair_el1, AMAIR_EL1 },\n\n\t/* VBAR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_val, VBAR_EL1, 0 },\n\n\t/* ICC_SGI1R_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b1011), Op2(0b101),\n\t  access_gic_sgi },\n\t/* ICC_SRE_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1100), CRm(0b1100), Op2(0b101),\n\t  access_gic_sre },\n\n\t/* CONTEXTIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1101), CRm(0b0000), Op2(0b001),\n\t  access_vm_reg, reset_val, CONTEXTIDR_EL1, 0 },\n\t/* TPIDR_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1101), CRm(0b0000), Op2(0b100),\n\t  NULL, reset_unknown, TPIDR_EL1 },\n\n\t/* CNTKCTL_EL1 */\n\t{ Op0(0b11), Op1(0b000), CRn(0b1110), CRm(0b0001), Op2(0b000),\n\t  NULL, reset_val, CNTKCTL_EL1, 0},\n\n\t/* CSSELR_EL1 */\n\t{ Op0(0b11), Op1(0b010), CRn(0b0000), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_unknown, CSSELR_EL1 },\n\n\t/* PMCR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b000),\n\t  access_pmcr, reset_pmcr, },\n\t/* PMCNTENSET_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b001),\n\t  access_pmcnten, reset_unknown, PMCNTENSET_EL0 },\n\t/* PMCNTENCLR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b010),\n\t  access_pmcnten, NULL, PMCNTENSET_EL0 },\n\t/* PMOVSCLR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b011),\n\t  access_pmovs, NULL, PMOVSSET_EL0 },\n\t/* PMSWINC_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b100),\n\t  access_pmswinc, reset_unknown, PMSWINC_EL0 },\n\t/* PMSELR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b101),\n\t  access_pmselr, reset_unknown, PMSELR_EL0 },\n\t/* PMCEID0_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b110),\n\t  access_pmceid },\n\t/* PMCEID1_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1100), Op2(0b111),\n\t  access_pmceid },\n\t/* PMCCNTR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b000),\n\t  access_pmu_evcntr, reset_unknown, PMCCNTR_EL0 },\n\t/* PMXEVTYPER_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b001),\n\t  access_pmu_evtyper },\n\t/* PMXEVCNTR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1101), Op2(0b010),\n\t  access_pmu_evcntr },\n\t/* PMUSERENR_EL0\n\t * This register resets as unknown in 64bit mode while it resets as zero\n\t * in 32bit mode. Here we choose to reset it as zero for consistency.\n\t */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1110), Op2(0b000),\n\t  access_pmuserenr, reset_val, PMUSERENR_EL0, 0 },\n\t/* PMOVSSET_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1001), CRm(0b1110), Op2(0b011),\n\t  access_pmovs, reset_unknown, PMOVSSET_EL0 },\n\n\t/* TPIDR_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1101), CRm(0b0000), Op2(0b010),\n\t  NULL, reset_unknown, TPIDR_EL0 },\n\t/* TPIDRRO_EL0 */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1101), CRm(0b0000), Op2(0b011),\n\t  NULL, reset_unknown, TPIDRRO_EL0 },\n\n\t/* PMEVCNTRn_EL0 */\n\tPMU_PMEVCNTR_EL0(0),\n\tPMU_PMEVCNTR_EL0(1),\n\tPMU_PMEVCNTR_EL0(2),\n\tPMU_PMEVCNTR_EL0(3),\n\tPMU_PMEVCNTR_EL0(4),\n\tPMU_PMEVCNTR_EL0(5),\n\tPMU_PMEVCNTR_EL0(6),\n\tPMU_PMEVCNTR_EL0(7),\n\tPMU_PMEVCNTR_EL0(8),\n\tPMU_PMEVCNTR_EL0(9),\n\tPMU_PMEVCNTR_EL0(10),\n\tPMU_PMEVCNTR_EL0(11),\n\tPMU_PMEVCNTR_EL0(12),\n\tPMU_PMEVCNTR_EL0(13),\n\tPMU_PMEVCNTR_EL0(14),\n\tPMU_PMEVCNTR_EL0(15),\n\tPMU_PMEVCNTR_EL0(16),\n\tPMU_PMEVCNTR_EL0(17),\n\tPMU_PMEVCNTR_EL0(18),\n\tPMU_PMEVCNTR_EL0(19),\n\tPMU_PMEVCNTR_EL0(20),\n\tPMU_PMEVCNTR_EL0(21),\n\tPMU_PMEVCNTR_EL0(22),\n\tPMU_PMEVCNTR_EL0(23),\n\tPMU_PMEVCNTR_EL0(24),\n\tPMU_PMEVCNTR_EL0(25),\n\tPMU_PMEVCNTR_EL0(26),\n\tPMU_PMEVCNTR_EL0(27),\n\tPMU_PMEVCNTR_EL0(28),\n\tPMU_PMEVCNTR_EL0(29),\n\tPMU_PMEVCNTR_EL0(30),\n\t/* PMEVTYPERn_EL0 */\n\tPMU_PMEVTYPER_EL0(0),\n\tPMU_PMEVTYPER_EL0(1),\n\tPMU_PMEVTYPER_EL0(2),\n\tPMU_PMEVTYPER_EL0(3),\n\tPMU_PMEVTYPER_EL0(4),\n\tPMU_PMEVTYPER_EL0(5),\n\tPMU_PMEVTYPER_EL0(6),\n\tPMU_PMEVTYPER_EL0(7),\n\tPMU_PMEVTYPER_EL0(8),\n\tPMU_PMEVTYPER_EL0(9),\n\tPMU_PMEVTYPER_EL0(10),\n\tPMU_PMEVTYPER_EL0(11),\n\tPMU_PMEVTYPER_EL0(12),\n\tPMU_PMEVTYPER_EL0(13),\n\tPMU_PMEVTYPER_EL0(14),\n\tPMU_PMEVTYPER_EL0(15),\n\tPMU_PMEVTYPER_EL0(16),\n\tPMU_PMEVTYPER_EL0(17),\n\tPMU_PMEVTYPER_EL0(18),\n\tPMU_PMEVTYPER_EL0(19),\n\tPMU_PMEVTYPER_EL0(20),\n\tPMU_PMEVTYPER_EL0(21),\n\tPMU_PMEVTYPER_EL0(22),\n\tPMU_PMEVTYPER_EL0(23),\n\tPMU_PMEVTYPER_EL0(24),\n\tPMU_PMEVTYPER_EL0(25),\n\tPMU_PMEVTYPER_EL0(26),\n\tPMU_PMEVTYPER_EL0(27),\n\tPMU_PMEVTYPER_EL0(28),\n\tPMU_PMEVTYPER_EL0(29),\n\tPMU_PMEVTYPER_EL0(30),\n\t/* PMCCFILTR_EL0\n\t * This register resets as unknown in 64bit mode while it resets as zero\n\t * in 32bit mode. Here we choose to reset it as zero for consistency.\n\t */\n\t{ Op0(0b11), Op1(0b011), CRn(0b1110), CRm(0b1111), Op2(0b111),\n\t  access_pmu_evtyper, reset_val, PMCCFILTR_EL0, 0 },\n\n\t/* DACR32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0011), CRm(0b0000), Op2(0b000),\n\t  NULL, reset_unknown, DACR32_EL2 },\n\t/* IFSR32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0101), CRm(0b0000), Op2(0b001),\n\t  NULL, reset_unknown, IFSR32_EL2 },\n\t/* FPEXC32_EL2 */\n\t{ Op0(0b11), Op1(0b100), CRn(0b0101), CRm(0b0011), Op2(0b000),\n\t  NULL, reset_val, FPEXC32_EL2, 0x70 },\n};\n\nstatic bool trap_dbgidr(struct kvm_vcpu *vcpu,\n\t\t\tstruct sys_reg_params *p,\n\t\t\tconst struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\treturn ignore_write(vcpu, p);\n\t} else {\n\t\tu64 dfr = read_system_reg(SYS_ID_AA64DFR0_EL1);\n\t\tu64 pfr = read_system_reg(SYS_ID_AA64PFR0_EL1);\n\t\tu32 el3 = !!cpuid_feature_extract_unsigned_field(pfr, ID_AA64PFR0_EL3_SHIFT);\n\n\t\tp->regval = ((((dfr >> ID_AA64DFR0_WRPS_SHIFT) & 0xf) << 28) |\n\t\t\t     (((dfr >> ID_AA64DFR0_BRPS_SHIFT) & 0xf) << 24) |\n\t\t\t     (((dfr >> ID_AA64DFR0_CTX_CMPS_SHIFT) & 0xf) << 20)\n\t\t\t     | (6 << 16) | (el3 << 14) | (el3 << 12));\n\t\treturn true;\n\t}\n}\n\nstatic bool trap_debug32(struct kvm_vcpu *vcpu,\n\t\t\t struct sys_reg_params *p,\n\t\t\t const struct sys_reg_desc *r)\n{\n\tif (p->is_write) {\n\t\tvcpu_cp14(vcpu, r->reg) = p->regval;\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = vcpu_cp14(vcpu, r->reg);\n\t}\n\n\treturn true;\n}\n\n/* AArch32 debug register mappings\n *\n * AArch32 DBGBVRn is mapped to DBGBVRn_EL1[31:0]\n * AArch32 DBGBXVRn is mapped to DBGBVRn_EL1[63:32]\n *\n * All control registers and watchpoint value registers are mapped to\n * the lower 32 bits of their AArch64 equivalents. We share the trap\n * handlers with the above AArch64 code which checks what mode the\n * system is in.\n */\n\nstatic bool trap_xvr(struct kvm_vcpu *vcpu,\n\t\t     struct sys_reg_params *p,\n\t\t     const struct sys_reg_desc *rd)\n{\n\tu64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];\n\n\tif (p->is_write) {\n\t\tu64 val = *dbg_reg;\n\n\t\tval &= 0xffffffffUL;\n\t\tval |= p->regval << 32;\n\t\t*dbg_reg = val;\n\n\t\tvcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;\n\t} else {\n\t\tp->regval = *dbg_reg >> 32;\n\t}\n\n\ttrace_trap_reg(__func__, rd->reg, p->is_write, *dbg_reg);\n\n\treturn true;\n}\n\n#define DBG_BCR_BVR_WCR_WVR(n)\t\t\t\t\t\t\\\n\t/* DBGBVRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 4), trap_bvr, NULL, n }, \t\\\n\t/* DBGBCRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 5), trap_bcr, NULL, n },\t\\\n\t/* DBGWVRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 6), trap_wvr, NULL, n },\t\\\n\t/* DBGWCRn */\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 0), CRm((n)), Op2( 7), trap_wcr, NULL, n }\n\n#define DBGBXVR(n)\t\t\t\t\t\t\t\\\n\t{ Op1( 0), CRn( 1), CRm((n)), Op2( 1), trap_xvr, NULL, n }\n\n/*\n * Trapped cp14 registers. We generally ignore most of the external\n * debug, on the principle that they don't really make sense to a\n * guest. Revisit this one day, would this principle change.\n */\nstatic const struct sys_reg_desc cp14_regs[] = {\n\t/* DBGIDR */\n\t{ Op1( 0), CRn( 0), CRm( 0), Op2( 0), trap_dbgidr },\n\t/* DBGDTRRXext */\n\t{ Op1( 0), CRn( 0), CRm( 0), Op2( 2), trap_raz_wi },\n\n\tDBG_BCR_BVR_WCR_WVR(0),\n\t/* DBGDSCRint */\n\t{ Op1( 0), CRn( 0), CRm( 1), Op2( 0), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(1),\n\t/* DBGDCCINT */\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 0), trap_debug32 },\n\t/* DBGDSCRext */\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 2), trap_debug32 },\n\tDBG_BCR_BVR_WCR_WVR(2),\n\t/* DBGDTR[RT]Xint */\n\t{ Op1( 0), CRn( 0), CRm( 3), Op2( 0), trap_raz_wi },\n\t/* DBGDTR[RT]Xext */\n\t{ Op1( 0), CRn( 0), CRm( 3), Op2( 2), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(3),\n\tDBG_BCR_BVR_WCR_WVR(4),\n\tDBG_BCR_BVR_WCR_WVR(5),\n\t/* DBGWFAR */\n\t{ Op1( 0), CRn( 0), CRm( 6), Op2( 0), trap_raz_wi },\n\t/* DBGOSECCR */\n\t{ Op1( 0), CRn( 0), CRm( 6), Op2( 2), trap_raz_wi },\n\tDBG_BCR_BVR_WCR_WVR(6),\n\t/* DBGVCR */\n\t{ Op1( 0), CRn( 0), CRm( 7), Op2( 0), trap_debug32 },\n\tDBG_BCR_BVR_WCR_WVR(7),\n\tDBG_BCR_BVR_WCR_WVR(8),\n\tDBG_BCR_BVR_WCR_WVR(9),\n\tDBG_BCR_BVR_WCR_WVR(10),\n\tDBG_BCR_BVR_WCR_WVR(11),\n\tDBG_BCR_BVR_WCR_WVR(12),\n\tDBG_BCR_BVR_WCR_WVR(13),\n\tDBG_BCR_BVR_WCR_WVR(14),\n\tDBG_BCR_BVR_WCR_WVR(15),\n\n\t/* DBGDRAR (32bit) */\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 0), trap_raz_wi },\n\n\tDBGBXVR(0),\n\t/* DBGOSLAR */\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 4), trap_raz_wi },\n\tDBGBXVR(1),\n\t/* DBGOSLSR */\n\t{ Op1( 0), CRn( 1), CRm( 1), Op2( 4), trap_oslsr_el1 },\n\tDBGBXVR(2),\n\tDBGBXVR(3),\n\t/* DBGOSDLR */\n\t{ Op1( 0), CRn( 1), CRm( 3), Op2( 4), trap_raz_wi },\n\tDBGBXVR(4),\n\t/* DBGPRCR */\n\t{ Op1( 0), CRn( 1), CRm( 4), Op2( 4), trap_raz_wi },\n\tDBGBXVR(5),\n\tDBGBXVR(6),\n\tDBGBXVR(7),\n\tDBGBXVR(8),\n\tDBGBXVR(9),\n\tDBGBXVR(10),\n\tDBGBXVR(11),\n\tDBGBXVR(12),\n\tDBGBXVR(13),\n\tDBGBXVR(14),\n\tDBGBXVR(15),\n\n\t/* DBGDSAR (32bit) */\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 0), trap_raz_wi },\n\n\t/* DBGDEVID2 */\n\t{ Op1( 0), CRn( 7), CRm( 0), Op2( 7), trap_raz_wi },\n\t/* DBGDEVID1 */\n\t{ Op1( 0), CRn( 7), CRm( 1), Op2( 7), trap_raz_wi },\n\t/* DBGDEVID */\n\t{ Op1( 0), CRn( 7), CRm( 2), Op2( 7), trap_raz_wi },\n\t/* DBGCLAIMSET */\n\t{ Op1( 0), CRn( 7), CRm( 8), Op2( 6), trap_raz_wi },\n\t/* DBGCLAIMCLR */\n\t{ Op1( 0), CRn( 7), CRm( 9), Op2( 6), trap_raz_wi },\n\t/* DBGAUTHSTATUS */\n\t{ Op1( 0), CRn( 7), CRm(14), Op2( 6), trap_dbgauthstatus_el1 },\n};\n\n/* Trapped cp14 64bit registers */\nstatic const struct sys_reg_desc cp14_64_regs[] = {\n\t/* DBGDRAR (64bit) */\n\t{ Op1( 0), CRm( 1), .access = trap_raz_wi },\n\n\t/* DBGDSAR (64bit) */\n\t{ Op1( 0), CRm( 2), .access = trap_raz_wi },\n};\n\n/* Macro to expand the PMEVCNTRn register */\n#define PMU_PMEVCNTR(n)\t\t\t\t\t\t\t\\\n\t/* PMEVCNTRn */\t\t\t\t\t\t\t\\\n\t{ Op1(0), CRn(0b1110),\t\t\t\t\t\t\\\n\t  CRm((0b1000 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evcntr }\n\n/* Macro to expand the PMEVTYPERn register */\n#define PMU_PMEVTYPER(n)\t\t\t\t\t\t\\\n\t/* PMEVTYPERn */\t\t\t\t\t\t\\\n\t{ Op1(0), CRn(0b1110),\t\t\t\t\t\t\\\n\t  CRm((0b1100 | (((n) >> 3) & 0x3))), Op2(((n) & 0x7)),\t\t\\\n\t  access_pmu_evtyper }\n\n/*\n * Trapped cp15 registers. TTBR0/TTBR1 get a double encoding,\n * depending on the way they are accessed (as a 32bit or a 64bit\n * register).\n */\nstatic const struct sys_reg_desc cp15_regs[] = {\n\t{ Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },\n\n\t{ Op1( 0), CRn( 1), CRm( 0), Op2( 0), access_vm_reg, NULL, c1_SCTLR },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 0), access_vm_reg, NULL, c2_TTBR0 },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 1), access_vm_reg, NULL, c2_TTBR1 },\n\t{ Op1( 0), CRn( 2), CRm( 0), Op2( 2), access_vm_reg, NULL, c2_TTBCR },\n\t{ Op1( 0), CRn( 3), CRm( 0), Op2( 0), access_vm_reg, NULL, c3_DACR },\n\t{ Op1( 0), CRn( 5), CRm( 0), Op2( 0), access_vm_reg, NULL, c5_DFSR },\n\t{ Op1( 0), CRn( 5), CRm( 0), Op2( 1), access_vm_reg, NULL, c5_IFSR },\n\t{ Op1( 0), CRn( 5), CRm( 1), Op2( 0), access_vm_reg, NULL, c5_ADFSR },\n\t{ Op1( 0), CRn( 5), CRm( 1), Op2( 1), access_vm_reg, NULL, c5_AIFSR },\n\t{ Op1( 0), CRn( 6), CRm( 0), Op2( 0), access_vm_reg, NULL, c6_DFAR },\n\t{ Op1( 0), CRn( 6), CRm( 0), Op2( 2), access_vm_reg, NULL, c6_IFAR },\n\n\t/*\n\t * DC{C,I,CI}SW operations:\n\t */\n\t{ Op1( 0), CRn( 7), CRm( 6), Op2( 2), access_dcsw },\n\t{ Op1( 0), CRn( 7), CRm(10), Op2( 2), access_dcsw },\n\t{ Op1( 0), CRn( 7), CRm(14), Op2( 2), access_dcsw },\n\n\t/* PMU */\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 0), access_pmcr },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 1), access_pmcnten },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 2), access_pmcnten },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 3), access_pmovs },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 4), access_pmswinc },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 5), access_pmselr },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 6), access_pmceid },\n\t{ Op1( 0), CRn( 9), CRm(12), Op2( 7), access_pmceid },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 0), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 1), access_pmu_evtyper },\n\t{ Op1( 0), CRn( 9), CRm(13), Op2( 2), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 0), access_pmuserenr },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 1), access_pminten },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 2), access_pminten },\n\t{ Op1( 0), CRn( 9), CRm(14), Op2( 3), access_pmovs },\n\n\t{ Op1( 0), CRn(10), CRm( 2), Op2( 0), access_vm_reg, NULL, c10_PRRR },\n\t{ Op1( 0), CRn(10), CRm( 2), Op2( 1), access_vm_reg, NULL, c10_NMRR },\n\t{ Op1( 0), CRn(10), CRm( 3), Op2( 0), access_vm_reg, NULL, c10_AMAIR0 },\n\t{ Op1( 0), CRn(10), CRm( 3), Op2( 1), access_vm_reg, NULL, c10_AMAIR1 },\n\n\t/* ICC_SRE */\n\t{ Op1( 0), CRn(12), CRm(12), Op2( 5), access_gic_sre },\n\n\t{ Op1( 0), CRn(13), CRm( 0), Op2( 1), access_vm_reg, NULL, c13_CID },\n\n\t/* PMEVCNTRn */\n\tPMU_PMEVCNTR(0),\n\tPMU_PMEVCNTR(1),\n\tPMU_PMEVCNTR(2),\n\tPMU_PMEVCNTR(3),\n\tPMU_PMEVCNTR(4),\n\tPMU_PMEVCNTR(5),\n\tPMU_PMEVCNTR(6),\n\tPMU_PMEVCNTR(7),\n\tPMU_PMEVCNTR(8),\n\tPMU_PMEVCNTR(9),\n\tPMU_PMEVCNTR(10),\n\tPMU_PMEVCNTR(11),\n\tPMU_PMEVCNTR(12),\n\tPMU_PMEVCNTR(13),\n\tPMU_PMEVCNTR(14),\n\tPMU_PMEVCNTR(15),\n\tPMU_PMEVCNTR(16),\n\tPMU_PMEVCNTR(17),\n\tPMU_PMEVCNTR(18),\n\tPMU_PMEVCNTR(19),\n\tPMU_PMEVCNTR(20),\n\tPMU_PMEVCNTR(21),\n\tPMU_PMEVCNTR(22),\n\tPMU_PMEVCNTR(23),\n\tPMU_PMEVCNTR(24),\n\tPMU_PMEVCNTR(25),\n\tPMU_PMEVCNTR(26),\n\tPMU_PMEVCNTR(27),\n\tPMU_PMEVCNTR(28),\n\tPMU_PMEVCNTR(29),\n\tPMU_PMEVCNTR(30),\n\t/* PMEVTYPERn */\n\tPMU_PMEVTYPER(0),\n\tPMU_PMEVTYPER(1),\n\tPMU_PMEVTYPER(2),\n\tPMU_PMEVTYPER(3),\n\tPMU_PMEVTYPER(4),\n\tPMU_PMEVTYPER(5),\n\tPMU_PMEVTYPER(6),\n\tPMU_PMEVTYPER(7),\n\tPMU_PMEVTYPER(8),\n\tPMU_PMEVTYPER(9),\n\tPMU_PMEVTYPER(10),\n\tPMU_PMEVTYPER(11),\n\tPMU_PMEVTYPER(12),\n\tPMU_PMEVTYPER(13),\n\tPMU_PMEVTYPER(14),\n\tPMU_PMEVTYPER(15),\n\tPMU_PMEVTYPER(16),\n\tPMU_PMEVTYPER(17),\n\tPMU_PMEVTYPER(18),\n\tPMU_PMEVTYPER(19),\n\tPMU_PMEVTYPER(20),\n\tPMU_PMEVTYPER(21),\n\tPMU_PMEVTYPER(22),\n\tPMU_PMEVTYPER(23),\n\tPMU_PMEVTYPER(24),\n\tPMU_PMEVTYPER(25),\n\tPMU_PMEVTYPER(26),\n\tPMU_PMEVTYPER(27),\n\tPMU_PMEVTYPER(28),\n\tPMU_PMEVTYPER(29),\n\tPMU_PMEVTYPER(30),\n\t/* PMCCFILTR */\n\t{ Op1(0), CRn(14), CRm(15), Op2(7), access_pmu_evtyper },\n};\n\nstatic const struct sys_reg_desc cp15_64_regs[] = {\n\t{ Op1( 0), CRn( 0), CRm( 2), Op2( 0), access_vm_reg, NULL, c2_TTBR0 },\n\t{ Op1( 0), CRn( 0), CRm( 9), Op2( 0), access_pmu_evcntr },\n\t{ Op1( 0), CRn( 0), CRm(12), Op2( 0), access_gic_sgi },\n\t{ Op1( 1), CRn( 0), CRm( 2), Op2( 0), access_vm_reg, NULL, c2_TTBR1 },\n};\n\n/* Target specific emulation tables */\nstatic struct kvm_sys_reg_target_table *target_tables[KVM_ARM_NUM_TARGETS];\n\nvoid kvm_register_target_sys_reg_table(unsigned int target,\n\t\t\t\t       struct kvm_sys_reg_target_table *table)\n{\n\ttarget_tables[target] = table;\n}\n\n/* Get specific register table for this target. */\nstatic const struct sys_reg_desc *get_target_table(unsigned target,\n\t\t\t\t\t\t   bool mode_is_64,\n\t\t\t\t\t\t   size_t *num)\n{\n\tstruct kvm_sys_reg_target_table *table;\n\n\ttable = target_tables[target];\n\tif (mode_is_64) {\n\t\t*num = table->table64.num;\n\t\treturn table->table64.table;\n\t} else {\n\t\t*num = table->table32.num;\n\t\treturn table->table32.table;\n\t}\n}\n\n#define reg_to_match_value(x)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tunsigned long val;\t\t\t\t\t\\\n\t\tval  = (x)->Op0 << 14;\t\t\t\t\t\\\n\t\tval |= (x)->Op1 << 11;\t\t\t\t\t\\\n\t\tval |= (x)->CRn << 7;\t\t\t\t\t\\\n\t\tval |= (x)->CRm << 3;\t\t\t\t\t\\\n\t\tval |= (x)->Op2;\t\t\t\t\t\\\n\t\tval;\t\t\t\t\t\t\t\\\n\t })\n\nstatic int match_sys_reg(const void *key, const void *elt)\n{\n\tconst unsigned long pval = (unsigned long)key;\n\tconst struct sys_reg_desc *r = elt;\n\n\treturn pval - reg_to_match_value(r);\n}\n\nstatic const struct sys_reg_desc *find_reg(const struct sys_reg_params *params,\n\t\t\t\t\t const struct sys_reg_desc table[],\n\t\t\t\t\t unsigned int num)\n{\n\tunsigned long pval = reg_to_match_value(params);\n\n\treturn bsearch((void *)pval, table, num, sizeof(table[0]), match_sys_reg);\n}\n\nint kvm_handle_cp14_load_store(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tkvm_inject_undefined(vcpu);\n\treturn 1;\n}\n\n/*\n * emulate_cp --  tries to match a sys_reg access in a handling table, and\n *                call the corresponding trap handler.\n *\n * @params: pointer to the descriptor of the access\n * @table: array of trap descriptors\n * @num: size of the trap descriptor array\n *\n * Return 0 if the access has been handled, and -1 if not.\n */\nstatic int emulate_cp(struct kvm_vcpu *vcpu,\n\t\t      struct sys_reg_params *params,\n\t\t      const struct sys_reg_desc *table,\n\t\t      size_t num)\n{\n\tconst struct sys_reg_desc *r;\n\n\tif (!table)\n\t\treturn -1;\t/* Not handled */\n\n\tr = find_reg(params, table, num);\n\n\tif (r) {\n\t\t/*\n\t\t * Not having an accessor means that we have\n\t\t * configured a trap that we don't know how to\n\t\t * handle. This certainly qualifies as a gross bug\n\t\t * that should be fixed right away.\n\t\t */\n\t\tBUG_ON(!r->access);\n\n\t\tif (likely(r->access(vcpu, params, r))) {\n\t\t\t/* Skip instruction, since it was emulated */\n\t\t\tkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\n\t\t\t/* Handled */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* Not handled */\n\treturn -1;\n}\n\nstatic void unhandled_cp_access(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct sys_reg_params *params)\n{\n\tu8 hsr_ec = kvm_vcpu_trap_get_class(vcpu);\n\tint cp = -1;\n\n\tswitch(hsr_ec) {\n\tcase ESR_ELx_EC_CP15_32:\n\tcase ESR_ELx_EC_CP15_64:\n\t\tcp = 15;\n\t\tbreak;\n\tcase ESR_ELx_EC_CP14_MR:\n\tcase ESR_ELx_EC_CP14_64:\n\t\tcp = 14;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t}\n\n\tkvm_err(\"Unsupported guest CP%d access at: %08lx\\n\",\n\t\tcp, *vcpu_pc(vcpu));\n\tprint_sys_reg_instr(params);\n\tkvm_inject_undefined(vcpu);\n}\n\n/**\n * kvm_handle_cp_64 -- handles a mrrc/mcrr trap on a guest CP14/CP15 access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nstatic int kvm_handle_cp_64(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt = (hsr >> 5) & 0xf;\n\tint Rt2 = (hsr >> 10) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = false;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.is_write = ((hsr & 1) == 0);\n\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 16) & 0xf;\n\tparams.Op2 = 0;\n\tparams.CRn = 0;\n\n\t/*\n\t * Make a 64-bit value out of Rt and Rt2. As we use the same trap\n\t * backends between AArch32 and AArch64, we get away with it.\n\t */\n\tif (params.is_write) {\n\t\tparams.regval = vcpu_get_reg(vcpu, Rt) & 0xffffffff;\n\t\tparams.regval |= vcpu_get_reg(vcpu, Rt2) << 32;\n\t}\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific))\n\t\tgoto out;\n\tif (!emulate_cp(vcpu, &params, global, nr_global))\n\t\tgoto out;\n\n\tunhandled_cp_access(vcpu, &params);\n\nout:\n\t/* Split up the value between registers for the read side */\n\tif (!params.is_write) {\n\t\tvcpu_set_reg(vcpu, Rt, lower_32_bits(params.regval));\n\t\tvcpu_set_reg(vcpu, Rt2, upper_32_bits(params.regval));\n\t}\n\n\treturn 1;\n}\n\n/**\n * kvm_handle_cp_32 -- handles a mrc/mcr trap on a guest CP14/CP15 access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nstatic int kvm_handle_cp_32(struct kvm_vcpu *vcpu,\n\t\t\t    const struct sys_reg_desc *global,\n\t\t\t    size_t nr_global,\n\t\t\t    const struct sys_reg_desc *target_specific,\n\t\t\t    size_t nr_specific)\n{\n\tstruct sys_reg_params params;\n\tu32 hsr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt  = (hsr >> 5) & 0xf;\n\n\tparams.is_aarch32 = true;\n\tparams.is_32bit = true;\n\tparams.CRm = (hsr >> 1) & 0xf;\n\tparams.regval = vcpu_get_reg(vcpu, Rt);\n\tparams.is_write = ((hsr & 1) == 0);\n\tparams.CRn = (hsr >> 10) & 0xf;\n\tparams.Op0 = 0;\n\tparams.Op1 = (hsr >> 14) & 0x7;\n\tparams.Op2 = (hsr >> 17) & 0x7;\n\n\tif (!emulate_cp(vcpu, &params, target_specific, nr_specific) ||\n\t    !emulate_cp(vcpu, &params, global, nr_global)) {\n\t\tif (!params.is_write)\n\t\t\tvcpu_set_reg(vcpu, Rt, params.regval);\n\t\treturn 1;\n\t}\n\n\tunhandled_cp_access(vcpu, &params);\n\treturn 1;\n}\n\nint kvm_handle_cp15_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tconst struct sys_reg_desc *target_specific;\n\tsize_t num;\n\n\ttarget_specific = get_target_table(vcpu->arch.target, false, &num);\n\treturn kvm_handle_cp_64(vcpu,\n\t\t\t\tcp15_64_regs, ARRAY_SIZE(cp15_64_regs),\n\t\t\t\ttarget_specific, num);\n}\n\nint kvm_handle_cp15_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tconst struct sys_reg_desc *target_specific;\n\tsize_t num;\n\n\ttarget_specific = get_target_table(vcpu->arch.target, false, &num);\n\treturn kvm_handle_cp_32(vcpu,\n\t\t\t\tcp15_regs, ARRAY_SIZE(cp15_regs),\n\t\t\t\ttarget_specific, num);\n}\n\nint kvm_handle_cp14_64(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\treturn kvm_handle_cp_64(vcpu,\n\t\t\t\tcp14_64_regs, ARRAY_SIZE(cp14_64_regs),\n\t\t\t\tNULL, 0);\n}\n\nint kvm_handle_cp14_32(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\treturn kvm_handle_cp_32(vcpu,\n\t\t\t\tcp14_regs, ARRAY_SIZE(cp14_regs),\n\t\t\t\tNULL, 0);\n}\n\nstatic int emulate_sys_reg(struct kvm_vcpu *vcpu,\n\t\t\t   struct sys_reg_params *params)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table, *r;\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\n\t/* Search target-specific then generic table. */\n\tr = find_reg(params, table, num);\n\tif (!r)\n\t\tr = find_reg(params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\tif (likely(r)) {\n\t\t/*\n\t\t * Not having an accessor means that we have\n\t\t * configured a trap that we don't know how to\n\t\t * handle. This certainly qualifies as a gross bug\n\t\t * that should be fixed right away.\n\t\t */\n\t\tBUG_ON(!r->access);\n\n\t\tif (likely(r->access(vcpu, params, r))) {\n\t\t\t/* Skip instruction, since it was emulated */\n\t\t\tkvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));\n\t\t\treturn 1;\n\t\t}\n\t\t/* If access function fails, it should complain. */\n\t} else {\n\t\tkvm_err(\"Unsupported guest sys_reg access at: %lx\\n\",\n\t\t\t*vcpu_pc(vcpu));\n\t\tprint_sys_reg_instr(params);\n\t}\n\tkvm_inject_undefined(vcpu);\n\treturn 1;\n}\n\nstatic void reset_sys_reg_descs(struct kvm_vcpu *vcpu,\n\t\t\t      const struct sys_reg_desc *table, size_t num)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < num; i++)\n\t\tif (table[i].reset)\n\t\t\ttable[i].reset(vcpu, &table[i]);\n}\n\n/**\n * kvm_handle_sys_reg -- handles a mrs/msr trap on a guest sys_reg access\n * @vcpu: The VCPU pointer\n * @run:  The kvm_run struct\n */\nint kvm_handle_sys_reg(struct kvm_vcpu *vcpu, struct kvm_run *run)\n{\n\tstruct sys_reg_params params;\n\tunsigned long esr = kvm_vcpu_get_hsr(vcpu);\n\tint Rt = (esr >> 5) & 0x1f;\n\tint ret;\n\n\ttrace_kvm_handle_sys_reg(esr);\n\n\tparams.is_aarch32 = false;\n\tparams.is_32bit = false;\n\tparams.Op0 = (esr >> 20) & 3;\n\tparams.Op1 = (esr >> 14) & 0x7;\n\tparams.CRn = (esr >> 10) & 0xf;\n\tparams.CRm = (esr >> 1) & 0xf;\n\tparams.Op2 = (esr >> 17) & 0x7;\n\tparams.regval = vcpu_get_reg(vcpu, Rt);\n\tparams.is_write = !(esr & 1);\n\n\tret = emulate_sys_reg(vcpu, &params);\n\n\tif (!params.is_write)\n\t\tvcpu_set_reg(vcpu, Rt, params.regval);\n\treturn ret;\n}\n\n/******************************************************************************\n * Userspace API\n *****************************************************************************/\n\nstatic bool index_to_params(u64 id, struct sys_reg_params *params)\n{\n\tswitch (id & KVM_REG_SIZE_MASK) {\n\tcase KVM_REG_SIZE_U64:\n\t\t/* Any unused index bits means it's not valid. */\n\t\tif (id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK\n\t\t\t      | KVM_REG_ARM_COPROC_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP0_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP1_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRN_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_CRM_MASK\n\t\t\t      | KVM_REG_ARM64_SYSREG_OP2_MASK))\n\t\t\treturn false;\n\t\tparams->Op0 = ((id & KVM_REG_ARM64_SYSREG_OP0_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP0_SHIFT);\n\t\tparams->Op1 = ((id & KVM_REG_ARM64_SYSREG_OP1_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP1_SHIFT);\n\t\tparams->CRn = ((id & KVM_REG_ARM64_SYSREG_CRN_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRN_SHIFT);\n\t\tparams->CRm = ((id & KVM_REG_ARM64_SYSREG_CRM_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_CRM_SHIFT);\n\t\tparams->Op2 = ((id & KVM_REG_ARM64_SYSREG_OP2_MASK)\n\t\t\t       >> KVM_REG_ARM64_SYSREG_OP2_SHIFT);\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Decode an index value, and find the sys_reg_desc entry. */\nstatic const struct sys_reg_desc *index_to_sys_reg_desc(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t    u64 id)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table, *r;\n\tstruct sys_reg_params params;\n\n\t/* We only do sys_reg for now. */\n\tif ((id & KVM_REG_ARM_COPROC_MASK) != KVM_REG_ARM64_SYSREG)\n\t\treturn NULL;\n\n\tif (!index_to_params(id, &params))\n\t\treturn NULL;\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\tr = find_reg(&params, table, num);\n\tif (!r)\n\t\tr = find_reg(&params, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\t/* Not saved in the sys_reg array? */\n\tif (r && !r->reg)\n\t\tr = NULL;\n\n\treturn r;\n}\n\n/*\n * These are the invariant sys_reg registers: we let the guest see the\n * host versions of these, so they're part of the guest state.\n *\n * A future CPU may provide a mechanism to present different values to\n * the guest, or a future kvm may trap them.\n */\n\n#define FUNCTION_INVARIANT(reg)\t\t\t\t\t\t\\\n\tstatic void get_##reg(struct kvm_vcpu *v,\t\t\t\\\n\t\t\t      const struct sys_reg_desc *r)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t((struct sys_reg_desc *)r)->val = read_sysreg(reg);\t\\\n\t}\n\nFUNCTION_INVARIANT(midr_el1)\nFUNCTION_INVARIANT(ctr_el0)\nFUNCTION_INVARIANT(revidr_el1)\nFUNCTION_INVARIANT(id_pfr0_el1)\nFUNCTION_INVARIANT(id_pfr1_el1)\nFUNCTION_INVARIANT(id_dfr0_el1)\nFUNCTION_INVARIANT(id_afr0_el1)\nFUNCTION_INVARIANT(id_mmfr0_el1)\nFUNCTION_INVARIANT(id_mmfr1_el1)\nFUNCTION_INVARIANT(id_mmfr2_el1)\nFUNCTION_INVARIANT(id_mmfr3_el1)\nFUNCTION_INVARIANT(id_isar0_el1)\nFUNCTION_INVARIANT(id_isar1_el1)\nFUNCTION_INVARIANT(id_isar2_el1)\nFUNCTION_INVARIANT(id_isar3_el1)\nFUNCTION_INVARIANT(id_isar4_el1)\nFUNCTION_INVARIANT(id_isar5_el1)\nFUNCTION_INVARIANT(clidr_el1)\nFUNCTION_INVARIANT(aidr_el1)\n\n/* ->val is filled in by kvm_sys_reg_table_init() */\nstatic struct sys_reg_desc invariant_sys_regs[] = {\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b000),\n\t  NULL, get_midr_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0000), Op2(0b110),\n\t  NULL, get_revidr_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b000),\n\t  NULL, get_id_pfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b001),\n\t  NULL, get_id_pfr1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b010),\n\t  NULL, get_id_dfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b011),\n\t  NULL, get_id_afr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b100),\n\t  NULL, get_id_mmfr0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b101),\n\t  NULL, get_id_mmfr1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b110),\n\t  NULL, get_id_mmfr2_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0001), Op2(0b111),\n\t  NULL, get_id_mmfr3_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b000),\n\t  NULL, get_id_isar0_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b001),\n\t  NULL, get_id_isar1_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b010),\n\t  NULL, get_id_isar2_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b011),\n\t  NULL, get_id_isar3_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b100),\n\t  NULL, get_id_isar4_el1 },\n\t{ Op0(0b11), Op1(0b000), CRn(0b0000), CRm(0b0010), Op2(0b101),\n\t  NULL, get_id_isar5_el1 },\n\t{ Op0(0b11), Op1(0b001), CRn(0b0000), CRm(0b0000), Op2(0b001),\n\t  NULL, get_clidr_el1 },\n\t{ Op0(0b11), Op1(0b001), CRn(0b0000), CRm(0b0000), Op2(0b111),\n\t  NULL, get_aidr_el1 },\n\t{ Op0(0b11), Op1(0b011), CRn(0b0000), CRm(0b0000), Op2(0b001),\n\t  NULL, get_ctr_el0 },\n};\n\nstatic int reg_from_user(u64 *val, const void __user *uaddr, u64 id)\n{\n\tif (copy_from_user(val, uaddr, KVM_REG_SIZE(id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int reg_to_user(void __user *uaddr, const u64 *val, u64 id)\n{\n\tif (copy_to_user(uaddr, val, KVM_REG_SIZE(id)) != 0)\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int get_invariant_sys_reg(u64 id, void __user *uaddr)\n{\n\tstruct sys_reg_params params;\n\tconst struct sys_reg_desc *r;\n\n\tif (!index_to_params(id, &params))\n\t\treturn -ENOENT;\n\n\tr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\n\tif (!r)\n\t\treturn -ENOENT;\n\n\treturn reg_to_user(uaddr, &r->val, id);\n}\n\nstatic int set_invariant_sys_reg(u64 id, void __user *uaddr)\n{\n\tstruct sys_reg_params params;\n\tconst struct sys_reg_desc *r;\n\tint err;\n\tu64 val = 0; /* Make sure high bits are 0 for 32-bit regs */\n\n\tif (!index_to_params(id, &params))\n\t\treturn -ENOENT;\n\tr = find_reg(&params, invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs));\n\tif (!r)\n\t\treturn -ENOENT;\n\n\terr = reg_from_user(&val, uaddr, id);\n\tif (err)\n\t\treturn err;\n\n\t/* This is what we mean by invariant: you can't change it. */\n\tif (r->val != val)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic bool is_valid_cache(u32 val)\n{\n\tu32 level, ctype;\n\n\tif (val >= CSSELR_MAX)\n\t\treturn false;\n\n\t/* Bottom bit is Instruction or Data bit.  Next 3 bits are level. */\n\tlevel = (val >> 1);\n\tctype = (cache_levels >> (level * 3)) & 7;\n\n\tswitch (ctype) {\n\tcase 0: /* No cache */\n\t\treturn false;\n\tcase 1: /* Instruction cache only */\n\t\treturn (val & 1);\n\tcase 2: /* Data cache only */\n\tcase 4: /* Unified cache */\n\t\treturn !(val & 1);\n\tcase 3: /* Separate instruction and data caches */\n\t\treturn true;\n\tdefault: /* Reserved: we can't know instruction or data. */\n\t\treturn false;\n\t}\n}\n\nstatic int demux_c15_get(u64 id, void __user *uaddr)\n{\n\tu32 val;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\treturn put_user(get_ccsidr(val), uval);\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n\nstatic int demux_c15_set(u64 id, void __user *uaddr)\n{\n\tu32 val, newval;\n\tu32 __user *uval = uaddr;\n\n\t/* Fail if we have unknown bits set. */\n\tif (id & ~(KVM_REG_ARCH_MASK|KVM_REG_SIZE_MASK|KVM_REG_ARM_COPROC_MASK\n\t\t   | ((1 << KVM_REG_ARM_COPROC_SHIFT)-1)))\n\t\treturn -ENOENT;\n\n\tswitch (id & KVM_REG_ARM_DEMUX_ID_MASK) {\n\tcase KVM_REG_ARM_DEMUX_ID_CCSIDR:\n\t\tif (KVM_REG_SIZE(id) != 4)\n\t\t\treturn -ENOENT;\n\t\tval = (id & KVM_REG_ARM_DEMUX_VAL_MASK)\n\t\t\t>> KVM_REG_ARM_DEMUX_VAL_SHIFT;\n\t\tif (!is_valid_cache(val))\n\t\t\treturn -ENOENT;\n\n\t\tif (get_user(newval, uval))\n\t\t\treturn -EFAULT;\n\n\t\t/* This is also invariant: you can't change it. */\n\t\tif (newval != get_ccsidr(val))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n}\n\nint kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_get(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn get_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->get_user)\n\t\treturn (r->get_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_to_user(uaddr, &vcpu_sys_reg(vcpu, r->reg), reg->id);\n}\n\nint kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tconst struct sys_reg_desc *r;\n\tvoid __user *uaddr = (void __user *)(unsigned long)reg->addr;\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_DEMUX)\n\t\treturn demux_c15_set(reg->id, uaddr);\n\n\tif (KVM_REG_SIZE(reg->id) != sizeof(__u64))\n\t\treturn -ENOENT;\n\n\tr = index_to_sys_reg_desc(vcpu, reg->id);\n\tif (!r)\n\t\treturn set_invariant_sys_reg(reg->id, uaddr);\n\n\tif (r->set_user)\n\t\treturn (r->set_user)(vcpu, r, reg, uaddr);\n\n\treturn reg_from_user(&vcpu_sys_reg(vcpu, r->reg), uaddr, reg->id);\n}\n\nstatic unsigned int num_demux_regs(void)\n{\n\tunsigned int i, count = 0;\n\n\tfor (i = 0; i < CSSELR_MAX; i++)\n\t\tif (is_valid_cache(i))\n\t\t\tcount++;\n\n\treturn count;\n}\n\nstatic int write_demux_regids(u64 __user *uindices)\n{\n\tu64 val = KVM_REG_ARM64 | KVM_REG_SIZE_U32 | KVM_REG_ARM_DEMUX;\n\tunsigned int i;\n\n\tval |= KVM_REG_ARM_DEMUX_ID_CCSIDR;\n\tfor (i = 0; i < CSSELR_MAX; i++) {\n\t\tif (!is_valid_cache(i))\n\t\t\tcontinue;\n\t\tif (put_user(val | i, uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\treturn 0;\n}\n\nstatic u64 sys_reg_to_index(const struct sys_reg_desc *reg)\n{\n\treturn (KVM_REG_ARM64 | KVM_REG_SIZE_U64 |\n\t\tKVM_REG_ARM64_SYSREG |\n\t\t(reg->Op0 << KVM_REG_ARM64_SYSREG_OP0_SHIFT) |\n\t\t(reg->Op1 << KVM_REG_ARM64_SYSREG_OP1_SHIFT) |\n\t\t(reg->CRn << KVM_REG_ARM64_SYSREG_CRN_SHIFT) |\n\t\t(reg->CRm << KVM_REG_ARM64_SYSREG_CRM_SHIFT) |\n\t\t(reg->Op2 << KVM_REG_ARM64_SYSREG_OP2_SHIFT));\n}\n\nstatic bool copy_reg_to_user(const struct sys_reg_desc *reg, u64 __user **uind)\n{\n\tif (!*uind)\n\t\treturn true;\n\n\tif (put_user(sys_reg_to_index(reg), *uind))\n\t\treturn false;\n\n\t(*uind)++;\n\treturn true;\n}\n\n/* Assumed ordered tables, see kvm_sys_reg_table_init. */\nstatic int walk_sys_regs(struct kvm_vcpu *vcpu, u64 __user *uind)\n{\n\tconst struct sys_reg_desc *i1, *i2, *end1, *end2;\n\tunsigned int total = 0;\n\tsize_t num;\n\n\t/* We check for duplicates here, to allow arch-specific overrides. */\n\ti1 = get_target_table(vcpu->arch.target, true, &num);\n\tend1 = i1 + num;\n\ti2 = sys_reg_descs;\n\tend2 = sys_reg_descs + ARRAY_SIZE(sys_reg_descs);\n\n\tBUG_ON(i1 == end1 || i2 == end2);\n\n\t/* Walk carefully, as both tables may refer to the same register. */\n\twhile (i1 || i2) {\n\t\tint cmp = cmp_sys_reg(i1, i2);\n\t\t/* target-specific overrides generic entry. */\n\t\tif (cmp <= 0) {\n\t\t\t/* Ignore registers we trap but don't save. */\n\t\t\tif (i1->reg) {\n\t\t\t\tif (!copy_reg_to_user(i1, &uind))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t} else {\n\t\t\t/* Ignore registers we trap but don't save. */\n\t\t\tif (i2->reg) {\n\t\t\t\tif (!copy_reg_to_user(i2, &uind))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\ttotal++;\n\t\t\t}\n\t\t}\n\n\t\tif (cmp <= 0 && ++i1 == end1)\n\t\t\ti1 = NULL;\n\t\tif (cmp >= 0 && ++i2 == end2)\n\t\t\ti2 = NULL;\n\t}\n\treturn total;\n}\n\nunsigned long kvm_arm_num_sys_reg_descs(struct kvm_vcpu *vcpu)\n{\n\treturn ARRAY_SIZE(invariant_sys_regs)\n\t\t+ num_demux_regs()\n\t\t+ walk_sys_regs(vcpu, (u64 __user *)NULL);\n}\n\nint kvm_arm_copy_sys_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tunsigned int i;\n\tint err;\n\n\t/* Then give them all the invariant registers' indices. */\n\tfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++) {\n\t\tif (put_user(sys_reg_to_index(&invariant_sys_regs[i]), uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\n\terr = walk_sys_regs(vcpu, uindices);\n\tif (err < 0)\n\t\treturn err;\n\tuindices += err;\n\n\treturn write_demux_regids(uindices);\n}\n\nstatic int check_sysreg_table(const struct sys_reg_desc *table, unsigned int n)\n{\n\tunsigned int i;\n\n\tfor (i = 1; i < n; i++) {\n\t\tif (cmp_sys_reg(&table[i-1], &table[i]) >= 0) {\n\t\t\tkvm_err(\"sys_reg table %p out of order (%d)\\n\", table, i - 1);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nvoid kvm_sys_reg_table_init(void)\n{\n\tunsigned int i;\n\tstruct sys_reg_desc clidr;\n\n\t/* Make sure tables are unique and in order. */\n\tBUG_ON(check_sysreg_table(sys_reg_descs, ARRAY_SIZE(sys_reg_descs)));\n\tBUG_ON(check_sysreg_table(cp14_regs, ARRAY_SIZE(cp14_regs)));\n\tBUG_ON(check_sysreg_table(cp14_64_regs, ARRAY_SIZE(cp14_64_regs)));\n\tBUG_ON(check_sysreg_table(cp15_regs, ARRAY_SIZE(cp15_regs)));\n\tBUG_ON(check_sysreg_table(cp15_64_regs, ARRAY_SIZE(cp15_64_regs)));\n\tBUG_ON(check_sysreg_table(invariant_sys_regs, ARRAY_SIZE(invariant_sys_regs)));\n\n\t/* We abuse the reset function to overwrite the table itself. */\n\tfor (i = 0; i < ARRAY_SIZE(invariant_sys_regs); i++)\n\t\tinvariant_sys_regs[i].reset(NULL, &invariant_sys_regs[i]);\n\n\t/*\n\t * CLIDR format is awkward, so clean it up.  See ARM B4.1.20:\n\t *\n\t *   If software reads the Cache Type fields from Ctype1\n\t *   upwards, once it has seen a value of 0b000, no caches\n\t *   exist at further-out levels of the hierarchy. So, for\n\t *   example, if Ctype3 is the first Cache Type field with a\n\t *   value of 0b000, the values of Ctype4 to Ctype7 must be\n\t *   ignored.\n\t */\n\tget_clidr_el1(NULL, &clidr); /* Ugly... */\n\tcache_levels = clidr.val;\n\tfor (i = 0; i < 7; i++)\n\t\tif (((cache_levels >> (i*3)) & 7) == 0)\n\t\t\tbreak;\n\t/* Clear all higher bits. */\n\tcache_levels &= (1 << (i*3))-1;\n}\n\n/**\n * kvm_reset_sys_regs - sets system registers to reset value\n * @vcpu: The VCPU pointer\n *\n * This function finds the right table above and sets the registers on the\n * virtual CPU struct to their architecturally defined reset values.\n */\nvoid kvm_reset_sys_regs(struct kvm_vcpu *vcpu)\n{\n\tsize_t num;\n\tconst struct sys_reg_desc *table;\n\n\t/* Catch someone adding a register without putting in reset entry. */\n\tmemset(&vcpu->arch.ctxt.sys_regs, 0x42, sizeof(vcpu->arch.ctxt.sys_regs));\n\n\t/* Generic chip reset first (so target could override). */\n\treset_sys_reg_descs(vcpu, sys_reg_descs, ARRAY_SIZE(sys_reg_descs));\n\n\ttable = get_target_table(vcpu->arch.target, true, &num);\n\treset_sys_reg_descs(vcpu, table, num);\n\n\tfor (num = 1; num < NR_SYS_REGS; num++)\n\t\tif (vcpu_sys_reg(vcpu, num) == 0x4242424242424242)\n\t\t\tpanic(\"Didn't reset vcpu_sys_reg(%zi)\", num);\n}\n"], "filenames": ["arch/arm64/kvm/sys_regs.c"], "buggy_code_start_loc": [600], "buggy_code_end_loc": [610], "fixing_code_start_loc": [600], "fixing_code_end_loc": [616], "type": "CWE-617", "message": "The access_pmu_evcntr function in arch/arm64/kvm/sys_regs.c in the Linux kernel before 4.8.11 allows privileged KVM guest OS users to cause a denial of service (assertion failure and host OS crash) by accessing the Performance Monitors Cycle Count Register (PMCCNTR).", "other": {"cve": {"id": "CVE-2017-12168", "sourceIdentifier": "secalert@redhat.com", "published": "2017-09-20T08:29:00.210", "lastModified": "2023-02-12T23:27:29.017", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The access_pmu_evcntr function in arch/arm64/kvm/sys_regs.c in the Linux kernel before 4.8.11 allows privileged KVM guest OS users to cause a denial of service (assertion failure and host OS crash) by accessing the Performance Monitors Cycle Count Register (PMCCNTR)."}, {"lang": "es", "value": "La funci\u00f3n access_pmu_evcntr en arch/arm64/kvm/sys_regs.c en el kernel Linux en versiones anteriores a la 4.8.11 permite que los usuarios KVM invitados del sistema operativo provoquen una denegaci\u00f3n de servicio (fallo de aserci\u00f3n y cierre inesperado del sistema operativo del host) accediendo a Performance Monitors Cycle Count Register (PMCCNTR)."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:H/UI:N/S:C/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.0, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.5, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "secalert@redhat.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.8.10", "matchCriteriaId": "BA4CF0D6-DBAC-4857-A977-D43B9018CE3D"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=9e3f7a29694049edd728e2400ab57ad7553e5aa9", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.8.11", "source": "secalert@redhat.com", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1492984", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/9e3f7a29694049edd728e2400ab57ad7553e5aa9", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/9e3f7a29694049edd728e2400ab57ad7553e5aa9"}}