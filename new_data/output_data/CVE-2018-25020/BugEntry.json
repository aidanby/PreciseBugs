{"buggy_code": ["/*\n * Linux Socket Filter - Kernel level socket filtering\n *\n * Based on the design of the Berkeley Packet Filter. The new\n * internal format has been designed by PLUMgrid:\n *\n *\tCopyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com\n *\n * Authors:\n *\n *\tJay Schulist <jschlst@samba.org>\n *\tAlexei Starovoitov <ast@plumgrid.com>\n *\tDaniel Borkmann <dborkman@redhat.com>\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n *\n * Andi Kleen - Fix a few bad bugs and races.\n * Kris Katterjohn - Added many additional checks in bpf_check_classic()\n */\n\n#include <linux/filter.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/random.h>\n#include <linux/moduleloader.h>\n#include <linux/bpf.h>\n#include <linux/frame.h>\n#include <linux/rbtree_latch.h>\n#include <linux/kallsyms.h>\n#include <linux/rcupdate.h>\n\n#include <asm/unaligned.h>\n\n/* Registers */\n#define BPF_R0\tregs[BPF_REG_0]\n#define BPF_R1\tregs[BPF_REG_1]\n#define BPF_R2\tregs[BPF_REG_2]\n#define BPF_R3\tregs[BPF_REG_3]\n#define BPF_R4\tregs[BPF_REG_4]\n#define BPF_R5\tregs[BPF_REG_5]\n#define BPF_R6\tregs[BPF_REG_6]\n#define BPF_R7\tregs[BPF_REG_7]\n#define BPF_R8\tregs[BPF_REG_8]\n#define BPF_R9\tregs[BPF_REG_9]\n#define BPF_R10\tregs[BPF_REG_10]\n\n/* Named registers */\n#define DST\tregs[insn->dst_reg]\n#define SRC\tregs[insn->src_reg]\n#define FP\tregs[BPF_REG_FP]\n#define ARG1\tregs[BPF_REG_ARG1]\n#define CTX\tregs[BPF_REG_CTX]\n#define IMM\tinsn->imm\n\n/* No hurry in this branch\n *\n * Exported for the bpf jit load helper.\n */\nvoid *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb, int k, unsigned int size)\n{\n\tu8 *ptr = NULL;\n\n\tif (k >= SKF_NET_OFF)\n\t\tptr = skb_network_header(skb) + k - SKF_NET_OFF;\n\telse if (k >= SKF_LL_OFF)\n\t\tptr = skb_mac_header(skb) + k - SKF_LL_OFF;\n\n\tif (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))\n\t\treturn ptr;\n\n\treturn NULL;\n}\n\nstruct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog_aux *aux;\n\tstruct bpf_prog *fp;\n\n\tsize = round_up(size, PAGE_SIZE);\n\tfp = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\tif (fp == NULL)\n\t\treturn NULL;\n\n\taux = kzalloc(sizeof(*aux), GFP_KERNEL | gfp_extra_flags);\n\tif (aux == NULL) {\n\t\tvfree(fp);\n\t\treturn NULL;\n\t}\n\n\tfp->pages = size / PAGE_SIZE;\n\tfp->aux = aux;\n\tfp->aux->prog = fp;\n\tfp->jit_requested = ebpf_jit_enabled();\n\n\tINIT_LIST_HEAD_RCU(&fp->aux->ksym_lnode);\n\n\treturn fp;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_alloc);\n\nstruct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,\n\t\t\t\t  gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog *fp;\n\tu32 pages, delta;\n\tint ret;\n\n\tBUG_ON(fp_old == NULL);\n\n\tsize = round_up(size, PAGE_SIZE);\n\tpages = size / PAGE_SIZE;\n\tif (pages <= fp_old->pages)\n\t\treturn fp_old;\n\n\tdelta = pages - fp_old->pages;\n\tret = __bpf_prog_charge(fp_old->aux->user, delta);\n\tif (ret)\n\t\treturn NULL;\n\n\tfp = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\tif (fp == NULL) {\n\t\t__bpf_prog_uncharge(fp_old->aux->user, delta);\n\t} else {\n\t\tmemcpy(fp, fp_old, fp_old->pages * PAGE_SIZE);\n\t\tfp->pages = pages;\n\t\tfp->aux->prog = fp;\n\n\t\t/* We keep fp->aux from fp_old around in the new\n\t\t * reallocated structure.\n\t\t */\n\t\tfp_old->aux = NULL;\n\t\t__bpf_prog_free(fp_old);\n\t}\n\n\treturn fp;\n}\n\nvoid __bpf_prog_free(struct bpf_prog *fp)\n{\n\tkfree(fp->aux);\n\tvfree(fp);\n}\n\nint bpf_prog_calc_tag(struct bpf_prog *fp)\n{\n\tconst u32 bits_offset = SHA_MESSAGE_BYTES - sizeof(__be64);\n\tu32 raw_size = bpf_prog_tag_scratch_size(fp);\n\tu32 digest[SHA_DIGEST_WORDS];\n\tu32 ws[SHA_WORKSPACE_WORDS];\n\tu32 i, bsize, psize, blocks;\n\tstruct bpf_insn *dst;\n\tbool was_ld_map;\n\tu8 *raw, *todo;\n\t__be32 *result;\n\t__be64 *bits;\n\n\traw = vmalloc(raw_size);\n\tif (!raw)\n\t\treturn -ENOMEM;\n\n\tsha_init(digest);\n\tmemset(ws, 0, sizeof(ws));\n\n\t/* We need to take out the map fd for the digest calculation\n\t * since they are unstable from user space side.\n\t */\n\tdst = (void *)raw;\n\tfor (i = 0, was_ld_map = false; i < fp->len; i++) {\n\t\tdst[i] = fp->insnsi[i];\n\t\tif (!was_ld_map &&\n\t\t    dst[i].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    dst[i].src_reg == BPF_PSEUDO_MAP_FD) {\n\t\t\twas_ld_map = true;\n\t\t\tdst[i].imm = 0;\n\t\t} else if (was_ld_map &&\n\t\t\t   dst[i].code == 0 &&\n\t\t\t   dst[i].dst_reg == 0 &&\n\t\t\t   dst[i].src_reg == 0 &&\n\t\t\t   dst[i].off == 0) {\n\t\t\twas_ld_map = false;\n\t\t\tdst[i].imm = 0;\n\t\t} else {\n\t\t\twas_ld_map = false;\n\t\t}\n\t}\n\n\tpsize = bpf_prog_insn_size(fp);\n\tmemset(&raw[psize], 0, raw_size - psize);\n\traw[psize++] = 0x80;\n\n\tbsize  = round_up(psize, SHA_MESSAGE_BYTES);\n\tblocks = bsize / SHA_MESSAGE_BYTES;\n\ttodo   = raw;\n\tif (bsize - psize >= sizeof(__be64)) {\n\t\tbits = (__be64 *)(todo + bsize - sizeof(__be64));\n\t} else {\n\t\tbits = (__be64 *)(todo + bsize + bits_offset);\n\t\tblocks++;\n\t}\n\t*bits = cpu_to_be64((psize - 1) << 3);\n\n\twhile (blocks--) {\n\t\tsha_transform(digest, todo, ws);\n\t\ttodo += SHA_MESSAGE_BYTES;\n\t}\n\n\tresult = (__force __be32 *)digest;\n\tfor (i = 0; i < SHA_DIGEST_WORDS; i++)\n\t\tresult[i] = cpu_to_be32(digest[i]);\n\tmemcpy(fp->tag, result, sizeof(fp->tag));\n\n\tvfree(raw);\n\treturn 0;\n}\n\nstatic void bpf_adj_branches(struct bpf_prog *prog, u32 pos, u32 delta)\n{\n\tstruct bpf_insn *insn = prog->insnsi;\n\tu32 i, insn_cnt = prog->len;\n\tbool pseudo_call;\n\tu8 code;\n\tint off;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tcode = insn->code;\n\t\tif (BPF_CLASS(code) != BPF_JMP)\n\t\t\tcontinue;\n\t\tif (BPF_OP(code) == BPF_EXIT)\n\t\t\tcontinue;\n\t\tif (BPF_OP(code) == BPF_CALL) {\n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\tpseudo_call = true;\n\t\t\telse\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\tpseudo_call = false;\n\t\t}\n\t\toff = pseudo_call ? insn->imm : insn->off;\n\n\t\t/* Adjust offset of jmps if we cross boundaries. */\n\t\tif (i < pos && i + off + 1 > pos)\n\t\t\toff += delta;\n\t\telse if (i > pos + delta && i + off + 1 <= pos + delta)\n\t\t\toff -= delta;\n\n\t\tif (pseudo_call)\n\t\t\tinsn->imm = off;\n\t\telse\n\t\t\tinsn->off = off;\n\t}\n}\n\nstruct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,\n\t\t\t\t       const struct bpf_insn *patch, u32 len)\n{\n\tu32 insn_adj_cnt, insn_rest, insn_delta = len - 1;\n\tstruct bpf_prog *prog_adj;\n\n\t/* Since our patchlet doesn't expand the image, we're done. */\n\tif (insn_delta == 0) {\n\t\tmemcpy(prog->insnsi + off, patch, sizeof(*patch));\n\t\treturn prog;\n\t}\n\n\tinsn_adj_cnt = prog->len + insn_delta;\n\n\t/* Several new instructions need to be inserted. Make room\n\t * for them. Likely, there's no need for a new allocation as\n\t * last page could have large enough tailroom.\n\t */\n\tprog_adj = bpf_prog_realloc(prog, bpf_prog_size(insn_adj_cnt),\n\t\t\t\t    GFP_USER);\n\tif (!prog_adj)\n\t\treturn NULL;\n\n\tprog_adj->len = insn_adj_cnt;\n\n\t/* Patching happens in 3 steps:\n\t *\n\t * 1) Move over tail of insnsi from next instruction onwards,\n\t *    so we can patch the single target insn with one or more\n\t *    new ones (patching is always from 1 to n insns, n > 0).\n\t * 2) Inject new instructions at the target location.\n\t * 3) Adjust branch offsets if necessary.\n\t */\n\tinsn_rest = insn_adj_cnt - off - len;\n\n\tmemmove(prog_adj->insnsi + off + len, prog_adj->insnsi + off + 1,\n\t\tsizeof(*patch) * insn_rest);\n\tmemcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);\n\n\tbpf_adj_branches(prog_adj, off, insn_delta);\n\n\treturn prog_adj;\n}\n\n#ifdef CONFIG_BPF_JIT\n/* All BPF JIT sysctl knobs here. */\nint bpf_jit_enable   __read_mostly = IS_BUILTIN(CONFIG_BPF_JIT_ALWAYS_ON);\nint bpf_jit_harden   __read_mostly;\nint bpf_jit_kallsyms __read_mostly;\n\nstatic __always_inline void\nbpf_get_prog_addr_region(const struct bpf_prog *prog,\n\t\t\t unsigned long *symbol_start,\n\t\t\t unsigned long *symbol_end)\n{\n\tconst struct bpf_binary_header *hdr = bpf_jit_binary_hdr(prog);\n\tunsigned long addr = (unsigned long)hdr;\n\n\tWARN_ON_ONCE(!bpf_prog_ebpf_jited(prog));\n\n\t*symbol_start = addr;\n\t*symbol_end   = addr + hdr->pages * PAGE_SIZE;\n}\n\nstatic void bpf_get_prog_name(const struct bpf_prog *prog, char *sym)\n{\n\tconst char *end = sym + KSYM_NAME_LEN;\n\n\tBUILD_BUG_ON(sizeof(\"bpf_prog_\") +\n\t\t     sizeof(prog->tag) * 2 +\n\t\t     /* name has been null terminated.\n\t\t      * We should need +1 for the '_' preceding\n\t\t      * the name.  However, the null character\n\t\t      * is double counted between the name and the\n\t\t      * sizeof(\"bpf_prog_\") above, so we omit\n\t\t      * the +1 here.\n\t\t      */\n\t\t     sizeof(prog->aux->name) > KSYM_NAME_LEN);\n\n\tsym += snprintf(sym, KSYM_NAME_LEN, \"bpf_prog_\");\n\tsym  = bin2hex(sym, prog->tag, sizeof(prog->tag));\n\tif (prog->aux->name[0])\n\t\tsnprintf(sym, (size_t)(end - sym), \"_%s\", prog->aux->name);\n\telse\n\t\t*sym = 0;\n}\n\nstatic __always_inline unsigned long\nbpf_get_prog_addr_start(struct latch_tree_node *n)\n{\n\tunsigned long symbol_start, symbol_end;\n\tconst struct bpf_prog_aux *aux;\n\n\taux = container_of(n, struct bpf_prog_aux, ksym_tnode);\n\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\n\treturn symbol_start;\n}\n\nstatic __always_inline bool bpf_tree_less(struct latch_tree_node *a,\n\t\t\t\t\t  struct latch_tree_node *b)\n{\n\treturn bpf_get_prog_addr_start(a) < bpf_get_prog_addr_start(b);\n}\n\nstatic __always_inline int bpf_tree_comp(void *key, struct latch_tree_node *n)\n{\n\tunsigned long val = (unsigned long)key;\n\tunsigned long symbol_start, symbol_end;\n\tconst struct bpf_prog_aux *aux;\n\n\taux = container_of(n, struct bpf_prog_aux, ksym_tnode);\n\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\n\tif (val < symbol_start)\n\t\treturn -1;\n\tif (val >= symbol_end)\n\t\treturn  1;\n\n\treturn 0;\n}\n\nstatic const struct latch_tree_ops bpf_tree_ops = {\n\t.less\t= bpf_tree_less,\n\t.comp\t= bpf_tree_comp,\n};\n\nstatic DEFINE_SPINLOCK(bpf_lock);\nstatic LIST_HEAD(bpf_kallsyms);\nstatic struct latch_tree_root bpf_tree __cacheline_aligned;\n\nstatic void bpf_prog_ksym_node_add(struct bpf_prog_aux *aux)\n{\n\tWARN_ON_ONCE(!list_empty(&aux->ksym_lnode));\n\tlist_add_tail_rcu(&aux->ksym_lnode, &bpf_kallsyms);\n\tlatch_tree_insert(&aux->ksym_tnode, &bpf_tree, &bpf_tree_ops);\n}\n\nstatic void bpf_prog_ksym_node_del(struct bpf_prog_aux *aux)\n{\n\tif (list_empty(&aux->ksym_lnode))\n\t\treturn;\n\n\tlatch_tree_erase(&aux->ksym_tnode, &bpf_tree, &bpf_tree_ops);\n\tlist_del_rcu(&aux->ksym_lnode);\n}\n\nstatic bool bpf_prog_kallsyms_candidate(const struct bpf_prog *fp)\n{\n\treturn fp->jited && !bpf_prog_was_classic(fp);\n}\n\nstatic bool bpf_prog_kallsyms_verify_off(const struct bpf_prog *fp)\n{\n\treturn list_empty(&fp->aux->ksym_lnode) ||\n\t       fp->aux->ksym_lnode.prev == LIST_POISON2;\n}\n\nvoid bpf_prog_kallsyms_add(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp) ||\n\t    !capable(CAP_SYS_ADMIN))\n\t\treturn;\n\n\tspin_lock_bh(&bpf_lock);\n\tbpf_prog_ksym_node_add(fp->aux);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nvoid bpf_prog_kallsyms_del(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp))\n\t\treturn;\n\n\tspin_lock_bh(&bpf_lock);\n\tbpf_prog_ksym_node_del(fp->aux);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nstatic struct bpf_prog *bpf_prog_kallsyms_find(unsigned long addr)\n{\n\tstruct latch_tree_node *n;\n\n\tif (!bpf_jit_kallsyms_enabled())\n\t\treturn NULL;\n\n\tn = latch_tree_find((void *)addr, &bpf_tree, &bpf_tree_ops);\n\treturn n ?\n\t       container_of(n, struct bpf_prog_aux, ksym_tnode)->prog :\n\t       NULL;\n}\n\nconst char *__bpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t\t\t unsigned long *off, char *sym)\n{\n\tunsigned long symbol_start, symbol_end;\n\tstruct bpf_prog *prog;\n\tchar *ret = NULL;\n\n\trcu_read_lock();\n\tprog = bpf_prog_kallsyms_find(addr);\n\tif (prog) {\n\t\tbpf_get_prog_addr_region(prog, &symbol_start, &symbol_end);\n\t\tbpf_get_prog_name(prog, sym);\n\n\t\tret = sym;\n\t\tif (size)\n\t\t\t*size = symbol_end - symbol_start;\n\t\tif (off)\n\t\t\t*off  = addr - symbol_start;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nbool is_bpf_text_address(unsigned long addr)\n{\n\tbool ret;\n\n\trcu_read_lock();\n\tret = bpf_prog_kallsyms_find(addr) != NULL;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nint bpf_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t    char *sym)\n{\n\tunsigned long symbol_start, symbol_end;\n\tstruct bpf_prog_aux *aux;\n\tunsigned int it = 0;\n\tint ret = -ERANGE;\n\n\tif (!bpf_jit_kallsyms_enabled())\n\t\treturn ret;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(aux, &bpf_kallsyms, ksym_lnode) {\n\t\tif (it++ != symnum)\n\t\t\tcontinue;\n\n\t\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\t\tbpf_get_prog_name(aux->prog, sym);\n\n\t\t*value = symbol_start;\n\t\t*type  = BPF_SYM_ELF_TYPE;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstruct bpf_binary_header *\nbpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,\n\t\t     unsigned int alignment,\n\t\t     bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tstruct bpf_binary_header *hdr;\n\tunsigned int size, hole, start;\n\n\t/* Most of BPF filters are really small, but if some of them\n\t * fill a page, allow at least 128 extra bytes to insert a\n\t * random section of illegal instructions.\n\t */\n\tsize = round_up(proglen + sizeof(*hdr) + 128, PAGE_SIZE);\n\thdr = module_alloc(size);\n\tif (hdr == NULL)\n\t\treturn NULL;\n\n\t/* Fill space with illegal/arch-dep instructions. */\n\tbpf_fill_ill_insns(hdr, size);\n\n\thdr->pages = size / PAGE_SIZE;\n\thole = min_t(unsigned int, size - (proglen + sizeof(*hdr)),\n\t\t     PAGE_SIZE - sizeof(*hdr));\n\tstart = (get_random_int() % hole) & ~(alignment - 1);\n\n\t/* Leave a random number of instructions before BPF code. */\n\t*image_ptr = &hdr->image[start];\n\n\treturn hdr;\n}\n\nvoid bpf_jit_binary_free(struct bpf_binary_header *hdr)\n{\n\tmodule_memfree(hdr);\n}\n\n/* This symbol is only overridden by archs that have different\n * requirements than the usual eBPF JITs, f.e. when they only\n * implement cBPF JIT, do not set images read-only, etc.\n */\nvoid __weak bpf_jit_free(struct bpf_prog *fp)\n{\n\tif (fp->jited) {\n\t\tstruct bpf_binary_header *hdr = bpf_jit_binary_hdr(fp);\n\n\t\tbpf_jit_binary_unlock_ro(hdr);\n\t\tbpf_jit_binary_free(hdr);\n\n\t\tWARN_ON_ONCE(!bpf_prog_kallsyms_verify_off(fp));\n\t}\n\n\tbpf_prog_unlock_free(fp);\n}\n\nstatic int bpf_jit_blind_insn(const struct bpf_insn *from,\n\t\t\t      const struct bpf_insn *aux,\n\t\t\t      struct bpf_insn *to_buff)\n{\n\tstruct bpf_insn *to = to_buff;\n\tu32 imm_rnd = get_random_int();\n\ts16 off;\n\n\tBUILD_BUG_ON(BPF_REG_AX  + 1 != MAX_BPF_JIT_REG);\n\tBUILD_BUG_ON(MAX_BPF_REG + 1 != MAX_BPF_JIT_REG);\n\n\tif (from->imm == 0 &&\n\t    (from->code == (BPF_ALU   | BPF_MOV | BPF_K) ||\n\t     from->code == (BPF_ALU64 | BPF_MOV | BPF_K))) {\n\t\t*to++ = BPF_ALU64_REG(BPF_XOR, from->dst_reg, from->dst_reg);\n\t\tgoto out;\n\t}\n\n\tswitch (from->code) {\n\tcase BPF_ALU | BPF_ADD | BPF_K:\n\tcase BPF_ALU | BPF_SUB | BPF_K:\n\tcase BPF_ALU | BPF_AND | BPF_K:\n\tcase BPF_ALU | BPF_OR  | BPF_K:\n\tcase BPF_ALU | BPF_XOR | BPF_K:\n\tcase BPF_ALU | BPF_MUL | BPF_K:\n\tcase BPF_ALU | BPF_MOV | BPF_K:\n\tcase BPF_ALU | BPF_DIV | BPF_K:\n\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU32_REG(from->code, from->dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_ALU64 | BPF_ADD | BPF_K:\n\tcase BPF_ALU64 | BPF_SUB | BPF_K:\n\tcase BPF_ALU64 | BPF_AND | BPF_K:\n\tcase BPF_ALU64 | BPF_OR  | BPF_K:\n\tcase BPF_ALU64 | BPF_XOR | BPF_K:\n\tcase BPF_ALU64 | BPF_MUL | BPF_K:\n\tcase BPF_ALU64 | BPF_MOV | BPF_K:\n\tcase BPF_ALU64 | BPF_DIV | BPF_K:\n\tcase BPF_ALU64 | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_REG(from->code, from->dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_JMP | BPF_JEQ  | BPF_K:\n\tcase BPF_JMP | BPF_JNE  | BPF_K:\n\tcase BPF_JMP | BPF_JGT  | BPF_K:\n\tcase BPF_JMP | BPF_JLT  | BPF_K:\n\tcase BPF_JMP | BPF_JGE  | BPF_K:\n\tcase BPF_JMP | BPF_JLE  | BPF_K:\n\tcase BPF_JMP | BPF_JSGT | BPF_K:\n\tcase BPF_JMP | BPF_JSLT | BPF_K:\n\tcase BPF_JMP | BPF_JSGE | BPF_K:\n\tcase BPF_JMP | BPF_JSLE | BPF_K:\n\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\t/* Accommodate for extra offset in case of a backjump. */\n\t\toff = from->off;\n\t\tif (off < 0)\n\t\t\toff -= 2;\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_JMP_REG(from->code, from->dst_reg, BPF_REG_AX, off);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_ABS | BPF_W:\n\tcase BPF_LD | BPF_ABS | BPF_H:\n\tcase BPF_LD | BPF_ABS | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_IND | BPF_W:\n\tcase BPF_LD | BPF_IND | BPF_H:\n\tcase BPF_LD | BPF_IND | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU32_REG(BPF_ADD, BPF_REG_AX, from->src_reg);\n\t\t*to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_IMM | BPF_DW:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[1].imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\t\t*to++ = BPF_ALU64_REG(BPF_MOV, aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\tcase 0: /* Part 2 of BPF_LD | BPF_IMM | BPF_DW. */\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[0].imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_REG(BPF_OR,  aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_ST | BPF_MEM | BPF_DW:\n\tcase BPF_ST | BPF_MEM | BPF_W:\n\tcase BPF_ST | BPF_MEM | BPF_H:\n\tcase BPF_ST | BPF_MEM | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_STX_MEM(from->code, from->dst_reg, BPF_REG_AX, from->off);\n\t\tbreak;\n\t}\nout:\n\treturn to - to_buff;\n}\n\nstatic struct bpf_prog *bpf_prog_clone_create(struct bpf_prog *fp_other,\n\t\t\t\t\t      gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog *fp;\n\n\tfp = __vmalloc(fp_other->pages * PAGE_SIZE, gfp_flags, PAGE_KERNEL);\n\tif (fp != NULL) {\n\t\t/* aux->prog still points to the fp_other one, so\n\t\t * when promoting the clone to the real program,\n\t\t * this still needs to be adapted.\n\t\t */\n\t\tmemcpy(fp, fp_other, fp_other->pages * PAGE_SIZE);\n\t}\n\n\treturn fp;\n}\n\nstatic void bpf_prog_clone_free(struct bpf_prog *fp)\n{\n\t/* aux was stolen by the other clone, so we cannot free\n\t * it from this path! It will be freed eventually by the\n\t * other program on release.\n\t *\n\t * At this point, we don't need a deferred release since\n\t * clone is guaranteed to not be locked.\n\t */\n\tfp->aux = NULL;\n\t__bpf_prog_free(fp);\n}\n\nvoid bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other)\n{\n\t/* We have to repoint aux->prog to self, as we don't\n\t * know whether fp here is the clone or the original.\n\t */\n\tfp->aux->prog = fp;\n\tbpf_prog_clone_free(fp_other);\n}\n\nstruct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *prog)\n{\n\tstruct bpf_insn insn_buff[16], aux[2];\n\tstruct bpf_prog *clone, *tmp;\n\tint insn_delta, insn_cnt;\n\tstruct bpf_insn *insn;\n\tint i, rewritten;\n\n\tif (!bpf_jit_blinding_enabled(prog) || prog->blinded)\n\t\treturn prog;\n\n\tclone = bpf_prog_clone_create(prog, GFP_USER);\n\tif (!clone)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinsn_cnt = clone->len;\n\tinsn = clone->insnsi;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\t/* We temporarily need to hold the original ld64 insn\n\t\t * so that we can still access the first part in the\n\t\t * second blinding run.\n\t\t */\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    insn[1].code == 0)\n\t\t\tmemcpy(aux, insn, sizeof(aux));\n\n\t\trewritten = bpf_jit_blind_insn(insn, aux, insn_buff);\n\t\tif (!rewritten)\n\t\t\tcontinue;\n\n\t\ttmp = bpf_patch_insn_single(clone, i, insn_buff, rewritten);\n\t\tif (!tmp) {\n\t\t\t/* Patching may have repointed aux->prog during\n\t\t\t * realloc from the original one, so we need to\n\t\t\t * fix it up here on error.\n\t\t\t */\n\t\t\tbpf_jit_prog_release_other(prog, clone);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\tclone = tmp;\n\t\tinsn_delta = rewritten - 1;\n\n\t\t/* Walk new program and skip insns we just inserted. */\n\t\tinsn = clone->insnsi + i + insn_delta;\n\t\tinsn_cnt += insn_delta;\n\t\ti        += insn_delta;\n\t}\n\n\tclone->blinded = 1;\n\treturn clone;\n}\n#endif /* CONFIG_BPF_JIT */\n\n/* Base function for offset calculation. Needs to go into .text section,\n * therefore keeping it non-static as well; will also be used by JITs\n * anyway later on, so do not let the compiler omit it. This also needs\n * to go into kallsyms for correlation from e.g. bpftool, so naming\n * must not change.\n */\nnoinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__bpf_call_base);\n\n/* All UAPI available opcodes. */\n#define BPF_INSN_MAP(INSN_2, INSN_3)\t\t\\\n\t/* 32 bit ALU operations. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(ALU, ADD, X),\t\t\t\\\n\tINSN_3(ALU, SUB, X),\t\t\t\\\n\tINSN_3(ALU, AND, X),\t\t\t\\\n\tINSN_3(ALU, OR,  X),\t\t\t\\\n\tINSN_3(ALU, LSH, X),\t\t\t\\\n\tINSN_3(ALU, RSH, X),\t\t\t\\\n\tINSN_3(ALU, XOR, X),\t\t\t\\\n\tINSN_3(ALU, MUL, X),\t\t\t\\\n\tINSN_3(ALU, MOV, X),\t\t\t\\\n\tINSN_3(ALU, DIV, X),\t\t\t\\\n\tINSN_3(ALU, MOD, X),\t\t\t\\\n\tINSN_2(ALU, NEG),\t\t\t\\\n\tINSN_3(ALU, END, TO_BE),\t\t\\\n\tINSN_3(ALU, END, TO_LE),\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ALU, ADD, K),\t\t\t\\\n\tINSN_3(ALU, SUB, K),\t\t\t\\\n\tINSN_3(ALU, AND, K),\t\t\t\\\n\tINSN_3(ALU, OR,  K),\t\t\t\\\n\tINSN_3(ALU, LSH, K),\t\t\t\\\n\tINSN_3(ALU, RSH, K),\t\t\t\\\n\tINSN_3(ALU, XOR, K),\t\t\t\\\n\tINSN_3(ALU, MUL, K),\t\t\t\\\n\tINSN_3(ALU, MOV, K),\t\t\t\\\n\tINSN_3(ALU, DIV, K),\t\t\t\\\n\tINSN_3(ALU, MOD, K),\t\t\t\\\n\t/* 64 bit ALU operations. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(ALU64, ADD,  X),\t\t\t\\\n\tINSN_3(ALU64, SUB,  X),\t\t\t\\\n\tINSN_3(ALU64, AND,  X),\t\t\t\\\n\tINSN_3(ALU64, OR,   X),\t\t\t\\\n\tINSN_3(ALU64, LSH,  X),\t\t\t\\\n\tINSN_3(ALU64, RSH,  X),\t\t\t\\\n\tINSN_3(ALU64, XOR,  X),\t\t\t\\\n\tINSN_3(ALU64, MUL,  X),\t\t\t\\\n\tINSN_3(ALU64, MOV,  X),\t\t\t\\\n\tINSN_3(ALU64, ARSH, X),\t\t\t\\\n\tINSN_3(ALU64, DIV,  X),\t\t\t\\\n\tINSN_3(ALU64, MOD,  X),\t\t\t\\\n\tINSN_2(ALU64, NEG),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ALU64, ADD,  K),\t\t\t\\\n\tINSN_3(ALU64, SUB,  K),\t\t\t\\\n\tINSN_3(ALU64, AND,  K),\t\t\t\\\n\tINSN_3(ALU64, OR,   K),\t\t\t\\\n\tINSN_3(ALU64, LSH,  K),\t\t\t\\\n\tINSN_3(ALU64, RSH,  K),\t\t\t\\\n\tINSN_3(ALU64, XOR,  K),\t\t\t\\\n\tINSN_3(ALU64, MUL,  K),\t\t\t\\\n\tINSN_3(ALU64, MOV,  K),\t\t\t\\\n\tINSN_3(ALU64, ARSH, K),\t\t\t\\\n\tINSN_3(ALU64, DIV,  K),\t\t\t\\\n\tINSN_3(ALU64, MOD,  K),\t\t\t\\\n\t/* Call instruction. */\t\t\t\\\n\tINSN_2(JMP, CALL),\t\t\t\\\n\t/* Exit instruction. */\t\t\t\\\n\tINSN_2(JMP, EXIT),\t\t\t\\\n\t/* Jump instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(JMP, JEQ,  X),\t\t\t\\\n\tINSN_3(JMP, JNE,  X),\t\t\t\\\n\tINSN_3(JMP, JGT,  X),\t\t\t\\\n\tINSN_3(JMP, JLT,  X),\t\t\t\\\n\tINSN_3(JMP, JGE,  X),\t\t\t\\\n\tINSN_3(JMP, JLE,  X),\t\t\t\\\n\tINSN_3(JMP, JSGT, X),\t\t\t\\\n\tINSN_3(JMP, JSLT, X),\t\t\t\\\n\tINSN_3(JMP, JSGE, X),\t\t\t\\\n\tINSN_3(JMP, JSLE, X),\t\t\t\\\n\tINSN_3(JMP, JSET, X),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(JMP, JEQ,  K),\t\t\t\\\n\tINSN_3(JMP, JNE,  K),\t\t\t\\\n\tINSN_3(JMP, JGT,  K),\t\t\t\\\n\tINSN_3(JMP, JLT,  K),\t\t\t\\\n\tINSN_3(JMP, JGE,  K),\t\t\t\\\n\tINSN_3(JMP, JLE,  K),\t\t\t\\\n\tINSN_3(JMP, JSGT, K),\t\t\t\\\n\tINSN_3(JMP, JSLT, K),\t\t\t\\\n\tINSN_3(JMP, JSGE, K),\t\t\t\\\n\tINSN_3(JMP, JSLE, K),\t\t\t\\\n\tINSN_3(JMP, JSET, K),\t\t\t\\\n\tINSN_2(JMP, JA),\t\t\t\\\n\t/* Store instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(STX, MEM,  B),\t\t\t\\\n\tINSN_3(STX, MEM,  H),\t\t\t\\\n\tINSN_3(STX, MEM,  W),\t\t\t\\\n\tINSN_3(STX, MEM,  DW),\t\t\t\\\n\tINSN_3(STX, XADD, W),\t\t\t\\\n\tINSN_3(STX, XADD, DW),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ST, MEM, B),\t\t\t\\\n\tINSN_3(ST, MEM, H),\t\t\t\\\n\tINSN_3(ST, MEM, W),\t\t\t\\\n\tINSN_3(ST, MEM, DW),\t\t\t\\\n\t/* Load instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(LDX, MEM, B),\t\t\t\\\n\tINSN_3(LDX, MEM, H),\t\t\t\\\n\tINSN_3(LDX, MEM, W),\t\t\t\\\n\tINSN_3(LDX, MEM, DW),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(LD, IMM, DW),\t\t\t\\\n\t/*   Misc (old cBPF carry-over). */\t\\\n\tINSN_3(LD, ABS, B),\t\t\t\\\n\tINSN_3(LD, ABS, H),\t\t\t\\\n\tINSN_3(LD, ABS, W),\t\t\t\\\n\tINSN_3(LD, IND, B),\t\t\t\\\n\tINSN_3(LD, IND, H),\t\t\t\\\n\tINSN_3(LD, IND, W)\n\nbool bpf_opcode_in_insntable(u8 code)\n{\n#define BPF_INSN_2_TBL(x, y)    [BPF_##x | BPF_##y] = true\n#define BPF_INSN_3_TBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = true\n\tstatic const bool public_insntable[256] = {\n\t\t[0 ... 255] = false,\n\t\t/* Now overwrite non-defaults ... */\n\t\tBPF_INSN_MAP(BPF_INSN_2_TBL, BPF_INSN_3_TBL),\n\t};\n#undef BPF_INSN_3_TBL\n#undef BPF_INSN_2_TBL\n\treturn public_insntable[code];\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n/**\n *\t__bpf_prog_run - run eBPF program on a given context\n *\t@ctx: is the data we are operating on\n *\t@insn: is the array of eBPF instructions\n *\n * Decode and execute eBPF instructions.\n */\nstatic u64 ___bpf_prog_run(u64 *regs, const struct bpf_insn *insn, u64 *stack)\n{\n\tu64 tmp;\n#define BPF_INSN_2_LBL(x, y)    [BPF_##x | BPF_##y] = &&x##_##y\n#define BPF_INSN_3_LBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = &&x##_##y##_##z\n\tstatic const void *jumptable[256] = {\n\t\t[0 ... 255] = &&default_label,\n\t\t/* Now overwrite non-defaults ... */\n\t\tBPF_INSN_MAP(BPF_INSN_2_LBL, BPF_INSN_3_LBL),\n\t\t/* Non-UAPI available opcodes. */\n\t\t[BPF_JMP | BPF_CALL_ARGS] = &&JMP_CALL_ARGS,\n\t\t[BPF_JMP | BPF_TAIL_CALL] = &&JMP_TAIL_CALL,\n\t};\n#undef BPF_INSN_3_LBL\n#undef BPF_INSN_2_LBL\n\tu32 tail_call_cnt = 0;\n\tvoid *ptr;\n\tint off;\n\n#define CONT\t ({ insn++; goto select_insn; })\n#define CONT_JMP ({ insn++; goto select_insn; })\n\nselect_insn:\n\tgoto *jumptable[insn->code];\n\n\t/* ALU */\n#define ALU(OPCODE, OP)\t\t\t\\\n\tALU64_##OPCODE##_X:\t\t\\\n\t\tDST = DST OP SRC;\t\\\n\t\tCONT;\t\t\t\\\n\tALU_##OPCODE##_X:\t\t\\\n\t\tDST = (u32) DST OP (u32) SRC;\t\\\n\t\tCONT;\t\t\t\\\n\tALU64_##OPCODE##_K:\t\t\\\n\t\tDST = DST OP IMM;\t\t\\\n\t\tCONT;\t\t\t\\\n\tALU_##OPCODE##_K:\t\t\\\n\t\tDST = (u32) DST OP (u32) IMM;\t\\\n\t\tCONT;\n\n\tALU(ADD,  +)\n\tALU(SUB,  -)\n\tALU(AND,  &)\n\tALU(OR,   |)\n\tALU(LSH, <<)\n\tALU(RSH, >>)\n\tALU(XOR,  ^)\n\tALU(MUL,  *)\n#undef ALU\n\tALU_NEG:\n\t\tDST = (u32) -DST;\n\t\tCONT;\n\tALU64_NEG:\n\t\tDST = -DST;\n\t\tCONT;\n\tALU_MOV_X:\n\t\tDST = (u32) SRC;\n\t\tCONT;\n\tALU_MOV_K:\n\t\tDST = (u32) IMM;\n\t\tCONT;\n\tALU64_MOV_X:\n\t\tDST = SRC;\n\t\tCONT;\n\tALU64_MOV_K:\n\t\tDST = IMM;\n\t\tCONT;\n\tLD_IMM_DW:\n\t\tDST = (u64) (u32) insn[0].imm | ((u64) (u32) insn[1].imm) << 32;\n\t\tinsn++;\n\t\tCONT;\n\tALU64_ARSH_X:\n\t\t(*(s64 *) &DST) >>= SRC;\n\t\tCONT;\n\tALU64_ARSH_K:\n\t\t(*(s64 *) &DST) >>= IMM;\n\t\tCONT;\n\tALU64_MOD_X:\n\t\tdiv64_u64_rem(DST, SRC, &tmp);\n\t\tDST = tmp;\n\t\tCONT;\n\tALU_MOD_X:\n\t\ttmp = (u32) DST;\n\t\tDST = do_div(tmp, (u32) SRC);\n\t\tCONT;\n\tALU64_MOD_K:\n\t\tdiv64_u64_rem(DST, IMM, &tmp);\n\t\tDST = tmp;\n\t\tCONT;\n\tALU_MOD_K:\n\t\ttmp = (u32) DST;\n\t\tDST = do_div(tmp, (u32) IMM);\n\t\tCONT;\n\tALU64_DIV_X:\n\t\tDST = div64_u64(DST, SRC);\n\t\tCONT;\n\tALU_DIV_X:\n\t\ttmp = (u32) DST;\n\t\tdo_div(tmp, (u32) SRC);\n\t\tDST = (u32) tmp;\n\t\tCONT;\n\tALU64_DIV_K:\n\t\tDST = div64_u64(DST, IMM);\n\t\tCONT;\n\tALU_DIV_K:\n\t\ttmp = (u32) DST;\n\t\tdo_div(tmp, (u32) IMM);\n\t\tDST = (u32) tmp;\n\t\tCONT;\n\tALU_END_TO_BE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_be16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_be32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_be64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_END_TO_LE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_le16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_le32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_le64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\n\t/* CALL */\n\tJMP_CALL:\n\t\t/* Function call scratches BPF_R1-BPF_R5 registers,\n\t\t * preserves BPF_R6-BPF_R9, and stores return value\n\t\t * into BPF_R0.\n\t\t */\n\t\tBPF_R0 = (__bpf_call_base + insn->imm)(BPF_R1, BPF_R2, BPF_R3,\n\t\t\t\t\t\t       BPF_R4, BPF_R5);\n\t\tCONT;\n\n\tJMP_CALL_ARGS:\n\t\tBPF_R0 = (__bpf_call_base_args + insn->imm)(BPF_R1, BPF_R2,\n\t\t\t\t\t\t\t    BPF_R3, BPF_R4,\n\t\t\t\t\t\t\t    BPF_R5,\n\t\t\t\t\t\t\t    insn + insn->off + 1);\n\t\tCONT;\n\n\tJMP_TAIL_CALL: {\n\t\tstruct bpf_map *map = (struct bpf_map *) (unsigned long) BPF_R2;\n\t\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\t\tstruct bpf_prog *prog;\n\t\tu32 index = BPF_R3;\n\n\t\tif (unlikely(index >= array->map.max_entries))\n\t\t\tgoto out;\n\t\tif (unlikely(tail_call_cnt > MAX_TAIL_CALL_CNT))\n\t\t\tgoto out;\n\n\t\ttail_call_cnt++;\n\n\t\tprog = READ_ONCE(array->ptrs[index]);\n\t\tif (!prog)\n\t\t\tgoto out;\n\n\t\t/* ARG1 at this point is guaranteed to point to CTX from\n\t\t * the verifier side due to the fact that the tail call is\n\t\t * handeled like a helper, that is, bpf_tail_call_proto,\n\t\t * where arg1_type is ARG_PTR_TO_CTX.\n\t\t */\n\t\tinsn = prog->insnsi;\n\t\tgoto select_insn;\nout:\n\t\tCONT;\n\t}\n\t/* JMP */\n\tJMP_JA:\n\t\tinsn += insn->off;\n\t\tCONT;\n\tJMP_JEQ_X:\n\t\tif (DST == SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JEQ_K:\n\t\tif (DST == IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JNE_X:\n\t\tif (DST != SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JNE_K:\n\t\tif (DST != IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGT_X:\n\t\tif (DST > SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGT_K:\n\t\tif (DST > IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLT_X:\n\t\tif (DST < SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLT_K:\n\t\tif (DST < IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGE_X:\n\t\tif (DST >= SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGE_K:\n\t\tif (DST >= IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLE_X:\n\t\tif (DST <= SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLE_K:\n\t\tif (DST <= IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGT_X:\n\t\tif (((s64) DST) > ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGT_K:\n\t\tif (((s64) DST) > ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLT_X:\n\t\tif (((s64) DST) < ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLT_K:\n\t\tif (((s64) DST) < ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGE_X:\n\t\tif (((s64) DST) >= ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGE_K:\n\t\tif (((s64) DST) >= ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLE_X:\n\t\tif (((s64) DST) <= ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLE_K:\n\t\tif (((s64) DST) <= ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSET_X:\n\t\tif (DST & SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSET_K:\n\t\tif (DST & IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_EXIT:\n\t\treturn BPF_R0;\n\n\t/* STX and ST and LDX*/\n#define LDST(SIZEOP, SIZE)\t\t\t\t\t\t\\\n\tSTX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = SRC;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tST_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = IMM;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tLDX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\tDST = *(SIZE *)(unsigned long) (SRC + insn->off);\t\\\n\t\tCONT;\n\n\tLDST(B,   u8)\n\tLDST(H,  u16)\n\tLDST(W,  u32)\n\tLDST(DW, u64)\n#undef LDST\n\tSTX_XADD_W: /* lock xadd *(u32 *)(dst_reg + off16) += src_reg */\n\t\tatomic_add((u32) SRC, (atomic_t *)(unsigned long)\n\t\t\t   (DST + insn->off));\n\t\tCONT;\n\tSTX_XADD_DW: /* lock xadd *(u64 *)(dst_reg + off16) += src_reg */\n\t\tatomic64_add((u64) SRC, (atomic64_t *)(unsigned long)\n\t\t\t     (DST + insn->off));\n\t\tCONT;\n\tLD_ABS_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + imm32)) */\n\t\toff = IMM;\nload_word:\n\t\t/* BPF_LD + BPD_ABS and BPF_LD + BPF_IND insns are only\n\t\t * appearing in the programs where ctx == skb\n\t\t * (see may_access_skb() in the verifier). All programs\n\t\t * keep 'ctx' in regs[BPF_REG_CTX] == BPF_R6,\n\t\t * bpf_convert_filter() saves it in BPF_R6, internal BPF\n\t\t * verifier will check that BPF_R6 == ctx.\n\t\t *\n\t\t * BPF_ABS and BPF_IND are wrappers of function calls,\n\t\t * so they scratch BPF_R1-BPF_R5 registers, preserve\n\t\t * BPF_R6-BPF_R9, and store return value into BPF_R0.\n\t\t *\n\t\t * Implicit input:\n\t\t *   ctx == skb == BPF_R6 == CTX\n\t\t *\n\t\t * Explicit input:\n\t\t *   SRC == any register\n\t\t *   IMM == 32-bit immediate\n\t\t *\n\t\t * Output:\n\t\t *   BPF_R0 - 8/16/32-bit skb data converted to cpu endianness\n\t\t */\n\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 4, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = get_unaligned_be32(ptr);\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_ABS_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + imm32)) */\n\t\toff = IMM;\nload_half:\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 2, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = get_unaligned_be16(ptr);\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_ABS_B: /* BPF_R0 = *(u8 *) (skb->data + imm32) */\n\t\toff = IMM;\nload_byte:\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 1, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = *(u8 *)ptr;\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_IND_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + src_reg + imm32)) */\n\t\toff = IMM + SRC;\n\t\tgoto load_word;\n\tLD_IND_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + src_reg + imm32)) */\n\t\toff = IMM + SRC;\n\t\tgoto load_half;\n\tLD_IND_B: /* BPF_R0 = *(u8 *) (skb->data + src_reg + imm32) */\n\t\toff = IMM + SRC;\n\t\tgoto load_byte;\n\n\tdefault_label:\n\t\t/* If we ever reach this, we have a bug somewhere. Die hard here\n\t\t * instead of just returning 0; we could be somewhere in a subprog,\n\t\t * so execution could continue otherwise which we do /not/ want.\n\t\t *\n\t\t * Note, verifier whitelists all opcodes in bpf_opcode_in_insntable().\n\t\t */\n\t\tpr_warn(\"BPF interpreter: unknown opcode %02x\\n\", insn->code);\n\t\tBUG_ON(1);\n\t\treturn 0;\n}\nSTACK_FRAME_NON_STANDARD(___bpf_prog_run); /* jump table */\n\n#define PROG_NAME(stack_size) __bpf_prog_run##stack_size\n#define DEFINE_BPF_PROG_RUN(stack_size) \\\nstatic unsigned int PROG_NAME(stack_size)(const void *ctx, const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_REG]; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tARG1 = (u64) (unsigned long) ctx; \\\n\treturn ___bpf_prog_run(regs, insn, stack); \\\n}\n\n#define PROG_NAME_ARGS(stack_size) __bpf_prog_run_args##stack_size\n#define DEFINE_BPF_PROG_RUN_ARGS(stack_size) \\\nstatic u64 PROG_NAME_ARGS(stack_size)(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5, \\\n\t\t\t\t      const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_REG]; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tBPF_R1 = r1; \\\n\tBPF_R2 = r2; \\\n\tBPF_R3 = r3; \\\n\tBPF_R4 = r4; \\\n\tBPF_R5 = r5; \\\n\treturn ___bpf_prog_run(regs, insn, stack); \\\n}\n\n#define EVAL1(FN, X) FN(X)\n#define EVAL2(FN, X, Y...) FN(X) EVAL1(FN, Y)\n#define EVAL3(FN, X, Y...) FN(X) EVAL2(FN, Y)\n#define EVAL4(FN, X, Y...) FN(X) EVAL3(FN, Y)\n#define EVAL5(FN, X, Y...) FN(X) EVAL4(FN, Y)\n#define EVAL6(FN, X, Y...) FN(X) EVAL5(FN, Y)\n\nEVAL6(DEFINE_BPF_PROG_RUN, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN, 416, 448, 480, 512);\n\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN_ARGS, 416, 448, 480, 512);\n\n#define PROG_NAME_LIST(stack_size) PROG_NAME(stack_size),\n\nstatic unsigned int (*interpreters[])(const void *ctx,\n\t\t\t\t      const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n#define PROG_NAME_LIST(stack_size) PROG_NAME_ARGS(stack_size),\nstatic u64 (*interpreters_args[])(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5,\n\t\t\t\t  const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n\nvoid bpf_patch_call_args(struct bpf_insn *insn, u32 stack_depth)\n{\n\tstack_depth = max_t(u32, stack_depth, 1);\n\tinsn->off = (s16) insn->imm;\n\tinsn->imm = interpreters_args[(round_up(stack_depth, 32) / 32) - 1] -\n\t\t__bpf_call_base_args;\n\tinsn->code = BPF_JMP | BPF_CALL_ARGS;\n}\n\n#else\nstatic unsigned int __bpf_prog_ret0_warn(const void *ctx,\n\t\t\t\t\t const struct bpf_insn *insn)\n{\n\t/* If this handler ever gets executed, then BPF_JIT_ALWAYS_ON\n\t * is not working properly, so warn about it!\n\t */\n\tWARN_ON_ONCE(1);\n\treturn 0;\n}\n#endif\n\nbool bpf_prog_array_compatible(struct bpf_array *array,\n\t\t\t       const struct bpf_prog *fp)\n{\n\tif (fp->kprobe_override)\n\t\treturn false;\n\n\tif (!array->owner_prog_type) {\n\t\t/* There's no owner yet where we could check for\n\t\t * compatibility.\n\t\t */\n\t\tarray->owner_prog_type = fp->type;\n\t\tarray->owner_jited = fp->jited;\n\n\t\treturn true;\n\t}\n\n\treturn array->owner_prog_type == fp->type &&\n\t       array->owner_jited == fp->jited;\n}\n\nstatic int bpf_check_tail_call(const struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\tint i;\n\n\tfor (i = 0; i < aux->used_map_cnt; i++) {\n\t\tstruct bpf_map *map = aux->used_maps[i];\n\t\tstruct bpf_array *array;\n\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tcontinue;\n\n\t\tarray = container_of(map, struct bpf_array, map);\n\t\tif (!bpf_prog_array_compatible(array, fp))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tbpf_prog_select_runtime - select exec runtime for BPF program\n *\t@fp: bpf_prog populated with internal BPF program\n *\t@err: pointer to error variable\n *\n * Try to JIT eBPF program, if JIT is not available, use interpreter.\n * The BPF program will be executed via BPF_PROG_RUN() macro.\n */\nstruct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tu32 stack_depth = max_t(u32, fp->aux->stack_depth, 1);\n\n\tfp->bpf_func = interpreters[(round_up(stack_depth, 32) / 32) - 1];\n#else\n\tfp->bpf_func = __bpf_prog_ret0_warn;\n#endif\n\n\t/* eBPF JITs can rewrite the program in case constant\n\t * blinding is active. However, in case of error during\n\t * blinding, bpf_int_jit_compile() must always return a\n\t * valid program, which in this case would simply not\n\t * be JITed, but falls back to the interpreter.\n\t */\n\tif (!bpf_prog_is_dev_bound(fp->aux)) {\n\t\tfp = bpf_int_jit_compile(fp);\n#ifdef CONFIG_BPF_JIT_ALWAYS_ON\n\t\tif (!fp->jited) {\n\t\t\t*err = -ENOTSUPP;\n\t\t\treturn fp;\n\t\t}\n#endif\n\t} else {\n\t\t*err = bpf_prog_offload_compile(fp);\n\t\tif (*err)\n\t\t\treturn fp;\n\t}\n\tbpf_prog_lock_ro(fp);\n\n\t/* The tail call compatibility check can only be done at\n\t * this late stage as we need to determine, if we deal\n\t * with JITed or non JITed program concatenations and not\n\t * all eBPF JITs might immediately support all features.\n\t */\n\t*err = bpf_check_tail_call(fp);\n\n\treturn fp;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_select_runtime);\n\nstatic unsigned int __bpf_prog_ret1(const void *ctx,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn 1;\n}\n\nstatic struct bpf_prog_dummy {\n\tstruct bpf_prog prog;\n} dummy_bpf_prog = {\n\t.prog = {\n\t\t.bpf_func = __bpf_prog_ret1,\n\t},\n};\n\n/* to avoid allocating empty bpf_prog_array for cgroups that\n * don't have bpf program attached use one global 'empty_prog_array'\n * It will not be modified the caller of bpf_prog_array_alloc()\n * (since caller requested prog_cnt == 0)\n * that pointer should be 'freed' by bpf_prog_array_free()\n */\nstatic struct {\n\tstruct bpf_prog_array hdr;\n\tstruct bpf_prog *null_prog;\n} empty_prog_array = {\n\t.null_prog = NULL,\n};\n\nstruct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags)\n{\n\tif (prog_cnt)\n\t\treturn kzalloc(sizeof(struct bpf_prog_array) +\n\t\t\t       sizeof(struct bpf_prog *) * (prog_cnt + 1),\n\t\t\t       flags);\n\n\treturn &empty_prog_array.hdr;\n}\n\nvoid bpf_prog_array_free(struct bpf_prog_array __rcu *progs)\n{\n\tif (!progs ||\n\t    progs == (struct bpf_prog_array __rcu *)&empty_prog_array.hdr)\n\t\treturn;\n\tkfree_rcu(progs, rcu);\n}\n\nint bpf_prog_array_length(struct bpf_prog_array __rcu *progs)\n{\n\tstruct bpf_prog **prog;\n\tu32 cnt = 0;\n\n\trcu_read_lock();\n\tprog = rcu_dereference(progs)->progs;\n\tfor (; *prog; prog++)\n\t\tif (*prog != &dummy_bpf_prog.prog)\n\t\t\tcnt++;\n\trcu_read_unlock();\n\treturn cnt;\n}\n\nstatic bool bpf_prog_array_copy_core(struct bpf_prog **prog,\n\t\t\t\t     u32 *prog_ids,\n\t\t\t\t     u32 request_cnt)\n{\n\tint i = 0;\n\n\tfor (; *prog; prog++) {\n\t\tif (*prog == &dummy_bpf_prog.prog)\n\t\t\tcontinue;\n\t\tprog_ids[i] = (*prog)->aux->id;\n\t\tif (++i == request_cnt) {\n\t\t\tprog++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn !!(*prog);\n}\n\nint bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,\n\t\t\t\t__u32 __user *prog_ids, u32 cnt)\n{\n\tstruct bpf_prog **prog;\n\tunsigned long err = 0;\n\tbool nospc;\n\tu32 *ids;\n\n\t/* users of this function are doing:\n\t * cnt = bpf_prog_array_length();\n\t * if (cnt > 0)\n\t *     bpf_prog_array_copy_to_user(..., cnt);\n\t * so below kcalloc doesn't need extra cnt > 0 check, but\n\t * bpf_prog_array_length() releases rcu lock and\n\t * prog array could have been swapped with empty or larger array,\n\t * so always copy 'cnt' prog_ids to the user.\n\t * In a rare race the user will see zero prog_ids\n\t */\n\tids = kcalloc(cnt, sizeof(u32), GFP_USER | __GFP_NOWARN);\n\tif (!ids)\n\t\treturn -ENOMEM;\n\trcu_read_lock();\n\tprog = rcu_dereference(progs)->progs;\n\tnospc = bpf_prog_array_copy_core(prog, ids, cnt);\n\trcu_read_unlock();\n\terr = copy_to_user(prog_ids, ids, cnt * sizeof(u32));\n\tkfree(ids);\n\tif (err)\n\t\treturn -EFAULT;\n\tif (nospc)\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array __rcu *progs,\n\t\t\t\tstruct bpf_prog *old_prog)\n{\n\tstruct bpf_prog **prog = progs->progs;\n\n\tfor (; *prog; prog++)\n\t\tif (*prog == old_prog) {\n\t\t\tWRITE_ONCE(*prog, &dummy_bpf_prog.prog);\n\t\t\tbreak;\n\t\t}\n}\n\nint bpf_prog_array_copy(struct bpf_prog_array __rcu *old_array,\n\t\t\tstruct bpf_prog *exclude_prog,\n\t\t\tstruct bpf_prog *include_prog,\n\t\t\tstruct bpf_prog_array **new_array)\n{\n\tint new_prog_cnt, carry_prog_cnt = 0;\n\tstruct bpf_prog **existing_prog;\n\tstruct bpf_prog_array *array;\n\tint new_prog_idx = 0;\n\n\t/* Figure out how many existing progs we need to carry over to\n\t * the new array.\n\t */\n\tif (old_array) {\n\t\texisting_prog = old_array->progs;\n\t\tfor (; *existing_prog; existing_prog++) {\n\t\t\tif (*existing_prog != exclude_prog &&\n\t\t\t    *existing_prog != &dummy_bpf_prog.prog)\n\t\t\t\tcarry_prog_cnt++;\n\t\t\tif (*existing_prog == include_prog)\n\t\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\t/* How many progs (not NULL) will be in the new array? */\n\tnew_prog_cnt = carry_prog_cnt;\n\tif (include_prog)\n\t\tnew_prog_cnt += 1;\n\n\t/* Do we have any prog (not NULL) in the new array? */\n\tif (!new_prog_cnt) {\n\t\t*new_array = NULL;\n\t\treturn 0;\n\t}\n\n\t/* +1 as the end of prog_array is marked with NULL */\n\tarray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\t/* Fill in the new prog array */\n\tif (carry_prog_cnt) {\n\t\texisting_prog = old_array->progs;\n\t\tfor (; *existing_prog; existing_prog++)\n\t\t\tif (*existing_prog != exclude_prog &&\n\t\t\t    *existing_prog != &dummy_bpf_prog.prog)\n\t\t\t\tarray->progs[new_prog_idx++] = *existing_prog;\n\t}\n\tif (include_prog)\n\t\tarray->progs[new_prog_idx++] = include_prog;\n\tarray->progs[new_prog_idx] = NULL;\n\t*new_array = array;\n\treturn 0;\n}\n\nint bpf_prog_array_copy_info(struct bpf_prog_array __rcu *array,\n\t\t\t     u32 *prog_ids, u32 request_cnt,\n\t\t\t     u32 *prog_cnt)\n{\n\tstruct bpf_prog **prog;\n\tu32 cnt = 0;\n\n\tif (array)\n\t\tcnt = bpf_prog_array_length(array);\n\n\t*prog_cnt = cnt;\n\n\t/* return early if user requested only program count or nothing to copy */\n\tif (!request_cnt || !cnt)\n\t\treturn 0;\n\n\t/* this function is called under trace/bpf_trace.c: bpf_event_mutex */\n\tprog = rcu_dereference_check(array, 1)->progs;\n\treturn bpf_prog_array_copy_core(prog, prog_ids, request_cnt) ? -ENOSPC\n\t\t\t\t\t\t\t\t     : 0;\n}\n\nstatic void bpf_prog_free_deferred(struct work_struct *work)\n{\n\tstruct bpf_prog_aux *aux;\n\tint i;\n\n\taux = container_of(work, struct bpf_prog_aux, work);\n\tif (bpf_prog_is_dev_bound(aux))\n\t\tbpf_prog_offload_destroy(aux->prog);\n\tfor (i = 0; i < aux->func_cnt; i++)\n\t\tbpf_jit_free(aux->func[i]);\n\tif (aux->func_cnt) {\n\t\tkfree(aux->func);\n\t\tbpf_prog_unlock_free(aux->prog);\n\t} else {\n\t\tbpf_jit_free(aux->prog);\n\t}\n}\n\n/* Free internal BPF program */\nvoid bpf_prog_free(struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\n\tINIT_WORK(&aux->work, bpf_prog_free_deferred);\n\tschedule_work(&aux->work);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_free);\n\n/* RNG for unpriviledged user space with separated state from prandom_u32(). */\nstatic DEFINE_PER_CPU(struct rnd_state, bpf_user_rnd_state);\n\nvoid bpf_user_rnd_init_once(void)\n{\n\tprandom_init_once(&bpf_user_rnd_state);\n}\n\nBPF_CALL_0(bpf_user_rnd_u32)\n{\n\t/* Should someone ever have the rather unwise idea to use some\n\t * of the registers passed into this function, then note that\n\t * this function is called from native eBPF and classic-to-eBPF\n\t * transformations. Register assignments from both sides are\n\t * different, f.e. classic always sets fn(ctx, A, X) here.\n\t */\n\tstruct rnd_state *state;\n\tu32 res;\n\n\tstate = &get_cpu_var(bpf_user_rnd_state);\n\tres = prandom_u32_state(state);\n\tput_cpu_var(bpf_user_rnd_state);\n\n\treturn res;\n}\n\n/* Weak definitions of helper functions in case we don't have bpf syscall. */\nconst struct bpf_func_proto bpf_map_lookup_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_update_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_delete_elem_proto __weak;\n\nconst struct bpf_func_proto bpf_get_prandom_u32_proto __weak;\nconst struct bpf_func_proto bpf_get_smp_processor_id_proto __weak;\nconst struct bpf_func_proto bpf_get_numa_node_id_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_ns_proto __weak;\n\nconst struct bpf_func_proto bpf_get_current_pid_tgid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_uid_gid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_comm_proto __weak;\nconst struct bpf_func_proto bpf_sock_map_update_proto __weak;\n\nconst struct bpf_func_proto * __weak bpf_get_trace_printk_proto(void)\n{\n\treturn NULL;\n}\n\nu64 __weak\nbpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\n\t\t void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)\n{\n\treturn -ENOTSUPP;\n}\n\n/* Always built-in helper functions. */\nconst struct bpf_func_proto bpf_tail_call_proto = {\n\t.func\t\t= NULL,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_VOID,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\n/* Stub for JITs that only support cBPF. eBPF programs are interpreted.\n * It is encouraged to implement bpf_int_jit_compile() instead, so that\n * eBPF and implicitly also cBPF can get JITed!\n */\nstruct bpf_prog * __weak bpf_int_jit_compile(struct bpf_prog *prog)\n{\n\treturn prog;\n}\n\n/* Stub for JITs that support eBPF. All cBPF code gets transformed into\n * eBPF by the kernel and is later compiled by bpf_int_jit_compile().\n */\nvoid __weak bpf_jit_compile(struct bpf_prog *prog)\n{\n}\n\nbool __weak bpf_helper_changes_pkt_data(void *func)\n{\n\treturn false;\n}\n\n/* To execute LD_ABS/LD_IND instructions __bpf_prog_run() may call\n * skb_copy_bits(), so provide a weak definition of it for NET-less config.\n */\nint __weak skb_copy_bits(const struct sk_buff *skb, int offset, void *to,\n\t\t\t int len)\n{\n\treturn -EFAULT;\n}\n\n/* All definitions of tracepoints related to BPF. */\n#define CREATE_TRACE_POINTS\n#include <linux/bpf_trace.h>\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(xdp_exception);\n\n/* These are only used within the BPF_SYSCALL code */\n#ifdef CONFIG_BPF_SYSCALL\nEXPORT_TRACEPOINT_SYMBOL_GPL(bpf_prog_get_type);\nEXPORT_TRACEPOINT_SYMBOL_GPL(bpf_prog_put_rcu);\n#endif\n", "/*\n * Linux Socket Filter - Kernel level socket filtering\n *\n * Based on the design of the Berkeley Packet Filter. The new\n * internal format has been designed by PLUMgrid:\n *\n *\tCopyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com\n *\n * Authors:\n *\n *\tJay Schulist <jschlst@samba.org>\n *\tAlexei Starovoitov <ast@plumgrid.com>\n *\tDaniel Borkmann <dborkman@redhat.com>\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n *\n * Andi Kleen - Fix a few bad bugs and races.\n * Kris Katterjohn - Added many additional checks in bpf_check_classic()\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/sock_diag.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/gfp.h>\n#include <net/inet_common.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/netlink.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/flow_dissector.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/unaligned.h>\n#include <asm/cmpxchg.h>\n#include <linux/filter.h>\n#include <linux/ratelimit.h>\n#include <linux/seccomp.h>\n#include <linux/if_vlan.h>\n#include <linux/bpf.h>\n#include <net/sch_generic.h>\n#include <net/cls_cgroup.h>\n#include <net/dst_metadata.h>\n#include <net/dst.h>\n#include <net/sock_reuseport.h>\n#include <net/busy_poll.h>\n#include <net/tcp.h>\n#include <linux/bpf_trace.h>\n\n/**\n *\tsk_filter_trim_cap - run a packet through a socket filter\n *\t@sk: sock associated with &sk_buff\n *\t@skb: buffer to filter\n *\t@cap: limit on how short the eBPF program may trim the packet\n *\n * Run the eBPF program and then cut skb->data to correct size returned by\n * the program. If pkt_len is 0 we toss packet. If skb->len is smaller\n * than pkt_len we keep whole skb->data. This is the socket level\n * wrapper to BPF_PROG_RUN. It returns 0 if the packet should\n * be accepted or -EPERM if the packet should be tossed.\n *\n */\nint sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap)\n{\n\tint err;\n\tstruct sk_filter *filter;\n\n\t/*\n\t * If the skb was allocated from pfmemalloc reserves, only\n\t * allow SOCK_MEMALLOC sockets to use it as this socket is\n\t * helping free memory\n\t */\n\tif (skb_pfmemalloc(skb) && !sock_flag(sk, SOCK_MEMALLOC)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PFMEMALLOCDROP);\n\t\treturn -ENOMEM;\n\t}\n\terr = BPF_CGROUP_RUN_PROG_INET_INGRESS(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\terr = security_sock_rcv_skb(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter) {\n\t\tstruct sock *save_sk = skb->sk;\n\t\tunsigned int pkt_len;\n\n\t\tskb->sk = sk;\n\t\tpkt_len = bpf_prog_run_save_cb(filter->prog, skb);\n\t\tskb->sk = save_sk;\n\t\terr = pkt_len ? pskb_trim(skb, max(cap, pkt_len)) : -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(sk_filter_trim_cap);\n\nBPF_CALL_1(__skb_get_pay_offset, struct sk_buff *, skb)\n{\n\treturn skb_get_poff(skb);\n}\n\nBPF_CALL_3(__skb_get_nlattr, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_3(__skb_get_nlattr_nest, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = (struct nlattr *) &skb->data[a];\n\tif (nla->nla_len > skb->len - a)\n\t\treturn 0;\n\n\tnla = nla_find_nested(nla, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_0(__get_raw_cpu_id)\n{\n\treturn raw_smp_processor_id();\n}\n\nstatic const struct bpf_func_proto bpf_get_raw_smp_processor_id_proto = {\n\t.func\t\t= __get_raw_cpu_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nstatic u32 convert_skb_access(int skb_field, int dst_reg, int src_reg,\n\t\t\t      struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (skb_field) {\n\tcase SKF_AD_MARK:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, mark));\n\t\tbreak;\n\n\tcase SKF_AD_PKTTYPE:\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, dst_reg, src_reg, PKT_TYPE_OFFSET());\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase SKF_AD_QUEUE:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, queue_mapping));\n\t\tbreak;\n\n\tcase SKF_AD_VLAN_TAG:\n\tcase SKF_AD_VLAN_TAG_PRESENT:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);\n\t\tBUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);\n\n\t\t/* dst_reg = *(u16 *) (src_reg + offsetof(vlan_tci)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, vlan_tci));\n\t\tif (skb_field == SKF_AD_VLAN_TAG) {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg,\n\t\t\t\t\t\t~VLAN_TAG_PRESENT);\n\t\t} else {\n\t\t\t/* dst_reg >>= 12 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 12);\n\t\t\t/* dst_reg &= 1 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, 1);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic bool convert_bpf_extensions(struct sock_filter *fp,\n\t\t\t\t   struct bpf_insn **insnp)\n{\n\tstruct bpf_insn *insn = *insnp;\n\tu32 cnt;\n\n\tswitch (fp->k) {\n\tcase SKF_AD_OFF + SKF_AD_PROTOCOL:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);\n\n\t\t/* A = *(u16 *) (CTX + offsetof(protocol)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, protocol));\n\t\t/* A = ntohs(A) [emitting a nop or swap16] */\n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PKTTYPE:\n\t\tcnt = convert_skb_access(SKF_AD_PKTTYPE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_IFINDEX:\n\tcase SKF_AD_OFF + SKF_AD_HATYPE:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      BPF_REG_TMP, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t/* if (tmp != 0) goto pc + 1 */\n\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);\n\t\t*insn++ = BPF_EXIT_INSN();\n\t\tif (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, ifindex));\n\t\telse\n\t\t\t*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, type));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_MARK:\n\t\tcnt = convert_skb_access(SKF_AD_MARK, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_RXHASH:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);\n\n\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t    offsetof(struct sk_buff, hash));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_QUEUE:\n\t\tcnt = convert_skb_access(SKF_AD_QUEUE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TPID:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_proto) != 2);\n\n\t\t/* A = *(u16 *) (CTX + offsetof(vlan_proto)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, vlan_proto));\n\t\t/* A = ntohs(A) [emitting a nop or swap16] */\n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\tcase SKF_AD_OFF + SKF_AD_CPU:\n\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t/* arg1 = CTX */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);\n\t\t/* arg2 = A */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);\n\t\t/* arg3 = X */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);\n\t\t/* Emit call(arg1=CTX, arg2=A, arg3=X) */\n\t\tswitch (fp->k) {\n\t\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_pay_offset);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_nlattr);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_nlattr_nest);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_CPU:\n\t\t\t*insn = BPF_EMIT_CALL(__get_raw_cpu_id);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_user_rnd_u32);\n\t\t\tbpf_user_rnd_init_once();\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_ALU_XOR_X:\n\t\t/* A ^= X */\n\t\t*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);\n\t\tbreak;\n\n\tdefault:\n\t\t/* This is just a dummy call to avoid letting the compiler\n\t\t * evict __bpf_call_base() as an optimization. Placed here\n\t\t * where no-one bothers.\n\t\t */\n\t\tBUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);\n\t\treturn false;\n\t}\n\n\t*insnp = insn;\n\treturn true;\n}\n\n/**\n *\tbpf_convert_filter - convert filter program\n *\t@prog: the user passed filter program\n *\t@len: the length of the user passed filter program\n *\t@new_prog: allocated 'struct bpf_prog' or NULL\n *\t@new_len: pointer to store length of converted program\n *\n * Remap 'sock_filter' style classic BPF (cBPF) instruction set to 'bpf_insn'\n * style extended BPF (eBPF).\n * Conversion workflow:\n *\n * 1) First pass for calculating the new program length:\n *   bpf_convert_filter(old_prog, old_len, NULL, &new_len)\n *\n * 2) 2nd pass to remap in two passes: 1st pass finds new\n *    jump offsets, 2nd pass remapping:\n *   bpf_convert_filter(old_prog, old_len, new_prog, &new_len);\n */\nstatic int bpf_convert_filter(struct sock_filter *prog, int len,\n\t\t\t      struct bpf_prog *new_prog, int *new_len)\n{\n\tint new_flen = 0, pass = 0, target, i, stack_off;\n\tstruct bpf_insn *new_insn, *first_insn = NULL;\n\tstruct sock_filter *fp;\n\tint *addrs = NULL;\n\tu8 bpf_src;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\n\tif (len <= 0 || len > BPF_MAXINSNS)\n\t\treturn -EINVAL;\n\n\tif (new_prog) {\n\t\tfirst_insn = new_prog->insnsi;\n\t\taddrs = kcalloc(len, sizeof(*addrs),\n\t\t\t\tGFP_KERNEL | __GFP_NOWARN);\n\t\tif (!addrs)\n\t\t\treturn -ENOMEM;\n\t}\n\ndo_pass:\n\tnew_insn = first_insn;\n\tfp = prog;\n\n\t/* Classic BPF related prologue emission. */\n\tif (new_prog) {\n\t\t/* Classic BPF expects A and X to be reset first. These need\n\t\t * to be guaranteed to be the first two instructions.\n\t\t */\n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_X, BPF_REG_X);\n\n\t\t/* All programs must keep CTX in callee saved BPF_REG_CTX.\n\t\t * In eBPF case it's done by the compiler, here we need to\n\t\t * do this ourself. Initial CTX is present in BPF_REG_ARG1.\n\t\t */\n\t\t*new_insn++ = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);\n\t} else {\n\t\tnew_insn += 3;\n\t}\n\n\tfor (i = 0; i < len; fp++, i++) {\n\t\tstruct bpf_insn tmp_insns[6] = { };\n\t\tstruct bpf_insn *insn = tmp_insns;\n\n\t\tif (addrs)\n\t\t\taddrs[i] = new_insn - first_insn;\n\n\t\tswitch (fp->code) {\n\t\t/* All arithmetic insns and skb loads map as-is. */\n\t\tcase BPF_ALU | BPF_ADD | BPF_X:\n\t\tcase BPF_ALU | BPF_ADD | BPF_K:\n\t\tcase BPF_ALU | BPF_SUB | BPF_X:\n\t\tcase BPF_ALU | BPF_SUB | BPF_K:\n\t\tcase BPF_ALU | BPF_AND | BPF_X:\n\t\tcase BPF_ALU | BPF_AND | BPF_K:\n\t\tcase BPF_ALU | BPF_OR | BPF_X:\n\t\tcase BPF_ALU | BPF_OR | BPF_K:\n\t\tcase BPF_ALU | BPF_LSH | BPF_X:\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_X:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\tcase BPF_ALU | BPF_XOR | BPF_X:\n\t\tcase BPF_ALU | BPF_XOR | BPF_K:\n\t\tcase BPF_ALU | BPF_MUL | BPF_X:\n\t\tcase BPF_ALU | BPF_MUL | BPF_K:\n\t\tcase BPF_ALU | BPF_DIV | BPF_X:\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_X:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\tcase BPF_ALU | BPF_NEG:\n\t\tcase BPF_LD | BPF_ABS | BPF_W:\n\t\tcase BPF_LD | BPF_ABS | BPF_H:\n\t\tcase BPF_LD | BPF_ABS | BPF_B:\n\t\tcase BPF_LD | BPF_IND | BPF_W:\n\t\tcase BPF_LD | BPF_IND | BPF_H:\n\t\tcase BPF_LD | BPF_IND | BPF_B:\n\t\t\t/* Check for overloaded BPF extension and\n\t\t\t * directly convert it if found, otherwise\n\t\t\t * just move on with mapping.\n\t\t\t */\n\t\t\tif (BPF_CLASS(fp->code) == BPF_LD &&\n\t\t\t    BPF_MODE(fp->code) == BPF_ABS &&\n\t\t\t    convert_bpf_extensions(fp, &insn))\n\t\t\t\tbreak;\n\n\t\t\tif (fp->code == (BPF_ALU | BPF_DIV | BPF_X) ||\n\t\t\t    fp->code == (BPF_ALU | BPF_MOD | BPF_X)) {\n\t\t\t\t*insn++ = BPF_MOV32_REG(BPF_REG_X, BPF_REG_X);\n\t\t\t\t/* Error with exception code on div/mod by 0.\n\t\t\t\t * For cBPF programs, this was always return 0.\n\t\t\t\t */\n\t\t\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_X, 0, 2);\n\t\t\t\t*insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t\t\t*insn++ = BPF_EXIT_INSN();\n\t\t\t}\n\n\t\t\t*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);\n\t\t\tbreak;\n\n\t\t/* Jump transformation cannot use BPF block macros\n\t\t * everywhere as offset calculation and target updates\n\t\t * require a bit more work than the rest, i.e. jump\n\t\t * opcodes map as-is, but offsets need adjustment.\n\t\t */\n\n#define BPF_EMIT_JMP\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (target >= len || target < 0)\t\t\t\\\n\t\t\tgoto err;\t\t\t\t\t\\\n\t\tinsn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;\t\\\n\t\t/* Adjust pc relative offset for 2nd or 3rd insn. */\t\\\n\t\tinsn->off -= insn - tmp_insns;\t\t\t\t\\\n\t} while (0)\n\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\ttarget = i + fp->k + 1;\n\t\t\tinsn->code = fp->code;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\t\tif (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {\n\t\t\t\t/* BPF immediates are signed, zero extend\n\t\t\t\t * immediate into tmp register and use it\n\t\t\t\t * in compare insn.\n\t\t\t\t */\n\t\t\t\t*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);\n\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->src_reg = BPF_REG_TMP;\n\t\t\t\tbpf_src = BPF_X;\n\t\t\t} else {\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->imm = fp->k;\n\t\t\t\tbpf_src = BPF_SRC(fp->code);\n\t\t\t\tinsn->src_reg = bpf_src == BPF_X ? BPF_REG_X : 0;\n\t\t\t}\n\n\t\t\t/* Common case where 'jump_false' is next insn. */\n\t\t\tif (fp->jf == 0) {\n\t\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\t\ttarget = i + fp->jt + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Convert some jumps when 'jump_true' is next insn. */\n\t\t\tif (fp->jt == 0) {\n\t\t\t\tswitch (BPF_OP(fp->code)) {\n\t\t\t\tcase BPF_JEQ:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JNE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGT:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGE:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLT | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tgoto jmp_rest;\n\t\t\t\t}\n\n\t\t\t\ttarget = i + fp->jf + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\njmp_rest:\n\t\t\t/* Other jumps are mapped into two insns: Jxx and JA. */\n\t\t\ttarget = i + fp->jt + 1;\n\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tinsn++;\n\n\t\t\tinsn->code = BPF_JMP | BPF_JA;\n\t\t\ttarget = i + fp->jf + 1;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\t/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */\n\t\tcase BPF_LDX | BPF_MSH | BPF_B:\n\t\t\t/* tmp = A */\n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_A);\n\t\t\t/* A = BPF_R0 = *(u8 *) (skb->data + K) */\n\t\t\t*insn++ = BPF_LD_ABS(BPF_B, fp->k);\n\t\t\t/* A &= 0xf */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);\n\t\t\t/* A <<= 2 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);\n\t\t\t/* X = A */\n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\t/* A = tmp */\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);\n\t\t\tbreak;\n\n\t\t/* RET_K is remaped into 2 insns. RET_A case doesn't need an\n\t\t * extra mov as BPF_REG_0 is already mapped into BPF_REG_A.\n\t\t */\n\t\tcase BPF_RET | BPF_A:\n\t\tcase BPF_RET | BPF_K:\n\t\t\tif (BPF_RVAL(fp->code) == BPF_K)\n\t\t\t\t*insn++ = BPF_MOV32_RAW(BPF_K, BPF_REG_0,\n\t\t\t\t\t\t\t0, fp->k);\n\t\t\t*insn = BPF_EXIT_INSN();\n\t\t\tbreak;\n\n\t\t/* Store to stack. */\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==\n\t\t\t\t\t    BPF_ST ? BPF_REG_A : BPF_REG_X,\n\t\t\t\t\t    -stack_off);\n\t\t\t/* check_load_and_stores() verifies that classic BPF can\n\t\t\t * load from stack only after write, so tracking\n\t\t\t * stack_depth for ST|STX insns is enough\n\t\t\t */\n\t\t\tif (new_prog && new_prog->aux->stack_depth < stack_off)\n\t\t\t\tnew_prog->aux->stack_depth = stack_off;\n\t\t\tbreak;\n\n\t\t/* Load from stack. */\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_FP,\n\t\t\t\t\t    -stack_off);\n\t\t\tbreak;\n\n\t\t/* A = K or X = K */\n\t\tcase BPF_LD | BPF_IMM:\n\t\tcase BPF_LDX | BPF_IMM:\n\t\t\t*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t      BPF_REG_A : BPF_REG_X, fp->k);\n\t\t\tbreak;\n\n\t\t/* X = A */\n\t\tcase BPF_MISC | BPF_TAX:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\tbreak;\n\n\t\t/* A = X */\n\t\tcase BPF_MISC | BPF_TXA:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);\n\t\t\tbreak;\n\n\t\t/* A = skb->len or X = skb->len */\n\t\tcase BPF_LD | BPF_W | BPF_LEN:\n\t\tcase BPF_LDX | BPF_W | BPF_LEN:\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,\n\t\t\t\t\t    offsetof(struct sk_buff, len));\n\t\t\tbreak;\n\n\t\t/* Access seccomp_data fields. */\n\t\tcase BPF_LDX | BPF_ABS | BPF_W:\n\t\t\t/* A = *(u32 *) (ctx + K) */\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);\n\t\t\tbreak;\n\n\t\t/* Unknown instruction. */\n\t\tdefault:\n\t\t\tgoto err;\n\t\t}\n\n\t\tinsn++;\n\t\tif (new_prog)\n\t\t\tmemcpy(new_insn, tmp_insns,\n\t\t\t       sizeof(*insn) * (insn - tmp_insns));\n\t\tnew_insn += insn - tmp_insns;\n\t}\n\n\tif (!new_prog) {\n\t\t/* Only calculating new length. */\n\t\t*new_len = new_insn - first_insn;\n\t\treturn 0;\n\t}\n\n\tpass++;\n\tif (new_flen != new_insn - first_insn) {\n\t\tnew_flen = new_insn - first_insn;\n\t\tif (pass > 2)\n\t\t\tgoto err;\n\t\tgoto do_pass;\n\t}\n\n\tkfree(addrs);\n\tBUG_ON(*new_len != new_flen);\n\treturn 0;\nerr:\n\tkfree(addrs);\n\treturn -EINVAL;\n}\n\n/* Security:\n *\n * As we dont want to clear mem[] array for each packet going through\n * __bpf_prog_run(), we check that filter loaded by user never try to read\n * a cell if not previously written, and we check all branches to be sure\n * a malicious user doesn't try to abuse us.\n */\nstatic int check_load_and_stores(const struct sock_filter *filter, int flen)\n{\n\tu16 *masks, memvalid = 0; /* One bit per cell, 16 cells */\n\tint pc, ret = 0;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS > 16);\n\n\tmasks = kmalloc_array(flen, sizeof(*masks), GFP_KERNEL);\n\tif (!masks)\n\t\treturn -ENOMEM;\n\n\tmemset(masks, 0xff, flen * sizeof(*masks));\n\n\tfor (pc = 0; pc < flen; pc++) {\n\t\tmemvalid &= masks[pc];\n\n\t\tswitch (filter[pc].code) {\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tmemvalid |= (1 << filter[pc].k);\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tif (!(memvalid & (1 << filter[pc].k))) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t/* A jump must set masks on target */\n\t\t\tmasks[pc + 1 + filter[pc].k] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t/* A jump must set masks on targets */\n\t\t\tmasks[pc + 1 + filter[pc].jt] &= memvalid;\n\t\t\tmasks[pc + 1 + filter[pc].jf] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\t}\n\t}\nerror:\n\tkfree(masks);\n\treturn ret;\n}\n\nstatic bool chk_code_allowed(u16 code_to_probe)\n{\n\tstatic const bool codes[] = {\n\t\t/* 32 bit ALU operations */\n\t\t[BPF_ALU | BPF_ADD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_ADD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_K] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_X] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_K] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_K] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_X] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_NEG] = true,\n\t\t/* Load instructions */\n\t\t[BPF_LD | BPF_W | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_H | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_B | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LD | BPF_W | BPF_IND] = true,\n\t\t[BPF_LD | BPF_H | BPF_IND] = true,\n\t\t[BPF_LD | BPF_B | BPF_IND] = true,\n\t\t[BPF_LD | BPF_IMM] = true,\n\t\t[BPF_LD | BPF_MEM] = true,\n\t\t[BPF_LDX | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LDX | BPF_B | BPF_MSH] = true,\n\t\t[BPF_LDX | BPF_IMM] = true,\n\t\t[BPF_LDX | BPF_MEM] = true,\n\t\t/* Store instructions */\n\t\t[BPF_ST] = true,\n\t\t[BPF_STX] = true,\n\t\t/* Misc instructions */\n\t\t[BPF_MISC | BPF_TAX] = true,\n\t\t[BPF_MISC | BPF_TXA] = true,\n\t\t/* Return instructions */\n\t\t[BPF_RET | BPF_K] = true,\n\t\t[BPF_RET | BPF_A] = true,\n\t\t/* Jump instructions */\n\t\t[BPF_JMP | BPF_JA] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_X] = true,\n\t};\n\n\tif (code_to_probe >= ARRAY_SIZE(codes))\n\t\treturn false;\n\n\treturn codes[code_to_probe];\n}\n\nstatic bool bpf_check_basics_ok(const struct sock_filter *filter,\n\t\t\t\tunsigned int flen)\n{\n\tif (filter == NULL)\n\t\treturn false;\n\tif (flen == 0 || flen > BPF_MAXINSNS)\n\t\treturn false;\n\n\treturn true;\n}\n\n/**\n *\tbpf_check_classic - verify socket filter code\n *\t@filter: filter to verify\n *\t@flen: length of filter\n *\n * Check the user's filter code. If we let some ugly\n * filter code slip through kaboom! The filter must contain\n * no references or jumps that are out of range, no illegal\n * instructions, and must end with a RET instruction.\n *\n * All jumps are forward as they are not signed.\n *\n * Returns 0 if the rule set is legal or -EINVAL if not.\n */\nstatic int bpf_check_classic(const struct sock_filter *filter,\n\t\t\t     unsigned int flen)\n{\n\tbool anc_found;\n\tint pc;\n\n\t/* Check the filter code now */\n\tfor (pc = 0; pc < flen; pc++) {\n\t\tconst struct sock_filter *ftest = &filter[pc];\n\n\t\t/* May we actually operate on this code? */\n\t\tif (!chk_code_allowed(ftest->code))\n\t\t\treturn -EINVAL;\n\n\t\t/* Some instructions need special checks */\n\t\tswitch (ftest->code) {\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t\t/* Check for division by zero */\n\t\t\tif (ftest->k == 0)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\t\tif (ftest->k >= 32)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\t/* Check for invalid memory addresses */\n\t\t\tif (ftest->k >= BPF_MEMWORDS)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t/* Note, the large ftest->k might cause loops.\n\t\t\t * Compare this with conditional jumps below,\n\t\t\t * where offsets are limited. --ANK (981016)\n\t\t\t */\n\t\t\tif (ftest->k >= (unsigned int)(flen - pc - 1))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t/* Both conditionals must be safe */\n\t\t\tif (pc + ftest->jt + 1 >= flen ||\n\t\t\t    pc + ftest->jf + 1 >= flen)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_W | BPF_ABS:\n\t\tcase BPF_LD | BPF_H | BPF_ABS:\n\t\tcase BPF_LD | BPF_B | BPF_ABS:\n\t\t\tanc_found = false;\n\t\t\tif (bpf_anc_helper(ftest) & BPF_ANC)\n\t\t\t\tanc_found = true;\n\t\t\t/* Ancillary operation unknown or unsupported */\n\t\t\tif (anc_found == false && ftest->k >= SKF_AD_OFF)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Last instruction must be a RET code */\n\tswitch (filter[flen - 1].code) {\n\tcase BPF_RET | BPF_K:\n\tcase BPF_RET | BPF_A:\n\t\treturn check_load_and_stores(filter, flen);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int bpf_prog_store_orig_filter(struct bpf_prog *fp,\n\t\t\t\t      const struct sock_fprog *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct sock_fprog_kern *fkprog;\n\n\tfp->orig_prog = kmalloc(sizeof(*fkprog), GFP_KERNEL);\n\tif (!fp->orig_prog)\n\t\treturn -ENOMEM;\n\n\tfkprog = fp->orig_prog;\n\tfkprog->len = fprog->len;\n\n\tfkprog->filter = kmemdup(fp->insns, fsize,\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!fkprog->filter) {\n\t\tkfree(fp->orig_prog);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void bpf_release_orig_filter(struct bpf_prog *fp)\n{\n\tstruct sock_fprog_kern *fprog = fp->orig_prog;\n\n\tif (fprog) {\n\t\tkfree(fprog->filter);\n\t\tkfree(fprog);\n\t}\n}\n\nstatic void __bpf_prog_release(struct bpf_prog *prog)\n{\n\tif (prog->type == BPF_PROG_TYPE_SOCKET_FILTER) {\n\t\tbpf_prog_put(prog);\n\t} else {\n\t\tbpf_release_orig_filter(prog);\n\t\tbpf_prog_free(prog);\n\t}\n}\n\nstatic void __sk_filter_release(struct sk_filter *fp)\n{\n\t__bpf_prog_release(fp->prog);\n\tkfree(fp);\n}\n\n/**\n * \tsk_filter_release_rcu - Release a socket filter by rcu_head\n *\t@rcu: rcu_head that contains the sk_filter to free\n */\nstatic void sk_filter_release_rcu(struct rcu_head *rcu)\n{\n\tstruct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);\n\n\t__sk_filter_release(fp);\n}\n\n/**\n *\tsk_filter_release - release a socket filter\n *\t@fp: filter to remove\n *\n *\tRemove a filter from a socket and release its resources.\n */\nstatic void sk_filter_release(struct sk_filter *fp)\n{\n\tif (refcount_dec_and_test(&fp->refcnt))\n\t\tcall_rcu(&fp->rcu, sk_filter_release_rcu);\n}\n\nvoid sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\n\tatomic_sub(filter_size, &sk->sk_omem_alloc);\n\tsk_filter_release(fp);\n}\n\n/* try to charge the socket memory if there is space available\n * return true on success\n */\nstatic bool __sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\n\t/* same check as in sock_kmalloc() */\n\tif (filter_size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + filter_size < sysctl_optmem_max) {\n\t\tatomic_add(filter_size, &sk->sk_omem_alloc);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tif (!refcount_inc_not_zero(&fp->refcnt))\n\t\treturn false;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tsk_filter_release(fp);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic struct bpf_prog *bpf_migrate_filter(struct bpf_prog *fp)\n{\n\tstruct sock_filter *old_prog;\n\tstruct bpf_prog *old_fp;\n\tint err, new_len, old_len = fp->len;\n\n\t/* We are free to overwrite insns et al right here as it\n\t * won't be used at this point in time anymore internally\n\t * after the migration to the internal BPF instruction\n\t * representation.\n\t */\n\tBUILD_BUG_ON(sizeof(struct sock_filter) !=\n\t\t     sizeof(struct bpf_insn));\n\n\t/* Conversion cannot happen on overlapping memory areas,\n\t * so we need to keep the user BPF around until the 2nd\n\t * pass. At this time, the user BPF is stored in fp->insns.\n\t */\n\told_prog = kmemdup(fp->insns, old_len * sizeof(struct sock_filter),\n\t\t\t   GFP_KERNEL | __GFP_NOWARN);\n\tif (!old_prog) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\t/* 1st pass: calculate the new program length. */\n\terr = bpf_convert_filter(old_prog, old_len, NULL, &new_len);\n\tif (err)\n\t\tgoto out_err_free;\n\n\t/* Expand fp for appending the new filter representation. */\n\told_fp = fp;\n\tfp = bpf_prog_realloc(old_fp, bpf_prog_size(new_len), 0);\n\tif (!fp) {\n\t\t/* The old_fp is still around in case we couldn't\n\t\t * allocate new memory, so uncharge on that one.\n\t\t */\n\t\tfp = old_fp;\n\t\terr = -ENOMEM;\n\t\tgoto out_err_free;\n\t}\n\n\tfp->len = new_len;\n\n\t/* 2nd pass: remap sock_filter insns into bpf_insn insns. */\n\terr = bpf_convert_filter(old_prog, old_len, fp, &new_len);\n\tif (err)\n\t\t/* 2nd bpf_convert_filter() can fail only if it fails\n\t\t * to allocate memory, remapping must succeed. Note,\n\t\t * that at this time old_fp has already been released\n\t\t * by krealloc().\n\t\t */\n\t\tgoto out_err_free;\n\n\tfp = bpf_prog_select_runtime(fp, &err);\n\tif (err)\n\t\tgoto out_err_free;\n\n\tkfree(old_prog);\n\treturn fp;\n\nout_err_free:\n\tkfree(old_prog);\nout_err:\n\t__bpf_prog_release(fp);\n\treturn ERR_PTR(err);\n}\n\nstatic struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,\n\t\t\t\t\t   bpf_aux_classic_check_t trans)\n{\n\tint err;\n\n\tfp->bpf_func = NULL;\n\tfp->jited = 0;\n\n\terr = bpf_check_classic(fp->insns, fp->len);\n\tif (err) {\n\t\t__bpf_prog_release(fp);\n\t\treturn ERR_PTR(err);\n\t}\n\n\t/* There might be additional checks and transformations\n\t * needed on classic filters, f.e. in case of seccomp.\n\t */\n\tif (trans) {\n\t\terr = trans(fp->insns, fp->len);\n\t\tif (err) {\n\t\t\t__bpf_prog_release(fp);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\t/* Probe if we can JIT compile the filter and if so, do\n\t * the compilation of the filter.\n\t */\n\tbpf_jit_compile(fp);\n\n\t/* JIT compiler couldn't process this filter, so do the\n\t * internal BPF translation for the optimized interpreter.\n\t */\n\tif (!fp->jited)\n\t\tfp = bpf_migrate_filter(fp);\n\n\treturn fp;\n}\n\n/**\n *\tbpf_prog_create - create an unattached filter\n *\t@pfp: the unattached filter that is created\n *\t@fprog: the filter program\n *\n * Create a filter independent of any socket. We first run some\n * sanity checks on it to make sure it does not explode on us later.\n * If an error occurs or there is insufficient memory for the filter\n * a negative errno code is returned. On success the return is zero.\n */\nint bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tmemcpy(fp->insns, fprog->filter, fsize);\n\n\tfp->len = fprog->len;\n\t/* Since unattached filters are not copied back to user\n\t * space through sk_get_filter(), we do not need to hold\n\t * a copy here, and can spare us the work.\n\t */\n\tfp->orig_prog = NULL;\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\tfp = bpf_prepare_filter(fp, NULL);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create);\n\n/**\n *\tbpf_prog_create_from_user - create an unattached filter from user buffer\n *\t@pfp: the unattached filter that is created\n *\t@fprog: the filter program\n *\t@trans: post-classic verifier transformation handler\n *\t@save_orig: save classic BPF program\n *\n * This function effectively does the same as bpf_prog_create(), only\n * that it builds up its insns buffer from user space provided buffer.\n * It also allows for passing a bpf_aux_classic_check_t handler.\n */\nint bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,\n\t\t\t      bpf_aux_classic_check_t trans, bool save_orig)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\tint err;\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(fp->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(fp);\n\t\treturn -EFAULT;\n\t}\n\n\tfp->len = fprog->len;\n\tfp->orig_prog = NULL;\n\n\tif (save_orig) {\n\t\terr = bpf_prog_store_orig_filter(fp, fprog);\n\t\tif (err) {\n\t\t\t__bpf_prog_free(fp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\tfp = bpf_prepare_filter(fp, trans);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create_from_user);\n\nvoid bpf_prog_destroy(struct bpf_prog *fp)\n{\n\t__bpf_prog_release(fp);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_destroy);\n\nstatic int __sk_attach_prog(struct bpf_prog *prog, struct sock *sk)\n{\n\tstruct sk_filter *fp, *old_fp;\n\n\tfp = kmalloc(sizeof(*fp), GFP_KERNEL);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tfp->prog = prog;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tkfree(fp);\n\t\treturn -ENOMEM;\n\t}\n\trefcount_set(&fp->refcnt, 1);\n\n\told_fp = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\trcu_assign_pointer(sk->sk_filter, fp);\n\n\tif (old_fp)\n\t\tsk_filter_uncharge(sk, old_fp);\n\n\treturn 0;\n}\n\nstatic int __reuseport_attach_prog(struct bpf_prog *prog, struct sock *sk)\n{\n\tstruct bpf_prog *old_prog;\n\tint err;\n\n\tif (bpf_prog_size(prog->len) > sysctl_optmem_max)\n\t\treturn -ENOMEM;\n\n\tif (sk_unhashed(sk) && sk->sk_reuseport) {\n\t\terr = reuseport_alloc(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (!rcu_access_pointer(sk->sk_reuseport_cb)) {\n\t\t/* The socket wasn't bound with SO_REUSEPORT */\n\t\treturn -EINVAL;\n\t}\n\n\told_prog = reuseport_attach_prog(sk, prog);\n\tif (old_prog)\n\t\tbpf_prog_destroy(old_prog);\n\n\treturn 0;\n}\n\nstatic\nstruct bpf_prog *__get_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *prog;\n\tint err;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tprog = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!prog)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(prog->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\tprog->len = fprog->len;\n\n\terr = bpf_prog_store_orig_filter(prog, fprog);\n\tif (err) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\treturn bpf_prepare_filter(prog, NULL);\n}\n\n/**\n *\tsk_attach_filter - attach a socket filter\n *\t@fprog: the filter program\n *\t@sk: the socket to use\n *\n * Attach the user's filter code. We first run some sanity checks on\n * it to make sure it does not explode on us later. If an error\n * occurs or there is insufficient memory for the filter a negative\n * errno code is returned. On success the return is zero.\n */\nint sk_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\t__bpf_prog_release(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_attach_filter);\n\nint sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __reuseport_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\t__bpf_prog_release(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic struct bpf_prog *__get_bpf(u32 ufd, struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\treturn bpf_prog_get_type(ufd, BPF_PROG_TYPE_SOCKET_FILTER);\n}\n\nint sk_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_bpf(ufd, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint sk_reuseport_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_bpf(ufd, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __reuseport_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstruct bpf_scratchpad {\n\tunion {\n\t\t__be32 diff[MAX_BPF_STACK / sizeof(__be32)];\n\t\tu8     buff[MAX_BPF_STACK];\n\t};\n};\n\nstatic DEFINE_PER_CPU(struct bpf_scratchpad, bpf_sp);\n\nstatic inline int __bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\t  unsigned int write_len)\n{\n\treturn skb_ensure_writable(skb, write_len);\n}\n\nstatic inline int bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\tint err = __bpf_try_make_writable(skb, write_len);\n\n\tbpf_compute_data_pointers(skb);\n\treturn err;\n}\n\nstatic int bpf_try_make_head_writable(struct sk_buff *skb)\n{\n\treturn bpf_try_make_writable(skb, skb_headlen(skb));\n}\n\nstatic inline void bpf_push_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpush_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nstatic inline void bpf_pull_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nBPF_CALL_5(bpf_skb_store_bytes, struct sk_buff *, skb, u32, offset,\n\t   const void *, from, u32, len, u64, flags)\n{\n\tvoid *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_RECOMPUTE_CSUM | BPF_F_INVALIDATE_HASH)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + len)))\n\t\treturn -EFAULT;\n\n\tptr = skb->data + offset;\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpull_rcsum(skb, ptr, len, offset);\n\n\tmemcpy(ptr, from, len);\n\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpush_rcsum(skb, ptr, len, offset);\n\tif (flags & BPF_F_INVALIDATE_HASH)\n\t\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_store_bytes_proto = {\n\t.func\t\t= bpf_skb_store_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_skb_load_bytes, const struct sk_buff *, skb, u32, offset,\n\t   void *, to, u32, len)\n{\n\tvoid *ptr;\n\n\tif (unlikely(offset > 0xffff))\n\t\tgoto err_clear;\n\n\tptr = skb_header_pointer(skb, offset, len, to);\n\tif (unlikely(!ptr))\n\t\tgoto err_clear;\n\tif (ptr != to)\n\t\tmemcpy(to, ptr, len);\n\n\treturn 0;\nerr_clear:\n\tmemset(to, 0, len);\n\treturn -EFAULT;\n}\n\nstatic const struct bpf_func_proto bpf_skb_load_bytes_proto = {\n\t.func\t\t= bpf_skb_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_2(bpf_skb_pull_data, struct sk_buff *, skb, u32, len)\n{\n\t/* Idea is the following: should the needed direct read/write\n\t * test fail during runtime, we can pull in more data and redo\n\t * again, since implicitly, we invalidate previous checks here.\n\t *\n\t * Or, since we know how much we need to make read/writeable,\n\t * this can be done once at the program beginning for direct\n\t * access case. By this we overcome limitations of only current\n\t * headroom being accessible.\n\t */\n\treturn bpf_try_make_writable(skb, len ? : skb_headlen(skb));\n}\n\nstatic const struct bpf_func_proto bpf_skb_pull_data_proto = {\n\t.func\t\t= bpf_skb_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l3_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tcsum_replace_by_diff(ptr, to);\n\t\tbreak;\n\tcase 2:\n\t\tcsum_replace2(ptr, from, to);\n\t\tbreak;\n\tcase 4:\n\t\tcsum_replace4(ptr, from, to);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l3_csum_replace_proto = {\n\t.func\t\t= bpf_l3_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l4_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\tbool is_pseudo = flags & BPF_F_PSEUDO_HDR;\n\tbool is_mmzero = flags & BPF_F_MARK_MANGLED_0;\n\tbool do_mforce = flags & BPF_F_MARK_ENFORCE;\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_MARK_MANGLED_0 | BPF_F_MARK_ENFORCE |\n\t\t\t       BPF_F_PSEUDO_HDR | BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tif (is_mmzero && !do_mforce && !*ptr)\n\t\treturn 0;\n\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tinet_proto_csum_replace_by_diff(ptr, skb, to, is_pseudo);\n\t\tbreak;\n\tcase 2:\n\t\tinet_proto_csum_replace2(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tcase 4:\n\t\tinet_proto_csum_replace4(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_mmzero && !*ptr)\n\t\t*ptr = CSUM_MANGLED_0;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l4_csum_replace_proto = {\n\t.func\t\t= bpf_l4_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_csum_diff, __be32 *, from, u32, from_size,\n\t   __be32 *, to, u32, to_size, __wsum, seed)\n{\n\tstruct bpf_scratchpad *sp = this_cpu_ptr(&bpf_sp);\n\tu32 diff_size = from_size + to_size;\n\tint i, j = 0;\n\n\t/* This is quite flexible, some examples:\n\t *\n\t * from_size == 0, to_size > 0,  seed := csum --> pushing data\n\t * from_size > 0,  to_size == 0, seed := csum --> pulling data\n\t * from_size > 0,  to_size > 0,  seed := 0    --> diffing data\n\t *\n\t * Even for diffing, from_size and to_size don't need to be equal.\n\t */\n\tif (unlikely(((from_size | to_size) & (sizeof(__be32) - 1)) ||\n\t\t     diff_size > sizeof(sp->diff)))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < from_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = ~from[i];\n\tfor (i = 0; i <   to_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = to[i];\n\n\treturn csum_partial(sp->diff, diff_size, seed);\n}\n\nstatic const struct bpf_func_proto bpf_csum_diff_proto = {\n\t.func\t\t= bpf_csum_diff,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM_OR_NULL,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_PTR_TO_MEM_OR_NULL,\n\t.arg4_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_csum_update, struct sk_buff *, skb, __wsum, csum)\n{\n\t/* The interface is to be used in combination with bpf_csum_diff()\n\t * for direct packet writes. csum rotation for alignment as well\n\t * as emulating csum_sub() can be done from the eBPF program.\n\t */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\treturn (skb->csum = csum_add(skb->csum, csum));\n\n\treturn -ENOTSUPP;\n}\n\nstatic const struct bpf_func_proto bpf_csum_update_proto = {\n\t.func\t\t= bpf_csum_update,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic inline int __bpf_rx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn dev_forward_skb(dev, skb);\n}\n\nstatic inline int __bpf_rx_skb_no_mac(struct net_device *dev,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb);\n\n\tif (likely(!ret)) {\n\t\tskb->dev = dev;\n\t\tret = netif_rx(skb);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int __bpf_tx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (unlikely(__this_cpu_read(xmit_recursion) > XMIT_RECURSION_LIMIT)) {\n\t\tnet_crit_ratelimited(\"bpf: recursion limit reached on datapath, buggy bpf program?\\n\");\n\t\tkfree_skb(skb);\n\t\treturn -ENETDOWN;\n\t}\n\n\tskb->dev = dev;\n\n\t__this_cpu_inc(xmit_recursion);\n\tret = dev_queue_xmit(skb);\n\t__this_cpu_dec(xmit_recursion);\n\n\treturn ret;\n}\n\nstatic int __bpf_redirect_no_mac(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\t/* skb->mac_len is not set on normal egress */\n\tunsigned int mlen = skb->network_header - skb->mac_header;\n\n\t__skb_pull(skb, mlen);\n\n\t/* At ingress, the mac header has already been pulled once.\n\t * At egress, skb_pospull_rcsum has to be done in case that\n\t * the skb is originated from ingress (i.e. a forwarded skb)\n\t * to ensure that rcsum starts at net header.\n\t */\n\tif (!skb_at_tc_ingress(skb))\n\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), mlen);\n\tskb_pop_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb_no_mac(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect_common(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\t/* Verify that a link layer header is carried */\n\tif (unlikely(skb->mac_header >= skb->network_header)) {\n\t\tkfree_skb(skb);\n\t\treturn -ERANGE;\n\t}\n\n\tbpf_push_mac_rcsum(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect(struct sk_buff *skb, struct net_device *dev,\n\t\t\t  u32 flags)\n{\n\tif (dev_is_mac_header_xmit(dev))\n\t\treturn __bpf_redirect_common(skb, dev, flags);\n\telse\n\t\treturn __bpf_redirect_no_mac(skb, dev, flags);\n}\n\nBPF_CALL_3(bpf_clone_redirect, struct sk_buff *, skb, u32, ifindex, u64, flags)\n{\n\tstruct net_device *dev;\n\tstruct sk_buff *clone;\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn -EINVAL;\n\n\tdev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);\n\tif (unlikely(!dev))\n\t\treturn -EINVAL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (unlikely(!clone))\n\t\treturn -ENOMEM;\n\n\t/* For direct write, we need to keep the invariant that the skbs\n\t * we're dealing with need to be uncloned. Should uncloning fail\n\t * here, we need to free the just generated clone to unclone once\n\t * again.\n\t */\n\tret = bpf_try_make_head_writable(skb);\n\tif (unlikely(ret)) {\n\t\tkfree_skb(clone);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn __bpf_redirect(clone, dev, flags);\n}\n\nstatic const struct bpf_func_proto bpf_clone_redirect_proto = {\n\t.func           = bpf_clone_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nstruct redirect_info {\n\tu32 ifindex;\n\tu32 flags;\n\tstruct bpf_map *map;\n\tstruct bpf_map *map_to_flush;\n\tunsigned long   map_owner;\n};\n\nstatic DEFINE_PER_CPU(struct redirect_info, redirect_info);\n\nBPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn TC_ACT_SHOT;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\n\treturn TC_ACT_REDIRECT;\n}\n\nint skb_do_redirect(struct sk_buff *skb)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_index_rcu(dev_net(skb->dev), ri->ifindex);\n\tri->ifindex = 0;\n\tif (unlikely(!dev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\n\treturn __bpf_redirect(skb, dev, ri->flags);\n}\n\nstatic const struct bpf_func_proto bpf_redirect_proto = {\n\t.func           = bpf_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_sk_redirect_map, struct sk_buff *, skb,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\t/* If user passes invalid input drop the packet. */\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\ttcb->bpf.key = key;\n\ttcb->bpf.flags = flags;\n\ttcb->bpf.map = map;\n\n\treturn SK_PASS;\n}\n\nstruct sock *do_sk_redirect_map(struct sk_buff *skb)\n{\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\tstruct sock *sk = NULL;\n\n\tif (tcb->bpf.map) {\n\t\tsk = __sock_map_lookup_elem(tcb->bpf.map, tcb->bpf.key);\n\n\t\ttcb->bpf.key = 0;\n\t\ttcb->bpf.map = NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic const struct bpf_func_proto bpf_sk_redirect_map_proto = {\n\t.func           = bpf_sk_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_redirect_map, struct sk_msg_buff *, msg,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\t/* If user passes invalid input drop the packet. */\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tmsg->key = key;\n\tmsg->flags = flags;\n\tmsg->map = map;\n\n\treturn SK_PASS;\n}\n\nstruct sock *do_msg_redirect_map(struct sk_msg_buff *msg)\n{\n\tstruct sock *sk = NULL;\n\n\tif (msg->map) {\n\t\tsk = __sock_map_lookup_elem(msg->map, msg->key);\n\n\t\tmsg->key = 0;\n\t\tmsg->map = NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic const struct bpf_func_proto bpf_msg_redirect_map_proto = {\n\t.func           = bpf_msg_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_apply_bytes, struct sk_msg_buff *, msg, u32, bytes)\n{\n\tmsg->apply_bytes = bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_apply_bytes_proto = {\n\t.func           = bpf_msg_apply_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_cork_bytes, struct sk_msg_buff *, msg, u32, bytes)\n{\n\tmsg->cork_bytes = bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_cork_bytes_proto = {\n\t.func           = bpf_msg_cork_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_pull_data,\n\t   struct sk_msg_buff *, msg, u32, start, u32, end, u64, flags)\n{\n\tunsigned int len = 0, offset = 0, copy = 0;\n\tstruct scatterlist *sg = msg->sg_data;\n\tint first_sg, last_sg, i, shift;\n\tunsigned char *p, *to, *from;\n\tint bytes = end - start;\n\tstruct page *page;\n\n\tif (unlikely(flags || end <= start))\n\t\treturn -EINVAL;\n\n\t/* First find the starting scatterlist element */\n\ti = msg->sg_start;\n\tdo {\n\t\tlen = sg[i].length;\n\t\toffset += len;\n\t\tif (start < offset + len)\n\t\t\tbreak;\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (i != msg->sg_end);\n\n\tif (unlikely(start >= offset + len))\n\t\treturn -EINVAL;\n\n\tif (!msg->sg_copy[i] && bytes <= len)\n\t\tgoto out;\n\n\tfirst_sg = i;\n\n\t/* At this point we need to linearize multiple scatterlist\n\t * elements or a single shared page. Either way we need to\n\t * copy into a linear buffer exclusively owned by BPF. Then\n\t * place the buffer in the scatterlist and fixup the original\n\t * entries by removing the entries now in the linear buffer\n\t * and shifting the remaining entries. For now we do not try\n\t * to copy partial entries to avoid complexity of running out\n\t * of sg_entry slots. The downside is reading a single byte\n\t * will copy the entire sg entry.\n\t */\n\tdo {\n\t\tcopy += sg[i].length;\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t\tif (bytes < copy)\n\t\t\tbreak;\n\t} while (i != msg->sg_end);\n\tlast_sg = i;\n\n\tif (unlikely(copy < end - start))\n\t\treturn -EINVAL;\n\n\tpage = alloc_pages(__GFP_NOWARN | GFP_ATOMIC, get_order(copy));\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\tp = page_address(page);\n\toffset = 0;\n\n\ti = first_sg;\n\tdo {\n\t\tfrom = sg_virt(&sg[i]);\n\t\tlen = sg[i].length;\n\t\tto = p + offset;\n\n\t\tmemcpy(to, from, len);\n\t\toffset += len;\n\t\tsg[i].length = 0;\n\t\tput_page(sg_page(&sg[i]));\n\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (i != last_sg);\n\n\tsg[first_sg].length = copy;\n\tsg_set_page(&sg[first_sg], page, copy, 0);\n\n\t/* To repair sg ring we need to shift entries. If we only\n\t * had a single entry though we can just replace it and\n\t * be done. Otherwise walk the ring and shift the entries.\n\t */\n\tshift = last_sg - first_sg - 1;\n\tif (!shift)\n\t\tgoto out;\n\n\ti = first_sg + 1;\n\tdo {\n\t\tint move_from;\n\n\t\tif (i + shift >= MAX_SKB_FRAGS)\n\t\t\tmove_from = i + shift - MAX_SKB_FRAGS;\n\t\telse\n\t\t\tmove_from = i + shift;\n\n\t\tif (move_from == msg->sg_end)\n\t\t\tbreak;\n\n\t\tsg[i] = sg[move_from];\n\t\tsg[move_from].length = 0;\n\t\tsg[move_from].page_link = 0;\n\t\tsg[move_from].offset = 0;\n\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (1);\n\tmsg->sg_end -= shift;\n\tif (msg->sg_end < 0)\n\t\tmsg->sg_end += MAX_SKB_FRAGS;\nout:\n\tmsg->data = sg_virt(&sg[i]) + start - offset;\n\tmsg->data_end = msg->data + bytes;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_pull_data_proto = {\n\t.func\t\t= bpf_msg_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_get_cgroup_classid, const struct sk_buff *, skb)\n{\n\treturn task_get_classid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_cgroup_classid_proto = {\n\t.func           = bpf_get_cgroup_classid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_route_realm, const struct sk_buff *, skb)\n{\n\treturn dst_tclassid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_route_realm_proto = {\n\t.func           = bpf_get_route_realm,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_hash_recalc, struct sk_buff *, skb)\n{\n\t/* If skb_clear_hash() was called due to mangling, we can\n\t * trigger SW recalculation here. Later access to hash\n\t * can then use the inline skb->hash via context directly\n\t * instead of calling this helper again.\n\t */\n\treturn skb_get_hash(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_hash_recalc_proto = {\n\t.func\t\t= bpf_get_hash_recalc,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_set_hash_invalid, struct sk_buff *, skb)\n{\n\t/* After all direct packet write, this can be used once for\n\t * triggering a lazy recalc on next skb_get_hash() invocation.\n\t */\n\tskb_clear_hash(skb);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_invalid_proto = {\n\t.func\t\t= bpf_set_hash_invalid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_2(bpf_set_hash, struct sk_buff *, skb, u32, hash)\n{\n\t/* Set user specified hash as L4(+), so that it gets returned\n\t * on skb_get_hash() call unless BPF prog later on triggers a\n\t * skb_clear_hash().\n\t */\n\t__skb_set_sw_hash(skb, hash, true);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_proto = {\n\t.func\t\t= bpf_set_hash,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_vlan_push, struct sk_buff *, skb, __be16, vlan_proto,\n\t   u16, vlan_tci)\n{\n\tint ret;\n\n\tif (unlikely(vlan_proto != htons(ETH_P_8021Q) &&\n\t\t     vlan_proto != htons(ETH_P_8021AD)))\n\t\tvlan_proto = htons(ETH_P_8021Q);\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_push(skb, vlan_proto, vlan_tci);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_skb_vlan_push_proto = {\n\t.func           = bpf_skb_vlan_push,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\nEXPORT_SYMBOL_GPL(bpf_skb_vlan_push_proto);\n\nBPF_CALL_1(bpf_skb_vlan_pop, struct sk_buff *, skb)\n{\n\tint ret;\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_pop(skb);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_skb_vlan_pop_proto = {\n\t.func           = bpf_skb_vlan_pop,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\nEXPORT_SYMBOL_GPL(bpf_skb_vlan_pop_proto);\n\nstatic int bpf_skb_generic_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\t/* Caller already did skb_cow() with len as headroom,\n\t * so no need to do it here.\n\t */\n\tskb_push(skb, len);\n\tmemmove(skb->data, skb->data + len, off);\n\tmemset(skb->data + off, 0, len);\n\n\t/* No skb_postpush_rcsum(skb, skb->data + off, len)\n\t * needed here as it does not change the skb->csum\n\t * result for checksum complete when summing over\n\t * zeroed blocks.\n\t */\n\treturn 0;\n}\n\nstatic int bpf_skb_generic_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\t/* skb_ensure_writable() is not needed here, as we're\n\t * already working on an uncloned skb.\n\t */\n\tif (unlikely(!pskb_may_pull(skb, off + len)))\n\t\treturn -ENOMEM;\n\n\tskb_postpull_rcsum(skb, skb->data + off, len);\n\tmemmove(skb->data + len, skb->data, off);\n\t__skb_pull(skb, len);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_hdr_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t/* There's no need for __skb_push()/__skb_pull() pair to\n\t * get to the start of the mac header as we're guaranteed\n\t * to always start from here under eBPF.\n\t */\n\tret = bpf_skb_generic_push(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header -= len;\n\t\tskb->network_header -= len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_net_hdr_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t/* Same here, __skb_push()/__skb_pull() pair not needed. */\n\tret = bpf_skb_generic_pop(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header += len;\n\t\tskb->network_header += len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_proto_4_to_6(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_cow(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* SKB_GSO_TCPV4 needs to be changed into\n\t\t * SKB_GSO_TCPV6.\n\t\t */\n\t\tif (shinfo->gso_type & SKB_GSO_TCPV4) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV4;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV6;\n\t\t}\n\n\t\t/* Due to IPv6 header, MSS needs to be downgraded. */\n\t\tskb_decrease_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_6_to_4(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* SKB_GSO_TCPV6 needs to be changed into\n\t\t * SKB_GSO_TCPV4.\n\t\t */\n\t\tif (shinfo->gso_type & SKB_GSO_TCPV6) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV6;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV4;\n\t\t}\n\n\t\t/* Due to IPv4 header, MSS can be upgraded. */\n\t\tskb_increase_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\tskb->protocol = htons(ETH_P_IP);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_xlat(struct sk_buff *skb, __be16 to_proto)\n{\n\t__be16 from_proto = skb->protocol;\n\n\tif (from_proto == htons(ETH_P_IP) &&\n\t      to_proto == htons(ETH_P_IPV6))\n\t\treturn bpf_skb_proto_4_to_6(skb);\n\n\tif (from_proto == htons(ETH_P_IPV6) &&\n\t      to_proto == htons(ETH_P_IP))\n\t\treturn bpf_skb_proto_6_to_4(skb);\n\n\treturn -ENOTSUPP;\n}\n\nBPF_CALL_3(bpf_skb_change_proto, struct sk_buff *, skb, __be16, proto,\n\t   u64, flags)\n{\n\tint ret;\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\t/* General idea is that this helper does the basic groundwork\n\t * needed for changing the protocol, and eBPF program fills the\n\t * rest through bpf_skb_store_bytes(), bpf_lX_csum_replace()\n\t * and other helpers, rather than passing a raw buffer here.\n\t *\n\t * The rationale is to keep this minimal and without a need to\n\t * deal with raw packet data. F.e. even if we would pass buffers\n\t * here, the program still needs to call the bpf_lX_csum_replace()\n\t * helpers anyway. Plus, this way we keep also separation of\n\t * concerns, since f.e. bpf_skb_store_bytes() should only take\n\t * care of stores.\n\t *\n\t * Currently, additional options and extension header space are\n\t * not supported, but flags register is reserved so we can adapt\n\t * that. For offloads, we mark packet as dodgy, so that headers\n\t * need to be verified first.\n\t */\n\tret = bpf_skb_proto_xlat(skb, proto);\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_proto_proto = {\n\t.func\t\t= bpf_skb_change_proto,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_skb_change_type, struct sk_buff *, skb, u32, pkt_type)\n{\n\t/* We only allow a restricted subset to be changed for now. */\n\tif (unlikely(!skb_pkt_type_ok(skb->pkt_type) ||\n\t\t     !skb_pkt_type_ok(pkt_type)))\n\t\treturn -EINVAL;\n\n\tskb->pkt_type = pkt_type;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_type_proto = {\n\t.func\t\t= bpf_skb_change_type,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic u32 bpf_skb_net_base_len(const struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn sizeof(struct iphdr);\n\tcase htons(ETH_P_IPV6):\n\t\treturn sizeof(struct ipv6hdr);\n\tdefault:\n\t\treturn ~0U;\n\t}\n}\n\nstatic int bpf_skb_net_grow(struct sk_buff *skb, u32 len_diff)\n{\n\tu32 off = skb_mac_header_len(skb) + bpf_skb_net_base_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_cow(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* Due to header grow, MSS needs to be downgraded. */\n\t\tskb_decrease_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_shrink(struct sk_buff *skb, u32 len_diff)\n{\n\tu32 off = skb_mac_header_len(skb) + bpf_skb_net_base_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* Due to header shrink, MSS can be upgraded. */\n\t\tskb_increase_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic u32 __bpf_skb_max_len(const struct sk_buff *skb)\n{\n\treturn skb->dev->mtu + skb->dev->hard_header_len;\n}\n\nstatic int bpf_skb_adjust_net(struct sk_buff *skb, s32 len_diff)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tu32 len_cur, len_diff_abs = abs(len_diff);\n\tu32 len_min = bpf_skb_net_base_len(skb);\n\tu32 len_max = __bpf_skb_max_len(skb);\n\t__be16 proto = skb->protocol;\n\tbool shrink = len_diff < 0;\n\tint ret;\n\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\tif (unlikely(proto != htons(ETH_P_IP) &&\n\t\t     proto != htons(ETH_P_IPV6)))\n\t\treturn -ENOTSUPP;\n\n\tlen_cur = skb->len - skb_network_offset(skb);\n\tif (skb_transport_header_was_set(skb) && !trans_same)\n\t\tlen_cur = skb_network_header_len(skb);\n\tif ((shrink && (len_diff_abs >= len_cur ||\n\t\t\tlen_cur - len_diff_abs < len_min)) ||\n\t    (!shrink && (skb->len + len_diff_abs > len_max &&\n\t\t\t !skb_is_gso(skb))))\n\t\treturn -ENOTSUPP;\n\n\tret = shrink ? bpf_skb_net_shrink(skb, len_diff_abs) :\n\t\t       bpf_skb_net_grow(skb, len_diff_abs);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nBPF_CALL_4(bpf_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\tif (likely(mode == BPF_ADJ_ROOM_NET))\n\t\treturn bpf_skb_adjust_net(skb, len_diff);\n\n\treturn -ENOTSUPP;\n}\n\nstatic const struct bpf_func_proto bpf_skb_adjust_room_proto = {\n\t.func\t\t= bpf_skb_adjust_room,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic u32 __bpf_skb_min_len(const struct sk_buff *skb)\n{\n\tu32 min_len = skb_network_offset(skb);\n\n\tif (skb_transport_header_was_set(skb))\n\t\tmin_len = skb_transport_offset(skb);\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tmin_len = skb_checksum_start_offset(skb) +\n\t\t\t  skb->csum_offset + sizeof(__sum16);\n\treturn min_len;\n}\n\nstatic int bpf_skb_grow_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\tunsigned int old_len = skb->len;\n\tint ret;\n\n\tret = __skb_grow_rcsum(skb, new_len);\n\tif (!ret)\n\t\tmemset(skb->data + old_len, 0, new_len - old_len);\n\treturn ret;\n}\n\nstatic int bpf_skb_trim_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\treturn __skb_trim_rcsum(skb, new_len);\n}\n\nBPF_CALL_3(bpf_skb_change_tail, struct sk_buff *, skb, u32, new_len,\n\t   u64, flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_tail_proto = {\n\t.func\t\t= bpf_skb_change_tail,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_change_head, struct sk_buff *, skb, u32, head_room,\n\t   u64, flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_head_proto = {\n\t.func\t\t= bpf_skb_change_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic unsigned long xdp_get_metalen(const struct xdp_buff *xdp)\n{\n\treturn xdp_data_meta_unsupported(xdp) ? 0 :\n\t       xdp->data - xdp->data_meta;\n}\n\nBPF_CALL_2(bpf_xdp_adjust_head, struct xdp_buff *, xdp, int, offset)\n{\n\tunsigned long metalen = xdp_get_metalen(xdp);\n\tvoid *data_start = xdp->data_hard_start + metalen;\n\tvoid *data = xdp->data + offset;\n\n\tif (unlikely(data < data_start ||\n\t\t     data > xdp->data_end - ETH_HLEN))\n\t\treturn -EINVAL;\n\n\tif (metalen)\n\t\tmemmove(xdp->data_meta + offset,\n\t\t\txdp->data_meta, metalen);\n\txdp->data_meta += offset;\n\txdp->data = data;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_head_proto = {\n\t.func\t\t= bpf_xdp_adjust_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_xdp_adjust_meta, struct xdp_buff *, xdp, int, offset)\n{\n\tvoid *meta = xdp->data_meta + offset;\n\tunsigned long metalen = xdp->data - meta;\n\n\tif (xdp_data_meta_unsupported(xdp))\n\t\treturn -ENOTSUPP;\n\tif (unlikely(meta < xdp->data_hard_start ||\n\t\t     meta > xdp->data))\n\t\treturn -EINVAL;\n\tif (unlikely((metalen & (sizeof(__u32) - 1)) ||\n\t\t     (metalen > 32)))\n\t\treturn -EACCES;\n\n\txdp->data_meta = meta;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_meta_proto = {\n\t.func\t\t= bpf_xdp_adjust_meta,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic int __bpf_tx_xdp(struct net_device *dev,\n\t\t\tstruct bpf_map *map,\n\t\t\tstruct xdp_buff *xdp,\n\t\t\tu32 index)\n{\n\tint err;\n\n\tif (!dev->netdev_ops->ndo_xdp_xmit) {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\terr = dev->netdev_ops->ndo_xdp_xmit(dev, xdp);\n\tif (err)\n\t\treturn err;\n\tdev->netdev_ops->ndo_xdp_flush(dev);\n\treturn 0;\n}\n\nstatic int __bpf_tx_xdp_map(struct net_device *dev_rx, void *fwd,\n\t\t\t    struct bpf_map *map,\n\t\t\t    struct xdp_buff *xdp,\n\t\t\t    u32 index)\n{\n\tint err;\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tstruct net_device *dev = fwd;\n\n\t\tif (!dev->netdev_ops->ndo_xdp_xmit)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = dev->netdev_ops->ndo_xdp_xmit(dev, xdp);\n\t\tif (err)\n\t\t\treturn err;\n\t\t__dev_map_insert_ctx(map, index);\n\n\t} else if (map->map_type == BPF_MAP_TYPE_CPUMAP) {\n\t\tstruct bpf_cpu_map_entry *rcpu = fwd;\n\n\t\terr = cpu_map_enqueue(rcpu, xdp, dev_rx);\n\t\tif (err)\n\t\t\treturn err;\n\t\t__cpu_map_insert_ctx(map, index);\n\t}\n\treturn 0;\n}\n\nvoid xdp_do_flush_map(void)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct bpf_map *map = ri->map_to_flush;\n\n\tri->map_to_flush = NULL;\n\tif (map) {\n\t\tswitch (map->map_type) {\n\t\tcase BPF_MAP_TYPE_DEVMAP:\n\t\t\t__dev_map_flush(map);\n\t\t\tbreak;\n\t\tcase BPF_MAP_TYPE_CPUMAP:\n\t\t\t__cpu_map_flush(map);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(xdp_do_flush_map);\n\nstatic void *__xdp_map_lookup_elem(struct bpf_map *map, u32 index)\n{\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\treturn __dev_map_lookup_elem(map, index);\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\treturn __cpu_map_lookup_elem(map, index);\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic inline bool xdp_map_invalid(const struct bpf_prog *xdp_prog,\n\t\t\t\t   unsigned long aux)\n{\n\treturn (unsigned long)xdp_prog->aux != aux;\n}\n\nstatic int xdp_do_redirect_map(struct net_device *dev, struct xdp_buff *xdp,\n\t\t\t       struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tunsigned long map_owner = ri->map_owner;\n\tstruct bpf_map *map = ri->map;\n\tu32 index = ri->ifindex;\n\tvoid *fwd = NULL;\n\tint err;\n\n\tri->ifindex = 0;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\tif (unlikely(xdp_map_invalid(xdp_prog, map_owner))) {\n\t\terr = -EFAULT;\n\t\tmap = NULL;\n\t\tgoto err;\n\t}\n\n\tfwd = __xdp_map_lookup_elem(map, index);\n\tif (!fwd) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\tif (ri->map_to_flush && ri->map_to_flush != map)\n\t\txdp_do_flush_map();\n\n\terr = __bpf_tx_xdp_map(dev, fwd, map, xdp, index);\n\tif (unlikely(err))\n\t\tgoto err;\n\n\tri->map_to_flush = map;\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map, index, err);\n\treturn err;\n}\n\nint xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp,\n\t\t    struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct net_device *fwd;\n\tu32 index = ri->ifindex;\n\tint err;\n\n\tif (ri->map)\n\t\treturn xdp_do_redirect_map(dev, xdp, xdp_prog);\n\n\tfwd = dev_get_by_index_rcu(dev_net(dev), index);\n\tri->ifindex = 0;\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = __bpf_tx_xdp(fwd, NULL, xdp, 0);\n\tif (unlikely(err))\n\t\tgoto err;\n\n\t_trace_xdp_redirect(dev, xdp_prog, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_err(dev, xdp_prog, index, err);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xdp_do_redirect);\n\nstatic int __xdp_generic_ok_fwd_dev(struct sk_buff *skb, struct net_device *fwd)\n{\n\tunsigned int len;\n\n\tif (unlikely(!(fwd->flags & IFF_UP)))\n\t\treturn -ENETDOWN;\n\n\tlen = fwd->mtu + fwd->hard_header_len + VLAN_HLEN;\n\tif (skb->len > len)\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}\n\nstatic int xdp_do_generic_redirect_map(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tunsigned long map_owner = ri->map_owner;\n\tstruct bpf_map *map = ri->map;\n\tstruct net_device *fwd = NULL;\n\tu32 index = ri->ifindex;\n\tint err = 0;\n\n\tri->ifindex = 0;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\tif (unlikely(xdp_map_invalid(xdp_prog, map_owner))) {\n\t\terr = -EFAULT;\n\t\tmap = NULL;\n\t\tgoto err;\n\t}\n\tfwd = __xdp_map_lookup_elem(map, index);\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tif (unlikely((err = __xdp_generic_ok_fwd_dev(skb, fwd))))\n\t\t\tgoto err;\n\t\tskb->dev = fwd;\n\t} else {\n\t\t/* TODO: Handle BPF_MAP_TYPE_CPUMAP */\n\t\terr = -EBADRQC;\n\t\tgoto err;\n\t}\n\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map, index, err);\n\treturn err;\n}\n\nint xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tu32 index = ri->ifindex;\n\tstruct net_device *fwd;\n\tint err = 0;\n\n\tif (ri->map)\n\t\treturn xdp_do_generic_redirect_map(dev, skb, xdp_prog);\n\n\tri->ifindex = 0;\n\tfwd = dev_get_by_index_rcu(dev_net(dev), index);\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (unlikely((err = __xdp_generic_ok_fwd_dev(skb, fwd))))\n\t\tgoto err;\n\n\tskb->dev = fwd;\n\t_trace_xdp_redirect(dev, xdp_prog, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_err(dev, xdp_prog, index, err);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xdp_do_generic_redirect);\n\nBPF_CALL_2(bpf_xdp_redirect, u32, ifindex, u64, flags)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn XDP_ABORTED;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\treturn XDP_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_redirect_proto = {\n\t.func           = bpf_xdp_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_xdp_redirect_map, struct bpf_map *, map, u32, ifindex, u64, flags,\n\t   unsigned long, map_owner)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn XDP_ABORTED;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\tri->map = map;\n\tri->map_owner = map_owner;\n\n\treturn XDP_REDIRECT;\n}\n\n/* Note, arg4 is hidden from users and populated by the verifier\n * with the right pointer.\n */\nstatic const struct bpf_func_proto bpf_xdp_redirect_map_proto = {\n\t.func           = bpf_xdp_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_CONST_MAP_PTR,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nbool bpf_helper_changes_pkt_data(void *func)\n{\n\tif (func == bpf_skb_vlan_push ||\n\t    func == bpf_skb_vlan_pop ||\n\t    func == bpf_skb_store_bytes ||\n\t    func == bpf_skb_change_proto ||\n\t    func == bpf_skb_change_head ||\n\t    func == bpf_skb_change_tail ||\n\t    func == bpf_skb_adjust_room ||\n\t    func == bpf_skb_pull_data ||\n\t    func == bpf_clone_redirect ||\n\t    func == bpf_l3_csum_replace ||\n\t    func == bpf_l4_csum_replace ||\n\t    func == bpf_xdp_adjust_head ||\n\t    func == bpf_xdp_adjust_meta ||\n\t    func == bpf_msg_pull_data)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic unsigned long bpf_skb_copy(void *dst_buff, const void *skb,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tvoid *ptr = skb_header_pointer(skb, off, len, dst_buff);\n\n\tif (unlikely(!ptr))\n\t\treturn len;\n\tif (ptr != dst_buff)\n\t\tmemcpy(dst_buff, ptr, len);\n\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_skb_event_output, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 skb_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(skb_size > skb->len))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, skb, skb_size,\n\t\t\t\tbpf_skb_copy);\n}\n\nstatic const struct bpf_func_proto bpf_skb_event_output_proto = {\n\t.func\t\t= bpf_skb_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nstatic unsigned short bpf_tunnel_key_af(u64 flags)\n{\n\treturn flags & BPF_F_TUNINFO_IPV6 ? AF_INET6 : AF_INET;\n}\n\nBPF_CALL_4(bpf_skb_get_tunnel_key, struct sk_buff *, skb, struct bpf_tunnel_key *, to,\n\t   u32, size, u64, flags)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tvoid *to_orig = to;\n\tint err;\n\n\tif (unlikely(!info || (flags & ~(BPF_F_TUNINFO_IPV6)))) {\n\t\terr = -EINVAL;\n\t\tgoto err_clear;\n\t}\n\tif (ip_tunnel_info_af(info) != bpf_tunnel_key_af(flags)) {\n\t\terr = -EPROTO;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\terr = -EINVAL;\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\t\tgoto set_compat;\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t/* Fixup deprecated structure layouts here, so we have\n\t\t\t * a common path later on.\n\t\t\t */\n\t\t\tif (ip_tunnel_info_af(info) != AF_INET)\n\t\t\t\tgoto err_clear;\nset_compat:\n\t\t\tto = (struct bpf_tunnel_key *)compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n\t}\n\n\tto->tunnel_id = be64_to_cpu(info->key.tun_id);\n\tto->tunnel_tos = info->key.tos;\n\tto->tunnel_ttl = info->key.ttl;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tmemcpy(to->remote_ipv6, &info->key.u.ipv6.src,\n\t\t       sizeof(to->remote_ipv6));\n\t\tto->tunnel_label = be32_to_cpu(info->key.label);\n\t} else {\n\t\tto->remote_ipv4 = be32_to_cpu(info->key.u.ipv4.src);\n\t}\n\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key)))\n\t\tmemcpy(to_orig, to, size);\n\n\treturn 0;\nerr_clear:\n\tmemset(to_orig, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_get_tunnel_opt, struct sk_buff *, skb, u8 *, to, u32, size)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tint err;\n\n\tif (unlikely(!info ||\n\t\t     !(info->key.tun_flags & TUNNEL_OPTIONS_PRESENT))) {\n\t\terr = -ENOENT;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size < info->options_len)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_clear;\n\t}\n\n\tip_tunnel_info_opts_get(to, info);\n\tif (size > info->options_len)\n\t\tmemset(to + info->options_len, 0, size - info->options_len);\n\n\treturn info->options_len;\nerr_clear:\n\tmemset(to, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic struct metadata_dst __percpu *md_dst;\n\nBPF_CALL_4(bpf_skb_set_tunnel_key, struct sk_buff *, skb,\n\t   const struct bpf_tunnel_key *, from, u32, size, u64, flags)\n{\n\tstruct metadata_dst *md = this_cpu_ptr(md_dst);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tstruct ip_tunnel_info *info;\n\n\tif (unlikely(flags & ~(BPF_F_TUNINFO_IPV6 | BPF_F_ZERO_CSUM_TX |\n\t\t\t       BPF_F_DONT_FRAGMENT | BPF_F_SEQ_NUMBER)))\n\t\treturn -EINVAL;\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t/* Fixup deprecated structure layouts here, so we have\n\t\t\t * a common path later on.\n\t\t\t */\n\t\t\tmemcpy(compat, from, size);\n\t\t\tmemset(compat + size, 0, sizeof(compat) - size);\n\t\t\tfrom = (const struct bpf_tunnel_key *) compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (unlikely((!(flags & BPF_F_TUNINFO_IPV6) && from->tunnel_label) ||\n\t\t     from->tunnel_ext))\n\t\treturn -EINVAL;\n\n\tskb_dst_drop(skb);\n\tdst_hold((struct dst_entry *) md);\n\tskb_dst_set(skb, (struct dst_entry *) md);\n\n\tinfo = &md->u.tun_info;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->mode = IP_TUNNEL_INFO_TX;\n\n\tinfo->key.tun_flags = TUNNEL_KEY | TUNNEL_CSUM | TUNNEL_NOCACHE;\n\tif (flags & BPF_F_DONT_FRAGMENT)\n\t\tinfo->key.tun_flags |= TUNNEL_DONT_FRAGMENT;\n\tif (flags & BPF_F_ZERO_CSUM_TX)\n\t\tinfo->key.tun_flags &= ~TUNNEL_CSUM;\n\tif (flags & BPF_F_SEQ_NUMBER)\n\t\tinfo->key.tun_flags |= TUNNEL_SEQ;\n\n\tinfo->key.tun_id = cpu_to_be64(from->tunnel_id);\n\tinfo->key.tos = from->tunnel_tos;\n\tinfo->key.ttl = from->tunnel_ttl;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tinfo->mode |= IP_TUNNEL_INFO_IPV6;\n\t\tmemcpy(&info->key.u.ipv6.dst, from->remote_ipv6,\n\t\t       sizeof(from->remote_ipv6));\n\t\tinfo->key.label = cpu_to_be32(from->tunnel_label) &\n\t\t\t\t  IPV6_FLOWLABEL_MASK;\n\t} else {\n\t\tinfo->key.u.ipv4.dst = cpu_to_be32(from->remote_ipv4);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_set_tunnel_opt, struct sk_buff *, skb,\n\t   const u8 *, from, u32, size)\n{\n\tstruct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tconst struct metadata_dst *md = this_cpu_ptr(md_dst);\n\n\tif (unlikely(info != &md->u.tun_info || (size & (sizeof(u32) - 1))))\n\t\treturn -EINVAL;\n\tif (unlikely(size > IP_TUNNEL_OPTS_MAX))\n\t\treturn -ENOMEM;\n\n\tip_tunnel_info_opts_set(info, from, size);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nbpf_get_skb_set_tunnel_proto(enum bpf_func_id which)\n{\n\tif (!md_dst) {\n\t\tstruct metadata_dst __percpu *tmp;\n\n\t\ttmp = metadata_dst_alloc_percpu(IP_TUNNEL_OPTS_MAX,\n\t\t\t\t\t\tMETADATA_IP_TUNNEL,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!tmp)\n\t\t\treturn NULL;\n\t\tif (cmpxchg(&md_dst, NULL, tmp))\n\t\t\tmetadata_dst_free_percpu(tmp);\n\t}\n\n\tswitch (which) {\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn &bpf_skb_set_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn &bpf_skb_set_tunnel_opt_proto;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nBPF_CALL_3(bpf_skb_under_cgroup, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u32, idx)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct cgroup *cgrp;\n\tstruct sock *sk;\n\n\tsk = skb_to_full_sk(skb);\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn -ENOENT;\n\tif (unlikely(idx >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tcgrp = READ_ONCE(array->ptrs[idx]);\n\tif (unlikely(!cgrp))\n\t\treturn -EAGAIN;\n\n\treturn sk_under_cgroup_hierarchy(sk, cgrp);\n}\n\nstatic const struct bpf_func_proto bpf_skb_under_cgroup_proto = {\n\t.func\t\t= bpf_skb_under_cgroup,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic unsigned long bpf_xdp_copy(void *dst_buff, const void *src_buff,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tmemcpy(dst_buff, src_buff + off, len);\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_xdp_event_output, struct xdp_buff *, xdp, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 xdp_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(xdp_size > (unsigned long)(xdp->data_end - xdp->data)))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, xdp->data,\n\t\t\t\txdp_size, bpf_xdp_copy);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_event_output_proto = {\n\t.func\t\t= bpf_xdp_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie, struct sk_buff *, skb)\n{\n\treturn skb->sk ? sock_gen_cookie(skb->sk) : 0;\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_proto = {\n\t.func           = bpf_get_socket_cookie,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_socket_uid, struct sk_buff *, skb)\n{\n\tstruct sock *sk = sk_to_full_sk(skb->sk);\n\tkuid_t kuid;\n\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn overflowuid;\n\tkuid = sock_net_uid(sock_net(sk), sk);\n\treturn from_kuid_munged(sock_net(sk)->user_ns, kuid);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_uid_proto = {\n\t.func           = bpf_get_socket_uid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_5(bpf_setsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\tint ret = 0;\n\tint val;\n\n\tif (!sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (level == SOL_SOCKET) {\n\t\tif (optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tval = *((int *)optval);\n\n\t\t/* Only some socketops are supported */\n\t\tswitch (optname) {\n\t\tcase SO_RCVBUF:\n\t\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t\tsk->sk_rcvbuf = max_t(int, val * 2, SOCK_MIN_RCVBUF);\n\t\t\tbreak;\n\t\tcase SO_SNDBUF:\n\t\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\t\tsk->sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF);\n\t\t\tbreak;\n\t\tcase SO_MAX_PACING_RATE:\n\t\t\tsk->sk_max_pacing_rate = val;\n\t\t\tsk->sk_pacing_rate = min(sk->sk_pacing_rate,\n\t\t\t\t\t\t sk->sk_max_pacing_rate);\n\t\t\tbreak;\n\t\tcase SO_PRIORITY:\n\t\t\tsk->sk_priority = val;\n\t\t\tbreak;\n\t\tcase SO_RCVLOWAT:\n\t\t\tif (val < 0)\n\t\t\t\tval = INT_MAX;\n\t\t\tsk->sk_rcvlowat = val ? : 1;\n\t\t\tbreak;\n\t\tcase SO_MARK:\n\t\t\tsk->sk_mark = val;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#ifdef CONFIG_INET\n\t} else if (level == SOL_IP) {\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET)\n\t\t\treturn -EINVAL;\n\n\t\tval = *((int *)optval);\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IP_TOS:\n\t\t\tif (val < -1 || val > 0xff) {\n\t\t\t\tret = -EINVAL;\n\t\t\t} else {\n\t\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\t\t\tif (val == -1)\n\t\t\t\t\tval = 0;\n\t\t\t\tinet->tos = val;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (level == SOL_IPV6) {\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET6)\n\t\t\treturn -EINVAL;\n\n\t\tval = *((int *)optval);\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IPV6_TCLASS:\n\t\t\tif (val < -1 || val > 0xff) {\n\t\t\t\tret = -EINVAL;\n\t\t\t} else {\n\t\t\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\t\t\tif (val == -1)\n\t\t\t\t\tval = 0;\n\t\t\t\tnp->tclass = val;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#endif\n\t} else if (level == SOL_TCP &&\n\t\t   sk->sk_prot->setsockopt == tcp_setsockopt) {\n\t\tif (optname == TCP_CONGESTION) {\n\t\t\tchar name[TCP_CA_NAME_MAX];\n\t\t\tbool reinit = bpf_sock->op > BPF_SOCK_OPS_NEEDS_ECN;\n\n\t\t\tstrncpy(name, optval, min_t(long, optlen,\n\t\t\t\t\t\t    TCP_CA_NAME_MAX-1));\n\t\t\tname[TCP_CA_NAME_MAX-1] = 0;\n\t\t\tret = tcp_set_congestion_control(sk, name, false,\n\t\t\t\t\t\t\t reinit);\n\t\t} else {\n\t\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\t\tif (optlen != sizeof(int))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tval = *((int *)optval);\n\t\t\t/* Only some options are supported */\n\t\t\tswitch (optname) {\n\t\t\tcase TCP_BPF_IW:\n\t\t\t\tif (val <= 0 || tp->data_segs_out > 0)\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\telse\n\t\t\t\t\ttp->snd_cwnd = val;\n\t\t\t\tbreak;\n\t\t\tcase TCP_BPF_SNDCWND_CLAMP:\n\t\t\t\tif (val <= 0) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t} else {\n\t\t\t\t\ttp->snd_cwnd_clamp = val;\n\t\t\t\t\ttp->snd_ssthresh = val;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tret = -EINVAL;\n\t\t\t}\n\t\t}\n#endif\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_setsockopt_proto = {\n\t.func\t\t= bpf_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_getsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\n\tif (!sk_fullsock(sk))\n\t\tgoto err_clear;\n\n#ifdef CONFIG_INET\n\tif (level == SOL_TCP && sk->sk_prot->getsockopt == tcp_getsockopt) {\n\t\tif (optname == TCP_CONGESTION) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\tif (!icsk->icsk_ca_ops || optlen <= 1)\n\t\t\t\tgoto err_clear;\n\t\t\tstrncpy(optval, icsk->icsk_ca_ops->name, optlen);\n\t\t\toptval[optlen - 1] = 0;\n\t\t} else {\n\t\t\tgoto err_clear;\n\t\t}\n\t} else if (level == SOL_IP) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET)\n\t\t\tgoto err_clear;\n\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IP_TOS:\n\t\t\t*((int *)optval) = (int)inet->tos;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (level == SOL_IPV6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET6)\n\t\t\tgoto err_clear;\n\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IPV6_TCLASS:\n\t\t\t*((int *)optval) = (int)np->tclass;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n#endif\n\t} else {\n\t\tgoto err_clear;\n\t}\n\treturn 0;\n#endif\nerr_clear:\n\tmemset(optval, 0, optlen);\n\treturn -EINVAL;\n}\n\nstatic const struct bpf_func_proto bpf_getsockopt_proto = {\n\t.func\t\t= bpf_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_2(bpf_sock_ops_cb_flags_set, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, argval)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\tint val = argval & BPF_SOCK_OPS_ALL_CB_FLAGS;\n\n\tif (!IS_ENABLED(CONFIG_INET) || !sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (val)\n\t\ttcp_sk(sk)->bpf_sock_ops_cb_flags = val;\n\n\treturn argval & (~BPF_SOCK_OPS_ALL_CB_FLAGS);\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_cb_flags_set_proto = {\n\t.func\t\t= bpf_sock_ops_cb_flags_set,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nconst struct ipv6_bpf_stub *ipv6_bpf_stub __read_mostly;\nEXPORT_SYMBOL_GPL(ipv6_bpf_stub);\n\nBPF_CALL_3(bpf_bind, struct bpf_sock_addr_kern *, ctx, struct sockaddr *, addr,\n\t   int, addr_len)\n{\n#ifdef CONFIG_INET\n\tstruct sock *sk = ctx->sk;\n\tint err;\n\n\t/* Binding to port can be expensive so it's prohibited in the helper.\n\t * Only binding to IP is supported.\n\t */\n\terr = -EINVAL;\n\tif (addr->sa_family == AF_INET) {\n\t\tif (addr_len < sizeof(struct sockaddr_in))\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in *)addr)->sin_port != htons(0))\n\t\t\treturn err;\n\t\treturn __inet_bind(sk, addr, addr_len, true, false);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (addr->sa_family == AF_INET6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in6 *)addr)->sin6_port != htons(0))\n\t\t\treturn err;\n\t\t/* ipv6_bpf_stub cannot be NULL, since it's called from\n\t\t * bpf_cgroup_inet6_connect hook and ipv6 is already loaded\n\t\t */\n\t\treturn ipv6_bpf_stub->inet6_bind(sk, addr, addr_len, true, false);\n#endif /* CONFIG_IPV6 */\n\t}\n#endif /* CONFIG_INET */\n\n\treturn -EAFNOSUPPORT;\n}\n\nstatic const struct bpf_func_proto bpf_bind_proto = {\n\t.func\t\t= bpf_bind,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nbpf_base_func_proto(enum bpf_func_id func_id)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_map_lookup_elem:\n\t\treturn &bpf_map_lookup_elem_proto;\n\tcase BPF_FUNC_map_update_elem:\n\t\treturn &bpf_map_update_elem_proto;\n\tcase BPF_FUNC_map_delete_elem:\n\t\treturn &bpf_map_delete_elem_proto;\n\tcase BPF_FUNC_get_prandom_u32:\n\t\treturn &bpf_get_prandom_u32_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_raw_smp_processor_id_proto;\n\tcase BPF_FUNC_get_numa_node_id:\n\t\treturn &bpf_get_numa_node_id_proto;\n\tcase BPF_FUNC_tail_call:\n\t\treturn &bpf_tail_call_proto;\n\tcase BPF_FUNC_ktime_get_ns:\n\t\treturn &bpf_ktime_get_ns_proto;\n\tcase BPF_FUNC_trace_printk:\n\t\tif (capable(CAP_SYS_ADMIN))\n\t\t\treturn bpf_get_trace_printk_proto();\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\t/* inet and inet6 sockets are created in a process\n\t * context so there is always a valid uid/gid\n\t */\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_addr_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\t/* inet and inet6 sockets are created in a process\n\t * context so there is always a valid uid/gid\n\t */\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tcase BPF_FUNC_bind:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\t\treturn &bpf_bind_proto;\n\t\tdefault:\n\t\t\treturn NULL;\n\t\t}\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\ntc_cls_act_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_skb_vlan_push:\n\t\treturn &bpf_skb_vlan_push_proto;\n\tcase BPF_FUNC_skb_vlan_pop:\n\t\treturn &bpf_skb_vlan_pop_proto;\n\tcase BPF_FUNC_skb_change_proto:\n\t\treturn &bpf_skb_change_proto_proto;\n\tcase BPF_FUNC_skb_change_type:\n\t\treturn &bpf_skb_change_type_proto;\n\tcase BPF_FUNC_skb_adjust_room:\n\t\treturn &bpf_skb_adjust_room_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tcase BPF_FUNC_set_hash:\n\t\treturn &bpf_set_hash_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nxdp_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_xdp_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_xdp_adjust_head:\n\t\treturn &bpf_xdp_adjust_head_proto;\n\tcase BPF_FUNC_xdp_adjust_meta:\n\t\treturn &bpf_xdp_adjust_meta_proto;\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_xdp_redirect_proto;\n\tcase BPF_FUNC_redirect_map:\n\t\treturn &bpf_xdp_redirect_map_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_inout_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_ops_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_setsockopt:\n\t\treturn &bpf_setsockopt_proto;\n\tcase BPF_FUNC_getsockopt:\n\t\treturn &bpf_getsockopt_proto;\n\tcase BPF_FUNC_sock_ops_cb_flags_set:\n\t\treturn &bpf_sock_ops_cb_flags_set_proto;\n\tcase BPF_FUNC_sock_map_update:\n\t\treturn &bpf_sock_map_update_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_msg_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_msg_redirect_map:\n\t\treturn &bpf_msg_redirect_map_proto;\n\tcase BPF_FUNC_msg_apply_bytes:\n\t\treturn &bpf_msg_apply_bytes_proto;\n\tcase BPF_FUNC_msg_cork_bytes:\n\t\treturn &bpf_msg_cork_bytes_proto;\n\tcase BPF_FUNC_msg_pull_data:\n\t\treturn &bpf_msg_pull_data_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_skb_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tcase BPF_FUNC_sk_redirect_map:\n\t\treturn &bpf_sk_redirect_map_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_xmit_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tdefault:\n\t\treturn lwt_inout_func_proto(func_id, prog);\n\t}\n}\n\nstatic bool bpf_skb_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t    const struct bpf_prog *prog,\n\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct __sk_buff))\n\t\treturn false;\n\n\t/* The verifier guarantees that size > 0. */\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\tif (off + size > offsetofend(struct __sk_buff, cb[4]))\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip6[0], remote_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip6[0], local_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip4, remote_ip4):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip4, local_ip4):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\t/* Only narrow read access allowed for now. */\n\t\tif (type == BPF_WRITE) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sk_filter_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool lwt_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\n\n/* Attach type specific accesses */\nstatic bool __sock_filter_check_attach_type(int off,\n\t\t\t\t\t    enum bpf_access_type access_type,\n\t\t\t\t\t    enum bpf_attach_type attach_type)\n{\n\tswitch (off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\tcase offsetof(struct bpf_sock, mark):\n\tcase offsetof(struct bpf_sock, priority):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\t\t\tgoto full_access;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_port):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\nread_only:\n\treturn access_type == BPF_READ;\nfull_access:\n\treturn true;\n}\n\nstatic bool __sock_filter_check_size(int off, int size,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\t}\n\n\treturn size == size_default;\n}\n\nstatic bool sock_filter_is_valid_access(int off, int size,\n\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= sizeof(struct bpf_sock))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (!__sock_filter_check_attach_type(off, type,\n\t\t\t\t\t     prog->expected_attach_type))\n\t\treturn false;\n\tif (!__sock_filter_check_size(off, size, info))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int bpf_unclone_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t\tconst struct bpf_prog *prog, int drop_verdict)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tif (!direct_write)\n\t\treturn 0;\n\n\t/* if (!skb->cloned)\n\t *       goto start;\n\t *\n\t * (Fast-path, otherwise approximation that we might be\n\t *  a clone, do the rest in helper.)\n\t */\n\t*insn++ = BPF_LDX_MEM(BPF_B, BPF_REG_6, BPF_REG_1, CLONED_OFFSET());\n\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_6, CLONED_MASK);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_6, 0, 7);\n\n\t/* ret = bpf_skb_pull_data(skb, 0); */\n\t*insn++ = BPF_MOV64_REG(BPF_REG_6, BPF_REG_1);\n\t*insn++ = BPF_ALU64_REG(BPF_XOR, BPF_REG_2, BPF_REG_2);\n\t*insn++ = BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,\n\t\t\t       BPF_FUNC_skb_pull_data);\n\t/* if (!ret)\n\t *      goto restore;\n\t * return TC_ACT_SHOT;\n\t */\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 2);\n\t*insn++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_0, drop_verdict);\n\t*insn++ = BPF_EXIT_INSN();\n\n\t/* restore: */\n\t*insn++ = BPF_MOV64_REG(BPF_REG_1, BPF_REG_6);\n\t/* start: */\n\t*insn++ = prog->insnsi[0];\n\n\treturn insn - insn_buf;\n}\n\nstatic int tc_cls_act_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t       const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, TC_ACT_SHOT);\n}\n\nstatic bool tc_cls_act_is_valid_access(int off, int size,\n\t\t\t\t       enum bpf_access_type type,\n\t\t\t\t       const struct bpf_prog *prog,\n\t\t\t\t       struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\t\treturn false;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool __is_valid_xdp_access(int off, int size)\n{\n\tif (off < 0 || off >= sizeof(struct xdp_md))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (size != sizeof(__u32))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool xdp_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct xdp_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn __is_valid_xdp_access(off, size);\n}\n\nvoid bpf_warn_invalid_xdp_action(u32 act)\n{\n\tconst u32 act_max = XDP_REDIRECT;\n\n\tWARN_ONCE(1, \"%s XDP return value %u, expect packet loss!\\n\",\n\t\t  act > act_max ? \"Illegal\" : \"Driver unsupported\",\n\t\t  act);\n}\nEXPORT_SYMBOL_GPL(bpf_warn_invalid_xdp_action);\n\nstatic bool sock_addr_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_addr))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\n\t/* Disallow access to IPv6 fields from IPv4 contex and vise\n\t * versa.\n\t */\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\t/* Only narrow read access allowed for now. */\n\t\tif (type == BPF_READ) {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_port):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\tif (type == BPF_READ) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sock_ops_is_valid_access(int off, int size,\n\t\t\t\t     enum bpf_access_type type,\n\t\t\t\t     const struct bpf_prog *prog,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_ops))\n\t\treturn false;\n\n\t/* The verifier guarantees that size > 0. */\n\tif (off % size != 0)\n\t\treturn false;\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase offsetof(struct bpf_sock_ops, reply):\n\t\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct bpf_sock_ops, bytes_received,\n\t\t\t\t\tbytes_acked):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int sk_skb_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t   const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, SK_DROP);\n}\n\nstatic bool sk_skb_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\treturn false;\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool sk_msg_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\tif (off < 0 || off >= sizeof(struct sk_msg_md))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (size != sizeof(__u64))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic u32 bpf_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, len):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, len, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, protocol, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_proto):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_proto, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, priority):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, skb_iif, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, hash):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, hash, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, mark):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, pkt_type):\n\t\t*target_size = 1;\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->src_reg,\n\t\t\t\t      PKT_TYPE_OFFSET());\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, queue_mapping):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, queue_mapping, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_present):\n\tcase offsetof(struct __sk_buff, vlan_tci):\n\t\tBUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_tci, 2,\n\t\t\t\t\t\t     target_size));\n\t\tif (si->off == offsetof(struct __sk_buff, vlan_tci)) {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg,\n\t\t\t\t\t\t~VLAN_TAG_PRESENT);\n\t\t} else {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, 12);\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, 1);\n\t\t}\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, cb[0]) ...\n\t     offsetofend(struct __sk_buff, cb[4]) - 1:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, data) < 20);\n\t\tBUILD_BUG_ON((offsetof(struct sk_buff, cb) +\n\t\t\t      offsetof(struct qdisc_skb_cb, data)) %\n\t\t\t     sizeof(__u64));\n\n\t\tprog->cb_access = 1;\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, cb[0]);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, data);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_classid):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, tc_classid) != 2);\n\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, tc_classid);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, tc_classid);\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_H, si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, data));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_meta):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_meta);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_meta);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_end):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_end);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_end);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_index):\n#ifdef CONFIG_NET_SCHED\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t     target_size));\n#else\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_MOV64_REG(si->dst_reg, si->dst_reg);\n\t\telse\n\t\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, napi_id):\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, napi_id, 4,\n\t\t\t\t\t\t     target_size));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, si->dst_reg, MIN_NAPI_ID, 1);\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#else\n\t\t*target_size = 4;\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_family,\n\t\t\t\t\t\t     2, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_daddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_rcv_saddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip6[0]) ...\n\t     offsetof(struct __sk_buff, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, remote_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip6[0]) ...\n\t     offsetof(struct __sk_buff, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, local_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, remote_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_dport,\n\t\t\t\t\t\t     2, target_size));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, local_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_num, 2, target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sock_filter_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_bound_dev_if) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_bound_dev_if));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_bound_dev_if));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, mark):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_mark) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_mark));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_mark));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, priority):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_priority) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_priority));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_priority));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_family));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, type):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, __sk_flags_offset));\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_TYPE_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_TYPE_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, __sk_flags_offset));\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_PROTO_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_PROTO_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_ip4):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_rcv_saddr,\n\t\t\t\t       FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t\t    skc_rcv_saddr),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock, src_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(\n\t\t\t\tstruct sock_common,\n\t\t\t\tskc_v6_rcv_saddr.s6_addr32[0],\n\t\t\t\tFIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t     skc_v6_rcv_saddr.s6_addr32[0]),\n\t\t\t\ttarget_size) + off);\n#else\n\t\t(void)off;\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_port):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_num),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_num,\n\t\t\t\t       FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t\t    skc_num),\n\t\t\t\t       target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 tc_cls_act_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_insn *si,\n\t\t\t\t\t struct bpf_insn *insn_buf,\n\t\t\t\t\t struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 xdp_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct xdp_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_meta),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_meta));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_end));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_rxq_info, dev),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct net_device, ifindex));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, rx_queue_index):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info,\n\t\t\t\t\t       queue_index));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\n/* SOCK_ADDR_LOAD_NESTED_FIELD() loads Nested Field S.F.NF where S is type of\n * context Structure, F is Field in context structure that contains a pointer\n * to Nested Structure of type NS that has the field NF.\n *\n * SIZE encodes the load size (BPF_B, BPF_H, etc). It's up to caller to make\n * sure that SIZE is not greater than actual size of S.F.NF.\n *\n * If offset OFF is provided, the load happens from that offset relative to\n * offset of NF.\n */\n#define SOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF)\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), si->dst_reg,     \\\n\t\t\t\t      si->src_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_LDX_MEM(\t\t\t\t\t       \\\n\t\t\tSIZE, si->dst_reg, si->dst_reg,\t\t\t       \\\n\t\t\tbpf_target_off(NS, NF, FIELD_SIZEOF(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t+ OFF);\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_NESTED_FIELD(S, NS, F, NF)\t\t\t       \\\n\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF,\t\t       \\\n\t\t\t\t\t     BPF_FIELD_SIZEOF(NS, NF), 0)\n\n/* SOCK_ADDR_STORE_NESTED_FIELD_OFF() has semantic similar to\n * SOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF() but for store operation.\n *\n * It doesn't support SIZE argument though since narrow stores are not\n * supported for now.\n *\n * In addition it uses Temporary Field TF (member of struct S) as the 3rd\n * \"register\" since two registers available in convert_ctx_access are not\n * enough: we can't override neither SRC, since it contains value to store, nor\n * DST since it contains pointer to context that may be used by later\n * instructions. But we need a temporary place to save pointer to nested\n * structure whose field we want to store to.\n */\n#define SOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, OFF, TF)\t\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tint tmp_reg = BPF_REG_9;\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, tmp_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), tmp_reg,\t       \\\n\t\t\t\t      si->dst_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_STX_MEM(\t\t\t\t\t       \\\n\t\t\tBPF_FIELD_SIZEOF(NS, NF), tmp_reg, si->src_reg,\t       \\\n\t\t\tbpf_target_off(NS, NF, FIELD_SIZEOF(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t+ OFF);\t\t\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, tmp_reg, si->dst_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF, \\\n\t\t\t\t\t\t      TF)\t\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tif (type == BPF_WRITE) {\t\t\t\t       \\\n\t\t\tSOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, OFF,    \\\n\t\t\t\t\t\t\t TF);\t\t       \\\n\t\t} else {\t\t\t\t\t\t       \\\n\t\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\t\t       \\\n\t\t\t\tS, NS, F, NF, SIZE, OFF);  \\\n\t\t}\t\t\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD(S, NS, F, NF, TF)\t\t       \\\n\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\t\t\t       \\\n\t\tS, NS, F, NF, BPF_FIELD_SIZEOF(NS, NF), 0, TF)\n\nstatic u32 sock_addr_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\t\tstruct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_addr, user_family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sockaddr, uaddr, sa_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_ip4):\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in, uaddr,\n\t\t\tsin_addr, BPF_SIZE(si->code), 0, tmp_reg);\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_addr, user_ip6[0]);\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in6, uaddr,\n\t\t\tsin6_addr.s6_addr32[0], BPF_SIZE(si->code), off,\n\t\t\ttmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_port):\n\t\t/* To get port we need to know sa_family first and then treat\n\t\t * sockaddr as either sockaddr_in or sockaddr_in6.\n\t\t * Though we can simplify since port field has same offset and\n\t\t * size in both structures.\n\t\t * Here we check this invariant and use just one of the\n\t\t * structures if it's true.\n\t\t */\n\t\tBUILD_BUG_ON(offsetof(struct sockaddr_in, sin_port) !=\n\t\t\t     offsetof(struct sockaddr_in6, sin6_port));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sockaddr_in, sin_port) !=\n\t\t\t     FIELD_SIZEOF(struct sockaddr_in6, sin6_port));\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t\t     struct sockaddr_in6, uaddr,\n\t\t\t\t\t\t     sin6_port, tmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sock, sk, sk_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, type):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sock, sk,\n\t\t\t__sk_flags_offset, BPF_W, 0);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_TYPE_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_TYPE_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, protocol):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sock, sk,\n\t\t\t__sk_flags_offset, BPF_W, 0);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_PROTO_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg,\n\t\t\t\t\tSK_FL_PROTO_SHIFT);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sock_ops_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t       const struct bpf_insn *si,\n\t\t\t\t       struct bpf_insn *insn_buf,\n\t\t\t\t       struct bpf_prog *prog,\n\t\t\t\t       u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_ops, op) ...\n\t     offsetof(struct bpf_sock_ops, replylong[3]):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, op) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, op));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, reply) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, reply));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, replylong) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, replylong));\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, op);\n\t\toff += offsetof(struct bpf_sock_ops_kern, op);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      off);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_family));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_daddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_rcv_saddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, remote_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, local_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_dport));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_num));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, is_fullsock):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,\n\t\t\t\t\t\tis_fullsock),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       is_fullsock));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, state):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_state) != 1);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_state));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rtt_min):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct tcp_sock, rtt_min) !=\n\t\t\t     sizeof(struct minmax));\n\t\tBUILD_BUG_ON(sizeof(struct minmax) <\n\t\t\t     sizeof(struct minmax_sample));\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct tcp_sock, rtt_min) +\n\t\t\t\t      FIELD_SIZEOF(struct minmax_sample, t));\n\t\tbreak;\n\n/* Helper macro for adding read access to tcp_sock or sock fields. */\n#define SOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 2);\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(OBJ,\t\t      \\\n\t\t\t\t\t\t       OBJ_FIELD),\t      \\\n\t\t\t\t      si->dst_reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(OBJ, OBJ_FIELD));\t      \\\n\t} while (0)\n\n/* Helper macro for adding write access to tcp_sock or sock fields.\n * The macro is called with two registers, dst_reg which contains a pointer\n * to ctx (context) and src_reg which contains the value that should be\n * stored. However, we need an additional register since we cannot overwrite\n * dst_reg because it may be used later in the program.\n * Instead we \"borrow\" one of the other register. We first save its value\n * into a new (temp) field in bpf_sock_ops_kern, use it, and then restore\n * it at the end of the macro.\n */\n#define SOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tint reg = BPF_REG_9;\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, reg, 0, 2);\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_STX_MEM(BPF_FIELD_SIZEOF(OBJ, OBJ_FIELD),\t      \\\n\t\t\t\t      reg, si->src_reg,\t\t\t      \\\n\t\t\t\t      offsetof(OBJ, OBJ_FIELD));\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t} while (0)\n\n#define SOCK_OPS_GET_OR_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ, TYPE)\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tif (TYPE == BPF_WRITE)\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t\telse\t\t\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t} while (0)\n\n\tcase offsetof(struct bpf_sock_ops, snd_cwnd):\n\t\tSOCK_OPS_GET_FIELD(snd_cwnd, snd_cwnd, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, srtt_us):\n\t\tSOCK_OPS_GET_FIELD(srtt_us, srtt_us, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bpf_sock_ops_cb_flags):\n\t\tSOCK_OPS_GET_FIELD(bpf_sock_ops_cb_flags, bpf_sock_ops_cb_flags,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_ssthresh):\n\t\tSOCK_OPS_GET_FIELD(snd_ssthresh, snd_ssthresh, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rcv_nxt):\n\t\tSOCK_OPS_GET_FIELD(rcv_nxt, rcv_nxt, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_nxt):\n\t\tSOCK_OPS_GET_FIELD(snd_nxt, snd_nxt, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_una):\n\t\tSOCK_OPS_GET_FIELD(snd_una, snd_una, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, mss_cache):\n\t\tSOCK_OPS_GET_FIELD(mss_cache, mss_cache, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, ecn_flags):\n\t\tSOCK_OPS_GET_FIELD(ecn_flags, ecn_flags, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rate_delivered):\n\t\tSOCK_OPS_GET_FIELD(rate_delivered, rate_delivered,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rate_interval_us):\n\t\tSOCK_OPS_GET_FIELD(rate_interval_us, rate_interval_us,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, packets_out):\n\t\tSOCK_OPS_GET_FIELD(packets_out, packets_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, retrans_out):\n\t\tSOCK_OPS_GET_FIELD(retrans_out, retrans_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, total_retrans):\n\t\tSOCK_OPS_GET_FIELD(total_retrans, total_retrans,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, segs_in):\n\t\tSOCK_OPS_GET_FIELD(segs_in, segs_in, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, data_segs_in):\n\t\tSOCK_OPS_GET_FIELD(data_segs_in, data_segs_in, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, segs_out):\n\t\tSOCK_OPS_GET_FIELD(segs_out, segs_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, data_segs_out):\n\t\tSOCK_OPS_GET_FIELD(data_segs_out, data_segs_out,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, lost_out):\n\t\tSOCK_OPS_GET_FIELD(lost_out, lost_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, sacked_out):\n\t\tSOCK_OPS_GET_FIELD(sacked_out, sacked_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\tSOCK_OPS_GET_OR_SET_FIELD(sk_txhash, sk_txhash,\n\t\t\t\t\t  struct sock, type);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bytes_received):\n\t\tSOCK_OPS_GET_FIELD(bytes_received, bytes_received,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bytes_acked):\n\t\tSOCK_OPS_GET_FIELD(bytes_acked, bytes_acked, struct tcp_sock);\n\t\tbreak;\n\n\t}\n\treturn insn - insn_buf;\n}\n\nstatic u32 sk_skb_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, data_end):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_end);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct tcp_skb_cb, bpf.data_end);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sk_msg_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg_buff, data));\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg_buff, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg_buff, data_end));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops sk_filter_verifier_ops = {\n\t.get_func_proto\t\t= sk_filter_func_proto,\n\t.is_valid_access\t= sk_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sk_filter_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops tc_cls_act_verifier_ops = {\n\t.get_func_proto\t\t= tc_cls_act_func_proto,\n\t.is_valid_access\t= tc_cls_act_is_valid_access,\n\t.convert_ctx_access\t= tc_cls_act_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n};\n\nconst struct bpf_prog_ops tc_cls_act_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops xdp_verifier_ops = {\n\t.get_func_proto\t\t= xdp_func_proto,\n\t.is_valid_access\t= xdp_is_valid_access,\n\t.convert_ctx_access\t= xdp_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops xdp_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_xdp,\n};\n\nconst struct bpf_verifier_ops cg_skb_verifier_ops = {\n\t.get_func_proto\t\t= sk_filter_func_proto,\n\t.is_valid_access\t= sk_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_skb_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_inout_verifier_ops = {\n\t.get_func_proto\t\t= lwt_inout_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops lwt_inout_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_xmit_verifier_ops = {\n\t.get_func_proto\t\t= lwt_xmit_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n};\n\nconst struct bpf_prog_ops lwt_xmit_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops cg_sock_verifier_ops = {\n\t.get_func_proto\t\t= sock_filter_func_proto,\n\t.is_valid_access\t= sock_filter_is_valid_access,\n\t.convert_ctx_access\t= sock_filter_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_prog_ops = {\n};\n\nconst struct bpf_verifier_ops cg_sock_addr_verifier_ops = {\n\t.get_func_proto\t\t= sock_addr_func_proto,\n\t.is_valid_access\t= sock_addr_is_valid_access,\n\t.convert_ctx_access\t= sock_addr_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_addr_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sock_ops_verifier_ops = {\n\t.get_func_proto\t\t= sock_ops_func_proto,\n\t.is_valid_access\t= sock_ops_is_valid_access,\n\t.convert_ctx_access\t= sock_ops_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sock_ops_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_skb_verifier_ops = {\n\t.get_func_proto\t\t= sk_skb_func_proto,\n\t.is_valid_access\t= sk_skb_is_valid_access,\n\t.convert_ctx_access\t= sk_skb_convert_ctx_access,\n\t.gen_prologue\t\t= sk_skb_prologue,\n};\n\nconst struct bpf_prog_ops sk_skb_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_msg_verifier_ops = {\n\t.get_func_proto\t\t= sk_msg_func_proto,\n\t.is_valid_access\t= sk_msg_is_valid_access,\n\t.convert_ctx_access\t= sk_msg_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sk_msg_prog_ops = {\n};\n\nint sk_detach_filter(struct sock *sk)\n{\n\tint ret = -ENOENT;\n\tstruct sk_filter *filter;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (filter) {\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t\tsk_filter_uncharge(sk, filter);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_detach_filter);\n\nint sk_get_filter(struct sock *sk, struct sock_filter __user *ubuf,\n\t\t  unsigned int len)\n{\n\tstruct sock_fprog_kern *fprog;\n\tstruct sk_filter *filter;\n\tint ret = 0;\n\n\tlock_sock(sk);\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (!filter)\n\t\tgoto out;\n\n\t/* We're copying the filter that has been originally attached,\n\t * so no conversion/decode needed anymore. eBPF programs that\n\t * have no original program cannot be dumped through this.\n\t */\n\tret = -EACCES;\n\tfprog = filter->prog->orig_prog;\n\tif (!fprog)\n\t\tgoto out;\n\n\tret = fprog->len;\n\tif (!len)\n\t\t/* User space only enquires number of filter blocks. */\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (len < fprog->len)\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_to_user(ubuf, fprog->filter, bpf_classic_proglen(fprog)))\n\t\tgoto out;\n\n\t/* Instead of bytes, the API requests to return the number\n\t * of filter blocks.\n\t */\n\tret = fprog->len;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * Linux Socket Filter - Kernel level socket filtering\n *\n * Based on the design of the Berkeley Packet Filter. The new\n * internal format has been designed by PLUMgrid:\n *\n *\tCopyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com\n *\n * Authors:\n *\n *\tJay Schulist <jschlst@samba.org>\n *\tAlexei Starovoitov <ast@plumgrid.com>\n *\tDaniel Borkmann <dborkman@redhat.com>\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n *\n * Andi Kleen - Fix a few bad bugs and races.\n * Kris Katterjohn - Added many additional checks in bpf_check_classic()\n */\n\n#include <linux/filter.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/random.h>\n#include <linux/moduleloader.h>\n#include <linux/bpf.h>\n#include <linux/frame.h>\n#include <linux/rbtree_latch.h>\n#include <linux/kallsyms.h>\n#include <linux/rcupdate.h>\n\n#include <asm/unaligned.h>\n\n/* Registers */\n#define BPF_R0\tregs[BPF_REG_0]\n#define BPF_R1\tregs[BPF_REG_1]\n#define BPF_R2\tregs[BPF_REG_2]\n#define BPF_R3\tregs[BPF_REG_3]\n#define BPF_R4\tregs[BPF_REG_4]\n#define BPF_R5\tregs[BPF_REG_5]\n#define BPF_R6\tregs[BPF_REG_6]\n#define BPF_R7\tregs[BPF_REG_7]\n#define BPF_R8\tregs[BPF_REG_8]\n#define BPF_R9\tregs[BPF_REG_9]\n#define BPF_R10\tregs[BPF_REG_10]\n\n/* Named registers */\n#define DST\tregs[insn->dst_reg]\n#define SRC\tregs[insn->src_reg]\n#define FP\tregs[BPF_REG_FP]\n#define ARG1\tregs[BPF_REG_ARG1]\n#define CTX\tregs[BPF_REG_CTX]\n#define IMM\tinsn->imm\n\n/* No hurry in this branch\n *\n * Exported for the bpf jit load helper.\n */\nvoid *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb, int k, unsigned int size)\n{\n\tu8 *ptr = NULL;\n\n\tif (k >= SKF_NET_OFF)\n\t\tptr = skb_network_header(skb) + k - SKF_NET_OFF;\n\telse if (k >= SKF_LL_OFF)\n\t\tptr = skb_mac_header(skb) + k - SKF_LL_OFF;\n\n\tif (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))\n\t\treturn ptr;\n\n\treturn NULL;\n}\n\nstruct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog_aux *aux;\n\tstruct bpf_prog *fp;\n\n\tsize = round_up(size, PAGE_SIZE);\n\tfp = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\tif (fp == NULL)\n\t\treturn NULL;\n\n\taux = kzalloc(sizeof(*aux), GFP_KERNEL | gfp_extra_flags);\n\tif (aux == NULL) {\n\t\tvfree(fp);\n\t\treturn NULL;\n\t}\n\n\tfp->pages = size / PAGE_SIZE;\n\tfp->aux = aux;\n\tfp->aux->prog = fp;\n\tfp->jit_requested = ebpf_jit_enabled();\n\n\tINIT_LIST_HEAD_RCU(&fp->aux->ksym_lnode);\n\n\treturn fp;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_alloc);\n\nstruct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,\n\t\t\t\t  gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog *fp;\n\tu32 pages, delta;\n\tint ret;\n\n\tBUG_ON(fp_old == NULL);\n\n\tsize = round_up(size, PAGE_SIZE);\n\tpages = size / PAGE_SIZE;\n\tif (pages <= fp_old->pages)\n\t\treturn fp_old;\n\n\tdelta = pages - fp_old->pages;\n\tret = __bpf_prog_charge(fp_old->aux->user, delta);\n\tif (ret)\n\t\treturn NULL;\n\n\tfp = __vmalloc(size, gfp_flags, PAGE_KERNEL);\n\tif (fp == NULL) {\n\t\t__bpf_prog_uncharge(fp_old->aux->user, delta);\n\t} else {\n\t\tmemcpy(fp, fp_old, fp_old->pages * PAGE_SIZE);\n\t\tfp->pages = pages;\n\t\tfp->aux->prog = fp;\n\n\t\t/* We keep fp->aux from fp_old around in the new\n\t\t * reallocated structure.\n\t\t */\n\t\tfp_old->aux = NULL;\n\t\t__bpf_prog_free(fp_old);\n\t}\n\n\treturn fp;\n}\n\nvoid __bpf_prog_free(struct bpf_prog *fp)\n{\n\tkfree(fp->aux);\n\tvfree(fp);\n}\n\nint bpf_prog_calc_tag(struct bpf_prog *fp)\n{\n\tconst u32 bits_offset = SHA_MESSAGE_BYTES - sizeof(__be64);\n\tu32 raw_size = bpf_prog_tag_scratch_size(fp);\n\tu32 digest[SHA_DIGEST_WORDS];\n\tu32 ws[SHA_WORKSPACE_WORDS];\n\tu32 i, bsize, psize, blocks;\n\tstruct bpf_insn *dst;\n\tbool was_ld_map;\n\tu8 *raw, *todo;\n\t__be32 *result;\n\t__be64 *bits;\n\n\traw = vmalloc(raw_size);\n\tif (!raw)\n\t\treturn -ENOMEM;\n\n\tsha_init(digest);\n\tmemset(ws, 0, sizeof(ws));\n\n\t/* We need to take out the map fd for the digest calculation\n\t * since they are unstable from user space side.\n\t */\n\tdst = (void *)raw;\n\tfor (i = 0, was_ld_map = false; i < fp->len; i++) {\n\t\tdst[i] = fp->insnsi[i];\n\t\tif (!was_ld_map &&\n\t\t    dst[i].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    dst[i].src_reg == BPF_PSEUDO_MAP_FD) {\n\t\t\twas_ld_map = true;\n\t\t\tdst[i].imm = 0;\n\t\t} else if (was_ld_map &&\n\t\t\t   dst[i].code == 0 &&\n\t\t\t   dst[i].dst_reg == 0 &&\n\t\t\t   dst[i].src_reg == 0 &&\n\t\t\t   dst[i].off == 0) {\n\t\t\twas_ld_map = false;\n\t\t\tdst[i].imm = 0;\n\t\t} else {\n\t\t\twas_ld_map = false;\n\t\t}\n\t}\n\n\tpsize = bpf_prog_insn_size(fp);\n\tmemset(&raw[psize], 0, raw_size - psize);\n\traw[psize++] = 0x80;\n\n\tbsize  = round_up(psize, SHA_MESSAGE_BYTES);\n\tblocks = bsize / SHA_MESSAGE_BYTES;\n\ttodo   = raw;\n\tif (bsize - psize >= sizeof(__be64)) {\n\t\tbits = (__be64 *)(todo + bsize - sizeof(__be64));\n\t} else {\n\t\tbits = (__be64 *)(todo + bsize + bits_offset);\n\t\tblocks++;\n\t}\n\t*bits = cpu_to_be64((psize - 1) << 3);\n\n\twhile (blocks--) {\n\t\tsha_transform(digest, todo, ws);\n\t\ttodo += SHA_MESSAGE_BYTES;\n\t}\n\n\tresult = (__force __be32 *)digest;\n\tfor (i = 0; i < SHA_DIGEST_WORDS; i++)\n\t\tresult[i] = cpu_to_be32(digest[i]);\n\tmemcpy(fp->tag, result, sizeof(fp->tag));\n\n\tvfree(raw);\n\treturn 0;\n}\n\nstatic int bpf_adj_delta_to_imm(struct bpf_insn *insn, u32 pos, u32 delta,\n\t\t\t\tu32 curr, const bool probe_pass)\n{\n\tconst s64 imm_min = S32_MIN, imm_max = S32_MAX;\n\ts64 imm = insn->imm;\n\n\tif (curr < pos && curr + imm + 1 > pos)\n\t\timm += delta;\n\telse if (curr > pos + delta && curr + imm + 1 <= pos + delta)\n\t\timm -= delta;\n\tif (imm < imm_min || imm > imm_max)\n\t\treturn -ERANGE;\n\tif (!probe_pass)\n\t\tinsn->imm = imm;\n\treturn 0;\n}\n\nstatic int bpf_adj_delta_to_off(struct bpf_insn *insn, u32 pos, u32 delta,\n\t\t\t\tu32 curr, const bool probe_pass)\n{\n\tconst s32 off_min = S16_MIN, off_max = S16_MAX;\n\ts32 off = insn->off;\n\n\tif (curr < pos && curr + off + 1 > pos)\n\t\toff += delta;\n\telse if (curr > pos + delta && curr + off + 1 <= pos + delta)\n\t\toff -= delta;\n\tif (off < off_min || off > off_max)\n\t\treturn -ERANGE;\n\tif (!probe_pass)\n\t\tinsn->off = off;\n\treturn 0;\n}\n\nstatic int bpf_adj_branches(struct bpf_prog *prog, u32 pos, u32 delta,\n\t\t\t    const bool probe_pass)\n{\n\tu32 i, insn_cnt = prog->len + (probe_pass ? delta : 0);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint ret = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tu8 code;\n\n\t\t/* In the probing pass we still operate on the original,\n\t\t * unpatched image in order to check overflows before we\n\t\t * do any other adjustments. Therefore skip the patchlet.\n\t\t */\n\t\tif (probe_pass && i == pos) {\n\t\t\ti += delta + 1;\n\t\t\tinsn++;\n\t\t}\n\t\tcode = insn->code;\n\t\tif (BPF_CLASS(code) != BPF_JMP ||\n\t\t    BPF_OP(code) == BPF_EXIT)\n\t\t\tcontinue;\n\t\t/* Adjust offset of jmps if we cross patch boundaries. */\n\t\tif (BPF_OP(code) == BPF_CALL) {\n\t\t\tif (insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tret = bpf_adj_delta_to_imm(insn, pos, delta, i,\n\t\t\t\t\t\t   probe_pass);\n\t\t} else {\n\t\t\tret = bpf_adj_delta_to_off(insn, pos, delta, i,\n\t\t\t\t\t\t   probe_pass);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstruct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,\n\t\t\t\t       const struct bpf_insn *patch, u32 len)\n{\n\tu32 insn_adj_cnt, insn_rest, insn_delta = len - 1;\n\tconst u32 cnt_max = S16_MAX;\n\tstruct bpf_prog *prog_adj;\n\n\t/* Since our patchlet doesn't expand the image, we're done. */\n\tif (insn_delta == 0) {\n\t\tmemcpy(prog->insnsi + off, patch, sizeof(*patch));\n\t\treturn prog;\n\t}\n\n\tinsn_adj_cnt = prog->len + insn_delta;\n\n\t/* Reject anything that would potentially let the insn->off\n\t * target overflow when we have excessive program expansions.\n\t * We need to probe here before we do any reallocation where\n\t * we afterwards may not fail anymore.\n\t */\n\tif (insn_adj_cnt > cnt_max &&\n\t    bpf_adj_branches(prog, off, insn_delta, true))\n\t\treturn NULL;\n\n\t/* Several new instructions need to be inserted. Make room\n\t * for them. Likely, there's no need for a new allocation as\n\t * last page could have large enough tailroom.\n\t */\n\tprog_adj = bpf_prog_realloc(prog, bpf_prog_size(insn_adj_cnt),\n\t\t\t\t    GFP_USER);\n\tif (!prog_adj)\n\t\treturn NULL;\n\n\tprog_adj->len = insn_adj_cnt;\n\n\t/* Patching happens in 3 steps:\n\t *\n\t * 1) Move over tail of insnsi from next instruction onwards,\n\t *    so we can patch the single target insn with one or more\n\t *    new ones (patching is always from 1 to n insns, n > 0).\n\t * 2) Inject new instructions at the target location.\n\t * 3) Adjust branch offsets if necessary.\n\t */\n\tinsn_rest = insn_adj_cnt - off - len;\n\n\tmemmove(prog_adj->insnsi + off + len, prog_adj->insnsi + off + 1,\n\t\tsizeof(*patch) * insn_rest);\n\tmemcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);\n\n\t/* We are guaranteed to not fail at this point, otherwise\n\t * the ship has sailed to reverse to the original state. An\n\t * overflow cannot happen at this point.\n\t */\n\tBUG_ON(bpf_adj_branches(prog_adj, off, insn_delta, false));\n\n\treturn prog_adj;\n}\n\n#ifdef CONFIG_BPF_JIT\n/* All BPF JIT sysctl knobs here. */\nint bpf_jit_enable   __read_mostly = IS_BUILTIN(CONFIG_BPF_JIT_ALWAYS_ON);\nint bpf_jit_harden   __read_mostly;\nint bpf_jit_kallsyms __read_mostly;\n\nstatic __always_inline void\nbpf_get_prog_addr_region(const struct bpf_prog *prog,\n\t\t\t unsigned long *symbol_start,\n\t\t\t unsigned long *symbol_end)\n{\n\tconst struct bpf_binary_header *hdr = bpf_jit_binary_hdr(prog);\n\tunsigned long addr = (unsigned long)hdr;\n\n\tWARN_ON_ONCE(!bpf_prog_ebpf_jited(prog));\n\n\t*symbol_start = addr;\n\t*symbol_end   = addr + hdr->pages * PAGE_SIZE;\n}\n\nstatic void bpf_get_prog_name(const struct bpf_prog *prog, char *sym)\n{\n\tconst char *end = sym + KSYM_NAME_LEN;\n\n\tBUILD_BUG_ON(sizeof(\"bpf_prog_\") +\n\t\t     sizeof(prog->tag) * 2 +\n\t\t     /* name has been null terminated.\n\t\t      * We should need +1 for the '_' preceding\n\t\t      * the name.  However, the null character\n\t\t      * is double counted between the name and the\n\t\t      * sizeof(\"bpf_prog_\") above, so we omit\n\t\t      * the +1 here.\n\t\t      */\n\t\t     sizeof(prog->aux->name) > KSYM_NAME_LEN);\n\n\tsym += snprintf(sym, KSYM_NAME_LEN, \"bpf_prog_\");\n\tsym  = bin2hex(sym, prog->tag, sizeof(prog->tag));\n\tif (prog->aux->name[0])\n\t\tsnprintf(sym, (size_t)(end - sym), \"_%s\", prog->aux->name);\n\telse\n\t\t*sym = 0;\n}\n\nstatic __always_inline unsigned long\nbpf_get_prog_addr_start(struct latch_tree_node *n)\n{\n\tunsigned long symbol_start, symbol_end;\n\tconst struct bpf_prog_aux *aux;\n\n\taux = container_of(n, struct bpf_prog_aux, ksym_tnode);\n\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\n\treturn symbol_start;\n}\n\nstatic __always_inline bool bpf_tree_less(struct latch_tree_node *a,\n\t\t\t\t\t  struct latch_tree_node *b)\n{\n\treturn bpf_get_prog_addr_start(a) < bpf_get_prog_addr_start(b);\n}\n\nstatic __always_inline int bpf_tree_comp(void *key, struct latch_tree_node *n)\n{\n\tunsigned long val = (unsigned long)key;\n\tunsigned long symbol_start, symbol_end;\n\tconst struct bpf_prog_aux *aux;\n\n\taux = container_of(n, struct bpf_prog_aux, ksym_tnode);\n\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\n\tif (val < symbol_start)\n\t\treturn -1;\n\tif (val >= symbol_end)\n\t\treturn  1;\n\n\treturn 0;\n}\n\nstatic const struct latch_tree_ops bpf_tree_ops = {\n\t.less\t= bpf_tree_less,\n\t.comp\t= bpf_tree_comp,\n};\n\nstatic DEFINE_SPINLOCK(bpf_lock);\nstatic LIST_HEAD(bpf_kallsyms);\nstatic struct latch_tree_root bpf_tree __cacheline_aligned;\n\nstatic void bpf_prog_ksym_node_add(struct bpf_prog_aux *aux)\n{\n\tWARN_ON_ONCE(!list_empty(&aux->ksym_lnode));\n\tlist_add_tail_rcu(&aux->ksym_lnode, &bpf_kallsyms);\n\tlatch_tree_insert(&aux->ksym_tnode, &bpf_tree, &bpf_tree_ops);\n}\n\nstatic void bpf_prog_ksym_node_del(struct bpf_prog_aux *aux)\n{\n\tif (list_empty(&aux->ksym_lnode))\n\t\treturn;\n\n\tlatch_tree_erase(&aux->ksym_tnode, &bpf_tree, &bpf_tree_ops);\n\tlist_del_rcu(&aux->ksym_lnode);\n}\n\nstatic bool bpf_prog_kallsyms_candidate(const struct bpf_prog *fp)\n{\n\treturn fp->jited && !bpf_prog_was_classic(fp);\n}\n\nstatic bool bpf_prog_kallsyms_verify_off(const struct bpf_prog *fp)\n{\n\treturn list_empty(&fp->aux->ksym_lnode) ||\n\t       fp->aux->ksym_lnode.prev == LIST_POISON2;\n}\n\nvoid bpf_prog_kallsyms_add(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp) ||\n\t    !capable(CAP_SYS_ADMIN))\n\t\treturn;\n\n\tspin_lock_bh(&bpf_lock);\n\tbpf_prog_ksym_node_add(fp->aux);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nvoid bpf_prog_kallsyms_del(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp))\n\t\treturn;\n\n\tspin_lock_bh(&bpf_lock);\n\tbpf_prog_ksym_node_del(fp->aux);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nstatic struct bpf_prog *bpf_prog_kallsyms_find(unsigned long addr)\n{\n\tstruct latch_tree_node *n;\n\n\tif (!bpf_jit_kallsyms_enabled())\n\t\treturn NULL;\n\n\tn = latch_tree_find((void *)addr, &bpf_tree, &bpf_tree_ops);\n\treturn n ?\n\t       container_of(n, struct bpf_prog_aux, ksym_tnode)->prog :\n\t       NULL;\n}\n\nconst char *__bpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t\t\t unsigned long *off, char *sym)\n{\n\tunsigned long symbol_start, symbol_end;\n\tstruct bpf_prog *prog;\n\tchar *ret = NULL;\n\n\trcu_read_lock();\n\tprog = bpf_prog_kallsyms_find(addr);\n\tif (prog) {\n\t\tbpf_get_prog_addr_region(prog, &symbol_start, &symbol_end);\n\t\tbpf_get_prog_name(prog, sym);\n\n\t\tret = sym;\n\t\tif (size)\n\t\t\t*size = symbol_end - symbol_start;\n\t\tif (off)\n\t\t\t*off  = addr - symbol_start;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nbool is_bpf_text_address(unsigned long addr)\n{\n\tbool ret;\n\n\trcu_read_lock();\n\tret = bpf_prog_kallsyms_find(addr) != NULL;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nint bpf_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t    char *sym)\n{\n\tunsigned long symbol_start, symbol_end;\n\tstruct bpf_prog_aux *aux;\n\tunsigned int it = 0;\n\tint ret = -ERANGE;\n\n\tif (!bpf_jit_kallsyms_enabled())\n\t\treturn ret;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(aux, &bpf_kallsyms, ksym_lnode) {\n\t\tif (it++ != symnum)\n\t\t\tcontinue;\n\n\t\tbpf_get_prog_addr_region(aux->prog, &symbol_start, &symbol_end);\n\t\tbpf_get_prog_name(aux->prog, sym);\n\n\t\t*value = symbol_start;\n\t\t*type  = BPF_SYM_ELF_TYPE;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstruct bpf_binary_header *\nbpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,\n\t\t     unsigned int alignment,\n\t\t     bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tstruct bpf_binary_header *hdr;\n\tunsigned int size, hole, start;\n\n\t/* Most of BPF filters are really small, but if some of them\n\t * fill a page, allow at least 128 extra bytes to insert a\n\t * random section of illegal instructions.\n\t */\n\tsize = round_up(proglen + sizeof(*hdr) + 128, PAGE_SIZE);\n\thdr = module_alloc(size);\n\tif (hdr == NULL)\n\t\treturn NULL;\n\n\t/* Fill space with illegal/arch-dep instructions. */\n\tbpf_fill_ill_insns(hdr, size);\n\n\thdr->pages = size / PAGE_SIZE;\n\thole = min_t(unsigned int, size - (proglen + sizeof(*hdr)),\n\t\t     PAGE_SIZE - sizeof(*hdr));\n\tstart = (get_random_int() % hole) & ~(alignment - 1);\n\n\t/* Leave a random number of instructions before BPF code. */\n\t*image_ptr = &hdr->image[start];\n\n\treturn hdr;\n}\n\nvoid bpf_jit_binary_free(struct bpf_binary_header *hdr)\n{\n\tmodule_memfree(hdr);\n}\n\n/* This symbol is only overridden by archs that have different\n * requirements than the usual eBPF JITs, f.e. when they only\n * implement cBPF JIT, do not set images read-only, etc.\n */\nvoid __weak bpf_jit_free(struct bpf_prog *fp)\n{\n\tif (fp->jited) {\n\t\tstruct bpf_binary_header *hdr = bpf_jit_binary_hdr(fp);\n\n\t\tbpf_jit_binary_unlock_ro(hdr);\n\t\tbpf_jit_binary_free(hdr);\n\n\t\tWARN_ON_ONCE(!bpf_prog_kallsyms_verify_off(fp));\n\t}\n\n\tbpf_prog_unlock_free(fp);\n}\n\nstatic int bpf_jit_blind_insn(const struct bpf_insn *from,\n\t\t\t      const struct bpf_insn *aux,\n\t\t\t      struct bpf_insn *to_buff)\n{\n\tstruct bpf_insn *to = to_buff;\n\tu32 imm_rnd = get_random_int();\n\ts16 off;\n\n\tBUILD_BUG_ON(BPF_REG_AX  + 1 != MAX_BPF_JIT_REG);\n\tBUILD_BUG_ON(MAX_BPF_REG + 1 != MAX_BPF_JIT_REG);\n\n\tif (from->imm == 0 &&\n\t    (from->code == (BPF_ALU   | BPF_MOV | BPF_K) ||\n\t     from->code == (BPF_ALU64 | BPF_MOV | BPF_K))) {\n\t\t*to++ = BPF_ALU64_REG(BPF_XOR, from->dst_reg, from->dst_reg);\n\t\tgoto out;\n\t}\n\n\tswitch (from->code) {\n\tcase BPF_ALU | BPF_ADD | BPF_K:\n\tcase BPF_ALU | BPF_SUB | BPF_K:\n\tcase BPF_ALU | BPF_AND | BPF_K:\n\tcase BPF_ALU | BPF_OR  | BPF_K:\n\tcase BPF_ALU | BPF_XOR | BPF_K:\n\tcase BPF_ALU | BPF_MUL | BPF_K:\n\tcase BPF_ALU | BPF_MOV | BPF_K:\n\tcase BPF_ALU | BPF_DIV | BPF_K:\n\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU32_REG(from->code, from->dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_ALU64 | BPF_ADD | BPF_K:\n\tcase BPF_ALU64 | BPF_SUB | BPF_K:\n\tcase BPF_ALU64 | BPF_AND | BPF_K:\n\tcase BPF_ALU64 | BPF_OR  | BPF_K:\n\tcase BPF_ALU64 | BPF_XOR | BPF_K:\n\tcase BPF_ALU64 | BPF_MUL | BPF_K:\n\tcase BPF_ALU64 | BPF_MOV | BPF_K:\n\tcase BPF_ALU64 | BPF_DIV | BPF_K:\n\tcase BPF_ALU64 | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_REG(from->code, from->dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_JMP | BPF_JEQ  | BPF_K:\n\tcase BPF_JMP | BPF_JNE  | BPF_K:\n\tcase BPF_JMP | BPF_JGT  | BPF_K:\n\tcase BPF_JMP | BPF_JLT  | BPF_K:\n\tcase BPF_JMP | BPF_JGE  | BPF_K:\n\tcase BPF_JMP | BPF_JLE  | BPF_K:\n\tcase BPF_JMP | BPF_JSGT | BPF_K:\n\tcase BPF_JMP | BPF_JSLT | BPF_K:\n\tcase BPF_JMP | BPF_JSGE | BPF_K:\n\tcase BPF_JMP | BPF_JSLE | BPF_K:\n\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\t/* Accommodate for extra offset in case of a backjump. */\n\t\toff = from->off;\n\t\tif (off < 0)\n\t\t\toff -= 2;\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_JMP_REG(from->code, from->dst_reg, BPF_REG_AX, off);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_ABS | BPF_W:\n\tcase BPF_LD | BPF_ABS | BPF_H:\n\tcase BPF_LD | BPF_ABS | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_IND | BPF_W:\n\tcase BPF_LD | BPF_IND | BPF_H:\n\tcase BPF_LD | BPF_IND | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU32_REG(BPF_ADD, BPF_REG_AX, from->src_reg);\n\t\t*to++ = BPF_LD_IND(from->code, BPF_REG_AX, 0);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_IMM | BPF_DW:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[1].imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\t\t*to++ = BPF_ALU64_REG(BPF_MOV, aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\tcase 0: /* Part 2 of BPF_LD | BPF_IMM | BPF_DW. */\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[0].imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_REG(BPF_OR,  aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_ST | BPF_MEM | BPF_DW:\n\tcase BPF_ST | BPF_MEM | BPF_W:\n\tcase BPF_ST | BPF_MEM | BPF_H:\n\tcase BPF_ST | BPF_MEM | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_STX_MEM(from->code, from->dst_reg, BPF_REG_AX, from->off);\n\t\tbreak;\n\t}\nout:\n\treturn to - to_buff;\n}\n\nstatic struct bpf_prog *bpf_prog_clone_create(struct bpf_prog *fp_other,\n\t\t\t\t\t      gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog *fp;\n\n\tfp = __vmalloc(fp_other->pages * PAGE_SIZE, gfp_flags, PAGE_KERNEL);\n\tif (fp != NULL) {\n\t\t/* aux->prog still points to the fp_other one, so\n\t\t * when promoting the clone to the real program,\n\t\t * this still needs to be adapted.\n\t\t */\n\t\tmemcpy(fp, fp_other, fp_other->pages * PAGE_SIZE);\n\t}\n\n\treturn fp;\n}\n\nstatic void bpf_prog_clone_free(struct bpf_prog *fp)\n{\n\t/* aux was stolen by the other clone, so we cannot free\n\t * it from this path! It will be freed eventually by the\n\t * other program on release.\n\t *\n\t * At this point, we don't need a deferred release since\n\t * clone is guaranteed to not be locked.\n\t */\n\tfp->aux = NULL;\n\t__bpf_prog_free(fp);\n}\n\nvoid bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other)\n{\n\t/* We have to repoint aux->prog to self, as we don't\n\t * know whether fp here is the clone or the original.\n\t */\n\tfp->aux->prog = fp;\n\tbpf_prog_clone_free(fp_other);\n}\n\nstruct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *prog)\n{\n\tstruct bpf_insn insn_buff[16], aux[2];\n\tstruct bpf_prog *clone, *tmp;\n\tint insn_delta, insn_cnt;\n\tstruct bpf_insn *insn;\n\tint i, rewritten;\n\n\tif (!bpf_jit_blinding_enabled(prog) || prog->blinded)\n\t\treturn prog;\n\n\tclone = bpf_prog_clone_create(prog, GFP_USER);\n\tif (!clone)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinsn_cnt = clone->len;\n\tinsn = clone->insnsi;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\t/* We temporarily need to hold the original ld64 insn\n\t\t * so that we can still access the first part in the\n\t\t * second blinding run.\n\t\t */\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    insn[1].code == 0)\n\t\t\tmemcpy(aux, insn, sizeof(aux));\n\n\t\trewritten = bpf_jit_blind_insn(insn, aux, insn_buff);\n\t\tif (!rewritten)\n\t\t\tcontinue;\n\n\t\ttmp = bpf_patch_insn_single(clone, i, insn_buff, rewritten);\n\t\tif (!tmp) {\n\t\t\t/* Patching may have repointed aux->prog during\n\t\t\t * realloc from the original one, so we need to\n\t\t\t * fix it up here on error.\n\t\t\t */\n\t\t\tbpf_jit_prog_release_other(prog, clone);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\tclone = tmp;\n\t\tinsn_delta = rewritten - 1;\n\n\t\t/* Walk new program and skip insns we just inserted. */\n\t\tinsn = clone->insnsi + i + insn_delta;\n\t\tinsn_cnt += insn_delta;\n\t\ti        += insn_delta;\n\t}\n\n\tclone->blinded = 1;\n\treturn clone;\n}\n#endif /* CONFIG_BPF_JIT */\n\n/* Base function for offset calculation. Needs to go into .text section,\n * therefore keeping it non-static as well; will also be used by JITs\n * anyway later on, so do not let the compiler omit it. This also needs\n * to go into kallsyms for correlation from e.g. bpftool, so naming\n * must not change.\n */\nnoinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__bpf_call_base);\n\n/* All UAPI available opcodes. */\n#define BPF_INSN_MAP(INSN_2, INSN_3)\t\t\\\n\t/* 32 bit ALU operations. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(ALU, ADD, X),\t\t\t\\\n\tINSN_3(ALU, SUB, X),\t\t\t\\\n\tINSN_3(ALU, AND, X),\t\t\t\\\n\tINSN_3(ALU, OR,  X),\t\t\t\\\n\tINSN_3(ALU, LSH, X),\t\t\t\\\n\tINSN_3(ALU, RSH, X),\t\t\t\\\n\tINSN_3(ALU, XOR, X),\t\t\t\\\n\tINSN_3(ALU, MUL, X),\t\t\t\\\n\tINSN_3(ALU, MOV, X),\t\t\t\\\n\tINSN_3(ALU, DIV, X),\t\t\t\\\n\tINSN_3(ALU, MOD, X),\t\t\t\\\n\tINSN_2(ALU, NEG),\t\t\t\\\n\tINSN_3(ALU, END, TO_BE),\t\t\\\n\tINSN_3(ALU, END, TO_LE),\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ALU, ADD, K),\t\t\t\\\n\tINSN_3(ALU, SUB, K),\t\t\t\\\n\tINSN_3(ALU, AND, K),\t\t\t\\\n\tINSN_3(ALU, OR,  K),\t\t\t\\\n\tINSN_3(ALU, LSH, K),\t\t\t\\\n\tINSN_3(ALU, RSH, K),\t\t\t\\\n\tINSN_3(ALU, XOR, K),\t\t\t\\\n\tINSN_3(ALU, MUL, K),\t\t\t\\\n\tINSN_3(ALU, MOV, K),\t\t\t\\\n\tINSN_3(ALU, DIV, K),\t\t\t\\\n\tINSN_3(ALU, MOD, K),\t\t\t\\\n\t/* 64 bit ALU operations. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(ALU64, ADD,  X),\t\t\t\\\n\tINSN_3(ALU64, SUB,  X),\t\t\t\\\n\tINSN_3(ALU64, AND,  X),\t\t\t\\\n\tINSN_3(ALU64, OR,   X),\t\t\t\\\n\tINSN_3(ALU64, LSH,  X),\t\t\t\\\n\tINSN_3(ALU64, RSH,  X),\t\t\t\\\n\tINSN_3(ALU64, XOR,  X),\t\t\t\\\n\tINSN_3(ALU64, MUL,  X),\t\t\t\\\n\tINSN_3(ALU64, MOV,  X),\t\t\t\\\n\tINSN_3(ALU64, ARSH, X),\t\t\t\\\n\tINSN_3(ALU64, DIV,  X),\t\t\t\\\n\tINSN_3(ALU64, MOD,  X),\t\t\t\\\n\tINSN_2(ALU64, NEG),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ALU64, ADD,  K),\t\t\t\\\n\tINSN_3(ALU64, SUB,  K),\t\t\t\\\n\tINSN_3(ALU64, AND,  K),\t\t\t\\\n\tINSN_3(ALU64, OR,   K),\t\t\t\\\n\tINSN_3(ALU64, LSH,  K),\t\t\t\\\n\tINSN_3(ALU64, RSH,  K),\t\t\t\\\n\tINSN_3(ALU64, XOR,  K),\t\t\t\\\n\tINSN_3(ALU64, MUL,  K),\t\t\t\\\n\tINSN_3(ALU64, MOV,  K),\t\t\t\\\n\tINSN_3(ALU64, ARSH, K),\t\t\t\\\n\tINSN_3(ALU64, DIV,  K),\t\t\t\\\n\tINSN_3(ALU64, MOD,  K),\t\t\t\\\n\t/* Call instruction. */\t\t\t\\\n\tINSN_2(JMP, CALL),\t\t\t\\\n\t/* Exit instruction. */\t\t\t\\\n\tINSN_2(JMP, EXIT),\t\t\t\\\n\t/* Jump instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(JMP, JEQ,  X),\t\t\t\\\n\tINSN_3(JMP, JNE,  X),\t\t\t\\\n\tINSN_3(JMP, JGT,  X),\t\t\t\\\n\tINSN_3(JMP, JLT,  X),\t\t\t\\\n\tINSN_3(JMP, JGE,  X),\t\t\t\\\n\tINSN_3(JMP, JLE,  X),\t\t\t\\\n\tINSN_3(JMP, JSGT, X),\t\t\t\\\n\tINSN_3(JMP, JSLT, X),\t\t\t\\\n\tINSN_3(JMP, JSGE, X),\t\t\t\\\n\tINSN_3(JMP, JSLE, X),\t\t\t\\\n\tINSN_3(JMP, JSET, X),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(JMP, JEQ,  K),\t\t\t\\\n\tINSN_3(JMP, JNE,  K),\t\t\t\\\n\tINSN_3(JMP, JGT,  K),\t\t\t\\\n\tINSN_3(JMP, JLT,  K),\t\t\t\\\n\tINSN_3(JMP, JGE,  K),\t\t\t\\\n\tINSN_3(JMP, JLE,  K),\t\t\t\\\n\tINSN_3(JMP, JSGT, K),\t\t\t\\\n\tINSN_3(JMP, JSLT, K),\t\t\t\\\n\tINSN_3(JMP, JSGE, K),\t\t\t\\\n\tINSN_3(JMP, JSLE, K),\t\t\t\\\n\tINSN_3(JMP, JSET, K),\t\t\t\\\n\tINSN_2(JMP, JA),\t\t\t\\\n\t/* Store instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(STX, MEM,  B),\t\t\t\\\n\tINSN_3(STX, MEM,  H),\t\t\t\\\n\tINSN_3(STX, MEM,  W),\t\t\t\\\n\tINSN_3(STX, MEM,  DW),\t\t\t\\\n\tINSN_3(STX, XADD, W),\t\t\t\\\n\tINSN_3(STX, XADD, DW),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(ST, MEM, B),\t\t\t\\\n\tINSN_3(ST, MEM, H),\t\t\t\\\n\tINSN_3(ST, MEM, W),\t\t\t\\\n\tINSN_3(ST, MEM, DW),\t\t\t\\\n\t/* Load instructions. */\t\t\\\n\t/*   Register based. */\t\t\t\\\n\tINSN_3(LDX, MEM, B),\t\t\t\\\n\tINSN_3(LDX, MEM, H),\t\t\t\\\n\tINSN_3(LDX, MEM, W),\t\t\t\\\n\tINSN_3(LDX, MEM, DW),\t\t\t\\\n\t/*   Immediate based. */\t\t\\\n\tINSN_3(LD, IMM, DW),\t\t\t\\\n\t/*   Misc (old cBPF carry-over). */\t\\\n\tINSN_3(LD, ABS, B),\t\t\t\\\n\tINSN_3(LD, ABS, H),\t\t\t\\\n\tINSN_3(LD, ABS, W),\t\t\t\\\n\tINSN_3(LD, IND, B),\t\t\t\\\n\tINSN_3(LD, IND, H),\t\t\t\\\n\tINSN_3(LD, IND, W)\n\nbool bpf_opcode_in_insntable(u8 code)\n{\n#define BPF_INSN_2_TBL(x, y)    [BPF_##x | BPF_##y] = true\n#define BPF_INSN_3_TBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = true\n\tstatic const bool public_insntable[256] = {\n\t\t[0 ... 255] = false,\n\t\t/* Now overwrite non-defaults ... */\n\t\tBPF_INSN_MAP(BPF_INSN_2_TBL, BPF_INSN_3_TBL),\n\t};\n#undef BPF_INSN_3_TBL\n#undef BPF_INSN_2_TBL\n\treturn public_insntable[code];\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n/**\n *\t__bpf_prog_run - run eBPF program on a given context\n *\t@ctx: is the data we are operating on\n *\t@insn: is the array of eBPF instructions\n *\n * Decode and execute eBPF instructions.\n */\nstatic u64 ___bpf_prog_run(u64 *regs, const struct bpf_insn *insn, u64 *stack)\n{\n\tu64 tmp;\n#define BPF_INSN_2_LBL(x, y)    [BPF_##x | BPF_##y] = &&x##_##y\n#define BPF_INSN_3_LBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = &&x##_##y##_##z\n\tstatic const void *jumptable[256] = {\n\t\t[0 ... 255] = &&default_label,\n\t\t/* Now overwrite non-defaults ... */\n\t\tBPF_INSN_MAP(BPF_INSN_2_LBL, BPF_INSN_3_LBL),\n\t\t/* Non-UAPI available opcodes. */\n\t\t[BPF_JMP | BPF_CALL_ARGS] = &&JMP_CALL_ARGS,\n\t\t[BPF_JMP | BPF_TAIL_CALL] = &&JMP_TAIL_CALL,\n\t};\n#undef BPF_INSN_3_LBL\n#undef BPF_INSN_2_LBL\n\tu32 tail_call_cnt = 0;\n\tvoid *ptr;\n\tint off;\n\n#define CONT\t ({ insn++; goto select_insn; })\n#define CONT_JMP ({ insn++; goto select_insn; })\n\nselect_insn:\n\tgoto *jumptable[insn->code];\n\n\t/* ALU */\n#define ALU(OPCODE, OP)\t\t\t\\\n\tALU64_##OPCODE##_X:\t\t\\\n\t\tDST = DST OP SRC;\t\\\n\t\tCONT;\t\t\t\\\n\tALU_##OPCODE##_X:\t\t\\\n\t\tDST = (u32) DST OP (u32) SRC;\t\\\n\t\tCONT;\t\t\t\\\n\tALU64_##OPCODE##_K:\t\t\\\n\t\tDST = DST OP IMM;\t\t\\\n\t\tCONT;\t\t\t\\\n\tALU_##OPCODE##_K:\t\t\\\n\t\tDST = (u32) DST OP (u32) IMM;\t\\\n\t\tCONT;\n\n\tALU(ADD,  +)\n\tALU(SUB,  -)\n\tALU(AND,  &)\n\tALU(OR,   |)\n\tALU(LSH, <<)\n\tALU(RSH, >>)\n\tALU(XOR,  ^)\n\tALU(MUL,  *)\n#undef ALU\n\tALU_NEG:\n\t\tDST = (u32) -DST;\n\t\tCONT;\n\tALU64_NEG:\n\t\tDST = -DST;\n\t\tCONT;\n\tALU_MOV_X:\n\t\tDST = (u32) SRC;\n\t\tCONT;\n\tALU_MOV_K:\n\t\tDST = (u32) IMM;\n\t\tCONT;\n\tALU64_MOV_X:\n\t\tDST = SRC;\n\t\tCONT;\n\tALU64_MOV_K:\n\t\tDST = IMM;\n\t\tCONT;\n\tLD_IMM_DW:\n\t\tDST = (u64) (u32) insn[0].imm | ((u64) (u32) insn[1].imm) << 32;\n\t\tinsn++;\n\t\tCONT;\n\tALU64_ARSH_X:\n\t\t(*(s64 *) &DST) >>= SRC;\n\t\tCONT;\n\tALU64_ARSH_K:\n\t\t(*(s64 *) &DST) >>= IMM;\n\t\tCONT;\n\tALU64_MOD_X:\n\t\tdiv64_u64_rem(DST, SRC, &tmp);\n\t\tDST = tmp;\n\t\tCONT;\n\tALU_MOD_X:\n\t\ttmp = (u32) DST;\n\t\tDST = do_div(tmp, (u32) SRC);\n\t\tCONT;\n\tALU64_MOD_K:\n\t\tdiv64_u64_rem(DST, IMM, &tmp);\n\t\tDST = tmp;\n\t\tCONT;\n\tALU_MOD_K:\n\t\ttmp = (u32) DST;\n\t\tDST = do_div(tmp, (u32) IMM);\n\t\tCONT;\n\tALU64_DIV_X:\n\t\tDST = div64_u64(DST, SRC);\n\t\tCONT;\n\tALU_DIV_X:\n\t\ttmp = (u32) DST;\n\t\tdo_div(tmp, (u32) SRC);\n\t\tDST = (u32) tmp;\n\t\tCONT;\n\tALU64_DIV_K:\n\t\tDST = div64_u64(DST, IMM);\n\t\tCONT;\n\tALU_DIV_K:\n\t\ttmp = (u32) DST;\n\t\tdo_div(tmp, (u32) IMM);\n\t\tDST = (u32) tmp;\n\t\tCONT;\n\tALU_END_TO_BE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_be16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_be32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_be64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_END_TO_LE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_le16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_le32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_le64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\n\t/* CALL */\n\tJMP_CALL:\n\t\t/* Function call scratches BPF_R1-BPF_R5 registers,\n\t\t * preserves BPF_R6-BPF_R9, and stores return value\n\t\t * into BPF_R0.\n\t\t */\n\t\tBPF_R0 = (__bpf_call_base + insn->imm)(BPF_R1, BPF_R2, BPF_R3,\n\t\t\t\t\t\t       BPF_R4, BPF_R5);\n\t\tCONT;\n\n\tJMP_CALL_ARGS:\n\t\tBPF_R0 = (__bpf_call_base_args + insn->imm)(BPF_R1, BPF_R2,\n\t\t\t\t\t\t\t    BPF_R3, BPF_R4,\n\t\t\t\t\t\t\t    BPF_R5,\n\t\t\t\t\t\t\t    insn + insn->off + 1);\n\t\tCONT;\n\n\tJMP_TAIL_CALL: {\n\t\tstruct bpf_map *map = (struct bpf_map *) (unsigned long) BPF_R2;\n\t\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\t\tstruct bpf_prog *prog;\n\t\tu32 index = BPF_R3;\n\n\t\tif (unlikely(index >= array->map.max_entries))\n\t\t\tgoto out;\n\t\tif (unlikely(tail_call_cnt > MAX_TAIL_CALL_CNT))\n\t\t\tgoto out;\n\n\t\ttail_call_cnt++;\n\n\t\tprog = READ_ONCE(array->ptrs[index]);\n\t\tif (!prog)\n\t\t\tgoto out;\n\n\t\t/* ARG1 at this point is guaranteed to point to CTX from\n\t\t * the verifier side due to the fact that the tail call is\n\t\t * handeled like a helper, that is, bpf_tail_call_proto,\n\t\t * where arg1_type is ARG_PTR_TO_CTX.\n\t\t */\n\t\tinsn = prog->insnsi;\n\t\tgoto select_insn;\nout:\n\t\tCONT;\n\t}\n\t/* JMP */\n\tJMP_JA:\n\t\tinsn += insn->off;\n\t\tCONT;\n\tJMP_JEQ_X:\n\t\tif (DST == SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JEQ_K:\n\t\tif (DST == IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JNE_X:\n\t\tif (DST != SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JNE_K:\n\t\tif (DST != IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGT_X:\n\t\tif (DST > SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGT_K:\n\t\tif (DST > IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLT_X:\n\t\tif (DST < SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLT_K:\n\t\tif (DST < IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGE_X:\n\t\tif (DST >= SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JGE_K:\n\t\tif (DST >= IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLE_X:\n\t\tif (DST <= SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JLE_K:\n\t\tif (DST <= IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGT_X:\n\t\tif (((s64) DST) > ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGT_K:\n\t\tif (((s64) DST) > ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLT_X:\n\t\tif (((s64) DST) < ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLT_K:\n\t\tif (((s64) DST) < ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGE_X:\n\t\tif (((s64) DST) >= ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSGE_K:\n\t\tif (((s64) DST) >= ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLE_X:\n\t\tif (((s64) DST) <= ((s64) SRC)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSLE_K:\n\t\tif (((s64) DST) <= ((s64) IMM)) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSET_X:\n\t\tif (DST & SRC) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_JSET_K:\n\t\tif (DST & IMM) {\n\t\t\tinsn += insn->off;\n\t\t\tCONT_JMP;\n\t\t}\n\t\tCONT;\n\tJMP_EXIT:\n\t\treturn BPF_R0;\n\n\t/* STX and ST and LDX*/\n#define LDST(SIZEOP, SIZE)\t\t\t\t\t\t\\\n\tSTX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = SRC;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tST_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = IMM;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tLDX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\tDST = *(SIZE *)(unsigned long) (SRC + insn->off);\t\\\n\t\tCONT;\n\n\tLDST(B,   u8)\n\tLDST(H,  u16)\n\tLDST(W,  u32)\n\tLDST(DW, u64)\n#undef LDST\n\tSTX_XADD_W: /* lock xadd *(u32 *)(dst_reg + off16) += src_reg */\n\t\tatomic_add((u32) SRC, (atomic_t *)(unsigned long)\n\t\t\t   (DST + insn->off));\n\t\tCONT;\n\tSTX_XADD_DW: /* lock xadd *(u64 *)(dst_reg + off16) += src_reg */\n\t\tatomic64_add((u64) SRC, (atomic64_t *)(unsigned long)\n\t\t\t     (DST + insn->off));\n\t\tCONT;\n\tLD_ABS_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + imm32)) */\n\t\toff = IMM;\nload_word:\n\t\t/* BPF_LD + BPD_ABS and BPF_LD + BPF_IND insns are only\n\t\t * appearing in the programs where ctx == skb\n\t\t * (see may_access_skb() in the verifier). All programs\n\t\t * keep 'ctx' in regs[BPF_REG_CTX] == BPF_R6,\n\t\t * bpf_convert_filter() saves it in BPF_R6, internal BPF\n\t\t * verifier will check that BPF_R6 == ctx.\n\t\t *\n\t\t * BPF_ABS and BPF_IND are wrappers of function calls,\n\t\t * so they scratch BPF_R1-BPF_R5 registers, preserve\n\t\t * BPF_R6-BPF_R9, and store return value into BPF_R0.\n\t\t *\n\t\t * Implicit input:\n\t\t *   ctx == skb == BPF_R6 == CTX\n\t\t *\n\t\t * Explicit input:\n\t\t *   SRC == any register\n\t\t *   IMM == 32-bit immediate\n\t\t *\n\t\t * Output:\n\t\t *   BPF_R0 - 8/16/32-bit skb data converted to cpu endianness\n\t\t */\n\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 4, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = get_unaligned_be32(ptr);\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_ABS_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + imm32)) */\n\t\toff = IMM;\nload_half:\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 2, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = get_unaligned_be16(ptr);\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_ABS_B: /* BPF_R0 = *(u8 *) (skb->data + imm32) */\n\t\toff = IMM;\nload_byte:\n\t\tptr = bpf_load_pointer((struct sk_buff *) (unsigned long) CTX, off, 1, &tmp);\n\t\tif (likely(ptr != NULL)) {\n\t\t\tBPF_R0 = *(u8 *)ptr;\n\t\t\tCONT;\n\t\t}\n\n\t\treturn 0;\n\tLD_IND_W: /* BPF_R0 = ntohl(*(u32 *) (skb->data + src_reg + imm32)) */\n\t\toff = IMM + SRC;\n\t\tgoto load_word;\n\tLD_IND_H: /* BPF_R0 = ntohs(*(u16 *) (skb->data + src_reg + imm32)) */\n\t\toff = IMM + SRC;\n\t\tgoto load_half;\n\tLD_IND_B: /* BPF_R0 = *(u8 *) (skb->data + src_reg + imm32) */\n\t\toff = IMM + SRC;\n\t\tgoto load_byte;\n\n\tdefault_label:\n\t\t/* If we ever reach this, we have a bug somewhere. Die hard here\n\t\t * instead of just returning 0; we could be somewhere in a subprog,\n\t\t * so execution could continue otherwise which we do /not/ want.\n\t\t *\n\t\t * Note, verifier whitelists all opcodes in bpf_opcode_in_insntable().\n\t\t */\n\t\tpr_warn(\"BPF interpreter: unknown opcode %02x\\n\", insn->code);\n\t\tBUG_ON(1);\n\t\treturn 0;\n}\nSTACK_FRAME_NON_STANDARD(___bpf_prog_run); /* jump table */\n\n#define PROG_NAME(stack_size) __bpf_prog_run##stack_size\n#define DEFINE_BPF_PROG_RUN(stack_size) \\\nstatic unsigned int PROG_NAME(stack_size)(const void *ctx, const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_REG]; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tARG1 = (u64) (unsigned long) ctx; \\\n\treturn ___bpf_prog_run(regs, insn, stack); \\\n}\n\n#define PROG_NAME_ARGS(stack_size) __bpf_prog_run_args##stack_size\n#define DEFINE_BPF_PROG_RUN_ARGS(stack_size) \\\nstatic u64 PROG_NAME_ARGS(stack_size)(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5, \\\n\t\t\t\t      const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_REG]; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tBPF_R1 = r1; \\\n\tBPF_R2 = r2; \\\n\tBPF_R3 = r3; \\\n\tBPF_R4 = r4; \\\n\tBPF_R5 = r5; \\\n\treturn ___bpf_prog_run(regs, insn, stack); \\\n}\n\n#define EVAL1(FN, X) FN(X)\n#define EVAL2(FN, X, Y...) FN(X) EVAL1(FN, Y)\n#define EVAL3(FN, X, Y...) FN(X) EVAL2(FN, Y)\n#define EVAL4(FN, X, Y...) FN(X) EVAL3(FN, Y)\n#define EVAL5(FN, X, Y...) FN(X) EVAL4(FN, Y)\n#define EVAL6(FN, X, Y...) FN(X) EVAL5(FN, Y)\n\nEVAL6(DEFINE_BPF_PROG_RUN, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN, 416, 448, 480, 512);\n\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN_ARGS, 416, 448, 480, 512);\n\n#define PROG_NAME_LIST(stack_size) PROG_NAME(stack_size),\n\nstatic unsigned int (*interpreters[])(const void *ctx,\n\t\t\t\t      const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n#define PROG_NAME_LIST(stack_size) PROG_NAME_ARGS(stack_size),\nstatic u64 (*interpreters_args[])(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5,\n\t\t\t\t  const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n\nvoid bpf_patch_call_args(struct bpf_insn *insn, u32 stack_depth)\n{\n\tstack_depth = max_t(u32, stack_depth, 1);\n\tinsn->off = (s16) insn->imm;\n\tinsn->imm = interpreters_args[(round_up(stack_depth, 32) / 32) - 1] -\n\t\t__bpf_call_base_args;\n\tinsn->code = BPF_JMP | BPF_CALL_ARGS;\n}\n\n#else\nstatic unsigned int __bpf_prog_ret0_warn(const void *ctx,\n\t\t\t\t\t const struct bpf_insn *insn)\n{\n\t/* If this handler ever gets executed, then BPF_JIT_ALWAYS_ON\n\t * is not working properly, so warn about it!\n\t */\n\tWARN_ON_ONCE(1);\n\treturn 0;\n}\n#endif\n\nbool bpf_prog_array_compatible(struct bpf_array *array,\n\t\t\t       const struct bpf_prog *fp)\n{\n\tif (fp->kprobe_override)\n\t\treturn false;\n\n\tif (!array->owner_prog_type) {\n\t\t/* There's no owner yet where we could check for\n\t\t * compatibility.\n\t\t */\n\t\tarray->owner_prog_type = fp->type;\n\t\tarray->owner_jited = fp->jited;\n\n\t\treturn true;\n\t}\n\n\treturn array->owner_prog_type == fp->type &&\n\t       array->owner_jited == fp->jited;\n}\n\nstatic int bpf_check_tail_call(const struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\tint i;\n\n\tfor (i = 0; i < aux->used_map_cnt; i++) {\n\t\tstruct bpf_map *map = aux->used_maps[i];\n\t\tstruct bpf_array *array;\n\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tcontinue;\n\n\t\tarray = container_of(map, struct bpf_array, map);\n\t\tif (!bpf_prog_array_compatible(array, fp))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n/**\n *\tbpf_prog_select_runtime - select exec runtime for BPF program\n *\t@fp: bpf_prog populated with internal BPF program\n *\t@err: pointer to error variable\n *\n * Try to JIT eBPF program, if JIT is not available, use interpreter.\n * The BPF program will be executed via BPF_PROG_RUN() macro.\n */\nstruct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tu32 stack_depth = max_t(u32, fp->aux->stack_depth, 1);\n\n\tfp->bpf_func = interpreters[(round_up(stack_depth, 32) / 32) - 1];\n#else\n\tfp->bpf_func = __bpf_prog_ret0_warn;\n#endif\n\n\t/* eBPF JITs can rewrite the program in case constant\n\t * blinding is active. However, in case of error during\n\t * blinding, bpf_int_jit_compile() must always return a\n\t * valid program, which in this case would simply not\n\t * be JITed, but falls back to the interpreter.\n\t */\n\tif (!bpf_prog_is_dev_bound(fp->aux)) {\n\t\tfp = bpf_int_jit_compile(fp);\n#ifdef CONFIG_BPF_JIT_ALWAYS_ON\n\t\tif (!fp->jited) {\n\t\t\t*err = -ENOTSUPP;\n\t\t\treturn fp;\n\t\t}\n#endif\n\t} else {\n\t\t*err = bpf_prog_offload_compile(fp);\n\t\tif (*err)\n\t\t\treturn fp;\n\t}\n\tbpf_prog_lock_ro(fp);\n\n\t/* The tail call compatibility check can only be done at\n\t * this late stage as we need to determine, if we deal\n\t * with JITed or non JITed program concatenations and not\n\t * all eBPF JITs might immediately support all features.\n\t */\n\t*err = bpf_check_tail_call(fp);\n\n\treturn fp;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_select_runtime);\n\nstatic unsigned int __bpf_prog_ret1(const void *ctx,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn 1;\n}\n\nstatic struct bpf_prog_dummy {\n\tstruct bpf_prog prog;\n} dummy_bpf_prog = {\n\t.prog = {\n\t\t.bpf_func = __bpf_prog_ret1,\n\t},\n};\n\n/* to avoid allocating empty bpf_prog_array for cgroups that\n * don't have bpf program attached use one global 'empty_prog_array'\n * It will not be modified the caller of bpf_prog_array_alloc()\n * (since caller requested prog_cnt == 0)\n * that pointer should be 'freed' by bpf_prog_array_free()\n */\nstatic struct {\n\tstruct bpf_prog_array hdr;\n\tstruct bpf_prog *null_prog;\n} empty_prog_array = {\n\t.null_prog = NULL,\n};\n\nstruct bpf_prog_array __rcu *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags)\n{\n\tif (prog_cnt)\n\t\treturn kzalloc(sizeof(struct bpf_prog_array) +\n\t\t\t       sizeof(struct bpf_prog *) * (prog_cnt + 1),\n\t\t\t       flags);\n\n\treturn &empty_prog_array.hdr;\n}\n\nvoid bpf_prog_array_free(struct bpf_prog_array __rcu *progs)\n{\n\tif (!progs ||\n\t    progs == (struct bpf_prog_array __rcu *)&empty_prog_array.hdr)\n\t\treturn;\n\tkfree_rcu(progs, rcu);\n}\n\nint bpf_prog_array_length(struct bpf_prog_array __rcu *progs)\n{\n\tstruct bpf_prog **prog;\n\tu32 cnt = 0;\n\n\trcu_read_lock();\n\tprog = rcu_dereference(progs)->progs;\n\tfor (; *prog; prog++)\n\t\tif (*prog != &dummy_bpf_prog.prog)\n\t\t\tcnt++;\n\trcu_read_unlock();\n\treturn cnt;\n}\n\nstatic bool bpf_prog_array_copy_core(struct bpf_prog **prog,\n\t\t\t\t     u32 *prog_ids,\n\t\t\t\t     u32 request_cnt)\n{\n\tint i = 0;\n\n\tfor (; *prog; prog++) {\n\t\tif (*prog == &dummy_bpf_prog.prog)\n\t\t\tcontinue;\n\t\tprog_ids[i] = (*prog)->aux->id;\n\t\tif (++i == request_cnt) {\n\t\t\tprog++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn !!(*prog);\n}\n\nint bpf_prog_array_copy_to_user(struct bpf_prog_array __rcu *progs,\n\t\t\t\t__u32 __user *prog_ids, u32 cnt)\n{\n\tstruct bpf_prog **prog;\n\tunsigned long err = 0;\n\tbool nospc;\n\tu32 *ids;\n\n\t/* users of this function are doing:\n\t * cnt = bpf_prog_array_length();\n\t * if (cnt > 0)\n\t *     bpf_prog_array_copy_to_user(..., cnt);\n\t * so below kcalloc doesn't need extra cnt > 0 check, but\n\t * bpf_prog_array_length() releases rcu lock and\n\t * prog array could have been swapped with empty or larger array,\n\t * so always copy 'cnt' prog_ids to the user.\n\t * In a rare race the user will see zero prog_ids\n\t */\n\tids = kcalloc(cnt, sizeof(u32), GFP_USER | __GFP_NOWARN);\n\tif (!ids)\n\t\treturn -ENOMEM;\n\trcu_read_lock();\n\tprog = rcu_dereference(progs)->progs;\n\tnospc = bpf_prog_array_copy_core(prog, ids, cnt);\n\trcu_read_unlock();\n\terr = copy_to_user(prog_ids, ids, cnt * sizeof(u32));\n\tkfree(ids);\n\tif (err)\n\t\treturn -EFAULT;\n\tif (nospc)\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array __rcu *progs,\n\t\t\t\tstruct bpf_prog *old_prog)\n{\n\tstruct bpf_prog **prog = progs->progs;\n\n\tfor (; *prog; prog++)\n\t\tif (*prog == old_prog) {\n\t\t\tWRITE_ONCE(*prog, &dummy_bpf_prog.prog);\n\t\t\tbreak;\n\t\t}\n}\n\nint bpf_prog_array_copy(struct bpf_prog_array __rcu *old_array,\n\t\t\tstruct bpf_prog *exclude_prog,\n\t\t\tstruct bpf_prog *include_prog,\n\t\t\tstruct bpf_prog_array **new_array)\n{\n\tint new_prog_cnt, carry_prog_cnt = 0;\n\tstruct bpf_prog **existing_prog;\n\tstruct bpf_prog_array *array;\n\tint new_prog_idx = 0;\n\n\t/* Figure out how many existing progs we need to carry over to\n\t * the new array.\n\t */\n\tif (old_array) {\n\t\texisting_prog = old_array->progs;\n\t\tfor (; *existing_prog; existing_prog++) {\n\t\t\tif (*existing_prog != exclude_prog &&\n\t\t\t    *existing_prog != &dummy_bpf_prog.prog)\n\t\t\t\tcarry_prog_cnt++;\n\t\t\tif (*existing_prog == include_prog)\n\t\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\t/* How many progs (not NULL) will be in the new array? */\n\tnew_prog_cnt = carry_prog_cnt;\n\tif (include_prog)\n\t\tnew_prog_cnt += 1;\n\n\t/* Do we have any prog (not NULL) in the new array? */\n\tif (!new_prog_cnt) {\n\t\t*new_array = NULL;\n\t\treturn 0;\n\t}\n\n\t/* +1 as the end of prog_array is marked with NULL */\n\tarray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\t/* Fill in the new prog array */\n\tif (carry_prog_cnt) {\n\t\texisting_prog = old_array->progs;\n\t\tfor (; *existing_prog; existing_prog++)\n\t\t\tif (*existing_prog != exclude_prog &&\n\t\t\t    *existing_prog != &dummy_bpf_prog.prog)\n\t\t\t\tarray->progs[new_prog_idx++] = *existing_prog;\n\t}\n\tif (include_prog)\n\t\tarray->progs[new_prog_idx++] = include_prog;\n\tarray->progs[new_prog_idx] = NULL;\n\t*new_array = array;\n\treturn 0;\n}\n\nint bpf_prog_array_copy_info(struct bpf_prog_array __rcu *array,\n\t\t\t     u32 *prog_ids, u32 request_cnt,\n\t\t\t     u32 *prog_cnt)\n{\n\tstruct bpf_prog **prog;\n\tu32 cnt = 0;\n\n\tif (array)\n\t\tcnt = bpf_prog_array_length(array);\n\n\t*prog_cnt = cnt;\n\n\t/* return early if user requested only program count or nothing to copy */\n\tif (!request_cnt || !cnt)\n\t\treturn 0;\n\n\t/* this function is called under trace/bpf_trace.c: bpf_event_mutex */\n\tprog = rcu_dereference_check(array, 1)->progs;\n\treturn bpf_prog_array_copy_core(prog, prog_ids, request_cnt) ? -ENOSPC\n\t\t\t\t\t\t\t\t     : 0;\n}\n\nstatic void bpf_prog_free_deferred(struct work_struct *work)\n{\n\tstruct bpf_prog_aux *aux;\n\tint i;\n\n\taux = container_of(work, struct bpf_prog_aux, work);\n\tif (bpf_prog_is_dev_bound(aux))\n\t\tbpf_prog_offload_destroy(aux->prog);\n\tfor (i = 0; i < aux->func_cnt; i++)\n\t\tbpf_jit_free(aux->func[i]);\n\tif (aux->func_cnt) {\n\t\tkfree(aux->func);\n\t\tbpf_prog_unlock_free(aux->prog);\n\t} else {\n\t\tbpf_jit_free(aux->prog);\n\t}\n}\n\n/* Free internal BPF program */\nvoid bpf_prog_free(struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\n\tINIT_WORK(&aux->work, bpf_prog_free_deferred);\n\tschedule_work(&aux->work);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_free);\n\n/* RNG for unpriviledged user space with separated state from prandom_u32(). */\nstatic DEFINE_PER_CPU(struct rnd_state, bpf_user_rnd_state);\n\nvoid bpf_user_rnd_init_once(void)\n{\n\tprandom_init_once(&bpf_user_rnd_state);\n}\n\nBPF_CALL_0(bpf_user_rnd_u32)\n{\n\t/* Should someone ever have the rather unwise idea to use some\n\t * of the registers passed into this function, then note that\n\t * this function is called from native eBPF and classic-to-eBPF\n\t * transformations. Register assignments from both sides are\n\t * different, f.e. classic always sets fn(ctx, A, X) here.\n\t */\n\tstruct rnd_state *state;\n\tu32 res;\n\n\tstate = &get_cpu_var(bpf_user_rnd_state);\n\tres = prandom_u32_state(state);\n\tput_cpu_var(bpf_user_rnd_state);\n\n\treturn res;\n}\n\n/* Weak definitions of helper functions in case we don't have bpf syscall. */\nconst struct bpf_func_proto bpf_map_lookup_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_update_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_delete_elem_proto __weak;\n\nconst struct bpf_func_proto bpf_get_prandom_u32_proto __weak;\nconst struct bpf_func_proto bpf_get_smp_processor_id_proto __weak;\nconst struct bpf_func_proto bpf_get_numa_node_id_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_ns_proto __weak;\n\nconst struct bpf_func_proto bpf_get_current_pid_tgid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_uid_gid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_comm_proto __weak;\nconst struct bpf_func_proto bpf_sock_map_update_proto __weak;\n\nconst struct bpf_func_proto * __weak bpf_get_trace_printk_proto(void)\n{\n\treturn NULL;\n}\n\nu64 __weak\nbpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\n\t\t void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)\n{\n\treturn -ENOTSUPP;\n}\n\n/* Always built-in helper functions. */\nconst struct bpf_func_proto bpf_tail_call_proto = {\n\t.func\t\t= NULL,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_VOID,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\n/* Stub for JITs that only support cBPF. eBPF programs are interpreted.\n * It is encouraged to implement bpf_int_jit_compile() instead, so that\n * eBPF and implicitly also cBPF can get JITed!\n */\nstruct bpf_prog * __weak bpf_int_jit_compile(struct bpf_prog *prog)\n{\n\treturn prog;\n}\n\n/* Stub for JITs that support eBPF. All cBPF code gets transformed into\n * eBPF by the kernel and is later compiled by bpf_int_jit_compile().\n */\nvoid __weak bpf_jit_compile(struct bpf_prog *prog)\n{\n}\n\nbool __weak bpf_helper_changes_pkt_data(void *func)\n{\n\treturn false;\n}\n\n/* To execute LD_ABS/LD_IND instructions __bpf_prog_run() may call\n * skb_copy_bits(), so provide a weak definition of it for NET-less config.\n */\nint __weak skb_copy_bits(const struct sk_buff *skb, int offset, void *to,\n\t\t\t int len)\n{\n\treturn -EFAULT;\n}\n\n/* All definitions of tracepoints related to BPF. */\n#define CREATE_TRACE_POINTS\n#include <linux/bpf_trace.h>\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(xdp_exception);\n\n/* These are only used within the BPF_SYSCALL code */\n#ifdef CONFIG_BPF_SYSCALL\nEXPORT_TRACEPOINT_SYMBOL_GPL(bpf_prog_get_type);\nEXPORT_TRACEPOINT_SYMBOL_GPL(bpf_prog_put_rcu);\n#endif\n", "/*\n * Linux Socket Filter - Kernel level socket filtering\n *\n * Based on the design of the Berkeley Packet Filter. The new\n * internal format has been designed by PLUMgrid:\n *\n *\tCopyright (c) 2011 - 2014 PLUMgrid, http://plumgrid.com\n *\n * Authors:\n *\n *\tJay Schulist <jschlst@samba.org>\n *\tAlexei Starovoitov <ast@plumgrid.com>\n *\tDaniel Borkmann <dborkman@redhat.com>\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License\n * as published by the Free Software Foundation; either version\n * 2 of the License, or (at your option) any later version.\n *\n * Andi Kleen - Fix a few bad bugs and races.\n * Kris Katterjohn - Added many additional checks in bpf_check_classic()\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/sock_diag.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/gfp.h>\n#include <net/inet_common.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <net/netlink.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/flow_dissector.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <linux/uaccess.h>\n#include <asm/unaligned.h>\n#include <asm/cmpxchg.h>\n#include <linux/filter.h>\n#include <linux/ratelimit.h>\n#include <linux/seccomp.h>\n#include <linux/if_vlan.h>\n#include <linux/bpf.h>\n#include <net/sch_generic.h>\n#include <net/cls_cgroup.h>\n#include <net/dst_metadata.h>\n#include <net/dst.h>\n#include <net/sock_reuseport.h>\n#include <net/busy_poll.h>\n#include <net/tcp.h>\n#include <linux/bpf_trace.h>\n\n/**\n *\tsk_filter_trim_cap - run a packet through a socket filter\n *\t@sk: sock associated with &sk_buff\n *\t@skb: buffer to filter\n *\t@cap: limit on how short the eBPF program may trim the packet\n *\n * Run the eBPF program and then cut skb->data to correct size returned by\n * the program. If pkt_len is 0 we toss packet. If skb->len is smaller\n * than pkt_len we keep whole skb->data. This is the socket level\n * wrapper to BPF_PROG_RUN. It returns 0 if the packet should\n * be accepted or -EPERM if the packet should be tossed.\n *\n */\nint sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap)\n{\n\tint err;\n\tstruct sk_filter *filter;\n\n\t/*\n\t * If the skb was allocated from pfmemalloc reserves, only\n\t * allow SOCK_MEMALLOC sockets to use it as this socket is\n\t * helping free memory\n\t */\n\tif (skb_pfmemalloc(skb) && !sock_flag(sk, SOCK_MEMALLOC)) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_PFMEMALLOCDROP);\n\t\treturn -ENOMEM;\n\t}\n\terr = BPF_CGROUP_RUN_PROG_INET_INGRESS(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\terr = security_sock_rcv_skb(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter) {\n\t\tstruct sock *save_sk = skb->sk;\n\t\tunsigned int pkt_len;\n\n\t\tskb->sk = sk;\n\t\tpkt_len = bpf_prog_run_save_cb(filter->prog, skb);\n\t\tskb->sk = save_sk;\n\t\terr = pkt_len ? pskb_trim(skb, max(cap, pkt_len)) : -EPERM;\n\t}\n\trcu_read_unlock();\n\n\treturn err;\n}\nEXPORT_SYMBOL(sk_filter_trim_cap);\n\nBPF_CALL_1(__skb_get_pay_offset, struct sk_buff *, skb)\n{\n\treturn skb_get_poff(skb);\n}\n\nBPF_CALL_3(__skb_get_nlattr, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = nla_find((struct nlattr *) &skb->data[a], skb->len - a, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_3(__skb_get_nlattr_nest, struct sk_buff *, skb, u32, a, u32, x)\n{\n\tstruct nlattr *nla;\n\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\tif (skb->len < sizeof(struct nlattr))\n\t\treturn 0;\n\n\tif (a > skb->len - sizeof(struct nlattr))\n\t\treturn 0;\n\n\tnla = (struct nlattr *) &skb->data[a];\n\tif (nla->nla_len > skb->len - a)\n\t\treturn 0;\n\n\tnla = nla_find_nested(nla, x);\n\tif (nla)\n\t\treturn (void *) nla - (void *) skb->data;\n\n\treturn 0;\n}\n\nBPF_CALL_0(__get_raw_cpu_id)\n{\n\treturn raw_smp_processor_id();\n}\n\nstatic const struct bpf_func_proto bpf_get_raw_smp_processor_id_proto = {\n\t.func\t\t= __get_raw_cpu_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nstatic u32 convert_skb_access(int skb_field, int dst_reg, int src_reg,\n\t\t\t      struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (skb_field) {\n\tcase SKF_AD_MARK:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, mark) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, mark));\n\t\tbreak;\n\n\tcase SKF_AD_PKTTYPE:\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, dst_reg, src_reg, PKT_TYPE_OFFSET());\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase SKF_AD_QUEUE:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, queue_mapping) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, queue_mapping));\n\t\tbreak;\n\n\tcase SKF_AD_VLAN_TAG:\n\tcase SKF_AD_VLAN_TAG_PRESENT:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_tci) != 2);\n\t\tBUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);\n\n\t\t/* dst_reg = *(u16 *) (src_reg + offsetof(vlan_tci)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, dst_reg, src_reg,\n\t\t\t\t      offsetof(struct sk_buff, vlan_tci));\n\t\tif (skb_field == SKF_AD_VLAN_TAG) {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg,\n\t\t\t\t\t\t~VLAN_TAG_PRESENT);\n\t\t} else {\n\t\t\t/* dst_reg >>= 12 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, dst_reg, 12);\n\t\t\t/* dst_reg &= 1 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, dst_reg, 1);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic bool convert_bpf_extensions(struct sock_filter *fp,\n\t\t\t\t   struct bpf_insn **insnp)\n{\n\tstruct bpf_insn *insn = *insnp;\n\tu32 cnt;\n\n\tswitch (fp->k) {\n\tcase SKF_AD_OFF + SKF_AD_PROTOCOL:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, protocol) != 2);\n\n\t\t/* A = *(u16 *) (CTX + offsetof(protocol)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, protocol));\n\t\t/* A = ntohs(A) [emitting a nop or swap16] */\n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PKTTYPE:\n\t\tcnt = convert_skb_access(SKF_AD_PKTTYPE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_IFINDEX:\n\tcase SKF_AD_OFF + SKF_AD_HATYPE:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct net_device, ifindex) != 4);\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct net_device, type) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      BPF_REG_TMP, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t/* if (tmp != 0) goto pc + 1 */\n\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_TMP, 0, 1);\n\t\t*insn++ = BPF_EXIT_INSN();\n\t\tif (fp->k == SKF_AD_OFF + SKF_AD_IFINDEX)\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, ifindex));\n\t\telse\n\t\t\t*insn = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_TMP,\n\t\t\t\t\t    offsetof(struct net_device, type));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_MARK:\n\t\tcnt = convert_skb_access(SKF_AD_MARK, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_RXHASH:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, hash) != 4);\n\n\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t    offsetof(struct sk_buff, hash));\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_QUEUE:\n\t\tcnt = convert_skb_access(SKF_AD_QUEUE, BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TAG_PRESENT:\n\t\tcnt = convert_skb_access(SKF_AD_VLAN_TAG_PRESENT,\n\t\t\t\t\t BPF_REG_A, BPF_REG_CTX, insn);\n\t\tinsn += cnt - 1;\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_VLAN_TPID:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, vlan_proto) != 2);\n\n\t\t/* A = *(u16 *) (CTX + offsetof(vlan_proto)) */\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, BPF_REG_A, BPF_REG_CTX,\n\t\t\t\t      offsetof(struct sk_buff, vlan_proto));\n\t\t/* A = ntohs(A) [emitting a nop or swap16] */\n\t\t*insn = BPF_ENDIAN(BPF_FROM_BE, BPF_REG_A, 16);\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\tcase SKF_AD_OFF + SKF_AD_CPU:\n\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t/* arg1 = CTX */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG1, BPF_REG_CTX);\n\t\t/* arg2 = A */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG2, BPF_REG_A);\n\t\t/* arg3 = X */\n\t\t*insn++ = BPF_MOV64_REG(BPF_REG_ARG3, BPF_REG_X);\n\t\t/* Emit call(arg1=CTX, arg2=A, arg3=X) */\n\t\tswitch (fp->k) {\n\t\tcase SKF_AD_OFF + SKF_AD_PAY_OFFSET:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_pay_offset);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_nlattr);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_NLATTR_NEST:\n\t\t\t*insn = BPF_EMIT_CALL(__skb_get_nlattr_nest);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_CPU:\n\t\t\t*insn = BPF_EMIT_CALL(__get_raw_cpu_id);\n\t\t\tbreak;\n\t\tcase SKF_AD_OFF + SKF_AD_RANDOM:\n\t\t\t*insn = BPF_EMIT_CALL(bpf_user_rnd_u32);\n\t\t\tbpf_user_rnd_init_once();\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase SKF_AD_OFF + SKF_AD_ALU_XOR_X:\n\t\t/* A ^= X */\n\t\t*insn = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_X);\n\t\tbreak;\n\n\tdefault:\n\t\t/* This is just a dummy call to avoid letting the compiler\n\t\t * evict __bpf_call_base() as an optimization. Placed here\n\t\t * where no-one bothers.\n\t\t */\n\t\tBUG_ON(__bpf_call_base(0, 0, 0, 0, 0) != 0);\n\t\treturn false;\n\t}\n\n\t*insnp = insn;\n\treturn true;\n}\n\n/**\n *\tbpf_convert_filter - convert filter program\n *\t@prog: the user passed filter program\n *\t@len: the length of the user passed filter program\n *\t@new_prog: allocated 'struct bpf_prog' or NULL\n *\t@new_len: pointer to store length of converted program\n *\n * Remap 'sock_filter' style classic BPF (cBPF) instruction set to 'bpf_insn'\n * style extended BPF (eBPF).\n * Conversion workflow:\n *\n * 1) First pass for calculating the new program length:\n *   bpf_convert_filter(old_prog, old_len, NULL, &new_len)\n *\n * 2) 2nd pass to remap in two passes: 1st pass finds new\n *    jump offsets, 2nd pass remapping:\n *   bpf_convert_filter(old_prog, old_len, new_prog, &new_len);\n */\nstatic int bpf_convert_filter(struct sock_filter *prog, int len,\n\t\t\t      struct bpf_prog *new_prog, int *new_len)\n{\n\tint new_flen = 0, pass = 0, target, i, stack_off;\n\tstruct bpf_insn *new_insn, *first_insn = NULL;\n\tstruct sock_filter *fp;\n\tint *addrs = NULL;\n\tu8 bpf_src;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS * sizeof(u32) > MAX_BPF_STACK);\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\n\tif (len <= 0 || len > BPF_MAXINSNS)\n\t\treturn -EINVAL;\n\n\tif (new_prog) {\n\t\tfirst_insn = new_prog->insnsi;\n\t\taddrs = kcalloc(len, sizeof(*addrs),\n\t\t\t\tGFP_KERNEL | __GFP_NOWARN);\n\t\tif (!addrs)\n\t\t\treturn -ENOMEM;\n\t}\n\ndo_pass:\n\tnew_insn = first_insn;\n\tfp = prog;\n\n\t/* Classic BPF related prologue emission. */\n\tif (new_prog) {\n\t\t/* Classic BPF expects A and X to be reset first. These need\n\t\t * to be guaranteed to be the first two instructions.\n\t\t */\n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t*new_insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_X, BPF_REG_X);\n\n\t\t/* All programs must keep CTX in callee saved BPF_REG_CTX.\n\t\t * In eBPF case it's done by the compiler, here we need to\n\t\t * do this ourself. Initial CTX is present in BPF_REG_ARG1.\n\t\t */\n\t\t*new_insn++ = BPF_MOV64_REG(BPF_REG_CTX, BPF_REG_ARG1);\n\t} else {\n\t\tnew_insn += 3;\n\t}\n\n\tfor (i = 0; i < len; fp++, i++) {\n\t\tstruct bpf_insn tmp_insns[6] = { };\n\t\tstruct bpf_insn *insn = tmp_insns;\n\n\t\tif (addrs)\n\t\t\taddrs[i] = new_insn - first_insn;\n\n\t\tswitch (fp->code) {\n\t\t/* All arithmetic insns and skb loads map as-is. */\n\t\tcase BPF_ALU | BPF_ADD | BPF_X:\n\t\tcase BPF_ALU | BPF_ADD | BPF_K:\n\t\tcase BPF_ALU | BPF_SUB | BPF_X:\n\t\tcase BPF_ALU | BPF_SUB | BPF_K:\n\t\tcase BPF_ALU | BPF_AND | BPF_X:\n\t\tcase BPF_ALU | BPF_AND | BPF_K:\n\t\tcase BPF_ALU | BPF_OR | BPF_X:\n\t\tcase BPF_ALU | BPF_OR | BPF_K:\n\t\tcase BPF_ALU | BPF_LSH | BPF_X:\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_X:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\tcase BPF_ALU | BPF_XOR | BPF_X:\n\t\tcase BPF_ALU | BPF_XOR | BPF_K:\n\t\tcase BPF_ALU | BPF_MUL | BPF_X:\n\t\tcase BPF_ALU | BPF_MUL | BPF_K:\n\t\tcase BPF_ALU | BPF_DIV | BPF_X:\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_X:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\tcase BPF_ALU | BPF_NEG:\n\t\tcase BPF_LD | BPF_ABS | BPF_W:\n\t\tcase BPF_LD | BPF_ABS | BPF_H:\n\t\tcase BPF_LD | BPF_ABS | BPF_B:\n\t\tcase BPF_LD | BPF_IND | BPF_W:\n\t\tcase BPF_LD | BPF_IND | BPF_H:\n\t\tcase BPF_LD | BPF_IND | BPF_B:\n\t\t\t/* Check for overloaded BPF extension and\n\t\t\t * directly convert it if found, otherwise\n\t\t\t * just move on with mapping.\n\t\t\t */\n\t\t\tif (BPF_CLASS(fp->code) == BPF_LD &&\n\t\t\t    BPF_MODE(fp->code) == BPF_ABS &&\n\t\t\t    convert_bpf_extensions(fp, &insn))\n\t\t\t\tbreak;\n\n\t\t\tif (fp->code == (BPF_ALU | BPF_DIV | BPF_X) ||\n\t\t\t    fp->code == (BPF_ALU | BPF_MOD | BPF_X)) {\n\t\t\t\t*insn++ = BPF_MOV32_REG(BPF_REG_X, BPF_REG_X);\n\t\t\t\t/* Error with exception code on div/mod by 0.\n\t\t\t\t * For cBPF programs, this was always return 0.\n\t\t\t\t */\n\t\t\t\t*insn++ = BPF_JMP_IMM(BPF_JNE, BPF_REG_X, 0, 2);\n\t\t\t\t*insn++ = BPF_ALU32_REG(BPF_XOR, BPF_REG_A, BPF_REG_A);\n\t\t\t\t*insn++ = BPF_EXIT_INSN();\n\t\t\t}\n\n\t\t\t*insn = BPF_RAW_INSN(fp->code, BPF_REG_A, BPF_REG_X, 0, fp->k);\n\t\t\tbreak;\n\n\t\t/* Jump transformation cannot use BPF block macros\n\t\t * everywhere as offset calculation and target updates\n\t\t * require a bit more work than the rest, i.e. jump\n\t\t * opcodes map as-is, but offsets need adjustment.\n\t\t */\n\n#define BPF_EMIT_JMP\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tconst s32 off_min = S16_MIN, off_max = S16_MAX;\t\t\\\n\t\ts32 off;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tif (target >= len || target < 0)\t\t\t\\\n\t\t\tgoto err;\t\t\t\t\t\\\n\t\toff = addrs ? addrs[target] - addrs[i] - 1 : 0;\t\t\\\n\t\t/* Adjust pc relative offset for 2nd or 3rd insn. */\t\\\n\t\toff -= insn - tmp_insns;\t\t\t\t\\\n\t\t/* Reject anything not fitting into insn->off. */\t\\\n\t\tif (off < off_min || off > off_max)\t\t\t\\\n\t\t\tgoto err;\t\t\t\t\t\\\n\t\tinsn->off = off;\t\t\t\t\t\\\n\t} while (0)\n\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\ttarget = i + fp->k + 1;\n\t\t\tinsn->code = fp->code;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\t\tif (BPF_SRC(fp->code) == BPF_K && (int) fp->k < 0) {\n\t\t\t\t/* BPF immediates are signed, zero extend\n\t\t\t\t * immediate into tmp register and use it\n\t\t\t\t * in compare insn.\n\t\t\t\t */\n\t\t\t\t*insn++ = BPF_MOV32_IMM(BPF_REG_TMP, fp->k);\n\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->src_reg = BPF_REG_TMP;\n\t\t\t\tbpf_src = BPF_X;\n\t\t\t} else {\n\t\t\t\tinsn->dst_reg = BPF_REG_A;\n\t\t\t\tinsn->imm = fp->k;\n\t\t\t\tbpf_src = BPF_SRC(fp->code);\n\t\t\t\tinsn->src_reg = bpf_src == BPF_X ? BPF_REG_X : 0;\n\t\t\t}\n\n\t\t\t/* Common case where 'jump_false' is next insn. */\n\t\t\tif (fp->jf == 0) {\n\t\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\t\ttarget = i + fp->jt + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Convert some jumps when 'jump_true' is next insn. */\n\t\t\tif (fp->jt == 0) {\n\t\t\t\tswitch (BPF_OP(fp->code)) {\n\t\t\t\tcase BPF_JEQ:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JNE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGT:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLE | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_JGE:\n\t\t\t\t\tinsn->code = BPF_JMP | BPF_JLT | bpf_src;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tgoto jmp_rest;\n\t\t\t\t}\n\n\t\t\t\ttarget = i + fp->jf + 1;\n\t\t\t\tBPF_EMIT_JMP;\n\t\t\t\tbreak;\n\t\t\t}\njmp_rest:\n\t\t\t/* Other jumps are mapped into two insns: Jxx and JA. */\n\t\t\ttarget = i + fp->jt + 1;\n\t\t\tinsn->code = BPF_JMP | BPF_OP(fp->code) | bpf_src;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tinsn++;\n\n\t\t\tinsn->code = BPF_JMP | BPF_JA;\n\t\t\ttarget = i + fp->jf + 1;\n\t\t\tBPF_EMIT_JMP;\n\t\t\tbreak;\n\n\t\t/* ldxb 4 * ([14] & 0xf) is remaped into 6 insns. */\n\t\tcase BPF_LDX | BPF_MSH | BPF_B:\n\t\t\t/* tmp = A */\n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_TMP, BPF_REG_A);\n\t\t\t/* A = BPF_R0 = *(u8 *) (skb->data + K) */\n\t\t\t*insn++ = BPF_LD_ABS(BPF_B, fp->k);\n\t\t\t/* A &= 0xf */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_A, 0xf);\n\t\t\t/* A <<= 2 */\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, BPF_REG_A, 2);\n\t\t\t/* X = A */\n\t\t\t*insn++ = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\t/* A = tmp */\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_TMP);\n\t\t\tbreak;\n\n\t\t/* RET_K is remaped into 2 insns. RET_A case doesn't need an\n\t\t * extra mov as BPF_REG_0 is already mapped into BPF_REG_A.\n\t\t */\n\t\tcase BPF_RET | BPF_A:\n\t\tcase BPF_RET | BPF_K:\n\t\t\tif (BPF_RVAL(fp->code) == BPF_K)\n\t\t\t\t*insn++ = BPF_MOV32_RAW(BPF_K, BPF_REG_0,\n\t\t\t\t\t\t\t0, fp->k);\n\t\t\t*insn = BPF_EXIT_INSN();\n\t\t\tbreak;\n\n\t\t/* Store to stack. */\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_STX_MEM(BPF_W, BPF_REG_FP, BPF_CLASS(fp->code) ==\n\t\t\t\t\t    BPF_ST ? BPF_REG_A : BPF_REG_X,\n\t\t\t\t\t    -stack_off);\n\t\t\t/* check_load_and_stores() verifies that classic BPF can\n\t\t\t * load from stack only after write, so tracking\n\t\t\t * stack_depth for ST|STX insns is enough\n\t\t\t */\n\t\t\tif (new_prog && new_prog->aux->stack_depth < stack_off)\n\t\t\t\tnew_prog->aux->stack_depth = stack_off;\n\t\t\tbreak;\n\n\t\t/* Load from stack. */\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tstack_off = fp->k * 4  + 4;\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD  ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_FP,\n\t\t\t\t\t    -stack_off);\n\t\t\tbreak;\n\n\t\t/* A = K or X = K */\n\t\tcase BPF_LD | BPF_IMM:\n\t\tcase BPF_LDX | BPF_IMM:\n\t\t\t*insn = BPF_MOV32_IMM(BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t      BPF_REG_A : BPF_REG_X, fp->k);\n\t\t\tbreak;\n\n\t\t/* X = A */\n\t\tcase BPF_MISC | BPF_TAX:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_X, BPF_REG_A);\n\t\t\tbreak;\n\n\t\t/* A = X */\n\t\tcase BPF_MISC | BPF_TXA:\n\t\t\t*insn = BPF_MOV64_REG(BPF_REG_A, BPF_REG_X);\n\t\t\tbreak;\n\n\t\t/* A = skb->len or X = skb->len */\n\t\tcase BPF_LD | BPF_W | BPF_LEN:\n\t\tcase BPF_LDX | BPF_W | BPF_LEN:\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_CLASS(fp->code) == BPF_LD ?\n\t\t\t\t\t    BPF_REG_A : BPF_REG_X, BPF_REG_CTX,\n\t\t\t\t\t    offsetof(struct sk_buff, len));\n\t\t\tbreak;\n\n\t\t/* Access seccomp_data fields. */\n\t\tcase BPF_LDX | BPF_ABS | BPF_W:\n\t\t\t/* A = *(u32 *) (ctx + K) */\n\t\t\t*insn = BPF_LDX_MEM(BPF_W, BPF_REG_A, BPF_REG_CTX, fp->k);\n\t\t\tbreak;\n\n\t\t/* Unknown instruction. */\n\t\tdefault:\n\t\t\tgoto err;\n\t\t}\n\n\t\tinsn++;\n\t\tif (new_prog)\n\t\t\tmemcpy(new_insn, tmp_insns,\n\t\t\t       sizeof(*insn) * (insn - tmp_insns));\n\t\tnew_insn += insn - tmp_insns;\n\t}\n\n\tif (!new_prog) {\n\t\t/* Only calculating new length. */\n\t\t*new_len = new_insn - first_insn;\n\t\treturn 0;\n\t}\n\n\tpass++;\n\tif (new_flen != new_insn - first_insn) {\n\t\tnew_flen = new_insn - first_insn;\n\t\tif (pass > 2)\n\t\t\tgoto err;\n\t\tgoto do_pass;\n\t}\n\n\tkfree(addrs);\n\tBUG_ON(*new_len != new_flen);\n\treturn 0;\nerr:\n\tkfree(addrs);\n\treturn -EINVAL;\n}\n\n/* Security:\n *\n * As we dont want to clear mem[] array for each packet going through\n * __bpf_prog_run(), we check that filter loaded by user never try to read\n * a cell if not previously written, and we check all branches to be sure\n * a malicious user doesn't try to abuse us.\n */\nstatic int check_load_and_stores(const struct sock_filter *filter, int flen)\n{\n\tu16 *masks, memvalid = 0; /* One bit per cell, 16 cells */\n\tint pc, ret = 0;\n\n\tBUILD_BUG_ON(BPF_MEMWORDS > 16);\n\n\tmasks = kmalloc_array(flen, sizeof(*masks), GFP_KERNEL);\n\tif (!masks)\n\t\treturn -ENOMEM;\n\n\tmemset(masks, 0xff, flen * sizeof(*masks));\n\n\tfor (pc = 0; pc < flen; pc++) {\n\t\tmemvalid &= masks[pc];\n\n\t\tswitch (filter[pc].code) {\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\tmemvalid |= (1 << filter[pc].k);\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\t\tif (!(memvalid & (1 << filter[pc].k))) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t/* A jump must set masks on target */\n\t\t\tmasks[pc + 1 + filter[pc].k] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t/* A jump must set masks on targets */\n\t\t\tmasks[pc + 1 + filter[pc].jt] &= memvalid;\n\t\t\tmasks[pc + 1 + filter[pc].jf] &= memvalid;\n\t\t\tmemvalid = ~0;\n\t\t\tbreak;\n\t\t}\n\t}\nerror:\n\tkfree(masks);\n\treturn ret;\n}\n\nstatic bool chk_code_allowed(u16 code_to_probe)\n{\n\tstatic const bool codes[] = {\n\t\t/* 32 bit ALU operations */\n\t\t[BPF_ALU | BPF_ADD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_ADD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_K] = true,\n\t\t[BPF_ALU | BPF_SUB | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MUL | BPF_X] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_K] = true,\n\t\t[BPF_ALU | BPF_DIV | BPF_X] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_K] = true,\n\t\t[BPF_ALU | BPF_MOD | BPF_X] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_K] = true,\n\t\t[BPF_ALU | BPF_AND | BPF_X] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_OR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_K] = true,\n\t\t[BPF_ALU | BPF_XOR | BPF_X] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_LSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_K] = true,\n\t\t[BPF_ALU | BPF_RSH | BPF_X] = true,\n\t\t[BPF_ALU | BPF_NEG] = true,\n\t\t/* Load instructions */\n\t\t[BPF_LD | BPF_W | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_H | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_B | BPF_ABS] = true,\n\t\t[BPF_LD | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LD | BPF_W | BPF_IND] = true,\n\t\t[BPF_LD | BPF_H | BPF_IND] = true,\n\t\t[BPF_LD | BPF_B | BPF_IND] = true,\n\t\t[BPF_LD | BPF_IMM] = true,\n\t\t[BPF_LD | BPF_MEM] = true,\n\t\t[BPF_LDX | BPF_W | BPF_LEN] = true,\n\t\t[BPF_LDX | BPF_B | BPF_MSH] = true,\n\t\t[BPF_LDX | BPF_IMM] = true,\n\t\t[BPF_LDX | BPF_MEM] = true,\n\t\t/* Store instructions */\n\t\t[BPF_ST] = true,\n\t\t[BPF_STX] = true,\n\t\t/* Misc instructions */\n\t\t[BPF_MISC | BPF_TAX] = true,\n\t\t[BPF_MISC | BPF_TXA] = true,\n\t\t/* Return instructions */\n\t\t[BPF_RET | BPF_K] = true,\n\t\t[BPF_RET | BPF_A] = true,\n\t\t/* Jump instructions */\n\t\t[BPF_JMP | BPF_JA] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JEQ | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGE | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JGT | BPF_X] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_K] = true,\n\t\t[BPF_JMP | BPF_JSET | BPF_X] = true,\n\t};\n\n\tif (code_to_probe >= ARRAY_SIZE(codes))\n\t\treturn false;\n\n\treturn codes[code_to_probe];\n}\n\nstatic bool bpf_check_basics_ok(const struct sock_filter *filter,\n\t\t\t\tunsigned int flen)\n{\n\tif (filter == NULL)\n\t\treturn false;\n\tif (flen == 0 || flen > BPF_MAXINSNS)\n\t\treturn false;\n\n\treturn true;\n}\n\n/**\n *\tbpf_check_classic - verify socket filter code\n *\t@filter: filter to verify\n *\t@flen: length of filter\n *\n * Check the user's filter code. If we let some ugly\n * filter code slip through kaboom! The filter must contain\n * no references or jumps that are out of range, no illegal\n * instructions, and must end with a RET instruction.\n *\n * All jumps are forward as they are not signed.\n *\n * Returns 0 if the rule set is legal or -EINVAL if not.\n */\nstatic int bpf_check_classic(const struct sock_filter *filter,\n\t\t\t     unsigned int flen)\n{\n\tbool anc_found;\n\tint pc;\n\n\t/* Check the filter code now */\n\tfor (pc = 0; pc < flen; pc++) {\n\t\tconst struct sock_filter *ftest = &filter[pc];\n\n\t\t/* May we actually operate on this code? */\n\t\tif (!chk_code_allowed(ftest->code))\n\t\t\treturn -EINVAL;\n\n\t\t/* Some instructions need special checks */\n\t\tswitch (ftest->code) {\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t\t/* Check for division by zero */\n\t\t\tif (ftest->k == 0)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\t\tif (ftest->k >= 32)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_MEM:\n\t\tcase BPF_LDX | BPF_MEM:\n\t\tcase BPF_ST:\n\t\tcase BPF_STX:\n\t\t\t/* Check for invalid memory addresses */\n\t\t\tif (ftest->k >= BPF_MEMWORDS)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JA:\n\t\t\t/* Note, the large ftest->k might cause loops.\n\t\t\t * Compare this with conditional jumps below,\n\t\t\t * where offsets are limited. --ANK (981016)\n\t\t\t */\n\t\t\tif (ftest->k >= (unsigned int)(flen - pc - 1))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\t\t/* Both conditionals must be safe */\n\t\t\tif (pc + ftest->jt + 1 >= flen ||\n\t\t\t    pc + ftest->jf + 1 >= flen)\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase BPF_LD | BPF_W | BPF_ABS:\n\t\tcase BPF_LD | BPF_H | BPF_ABS:\n\t\tcase BPF_LD | BPF_B | BPF_ABS:\n\t\t\tanc_found = false;\n\t\t\tif (bpf_anc_helper(ftest) & BPF_ANC)\n\t\t\t\tanc_found = true;\n\t\t\t/* Ancillary operation unknown or unsupported */\n\t\t\tif (anc_found == false && ftest->k >= SKF_AD_OFF)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Last instruction must be a RET code */\n\tswitch (filter[flen - 1].code) {\n\tcase BPF_RET | BPF_K:\n\tcase BPF_RET | BPF_A:\n\t\treturn check_load_and_stores(filter, flen);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int bpf_prog_store_orig_filter(struct bpf_prog *fp,\n\t\t\t\t      const struct sock_fprog *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct sock_fprog_kern *fkprog;\n\n\tfp->orig_prog = kmalloc(sizeof(*fkprog), GFP_KERNEL);\n\tif (!fp->orig_prog)\n\t\treturn -ENOMEM;\n\n\tfkprog = fp->orig_prog;\n\tfkprog->len = fprog->len;\n\n\tfkprog->filter = kmemdup(fp->insns, fsize,\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!fkprog->filter) {\n\t\tkfree(fp->orig_prog);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void bpf_release_orig_filter(struct bpf_prog *fp)\n{\n\tstruct sock_fprog_kern *fprog = fp->orig_prog;\n\n\tif (fprog) {\n\t\tkfree(fprog->filter);\n\t\tkfree(fprog);\n\t}\n}\n\nstatic void __bpf_prog_release(struct bpf_prog *prog)\n{\n\tif (prog->type == BPF_PROG_TYPE_SOCKET_FILTER) {\n\t\tbpf_prog_put(prog);\n\t} else {\n\t\tbpf_release_orig_filter(prog);\n\t\tbpf_prog_free(prog);\n\t}\n}\n\nstatic void __sk_filter_release(struct sk_filter *fp)\n{\n\t__bpf_prog_release(fp->prog);\n\tkfree(fp);\n}\n\n/**\n * \tsk_filter_release_rcu - Release a socket filter by rcu_head\n *\t@rcu: rcu_head that contains the sk_filter to free\n */\nstatic void sk_filter_release_rcu(struct rcu_head *rcu)\n{\n\tstruct sk_filter *fp = container_of(rcu, struct sk_filter, rcu);\n\n\t__sk_filter_release(fp);\n}\n\n/**\n *\tsk_filter_release - release a socket filter\n *\t@fp: filter to remove\n *\n *\tRemove a filter from a socket and release its resources.\n */\nstatic void sk_filter_release(struct sk_filter *fp)\n{\n\tif (refcount_dec_and_test(&fp->refcnt))\n\t\tcall_rcu(&fp->rcu, sk_filter_release_rcu);\n}\n\nvoid sk_filter_uncharge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\n\tatomic_sub(filter_size, &sk->sk_omem_alloc);\n\tsk_filter_release(fp);\n}\n\n/* try to charge the socket memory if there is space available\n * return true on success\n */\nstatic bool __sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tu32 filter_size = bpf_prog_size(fp->prog->len);\n\n\t/* same check as in sock_kmalloc() */\n\tif (filter_size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + filter_size < sysctl_optmem_max) {\n\t\tatomic_add(filter_size, &sk->sk_omem_alloc);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool sk_filter_charge(struct sock *sk, struct sk_filter *fp)\n{\n\tif (!refcount_inc_not_zero(&fp->refcnt))\n\t\treturn false;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tsk_filter_release(fp);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic struct bpf_prog *bpf_migrate_filter(struct bpf_prog *fp)\n{\n\tstruct sock_filter *old_prog;\n\tstruct bpf_prog *old_fp;\n\tint err, new_len, old_len = fp->len;\n\n\t/* We are free to overwrite insns et al right here as it\n\t * won't be used at this point in time anymore internally\n\t * after the migration to the internal BPF instruction\n\t * representation.\n\t */\n\tBUILD_BUG_ON(sizeof(struct sock_filter) !=\n\t\t     sizeof(struct bpf_insn));\n\n\t/* Conversion cannot happen on overlapping memory areas,\n\t * so we need to keep the user BPF around until the 2nd\n\t * pass. At this time, the user BPF is stored in fp->insns.\n\t */\n\told_prog = kmemdup(fp->insns, old_len * sizeof(struct sock_filter),\n\t\t\t   GFP_KERNEL | __GFP_NOWARN);\n\tif (!old_prog) {\n\t\terr = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\t/* 1st pass: calculate the new program length. */\n\terr = bpf_convert_filter(old_prog, old_len, NULL, &new_len);\n\tif (err)\n\t\tgoto out_err_free;\n\n\t/* Expand fp for appending the new filter representation. */\n\told_fp = fp;\n\tfp = bpf_prog_realloc(old_fp, bpf_prog_size(new_len), 0);\n\tif (!fp) {\n\t\t/* The old_fp is still around in case we couldn't\n\t\t * allocate new memory, so uncharge on that one.\n\t\t */\n\t\tfp = old_fp;\n\t\terr = -ENOMEM;\n\t\tgoto out_err_free;\n\t}\n\n\tfp->len = new_len;\n\n\t/* 2nd pass: remap sock_filter insns into bpf_insn insns. */\n\terr = bpf_convert_filter(old_prog, old_len, fp, &new_len);\n\tif (err)\n\t\t/* 2nd bpf_convert_filter() can fail only if it fails\n\t\t * to allocate memory, remapping must succeed. Note,\n\t\t * that at this time old_fp has already been released\n\t\t * by krealloc().\n\t\t */\n\t\tgoto out_err_free;\n\n\tfp = bpf_prog_select_runtime(fp, &err);\n\tif (err)\n\t\tgoto out_err_free;\n\n\tkfree(old_prog);\n\treturn fp;\n\nout_err_free:\n\tkfree(old_prog);\nout_err:\n\t__bpf_prog_release(fp);\n\treturn ERR_PTR(err);\n}\n\nstatic struct bpf_prog *bpf_prepare_filter(struct bpf_prog *fp,\n\t\t\t\t\t   bpf_aux_classic_check_t trans)\n{\n\tint err;\n\n\tfp->bpf_func = NULL;\n\tfp->jited = 0;\n\n\terr = bpf_check_classic(fp->insns, fp->len);\n\tif (err) {\n\t\t__bpf_prog_release(fp);\n\t\treturn ERR_PTR(err);\n\t}\n\n\t/* There might be additional checks and transformations\n\t * needed on classic filters, f.e. in case of seccomp.\n\t */\n\tif (trans) {\n\t\terr = trans(fp->insns, fp->len);\n\t\tif (err) {\n\t\t\t__bpf_prog_release(fp);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\t/* Probe if we can JIT compile the filter and if so, do\n\t * the compilation of the filter.\n\t */\n\tbpf_jit_compile(fp);\n\n\t/* JIT compiler couldn't process this filter, so do the\n\t * internal BPF translation for the optimized interpreter.\n\t */\n\tif (!fp->jited)\n\t\tfp = bpf_migrate_filter(fp);\n\n\treturn fp;\n}\n\n/**\n *\tbpf_prog_create - create an unattached filter\n *\t@pfp: the unattached filter that is created\n *\t@fprog: the filter program\n *\n * Create a filter independent of any socket. We first run some\n * sanity checks on it to make sure it does not explode on us later.\n * If an error occurs or there is insufficient memory for the filter\n * a negative errno code is returned. On success the return is zero.\n */\nint bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tmemcpy(fp->insns, fprog->filter, fsize);\n\n\tfp->len = fprog->len;\n\t/* Since unattached filters are not copied back to user\n\t * space through sk_get_filter(), we do not need to hold\n\t * a copy here, and can spare us the work.\n\t */\n\tfp->orig_prog = NULL;\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\tfp = bpf_prepare_filter(fp, NULL);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create);\n\n/**\n *\tbpf_prog_create_from_user - create an unattached filter from user buffer\n *\t@pfp: the unattached filter that is created\n *\t@fprog: the filter program\n *\t@trans: post-classic verifier transformation handler\n *\t@save_orig: save classic BPF program\n *\n * This function effectively does the same as bpf_prog_create(), only\n * that it builds up its insns buffer from user space provided buffer.\n * It also allows for passing a bpf_aux_classic_check_t handler.\n */\nint bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,\n\t\t\t      bpf_aux_classic_check_t trans, bool save_orig)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *fp;\n\tint err;\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn -EINVAL;\n\n\tfp = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(fp->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(fp);\n\t\treturn -EFAULT;\n\t}\n\n\tfp->len = fprog->len;\n\tfp->orig_prog = NULL;\n\n\tif (save_orig) {\n\t\terr = bpf_prog_store_orig_filter(fp, fprog);\n\t\tif (err) {\n\t\t\t__bpf_prog_free(fp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\tfp = bpf_prepare_filter(fp, trans);\n\tif (IS_ERR(fp))\n\t\treturn PTR_ERR(fp);\n\n\t*pfp = fp;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_create_from_user);\n\nvoid bpf_prog_destroy(struct bpf_prog *fp)\n{\n\t__bpf_prog_release(fp);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_destroy);\n\nstatic int __sk_attach_prog(struct bpf_prog *prog, struct sock *sk)\n{\n\tstruct sk_filter *fp, *old_fp;\n\n\tfp = kmalloc(sizeof(*fp), GFP_KERNEL);\n\tif (!fp)\n\t\treturn -ENOMEM;\n\n\tfp->prog = prog;\n\n\tif (!__sk_filter_charge(sk, fp)) {\n\t\tkfree(fp);\n\t\treturn -ENOMEM;\n\t}\n\trefcount_set(&fp->refcnt, 1);\n\n\told_fp = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\trcu_assign_pointer(sk->sk_filter, fp);\n\n\tif (old_fp)\n\t\tsk_filter_uncharge(sk, old_fp);\n\n\treturn 0;\n}\n\nstatic int __reuseport_attach_prog(struct bpf_prog *prog, struct sock *sk)\n{\n\tstruct bpf_prog *old_prog;\n\tint err;\n\n\tif (bpf_prog_size(prog->len) > sysctl_optmem_max)\n\t\treturn -ENOMEM;\n\n\tif (sk_unhashed(sk) && sk->sk_reuseport) {\n\t\terr = reuseport_alloc(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t} else if (!rcu_access_pointer(sk->sk_reuseport_cb)) {\n\t\t/* The socket wasn't bound with SO_REUSEPORT */\n\t\treturn -EINVAL;\n\t}\n\n\told_prog = reuseport_attach_prog(sk, prog);\n\tif (old_prog)\n\t\tbpf_prog_destroy(old_prog);\n\n\treturn 0;\n}\n\nstatic\nstruct bpf_prog *__get_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tunsigned int fsize = bpf_classic_proglen(fprog);\n\tstruct bpf_prog *prog;\n\tint err;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\t/* Make sure new filter is there and in the right amounts. */\n\tif (!bpf_check_basics_ok(fprog->filter, fprog->len))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tprog = bpf_prog_alloc(bpf_prog_size(fprog->len), 0);\n\tif (!prog)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (copy_from_user(prog->insns, fprog->filter, fsize)) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\tprog->len = fprog->len;\n\n\terr = bpf_prog_store_orig_filter(prog, fprog);\n\tif (err) {\n\t\t__bpf_prog_free(prog);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t/* bpf_prepare_filter() already takes care of freeing\n\t * memory in case something goes wrong.\n\t */\n\treturn bpf_prepare_filter(prog, NULL);\n}\n\n/**\n *\tsk_attach_filter - attach a socket filter\n *\t@fprog: the filter program\n *\t@sk: the socket to use\n *\n * Attach the user's filter code. We first run some sanity checks on\n * it to make sure it does not explode on us later. If an error\n * occurs or there is insufficient memory for the filter a negative\n * errno code is returned. On success the return is zero.\n */\nint sk_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\t__bpf_prog_release(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sk_attach_filter);\n\nint sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_filter(fprog, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __reuseport_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\t__bpf_prog_release(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic struct bpf_prog *__get_bpf(u32 ufd, struct sock *sk)\n{\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn ERR_PTR(-EPERM);\n\n\treturn bpf_prog_get_type(ufd, BPF_PROG_TYPE_SOCKET_FILTER);\n}\n\nint sk_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_bpf(ufd, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __sk_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint sk_reuseport_attach_bpf(u32 ufd, struct sock *sk)\n{\n\tstruct bpf_prog *prog = __get_bpf(ufd, sk);\n\tint err;\n\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\terr = __reuseport_attach_prog(prog, sk);\n\tif (err < 0) {\n\t\tbpf_prog_put(prog);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstruct bpf_scratchpad {\n\tunion {\n\t\t__be32 diff[MAX_BPF_STACK / sizeof(__be32)];\n\t\tu8     buff[MAX_BPF_STACK];\n\t};\n};\n\nstatic DEFINE_PER_CPU(struct bpf_scratchpad, bpf_sp);\n\nstatic inline int __bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\t  unsigned int write_len)\n{\n\treturn skb_ensure_writable(skb, write_len);\n}\n\nstatic inline int bpf_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\tint err = __bpf_try_make_writable(skb, write_len);\n\n\tbpf_compute_data_pointers(skb);\n\treturn err;\n}\n\nstatic int bpf_try_make_head_writable(struct sk_buff *skb)\n{\n\treturn bpf_try_make_writable(skb, skb_headlen(skb));\n}\n\nstatic inline void bpf_push_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpush_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nstatic inline void bpf_pull_mac_rcsum(struct sk_buff *skb)\n{\n\tif (skb_at_tc_ingress(skb))\n\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), skb->mac_len);\n}\n\nBPF_CALL_5(bpf_skb_store_bytes, struct sk_buff *, skb, u32, offset,\n\t   const void *, from, u32, len, u64, flags)\n{\n\tvoid *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_RECOMPUTE_CSUM | BPF_F_INVALIDATE_HASH)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + len)))\n\t\treturn -EFAULT;\n\n\tptr = skb->data + offset;\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpull_rcsum(skb, ptr, len, offset);\n\n\tmemcpy(ptr, from, len);\n\n\tif (flags & BPF_F_RECOMPUTE_CSUM)\n\t\t__skb_postpush_rcsum(skb, ptr, len, offset);\n\tif (flags & BPF_F_INVALIDATE_HASH)\n\t\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_store_bytes_proto = {\n\t.func\t\t= bpf_skb_store_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_skb_load_bytes, const struct sk_buff *, skb, u32, offset,\n\t   void *, to, u32, len)\n{\n\tvoid *ptr;\n\n\tif (unlikely(offset > 0xffff))\n\t\tgoto err_clear;\n\n\tptr = skb_header_pointer(skb, offset, len, to);\n\tif (unlikely(!ptr))\n\t\tgoto err_clear;\n\tif (ptr != to)\n\t\tmemcpy(to, ptr, len);\n\n\treturn 0;\nerr_clear:\n\tmemset(to, 0, len);\n\treturn -EFAULT;\n}\n\nstatic const struct bpf_func_proto bpf_skb_load_bytes_proto = {\n\t.func\t\t= bpf_skb_load_bytes,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_2(bpf_skb_pull_data, struct sk_buff *, skb, u32, len)\n{\n\t/* Idea is the following: should the needed direct read/write\n\t * test fail during runtime, we can pull in more data and redo\n\t * again, since implicitly, we invalidate previous checks here.\n\t *\n\t * Or, since we know how much we need to make read/writeable,\n\t * this can be done once at the program beginning for direct\n\t * access case. By this we overcome limitations of only current\n\t * headroom being accessible.\n\t */\n\treturn bpf_try_make_writable(skb, len ? : skb_headlen(skb));\n}\n\nstatic const struct bpf_func_proto bpf_skb_pull_data_proto = {\n\t.func\t\t= bpf_skb_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l3_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tcsum_replace_by_diff(ptr, to);\n\t\tbreak;\n\tcase 2:\n\t\tcsum_replace2(ptr, from, to);\n\t\tbreak;\n\tcase 4:\n\t\tcsum_replace4(ptr, from, to);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l3_csum_replace_proto = {\n\t.func\t\t= bpf_l3_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_l4_csum_replace, struct sk_buff *, skb, u32, offset,\n\t   u64, from, u64, to, u64, flags)\n{\n\tbool is_pseudo = flags & BPF_F_PSEUDO_HDR;\n\tbool is_mmzero = flags & BPF_F_MARK_MANGLED_0;\n\tbool do_mforce = flags & BPF_F_MARK_ENFORCE;\n\t__sum16 *ptr;\n\n\tif (unlikely(flags & ~(BPF_F_MARK_MANGLED_0 | BPF_F_MARK_ENFORCE |\n\t\t\t       BPF_F_PSEUDO_HDR | BPF_F_HDR_FIELD_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(offset > 0xffff || offset & 1))\n\t\treturn -EFAULT;\n\tif (unlikely(bpf_try_make_writable(skb, offset + sizeof(*ptr))))\n\t\treturn -EFAULT;\n\n\tptr = (__sum16 *)(skb->data + offset);\n\tif (is_mmzero && !do_mforce && !*ptr)\n\t\treturn 0;\n\n\tswitch (flags & BPF_F_HDR_FIELD_MASK) {\n\tcase 0:\n\t\tif (unlikely(from != 0))\n\t\t\treturn -EINVAL;\n\n\t\tinet_proto_csum_replace_by_diff(ptr, skb, to, is_pseudo);\n\t\tbreak;\n\tcase 2:\n\t\tinet_proto_csum_replace2(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tcase 4:\n\t\tinet_proto_csum_replace4(ptr, skb, from, to, is_pseudo);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_mmzero && !*ptr)\n\t\t*ptr = CSUM_MANGLED_0;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_l4_csum_replace_proto = {\n\t.func\t\t= bpf_l4_csum_replace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_csum_diff, __be32 *, from, u32, from_size,\n\t   __be32 *, to, u32, to_size, __wsum, seed)\n{\n\tstruct bpf_scratchpad *sp = this_cpu_ptr(&bpf_sp);\n\tu32 diff_size = from_size + to_size;\n\tint i, j = 0;\n\n\t/* This is quite flexible, some examples:\n\t *\n\t * from_size == 0, to_size > 0,  seed := csum --> pushing data\n\t * from_size > 0,  to_size == 0, seed := csum --> pulling data\n\t * from_size > 0,  to_size > 0,  seed := 0    --> diffing data\n\t *\n\t * Even for diffing, from_size and to_size don't need to be equal.\n\t */\n\tif (unlikely(((from_size | to_size) & (sizeof(__be32) - 1)) ||\n\t\t     diff_size > sizeof(sp->diff)))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < from_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = ~from[i];\n\tfor (i = 0; i <   to_size / sizeof(__be32); i++, j++)\n\t\tsp->diff[j] = to[i];\n\n\treturn csum_partial(sp->diff, diff_size, seed);\n}\n\nstatic const struct bpf_func_proto bpf_csum_diff_proto = {\n\t.func\t\t= bpf_csum_diff,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM_OR_NULL,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_PTR_TO_MEM_OR_NULL,\n\t.arg4_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_csum_update, struct sk_buff *, skb, __wsum, csum)\n{\n\t/* The interface is to be used in combination with bpf_csum_diff()\n\t * for direct packet writes. csum rotation for alignment as well\n\t * as emulating csum_sub() can be done from the eBPF program.\n\t */\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\treturn (skb->csum = csum_add(skb->csum, csum));\n\n\treturn -ENOTSUPP;\n}\n\nstatic const struct bpf_func_proto bpf_csum_update_proto = {\n\t.func\t\t= bpf_csum_update,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic inline int __bpf_rx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\treturn dev_forward_skb(dev, skb);\n}\n\nstatic inline int __bpf_rx_skb_no_mac(struct net_device *dev,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tint ret = ____dev_forward_skb(dev, skb);\n\n\tif (likely(!ret)) {\n\t\tskb->dev = dev;\n\t\tret = netif_rx(skb);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int __bpf_tx_skb(struct net_device *dev, struct sk_buff *skb)\n{\n\tint ret;\n\n\tif (unlikely(__this_cpu_read(xmit_recursion) > XMIT_RECURSION_LIMIT)) {\n\t\tnet_crit_ratelimited(\"bpf: recursion limit reached on datapath, buggy bpf program?\\n\");\n\t\tkfree_skb(skb);\n\t\treturn -ENETDOWN;\n\t}\n\n\tskb->dev = dev;\n\n\t__this_cpu_inc(xmit_recursion);\n\tret = dev_queue_xmit(skb);\n\t__this_cpu_dec(xmit_recursion);\n\n\treturn ret;\n}\n\nstatic int __bpf_redirect_no_mac(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\t/* skb->mac_len is not set on normal egress */\n\tunsigned int mlen = skb->network_header - skb->mac_header;\n\n\t__skb_pull(skb, mlen);\n\n\t/* At ingress, the mac header has already been pulled once.\n\t * At egress, skb_pospull_rcsum has to be done in case that\n\t * the skb is originated from ingress (i.e. a forwarded skb)\n\t * to ensure that rcsum starts at net header.\n\t */\n\tif (!skb_at_tc_ingress(skb))\n\t\tskb_postpull_rcsum(skb, skb_mac_header(skb), mlen);\n\tskb_pop_mac_header(skb);\n\tskb_reset_mac_len(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb_no_mac(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect_common(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t u32 flags)\n{\n\t/* Verify that a link layer header is carried */\n\tif (unlikely(skb->mac_header >= skb->network_header)) {\n\t\tkfree_skb(skb);\n\t\treturn -ERANGE;\n\t}\n\n\tbpf_push_mac_rcsum(skb);\n\treturn flags & BPF_F_INGRESS ?\n\t       __bpf_rx_skb(dev, skb) : __bpf_tx_skb(dev, skb);\n}\n\nstatic int __bpf_redirect(struct sk_buff *skb, struct net_device *dev,\n\t\t\t  u32 flags)\n{\n\tif (dev_is_mac_header_xmit(dev))\n\t\treturn __bpf_redirect_common(skb, dev, flags);\n\telse\n\t\treturn __bpf_redirect_no_mac(skb, dev, flags);\n}\n\nBPF_CALL_3(bpf_clone_redirect, struct sk_buff *, skb, u32, ifindex, u64, flags)\n{\n\tstruct net_device *dev;\n\tstruct sk_buff *clone;\n\tint ret;\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn -EINVAL;\n\n\tdev = dev_get_by_index_rcu(dev_net(skb->dev), ifindex);\n\tif (unlikely(!dev))\n\t\treturn -EINVAL;\n\n\tclone = skb_clone(skb, GFP_ATOMIC);\n\tif (unlikely(!clone))\n\t\treturn -ENOMEM;\n\n\t/* For direct write, we need to keep the invariant that the skbs\n\t * we're dealing with need to be uncloned. Should uncloning fail\n\t * here, we need to free the just generated clone to unclone once\n\t * again.\n\t */\n\tret = bpf_try_make_head_writable(skb);\n\tif (unlikely(ret)) {\n\t\tkfree_skb(clone);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn __bpf_redirect(clone, dev, flags);\n}\n\nstatic const struct bpf_func_proto bpf_clone_redirect_proto = {\n\t.func           = bpf_clone_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nstruct redirect_info {\n\tu32 ifindex;\n\tu32 flags;\n\tstruct bpf_map *map;\n\tstruct bpf_map *map_to_flush;\n\tunsigned long   map_owner;\n};\n\nstatic DEFINE_PER_CPU(struct redirect_info, redirect_info);\n\nBPF_CALL_2(bpf_redirect, u32, ifindex, u64, flags)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn TC_ACT_SHOT;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\n\treturn TC_ACT_REDIRECT;\n}\n\nint skb_do_redirect(struct sk_buff *skb)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct net_device *dev;\n\n\tdev = dev_get_by_index_rcu(dev_net(skb->dev), ri->ifindex);\n\tri->ifindex = 0;\n\tif (unlikely(!dev)) {\n\t\tkfree_skb(skb);\n\t\treturn -EINVAL;\n\t}\n\n\treturn __bpf_redirect(skb, dev, ri->flags);\n}\n\nstatic const struct bpf_func_proto bpf_redirect_proto = {\n\t.func           = bpf_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_sk_redirect_map, struct sk_buff *, skb,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\t/* If user passes invalid input drop the packet. */\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\ttcb->bpf.key = key;\n\ttcb->bpf.flags = flags;\n\ttcb->bpf.map = map;\n\n\treturn SK_PASS;\n}\n\nstruct sock *do_sk_redirect_map(struct sk_buff *skb)\n{\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\tstruct sock *sk = NULL;\n\n\tif (tcb->bpf.map) {\n\t\tsk = __sock_map_lookup_elem(tcb->bpf.map, tcb->bpf.key);\n\n\t\ttcb->bpf.key = 0;\n\t\ttcb->bpf.map = NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic const struct bpf_func_proto bpf_sk_redirect_map_proto = {\n\t.func           = bpf_sk_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_redirect_map, struct sk_msg_buff *, msg,\n\t   struct bpf_map *, map, u32, key, u64, flags)\n{\n\t/* If user passes invalid input drop the packet. */\n\tif (unlikely(flags & ~(BPF_F_INGRESS)))\n\t\treturn SK_DROP;\n\n\tmsg->key = key;\n\tmsg->flags = flags;\n\tmsg->map = map;\n\n\treturn SK_PASS;\n}\n\nstruct sock *do_msg_redirect_map(struct sk_msg_buff *msg)\n{\n\tstruct sock *sk = NULL;\n\n\tif (msg->map) {\n\t\tsk = __sock_map_lookup_elem(msg->map, msg->key);\n\n\t\tmsg->key = 0;\n\t\tmsg->map = NULL;\n\t}\n\n\treturn sk;\n}\n\nstatic const struct bpf_func_proto bpf_msg_redirect_map_proto = {\n\t.func           = bpf_msg_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_apply_bytes, struct sk_msg_buff *, msg, u32, bytes)\n{\n\tmsg->apply_bytes = bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_apply_bytes_proto = {\n\t.func           = bpf_msg_apply_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_msg_cork_bytes, struct sk_msg_buff *, msg, u32, bytes)\n{\n\tmsg->cork_bytes = bytes;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_cork_bytes_proto = {\n\t.func           = bpf_msg_cork_bytes,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_msg_pull_data,\n\t   struct sk_msg_buff *, msg, u32, start, u32, end, u64, flags)\n{\n\tunsigned int len = 0, offset = 0, copy = 0;\n\tstruct scatterlist *sg = msg->sg_data;\n\tint first_sg, last_sg, i, shift;\n\tunsigned char *p, *to, *from;\n\tint bytes = end - start;\n\tstruct page *page;\n\n\tif (unlikely(flags || end <= start))\n\t\treturn -EINVAL;\n\n\t/* First find the starting scatterlist element */\n\ti = msg->sg_start;\n\tdo {\n\t\tlen = sg[i].length;\n\t\toffset += len;\n\t\tif (start < offset + len)\n\t\t\tbreak;\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (i != msg->sg_end);\n\n\tif (unlikely(start >= offset + len))\n\t\treturn -EINVAL;\n\n\tif (!msg->sg_copy[i] && bytes <= len)\n\t\tgoto out;\n\n\tfirst_sg = i;\n\n\t/* At this point we need to linearize multiple scatterlist\n\t * elements or a single shared page. Either way we need to\n\t * copy into a linear buffer exclusively owned by BPF. Then\n\t * place the buffer in the scatterlist and fixup the original\n\t * entries by removing the entries now in the linear buffer\n\t * and shifting the remaining entries. For now we do not try\n\t * to copy partial entries to avoid complexity of running out\n\t * of sg_entry slots. The downside is reading a single byte\n\t * will copy the entire sg entry.\n\t */\n\tdo {\n\t\tcopy += sg[i].length;\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t\tif (bytes < copy)\n\t\t\tbreak;\n\t} while (i != msg->sg_end);\n\tlast_sg = i;\n\n\tif (unlikely(copy < end - start))\n\t\treturn -EINVAL;\n\n\tpage = alloc_pages(__GFP_NOWARN | GFP_ATOMIC, get_order(copy));\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\tp = page_address(page);\n\toffset = 0;\n\n\ti = first_sg;\n\tdo {\n\t\tfrom = sg_virt(&sg[i]);\n\t\tlen = sg[i].length;\n\t\tto = p + offset;\n\n\t\tmemcpy(to, from, len);\n\t\toffset += len;\n\t\tsg[i].length = 0;\n\t\tput_page(sg_page(&sg[i]));\n\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (i != last_sg);\n\n\tsg[first_sg].length = copy;\n\tsg_set_page(&sg[first_sg], page, copy, 0);\n\n\t/* To repair sg ring we need to shift entries. If we only\n\t * had a single entry though we can just replace it and\n\t * be done. Otherwise walk the ring and shift the entries.\n\t */\n\tshift = last_sg - first_sg - 1;\n\tif (!shift)\n\t\tgoto out;\n\n\ti = first_sg + 1;\n\tdo {\n\t\tint move_from;\n\n\t\tif (i + shift >= MAX_SKB_FRAGS)\n\t\t\tmove_from = i + shift - MAX_SKB_FRAGS;\n\t\telse\n\t\t\tmove_from = i + shift;\n\n\t\tif (move_from == msg->sg_end)\n\t\t\tbreak;\n\n\t\tsg[i] = sg[move_from];\n\t\tsg[move_from].length = 0;\n\t\tsg[move_from].page_link = 0;\n\t\tsg[move_from].offset = 0;\n\n\t\ti++;\n\t\tif (i == MAX_SKB_FRAGS)\n\t\t\ti = 0;\n\t} while (1);\n\tmsg->sg_end -= shift;\n\tif (msg->sg_end < 0)\n\t\tmsg->sg_end += MAX_SKB_FRAGS;\nout:\n\tmsg->data = sg_virt(&sg[i]) + start - offset;\n\tmsg->data_end = msg->data + bytes;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_msg_pull_data_proto = {\n\t.func\t\t= bpf_msg_pull_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_get_cgroup_classid, const struct sk_buff *, skb)\n{\n\treturn task_get_classid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_cgroup_classid_proto = {\n\t.func           = bpf_get_cgroup_classid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_route_realm, const struct sk_buff *, skb)\n{\n\treturn dst_tclassid(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_route_realm_proto = {\n\t.func           = bpf_get_route_realm,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_hash_recalc, struct sk_buff *, skb)\n{\n\t/* If skb_clear_hash() was called due to mangling, we can\n\t * trigger SW recalculation here. Later access to hash\n\t * can then use the inline skb->hash via context directly\n\t * instead of calling this helper again.\n\t */\n\treturn skb_get_hash(skb);\n}\n\nstatic const struct bpf_func_proto bpf_get_hash_recalc_proto = {\n\t.func\t\t= bpf_get_hash_recalc,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_set_hash_invalid, struct sk_buff *, skb)\n{\n\t/* After all direct packet write, this can be used once for\n\t * triggering a lazy recalc on next skb_get_hash() invocation.\n\t */\n\tskb_clear_hash(skb);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_invalid_proto = {\n\t.func\t\t= bpf_set_hash_invalid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_2(bpf_set_hash, struct sk_buff *, skb, u32, hash)\n{\n\t/* Set user specified hash as L4(+), so that it gets returned\n\t * on skb_get_hash() call unless BPF prog later on triggers a\n\t * skb_clear_hash().\n\t */\n\t__skb_set_sw_hash(skb, hash, true);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_set_hash_proto = {\n\t.func\t\t= bpf_set_hash,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_vlan_push, struct sk_buff *, skb, __be16, vlan_proto,\n\t   u16, vlan_tci)\n{\n\tint ret;\n\n\tif (unlikely(vlan_proto != htons(ETH_P_8021Q) &&\n\t\t     vlan_proto != htons(ETH_P_8021AD)))\n\t\tvlan_proto = htons(ETH_P_8021Q);\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_push(skb, vlan_proto, vlan_tci);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_skb_vlan_push_proto = {\n\t.func           = bpf_skb_vlan_push,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\nEXPORT_SYMBOL_GPL(bpf_skb_vlan_push_proto);\n\nBPF_CALL_1(bpf_skb_vlan_pop, struct sk_buff *, skb)\n{\n\tint ret;\n\n\tbpf_push_mac_rcsum(skb);\n\tret = skb_vlan_pop(skb);\n\tbpf_pull_mac_rcsum(skb);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_skb_vlan_pop_proto = {\n\t.func           = bpf_skb_vlan_pop,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\nEXPORT_SYMBOL_GPL(bpf_skb_vlan_pop_proto);\n\nstatic int bpf_skb_generic_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\t/* Caller already did skb_cow() with len as headroom,\n\t * so no need to do it here.\n\t */\n\tskb_push(skb, len);\n\tmemmove(skb->data, skb->data + len, off);\n\tmemset(skb->data + off, 0, len);\n\n\t/* No skb_postpush_rcsum(skb, skb->data + off, len)\n\t * needed here as it does not change the skb->csum\n\t * result for checksum complete when summing over\n\t * zeroed blocks.\n\t */\n\treturn 0;\n}\n\nstatic int bpf_skb_generic_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\t/* skb_ensure_writable() is not needed here, as we're\n\t * already working on an uncloned skb.\n\t */\n\tif (unlikely(!pskb_may_pull(skb, off + len)))\n\t\treturn -ENOMEM;\n\n\tskb_postpull_rcsum(skb, skb->data + off, len);\n\tmemmove(skb->data + len, skb->data, off);\n\t__skb_pull(skb, len);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_hdr_push(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t/* There's no need for __skb_push()/__skb_pull() pair to\n\t * get to the start of the mac header as we're guaranteed\n\t * to always start from here under eBPF.\n\t */\n\tret = bpf_skb_generic_push(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header -= len;\n\t\tskb->network_header -= len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_net_hdr_pop(struct sk_buff *skb, u32 off, u32 len)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tint ret;\n\n\t/* Same here, __skb_push()/__skb_pull() pair not needed. */\n\tret = bpf_skb_generic_pop(skb, off, len);\n\tif (likely(!ret)) {\n\t\tskb->mac_header += len;\n\t\tskb->network_header += len;\n\t\tif (trans_same)\n\t\t\tskb->transport_header = skb->network_header;\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_skb_proto_4_to_6(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_cow(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* SKB_GSO_TCPV4 needs to be changed into\n\t\t * SKB_GSO_TCPV6.\n\t\t */\n\t\tif (shinfo->gso_type & SKB_GSO_TCPV4) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV4;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV6;\n\t\t}\n\n\t\t/* Due to IPv6 header, MSS needs to be downgraded. */\n\t\tskb_decrease_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_6_to_4(struct sk_buff *skb)\n{\n\tconst u32 len_diff = sizeof(struct ipv6hdr) - sizeof(struct iphdr);\n\tu32 off = skb_mac_header_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* SKB_GSO_TCPV6 needs to be changed into\n\t\t * SKB_GSO_TCPV4.\n\t\t */\n\t\tif (shinfo->gso_type & SKB_GSO_TCPV6) {\n\t\t\tshinfo->gso_type &= ~SKB_GSO_TCPV6;\n\t\t\tshinfo->gso_type |=  SKB_GSO_TCPV4;\n\t\t}\n\n\t\t/* Due to IPv4 header, MSS can be upgraded. */\n\t\tskb_increase_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\tskb->protocol = htons(ETH_P_IP);\n\tskb_clear_hash(skb);\n\n\treturn 0;\n}\n\nstatic int bpf_skb_proto_xlat(struct sk_buff *skb, __be16 to_proto)\n{\n\t__be16 from_proto = skb->protocol;\n\n\tif (from_proto == htons(ETH_P_IP) &&\n\t      to_proto == htons(ETH_P_IPV6))\n\t\treturn bpf_skb_proto_4_to_6(skb);\n\n\tif (from_proto == htons(ETH_P_IPV6) &&\n\t      to_proto == htons(ETH_P_IP))\n\t\treturn bpf_skb_proto_6_to_4(skb);\n\n\treturn -ENOTSUPP;\n}\n\nBPF_CALL_3(bpf_skb_change_proto, struct sk_buff *, skb, __be16, proto,\n\t   u64, flags)\n{\n\tint ret;\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\t/* General idea is that this helper does the basic groundwork\n\t * needed for changing the protocol, and eBPF program fills the\n\t * rest through bpf_skb_store_bytes(), bpf_lX_csum_replace()\n\t * and other helpers, rather than passing a raw buffer here.\n\t *\n\t * The rationale is to keep this minimal and without a need to\n\t * deal with raw packet data. F.e. even if we would pass buffers\n\t * here, the program still needs to call the bpf_lX_csum_replace()\n\t * helpers anyway. Plus, this way we keep also separation of\n\t * concerns, since f.e. bpf_skb_store_bytes() should only take\n\t * care of stores.\n\t *\n\t * Currently, additional options and extension header space are\n\t * not supported, but flags register is reserved so we can adapt\n\t * that. For offloads, we mark packet as dodgy, so that headers\n\t * need to be verified first.\n\t */\n\tret = bpf_skb_proto_xlat(skb, proto);\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_proto_proto = {\n\t.func\t\t= bpf_skb_change_proto,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_skb_change_type, struct sk_buff *, skb, u32, pkt_type)\n{\n\t/* We only allow a restricted subset to be changed for now. */\n\tif (unlikely(!skb_pkt_type_ok(skb->pkt_type) ||\n\t\t     !skb_pkt_type_ok(pkt_type)))\n\t\treturn -EINVAL;\n\n\tskb->pkt_type = pkt_type;\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_type_proto = {\n\t.func\t\t= bpf_skb_change_type,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic u32 bpf_skb_net_base_len(const struct sk_buff *skb)\n{\n\tswitch (skb->protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn sizeof(struct iphdr);\n\tcase htons(ETH_P_IPV6):\n\t\treturn sizeof(struct ipv6hdr);\n\tdefault:\n\t\treturn ~0U;\n\t}\n}\n\nstatic int bpf_skb_net_grow(struct sk_buff *skb, u32 len_diff)\n{\n\tu32 off = skb_mac_header_len(skb) + bpf_skb_net_base_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_cow(skb, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_push(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* Due to header grow, MSS needs to be downgraded. */\n\t\tskb_decrease_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int bpf_skb_net_shrink(struct sk_buff *skb, u32 len_diff)\n{\n\tu32 off = skb_mac_header_len(skb) + bpf_skb_net_base_len(skb);\n\tint ret;\n\n\t/* SCTP uses GSO_BY_FRAGS, thus cannot adjust it. */\n\tif (skb_is_gso(skb) && unlikely(skb_is_gso_sctp(skb)))\n\t\treturn -ENOTSUPP;\n\n\tret = skb_unclone(skb, GFP_ATOMIC);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = bpf_skb_net_hdr_pop(skb, off, len_diff);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (skb_is_gso(skb)) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t\t/* Due to header shrink, MSS can be upgraded. */\n\t\tskb_increase_gso_size(shinfo, len_diff);\n\t\t/* Header must be checked, and gso_segs recomputed. */\n\t\tshinfo->gso_type |= SKB_GSO_DODGY;\n\t\tshinfo->gso_segs = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic u32 __bpf_skb_max_len(const struct sk_buff *skb)\n{\n\treturn skb->dev->mtu + skb->dev->hard_header_len;\n}\n\nstatic int bpf_skb_adjust_net(struct sk_buff *skb, s32 len_diff)\n{\n\tbool trans_same = skb->transport_header == skb->network_header;\n\tu32 len_cur, len_diff_abs = abs(len_diff);\n\tu32 len_min = bpf_skb_net_base_len(skb);\n\tu32 len_max = __bpf_skb_max_len(skb);\n\t__be16 proto = skb->protocol;\n\tbool shrink = len_diff < 0;\n\tint ret;\n\n\tif (unlikely(len_diff_abs > 0xfffU))\n\t\treturn -EFAULT;\n\tif (unlikely(proto != htons(ETH_P_IP) &&\n\t\t     proto != htons(ETH_P_IPV6)))\n\t\treturn -ENOTSUPP;\n\n\tlen_cur = skb->len - skb_network_offset(skb);\n\tif (skb_transport_header_was_set(skb) && !trans_same)\n\t\tlen_cur = skb_network_header_len(skb);\n\tif ((shrink && (len_diff_abs >= len_cur ||\n\t\t\tlen_cur - len_diff_abs < len_min)) ||\n\t    (!shrink && (skb->len + len_diff_abs > len_max &&\n\t\t\t !skb_is_gso(skb))))\n\t\treturn -ENOTSUPP;\n\n\tret = shrink ? bpf_skb_net_shrink(skb, len_diff_abs) :\n\t\t       bpf_skb_net_grow(skb, len_diff_abs);\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nBPF_CALL_4(bpf_skb_adjust_room, struct sk_buff *, skb, s32, len_diff,\n\t   u32, mode, u64, flags)\n{\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\tif (likely(mode == BPF_ADJ_ROOM_NET))\n\t\treturn bpf_skb_adjust_net(skb, len_diff);\n\n\treturn -ENOTSUPP;\n}\n\nstatic const struct bpf_func_proto bpf_skb_adjust_room_proto = {\n\t.func\t\t= bpf_skb_adjust_room,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic u32 __bpf_skb_min_len(const struct sk_buff *skb)\n{\n\tu32 min_len = skb_network_offset(skb);\n\n\tif (skb_transport_header_was_set(skb))\n\t\tmin_len = skb_transport_offset(skb);\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tmin_len = skb_checksum_start_offset(skb) +\n\t\t\t  skb->csum_offset + sizeof(__sum16);\n\treturn min_len;\n}\n\nstatic int bpf_skb_grow_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\tunsigned int old_len = skb->len;\n\tint ret;\n\n\tret = __skb_grow_rcsum(skb, new_len);\n\tif (!ret)\n\t\tmemset(skb->data + old_len, 0, new_len - old_len);\n\treturn ret;\n}\n\nstatic int bpf_skb_trim_rcsum(struct sk_buff *skb, unsigned int new_len)\n{\n\treturn __skb_trim_rcsum(skb, new_len);\n}\n\nBPF_CALL_3(bpf_skb_change_tail, struct sk_buff *, skb, u32, new_len,\n\t   u64, flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 min_len = __bpf_skb_min_len(skb);\n\tint ret;\n\n\tif (unlikely(flags || new_len > max_len || new_len < min_len))\n\t\treturn -EINVAL;\n\tif (skb->encapsulation)\n\t\treturn -ENOTSUPP;\n\n\t/* The basic idea of this helper is that it's performing the\n\t * needed work to either grow or trim an skb, and eBPF program\n\t * rewrites the rest via helpers like bpf_skb_store_bytes(),\n\t * bpf_lX_csum_replace() and others rather than passing a raw\n\t * buffer here. This one is a slow path helper and intended\n\t * for replies with control messages.\n\t *\n\t * Like in bpf_skb_change_proto(), we want to keep this rather\n\t * minimal and without protocol specifics so that we are able\n\t * to separate concerns as in bpf_skb_store_bytes() should only\n\t * be the one responsible for writing buffers.\n\t *\n\t * It's really expected to be a slow path operation here for\n\t * control message replies, so we're implicitly linearizing,\n\t * uncloning and drop offloads from the skb by this.\n\t */\n\tret = __bpf_try_make_writable(skb, skb->len);\n\tif (!ret) {\n\t\tif (new_len > skb->len)\n\t\t\tret = bpf_skb_grow_rcsum(skb, new_len);\n\t\telse if (new_len < skb->len)\n\t\t\tret = bpf_skb_trim_rcsum(skb, new_len);\n\t\tif (!ret && skb_is_gso(skb))\n\t\t\tskb_gso_reset(skb);\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_tail_proto = {\n\t.func\t\t= bpf_skb_change_tail,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_change_head, struct sk_buff *, skb, u32, head_room,\n\t   u64, flags)\n{\n\tu32 max_len = __bpf_skb_max_len(skb);\n\tu32 new_len = skb->len + head_room;\n\tint ret;\n\n\tif (unlikely(flags || (!skb_is_gso(skb) && new_len > max_len) ||\n\t\t     new_len < skb->len))\n\t\treturn -EINVAL;\n\n\tret = skb_cow(skb, head_room);\n\tif (likely(!ret)) {\n\t\t/* Idea for this helper is that we currently only\n\t\t * allow to expand on mac header. This means that\n\t\t * skb->protocol network header, etc, stay as is.\n\t\t * Compared to bpf_skb_change_tail(), we're more\n\t\t * flexible due to not needing to linearize or\n\t\t * reset GSO. Intention for this helper is to be\n\t\t * used by an L3 skb that needs to push mac header\n\t\t * for redirection into L2 device.\n\t\t */\n\t\t__skb_push(skb, head_room);\n\t\tmemset(skb->data, 0, head_room);\n\t\tskb_reset_mac_header(skb);\n\t}\n\n\tbpf_compute_data_pointers(skb);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_change_head_proto = {\n\t.func\t\t= bpf_skb_change_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic unsigned long xdp_get_metalen(const struct xdp_buff *xdp)\n{\n\treturn xdp_data_meta_unsupported(xdp) ? 0 :\n\t       xdp->data - xdp->data_meta;\n}\n\nBPF_CALL_2(bpf_xdp_adjust_head, struct xdp_buff *, xdp, int, offset)\n{\n\tunsigned long metalen = xdp_get_metalen(xdp);\n\tvoid *data_start = xdp->data_hard_start + metalen;\n\tvoid *data = xdp->data + offset;\n\n\tif (unlikely(data < data_start ||\n\t\t     data > xdp->data_end - ETH_HLEN))\n\t\treturn -EINVAL;\n\n\tif (metalen)\n\t\tmemmove(xdp->data_meta + offset,\n\t\t\txdp->data_meta, metalen);\n\txdp->data_meta += offset;\n\txdp->data = data;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_head_proto = {\n\t.func\t\t= bpf_xdp_adjust_head,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_xdp_adjust_meta, struct xdp_buff *, xdp, int, offset)\n{\n\tvoid *meta = xdp->data_meta + offset;\n\tunsigned long metalen = xdp->data - meta;\n\n\tif (xdp_data_meta_unsupported(xdp))\n\t\treturn -ENOTSUPP;\n\tif (unlikely(meta < xdp->data_hard_start ||\n\t\t     meta > xdp->data))\n\t\treturn -EINVAL;\n\tif (unlikely((metalen & (sizeof(__u32) - 1)) ||\n\t\t     (metalen > 32)))\n\t\treturn -EACCES;\n\n\txdp->data_meta = meta;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_adjust_meta_proto = {\n\t.func\t\t= bpf_xdp_adjust_meta,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nstatic int __bpf_tx_xdp(struct net_device *dev,\n\t\t\tstruct bpf_map *map,\n\t\t\tstruct xdp_buff *xdp,\n\t\t\tu32 index)\n{\n\tint err;\n\n\tif (!dev->netdev_ops->ndo_xdp_xmit) {\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\terr = dev->netdev_ops->ndo_xdp_xmit(dev, xdp);\n\tif (err)\n\t\treturn err;\n\tdev->netdev_ops->ndo_xdp_flush(dev);\n\treturn 0;\n}\n\nstatic int __bpf_tx_xdp_map(struct net_device *dev_rx, void *fwd,\n\t\t\t    struct bpf_map *map,\n\t\t\t    struct xdp_buff *xdp,\n\t\t\t    u32 index)\n{\n\tint err;\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tstruct net_device *dev = fwd;\n\n\t\tif (!dev->netdev_ops->ndo_xdp_xmit)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\terr = dev->netdev_ops->ndo_xdp_xmit(dev, xdp);\n\t\tif (err)\n\t\t\treturn err;\n\t\t__dev_map_insert_ctx(map, index);\n\n\t} else if (map->map_type == BPF_MAP_TYPE_CPUMAP) {\n\t\tstruct bpf_cpu_map_entry *rcpu = fwd;\n\n\t\terr = cpu_map_enqueue(rcpu, xdp, dev_rx);\n\t\tif (err)\n\t\t\treturn err;\n\t\t__cpu_map_insert_ctx(map, index);\n\t}\n\treturn 0;\n}\n\nvoid xdp_do_flush_map(void)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct bpf_map *map = ri->map_to_flush;\n\n\tri->map_to_flush = NULL;\n\tif (map) {\n\t\tswitch (map->map_type) {\n\t\tcase BPF_MAP_TYPE_DEVMAP:\n\t\t\t__dev_map_flush(map);\n\t\t\tbreak;\n\t\tcase BPF_MAP_TYPE_CPUMAP:\n\t\t\t__cpu_map_flush(map);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(xdp_do_flush_map);\n\nstatic void *__xdp_map_lookup_elem(struct bpf_map *map, u32 index)\n{\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_DEVMAP:\n\t\treturn __dev_map_lookup_elem(map, index);\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\treturn __cpu_map_lookup_elem(map, index);\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic inline bool xdp_map_invalid(const struct bpf_prog *xdp_prog,\n\t\t\t\t   unsigned long aux)\n{\n\treturn (unsigned long)xdp_prog->aux != aux;\n}\n\nstatic int xdp_do_redirect_map(struct net_device *dev, struct xdp_buff *xdp,\n\t\t\t       struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tunsigned long map_owner = ri->map_owner;\n\tstruct bpf_map *map = ri->map;\n\tu32 index = ri->ifindex;\n\tvoid *fwd = NULL;\n\tint err;\n\n\tri->ifindex = 0;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\tif (unlikely(xdp_map_invalid(xdp_prog, map_owner))) {\n\t\terr = -EFAULT;\n\t\tmap = NULL;\n\t\tgoto err;\n\t}\n\n\tfwd = __xdp_map_lookup_elem(map, index);\n\tif (!fwd) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\tif (ri->map_to_flush && ri->map_to_flush != map)\n\t\txdp_do_flush_map();\n\n\terr = __bpf_tx_xdp_map(dev, fwd, map, xdp, index);\n\tif (unlikely(err))\n\t\tgoto err;\n\n\tri->map_to_flush = map;\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map, index, err);\n\treturn err;\n}\n\nint xdp_do_redirect(struct net_device *dev, struct xdp_buff *xdp,\n\t\t    struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tstruct net_device *fwd;\n\tu32 index = ri->ifindex;\n\tint err;\n\n\tif (ri->map)\n\t\treturn xdp_do_redirect_map(dev, xdp, xdp_prog);\n\n\tfwd = dev_get_by_index_rcu(dev_net(dev), index);\n\tri->ifindex = 0;\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\terr = __bpf_tx_xdp(fwd, NULL, xdp, 0);\n\tif (unlikely(err))\n\t\tgoto err;\n\n\t_trace_xdp_redirect(dev, xdp_prog, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_err(dev, xdp_prog, index, err);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xdp_do_redirect);\n\nstatic int __xdp_generic_ok_fwd_dev(struct sk_buff *skb, struct net_device *fwd)\n{\n\tunsigned int len;\n\n\tif (unlikely(!(fwd->flags & IFF_UP)))\n\t\treturn -ENETDOWN;\n\n\tlen = fwd->mtu + fwd->hard_header_len + VLAN_HLEN;\n\tif (skb->len > len)\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}\n\nstatic int xdp_do_generic_redirect_map(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tunsigned long map_owner = ri->map_owner;\n\tstruct bpf_map *map = ri->map;\n\tstruct net_device *fwd = NULL;\n\tu32 index = ri->ifindex;\n\tint err = 0;\n\n\tri->ifindex = 0;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\tif (unlikely(xdp_map_invalid(xdp_prog, map_owner))) {\n\t\terr = -EFAULT;\n\t\tmap = NULL;\n\t\tgoto err;\n\t}\n\tfwd = __xdp_map_lookup_elem(map, index);\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tif (unlikely((err = __xdp_generic_ok_fwd_dev(skb, fwd))))\n\t\t\tgoto err;\n\t\tskb->dev = fwd;\n\t} else {\n\t\t/* TODO: Handle BPF_MAP_TYPE_CPUMAP */\n\t\terr = -EBADRQC;\n\t\tgoto err;\n\t}\n\n\t_trace_xdp_redirect_map(dev, xdp_prog, fwd, map, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_map_err(dev, xdp_prog, fwd, map, index, err);\n\treturn err;\n}\n\nint xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    struct bpf_prog *xdp_prog)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\tu32 index = ri->ifindex;\n\tstruct net_device *fwd;\n\tint err = 0;\n\n\tif (ri->map)\n\t\treturn xdp_do_generic_redirect_map(dev, skb, xdp_prog);\n\n\tri->ifindex = 0;\n\tfwd = dev_get_by_index_rcu(dev_net(dev), index);\n\tif (unlikely(!fwd)) {\n\t\terr = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tif (unlikely((err = __xdp_generic_ok_fwd_dev(skb, fwd))))\n\t\tgoto err;\n\n\tskb->dev = fwd;\n\t_trace_xdp_redirect(dev, xdp_prog, index);\n\treturn 0;\nerr:\n\t_trace_xdp_redirect_err(dev, xdp_prog, index, err);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(xdp_do_generic_redirect);\n\nBPF_CALL_2(bpf_xdp_redirect, u32, ifindex, u64, flags)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn XDP_ABORTED;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\tri->map = NULL;\n\tri->map_owner = 0;\n\n\treturn XDP_REDIRECT;\n}\n\nstatic const struct bpf_func_proto bpf_xdp_redirect_proto = {\n\t.func           = bpf_xdp_redirect,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_ANYTHING,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_xdp_redirect_map, struct bpf_map *, map, u32, ifindex, u64, flags,\n\t   unsigned long, map_owner)\n{\n\tstruct redirect_info *ri = this_cpu_ptr(&redirect_info);\n\n\tif (unlikely(flags))\n\t\treturn XDP_ABORTED;\n\n\tri->ifindex = ifindex;\n\tri->flags = flags;\n\tri->map = map;\n\tri->map_owner = map_owner;\n\n\treturn XDP_REDIRECT;\n}\n\n/* Note, arg4 is hidden from users and populated by the verifier\n * with the right pointer.\n */\nstatic const struct bpf_func_proto bpf_xdp_redirect_map_proto = {\n\t.func           = bpf_xdp_redirect_map,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_CONST_MAP_PTR,\n\t.arg2_type      = ARG_ANYTHING,\n\t.arg3_type      = ARG_ANYTHING,\n};\n\nbool bpf_helper_changes_pkt_data(void *func)\n{\n\tif (func == bpf_skb_vlan_push ||\n\t    func == bpf_skb_vlan_pop ||\n\t    func == bpf_skb_store_bytes ||\n\t    func == bpf_skb_change_proto ||\n\t    func == bpf_skb_change_head ||\n\t    func == bpf_skb_change_tail ||\n\t    func == bpf_skb_adjust_room ||\n\t    func == bpf_skb_pull_data ||\n\t    func == bpf_clone_redirect ||\n\t    func == bpf_l3_csum_replace ||\n\t    func == bpf_l4_csum_replace ||\n\t    func == bpf_xdp_adjust_head ||\n\t    func == bpf_xdp_adjust_meta ||\n\t    func == bpf_msg_pull_data)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic unsigned long bpf_skb_copy(void *dst_buff, const void *skb,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tvoid *ptr = skb_header_pointer(skb, off, len, dst_buff);\n\n\tif (unlikely(!ptr))\n\t\treturn len;\n\tif (ptr != dst_buff)\n\t\tmemcpy(dst_buff, ptr, len);\n\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_skb_event_output, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 skb_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(skb_size > skb->len))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, skb, skb_size,\n\t\t\t\tbpf_skb_copy);\n}\n\nstatic const struct bpf_func_proto bpf_skb_event_output_proto = {\n\t.func\t\t= bpf_skb_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nstatic unsigned short bpf_tunnel_key_af(u64 flags)\n{\n\treturn flags & BPF_F_TUNINFO_IPV6 ? AF_INET6 : AF_INET;\n}\n\nBPF_CALL_4(bpf_skb_get_tunnel_key, struct sk_buff *, skb, struct bpf_tunnel_key *, to,\n\t   u32, size, u64, flags)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tvoid *to_orig = to;\n\tint err;\n\n\tif (unlikely(!info || (flags & ~(BPF_F_TUNINFO_IPV6)))) {\n\t\terr = -EINVAL;\n\t\tgoto err_clear;\n\t}\n\tif (ip_tunnel_info_af(info) != bpf_tunnel_key_af(flags)) {\n\t\terr = -EPROTO;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\terr = -EINVAL;\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\t\tgoto set_compat;\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t/* Fixup deprecated structure layouts here, so we have\n\t\t\t * a common path later on.\n\t\t\t */\n\t\t\tif (ip_tunnel_info_af(info) != AF_INET)\n\t\t\t\tgoto err_clear;\nset_compat:\n\t\t\tto = (struct bpf_tunnel_key *)compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n\t}\n\n\tto->tunnel_id = be64_to_cpu(info->key.tun_id);\n\tto->tunnel_tos = info->key.tos;\n\tto->tunnel_ttl = info->key.ttl;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tmemcpy(to->remote_ipv6, &info->key.u.ipv6.src,\n\t\t       sizeof(to->remote_ipv6));\n\t\tto->tunnel_label = be32_to_cpu(info->key.label);\n\t} else {\n\t\tto->remote_ipv4 = be32_to_cpu(info->key.u.ipv4.src);\n\t}\n\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key)))\n\t\tmemcpy(to_orig, to, size);\n\n\treturn 0;\nerr_clear:\n\tmemset(to_orig, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_get_tunnel_opt, struct sk_buff *, skb, u8 *, to, u32, size)\n{\n\tconst struct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tint err;\n\n\tif (unlikely(!info ||\n\t\t     !(info->key.tun_flags & TUNNEL_OPTIONS_PRESENT))) {\n\t\terr = -ENOENT;\n\t\tgoto err_clear;\n\t}\n\tif (unlikely(size < info->options_len)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_clear;\n\t}\n\n\tip_tunnel_info_opts_get(to, info);\n\tif (size > info->options_len)\n\t\tmemset(to + info->options_len, 0, size - info->options_len);\n\n\treturn info->options_len;\nerr_clear:\n\tmemset(to, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_skb_get_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_get_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic struct metadata_dst __percpu *md_dst;\n\nBPF_CALL_4(bpf_skb_set_tunnel_key, struct sk_buff *, skb,\n\t   const struct bpf_tunnel_key *, from, u32, size, u64, flags)\n{\n\tstruct metadata_dst *md = this_cpu_ptr(md_dst);\n\tu8 compat[sizeof(struct bpf_tunnel_key)];\n\tstruct ip_tunnel_info *info;\n\n\tif (unlikely(flags & ~(BPF_F_TUNINFO_IPV6 | BPF_F_ZERO_CSUM_TX |\n\t\t\t       BPF_F_DONT_FRAGMENT | BPF_F_SEQ_NUMBER)))\n\t\treturn -EINVAL;\n\tif (unlikely(size != sizeof(struct bpf_tunnel_key))) {\n\t\tswitch (size) {\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_label):\n\t\tcase offsetof(struct bpf_tunnel_key, tunnel_ext):\n\t\tcase offsetof(struct bpf_tunnel_key, remote_ipv6[1]):\n\t\t\t/* Fixup deprecated structure layouts here, so we have\n\t\t\t * a common path later on.\n\t\t\t */\n\t\t\tmemcpy(compat, from, size);\n\t\t\tmemset(compat + size, 0, sizeof(compat) - size);\n\t\t\tfrom = (const struct bpf_tunnel_key *) compat;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tif (unlikely((!(flags & BPF_F_TUNINFO_IPV6) && from->tunnel_label) ||\n\t\t     from->tunnel_ext))\n\t\treturn -EINVAL;\n\n\tskb_dst_drop(skb);\n\tdst_hold((struct dst_entry *) md);\n\tskb_dst_set(skb, (struct dst_entry *) md);\n\n\tinfo = &md->u.tun_info;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->mode = IP_TUNNEL_INFO_TX;\n\n\tinfo->key.tun_flags = TUNNEL_KEY | TUNNEL_CSUM | TUNNEL_NOCACHE;\n\tif (flags & BPF_F_DONT_FRAGMENT)\n\t\tinfo->key.tun_flags |= TUNNEL_DONT_FRAGMENT;\n\tif (flags & BPF_F_ZERO_CSUM_TX)\n\t\tinfo->key.tun_flags &= ~TUNNEL_CSUM;\n\tif (flags & BPF_F_SEQ_NUMBER)\n\t\tinfo->key.tun_flags |= TUNNEL_SEQ;\n\n\tinfo->key.tun_id = cpu_to_be64(from->tunnel_id);\n\tinfo->key.tos = from->tunnel_tos;\n\tinfo->key.ttl = from->tunnel_ttl;\n\n\tif (flags & BPF_F_TUNINFO_IPV6) {\n\t\tinfo->mode |= IP_TUNNEL_INFO_IPV6;\n\t\tmemcpy(&info->key.u.ipv6.dst, from->remote_ipv6,\n\t\t       sizeof(from->remote_ipv6));\n\t\tinfo->key.label = cpu_to_be32(from->tunnel_label) &\n\t\t\t\t  IPV6_FLOWLABEL_MASK;\n\t} else {\n\t\tinfo->key.u.ipv4.dst = cpu_to_be32(from->remote_ipv4);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_key_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_key,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_skb_set_tunnel_opt, struct sk_buff *, skb,\n\t   const u8 *, from, u32, size)\n{\n\tstruct ip_tunnel_info *info = skb_tunnel_info(skb);\n\tconst struct metadata_dst *md = this_cpu_ptr(md_dst);\n\n\tif (unlikely(info != &md->u.tun_info || (size & (sizeof(u32) - 1))))\n\t\treturn -EINVAL;\n\tif (unlikely(size > IP_TUNNEL_OPTS_MAX))\n\t\treturn -ENOMEM;\n\n\tip_tunnel_info_opts_set(info, from, size);\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_skb_set_tunnel_opt_proto = {\n\t.func\t\t= bpf_skb_set_tunnel_opt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nbpf_get_skb_set_tunnel_proto(enum bpf_func_id which)\n{\n\tif (!md_dst) {\n\t\tstruct metadata_dst __percpu *tmp;\n\n\t\ttmp = metadata_dst_alloc_percpu(IP_TUNNEL_OPTS_MAX,\n\t\t\t\t\t\tMETADATA_IP_TUNNEL,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!tmp)\n\t\t\treturn NULL;\n\t\tif (cmpxchg(&md_dst, NULL, tmp))\n\t\t\tmetadata_dst_free_percpu(tmp);\n\t}\n\n\tswitch (which) {\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn &bpf_skb_set_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn &bpf_skb_set_tunnel_opt_proto;\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nBPF_CALL_3(bpf_skb_under_cgroup, struct sk_buff *, skb, struct bpf_map *, map,\n\t   u32, idx)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct cgroup *cgrp;\n\tstruct sock *sk;\n\n\tsk = skb_to_full_sk(skb);\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn -ENOENT;\n\tif (unlikely(idx >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tcgrp = READ_ONCE(array->ptrs[idx]);\n\tif (unlikely(!cgrp))\n\t\treturn -EAGAIN;\n\n\treturn sk_under_cgroup_hierarchy(sk, cgrp);\n}\n\nstatic const struct bpf_func_proto bpf_skb_under_cgroup_proto = {\n\t.func\t\t= bpf_skb_under_cgroup,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic unsigned long bpf_xdp_copy(void *dst_buff, const void *src_buff,\n\t\t\t\t  unsigned long off, unsigned long len)\n{\n\tmemcpy(dst_buff, src_buff + off, len);\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_xdp_event_output, struct xdp_buff *, xdp, struct bpf_map *, map,\n\t   u64, flags, void *, meta, u64, meta_size)\n{\n\tu64 xdp_size = (flags & BPF_F_CTXLEN_MASK) >> 32;\n\n\tif (unlikely(flags & ~(BPF_F_CTXLEN_MASK | BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (unlikely(xdp_size > (unsigned long)(xdp->data_end - xdp->data)))\n\t\treturn -EFAULT;\n\n\treturn bpf_event_output(map, flags, meta, meta_size, xdp->data,\n\t\t\t\txdp_size, bpf_xdp_copy);\n}\n\nstatic const struct bpf_func_proto bpf_xdp_event_output_proto = {\n\t.func\t\t= bpf_xdp_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_1(bpf_get_socket_cookie, struct sk_buff *, skb)\n{\n\treturn skb->sk ? sock_gen_cookie(skb->sk) : 0;\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_cookie_proto = {\n\t.func           = bpf_get_socket_cookie,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_socket_uid, struct sk_buff *, skb)\n{\n\tstruct sock *sk = sk_to_full_sk(skb->sk);\n\tkuid_t kuid;\n\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn overflowuid;\n\tkuid = sock_net_uid(sock_net(sk), sk);\n\treturn from_kuid_munged(sock_net(sk)->user_ns, kuid);\n}\n\nstatic const struct bpf_func_proto bpf_get_socket_uid_proto = {\n\t.func           = bpf_get_socket_uid,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_5(bpf_setsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\tint ret = 0;\n\tint val;\n\n\tif (!sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (level == SOL_SOCKET) {\n\t\tif (optlen != sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tval = *((int *)optval);\n\n\t\t/* Only some socketops are supported */\n\t\tswitch (optname) {\n\t\tcase SO_RCVBUF:\n\t\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t\tsk->sk_rcvbuf = max_t(int, val * 2, SOCK_MIN_RCVBUF);\n\t\t\tbreak;\n\t\tcase SO_SNDBUF:\n\t\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\t\tsk->sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF);\n\t\t\tbreak;\n\t\tcase SO_MAX_PACING_RATE:\n\t\t\tsk->sk_max_pacing_rate = val;\n\t\t\tsk->sk_pacing_rate = min(sk->sk_pacing_rate,\n\t\t\t\t\t\t sk->sk_max_pacing_rate);\n\t\t\tbreak;\n\t\tcase SO_PRIORITY:\n\t\t\tsk->sk_priority = val;\n\t\t\tbreak;\n\t\tcase SO_RCVLOWAT:\n\t\t\tif (val < 0)\n\t\t\t\tval = INT_MAX;\n\t\t\tsk->sk_rcvlowat = val ? : 1;\n\t\t\tbreak;\n\t\tcase SO_MARK:\n\t\t\tsk->sk_mark = val;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#ifdef CONFIG_INET\n\t} else if (level == SOL_IP) {\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET)\n\t\t\treturn -EINVAL;\n\n\t\tval = *((int *)optval);\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IP_TOS:\n\t\t\tif (val < -1 || val > 0xff) {\n\t\t\t\tret = -EINVAL;\n\t\t\t} else {\n\t\t\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\t\t\tif (val == -1)\n\t\t\t\t\tval = 0;\n\t\t\t\tinet->tos = val;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (level == SOL_IPV6) {\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET6)\n\t\t\treturn -EINVAL;\n\n\t\tval = *((int *)optval);\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IPV6_TCLASS:\n\t\t\tif (val < -1 || val > 0xff) {\n\t\t\t\tret = -EINVAL;\n\t\t\t} else {\n\t\t\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\t\t\tif (val == -1)\n\t\t\t\t\tval = 0;\n\t\t\t\tnp->tclass = val;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n#endif\n\t} else if (level == SOL_TCP &&\n\t\t   sk->sk_prot->setsockopt == tcp_setsockopt) {\n\t\tif (optname == TCP_CONGESTION) {\n\t\t\tchar name[TCP_CA_NAME_MAX];\n\t\t\tbool reinit = bpf_sock->op > BPF_SOCK_OPS_NEEDS_ECN;\n\n\t\t\tstrncpy(name, optval, min_t(long, optlen,\n\t\t\t\t\t\t    TCP_CA_NAME_MAX-1));\n\t\t\tname[TCP_CA_NAME_MAX-1] = 0;\n\t\t\tret = tcp_set_congestion_control(sk, name, false,\n\t\t\t\t\t\t\t reinit);\n\t\t} else {\n\t\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\t\tif (optlen != sizeof(int))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tval = *((int *)optval);\n\t\t\t/* Only some options are supported */\n\t\t\tswitch (optname) {\n\t\t\tcase TCP_BPF_IW:\n\t\t\t\tif (val <= 0 || tp->data_segs_out > 0)\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\telse\n\t\t\t\t\ttp->snd_cwnd = val;\n\t\t\t\tbreak;\n\t\t\tcase TCP_BPF_SNDCWND_CLAMP:\n\t\t\t\tif (val <= 0) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t} else {\n\t\t\t\t\ttp->snd_cwnd_clamp = val;\n\t\t\t\t\ttp->snd_ssthresh = val;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tret = -EINVAL;\n\t\t\t}\n\t\t}\n#endif\n\t} else {\n\t\tret = -EINVAL;\n\t}\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_setsockopt_proto = {\n\t.func\t\t= bpf_setsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_5(bpf_getsockopt, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, level, int, optname, char *, optval, int, optlen)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\n\tif (!sk_fullsock(sk))\n\t\tgoto err_clear;\n\n#ifdef CONFIG_INET\n\tif (level == SOL_TCP && sk->sk_prot->getsockopt == tcp_getsockopt) {\n\t\tif (optname == TCP_CONGESTION) {\n\t\t\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t\t\tif (!icsk->icsk_ca_ops || optlen <= 1)\n\t\t\t\tgoto err_clear;\n\t\t\tstrncpy(optval, icsk->icsk_ca_ops->name, optlen);\n\t\t\toptval[optlen - 1] = 0;\n\t\t} else {\n\t\t\tgoto err_clear;\n\t\t}\n\t} else if (level == SOL_IP) {\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET)\n\t\t\tgoto err_clear;\n\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IP_TOS:\n\t\t\t*((int *)optval) = (int)inet->tos;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (level == SOL_IPV6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tif (optlen != sizeof(int) || sk->sk_family != AF_INET6)\n\t\t\tgoto err_clear;\n\n\t\t/* Only some options are supported */\n\t\tswitch (optname) {\n\t\tcase IPV6_TCLASS:\n\t\t\t*((int *)optval) = (int)np->tclass;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto err_clear;\n\t\t}\n#endif\n\t} else {\n\t\tgoto err_clear;\n\t}\n\treturn 0;\n#endif\nerr_clear:\n\tmemset(optval, 0, optlen);\n\treturn -EINVAL;\n}\n\nstatic const struct bpf_func_proto bpf_getsockopt_proto = {\n\t.func\t\t= bpf_getsockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg5_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_2(bpf_sock_ops_cb_flags_set, struct bpf_sock_ops_kern *, bpf_sock,\n\t   int, argval)\n{\n\tstruct sock *sk = bpf_sock->sk;\n\tint val = argval & BPF_SOCK_OPS_ALL_CB_FLAGS;\n\n\tif (!IS_ENABLED(CONFIG_INET) || !sk_fullsock(sk))\n\t\treturn -EINVAL;\n\n\tif (val)\n\t\ttcp_sk(sk)->bpf_sock_ops_cb_flags = val;\n\n\treturn argval & (~BPF_SOCK_OPS_ALL_CB_FLAGS);\n}\n\nstatic const struct bpf_func_proto bpf_sock_ops_cb_flags_set_proto = {\n\t.func\t\t= bpf_sock_ops_cb_flags_set,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nconst struct ipv6_bpf_stub *ipv6_bpf_stub __read_mostly;\nEXPORT_SYMBOL_GPL(ipv6_bpf_stub);\n\nBPF_CALL_3(bpf_bind, struct bpf_sock_addr_kern *, ctx, struct sockaddr *, addr,\n\t   int, addr_len)\n{\n#ifdef CONFIG_INET\n\tstruct sock *sk = ctx->sk;\n\tint err;\n\n\t/* Binding to port can be expensive so it's prohibited in the helper.\n\t * Only binding to IP is supported.\n\t */\n\terr = -EINVAL;\n\tif (addr->sa_family == AF_INET) {\n\t\tif (addr_len < sizeof(struct sockaddr_in))\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in *)addr)->sin_port != htons(0))\n\t\t\treturn err;\n\t\treturn __inet_bind(sk, addr, addr_len, true, false);\n#if IS_ENABLED(CONFIG_IPV6)\n\t} else if (addr->sa_family == AF_INET6) {\n\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\treturn err;\n\t\tif (((struct sockaddr_in6 *)addr)->sin6_port != htons(0))\n\t\t\treturn err;\n\t\t/* ipv6_bpf_stub cannot be NULL, since it's called from\n\t\t * bpf_cgroup_inet6_connect hook and ipv6 is already loaded\n\t\t */\n\t\treturn ipv6_bpf_stub->inet6_bind(sk, addr, addr_len, true, false);\n#endif /* CONFIG_IPV6 */\n\t}\n#endif /* CONFIG_INET */\n\n\treturn -EAFNOSUPPORT;\n}\n\nstatic const struct bpf_func_proto bpf_bind_proto = {\n\t.func\t\t= bpf_bind,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nbpf_base_func_proto(enum bpf_func_id func_id)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_map_lookup_elem:\n\t\treturn &bpf_map_lookup_elem_proto;\n\tcase BPF_FUNC_map_update_elem:\n\t\treturn &bpf_map_update_elem_proto;\n\tcase BPF_FUNC_map_delete_elem:\n\t\treturn &bpf_map_delete_elem_proto;\n\tcase BPF_FUNC_get_prandom_u32:\n\t\treturn &bpf_get_prandom_u32_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_raw_smp_processor_id_proto;\n\tcase BPF_FUNC_get_numa_node_id:\n\t\treturn &bpf_get_numa_node_id_proto;\n\tcase BPF_FUNC_tail_call:\n\t\treturn &bpf_tail_call_proto;\n\tcase BPF_FUNC_ktime_get_ns:\n\t\treturn &bpf_ktime_get_ns_proto;\n\tcase BPF_FUNC_trace_printk:\n\t\tif (capable(CAP_SYS_ADMIN))\n\t\t\treturn bpf_get_trace_printk_proto();\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\t/* inet and inet6 sockets are created in a process\n\t * context so there is always a valid uid/gid\n\t */\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_addr_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\t/* inet and inet6 sockets are created in a process\n\t * context so there is always a valid uid/gid\n\t */\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tcase BPF_FUNC_bind:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\t\treturn &bpf_bind_proto;\n\t\tdefault:\n\t\t\treturn NULL;\n\t\t}\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_filter_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\ntc_cls_act_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_skb_vlan_push:\n\t\treturn &bpf_skb_vlan_push_proto;\n\tcase BPF_FUNC_skb_vlan_pop:\n\t\treturn &bpf_skb_vlan_pop_proto;\n\tcase BPF_FUNC_skb_change_proto:\n\t\treturn &bpf_skb_change_proto_proto;\n\tcase BPF_FUNC_skb_change_type:\n\t\treturn &bpf_skb_change_type_proto;\n\tcase BPF_FUNC_skb_adjust_room:\n\t\treturn &bpf_skb_adjust_room_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tcase BPF_FUNC_set_hash:\n\t\treturn &bpf_set_hash_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nxdp_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_xdp_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_xdp_adjust_head:\n\t\treturn &bpf_xdp_adjust_head_proto;\n\tcase BPF_FUNC_xdp_adjust_meta:\n\t\treturn &bpf_xdp_adjust_meta_proto;\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_xdp_redirect_proto;\n\tcase BPF_FUNC_redirect_map:\n\t\treturn &bpf_xdp_redirect_map_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_inout_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_csum_diff:\n\t\treturn &bpf_csum_diff_proto;\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_proto;\n\tcase BPF_FUNC_get_route_realm:\n\t\treturn &bpf_get_route_realm_proto;\n\tcase BPF_FUNC_get_hash_recalc:\n\t\treturn &bpf_get_hash_recalc_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_skb_event_output_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\treturn &bpf_skb_under_cgroup_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsock_ops_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_setsockopt:\n\t\treturn &bpf_setsockopt_proto;\n\tcase BPF_FUNC_getsockopt:\n\t\treturn &bpf_getsockopt_proto;\n\tcase BPF_FUNC_sock_ops_cb_flags_set:\n\t\treturn &bpf_sock_ops_cb_flags_set_proto;\n\tcase BPF_FUNC_sock_map_update:\n\t\treturn &bpf_sock_map_update_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_msg_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_msg_redirect_map:\n\t\treturn &bpf_msg_redirect_map_proto;\n\tcase BPF_FUNC_msg_apply_bytes:\n\t\treturn &bpf_msg_apply_bytes_proto;\n\tcase BPF_FUNC_msg_cork_bytes:\n\t\treturn &bpf_msg_cork_bytes_proto;\n\tcase BPF_FUNC_msg_pull_data:\n\t\treturn &bpf_msg_pull_data_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nsk_skb_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_skb_load_bytes:\n\t\treturn &bpf_skb_load_bytes_proto;\n\tcase BPF_FUNC_skb_pull_data:\n\t\treturn &bpf_skb_pull_data_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_cookie_proto;\n\tcase BPF_FUNC_get_socket_uid:\n\t\treturn &bpf_get_socket_uid_proto;\n\tcase BPF_FUNC_sk_redirect_map:\n\t\treturn &bpf_sk_redirect_map_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nlwt_xmit_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_skb_get_tunnel_key:\n\t\treturn &bpf_skb_get_tunnel_key_proto;\n\tcase BPF_FUNC_skb_set_tunnel_key:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_skb_get_tunnel_opt:\n\t\treturn &bpf_skb_get_tunnel_opt_proto;\n\tcase BPF_FUNC_skb_set_tunnel_opt:\n\t\treturn bpf_get_skb_set_tunnel_proto(func_id);\n\tcase BPF_FUNC_redirect:\n\t\treturn &bpf_redirect_proto;\n\tcase BPF_FUNC_clone_redirect:\n\t\treturn &bpf_clone_redirect_proto;\n\tcase BPF_FUNC_skb_change_tail:\n\t\treturn &bpf_skb_change_tail_proto;\n\tcase BPF_FUNC_skb_change_head:\n\t\treturn &bpf_skb_change_head_proto;\n\tcase BPF_FUNC_skb_store_bytes:\n\t\treturn &bpf_skb_store_bytes_proto;\n\tcase BPF_FUNC_csum_update:\n\t\treturn &bpf_csum_update_proto;\n\tcase BPF_FUNC_l3_csum_replace:\n\t\treturn &bpf_l3_csum_replace_proto;\n\tcase BPF_FUNC_l4_csum_replace:\n\t\treturn &bpf_l4_csum_replace_proto;\n\tcase BPF_FUNC_set_hash_invalid:\n\t\treturn &bpf_set_hash_invalid_proto;\n\tdefault:\n\t\treturn lwt_inout_func_proto(func_id, prog);\n\t}\n}\n\nstatic bool bpf_skb_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t    const struct bpf_prog *prog,\n\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct __sk_buff))\n\t\treturn false;\n\n\t/* The verifier guarantees that size > 0. */\n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\tif (off + size > offsetofend(struct __sk_buff, cb[4]))\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip6[0], remote_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip6[0], local_ip6[3]):\n\tcase bpf_ctx_range_till(struct __sk_buff, remote_ip4, remote_ip4):\n\tcase bpf_ctx_range_till(struct __sk_buff, local_ip4, local_ip4):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\t/* Only narrow read access allowed for now. */\n\t\tif (type == BPF_WRITE) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sk_filter_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool lwt_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\n\n/* Attach type specific accesses */\nstatic bool __sock_filter_check_attach_type(int off,\n\t\t\t\t\t    enum bpf_access_type access_type,\n\t\t\t\t\t    enum bpf_attach_type attach_type)\n{\n\tswitch (off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\tcase offsetof(struct bpf_sock, mark):\n\tcase offsetof(struct bpf_sock, priority):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\t\t\tgoto full_access;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\tcase bpf_ctx_range(struct bpf_sock, src_port):\n\t\tswitch (attach_type) {\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\tgoto read_only;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\nread_only:\n\treturn access_type == BPF_READ;\nfull_access:\n\treturn true;\n}\n\nstatic bool __sock_filter_check_size(int off, int size,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock, src_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\t}\n\n\treturn size == size_default;\n}\n\nstatic bool sock_filter_is_valid_access(int off, int size,\n\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= sizeof(struct bpf_sock))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (!__sock_filter_check_attach_type(off, type,\n\t\t\t\t\t     prog->expected_attach_type))\n\t\treturn false;\n\tif (!__sock_filter_check_size(off, size, info))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int bpf_unclone_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t\tconst struct bpf_prog *prog, int drop_verdict)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tif (!direct_write)\n\t\treturn 0;\n\n\t/* if (!skb->cloned)\n\t *       goto start;\n\t *\n\t * (Fast-path, otherwise approximation that we might be\n\t *  a clone, do the rest in helper.)\n\t */\n\t*insn++ = BPF_LDX_MEM(BPF_B, BPF_REG_6, BPF_REG_1, CLONED_OFFSET());\n\t*insn++ = BPF_ALU32_IMM(BPF_AND, BPF_REG_6, CLONED_MASK);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_6, 0, 7);\n\n\t/* ret = bpf_skb_pull_data(skb, 0); */\n\t*insn++ = BPF_MOV64_REG(BPF_REG_6, BPF_REG_1);\n\t*insn++ = BPF_ALU64_REG(BPF_XOR, BPF_REG_2, BPF_REG_2);\n\t*insn++ = BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0,\n\t\t\t       BPF_FUNC_skb_pull_data);\n\t/* if (!ret)\n\t *      goto restore;\n\t * return TC_ACT_SHOT;\n\t */\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 2);\n\t*insn++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_0, drop_verdict);\n\t*insn++ = BPF_EXIT_INSN();\n\n\t/* restore: */\n\t*insn++ = BPF_MOV64_REG(BPF_REG_1, BPF_REG_6);\n\t/* start: */\n\t*insn++ = prog->insnsi[0];\n\n\treturn insn - insn_buf;\n}\n\nstatic int tc_cls_act_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t       const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, TC_ACT_SHOT);\n}\n\nstatic bool tc_cls_act_is_valid_access(int off, int size,\n\t\t\t\t       enum bpf_access_type type,\n\t\t\t\t       const struct bpf_prog *prog,\n\t\t\t\t       struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\t\tcase bpf_ctx_range_till(struct __sk_buff, cb[0], cb[4]):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct __sk_buff, family, local_port):\n\t\treturn false;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool __is_valid_xdp_access(int off, int size)\n{\n\tif (off < 0 || off >= sizeof(struct xdp_md))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (size != sizeof(__u32))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool xdp_is_valid_access(int off, int size,\n\t\t\t\tenum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct xdp_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\tinfo->reg_type = PTR_TO_PACKET_META;\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn __is_valid_xdp_access(off, size);\n}\n\nvoid bpf_warn_invalid_xdp_action(u32 act)\n{\n\tconst u32 act_max = XDP_REDIRECT;\n\n\tWARN_ONCE(1, \"%s XDP return value %u, expect packet loss!\\n\",\n\t\t  act > act_max ? \"Illegal\" : \"Driver unsupported\",\n\t\t  act);\n}\nEXPORT_SYMBOL_GPL(bpf_warn_invalid_xdp_action);\n\nstatic bool sock_addr_is_valid_access(int off, int size,\n\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_addr))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\n\t/* Disallow access to IPv6 fields from IPv4 contex and vise\n\t * versa.\n\t */\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_ip4):\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\t/* Only narrow read access allowed for now. */\n\t\tif (type == BPF_READ) {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase bpf_ctx_range(struct bpf_sock_addr, user_port):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\tif (type == BPF_READ) {\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic bool sock_ops_is_valid_access(int off, int size,\n\t\t\t\t     enum bpf_access_type type,\n\t\t\t\t     const struct bpf_prog *prog,\n\t\t\t\t     struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sock_ops))\n\t\treturn false;\n\n\t/* The verifier guarantees that size > 0. */\n\tif (off % size != 0)\n\t\treturn false;\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase offsetof(struct bpf_sock_ops, reply):\n\t\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range_till(struct bpf_sock_ops, bytes_received,\n\t\t\t\t\tbytes_acked):\n\t\t\tif (size != sizeof(__u64))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int sk_skb_prologue(struct bpf_insn *insn_buf, bool direct_write,\n\t\t\t   const struct bpf_prog *prog)\n{\n\treturn bpf_unclone_prologue(insn_buf, direct_write, prog, SK_DROP);\n}\n\nstatic bool sk_skb_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, tc_classid):\n\tcase bpf_ctx_range(struct __sk_buff, data_meta):\n\t\treturn false;\n\t}\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase bpf_ctx_range(struct __sk_buff, tc_index):\n\t\tcase bpf_ctx_range(struct __sk_buff, priority):\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct __sk_buff, mark):\n\t\treturn false;\n\tcase bpf_ctx_range(struct __sk_buff, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase bpf_ctx_range(struct __sk_buff, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\treturn bpf_skb_is_valid_access(off, size, type, prog, info);\n}\n\nstatic bool sk_msg_is_valid_access(int off, int size,\n\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\t}\n\n\tif (off < 0 || off >= sizeof(struct sk_msg_md))\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\tif (size != sizeof(__u64))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic u32 bpf_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, len):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, len, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, protocol, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_proto):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_proto, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, priority):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, priority, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, skb_iif, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 1);\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, hash):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, hash, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, mark):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, mark, 4,\n\t\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, pkt_type):\n\t\t*target_size = 1;\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->src_reg,\n\t\t\t\t      PKT_TYPE_OFFSET());\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, PKT_TYPE_MAX);\n#ifdef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, 5);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, queue_mapping):\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, queue_mapping, 2,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, vlan_present):\n\tcase offsetof(struct __sk_buff, vlan_tci):\n\t\tBUILD_BUG_ON(VLAN_TAG_PRESENT != 0x1000);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, vlan_tci, 2,\n\t\t\t\t\t\t     target_size));\n\t\tif (si->off == offsetof(struct __sk_buff, vlan_tci)) {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg,\n\t\t\t\t\t\t~VLAN_TAG_PRESENT);\n\t\t} else {\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, 12);\n\t\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, 1);\n\t\t}\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, cb[0]) ...\n\t     offsetofend(struct __sk_buff, cb[4]) - 1:\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, data) < 20);\n\t\tBUILD_BUG_ON((offsetof(struct sk_buff, cb) +\n\t\t\t      offsetof(struct qdisc_skb_cb, data)) %\n\t\t\t     sizeof(__u64));\n\n\t\tprog->cb_access = 1;\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, cb[0]);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, data);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_SIZE(si->code), si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_classid):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct qdisc_skb_cb, tc_classid) != 2);\n\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, tc_classid);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct qdisc_skb_cb, tc_classid);\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_H, si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg,\n\t\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, data));\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_meta):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_meta);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_meta);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, data_end):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_end);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct bpf_skb_data_end, data_end);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, tc_index):\n#ifdef CONFIG_NET_SCHED\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t     target_size));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t\t      bpf_target_off(struct sk_buff, tc_index, 2,\n\t\t\t\t\t\t\t     target_size));\n#else\n\t\t*target_size = 2;\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_MOV64_REG(si->dst_reg, si->dst_reg);\n\t\telse\n\t\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, napi_id):\n#if defined(CONFIG_NET_RX_BUSY_POLL)\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      bpf_target_off(struct sk_buff, napi_id, 4,\n\t\t\t\t\t\t     target_size));\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, si->dst_reg, MIN_NAPI_ID, 1);\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#else\n\t\t*target_size = 4;\n\t\t*insn++ = BPF_MOV64_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_family,\n\t\t\t\t\t\t     2, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_daddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_rcv_saddr,\n\t\t\t\t\t\t     4, target_size));\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, remote_ip6[0]) ...\n\t     offsetof(struct __sk_buff, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, remote_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\tcase offsetof(struct __sk_buff, local_ip6[0]) ...\n\t     offsetof(struct __sk_buff, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct __sk_buff, local_ip6[0]);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, remote_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_dport,\n\t\t\t\t\t\t     2, target_size));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct __sk_buff, local_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct sock_common,\n\t\t\t\t\t\t     skc_num, 2, target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sock_filter_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock, bound_dev_if):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_bound_dev_if) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_bound_dev_if));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_bound_dev_if));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, mark):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_mark) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_mark));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_mark));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, priority):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_priority) != 4);\n\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\toffsetof(struct sock, sk_priority));\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_priority));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock, sk_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, sk_family));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, type):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, __sk_flags_offset));\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_TYPE_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_TYPE_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, protocol):\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sock, __sk_flags_offset));\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_PROTO_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_PROTO_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_ip4):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_rcv_saddr,\n\t\t\t\t       FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t\t    skc_rcv_saddr),\n\t\t\t\t       target_size));\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock, src_ip6[0], src_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock, src_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(\n\t\t\t\tstruct sock_common,\n\t\t\t\tskc_v6_rcv_saddr.s6_addr32[0],\n\t\t\t\tFIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t     skc_v6_rcv_saddr.s6_addr32[0]),\n\t\t\t\ttarget_size) + off);\n#else\n\t\t(void)off;\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock, src_port):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_FIELD_SIZEOF(struct sock_common, skc_num),\n\t\t\tsi->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct sock_common, skc_num,\n\t\t\t\t       FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t\t    skc_num),\n\t\t\t\t       target_size));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 tc_cls_act_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_insn *si,\n\t\t\t\t\t struct bpf_insn *insn_buf,\n\t\t\t\t\t struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_buff, dev),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_buff, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct net_device, ifindex, 4,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 xdp_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct xdp_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_meta):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_meta),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_meta));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, data_end));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, ingress_ifindex):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_rxq_info, dev),\n\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info, dev));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct net_device, ifindex));\n\t\tbreak;\n\tcase offsetof(struct xdp_md, rx_queue_index):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct xdp_buff, rxq),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct xdp_buff, rxq));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct xdp_rxq_info,\n\t\t\t\t\t       queue_index));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\n/* SOCK_ADDR_LOAD_NESTED_FIELD() loads Nested Field S.F.NF where S is type of\n * context Structure, F is Field in context structure that contains a pointer\n * to Nested Structure of type NS that has the field NF.\n *\n * SIZE encodes the load size (BPF_B, BPF_H, etc). It's up to caller to make\n * sure that SIZE is not greater than actual size of S.F.NF.\n *\n * If offset OFF is provided, the load happens from that offset relative to\n * offset of NF.\n */\n#define SOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF)\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), si->dst_reg,     \\\n\t\t\t\t      si->src_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_LDX_MEM(\t\t\t\t\t       \\\n\t\t\tSIZE, si->dst_reg, si->dst_reg,\t\t\t       \\\n\t\t\tbpf_target_off(NS, NF, FIELD_SIZEOF(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t+ OFF);\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_NESTED_FIELD(S, NS, F, NF)\t\t\t       \\\n\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(S, NS, F, NF,\t\t       \\\n\t\t\t\t\t     BPF_FIELD_SIZEOF(NS, NF), 0)\n\n/* SOCK_ADDR_STORE_NESTED_FIELD_OFF() has semantic similar to\n * SOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF() but for store operation.\n *\n * It doesn't support SIZE argument though since narrow stores are not\n * supported for now.\n *\n * In addition it uses Temporary Field TF (member of struct S) as the 3rd\n * \"register\" since two registers available in convert_ctx_access are not\n * enough: we can't override neither SRC, since it contains value to store, nor\n * DST since it contains pointer to context that may be used by later\n * instructions. But we need a temporary place to save pointer to nested\n * structure whose field we want to store to.\n */\n#define SOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, OFF, TF)\t\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tint tmp_reg = BPF_REG_9;\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\tif (si->src_reg == tmp_reg || si->dst_reg == tmp_reg)\t       \\\n\t\t\t--tmp_reg;\t\t\t\t\t       \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, tmp_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(S, F), tmp_reg,\t       \\\n\t\t\t\t      si->dst_reg, offsetof(S, F));\t       \\\n\t\t*insn++ = BPF_STX_MEM(\t\t\t\t\t       \\\n\t\t\tBPF_FIELD_SIZEOF(NS, NF), tmp_reg, si->src_reg,\t       \\\n\t\t\tbpf_target_off(NS, NF, FIELD_SIZEOF(NS, NF),\t       \\\n\t\t\t\t       target_size)\t\t\t       \\\n\t\t\t\t+ OFF);\t\t\t\t\t       \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, tmp_reg, si->dst_reg,\t       \\\n\t\t\t\t      offsetof(S, TF));\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(S, NS, F, NF, SIZE, OFF, \\\n\t\t\t\t\t\t      TF)\t\t       \\\n\tdo {\t\t\t\t\t\t\t\t       \\\n\t\tif (type == BPF_WRITE) {\t\t\t\t       \\\n\t\t\tSOCK_ADDR_STORE_NESTED_FIELD_OFF(S, NS, F, NF, OFF,    \\\n\t\t\t\t\t\t\t TF);\t\t       \\\n\t\t} else {\t\t\t\t\t\t       \\\n\t\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\t\t       \\\n\t\t\t\tS, NS, F, NF, SIZE, OFF);  \\\n\t\t}\t\t\t\t\t\t\t       \\\n\t} while (0)\n\n#define SOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD(S, NS, F, NF, TF)\t\t       \\\n\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\t\t\t       \\\n\t\tS, NS, F, NF, BPF_FIELD_SIZEOF(NS, NF), 0, TF)\n\nstatic u32 sock_addr_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\t\tstruct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_addr, user_family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sockaddr, uaddr, sa_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_ip4):\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in, uaddr,\n\t\t\tsin_addr, BPF_SIZE(si->code), 0, tmp_reg);\n\t\tbreak;\n\n\tcase bpf_ctx_range_till(struct bpf_sock_addr, user_ip6[0], user_ip6[3]):\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_addr, user_ip6[0]);\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sockaddr_in6, uaddr,\n\t\t\tsin6_addr.s6_addr32[0], BPF_SIZE(si->code), off,\n\t\t\ttmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, user_port):\n\t\t/* To get port we need to know sa_family first and then treat\n\t\t * sockaddr as either sockaddr_in or sockaddr_in6.\n\t\t * Though we can simplify since port field has same offset and\n\t\t * size in both structures.\n\t\t * Here we check this invariant and use just one of the\n\t\t * structures if it's true.\n\t\t */\n\t\tBUILD_BUG_ON(offsetof(struct sockaddr_in, sin_port) !=\n\t\t\t     offsetof(struct sockaddr_in6, sin6_port));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sockaddr_in, sin_port) !=\n\t\t\t     FIELD_SIZEOF(struct sockaddr_in6, sin6_port));\n\t\tSOCK_ADDR_LOAD_OR_STORE_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t\t     struct sockaddr_in6, uaddr,\n\t\t\t\t\t\t     sin6_port, tmp_reg);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, family):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD(struct bpf_sock_addr_kern,\n\t\t\t\t\t    struct sock, sk, sk_family);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, type):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sock, sk,\n\t\t\t__sk_flags_offset, BPF_W, 0);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_TYPE_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg, SK_FL_TYPE_SHIFT);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_addr, protocol):\n\t\tSOCK_ADDR_LOAD_NESTED_FIELD_SIZE_OFF(\n\t\t\tstruct bpf_sock_addr_kern, struct sock, sk,\n\t\t\t__sk_flags_offset, BPF_W, 0);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, si->dst_reg, SK_FL_PROTO_MASK);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_RSH, si->dst_reg,\n\t\t\t\t\tSK_FL_PROTO_SHIFT);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sock_ops_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t       const struct bpf_insn *si,\n\t\t\t\t       struct bpf_insn *insn_buf,\n\t\t\t\t       struct bpf_prog *prog,\n\t\t\t\t       u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sock_ops, op) ...\n\t     offsetof(struct bpf_sock_ops, replylong[3]):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, op) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, op));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, reply) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, reply));\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct bpf_sock_ops, replylong) !=\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops_kern, replylong));\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, op);\n\t\toff += offsetof(struct bpf_sock_ops_kern, op);\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = BPF_STX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      off);\n\t\telse\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->src_reg,\n\t\t\t\t\t      off);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, family):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_family) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_family));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_daddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_daddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip4):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_rcv_saddr) != 4);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t      struct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_rcv_saddr));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, remote_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_daddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, remote_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_daddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_ip6[0]) ...\n\t     offsetof(struct bpf_sock_ops, local_ip6[3]):\n#if IS_ENABLED(CONFIG_IPV6)\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common,\n\t\t\t\t\t  skc_v6_rcv_saddr.s6_addr32[0]) != 4);\n\n\t\toff = si->off;\n\t\toff -= offsetof(struct bpf_sock_ops, local_ip6[0]);\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common,\n\t\t\t\t\t       skc_v6_rcv_saddr.s6_addr32[0]) +\n\t\t\t\t      off);\n#else\n\t\t*insn++ = BPF_MOV32_IMM(si->dst_reg, 0);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, remote_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_dport) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_dport));\n#ifndef __BIG_ENDIAN_BITFIELD\n\t\t*insn++ = BPF_ALU32_IMM(BPF_LSH, si->dst_reg, 16);\n#endif\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, local_port):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_num) != 2);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_H, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_num));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, is_fullsock):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,\n\t\t\t\t\t\tis_fullsock),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,\n\t\t\t\t\t       is_fullsock));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, state):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct sock_common, skc_state) != 1);\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_B, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct sock_common, skc_state));\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rtt_min):\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(struct tcp_sock, rtt_min) !=\n\t\t\t     sizeof(struct minmax));\n\t\tBUILD_BUG_ON(sizeof(struct minmax) <\n\t\t\t     sizeof(struct minmax_sample));\n\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\n\t\t*insn++ = BPF_LDX_MEM(BPF_W, si->dst_reg, si->dst_reg,\n\t\t\t\t      offsetof(struct tcp_sock, rtt_min) +\n\t\t\t\t      FIELD_SIZEOF(struct minmax_sample, t));\n\t\tbreak;\n\n/* Helper macro for adding read access to tcp_sock or sock fields. */\n#define SOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, si->dst_reg, 0, 2);\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      si->dst_reg, si->src_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(OBJ,\t\t      \\\n\t\t\t\t\t\t       OBJ_FIELD),\t      \\\n\t\t\t\t      si->dst_reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(OBJ, OBJ_FIELD));\t      \\\n\t} while (0)\n\n/* Helper macro for adding write access to tcp_sock or sock fields.\n * The macro is called with two registers, dst_reg which contains a pointer\n * to ctx (context) and src_reg which contains the value that should be\n * stored. However, we need an additional register since we cannot overwrite\n * dst_reg because it may be used later in the program.\n * Instead we \"borrow\" one of the other register. We first save its value\n * into a new (temp) field in bpf_sock_ops_kern, use it, and then restore\n * it at the end of the macro.\n */\n#define SOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ)\t\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tint reg = BPF_REG_9;\t\t\t\t\t      \\\n\t\tBUILD_BUG_ON(FIELD_SIZEOF(OBJ, OBJ_FIELD) >\t\t      \\\n\t\t\t     FIELD_SIZEOF(struct bpf_sock_ops, BPF_FIELD));   \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\tif (si->dst_reg == reg || si->src_reg == reg)\t\t      \\\n\t\t\treg--;\t\t\t\t\t\t      \\\n\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern,     \\\n\t\t\t\t\t\tis_fullsock),\t\t      \\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       is_fullsock));\t\t      \\\n\t\t*insn++ = BPF_JMP_IMM(BPF_JEQ, reg, 0, 2);\t\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(\t\t\t      \\\n\t\t\t\t\t\tstruct bpf_sock_ops_kern, sk),\\\n\t\t\t\t      reg, si->dst_reg,\t\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern, sk));\\\n\t\t*insn++ = BPF_STX_MEM(BPF_FIELD_SIZEOF(OBJ, OBJ_FIELD),\t      \\\n\t\t\t\t      reg, si->src_reg,\t\t\t      \\\n\t\t\t\t      offsetof(OBJ, OBJ_FIELD));\t      \\\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, reg, si->dst_reg,\t\t      \\\n\t\t\t\t      offsetof(struct bpf_sock_ops_kern,      \\\n\t\t\t\t\t       temp));\t\t\t      \\\n\t} while (0)\n\n#define SOCK_OPS_GET_OR_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ, TYPE)\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tif (TYPE == BPF_WRITE)\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_SET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t\telse\t\t\t\t\t\t\t      \\\n\t\t\tSOCK_OPS_GET_FIELD(BPF_FIELD, OBJ_FIELD, OBJ);\t      \\\n\t} while (0)\n\n\tcase offsetof(struct bpf_sock_ops, snd_cwnd):\n\t\tSOCK_OPS_GET_FIELD(snd_cwnd, snd_cwnd, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, srtt_us):\n\t\tSOCK_OPS_GET_FIELD(srtt_us, srtt_us, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bpf_sock_ops_cb_flags):\n\t\tSOCK_OPS_GET_FIELD(bpf_sock_ops_cb_flags, bpf_sock_ops_cb_flags,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_ssthresh):\n\t\tSOCK_OPS_GET_FIELD(snd_ssthresh, snd_ssthresh, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rcv_nxt):\n\t\tSOCK_OPS_GET_FIELD(rcv_nxt, rcv_nxt, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_nxt):\n\t\tSOCK_OPS_GET_FIELD(snd_nxt, snd_nxt, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, snd_una):\n\t\tSOCK_OPS_GET_FIELD(snd_una, snd_una, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, mss_cache):\n\t\tSOCK_OPS_GET_FIELD(mss_cache, mss_cache, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, ecn_flags):\n\t\tSOCK_OPS_GET_FIELD(ecn_flags, ecn_flags, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rate_delivered):\n\t\tSOCK_OPS_GET_FIELD(rate_delivered, rate_delivered,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, rate_interval_us):\n\t\tSOCK_OPS_GET_FIELD(rate_interval_us, rate_interval_us,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, packets_out):\n\t\tSOCK_OPS_GET_FIELD(packets_out, packets_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, retrans_out):\n\t\tSOCK_OPS_GET_FIELD(retrans_out, retrans_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, total_retrans):\n\t\tSOCK_OPS_GET_FIELD(total_retrans, total_retrans,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, segs_in):\n\t\tSOCK_OPS_GET_FIELD(segs_in, segs_in, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, data_segs_in):\n\t\tSOCK_OPS_GET_FIELD(data_segs_in, data_segs_in, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, segs_out):\n\t\tSOCK_OPS_GET_FIELD(segs_out, segs_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, data_segs_out):\n\t\tSOCK_OPS_GET_FIELD(data_segs_out, data_segs_out,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, lost_out):\n\t\tSOCK_OPS_GET_FIELD(lost_out, lost_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, sacked_out):\n\t\tSOCK_OPS_GET_FIELD(sacked_out, sacked_out, struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, sk_txhash):\n\t\tSOCK_OPS_GET_OR_SET_FIELD(sk_txhash, sk_txhash,\n\t\t\t\t\t  struct sock, type);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bytes_received):\n\t\tSOCK_OPS_GET_FIELD(bytes_received, bytes_received,\n\t\t\t\t   struct tcp_sock);\n\t\tbreak;\n\n\tcase offsetof(struct bpf_sock_ops, bytes_acked):\n\t\tSOCK_OPS_GET_FIELD(bytes_acked, bytes_acked, struct tcp_sock);\n\t\tbreak;\n\n\t}\n\treturn insn - insn_buf;\n}\n\nstatic u32 sk_skb_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tint off;\n\n\tswitch (si->off) {\n\tcase offsetof(struct __sk_buff, data_end):\n\t\toff  = si->off;\n\t\toff -= offsetof(struct __sk_buff, data_end);\n\t\toff += offsetof(struct sk_buff, cb);\n\t\toff += offsetof(struct tcp_skb_cb, bpf.data_end);\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(void *), si->dst_reg,\n\t\t\t\t      si->src_reg, off);\n\t\tbreak;\n\tdefault:\n\t\treturn bpf_convert_ctx_access(type, si, insn_buf, prog,\n\t\t\t\t\t      target_size);\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic u32 sk_msg_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct sk_msg_md, data):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg_buff, data),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg_buff, data));\n\t\tbreak;\n\tcase offsetof(struct sk_msg_md, data_end):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct sk_msg_buff, data_end),\n\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct sk_msg_buff, data_end));\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops sk_filter_verifier_ops = {\n\t.get_func_proto\t\t= sk_filter_func_proto,\n\t.is_valid_access\t= sk_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sk_filter_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops tc_cls_act_verifier_ops = {\n\t.get_func_proto\t\t= tc_cls_act_func_proto,\n\t.is_valid_access\t= tc_cls_act_is_valid_access,\n\t.convert_ctx_access\t= tc_cls_act_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n};\n\nconst struct bpf_prog_ops tc_cls_act_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops xdp_verifier_ops = {\n\t.get_func_proto\t\t= xdp_func_proto,\n\t.is_valid_access\t= xdp_is_valid_access,\n\t.convert_ctx_access\t= xdp_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops xdp_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_xdp,\n};\n\nconst struct bpf_verifier_ops cg_skb_verifier_ops = {\n\t.get_func_proto\t\t= sk_filter_func_proto,\n\t.is_valid_access\t= sk_filter_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_skb_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_inout_verifier_ops = {\n\t.get_func_proto\t\t= lwt_inout_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops lwt_inout_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops lwt_xmit_verifier_ops = {\n\t.get_func_proto\t\t= lwt_xmit_func_proto,\n\t.is_valid_access\t= lwt_is_valid_access,\n\t.convert_ctx_access\t= bpf_convert_ctx_access,\n\t.gen_prologue\t\t= tc_cls_act_prologue,\n};\n\nconst struct bpf_prog_ops lwt_xmit_prog_ops = {\n\t.test_run\t\t= bpf_prog_test_run_skb,\n};\n\nconst struct bpf_verifier_ops cg_sock_verifier_ops = {\n\t.get_func_proto\t\t= sock_filter_func_proto,\n\t.is_valid_access\t= sock_filter_is_valid_access,\n\t.convert_ctx_access\t= sock_filter_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_prog_ops = {\n};\n\nconst struct bpf_verifier_ops cg_sock_addr_verifier_ops = {\n\t.get_func_proto\t\t= sock_addr_func_proto,\n\t.is_valid_access\t= sock_addr_is_valid_access,\n\t.convert_ctx_access\t= sock_addr_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sock_addr_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sock_ops_verifier_ops = {\n\t.get_func_proto\t\t= sock_ops_func_proto,\n\t.is_valid_access\t= sock_ops_is_valid_access,\n\t.convert_ctx_access\t= sock_ops_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sock_ops_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_skb_verifier_ops = {\n\t.get_func_proto\t\t= sk_skb_func_proto,\n\t.is_valid_access\t= sk_skb_is_valid_access,\n\t.convert_ctx_access\t= sk_skb_convert_ctx_access,\n\t.gen_prologue\t\t= sk_skb_prologue,\n};\n\nconst struct bpf_prog_ops sk_skb_prog_ops = {\n};\n\nconst struct bpf_verifier_ops sk_msg_verifier_ops = {\n\t.get_func_proto\t\t= sk_msg_func_proto,\n\t.is_valid_access\t= sk_msg_is_valid_access,\n\t.convert_ctx_access\t= sk_msg_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops sk_msg_prog_ops = {\n};\n\nint sk_detach_filter(struct sock *sk)\n{\n\tint ret = -ENOENT;\n\tstruct sk_filter *filter;\n\n\tif (sock_flag(sk, SOCK_FILTER_LOCKED))\n\t\treturn -EPERM;\n\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (filter) {\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t\tsk_filter_uncharge(sk, filter);\n\t\tret = 0;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(sk_detach_filter);\n\nint sk_get_filter(struct sock *sk, struct sock_filter __user *ubuf,\n\t\t  unsigned int len)\n{\n\tstruct sock_fprog_kern *fprog;\n\tstruct sk_filter *filter;\n\tint ret = 0;\n\n\tlock_sock(sk);\n\tfilter = rcu_dereference_protected(sk->sk_filter,\n\t\t\t\t\t   lockdep_sock_is_held(sk));\n\tif (!filter)\n\t\tgoto out;\n\n\t/* We're copying the filter that has been originally attached,\n\t * so no conversion/decode needed anymore. eBPF programs that\n\t * have no original program cannot be dumped through this.\n\t */\n\tret = -EACCES;\n\tfprog = filter->prog->orig_prog;\n\tif (!fprog)\n\t\tgoto out;\n\n\tret = fprog->len;\n\tif (!len)\n\t\t/* User space only enquires number of filter blocks. */\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (len < fprog->len)\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_to_user(ubuf, fprog->filter, bpf_classic_proglen(fprog)))\n\t\tgoto out;\n\n\t/* Instead of bytes, the API requests to return the number\n\t * of filter blocks.\n\t */\n\tret = fprog->len;\nout:\n\trelease_sock(sk);\n\treturn ret;\n}\n"], "filenames": ["kernel/bpf/core.c", "net/core/filter.c"], "buggy_code_start_loc": [221, 483], "buggy_code_end_loc": [298, 489], "fixing_code_start_loc": [221, 484], "fixing_code_end_loc": [348, 496], "type": "CWE-120", "message": "The BPF subsystem in the Linux kernel before 4.17 mishandles situations with a long jump over an instruction sequence where inner instructions require substantial expansions into multiple BPF instructions, leading to an overflow. This affects kernel/bpf/core.c and net/core/filter.c.", "other": {"cve": {"id": "CVE-2018-25020", "sourceIdentifier": "cve@mitre.org", "published": "2021-12-08T05:15:07.910", "lastModified": "2022-04-05T20:55:39.597", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The BPF subsystem in the Linux kernel before 4.17 mishandles situations with a long jump over an instruction sequence where inner instructions require substantial expansions into multiple BPF instructions, leading to an overflow. This affects kernel/bpf/core.c and net/core/filter.c."}, {"lang": "es", "value": "El subsistema BPF en el kernel de Linux versiones anteriores a 4.17, maneja inapropiadamente las situaciones con un salto largo sobre una secuencia de instrucciones donde las instrucciones internas requieren expansiones sustanciales en m\u00faltiples instrucciones BPF, conllevando a un desbordamiento. Esto afecta a los archivos kernel/bpf/core.c y net/core/filter.c"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.17", "matchCriteriaId": "0667D0B1-8AC7-46D8-BB4B-68157115D405"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:netapp:cloud_backup:-:*:*:*:*:*:*:*", "matchCriteriaId": "5C2089EE-5D7F-47EC-8EA5-0F69790564C4"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410c_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "234DEFE0-5CE5-4B0A-96B8-5D227CB8ED31"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410c:-:*:*:*:*:*:*:*", "matchCriteriaId": "CDDF61B7-EC5C-467C-B710-B89F502CD04F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "6770B6C3-732E-4E22-BF1C-2D2FD610061C"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300s:-:*:*:*:*:*:*:*", "matchCriteriaId": "9F9C8C20-42EB-4AB5-BD97-212DEB070C43"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "7FFF7106-ED78-49BA-9EC5-B889E3685D53"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500s:-:*:*:*:*:*:*:*", "matchCriteriaId": "E63D8B0F-006E-4801-BF9D-1C001BBFB4F9"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "56409CEC-5A1E-4450-AA42-641E459CC2AF"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700s:-:*:*:*:*:*:*:*", "matchCriteriaId": "B06F4839-D16A-4A61-9BB5-55B13F41E47F"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h300e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "108A2215-50FB-4074-94CF-C130FA14566D"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h300e:-:*:*:*:*:*:*:*", "matchCriteriaId": "7AFC73CE-ABB9-42D3-9A71-3F5BC5381E0E"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h500e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "32F0B6C0-F930-480D-962B-3F4EFDCC13C7"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h500e:-:*:*:*:*:*:*:*", "matchCriteriaId": "803BC414-B250-4E3A-A478-A3881340D6B8"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h700e_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "0FEB3337-BFDE-462A-908B-176F92053CEC"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h700e:-:*:*:*:*:*:*:*", "matchCriteriaId": "736AEAE9-782B-4F71-9893-DED53367E102"}]}]}, {"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:netapp:h410s_firmware:-:*:*:*:*:*:*:*", "matchCriteriaId": "D0B4AD8A-F172-4558-AEC6-FF424BA2D912"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:h:netapp:h410s:-:*:*:*:*:*:*:*", "matchCriteriaId": "8497A4C9-8474-4A62-8331-3FE862ED4098"}]}]}], "references": [{"url": "http://packetstormsecurity.com/files/165477/Kernel-Live-Patch-Security-Notice-LSN-0083-1.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/050fad7c4534c13c8eb1d9c2ba66012e014773cb", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20211229-0005/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/050fad7c4534c13c8eb1d9c2ba66012e014773cb"}}