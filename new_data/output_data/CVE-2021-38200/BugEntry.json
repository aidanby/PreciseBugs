{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <linux/uaccess.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n#include <asm/code-patching.h>\n#include <asm/interrupt.h>\n\n#ifdef CONFIG_PPC64\n#include \"internal.h\"\n#endif\n\n#define BHRB_MAX_ENTRIES\t32\n#define BHRB_TARGET\t\t0x0000000000000002\n#define BHRB_PREDICTION\t\t0x0000000000000001\n#define BHRB_EA\t\t\t0xFFFFFFFFFFFFFFFCUL\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tstruct mmcr_regs mmcr;\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int txn_flags;\n\tint n_txn_start;\n\n\t/* BHRB bits */\n\tu64\t\t\t\tbhrb_filter;\t/* BHRB HW branch filter */\n\tunsigned int\t\t\tbhrb_users;\n\tvoid\t\t\t\t*bhrb_context;\n\tstruct\tperf_branch_stack\tbhrb_stack;\n\tstruct\tperf_branch_entry\tbhrb_entries[BHRB_MAX_ENTRIES];\n\tu64\t\t\t\tic_init;\n\n\t/* Store the PMC values */\n\tunsigned long pmcs[MAX_HWEVENTS];\n};\n\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstatic struct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n * Also 32-bit doesn't have MMCR3, SIER2 and SIER3.\n * Define them as zero knowing that any code path accessing\n * these registers (via mtspr/mfspr) are done under ppmu flag\n * check for PPMU_ARCH_31 and we will not enter that code path\n * for 32-bit.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n#define MMCR0_FC56\t\t0\n#define MMCR0_PMAO\t\t0\n#define MMCR0_EBE\t\t0\n#define MMCR0_BHRBA\t\t0\n#define MMCR0_PMCC\t\t0\n#define MMCR0_PMCC_U6\t\t0\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define SPRN_MMCR3\t\t0\n#define SPRN_SIER2\t\t0\n#define SPRN_SIER3\t\t0\n#define MMCRA_SAMPLE_ENABLE\t0\n#define MMCRA_BHRB_DISABLE     0\n#define MMCR0_PMCCEXT\t\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct perf_event *event, struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->result = 0;\n}\n\nstatic inline int siar_valid(struct pt_regs *regs)\n{\n\treturn 1;\n}\n\nstatic bool is_ebb_event(struct perf_event *event) { return false; }\nstatic int ebb_event_check(struct perf_event *event) { return 0; }\nstatic void ebb_event_add(struct perf_event *event) { }\nstatic void ebb_switch_out(unsigned long mmcr0) { }\nstatic unsigned long ebb_switch_in(bool ebb, struct cpu_hw_events *cpuhw)\n{\n\treturn cpuhw->mmcr.mmcr0;\n}\n\nstatic inline void power_pmu_bhrb_enable(struct perf_event *event) {}\nstatic inline void power_pmu_bhrb_disable(struct perf_event *event) {}\nstatic void power_pmu_sched_task(struct perf_event_context *ctx, bool sched_in) {}\nstatic inline void power_pmu_bhrb_read(struct perf_event *event, struct cpu_hw_events *cpuhw) {}\nstatic void pmao_restore_workaround(bool ebb) { }\n#endif /* CONFIG_PPC32 */\n\nbool is_sier_available(void)\n{\n\tif (!ppmu)\n\t\treturn false;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Return PMC value corresponding to the\n * index passed.\n */\nunsigned long get_pmcs_ext_regs(int idx)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\treturn cpuhw->pmcs[idx];\n}\n\nstatic bool regs_use_siar(struct pt_regs *regs)\n{\n\t/*\n\t * When we take a performance monitor exception the regs are setup\n\t * using perf_read_regs() which overloads some fields, in particular\n\t * regs->result to tell us whether to use SIAR.\n\t *\n\t * However if the regs are from another exception, eg. a syscall, then\n\t * they have not been setup using perf_read_regs() and so regs->result\n\t * is something random.\n\t */\n\treturn ((TRAP(regs) == INTERRUPT_PERFMON) && regs->result);\n}\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((ppmu->flags & PPMU_HAS_SSLOT) && (mmcra & MMCRA_SAMPLE_ENABLE)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC, the\n * [POWER7P_]MMCRA_SDAR_VALID bit in MMCRA, or the SDAR_VALID bit in SIER.\n */\nstatic inline void perf_get_data_addr(struct perf_event *event, struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tbool sdar_valid;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tsdar_valid = regs->dar & SIER_SDAR_VALID;\n\telse {\n\t\tunsigned long sdsync;\n\n\t\tif (ppmu->flags & PPMU_SIAR_VALID)\n\t\t\tsdsync = POWER7P_MMCRA_SDAR_VALID;\n\t\telse if (ppmu->flags & PPMU_ALT_SIPR)\n\t\t\tsdsync = POWER6_MMCRA_SDSYNC;\n\t\telse if (ppmu->flags & PPMU_NO_SIAR)\n\t\t\tsdsync = MMCRA_SAMPLE_ENABLE;\n\t\telse\n\t\t\tsdsync = MMCRA_SDSYNC;\n\n\t\tsdar_valid = mmcra & sdsync;\n\t}\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || sdar_valid)\n\t\t*addrp = mfspr(SPRN_SDAR);\n\n\tif (is_kernel_addr(mfspr(SPRN_SDAR)) && event->attr.exclude_kernel)\n\t\t*addrp = 0;\n}\n\nstatic bool regs_sihv(struct pt_regs *regs)\n{\n\tunsigned long sihv = MMCRA_SIHV;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn !!(regs->dar & SIER_SIHV);\n\n\tif (ppmu->flags & PPMU_ALT_SIPR)\n\t\tsihv = POWER6_MMCRA_SIHV;\n\n\treturn !!(regs->dsisr & sihv);\n}\n\nstatic bool regs_sipr(struct pt_regs *regs)\n{\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn !!(regs->dar & SIER_SIPR);\n\n\tif (ppmu->flags & PPMU_ALT_SIPR)\n\t\tsipr = POWER6_MMCRA_SIPR;\n\n\treturn !!(regs->dsisr & sipr);\n}\n\nstatic inline u32 perf_flags_from_msr(struct pt_regs *regs)\n{\n\tif (regs->msr & MSR_PR)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((regs->msr & MSR_HV) && freeze_events_kernel != MMCR0_FCHV)\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long mmcra = regs->dsisr;\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\n\tif (!use_siar)\n\t\treturn perf_flags_from_msr(regs);\n\n\t/*\n\t * Check the address in SIAR to identify the\n\t * privilege levels since the SIER[MSR_HV, MSR_PR]\n\t * bits are not set for marked events in power10\n\t * DD1.\n\t */\n\tif (marked && (ppmu->flags & PPMU_P10_DD1)) {\n\t\tunsigned long siar = mfspr(SPRN_SIAR);\n\t\tif (siar) {\n\t\t\tif (is_kernel_addr(siar))\n\t\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\t\treturn PERF_RECORD_MISC_USER;\n\t\t} else {\n\t\t\tif (is_kernel_addr(regs->nip))\n\t\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\t\treturn PERF_RECORD_MISC_USER;\n\t\t}\n\t}\n\n\t/*\n\t * If we don't have flags in MMCRA, rather than using\n\t * the MSR, we intuit the flags from the address in\n\t * SIAR which should give slightly more reliable\n\t * results\n\t */\n\tif (ppmu->flags & PPMU_NO_SIPR) {\n\t\tunsigned long siar = mfspr(SPRN_SIAR);\n\t\tif (is_kernel_addr(siar))\n\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\treturn PERF_RECORD_MISC_USER;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (regs_sipr(regs))\n\t\treturn PERF_RECORD_MISC_USER;\n\n\tif (regs_sihv(regs) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n * Overload regs->dar to store SIER if we have it.\n * Overload regs->result to specify whether we should use the MSR (result\n * is zero) or the SIAR (result is non zero).\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tunsigned long mmcra = mfspr(SPRN_MMCRA);\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\tint use_siar;\n\n\tregs->dsisr = mmcra;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tregs->dar = mfspr(SPRN_SIER);\n\n\t/*\n\t * If this isn't a PMU exception (eg a software event) the SIAR is\n\t * not valid. Use pt_regs.\n\t *\n\t * If it is a marked event use the SIAR.\n\t *\n\t * If the PMU doesn't update the SIAR for non marked events use\n\t * pt_regs.\n\t *\n\t * If the PMU has HV/PR flags then check to see if they\n\t * place the exception in userspace. If so, use pt_regs. In\n\t * continuous sampling mode the SIAR and the PMU exception are\n\t * not synchronised, so they may be many instructions apart.\n\t * This can result in confusing backtraces. We still want\n\t * hypervisor samples as well as samples in the kernel with\n\t * interrupts off hence the userspace check.\n\t */\n\tif (TRAP(regs) != INTERRUPT_PERFMON)\n\t\tuse_siar = 0;\n\telse if ((ppmu->flags & PPMU_NO_SIAR))\n\t\tuse_siar = 0;\n\telse if (marked)\n\t\tuse_siar = 1;\n\telse if ((ppmu->flags & PPMU_NO_CONT_SAMPLING))\n\t\tuse_siar = 0;\n\telse if (!(ppmu->flags & PPMU_NO_SIPR) && regs_sipr(regs))\n\t\tuse_siar = 0;\n\telse\n\t\tuse_siar = 1;\n\n\tregs->result = use_siar;\n}\n\n/*\n * On processors like P7+ that have the SIAR-Valid bit, marked instructions\n * must be sampled only if the SIAR-valid bit is set.\n *\n * For unmarked instructions and for processors that don't have the SIAR-Valid\n * bit, assume that SIAR is valid.\n */\nstatic inline int siar_valid(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\n\tif (marked) {\n\t\t/*\n\t\t * SIER[SIAR_VALID] is not set for some\n\t\t * marked events on power10 DD1, so drop\n\t\t * the check for SIER[SIAR_VALID] and return true.\n\t\t */\n\t\tif (ppmu->flags & PPMU_P10_DD1)\n\t\t\treturn 0x1;\n\t\telse if (ppmu->flags & PPMU_HAS_SIER)\n\t\t\treturn regs->dar & SIER_SIAR_VALID;\n\n\t\tif (ppmu->flags & PPMU_SIAR_VALID)\n\t\t\treturn mmcra & POWER7P_MMCRA_SIAR_VALID;\n\t}\n\n\treturn 1;\n}\n\n\n/* Reset all possible BHRB entries */\nstatic void power_pmu_bhrb_reset(void)\n{\n\tasm volatile(PPC_CLRBHRB);\n}\n\nstatic void power_pmu_bhrb_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\t/* Clear BHRB if we changed task context to avoid data leaks */\n\tif (event->ctx->task && cpuhw->bhrb_context != event->ctx) {\n\t\tpower_pmu_bhrb_reset();\n\t\tcpuhw->bhrb_context = event->ctx;\n\t}\n\tcpuhw->bhrb_users++;\n\tperf_sched_cb_inc(event->ctx->pmu);\n}\n\nstatic void power_pmu_bhrb_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\tWARN_ON_ONCE(!cpuhw->bhrb_users);\n\tcpuhw->bhrb_users--;\n\tperf_sched_cb_dec(event->ctx->pmu);\n\n\tif (!cpuhw->disabled && !cpuhw->bhrb_users) {\n\t\t/* BHRB cannot be turned off when other\n\t\t * events are active on the PMU.\n\t\t */\n\n\t\t/* avoid stale pointer */\n\t\tcpuhw->bhrb_context = NULL;\n\t}\n}\n\n/* Called from ctxsw to prevent one process's branch entries to\n * mingle with the other process's entries during context switch.\n */\nstatic void power_pmu_sched_task(struct perf_event_context *ctx, bool sched_in)\n{\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\tif (sched_in)\n\t\tpower_pmu_bhrb_reset();\n}\n/* Calculate the to address for a branch */\nstatic __u64 power_pmu_bhrb_to(u64 addr)\n{\n\tunsigned int instr;\n\t__u64 target;\n\n\tif (is_kernel_addr(addr)) {\n\t\tif (copy_from_kernel_nofault(&instr, (void *)addr,\n\t\t\t\tsizeof(instr)))\n\t\t\treturn 0;\n\n\t\treturn branch_target((struct ppc_inst *)&instr);\n\t}\n\n\t/* Userspace: need copy instruction here then translate it */\n\tif (copy_from_user_nofault(&instr, (unsigned int __user *)addr,\n\t\t\tsizeof(instr)))\n\t\treturn 0;\n\n\ttarget = branch_target((struct ppc_inst *)&instr);\n\tif ((!target) || (instr & BRANCH_ABSOLUTE))\n\t\treturn target;\n\n\t/* Translate relative branch target from kernel to user address */\n\treturn target - (unsigned long)&instr + addr;\n}\n\n/* Processing BHRB entries */\nstatic void power_pmu_bhrb_read(struct perf_event *event, struct cpu_hw_events *cpuhw)\n{\n\tu64 val;\n\tu64 addr;\n\tint r_index, u_index, pred;\n\n\tr_index = 0;\n\tu_index = 0;\n\twhile (r_index < ppmu->bhrb_nr) {\n\t\t/* Assembly read function */\n\t\tval = read_bhrb(r_index++);\n\t\tif (!val)\n\t\t\t/* Terminal marker: End of valid BHRB entries */\n\t\t\tbreak;\n\t\telse {\n\t\t\taddr = val & BHRB_EA;\n\t\t\tpred = val & BHRB_PREDICTION;\n\n\t\t\tif (!addr)\n\t\t\t\t/* invalid entry */\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * BHRB rolling buffer could very much contain the kernel\n\t\t\t * addresses at this point. Check the privileges before\n\t\t\t * exporting it to userspace (avoid exposure of regions\n\t\t\t * where we could have speculative execution)\n\t\t\t * Incase of ISA v3.1, BHRB will capture only user-space\n\t\t\t * addresses, hence include a check before filtering code\n\t\t\t */\n\t\t\tif (!(ppmu->flags & PPMU_ARCH_31) &&\n\t\t\t    is_kernel_addr(addr) && event->attr.exclude_kernel)\n\t\t\t\tcontinue;\n\n\t\t\t/* Branches are read most recent first (ie. mfbhrb 0 is\n\t\t\t * the most recent branch).\n\t\t\t * There are two types of valid entries:\n\t\t\t * 1) a target entry which is the to address of a\n\t\t\t *    computed goto like a blr,bctr,btar.  The next\n\t\t\t *    entry read from the bhrb will be branch\n\t\t\t *    corresponding to this target (ie. the actual\n\t\t\t *    blr/bctr/btar instruction).\n\t\t\t * 2) a from address which is an actual branch.  If a\n\t\t\t *    target entry proceeds this, then this is the\n\t\t\t *    matching branch for that target.  If this is not\n\t\t\t *    following a target entry, then this is a branch\n\t\t\t *    where the target is given as an immediate field\n\t\t\t *    in the instruction (ie. an i or b form branch).\n\t\t\t *    In this case we need to read the instruction from\n\t\t\t *    memory to determine the target/to address.\n\t\t\t */\n\n\t\t\tif (val & BHRB_TARGET) {\n\t\t\t\t/* Target branches use two entries\n\t\t\t\t * (ie. computed gotos/XL form)\n\t\t\t\t */\n\t\t\t\tcpuhw->bhrb_entries[u_index].to = addr;\n\t\t\t\tcpuhw->bhrb_entries[u_index].mispred = pred;\n\t\t\t\tcpuhw->bhrb_entries[u_index].predicted = ~pred;\n\n\t\t\t\t/* Get from address in next entry */\n\t\t\t\tval = read_bhrb(r_index++);\n\t\t\t\taddr = val & BHRB_EA;\n\t\t\t\tif (val & BHRB_TARGET) {\n\t\t\t\t\t/* Shouldn't have two targets in a\n\t\t\t\t\t   row.. Reset index and try again */\n\t\t\t\t\tr_index--;\n\t\t\t\t\taddr = 0;\n\t\t\t\t}\n\t\t\t\tcpuhw->bhrb_entries[u_index].from = addr;\n\t\t\t} else {\n\t\t\t\t/* Branches to immediate field \n\t\t\t\t   (ie I or B form) */\n\t\t\t\tcpuhw->bhrb_entries[u_index].from = addr;\n\t\t\t\tcpuhw->bhrb_entries[u_index].to =\n\t\t\t\t\tpower_pmu_bhrb_to(addr);\n\t\t\t\tcpuhw->bhrb_entries[u_index].mispred = pred;\n\t\t\t\tcpuhw->bhrb_entries[u_index].predicted = ~pred;\n\t\t\t}\n\t\t\tu_index++;\n\n\t\t}\n\t}\n\tcpuhw->bhrb_stack.nr = u_index;\n\tcpuhw->bhrb_stack.hw_idx = -1ULL;\n\treturn;\n}\n\nstatic bool is_ebb_event(struct perf_event *event)\n{\n\t/*\n\t * This could be a per-PMU callback, but we'd rather avoid the cost. We\n\t * check that the PMU supports EBB, meaning those that don't can still\n\t * use bit 63 of the event code for something else if they wish.\n\t */\n\treturn (ppmu->flags & PPMU_ARCH_207S) &&\n\t       ((event->attr.config >> PERF_EVENT_CONFIG_EBB_SHIFT) & 1);\n}\n\nstatic int ebb_event_check(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\n\t/* Event and group leader must agree on EBB */\n\tif (is_ebb_event(leader) != is_ebb_event(event))\n\t\treturn -EINVAL;\n\n\tif (is_ebb_event(event)) {\n\t\tif (!(event->attach_state & PERF_ATTACH_TASK))\n\t\t\treturn -EINVAL;\n\n\t\tif (!leader->attr.pinned || !leader->attr.exclusive)\n\t\t\treturn -EINVAL;\n\n\t\tif (event->attr.freq ||\n\t\t    event->attr.inherit ||\n\t\t    event->attr.sample_type ||\n\t\t    event->attr.sample_period ||\n\t\t    event->attr.enable_on_exec)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void ebb_event_add(struct perf_event *event)\n{\n\tif (!is_ebb_event(event) || current->thread.used_ebb)\n\t\treturn;\n\n\t/*\n\t * IFF this is the first time we've added an EBB event, set\n\t * PMXE in the user MMCR0 so we can detect when it's cleared by\n\t * userspace. We need this so that we can context switch while\n\t * userspace is in the EBB handler (where PMXE is 0).\n\t */\n\tcurrent->thread.used_ebb = 1;\n\tcurrent->thread.mmcr0 |= MMCR0_PMXE;\n}\n\nstatic void ebb_switch_out(unsigned long mmcr0)\n{\n\tif (!(mmcr0 & MMCR0_EBE))\n\t\treturn;\n\n\tcurrent->thread.siar  = mfspr(SPRN_SIAR);\n\tcurrent->thread.sier  = mfspr(SPRN_SIER);\n\tcurrent->thread.sdar  = mfspr(SPRN_SDAR);\n\tcurrent->thread.mmcr0 = mmcr0 & MMCR0_USER_MASK;\n\tcurrent->thread.mmcr2 = mfspr(SPRN_MMCR2) & MMCR2_USER_MASK;\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tcurrent->thread.mmcr3 = mfspr(SPRN_MMCR3);\n\t\tcurrent->thread.sier2 = mfspr(SPRN_SIER2);\n\t\tcurrent->thread.sier3 = mfspr(SPRN_SIER3);\n\t}\n}\n\nstatic unsigned long ebb_switch_in(bool ebb, struct cpu_hw_events *cpuhw)\n{\n\tunsigned long mmcr0 = cpuhw->mmcr.mmcr0;\n\n\tif (!ebb)\n\t\tgoto out;\n\n\t/* Enable EBB and read/write to all 6 PMCs and BHRB for userspace */\n\tmmcr0 |= MMCR0_EBE | MMCR0_BHRBA | MMCR0_PMCC_U6;\n\n\t/*\n\t * Add any bits from the user MMCR0, FC or PMAO. This is compatible\n\t * with pmao_restore_workaround() because we may add PMAO but we never\n\t * clear it here.\n\t */\n\tmmcr0 |= current->thread.mmcr0;\n\n\t/*\n\t * Be careful not to set PMXE if userspace had it cleared. This is also\n\t * compatible with pmao_restore_workaround() because it has already\n\t * cleared PMXE and we leave PMAO alone.\n\t */\n\tif (!(current->thread.mmcr0 & MMCR0_PMXE))\n\t\tmmcr0 &= ~MMCR0_PMXE;\n\n\tmtspr(SPRN_SIAR, current->thread.siar);\n\tmtspr(SPRN_SIER, current->thread.sier);\n\tmtspr(SPRN_SDAR, current->thread.sdar);\n\n\t/*\n\t * Merge the kernel & user values of MMCR2. The semantics we implement\n\t * are that the user MMCR2 can set bits, ie. cause counters to freeze,\n\t * but not clear bits. If a task wants to be able to clear bits, ie.\n\t * unfreeze counters, it should not set exclude_xxx in its events and\n\t * instead manage the MMCR2 entirely by itself.\n\t */\n\tmtspr(SPRN_MMCR2, cpuhw->mmcr.mmcr2 | current->thread.mmcr2);\n\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tmtspr(SPRN_MMCR3, current->thread.mmcr3);\n\t\tmtspr(SPRN_SIER2, current->thread.sier2);\n\t\tmtspr(SPRN_SIER3, current->thread.sier3);\n\t}\nout:\n\treturn mmcr0;\n}\n\nstatic void pmao_restore_workaround(bool ebb)\n{\n\tunsigned pmcs[6];\n\n\tif (!cpu_has_feature(CPU_FTR_PMAO_BUG))\n\t\treturn;\n\n\t/*\n\t * On POWER8E there is a hardware defect which affects the PMU context\n\t * switch logic, ie. power_pmu_disable/enable().\n\t *\n\t * When a counter overflows PMXE is cleared and FC/PMAO is set in MMCR0\n\t * by the hardware. Sometime later the actual PMU exception is\n\t * delivered.\n\t *\n\t * If we context switch, or simply disable/enable, the PMU prior to the\n\t * exception arriving, the exception will be lost when we clear PMAO.\n\t *\n\t * When we reenable the PMU, we will write the saved MMCR0 with PMAO\n\t * set, and this _should_ generate an exception. However because of the\n\t * defect no exception is generated when we write PMAO, and we get\n\t * stuck with no counters counting but no exception delivered.\n\t *\n\t * The workaround is to detect this case and tweak the hardware to\n\t * create another pending PMU exception.\n\t *\n\t * We do that by setting up PMC6 (cycles) for an imminent overflow and\n\t * enabling the PMU. That causes a new exception to be generated in the\n\t * chip, but we don't take it yet because we have interrupts hard\n\t * disabled. We then write back the PMU state as we want it to be seen\n\t * by the exception handler. When we reenable interrupts the exception\n\t * handler will be called and see the correct state.\n\t *\n\t * The logic is the same for EBB, except that the exception is gated by\n\t * us having interrupts hard disabled as well as the fact that we are\n\t * not in userspace. The exception is finally delivered when we return\n\t * to userspace.\n\t */\n\n\t/* Only if PMAO is set and PMAO_SYNC is clear */\n\tif ((current->thread.mmcr0 & (MMCR0_PMAO | MMCR0_PMAO_SYNC)) != MMCR0_PMAO)\n\t\treturn;\n\n\t/* If we're doing EBB, only if BESCR[GE] is set */\n\tif (ebb && !(current->thread.bescr & BESCR_GE))\n\t\treturn;\n\n\t/*\n\t * We are already soft-disabled in power_pmu_enable(). We need to hard\n\t * disable to actually prevent the PMU exception from firing.\n\t */\n\thard_irq_disable();\n\n\t/*\n\t * This is a bit gross, but we know we're on POWER8E and have 6 PMCs.\n\t * Using read/write_pmc() in a for loop adds 12 function calls and\n\t * almost doubles our code size.\n\t */\n\tpmcs[0] = mfspr(SPRN_PMC1);\n\tpmcs[1] = mfspr(SPRN_PMC2);\n\tpmcs[2] = mfspr(SPRN_PMC3);\n\tpmcs[3] = mfspr(SPRN_PMC4);\n\tpmcs[4] = mfspr(SPRN_PMC5);\n\tpmcs[5] = mfspr(SPRN_PMC6);\n\n\t/* Ensure all freeze bits are unset */\n\tmtspr(SPRN_MMCR2, 0);\n\n\t/* Set up PMC6 to overflow in one cycle */\n\tmtspr(SPRN_PMC6, 0x7FFFFFFE);\n\n\t/* Enable exceptions and unfreeze PMC6 */\n\tmtspr(SPRN_MMCR0, MMCR0_PMXE | MMCR0_PMCjCE | MMCR0_PMAO);\n\n\t/* Now we need to refreeze and restore the PMCs */\n\tmtspr(SPRN_MMCR0, MMCR0_FC | MMCR0_PMAO);\n\n\tmtspr(SPRN_PMC1, pmcs[0]);\n\tmtspr(SPRN_PMC2, pmcs[1]);\n\tmtspr(SPRN_PMC3, pmcs[2]);\n\tmtspr(SPRN_PMC4, pmcs[3]);\n\tmtspr(SPRN_PMC5, pmcs[4]);\n\tmtspr(SPRN_PMC6, pmcs[5]);\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/* Called from sysrq_handle_showregs() */\nvoid perf_event_print_debug(void)\n{\n\tunsigned long sdar, sier, flags;\n\tu32 pmcs[MAX_HWEVENTS];\n\tint i;\n\n\tif (!ppmu) {\n\t\tpr_info(\"Performance monitor hardware not registered.\\n\");\n\t\treturn;\n\t}\n\n\tif (!ppmu->n_counter)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpr_info(\"CPU: %d PMU registers, ppmu = %s n_counters = %d\",\n\t\t smp_processor_id(), ppmu->name, ppmu->n_counter);\n\n\tfor (i = 0; i < ppmu->n_counter; i++)\n\t\tpmcs[i] = read_pmc(i + 1);\n\n\tfor (; i < MAX_HWEVENTS; i++)\n\t\tpmcs[i] = 0xdeadbeef;\n\n\tpr_info(\"PMC1:  %08x PMC2: %08x PMC3: %08x PMC4: %08x\\n\",\n\t\t pmcs[0], pmcs[1], pmcs[2], pmcs[3]);\n\n\tif (ppmu->n_counter > 4)\n\t\tpr_info(\"PMC5:  %08x PMC6: %08x PMC7: %08x PMC8: %08x\\n\",\n\t\t\t pmcs[4], pmcs[5], pmcs[6], pmcs[7]);\n\n\tpr_info(\"MMCR0: %016lx MMCR1: %016lx MMCRA: %016lx\\n\",\n\t\tmfspr(SPRN_MMCR0), mfspr(SPRN_MMCR1), mfspr(SPRN_MMCRA));\n\n\tsdar = sier = 0;\n#ifdef CONFIG_PPC64\n\tsdar = mfspr(SPRN_SDAR);\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tsier = mfspr(SPRN_SIER);\n\n\tif (ppmu->flags & PPMU_ARCH_207S) {\n\t\tpr_info(\"MMCR2: %016lx EBBHR: %016lx\\n\",\n\t\t\tmfspr(SPRN_MMCR2), mfspr(SPRN_EBBHR));\n\t\tpr_info(\"EBBRR: %016lx BESCR: %016lx\\n\",\n\t\t\tmfspr(SPRN_EBBRR), mfspr(SPRN_BESCR));\n\t}\n\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tpr_info(\"MMCR3: %016lx SIER2: %016lx SIER3: %016lx\\n\",\n\t\t\tmfspr(SPRN_MMCR3), mfspr(SPRN_SIER2), mfspr(SPRN_SIER3));\n\t}\n#endif\n\tpr_info(\"SIAR:  %016lx SDAR:  %016lx SIER:  %016lx\\n\",\n\t\tmfspr(SPRN_SIAR), sdar, sier);\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev, struct perf_event **event)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\tunsigned long grp_mask = ppmu->group_constraint_mask;\n\tunsigned long grp_val = ppmu->group_constraint_val;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0], event[i]->attr.config1))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\n\t\tif (((((nv + tadd) ^ value) & mask) & (~grp_mask)) != 0)\n\t\t\tbreak;\n\n\t\tif (((((nv + tadd) ^ cpuhw->avalues[i][0]) & cpuhw->amasks[i][0])\n\t\t\t& (~grp_mask)) != 0)\n\t\t\tbreak;\n\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev) {\n\t\tif ((value & mask & grp_mask) != (mask & grp_val))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\t/* all OK */\n\t}\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j],\n\t\t\t\t\t     event[i]->attr.config1);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\t/*\n\t * If the PMU we're on supports per event exclude settings then we\n\t * don't need to do any of this logic. NB. This assumes no PMU has both\n\t * per event exclude and limited PMCs.\n\t */\n\tif (ppmu->flags & PPMU_ARCH_207S)\n\t\treturn 0;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic u64 check_and_compute_delta(u64 prev, u64 val)\n{\n\tu64 delta = (val - prev) & 0xfffffffful;\n\n\t/*\n\t * POWER7 can roll back counter values, if the new value is smaller\n\t * than the previous value it will cause the delta and the counter to\n\t * have bogus values unless we rolled a counter over.  If a coutner is\n\t * rolled back, it will be smaller, but within 256, which is the maximum\n\t * number of events to rollback at once.  If we detect a rollback\n\t * return 0.  This can lead to a small lack of precision in the\n\t * counters.\n\t */\n\tif (prev > val && (prev - val) < 256)\n\t\tdelta = 0;\n\n\treturn delta;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\n\tif (is_ebb_event(event)) {\n\t\tval = read_pmc(event->hw.idx);\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\treturn;\n\t}\n\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (!delta)\n\t\t\treturn;\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * A number of places program the PMC with (0x80000000 - period_left).\n\t * We never want period_left to be less than 1 because we will program\n\t * the PMC with a value >= 0x800000000 and an edge detected PMC will\n\t * roll around to 0 before taking an exception. We have seen this\n\t * on POWER8.\n\t *\n\t * To fix this, clamp the minimum value of period_left to 1.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.period_left);\n\t\tval = prev - delta;\n\t\tif (val < 1)\n\t\t\tval = 1;\n\t} while (local64_cmpxchg(&event->hw.period_left, prev, val) != prev);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (delta)\n\t\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tif (check_and_compute_delta(prev, val))\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags, mmcr0, val, mmcra;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit, clear EBE/BHRBA/PMCC/PMAO/FC56\n\t\t */\n\t\tval  = mmcr0 = mfspr(SPRN_MMCR0);\n\t\tval |= MMCR0_FC;\n\t\tval &= ~(MMCR0_EBE | MMCR0_BHRBA | MMCR0_PMCC | MMCR0_PMAO |\n\t\t\t MMCR0_FC56);\n\t\t/* Set mmcr0 PMCCEXT for p10 */\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tval |= MMCR0_PMCCEXT;\n\n\t\t/*\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events etc.\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, val);\n\t\tmb();\n\t\tisync();\n\n\t\tval = mmcra = cpuhw->mmcr.mmcra;\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr.mmcra & MMCRA_SAMPLE_ENABLE)\n\t\t\tval &= ~MMCRA_SAMPLE_ENABLE;\n\n\t\t/* Disable BHRB via mmcra (BHRBRD) for p10 */\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tval |= MMCRA_BHRB_DISABLE;\n\n\t\t/*\n\t\t * Write SPRN_MMCRA if mmcra has either disabled\n\t\t * instruction sampling or BHRB.\n\t\t */\n\t\tif (val != mmcra) {\n\t\t\tmtspr(SPRN_MMCRA, mmcra);\n\t\t\tmb();\n\t\t\tisync();\n\t\t}\n\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\tebb_switch_out(mmcr0);\n\n#ifdef CONFIG_PPC64\n\t\t/*\n\t\t * These are readable by userspace, may contain kernel\n\t\t * addresses and are not switched by context switch, so clear\n\t\t * them now to avoid leaking anything to userspace in general\n\t\t * including to another process.\n\t\t */\n\t\tif (ppmu->flags & PPMU_ARCH_207S) {\n\t\t\tmtspr(SPRN_SDAR, 0);\n\t\t\tmtspr(SPRN_SIAR, 0);\n\t\t}\n#endif\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val, mmcr0;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\tbool ebb;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tif (!cpuhw->disabled)\n\t\tgoto out;\n\n\tif (cpuhw->n_events == 0) {\n\t\tppc_set_pmu_inuse(0);\n\t\tgoto out;\n\t}\n\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * EBB requires an exclusive group and all events must have the EBB\n\t * flag set, or not set, so we can just check a single event. Also we\n\t * know we have at least one event.\n\t */\n\tebb = is_ebb_event(cpuhw->event[0]);\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr.mmcr1);\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tmtspr(SPRN_MMCR3, cpuhw->mmcr.mmcr3);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Clear all MMCR settings and recompute them for the new set of events.\n\t */\n\tmemset(&cpuhw->mmcr, 0, sizeof(cpuhw->mmcr));\n\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       &cpuhw->mmcr, cpuhw->event, ppmu->flags)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\tif (!(ppmu->flags & PPMU_ARCH_207S)) {\n\t\t/*\n\t\t * Add in MMCR0 freeze bits corresponding to the attr.exclude_*\n\t\t * bits for the first event. We have already checked that all\n\t\t * events have the same value for these bits as the first event.\n\t\t */\n\t\tevent = cpuhw->event[0];\n\t\tif (event->attr.exclude_user)\n\t\t\tcpuhw->mmcr.mmcr0 |= MMCR0_FCP;\n\t\tif (event->attr.exclude_kernel)\n\t\t\tcpuhw->mmcr.mmcr0 |= freeze_events_kernel;\n\t\tif (event->attr.exclude_hv)\n\t\t\tcpuhw->mmcr.mmcr0 |= MMCR0_FCHV;\n\t}\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr.mmcr1);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr.mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\tif (ppmu->flags & PPMU_ARCH_207S)\n\t\tmtspr(SPRN_MMCR2, cpuhw->mmcr.mmcr2);\n\n\tif (ppmu->flags & PPMU_ARCH_31)\n\t\tmtspr(SPRN_MMCR3, cpuhw->mmcr.mmcr3);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ebb)\n\t\t\tval = local64_read(&event->hw.prev_count);\n\t\telse {\n\t\t\tval = 0;\n\t\t\tif (event->hw.sample_period) {\n\t\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\t\tif (left < 0x80000000L)\n\t\t\t\t\tval = 0x80000000L - left;\n\t\t\t}\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\t}\n\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr.mmcr0 |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tpmao_restore_workaround(ebb);\n\n\tmmcr0 = ebb_switch_in(ebb, cpuhw);\n\n\tmb();\n\tif (cpuhw->bhrb_users)\n\t\tppmu->config_bhrb(cpuhw->bhrb_filter);\n\n\twrite_mmcr0(cpuhw, mmcr0);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr.mmcra & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra);\n\t}\n\n out:\n\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (group->pmu->task_ctx_nr == perf_hw_context) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tfor_each_sibling_event(event, group) {\n\t\tif (event->pmu->task_ctx_nr == perf_hw_context &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add an event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\t/*\n\t * This event may have been disabled/stopped in record_and_restart()\n\t * because we exceeded the ->event_limit. If re-starting the event,\n\t * clear the ->hw.state (STOPPED and UPTODATE flags), so the user\n\t * notification is re-enabled.\n\t */\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\telse\n\t\tevent->hw.state = 0;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->txn_flags & PERF_PMU_TXN_ADD)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1, cpuhw->event))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\tebb_event_add(event);\n\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tif (has_branch_stack(event)) {\n\t\tu64 bhrb_filter = -1;\n\n\t\tif (ppmu->bhrb_filter_map)\n\t\t\tbhrb_filter = ppmu->bhrb_filter_map(\n\t\t\t\tevent->attr.branch_sample_type);\n\n\t\tif (bhrb_filter != -1) {\n\t\t\tcpuhw->bhrb_filter = bhrb_filter;\n\t\t\tpower_pmu_bhrb_enable(event);\n\t\t}\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove an event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, &cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr.mmcr0 &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tif (has_branch_stack(event))\n\t\tpower_pmu_bhrb_disable(event);\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\tunsigned long val;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\n\tval = 0;\n\tif (left < 0x80000000L)\n\t\tval = 0x80000000L - left;\n\n\twrite_pmc(event->hw.idx, val);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n *\n * We only support PERF_PMU_TXN_ADD transactions. Save the\n * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD\n * transactions.\n */\nstatic void power_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tWARN_ON_ONCE(cpuhw->txn_flags);\t\t/* txn already in flight */\n\n\tcpuhw->txn_flags = txn_flags;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\tunsigned int txn_flags;\n\n\tWARN_ON_ONCE(!cpuhw->txn_flags);\t/* no txn in flight */\n\n\ttxn_flags = cpuhw->txn_flags;\n\tcpuhw->txn_flags = 0;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tWARN_ON_ONCE(!cpuhw->txn_flags);\t/* no txn in flight */\n\n\tif (cpuhw->txn_flags & ~PERF_PMU_TXN_ADD) {\n\t\tcpuhw->txn_flags = 0;\n\t\treturn 0;\n\t}\n\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n, cpuhw->event);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->txn_flags = 0;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * An event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tu64 ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic bool is_event_blacklisted(u64 ev)\n{\n\tint i;\n\n\tfor (i=0; i < ppmu->n_blacklist_ev; i++) {\n\t\tif (ppmu->blacklist_ev[i] == ev)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags, irq_flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tif (has_branch_stack(event)) {\n\t        /* PMU has BHRB enabled */\n\t\tif (!(ppmu->flags & PPMU_ARCH_207S))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * PMU config registers have fields that are\n\t * reserved and some specific values for bit fields are reserved.\n\t * For ex., MMCRA[61:62] is Randome Sampling Mode (SM)\n\t * and value of 0b11 to this field is reserved.\n\t * Check for invalid values in attr.config.\n\t */\n\tif (ppmu->check_attr_config &&\n\t    ppmu->check_attr_config(event))\n\t\treturn -EINVAL;\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Extra checks for EBB */\n\terr = ebb_event_check(event);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tlocal_irq_save(irq_flags);\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1, ctrs);\n\n\tif (has_branch_stack(event)) {\n\t\tu64 bhrb_filter = -1;\n\n\t\tif (ppmu->bhrb_filter_map)\n\t\t\tbhrb_filter = ppmu->bhrb_filter_map(\n\t\t\t\t\tevent->attr.branch_sample_type);\n\n\t\tif (bhrb_filter == -1) {\n\t\t\tlocal_irq_restore(irq_flags);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t\tcpuhw->bhrb_filter = bhrb_filter;\n\t}\n\n\tlocal_irq_restore(irq_flags);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * For EBB events we just context switch the PMC value, we don't do any\n\t * of the sample_period logic. We use hw.prev_count for this.\n\t */\n\tif (is_ebb_event(event))\n\t\tlocal64_set(&event->hw.prev_count, 0);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstatic int power_pmu_event_idx(struct perf_event *event)\n{\n\treturn event->hw.idx;\n}\n\nssize_t power_events_sysfs_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\n\treturn sprintf(page, \"event=0x%02llx\\n\", pmu_attr->id);\n}\n\nstatic struct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n\t.event_idx\t= power_pmu_event_idx,\n\t.sched_task\t= power_pmu_sched_task,\n};\n\n#define PERF_SAMPLE_ADDR_TYPE  (PERF_SAMPLE_ADDR |\t\t\\\n\t\t\t\tPERF_SAMPLE_PHYS_ADDR |\t\t\\\n\t\t\t\tPERF_SAMPLE_DATA_PAGE_SIZE)\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = check_and_compute_delta(prev, val);\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (delta == 0)\n\t\tleft++;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\n\t\t\t/*\n\t\t\t * If address is not requested in the sample via\n\t\t\t * PERF_SAMPLE_IP, just record that sample irrespective\n\t\t\t * of SIAR valid check.\n\t\t\t */\n\t\t\tif (event->attr.sample_type & PERF_SAMPLE_IP)\n\t\t\t\trecord = siar_valid(regs);\n\t\t\telse\n\t\t\t\trecord = 1;\n\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Due to hardware limitation, sometimes SIAR could sample a kernel\n\t * address even when freeze on supervisor state (kernel) is set in\n\t * MMCR2. Check attr.exclude_kernel and address to drop the sample in\n\t * these cases.\n\t */\n\tif (event->attr.exclude_kernel &&\n\t    (event->attr.sample_type & PERF_SAMPLE_IP) &&\n\t    is_kernel_addr(mfspr(SPRN_SIAR)))\n\t\trecord = 0;\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL, event->hw.last_period);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR_TYPE)\n\t\t\tperf_get_data_addr(event, regs, &data.addr);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\t\tstruct cpu_hw_events *cpuhw;\n\t\t\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\t\t\tpower_pmu_bhrb_read(event, cpuhw);\n\t\t\tdata.br_stack = &cpuhw->bhrb_stack;\n\t\t}\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_DATA_SRC &&\n\t\t\t\t\t\tppmu->get_mem_data_src)\n\t\t\tppmu->get_mem_data_src(&data.data_src, ppmu->flags, regs);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_WEIGHT_TYPE &&\n\t\t\t\t\t\tppmu->get_mem_weight)\n\t\t\tppmu->get_mem_weight(&data.weight.full, event->attr.sample_type);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t} else if (period) {\n\t\t/* Account for interrupt in case of invalid SIAR */\n\t\tif (perf_event_account_interrupt(event))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long siar = mfspr(SPRN_SIAR);\n\n\tif (ppmu->flags & PPMU_P10_DD1) {\n\t\tif (siar)\n\t\t\treturn siar;\n\t\telse\n\t\t\treturn regs->nip;\n\t} else if (use_siar && siar_valid(regs))\n\t\treturn mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\telse if (use_siar)\n\t\treturn 0;\t\t// no valid instruction pointer\n\telse\n\t\treturn regs->nip;\n}\n\nstatic bool pmc_overflow_power7(unsigned long val)\n{\n\t/*\n\t * Events on POWER7 can roll back if a speculative event doesn't\n\t * eventually complete. Unfortunately in some rare cases they will\n\t * raise a performance monitor exception. We need to catch this to\n\t * ensure we reset the PMC. In all cases the PMC will be 256 or less\n\t * cycles from overflow.\n\t *\n\t * We only do this if the first pass fails to find any overflowing\n\t * PMCs because a user might set a period of less than 256 and we\n\t * don't want to mistakenly reset them.\n\t */\n\tif ((0x80000000 - val) <= 256)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool pmc_overflow(unsigned long val)\n{\n\tif ((int)val < 0)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void __perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i, j;\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_event *event;\n\tint found, active;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\t/* Read all the PMCs since we'll need them a bunch of times */\n\tfor (i = 0; i < ppmu->n_counter; ++i)\n\t\tcpuhw->pmcs[i] = read_pmc(i + 1);\n\n\t/* Try to find what caused the IRQ */\n\tfound = 0;\n\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\tif (!pmc_overflow(cpuhw->pmcs[i]))\n\t\t\tcontinue;\n\t\tif (is_limited_pmc(i + 1))\n\t\t\tcontinue; /* these won't generate IRQs */\n\t\t/*\n\t\t * We've found one that's overflowed.  For active\n\t\t * counters we need to log this.  For inactive\n\t\t * counters, we need to reset it anyway\n\t\t */\n\t\tfound = 1;\n\t\tactive = 0;\n\t\tfor (j = 0; j < cpuhw->n_events; ++j) {\n\t\t\tevent = cpuhw->event[j];\n\t\t\tif (event->hw.idx == (i + 1)) {\n\t\t\t\tactive = 1;\n\t\t\t\trecord_and_restart(event, cpuhw->pmcs[i], regs);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!active)\n\t\t\t/* reset non active counters that have overflowed */\n\t\t\twrite_pmc(i + 1, 0);\n\t}\n\tif (!found && pvr_version_is(PVR_POWER7)) {\n\t\t/* check active counters for special buggy p7 overflow */\n\t\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\t\tevent = cpuhw->event[i];\n\t\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\t\tcontinue;\n\t\t\tif (pmc_overflow_power7(cpuhw->pmcs[event->hw.idx - 1])) {\n\t\t\t\t/* event has overflowed in a buggy way*/\n\t\t\t\tfound = 1;\n\t\t\t\trecord_and_restart(event,\n\t\t\t\t\t\t   cpuhw->pmcs[event->hw.idx - 1],\n\t\t\t\t\t\t   regs);\n\t\t\t}\n\t\t}\n\t}\n\tif (unlikely(!found) && !arch_irq_disabled_regs(regs))\n\t\tprintk_ratelimited(KERN_WARNING \"Can't find PMC that caused IRQ\\n\");\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr.mmcr0);\n\n\t/* Clear the cpuhw->pmcs */\n\tmemset(&cpuhw->pmcs, 0, sizeof(cpuhw->pmcs));\n\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tu64 start_clock = sched_clock();\n\n\t__perf_event_interrupt(regs);\n\tperf_sample_event_took(sched_clock() - start_clock);\n}\n\nstatic int power_pmu_prepare_cpu(unsigned int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (ppmu) {\n\t\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\t\tcpuhw->mmcr.mmcr0 = MMCR0_FC;\n\t}\n\treturn 0;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n\tpower_pmu.attr_groups = ppmu->attr_groups;\n\tpower_pmu.capabilities |= (ppmu->capabilities & PERF_PMU_CAP_EXTENDED_REGS);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tcpuhp_setup_state(CPUHP_PERF_POWER, \"perf/powerpc:prepare\",\n\t\t\t  power_pmu_prepare_cpu, NULL);\n\treturn 0;\n}\n\n#ifdef CONFIG_PPC64\nstatic int __init init_ppc64_pmu(void)\n{\n\t/* run through all the pmu drivers one at a time */\n\tif (!init_power5_pmu())\n\t\treturn 0;\n\telse if (!init_power5p_pmu())\n\t\treturn 0;\n\telse if (!init_power6_pmu())\n\t\treturn 0;\n\telse if (!init_power7_pmu())\n\t\treturn 0;\n\telse if (!init_power8_pmu())\n\t\treturn 0;\n\telse if (!init_power9_pmu())\n\t\treturn 0;\n\telse if (!init_power10_pmu())\n\t\treturn 0;\n\telse if (!init_ppc970_pmu())\n\t\treturn 0;\n\telse\n\t\treturn init_generic_compat_pmu();\n}\nearly_initcall(init_ppc64_pmu);\n#endif\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-or-later\n/*\n * Performance event support - powerpc architecture code\n *\n * Copyright 2008-2009 Paul Mackerras, IBM Corporation.\n */\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <linux/uaccess.h>\n#include <asm/reg.h>\n#include <asm/pmc.h>\n#include <asm/machdep.h>\n#include <asm/firmware.h>\n#include <asm/ptrace.h>\n#include <asm/code-patching.h>\n#include <asm/interrupt.h>\n\n#ifdef CONFIG_PPC64\n#include \"internal.h\"\n#endif\n\n#define BHRB_MAX_ENTRIES\t32\n#define BHRB_TARGET\t\t0x0000000000000002\n#define BHRB_PREDICTION\t\t0x0000000000000001\n#define BHRB_EA\t\t\t0xFFFFFFFFFFFFFFFCUL\n\nstruct cpu_hw_events {\n\tint n_events;\n\tint n_percpu;\n\tint disabled;\n\tint n_added;\n\tint n_limited;\n\tu8  pmcs_enabled;\n\tstruct perf_event *event[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int flags[MAX_HWEVENTS];\n\tstruct mmcr_regs mmcr;\n\tstruct perf_event *limited_counter[MAX_LIMITED_HWCOUNTERS];\n\tu8  limited_hwidx[MAX_LIMITED_HWCOUNTERS];\n\tu64 alternatives[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long amasks[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\tunsigned long avalues[MAX_HWEVENTS][MAX_EVENT_ALTERNATIVES];\n\n\tunsigned int txn_flags;\n\tint n_txn_start;\n\n\t/* BHRB bits */\n\tu64\t\t\t\tbhrb_filter;\t/* BHRB HW branch filter */\n\tunsigned int\t\t\tbhrb_users;\n\tvoid\t\t\t\t*bhrb_context;\n\tstruct\tperf_branch_stack\tbhrb_stack;\n\tstruct\tperf_branch_entry\tbhrb_entries[BHRB_MAX_ENTRIES];\n\tu64\t\t\t\tic_init;\n\n\t/* Store the PMC values */\n\tunsigned long pmcs[MAX_HWEVENTS];\n};\n\nstatic DEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\n\nstatic struct power_pmu *ppmu;\n\n/*\n * Normally, to ignore kernel events we set the FCS (freeze counters\n * in supervisor mode) bit in MMCR0, but if the kernel runs with the\n * hypervisor bit set in the MSR, or if we are running on a processor\n * where the hypervisor bit is forced to 1 (as on Apple G5 processors),\n * then we need to use the FCHV bit to ignore kernel events.\n */\nstatic unsigned int freeze_events_kernel = MMCR0_FCS;\n\n/*\n * 32-bit doesn't have MMCRA but does have an MMCR2,\n * and a few other names are different.\n * Also 32-bit doesn't have MMCR3, SIER2 and SIER3.\n * Define them as zero knowing that any code path accessing\n * these registers (via mtspr/mfspr) are done under ppmu flag\n * check for PPMU_ARCH_31 and we will not enter that code path\n * for 32-bit.\n */\n#ifdef CONFIG_PPC32\n\n#define MMCR0_FCHV\t\t0\n#define MMCR0_PMCjCE\t\tMMCR0_PMCnCE\n#define MMCR0_FC56\t\t0\n#define MMCR0_PMAO\t\t0\n#define MMCR0_EBE\t\t0\n#define MMCR0_BHRBA\t\t0\n#define MMCR0_PMCC\t\t0\n#define MMCR0_PMCC_U6\t\t0\n\n#define SPRN_MMCRA\t\tSPRN_MMCR2\n#define SPRN_MMCR3\t\t0\n#define SPRN_SIER2\t\t0\n#define SPRN_SIER3\t\t0\n#define MMCRA_SAMPLE_ENABLE\t0\n#define MMCRA_BHRB_DISABLE     0\n#define MMCR0_PMCCEXT\t\t0\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_get_data_addr(struct perf_event *event, struct pt_regs *regs, u64 *addrp) { }\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\treturn 0;\n}\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tregs->result = 0;\n}\n\nstatic inline int siar_valid(struct pt_regs *regs)\n{\n\treturn 1;\n}\n\nstatic bool is_ebb_event(struct perf_event *event) { return false; }\nstatic int ebb_event_check(struct perf_event *event) { return 0; }\nstatic void ebb_event_add(struct perf_event *event) { }\nstatic void ebb_switch_out(unsigned long mmcr0) { }\nstatic unsigned long ebb_switch_in(bool ebb, struct cpu_hw_events *cpuhw)\n{\n\treturn cpuhw->mmcr.mmcr0;\n}\n\nstatic inline void power_pmu_bhrb_enable(struct perf_event *event) {}\nstatic inline void power_pmu_bhrb_disable(struct perf_event *event) {}\nstatic void power_pmu_sched_task(struct perf_event_context *ctx, bool sched_in) {}\nstatic inline void power_pmu_bhrb_read(struct perf_event *event, struct cpu_hw_events *cpuhw) {}\nstatic void pmao_restore_workaround(bool ebb) { }\n#endif /* CONFIG_PPC32 */\n\nbool is_sier_available(void)\n{\n\tif (!ppmu)\n\t\treturn false;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Return PMC value corresponding to the\n * index passed.\n */\nunsigned long get_pmcs_ext_regs(int idx)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\treturn cpuhw->pmcs[idx];\n}\n\nstatic bool regs_use_siar(struct pt_regs *regs)\n{\n\t/*\n\t * When we take a performance monitor exception the regs are setup\n\t * using perf_read_regs() which overloads some fields, in particular\n\t * regs->result to tell us whether to use SIAR.\n\t *\n\t * However if the regs are from another exception, eg. a syscall, then\n\t * they have not been setup using perf_read_regs() and so regs->result\n\t * is something random.\n\t */\n\treturn ((TRAP(regs) == INTERRUPT_PERFMON) && regs->result);\n}\n\n/*\n * Things that are specific to 64-bit implementations.\n */\n#ifdef CONFIG_PPC64\n\nstatic inline unsigned long perf_ip_adjust(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\n\tif ((ppmu->flags & PPMU_HAS_SSLOT) && (mmcra & MMCRA_SAMPLE_ENABLE)) {\n\t\tunsigned long slot = (mmcra & MMCRA_SLOT) >> MMCRA_SLOT_SHIFT;\n\t\tif (slot > 1)\n\t\t\treturn 4 * (slot - 1);\n\t}\n\n\treturn 0;\n}\n\n/*\n * The user wants a data address recorded.\n * If we're not doing instruction sampling, give them the SDAR\n * (sampled data address).  If we are doing instruction sampling, then\n * only give them the SDAR if it corresponds to the instruction\n * pointed to by SIAR; this is indicated by the [POWER6_]MMCRA_SDSYNC, the\n * [POWER7P_]MMCRA_SDAR_VALID bit in MMCRA, or the SDAR_VALID bit in SIER.\n */\nstatic inline void perf_get_data_addr(struct perf_event *event, struct pt_regs *regs, u64 *addrp)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tbool sdar_valid;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tsdar_valid = regs->dar & SIER_SDAR_VALID;\n\telse {\n\t\tunsigned long sdsync;\n\n\t\tif (ppmu->flags & PPMU_SIAR_VALID)\n\t\t\tsdsync = POWER7P_MMCRA_SDAR_VALID;\n\t\telse if (ppmu->flags & PPMU_ALT_SIPR)\n\t\t\tsdsync = POWER6_MMCRA_SDSYNC;\n\t\telse if (ppmu->flags & PPMU_NO_SIAR)\n\t\t\tsdsync = MMCRA_SAMPLE_ENABLE;\n\t\telse\n\t\t\tsdsync = MMCRA_SDSYNC;\n\n\t\tsdar_valid = mmcra & sdsync;\n\t}\n\n\tif (!(mmcra & MMCRA_SAMPLE_ENABLE) || sdar_valid)\n\t\t*addrp = mfspr(SPRN_SDAR);\n\n\tif (is_kernel_addr(mfspr(SPRN_SDAR)) && event->attr.exclude_kernel)\n\t\t*addrp = 0;\n}\n\nstatic bool regs_sihv(struct pt_regs *regs)\n{\n\tunsigned long sihv = MMCRA_SIHV;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn !!(regs->dar & SIER_SIHV);\n\n\tif (ppmu->flags & PPMU_ALT_SIPR)\n\t\tsihv = POWER6_MMCRA_SIHV;\n\n\treturn !!(regs->dsisr & sihv);\n}\n\nstatic bool regs_sipr(struct pt_regs *regs)\n{\n\tunsigned long sipr = MMCRA_SIPR;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\treturn !!(regs->dar & SIER_SIPR);\n\n\tif (ppmu->flags & PPMU_ALT_SIPR)\n\t\tsipr = POWER6_MMCRA_SIPR;\n\n\treturn !!(regs->dsisr & sipr);\n}\n\nstatic inline u32 perf_flags_from_msr(struct pt_regs *regs)\n{\n\tif (regs->msr & MSR_PR)\n\t\treturn PERF_RECORD_MISC_USER;\n\tif ((regs->msr & MSR_HV) && freeze_events_kernel != MMCR0_FCHV)\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\nstatic inline u32 perf_get_misc_flags(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long mmcra = regs->dsisr;\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\n\tif (!use_siar)\n\t\treturn perf_flags_from_msr(regs);\n\n\t/*\n\t * Check the address in SIAR to identify the\n\t * privilege levels since the SIER[MSR_HV, MSR_PR]\n\t * bits are not set for marked events in power10\n\t * DD1.\n\t */\n\tif (marked && (ppmu->flags & PPMU_P10_DD1)) {\n\t\tunsigned long siar = mfspr(SPRN_SIAR);\n\t\tif (siar) {\n\t\t\tif (is_kernel_addr(siar))\n\t\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\t\treturn PERF_RECORD_MISC_USER;\n\t\t} else {\n\t\t\tif (is_kernel_addr(regs->nip))\n\t\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\t\treturn PERF_RECORD_MISC_USER;\n\t\t}\n\t}\n\n\t/*\n\t * If we don't have flags in MMCRA, rather than using\n\t * the MSR, we intuit the flags from the address in\n\t * SIAR which should give slightly more reliable\n\t * results\n\t */\n\tif (ppmu->flags & PPMU_NO_SIPR) {\n\t\tunsigned long siar = mfspr(SPRN_SIAR);\n\t\tif (is_kernel_addr(siar))\n\t\t\treturn PERF_RECORD_MISC_KERNEL;\n\t\treturn PERF_RECORD_MISC_USER;\n\t}\n\n\t/* PR has priority over HV, so order below is important */\n\tif (regs_sipr(regs))\n\t\treturn PERF_RECORD_MISC_USER;\n\n\tif (regs_sihv(regs) && (freeze_events_kernel != MMCR0_FCHV))\n\t\treturn PERF_RECORD_MISC_HYPERVISOR;\n\n\treturn PERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Overload regs->dsisr to store MMCRA so we only need to read it once\n * on each interrupt.\n * Overload regs->dar to store SIER if we have it.\n * Overload regs->result to specify whether we should use the MSR (result\n * is zero) or the SIAR (result is non zero).\n */\nstatic inline void perf_read_regs(struct pt_regs *regs)\n{\n\tunsigned long mmcra = mfspr(SPRN_MMCRA);\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\tint use_siar;\n\n\tregs->dsisr = mmcra;\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tregs->dar = mfspr(SPRN_SIER);\n\n\t/*\n\t * If this isn't a PMU exception (eg a software event) the SIAR is\n\t * not valid. Use pt_regs.\n\t *\n\t * If it is a marked event use the SIAR.\n\t *\n\t * If the PMU doesn't update the SIAR for non marked events use\n\t * pt_regs.\n\t *\n\t * If the PMU has HV/PR flags then check to see if they\n\t * place the exception in userspace. If so, use pt_regs. In\n\t * continuous sampling mode the SIAR and the PMU exception are\n\t * not synchronised, so they may be many instructions apart.\n\t * This can result in confusing backtraces. We still want\n\t * hypervisor samples as well as samples in the kernel with\n\t * interrupts off hence the userspace check.\n\t */\n\tif (TRAP(regs) != INTERRUPT_PERFMON)\n\t\tuse_siar = 0;\n\telse if ((ppmu->flags & PPMU_NO_SIAR))\n\t\tuse_siar = 0;\n\telse if (marked)\n\t\tuse_siar = 1;\n\telse if ((ppmu->flags & PPMU_NO_CONT_SAMPLING))\n\t\tuse_siar = 0;\n\telse if (!(ppmu->flags & PPMU_NO_SIPR) && regs_sipr(regs))\n\t\tuse_siar = 0;\n\telse\n\t\tuse_siar = 1;\n\n\tregs->result = use_siar;\n}\n\n/*\n * On processors like P7+ that have the SIAR-Valid bit, marked instructions\n * must be sampled only if the SIAR-valid bit is set.\n *\n * For unmarked instructions and for processors that don't have the SIAR-Valid\n * bit, assume that SIAR is valid.\n */\nstatic inline int siar_valid(struct pt_regs *regs)\n{\n\tunsigned long mmcra = regs->dsisr;\n\tint marked = mmcra & MMCRA_SAMPLE_ENABLE;\n\n\tif (marked) {\n\t\t/*\n\t\t * SIER[SIAR_VALID] is not set for some\n\t\t * marked events on power10 DD1, so drop\n\t\t * the check for SIER[SIAR_VALID] and return true.\n\t\t */\n\t\tif (ppmu->flags & PPMU_P10_DD1)\n\t\t\treturn 0x1;\n\t\telse if (ppmu->flags & PPMU_HAS_SIER)\n\t\t\treturn regs->dar & SIER_SIAR_VALID;\n\n\t\tif (ppmu->flags & PPMU_SIAR_VALID)\n\t\t\treturn mmcra & POWER7P_MMCRA_SIAR_VALID;\n\t}\n\n\treturn 1;\n}\n\n\n/* Reset all possible BHRB entries */\nstatic void power_pmu_bhrb_reset(void)\n{\n\tasm volatile(PPC_CLRBHRB);\n}\n\nstatic void power_pmu_bhrb_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\t/* Clear BHRB if we changed task context to avoid data leaks */\n\tif (event->ctx->task && cpuhw->bhrb_context != event->ctx) {\n\t\tpower_pmu_bhrb_reset();\n\t\tcpuhw->bhrb_context = event->ctx;\n\t}\n\tcpuhw->bhrb_users++;\n\tperf_sched_cb_inc(event->ctx->pmu);\n}\n\nstatic void power_pmu_bhrb_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\tWARN_ON_ONCE(!cpuhw->bhrb_users);\n\tcpuhw->bhrb_users--;\n\tperf_sched_cb_dec(event->ctx->pmu);\n\n\tif (!cpuhw->disabled && !cpuhw->bhrb_users) {\n\t\t/* BHRB cannot be turned off when other\n\t\t * events are active on the PMU.\n\t\t */\n\n\t\t/* avoid stale pointer */\n\t\tcpuhw->bhrb_context = NULL;\n\t}\n}\n\n/* Called from ctxsw to prevent one process's branch entries to\n * mingle with the other process's entries during context switch.\n */\nstatic void power_pmu_sched_task(struct perf_event_context *ctx, bool sched_in)\n{\n\tif (!ppmu->bhrb_nr)\n\t\treturn;\n\n\tif (sched_in)\n\t\tpower_pmu_bhrb_reset();\n}\n/* Calculate the to address for a branch */\nstatic __u64 power_pmu_bhrb_to(u64 addr)\n{\n\tunsigned int instr;\n\t__u64 target;\n\n\tif (is_kernel_addr(addr)) {\n\t\tif (copy_from_kernel_nofault(&instr, (void *)addr,\n\t\t\t\tsizeof(instr)))\n\t\t\treturn 0;\n\n\t\treturn branch_target((struct ppc_inst *)&instr);\n\t}\n\n\t/* Userspace: need copy instruction here then translate it */\n\tif (copy_from_user_nofault(&instr, (unsigned int __user *)addr,\n\t\t\tsizeof(instr)))\n\t\treturn 0;\n\n\ttarget = branch_target((struct ppc_inst *)&instr);\n\tif ((!target) || (instr & BRANCH_ABSOLUTE))\n\t\treturn target;\n\n\t/* Translate relative branch target from kernel to user address */\n\treturn target - (unsigned long)&instr + addr;\n}\n\n/* Processing BHRB entries */\nstatic void power_pmu_bhrb_read(struct perf_event *event, struct cpu_hw_events *cpuhw)\n{\n\tu64 val;\n\tu64 addr;\n\tint r_index, u_index, pred;\n\n\tr_index = 0;\n\tu_index = 0;\n\twhile (r_index < ppmu->bhrb_nr) {\n\t\t/* Assembly read function */\n\t\tval = read_bhrb(r_index++);\n\t\tif (!val)\n\t\t\t/* Terminal marker: End of valid BHRB entries */\n\t\t\tbreak;\n\t\telse {\n\t\t\taddr = val & BHRB_EA;\n\t\t\tpred = val & BHRB_PREDICTION;\n\n\t\t\tif (!addr)\n\t\t\t\t/* invalid entry */\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * BHRB rolling buffer could very much contain the kernel\n\t\t\t * addresses at this point. Check the privileges before\n\t\t\t * exporting it to userspace (avoid exposure of regions\n\t\t\t * where we could have speculative execution)\n\t\t\t * Incase of ISA v3.1, BHRB will capture only user-space\n\t\t\t * addresses, hence include a check before filtering code\n\t\t\t */\n\t\t\tif (!(ppmu->flags & PPMU_ARCH_31) &&\n\t\t\t    is_kernel_addr(addr) && event->attr.exclude_kernel)\n\t\t\t\tcontinue;\n\n\t\t\t/* Branches are read most recent first (ie. mfbhrb 0 is\n\t\t\t * the most recent branch).\n\t\t\t * There are two types of valid entries:\n\t\t\t * 1) a target entry which is the to address of a\n\t\t\t *    computed goto like a blr,bctr,btar.  The next\n\t\t\t *    entry read from the bhrb will be branch\n\t\t\t *    corresponding to this target (ie. the actual\n\t\t\t *    blr/bctr/btar instruction).\n\t\t\t * 2) a from address which is an actual branch.  If a\n\t\t\t *    target entry proceeds this, then this is the\n\t\t\t *    matching branch for that target.  If this is not\n\t\t\t *    following a target entry, then this is a branch\n\t\t\t *    where the target is given as an immediate field\n\t\t\t *    in the instruction (ie. an i or b form branch).\n\t\t\t *    In this case we need to read the instruction from\n\t\t\t *    memory to determine the target/to address.\n\t\t\t */\n\n\t\t\tif (val & BHRB_TARGET) {\n\t\t\t\t/* Target branches use two entries\n\t\t\t\t * (ie. computed gotos/XL form)\n\t\t\t\t */\n\t\t\t\tcpuhw->bhrb_entries[u_index].to = addr;\n\t\t\t\tcpuhw->bhrb_entries[u_index].mispred = pred;\n\t\t\t\tcpuhw->bhrb_entries[u_index].predicted = ~pred;\n\n\t\t\t\t/* Get from address in next entry */\n\t\t\t\tval = read_bhrb(r_index++);\n\t\t\t\taddr = val & BHRB_EA;\n\t\t\t\tif (val & BHRB_TARGET) {\n\t\t\t\t\t/* Shouldn't have two targets in a\n\t\t\t\t\t   row.. Reset index and try again */\n\t\t\t\t\tr_index--;\n\t\t\t\t\taddr = 0;\n\t\t\t\t}\n\t\t\t\tcpuhw->bhrb_entries[u_index].from = addr;\n\t\t\t} else {\n\t\t\t\t/* Branches to immediate field \n\t\t\t\t   (ie I or B form) */\n\t\t\t\tcpuhw->bhrb_entries[u_index].from = addr;\n\t\t\t\tcpuhw->bhrb_entries[u_index].to =\n\t\t\t\t\tpower_pmu_bhrb_to(addr);\n\t\t\t\tcpuhw->bhrb_entries[u_index].mispred = pred;\n\t\t\t\tcpuhw->bhrb_entries[u_index].predicted = ~pred;\n\t\t\t}\n\t\t\tu_index++;\n\n\t\t}\n\t}\n\tcpuhw->bhrb_stack.nr = u_index;\n\tcpuhw->bhrb_stack.hw_idx = -1ULL;\n\treturn;\n}\n\nstatic bool is_ebb_event(struct perf_event *event)\n{\n\t/*\n\t * This could be a per-PMU callback, but we'd rather avoid the cost. We\n\t * check that the PMU supports EBB, meaning those that don't can still\n\t * use bit 63 of the event code for something else if they wish.\n\t */\n\treturn (ppmu->flags & PPMU_ARCH_207S) &&\n\t       ((event->attr.config >> PERF_EVENT_CONFIG_EBB_SHIFT) & 1);\n}\n\nstatic int ebb_event_check(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\n\t/* Event and group leader must agree on EBB */\n\tif (is_ebb_event(leader) != is_ebb_event(event))\n\t\treturn -EINVAL;\n\n\tif (is_ebb_event(event)) {\n\t\tif (!(event->attach_state & PERF_ATTACH_TASK))\n\t\t\treturn -EINVAL;\n\n\t\tif (!leader->attr.pinned || !leader->attr.exclusive)\n\t\t\treturn -EINVAL;\n\n\t\tif (event->attr.freq ||\n\t\t    event->attr.inherit ||\n\t\t    event->attr.sample_type ||\n\t\t    event->attr.sample_period ||\n\t\t    event->attr.enable_on_exec)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void ebb_event_add(struct perf_event *event)\n{\n\tif (!is_ebb_event(event) || current->thread.used_ebb)\n\t\treturn;\n\n\t/*\n\t * IFF this is the first time we've added an EBB event, set\n\t * PMXE in the user MMCR0 so we can detect when it's cleared by\n\t * userspace. We need this so that we can context switch while\n\t * userspace is in the EBB handler (where PMXE is 0).\n\t */\n\tcurrent->thread.used_ebb = 1;\n\tcurrent->thread.mmcr0 |= MMCR0_PMXE;\n}\n\nstatic void ebb_switch_out(unsigned long mmcr0)\n{\n\tif (!(mmcr0 & MMCR0_EBE))\n\t\treturn;\n\n\tcurrent->thread.siar  = mfspr(SPRN_SIAR);\n\tcurrent->thread.sier  = mfspr(SPRN_SIER);\n\tcurrent->thread.sdar  = mfspr(SPRN_SDAR);\n\tcurrent->thread.mmcr0 = mmcr0 & MMCR0_USER_MASK;\n\tcurrent->thread.mmcr2 = mfspr(SPRN_MMCR2) & MMCR2_USER_MASK;\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tcurrent->thread.mmcr3 = mfspr(SPRN_MMCR3);\n\t\tcurrent->thread.sier2 = mfspr(SPRN_SIER2);\n\t\tcurrent->thread.sier3 = mfspr(SPRN_SIER3);\n\t}\n}\n\nstatic unsigned long ebb_switch_in(bool ebb, struct cpu_hw_events *cpuhw)\n{\n\tunsigned long mmcr0 = cpuhw->mmcr.mmcr0;\n\n\tif (!ebb)\n\t\tgoto out;\n\n\t/* Enable EBB and read/write to all 6 PMCs and BHRB for userspace */\n\tmmcr0 |= MMCR0_EBE | MMCR0_BHRBA | MMCR0_PMCC_U6;\n\n\t/*\n\t * Add any bits from the user MMCR0, FC or PMAO. This is compatible\n\t * with pmao_restore_workaround() because we may add PMAO but we never\n\t * clear it here.\n\t */\n\tmmcr0 |= current->thread.mmcr0;\n\n\t/*\n\t * Be careful not to set PMXE if userspace had it cleared. This is also\n\t * compatible with pmao_restore_workaround() because it has already\n\t * cleared PMXE and we leave PMAO alone.\n\t */\n\tif (!(current->thread.mmcr0 & MMCR0_PMXE))\n\t\tmmcr0 &= ~MMCR0_PMXE;\n\n\tmtspr(SPRN_SIAR, current->thread.siar);\n\tmtspr(SPRN_SIER, current->thread.sier);\n\tmtspr(SPRN_SDAR, current->thread.sdar);\n\n\t/*\n\t * Merge the kernel & user values of MMCR2. The semantics we implement\n\t * are that the user MMCR2 can set bits, ie. cause counters to freeze,\n\t * but not clear bits. If a task wants to be able to clear bits, ie.\n\t * unfreeze counters, it should not set exclude_xxx in its events and\n\t * instead manage the MMCR2 entirely by itself.\n\t */\n\tmtspr(SPRN_MMCR2, cpuhw->mmcr.mmcr2 | current->thread.mmcr2);\n\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tmtspr(SPRN_MMCR3, current->thread.mmcr3);\n\t\tmtspr(SPRN_SIER2, current->thread.sier2);\n\t\tmtspr(SPRN_SIER3, current->thread.sier3);\n\t}\nout:\n\treturn mmcr0;\n}\n\nstatic void pmao_restore_workaround(bool ebb)\n{\n\tunsigned pmcs[6];\n\n\tif (!cpu_has_feature(CPU_FTR_PMAO_BUG))\n\t\treturn;\n\n\t/*\n\t * On POWER8E there is a hardware defect which affects the PMU context\n\t * switch logic, ie. power_pmu_disable/enable().\n\t *\n\t * When a counter overflows PMXE is cleared and FC/PMAO is set in MMCR0\n\t * by the hardware. Sometime later the actual PMU exception is\n\t * delivered.\n\t *\n\t * If we context switch, or simply disable/enable, the PMU prior to the\n\t * exception arriving, the exception will be lost when we clear PMAO.\n\t *\n\t * When we reenable the PMU, we will write the saved MMCR0 with PMAO\n\t * set, and this _should_ generate an exception. However because of the\n\t * defect no exception is generated when we write PMAO, and we get\n\t * stuck with no counters counting but no exception delivered.\n\t *\n\t * The workaround is to detect this case and tweak the hardware to\n\t * create another pending PMU exception.\n\t *\n\t * We do that by setting up PMC6 (cycles) for an imminent overflow and\n\t * enabling the PMU. That causes a new exception to be generated in the\n\t * chip, but we don't take it yet because we have interrupts hard\n\t * disabled. We then write back the PMU state as we want it to be seen\n\t * by the exception handler. When we reenable interrupts the exception\n\t * handler will be called and see the correct state.\n\t *\n\t * The logic is the same for EBB, except that the exception is gated by\n\t * us having interrupts hard disabled as well as the fact that we are\n\t * not in userspace. The exception is finally delivered when we return\n\t * to userspace.\n\t */\n\n\t/* Only if PMAO is set and PMAO_SYNC is clear */\n\tif ((current->thread.mmcr0 & (MMCR0_PMAO | MMCR0_PMAO_SYNC)) != MMCR0_PMAO)\n\t\treturn;\n\n\t/* If we're doing EBB, only if BESCR[GE] is set */\n\tif (ebb && !(current->thread.bescr & BESCR_GE))\n\t\treturn;\n\n\t/*\n\t * We are already soft-disabled in power_pmu_enable(). We need to hard\n\t * disable to actually prevent the PMU exception from firing.\n\t */\n\thard_irq_disable();\n\n\t/*\n\t * This is a bit gross, but we know we're on POWER8E and have 6 PMCs.\n\t * Using read/write_pmc() in a for loop adds 12 function calls and\n\t * almost doubles our code size.\n\t */\n\tpmcs[0] = mfspr(SPRN_PMC1);\n\tpmcs[1] = mfspr(SPRN_PMC2);\n\tpmcs[2] = mfspr(SPRN_PMC3);\n\tpmcs[3] = mfspr(SPRN_PMC4);\n\tpmcs[4] = mfspr(SPRN_PMC5);\n\tpmcs[5] = mfspr(SPRN_PMC6);\n\n\t/* Ensure all freeze bits are unset */\n\tmtspr(SPRN_MMCR2, 0);\n\n\t/* Set up PMC6 to overflow in one cycle */\n\tmtspr(SPRN_PMC6, 0x7FFFFFFE);\n\n\t/* Enable exceptions and unfreeze PMC6 */\n\tmtspr(SPRN_MMCR0, MMCR0_PMXE | MMCR0_PMCjCE | MMCR0_PMAO);\n\n\t/* Now we need to refreeze and restore the PMCs */\n\tmtspr(SPRN_MMCR0, MMCR0_FC | MMCR0_PMAO);\n\n\tmtspr(SPRN_PMC1, pmcs[0]);\n\tmtspr(SPRN_PMC2, pmcs[1]);\n\tmtspr(SPRN_PMC3, pmcs[2]);\n\tmtspr(SPRN_PMC4, pmcs[3]);\n\tmtspr(SPRN_PMC5, pmcs[4]);\n\tmtspr(SPRN_PMC6, pmcs[5]);\n}\n\n#endif /* CONFIG_PPC64 */\n\nstatic void perf_event_interrupt(struct pt_regs *regs);\n\n/*\n * Read one performance monitor counter (PMC).\n */\nstatic unsigned long read_pmc(int idx)\n{\n\tunsigned long val;\n\n\tswitch (idx) {\n\tcase 1:\n\t\tval = mfspr(SPRN_PMC1);\n\t\tbreak;\n\tcase 2:\n\t\tval = mfspr(SPRN_PMC2);\n\t\tbreak;\n\tcase 3:\n\t\tval = mfspr(SPRN_PMC3);\n\t\tbreak;\n\tcase 4:\n\t\tval = mfspr(SPRN_PMC4);\n\t\tbreak;\n\tcase 5:\n\t\tval = mfspr(SPRN_PMC5);\n\t\tbreak;\n\tcase 6:\n\t\tval = mfspr(SPRN_PMC6);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tval = mfspr(SPRN_PMC7);\n\t\tbreak;\n\tcase 8:\n\t\tval = mfspr(SPRN_PMC8);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to read PMC%d\\n\", idx);\n\t\tval = 0;\n\t}\n\treturn val;\n}\n\n/*\n * Write one PMC.\n */\nstatic void write_pmc(int idx, unsigned long val)\n{\n\tswitch (idx) {\n\tcase 1:\n\t\tmtspr(SPRN_PMC1, val);\n\t\tbreak;\n\tcase 2:\n\t\tmtspr(SPRN_PMC2, val);\n\t\tbreak;\n\tcase 3:\n\t\tmtspr(SPRN_PMC3, val);\n\t\tbreak;\n\tcase 4:\n\t\tmtspr(SPRN_PMC4, val);\n\t\tbreak;\n\tcase 5:\n\t\tmtspr(SPRN_PMC5, val);\n\t\tbreak;\n\tcase 6:\n\t\tmtspr(SPRN_PMC6, val);\n\t\tbreak;\n#ifdef CONFIG_PPC64\n\tcase 7:\n\t\tmtspr(SPRN_PMC7, val);\n\t\tbreak;\n\tcase 8:\n\t\tmtspr(SPRN_PMC8, val);\n\t\tbreak;\n#endif /* CONFIG_PPC64 */\n\tdefault:\n\t\tprintk(KERN_ERR \"oops trying to write PMC%d\\n\", idx);\n\t}\n}\n\n/* Called from sysrq_handle_showregs() */\nvoid perf_event_print_debug(void)\n{\n\tunsigned long sdar, sier, flags;\n\tu32 pmcs[MAX_HWEVENTS];\n\tint i;\n\n\tif (!ppmu) {\n\t\tpr_info(\"Performance monitor hardware not registered.\\n\");\n\t\treturn;\n\t}\n\n\tif (!ppmu->n_counter)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpr_info(\"CPU: %d PMU registers, ppmu = %s n_counters = %d\",\n\t\t smp_processor_id(), ppmu->name, ppmu->n_counter);\n\n\tfor (i = 0; i < ppmu->n_counter; i++)\n\t\tpmcs[i] = read_pmc(i + 1);\n\n\tfor (; i < MAX_HWEVENTS; i++)\n\t\tpmcs[i] = 0xdeadbeef;\n\n\tpr_info(\"PMC1:  %08x PMC2: %08x PMC3: %08x PMC4: %08x\\n\",\n\t\t pmcs[0], pmcs[1], pmcs[2], pmcs[3]);\n\n\tif (ppmu->n_counter > 4)\n\t\tpr_info(\"PMC5:  %08x PMC6: %08x PMC7: %08x PMC8: %08x\\n\",\n\t\t\t pmcs[4], pmcs[5], pmcs[6], pmcs[7]);\n\n\tpr_info(\"MMCR0: %016lx MMCR1: %016lx MMCRA: %016lx\\n\",\n\t\tmfspr(SPRN_MMCR0), mfspr(SPRN_MMCR1), mfspr(SPRN_MMCRA));\n\n\tsdar = sier = 0;\n#ifdef CONFIG_PPC64\n\tsdar = mfspr(SPRN_SDAR);\n\n\tif (ppmu->flags & PPMU_HAS_SIER)\n\t\tsier = mfspr(SPRN_SIER);\n\n\tif (ppmu->flags & PPMU_ARCH_207S) {\n\t\tpr_info(\"MMCR2: %016lx EBBHR: %016lx\\n\",\n\t\t\tmfspr(SPRN_MMCR2), mfspr(SPRN_EBBHR));\n\t\tpr_info(\"EBBRR: %016lx BESCR: %016lx\\n\",\n\t\t\tmfspr(SPRN_EBBRR), mfspr(SPRN_BESCR));\n\t}\n\n\tif (ppmu->flags & PPMU_ARCH_31) {\n\t\tpr_info(\"MMCR3: %016lx SIER2: %016lx SIER3: %016lx\\n\",\n\t\t\tmfspr(SPRN_MMCR3), mfspr(SPRN_SIER2), mfspr(SPRN_SIER3));\n\t}\n#endif\n\tpr_info(\"SIAR:  %016lx SDAR:  %016lx SIER:  %016lx\\n\",\n\t\tmfspr(SPRN_SIAR), sdar, sier);\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Check if a set of events can all go on the PMU at once.\n * If they can't, this will look at alternative codes for the events\n * and see if any combination of alternative codes is feasible.\n * The feasible set is returned in event_id[].\n */\nstatic int power_check_constraints(struct cpu_hw_events *cpuhw,\n\t\t\t\t   u64 event_id[], unsigned int cflags[],\n\t\t\t\t   int n_ev, struct perf_event **event)\n{\n\tunsigned long mask, value, nv;\n\tunsigned long smasks[MAX_HWEVENTS], svalues[MAX_HWEVENTS];\n\tint n_alt[MAX_HWEVENTS], choice[MAX_HWEVENTS];\n\tint i, j;\n\tunsigned long addf = ppmu->add_fields;\n\tunsigned long tadd = ppmu->test_adder;\n\tunsigned long grp_mask = ppmu->group_constraint_mask;\n\tunsigned long grp_val = ppmu->group_constraint_val;\n\n\tif (n_ev > ppmu->n_counter)\n\t\treturn -1;\n\n\t/* First see if the events will go on as-is */\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tif ((cflags[i] & PPMU_LIMITED_PMC_REQD)\n\t\t    && !ppmu->limited_pmc_event(event_id[i])) {\n\t\t\tppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t       cpuhw->alternatives[i]);\n\t\t\tevent_id[i] = cpuhw->alternatives[i][0];\n\t\t}\n\t\tif (ppmu->get_constraint(event_id[i], &cpuhw->amasks[i][0],\n\t\t\t\t\t &cpuhw->avalues[i][0], event[i]->attr.config1))\n\t\t\treturn -1;\n\t}\n\tvalue = mask = 0;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tnv = (value | cpuhw->avalues[i][0]) +\n\t\t\t(value & cpuhw->avalues[i][0] & addf);\n\n\t\tif (((((nv + tadd) ^ value) & mask) & (~grp_mask)) != 0)\n\t\t\tbreak;\n\n\t\tif (((((nv + tadd) ^ cpuhw->avalues[i][0]) & cpuhw->amasks[i][0])\n\t\t\t& (~grp_mask)) != 0)\n\t\t\tbreak;\n\n\t\tvalue = nv;\n\t\tmask |= cpuhw->amasks[i][0];\n\t}\n\tif (i == n_ev) {\n\t\tif ((value & mask & grp_mask) != (mask & grp_val))\n\t\t\treturn -1;\n\t\telse\n\t\t\treturn 0;\t/* all OK */\n\t}\n\n\t/* doesn't work, gather alternatives... */\n\tif (!ppmu->get_alternatives)\n\t\treturn -1;\n\tfor (i = 0; i < n_ev; ++i) {\n\t\tchoice[i] = 0;\n\t\tn_alt[i] = ppmu->get_alternatives(event_id[i], cflags[i],\n\t\t\t\t\t\t  cpuhw->alternatives[i]);\n\t\tfor (j = 1; j < n_alt[i]; ++j)\n\t\t\tppmu->get_constraint(cpuhw->alternatives[i][j],\n\t\t\t\t\t     &cpuhw->amasks[i][j],\n\t\t\t\t\t     &cpuhw->avalues[i][j],\n\t\t\t\t\t     event[i]->attr.config1);\n\t}\n\n\t/* enumerate all possibilities and see if any will work */\n\ti = 0;\n\tj = -1;\n\tvalue = mask = nv = 0;\n\twhile (i < n_ev) {\n\t\tif (j >= 0) {\n\t\t\t/* we're backtracking, restore context */\n\t\t\tvalue = svalues[i];\n\t\t\tmask = smasks[i];\n\t\t\tj = choice[i];\n\t\t}\n\t\t/*\n\t\t * See if any alternative k for event_id i,\n\t\t * where k > j, will satisfy the constraints.\n\t\t */\n\t\twhile (++j < n_alt[i]) {\n\t\t\tnv = (value | cpuhw->avalues[i][j]) +\n\t\t\t\t(value & cpuhw->avalues[i][j] & addf);\n\t\t\tif ((((nv + tadd) ^ value) & mask) == 0 &&\n\t\t\t    (((nv + tadd) ^ cpuhw->avalues[i][j])\n\t\t\t     & cpuhw->amasks[i][j]) == 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= n_alt[i]) {\n\t\t\t/*\n\t\t\t * No feasible alternative, backtrack\n\t\t\t * to event_id i-1 and continue enumerating its\n\t\t\t * alternatives from where we got up to.\n\t\t\t */\n\t\t\tif (--i < 0)\n\t\t\t\treturn -1;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Found a feasible alternative for event_id i,\n\t\t\t * remember where we got up to with this event_id,\n\t\t\t * go on to the next event_id, and start with\n\t\t\t * the first alternative for it.\n\t\t\t */\n\t\t\tchoice[i] = j;\n\t\t\tsvalues[i] = value;\n\t\t\tsmasks[i] = mask;\n\t\t\tvalue = nv;\n\t\t\tmask |= cpuhw->amasks[i][j];\n\t\t\t++i;\n\t\t\tj = -1;\n\t\t}\n\t}\n\n\t/* OK, we have a feasible combination, tell the caller the solution */\n\tfor (i = 0; i < n_ev; ++i)\n\t\tevent_id[i] = cpuhw->alternatives[i][choice[i]];\n\treturn 0;\n}\n\n/*\n * Check if newly-added events have consistent settings for\n * exclude_{user,kernel,hv} with each other and any previously\n * added events.\n */\nstatic int check_excludes(struct perf_event **ctrs, unsigned int cflags[],\n\t\t\t  int n_prev, int n_new)\n{\n\tint eu = 0, ek = 0, eh = 0;\n\tint i, n, first;\n\tstruct perf_event *event;\n\n\t/*\n\t * If the PMU we're on supports per event exclude settings then we\n\t * don't need to do any of this logic. NB. This assumes no PMU has both\n\t * per event exclude and limited PMCs.\n\t */\n\tif (ppmu->flags & PPMU_ARCH_207S)\n\t\treturn 0;\n\n\tn = n_prev + n_new;\n\tif (n <= 1)\n\t\treturn 0;\n\n\tfirst = 1;\n\tfor (i = 0; i < n; ++i) {\n\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK) {\n\t\t\tcflags[i] &= ~PPMU_LIMITED_PMC_REQD;\n\t\t\tcontinue;\n\t\t}\n\t\tevent = ctrs[i];\n\t\tif (first) {\n\t\t\teu = event->attr.exclude_user;\n\t\t\tek = event->attr.exclude_kernel;\n\t\t\teh = event->attr.exclude_hv;\n\t\t\tfirst = 0;\n\t\t} else if (event->attr.exclude_user != eu ||\n\t\t\t   event->attr.exclude_kernel != ek ||\n\t\t\t   event->attr.exclude_hv != eh) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\tif (eu || ek || eh)\n\t\tfor (i = 0; i < n; ++i)\n\t\t\tif (cflags[i] & PPMU_LIMITED_PMC_OK)\n\t\t\t\tcflags[i] |= PPMU_LIMITED_PMC_REQD;\n\n\treturn 0;\n}\n\nstatic u64 check_and_compute_delta(u64 prev, u64 val)\n{\n\tu64 delta = (val - prev) & 0xfffffffful;\n\n\t/*\n\t * POWER7 can roll back counter values, if the new value is smaller\n\t * than the previous value it will cause the delta and the counter to\n\t * have bogus values unless we rolled a counter over.  If a coutner is\n\t * rolled back, it will be smaller, but within 256, which is the maximum\n\t * number of events to rollback at once.  If we detect a rollback\n\t * return 0.  This can lead to a small lack of precision in the\n\t * counters.\n\t */\n\tif (prev > val && (prev - val) < 256)\n\t\tdelta = 0;\n\n\treturn delta;\n}\n\nstatic void power_pmu_read(struct perf_event *event)\n{\n\ts64 val, delta, prev;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (!event->hw.idx)\n\t\treturn;\n\n\tif (is_ebb_event(event)) {\n\t\tval = read_pmc(event->hw.idx);\n\t\tlocal64_set(&event->hw.prev_count, val);\n\t\treturn;\n\t}\n\n\t/*\n\t * Performance monitor interrupts come even when interrupts\n\t * are soft-disabled, as long as interrupts are hard-enabled.\n\t * Therefore we treat them like NMIs.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tbarrier();\n\t\tval = read_pmc(event->hw.idx);\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (!delta)\n\t\t\treturn;\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, val) != prev);\n\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * A number of places program the PMC with (0x80000000 - period_left).\n\t * We never want period_left to be less than 1 because we will program\n\t * the PMC with a value >= 0x800000000 and an edge detected PMC will\n\t * roll around to 0 before taking an exception. We have seen this\n\t * on POWER8.\n\t *\n\t * To fix this, clamp the minimum value of period_left to 1.\n\t */\n\tdo {\n\t\tprev = local64_read(&event->hw.period_left);\n\t\tval = prev - delta;\n\t\tif (val < 1)\n\t\t\tval = 1;\n\t} while (local64_cmpxchg(&event->hw.period_left, prev, val) != prev);\n}\n\n/*\n * On some machines, PMC5 and PMC6 can't be written, don't respect\n * the freeze conditions, and don't generate interrupts.  This tells\n * us if `event' is using such a PMC.\n */\nstatic int is_limited_pmc(int pmcnum)\n{\n\treturn (ppmu->flags & PPMU_LIMITED_PMC5_6)\n\t\t&& (pmcnum == 5 || pmcnum == 6);\n}\n\nstatic void freeze_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t    unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev, delta;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tif (!event->hw.idx)\n\t\t\tcontinue;\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tevent->hw.idx = 0;\n\t\tdelta = check_and_compute_delta(prev, val);\n\t\tif (delta)\n\t\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void thaw_limited_counters(struct cpu_hw_events *cpuhw,\n\t\t\t\t  unsigned long pmc5, unsigned long pmc6)\n{\n\tstruct perf_event *event;\n\tu64 val, prev;\n\tint i;\n\n\tfor (i = 0; i < cpuhw->n_limited; ++i) {\n\t\tevent = cpuhw->limited_counter[i];\n\t\tevent->hw.idx = cpuhw->limited_hwidx[i];\n\t\tval = (event->hw.idx == 5) ? pmc5 : pmc6;\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tif (check_and_compute_delta(prev, val))\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\n/*\n * Since limited events don't respect the freeze conditions, we\n * have to read them immediately after freezing or unfreezing the\n * other events.  We try to keep the values from the limited\n * events as consistent as possible by keeping the delay (in\n * cycles and instructions) between freezing/unfreezing and reading\n * the limited events as small and consistent as possible.\n * Therefore, if any limited events are in use, we read them\n * both, and always in the same order, to minimize variability,\n * and do it inside the same asm that writes MMCR0.\n */\nstatic void write_mmcr0(struct cpu_hw_events *cpuhw, unsigned long mmcr0)\n{\n\tunsigned long pmc5, pmc6;\n\n\tif (!cpuhw->n_limited) {\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n\t\treturn;\n\t}\n\n\t/*\n\t * Write MMCR0, then read PMC5 and PMC6 immediately.\n\t * To ensure we don't get a performance monitor interrupt\n\t * between writing MMCR0 and freezing/thawing the limited\n\t * events, we first write MMCR0 with the event overflow\n\t * interrupt enable bits turned off.\n\t */\n\tasm volatile(\"mtspr %3,%2; mfspr %0,%4; mfspr %1,%5\"\n\t\t     : \"=&r\" (pmc5), \"=&r\" (pmc6)\n\t\t     : \"r\" (mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE)),\n\t\t       \"i\" (SPRN_MMCR0),\n\t\t       \"i\" (SPRN_PMC5), \"i\" (SPRN_PMC6));\n\n\tif (mmcr0 & MMCR0_FC)\n\t\tfreeze_limited_counters(cpuhw, pmc5, pmc6);\n\telse\n\t\tthaw_limited_counters(cpuhw, pmc5, pmc6);\n\n\t/*\n\t * Write the full MMCR0 including the event overflow interrupt\n\t * enable bits, if necessary.\n\t */\n\tif (mmcr0 & (MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\tmtspr(SPRN_MMCR0, mmcr0);\n}\n\n/*\n * Disable all events to prevent PMU interrupts and to allow\n * events to be added or removed.\n */\nstatic void power_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags, mmcr0, val, mmcra;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!cpuhw->disabled) {\n\t\t/*\n\t\t * Check if we ever enabled the PMU on this cpu.\n\t\t */\n\t\tif (!cpuhw->pmcs_enabled) {\n\t\t\tppc_enable_pmcs();\n\t\t\tcpuhw->pmcs_enabled = 1;\n\t\t}\n\n\t\t/*\n\t\t * Set the 'freeze counters' bit, clear EBE/BHRBA/PMCC/PMAO/FC56\n\t\t */\n\t\tval  = mmcr0 = mfspr(SPRN_MMCR0);\n\t\tval |= MMCR0_FC;\n\t\tval &= ~(MMCR0_EBE | MMCR0_BHRBA | MMCR0_PMCC | MMCR0_PMAO |\n\t\t\t MMCR0_FC56);\n\t\t/* Set mmcr0 PMCCEXT for p10 */\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tval |= MMCR0_PMCCEXT;\n\n\t\t/*\n\t\t * The barrier is to make sure the mtspr has been\n\t\t * executed and the PMU has frozen the events etc.\n\t\t * before we return.\n\t\t */\n\t\twrite_mmcr0(cpuhw, val);\n\t\tmb();\n\t\tisync();\n\n\t\tval = mmcra = cpuhw->mmcr.mmcra;\n\n\t\t/*\n\t\t * Disable instruction sampling if it was enabled\n\t\t */\n\t\tif (cpuhw->mmcr.mmcra & MMCRA_SAMPLE_ENABLE)\n\t\t\tval &= ~MMCRA_SAMPLE_ENABLE;\n\n\t\t/* Disable BHRB via mmcra (BHRBRD) for p10 */\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tval |= MMCRA_BHRB_DISABLE;\n\n\t\t/*\n\t\t * Write SPRN_MMCRA if mmcra has either disabled\n\t\t * instruction sampling or BHRB.\n\t\t */\n\t\tif (val != mmcra) {\n\t\t\tmtspr(SPRN_MMCRA, mmcra);\n\t\t\tmb();\n\t\t\tisync();\n\t\t}\n\n\t\tcpuhw->disabled = 1;\n\t\tcpuhw->n_added = 0;\n\n\t\tebb_switch_out(mmcr0);\n\n#ifdef CONFIG_PPC64\n\t\t/*\n\t\t * These are readable by userspace, may contain kernel\n\t\t * addresses and are not switched by context switch, so clear\n\t\t * them now to avoid leaking anything to userspace in general\n\t\t * including to another process.\n\t\t */\n\t\tif (ppmu->flags & PPMU_ARCH_207S) {\n\t\t\tmtspr(SPRN_SDAR, 0);\n\t\t\tmtspr(SPRN_SIAR, 0);\n\t\t}\n#endif\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Re-enable all events if disable == 0.\n * If we were previously disabled and events were added, then\n * put the new config on the PMU.\n */\nstatic void power_pmu_enable(struct pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tlong i;\n\tunsigned long val, mmcr0;\n\ts64 left;\n\tunsigned int hwc_index[MAX_HWEVENTS];\n\tint n_lim;\n\tint idx;\n\tbool ebb;\n\n\tif (!ppmu)\n\t\treturn;\n\tlocal_irq_save(flags);\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tif (!cpuhw->disabled)\n\t\tgoto out;\n\n\tif (cpuhw->n_events == 0) {\n\t\tppc_set_pmu_inuse(0);\n\t\tgoto out;\n\t}\n\n\tcpuhw->disabled = 0;\n\n\t/*\n\t * EBB requires an exclusive group and all events must have the EBB\n\t * flag set, or not set, so we can just check a single event. Also we\n\t * know we have at least one event.\n\t */\n\tebb = is_ebb_event(cpuhw->event[0]);\n\n\t/*\n\t * If we didn't change anything, or only removed events,\n\t * no need to recalculate MMCR* settings and reset the PMCs.\n\t * Just reenable the PMU with the current MMCR* settings\n\t * (possibly updated for removal of events).\n\t */\n\tif (!cpuhw->n_added) {\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra & ~MMCRA_SAMPLE_ENABLE);\n\t\tmtspr(SPRN_MMCR1, cpuhw->mmcr.mmcr1);\n\t\tif (ppmu->flags & PPMU_ARCH_31)\n\t\t\tmtspr(SPRN_MMCR3, cpuhw->mmcr.mmcr3);\n\t\tgoto out_enable;\n\t}\n\n\t/*\n\t * Clear all MMCR settings and recompute them for the new set of events.\n\t */\n\tmemset(&cpuhw->mmcr, 0, sizeof(cpuhw->mmcr));\n\n\tif (ppmu->compute_mmcr(cpuhw->events, cpuhw->n_events, hwc_index,\n\t\t\t       &cpuhw->mmcr, cpuhw->event, ppmu->flags)) {\n\t\t/* shouldn't ever get here */\n\t\tprintk(KERN_ERR \"oops compute_mmcr failed\\n\");\n\t\tgoto out;\n\t}\n\n\tif (!(ppmu->flags & PPMU_ARCH_207S)) {\n\t\t/*\n\t\t * Add in MMCR0 freeze bits corresponding to the attr.exclude_*\n\t\t * bits for the first event. We have already checked that all\n\t\t * events have the same value for these bits as the first event.\n\t\t */\n\t\tevent = cpuhw->event[0];\n\t\tif (event->attr.exclude_user)\n\t\t\tcpuhw->mmcr.mmcr0 |= MMCR0_FCP;\n\t\tif (event->attr.exclude_kernel)\n\t\t\tcpuhw->mmcr.mmcr0 |= freeze_events_kernel;\n\t\tif (event->attr.exclude_hv)\n\t\t\tcpuhw->mmcr.mmcr0 |= MMCR0_FCHV;\n\t}\n\n\t/*\n\t * Write the new configuration to MMCR* with the freeze\n\t * bit set and set the hardware events to their initial values.\n\t * Then unfreeze the events.\n\t */\n\tppc_set_pmu_inuse(1);\n\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra & ~MMCRA_SAMPLE_ENABLE);\n\tmtspr(SPRN_MMCR1, cpuhw->mmcr.mmcr1);\n\tmtspr(SPRN_MMCR0, (cpuhw->mmcr.mmcr0 & ~(MMCR0_PMC1CE | MMCR0_PMCjCE))\n\t\t\t\t| MMCR0_FC);\n\tif (ppmu->flags & PPMU_ARCH_207S)\n\t\tmtspr(SPRN_MMCR2, cpuhw->mmcr.mmcr2);\n\n\tif (ppmu->flags & PPMU_ARCH_31)\n\t\tmtspr(SPRN_MMCR3, cpuhw->mmcr.mmcr3);\n\n\t/*\n\t * Read off any pre-existing events that need to move\n\t * to another PMC.\n\t */\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx && event->hw.idx != hwc_index[i] + 1) {\n\t\t\tpower_pmu_read(event);\n\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\tevent->hw.idx = 0;\n\t\t}\n\t}\n\n\t/*\n\t * Initialize the PMCs for all the new and moved events.\n\t */\n\tcpuhw->n_limited = n_lim = 0;\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tevent = cpuhw->event[i];\n\t\tif (event->hw.idx)\n\t\t\tcontinue;\n\t\tidx = hwc_index[i] + 1;\n\t\tif (is_limited_pmc(idx)) {\n\t\t\tcpuhw->limited_counter[n_lim] = event;\n\t\t\tcpuhw->limited_hwidx[n_lim] = idx;\n\t\t\t++n_lim;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ebb)\n\t\t\tval = local64_read(&event->hw.prev_count);\n\t\telse {\n\t\t\tval = 0;\n\t\t\tif (event->hw.sample_period) {\n\t\t\t\tleft = local64_read(&event->hw.period_left);\n\t\t\t\tif (left < 0x80000000L)\n\t\t\t\t\tval = 0x80000000L - left;\n\t\t\t}\n\t\t\tlocal64_set(&event->hw.prev_count, val);\n\t\t}\n\n\t\tevent->hw.idx = idx;\n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tval = 0;\n\t\twrite_pmc(idx, val);\n\n\t\tperf_event_update_userpage(event);\n\t}\n\tcpuhw->n_limited = n_lim;\n\tcpuhw->mmcr.mmcr0 |= MMCR0_PMXE | MMCR0_FCECE;\n\n out_enable:\n\tpmao_restore_workaround(ebb);\n\n\tmmcr0 = ebb_switch_in(ebb, cpuhw);\n\n\tmb();\n\tif (cpuhw->bhrb_users)\n\t\tppmu->config_bhrb(cpuhw->bhrb_filter);\n\n\twrite_mmcr0(cpuhw, mmcr0);\n\n\t/*\n\t * Enable instruction sampling if necessary\n\t */\n\tif (cpuhw->mmcr.mmcra & MMCRA_SAMPLE_ENABLE) {\n\t\tmb();\n\t\tmtspr(SPRN_MMCRA, cpuhw->mmcr.mmcra);\n\t}\n\n out:\n\n\tlocal_irq_restore(flags);\n}\n\nstatic int collect_events(struct perf_event *group, int max_count,\n\t\t\t  struct perf_event *ctrs[], u64 *events,\n\t\t\t  unsigned int *flags)\n{\n\tint n = 0;\n\tstruct perf_event *event;\n\n\tif (group->pmu->task_ctx_nr == perf_hw_context) {\n\t\tif (n >= max_count)\n\t\t\treturn -1;\n\t\tctrs[n] = group;\n\t\tflags[n] = group->hw.event_base;\n\t\tevents[n++] = group->hw.config;\n\t}\n\tfor_each_sibling_event(event, group) {\n\t\tif (event->pmu->task_ctx_nr == perf_hw_context &&\n\t\t    event->state != PERF_EVENT_STATE_OFF) {\n\t\t\tif (n >= max_count)\n\t\t\t\treturn -1;\n\t\t\tctrs[n] = event;\n\t\t\tflags[n] = event->hw.event_base;\n\t\t\tevents[n++] = event->hw.config;\n\t\t}\n\t}\n\treturn n;\n}\n\n/*\n * Add an event to the PMU.\n * If all events are not already frozen, then we disable and\n * re-enable the PMU in order to get hw_perf_enable to do the\n * actual work of reconfiguring the PMU.\n */\nstatic int power_pmu_add(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tunsigned long flags;\n\tint n0;\n\tint ret = -EAGAIN;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\t/*\n\t * Add the event to the list (if there is room)\n\t * and check whether the total set is still feasible.\n\t */\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tn0 = cpuhw->n_events;\n\tif (n0 >= ppmu->n_counter)\n\t\tgoto out;\n\tcpuhw->event[n0] = event;\n\tcpuhw->events[n0] = event->hw.config;\n\tcpuhw->flags[n0] = event->hw.event_base;\n\n\t/*\n\t * This event may have been disabled/stopped in record_and_restart()\n\t * because we exceeded the ->event_limit. If re-starting the event,\n\t * clear the ->hw.state (STOPPED and UPTODATE flags), so the user\n\t * notification is re-enabled.\n\t */\n\tif (!(ef_flags & PERF_EF_START))\n\t\tevent->hw.state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\telse\n\t\tevent->hw.state = 0;\n\n\t/*\n\t * If group events scheduling transaction was started,\n\t * skip the schedulability test here, it will be performed\n\t * at commit time(->commit_txn) as a whole\n\t */\n\tif (cpuhw->txn_flags & PERF_PMU_TXN_ADD)\n\t\tgoto nocheck;\n\n\tif (check_excludes(cpuhw->event, cpuhw->flags, n0, 1))\n\t\tgoto out;\n\tif (power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n0 + 1, cpuhw->event))\n\t\tgoto out;\n\tevent->hw.config = cpuhw->events[n0];\n\nnocheck:\n\tebb_event_add(event);\n\n\t++cpuhw->n_events;\n\t++cpuhw->n_added;\n\n\tret = 0;\n out:\n\tif (has_branch_stack(event)) {\n\t\tu64 bhrb_filter = -1;\n\n\t\tif (ppmu->bhrb_filter_map)\n\t\t\tbhrb_filter = ppmu->bhrb_filter_map(\n\t\t\t\tevent->attr.branch_sample_type);\n\n\t\tif (bhrb_filter != -1) {\n\t\t\tcpuhw->bhrb_filter = bhrb_filter;\n\t\t\tpower_pmu_bhrb_enable(event);\n\t\t}\n\t}\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n/*\n * Remove an event from the PMU.\n */\nstatic void power_pmu_del(struct perf_event *event, int ef_flags)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\tif (event == cpuhw->event[i]) {\n\t\t\twhile (++i < cpuhw->n_events) {\n\t\t\t\tcpuhw->event[i-1] = cpuhw->event[i];\n\t\t\t\tcpuhw->events[i-1] = cpuhw->events[i];\n\t\t\t\tcpuhw->flags[i-1] = cpuhw->flags[i];\n\t\t\t}\n\t\t\t--cpuhw->n_events;\n\t\t\tppmu->disable_pmc(event->hw.idx - 1, &cpuhw->mmcr);\n\t\t\tif (event->hw.idx) {\n\t\t\t\twrite_pmc(event->hw.idx, 0);\n\t\t\t\tevent->hw.idx = 0;\n\t\t\t}\n\t\t\tperf_event_update_userpage(event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuhw->n_limited; ++i)\n\t\tif (event == cpuhw->limited_counter[i])\n\t\t\tbreak;\n\tif (i < cpuhw->n_limited) {\n\t\twhile (++i < cpuhw->n_limited) {\n\t\t\tcpuhw->limited_counter[i-1] = cpuhw->limited_counter[i];\n\t\t\tcpuhw->limited_hwidx[i-1] = cpuhw->limited_hwidx[i];\n\t\t}\n\t\t--cpuhw->n_limited;\n\t}\n\tif (cpuhw->n_events == 0) {\n\t\t/* disable exceptions if no events are running */\n\t\tcpuhw->mmcr.mmcr0 &= ~(MMCR0_PMXE | MMCR0_FCECE);\n\t}\n\n\tif (has_branch_stack(event))\n\t\tpower_pmu_bhrb_disable(event);\n\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * POWER-PMU does not support disabling individual counters, hence\n * program their cycle counter to their max value and ignore the interrupts.\n */\n\nstatic void power_pmu_start(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\ts64 left;\n\tunsigned long val;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (!(event->hw.state & PERF_HES_STOPPED))\n\t\treturn;\n\n\tif (ef_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tevent->hw.state = 0;\n\tleft = local64_read(&event->hw.period_left);\n\n\tval = 0;\n\tif (left < 0x80000000L)\n\t\tval = 0x80000000L - left;\n\n\twrite_pmc(event->hw.idx, val);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\nstatic void power_pmu_stop(struct perf_event *event, int ef_flags)\n{\n\tunsigned long flags;\n\n\tif (!event->hw.idx || !event->hw.sample_period)\n\t\treturn;\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tperf_pmu_disable(event->pmu);\n\n\tpower_pmu_read(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\twrite_pmc(event->hw.idx, 0);\n\n\tperf_event_update_userpage(event);\n\tperf_pmu_enable(event->pmu);\n\tlocal_irq_restore(flags);\n}\n\n/*\n * Start group events scheduling transaction\n * Set the flag to make pmu::enable() not perform the\n * schedulability test, it will be performed at commit time\n *\n * We only support PERF_PMU_TXN_ADD transactions. Save the\n * transaction flags but otherwise ignore non-PERF_PMU_TXN_ADD\n * transactions.\n */\nstatic void power_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\tWARN_ON_ONCE(cpuhw->txn_flags);\t\t/* txn already in flight */\n\n\tcpuhw->txn_flags = txn_flags;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_disable(pmu);\n\tcpuhw->n_txn_start = cpuhw->n_events;\n}\n\n/*\n * Stop group events scheduling transaction\n * Clear the flag and pmu::enable() will perform the\n * schedulability test.\n */\nstatic void power_pmu_cancel_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\tunsigned int txn_flags;\n\n\tWARN_ON_ONCE(!cpuhw->txn_flags);\t/* no txn in flight */\n\n\ttxn_flags = cpuhw->txn_flags;\n\tcpuhw->txn_flags = 0;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_enable(pmu);\n}\n\n/*\n * Commit group events scheduling transaction\n * Perform the group schedulability test as a whole\n * Return 0 if success\n */\nstatic int power_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuhw;\n\tlong i, n;\n\n\tif (!ppmu)\n\t\treturn -EAGAIN;\n\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\tWARN_ON_ONCE(!cpuhw->txn_flags);\t/* no txn in flight */\n\n\tif (cpuhw->txn_flags & ~PERF_PMU_TXN_ADD) {\n\t\tcpuhw->txn_flags = 0;\n\t\treturn 0;\n\t}\n\n\tn = cpuhw->n_events;\n\tif (check_excludes(cpuhw->event, cpuhw->flags, 0, n))\n\t\treturn -EAGAIN;\n\ti = power_check_constraints(cpuhw, cpuhw->events, cpuhw->flags, n, cpuhw->event);\n\tif (i < 0)\n\t\treturn -EAGAIN;\n\n\tfor (i = cpuhw->n_txn_start; i < n; ++i)\n\t\tcpuhw->event[i]->hw.config = cpuhw->events[i];\n\n\tcpuhw->txn_flags = 0;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n\n/*\n * Return 1 if we might be able to put event on a limited PMC,\n * or 0 if not.\n * An event can only go on a limited PMC if it counts something\n * that a limited PMC can count, doesn't require interrupts, and\n * doesn't exclude any processor mode.\n */\nstatic int can_go_on_limited_pmc(struct perf_event *event, u64 ev,\n\t\t\t\t unsigned int flags)\n{\n\tint n;\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\n\tif (event->attr.exclude_user\n\t    || event->attr.exclude_kernel\n\t    || event->attr.exclude_hv\n\t    || event->attr.sample_period)\n\t\treturn 0;\n\n\tif (ppmu->limited_pmc_event(ev))\n\t\treturn 1;\n\n\t/*\n\t * The requested event_id isn't on a limited PMC already;\n\t * see if any alternative code goes on a limited PMC.\n\t */\n\tif (!ppmu->get_alternatives)\n\t\treturn 0;\n\n\tflags |= PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD;\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\n\treturn n > 0;\n}\n\n/*\n * Find an alternative event_id that goes on a normal PMC, if possible,\n * and return the event_id code, or 0 if there is no such alternative.\n * (Note: event_id code 0 is \"don't count\" on all machines.)\n */\nstatic u64 normal_pmc_alternative(u64 ev, unsigned long flags)\n{\n\tu64 alt[MAX_EVENT_ALTERNATIVES];\n\tint n;\n\n\tflags &= ~(PPMU_LIMITED_PMC_OK | PPMU_LIMITED_PMC_REQD);\n\tn = ppmu->get_alternatives(ev, flags, alt);\n\tif (!n)\n\t\treturn 0;\n\treturn alt[0];\n}\n\n/* Number of perf_events counting hardware events */\nstatic atomic_t num_events;\n/* Used to avoid races in calling reserve/release_pmc_hardware */\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n/*\n * Release the PMU if this is the last perf_event.\n */\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tif (!atomic_add_unless(&num_events, -1, 1)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_dec_return(&num_events) == 0)\n\t\t\trelease_pmc_hardware();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n/*\n * Translate a generic cache event_id config to a raw event_id code.\n */\nstatic int hw_perf_cache_event(u64 config, u64 *eventp)\n{\n\tunsigned long type, op, result;\n\tu64 ev;\n\n\tif (!ppmu->cache_events)\n\t\treturn -EINVAL;\n\n\t/* unpack config */\n\ttype = config & 0xff;\n\top = (config >> 8) & 0xff;\n\tresult = (config >> 16) & 0xff;\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX ||\n\t    op >= PERF_COUNT_HW_CACHE_OP_MAX ||\n\t    result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tev = (*ppmu->cache_events)[type][op][result];\n\tif (ev == 0)\n\t\treturn -EOPNOTSUPP;\n\tif (ev == -1)\n\t\treturn -EINVAL;\n\t*eventp = ev;\n\treturn 0;\n}\n\nstatic bool is_event_blacklisted(u64 ev)\n{\n\tint i;\n\n\tfor (i=0; i < ppmu->n_blacklist_ev; i++) {\n\t\tif (ppmu->blacklist_ev[i] == ev)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int power_pmu_event_init(struct perf_event *event)\n{\n\tu64 ev;\n\tunsigned long flags, irq_flags;\n\tstruct perf_event *ctrs[MAX_HWEVENTS];\n\tu64 events[MAX_HWEVENTS];\n\tunsigned int cflags[MAX_HWEVENTS];\n\tint n;\n\tint err;\n\tstruct cpu_hw_events *cpuhw;\n\n\tif (!ppmu)\n\t\treturn -ENOENT;\n\n\tif (has_branch_stack(event)) {\n\t        /* PMU has BHRB enabled */\n\t\tif (!(ppmu->flags & PPMU_ARCH_207S))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tev = event->attr.config;\n\t\tif (ev >= ppmu->n_generic || ppmu->generic_events[ev] == 0)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tev = ppmu->generic_events[ev];\n\t\tbreak;\n\tcase PERF_TYPE_HW_CACHE:\n\t\terr = hw_perf_cache_event(event->attr.config, &ev);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tev = event->attr.config;\n\n\t\tif (ppmu->blacklist_ev && is_event_blacklisted(ev))\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\t/*\n\t * PMU config registers have fields that are\n\t * reserved and some specific values for bit fields are reserved.\n\t * For ex., MMCRA[61:62] is Randome Sampling Mode (SM)\n\t * and value of 0b11 to this field is reserved.\n\t * Check for invalid values in attr.config.\n\t */\n\tif (ppmu->check_attr_config &&\n\t    ppmu->check_attr_config(event))\n\t\treturn -EINVAL;\n\n\tevent->hw.config_base = ev;\n\tevent->hw.idx = 0;\n\n\t/*\n\t * If we are not running on a hypervisor, force the\n\t * exclude_hv bit to 0 so that we don't care what\n\t * the user set it to.\n\t */\n\tif (!firmware_has_feature(FW_FEATURE_LPAR))\n\t\tevent->attr.exclude_hv = 0;\n\n\t/*\n\t * If this is a per-task event, then we can use\n\t * PM_RUN_* events interchangeably with their non RUN_*\n\t * equivalents, e.g. PM_RUN_CYC instead of PM_CYC.\n\t * XXX we should check if the task is an idle task.\n\t */\n\tflags = 0;\n\tif (event->attach_state & PERF_ATTACH_TASK)\n\t\tflags |= PPMU_ONLY_COUNT_RUN;\n\n\t/*\n\t * If this machine has limited events, check whether this\n\t * event_id could go on a limited event.\n\t */\n\tif (ppmu->flags & PPMU_LIMITED_PMC5_6) {\n\t\tif (can_go_on_limited_pmc(event, ev, flags)) {\n\t\t\tflags |= PPMU_LIMITED_PMC_OK;\n\t\t} else if (ppmu->limited_pmc_event(ev)) {\n\t\t\t/*\n\t\t\t * The requested event_id is on a limited PMC,\n\t\t\t * but we can't use a limited PMC; see if any\n\t\t\t * alternative goes on a normal PMC.\n\t\t\t */\n\t\t\tev = normal_pmc_alternative(ev, flags);\n\t\t\tif (!ev)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* Extra checks for EBB */\n\terr = ebb_event_check(event);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * If this is in a group, check if it can go on with all the\n\t * other hardware events in the group.  We assume the event\n\t * hasn't been linked into its leader's sibling list at this point.\n\t */\n\tn = 0;\n\tif (event->group_leader != event) {\n\t\tn = collect_events(event->group_leader, ppmu->n_counter - 1,\n\t\t\t\t   ctrs, events, cflags);\n\t\tif (n < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tevents[n] = ev;\n\tctrs[n] = event;\n\tcflags[n] = flags;\n\tif (check_excludes(ctrs, cflags, n, 1))\n\t\treturn -EINVAL;\n\n\tlocal_irq_save(irq_flags);\n\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\n\terr = power_check_constraints(cpuhw, events, cflags, n + 1, ctrs);\n\n\tif (has_branch_stack(event)) {\n\t\tu64 bhrb_filter = -1;\n\n\t\tif (ppmu->bhrb_filter_map)\n\t\t\tbhrb_filter = ppmu->bhrb_filter_map(\n\t\t\t\t\tevent->attr.branch_sample_type);\n\n\t\tif (bhrb_filter == -1) {\n\t\t\tlocal_irq_restore(irq_flags);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t\tcpuhw->bhrb_filter = bhrb_filter;\n\t}\n\n\tlocal_irq_restore(irq_flags);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tevent->hw.config = events[n];\n\tevent->hw.event_base = cflags[n];\n\tevent->hw.last_period = event->hw.sample_period;\n\tlocal64_set(&event->hw.period_left, event->hw.last_period);\n\n\t/*\n\t * For EBB events we just context switch the PMC value, we don't do any\n\t * of the sample_period logic. We use hw.prev_count for this.\n\t */\n\tif (is_ebb_event(event))\n\t\tlocal64_set(&event->hw.prev_count, 0);\n\n\t/*\n\t * See if we need to reserve the PMU.\n\t * If no events are currently in use, then we have to take a\n\t * mutex to ensure that we don't race with another task doing\n\t * reserve_pmc_hardware or release_pmc_hardware.\n\t */\n\terr = 0;\n\tif (!atomic_inc_not_zero(&num_events)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&num_events) == 0 &&\n\t\t    reserve_pmc_hardware(perf_event_interrupt))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\tatomic_inc(&num_events);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\tevent->destroy = hw_perf_event_destroy;\n\n\treturn err;\n}\n\nstatic int power_pmu_event_idx(struct perf_event *event)\n{\n\treturn event->hw.idx;\n}\n\nssize_t power_events_sysfs_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\n\treturn sprintf(page, \"event=0x%02llx\\n\", pmu_attr->id);\n}\n\nstatic struct pmu power_pmu = {\n\t.pmu_enable\t= power_pmu_enable,\n\t.pmu_disable\t= power_pmu_disable,\n\t.event_init\t= power_pmu_event_init,\n\t.add\t\t= power_pmu_add,\n\t.del\t\t= power_pmu_del,\n\t.start\t\t= power_pmu_start,\n\t.stop\t\t= power_pmu_stop,\n\t.read\t\t= power_pmu_read,\n\t.start_txn\t= power_pmu_start_txn,\n\t.cancel_txn\t= power_pmu_cancel_txn,\n\t.commit_txn\t= power_pmu_commit_txn,\n\t.event_idx\t= power_pmu_event_idx,\n\t.sched_task\t= power_pmu_sched_task,\n};\n\n#define PERF_SAMPLE_ADDR_TYPE  (PERF_SAMPLE_ADDR |\t\t\\\n\t\t\t\tPERF_SAMPLE_PHYS_ADDR |\t\t\\\n\t\t\t\tPERF_SAMPLE_DATA_PAGE_SIZE)\n/*\n * A counter has overflowed; update its count and record\n * things if requested.  Note that interrupts are hard-disabled\n * here so there is no possibility of being interrupted.\n */\nstatic void record_and_restart(struct perf_event *event, unsigned long val,\n\t\t\t       struct pt_regs *regs)\n{\n\tu64 period = event->hw.sample_period;\n\ts64 prev, delta, left;\n\tint record = 0;\n\n\tif (event->hw.state & PERF_HES_STOPPED) {\n\t\twrite_pmc(event->hw.idx, 0);\n\t\treturn;\n\t}\n\n\t/* we don't have to worry about interrupts here */\n\tprev = local64_read(&event->hw.prev_count);\n\tdelta = check_and_compute_delta(prev, val);\n\tlocal64_add(delta, &event->count);\n\n\t/*\n\t * See if the total period for this event has expired,\n\t * and update for the next period.\n\t */\n\tval = 0;\n\tleft = local64_read(&event->hw.period_left) - delta;\n\tif (delta == 0)\n\t\tleft++;\n\tif (period) {\n\t\tif (left <= 0) {\n\t\t\tleft += period;\n\t\t\tif (left <= 0)\n\t\t\t\tleft = period;\n\n\t\t\t/*\n\t\t\t * If address is not requested in the sample via\n\t\t\t * PERF_SAMPLE_IP, just record that sample irrespective\n\t\t\t * of SIAR valid check.\n\t\t\t */\n\t\t\tif (event->attr.sample_type & PERF_SAMPLE_IP)\n\t\t\t\trecord = siar_valid(regs);\n\t\t\telse\n\t\t\t\trecord = 1;\n\n\t\t\tevent->hw.last_period = event->hw.sample_period;\n\t\t}\n\t\tif (left < 0x80000000LL)\n\t\t\tval = 0x80000000LL - left;\n\t}\n\n\twrite_pmc(event->hw.idx, val);\n\tlocal64_set(&event->hw.prev_count, val);\n\tlocal64_set(&event->hw.period_left, left);\n\tperf_event_update_userpage(event);\n\n\t/*\n\t * Due to hardware limitation, sometimes SIAR could sample a kernel\n\t * address even when freeze on supervisor state (kernel) is set in\n\t * MMCR2. Check attr.exclude_kernel and address to drop the sample in\n\t * these cases.\n\t */\n\tif (event->attr.exclude_kernel &&\n\t    (event->attr.sample_type & PERF_SAMPLE_IP) &&\n\t    is_kernel_addr(mfspr(SPRN_SIAR)))\n\t\trecord = 0;\n\n\t/*\n\t * Finally record data if requested.\n\t */\n\tif (record) {\n\t\tstruct perf_sample_data data;\n\n\t\tperf_sample_data_init(&data, ~0ULL, event->hw.last_period);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_ADDR_TYPE)\n\t\t\tperf_get_data_addr(event, regs, &data.addr);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\t\tstruct cpu_hw_events *cpuhw;\n\t\t\tcpuhw = this_cpu_ptr(&cpu_hw_events);\n\t\t\tpower_pmu_bhrb_read(event, cpuhw);\n\t\t\tdata.br_stack = &cpuhw->bhrb_stack;\n\t\t}\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_DATA_SRC &&\n\t\t\t\t\t\tppmu->get_mem_data_src)\n\t\t\tppmu->get_mem_data_src(&data.data_src, ppmu->flags, regs);\n\n\t\tif (event->attr.sample_type & PERF_SAMPLE_WEIGHT_TYPE &&\n\t\t\t\t\t\tppmu->get_mem_weight)\n\t\t\tppmu->get_mem_weight(&data.weight.full, event->attr.sample_type);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tpower_pmu_stop(event, 0);\n\t} else if (period) {\n\t\t/* Account for interrupt in case of invalid SIAR */\n\t\tif (perf_event_account_interrupt(event))\n\t\t\tpower_pmu_stop(event, 0);\n\t}\n}\n\n/*\n * Called from generic code to get the misc flags (i.e. processor mode)\n * for an event_id.\n */\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tu32 flags = perf_get_misc_flags(regs);\n\n\tif (flags)\n\t\treturn flags;\n\treturn user_mode(regs) ? PERF_RECORD_MISC_USER :\n\t\tPERF_RECORD_MISC_KERNEL;\n}\n\n/*\n * Called from generic code to get the instruction pointer\n * for an event_id.\n */\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long siar = mfspr(SPRN_SIAR);\n\n\tif (ppmu && (ppmu->flags & PPMU_P10_DD1)) {\n\t\tif (siar)\n\t\t\treturn siar;\n\t\telse\n\t\t\treturn regs->nip;\n\t} else if (use_siar && siar_valid(regs))\n\t\treturn mfspr(SPRN_SIAR) + perf_ip_adjust(regs);\n\telse if (use_siar)\n\t\treturn 0;\t\t// no valid instruction pointer\n\telse\n\t\treturn regs->nip;\n}\n\nstatic bool pmc_overflow_power7(unsigned long val)\n{\n\t/*\n\t * Events on POWER7 can roll back if a speculative event doesn't\n\t * eventually complete. Unfortunately in some rare cases they will\n\t * raise a performance monitor exception. We need to catch this to\n\t * ensure we reset the PMC. In all cases the PMC will be 256 or less\n\t * cycles from overflow.\n\t *\n\t * We only do this if the first pass fails to find any overflowing\n\t * PMCs because a user might set a period of less than 256 and we\n\t * don't want to mistakenly reset them.\n\t */\n\tif ((0x80000000 - val) <= 256)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool pmc_overflow(unsigned long val)\n{\n\tif ((int)val < 0)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Performance monitor interrupt stuff\n */\nstatic void __perf_event_interrupt(struct pt_regs *regs)\n{\n\tint i, j;\n\tstruct cpu_hw_events *cpuhw = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_event *event;\n\tint found, active;\n\n\tif (cpuhw->n_limited)\n\t\tfreeze_limited_counters(cpuhw, mfspr(SPRN_PMC5),\n\t\t\t\t\tmfspr(SPRN_PMC6));\n\n\tperf_read_regs(regs);\n\n\t/* Read all the PMCs since we'll need them a bunch of times */\n\tfor (i = 0; i < ppmu->n_counter; ++i)\n\t\tcpuhw->pmcs[i] = read_pmc(i + 1);\n\n\t/* Try to find what caused the IRQ */\n\tfound = 0;\n\tfor (i = 0; i < ppmu->n_counter; ++i) {\n\t\tif (!pmc_overflow(cpuhw->pmcs[i]))\n\t\t\tcontinue;\n\t\tif (is_limited_pmc(i + 1))\n\t\t\tcontinue; /* these won't generate IRQs */\n\t\t/*\n\t\t * We've found one that's overflowed.  For active\n\t\t * counters we need to log this.  For inactive\n\t\t * counters, we need to reset it anyway\n\t\t */\n\t\tfound = 1;\n\t\tactive = 0;\n\t\tfor (j = 0; j < cpuhw->n_events; ++j) {\n\t\t\tevent = cpuhw->event[j];\n\t\t\tif (event->hw.idx == (i + 1)) {\n\t\t\t\tactive = 1;\n\t\t\t\trecord_and_restart(event, cpuhw->pmcs[i], regs);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!active)\n\t\t\t/* reset non active counters that have overflowed */\n\t\t\twrite_pmc(i + 1, 0);\n\t}\n\tif (!found && pvr_version_is(PVR_POWER7)) {\n\t\t/* check active counters for special buggy p7 overflow */\n\t\tfor (i = 0; i < cpuhw->n_events; ++i) {\n\t\t\tevent = cpuhw->event[i];\n\t\t\tif (!event->hw.idx || is_limited_pmc(event->hw.idx))\n\t\t\t\tcontinue;\n\t\t\tif (pmc_overflow_power7(cpuhw->pmcs[event->hw.idx - 1])) {\n\t\t\t\t/* event has overflowed in a buggy way*/\n\t\t\t\tfound = 1;\n\t\t\t\trecord_and_restart(event,\n\t\t\t\t\t\t   cpuhw->pmcs[event->hw.idx - 1],\n\t\t\t\t\t\t   regs);\n\t\t\t}\n\t\t}\n\t}\n\tif (unlikely(!found) && !arch_irq_disabled_regs(regs))\n\t\tprintk_ratelimited(KERN_WARNING \"Can't find PMC that caused IRQ\\n\");\n\n\t/*\n\t * Reset MMCR0 to its normal value.  This will set PMXE and\n\t * clear FC (freeze counters) and PMAO (perf mon alert occurred)\n\t * and thus allow interrupts to occur again.\n\t * XXX might want to use MSR.PM to keep the events frozen until\n\t * we get back out of this interrupt.\n\t */\n\twrite_mmcr0(cpuhw, cpuhw->mmcr.mmcr0);\n\n\t/* Clear the cpuhw->pmcs */\n\tmemset(&cpuhw->pmcs, 0, sizeof(cpuhw->pmcs));\n\n}\n\nstatic void perf_event_interrupt(struct pt_regs *regs)\n{\n\tu64 start_clock = sched_clock();\n\n\t__perf_event_interrupt(regs);\n\tperf_sample_event_took(sched_clock() - start_clock);\n}\n\nstatic int power_pmu_prepare_cpu(unsigned int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tif (ppmu) {\n\t\tmemset(cpuhw, 0, sizeof(*cpuhw));\n\t\tcpuhw->mmcr.mmcr0 = MMCR0_FC;\n\t}\n\treturn 0;\n}\n\nint register_power_pmu(struct power_pmu *pmu)\n{\n\tif (ppmu)\n\t\treturn -EBUSY;\t\t/* something's already registered */\n\n\tppmu = pmu;\n\tpr_info(\"%s performance monitor hardware support registered\\n\",\n\t\tpmu->name);\n\n\tpower_pmu.attr_groups = ppmu->attr_groups;\n\tpower_pmu.capabilities |= (ppmu->capabilities & PERF_PMU_CAP_EXTENDED_REGS);\n\n#ifdef MSR_HV\n\t/*\n\t * Use FCHV to ignore kernel events if MSR.HV is set.\n\t */\n\tif (mfmsr() & MSR_HV)\n\t\tfreeze_events_kernel = MMCR0_FCHV;\n#endif /* CONFIG_PPC64 */\n\n\tperf_pmu_register(&power_pmu, \"cpu\", PERF_TYPE_RAW);\n\tcpuhp_setup_state(CPUHP_PERF_POWER, \"perf/powerpc:prepare\",\n\t\t\t  power_pmu_prepare_cpu, NULL);\n\treturn 0;\n}\n\n#ifdef CONFIG_PPC64\nstatic int __init init_ppc64_pmu(void)\n{\n\t/* run through all the pmu drivers one at a time */\n\tif (!init_power5_pmu())\n\t\treturn 0;\n\telse if (!init_power5p_pmu())\n\t\treturn 0;\n\telse if (!init_power6_pmu())\n\t\treturn 0;\n\telse if (!init_power7_pmu())\n\t\treturn 0;\n\telse if (!init_power8_pmu())\n\t\treturn 0;\n\telse if (!init_power9_pmu())\n\t\treturn 0;\n\telse if (!init_power10_pmu())\n\t\treturn 0;\n\telse if (!init_ppc970_pmu())\n\t\treturn 0;\n\telse\n\t\treturn init_generic_compat_pmu();\n}\nearly_initcall(init_ppc64_pmu);\n#endif\n"], "filenames": ["arch/powerpc/perf/core-book3s.c"], "buggy_code_start_loc": [2257], "buggy_code_end_loc": [2258], "fixing_code_start_loc": [2257], "fixing_code_end_loc": [2258], "type": "CWE-476", "message": "arch/powerpc/perf/core-book3s.c in the Linux kernel before 5.12.13, on systems with perf_event_paranoid=-1 and no specific PMU driver support registered, allows local users to cause a denial of service (perf_instruction_pointer NULL pointer dereference and OOPS) via a \"perf record\" command.", "other": {"cve": {"id": "CVE-2021-38200", "sourceIdentifier": "cve@mitre.org", "published": "2021-08-08T20:15:07.110", "lastModified": "2021-08-12T14:24:53.037", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "arch/powerpc/perf/core-book3s.c in the Linux kernel before 5.12.13, on systems with perf_event_paranoid=-1 and no specific PMU driver support registered, allows local users to cause a denial of service (perf_instruction_pointer NULL pointer dereference and OOPS) via a \"perf record\" command."}, {"lang": "es", "value": "El archivo arch/powerpc/perf/core-book3s.c en el kernel de Linux versiones anteriores a 5.12.13, en sistemas con perf_event_paranoid=-1 y sin soporte espec\u00edfico de controlador PMU registrado, permite a usuarios locales causar una denegaci\u00f3n de servicio (desreferencia de puntero NULL perf_instruction_pointer y OOPS) por medio de un comando \"perf record\""}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.12.13", "matchCriteriaId": "4C096241-6457-4DB4-80C3-DD650F628184"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.12.13", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/60b7ed54a41b550d50caf7f2418db4a7e75b5bdc", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/60b7ed54a41b550d50caf7f2418db4a7e75b5bdc"}}