{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/cred.h>\n#include <linux/idr.h>\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/file.h>\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/memblock.h>\n#include <linux/proc_fs.h>\n#include <linux/task_work.h>\n#include <linux/sched/task.h>\n#include <uapi/linux/mount.h>\n#include <linux/fs_context.h>\n#include <linux/shmem_fs.h>\n\n#include \"pnode.h\"\n#include \"internal.h\"\n\n/* Maximum number of mounts in a mount namespace */\nunsigned int sysctl_mount_max __read_mostly = 100000;\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\nstatic LIST_HEAD(ex_mountpoints); /* protected by namespace_sem */\n\nstruct mount_kattr {\n\tunsigned int attr_set;\n\tunsigned int attr_clr;\n\tunsigned int propagation;\n\tunsigned int lookup_flags;\n\tbool recurse;\n\tstruct user_namespace *mnt_userns;\n};\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline void lock_mount_hash(void)\n{\n\twrite_seqlock(&mount_lock);\n}\n\nstatic inline void unlock_mount_hash(void)\n{\n\twrite_sequnlock(&mount_lock);\n}\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res = ida_alloc(&mnt_id_ida, GFP_KERNEL);\n\n\tif (res < 0)\n\t\treturn res;\n\tmnt->mnt_id = res;\n\treturn 0;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tida_free(&mnt_id_ida, mnt->mnt_id);\n}\n\n/*\n * Allocate a new peer group ID\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res = ida_alloc_min(&mnt_group_ida, 1, GFP_KERNEL);\n\n\tif (res < 0)\n\t\treturn res;\n\tmnt->mnt_group_id = res;\n\treturn 0;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tida_free(&mnt_group_ida, mnt->mnt_group_id);\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nint mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tint count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup_const(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n\t\tINIT_HLIST_NODE(&mnt->mnt_mp_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_umounting);\n\t\tINIT_HLIST_HEAD(&mnt->mnt_stuck_children);\n\t\tmnt->mnt.mnt_userns = &init_user_ns;\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree_const(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nbool __mnt_is_readonly(struct vfsmount *mnt)\n{\n\treturn (mnt->mnt_flags & MNT_READONLY) || sb_rdonly(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but if the file is already open for writing it\n * skips incrementing mnt_writers (since the open file already has a reference)\n * and instead only does the check for emergency r/o remounts.  This must be\n * paired with __mnt_drop_write_file.\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (file->f_mode & FMODE_WRITER) {\n\t\t/*\n\t\t * Superblock may have become readonly while there are still\n\t\t * writable fd's, e.g. due to a fs error with errors=remount-ro\n\t\t */\n\t\tif (__mnt_is_readonly(file->f_path.mnt))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\t}\n\treturn __mnt_want_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but if the file is already open for writing it\n * skips incrementing mnt_writers (since the open file already has a reference)\n * and instead only does the freeze protection and the check for emergency r/o\n * remounts.  This must be paired with mnt_drop_write_file.\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file_inode(file)->i_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file_inode(file)->i_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write_file(file);\n\tsb_end_write(file_inode(file)->i_sb);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic inline int mnt_hold_writers(struct mount *mnt)\n{\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic inline void mnt_unhold_writers(struct mount *mnt)\n{\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n}\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret;\n\n\tret = mnt_hold_writers(mnt);\n\tif (!ret)\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\tmnt_unhold_writers(mnt);\n\treturn ret;\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tstruct user_namespace *mnt_userns;\n\n\tmnt_userns = mnt_user_ns(&mnt->mnt);\n\tif (mnt_userns != &init_user_ns)\n\t\tput_user_ns(mnt_userns);\n\tkfree_const(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nint __legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn 1;\n\tif (bastard == NULL)\n\t\treturn 0;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tsmp_mb();\t\t\t// see mntput_no_expire()\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn 0;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn 1;\n\t}\n\tlock_mount_hash();\n\tif (unlikely(bastard->mnt_flags & MNT_DOOMED)) {\n\t\tmnt_add_count(mnt, -1);\n\t\tunlock_mount_hash();\n\t\treturn 1;\n\t}\n\tunlock_mount_hash();\n\t/* caller will mntput() */\n\treturn -1;\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tint res = __legitimize_mnt(bastard, seq);\n\tif (likely(!res))\n\t\treturn true;\n\tif (unlikely(res < 0)) {\n\t\trcu_read_unlock();\n\t\tmntput(bastard);\n\t\trcu_read_lock();\n\t}\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(const struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\nstatic inline void lock_ns_list(struct mnt_namespace *ns)\n{\n\tspin_lock(&ns->ns_lock);\n}\n\nstatic inline void unlock_ns_list(struct mnt_namespace *ns)\n{\n\tspin_unlock(&ns->ns_lock);\n}\n\nstatic inline bool mnt_is_cursor(struct mount *mnt)\n{\n\treturn mnt->mnt.mnt_flags & MNT_CURSOR;\n}\n\n/*\n * __is_local_mountpoint - Test to see if dentry is a mountpoint in the\n *                         current mount namespace.\n *\n * The common case is dentries are not mountpoints at all and that\n * test is handled inline.  For the slow case when we are actually\n * dealing with a mountpoint of some kind, walk through all of the\n * mounts in the current mount namespace and test to see if the dentry\n * is a mountpoint.\n *\n * The mount_hashtable is not usable in the context because we\n * need to identify all mounts that may be in the current mount\n * namespace not just a mount that happens to have some specified\n * parent mount.\n */\nbool __is_local_mountpoint(struct dentry *dentry)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool is_covered = false;\n\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tif (mnt_is_cursor(mnt))\n\t\t\tcontinue;\n\t\tis_covered = (mnt->mnt_mountpoint == dentry);\n\t\tif (is_covered)\n\t\t\tbreak;\n\t}\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n\n\treturn is_covered;\n}\n\nstatic struct mountpoint *lookup_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dget(dentry);\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}\n\n/*\n * vfsmount lock must be held.  Additionally, the caller is responsible\n * for serializing calls for given disposal list.\n */\nstatic void __put_mountpoint(struct mountpoint *mp, struct list_head *list)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tBUG_ON(!hlist_empty(&mp->m_list));\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\tdput_to_list(dentry, list);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\n/* called with namespace_lock and vfsmount lock */\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\t__put_mountpoint(mp, &ex_mountpoints);\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic struct mountpoint *unhash_mnt(struct mount *mnt)\n{\n\tstruct mountpoint *mp;\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\thlist_del_init(&mnt->mnt_mp_list);\n\tmp = mnt->mnt_mp;\n\tmnt->mnt_mp = NULL;\n\treturn mp;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void umount_mnt(struct mount *mnt)\n{\n\tput_mountpoint(unhash_mnt(mnt));\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = mp->m_dentry;\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}\n\nstatic void __attach_mnt(struct mount *mnt, struct mount *parent)\n{\n\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t   m_hash(&parent->mnt, mnt->mnt_mountpoint));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\t__attach_mnt(mnt, parent);\n}\n\nvoid mnt_change_mountpoint(struct mount *parent, struct mountpoint *mp, struct mount *mnt)\n{\n\tstruct mountpoint *old_mp = mnt->mnt_mp;\n\tstruct mount *old_parent = mnt->mnt_parent;\n\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init(&mnt->mnt_mp_list);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\n\tattach_mnt(mnt, parent, mp);\n\n\tput_mountpoint(old_mp);\n\tmnt_add_count(old_parent, -1);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tn->mounts += n->pending_mounts;\n\tn->pending_mounts = 0;\n\n\t__attach_mnt(mnt, parent);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\n/**\n * vfs_create_mount - Create a mount for a configured superblock\n * @fc: The configuration context with the superblock attached\n *\n * Create a mount to an already configured superblock.  If necessary, the\n * caller should invoke vfs_get_tree() before calling this.\n *\n * Note that this does not attach the mount to anything.\n */\nstruct vfsmount *vfs_create_mount(struct fs_context *fc)\n{\n\tstruct mount *mnt;\n\n\tif (!fc->root)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmnt = alloc_vfsmnt(fc->source ?: \"none\");\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (fc->sb_flags & SB_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\tatomic_inc(&fc->root->d_sb->s_active);\n\tmnt->mnt.mnt_sb\t\t= fc->root->d_sb;\n\tmnt->mnt.mnt_root\t= dget(fc->root);\n\tmnt->mnt_mountpoint\t= mnt->mnt.mnt_root;\n\tmnt->mnt_parent\t\t= mnt;\n\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &mnt->mnt.mnt_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL(vfs_create_mount);\n\nstruct vfsmount *fc_mount(struct fs_context *fc)\n{\n\tint err = vfs_get_tree(fc);\n\tif (!err) {\n\t\tup_write(&fc->root->d_sb->s_umount);\n\t\treturn vfs_create_mount(fc);\n\t}\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(fc_mount);\n\nstruct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\tint flags, const char *name,\n\t\t\t\tvoid *data)\n{\n\tstruct fs_context *fc;\n\tstruct vfsmount *mnt;\n\tint ret = 0;\n\n\tif (!type)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tfc = fs_context_for_mount(type, flags);\n\tif (IS_ERR(fc))\n\t\treturn ERR_CAST(fc);\n\n\tif (name)\n\t\tret = vfs_parse_fs_string(fc, \"source\",\n\t\t\t\t\t  name, strlen(name));\n\tif (!ret)\n\t\tret = parse_monolithic_mount_data(fc, data);\n\tif (!ret)\n\t\tmnt = fc_mount(fc);\n\telse\n\t\tmnt = ERR_PTR(ret);\n\n\tput_fs_context(fc);\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstruct vfsmount *\nvfs_submount(const struct dentry *mountpoint, struct file_system_type *type,\n\t     const char *name, void *data)\n{\n\t/* Until it is worked out how to pass the user namespace\n\t * through from the parent mount to the submount don't support\n\t * unprivileged mounts with submounts.\n\t */\n\tif (mountpoint->d_sb->s_user_ns != &init_user_ns)\n\t\treturn ERR_PTR(-EPERM);\n\n\treturn vfs_kern_mount(type, SB_SUBMOUNT, name, data);\n}\nEXPORT_SYMBOL_GPL(vfs_submount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags;\n\tmnt->mnt.mnt_flags &= ~(MNT_WRITE_HOLD|MNT_MARKED|MNT_INTERNAL);\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_userns = mnt_user_ns(&old->mnt);\n\tif (mnt->mnt.mnt_userns != &init_user_ns)\n\t\tmnt->mnt.mnt_userns = get_user_ns(mnt->mnt.mnt_userns);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t} else {\n\t\tCLEAR_MNT_SHARED(mnt);\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void cleanup_mnt(struct mount *mnt)\n{\n\tstruct hlist_node *p;\n\tstruct mount *m;\n\t/*\n\t * The warning here probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this happens, the\n\t * filesystem was probably unable to make r/w->r/o transitions.\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tif (unlikely(mnt->mnt_pins.first))\n\t\tmnt_pin_kill(mnt);\n\thlist_for_each_entry_safe(m, p, &mnt->mnt_stuck_children, mnt_umount) {\n\t\thlist_del(&m->mnt_umount);\n\t\tmntput(&m->mnt);\n\t}\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nstatic void __cleanup_mnt(struct rcu_head *head)\n{\n\tcleanup_mnt(container_of(head, struct mount, mnt_rcu));\n}\n\nstatic LLIST_HEAD(delayed_mntput_list);\nstatic void delayed_mntput(struct work_struct *unused)\n{\n\tstruct llist_node *node = llist_del_all(&delayed_mntput_list);\n\tstruct mount *m, *t;\n\n\tllist_for_each_entry_safe(m, t, node, mnt_llist)\n\t\tcleanup_mnt(m);\n}\nstatic DECLARE_DELAYED_WORK(delayed_mntput_work, delayed_mntput);\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\n\tLIST_HEAD(list);\n\tint count;\n\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tcount = mnt_get_count(mnt);\n\tif (count != 0) {\n\t\tWARN_ON(count < 0);\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n\t\t\thlist_add_head(&p->mnt_umount, &mnt->mnt_stuck_children);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tshrink_dentry_list(&list);\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, TWA_RESUME))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\n/**\n * path_is_mountpoint() - Check if path is a mount in the current namespace.\n * @path: path to check\n *\n *  d_mountpoint() can only be used reliably to establish if a dentry is\n *  not mounted in any namespace and that common case is handled inline.\n *  d_mountpoint() isn't aware of the possibility there may be multiple\n *  mounts using a given dentry in a different namespace. This function\n *  checks if the passed in path is a mountpoint rather than the dentry\n *  alone.\n */\nbool path_is_mountpoint(const struct path *path)\n{\n\tunsigned seq;\n\tbool res;\n\n\tif (!d_mountpoint(path->dentry))\n\t\treturn false;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tres = __path_is_mountpoint(path);\n\t} while (read_seqretry(&mount_lock, seq));\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_mountpoint);\n\nstruct vfsmount *mnt_clone_internal(const struct path *path)\n{\n\tstruct mount *p;\n\tp = clone_mnt(real_mount(path->mnt), path->dentry, CL_PRIVATE);\n\tif (IS_ERR(p))\n\t\treturn ERR_CAST(p);\n\tp->mnt.mnt_flags |= MNT_INTERNAL;\n\treturn &p->mnt;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic struct mount *mnt_list_next(struct mnt_namespace *ns,\n\t\t\t\t   struct list_head *p)\n{\n\tstruct mount *mnt, *ret = NULL;\n\n\tlock_ns_list(ns);\n\tlist_for_each_continue(p, &ns->list) {\n\t\tmnt = list_entry(p, typeof(*mnt), mnt_list);\n\t\tif (!mnt_is_cursor(mnt)) {\n\t\t\tret = mnt;\n\t\t\tbreak;\n\t\t}\n\t}\n\tunlock_ns_list(ns);\n\n\treturn ret;\n}\n\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct list_head *prev;\n\n\tdown_read(&namespace_sem);\n\tif (!*pos) {\n\t\tprev = &p->ns->list;\n\t} else {\n\t\tprev = &p->cursor.mnt_list;\n\n\t\t/* Read after we'd reached the end? */\n\t\tif (list_empty(prev))\n\t\t\treturn NULL;\n\t}\n\n\treturn mnt_list_next(p->ns, prev);\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *mnt = v;\n\n\t++*pos;\n\treturn mnt_list_next(p->ns, &mnt->mnt_list);\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *mnt = v;\n\n\tlock_ns_list(p->ns);\n\tif (mnt)\n\t\tlist_move_tail(&p->cursor.mnt_list, &mnt->mnt_list);\n\telse\n\t\tlist_del_init(&p->cursor.mnt_list);\n\tunlock_ns_list(p->ns);\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *r = v;\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n\nvoid mnt_cursor_del(struct mnt_namespace *ns, struct mount *cursor)\n{\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_del(&cursor->mnt_list);\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n}\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @m: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\tstruct hlist_node *p;\n\tstruct mount *m;\n\tLIST_HEAD(list);\n\n\thlist_move_list(&unmounted, &head);\n\tlist_splice_init(&ex_mountpoints, &list);\n\n\tup_write(&namespace_sem);\n\n\tshrink_dentry_list(&list);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu_expedited();\n\n\thlist_for_each_entry_safe(m, p, &head, mnt_umount) {\n\t\thlist_del(&m->mnt_umount);\n\t\tmntput(&m->mnt);\n\t}\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\nenum umount_tree_flags {\n\tUMOUNT_SYNC = 1,\n\tUMOUNT_PROPAGATE = 2,\n\tUMOUNT_CONNECTED = 4,\n};\n\nstatic bool disconnect_mount(struct mount *mnt, enum umount_tree_flags how)\n{\n\t/* Leaving mounts connected is only valid for lazy umounts */\n\tif (how & UMOUNT_SYNC)\n\t\treturn true;\n\n\t/* A mount without a parent has nothing to be connected to */\n\tif (!mnt_has_parent(mnt))\n\t\treturn true;\n\n\t/* Because the reference counting rules change when mounts are\n\t * unmounted and connected, umounted mounts may not be\n\t * connected to mounted mounts.\n\t */\n\tif (!(mnt->mnt_parent->mnt.mnt_flags & MNT_UMOUNT))\n\t\treturn true;\n\n\t/* Has it been requested that the mount remain connected? */\n\tif (how & UMOUNT_CONNECTED)\n\t\treturn false;\n\n\t/* Is the mount locked such that it needs to remain connected? */\n\tif (IS_MNT_LOCKED(mnt))\n\t\treturn false;\n\n\t/* By default disconnect the mount */\n\treturn true;\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n */\nstatic void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tstruct mnt_namespace *ns;\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\tns = p->mnt_ns;\n\t\tif (ns) {\n\t\t\tns->mounts--;\n\t\t\t__touch_mnt_namespace(ns);\n\t\t}\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t\tif (disconnect)\n\t\t\thlist_add_head(&p->mnt_umount, &unmounted);\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount_root(struct super_block *sb)\n{\n\tint ret = 0;\n\n\tdown_write(&sb->s_umount);\n\tif (!sb_rdonly(sb)) {\n\t\tstruct fs_context *fc;\n\n\t\tfc = fs_context_for_reconfigure(sb->s_root, SB_RDONLY,\n\t\t\t\t\t\tSB_RDONLY);\n\t\tif (IS_ERR(fc)) {\n\t\t\tret = PTR_ERR(fc);\n\t\t} else {\n\t\t\tret = parse_monolithic_mount_data(fc, NULL);\n\t\t\tif (!ret)\n\t\t\t\tret = reconfigure_super(fc);\n\t\t\tput_fs_context(fc);\n\t\t}\n\t}\n\tup_write(&sb->s_umount);\n\treturn ret;\n}\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tif (!ns_capable(sb->s_user_ns, CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\treturn do_umount_root(sb);\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* Recheck MNT_LOCKED with the locks held */\n\tretval = -EINVAL;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out;\n\n\tevent++;\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t\tretval = 0;\n\t\t}\n\t}\nout:\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/*\n * __detach_mounts - lazily unmount all mounts on the specified dentry\n *\n * During unlink, rmdir, and d_drop it is possible to loose the path\n * to an existing mountpoint, and wind up leaking the mount.\n * detach_mounts allows lazily unmounting those mounts instead of\n * leaking them.\n *\n * The caller may hold dentry->d_inode->i_mutex.\n */\nvoid __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tmp = lookup_mountpoint(dentry);\n\tif (!mp)\n\t\tgoto out_unlock;\n\n\tevent++;\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tumount_mnt(mnt);\n\t\t\thlist_add_head(&mnt->mnt_umount, &unmounted);\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tput_mountpoint(mp);\nout_unlock:\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\n/*\n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\n#ifdef\tCONFIG_MANDATORY_FILE_LOCKING\nstatic inline bool may_mandlock(void)\n{\n\treturn capable(CAP_SYS_ADMIN);\n}\n#else\nstatic inline bool may_mandlock(void)\n{\n\tpr_warn(\"VFS: \\\"mand\\\" mount option not supported\");\n\treturn false;\n}\n#endif\n\nstatic int can_umount(const struct path *path, int flags)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED) /* Check optimistically */\n\t\treturn -EINVAL;\n\tif (flags & MNT_FORCE && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\treturn 0;\n}\n\n// caller is responsible for flags being sane\nint path_umount(struct path *path, int flags)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint ret;\n\n\tret = can_umount(path, flags);\n\tif (!ret)\n\t\tret = do_umount(mnt, flags);\n\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path->dentry);\n\tmntput_no_expire(mnt);\n\treturn ret;\n}\n\nstatic int ksys_umount(char __user *name, int flags)\n{\n\tint lookup_flags = LOOKUP_MOUNTPOINT;\n\tstruct path path;\n\tint ret;\n\n\t// basic validity checks done first\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\tret = user_path_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (ret)\n\t\treturn ret;\n\treturn path_umount(&path, flags);\n}\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\treturn ksys_umount(name, flags);\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn ksys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\treturn dentry->d_op == &ns_dentry_operations &&\n\t       dentry->d_fsdata == &mntns_operations;\n}\n\nstatic struct mnt_namespace *to_mnt_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct mnt_namespace, ns);\n}\n\nstruct ns_common *from_mnt_ns(struct mnt_namespace *mnt)\n{\n\treturn &mnt->ns;\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = to_mnt_ns(get_proc_ns(dentry->d_inode));\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\tif (s->mnt.mnt_flags & MNT_LOCKED) {\n\t\t\t\t\t/* Both unbindable and locked. */\n\t\t\t\t\tq = ERR_PTR(-EPERM);\n\t\t\t\t\tgoto out;\n\t\t\t\t} else {\n\t\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tattach_mnt(q, parent, p->mnt_mp);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(const struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\tif (!check_mnt(real_mount(path->mnt)))\n\t\ttree = ERR_PTR(-EINVAL);\n\telse\n\t\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *);\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *, bool);\n\nvoid dissolve_on_fput(struct vfsmount *mnt)\n{\n\tstruct mnt_namespace *ns;\n\tnamespace_lock();\n\tlock_mount_hash();\n\tns = real_mount(mnt)->mnt_ns;\n\tif (ns) {\n\t\tif (is_anon_ns(ns))\n\t\t\tumount_tree(real_mount(mnt), UMOUNT_CONNECTED);\n\t\telse\n\t\t\tns = NULL;\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\tif (ns)\n\t\tfree_mnt_ns(ns);\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), 0);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\n/**\n * clone_private_mount - create a private clone of a path\n * @path: path to clone\n *\n * This creates a new vfsmount, which will be the clone of @path.  The new mount\n * will not be attached anywhere in the namespace and will be private (i.e.\n * changes to the originating mount won't be propagated into this).\n *\n * Release with mntput().\n */\nstruct vfsmount *clone_private_mount(const struct path *path)\n{\n\tstruct mount *old_mnt = real_mount(path->mnt);\n\tstruct mount *new_mnt;\n\n\tif (IS_MNT_UNBINDABLE(old_mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnew_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);\n\tif (IS_ERR(new_mnt))\n\t\treturn ERR_CAST(new_mnt);\n\n\t/* Longterm mount to be removed by kern_unmount*() */\n\tnew_mnt->mnt_ns = MNT_NS_INTERNAL;\n\n\treturn &new_mnt->mnt;\n}\nEXPORT_SYMBOL_GPL(clone_private_mount);\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void lock_mnt_tree(struct mount *mnt)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tint flags = p->mnt.mnt_flags;\n\t\t/* Don't allow unprivileged users to change mount flags */\n\t\tflags |= MNT_LOCK_ATIME;\n\n\t\tif (flags & MNT_READONLY)\n\t\t\tflags |= MNT_LOCK_READONLY;\n\n\t\tif (flags & MNT_NODEV)\n\t\t\tflags |= MNT_LOCK_NODEV;\n\n\t\tif (flags & MNT_NOSUID)\n\t\t\tflags |= MNT_LOCK_NOSUID;\n\n\t\tif (flags & MNT_NOEXEC)\n\t\t\tflags |= MNT_LOCK_NOEXEC;\n\t\t/* Don't allow unprivileged users to reveal what is under a mount */\n\t\tif (list_empty(&p->mnt_expire))\n\t\t\tflags |= MNT_LOCKED;\n\t\tp->mnt.mnt_flags = flags;\n\t}\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint count_mounts(struct mnt_namespace *ns, struct mount *mnt)\n{\n\tunsigned int max = READ_ONCE(sysctl_mount_max);\n\tunsigned int mounts = 0, old, pending, sum;\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt))\n\t\tmounts++;\n\n\told = ns->mounts;\n\tpending = ns->pending_mounts;\n\tsum = old + pending;\n\tif ((old > sum) ||\n\t    (pending > sum) ||\n\t    (max < sum) ||\n\t    (mounts > (max - sum)))\n\t\treturn -ENOSPC;\n\n\tns->pending_mounts = pending + mounts;\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tbool moving)\n{\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tHLIST_HEAD(tree_list);\n\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;\n\tstruct mountpoint *smp;\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\t/* Preallocate a mountpoint in case the new mounts need\n\t * to be tucked under other mounts.\n\t */\n\tsmp = get_mountpoint(source_mnt->mnt.mnt_root);\n\tif (IS_ERR(smp))\n\t\treturn PTR_ERR(smp);\n\n\t/* Is there space to add these mounts to the mount namespace? */\n\tif (!moving) {\n\t\terr = count_mounts(ns, source_mnt);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (moving) {\n\t\tunhash_mnt(source_mnt);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tif (source_mnt->mnt_ns) {\n\t\t\t/* move from anon - the caller will destroy */\n\t\t\tlist_del_init(&source_mnt->mnt_ns->list);\n\t\t}\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt(&child->mnt_parent->mnt,\n\t\t\t\t child->mnt_mountpoint);\n\t\tif (q)\n\t\t\tmnt_change_mountpoint(child, smp, q);\n\t\t/* Notice when we are propagating across user namespaces */\n\t\tif (child->mnt_parent->mnt_ns->user_ns != user_ns)\n\t\t\tlock_mnt_tree(child);\n\t\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\t\tcommit_tree(child);\n\t}\n\tput_mountpoint(smp);\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\tns->pending_mounts = 0;\n\n\tread_seqlock_excl(&mount_lock);\n\tput_mountpoint(smp);\n\tread_sequnlock_excl(&mount_lock);\n\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tinode_lock(dentry->d_inode);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tinode_unlock(dentry->d_inode);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = get_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tinode_unlock(dentry->d_inode);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tinode_unlock(path->dentry->d_inode);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\n\tread_seqlock_excl(&mount_lock);\n\tput_mountpoint(where);\n\tread_sequnlock_excl(&mount_lock);\n\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & SB_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (d_is_dir(mp->m_dentry) !=\n\t      d_is_dir(mnt->mnt.mnt_root))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, false);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int ms_flags)\n{\n\tint type = ms_flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int ms_flags)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = ms_flags & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(ms_flags);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct mount *__do_loopback(struct path *old_path, int recurse)\n{\n\tstruct mount *mnt = ERR_PTR(-EINVAL), *old = real_mount(old_path->mnt);\n\n\tif (IS_MNT_UNBINDABLE(old))\n\t\treturn mnt;\n\n\tif (!check_mnt(old) && old_path->dentry->d_op != &ns_dentry_operations)\n\t\treturn mnt;\n\n\tif (!recurse && has_locked_children(old, old_path->dentry))\n\t\treturn mnt;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path->dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path->dentry, 0);\n\n\tif (!IS_ERR(mnt))\n\t\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\treturn mnt;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp)) {\n\t\terr = PTR_ERR(mp);\n\t\tgoto out;\n\t}\n\n\tparent = real_mount(path->mnt);\n\tif (!check_mnt(parent))\n\t\tgoto out2;\n\n\tmnt = __do_loopback(&old_path, recurse);\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct file *open_detached_copy(struct path *path, bool recursive)\n{\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tstruct mnt_namespace *ns = alloc_mnt_ns(user_ns, true);\n\tstruct mount *mnt, *p;\n\tstruct file *file;\n\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\tnamespace_lock();\n\tmnt = __do_loopback(path, recursive);\n\tif (IS_ERR(mnt)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(ns);\n\t\treturn ERR_CAST(mnt);\n\t}\n\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt_ns = ns;\n\t\tns->mounts++;\n\t}\n\tns->root = mnt;\n\tlist_add_tail(&ns->list, &mnt->mnt_list);\n\tmntget(&mnt->mnt);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\n\tmntput(path->mnt);\n\tpath->mnt = &mnt->mnt;\n\tfile = dentry_open(path, O_PATH, current_cred());\n\tif (IS_ERR(file))\n\t\tdissolve_on_fput(path->mnt);\n\telse\n\t\tfile->f_mode |= FMODE_NEED_UNMOUNT;\n\treturn file;\n}\n\nSYSCALL_DEFINE3(open_tree, int, dfd, const char __user *, filename, unsigned, flags)\n{\n\tstruct file *file;\n\tstruct path path;\n\tint lookup_flags = LOOKUP_AUTOMOUNT | LOOKUP_FOLLOW;\n\tbool detached = flags & OPEN_TREE_CLONE;\n\tint error;\n\tint fd;\n\n\tBUILD_BUG_ON(OPEN_TREE_CLOEXEC != O_CLOEXEC);\n\n\tif (flags & ~(AT_EMPTY_PATH | AT_NO_AUTOMOUNT | AT_RECURSIVE |\n\t\t      AT_SYMLINK_NOFOLLOW | OPEN_TREE_CLONE |\n\t\t      OPEN_TREE_CLOEXEC))\n\t\treturn -EINVAL;\n\n\tif ((flags & (AT_RECURSIVE | OPEN_TREE_CLONE)) == AT_RECURSIVE)\n\t\treturn -EINVAL;\n\n\tif (flags & AT_NO_AUTOMOUNT)\n\t\tlookup_flags &= ~LOOKUP_AUTOMOUNT;\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\tlookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\n\tif (detached && !may_mount())\n\t\treturn -EPERM;\n\n\tfd = get_unused_fd_flags(flags & O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (unlikely(error)) {\n\t\tfile = ERR_PTR(error);\n\t} else {\n\t\tif (detached)\n\t\t\tfile = open_detached_copy(&path, flags & AT_RECURSIVE);\n\t\telse\n\t\t\tfile = dentry_open(&path, O_PATH, current_cred());\n\t\tpath_put(&path);\n\t}\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(fd);\n\t\treturn PTR_ERR(file);\n\t}\n\tfd_install(fd, file);\n\treturn fd;\n}\n\n/*\n * Don't allow locked mount flags to be cleared.\n *\n * No locks need to be held here while testing the various MNT_LOCK\n * flags because those flags can never be cleared once they are set.\n */\nstatic bool can_change_locked_flags(struct mount *mnt, unsigned int mnt_flags)\n{\n\tunsigned int fl = mnt->mnt.mnt_flags;\n\n\tif ((fl & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NODEV) &&\n\t    !(mnt_flags & MNT_NODEV))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NOSUID) &&\n\t    !(mnt_flags & MNT_NOSUID))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NOEXEC) &&\n\t    !(mnt_flags & MNT_NOEXEC))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_ATIME) &&\n\t    ((fl & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int change_mount_ro_state(struct mount *mnt, unsigned int mnt_flags)\n{\n\tbool readonly_request = (mnt_flags & MNT_READONLY);\n\n\tif (readonly_request == __mnt_is_readonly(&mnt->mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\treturn mnt_make_readonly(mnt);\n\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\treturn 0;\n}\n\nstatic void set_mount_attributes(struct mount *mnt, unsigned int mnt_flags)\n{\n\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\tmnt->mnt.mnt_flags = mnt_flags;\n\ttouch_mnt_namespace(mnt->mnt_ns);\n}\n\nstatic void mnt_warn_timestamp_expiry(struct path *mountpoint, struct vfsmount *mnt)\n{\n\tstruct super_block *sb = mnt->mnt_sb;\n\n\tif (!__mnt_is_readonly(mnt) &&\n\t   (ktime_get_real_seconds() + TIME_UPTIME_SEC_MAX > sb->s_time_max)) {\n\t\tchar *buf = (char *)__get_free_page(GFP_KERNEL);\n\t\tchar *mntpath = buf ? d_path(mountpoint, buf, PAGE_SIZE) : ERR_PTR(-ENOMEM);\n\t\tstruct tm tm;\n\n\t\ttime64_to_tm(sb->s_time_max, 0, &tm);\n\n\t\tpr_warn(\"%s filesystem being %s at %s supports timestamps until %04ld (0x%llx)\\n\",\n\t\t\tsb->s_type->name,\n\t\t\tis_mounted(mnt) ? \"remounted\" : \"mounted\",\n\t\t\tmntpath,\n\t\t\ttm.tm_year+1900, (unsigned long long)sb->s_time_max);\n\n\t\tfree_page((unsigned long)buf);\n\t}\n}\n\n/*\n * Handle reconfiguration of the mountpoint only without alteration of the\n * superblock it refers to.  This is triggered by specifying MS_REMOUNT|MS_BIND\n * to mount(2).\n */\nstatic int do_reconfigure_mnt(struct path *path, unsigned int mnt_flags)\n{\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint ret;\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != mnt->mnt.mnt_root)\n\t\treturn -EINVAL;\n\n\tif (!can_change_locked_flags(mnt, mnt_flags))\n\t\treturn -EPERM;\n\n\t/*\n\t * We're only checking whether the superblock is read-only not\n\t * changing it, so only take down_read(&sb->s_umount).\n\t */\n\tdown_read(&sb->s_umount);\n\tlock_mount_hash();\n\tret = change_mount_ro_state(mnt, mnt_flags);\n\tif (ret == 0)\n\t\tset_mount_attributes(mnt, mnt_flags);\n\tunlock_mount_hash();\n\tup_read(&sb->s_umount);\n\n\tmnt_warn_timestamp_expiry(path, &mnt->mnt);\n\n\treturn ret;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int ms_flags, int sb_flags,\n\t\t      int mnt_flags, void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tstruct fs_context *fc;\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\tif (!can_change_locked_flags(mnt, mnt_flags))\n\t\treturn -EPERM;\n\n\tfc = fs_context_for_reconfigure(path->dentry, sb_flags, MS_RMT_MASK);\n\tif (IS_ERR(fc))\n\t\treturn PTR_ERR(fc);\n\n\tfc->oldapi = true;\n\terr = parse_monolithic_mount_data(fc, data);\n\tif (!err) {\n\t\tdown_write(&sb->s_umount);\n\t\terr = -EPERM;\n\t\tif (ns_capable(sb->s_user_ns, CAP_SYS_ADMIN)) {\n\t\t\terr = reconfigure_super(fc);\n\t\t\tif (!err) {\n\t\t\t\tlock_mount_hash();\n\t\t\t\tset_mount_attributes(mnt, mnt_flags);\n\t\t\t\tunlock_mount_hash();\n\t\t\t}\n\t\t}\n\t\tup_write(&sb->s_umount);\n\t}\n\n\tmnt_warn_timestamp_expiry(path, &mnt->mnt);\n\n\tput_fs_context(fc);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Check that there aren't references to earlier/same mount namespaces in the\n * specified subtree.  Such references can act as pins for mount namespaces\n * that aren't checked by the mount-cycle checking code, thereby allowing\n * cycles to be made.\n */\nstatic bool check_for_nsfs_mounts(struct mount *subtree)\n{\n\tstruct mount *p;\n\tbool ret = false;\n\n\tlock_mount_hash();\n\tfor (p = subtree; p; p = next_mnt(p, subtree))\n\t\tif (mnt_ns_loop(p->mnt.mnt_root))\n\t\t\tgoto out;\n\n\tret = true;\nout:\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic int do_move_mount(struct path *old_path, struct path *new_path)\n{\n\tstruct mnt_namespace *ns;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mount *parent;\n\tstruct mountpoint *mp, *old_mp;\n\tint err;\n\tbool attached;\n\n\tmp = lock_mount(new_path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\told = real_mount(old_path->mnt);\n\tp = real_mount(new_path->mnt);\n\tparent = old->mnt_parent;\n\tattached = mnt_has_parent(old);\n\told_mp = old->mnt_mp;\n\tns = old->mnt_ns;\n\n\terr = -EINVAL;\n\t/* The mountpoint must be in our namespace. */\n\tif (!check_mnt(p))\n\t\tgoto out;\n\n\t/* The thing moved must be mounted... */\n\tif (!is_mounted(&old->mnt))\n\t\tgoto out;\n\n\t/* ... and either ours or the root of anon namespace */\n\tif (!(attached ? check_mnt(old) : is_anon_ns(ns)))\n\t\tgoto out;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out;\n\n\tif (old_path->dentry != old_path->mnt->mnt_root)\n\t\tgoto out;\n\n\tif (d_is_dir(new_path->dentry) !=\n\t    d_is_dir(old_path->dentry))\n\t\tgoto out;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (attached && IS_MNT_SHARED(parent))\n\t\tgoto out;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out;\n\terr = -ELOOP;\n\tif (!check_for_nsfs_mounts(old))\n\t\tgoto out;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out;\n\n\terr = attach_recursive_mnt(old, real_mount(new_path->mnt), mp,\n\t\t\t\t   attached);\n\tif (err)\n\t\tgoto out;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\n\tif (attached)\n\t\tput_mountpoint(old_mp);\nout:\n\tunlock_mount(mp);\n\tif (!err) {\n\t\tif (attached)\n\t\t\tmntput_no_expire(parent);\n\t\telse\n\t\t\tfree_mnt_ns(ns);\n\t}\n\treturn err;\n}\n\nstatic int do_move_mount_old(struct path *path, const char *old_name)\n{\n\tstruct path old_path;\n\tint err;\n\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = do_move_mount(&old_path, path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct mountpoint *mp,\n\t\t\tstruct path *path, int mnt_flags)\n{\n\tstruct mount *parent = real_mount(path->mnt);\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\treturn -EINVAL;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\treturn -EBUSY;\n\n\tif (d_is_symlink(newmnt->mnt.mnt_root))\n\t\treturn -EINVAL;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\treturn graft_tree(newmnt, parent, mp);\n}\n\nstatic bool mount_too_revealing(const struct super_block *sb, int *new_mnt_flags);\n\n/*\n * Create a new mount using a superblock configuration and request it\n * be added to the namespace tree.\n */\nstatic int do_new_mount_fc(struct fs_context *fc, struct path *mountpoint,\n\t\t\t   unsigned int mnt_flags)\n{\n\tstruct vfsmount *mnt;\n\tstruct mountpoint *mp;\n\tstruct super_block *sb = fc->root->d_sb;\n\tint error;\n\n\terror = security_sb_kern_mount(sb);\n\tif (!error && mount_too_revealing(sb, &mnt_flags))\n\t\terror = -EPERM;\n\n\tif (unlikely(error)) {\n\t\tfc_drop_locked(fc);\n\t\treturn error;\n\t}\n\n\tup_write(&sb->s_umount);\n\n\tmnt = vfs_create_mount(fc);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\tmnt_warn_timestamp_expiry(mountpoint, mnt);\n\n\tmp = lock_mount(mountpoint);\n\tif (IS_ERR(mp)) {\n\t\tmntput(mnt);\n\t\treturn PTR_ERR(mp);\n\t}\n\terror = do_add_mount(real_mount(mnt), mp, mountpoint, mnt_flags);\n\tunlock_mount(mp);\n\tif (error < 0)\n\t\tmntput(mnt);\n\treturn error;\n}\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int sb_flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct fs_context *fc;\n\tconst char *subtype = NULL;\n\tint err = 0;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tif (type->fs_flags & FS_HAS_SUBTYPE) {\n\t\tsubtype = strchr(fstype, '.');\n\t\tif (subtype) {\n\t\t\tsubtype++;\n\t\t\tif (!*subtype) {\n\t\t\t\tput_filesystem(type);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\tfc = fs_context_for_mount(type, sb_flags);\n\tput_filesystem(type);\n\tif (IS_ERR(fc))\n\t\treturn PTR_ERR(fc);\n\n\tif (subtype)\n\t\terr = vfs_parse_fs_string(fc, \"subtype\",\n\t\t\t\t\t  subtype, strlen(subtype));\n\tif (!err && name)\n\t\terr = vfs_parse_fs_string(fc, \"source\", name, strlen(name));\n\tif (!err)\n\t\terr = parse_monolithic_mount_data(fc, data);\n\tif (!err && !mount_capable(fc))\n\t\terr = -EPERM;\n\tif (!err)\n\t\terr = vfs_get_tree(fc);\n\tif (!err)\n\t\terr = do_new_mount_fc(fc, path, mnt_flags);\n\n\tput_fs_context(fc);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct dentry *dentry = path->dentry;\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\tint err;\n\n\tif (!m)\n\t\treturn 0;\n\tif (IS_ERR(m))\n\t\treturn PTR_ERR(m);\n\n\tmnt = real_mount(m);\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == dentry) {\n\t\terr = -ELOOP;\n\t\tgoto discard;\n\t}\n\n\t/*\n\t * we don't want to use lock_mount() - in this case finding something\n\t * that overmounts our mountpoint to be means \"quitely drop what we've\n\t * got\", not \"try to mount it on top\".\n\t */\n\tinode_lock(dentry->d_inode);\n\tnamespace_lock();\n\tif (unlikely(cant_mount(dentry))) {\n\t\terr = -ENOENT;\n\t\tgoto discard_locked;\n\t}\n\trcu_read_lock();\n\tif (unlikely(__lookup_mnt(path->mnt, dentry))) {\n\t\trcu_read_unlock();\n\t\terr = 0;\n\t\tgoto discard_locked;\n\t}\n\trcu_read_unlock();\n\tmp = get_mountpoint(dentry);\n\tif (IS_ERR(mp)) {\n\t\terr = PTR_ERR(mp);\n\t\tgoto discard_locked;\n\t}\n\n\terr = do_add_mount(mnt, mp, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tunlock_mount(mp);\n\tif (unlikely(err))\n\t\tgoto discard;\n\tmntput(m);\n\treturn 0;\n\ndiscard_locked:\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\ndiscard:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t}\n\t}\n}\n\nstatic void *copy_mount_options(const void __user * data)\n{\n\tchar *copy;\n\tunsigned left, offset;\n\n\tif (!data)\n\t\treturn NULL;\n\n\tcopy = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!copy)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tleft = copy_from_user(copy, data, PAGE_SIZE);\n\n\t/*\n\t * Not all architectures have an exact copy_from_user(). Resort to\n\t * byte at a time.\n\t */\n\toffset = PAGE_SIZE - left;\n\twhile (left) {\n\t\tchar c;\n\t\tif (get_user(c, (const char __user *)data + offset))\n\t\t\tbreak;\n\t\tcopy[offset] = c;\n\t\tleft--;\n\t\toffset++;\n\t}\n\n\tif (left == PAGE_SIZE) {\n\t\tkfree(copy);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn copy;\n}\n\nstatic char *copy_mount_string(const void __user *data)\n{\n\treturn data ? strndup_user(data, PATH_MAX) : NULL;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nint path_mount(const char *dev_name, struct path *path,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tunsigned int mnt_flags = 0, sb_flags;\n\tint ret;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\tif (flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tret = security_sb_mount(dev_name, path, type_page, flags, data_page);\n\tif (ret)\n\t\treturn ret;\n\tif (!may_mount())\n\t\treturn -EPERM;\n\tif ((flags & SB_MANDLOCK) && !may_mandlock())\n\t\treturn -EPERM;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\tif (flags & MS_NOSYMFOLLOW)\n\t\tmnt_flags |= MNT_NOSYMFOLLOW;\n\n\t/* The default atime for remount is preservation */\n\tif ((flags & MS_REMOUNT) &&\n\t    ((flags & (MS_NOATIME | MS_NODIRATIME | MS_RELATIME |\n\t\t       MS_STRICTATIME)) == 0)) {\n\t\tmnt_flags &= ~MNT_ATIME_MASK;\n\t\tmnt_flags |= path->mnt->mnt_flags & MNT_ATIME_MASK;\n\t}\n\n\tsb_flags = flags & (SB_RDONLY |\n\t\t\t    SB_SYNCHRONOUS |\n\t\t\t    SB_MANDLOCK |\n\t\t\t    SB_DIRSYNC |\n\t\t\t    SB_SILENT |\n\t\t\t    SB_POSIXACL |\n\t\t\t    SB_LAZYTIME |\n\t\t\t    SB_I_VERSION);\n\n\tif ((flags & (MS_REMOUNT | MS_BIND)) == (MS_REMOUNT | MS_BIND))\n\t\treturn do_reconfigure_mnt(path, mnt_flags);\n\tif (flags & MS_REMOUNT)\n\t\treturn do_remount(path, flags, sb_flags, mnt_flags, data_page);\n\tif (flags & MS_BIND)\n\t\treturn do_loopback(path, dev_name, flags & MS_REC);\n\tif (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn do_change_type(path, flags);\n\tif (flags & MS_MOVE)\n\t\treturn do_move_mount_old(path, dev_name);\n\n\treturn do_new_mount(path, type_page, sb_flags, mnt_flags, dev_name,\n\t\t\t    data_page);\n}\n\nlong do_mount(const char *dev_name, const char __user *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint ret;\n\n\tret = user_path_at(AT_FDCWD, dir_name, LOOKUP_FOLLOW, &path);\n\tif (ret)\n\t\treturn ret;\n\tret = path_mount(dev_name, &path, type_page, flags, data_page);\n\tpath_put(&path);\n\treturn ret;\n}\n\nstatic struct ucounts *inc_mnt_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void dec_mnt_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!is_anon_ns(ns))\n\t\tns_free_inum(&ns->ns);\n\tdec_mnt_namespaces(ns->ucounts);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns, bool anon)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kzalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (!anon) {\n\t\tret = ns_alloc_inum(&new_ns->ns);\n\t\tif (ret) {\n\t\t\tkfree(new_ns);\n\t\t\tdec_mnt_namespaces(ucounts);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tif (!anon)\n\t\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\trefcount_set(&new_ns->ns.count, 1);\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tspin_lock_init(&new_ns->ns_lock);\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\treturn new_ns;\n}\n\n__latent_entropy\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns, false);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tif (user_ns != ns->user_ns) {\n\t\tlock_mount_hash();\n\t\tlock_mnt_tree(new);\n\t\tunlock_mount_hash();\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tnew_ns->mounts++;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *m, const char *name)\n{\n\tstruct mount *mnt = real_mount(m);\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = alloc_mnt_ns(&init_user_ns, true);\n\tif (IS_ERR(ns)) {\n\t\tmntput(m);\n\t\treturn ERR_CAST(ns);\n\t}\n\tmnt->mnt_ns = ns;\n\tns->root = mnt;\n\tns->mounts++;\n\tlist_add(&mnt->mnt_list, &ns->list);\n\n\terr = vfs_path_lookup(m->mnt_root, m,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tchar *kernel_dev;\n\tvoid *options;\n\n\tkernel_type = copy_mount_string(type);\n\tret = PTR_ERR(kernel_type);\n\tif (IS_ERR(kernel_type))\n\t\tgoto out_type;\n\n\tkernel_dev = copy_mount_string(dev_name);\n\tret = PTR_ERR(kernel_dev);\n\tif (IS_ERR(kernel_dev))\n\t\tgoto out_dev;\n\n\toptions = copy_mount_options(data);\n\tret = PTR_ERR(options);\n\tif (IS_ERR(options))\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);\n\n\tkfree(options);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n#define FSMOUNT_VALID_FLAGS                                                    \\\n\t(MOUNT_ATTR_RDONLY | MOUNT_ATTR_NOSUID | MOUNT_ATTR_NODEV |            \\\n\t MOUNT_ATTR_NOEXEC | MOUNT_ATTR__ATIME | MOUNT_ATTR_NODIRATIME |       \\\n\t MOUNT_ATTR_NOSYMFOLLOW)\n\n#define MOUNT_SETATTR_VALID_FLAGS (FSMOUNT_VALID_FLAGS | MOUNT_ATTR_IDMAP)\n\n#define MOUNT_SETATTR_PROPAGATION_FLAGS \\\n\t(MS_UNBINDABLE | MS_PRIVATE | MS_SLAVE | MS_SHARED)\n\nstatic unsigned int attr_flags_to_mnt_flags(u64 attr_flags)\n{\n\tunsigned int mnt_flags = 0;\n\n\tif (attr_flags & MOUNT_ATTR_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\tif (attr_flags & MOUNT_ATTR_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (attr_flags & MOUNT_ATTR_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (attr_flags & MOUNT_ATTR_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (attr_flags & MOUNT_ATTR_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (attr_flags & MOUNT_ATTR_NOSYMFOLLOW)\n\t\tmnt_flags |= MNT_NOSYMFOLLOW;\n\n\treturn mnt_flags;\n}\n\n/*\n * Create a kernel mount representation for a new, prepared superblock\n * (specified by fs_fd) and attach to an open_tree-like file descriptor.\n */\nSYSCALL_DEFINE3(fsmount, int, fs_fd, unsigned int, flags,\n\t\tunsigned int, attr_flags)\n{\n\tstruct mnt_namespace *ns;\n\tstruct fs_context *fc;\n\tstruct file *file;\n\tstruct path newmount;\n\tstruct mount *mnt;\n\tstruct fd f;\n\tunsigned int mnt_flags = 0;\n\tlong ret;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif ((flags & ~(FSMOUNT_CLOEXEC)) != 0)\n\t\treturn -EINVAL;\n\n\tif (attr_flags & ~FSMOUNT_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tmnt_flags = attr_flags_to_mnt_flags(attr_flags);\n\n\tswitch (attr_flags & MOUNT_ATTR__ATIME) {\n\tcase MOUNT_ATTR_STRICTATIME:\n\t\tbreak;\n\tcase MOUNT_ATTR_NOATIME:\n\t\tmnt_flags |= MNT_NOATIME;\n\t\tbreak;\n\tcase MOUNT_ATTR_RELATIME:\n\t\tmnt_flags |= MNT_RELATIME;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tf = fdget(fs_fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EINVAL;\n\tif (f.file->f_op != &fscontext_fops)\n\t\tgoto err_fsfd;\n\n\tfc = f.file->private_data;\n\n\tret = mutex_lock_interruptible(&fc->uapi_mutex);\n\tif (ret < 0)\n\t\tgoto err_fsfd;\n\n\t/* There must be a valid superblock or we can't mount it */\n\tret = -EINVAL;\n\tif (!fc->root)\n\t\tgoto err_unlock;\n\n\tret = -EPERM;\n\tif (mount_too_revealing(fc->root->d_sb, &mnt_flags)) {\n\t\tpr_warn(\"VFS: Mount too revealing\\n\");\n\t\tgoto err_unlock;\n\t}\n\n\tret = -EBUSY;\n\tif (fc->phase != FS_CONTEXT_AWAITING_MOUNT)\n\t\tgoto err_unlock;\n\n\tret = -EPERM;\n\tif ((fc->sb_flags & SB_MANDLOCK) && !may_mandlock())\n\t\tgoto err_unlock;\n\n\tnewmount.mnt = vfs_create_mount(fc);\n\tif (IS_ERR(newmount.mnt)) {\n\t\tret = PTR_ERR(newmount.mnt);\n\t\tgoto err_unlock;\n\t}\n\tnewmount.dentry = dget(fc->root);\n\tnewmount.mnt->mnt_flags = mnt_flags;\n\n\t/* We've done the mount bit - now move the file context into more or\n\t * less the same state as if we'd done an fspick().  We don't want to\n\t * do any memory allocation or anything like that at this point as we\n\t * don't want to have to handle any errors incurred.\n\t */\n\tvfs_clean_context(fc);\n\n\tns = alloc_mnt_ns(current->nsproxy->mnt_ns->user_ns, true);\n\tif (IS_ERR(ns)) {\n\t\tret = PTR_ERR(ns);\n\t\tgoto err_path;\n\t}\n\tmnt = real_mount(newmount.mnt);\n\tmnt->mnt_ns = ns;\n\tns->root = mnt;\n\tns->mounts = 1;\n\tlist_add(&mnt->mnt_list, &ns->list);\n\tmntget(newmount.mnt);\n\n\t/* Attach to an apparent O_PATH fd with a note that we need to unmount\n\t * it, not just simply put it.\n\t */\n\tfile = dentry_open(&newmount, O_PATH, fc->cred);\n\tif (IS_ERR(file)) {\n\t\tdissolve_on_fput(newmount.mnt);\n\t\tret = PTR_ERR(file);\n\t\tgoto err_path;\n\t}\n\tfile->f_mode |= FMODE_NEED_UNMOUNT;\n\n\tret = get_unused_fd_flags((flags & FSMOUNT_CLOEXEC) ? O_CLOEXEC : 0);\n\tif (ret >= 0)\n\t\tfd_install(ret, file);\n\telse\n\t\tfput(file);\n\nerr_path:\n\tpath_put(&newmount);\nerr_unlock:\n\tmutex_unlock(&fc->uapi_mutex);\nerr_fsfd:\n\tfdput(f);\n\treturn ret;\n}\n\n/*\n * Move a mount from one place to another.  In combination with\n * fsopen()/fsmount() this is used to install a new mount and in combination\n * with open_tree(OPEN_TREE_CLONE [| AT_RECURSIVE]) it can be used to copy\n * a mount subtree.\n *\n * Note the flags value is a combination of MOVE_MOUNT_* flags.\n */\nSYSCALL_DEFINE5(move_mount,\n\t\tint, from_dfd, const char __user *, from_pathname,\n\t\tint, to_dfd, const char __user *, to_pathname,\n\t\tunsigned int, flags)\n{\n\tstruct path from_path, to_path;\n\tunsigned int lflags;\n\tint ret = 0;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (flags & ~MOVE_MOUNT__MASK)\n\t\treturn -EINVAL;\n\n\t/* If someone gives a pathname, they aren't permitted to move\n\t * from an fd that requires unmount as we can't get at the flag\n\t * to clear it afterwards.\n\t */\n\tlflags = 0;\n\tif (flags & MOVE_MOUNT_F_SYMLINKS)\tlflags |= LOOKUP_FOLLOW;\n\tif (flags & MOVE_MOUNT_F_AUTOMOUNTS)\tlflags |= LOOKUP_AUTOMOUNT;\n\tif (flags & MOVE_MOUNT_F_EMPTY_PATH)\tlflags |= LOOKUP_EMPTY;\n\n\tret = user_path_at(from_dfd, from_pathname, lflags, &from_path);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tlflags = 0;\n\tif (flags & MOVE_MOUNT_T_SYMLINKS)\tlflags |= LOOKUP_FOLLOW;\n\tif (flags & MOVE_MOUNT_T_AUTOMOUNTS)\tlflags |= LOOKUP_AUTOMOUNT;\n\tif (flags & MOVE_MOUNT_T_EMPTY_PATH)\tlflags |= LOOKUP_EMPTY;\n\n\tret = user_path_at(to_dfd, to_pathname, lflags, &to_path);\n\tif (ret < 0)\n\t\tgoto out_from;\n\n\tret = security_move_mount(&from_path, &to_path);\n\tif (ret < 0)\n\t\tgoto out_to;\n\n\tret = do_move_mount(&from_path, &to_path);\n\nout_to:\n\tpath_put(&to_path);\nout_from:\n\tpath_put(&from_path);\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nbool path_is_under(const struct path *path1, const struct path *path2)\n{\n\tbool res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.rst for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt, *root_parent, *ex_parent;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_at(AT_FDCWD, new_root,\n\t\t\t     LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_at(AT_FDCWD, put_old,\n\t\t\t     LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tex_parent = new_mnt->mnt_parent;\n\troot_parent = root_mnt->mnt_parent;\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(ex_parent) ||\n\t\tIS_MNT_SHARED(root_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\t/* make certain new is below the root */\n\tif (!is_path_reachable(new_mnt, new.dentry, &root))\n\t\tgoto out4;\n\tlock_mount_hash();\n\tumount_mnt(new_mnt);\n\troot_mp = unhash_mnt(root_mnt);  /* we'll need its mountpoint */\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, root_parent, root_mp);\n\tmnt_add_count(root_parent, -1);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\t/* A moved mount should not expire automatically */\n\tlist_del_init(&new_mnt->mnt_expire);\n\tput_mountpoint(root_mp);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error)\n\t\tmntput_no_expire(ex_parent);\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic unsigned int recalc_flags(struct mount_kattr *kattr, struct mount *mnt)\n{\n\tunsigned int flags = mnt->mnt.mnt_flags;\n\n\t/*  flags to clear */\n\tflags &= ~kattr->attr_clr;\n\t/* flags to raise */\n\tflags |= kattr->attr_set;\n\n\treturn flags;\n}\n\nstatic int can_idmap_mount(const struct mount_kattr *kattr, struct mount *mnt)\n{\n\tstruct vfsmount *m = &mnt->mnt;\n\n\tif (!kattr->mnt_userns)\n\t\treturn 0;\n\n\t/*\n\t * Once a mount has been idmapped we don't allow it to change its\n\t * mapping. It makes things simpler and callers can just create\n\t * another bind-mount they can idmap if they want to.\n\t */\n\tif (mnt_user_ns(m) != &init_user_ns)\n\t\treturn -EPERM;\n\n\t/* The underlying filesystem doesn't support idmapped mounts yet. */\n\tif (!(m->mnt_sb->s_type->fs_flags & FS_ALLOW_IDMAP))\n\t\treturn -EINVAL;\n\n\t/* Don't yet support filesystem mountable in user namespaces. */\n\tif (m->mnt_sb->s_user_ns != &init_user_ns)\n\t\treturn -EINVAL;\n\n\t/* We're not controlling the superblock. */\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* Mount has already been visible in the filesystem hierarchy. */\n\tif (!is_anon_ns(mnt->mnt_ns))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct mount *mount_setattr_prepare(struct mount_kattr *kattr,\n\t\t\t\t\t   struct mount *mnt, int *err)\n{\n\tstruct mount *m = mnt, *last = NULL;\n\n\tif (!is_mounted(&m->mnt)) {\n\t\t*err = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!(mnt_has_parent(m) ? check_mnt(m) : is_anon_ns(m->mnt_ns))) {\n\t\t*err = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tunsigned int flags;\n\n\t\tflags = recalc_flags(kattr, m);\n\t\tif (!can_change_locked_flags(m, flags)) {\n\t\t\t*err = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*err = can_idmap_mount(kattr, m);\n\t\tif (*err)\n\t\t\tgoto out;\n\n\t\tlast = m;\n\n\t\tif ((kattr->attr_set & MNT_READONLY) &&\n\t\t    !(m->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\t*err = mnt_hold_writers(m);\n\t\t\tif (*err)\n\t\t\t\tgoto out;\n\t\t}\n\t} while (kattr->recurse && (m = next_mnt(m, mnt)));\n\nout:\n\treturn last;\n}\n\nstatic void do_idmap_mount(const struct mount_kattr *kattr, struct mount *mnt)\n{\n\tstruct user_namespace *mnt_userns;\n\n\tif (!kattr->mnt_userns)\n\t\treturn;\n\n\tmnt_userns = get_user_ns(kattr->mnt_userns);\n\t/* Pairs with smp_load_acquire() in mnt_user_ns(). */\n\tsmp_store_release(&mnt->mnt.mnt_userns, mnt_userns);\n}\n\nstatic void mount_setattr_commit(struct mount_kattr *kattr,\n\t\t\t\t struct mount *mnt, struct mount *last,\n\t\t\t\t int err)\n{\n\tstruct mount *m = mnt;\n\n\tdo {\n\t\tif (!err) {\n\t\t\tunsigned int flags;\n\n\t\t\tdo_idmap_mount(kattr, m);\n\t\t\tflags = recalc_flags(kattr, m);\n\t\t\tWRITE_ONCE(m->mnt.mnt_flags, flags);\n\t\t}\n\n\t\t/*\n\t\t * We either set MNT_READONLY above so make it visible\n\t\t * before ~MNT_WRITE_HOLD or we failed to recursively\n\t\t * apply mount options.\n\t\t */\n\t\tif ((kattr->attr_set & MNT_READONLY) &&\n\t\t    (m->mnt.mnt_flags & MNT_WRITE_HOLD))\n\t\t\tmnt_unhold_writers(m);\n\n\t\tif (!err && kattr->propagation)\n\t\t\tchange_mnt_propagation(m, kattr->propagation);\n\n\t\t/*\n\t\t * On failure, only cleanup until we found the first mount\n\t\t * we failed to handle.\n\t\t */\n\t\tif (err && m == last)\n\t\t\tbreak;\n\t} while (kattr->recurse && (m = next_mnt(m, mnt)));\n\n\tif (!err)\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n}\n\nstatic int do_mount_setattr(struct path *path, struct mount_kattr *kattr)\n{\n\tstruct mount *mnt = real_mount(path->mnt), *last = NULL;\n\tint err = 0;\n\n\tif (path->dentry != mnt->mnt.mnt_root)\n\t\treturn -EINVAL;\n\n\tif (kattr->propagation) {\n\t\t/*\n\t\t * Only take namespace_lock() if we're actually changing\n\t\t * propagation.\n\t\t */\n\t\tnamespace_lock();\n\t\tif (kattr->propagation == MS_SHARED) {\n\t\t\terr = invent_group_ids(mnt, kattr->recurse);\n\t\t\tif (err) {\n\t\t\t\tnamespace_unlock();\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\tlock_mount_hash();\n\n\t/*\n\t * Get the mount tree in a shape where we can change mount\n\t * properties without failure.\n\t */\n\tlast = mount_setattr_prepare(kattr, mnt, &err);\n\tif (last) /* Commit all changes or revert to the old state. */\n\t\tmount_setattr_commit(kattr, mnt, last, err);\n\n\tunlock_mount_hash();\n\n\tif (kattr->propagation) {\n\t\tnamespace_unlock();\n\t\tif (err)\n\t\t\tcleanup_group_ids(mnt, NULL);\n\t}\n\n\treturn err;\n}\n\nstatic int build_mount_idmapped(const struct mount_attr *attr, size_t usize,\n\t\t\t\tstruct mount_kattr *kattr, unsigned int flags)\n{\n\tint err = 0;\n\tstruct ns_common *ns;\n\tstruct user_namespace *mnt_userns;\n\tstruct file *file;\n\n\tif (!((attr->attr_set | attr->attr_clr) & MOUNT_ATTR_IDMAP))\n\t\treturn 0;\n\n\t/*\n\t * We currently do not support clearing an idmapped mount. If this ever\n\t * is a use-case we can revisit this but for now let's keep it simple\n\t * and not allow it.\n\t */\n\tif (attr->attr_clr & MOUNT_ATTR_IDMAP)\n\t\treturn -EINVAL;\n\n\tif (attr->userns_fd > INT_MAX)\n\t\treturn -EINVAL;\n\n\tfile = fget(attr->userns_fd);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tif (!proc_ns_file(file)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tns = get_proc_ns(file_inode(file));\n\tif (ns->ops->type != CLONE_NEWUSER) {\n\t\terr = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\t/*\n\t * The init_user_ns is used to indicate that a vfsmount is not idmapped.\n\t * This is simpler than just having to treat NULL as unmapped. Users\n\t * wanting to idmap a mount to init_user_ns can just use a namespace\n\t * with an identity mapping.\n\t */\n\tmnt_userns = container_of(ns, struct user_namespace, ns);\n\tif (mnt_userns == &init_user_ns) {\n\t\terr = -EPERM;\n\t\tgoto out_fput;\n\t}\n\tkattr->mnt_userns = get_user_ns(mnt_userns);\n\nout_fput:\n\tfput(file);\n\treturn err;\n}\n\nstatic int build_mount_kattr(const struct mount_attr *attr, size_t usize,\n\t\t\t     struct mount_kattr *kattr, unsigned int flags)\n{\n\tunsigned int lookup_flags = LOOKUP_AUTOMOUNT | LOOKUP_FOLLOW;\n\n\tif (flags & AT_NO_AUTOMOUNT)\n\t\tlookup_flags &= ~LOOKUP_AUTOMOUNT;\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\tlookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\n\t*kattr = (struct mount_kattr) {\n\t\t.lookup_flags\t= lookup_flags,\n\t\t.recurse\t= !!(flags & AT_RECURSIVE),\n\t};\n\n\tif (attr->propagation & ~MOUNT_SETATTR_PROPAGATION_FLAGS)\n\t\treturn -EINVAL;\n\tif (hweight32(attr->propagation & MOUNT_SETATTR_PROPAGATION_FLAGS) > 1)\n\t\treturn -EINVAL;\n\tkattr->propagation = attr->propagation;\n\n\tif ((attr->attr_set | attr->attr_clr) & ~MOUNT_SETATTR_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tkattr->attr_set = attr_flags_to_mnt_flags(attr->attr_set);\n\tkattr->attr_clr = attr_flags_to_mnt_flags(attr->attr_clr);\n\n\t/*\n\t * Since the MOUNT_ATTR_<atime> values are an enum, not a bitmap,\n\t * users wanting to transition to a different atime setting cannot\n\t * simply specify the atime setting in @attr_set, but must also\n\t * specify MOUNT_ATTR__ATIME in the @attr_clr field.\n\t * So ensure that MOUNT_ATTR__ATIME can't be partially set in\n\t * @attr_clr and that @attr_set can't have any atime bits set if\n\t * MOUNT_ATTR__ATIME isn't set in @attr_clr.\n\t */\n\tif (attr->attr_clr & MOUNT_ATTR__ATIME) {\n\t\tif ((attr->attr_clr & MOUNT_ATTR__ATIME) != MOUNT_ATTR__ATIME)\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * Clear all previous time settings as they are mutually\n\t\t * exclusive.\n\t\t */\n\t\tkattr->attr_clr |= MNT_RELATIME | MNT_NOATIME;\n\t\tswitch (attr->attr_set & MOUNT_ATTR__ATIME) {\n\t\tcase MOUNT_ATTR_RELATIME:\n\t\t\tkattr->attr_set |= MNT_RELATIME;\n\t\t\tbreak;\n\t\tcase MOUNT_ATTR_NOATIME:\n\t\t\tkattr->attr_set |= MNT_NOATIME;\n\t\t\tbreak;\n\t\tcase MOUNT_ATTR_STRICTATIME:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (attr->attr_set & MOUNT_ATTR__ATIME)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn build_mount_idmapped(attr, usize, kattr, flags);\n}\n\nstatic void finish_mount_kattr(struct mount_kattr *kattr)\n{\n\tput_user_ns(kattr->mnt_userns);\n\tkattr->mnt_userns = NULL;\n}\n\nSYSCALL_DEFINE5(mount_setattr, int, dfd, const char __user *, path,\n\t\tunsigned int, flags, struct mount_attr __user *, uattr,\n\t\tsize_t, usize)\n{\n\tint err;\n\tstruct path target;\n\tstruct mount_attr attr;\n\tstruct mount_kattr kattr;\n\n\tBUILD_BUG_ON(sizeof(struct mount_attr) != MOUNT_ATTR_SIZE_VER0);\n\n\tif (flags & ~(AT_EMPTY_PATH |\n\t\t      AT_RECURSIVE |\n\t\t      AT_SYMLINK_NOFOLLOW |\n\t\t      AT_NO_AUTOMOUNT))\n\t\treturn -EINVAL;\n\n\tif (unlikely(usize > PAGE_SIZE))\n\t\treturn -E2BIG;\n\tif (unlikely(usize < MOUNT_ATTR_SIZE_VER0))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terr = copy_struct_from_user(&attr, sizeof(attr), uattr, usize);\n\tif (err)\n\t\treturn err;\n\n\t/* Don't bother walking through the mounts if this is a nop. */\n\tif (attr.attr_set == 0 &&\n\t    attr.attr_clr == 0 &&\n\t    attr.propagation == 0)\n\t\treturn 0;\n\n\terr = build_mount_kattr(&attr, usize, &kattr, flags);\n\tif (err)\n\t\treturn err;\n\n\terr = user_path_at(dfd, path, kattr.lookup_flags, &target);\n\tif (err)\n\t\treturn err;\n\n\terr = do_mount_setattr(&target, &kattr);\n\tfinish_mount_kattr(&kattr);\n\tpath_put(&target);\n\treturn err;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mount *m;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\n\tmnt = vfs_kern_mount(&rootfs_fs_type, 0, \"rootfs\", NULL);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = alloc_mnt_ns(&init_user_ns, false);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\tm = real_mount(mnt);\n\tm->mnt_ns = ns;\n\tns->root = m;\n\tns->mounts = 1;\n\tlist_add(&m->mnt_list, &ns->list);\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\tmnt->mnt_flags |= MNT_LOCKED;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\tHASH_ZERO,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\tHASH_ZERO,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tshmem_init();\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!refcount_dec_and_test(&ns->ns.count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount(struct file_system_type *type)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, SB_KERNMOUNT, type->name, NULL);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nvoid kern_unmount_array(struct vfsmount *mnt[], unsigned int num)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < num; i++)\n\t\tif (mnt[i])\n\t\t\treal_mount(mnt[i])->mnt_ns = NULL;\n\tsynchronize_rcu_expedited();\n\tfor (i = 0; i < num; i++)\n\t\tmntput(mnt[i]);\n}\nEXPORT_SYMBOL(kern_unmount_array);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nstatic bool mnt_already_visible(struct mnt_namespace *ns,\n\t\t\t\tconst struct super_block *sb,\n\t\t\t\tint *new_mnt_flags)\n{\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt_is_cursor(mnt))\n\t\t\tcontinue;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != sb->s_type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* A local view of the mount flags */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\n\t\t/* Don't miss readonly hidden in the superblock flags */\n\t\tif (sb_rdonly(mnt->mnt.mnt_sb))\n\t\t\tmnt_flags |= MNT_LOCK_READONLY;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic bool mount_too_revealing(const struct super_block *sb, int *new_mnt_flags)\n{\n\tconst unsigned long required_iflags = SB_I_NOEXEC | SB_I_NODEV;\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tunsigned long s_iflags;\n\n\tif (ns->user_ns == &init_user_ns)\n\t\treturn false;\n\n\t/* Can this filesystem be too revealing? */\n\ts_iflags = sb->s_iflags;\n\tif (!(s_iflags & SB_I_USERNS_VISIBLE))\n\t\treturn false;\n\n\tif ((s_iflags & required_iflags) != required_iflags) {\n\t\tWARN_ONCE(1, \"Expected s_iflags to contain 0x%lx\\n\",\n\t\t\t  required_iflags);\n\t\treturn true;\n\t}\n\n\treturn !mnt_already_visible(ns, sb, new_mnt_flags);\n}\n\nbool mnt_may_suid(struct vfsmount *mnt)\n{\n\t/*\n\t * Foreign mounts (accessed via fchdir or through /proc\n\t * symlinks) are always treated as if they are nosuid.  This\n\t * prevents namespaces from trusting potentially unsafe\n\t * suid/sgid bits, file caps, or security labels that originate\n\t * in other namespaces.\n\t */\n\treturn !(mnt->mnt_flags & MNT_NOSUID) && check_mnt(real_mount(mnt)) &&\n\t       current_in_userns(mnt->mnt_sb->s_user_ns);\n}\n\nstatic struct ns_common *mntns_get(struct task_struct *task)\n{\n\tstruct ns_common *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = &nsproxy->mnt_ns->ns;\n\t\tget_mnt_ns(to_mnt_ns(ns));\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(struct ns_common *ns)\n{\n\tput_mnt_ns(to_mnt_ns(ns));\n}\n\nstatic int mntns_install(struct nsset *nsset, struct ns_common *ns)\n{\n\tstruct nsproxy *nsproxy = nsset->nsproxy;\n\tstruct fs_struct *fs = nsset->fs;\n\tstruct mnt_namespace *mnt_ns = to_mnt_ns(ns), *old_mnt_ns;\n\tstruct user_namespace *user_ns = nsset->cred->user_ns;\n\tstruct path root;\n\tint err;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(user_ns, CAP_SYS_CHROOT) ||\n\t    !ns_capable(user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (is_anon_ns(mnt_ns))\n\t\treturn -EINVAL;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\told_mnt_ns = nsproxy->mnt_ns;\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\terr = vfs_path_lookup(mnt_ns->root->mnt.mnt_root, &mnt_ns->root->mnt,\n\t\t\t\t\"/\", LOOKUP_DOWN, &root);\n\tif (err) {\n\t\t/* revert to old namespace */\n\t\tnsproxy->mnt_ns = old_mnt_ns;\n\t\tput_mnt_ns(mnt_ns);\n\t\treturn err;\n\t}\n\n\tput_mnt_ns(old_mnt_ns);\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic struct user_namespace *mntns_owner(struct ns_common *ns)\n{\n\treturn to_mnt_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.owner\t\t= mntns_owner,\n};\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n *  linux/fs/namespace.c\n *\n * (C) Copyright Al Viro 2000, 2001\n *\n * Based on code from fs/super.c, copyright Linus Torvalds and others.\n * Heavily rewritten.\n */\n\n#include <linux/syscalls.h>\n#include <linux/export.h>\n#include <linux/capability.h>\n#include <linux/mnt_namespace.h>\n#include <linux/user_namespace.h>\n#include <linux/namei.h>\n#include <linux/security.h>\n#include <linux/cred.h>\n#include <linux/idr.h>\n#include <linux/init.h>\t\t/* init_rootfs */\n#include <linux/fs_struct.h>\t/* get_fs_root et.al. */\n#include <linux/fsnotify.h>\t/* fsnotify_vfsmount_delete */\n#include <linux/file.h>\n#include <linux/uaccess.h>\n#include <linux/proc_ns.h>\n#include <linux/magic.h>\n#include <linux/memblock.h>\n#include <linux/proc_fs.h>\n#include <linux/task_work.h>\n#include <linux/sched/task.h>\n#include <uapi/linux/mount.h>\n#include <linux/fs_context.h>\n#include <linux/shmem_fs.h>\n\n#include \"pnode.h\"\n#include \"internal.h\"\n\n/* Maximum number of mounts in a mount namespace */\nunsigned int sysctl_mount_max __read_mostly = 100000;\n\nstatic unsigned int m_hash_mask __read_mostly;\nstatic unsigned int m_hash_shift __read_mostly;\nstatic unsigned int mp_hash_mask __read_mostly;\nstatic unsigned int mp_hash_shift __read_mostly;\n\nstatic __initdata unsigned long mhash_entries;\nstatic int __init set_mhash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmhash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mhash_entries=\", set_mhash_entries);\n\nstatic __initdata unsigned long mphash_entries;\nstatic int __init set_mphash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tmphash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"mphash_entries=\", set_mphash_entries);\n\nstatic u64 event;\nstatic DEFINE_IDA(mnt_id_ida);\nstatic DEFINE_IDA(mnt_group_ida);\n\nstatic struct hlist_head *mount_hashtable __read_mostly;\nstatic struct hlist_head *mountpoint_hashtable __read_mostly;\nstatic struct kmem_cache *mnt_cache __read_mostly;\nstatic DECLARE_RWSEM(namespace_sem);\nstatic HLIST_HEAD(unmounted);\t/* protected by namespace_sem */\nstatic LIST_HEAD(ex_mountpoints); /* protected by namespace_sem */\n\nstruct mount_kattr {\n\tunsigned int attr_set;\n\tunsigned int attr_clr;\n\tunsigned int propagation;\n\tunsigned int lookup_flags;\n\tbool recurse;\n\tstruct user_namespace *mnt_userns;\n};\n\n/* /sys/fs */\nstruct kobject *fs_kobj;\nEXPORT_SYMBOL_GPL(fs_kobj);\n\n/*\n * vfsmount lock may be taken for read to prevent changes to the\n * vfsmount hash, ie. during mountpoint lookups or walking back\n * up the tree.\n *\n * It should be taken for write in all cases where the vfsmount\n * tree or hash is modified or when a vfsmount structure is modified.\n */\n__cacheline_aligned_in_smp DEFINE_SEQLOCK(mount_lock);\n\nstatic inline void lock_mount_hash(void)\n{\n\twrite_seqlock(&mount_lock);\n}\n\nstatic inline void unlock_mount_hash(void)\n{\n\twrite_sequnlock(&mount_lock);\n}\n\nstatic inline struct hlist_head *m_hash(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)mnt / L1_CACHE_BYTES);\n\ttmp += ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> m_hash_shift);\n\treturn &mount_hashtable[tmp & m_hash_mask];\n}\n\nstatic inline struct hlist_head *mp_hash(struct dentry *dentry)\n{\n\tunsigned long tmp = ((unsigned long)dentry / L1_CACHE_BYTES);\n\ttmp = tmp + (tmp >> mp_hash_shift);\n\treturn &mountpoint_hashtable[tmp & mp_hash_mask];\n}\n\nstatic int mnt_alloc_id(struct mount *mnt)\n{\n\tint res = ida_alloc(&mnt_id_ida, GFP_KERNEL);\n\n\tif (res < 0)\n\t\treturn res;\n\tmnt->mnt_id = res;\n\treturn 0;\n}\n\nstatic void mnt_free_id(struct mount *mnt)\n{\n\tida_free(&mnt_id_ida, mnt->mnt_id);\n}\n\n/*\n * Allocate a new peer group ID\n */\nstatic int mnt_alloc_group_id(struct mount *mnt)\n{\n\tint res = ida_alloc_min(&mnt_group_ida, 1, GFP_KERNEL);\n\n\tif (res < 0)\n\t\treturn res;\n\tmnt->mnt_group_id = res;\n\treturn 0;\n}\n\n/*\n * Release a peer group ID\n */\nvoid mnt_release_group_id(struct mount *mnt)\n{\n\tida_free(&mnt_group_ida, mnt->mnt_group_id);\n\tmnt->mnt_group_id = 0;\n}\n\n/*\n * vfsmount lock must be held for read\n */\nstatic inline void mnt_add_count(struct mount *mnt, int n)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_add(mnt->mnt_pcp->mnt_count, n);\n#else\n\tpreempt_disable();\n\tmnt->mnt_count += n;\n\tpreempt_enable();\n#endif\n}\n\n/*\n * vfsmount lock must be held for write\n */\nint mnt_get_count(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tint count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_count;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_count;\n#endif\n}\n\nstatic struct mount *alloc_vfsmnt(const char *name)\n{\n\tstruct mount *mnt = kmem_cache_zalloc(mnt_cache, GFP_KERNEL);\n\tif (mnt) {\n\t\tint err;\n\n\t\terr = mnt_alloc_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free_cache;\n\n\t\tif (name) {\n\t\t\tmnt->mnt_devname = kstrdup_const(name, GFP_KERNEL);\n\t\t\tif (!mnt->mnt_devname)\n\t\t\t\tgoto out_free_id;\n\t\t}\n\n#ifdef CONFIG_SMP\n\t\tmnt->mnt_pcp = alloc_percpu(struct mnt_pcp);\n\t\tif (!mnt->mnt_pcp)\n\t\t\tgoto out_free_devname;\n\n\t\tthis_cpu_add(mnt->mnt_pcp->mnt_count, 1);\n#else\n\t\tmnt->mnt_count = 1;\n\t\tmnt->mnt_writers = 0;\n#endif\n\n\t\tINIT_HLIST_NODE(&mnt->mnt_hash);\n\t\tINIT_LIST_HEAD(&mnt->mnt_child);\n\t\tINIT_LIST_HEAD(&mnt->mnt_mounts);\n\t\tINIT_LIST_HEAD(&mnt->mnt_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_expire);\n\t\tINIT_LIST_HEAD(&mnt->mnt_share);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_slave);\n\t\tINIT_HLIST_NODE(&mnt->mnt_mp_list);\n\t\tINIT_LIST_HEAD(&mnt->mnt_umounting);\n\t\tINIT_HLIST_HEAD(&mnt->mnt_stuck_children);\n\t\tmnt->mnt.mnt_userns = &init_user_ns;\n\t}\n\treturn mnt;\n\n#ifdef CONFIG_SMP\nout_free_devname:\n\tkfree_const(mnt->mnt_devname);\n#endif\nout_free_id:\n\tmnt_free_id(mnt);\nout_free_cache:\n\tkmem_cache_free(mnt_cache, mnt);\n\treturn NULL;\n}\n\n/*\n * Most r/o checks on a fs are for operations that take\n * discrete amounts of time, like a write() or unlink().\n * We must keep track of when those operations start\n * (for permission checks) and when they end, so that\n * we can determine when writes are able to occur to\n * a filesystem.\n */\n/*\n * __mnt_is_readonly: check whether a mount is read-only\n * @mnt: the mount to check for its write status\n *\n * This shouldn't be used directly ouside of the VFS.\n * It does not guarantee that the filesystem will stay\n * r/w, just that it is right *now*.  This can not and\n * should not be used in place of IS_RDONLY(inode).\n * mnt_want/drop_write() will _keep_ the filesystem\n * r/w.\n */\nbool __mnt_is_readonly(struct vfsmount *mnt)\n{\n\treturn (mnt->mnt_flags & MNT_READONLY) || sb_rdonly(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(__mnt_is_readonly);\n\nstatic inline void mnt_inc_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_inc(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers++;\n#endif\n}\n\nstatic inline void mnt_dec_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tthis_cpu_dec(mnt->mnt_pcp->mnt_writers);\n#else\n\tmnt->mnt_writers--;\n#endif\n}\n\nstatic unsigned int mnt_get_writers(struct mount *mnt)\n{\n#ifdef CONFIG_SMP\n\tunsigned int count = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcount += per_cpu_ptr(mnt->mnt_pcp, cpu)->mnt_writers;\n\t}\n\n\treturn count;\n#else\n\treturn mnt->mnt_writers;\n#endif\n}\n\nstatic int mnt_is_readonly(struct vfsmount *mnt)\n{\n\tif (mnt->mnt_sb->s_readonly_remount)\n\t\treturn 1;\n\t/* Order wrt setting s_flags/s_readonly_remount in do_remount() */\n\tsmp_rmb();\n\treturn __mnt_is_readonly(mnt);\n}\n\n/*\n * Most r/o & frozen checks on a fs are for operations that take discrete\n * amounts of time, like a write() or unlink().  We must keep track of when\n * those operations start (for permission checks) and when they end, so that we\n * can determine when writes are able to occur to a filesystem.\n */\n/**\n * __mnt_want_write - get write access to a mount without freeze protection\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mnt it read-write) before\n * returning success. This operation does not protect against filesystem being\n * frozen. When the write operation is finished, __mnt_drop_write() must be\n * called. This is effectively a refcount.\n */\nint __mnt_want_write(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint ret = 0;\n\n\tpreempt_disable();\n\tmnt_inc_writers(mnt);\n\t/*\n\t * The store to mnt_inc_writers must be visible before we pass\n\t * MNT_WRITE_HOLD loop below, so that the slowpath can see our\n\t * incremented count after it has set MNT_WRITE_HOLD.\n\t */\n\tsmp_mb();\n\twhile (READ_ONCE(mnt->mnt.mnt_flags) & MNT_WRITE_HOLD)\n\t\tcpu_relax();\n\t/*\n\t * After the slowpath clears MNT_WRITE_HOLD, mnt_is_readonly will\n\t * be set to match its requirements. So we must not load that until\n\t * MNT_WRITE_HOLD is cleared.\n\t */\n\tsmp_rmb();\n\tif (mnt_is_readonly(m)) {\n\t\tmnt_dec_writers(mnt);\n\t\tret = -EROFS;\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\n/**\n * mnt_want_write - get write access to a mount\n * @m: the mount on which to take a write\n *\n * This tells the low-level filesystem that a write is about to be performed to\n * it, and makes sure that writes are allowed (mount is read-write, filesystem\n * is not frozen) before returning success.  When the write operation is\n * finished, mnt_drop_write() must be called.  This is effectively a refcount.\n */\nint mnt_want_write(struct vfsmount *m)\n{\n\tint ret;\n\n\tsb_start_write(m->mnt_sb);\n\tret = __mnt_want_write(m);\n\tif (ret)\n\t\tsb_end_write(m->mnt_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write);\n\n/**\n * __mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like __mnt_want_write, but if the file is already open for writing it\n * skips incrementing mnt_writers (since the open file already has a reference)\n * and instead only does the check for emergency r/o remounts.  This must be\n * paired with __mnt_drop_write_file.\n */\nint __mnt_want_write_file(struct file *file)\n{\n\tif (file->f_mode & FMODE_WRITER) {\n\t\t/*\n\t\t * Superblock may have become readonly while there are still\n\t\t * writable fd's, e.g. due to a fs error with errors=remount-ro\n\t\t */\n\t\tif (__mnt_is_readonly(file->f_path.mnt))\n\t\t\treturn -EROFS;\n\t\treturn 0;\n\t}\n\treturn __mnt_want_write(file->f_path.mnt);\n}\n\n/**\n * mnt_want_write_file - get write access to a file's mount\n * @file: the file who's mount on which to take a write\n *\n * This is like mnt_want_write, but if the file is already open for writing it\n * skips incrementing mnt_writers (since the open file already has a reference)\n * and instead only does the freeze protection and the check for emergency r/o\n * remounts.  This must be paired with mnt_drop_write_file.\n */\nint mnt_want_write_file(struct file *file)\n{\n\tint ret;\n\n\tsb_start_write(file_inode(file)->i_sb);\n\tret = __mnt_want_write_file(file);\n\tif (ret)\n\t\tsb_end_write(file_inode(file)->i_sb);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mnt_want_write_file);\n\n/**\n * __mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done\n * performing writes to it.  Must be matched with\n * __mnt_want_write() call above.\n */\nvoid __mnt_drop_write(struct vfsmount *mnt)\n{\n\tpreempt_disable();\n\tmnt_dec_writers(real_mount(mnt));\n\tpreempt_enable();\n}\n\n/**\n * mnt_drop_write - give up write access to a mount\n * @mnt: the mount on which to give up write access\n *\n * Tells the low-level filesystem that we are done performing writes to it and\n * also allows filesystem to be frozen again.  Must be matched with\n * mnt_want_write() call above.\n */\nvoid mnt_drop_write(struct vfsmount *mnt)\n{\n\t__mnt_drop_write(mnt);\n\tsb_end_write(mnt->mnt_sb);\n}\nEXPORT_SYMBOL_GPL(mnt_drop_write);\n\nvoid __mnt_drop_write_file(struct file *file)\n{\n\tif (!(file->f_mode & FMODE_WRITER))\n\t\t__mnt_drop_write(file->f_path.mnt);\n}\n\nvoid mnt_drop_write_file(struct file *file)\n{\n\t__mnt_drop_write_file(file);\n\tsb_end_write(file_inode(file)->i_sb);\n}\nEXPORT_SYMBOL(mnt_drop_write_file);\n\nstatic inline int mnt_hold_writers(struct mount *mnt)\n{\n\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t/*\n\t * After storing MNT_WRITE_HOLD, we'll read the counters. This store\n\t * should be visible before we do.\n\t */\n\tsmp_mb();\n\n\t/*\n\t * With writers on hold, if this value is zero, then there are\n\t * definitely no active writers (although held writers may subsequently\n\t * increment the count, they'll have to wait, and decrement it after\n\t * seeing MNT_READONLY).\n\t *\n\t * It is OK to have counter incremented on one CPU and decremented on\n\t * another: the sum will add up correctly. The danger would be when we\n\t * sum up each counter, if we read a counter before it is incremented,\n\t * but then read another CPU's count which it has been subsequently\n\t * decremented from -- we would see more decrements than we should.\n\t * MNT_WRITE_HOLD protects against this scenario, because\n\t * mnt_want_write first increments count, then smp_mb, then spins on\n\t * MNT_WRITE_HOLD, so it can't be decremented by another CPU while\n\t * we're counting up here.\n\t */\n\tif (mnt_get_writers(mnt) > 0)\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\nstatic inline void mnt_unhold_writers(struct mount *mnt)\n{\n\t/*\n\t * MNT_READONLY must become visible before ~MNT_WRITE_HOLD, so writers\n\t * that become unheld will see MNT_READONLY.\n\t */\n\tsmp_wmb();\n\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n}\n\nstatic int mnt_make_readonly(struct mount *mnt)\n{\n\tint ret;\n\n\tret = mnt_hold_writers(mnt);\n\tif (!ret)\n\t\tmnt->mnt.mnt_flags |= MNT_READONLY;\n\tmnt_unhold_writers(mnt);\n\treturn ret;\n}\n\nint sb_prepare_remount_readonly(struct super_block *sb)\n{\n\tstruct mount *mnt;\n\tint err = 0;\n\n\t/* Racy optimization.  Recheck the counter under MNT_WRITE_HOLD */\n\tif (atomic_long_read(&sb->s_remove_count))\n\t\treturn -EBUSY;\n\n\tlock_mount_hash();\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (!(mnt->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\tmnt->mnt.mnt_flags |= MNT_WRITE_HOLD;\n\t\t\tsmp_mb();\n\t\t\tif (mnt_get_writers(mnt) > 0) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (!err && atomic_long_read(&sb->s_remove_count))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tsb->s_readonly_remount = 1;\n\t\tsmp_wmb();\n\t}\n\tlist_for_each_entry(mnt, &sb->s_mounts, mnt_instance) {\n\t\tif (mnt->mnt.mnt_flags & MNT_WRITE_HOLD)\n\t\t\tmnt->mnt.mnt_flags &= ~MNT_WRITE_HOLD;\n\t}\n\tunlock_mount_hash();\n\n\treturn err;\n}\n\nstatic void free_vfsmnt(struct mount *mnt)\n{\n\tstruct user_namespace *mnt_userns;\n\n\tmnt_userns = mnt_user_ns(&mnt->mnt);\n\tif (mnt_userns != &init_user_ns)\n\t\tput_user_ns(mnt_userns);\n\tkfree_const(mnt->mnt_devname);\n#ifdef CONFIG_SMP\n\tfree_percpu(mnt->mnt_pcp);\n#endif\n\tkmem_cache_free(mnt_cache, mnt);\n}\n\nstatic void delayed_free_vfsmnt(struct rcu_head *head)\n{\n\tfree_vfsmnt(container_of(head, struct mount, mnt_rcu));\n}\n\n/* call under rcu_read_lock */\nint __legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tstruct mount *mnt;\n\tif (read_seqretry(&mount_lock, seq))\n\t\treturn 1;\n\tif (bastard == NULL)\n\t\treturn 0;\n\tmnt = real_mount(bastard);\n\tmnt_add_count(mnt, 1);\n\tsmp_mb();\t\t\t// see mntput_no_expire()\n\tif (likely(!read_seqretry(&mount_lock, seq)))\n\t\treturn 0;\n\tif (bastard->mnt_flags & MNT_SYNC_UMOUNT) {\n\t\tmnt_add_count(mnt, -1);\n\t\treturn 1;\n\t}\n\tlock_mount_hash();\n\tif (unlikely(bastard->mnt_flags & MNT_DOOMED)) {\n\t\tmnt_add_count(mnt, -1);\n\t\tunlock_mount_hash();\n\t\treturn 1;\n\t}\n\tunlock_mount_hash();\n\t/* caller will mntput() */\n\treturn -1;\n}\n\n/* call under rcu_read_lock */\nbool legitimize_mnt(struct vfsmount *bastard, unsigned seq)\n{\n\tint res = __legitimize_mnt(bastard, seq);\n\tif (likely(!res))\n\t\treturn true;\n\tif (unlikely(res < 0)) {\n\t\trcu_read_unlock();\n\t\tmntput(bastard);\n\t\trcu_read_lock();\n\t}\n\treturn false;\n}\n\n/*\n * find the first mount at @dentry on vfsmount @mnt.\n * call under rcu_read_lock()\n */\nstruct mount *__lookup_mnt(struct vfsmount *mnt, struct dentry *dentry)\n{\n\tstruct hlist_head *head = m_hash(mnt, dentry);\n\tstruct mount *p;\n\n\thlist_for_each_entry_rcu(p, head, mnt_hash)\n\t\tif (&p->mnt_parent->mnt == mnt && p->mnt_mountpoint == dentry)\n\t\t\treturn p;\n\treturn NULL;\n}\n\n/*\n * lookup_mnt - Return the first child mount mounted at path\n *\n * \"First\" means first mounted chronologically.  If you create the\n * following mounts:\n *\n * mount /dev/sda1 /mnt\n * mount /dev/sda2 /mnt\n * mount /dev/sda3 /mnt\n *\n * Then lookup_mnt() on the base /mnt dentry in the root mount will\n * return successively the root dentry and vfsmount of /dev/sda1, then\n * /dev/sda2, then /dev/sda3, then NULL.\n *\n * lookup_mnt takes a reference to the found vfsmount.\n */\nstruct vfsmount *lookup_mnt(const struct path *path)\n{\n\tstruct mount *child_mnt;\n\tstruct vfsmount *m;\n\tunsigned seq;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tchild_mnt = __lookup_mnt(path->mnt, path->dentry);\n\t\tm = child_mnt ? &child_mnt->mnt : NULL;\n\t} while (!legitimize_mnt(m, seq));\n\trcu_read_unlock();\n\treturn m;\n}\n\nstatic inline void lock_ns_list(struct mnt_namespace *ns)\n{\n\tspin_lock(&ns->ns_lock);\n}\n\nstatic inline void unlock_ns_list(struct mnt_namespace *ns)\n{\n\tspin_unlock(&ns->ns_lock);\n}\n\nstatic inline bool mnt_is_cursor(struct mount *mnt)\n{\n\treturn mnt->mnt.mnt_flags & MNT_CURSOR;\n}\n\n/*\n * __is_local_mountpoint - Test to see if dentry is a mountpoint in the\n *                         current mount namespace.\n *\n * The common case is dentries are not mountpoints at all and that\n * test is handled inline.  For the slow case when we are actually\n * dealing with a mountpoint of some kind, walk through all of the\n * mounts in the current mount namespace and test to see if the dentry\n * is a mountpoint.\n *\n * The mount_hashtable is not usable in the context because we\n * need to identify all mounts that may be in the current mount\n * namespace not just a mount that happens to have some specified\n * parent mount.\n */\nbool __is_local_mountpoint(struct dentry *dentry)\n{\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tstruct mount *mnt;\n\tbool is_covered = false;\n\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tif (mnt_is_cursor(mnt))\n\t\t\tcontinue;\n\t\tis_covered = (mnt->mnt_mountpoint == dentry);\n\t\tif (is_covered)\n\t\t\tbreak;\n\t}\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n\n\treturn is_covered;\n}\n\nstatic struct mountpoint *lookup_mountpoint(struct dentry *dentry)\n{\n\tstruct hlist_head *chain = mp_hash(dentry);\n\tstruct mountpoint *mp;\n\n\thlist_for_each_entry(mp, chain, m_hash) {\n\t\tif (mp->m_dentry == dentry) {\n\t\t\tmp->m_count++;\n\t\t\treturn mp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct mountpoint *get_mountpoint(struct dentry *dentry)\n{\n\tstruct mountpoint *mp, *new = NULL;\n\tint ret;\n\n\tif (d_mountpoint(dentry)) {\n\t\t/* might be worth a WARN_ON() */\n\t\tif (d_unlinked(dentry))\n\t\t\treturn ERR_PTR(-ENOENT);\nmountpoint:\n\t\tread_seqlock_excl(&mount_lock);\n\t\tmp = lookup_mountpoint(dentry);\n\t\tread_sequnlock_excl(&mount_lock);\n\t\tif (mp)\n\t\t\tgoto done;\n\t}\n\n\tif (!new)\n\t\tnew = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\n\t/* Exactly one processes may set d_mounted */\n\tret = d_set_mounted(dentry);\n\n\t/* Someone else set d_mounted? */\n\tif (ret == -EBUSY)\n\t\tgoto mountpoint;\n\n\t/* The dentry is not available as a mountpoint? */\n\tmp = ERR_PTR(ret);\n\tif (ret)\n\t\tgoto done;\n\n\t/* Add the new mountpoint to the hash table */\n\tread_seqlock_excl(&mount_lock);\n\tnew->m_dentry = dget(dentry);\n\tnew->m_count = 1;\n\thlist_add_head(&new->m_hash, mp_hash(dentry));\n\tINIT_HLIST_HEAD(&new->m_list);\n\tread_sequnlock_excl(&mount_lock);\n\n\tmp = new;\n\tnew = NULL;\ndone:\n\tkfree(new);\n\treturn mp;\n}\n\n/*\n * vfsmount lock must be held.  Additionally, the caller is responsible\n * for serializing calls for given disposal list.\n */\nstatic void __put_mountpoint(struct mountpoint *mp, struct list_head *list)\n{\n\tif (!--mp->m_count) {\n\t\tstruct dentry *dentry = mp->m_dentry;\n\t\tBUG_ON(!hlist_empty(&mp->m_list));\n\t\tspin_lock(&dentry->d_lock);\n\t\tdentry->d_flags &= ~DCACHE_MOUNTED;\n\t\tspin_unlock(&dentry->d_lock);\n\t\tdput_to_list(dentry, list);\n\t\thlist_del(&mp->m_hash);\n\t\tkfree(mp);\n\t}\n}\n\n/* called with namespace_lock and vfsmount lock */\nstatic void put_mountpoint(struct mountpoint *mp)\n{\n\t__put_mountpoint(mp, &ex_mountpoints);\n}\n\nstatic inline int check_mnt(struct mount *mnt)\n{\n\treturn mnt->mnt_ns == current->nsproxy->mnt_ns;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns) {\n\t\tns->event = ++event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void __touch_mnt_namespace(struct mnt_namespace *ns)\n{\n\tif (ns && ns->event != event) {\n\t\tns->event = event;\n\t\twake_up_interruptible(&ns->poll);\n\t}\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic struct mountpoint *unhash_mnt(struct mount *mnt)\n{\n\tstruct mountpoint *mp;\n\tmnt->mnt_parent = mnt;\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\thlist_del_init(&mnt->mnt_mp_list);\n\tmp = mnt->mnt_mp;\n\tmnt->mnt_mp = NULL;\n\treturn mp;\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void umount_mnt(struct mount *mnt)\n{\n\tput_mountpoint(unhash_mnt(mnt));\n}\n\n/*\n * vfsmount lock must be held for write\n */\nvoid mnt_set_mountpoint(struct mount *mnt,\n\t\t\tstruct mountpoint *mp,\n\t\t\tstruct mount *child_mnt)\n{\n\tmp->m_count++;\n\tmnt_add_count(mnt, 1);\t/* essentially, that's mntget */\n\tchild_mnt->mnt_mountpoint = mp->m_dentry;\n\tchild_mnt->mnt_parent = mnt;\n\tchild_mnt->mnt_mp = mp;\n\thlist_add_head(&child_mnt->mnt_mp_list, &mp->m_list);\n}\n\nstatic void __attach_mnt(struct mount *mnt, struct mount *parent)\n{\n\thlist_add_head_rcu(&mnt->mnt_hash,\n\t\t\t   m_hash(&parent->mnt, mnt->mnt_mountpoint));\n\tlist_add_tail(&mnt->mnt_child, &parent->mnt_mounts);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void attach_mnt(struct mount *mnt,\n\t\t\tstruct mount *parent,\n\t\t\tstruct mountpoint *mp)\n{\n\tmnt_set_mountpoint(parent, mp, mnt);\n\t__attach_mnt(mnt, parent);\n}\n\nvoid mnt_change_mountpoint(struct mount *parent, struct mountpoint *mp, struct mount *mnt)\n{\n\tstruct mountpoint *old_mp = mnt->mnt_mp;\n\tstruct mount *old_parent = mnt->mnt_parent;\n\n\tlist_del_init(&mnt->mnt_child);\n\thlist_del_init(&mnt->mnt_mp_list);\n\thlist_del_init_rcu(&mnt->mnt_hash);\n\n\tattach_mnt(mnt, parent, mp);\n\n\tput_mountpoint(old_mp);\n\tmnt_add_count(old_parent, -1);\n}\n\n/*\n * vfsmount lock must be held for write\n */\nstatic void commit_tree(struct mount *mnt)\n{\n\tstruct mount *parent = mnt->mnt_parent;\n\tstruct mount *m;\n\tLIST_HEAD(head);\n\tstruct mnt_namespace *n = parent->mnt_ns;\n\n\tBUG_ON(parent == mnt);\n\n\tlist_add_tail(&head, &mnt->mnt_list);\n\tlist_for_each_entry(m, &head, mnt_list)\n\t\tm->mnt_ns = n;\n\n\tlist_splice(&head, n->list.prev);\n\n\tn->mounts += n->pending_mounts;\n\tn->pending_mounts = 0;\n\n\t__attach_mnt(mnt, parent);\n\ttouch_mnt_namespace(n);\n}\n\nstatic struct mount *next_mnt(struct mount *p, struct mount *root)\n{\n\tstruct list_head *next = p->mnt_mounts.next;\n\tif (next == &p->mnt_mounts) {\n\t\twhile (1) {\n\t\t\tif (p == root)\n\t\t\t\treturn NULL;\n\t\t\tnext = p->mnt_child.next;\n\t\t\tif (next != &p->mnt_parent->mnt_mounts)\n\t\t\t\tbreak;\n\t\t\tp = p->mnt_parent;\n\t\t}\n\t}\n\treturn list_entry(next, struct mount, mnt_child);\n}\n\nstatic struct mount *skip_mnt_tree(struct mount *p)\n{\n\tstruct list_head *prev = p->mnt_mounts.prev;\n\twhile (prev != &p->mnt_mounts) {\n\t\tp = list_entry(prev, struct mount, mnt_child);\n\t\tprev = p->mnt_mounts.prev;\n\t}\n\treturn p;\n}\n\n/**\n * vfs_create_mount - Create a mount for a configured superblock\n * @fc: The configuration context with the superblock attached\n *\n * Create a mount to an already configured superblock.  If necessary, the\n * caller should invoke vfs_get_tree() before calling this.\n *\n * Note that this does not attach the mount to anything.\n */\nstruct vfsmount *vfs_create_mount(struct fs_context *fc)\n{\n\tstruct mount *mnt;\n\n\tif (!fc->root)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmnt = alloc_vfsmnt(fc->source ?: \"none\");\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (fc->sb_flags & SB_KERNMOUNT)\n\t\tmnt->mnt.mnt_flags = MNT_INTERNAL;\n\n\tatomic_inc(&fc->root->d_sb->s_active);\n\tmnt->mnt.mnt_sb\t\t= fc->root->d_sb;\n\tmnt->mnt.mnt_root\t= dget(fc->root);\n\tmnt->mnt_mountpoint\t= mnt->mnt.mnt_root;\n\tmnt->mnt_parent\t\t= mnt;\n\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &mnt->mnt.mnt_sb->s_mounts);\n\tunlock_mount_hash();\n\treturn &mnt->mnt;\n}\nEXPORT_SYMBOL(vfs_create_mount);\n\nstruct vfsmount *fc_mount(struct fs_context *fc)\n{\n\tint err = vfs_get_tree(fc);\n\tif (!err) {\n\t\tup_write(&fc->root->d_sb->s_umount);\n\t\treturn vfs_create_mount(fc);\n\t}\n\treturn ERR_PTR(err);\n}\nEXPORT_SYMBOL(fc_mount);\n\nstruct vfsmount *vfs_kern_mount(struct file_system_type *type,\n\t\t\t\tint flags, const char *name,\n\t\t\t\tvoid *data)\n{\n\tstruct fs_context *fc;\n\tstruct vfsmount *mnt;\n\tint ret = 0;\n\n\tif (!type)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tfc = fs_context_for_mount(type, flags);\n\tif (IS_ERR(fc))\n\t\treturn ERR_CAST(fc);\n\n\tif (name)\n\t\tret = vfs_parse_fs_string(fc, \"source\",\n\t\t\t\t\t  name, strlen(name));\n\tif (!ret)\n\t\tret = parse_monolithic_mount_data(fc, data);\n\tif (!ret)\n\t\tmnt = fc_mount(fc);\n\telse\n\t\tmnt = ERR_PTR(ret);\n\n\tput_fs_context(fc);\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(vfs_kern_mount);\n\nstruct vfsmount *\nvfs_submount(const struct dentry *mountpoint, struct file_system_type *type,\n\t     const char *name, void *data)\n{\n\t/* Until it is worked out how to pass the user namespace\n\t * through from the parent mount to the submount don't support\n\t * unprivileged mounts with submounts.\n\t */\n\tif (mountpoint->d_sb->s_user_ns != &init_user_ns)\n\t\treturn ERR_PTR(-EPERM);\n\n\treturn vfs_kern_mount(type, SB_SUBMOUNT, name, data);\n}\nEXPORT_SYMBOL_GPL(vfs_submount);\n\nstatic struct mount *clone_mnt(struct mount *old, struct dentry *root,\n\t\t\t\t\tint flag)\n{\n\tstruct super_block *sb = old->mnt.mnt_sb;\n\tstruct mount *mnt;\n\tint err;\n\n\tmnt = alloc_vfsmnt(old->mnt_devname);\n\tif (!mnt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (flag & (CL_SLAVE | CL_PRIVATE | CL_SHARED_TO_SLAVE))\n\t\tmnt->mnt_group_id = 0; /* not a peer of original */\n\telse\n\t\tmnt->mnt_group_id = old->mnt_group_id;\n\n\tif ((flag & CL_MAKE_SHARED) && !mnt->mnt_group_id) {\n\t\terr = mnt_alloc_group_id(mnt);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\tmnt->mnt.mnt_flags = old->mnt.mnt_flags;\n\tmnt->mnt.mnt_flags &= ~(MNT_WRITE_HOLD|MNT_MARKED|MNT_INTERNAL);\n\n\tatomic_inc(&sb->s_active);\n\tmnt->mnt.mnt_userns = mnt_user_ns(&old->mnt);\n\tif (mnt->mnt.mnt_userns != &init_user_ns)\n\t\tmnt->mnt.mnt_userns = get_user_ns(mnt->mnt.mnt_userns);\n\tmnt->mnt.mnt_sb = sb;\n\tmnt->mnt.mnt_root = dget(root);\n\tmnt->mnt_mountpoint = mnt->mnt.mnt_root;\n\tmnt->mnt_parent = mnt;\n\tlock_mount_hash();\n\tlist_add_tail(&mnt->mnt_instance, &sb->s_mounts);\n\tunlock_mount_hash();\n\n\tif ((flag & CL_SLAVE) ||\n\t    ((flag & CL_SHARED_TO_SLAVE) && IS_MNT_SHARED(old))) {\n\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave_list);\n\t\tmnt->mnt_master = old;\n\t\tCLEAR_MNT_SHARED(mnt);\n\t} else if (!(flag & CL_PRIVATE)) {\n\t\tif ((flag & CL_MAKE_SHARED) || IS_MNT_SHARED(old))\n\t\t\tlist_add(&mnt->mnt_share, &old->mnt_share);\n\t\tif (IS_MNT_SLAVE(old))\n\t\t\tlist_add(&mnt->mnt_slave, &old->mnt_slave);\n\t\tmnt->mnt_master = old->mnt_master;\n\t} else {\n\t\tCLEAR_MNT_SHARED(mnt);\n\t}\n\tif (flag & CL_MAKE_SHARED)\n\t\tset_mnt_shared(mnt);\n\n\t/* stick the duplicate mount on the same expiry list\n\t * as the original if that was on one */\n\tif (flag & CL_EXPIRE) {\n\t\tif (!list_empty(&old->mnt_expire))\n\t\t\tlist_add(&mnt->mnt_expire, &old->mnt_expire);\n\t}\n\n\treturn mnt;\n\n out_free:\n\tmnt_free_id(mnt);\n\tfree_vfsmnt(mnt);\n\treturn ERR_PTR(err);\n}\n\nstatic void cleanup_mnt(struct mount *mnt)\n{\n\tstruct hlist_node *p;\n\tstruct mount *m;\n\t/*\n\t * The warning here probably indicates that somebody messed\n\t * up a mnt_want/drop_write() pair.  If this happens, the\n\t * filesystem was probably unable to make r/w->r/o transitions.\n\t * The locking used to deal with mnt_count decrement provides barriers,\n\t * so mnt_get_writers() below is safe.\n\t */\n\tWARN_ON(mnt_get_writers(mnt));\n\tif (unlikely(mnt->mnt_pins.first))\n\t\tmnt_pin_kill(mnt);\n\thlist_for_each_entry_safe(m, p, &mnt->mnt_stuck_children, mnt_umount) {\n\t\thlist_del(&m->mnt_umount);\n\t\tmntput(&m->mnt);\n\t}\n\tfsnotify_vfsmount_delete(&mnt->mnt);\n\tdput(mnt->mnt.mnt_root);\n\tdeactivate_super(mnt->mnt.mnt_sb);\n\tmnt_free_id(mnt);\n\tcall_rcu(&mnt->mnt_rcu, delayed_free_vfsmnt);\n}\n\nstatic void __cleanup_mnt(struct rcu_head *head)\n{\n\tcleanup_mnt(container_of(head, struct mount, mnt_rcu));\n}\n\nstatic LLIST_HEAD(delayed_mntput_list);\nstatic void delayed_mntput(struct work_struct *unused)\n{\n\tstruct llist_node *node = llist_del_all(&delayed_mntput_list);\n\tstruct mount *m, *t;\n\n\tllist_for_each_entry_safe(m, t, node, mnt_llist)\n\t\tcleanup_mnt(m);\n}\nstatic DECLARE_DELAYED_WORK(delayed_mntput_work, delayed_mntput);\n\nstatic void mntput_no_expire(struct mount *mnt)\n{\n\tLIST_HEAD(list);\n\tint count;\n\n\trcu_read_lock();\n\tif (likely(READ_ONCE(mnt->mnt_ns))) {\n\t\t/*\n\t\t * Since we don't do lock_mount_hash() here,\n\t\t * ->mnt_ns can change under us.  However, if it's\n\t\t * non-NULL, then there's a reference that won't\n\t\t * be dropped until after an RCU delay done after\n\t\t * turning ->mnt_ns NULL.  So if we observe it\n\t\t * non-NULL under rcu_read_lock(), the reference\n\t\t * we are dropping is not the final one.\n\t\t */\n\t\tmnt_add_count(mnt, -1);\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\tlock_mount_hash();\n\t/*\n\t * make sure that if __legitimize_mnt() has not seen us grab\n\t * mount_lock, we'll see their refcount increment here.\n\t */\n\tsmp_mb();\n\tmnt_add_count(mnt, -1);\n\tcount = mnt_get_count(mnt);\n\tif (count != 0) {\n\t\tWARN_ON(count < 0);\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tif (unlikely(mnt->mnt.mnt_flags & MNT_DOOMED)) {\n\t\trcu_read_unlock();\n\t\tunlock_mount_hash();\n\t\treturn;\n\t}\n\tmnt->mnt.mnt_flags |= MNT_DOOMED;\n\trcu_read_unlock();\n\n\tlist_del(&mnt->mnt_instance);\n\n\tif (unlikely(!list_empty(&mnt->mnt_mounts))) {\n\t\tstruct mount *p, *tmp;\n\t\tlist_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {\n\t\t\t__put_mountpoint(unhash_mnt(p), &list);\n\t\t\thlist_add_head(&p->mnt_umount, &mnt->mnt_stuck_children);\n\t\t}\n\t}\n\tunlock_mount_hash();\n\tshrink_dentry_list(&list);\n\n\tif (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {\n\t\tstruct task_struct *task = current;\n\t\tif (likely(!(task->flags & PF_KTHREAD))) {\n\t\t\tinit_task_work(&mnt->mnt_rcu, __cleanup_mnt);\n\t\t\tif (!task_work_add(task, &mnt->mnt_rcu, TWA_RESUME))\n\t\t\t\treturn;\n\t\t}\n\t\tif (llist_add(&mnt->mnt_llist, &delayed_mntput_list))\n\t\t\tschedule_delayed_work(&delayed_mntput_work, 1);\n\t\treturn;\n\t}\n\tcleanup_mnt(mnt);\n}\n\nvoid mntput(struct vfsmount *mnt)\n{\n\tif (mnt) {\n\t\tstruct mount *m = real_mount(mnt);\n\t\t/* avoid cacheline pingpong, hope gcc doesn't get \"smart\" */\n\t\tif (unlikely(m->mnt_expiry_mark))\n\t\t\tm->mnt_expiry_mark = 0;\n\t\tmntput_no_expire(m);\n\t}\n}\nEXPORT_SYMBOL(mntput);\n\nstruct vfsmount *mntget(struct vfsmount *mnt)\n{\n\tif (mnt)\n\t\tmnt_add_count(real_mount(mnt), 1);\n\treturn mnt;\n}\nEXPORT_SYMBOL(mntget);\n\n/**\n * path_is_mountpoint() - Check if path is a mount in the current namespace.\n * @path: path to check\n *\n *  d_mountpoint() can only be used reliably to establish if a dentry is\n *  not mounted in any namespace and that common case is handled inline.\n *  d_mountpoint() isn't aware of the possibility there may be multiple\n *  mounts using a given dentry in a different namespace. This function\n *  checks if the passed in path is a mountpoint rather than the dentry\n *  alone.\n */\nbool path_is_mountpoint(const struct path *path)\n{\n\tunsigned seq;\n\tbool res;\n\n\tif (!d_mountpoint(path->dentry))\n\t\treturn false;\n\n\trcu_read_lock();\n\tdo {\n\t\tseq = read_seqbegin(&mount_lock);\n\t\tres = __path_is_mountpoint(path);\n\t} while (read_seqretry(&mount_lock, seq));\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_mountpoint);\n\nstruct vfsmount *mnt_clone_internal(const struct path *path)\n{\n\tstruct mount *p;\n\tp = clone_mnt(real_mount(path->mnt), path->dentry, CL_PRIVATE);\n\tif (IS_ERR(p))\n\t\treturn ERR_CAST(p);\n\tp->mnt.mnt_flags |= MNT_INTERNAL;\n\treturn &p->mnt;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic struct mount *mnt_list_next(struct mnt_namespace *ns,\n\t\t\t\t   struct list_head *p)\n{\n\tstruct mount *mnt, *ret = NULL;\n\n\tlock_ns_list(ns);\n\tlist_for_each_continue(p, &ns->list) {\n\t\tmnt = list_entry(p, typeof(*mnt), mnt_list);\n\t\tif (!mnt_is_cursor(mnt)) {\n\t\t\tret = mnt;\n\t\t\tbreak;\n\t\t}\n\t}\n\tunlock_ns_list(ns);\n\n\treturn ret;\n}\n\n/* iterator; we want it to have access to namespace_sem, thus here... */\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct list_head *prev;\n\n\tdown_read(&namespace_sem);\n\tif (!*pos) {\n\t\tprev = &p->ns->list;\n\t} else {\n\t\tprev = &p->cursor.mnt_list;\n\n\t\t/* Read after we'd reached the end? */\n\t\tif (list_empty(prev))\n\t\t\treturn NULL;\n\t}\n\n\treturn mnt_list_next(p->ns, prev);\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *mnt = v;\n\n\t++*pos;\n\treturn mnt_list_next(p->ns, &mnt->mnt_list);\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *mnt = v;\n\n\tlock_ns_list(p->ns);\n\tif (mnt)\n\t\tlist_move_tail(&p->cursor.mnt_list, &mnt->mnt_list);\n\telse\n\t\tlist_del_init(&p->cursor.mnt_list);\n\tunlock_ns_list(p->ns);\n\tup_read(&namespace_sem);\n}\n\nstatic int m_show(struct seq_file *m, void *v)\n{\n\tstruct proc_mounts *p = m->private;\n\tstruct mount *r = v;\n\treturn p->show(m, &r->mnt);\n}\n\nconst struct seq_operations mounts_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= m_show,\n};\n\nvoid mnt_cursor_del(struct mnt_namespace *ns, struct mount *cursor)\n{\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_del(&cursor->mnt_list);\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n}\n#endif  /* CONFIG_PROC_FS */\n\n/**\n * may_umount_tree - check if a mount tree is busy\n * @m: root of mount tree\n *\n * This is called to check if a tree of mounts has any\n * open files, pwds, chroots or sub mounts that are\n * busy.\n */\nint may_umount_tree(struct vfsmount *m)\n{\n\tstruct mount *mnt = real_mount(m);\n\tint actual_refs = 0;\n\tint minimum_refs = 0;\n\tstruct mount *p;\n\tBUG_ON(!m);\n\n\t/* write lock needed for mnt_get_count */\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tactual_refs += mnt_get_count(p);\n\t\tminimum_refs += 2;\n\t}\n\tunlock_mount_hash();\n\n\tif (actual_refs > minimum_refs)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nEXPORT_SYMBOL(may_umount_tree);\n\n/**\n * may_umount - check if a mount point is busy\n * @mnt: root of mount\n *\n * This is called to check if a mount point has any\n * open files, pwds, chroots or sub mounts. If the\n * mount has sub mounts this will return busy\n * regardless of whether the sub mounts are busy.\n *\n * Doesn't take quota and stuff into account. IOW, in some cases it will\n * give false negatives. The main reason why it's here is that we need\n * a non-destructive way to look for easily umountable filesystems.\n */\nint may_umount(struct vfsmount *mnt)\n{\n\tint ret = 1;\n\tdown_read(&namespace_sem);\n\tlock_mount_hash();\n\tif (propagate_mount_busy(real_mount(mnt), 2))\n\t\tret = 0;\n\tunlock_mount_hash();\n\tup_read(&namespace_sem);\n\treturn ret;\n}\n\nEXPORT_SYMBOL(may_umount);\n\nstatic void namespace_unlock(void)\n{\n\tstruct hlist_head head;\n\tstruct hlist_node *p;\n\tstruct mount *m;\n\tLIST_HEAD(list);\n\n\thlist_move_list(&unmounted, &head);\n\tlist_splice_init(&ex_mountpoints, &list);\n\n\tup_write(&namespace_sem);\n\n\tshrink_dentry_list(&list);\n\n\tif (likely(hlist_empty(&head)))\n\t\treturn;\n\n\tsynchronize_rcu_expedited();\n\n\thlist_for_each_entry_safe(m, p, &head, mnt_umount) {\n\t\thlist_del(&m->mnt_umount);\n\t\tmntput(&m->mnt);\n\t}\n}\n\nstatic inline void namespace_lock(void)\n{\n\tdown_write(&namespace_sem);\n}\n\nenum umount_tree_flags {\n\tUMOUNT_SYNC = 1,\n\tUMOUNT_PROPAGATE = 2,\n\tUMOUNT_CONNECTED = 4,\n};\n\nstatic bool disconnect_mount(struct mount *mnt, enum umount_tree_flags how)\n{\n\t/* Leaving mounts connected is only valid for lazy umounts */\n\tif (how & UMOUNT_SYNC)\n\t\treturn true;\n\n\t/* A mount without a parent has nothing to be connected to */\n\tif (!mnt_has_parent(mnt))\n\t\treturn true;\n\n\t/* Because the reference counting rules change when mounts are\n\t * unmounted and connected, umounted mounts may not be\n\t * connected to mounted mounts.\n\t */\n\tif (!(mnt->mnt_parent->mnt.mnt_flags & MNT_UMOUNT))\n\t\treturn true;\n\n\t/* Has it been requested that the mount remain connected? */\n\tif (how & UMOUNT_CONNECTED)\n\t\treturn false;\n\n\t/* Is the mount locked such that it needs to remain connected? */\n\tif (IS_MNT_LOCKED(mnt))\n\t\treturn false;\n\n\t/* By default disconnect the mount */\n\treturn true;\n}\n\n/*\n * mount_lock must be held\n * namespace_sem must be held for write\n */\nstatic void umount_tree(struct mount *mnt, enum umount_tree_flags how)\n{\n\tLIST_HEAD(tmp_list);\n\tstruct mount *p;\n\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_mount_unlock(mnt);\n\n\t/* Gather the mounts to umount */\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt.mnt_flags |= MNT_UMOUNT;\n\t\tlist_move(&p->mnt_list, &tmp_list);\n\t}\n\n\t/* Hide the mounts from mnt_mounts */\n\tlist_for_each_entry(p, &tmp_list, mnt_list) {\n\t\tlist_del_init(&p->mnt_child);\n\t}\n\n\t/* Add propogated mounts to the tmp_list */\n\tif (how & UMOUNT_PROPAGATE)\n\t\tpropagate_umount(&tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tstruct mnt_namespace *ns;\n\t\tbool disconnect;\n\t\tp = list_first_entry(&tmp_list, struct mount, mnt_list);\n\t\tlist_del_init(&p->mnt_expire);\n\t\tlist_del_init(&p->mnt_list);\n\t\tns = p->mnt_ns;\n\t\tif (ns) {\n\t\t\tns->mounts--;\n\t\t\t__touch_mnt_namespace(ns);\n\t\t}\n\t\tp->mnt_ns = NULL;\n\t\tif (how & UMOUNT_SYNC)\n\t\t\tp->mnt.mnt_flags |= MNT_SYNC_UMOUNT;\n\n\t\tdisconnect = disconnect_mount(p, how);\n\t\tif (mnt_has_parent(p)) {\n\t\t\tmnt_add_count(p->mnt_parent, -1);\n\t\t\tif (!disconnect) {\n\t\t\t\t/* Don't forget about p */\n\t\t\t\tlist_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);\n\t\t\t} else {\n\t\t\t\tumount_mnt(p);\n\t\t\t}\n\t\t}\n\t\tchange_mnt_propagation(p, MS_PRIVATE);\n\t\tif (disconnect)\n\t\t\thlist_add_head(&p->mnt_umount, &unmounted);\n\t}\n}\n\nstatic void shrink_submounts(struct mount *mnt);\n\nstatic int do_umount_root(struct super_block *sb)\n{\n\tint ret = 0;\n\n\tdown_write(&sb->s_umount);\n\tif (!sb_rdonly(sb)) {\n\t\tstruct fs_context *fc;\n\n\t\tfc = fs_context_for_reconfigure(sb->s_root, SB_RDONLY,\n\t\t\t\t\t\tSB_RDONLY);\n\t\tif (IS_ERR(fc)) {\n\t\t\tret = PTR_ERR(fc);\n\t\t} else {\n\t\t\tret = parse_monolithic_mount_data(fc, NULL);\n\t\t\tif (!ret)\n\t\t\t\tret = reconfigure_super(fc);\n\t\t\tput_fs_context(fc);\n\t\t}\n\t}\n\tup_write(&sb->s_umount);\n\treturn ret;\n}\n\nstatic int do_umount(struct mount *mnt, int flags)\n{\n\tstruct super_block *sb = mnt->mnt.mnt_sb;\n\tint retval;\n\n\tretval = security_sb_umount(&mnt->mnt, flags);\n\tif (retval)\n\t\treturn retval;\n\n\t/*\n\t * Allow userspace to request a mountpoint be expired rather than\n\t * unmounting unconditionally. Unmount only happens if:\n\t *  (1) the mark is already set (the mark is cleared by mntput())\n\t *  (2) the usage count == 1 [parent vfsmount] + 1 [sys_umount]\n\t */\n\tif (flags & MNT_EXPIRE) {\n\t\tif (&mnt->mnt == current->fs->root.mnt ||\n\t\t    flags & (MNT_FORCE | MNT_DETACH))\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * probably don't strictly need the lock here if we examined\n\t\t * all race cases, but it's a slowpath.\n\t\t */\n\t\tlock_mount_hash();\n\t\tif (mnt_get_count(mnt) != 2) {\n\t\t\tunlock_mount_hash();\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tunlock_mount_hash();\n\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1))\n\t\t\treturn -EAGAIN;\n\t}\n\n\t/*\n\t * If we may have to abort operations to get out of this\n\t * mount, and they will themselves hold resources we must\n\t * allow the fs to do things. In the Unix tradition of\n\t * 'Gee thats tricky lets do it in userspace' the umount_begin\n\t * might fail to complete on the first run through as other tasks\n\t * must return, and the like. Thats for the mount program to worry\n\t * about for the moment.\n\t */\n\n\tif (flags & MNT_FORCE && sb->s_op->umount_begin) {\n\t\tsb->s_op->umount_begin(sb);\n\t}\n\n\t/*\n\t * No sense to grab the lock for this test, but test itself looks\n\t * somewhat bogus. Suggestions for better replacement?\n\t * Ho-hum... In principle, we might treat that as umount + switch\n\t * to rootfs. GC would eventually take care of the old vfsmount.\n\t * Actually it makes sense, especially if rootfs would contain a\n\t * /reboot - static binary that would close all descriptors and\n\t * call reboot(9). Then init(8) could umount root and exec /reboot.\n\t */\n\tif (&mnt->mnt == current->fs->root.mnt && !(flags & MNT_DETACH)) {\n\t\t/*\n\t\t * Special case for \"unmounting\" root ...\n\t\t * we just try to remount it readonly.\n\t\t */\n\t\tif (!ns_capable(sb->s_user_ns, CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\treturn do_umount_root(sb);\n\t}\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* Recheck MNT_LOCKED with the locks held */\n\tretval = -EINVAL;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out;\n\n\tevent++;\n\tif (flags & MNT_DETACH) {\n\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE);\n\t\tretval = 0;\n\t} else {\n\t\tshrink_submounts(mnt);\n\t\tretval = -EBUSY;\n\t\tif (!propagate_mount_busy(mnt, 2)) {\n\t\t\tif (!list_empty(&mnt->mnt_list))\n\t\t\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t\tretval = 0;\n\t\t}\n\t}\nout:\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\treturn retval;\n}\n\n/*\n * __detach_mounts - lazily unmount all mounts on the specified dentry\n *\n * During unlink, rmdir, and d_drop it is possible to loose the path\n * to an existing mountpoint, and wind up leaking the mount.\n * detach_mounts allows lazily unmounting those mounts instead of\n * leaking them.\n *\n * The caller may hold dentry->d_inode->i_mutex.\n */\nvoid __detach_mounts(struct dentry *dentry)\n{\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\tmp = lookup_mountpoint(dentry);\n\tif (!mp)\n\t\tgoto out_unlock;\n\n\tevent++;\n\twhile (!hlist_empty(&mp->m_list)) {\n\t\tmnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);\n\t\tif (mnt->mnt.mnt_flags & MNT_UMOUNT) {\n\t\t\tumount_mnt(mnt);\n\t\t\thlist_add_head(&mnt->mnt_umount, &unmounted);\n\t\t}\n\t\telse umount_tree(mnt, UMOUNT_CONNECTED);\n\t}\n\tput_mountpoint(mp);\nout_unlock:\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\n/*\n * Is the caller allowed to modify his namespace?\n */\nstatic inline bool may_mount(void)\n{\n\treturn ns_capable(current->nsproxy->mnt_ns->user_ns, CAP_SYS_ADMIN);\n}\n\n#ifdef\tCONFIG_MANDATORY_FILE_LOCKING\nstatic inline bool may_mandlock(void)\n{\n\treturn capable(CAP_SYS_ADMIN);\n}\n#else\nstatic inline bool may_mandlock(void)\n{\n\tpr_warn(\"VFS: \\\"mand\\\" mount option not supported\");\n\treturn false;\n}\n#endif\n\nstatic int can_umount(const struct path *path, int flags)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\tif (mnt->mnt.mnt_flags & MNT_LOCKED) /* Check optimistically */\n\t\treturn -EINVAL;\n\tif (flags & MNT_FORCE && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\treturn 0;\n}\n\n// caller is responsible for flags being sane\nint path_umount(struct path *path, int flags)\n{\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint ret;\n\n\tret = can_umount(path, flags);\n\tif (!ret)\n\t\tret = do_umount(mnt, flags);\n\n\t/* we mustn't call path_put() as that would clear mnt_expiry_mark */\n\tdput(path->dentry);\n\tmntput_no_expire(mnt);\n\treturn ret;\n}\n\nstatic int ksys_umount(char __user *name, int flags)\n{\n\tint lookup_flags = LOOKUP_MOUNTPOINT;\n\tstruct path path;\n\tint ret;\n\n\t// basic validity checks done first\n\tif (flags & ~(MNT_FORCE | MNT_DETACH | MNT_EXPIRE | UMOUNT_NOFOLLOW))\n\t\treturn -EINVAL;\n\n\tif (!(flags & UMOUNT_NOFOLLOW))\n\t\tlookup_flags |= LOOKUP_FOLLOW;\n\tret = user_path_at(AT_FDCWD, name, lookup_flags, &path);\n\tif (ret)\n\t\treturn ret;\n\treturn path_umount(&path, flags);\n}\n\nSYSCALL_DEFINE2(umount, char __user *, name, int, flags)\n{\n\treturn ksys_umount(name, flags);\n}\n\n#ifdef __ARCH_WANT_SYS_OLDUMOUNT\n\n/*\n *\tThe 2.0 compatible umount. No flags.\n */\nSYSCALL_DEFINE1(oldumount, char __user *, name)\n{\n\treturn ksys_umount(name, 0);\n}\n\n#endif\n\nstatic bool is_mnt_ns_file(struct dentry *dentry)\n{\n\t/* Is this a proxy for a mount namespace? */\n\treturn dentry->d_op == &ns_dentry_operations &&\n\t       dentry->d_fsdata == &mntns_operations;\n}\n\nstatic struct mnt_namespace *to_mnt_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct mnt_namespace, ns);\n}\n\nstruct ns_common *from_mnt_ns(struct mnt_namespace *mnt)\n{\n\treturn &mnt->ns;\n}\n\nstatic bool mnt_ns_loop(struct dentry *dentry)\n{\n\t/* Could bind mounting the mount namespace inode cause a\n\t * mount namespace loop?\n\t */\n\tstruct mnt_namespace *mnt_ns;\n\tif (!is_mnt_ns_file(dentry))\n\t\treturn false;\n\n\tmnt_ns = to_mnt_ns(get_proc_ns(dentry->d_inode));\n\treturn current->nsproxy->mnt_ns->seq >= mnt_ns->seq;\n}\n\nstruct mount *copy_tree(struct mount *mnt, struct dentry *dentry,\n\t\t\t\t\tint flag)\n{\n\tstruct mount *res, *p, *q, *r, *parent;\n\n\tif (!(flag & CL_COPY_UNBINDABLE) && IS_MNT_UNBINDABLE(mnt))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (!(flag & CL_COPY_MNT_NS_FILE) && is_mnt_ns_file(dentry))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tres = q = clone_mnt(mnt, dentry, flag);\n\tif (IS_ERR(q))\n\t\treturn q;\n\n\tq->mnt_mountpoint = mnt->mnt_mountpoint;\n\n\tp = mnt;\n\tlist_for_each_entry(r, &mnt->mnt_mounts, mnt_child) {\n\t\tstruct mount *s;\n\t\tif (!is_subdir(r->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tfor (s = r; s; s = next_mnt(s, r)) {\n\t\t\tif (!(flag & CL_COPY_UNBINDABLE) &&\n\t\t\t    IS_MNT_UNBINDABLE(s)) {\n\t\t\t\tif (s->mnt.mnt_flags & MNT_LOCKED) {\n\t\t\t\t\t/* Both unbindable and locked. */\n\t\t\t\t\tq = ERR_PTR(-EPERM);\n\t\t\t\t\tgoto out;\n\t\t\t\t} else {\n\t\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!(flag & CL_COPY_MNT_NS_FILE) &&\n\t\t\t    is_mnt_ns_file(s->mnt.mnt_root)) {\n\t\t\t\ts = skip_mnt_tree(s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\twhile (p != s->mnt_parent) {\n\t\t\t\tp = p->mnt_parent;\n\t\t\t\tq = q->mnt_parent;\n\t\t\t}\n\t\t\tp = s;\n\t\t\tparent = q;\n\t\t\tq = clone_mnt(p, p->mnt.mnt_root, flag);\n\t\t\tif (IS_ERR(q))\n\t\t\t\tgoto out;\n\t\t\tlock_mount_hash();\n\t\t\tlist_add_tail(&q->mnt_list, &res->mnt_list);\n\t\t\tattach_mnt(q, parent, p->mnt_mp);\n\t\t\tunlock_mount_hash();\n\t\t}\n\t}\n\treturn res;\nout:\n\tif (res) {\n\t\tlock_mount_hash();\n\t\tumount_tree(res, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\n\treturn q;\n}\n\n/* Caller should check returned pointer for errors */\n\nstruct vfsmount *collect_mounts(const struct path *path)\n{\n\tstruct mount *tree;\n\tnamespace_lock();\n\tif (!check_mnt(real_mount(path->mnt)))\n\t\ttree = ERR_PTR(-EINVAL);\n\telse\n\t\ttree = copy_tree(real_mount(path->mnt), path->dentry,\n\t\t\t\t CL_COPY_ALL | CL_PRIVATE);\n\tnamespace_unlock();\n\tif (IS_ERR(tree))\n\t\treturn ERR_CAST(tree);\n\treturn &tree->mnt;\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *);\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *, bool);\n\nvoid dissolve_on_fput(struct vfsmount *mnt)\n{\n\tstruct mnt_namespace *ns;\n\tnamespace_lock();\n\tlock_mount_hash();\n\tns = real_mount(mnt)->mnt_ns;\n\tif (ns) {\n\t\tif (is_anon_ns(ns))\n\t\t\tumount_tree(real_mount(mnt), UMOUNT_CONNECTED);\n\t\telse\n\t\t\tns = NULL;\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\tif (ns)\n\t\tfree_mnt_ns(ns);\n}\n\nvoid drop_collected_mounts(struct vfsmount *mnt)\n{\n\tnamespace_lock();\n\tlock_mount_hash();\n\tumount_tree(real_mount(mnt), 0);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nstatic bool has_locked_children(struct mount *mnt, struct dentry *dentry)\n{\n\tstruct mount *child;\n\n\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\tif (!is_subdir(child->mnt_mountpoint, dentry))\n\t\t\tcontinue;\n\n\t\tif (child->mnt.mnt_flags & MNT_LOCKED)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n/**\n * clone_private_mount - create a private clone of a path\n * @path: path to clone\n *\n * This creates a new vfsmount, which will be the clone of @path.  The new mount\n * will not be attached anywhere in the namespace and will be private (i.e.\n * changes to the originating mount won't be propagated into this).\n *\n * Release with mntput().\n */\nstruct vfsmount *clone_private_mount(const struct path *path)\n{\n\tstruct mount *old_mnt = real_mount(path->mnt);\n\tstruct mount *new_mnt;\n\n\tdown_read(&namespace_sem);\n\tif (IS_MNT_UNBINDABLE(old_mnt))\n\t\tgoto invalid;\n\n\tif (!check_mnt(old_mnt))\n\t\tgoto invalid;\n\n\tif (has_locked_children(old_mnt, path->dentry))\n\t\tgoto invalid;\n\n\tnew_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);\n\tup_read(&namespace_sem);\n\n\tif (IS_ERR(new_mnt))\n\t\treturn ERR_CAST(new_mnt);\n\n\t/* Longterm mount to be removed by kern_unmount*() */\n\tnew_mnt->mnt_ns = MNT_NS_INTERNAL;\n\n\treturn &new_mnt->mnt;\n\ninvalid:\n\tup_read(&namespace_sem);\n\treturn ERR_PTR(-EINVAL);\n}\nEXPORT_SYMBOL_GPL(clone_private_mount);\n\nint iterate_mounts(int (*f)(struct vfsmount *, void *), void *arg,\n\t\t   struct vfsmount *root)\n{\n\tstruct mount *mnt;\n\tint res = f(root, arg);\n\tif (res)\n\t\treturn res;\n\tlist_for_each_entry(mnt, &real_mount(root)->mnt_list, mnt_list) {\n\t\tres = f(&mnt->mnt, arg);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\treturn 0;\n}\n\nstatic void lock_mnt_tree(struct mount *mnt)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tint flags = p->mnt.mnt_flags;\n\t\t/* Don't allow unprivileged users to change mount flags */\n\t\tflags |= MNT_LOCK_ATIME;\n\n\t\tif (flags & MNT_READONLY)\n\t\t\tflags |= MNT_LOCK_READONLY;\n\n\t\tif (flags & MNT_NODEV)\n\t\t\tflags |= MNT_LOCK_NODEV;\n\n\t\tif (flags & MNT_NOSUID)\n\t\t\tflags |= MNT_LOCK_NOSUID;\n\n\t\tif (flags & MNT_NOEXEC)\n\t\t\tflags |= MNT_LOCK_NOEXEC;\n\t\t/* Don't allow unprivileged users to reveal what is under a mount */\n\t\tif (list_empty(&p->mnt_expire))\n\t\t\tflags |= MNT_LOCKED;\n\t\tp->mnt.mnt_flags = flags;\n\t}\n}\n\nstatic void cleanup_group_ids(struct mount *mnt, struct mount *end)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p != end; p = next_mnt(p, mnt)) {\n\t\tif (p->mnt_group_id && !IS_MNT_SHARED(p))\n\t\t\tmnt_release_group_id(p);\n\t}\n}\n\nstatic int invent_group_ids(struct mount *mnt, bool recurse)\n{\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = recurse ? next_mnt(p, mnt) : NULL) {\n\t\tif (!p->mnt_group_id && !IS_MNT_SHARED(p)) {\n\t\t\tint err = mnt_alloc_group_id(p);\n\t\t\tif (err) {\n\t\t\t\tcleanup_group_ids(mnt, p);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint count_mounts(struct mnt_namespace *ns, struct mount *mnt)\n{\n\tunsigned int max = READ_ONCE(sysctl_mount_max);\n\tunsigned int mounts = 0, old, pending, sum;\n\tstruct mount *p;\n\n\tfor (p = mnt; p; p = next_mnt(p, mnt))\n\t\tmounts++;\n\n\told = ns->mounts;\n\tpending = ns->pending_mounts;\n\tsum = old + pending;\n\tif ((old > sum) ||\n\t    (pending > sum) ||\n\t    (max < sum) ||\n\t    (mounts > (max - sum)))\n\t\treturn -ENOSPC;\n\n\tns->pending_mounts = pending + mounts;\n\treturn 0;\n}\n\n/*\n *  @source_mnt : mount tree to be attached\n *  @nd         : place the mount tree @source_mnt is attached\n *  @parent_nd  : if non-null, detach the source_mnt from its parent and\n *  \t\t   store the parent mount and mountpoint dentry.\n *  \t\t   (done when source_mnt is moved)\n *\n *  NOTE: in the table below explains the semantics when a source mount\n *  of a given type is attached to a destination mount of a given type.\n * ---------------------------------------------------------------------------\n * |         BIND MOUNT OPERATION                                            |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (++)   |     shared (+) |     shared(+++)|  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+)    |      private   |      slave (*) |  invalid   |\n * ***************************************************************************\n * A bind operation clones the source mount and mounts the clone on the\n * destination mount.\n *\n * (++)  the cloned mount is propagated to all the mounts in the propagation\n * \t tree of the destination mount and the cloned mount is added to\n * \t the peer group of the source mount.\n * (+)   the cloned mount is created under the destination mount and is marked\n *       as shared. The cloned mount is added to the peer group of the source\n *       mount.\n * (+++) the mount is propagated to all the mounts in the propagation tree\n *       of the destination mount and the cloned mount is made slave\n *       of the same master as that of the source mount. The cloned mount\n *       is marked as 'shared and slave'.\n * (*)   the cloned mount is made a slave of the same master as that of the\n * \t source mount.\n *\n * ---------------------------------------------------------------------------\n * |         \t\tMOVE MOUNT OPERATION                                 |\n * |**************************************************************************\n * | source-->| shared        |       private  |       slave    | unbindable |\n * | dest     |               |                |                |            |\n * |   |      |               |                |                |            |\n * |   v      |               |                |                |            |\n * |**************************************************************************\n * |  shared  | shared (+)    |     shared (+) |    shared(+++) |  invalid   |\n * |          |               |                |                |            |\n * |non-shared| shared (+*)   |      private   |    slave (*)   | unbindable |\n * ***************************************************************************\n *\n * (+)  the mount is moved to the destination. And is then propagated to\n * \tall the mounts in the propagation tree of the destination mount.\n * (+*)  the mount is moved to the destination.\n * (+++)  the mount is moved to the destination and is then propagated to\n * \tall the mounts belonging to the destination mount's propagation tree.\n * \tthe mount is marked as 'shared and slave'.\n * (*)\tthe mount continues to be a slave at the new location.\n *\n * if the source mount is a tree, the operations explained above is\n * applied to each mount in the tree.\n * Must be called without spinlocks held, since this function can sleep\n * in allocations.\n */\nstatic int attach_recursive_mnt(struct mount *source_mnt,\n\t\t\tstruct mount *dest_mnt,\n\t\t\tstruct mountpoint *dest_mp,\n\t\t\tbool moving)\n{\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tHLIST_HEAD(tree_list);\n\tstruct mnt_namespace *ns = dest_mnt->mnt_ns;\n\tstruct mountpoint *smp;\n\tstruct mount *child, *p;\n\tstruct hlist_node *n;\n\tint err;\n\n\t/* Preallocate a mountpoint in case the new mounts need\n\t * to be tucked under other mounts.\n\t */\n\tsmp = get_mountpoint(source_mnt->mnt.mnt_root);\n\tif (IS_ERR(smp))\n\t\treturn PTR_ERR(smp);\n\n\t/* Is there space to add these mounts to the mount namespace? */\n\tif (!moving) {\n\t\terr = count_mounts(ns, source_mnt);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (IS_MNT_SHARED(dest_mnt)) {\n\t\terr = invent_group_ids(source_mnt, true);\n\t\tif (err)\n\t\t\tgoto out;\n\t\terr = propagate_mnt(dest_mnt, dest_mp, source_mnt, &tree_list);\n\t\tlock_mount_hash();\n\t\tif (err)\n\t\t\tgoto out_cleanup_ids;\n\t\tfor (p = source_mnt; p; p = next_mnt(p, source_mnt))\n\t\t\tset_mnt_shared(p);\n\t} else {\n\t\tlock_mount_hash();\n\t}\n\tif (moving) {\n\t\tunhash_mnt(source_mnt);\n\t\tattach_mnt(source_mnt, dest_mnt, dest_mp);\n\t\ttouch_mnt_namespace(source_mnt->mnt_ns);\n\t} else {\n\t\tif (source_mnt->mnt_ns) {\n\t\t\t/* move from anon - the caller will destroy */\n\t\t\tlist_del_init(&source_mnt->mnt_ns->list);\n\t\t}\n\t\tmnt_set_mountpoint(dest_mnt, dest_mp, source_mnt);\n\t\tcommit_tree(source_mnt);\n\t}\n\n\thlist_for_each_entry_safe(child, n, &tree_list, mnt_hash) {\n\t\tstruct mount *q;\n\t\thlist_del_init(&child->mnt_hash);\n\t\tq = __lookup_mnt(&child->mnt_parent->mnt,\n\t\t\t\t child->mnt_mountpoint);\n\t\tif (q)\n\t\t\tmnt_change_mountpoint(child, smp, q);\n\t\t/* Notice when we are propagating across user namespaces */\n\t\tif (child->mnt_parent->mnt_ns->user_ns != user_ns)\n\t\t\tlock_mnt_tree(child);\n\t\tchild->mnt.mnt_flags &= ~MNT_LOCKED;\n\t\tcommit_tree(child);\n\t}\n\tput_mountpoint(smp);\n\tunlock_mount_hash();\n\n\treturn 0;\n\n out_cleanup_ids:\n\twhile (!hlist_empty(&tree_list)) {\n\t\tchild = hlist_entry(tree_list.first, struct mount, mnt_hash);\n\t\tchild->mnt_parent->mnt_ns->pending_mounts = 0;\n\t\tumount_tree(child, UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tcleanup_group_ids(source_mnt, NULL);\n out:\n\tns->pending_mounts = 0;\n\n\tread_seqlock_excl(&mount_lock);\n\tput_mountpoint(smp);\n\tread_sequnlock_excl(&mount_lock);\n\n\treturn err;\n}\n\nstatic struct mountpoint *lock_mount(struct path *path)\n{\n\tstruct vfsmount *mnt;\n\tstruct dentry *dentry = path->dentry;\nretry:\n\tinode_lock(dentry->d_inode);\n\tif (unlikely(cant_mount(dentry))) {\n\t\tinode_unlock(dentry->d_inode);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\tnamespace_lock();\n\tmnt = lookup_mnt(path);\n\tif (likely(!mnt)) {\n\t\tstruct mountpoint *mp = get_mountpoint(dentry);\n\t\tif (IS_ERR(mp)) {\n\t\t\tnamespace_unlock();\n\t\t\tinode_unlock(dentry->d_inode);\n\t\t\treturn mp;\n\t\t}\n\t\treturn mp;\n\t}\n\tnamespace_unlock();\n\tinode_unlock(path->dentry->d_inode);\n\tpath_put(path);\n\tpath->mnt = mnt;\n\tdentry = path->dentry = dget(mnt->mnt_root);\n\tgoto retry;\n}\n\nstatic void unlock_mount(struct mountpoint *where)\n{\n\tstruct dentry *dentry = where->m_dentry;\n\n\tread_seqlock_excl(&mount_lock);\n\tput_mountpoint(where);\n\tread_sequnlock_excl(&mount_lock);\n\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\n}\n\nstatic int graft_tree(struct mount *mnt, struct mount *p, struct mountpoint *mp)\n{\n\tif (mnt->mnt.mnt_sb->s_flags & SB_NOUSER)\n\t\treturn -EINVAL;\n\n\tif (d_is_dir(mp->m_dentry) !=\n\t      d_is_dir(mnt->mnt.mnt_root))\n\t\treturn -ENOTDIR;\n\n\treturn attach_recursive_mnt(mnt, p, mp, false);\n}\n\n/*\n * Sanity check the flags to change_mnt_propagation.\n */\n\nstatic int flags_to_propagation_type(int ms_flags)\n{\n\tint type = ms_flags & ~(MS_REC | MS_SILENT);\n\n\t/* Fail if any non-propagation flags are set */\n\tif (type & ~(MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn 0;\n\t/* Only one propagation flag should be set */\n\tif (!is_power_of_2(type))\n\t\treturn 0;\n\treturn type;\n}\n\n/*\n * recursively change the type of the mountpoint.\n */\nstatic int do_change_type(struct path *path, int ms_flags)\n{\n\tstruct mount *m;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint recurse = ms_flags & MS_REC;\n\tint type;\n\tint err = 0;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\ttype = flags_to_propagation_type(ms_flags);\n\tif (!type)\n\t\treturn -EINVAL;\n\n\tnamespace_lock();\n\tif (type == MS_SHARED) {\n\t\terr = invent_group_ids(mnt, recurse);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tlock_mount_hash();\n\tfor (m = mnt; m; m = (recurse ? next_mnt(m, mnt) : NULL))\n\t\tchange_mnt_propagation(m, type);\n\tunlock_mount_hash();\n\n out_unlock:\n\tnamespace_unlock();\n\treturn err;\n}\n\nstatic struct mount *__do_loopback(struct path *old_path, int recurse)\n{\n\tstruct mount *mnt = ERR_PTR(-EINVAL), *old = real_mount(old_path->mnt);\n\n\tif (IS_MNT_UNBINDABLE(old))\n\t\treturn mnt;\n\n\tif (!check_mnt(old) && old_path->dentry->d_op != &ns_dentry_operations)\n\t\treturn mnt;\n\n\tif (!recurse && has_locked_children(old, old_path->dentry))\n\t\treturn mnt;\n\n\tif (recurse)\n\t\tmnt = copy_tree(old, old_path->dentry, CL_COPY_MNT_NS_FILE);\n\telse\n\t\tmnt = clone_mnt(old, old_path->dentry, 0);\n\n\tif (!IS_ERR(mnt))\n\t\tmnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\n\treturn mnt;\n}\n\n/*\n * do loopback mount.\n */\nstatic int do_loopback(struct path *path, const char *old_name,\n\t\t\t\tint recurse)\n{\n\tstruct path old_path;\n\tstruct mount *mnt = NULL, *parent;\n\tstruct mountpoint *mp;\n\tint err;\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\terr = kern_path(old_name, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = -EINVAL;\n\tif (mnt_ns_loop(old_path.dentry))\n\t\tgoto out;\n\n\tmp = lock_mount(path);\n\tif (IS_ERR(mp)) {\n\t\terr = PTR_ERR(mp);\n\t\tgoto out;\n\t}\n\n\tparent = real_mount(path->mnt);\n\tif (!check_mnt(parent))\n\t\tgoto out2;\n\n\tmnt = __do_loopback(&old_path, recurse);\n\tif (IS_ERR(mnt)) {\n\t\terr = PTR_ERR(mnt);\n\t\tgoto out2;\n\t}\n\n\terr = graft_tree(mnt, parent, mp);\n\tif (err) {\n\t\tlock_mount_hash();\n\t\tumount_tree(mnt, UMOUNT_SYNC);\n\t\tunlock_mount_hash();\n\t}\nout2:\n\tunlock_mount(mp);\nout:\n\tpath_put(&old_path);\n\treturn err;\n}\n\nstatic struct file *open_detached_copy(struct path *path, bool recursive)\n{\n\tstruct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;\n\tstruct mnt_namespace *ns = alloc_mnt_ns(user_ns, true);\n\tstruct mount *mnt, *p;\n\tstruct file *file;\n\n\tif (IS_ERR(ns))\n\t\treturn ERR_CAST(ns);\n\n\tnamespace_lock();\n\tmnt = __do_loopback(path, recursive);\n\tif (IS_ERR(mnt)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(ns);\n\t\treturn ERR_CAST(mnt);\n\t}\n\n\tlock_mount_hash();\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tp->mnt_ns = ns;\n\t\tns->mounts++;\n\t}\n\tns->root = mnt;\n\tlist_add_tail(&ns->list, &mnt->mnt_list);\n\tmntget(&mnt->mnt);\n\tunlock_mount_hash();\n\tnamespace_unlock();\n\n\tmntput(path->mnt);\n\tpath->mnt = &mnt->mnt;\n\tfile = dentry_open(path, O_PATH, current_cred());\n\tif (IS_ERR(file))\n\t\tdissolve_on_fput(path->mnt);\n\telse\n\t\tfile->f_mode |= FMODE_NEED_UNMOUNT;\n\treturn file;\n}\n\nSYSCALL_DEFINE3(open_tree, int, dfd, const char __user *, filename, unsigned, flags)\n{\n\tstruct file *file;\n\tstruct path path;\n\tint lookup_flags = LOOKUP_AUTOMOUNT | LOOKUP_FOLLOW;\n\tbool detached = flags & OPEN_TREE_CLONE;\n\tint error;\n\tint fd;\n\n\tBUILD_BUG_ON(OPEN_TREE_CLOEXEC != O_CLOEXEC);\n\n\tif (flags & ~(AT_EMPTY_PATH | AT_NO_AUTOMOUNT | AT_RECURSIVE |\n\t\t      AT_SYMLINK_NOFOLLOW | OPEN_TREE_CLONE |\n\t\t      OPEN_TREE_CLOEXEC))\n\t\treturn -EINVAL;\n\n\tif ((flags & (AT_RECURSIVE | OPEN_TREE_CLONE)) == AT_RECURSIVE)\n\t\treturn -EINVAL;\n\n\tif (flags & AT_NO_AUTOMOUNT)\n\t\tlookup_flags &= ~LOOKUP_AUTOMOUNT;\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\tlookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\n\tif (detached && !may_mount())\n\t\treturn -EPERM;\n\n\tfd = get_unused_fd_flags(flags & O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\n\terror = user_path_at(dfd, filename, lookup_flags, &path);\n\tif (unlikely(error)) {\n\t\tfile = ERR_PTR(error);\n\t} else {\n\t\tif (detached)\n\t\t\tfile = open_detached_copy(&path, flags & AT_RECURSIVE);\n\t\telse\n\t\t\tfile = dentry_open(&path, O_PATH, current_cred());\n\t\tpath_put(&path);\n\t}\n\tif (IS_ERR(file)) {\n\t\tput_unused_fd(fd);\n\t\treturn PTR_ERR(file);\n\t}\n\tfd_install(fd, file);\n\treturn fd;\n}\n\n/*\n * Don't allow locked mount flags to be cleared.\n *\n * No locks need to be held here while testing the various MNT_LOCK\n * flags because those flags can never be cleared once they are set.\n */\nstatic bool can_change_locked_flags(struct mount *mnt, unsigned int mnt_flags)\n{\n\tunsigned int fl = mnt->mnt.mnt_flags;\n\n\tif ((fl & MNT_LOCK_READONLY) &&\n\t    !(mnt_flags & MNT_READONLY))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NODEV) &&\n\t    !(mnt_flags & MNT_NODEV))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NOSUID) &&\n\t    !(mnt_flags & MNT_NOSUID))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_NOEXEC) &&\n\t    !(mnt_flags & MNT_NOEXEC))\n\t\treturn false;\n\n\tif ((fl & MNT_LOCK_ATIME) &&\n\t    ((fl & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int change_mount_ro_state(struct mount *mnt, unsigned int mnt_flags)\n{\n\tbool readonly_request = (mnt_flags & MNT_READONLY);\n\n\tif (readonly_request == __mnt_is_readonly(&mnt->mnt))\n\t\treturn 0;\n\n\tif (readonly_request)\n\t\treturn mnt_make_readonly(mnt);\n\n\tmnt->mnt.mnt_flags &= ~MNT_READONLY;\n\treturn 0;\n}\n\nstatic void set_mount_attributes(struct mount *mnt, unsigned int mnt_flags)\n{\n\tmnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;\n\tmnt->mnt.mnt_flags = mnt_flags;\n\ttouch_mnt_namespace(mnt->mnt_ns);\n}\n\nstatic void mnt_warn_timestamp_expiry(struct path *mountpoint, struct vfsmount *mnt)\n{\n\tstruct super_block *sb = mnt->mnt_sb;\n\n\tif (!__mnt_is_readonly(mnt) &&\n\t   (ktime_get_real_seconds() + TIME_UPTIME_SEC_MAX > sb->s_time_max)) {\n\t\tchar *buf = (char *)__get_free_page(GFP_KERNEL);\n\t\tchar *mntpath = buf ? d_path(mountpoint, buf, PAGE_SIZE) : ERR_PTR(-ENOMEM);\n\t\tstruct tm tm;\n\n\t\ttime64_to_tm(sb->s_time_max, 0, &tm);\n\n\t\tpr_warn(\"%s filesystem being %s at %s supports timestamps until %04ld (0x%llx)\\n\",\n\t\t\tsb->s_type->name,\n\t\t\tis_mounted(mnt) ? \"remounted\" : \"mounted\",\n\t\t\tmntpath,\n\t\t\ttm.tm_year+1900, (unsigned long long)sb->s_time_max);\n\n\t\tfree_page((unsigned long)buf);\n\t}\n}\n\n/*\n * Handle reconfiguration of the mountpoint only without alteration of the\n * superblock it refers to.  This is triggered by specifying MS_REMOUNT|MS_BIND\n * to mount(2).\n */\nstatic int do_reconfigure_mnt(struct path *path, unsigned int mnt_flags)\n{\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tint ret;\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != mnt->mnt.mnt_root)\n\t\treturn -EINVAL;\n\n\tif (!can_change_locked_flags(mnt, mnt_flags))\n\t\treturn -EPERM;\n\n\t/*\n\t * We're only checking whether the superblock is read-only not\n\t * changing it, so only take down_read(&sb->s_umount).\n\t */\n\tdown_read(&sb->s_umount);\n\tlock_mount_hash();\n\tret = change_mount_ro_state(mnt, mnt_flags);\n\tif (ret == 0)\n\t\tset_mount_attributes(mnt, mnt_flags);\n\tunlock_mount_hash();\n\tup_read(&sb->s_umount);\n\n\tmnt_warn_timestamp_expiry(path, &mnt->mnt);\n\n\treturn ret;\n}\n\n/*\n * change filesystem flags. dir should be a physical root of filesystem.\n * If you've mounted a non-root directory somewhere and want to do remount\n * on it - tough luck.\n */\nstatic int do_remount(struct path *path, int ms_flags, int sb_flags,\n\t\t      int mnt_flags, void *data)\n{\n\tint err;\n\tstruct super_block *sb = path->mnt->mnt_sb;\n\tstruct mount *mnt = real_mount(path->mnt);\n\tstruct fs_context *fc;\n\n\tif (!check_mnt(mnt))\n\t\treturn -EINVAL;\n\n\tif (path->dentry != path->mnt->mnt_root)\n\t\treturn -EINVAL;\n\n\tif (!can_change_locked_flags(mnt, mnt_flags))\n\t\treturn -EPERM;\n\n\tfc = fs_context_for_reconfigure(path->dentry, sb_flags, MS_RMT_MASK);\n\tif (IS_ERR(fc))\n\t\treturn PTR_ERR(fc);\n\n\tfc->oldapi = true;\n\terr = parse_monolithic_mount_data(fc, data);\n\tif (!err) {\n\t\tdown_write(&sb->s_umount);\n\t\terr = -EPERM;\n\t\tif (ns_capable(sb->s_user_ns, CAP_SYS_ADMIN)) {\n\t\t\terr = reconfigure_super(fc);\n\t\t\tif (!err) {\n\t\t\t\tlock_mount_hash();\n\t\t\t\tset_mount_attributes(mnt, mnt_flags);\n\t\t\t\tunlock_mount_hash();\n\t\t\t}\n\t\t}\n\t\tup_write(&sb->s_umount);\n\t}\n\n\tmnt_warn_timestamp_expiry(path, &mnt->mnt);\n\n\tput_fs_context(fc);\n\treturn err;\n}\n\nstatic inline int tree_contains_unbindable(struct mount *mnt)\n{\n\tstruct mount *p;\n\tfor (p = mnt; p; p = next_mnt(p, mnt)) {\n\t\tif (IS_MNT_UNBINDABLE(p))\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n/*\n * Check that there aren't references to earlier/same mount namespaces in the\n * specified subtree.  Such references can act as pins for mount namespaces\n * that aren't checked by the mount-cycle checking code, thereby allowing\n * cycles to be made.\n */\nstatic bool check_for_nsfs_mounts(struct mount *subtree)\n{\n\tstruct mount *p;\n\tbool ret = false;\n\n\tlock_mount_hash();\n\tfor (p = subtree; p; p = next_mnt(p, subtree))\n\t\tif (mnt_ns_loop(p->mnt.mnt_root))\n\t\t\tgoto out;\n\n\tret = true;\nout:\n\tunlock_mount_hash();\n\treturn ret;\n}\n\nstatic int do_move_mount(struct path *old_path, struct path *new_path)\n{\n\tstruct mnt_namespace *ns;\n\tstruct mount *p;\n\tstruct mount *old;\n\tstruct mount *parent;\n\tstruct mountpoint *mp, *old_mp;\n\tint err;\n\tbool attached;\n\n\tmp = lock_mount(new_path);\n\tif (IS_ERR(mp))\n\t\treturn PTR_ERR(mp);\n\n\told = real_mount(old_path->mnt);\n\tp = real_mount(new_path->mnt);\n\tparent = old->mnt_parent;\n\tattached = mnt_has_parent(old);\n\told_mp = old->mnt_mp;\n\tns = old->mnt_ns;\n\n\terr = -EINVAL;\n\t/* The mountpoint must be in our namespace. */\n\tif (!check_mnt(p))\n\t\tgoto out;\n\n\t/* The thing moved must be mounted... */\n\tif (!is_mounted(&old->mnt))\n\t\tgoto out;\n\n\t/* ... and either ours or the root of anon namespace */\n\tif (!(attached ? check_mnt(old) : is_anon_ns(ns)))\n\t\tgoto out;\n\n\tif (old->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out;\n\n\tif (old_path->dentry != old_path->mnt->mnt_root)\n\t\tgoto out;\n\n\tif (d_is_dir(new_path->dentry) !=\n\t    d_is_dir(old_path->dentry))\n\t\tgoto out;\n\t/*\n\t * Don't move a mount residing in a shared parent.\n\t */\n\tif (attached && IS_MNT_SHARED(parent))\n\t\tgoto out;\n\t/*\n\t * Don't move a mount tree containing unbindable mounts to a destination\n\t * mount which is shared.\n\t */\n\tif (IS_MNT_SHARED(p) && tree_contains_unbindable(old))\n\t\tgoto out;\n\terr = -ELOOP;\n\tif (!check_for_nsfs_mounts(old))\n\t\tgoto out;\n\tfor (; mnt_has_parent(p); p = p->mnt_parent)\n\t\tif (p == old)\n\t\t\tgoto out;\n\n\terr = attach_recursive_mnt(old, real_mount(new_path->mnt), mp,\n\t\t\t\t   attached);\n\tif (err)\n\t\tgoto out;\n\n\t/* if the mount is moved, it should no longer be expire\n\t * automatically */\n\tlist_del_init(&old->mnt_expire);\n\tif (attached)\n\t\tput_mountpoint(old_mp);\nout:\n\tunlock_mount(mp);\n\tif (!err) {\n\t\tif (attached)\n\t\t\tmntput_no_expire(parent);\n\t\telse\n\t\t\tfree_mnt_ns(ns);\n\t}\n\treturn err;\n}\n\nstatic int do_move_mount_old(struct path *path, const char *old_name)\n{\n\tstruct path old_path;\n\tint err;\n\n\tif (!old_name || !*old_name)\n\t\treturn -EINVAL;\n\n\terr = kern_path(old_name, LOOKUP_FOLLOW, &old_path);\n\tif (err)\n\t\treturn err;\n\n\terr = do_move_mount(&old_path, path);\n\tpath_put(&old_path);\n\treturn err;\n}\n\n/*\n * add a mount into a namespace's mount tree\n */\nstatic int do_add_mount(struct mount *newmnt, struct mountpoint *mp,\n\t\t\tstruct path *path, int mnt_flags)\n{\n\tstruct mount *parent = real_mount(path->mnt);\n\n\tmnt_flags &= ~MNT_INTERNAL_FLAGS;\n\n\tif (unlikely(!check_mnt(parent))) {\n\t\t/* that's acceptable only for automounts done in private ns */\n\t\tif (!(mnt_flags & MNT_SHRINKABLE))\n\t\t\treturn -EINVAL;\n\t\t/* ... and for those we'd better have mountpoint still alive */\n\t\tif (!parent->mnt_ns)\n\t\t\treturn -EINVAL;\n\t}\n\n\t/* Refuse the same filesystem on the same mount point */\n\tif (path->mnt->mnt_sb == newmnt->mnt.mnt_sb &&\n\t    path->mnt->mnt_root == path->dentry)\n\t\treturn -EBUSY;\n\n\tif (d_is_symlink(newmnt->mnt.mnt_root))\n\t\treturn -EINVAL;\n\n\tnewmnt->mnt.mnt_flags = mnt_flags;\n\treturn graft_tree(newmnt, parent, mp);\n}\n\nstatic bool mount_too_revealing(const struct super_block *sb, int *new_mnt_flags);\n\n/*\n * Create a new mount using a superblock configuration and request it\n * be added to the namespace tree.\n */\nstatic int do_new_mount_fc(struct fs_context *fc, struct path *mountpoint,\n\t\t\t   unsigned int mnt_flags)\n{\n\tstruct vfsmount *mnt;\n\tstruct mountpoint *mp;\n\tstruct super_block *sb = fc->root->d_sb;\n\tint error;\n\n\terror = security_sb_kern_mount(sb);\n\tif (!error && mount_too_revealing(sb, &mnt_flags))\n\t\terror = -EPERM;\n\n\tif (unlikely(error)) {\n\t\tfc_drop_locked(fc);\n\t\treturn error;\n\t}\n\n\tup_write(&sb->s_umount);\n\n\tmnt = vfs_create_mount(fc);\n\tif (IS_ERR(mnt))\n\t\treturn PTR_ERR(mnt);\n\n\tmnt_warn_timestamp_expiry(mountpoint, mnt);\n\n\tmp = lock_mount(mountpoint);\n\tif (IS_ERR(mp)) {\n\t\tmntput(mnt);\n\t\treturn PTR_ERR(mp);\n\t}\n\terror = do_add_mount(real_mount(mnt), mp, mountpoint, mnt_flags);\n\tunlock_mount(mp);\n\tif (error < 0)\n\t\tmntput(mnt);\n\treturn error;\n}\n\n/*\n * create a new mount for userspace and request it to be added into the\n * namespace's tree\n */\nstatic int do_new_mount(struct path *path, const char *fstype, int sb_flags,\n\t\t\tint mnt_flags, const char *name, void *data)\n{\n\tstruct file_system_type *type;\n\tstruct fs_context *fc;\n\tconst char *subtype = NULL;\n\tint err = 0;\n\n\tif (!fstype)\n\t\treturn -EINVAL;\n\n\ttype = get_fs_type(fstype);\n\tif (!type)\n\t\treturn -ENODEV;\n\n\tif (type->fs_flags & FS_HAS_SUBTYPE) {\n\t\tsubtype = strchr(fstype, '.');\n\t\tif (subtype) {\n\t\t\tsubtype++;\n\t\t\tif (!*subtype) {\n\t\t\t\tput_filesystem(type);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\tfc = fs_context_for_mount(type, sb_flags);\n\tput_filesystem(type);\n\tif (IS_ERR(fc))\n\t\treturn PTR_ERR(fc);\n\n\tif (subtype)\n\t\terr = vfs_parse_fs_string(fc, \"subtype\",\n\t\t\t\t\t  subtype, strlen(subtype));\n\tif (!err && name)\n\t\terr = vfs_parse_fs_string(fc, \"source\", name, strlen(name));\n\tif (!err)\n\t\terr = parse_monolithic_mount_data(fc, data);\n\tif (!err && !mount_capable(fc))\n\t\terr = -EPERM;\n\tif (!err)\n\t\terr = vfs_get_tree(fc);\n\tif (!err)\n\t\terr = do_new_mount_fc(fc, path, mnt_flags);\n\n\tput_fs_context(fc);\n\treturn err;\n}\n\nint finish_automount(struct vfsmount *m, struct path *path)\n{\n\tstruct dentry *dentry = path->dentry;\n\tstruct mountpoint *mp;\n\tstruct mount *mnt;\n\tint err;\n\n\tif (!m)\n\t\treturn 0;\n\tif (IS_ERR(m))\n\t\treturn PTR_ERR(m);\n\n\tmnt = real_mount(m);\n\t/* The new mount record should have at least 2 refs to prevent it being\n\t * expired before we get a chance to add it\n\t */\n\tBUG_ON(mnt_get_count(mnt) < 2);\n\n\tif (m->mnt_sb == path->mnt->mnt_sb &&\n\t    m->mnt_root == dentry) {\n\t\terr = -ELOOP;\n\t\tgoto discard;\n\t}\n\n\t/*\n\t * we don't want to use lock_mount() - in this case finding something\n\t * that overmounts our mountpoint to be means \"quitely drop what we've\n\t * got\", not \"try to mount it on top\".\n\t */\n\tinode_lock(dentry->d_inode);\n\tnamespace_lock();\n\tif (unlikely(cant_mount(dentry))) {\n\t\terr = -ENOENT;\n\t\tgoto discard_locked;\n\t}\n\trcu_read_lock();\n\tif (unlikely(__lookup_mnt(path->mnt, dentry))) {\n\t\trcu_read_unlock();\n\t\terr = 0;\n\t\tgoto discard_locked;\n\t}\n\trcu_read_unlock();\n\tmp = get_mountpoint(dentry);\n\tif (IS_ERR(mp)) {\n\t\terr = PTR_ERR(mp);\n\t\tgoto discard_locked;\n\t}\n\n\terr = do_add_mount(mnt, mp, path, path->mnt->mnt_flags | MNT_SHRINKABLE);\n\tunlock_mount(mp);\n\tif (unlikely(err))\n\t\tgoto discard;\n\tmntput(m);\n\treturn 0;\n\ndiscard_locked:\n\tnamespace_unlock();\n\tinode_unlock(dentry->d_inode);\ndiscard:\n\t/* remove m from any expiration list it may be on */\n\tif (!list_empty(&mnt->mnt_expire)) {\n\t\tnamespace_lock();\n\t\tlist_del_init(&mnt->mnt_expire);\n\t\tnamespace_unlock();\n\t}\n\tmntput(m);\n\tmntput(m);\n\treturn err;\n}\n\n/**\n * mnt_set_expiry - Put a mount on an expiration list\n * @mnt: The mount to list.\n * @expiry_list: The list to add the mount to.\n */\nvoid mnt_set_expiry(struct vfsmount *mnt, struct list_head *expiry_list)\n{\n\tnamespace_lock();\n\n\tlist_add_tail(&real_mount(mnt)->mnt_expire, expiry_list);\n\n\tnamespace_unlock();\n}\nEXPORT_SYMBOL(mnt_set_expiry);\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * mountpoints that aren't in use and haven't been touched since last we came\n * here\n */\nvoid mark_mounts_for_expiry(struct list_head *mounts)\n{\n\tstruct mount *mnt, *next;\n\tLIST_HEAD(graveyard);\n\n\tif (list_empty(mounts))\n\t\treturn;\n\n\tnamespace_lock();\n\tlock_mount_hash();\n\n\t/* extract from the expiration list every vfsmount that matches the\n\t * following criteria:\n\t * - only referenced by its parent vfsmount\n\t * - still marked for expiry (marked on the last call here; marks are\n\t *   cleared by mntput())\n\t */\n\tlist_for_each_entry_safe(mnt, next, mounts, mnt_expire) {\n\t\tif (!xchg(&mnt->mnt_expiry_mark, 1) ||\n\t\t\tpropagate_mount_busy(mnt, 1))\n\t\t\tcontinue;\n\t\tlist_move(&mnt->mnt_expire, &graveyard);\n\t}\n\twhile (!list_empty(&graveyard)) {\n\t\tmnt = list_first_entry(&graveyard, struct mount, mnt_expire);\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n\t\tumount_tree(mnt, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t}\n\tunlock_mount_hash();\n\tnamespace_unlock();\n}\n\nEXPORT_SYMBOL_GPL(mark_mounts_for_expiry);\n\n/*\n * Ripoff of 'select_parent()'\n *\n * search the list of submounts for a given mountpoint, and move any\n * shrinkable submounts to the 'graveyard' list.\n */\nstatic int select_submounts(struct mount *parent, struct list_head *graveyard)\n{\n\tstruct mount *this_parent = parent;\n\tstruct list_head *next;\n\tint found = 0;\n\nrepeat:\n\tnext = this_parent->mnt_mounts.next;\nresume:\n\twhile (next != &this_parent->mnt_mounts) {\n\t\tstruct list_head *tmp = next;\n\t\tstruct mount *mnt = list_entry(tmp, struct mount, mnt_child);\n\n\t\tnext = tmp->next;\n\t\tif (!(mnt->mnt.mnt_flags & MNT_SHRINKABLE))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Descend a level if the d_mounts list is non-empty.\n\t\t */\n\t\tif (!list_empty(&mnt->mnt_mounts)) {\n\t\t\tthis_parent = mnt;\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tif (!propagate_mount_busy(mnt, 1)) {\n\t\t\tlist_move_tail(&mnt->mnt_expire, graveyard);\n\t\t\tfound++;\n\t\t}\n\t}\n\t/*\n\t * All done at this level ... ascend and resume the search\n\t */\n\tif (this_parent != parent) {\n\t\tnext = this_parent->mnt_child.next;\n\t\tthis_parent = this_parent->mnt_parent;\n\t\tgoto resume;\n\t}\n\treturn found;\n}\n\n/*\n * process a list of expirable mountpoints with the intent of discarding any\n * submounts of a specific parent mountpoint\n *\n * mount_lock must be held for write\n */\nstatic void shrink_submounts(struct mount *mnt)\n{\n\tLIST_HEAD(graveyard);\n\tstruct mount *m;\n\n\t/* extract submounts of 'mountpoint' from the expiration list */\n\twhile (select_submounts(mnt, &graveyard)) {\n\t\twhile (!list_empty(&graveyard)) {\n\t\t\tm = list_first_entry(&graveyard, struct mount,\n\t\t\t\t\t\tmnt_expire);\n\t\t\ttouch_mnt_namespace(m->mnt_ns);\n\t\t\tumount_tree(m, UMOUNT_PROPAGATE|UMOUNT_SYNC);\n\t\t}\n\t}\n}\n\nstatic void *copy_mount_options(const void __user * data)\n{\n\tchar *copy;\n\tunsigned left, offset;\n\n\tif (!data)\n\t\treturn NULL;\n\n\tcopy = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!copy)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tleft = copy_from_user(copy, data, PAGE_SIZE);\n\n\t/*\n\t * Not all architectures have an exact copy_from_user(). Resort to\n\t * byte at a time.\n\t */\n\toffset = PAGE_SIZE - left;\n\twhile (left) {\n\t\tchar c;\n\t\tif (get_user(c, (const char __user *)data + offset))\n\t\t\tbreak;\n\t\tcopy[offset] = c;\n\t\tleft--;\n\t\toffset++;\n\t}\n\n\tif (left == PAGE_SIZE) {\n\t\tkfree(copy);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn copy;\n}\n\nstatic char *copy_mount_string(const void __user *data)\n{\n\treturn data ? strndup_user(data, PATH_MAX) : NULL;\n}\n\n/*\n * Flags is a 32-bit value that allows up to 31 non-fs dependent flags to\n * be given to the mount() call (ie: read-only, no-dev, no-suid etc).\n *\n * data is a (void *) that can point to any structure up to\n * PAGE_SIZE-1 bytes, which can contain arbitrary fs-dependent\n * information (or be NULL).\n *\n * Pre-0.97 versions of mount() didn't have a flags word.\n * When the flags word was introduced its top half was required\n * to have the magic value 0xC0ED, and this remained so until 2.4.0-test9.\n * Therefore, if this magic number is present, it carries no information\n * and must be discarded.\n */\nint path_mount(const char *dev_name, struct path *path,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tunsigned int mnt_flags = 0, sb_flags;\n\tint ret;\n\n\t/* Discard magic */\n\tif ((flags & MS_MGC_MSK) == MS_MGC_VAL)\n\t\tflags &= ~MS_MGC_MSK;\n\n\t/* Basic sanity checks */\n\tif (data_page)\n\t\t((char *)data_page)[PAGE_SIZE - 1] = 0;\n\n\tif (flags & MS_NOUSER)\n\t\treturn -EINVAL;\n\n\tret = security_sb_mount(dev_name, path, type_page, flags, data_page);\n\tif (ret)\n\t\treturn ret;\n\tif (!may_mount())\n\t\treturn -EPERM;\n\tif ((flags & SB_MANDLOCK) && !may_mandlock())\n\t\treturn -EPERM;\n\n\t/* Default to relatime unless overriden */\n\tif (!(flags & MS_NOATIME))\n\t\tmnt_flags |= MNT_RELATIME;\n\n\t/* Separate the per-mountpoint flags */\n\tif (flags & MS_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (flags & MS_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (flags & MS_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (flags & MS_NOATIME)\n\t\tmnt_flags |= MNT_NOATIME;\n\tif (flags & MS_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (flags & MS_STRICTATIME)\n\t\tmnt_flags &= ~(MNT_RELATIME | MNT_NOATIME);\n\tif (flags & MS_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\tif (flags & MS_NOSYMFOLLOW)\n\t\tmnt_flags |= MNT_NOSYMFOLLOW;\n\n\t/* The default atime for remount is preservation */\n\tif ((flags & MS_REMOUNT) &&\n\t    ((flags & (MS_NOATIME | MS_NODIRATIME | MS_RELATIME |\n\t\t       MS_STRICTATIME)) == 0)) {\n\t\tmnt_flags &= ~MNT_ATIME_MASK;\n\t\tmnt_flags |= path->mnt->mnt_flags & MNT_ATIME_MASK;\n\t}\n\n\tsb_flags = flags & (SB_RDONLY |\n\t\t\t    SB_SYNCHRONOUS |\n\t\t\t    SB_MANDLOCK |\n\t\t\t    SB_DIRSYNC |\n\t\t\t    SB_SILENT |\n\t\t\t    SB_POSIXACL |\n\t\t\t    SB_LAZYTIME |\n\t\t\t    SB_I_VERSION);\n\n\tif ((flags & (MS_REMOUNT | MS_BIND)) == (MS_REMOUNT | MS_BIND))\n\t\treturn do_reconfigure_mnt(path, mnt_flags);\n\tif (flags & MS_REMOUNT)\n\t\treturn do_remount(path, flags, sb_flags, mnt_flags, data_page);\n\tif (flags & MS_BIND)\n\t\treturn do_loopback(path, dev_name, flags & MS_REC);\n\tif (flags & (MS_SHARED | MS_PRIVATE | MS_SLAVE | MS_UNBINDABLE))\n\t\treturn do_change_type(path, flags);\n\tif (flags & MS_MOVE)\n\t\treturn do_move_mount_old(path, dev_name);\n\n\treturn do_new_mount(path, type_page, sb_flags, mnt_flags, dev_name,\n\t\t\t    data_page);\n}\n\nlong do_mount(const char *dev_name, const char __user *dir_name,\n\t\tconst char *type_page, unsigned long flags, void *data_page)\n{\n\tstruct path path;\n\tint ret;\n\n\tret = user_path_at(AT_FDCWD, dir_name, LOOKUP_FOLLOW, &path);\n\tif (ret)\n\t\treturn ret;\n\tret = path_mount(dev_name, &path, type_page, flags, data_page);\n\tpath_put(&path);\n\treturn ret;\n}\n\nstatic struct ucounts *inc_mnt_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void dec_mnt_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_MNT_NAMESPACES);\n}\n\nstatic void free_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!is_anon_ns(ns))\n\t\tns_free_inum(&ns->ns);\n\tdec_mnt_namespaces(ns->ucounts);\n\tput_user_ns(ns->user_ns);\n\tkfree(ns);\n}\n\n/*\n * Assign a sequence number so we can detect when we attempt to bind\n * mount a reference to an older mount namespace into the current\n * mount namespace, preventing reference counting loops.  A 64bit\n * number incrementing at 10Ghz will take 12,427 years to wrap which\n * is effectively never, so we can ignore the possibility.\n */\nstatic atomic64_t mnt_ns_seq = ATOMIC64_INIT(1);\n\nstatic struct mnt_namespace *alloc_mnt_ns(struct user_namespace *user_ns, bool anon)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct ucounts *ucounts;\n\tint ret;\n\n\tucounts = inc_mnt_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnew_ns = kzalloc(sizeof(struct mnt_namespace), GFP_KERNEL);\n\tif (!new_ns) {\n\t\tdec_mnt_namespaces(ucounts);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tif (!anon) {\n\t\tret = ns_alloc_inum(&new_ns->ns);\n\t\tif (ret) {\n\t\t\tkfree(new_ns);\n\t\t\tdec_mnt_namespaces(ucounts);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\tnew_ns->ns.ops = &mntns_operations;\n\tif (!anon)\n\t\tnew_ns->seq = atomic64_add_return(1, &mnt_ns_seq);\n\trefcount_set(&new_ns->ns.count, 1);\n\tINIT_LIST_HEAD(&new_ns->list);\n\tinit_waitqueue_head(&new_ns->poll);\n\tspin_lock_init(&new_ns->ns_lock);\n\tnew_ns->user_ns = get_user_ns(user_ns);\n\tnew_ns->ucounts = ucounts;\n\treturn new_ns;\n}\n\n__latent_entropy\nstruct mnt_namespace *copy_mnt_ns(unsigned long flags, struct mnt_namespace *ns,\n\t\tstruct user_namespace *user_ns, struct fs_struct *new_fs)\n{\n\tstruct mnt_namespace *new_ns;\n\tstruct vfsmount *rootmnt = NULL, *pwdmnt = NULL;\n\tstruct mount *p, *q;\n\tstruct mount *old;\n\tstruct mount *new;\n\tint copy_flags;\n\n\tBUG_ON(!ns);\n\n\tif (likely(!(flags & CLONE_NEWNS))) {\n\t\tget_mnt_ns(ns);\n\t\treturn ns;\n\t}\n\n\told = ns->root;\n\n\tnew_ns = alloc_mnt_ns(user_ns, false);\n\tif (IS_ERR(new_ns))\n\t\treturn new_ns;\n\n\tnamespace_lock();\n\t/* First pass: copy the tree topology */\n\tcopy_flags = CL_COPY_UNBINDABLE | CL_EXPIRE;\n\tif (user_ns != ns->user_ns)\n\t\tcopy_flags |= CL_SHARED_TO_SLAVE;\n\tnew = copy_tree(old, old->mnt.mnt_root, copy_flags);\n\tif (IS_ERR(new)) {\n\t\tnamespace_unlock();\n\t\tfree_mnt_ns(new_ns);\n\t\treturn ERR_CAST(new);\n\t}\n\tif (user_ns != ns->user_ns) {\n\t\tlock_mount_hash();\n\t\tlock_mnt_tree(new);\n\t\tunlock_mount_hash();\n\t}\n\tnew_ns->root = new;\n\tlist_add_tail(&new_ns->list, &new->mnt_list);\n\n\t/*\n\t * Second pass: switch the tsk->fs->* elements and mark new vfsmounts\n\t * as belonging to new namespace.  We have already acquired a private\n\t * fs_struct, so tsk->fs->lock is not needed.\n\t */\n\tp = old;\n\tq = new;\n\twhile (p) {\n\t\tq->mnt_ns = new_ns;\n\t\tnew_ns->mounts++;\n\t\tif (new_fs) {\n\t\t\tif (&p->mnt == new_fs->root.mnt) {\n\t\t\t\tnew_fs->root.mnt = mntget(&q->mnt);\n\t\t\t\trootmnt = &p->mnt;\n\t\t\t}\n\t\t\tif (&p->mnt == new_fs->pwd.mnt) {\n\t\t\t\tnew_fs->pwd.mnt = mntget(&q->mnt);\n\t\t\t\tpwdmnt = &p->mnt;\n\t\t\t}\n\t\t}\n\t\tp = next_mnt(p, old);\n\t\tq = next_mnt(q, new);\n\t\tif (!q)\n\t\t\tbreak;\n\t\twhile (p->mnt.mnt_root != q->mnt.mnt_root)\n\t\t\tp = next_mnt(p, old);\n\t}\n\tnamespace_unlock();\n\n\tif (rootmnt)\n\t\tmntput(rootmnt);\n\tif (pwdmnt)\n\t\tmntput(pwdmnt);\n\n\treturn new_ns;\n}\n\nstruct dentry *mount_subtree(struct vfsmount *m, const char *name)\n{\n\tstruct mount *mnt = real_mount(m);\n\tstruct mnt_namespace *ns;\n\tstruct super_block *s;\n\tstruct path path;\n\tint err;\n\n\tns = alloc_mnt_ns(&init_user_ns, true);\n\tif (IS_ERR(ns)) {\n\t\tmntput(m);\n\t\treturn ERR_CAST(ns);\n\t}\n\tmnt->mnt_ns = ns;\n\tns->root = mnt;\n\tns->mounts++;\n\tlist_add(&mnt->mnt_list, &ns->list);\n\n\terr = vfs_path_lookup(m->mnt_root, m,\n\t\t\tname, LOOKUP_FOLLOW|LOOKUP_AUTOMOUNT, &path);\n\n\tput_mnt_ns(ns);\n\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\t/* trade a vfsmount reference for active sb one */\n\ts = path.mnt->mnt_sb;\n\tatomic_inc(&s->s_active);\n\tmntput(path.mnt);\n\t/* lock the sucker */\n\tdown_write(&s->s_umount);\n\t/* ... and return the root of (sub)tree on it */\n\treturn path.dentry;\n}\nEXPORT_SYMBOL(mount_subtree);\n\nSYSCALL_DEFINE5(mount, char __user *, dev_name, char __user *, dir_name,\n\t\tchar __user *, type, unsigned long, flags, void __user *, data)\n{\n\tint ret;\n\tchar *kernel_type;\n\tchar *kernel_dev;\n\tvoid *options;\n\n\tkernel_type = copy_mount_string(type);\n\tret = PTR_ERR(kernel_type);\n\tif (IS_ERR(kernel_type))\n\t\tgoto out_type;\n\n\tkernel_dev = copy_mount_string(dev_name);\n\tret = PTR_ERR(kernel_dev);\n\tif (IS_ERR(kernel_dev))\n\t\tgoto out_dev;\n\n\toptions = copy_mount_options(data);\n\tret = PTR_ERR(options);\n\tif (IS_ERR(options))\n\t\tgoto out_data;\n\n\tret = do_mount(kernel_dev, dir_name, kernel_type, flags, options);\n\n\tkfree(options);\nout_data:\n\tkfree(kernel_dev);\nout_dev:\n\tkfree(kernel_type);\nout_type:\n\treturn ret;\n}\n\n#define FSMOUNT_VALID_FLAGS                                                    \\\n\t(MOUNT_ATTR_RDONLY | MOUNT_ATTR_NOSUID | MOUNT_ATTR_NODEV |            \\\n\t MOUNT_ATTR_NOEXEC | MOUNT_ATTR__ATIME | MOUNT_ATTR_NODIRATIME |       \\\n\t MOUNT_ATTR_NOSYMFOLLOW)\n\n#define MOUNT_SETATTR_VALID_FLAGS (FSMOUNT_VALID_FLAGS | MOUNT_ATTR_IDMAP)\n\n#define MOUNT_SETATTR_PROPAGATION_FLAGS \\\n\t(MS_UNBINDABLE | MS_PRIVATE | MS_SLAVE | MS_SHARED)\n\nstatic unsigned int attr_flags_to_mnt_flags(u64 attr_flags)\n{\n\tunsigned int mnt_flags = 0;\n\n\tif (attr_flags & MOUNT_ATTR_RDONLY)\n\t\tmnt_flags |= MNT_READONLY;\n\tif (attr_flags & MOUNT_ATTR_NOSUID)\n\t\tmnt_flags |= MNT_NOSUID;\n\tif (attr_flags & MOUNT_ATTR_NODEV)\n\t\tmnt_flags |= MNT_NODEV;\n\tif (attr_flags & MOUNT_ATTR_NOEXEC)\n\t\tmnt_flags |= MNT_NOEXEC;\n\tif (attr_flags & MOUNT_ATTR_NODIRATIME)\n\t\tmnt_flags |= MNT_NODIRATIME;\n\tif (attr_flags & MOUNT_ATTR_NOSYMFOLLOW)\n\t\tmnt_flags |= MNT_NOSYMFOLLOW;\n\n\treturn mnt_flags;\n}\n\n/*\n * Create a kernel mount representation for a new, prepared superblock\n * (specified by fs_fd) and attach to an open_tree-like file descriptor.\n */\nSYSCALL_DEFINE3(fsmount, int, fs_fd, unsigned int, flags,\n\t\tunsigned int, attr_flags)\n{\n\tstruct mnt_namespace *ns;\n\tstruct fs_context *fc;\n\tstruct file *file;\n\tstruct path newmount;\n\tstruct mount *mnt;\n\tstruct fd f;\n\tunsigned int mnt_flags = 0;\n\tlong ret;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif ((flags & ~(FSMOUNT_CLOEXEC)) != 0)\n\t\treturn -EINVAL;\n\n\tif (attr_flags & ~FSMOUNT_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tmnt_flags = attr_flags_to_mnt_flags(attr_flags);\n\n\tswitch (attr_flags & MOUNT_ATTR__ATIME) {\n\tcase MOUNT_ATTR_STRICTATIME:\n\t\tbreak;\n\tcase MOUNT_ATTR_NOATIME:\n\t\tmnt_flags |= MNT_NOATIME;\n\t\tbreak;\n\tcase MOUNT_ATTR_RELATIME:\n\t\tmnt_flags |= MNT_RELATIME;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tf = fdget(fs_fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = -EINVAL;\n\tif (f.file->f_op != &fscontext_fops)\n\t\tgoto err_fsfd;\n\n\tfc = f.file->private_data;\n\n\tret = mutex_lock_interruptible(&fc->uapi_mutex);\n\tif (ret < 0)\n\t\tgoto err_fsfd;\n\n\t/* There must be a valid superblock or we can't mount it */\n\tret = -EINVAL;\n\tif (!fc->root)\n\t\tgoto err_unlock;\n\n\tret = -EPERM;\n\tif (mount_too_revealing(fc->root->d_sb, &mnt_flags)) {\n\t\tpr_warn(\"VFS: Mount too revealing\\n\");\n\t\tgoto err_unlock;\n\t}\n\n\tret = -EBUSY;\n\tif (fc->phase != FS_CONTEXT_AWAITING_MOUNT)\n\t\tgoto err_unlock;\n\n\tret = -EPERM;\n\tif ((fc->sb_flags & SB_MANDLOCK) && !may_mandlock())\n\t\tgoto err_unlock;\n\n\tnewmount.mnt = vfs_create_mount(fc);\n\tif (IS_ERR(newmount.mnt)) {\n\t\tret = PTR_ERR(newmount.mnt);\n\t\tgoto err_unlock;\n\t}\n\tnewmount.dentry = dget(fc->root);\n\tnewmount.mnt->mnt_flags = mnt_flags;\n\n\t/* We've done the mount bit - now move the file context into more or\n\t * less the same state as if we'd done an fspick().  We don't want to\n\t * do any memory allocation or anything like that at this point as we\n\t * don't want to have to handle any errors incurred.\n\t */\n\tvfs_clean_context(fc);\n\n\tns = alloc_mnt_ns(current->nsproxy->mnt_ns->user_ns, true);\n\tif (IS_ERR(ns)) {\n\t\tret = PTR_ERR(ns);\n\t\tgoto err_path;\n\t}\n\tmnt = real_mount(newmount.mnt);\n\tmnt->mnt_ns = ns;\n\tns->root = mnt;\n\tns->mounts = 1;\n\tlist_add(&mnt->mnt_list, &ns->list);\n\tmntget(newmount.mnt);\n\n\t/* Attach to an apparent O_PATH fd with a note that we need to unmount\n\t * it, not just simply put it.\n\t */\n\tfile = dentry_open(&newmount, O_PATH, fc->cred);\n\tif (IS_ERR(file)) {\n\t\tdissolve_on_fput(newmount.mnt);\n\t\tret = PTR_ERR(file);\n\t\tgoto err_path;\n\t}\n\tfile->f_mode |= FMODE_NEED_UNMOUNT;\n\n\tret = get_unused_fd_flags((flags & FSMOUNT_CLOEXEC) ? O_CLOEXEC : 0);\n\tif (ret >= 0)\n\t\tfd_install(ret, file);\n\telse\n\t\tfput(file);\n\nerr_path:\n\tpath_put(&newmount);\nerr_unlock:\n\tmutex_unlock(&fc->uapi_mutex);\nerr_fsfd:\n\tfdput(f);\n\treturn ret;\n}\n\n/*\n * Move a mount from one place to another.  In combination with\n * fsopen()/fsmount() this is used to install a new mount and in combination\n * with open_tree(OPEN_TREE_CLONE [| AT_RECURSIVE]) it can be used to copy\n * a mount subtree.\n *\n * Note the flags value is a combination of MOVE_MOUNT_* flags.\n */\nSYSCALL_DEFINE5(move_mount,\n\t\tint, from_dfd, const char __user *, from_pathname,\n\t\tint, to_dfd, const char __user *, to_pathname,\n\t\tunsigned int, flags)\n{\n\tstruct path from_path, to_path;\n\tunsigned int lflags;\n\tint ret = 0;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\tif (flags & ~MOVE_MOUNT__MASK)\n\t\treturn -EINVAL;\n\n\t/* If someone gives a pathname, they aren't permitted to move\n\t * from an fd that requires unmount as we can't get at the flag\n\t * to clear it afterwards.\n\t */\n\tlflags = 0;\n\tif (flags & MOVE_MOUNT_F_SYMLINKS)\tlflags |= LOOKUP_FOLLOW;\n\tif (flags & MOVE_MOUNT_F_AUTOMOUNTS)\tlflags |= LOOKUP_AUTOMOUNT;\n\tif (flags & MOVE_MOUNT_F_EMPTY_PATH)\tlflags |= LOOKUP_EMPTY;\n\n\tret = user_path_at(from_dfd, from_pathname, lflags, &from_path);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tlflags = 0;\n\tif (flags & MOVE_MOUNT_T_SYMLINKS)\tlflags |= LOOKUP_FOLLOW;\n\tif (flags & MOVE_MOUNT_T_AUTOMOUNTS)\tlflags |= LOOKUP_AUTOMOUNT;\n\tif (flags & MOVE_MOUNT_T_EMPTY_PATH)\tlflags |= LOOKUP_EMPTY;\n\n\tret = user_path_at(to_dfd, to_pathname, lflags, &to_path);\n\tif (ret < 0)\n\t\tgoto out_from;\n\n\tret = security_move_mount(&from_path, &to_path);\n\tif (ret < 0)\n\t\tgoto out_to;\n\n\tret = do_move_mount(&from_path, &to_path);\n\nout_to:\n\tpath_put(&to_path);\nout_from:\n\tpath_put(&from_path);\n\treturn ret;\n}\n\n/*\n * Return true if path is reachable from root\n *\n * namespace_sem or mount_lock is held\n */\nbool is_path_reachable(struct mount *mnt, struct dentry *dentry,\n\t\t\t const struct path *root)\n{\n\twhile (&mnt->mnt != root->mnt && mnt_has_parent(mnt)) {\n\t\tdentry = mnt->mnt_mountpoint;\n\t\tmnt = mnt->mnt_parent;\n\t}\n\treturn &mnt->mnt == root->mnt && is_subdir(dentry, root->dentry);\n}\n\nbool path_is_under(const struct path *path1, const struct path *path2)\n{\n\tbool res;\n\tread_seqlock_excl(&mount_lock);\n\tres = is_path_reachable(real_mount(path1->mnt), path1->dentry, path2);\n\tread_sequnlock_excl(&mount_lock);\n\treturn res;\n}\nEXPORT_SYMBOL(path_is_under);\n\n/*\n * pivot_root Semantics:\n * Moves the root file system of the current process to the directory put_old,\n * makes new_root as the new root file system of the current process, and sets\n * root/cwd of all processes which had them on the current root to new_root.\n *\n * Restrictions:\n * The new_root and put_old must be directories, and  must not be on the\n * same file  system as the current process root. The put_old  must  be\n * underneath new_root,  i.e. adding a non-zero number of /.. to the string\n * pointed to by put_old must yield the same directory as new_root. No other\n * file system may be mounted on put_old. After all, new_root is a mountpoint.\n *\n * Also, the current root cannot be on the 'rootfs' (initial ramfs) filesystem.\n * See Documentation/filesystems/ramfs-rootfs-initramfs.rst for alternatives\n * in this situation.\n *\n * Notes:\n *  - we don't move root/cwd if they are not at the root (reason: if something\n *    cared enough to change them, it's probably wrong to force them elsewhere)\n *  - it's okay to pick a root that isn't the root of a file system, e.g.\n *    /nfs/my_root where /nfs is the mount point. It must be a mountpoint,\n *    though, so you may need to say mount --bind /nfs/my_root /nfs/my_root\n *    first.\n */\nSYSCALL_DEFINE2(pivot_root, const char __user *, new_root,\n\t\tconst char __user *, put_old)\n{\n\tstruct path new, old, root;\n\tstruct mount *new_mnt, *root_mnt, *old_mnt, *root_parent, *ex_parent;\n\tstruct mountpoint *old_mp, *root_mp;\n\tint error;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terror = user_path_at(AT_FDCWD, new_root,\n\t\t\t     LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &new);\n\tif (error)\n\t\tgoto out0;\n\n\terror = user_path_at(AT_FDCWD, put_old,\n\t\t\t     LOOKUP_FOLLOW | LOOKUP_DIRECTORY, &old);\n\tif (error)\n\t\tgoto out1;\n\n\terror = security_sb_pivotroot(&old, &new);\n\tif (error)\n\t\tgoto out2;\n\n\tget_fs_root(current->fs, &root);\n\told_mp = lock_mount(&old);\n\terror = PTR_ERR(old_mp);\n\tif (IS_ERR(old_mp))\n\t\tgoto out3;\n\n\terror = -EINVAL;\n\tnew_mnt = real_mount(new.mnt);\n\troot_mnt = real_mount(root.mnt);\n\told_mnt = real_mount(old.mnt);\n\tex_parent = new_mnt->mnt_parent;\n\troot_parent = root_mnt->mnt_parent;\n\tif (IS_MNT_SHARED(old_mnt) ||\n\t\tIS_MNT_SHARED(ex_parent) ||\n\t\tIS_MNT_SHARED(root_parent))\n\t\tgoto out4;\n\tif (!check_mnt(root_mnt) || !check_mnt(new_mnt))\n\t\tgoto out4;\n\tif (new_mnt->mnt.mnt_flags & MNT_LOCKED)\n\t\tgoto out4;\n\terror = -ENOENT;\n\tif (d_unlinked(new.dentry))\n\t\tgoto out4;\n\terror = -EBUSY;\n\tif (new_mnt == root_mnt || old_mnt == root_mnt)\n\t\tgoto out4; /* loop, on the same file system  */\n\terror = -EINVAL;\n\tif (root.mnt->mnt_root != root.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(root_mnt))\n\t\tgoto out4; /* not attached */\n\tif (new.mnt->mnt_root != new.dentry)\n\t\tgoto out4; /* not a mountpoint */\n\tif (!mnt_has_parent(new_mnt))\n\t\tgoto out4; /* not attached */\n\t/* make sure we can reach put_old from new_root */\n\tif (!is_path_reachable(old_mnt, old.dentry, &new))\n\t\tgoto out4;\n\t/* make certain new is below the root */\n\tif (!is_path_reachable(new_mnt, new.dentry, &root))\n\t\tgoto out4;\n\tlock_mount_hash();\n\tumount_mnt(new_mnt);\n\troot_mp = unhash_mnt(root_mnt);  /* we'll need its mountpoint */\n\tif (root_mnt->mnt.mnt_flags & MNT_LOCKED) {\n\t\tnew_mnt->mnt.mnt_flags |= MNT_LOCKED;\n\t\troot_mnt->mnt.mnt_flags &= ~MNT_LOCKED;\n\t}\n\t/* mount old root on put_old */\n\tattach_mnt(root_mnt, old_mnt, old_mp);\n\t/* mount new_root on / */\n\tattach_mnt(new_mnt, root_parent, root_mp);\n\tmnt_add_count(root_parent, -1);\n\ttouch_mnt_namespace(current->nsproxy->mnt_ns);\n\t/* A moved mount should not expire automatically */\n\tlist_del_init(&new_mnt->mnt_expire);\n\tput_mountpoint(root_mp);\n\tunlock_mount_hash();\n\tchroot_fs_refs(&root, &new);\n\terror = 0;\nout4:\n\tunlock_mount(old_mp);\n\tif (!error)\n\t\tmntput_no_expire(ex_parent);\nout3:\n\tpath_put(&root);\nout2:\n\tpath_put(&old);\nout1:\n\tpath_put(&new);\nout0:\n\treturn error;\n}\n\nstatic unsigned int recalc_flags(struct mount_kattr *kattr, struct mount *mnt)\n{\n\tunsigned int flags = mnt->mnt.mnt_flags;\n\n\t/*  flags to clear */\n\tflags &= ~kattr->attr_clr;\n\t/* flags to raise */\n\tflags |= kattr->attr_set;\n\n\treturn flags;\n}\n\nstatic int can_idmap_mount(const struct mount_kattr *kattr, struct mount *mnt)\n{\n\tstruct vfsmount *m = &mnt->mnt;\n\n\tif (!kattr->mnt_userns)\n\t\treturn 0;\n\n\t/*\n\t * Once a mount has been idmapped we don't allow it to change its\n\t * mapping. It makes things simpler and callers can just create\n\t * another bind-mount they can idmap if they want to.\n\t */\n\tif (mnt_user_ns(m) != &init_user_ns)\n\t\treturn -EPERM;\n\n\t/* The underlying filesystem doesn't support idmapped mounts yet. */\n\tif (!(m->mnt_sb->s_type->fs_flags & FS_ALLOW_IDMAP))\n\t\treturn -EINVAL;\n\n\t/* Don't yet support filesystem mountable in user namespaces. */\n\tif (m->mnt_sb->s_user_ns != &init_user_ns)\n\t\treturn -EINVAL;\n\n\t/* We're not controlling the superblock. */\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\t/* Mount has already been visible in the filesystem hierarchy. */\n\tif (!is_anon_ns(mnt->mnt_ns))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct mount *mount_setattr_prepare(struct mount_kattr *kattr,\n\t\t\t\t\t   struct mount *mnt, int *err)\n{\n\tstruct mount *m = mnt, *last = NULL;\n\n\tif (!is_mounted(&m->mnt)) {\n\t\t*err = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!(mnt_has_parent(m) ? check_mnt(m) : is_anon_ns(m->mnt_ns))) {\n\t\t*err = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tunsigned int flags;\n\n\t\tflags = recalc_flags(kattr, m);\n\t\tif (!can_change_locked_flags(m, flags)) {\n\t\t\t*err = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\n\t\t*err = can_idmap_mount(kattr, m);\n\t\tif (*err)\n\t\t\tgoto out;\n\n\t\tlast = m;\n\n\t\tif ((kattr->attr_set & MNT_READONLY) &&\n\t\t    !(m->mnt.mnt_flags & MNT_READONLY)) {\n\t\t\t*err = mnt_hold_writers(m);\n\t\t\tif (*err)\n\t\t\t\tgoto out;\n\t\t}\n\t} while (kattr->recurse && (m = next_mnt(m, mnt)));\n\nout:\n\treturn last;\n}\n\nstatic void do_idmap_mount(const struct mount_kattr *kattr, struct mount *mnt)\n{\n\tstruct user_namespace *mnt_userns;\n\n\tif (!kattr->mnt_userns)\n\t\treturn;\n\n\tmnt_userns = get_user_ns(kattr->mnt_userns);\n\t/* Pairs with smp_load_acquire() in mnt_user_ns(). */\n\tsmp_store_release(&mnt->mnt.mnt_userns, mnt_userns);\n}\n\nstatic void mount_setattr_commit(struct mount_kattr *kattr,\n\t\t\t\t struct mount *mnt, struct mount *last,\n\t\t\t\t int err)\n{\n\tstruct mount *m = mnt;\n\n\tdo {\n\t\tif (!err) {\n\t\t\tunsigned int flags;\n\n\t\t\tdo_idmap_mount(kattr, m);\n\t\t\tflags = recalc_flags(kattr, m);\n\t\t\tWRITE_ONCE(m->mnt.mnt_flags, flags);\n\t\t}\n\n\t\t/*\n\t\t * We either set MNT_READONLY above so make it visible\n\t\t * before ~MNT_WRITE_HOLD or we failed to recursively\n\t\t * apply mount options.\n\t\t */\n\t\tif ((kattr->attr_set & MNT_READONLY) &&\n\t\t    (m->mnt.mnt_flags & MNT_WRITE_HOLD))\n\t\t\tmnt_unhold_writers(m);\n\n\t\tif (!err && kattr->propagation)\n\t\t\tchange_mnt_propagation(m, kattr->propagation);\n\n\t\t/*\n\t\t * On failure, only cleanup until we found the first mount\n\t\t * we failed to handle.\n\t\t */\n\t\tif (err && m == last)\n\t\t\tbreak;\n\t} while (kattr->recurse && (m = next_mnt(m, mnt)));\n\n\tif (!err)\n\t\ttouch_mnt_namespace(mnt->mnt_ns);\n}\n\nstatic int do_mount_setattr(struct path *path, struct mount_kattr *kattr)\n{\n\tstruct mount *mnt = real_mount(path->mnt), *last = NULL;\n\tint err = 0;\n\n\tif (path->dentry != mnt->mnt.mnt_root)\n\t\treturn -EINVAL;\n\n\tif (kattr->propagation) {\n\t\t/*\n\t\t * Only take namespace_lock() if we're actually changing\n\t\t * propagation.\n\t\t */\n\t\tnamespace_lock();\n\t\tif (kattr->propagation == MS_SHARED) {\n\t\t\terr = invent_group_ids(mnt, kattr->recurse);\n\t\t\tif (err) {\n\t\t\t\tnamespace_unlock();\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\n\tlock_mount_hash();\n\n\t/*\n\t * Get the mount tree in a shape where we can change mount\n\t * properties without failure.\n\t */\n\tlast = mount_setattr_prepare(kattr, mnt, &err);\n\tif (last) /* Commit all changes or revert to the old state. */\n\t\tmount_setattr_commit(kattr, mnt, last, err);\n\n\tunlock_mount_hash();\n\n\tif (kattr->propagation) {\n\t\tnamespace_unlock();\n\t\tif (err)\n\t\t\tcleanup_group_ids(mnt, NULL);\n\t}\n\n\treturn err;\n}\n\nstatic int build_mount_idmapped(const struct mount_attr *attr, size_t usize,\n\t\t\t\tstruct mount_kattr *kattr, unsigned int flags)\n{\n\tint err = 0;\n\tstruct ns_common *ns;\n\tstruct user_namespace *mnt_userns;\n\tstruct file *file;\n\n\tif (!((attr->attr_set | attr->attr_clr) & MOUNT_ATTR_IDMAP))\n\t\treturn 0;\n\n\t/*\n\t * We currently do not support clearing an idmapped mount. If this ever\n\t * is a use-case we can revisit this but for now let's keep it simple\n\t * and not allow it.\n\t */\n\tif (attr->attr_clr & MOUNT_ATTR_IDMAP)\n\t\treturn -EINVAL;\n\n\tif (attr->userns_fd > INT_MAX)\n\t\treturn -EINVAL;\n\n\tfile = fget(attr->userns_fd);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tif (!proc_ns_file(file)) {\n\t\terr = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\tns = get_proc_ns(file_inode(file));\n\tif (ns->ops->type != CLONE_NEWUSER) {\n\t\terr = -EINVAL;\n\t\tgoto out_fput;\n\t}\n\n\t/*\n\t * The init_user_ns is used to indicate that a vfsmount is not idmapped.\n\t * This is simpler than just having to treat NULL as unmapped. Users\n\t * wanting to idmap a mount to init_user_ns can just use a namespace\n\t * with an identity mapping.\n\t */\n\tmnt_userns = container_of(ns, struct user_namespace, ns);\n\tif (mnt_userns == &init_user_ns) {\n\t\terr = -EPERM;\n\t\tgoto out_fput;\n\t}\n\tkattr->mnt_userns = get_user_ns(mnt_userns);\n\nout_fput:\n\tfput(file);\n\treturn err;\n}\n\nstatic int build_mount_kattr(const struct mount_attr *attr, size_t usize,\n\t\t\t     struct mount_kattr *kattr, unsigned int flags)\n{\n\tunsigned int lookup_flags = LOOKUP_AUTOMOUNT | LOOKUP_FOLLOW;\n\n\tif (flags & AT_NO_AUTOMOUNT)\n\t\tlookup_flags &= ~LOOKUP_AUTOMOUNT;\n\tif (flags & AT_SYMLINK_NOFOLLOW)\n\t\tlookup_flags &= ~LOOKUP_FOLLOW;\n\tif (flags & AT_EMPTY_PATH)\n\t\tlookup_flags |= LOOKUP_EMPTY;\n\n\t*kattr = (struct mount_kattr) {\n\t\t.lookup_flags\t= lookup_flags,\n\t\t.recurse\t= !!(flags & AT_RECURSIVE),\n\t};\n\n\tif (attr->propagation & ~MOUNT_SETATTR_PROPAGATION_FLAGS)\n\t\treturn -EINVAL;\n\tif (hweight32(attr->propagation & MOUNT_SETATTR_PROPAGATION_FLAGS) > 1)\n\t\treturn -EINVAL;\n\tkattr->propagation = attr->propagation;\n\n\tif ((attr->attr_set | attr->attr_clr) & ~MOUNT_SETATTR_VALID_FLAGS)\n\t\treturn -EINVAL;\n\n\tkattr->attr_set = attr_flags_to_mnt_flags(attr->attr_set);\n\tkattr->attr_clr = attr_flags_to_mnt_flags(attr->attr_clr);\n\n\t/*\n\t * Since the MOUNT_ATTR_<atime> values are an enum, not a bitmap,\n\t * users wanting to transition to a different atime setting cannot\n\t * simply specify the atime setting in @attr_set, but must also\n\t * specify MOUNT_ATTR__ATIME in the @attr_clr field.\n\t * So ensure that MOUNT_ATTR__ATIME can't be partially set in\n\t * @attr_clr and that @attr_set can't have any atime bits set if\n\t * MOUNT_ATTR__ATIME isn't set in @attr_clr.\n\t */\n\tif (attr->attr_clr & MOUNT_ATTR__ATIME) {\n\t\tif ((attr->attr_clr & MOUNT_ATTR__ATIME) != MOUNT_ATTR__ATIME)\n\t\t\treturn -EINVAL;\n\n\t\t/*\n\t\t * Clear all previous time settings as they are mutually\n\t\t * exclusive.\n\t\t */\n\t\tkattr->attr_clr |= MNT_RELATIME | MNT_NOATIME;\n\t\tswitch (attr->attr_set & MOUNT_ATTR__ATIME) {\n\t\tcase MOUNT_ATTR_RELATIME:\n\t\t\tkattr->attr_set |= MNT_RELATIME;\n\t\t\tbreak;\n\t\tcase MOUNT_ATTR_NOATIME:\n\t\t\tkattr->attr_set |= MNT_NOATIME;\n\t\t\tbreak;\n\t\tcase MOUNT_ATTR_STRICTATIME:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (attr->attr_set & MOUNT_ATTR__ATIME)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn build_mount_idmapped(attr, usize, kattr, flags);\n}\n\nstatic void finish_mount_kattr(struct mount_kattr *kattr)\n{\n\tput_user_ns(kattr->mnt_userns);\n\tkattr->mnt_userns = NULL;\n}\n\nSYSCALL_DEFINE5(mount_setattr, int, dfd, const char __user *, path,\n\t\tunsigned int, flags, struct mount_attr __user *, uattr,\n\t\tsize_t, usize)\n{\n\tint err;\n\tstruct path target;\n\tstruct mount_attr attr;\n\tstruct mount_kattr kattr;\n\n\tBUILD_BUG_ON(sizeof(struct mount_attr) != MOUNT_ATTR_SIZE_VER0);\n\n\tif (flags & ~(AT_EMPTY_PATH |\n\t\t      AT_RECURSIVE |\n\t\t      AT_SYMLINK_NOFOLLOW |\n\t\t      AT_NO_AUTOMOUNT))\n\t\treturn -EINVAL;\n\n\tif (unlikely(usize > PAGE_SIZE))\n\t\treturn -E2BIG;\n\tif (unlikely(usize < MOUNT_ATTR_SIZE_VER0))\n\t\treturn -EINVAL;\n\n\tif (!may_mount())\n\t\treturn -EPERM;\n\n\terr = copy_struct_from_user(&attr, sizeof(attr), uattr, usize);\n\tif (err)\n\t\treturn err;\n\n\t/* Don't bother walking through the mounts if this is a nop. */\n\tif (attr.attr_set == 0 &&\n\t    attr.attr_clr == 0 &&\n\t    attr.propagation == 0)\n\t\treturn 0;\n\n\terr = build_mount_kattr(&attr, usize, &kattr, flags);\n\tif (err)\n\t\treturn err;\n\n\terr = user_path_at(dfd, path, kattr.lookup_flags, &target);\n\tif (err)\n\t\treturn err;\n\n\terr = do_mount_setattr(&target, &kattr);\n\tfinish_mount_kattr(&kattr);\n\tpath_put(&target);\n\treturn err;\n}\n\nstatic void __init init_mount_tree(void)\n{\n\tstruct vfsmount *mnt;\n\tstruct mount *m;\n\tstruct mnt_namespace *ns;\n\tstruct path root;\n\n\tmnt = vfs_kern_mount(&rootfs_fs_type, 0, \"rootfs\", NULL);\n\tif (IS_ERR(mnt))\n\t\tpanic(\"Can't create rootfs\");\n\n\tns = alloc_mnt_ns(&init_user_ns, false);\n\tif (IS_ERR(ns))\n\t\tpanic(\"Can't allocate initial namespace\");\n\tm = real_mount(mnt);\n\tm->mnt_ns = ns;\n\tns->root = m;\n\tns->mounts = 1;\n\tlist_add(&m->mnt_list, &ns->list);\n\tinit_task.nsproxy->mnt_ns = ns;\n\tget_mnt_ns(ns);\n\n\troot.mnt = mnt;\n\troot.dentry = mnt->mnt_root;\n\tmnt->mnt_flags |= MNT_LOCKED;\n\n\tset_fs_pwd(current->fs, &root);\n\tset_fs_root(current->fs, &root);\n}\n\nvoid __init mnt_init(void)\n{\n\tint err;\n\n\tmnt_cache = kmem_cache_create(\"mnt_cache\", sizeof(struct mount),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\n\tmount_hashtable = alloc_large_system_hash(\"Mount-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmhash_entries, 19,\n\t\t\t\tHASH_ZERO,\n\t\t\t\t&m_hash_shift, &m_hash_mask, 0, 0);\n\tmountpoint_hashtable = alloc_large_system_hash(\"Mountpoint-cache\",\n\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\tmphash_entries, 19,\n\t\t\t\tHASH_ZERO,\n\t\t\t\t&mp_hash_shift, &mp_hash_mask, 0, 0);\n\n\tif (!mount_hashtable || !mountpoint_hashtable)\n\t\tpanic(\"Failed to allocate mount hash table\\n\");\n\n\tkernfs_init();\n\n\terr = sysfs_init();\n\tif (err)\n\t\tprintk(KERN_WARNING \"%s: sysfs_init error: %d\\n\",\n\t\t\t__func__, err);\n\tfs_kobj = kobject_create_and_add(\"fs\", NULL);\n\tif (!fs_kobj)\n\t\tprintk(KERN_WARNING \"%s: kobj create error\\n\", __func__);\n\tshmem_init();\n\tinit_rootfs();\n\tinit_mount_tree();\n}\n\nvoid put_mnt_ns(struct mnt_namespace *ns)\n{\n\tif (!refcount_dec_and_test(&ns->ns.count))\n\t\treturn;\n\tdrop_collected_mounts(&ns->root->mnt);\n\tfree_mnt_ns(ns);\n}\n\nstruct vfsmount *kern_mount(struct file_system_type *type)\n{\n\tstruct vfsmount *mnt;\n\tmnt = vfs_kern_mount(type, SB_KERNMOUNT, type->name, NULL);\n\tif (!IS_ERR(mnt)) {\n\t\t/*\n\t\t * it is a longterm mount, don't release mnt until\n\t\t * we unmount before file sys is unregistered\n\t\t*/\n\t\treal_mount(mnt)->mnt_ns = MNT_NS_INTERNAL;\n\t}\n\treturn mnt;\n}\nEXPORT_SYMBOL_GPL(kern_mount);\n\nvoid kern_unmount(struct vfsmount *mnt)\n{\n\t/* release long term mount so mount point can be released */\n\tif (!IS_ERR_OR_NULL(mnt)) {\n\t\treal_mount(mnt)->mnt_ns = NULL;\n\t\tsynchronize_rcu();\t/* yecchhh... */\n\t\tmntput(mnt);\n\t}\n}\nEXPORT_SYMBOL(kern_unmount);\n\nvoid kern_unmount_array(struct vfsmount *mnt[], unsigned int num)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < num; i++)\n\t\tif (mnt[i])\n\t\t\treal_mount(mnt[i])->mnt_ns = NULL;\n\tsynchronize_rcu_expedited();\n\tfor (i = 0; i < num; i++)\n\t\tmntput(mnt[i]);\n}\nEXPORT_SYMBOL(kern_unmount_array);\n\nbool our_mnt(struct vfsmount *mnt)\n{\n\treturn check_mnt(real_mount(mnt));\n}\n\nbool current_chrooted(void)\n{\n\t/* Does the current process have a non-standard root */\n\tstruct path ns_root;\n\tstruct path fs_root;\n\tbool chrooted;\n\n\t/* Find the namespace root */\n\tns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;\n\tns_root.dentry = ns_root.mnt->mnt_root;\n\tpath_get(&ns_root);\n\twhile (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))\n\t\t;\n\n\tget_fs_root(current->fs, &fs_root);\n\n\tchrooted = !path_equal(&fs_root, &ns_root);\n\n\tpath_put(&fs_root);\n\tpath_put(&ns_root);\n\n\treturn chrooted;\n}\n\nstatic bool mnt_already_visible(struct mnt_namespace *ns,\n\t\t\t\tconst struct super_block *sb,\n\t\t\t\tint *new_mnt_flags)\n{\n\tint new_flags = *new_mnt_flags;\n\tstruct mount *mnt;\n\tbool visible = false;\n\n\tdown_read(&namespace_sem);\n\tlock_ns_list(ns);\n\tlist_for_each_entry(mnt, &ns->list, mnt_list) {\n\t\tstruct mount *child;\n\t\tint mnt_flags;\n\n\t\tif (mnt_is_cursor(mnt))\n\t\t\tcontinue;\n\n\t\tif (mnt->mnt.mnt_sb->s_type != sb->s_type)\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if it's root directory\n\t\t * is not the root directory of the filesystem.\n\t\t */\n\t\tif (mnt->mnt.mnt_root != mnt->mnt.mnt_sb->s_root)\n\t\t\tcontinue;\n\n\t\t/* A local view of the mount flags */\n\t\tmnt_flags = mnt->mnt.mnt_flags;\n\n\t\t/* Don't miss readonly hidden in the superblock flags */\n\t\tif (sb_rdonly(mnt->mnt.mnt_sb))\n\t\t\tmnt_flags |= MNT_LOCK_READONLY;\n\n\t\t/* Verify the mount flags are equal to or more permissive\n\t\t * than the proposed new mount.\n\t\t */\n\t\tif ((mnt_flags & MNT_LOCK_READONLY) &&\n\t\t    !(new_flags & MNT_READONLY))\n\t\t\tcontinue;\n\t\tif ((mnt_flags & MNT_LOCK_ATIME) &&\n\t\t    ((mnt_flags & MNT_ATIME_MASK) != (new_flags & MNT_ATIME_MASK)))\n\t\t\tcontinue;\n\n\t\t/* This mount is not fully visible if there are any\n\t\t * locked child mounts that cover anything except for\n\t\t * empty directories.\n\t\t */\n\t\tlist_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {\n\t\t\tstruct inode *inode = child->mnt_mountpoint->d_inode;\n\t\t\t/* Only worry about locked mounts */\n\t\t\tif (!(child->mnt.mnt_flags & MNT_LOCKED))\n\t\t\t\tcontinue;\n\t\t\t/* Is the directory permanetly empty? */\n\t\t\tif (!is_empty_dir_inode(inode))\n\t\t\t\tgoto next;\n\t\t}\n\t\t/* Preserve the locked attributes */\n\t\t*new_mnt_flags |= mnt_flags & (MNT_LOCK_READONLY | \\\n\t\t\t\t\t       MNT_LOCK_ATIME);\n\t\tvisible = true;\n\t\tgoto found;\n\tnext:\t;\n\t}\nfound:\n\tunlock_ns_list(ns);\n\tup_read(&namespace_sem);\n\treturn visible;\n}\n\nstatic bool mount_too_revealing(const struct super_block *sb, int *new_mnt_flags)\n{\n\tconst unsigned long required_iflags = SB_I_NOEXEC | SB_I_NODEV;\n\tstruct mnt_namespace *ns = current->nsproxy->mnt_ns;\n\tunsigned long s_iflags;\n\n\tif (ns->user_ns == &init_user_ns)\n\t\treturn false;\n\n\t/* Can this filesystem be too revealing? */\n\ts_iflags = sb->s_iflags;\n\tif (!(s_iflags & SB_I_USERNS_VISIBLE))\n\t\treturn false;\n\n\tif ((s_iflags & required_iflags) != required_iflags) {\n\t\tWARN_ONCE(1, \"Expected s_iflags to contain 0x%lx\\n\",\n\t\t\t  required_iflags);\n\t\treturn true;\n\t}\n\n\treturn !mnt_already_visible(ns, sb, new_mnt_flags);\n}\n\nbool mnt_may_suid(struct vfsmount *mnt)\n{\n\t/*\n\t * Foreign mounts (accessed via fchdir or through /proc\n\t * symlinks) are always treated as if they are nosuid.  This\n\t * prevents namespaces from trusting potentially unsafe\n\t * suid/sgid bits, file caps, or security labels that originate\n\t * in other namespaces.\n\t */\n\treturn !(mnt->mnt_flags & MNT_NOSUID) && check_mnt(real_mount(mnt)) &&\n\t       current_in_userns(mnt->mnt_sb->s_user_ns);\n}\n\nstatic struct ns_common *mntns_get(struct task_struct *task)\n{\n\tstruct ns_common *ns = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy) {\n\t\tns = &nsproxy->mnt_ns->ns;\n\t\tget_mnt_ns(to_mnt_ns(ns));\n\t}\n\ttask_unlock(task);\n\n\treturn ns;\n}\n\nstatic void mntns_put(struct ns_common *ns)\n{\n\tput_mnt_ns(to_mnt_ns(ns));\n}\n\nstatic int mntns_install(struct nsset *nsset, struct ns_common *ns)\n{\n\tstruct nsproxy *nsproxy = nsset->nsproxy;\n\tstruct fs_struct *fs = nsset->fs;\n\tstruct mnt_namespace *mnt_ns = to_mnt_ns(ns), *old_mnt_ns;\n\tstruct user_namespace *user_ns = nsset->cred->user_ns;\n\tstruct path root;\n\tint err;\n\n\tif (!ns_capable(mnt_ns->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(user_ns, CAP_SYS_CHROOT) ||\n\t    !ns_capable(user_ns, CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (is_anon_ns(mnt_ns))\n\t\treturn -EINVAL;\n\n\tif (fs->users != 1)\n\t\treturn -EINVAL;\n\n\tget_mnt_ns(mnt_ns);\n\told_mnt_ns = nsproxy->mnt_ns;\n\tnsproxy->mnt_ns = mnt_ns;\n\n\t/* Find the root */\n\terr = vfs_path_lookup(mnt_ns->root->mnt.mnt_root, &mnt_ns->root->mnt,\n\t\t\t\t\"/\", LOOKUP_DOWN, &root);\n\tif (err) {\n\t\t/* revert to old namespace */\n\t\tnsproxy->mnt_ns = old_mnt_ns;\n\t\tput_mnt_ns(mnt_ns);\n\t\treturn err;\n\t}\n\n\tput_mnt_ns(old_mnt_ns);\n\n\t/* Update the pwd and root */\n\tset_fs_pwd(fs, &root);\n\tset_fs_root(fs, &root);\n\n\tpath_put(&root);\n\treturn 0;\n}\n\nstatic struct user_namespace *mntns_owner(struct ns_common *ns)\n{\n\treturn to_mnt_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations mntns_operations = {\n\t.name\t\t= \"mnt\",\n\t.type\t\t= CLONE_NEWNS,\n\t.get\t\t= mntns_get,\n\t.put\t\t= mntns_put,\n\t.install\t= mntns_install,\n\t.owner\t\t= mntns_owner,\n};\n"], "filenames": ["fs/namespace.c"], "buggy_code_start_loc": [1940], "buggy_code_end_loc": [2331], "fixing_code_start_loc": [1941], "fixing_code_end_loc": [2344], "type": "NVD-CWE-noinfo", "message": "A flaw was found in the Linux kernel's OverlayFS subsystem in the way the user mounts the TmpFS filesystem with OverlayFS. This flaw allows a local user to gain access to hidden files that should not be accessible.", "other": {"cve": {"id": "CVE-2021-3732", "sourceIdentifier": "secalert@redhat.com", "published": "2022-03-10T17:42:59.287", "lastModified": "2022-12-13T19:50:35.853", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A flaw was found in the Linux kernel's OverlayFS subsystem in the way the user mounts the TmpFS filesystem with OverlayFS. This flaw allows a local user to gain access to hidden files that should not be accessible."}, {"lang": "es", "value": "Se ha encontrado un fallo en el subsistema OverlayFS del kernel de Linux en la forma en que el usuario monta el sistema de archivos TmpFS con OverlayFS. Este fallo permite a un usuario local acceder a archivos ocultos que no deber\u00edan ser accesibles"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.14", "matchCriteriaId": "202CAD0B-4BA5-4F2B-8C10-8290E5F5434C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:-:*:*:*:*:*:*", "matchCriteriaId": "6A05198E-F8FA-4517-8D0E-8C95066AED38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc1:*:*:*:*:*:*", "matchCriteriaId": "71268287-21A8-4488-AA4F-23C473153131"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc2:*:*:*:*:*:*", "matchCriteriaId": "23B9E5C6-FAB5-4A02-9E39-27C8787B0991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc3:*:*:*:*:*:*", "matchCriteriaId": "D185CF67-7E4A-4154-93DB-CE379C67DB56"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc4:*:*:*:*:*:*", "matchCriteriaId": "D1DA0AF6-02F4-47C7-A318-8C006ED0C665"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.14:rc5:*:*:*:*:*:*", "matchCriteriaId": "49DD30B1-8C99-4C38-A66B-CAB3827BEE8A"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1995249", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=427215d85e8d1476da1a86b8d67aceb485eb3631", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/427215d85e8d1476da1a86b8d67aceb485eb3631", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://ubuntu.com/security/CVE-2021-3732", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/427215d85e8d1476da1a86b8d67aceb485eb3631"}}