{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include \"tensorflow/core/platform/errors.h\"\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/bincount_op.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nnamespace tensorflow {\n\nusing thread::ThreadPool;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, true> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    Tensor partial_bins_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({num_threads, num_bins}), &partial_bins_t));\n    auto partial_bins = partial_bins_t.matrix<bool>();\n    partial_bins.setZero();\n    thread_pool->ParallelForWithWorkerId(\n        arr.size(), 8 /* cost */,\n        [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n          for (int64_t i = start_ind; i < limit_ind; i++) {\n            Tidx value = arr(i);\n            if (value < num_bins) {\n              partial_bins(worker_id, value) = true;\n            }\n          }\n        });\n\n    // Sum the partial bins along the 0th axis.\n    Eigen::array<int, 1> reduce_dim({0});\n    output.device(context->eigen_cpu_device()) =\n        partial_bins.any(reduce_dim).cast<T>();\n    return Status::OK();\n  }\n};\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, false> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    const Tidx* arr_data = arr.data();\n    const std::ptrdiff_t arr_size = arr.size();\n    const T* weight_data = weights.data();\n    if (weights.size() && weights.size() != arr_size) {\n      return errors::InvalidArgument(\n          \"Input indices and weights must have the same size.\");\n    }\n    if (num_threads == 1) {\n      output.setZero();\n      T* output_data = output.data();\n      if (weights.size()) {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            output_data[value] += weight_data[i];\n          }\n        }\n      } else {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            // Complex numbers don't support \"++\".\n            output_data[value] += T(1);\n          }\n        }\n      }\n    } else {\n      Tensor partial_bins_t;\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DataTypeToEnum<T>::value, TensorShape({num_threads, num_bins}),\n          &partial_bins_t));\n      auto partial_bins = partial_bins_t.matrix<T>();\n      partial_bins.setZero();\n      thread_pool->ParallelForWithWorkerId(\n          arr_size, 8 /* cost */,\n          [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n            if (weights.size()) {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  partial_bins(worker_id, value) += weight_data[i];\n                }\n              }\n            } else {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  // Complex numbers don't support \"++\".\n                  partial_bins(worker_id, value) += T(1);\n                }\n              }\n            }\n          });\n\n      // Sum the partial bins along the 0th axis.\n      Eigen::array<int, 1> reduce_dim({0});\n      output.device(context->eigen_cpu_device()) = partial_bins.sum(reduce_dim);\n    }\n    return Status::OK();\n  }\n};\n\ntemplate <typename Tidx, typename T, bool binary_output>\nstruct BincountReduceFunctor<CPUDevice, Tidx, T, binary_output> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 2>::ConstTensor& in,\n                        const typename TTypes<T, 2>::ConstTensor& weights,\n                        typename TTypes<T, 2>::Tensor& out,\n                        const Tidx num_bins) {\n    const int num_rows = out.dimension(0);\n    const int num_cols = in.dimension(1);\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    thread_pool->ParallelForWithWorkerId(\n        num_rows, 8 /* cost */,\n        [&](int64_t start_row, int64_t end_row, int64_t worker_id) {\n          for (int64_t i = start_row; i < end_row; ++i) {\n            for (int64_t j = 0; j < num_cols; ++j) {\n              Tidx value = in(i, j);\n              if (value < num_bins) {\n                if (binary_output) {\n                  out(i, value) = T(1);\n                } else {\n                  if (weights.size()) {\n                    out(i, value) += weights(i, j);\n                  } else {\n                    out(i, value) += T(1);\n                  }\n                }\n              }\n            }\n          }\n        });\n    return Status::OK();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass BincountOp : public OpKernel {\n public:\n  explicit BincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& arr_t = ctx->input(0);\n    const Tensor& size_tensor = ctx->input(1);\n    OP_REQUIRES(ctx, size_tensor.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_tensor.dims()));\n    int32_t size = size_tensor.scalar<int32_t>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    const Tensor& weights_t = ctx->input(2);\n    const auto arr = arr_t.flat<int32_t>();\n    const auto weights = weights_t.flat<T>();\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({size}), &output_t));\n    auto output = output_t->flat<T>();\n    OP_REQUIRES_OK(ctx,\n                   functor::BincountFunctor<Device, int32_t, T, false>::Compute(\n                       ctx, arr, weights, output, size));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"Bincount\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      BincountOp<CPUDevice, type>)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"Bincount\")                \\\n                              .Device(DEVICE_GPU)         \\\n                              .HostMemory(\"size\")         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          BincountOp<GPUDevice, type>)\n\nTF_CALL_int32(REGISTER_KERNELS);\nTF_CALL_float(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass DenseBincountOp : public OpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(\n          ctx, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"DenseBincount.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .HostMemory(\"size\")            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<GPUDevice, Tidx, T>);\n#define REGISTER_GPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_int32(REGISTER_GPU_KERNELS);\nTF_CALL_float(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass SparseBincountOp : public OpKernel {\n public:\n  explicit SparseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          SparseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\ntemplate <typename Device, typename Tidx, typename T>\nclass RaggedBincountOp : public OpKernel {\n public:\n  explicit RaggedBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          RaggedBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n}  // end namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"AddN\")\n    .Input(\"inputs: N * T\")\n    .Output(\"sum: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: {numbertype, variant}\")\n    .SetIsCommutative()\n    .SetIsAggregate()\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      c->set_output(0, cur);\n\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"T\", &dtype));\n\n      if (dtype != DT_VARIANT) {\n        // Exit early if not DT_VARIANT.\n        return Status::OK();\n      } else {\n        // DT_VARIANT shape handle shape inference.  All sizes and dtypes must\n        // be the same; all shapes must be compatible via Merge.\n        std::vector<shape_inference::ShapeAndType> cur_shapes_and_types;\n        auto* shapes_and_types =\n            c->input_handle_shapes_and_types(c->num_inputs() - 1);\n        if (shapes_and_types) {\n          cur_shapes_and_types = *shapes_and_types;\n        }\n\n        for (int i = c->num_inputs() - 2; i >= 0; --i) {\n          auto shapes_and_types_i = c->input_handle_shapes_and_types(i);\n          if (!shapes_and_types && shapes_and_types_i) {\n            // TODO(ebrevdo): Find cases where this happens and fix their shape\n            // inference.  If we are calling AddN on variant types, they should\n            // all have consistent shape_and_type info.\n            shapes_and_types = shapes_and_types_i;\n          } else if (shapes_and_types && shapes_and_types_i) {\n            if (shapes_and_types_i->size() != shapes_and_types->size()) {\n              return errors::InvalidArgument(\n                  \"shapes_and_types[\", i,\n                  \"].size() == \", shapes_and_types_i->size(),\n                  \" != shapes_and_types[0].size() == \",\n                  shapes_and_types->size());\n            }\n            for (int j = 0; j < shapes_and_types->size(); ++j) {\n              if (shapes_and_types->at(j).dtype !=\n                  shapes_and_types_i->at(j).dtype) {\n                return errors::InvalidArgument(\n                    \"shapes_and_types[\", i, \"][\", j, \"].dtype() == \",\n                    DataTypeString(shapes_and_types_i->at(j).dtype),\n                    \" != shapes_and_types[0][\", j, \"].dtype == \",\n                    DataTypeString(shapes_and_types->at(j).dtype));\n              }\n              TF_RETURN_WITH_CONTEXT_IF_ERROR(\n                  c->Merge(shapes_and_types_i->at(j).shape,\n                           cur_shapes_and_types.at(j).shape,\n                           &cur_shapes_and_types.at(j).shape),\n                  \"From merging shapes_and_types[\", i, \"][\", j, \"].shape with \",\n                  \"shapes_and_types[0][\", j, \"].shape\");\n            }\n          }\n        }\n        if (shapes_and_types) {\n          c->set_output_handle_shapes_and_types(0, cur_shapes_and_types);\n        }\n        return Status::OK();\n      }\n    });\n\n// --------------------------------------------------------------------------\n\n// Note that the following operator is just a placeholder and has no\n// associated kernel. The code in accumulate_n_optimizer.cc replaces\n// this placeholder with a graph of operators that do have kernels.\n// The Python code that generates instances of this op is currently in\n// contrib/framework/python/ops/accumulate_n_v2.py\nREGISTER_OP(\"AccumulateNV2\")\n    .Input(\"inputs: N * T\")\n    .Output(\"sum: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"shape: shape\")\n    .SetIsCommutative()\n    .SetIsAggregate()\n    .SetShapeFn(shape_inference::ExplicitShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchMatMul\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulShape);\n\nREGISTER_OP(\"BatchMatMulV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int16, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n\nREGISTER_OP(\"BatchMatMulV3\")\n    .Input(\"x: Ta\")\n    .Input(\"y: Tb\")\n    .Output(\"output: Tout\")\n    .Attr(\n        \"Ta: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .Attr(\n        \"Tb: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .Attr(\n        \"Tout: {bfloat16, half, float, double, int16, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklBatchMatMul\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulShape);\n\nREGISTER_OP(\"_MklBatchMatMulV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\n// Casting Ops\n//\n// NOTE: Only a smaller number of types are supported by\n// Cast. The exact casting rule is TBD. The current\n// implementation uses C++ static cast rules for numeric\n// types, which may be changed in the future.\nREGISTER_OP(\"Cast\")\n    .Input(\"x: SrcT\")\n    .Output(\"y: DstT\")\n    .Attr(\"SrcT: type\")\n    .Attr(\"DstT: type\")\n    .Attr(\"Truncate: bool = false\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"_HostCast\")\n    .Input(\"x: SrcT\")\n    .Output(\"y: DstT\")\n    .Attr(\"SrcT: type\")\n    .Attr(\"DstT: type\")\n    .Attr(\"Truncate: bool = false\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nCast x of type SrcT to y of DstT.\n\n_HostCast requires its input and produces its output in host memory.\n)doc\");\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Abs\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double, int8, int16, int32, int64}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ComplexAbs\")\n    .Input(\"x: T\")\n    .Output(\"y: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// Declares cwise unary operations signature: 't -> 't\n#define UNARY()                                                            \\\n  Input(\"x: T\")                                                            \\\n      .Output(\"y: T\")                                                      \\\n      .Attr(                                                               \\\n          \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \" \\\n          \"complex64, complex128}\")                                        \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_UNSIGNED()                                                   \\\n  Input(\"x: T\")                                                            \\\n      .Output(\"y: T\")                                                      \\\n      .Attr(                                                               \\\n          \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \" \\\n          \"uint8, uint16, uint32, uint64, complex64, complex128}\")         \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_REAL()                              \\\n  Input(\"x: T\")                                   \\\n      .Output(\"y: T\")                             \\\n      .Attr(\"T: {bfloat16, half, float, double}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_COMPLEX()                                                  \\\n  Input(\"x: T\")                                                          \\\n      .Output(\"y: T\")                                                    \\\n      .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_GRADIENT_COMPLEX()                                         \\\n  Input(\"y: T\")                                                          \\\n      .Input(\"dy: T\")                                                    \\\n      .Output(\"z: T\")                                                    \\\n      .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\nREGISTER_OP(\"Neg\").UNARY();\n\nREGISTER_OP(\"Inv\").UNARY();\n\nREGISTER_OP(\"InvGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Reciprocal\").UNARY();\n\nREGISTER_OP(\"ReciprocalGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Square\").UNARY_UNSIGNED();\n\nREGISTER_OP(\"Sqrt\").UNARY_COMPLEX();\n\nREGISTER_OP(\"SqrtGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Rsqrt\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Round\").UNARY();\n\nREGISTER_OP(\"RsqrtGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Exp\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Expm1\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Log\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Log1p\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Sinh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Cosh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Tanh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Asinh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Acosh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Atanh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"TanhGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Lgamma\").UNARY_REAL();\n\nREGISTER_OP(\"Digamma\").UNARY_REAL();\n\nREGISTER_OP(\"Erf\").UNARY_REAL();\nREGISTER_OP(\"Erfinv\").UNARY_REAL();\nREGISTER_OP(\"Ndtri\").UNARY_REAL();\nREGISTER_OP(\"Erfc\").UNARY_REAL();\n\nREGISTER_OP(\"Sigmoid\").UNARY_COMPLEX();\n\nREGISTER_OP(\"SigmoidGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Sin\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Cos\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Tan\").UNARY();\n\nREGISTER_OP(\"Asin\").UNARY();\n\nREGISTER_OP(\"Acos\").UNARY();\n\nREGISTER_OP(\"Atan\").UNARY();\n\nREGISTER_OP(\"_UnaryOpsComposition\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {float, half, double}\")\n    .Attr(\"op_names: list(string)\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to create these operators.\n)doc\");\n\n#undef UNARY\n#undef UNARY_REAL\n#undef UNARY_COMPLEX\n\nREGISTER_OP(\"IsNan\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"IsInf\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"IsFinite\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Sign\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Floor\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Ceil\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Rint\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// Declares cwise binary operations signature: 't, 't -> 't.\n\n#define BINARY_MORE()                                                          \\\n  Input(\"x: T\").Input(\"y: T\").Output(\"z: T\").Attr(                             \\\n      \"T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, \" \\\n      \"uint32, uint64, int64, complex64, complex128}\")\n\n#define BINARY_FEWER()                                               \\\n  Input(\"x: T\").Input(\"y: T\").Output(\"z: T\").Attr(                   \\\n      \"T: {bfloat16, half, float, double, int32, int64, complex64, \" \\\n      \"complex128}\")\n\nREGISTER_OP(\"Add\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128, string}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"AddV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, uint16, uint32, uint64, \"\n        \"int8, int16, int32, int64, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .SetIsAggregate()\n    .SetIsCommutative();\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklAdd\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\n        \"T: {half, float, double, uint8, int8, int16, int32, int64, complex64, \"\n        \"complex128, string, bfloat16}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns `x` + `y` element-wise.\n\n*NOTE*: `tf.math.add` supports broadcasting. `tf.math.add_n` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n)doc\");\n\nREGISTER_OP(\"_MklAddV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .SetIsAggregate()\n    .SetIsCommutative()\n    .Doc(R\"doc(\nReturns `x` + `y` element-wise.\n*NOTE*: `tf.math.add` supports broadcasting. `tf.math.add_n` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n)doc\");\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"Sub\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128, uint32, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"_MklSub\")\n    .BINARY_FEWER()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns x - y element-wise.\n\n*NOTE*: `Sub` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Mul\").BINARY_MORE().SetIsCommutative().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"MulNoNan\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklMul\")\n    .BINARY_MORE()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns x * y element-wise.\n\n*NOTE*: `Mul` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Div\").BINARY_MORE().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"DivNoNan\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, bfloat16, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"FloorDiv\")\n    .BINARY_MORE()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"TruncateDiv\")\n    .BINARY_MORE()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"RealDiv\").BINARY_MORE().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note SquaredDifference implements conj(x - y)*(x - y).\nREGISTER_OP(\"SquaredDifference\")\n    .BINARY_FEWER()\n    .SetIsCommutative()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklSquaredDifference\")\n    .BINARY_FEWER()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns (x - y)(x - y) element-wise.\n\n*NOTE*: `SquaredDifference` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Xlogy\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Xlog1py\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Xdivy\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n#undef BINARY_FEWER\n#undef BINARY_MORE\n\nREGISTER_OP(\"Maximum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, \"\n        \"int32, uint32, int64, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklMaximum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\"T: {half, float, double, int32, int64, bfloat16}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns the max of x and y (i.e. x > y ? x : y) element-wise.\n\n*NOTE*: `Maximum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Minimum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, \"\n        \"int32, uint32, int64, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Mod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {int32, int64, float16, half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"FloorMod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {int8, int16, int32, int64, uint8, uint16, uint32, uint64, \"\n        \"bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"TruncateMod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {int32, int64, bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Pow\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, float, half, double, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Igammac\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Igamma\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"IgammaGradA\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Zeta\")\n    .Input(\"x: T\")\n    .Input(\"q: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Polygamma\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Atan2\")\n    .Input(\"y: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Betainc\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      const int num_inputs = 3;\n      ShapeHandle output = c->UnknownShape();\n      int num_scalars = 0;\n      ShapeHandle some_non_scalar;\n      for (int i = 0; i < num_inputs; ++i) {\n        ShapeHandle in = c->input(i);\n        if (!c->RankKnown(in)) {\n          some_non_scalar = in;\n          // An input with unknown rank could be either a scalar (to be\n          // broadcast) or some other shape.\n        } else if (c->Rank(in) == 0) {\n          // Input is a scalar, it will be broadcast to the output shape.\n          ++num_scalars;\n        } else {\n          TF_RETURN_IF_ERROR(c->Merge(output, in, &output));\n          some_non_scalar = output;\n        }\n      }\n\n      if (num_scalars == num_inputs - 1) {\n        // If all but one input is known to be a scalar, then output is the\n        // remaining input.\n        output = some_non_scalar;\n      } else if (num_scalars == num_inputs) {\n        // If all are scalars, output is scalar; pick the first one arbitrarily.\n        output = c->input(0);\n      }\n\n      c->set_output(0, output);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\n// Declares cwise binary comparison operations signature: 't, 't -> bool,\n// where 't has a natural total order.\n#define COMPARISON()             \\\n  Input(\"x: T\")                  \\\n      .Input(\"y: T\")             \\\n      .Output(\"z: bool\")         \\\n      .Attr(\"T: realnumbertype\") \\\n      .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n\nREGISTER_OP(\"Less\").COMPARISON();\n\nREGISTER_OP(\"LessEqual\").COMPARISON();\n\nREGISTER_OP(\"Greater\").COMPARISON();\n\nREGISTER_OP(\"GreaterEqual\").COMPARISON();\n\n#undef COMPARISON\n\n// --------------------------------------------------------------------------\n\n#define EQUALITY_COMPARISON()                                      \\\n  Input(\"x: T\")                                                    \\\n      .Input(\"y: T\")                                               \\\n      .Output(\"z: bool\")                                           \\\n      .SetIsCommutative()                                          \\\n      .Attr(\"T: type\")                                             \\\n      .Attr(\"incompatible_shape_error: bool = true\")               \\\n      .SetShapeFn([](InferenceContext* c) {                        \\\n        ShapeHandle x = c->input(0);                               \\\n        ShapeHandle y = c->input(1);                               \\\n        ShapeHandle output;                                        \\\n        bool incompatible_shape_error;                             \\\n        TF_RETURN_IF_ERROR(c->GetAttr(\"incompatible_shape_error\",  \\\n                                      &incompatible_shape_error)); \\\n        TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFnHelper(   \\\n            c, x, y, incompatible_shape_error, &output));          \\\n        c->set_output(0, output);                                  \\\n        return Status::OK();                                       \\\n      })\n\nREGISTER_OP(\"Equal\").EQUALITY_COMPARISON();\n\nREGISTER_OP(\"NotEqual\").EQUALITY_COMPARISON();\n\n#undef EQUALITY_COMPARISON\n\nREGISTER_OP(\"ApproximateEqual\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: bool\")\n    .SetIsCommutative()\n    .Attr(\"T: numbertype\")\n    .Attr(\"tolerance: float = 0.00001\")\n    .SetShapeFn([](InferenceContext* c) {\n      // The inputs 'x' and 'y' must have the same shape.\n      ShapeHandle data_x = c->input(0);\n      ShapeHandle data_y = c->input(1);\n      TF_RETURN_IF_ERROR(c->Merge(data_x, data_y, &data_x));\n      return shape_inference::UnchangedShape(c);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogicalNot\")\n    .Input(\"x: bool\")\n    .Output(\"y: bool\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#define BINARY_LOGICAL()  \\\n  Input(\"x: bool\")        \\\n      .Input(\"y: bool\")   \\\n      .Output(\"z: bool\")  \\\n      .SetIsCommutative() \\\n      .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n\nREGISTER_OP(\"LogicalAnd\").BINARY_LOGICAL();\n\nREGISTER_OP(\"LogicalOr\").BINARY_LOGICAL();\n\n#undef BINARY_LOGICAL\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Select\")\n    .Input(\"condition: bool\")\n    .Input(\"t: T\")\n    .Input(\"e: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto* handle_data_1 = c->input_handle_shapes_and_types(1);\n      auto* handle_data_2 = c->input_handle_shapes_and_types(2);\n      // Merge handle shape and dtype if applicable.\n      if (handle_data_1 != nullptr && handle_data_2 != nullptr) {\n        const auto size = handle_data_1->size();\n        std::vector<shape_inference::ShapeAndType> merged_handle_data(size);\n        if (size != handle_data_2->size()) {\n          return errors::InvalidArgument(\n              \"Trying to merge handles pointing to different numbers of \"\n              \"tensors.\");\n        }\n\n        for (int i = 0; i < size; ++i) {\n          const shape_inference::ShapeAndType& s1 = (*handle_data_1)[i];\n          const shape_inference::ShapeAndType& s2 = (*handle_data_2)[i];\n          if (s1.dtype != s2.dtype) {\n            // TODO(apassos) resolve this in the manner of b/32476923\n            return errors::InvalidArgument(\n                \"Trying to merge handles pointing to different dtypes.\");\n          }\n          merged_handle_data[i].dtype = s1.dtype;\n          TF_RETURN_IF_ERROR(\n              c->Merge(s1.shape, s2.shape, &merged_handle_data[i].shape));\n        }\n\n        c->set_output_handle_shapes_and_types(0, merged_handle_data);\n      }\n\n      // The inputs 'then' and 'else' must have the same shape.\n      ShapeHandle data = c->input(1);\n      ShapeHandle other = c->input(2);\n      TF_RETURN_IF_ERROR(c->Merge(data, other, &data));\n\n      // The input 'cond' must either have the same shape as 'then' and\n      // 'else', or be a vector if 'then' and 'else' are at least vectors.\n      ShapeHandle cond = c->input(0);\n\n      if (!c->RankKnown(cond) || !c->RankKnown(data)) {\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      // rank of shape and data is known.\n\n      const int32_t cond_rank = c->Rank(cond);\n      const int32_t data_rank = c->Rank(data);\n\n      if (cond_rank == 0) {\n        // The rank of 'cond' is a scalar.\n        // t and e can have any shape.\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (cond_rank != 1) {\n        // If 'cond' is not a vector, and not a scalar,\n        // then shape must match 'then' and 'else'\n        TF_RETURN_IF_ERROR(c->Merge(data, cond, &data));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (data_rank == 0) {\n        // if 'then' and 'else' are scalar also the cond must be\n        TF_RETURN_IF_ERROR(c->Merge(data, cond, &data));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (cond_rank == 1) {\n        // if the cond is a vector and the 'then' is not a scalar,\n        // the first dimension of 'then' and 'else'\n        TF_RETURN_IF_ERROR(c->Merge(cond, c->Vector(c->Dim(data, 0)), &cond));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      c->set_output(0, data);\n\n      return Status::OK();\n    });\n\nREGISTER_OP(\"SelectV2\")\n    .Input(\"condition: bool\")\n    .Input(\"t: T\")\n    .Input(\"e: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto* handle_data_1 = c->input_handle_shapes_and_types(1);\n      auto* handle_data_2 = c->input_handle_shapes_and_types(2);\n      // Merge handle shape and dtype if applicable.\n      if (handle_data_1 != nullptr && handle_data_2 != nullptr) {\n        const auto size = handle_data_1->size();\n        std::vector<shape_inference::ShapeAndType> merged_handle_data(size);\n        if (size != handle_data_2->size()) {\n          return errors::InvalidArgument(\n              \"Trying to merge handles pointing to different numbers of \"\n              \"tensors.\");\n        }\n\n        for (int i = 0; i < size; ++i) {\n          const shape_inference::ShapeAndType& s1 = (*handle_data_1)[i];\n          const shape_inference::ShapeAndType& s2 = (*handle_data_2)[i];\n          if (s1.dtype != s2.dtype) {\n            // TODO(apassos) resolve this in the manner of b/32476923\n            return errors::InvalidArgument(\n                \"Trying to merge handles pointing to different dtypes.\");\n          }\n          merged_handle_data[i].dtype = s1.dtype;\n          TF_RETURN_IF_ERROR(\n              c->Merge(s1.shape, s2.shape, &merged_handle_data[i].shape));\n        }\n\n        c->set_output_handle_shapes_and_types(0, merged_handle_data);\n      }\n\n      // The inputs 'cond', 'then', and 'else' must be broadcastable.\n      // TODO (yongtang): Consolidate 3-ary broadcast instead of\n      // multiple 2-ary broadcast.\n      ShapeHandle cond = c->input(0);\n      ShapeHandle then = c->input(1);\n      ShapeHandle else_ = c->input(2);\n      ShapeHandle other;\n      TF_RETURN_IF_ERROR(\n          BroadcastBinaryOpOutputShapeFnHelper(c, then, else_, true, &other));\n      ShapeHandle output;\n      TF_RETURN_IF_ERROR(\n          BroadcastBinaryOpOutputShapeFnHelper(c, cond, other, true, &output));\n      c->set_output(0, output);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn(shape_inference::MatMulShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklMatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"T: {bfloat16, float}\")\n    .SetShapeFn(shape_inference::MatMulShape);\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"SparseMatMul\")\n    .Input(\"a: Ta\")\n    .Input(\"b: Tb\")\n    .Output(\"product: float\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"a_is_sparse: bool = false\")\n    .Attr(\"b_is_sparse: bool = false\")\n    .Attr(\"Ta: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"Tb: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MatMulShape);\n\nREGISTER_OP(\"_FusedMatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ----------- //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ---------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // --------------------------------------------- //\n    .SetShapeFn(shape_inference::MatMulShape)\n    .Doc(R\"doc(\nPerforms a MatMul followed by a specified series of operations.\n\nThe inputs to the MatMul are specified by `a` and `b`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [\"BiasAdd\"] and [\"BiasAdd\",A],\nwhere A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to BiasAdd is the Conv2D result, and the additional BiasAdd\ninput is specified by `args`.\n* If there is an op A specified, the output of the BiasAdd is the input to op A,\nand op A produces the _FusedConv2D output. Otherwise, the BiasAdd produces the\n_FusedConv2D output.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is\nexpected to create these operators.\n)doc\");\n\n// --------------------------------------------------------------------------\n\n// For operations where the output is a reduction function along some\n// dimensions of the input.\nREGISTER_OP(\"Sum\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"EuclideanNorm\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Mean\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Prod\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Min\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: {realnumbertype, quantizedtype}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Max\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: {realnumbertype, quantizedtype}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nnamespace {\n\nStatus ArgOpShape(shape_inference::InferenceContext* c) {\n  ShapeHandle dimension_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &dimension_shape));\n\n  ShapeHandle input_shape = c->input(0);\n  if (!c->RankKnown(input_shape)) {\n    return shape_inference::UnknownShape(c);\n  }\n\n  const int32_t input_rank = c->Rank(input_shape);\n  if (input_rank <= 1) {\n    // Reducing a scalar/vector must return a scalar.\n    return shape_inference::ScalarShape(c);\n  }\n\n  const Tensor* dim_t = c->input_tensor(1);\n  if (dim_t == nullptr) {\n    // We don't know the value of the dimension, but we\n    // know the rank of the input, so return the correct\n    // rank with unknown dimensions.\n    std::vector<DimensionHandle> dims(input_rank - 1);\n    for (int i = 0; i < dims.size(); ++i) {\n      dims[i] = c->UnknownDim();\n    }\n\n    c->set_output(0, c->MakeShape(dims));\n    return Status::OK();\n  }\n\n  int64_t dimension_val;\n  if (dim_t->dtype() == DT_INT32) {\n    dimension_val = dim_t->scalar<int32>()();\n  } else {\n    dimension_val = dim_t->scalar<int64_t>()();\n  }\n\n  int64_t axis = dimension_val < 0 ? dimension_val + input_rank : dimension_val;\n  if (axis < 0 || axis >= input_rank) {\n    return errors::InvalidArgument(\n        \"Dimension (\", dimension_val, \") must be in the range [\", -input_rank,\n        \", \", input_rank, \"), where \", input_rank,\n        \" is the number of dimensions in the input.\");\n  }\n\n  // Return the input shape without the dimension being reduced.\n  std::vector<DimensionHandle> dims;\n  for (int i = 0; i < input_rank; ++i) {\n    if (axis != i) {\n      dims.emplace_back(c->Dim(input_shape, i));\n    }\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return Status::OK();\n}\n\n}  // namespace\n\nREGISTER_OP(\"ArgMax\")\n    .Input(\"input: T\")\n    .Input(\"dimension: Tidx\")\n    .Output(\"output: output_type\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"output_type: {int32, int64} = DT_INT64\")\n    .SetShapeFn(ArgOpShape);\n\nREGISTER_OP(\"ArgMin\")\n    .Input(\"input: T\")\n    .Input(\"dimension: Tidx\")\n    .Output(\"output: output_type\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"output_type: {int32, int64} = DT_INT64\")\n    .SetShapeFn(ArgOpShape);\n\nnamespace {\n\nStatus SegmentReductionShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &segment_ids_shape));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(\n      c->Concatenate(c->Vector(InferenceContext::kUnknownDim), subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &segment_ids_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(indices_shape, segment_ids_shape, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(\n      c->Concatenate(c->Vector(InferenceContext::kUnknownDim), subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionGradShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->input(2), indices_shape, &unused));\n\n  // output_dim0 should be a scalar\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  const Tensor* dim0 = c->input_tensor(3);\n  ShapeHandle dim0_shape;\n  if (dim0 == nullptr) {\n    // We don't have the value at inference time, so the output\n    // shape is unknown.\n    dim0_shape = c->Vector(InferenceContext::kUnknownDim);\n  } else {\n    auto dim0_value = dim0->scalar<int32>()();\n    if (dim0_value < 0) {\n      return errors::InvalidArgument(\n          \"Cannot specify a negative value for output_dim0\");\n    }\n    dim0_shape = c->Vector(dim0_value);\n  }\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->Concatenate(dim0_shape, subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionWithNumSegmentsShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &segment_ids_shape));\n\n  ShapeHandle num_segments_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &num_segments_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(indices_shape, segment_ids_shape, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  const Tensor* dim0 = c->input_tensor(3);\n  if (dim0 == nullptr) {\n    // We don't have the value at inference time, so the output\n    // shape is unknown.\n    TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(InferenceContext::kUnknownDim),\n                                      subshape, &out));\n  } else {\n    auto dim0_value = dim0->scalar<int32>()();\n    if (dim0_value < 0) {\n      return errors::InvalidArgument(\n          \"Cannot specify a negative value for num_segments\");\n    }\n    TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(dim0_value), subshape, &out));\n  }\n  c->set_output(0, out);\n  return Status::OK();\n}\n}  // namespace\n\nREGISTER_OP(\"SegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMean\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentProd\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMin\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMax\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentMax\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentMin\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentProd\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSumWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentSumGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"SparseSegmentMean\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentMeanWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentMeanGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtN\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtNWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtNGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"All\")\n    .Input(\"input: bool\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: bool\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Any\")\n    .Input(\"input: bool\")\n    .Input(\"reduction_indices: Tidx\")\n    .Attr(\"keep_dims: bool = false\")\n    .Output(\"output: bool\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\n// --------------------------------------------------------------------------\n\nnamespace {\n\ntemplate <typename T>\nStatus RangeSize(const Tensor* start_t, const Tensor* limit_t,\n                 const Tensor* delta_t, InferenceContext* const c) {\n  T start = start_t->scalar<T>()();\n  T limit = limit_t->scalar<T>()();\n  T delta = delta_t->scalar<T>()();\n  if (start > limit && delta > T(0)) {\n    return errors::InvalidArgument(\n        \"Requires start <= limit when delta > 0: \", start, \"/\", limit);\n  }\n  if (start < limit && delta < T(0)) {\n    return errors::InvalidArgument(\n        \"Requires start >= limit when delta < 0: \", start, \"/\", limit);\n  }\n  if (delta == T(0)) {\n    return errors::InvalidArgument(\"Requires delta != 0\");\n  }\n\n  auto size = (std::is_integral<T>::value\n                   ? ((Eigen::numext::abs(limit - start) +\n                       Eigen::numext::abs(delta) - T(1)) /\n                      Eigen::numext::abs(delta))\n                   : (Eigen::numext::ceil(\n                         Eigen::numext::abs((limit - start) / delta))));\n  c->set_output(0, c->Vector(static_cast<int64_t>(size)));\n  return Status::OK();\n}\n\n}  // namespace\n\nREGISTER_OP(\"Range\")\n    .Input(\"start: Tidx\")\n    .Input(\"limit: Tidx\")\n    .Input(\"delta: Tidx\")\n    .Output(\"output: Tidx\")\n    .Attr(\n        \"Tidx: \"\n        \"{bfloat16, half, float, double, int8, int16, int32, int64, uint32} = \"\n        \"DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(0), 0, &unused),\n                                      \" for 'start'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(1), 0, &unused),\n                                      \" for 'limit'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(2), 0, &unused),\n                                      \" for 'delta'\");\n      const Tensor* start_t = c->input_tensor(0);\n      const Tensor* limit_t = c->input_tensor(1);\n      const Tensor* delta_t = c->input_tensor(2);\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (start_t == nullptr || limit_t == nullptr || delta_t == nullptr) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return Status::OK();\n      }\n      if (dtype == DT_INT32) {\n        return RangeSize<int32>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT16) {\n        return RangeSize<int16>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT8) {\n        return RangeSize<int8>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT64) {\n        return RangeSize<int64_t>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_UINT32) {\n        return RangeSize<uint32>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_FLOAT) {\n        return RangeSize<float>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_DOUBLE) {\n        return RangeSize<double>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_BFLOAT16) {\n        return RangeSize<bfloat16>(start_t, limit_t, delta_t, c);\n      } else {\n        return errors::InvalidArgument(\"Unsupported dtype\", dtype);\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"LinSpace\")\n    .Input(\"start: T\")\n    .Input(\"stop: T\")\n    .Input(\"num: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(0), 0, &unused),\n                                      \" for 'start'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(1), 0, &unused),\n                                      \" for 'stop'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(2), 0, &unused),\n                                      \" for 'num'\");\n      const Tensor* num_t = c->input_tensor(2);\n      if (num_t == nullptr) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return Status::OK();\n      }\n\n      int64_t num;\n      if (num_t->dtype() == DT_INT32) {\n        num = num_t->scalar<int32>()();\n      } else {\n        num = num_t->scalar<int64_t>()();\n      }\n      if (num <= 0) return errors::InvalidArgument(\"Requires num > 0: \", num);\n      c->set_output(0, c->Vector(num));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Complex\")\n    .Input(\"real: T\")\n    .Input(\"imag: T\")\n    .Output(\"out: Tout\")\n    .Attr(\"T: {float, double} = DT_FLOAT\")\n    .Attr(\"Tout: {complex64, complex128} = DT_COMPLEX64\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Real\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Imag\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Angle\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Conj\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {complex64, complex128, variant} = DT_COMPLEX64\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(0));\n      auto* handle_data = c->input_handle_shapes_and_types(0);\n      if (handle_data != nullptr) {\n        c->set_output_handle_shapes_and_types(0, *handle_data);\n      }\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Cross\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle a_shape;\n      ShapeHandle b_shape;\n      // * Input rank >= 1.\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &a_shape));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &b_shape));\n\n      // * Both inputs have the same shape.\n      TF_RETURN_IF_ERROR(c->Merge(a_shape, b_shape, &a_shape));\n\n      // * input_shape[-1] == 3.\n      if (c->RankKnown(a_shape)) {\n        int rank = c->Rank(a_shape);\n        auto dim = c->Dim(a_shape, rank - 1);\n        TF_RETURN_IF_ERROR(c->WithValue(dim, 3, &dim));\n      }\n      c->set_output(0, a_shape);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"HistogramFixedWidth\")\n    .Input(\"values: T\")\n    .Input(\"value_range: T\")\n    .Input(\"nbins: int32\")\n    .Output(\"out: dtype\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"dtype: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      // value_range should be a vector.\n      ShapeHandle value_range_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &value_range_shape));\n      // value_range should have two elements.\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->WithValue(c->Dim(value_range_shape, 0), 2, &unused));\n      // nbins should be a scalar.\n      ShapeHandle nbins_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &nbins_shape));\n\n      // If nbins is available, set the shape from nbins.\n      const Tensor* nbins_input = c->input_tensor(2);\n      if (nbins_input != nullptr) {\n        int64_t nbins;\n        TF_RETURN_IF_ERROR(c->GetScalarFromTensor(nbins_input, &nbins));\n        // nbins has to be positive.\n        if (nbins <= 0) {\n          return errors::InvalidArgument(\"Requires nbins > 0: \", nbins);\n        }\n        c->set_output(0, c->Vector(nbins));\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(1));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Bincount\")\n    .Input(\"arr: int32\")\n    .Input(\"size: int32\")\n    .Input(\"weights: T\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Output(\"bins: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // The input `size` must be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* size_tensor = c->input_tensor(1);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShapeOfRank(1));\n        return Status::OK();\n      }\n\n      // Return `[size]` shape if size is known.\n      int32_t size_val = size_tensor->scalar<int32>()();\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n      c->set_output(0, c->MakeShape({size_val}));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"DenseBincount\")\n    .Input(\"input: Tidx\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // The input `input` must be at most matrix.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 2, &unused));\n      // The input `size` must be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* size_tensor = c->input_tensor(1);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n\n      int64_t size_val;\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (dtype == DT_INT32) {\n        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n      } else if (dtype == DT_INT64) {\n        size_val = size_tensor->scalar<int64_t>()();\n      } else {\n        return errors::InvalidArgument(\"size dtype must be int32 or int64\");\n      }\n      // Return `[size]` shape if size is known.\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n      if (c->Rank(c->input(0)) == 1) {\n        c->set_output(0, c->MakeShape({size_val}));\n      } else if (c->Rank(c->input(0)) == 2) {\n        c->set_output(0, c->MakeShape({c->Dim(c->input(0), 0), size_val}));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"SparseBincount\")\n    .Input(\"indices: int64\")\n    .Input(\"values: Tidx\")\n    .Input(\"dense_shape: int64\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      const Tensor* size_tensor = c->input_tensor(3);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n\n      int64_t size_val;\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (dtype == DT_INT32) {\n        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n      } else if (dtype == DT_INT64) {\n        size_val = size_tensor->scalar<int64_t>()();\n      } else {\n        return errors::InvalidArgument(\"size dtype must be int32 or int64\");\n      }\n      // Return `[size]` shape if size is known.\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n\n      const Tensor* shape_tensor = c->input_tensor(2);\n      if (shape_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n      if (shape_tensor->NumElements() == 1) {\n        c->set_output(0, c->MakeShape({size_val}));\n      } else if (shape_tensor->NumElements() == 2) {\n        c->set_output(\n            0, c->MakeShape({shape_tensor->flat<int64_t>()(0), size_val}));\n      } else {\n        return errors::InvalidArgument(\"Input must be less than rank 2\");\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"RaggedBincount\")\n    .Input(\"splits: int64\")\n    .Input(\"values: Tidx\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Cumsum\")\n    .Input(\"x: T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Cumprod\")\n    .Input(\"x: T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"CumulativeLogsumexp\")\n    .Input(\"x : T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"QuantizedMatMul\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"Tactivation: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"QuantizedMul\")\n    .Input(\"x: T1\")\n    .Input(\"y: T2\")\n    .Input(\"min_x: float\")\n    .Input(\"max_x: float\")\n    .Input(\"min_y: float\")\n    .Input(\"max_y: float\")\n    .Output(\"z: Toutput\")\n    .Output(\"min_z: float\")\n    .Output(\"max_z: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BroadcastBinaryOpShapeFn(c));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"QuantizedAdd\")\n    .Input(\"x: T1\")\n    .Input(\"y: T2\")\n    .Input(\"min_x: float\")\n    .Input(\"max_x: float\")\n    .Input(\"min_y: float\")\n    .Input(\"max_y: float\")\n    .Output(\"z: Toutput\")\n    .Output(\"min_z: float\")\n    .Output(\"max_z: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BroadcastBinaryOpShapeFn(c));\n      // min_x, max_x, min_y, max_y should be scalar.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"QuantizeDownAndShrinkRange\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Requantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Input(\"requested_output_min: float\")\n    .Input(\"requested_output_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"RequantizationRange\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(0, c->Scalar());\n      c->set_output(1, c->Scalar());\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Bucketize\")\n    .Input(\"input: T\")\n    .Output(\"output: int32\")\n    .Attr(\"T: {int32, int64, float, double}\")\n    .Attr(\"boundaries: list(float)\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ClipByValue\")\n    .Input(\"t: T\")\n    .Input(\"clip_value_min: T\")\n    .Input(\"clip_value_max: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#ifdef INTEL_MKL\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklAddN\")\n    .Input(\"inputs: N * T\")\n    .Input(\"mkl_input: N * uint8\")\n    .Output(\"sum: T\")\n    .Output(\"mkl_sum: uint8\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      c->set_output(0, cur);\n      return Status::OK();\n    })\n    .Doc(R\"doc(\nAdd two input tensors element wise using mkl kernel sum.\ninputs: Must all be the same size and shape.\n)doc\");\n\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"RequantizePerChannel\")\n    .Input(\"input: T\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Input(\"requested_output_min: float\")\n    .Input(\"requested_output_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype = DT_QINT32\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\nREGISTER_OP(\"RequantizationRangePerChannel\")\n    .Input(\"input: T\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype = DT_QINT32\")\n    .Attr(\"clip_value_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      c->set_output(0, c->Scalar());\n      c->set_output(1, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"NextAfter\")\n    .Attr(\"T: {float64, float32} = DT_FLOAT\")\n    .Input(\"x1: T\")\n    .Input(\"x2: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"SobolSample\")\n    .Input(\"dim: int32\")\n    .Input(\"num_results: int32\")\n    .Input(\"skip: int32\")\n    .Attr(\"dtype: {float, double} = DT_FLOAT\")\n    .Output(\"samples: dtype\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      ShapeHandle unused;\n\n      // inputs must be scalars\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n\n      const Tensor* dim_t = c->input_tensor(0);\n      const Tensor* num_results_t = c->input_tensor(1);\n\n      int32_t dim = dim_t == nullptr ? InferenceContext::kUnknownDim\n                                     : dim_t->scalar<int32>()();\n\n      int32_t num_results = num_results_t == nullptr\n                                ? InferenceContext::kUnknownDim\n                                : num_results_t->scalar<int32>()();\n\n      c->set_output(0, c->Matrix(num_results, dim));\n      return Status::OK();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for bincount_ops.bincount.\"\"\"\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import bincount_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\nclass BincountTest(test_util.TensorFlowTestCase):\n\n  def test_empty(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=5)),\n          [0, 0, 0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=1)), [0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=0)), [])\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=0, dtype=np.float32)).dtype,\n          np.float32)\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=3, dtype=np.float64)).dtype,\n          np.float64)\n\n  def test_values(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1, 1, 1, 2, 2, 3])),\n          [0, 3, 2, 1])\n      arr = [1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [0, 5, 4, 3, 2, 1])\n      arr += [0, 0, 0, 0, 0, 0]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [6, 5, 4, 3, 2, 1])\n\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([])), [])\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([0, 0, 0])), [3])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5])), [0, 0, 0, 0, 0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(10000))),\n          np.ones(10000))\n\n  def test_maxlength(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5], maxlength=3)), [0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1], maxlength=3)), [0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], maxlength=3)), [])\n\n  def test_random_with_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n        arr = np.random.randint(0, 1000, num_samples)\n        if dtype == dtypes.int32 or dtype == dtypes.int64:\n          weights = np.random.randint(-100, 100, num_samples)\n        else:\n          weights = np.random.random(num_samples)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, weights)),\n            np.bincount(arr, weights))\n\n  def test_random_without_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [np.int32, np.float32]:\n        arr = np.random.randint(0, 1000, num_samples)\n        weights = np.ones(num_samples).astype(dtype)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, None)),\n            np.bincount(arr, weights))\n\n  @test_util.run_gpu_only\n  def test_bincount_determinism_error(self):\n    arr = np.random.randint(0, 1000, size=1000)\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of Bincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=None))\n    arr = np.random.randint(0, 1000, size=(100, 100))\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of \"\n        \"DenseBincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=-1))\n\n  def test_zero_weights(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(1000), np.zeros(1000))),\n          np.zeros(1000))\n\n  def test_negative(self):\n    # unsorted_segment_sum will only report InvalidArgumentError on CPU\n    with self.cached_session(), ops.device(\"/CPU:0\"):\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(bincount_ops.bincount([1, 2, 3, -1, 6, 8]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_shape_function(self):\n    # size must be scalar.\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Shape must be rank 0 but is rank 1(?s).*Bincount\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], [1], [])\n    # size must be positive.\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be non-negative\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], -5, [])\n    # if size is a constant then the shape is known.\n    v1 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], 5, [])\n    self.assertAllEqual(v1.get_shape().as_list(), [5])\n    # if size is a placeholder then the shape is unknown.\n    with ops.Graph().as_default():\n      s = array_ops.placeholder(dtype=dtypes.int32)\n      v2 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], s, [])\n      self.assertAllEqual(v2.get_shape().as_list(), [None])\n\n\nclass BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.bincount(inp, minlength=size)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.bincount(inp, minlength=size, weights=np_weight)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size, binary_output=True)))\n\n  def _test_bincount_col_count(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  def _test_bincount_col_binary(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  def _test_bincount_col_count_with_weights(self, num_rows, num_cols, size,\n                                            dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  def test_col_reduce_basic(self):\n    with test_util.use_gpu():\n      v = self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[[1, 2, 3], [0, 3, 2]], weights=[], size=4))\n    expected_out = [[0., 1., 1., 1.], [1., 0., 1., 1.]]\n    self.assertAllEqual(expected_out, v)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 10\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory_with_weights(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory_with_weights(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 7\n    size = 10\n    self._test_bincount_col_binary(num_rows, num_cols, size, dtype)\n\n  def test_invalid_rank(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"at most rank 2\"):\n      with test_util.use_gpu():\n        self.evaluate(\n            gen_math_ops.dense_bincount(\n                input=[[[1, 2, 3], [0, 3, 2]]], weights=[], size=10))\n\n\nclass SparseBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_count(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=6,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_with_weights(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    weights = ragged_factory_ops.constant([[], [], [.1, .2, .3], [],\n                                           [.2, .5, .6, .3]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.2, .3, 0, .1, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.5, 0, 0, 0, .9, .2]]\n    self.assertAllClose(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=weights.values,\n                size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=np_weight,\n                size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=size,\n                binary_output=True)))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/math_ops.cc.\n\n#include \"tensorflow/core/platform/errors.h\"\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/kernels/bincount_op.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/determinism.h\"\n\nnamespace tensorflow {\n\nusing thread::ThreadPool;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, true> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    Tensor partial_bins_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({num_threads, num_bins}), &partial_bins_t));\n    auto partial_bins = partial_bins_t.matrix<bool>();\n    partial_bins.setZero();\n    thread_pool->ParallelForWithWorkerId(\n        arr.size(), 8 /* cost */,\n        [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n          for (int64_t i = start_ind; i < limit_ind; i++) {\n            Tidx value = arr(i);\n            if (value < num_bins) {\n              partial_bins(worker_id, value) = true;\n            }\n          }\n        });\n\n    // Sum the partial bins along the 0th axis.\n    Eigen::array<int, 1> reduce_dim({0});\n    output.device(context->eigen_cpu_device()) =\n        partial_bins.any(reduce_dim).cast<T>();\n    return Status::OK();\n  }\n};\n\ntemplate <typename Tidx, typename T>\nstruct BincountFunctor<CPUDevice, Tidx, T, false> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 1>::ConstTensor& arr,\n                        const typename TTypes<T, 1>::ConstTensor& weights,\n                        typename TTypes<T, 1>::Tensor& output,\n                        const Tidx num_bins) {\n    Tensor all_nonneg_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_BOOL, TensorShape({}), &all_nonneg_t, AllocatorAttributes()));\n    all_nonneg_t.scalar<bool>().device(context->eigen_cpu_device()) =\n        (arr >= Tidx(0)).all();\n    if (!all_nonneg_t.scalar<bool>()()) {\n      return errors::InvalidArgument(\"Input arr must be non-negative!\");\n    }\n\n    // Allocate partial output bin sums for each worker thread. Worker ids in\n    // ParallelForWithWorkerId range from 0 to NumThreads() inclusive.\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    const int64_t num_threads = thread_pool->NumThreads() + 1;\n    const Tidx* arr_data = arr.data();\n    const std::ptrdiff_t arr_size = arr.size();\n    const T* weight_data = weights.data();\n    if (weights.size() && weights.size() != arr_size) {\n      return errors::InvalidArgument(\n          \"Input indices and weights must have the same size.\");\n    }\n    if (num_threads == 1) {\n      output.setZero();\n      T* output_data = output.data();\n      if (weights.size()) {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            output_data[value] += weight_data[i];\n          }\n        }\n      } else {\n        for (int64_t i = 0; i < arr_size; i++) {\n          const Tidx value = arr_data[i];\n          if (value < num_bins) {\n            // Complex numbers don't support \"++\".\n            output_data[value] += T(1);\n          }\n        }\n      }\n    } else {\n      Tensor partial_bins_t;\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DataTypeToEnum<T>::value, TensorShape({num_threads, num_bins}),\n          &partial_bins_t));\n      auto partial_bins = partial_bins_t.matrix<T>();\n      partial_bins.setZero();\n      thread_pool->ParallelForWithWorkerId(\n          arr_size, 8 /* cost */,\n          [&](int64_t start_ind, int64_t limit_ind, int64_t worker_id) {\n            if (weights.size()) {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  partial_bins(worker_id, value) += weight_data[i];\n                }\n              }\n            } else {\n              for (int64_t i = start_ind; i < limit_ind; i++) {\n                Tidx value = arr_data[i];\n                if (value < num_bins) {\n                  // Complex numbers don't support \"++\".\n                  partial_bins(worker_id, value) += T(1);\n                }\n              }\n            }\n          });\n\n      // Sum the partial bins along the 0th axis.\n      Eigen::array<int, 1> reduce_dim({0});\n      output.device(context->eigen_cpu_device()) = partial_bins.sum(reduce_dim);\n    }\n    return Status::OK();\n  }\n};\n\ntemplate <typename Tidx, typename T, bool binary_output>\nstruct BincountReduceFunctor<CPUDevice, Tidx, T, binary_output> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<Tidx, 2>::ConstTensor& in,\n                        const typename TTypes<T, 2>::ConstTensor& weights,\n                        typename TTypes<T, 2>::Tensor& out,\n                        const Tidx num_bins) {\n    const int num_rows = out.dimension(0);\n    const int num_cols = in.dimension(1);\n    ThreadPool* thread_pool =\n        context->device()->tensorflow_cpu_worker_threads()->workers;\n    thread_pool->ParallelForWithWorkerId(\n        num_rows, 8 /* cost */,\n        [&](int64_t start_row, int64_t end_row, int64_t worker_id) {\n          for (int64_t i = start_row; i < end_row; ++i) {\n            for (int64_t j = 0; j < num_cols; ++j) {\n              Tidx value = in(i, j);\n              if (value < num_bins) {\n                if (binary_output) {\n                  out(i, value) = T(1);\n                } else {\n                  if (weights.size()) {\n                    out(i, value) += weights(i, j);\n                  } else {\n                    out(i, value) += T(1);\n                  }\n                }\n              }\n            }\n          }\n        });\n    return Status::OK();\n  }\n};\n\n}  // namespace functor\n\ntemplate <typename Device, typename T>\nclass BincountOp : public OpKernel {\n public:\n  explicit BincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& arr_t = ctx->input(0);\n    const Tensor& size_tensor = ctx->input(1);\n    OP_REQUIRES(ctx, size_tensor.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_tensor.dims()));\n    int32_t size = size_tensor.scalar<int32_t>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    const Tensor& weights_t = ctx->input(2);\n    const auto arr = arr_t.flat<int32_t>();\n    const auto weights = weights_t.flat<T>();\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({size}), &output_t));\n    auto output = output_t->flat<T>();\n    OP_REQUIRES_OK(ctx,\n                   functor::BincountFunctor<Device, int32_t, T, false>::Compute(\n                       ctx, arr, weights, output, size));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(                                           \\\n      Name(\"Bincount\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      BincountOp<CPUDevice, type>)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"Bincount\")                \\\n                              .Device(DEVICE_GPU)         \\\n                              .HostMemory(\"size\")         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          BincountOp<GPUDevice, type>)\n\nTF_CALL_int32(REGISTER_KERNELS);\nTF_CALL_float(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass DenseBincountOp : public OpKernel {\n public:\n  explicit DenseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(\n          ctx, !OpDeterminismRequired(),\n          errors::Unimplemented(\n              \"Determinism is not yet supported in GPU implementation of \"\n              \"DenseBincount.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& data = ctx->input(0);\n    OP_REQUIRES(ctx, data.dims() <= 2,\n                errors::InvalidArgument(\n                    \"Shape must be at most rank 2 but is rank \", data.dims()));\n\n    const Tensor& size_t = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (data.dims() == 1) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, data.flat<Tidx>(), weights.flat<T>(), out, size));\n      }\n    } else if (data.dims() == 2) {\n      const int64_t num_rows = data.dim_size(0);\n      auto weight_matrix =\n          (weights.NumElements() == 0)\n              ? weights.shaped<T, 2>(gtl::InlinedVector<int64_t, 2>(2, 0))\n              : weights.matrix<T>();\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      if (binary_output_) {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountReduceFunctor<Device, Tidx, T, true>::Compute(\n                     ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx,\n            functor::BincountReduceFunctor<Device, Tidx, T, false>::Compute(\n                ctx, data.matrix<Tidx>(), weight_matrix, out, size));\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"DenseBincount\")              \\\n                              .Device(DEVICE_GPU)            \\\n                              .HostMemory(\"size\")            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          DenseBincountOp<GPUDevice, Tidx, T>);\n#define REGISTER_GPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_int32(REGISTER_GPU_KERNELS);\nTF_CALL_float(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename Tidx, typename T>\nclass SparseBincountOp : public OpKernel {\n public:\n  explicit SparseBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& indices = ctx->input(0);\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& dense_shape = ctx->input(2);\n    const Tensor& size_t = ctx->input(3);\n    const auto weights = ctx->input(4).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    bool is_1d = dense_shape.NumElements() == 1;\n\n    Tensor* out_t;\n    functor::SetZeroFunctor<Device, T> fill;\n    if (is_1d) {\n      OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({size}), &out_t));\n      auto out = out_t->flat<T>();\n      fill(ctx->eigen_device<Device>(), out);\n      if (binary_output_) {\n        OP_REQUIRES_OK(ctx,\n                       functor::BincountFunctor<Device, Tidx, T, true>::Compute(\n                           ctx, values, weights, out, size));\n      } else {\n        OP_REQUIRES_OK(\n            ctx, functor::BincountFunctor<Device, Tidx, T, false>::Compute(\n                     ctx, values, weights, out, size));\n      }\n    } else {\n      const auto shape = dense_shape.flat<int64_t>();\n      const int64_t num_rows = shape(0);\n      OP_REQUIRES_OK(\n          ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n      const auto out = out_t->matrix<T>();\n      fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n      const auto indices_mat = indices.matrix<int64_t>();\n      for (int64_t i = 0; i < indices_mat.dimension(0); ++i) {\n        const int64_t batch = indices_mat(i, 0);\n        const Tidx bin = values(i);\n        OP_REQUIRES(\n            ctx, batch < out.dimension(0),\n            errors::InvalidArgument(\"Index out of bound. `batch` (\", batch,\n                                    \") must be less than the dimension size (\",\n                                    out.dimension(0), \").\"));\n        OP_REQUIRES(\n            ctx, bin < out.dimension(1),\n            errors::InvalidArgument(\"Index out ouf bound. `bin` (\", bin,\n                                    \") must be less then the dimension size (\",\n                                    out.dimension(1), \").\"));\n        if (bin < size) {\n          if (binary_output_) {\n            out(batch, bin) = T(1);\n          } else {\n            if (weights_size) {\n              out(batch, bin) += weights(i);\n            } else {\n              out(batch, bin) += T(1);\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          SparseBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\ntemplate <typename Device, typename Tidx, typename T>\nclass RaggedBincountOp : public OpKernel {\n public:\n  explicit RaggedBincountOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"binary_output\", &binary_output_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const auto splits = ctx->input(0).flat<int64_t>();\n    const auto values = ctx->input(1).flat<Tidx>();\n    const Tensor& size_t = ctx->input(2);\n    const auto weights = ctx->input(3).flat<T>();\n    const int64_t weights_size = weights.size();\n\n    OP_REQUIRES(ctx, size_t.dims() == 0,\n                errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                        size_t.dims()));\n    Tidx size = size_t.scalar<Tidx>()();\n    OP_REQUIRES(\n        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;\n    fill(ctx->eigen_device<Device>(), out_t->flat<T>());\n    const auto out = out_t->matrix<T>();\n\n    for (int idx = 0; idx < num_values; ++idx) {\n      while (idx >= splits(batch_idx)) {\n        batch_idx++;\n      }\n      Tidx bin = values(idx);\n      OP_REQUIRES(ctx, bin >= 0,\n                  errors::InvalidArgument(\"Input must be non-negative\"));\n      if (bin < size) {\n        if (binary_output_) {\n          out(batch_idx - 1, bin) = T(1);\n        } else {\n          T value = (weights_size > 0) ? weights(idx) : T(1);\n          out(batch_idx - 1, bin) += value;\n        }\n      }\n    }\n  }\n\n private:\n  bool binary_output_;\n};\n\n#define REGISTER_KERNELS(Tidx, T)                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"RaggedBincount\")             \\\n                              .Device(DEVICE_CPU)            \\\n                              .TypeConstraint<T>(\"T\")        \\\n                              .TypeConstraint<Tidx>(\"Tidx\"), \\\n                          RaggedBincountOp<CPUDevice, Tidx, T>);\n#define REGISTER_CPU_KERNELS(T) \\\n  REGISTER_KERNELS(int32, T);   \\\n  REGISTER_KERNELS(int64_t, T);\n\nTF_CALL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n#undef REGISTER_CPU_KERNELS\n#undef REGISTER_KERNELS\n\n}  // end namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"AddN\")\n    .Input(\"inputs: N * T\")\n    .Output(\"sum: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: {numbertype, variant}\")\n    .SetIsCommutative()\n    .SetIsAggregate()\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      c->set_output(0, cur);\n\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"T\", &dtype));\n\n      if (dtype != DT_VARIANT) {\n        // Exit early if not DT_VARIANT.\n        return Status::OK();\n      } else {\n        // DT_VARIANT shape handle shape inference.  All sizes and dtypes must\n        // be the same; all shapes must be compatible via Merge.\n        std::vector<shape_inference::ShapeAndType> cur_shapes_and_types;\n        auto* shapes_and_types =\n            c->input_handle_shapes_and_types(c->num_inputs() - 1);\n        if (shapes_and_types) {\n          cur_shapes_and_types = *shapes_and_types;\n        }\n\n        for (int i = c->num_inputs() - 2; i >= 0; --i) {\n          auto shapes_and_types_i = c->input_handle_shapes_and_types(i);\n          if (!shapes_and_types && shapes_and_types_i) {\n            // TODO(ebrevdo): Find cases where this happens and fix their shape\n            // inference.  If we are calling AddN on variant types, they should\n            // all have consistent shape_and_type info.\n            shapes_and_types = shapes_and_types_i;\n          } else if (shapes_and_types && shapes_and_types_i) {\n            if (shapes_and_types_i->size() != shapes_and_types->size()) {\n              return errors::InvalidArgument(\n                  \"shapes_and_types[\", i,\n                  \"].size() == \", shapes_and_types_i->size(),\n                  \" != shapes_and_types[0].size() == \",\n                  shapes_and_types->size());\n            }\n            for (int j = 0; j < shapes_and_types->size(); ++j) {\n              if (shapes_and_types->at(j).dtype !=\n                  shapes_and_types_i->at(j).dtype) {\n                return errors::InvalidArgument(\n                    \"shapes_and_types[\", i, \"][\", j, \"].dtype() == \",\n                    DataTypeString(shapes_and_types_i->at(j).dtype),\n                    \" != shapes_and_types[0][\", j, \"].dtype == \",\n                    DataTypeString(shapes_and_types->at(j).dtype));\n              }\n              TF_RETURN_WITH_CONTEXT_IF_ERROR(\n                  c->Merge(shapes_and_types_i->at(j).shape,\n                           cur_shapes_and_types.at(j).shape,\n                           &cur_shapes_and_types.at(j).shape),\n                  \"From merging shapes_and_types[\", i, \"][\", j, \"].shape with \",\n                  \"shapes_and_types[0][\", j, \"].shape\");\n            }\n          }\n        }\n        if (shapes_and_types) {\n          c->set_output_handle_shapes_and_types(0, cur_shapes_and_types);\n        }\n        return Status::OK();\n      }\n    });\n\n// --------------------------------------------------------------------------\n\n// Note that the following operator is just a placeholder and has no\n// associated kernel. The code in accumulate_n_optimizer.cc replaces\n// this placeholder with a graph of operators that do have kernels.\n// The Python code that generates instances of this op is currently in\n// contrib/framework/python/ops/accumulate_n_v2.py\nREGISTER_OP(\"AccumulateNV2\")\n    .Input(\"inputs: N * T\")\n    .Output(\"sum: T\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"shape: shape\")\n    .SetIsCommutative()\n    .SetIsAggregate()\n    .SetShapeFn(shape_inference::ExplicitShape);\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"BatchMatMul\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulShape);\n\nREGISTER_OP(\"BatchMatMulV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int16, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n\nREGISTER_OP(\"BatchMatMulV3\")\n    .Input(\"x: Ta\")\n    .Input(\"y: Tb\")\n    .Output(\"output: Tout\")\n    .Attr(\n        \"Ta: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .Attr(\n        \"Tb: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .Attr(\n        \"Tout: {bfloat16, half, float, double, int16, int32, int64, complex64, \"\n        \"complex128}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklBatchMatMul\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulShape);\n\nREGISTER_OP(\"_MklBatchMatMulV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"adj_x: bool = false\")\n    .Attr(\"adj_y: bool = false\")\n    .SetShapeFn(shape_inference::BatchMatMulV2Shape);\n#endif  // INTEL_MKL\n\n// --------------------------------------------------------------------------\n// Casting Ops\n//\n// NOTE: Only a smaller number of types are supported by\n// Cast. The exact casting rule is TBD. The current\n// implementation uses C++ static cast rules for numeric\n// types, which may be changed in the future.\nREGISTER_OP(\"Cast\")\n    .Input(\"x: SrcT\")\n    .Output(\"y: DstT\")\n    .Attr(\"SrcT: type\")\n    .Attr(\"DstT: type\")\n    .Attr(\"Truncate: bool = false\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"_HostCast\")\n    .Input(\"x: SrcT\")\n    .Output(\"y: DstT\")\n    .Attr(\"SrcT: type\")\n    .Attr(\"DstT: type\")\n    .Attr(\"Truncate: bool = false\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\nCast x of type SrcT to y of DstT.\n\n_HostCast requires its input and produces its output in host memory.\n)doc\");\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Abs\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double, int8, int16, int32, int64}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ComplexAbs\")\n    .Input(\"x: T\")\n    .Output(\"y: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// Declares cwise unary operations signature: 't -> 't\n#define UNARY()                                                            \\\n  Input(\"x: T\")                                                            \\\n      .Output(\"y: T\")                                                      \\\n      .Attr(                                                               \\\n          \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \" \\\n          \"complex64, complex128}\")                                        \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_UNSIGNED()                                                   \\\n  Input(\"x: T\")                                                            \\\n      .Output(\"y: T\")                                                      \\\n      .Attr(                                                               \\\n          \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \" \\\n          \"uint8, uint16, uint32, uint64, complex64, complex128}\")         \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_REAL()                              \\\n  Input(\"x: T\")                                   \\\n      .Output(\"y: T\")                             \\\n      .Attr(\"T: {bfloat16, half, float, double}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_COMPLEX()                                                  \\\n  Input(\"x: T\")                                                          \\\n      .Output(\"y: T\")                                                    \\\n      .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\n#define UNARY_GRADIENT_COMPLEX()                                         \\\n  Input(\"y: T\")                                                          \\\n      .Input(\"dy: T\")                                                    \\\n      .Output(\"z: T\")                                                    \\\n      .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\") \\\n      .SetShapeFn(shape_inference::UnchangedShape)\n\nREGISTER_OP(\"Neg\").UNARY();\n\nREGISTER_OP(\"Inv\").UNARY();\n\nREGISTER_OP(\"InvGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Reciprocal\").UNARY();\n\nREGISTER_OP(\"ReciprocalGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Square\").UNARY_UNSIGNED();\n\nREGISTER_OP(\"Sqrt\").UNARY_COMPLEX();\n\nREGISTER_OP(\"SqrtGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Rsqrt\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Round\").UNARY();\n\nREGISTER_OP(\"RsqrtGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Exp\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Expm1\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Log\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Log1p\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Sinh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Cosh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Tanh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Asinh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Acosh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Atanh\").UNARY_COMPLEX();\n\nREGISTER_OP(\"TanhGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Lgamma\").UNARY_REAL();\n\nREGISTER_OP(\"Digamma\").UNARY_REAL();\n\nREGISTER_OP(\"Erf\").UNARY_REAL();\nREGISTER_OP(\"Erfinv\").UNARY_REAL();\nREGISTER_OP(\"Ndtri\").UNARY_REAL();\nREGISTER_OP(\"Erfc\").UNARY_REAL();\n\nREGISTER_OP(\"Sigmoid\").UNARY_COMPLEX();\n\nREGISTER_OP(\"SigmoidGrad\").UNARY_GRADIENT_COMPLEX();\n\nREGISTER_OP(\"Sin\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Cos\").UNARY_COMPLEX();\n\nREGISTER_OP(\"Tan\").UNARY();\n\nREGISTER_OP(\"Asin\").UNARY();\n\nREGISTER_OP(\"Acos\").UNARY();\n\nREGISTER_OP(\"Atan\").UNARY();\n\nREGISTER_OP(\"_UnaryOpsComposition\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {float, half, double}\")\n    .Attr(\"op_names: list(string)\")\n    .SetShapeFn(shape_inference::UnchangedShape)\n    .Doc(R\"doc(\n*NOTE*: Do not invoke this operator directly in Python. Graph rewrite pass is\nexpected to create these operators.\n)doc\");\n\n#undef UNARY\n#undef UNARY_REAL\n#undef UNARY_COMPLEX\n\nREGISTER_OP(\"IsNan\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"IsInf\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"IsFinite\")\n    .Input(\"x: T\")\n    .Output(\"y: bool\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Sign\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Floor\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Ceil\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Rint\")\n    .Input(\"x: T\")\n    .Output(\"y: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n// Declares cwise binary operations signature: 't, 't -> 't.\n\n#define BINARY_MORE()                                                          \\\n  Input(\"x: T\").Input(\"y: T\").Output(\"z: T\").Attr(                             \\\n      \"T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, \" \\\n      \"uint32, uint64, int64, complex64, complex128}\")\n\n#define BINARY_FEWER()                                               \\\n  Input(\"x: T\").Input(\"y: T\").Output(\"z: T\").Attr(                   \\\n      \"T: {bfloat16, half, float, double, int32, int64, complex64, \" \\\n      \"complex128}\")\n\nREGISTER_OP(\"Add\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128, string}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"AddV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, uint16, uint32, uint64, \"\n        \"int8, int16, int32, int64, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .SetIsAggregate()\n    .SetIsCommutative();\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklAdd\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\n        \"T: {half, float, double, uint8, int8, int16, int32, int64, complex64, \"\n        \"complex128, string, bfloat16}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns `x` + `y` element-wise.\n\n*NOTE*: `tf.math.add` supports broadcasting. `tf.math.add_n` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n)doc\");\n\nREGISTER_OP(\"_MklAddV2\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .SetIsAggregate()\n    .SetIsCommutative()\n    .Doc(R\"doc(\nReturns `x` + `y` element-wise.\n*NOTE*: `tf.math.add` supports broadcasting. `tf.math.add_n` does not. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n)doc\");\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"Sub\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, uint8, int8, uint16, int16, int32, \"\n        \"int64, complex64, complex128, uint32, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"_MklSub\")\n    .BINARY_FEWER()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns x - y element-wise.\n\n*NOTE*: `Sub` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Mul\").BINARY_MORE().SetIsCommutative().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"MulNoNan\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklMul\")\n    .BINARY_MORE()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns x * y element-wise.\n\n*NOTE*: `Mul` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Div\").BINARY_MORE().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"DivNoNan\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, bfloat16, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"FloorDiv\")\n    .BINARY_MORE()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"TruncateDiv\")\n    .BINARY_MORE()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"RealDiv\").BINARY_MORE().SetShapeFn(\n    shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note SquaredDifference implements conj(x - y)*(x - y).\nREGISTER_OP(\"SquaredDifference\")\n    .BINARY_FEWER()\n    .SetIsCommutative()\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklSquaredDifference\")\n    .BINARY_FEWER()\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"mkl_z: uint8\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns (x - y)(x - y) element-wise.\n\n*NOTE*: `SquaredDifference` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Xlogy\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Xlog1py\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Xdivy\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {half, float, double, complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n#undef BINARY_FEWER\n#undef BINARY_MORE\n\nREGISTER_OP(\"Maximum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, \"\n        \"int32, uint32, int64, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklMaximum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Input(\"mkl_x: uint8\")\n    .Input(\"mkl_y: uint8\")\n    .Output(\"z: T\")\n    .Output(\"mkl_z: uint8\")\n    .Attr(\"T: {half, float, double, int32, int64, bfloat16}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n    .Doc(R\"doc(\nReturns the max of x and y (i.e. x > y ? x : y) element-wise.\n\n*NOTE*: `Maximum` supports broadcasting. More about broadcasting\n[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n)doc\");\n\nREGISTER_OP(\"Minimum\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int8, uint8, int16, uint16, \"\n        \"int32, uint32, int64, uint64}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Mod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {int32, int64, float16, half, bfloat16, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"FloorMod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {int8, int16, int32, int64, uint8, uint16, uint32, uint64, \"\n        \"bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"TruncateMod\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {int32, int64, bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Pow\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: T\")\n    .Attr(\n        \"T: {bfloat16, float, half, double, int8, int16, int32, int64, \"\n        \"complex64, complex128}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Igammac\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Igamma\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"IgammaGradA\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Zeta\")\n    .Input(\"x: T\")\n    .Input(\"q: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Polygamma\")\n    .Input(\"a: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Atan2\")\n    .Input(\"y: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Betainc\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Input(\"x: T\")\n    .Output(\"z: T\")\n    .Attr(\"T: {float, double}\")\n    .SetShapeFn([](InferenceContext* c) {\n      const int num_inputs = 3;\n      ShapeHandle output = c->UnknownShape();\n      int num_scalars = 0;\n      ShapeHandle some_non_scalar;\n      for (int i = 0; i < num_inputs; ++i) {\n        ShapeHandle in = c->input(i);\n        if (!c->RankKnown(in)) {\n          some_non_scalar = in;\n          // An input with unknown rank could be either a scalar (to be\n          // broadcast) or some other shape.\n        } else if (c->Rank(in) == 0) {\n          // Input is a scalar, it will be broadcast to the output shape.\n          ++num_scalars;\n        } else {\n          TF_RETURN_IF_ERROR(c->Merge(output, in, &output));\n          some_non_scalar = output;\n        }\n      }\n\n      if (num_scalars == num_inputs - 1) {\n        // If all but one input is known to be a scalar, then output is the\n        // remaining input.\n        output = some_non_scalar;\n      } else if (num_scalars == num_inputs) {\n        // If all are scalars, output is scalar; pick the first one arbitrarily.\n        output = c->input(0);\n      }\n\n      c->set_output(0, output);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\n// Declares cwise binary comparison operations signature: 't, 't -> bool,\n// where 't has a natural total order.\n#define COMPARISON()             \\\n  Input(\"x: T\")                  \\\n      .Input(\"y: T\")             \\\n      .Output(\"z: bool\")         \\\n      .Attr(\"T: realnumbertype\") \\\n      .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n\nREGISTER_OP(\"Less\").COMPARISON();\n\nREGISTER_OP(\"LessEqual\").COMPARISON();\n\nREGISTER_OP(\"Greater\").COMPARISON();\n\nREGISTER_OP(\"GreaterEqual\").COMPARISON();\n\n#undef COMPARISON\n\n// --------------------------------------------------------------------------\n\n#define EQUALITY_COMPARISON()                                      \\\n  Input(\"x: T\")                                                    \\\n      .Input(\"y: T\")                                               \\\n      .Output(\"z: bool\")                                           \\\n      .SetIsCommutative()                                          \\\n      .Attr(\"T: type\")                                             \\\n      .Attr(\"incompatible_shape_error: bool = true\")               \\\n      .SetShapeFn([](InferenceContext* c) {                        \\\n        ShapeHandle x = c->input(0);                               \\\n        ShapeHandle y = c->input(1);                               \\\n        ShapeHandle output;                                        \\\n        bool incompatible_shape_error;                             \\\n        TF_RETURN_IF_ERROR(c->GetAttr(\"incompatible_shape_error\",  \\\n                                      &incompatible_shape_error)); \\\n        TF_RETURN_IF_ERROR(BroadcastBinaryOpOutputShapeFnHelper(   \\\n            c, x, y, incompatible_shape_error, &output));          \\\n        c->set_output(0, output);                                  \\\n        return Status::OK();                                       \\\n      })\n\nREGISTER_OP(\"Equal\").EQUALITY_COMPARISON();\n\nREGISTER_OP(\"NotEqual\").EQUALITY_COMPARISON();\n\n#undef EQUALITY_COMPARISON\n\nREGISTER_OP(\"ApproximateEqual\")\n    .Input(\"x: T\")\n    .Input(\"y: T\")\n    .Output(\"z: bool\")\n    .SetIsCommutative()\n    .Attr(\"T: numbertype\")\n    .Attr(\"tolerance: float = 0.00001\")\n    .SetShapeFn([](InferenceContext* c) {\n      // The inputs 'x' and 'y' must have the same shape.\n      ShapeHandle data_x = c->input(0);\n      ShapeHandle data_y = c->input(1);\n      TF_RETURN_IF_ERROR(c->Merge(data_x, data_y, &data_x));\n      return shape_inference::UnchangedShape(c);\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"LogicalNot\")\n    .Input(\"x: bool\")\n    .Output(\"y: bool\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#define BINARY_LOGICAL()  \\\n  Input(\"x: bool\")        \\\n      .Input(\"y: bool\")   \\\n      .Output(\"z: bool\")  \\\n      .SetIsCommutative() \\\n      .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn)\n\nREGISTER_OP(\"LogicalAnd\").BINARY_LOGICAL();\n\nREGISTER_OP(\"LogicalOr\").BINARY_LOGICAL();\n\n#undef BINARY_LOGICAL\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Select\")\n    .Input(\"condition: bool\")\n    .Input(\"t: T\")\n    .Input(\"e: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto* handle_data_1 = c->input_handle_shapes_and_types(1);\n      auto* handle_data_2 = c->input_handle_shapes_and_types(2);\n      // Merge handle shape and dtype if applicable.\n      if (handle_data_1 != nullptr && handle_data_2 != nullptr) {\n        const auto size = handle_data_1->size();\n        std::vector<shape_inference::ShapeAndType> merged_handle_data(size);\n        if (size != handle_data_2->size()) {\n          return errors::InvalidArgument(\n              \"Trying to merge handles pointing to different numbers of \"\n              \"tensors.\");\n        }\n\n        for (int i = 0; i < size; ++i) {\n          const shape_inference::ShapeAndType& s1 = (*handle_data_1)[i];\n          const shape_inference::ShapeAndType& s2 = (*handle_data_2)[i];\n          if (s1.dtype != s2.dtype) {\n            // TODO(apassos) resolve this in the manner of b/32476923\n            return errors::InvalidArgument(\n                \"Trying to merge handles pointing to different dtypes.\");\n          }\n          merged_handle_data[i].dtype = s1.dtype;\n          TF_RETURN_IF_ERROR(\n              c->Merge(s1.shape, s2.shape, &merged_handle_data[i].shape));\n        }\n\n        c->set_output_handle_shapes_and_types(0, merged_handle_data);\n      }\n\n      // The inputs 'then' and 'else' must have the same shape.\n      ShapeHandle data = c->input(1);\n      ShapeHandle other = c->input(2);\n      TF_RETURN_IF_ERROR(c->Merge(data, other, &data));\n\n      // The input 'cond' must either have the same shape as 'then' and\n      // 'else', or be a vector if 'then' and 'else' are at least vectors.\n      ShapeHandle cond = c->input(0);\n\n      if (!c->RankKnown(cond) || !c->RankKnown(data)) {\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      // rank of shape and data is known.\n\n      const int32_t cond_rank = c->Rank(cond);\n      const int32_t data_rank = c->Rank(data);\n\n      if (cond_rank == 0) {\n        // The rank of 'cond' is a scalar.\n        // t and e can have any shape.\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (cond_rank != 1) {\n        // If 'cond' is not a vector, and not a scalar,\n        // then shape must match 'then' and 'else'\n        TF_RETURN_IF_ERROR(c->Merge(data, cond, &data));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (data_rank == 0) {\n        // if 'then' and 'else' are scalar also the cond must be\n        TF_RETURN_IF_ERROR(c->Merge(data, cond, &data));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      if (cond_rank == 1) {\n        // if the cond is a vector and the 'then' is not a scalar,\n        // the first dimension of 'then' and 'else'\n        TF_RETURN_IF_ERROR(c->Merge(cond, c->Vector(c->Dim(data, 0)), &cond));\n        c->set_output(0, data);\n        return Status::OK();\n      }\n\n      c->set_output(0, data);\n\n      return Status::OK();\n    });\n\nREGISTER_OP(\"SelectV2\")\n    .Input(\"condition: bool\")\n    .Input(\"t: T\")\n    .Input(\"e: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: type\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto* handle_data_1 = c->input_handle_shapes_and_types(1);\n      auto* handle_data_2 = c->input_handle_shapes_and_types(2);\n      // Merge handle shape and dtype if applicable.\n      if (handle_data_1 != nullptr && handle_data_2 != nullptr) {\n        const auto size = handle_data_1->size();\n        std::vector<shape_inference::ShapeAndType> merged_handle_data(size);\n        if (size != handle_data_2->size()) {\n          return errors::InvalidArgument(\n              \"Trying to merge handles pointing to different numbers of \"\n              \"tensors.\");\n        }\n\n        for (int i = 0; i < size; ++i) {\n          const shape_inference::ShapeAndType& s1 = (*handle_data_1)[i];\n          const shape_inference::ShapeAndType& s2 = (*handle_data_2)[i];\n          if (s1.dtype != s2.dtype) {\n            // TODO(apassos) resolve this in the manner of b/32476923\n            return errors::InvalidArgument(\n                \"Trying to merge handles pointing to different dtypes.\");\n          }\n          merged_handle_data[i].dtype = s1.dtype;\n          TF_RETURN_IF_ERROR(\n              c->Merge(s1.shape, s2.shape, &merged_handle_data[i].shape));\n        }\n\n        c->set_output_handle_shapes_and_types(0, merged_handle_data);\n      }\n\n      // The inputs 'cond', 'then', and 'else' must be broadcastable.\n      // TODO (yongtang): Consolidate 3-ary broadcast instead of\n      // multiple 2-ary broadcast.\n      ShapeHandle cond = c->input(0);\n      ShapeHandle then = c->input(1);\n      ShapeHandle else_ = c->input(2);\n      ShapeHandle other;\n      TF_RETURN_IF_ERROR(\n          BroadcastBinaryOpOutputShapeFnHelper(c, then, else_, true, &other));\n      ShapeHandle output;\n      TF_RETURN_IF_ERROR(\n          BroadcastBinaryOpOutputShapeFnHelper(c, cond, other, true, &output));\n      c->set_output(0, output);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"MatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\n        \"T: {bfloat16, half, float, double, int32, int64, complex64, \"\n        \"complex128}\")\n    .SetShapeFn(shape_inference::MatMulShape);\n\n#ifdef INTEL_MKL\nREGISTER_OP(\"_MklMatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"T: {bfloat16, float}\")\n    .SetShapeFn(shape_inference::MatMulShape);\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"SparseMatMul\")\n    .Input(\"a: Ta\")\n    .Input(\"b: Tb\")\n    .Output(\"product: float\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"a_is_sparse: bool = false\")\n    .Attr(\"b_is_sparse: bool = false\")\n    .Attr(\"Ta: {float, bfloat16} = DT_FLOAT\")\n    .Attr(\"Tb: {float, bfloat16} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::MatMulShape);\n\nREGISTER_OP(\"_FusedMatMul\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Input(\"args: num_args * T\")\n    .Output(\"product: T\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"T: {bfloat16, float}\")\n    .Attr(\"num_args: int >= 0\")\n    .Attr(\"fused_ops: list(string) = []\")\n    // Attributes for the FusedBatchNorm ----------- //\n    .Attr(\"epsilon: float = 0.0001\")\n    // Attributes for the LeakyRelu ---------------- //\n    .Attr(\"leakyrelu_alpha: float = 0.2\")\n    // --------------------------------------------- //\n    .SetShapeFn(shape_inference::MatMulShape)\n    .Doc(R\"doc(\nPerforms a MatMul followed by a specified series of operations.\n\nThe inputs to the MatMul are specified by `a` and `b`. The series of operations\nthat follows is specified by the `fused_ops` attribute, which is a list of TF op\nnames specified as strings (e.g. \"Relu\"). They are performed in order, where the\n(first) input to each op is the output of the preceding op. The first input and\nthe output of each fused_op must be of type T.\n\nCurrently supported fused_op combinations are: [\"BiasAdd\"] and [\"BiasAdd\",A],\nwhere A is one of {\"Elu\",\"Relu\",\"Relu6\"}.\n\n* The first input to BiasAdd is the Conv2D result, and the additional BiasAdd\ninput is specified by `args`.\n* If there is an op A specified, the output of the BiasAdd is the input to op A,\nand op A produces the _FusedConv2D output. Otherwise, the BiasAdd produces the\n_FusedConv2D output.\n\n*NOTE*: Do not invoke this operator directly in Python. Grappler is\nexpected to create these operators.\n)doc\");\n\n// --------------------------------------------------------------------------\n\n// For operations where the output is a reduction function along some\n// dimensions of the input.\nREGISTER_OP(\"Sum\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"EuclideanNorm\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Mean\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Prod\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Min\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: {realnumbertype, quantizedtype}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Max\")\n    .Input(\"input: T\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"T: {realnumbertype, quantizedtype}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nnamespace {\n\nStatus ArgOpShape(shape_inference::InferenceContext* c) {\n  ShapeHandle dimension_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &dimension_shape));\n\n  ShapeHandle input_shape = c->input(0);\n  if (!c->RankKnown(input_shape)) {\n    return shape_inference::UnknownShape(c);\n  }\n\n  const int32_t input_rank = c->Rank(input_shape);\n  if (input_rank <= 1) {\n    // Reducing a scalar/vector must return a scalar.\n    return shape_inference::ScalarShape(c);\n  }\n\n  const Tensor* dim_t = c->input_tensor(1);\n  if (dim_t == nullptr) {\n    // We don't know the value of the dimension, but we\n    // know the rank of the input, so return the correct\n    // rank with unknown dimensions.\n    std::vector<DimensionHandle> dims(input_rank - 1);\n    for (int i = 0; i < dims.size(); ++i) {\n      dims[i] = c->UnknownDim();\n    }\n\n    c->set_output(0, c->MakeShape(dims));\n    return Status::OK();\n  }\n\n  int64_t dimension_val;\n  if (dim_t->dtype() == DT_INT32) {\n    dimension_val = dim_t->scalar<int32>()();\n  } else {\n    dimension_val = dim_t->scalar<int64_t>()();\n  }\n\n  int64_t axis = dimension_val < 0 ? dimension_val + input_rank : dimension_val;\n  if (axis < 0 || axis >= input_rank) {\n    return errors::InvalidArgument(\n        \"Dimension (\", dimension_val, \") must be in the range [\", -input_rank,\n        \", \", input_rank, \"), where \", input_rank,\n        \" is the number of dimensions in the input.\");\n  }\n\n  // Return the input shape without the dimension being reduced.\n  std::vector<DimensionHandle> dims;\n  for (int i = 0; i < input_rank; ++i) {\n    if (axis != i) {\n      dims.emplace_back(c->Dim(input_shape, i));\n    }\n  }\n  c->set_output(0, c->MakeShape(dims));\n  return Status::OK();\n}\n\n}  // namespace\n\nREGISTER_OP(\"ArgMax\")\n    .Input(\"input: T\")\n    .Input(\"dimension: Tidx\")\n    .Output(\"output: output_type\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"output_type: {int32, int64} = DT_INT64\")\n    .SetShapeFn(ArgOpShape);\n\nREGISTER_OP(\"ArgMin\")\n    .Input(\"input: T\")\n    .Input(\"dimension: Tidx\")\n    .Output(\"output: output_type\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"output_type: {int32, int64} = DT_INT64\")\n    .SetShapeFn(ArgOpShape);\n\nnamespace {\n\nStatus SegmentReductionShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &segment_ids_shape));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(\n      c->Concatenate(c->Vector(InferenceContext::kUnknownDim), subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &segment_ids_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(indices_shape, segment_ids_shape, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(\n      c->Concatenate(c->Vector(InferenceContext::kUnknownDim), subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionGradShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(c->input(2), indices_shape, &unused));\n\n  // output_dim0 should be a scalar\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  const Tensor* dim0 = c->input_tensor(3);\n  ShapeHandle dim0_shape;\n  if (dim0 == nullptr) {\n    // We don't have the value at inference time, so the output\n    // shape is unknown.\n    dim0_shape = c->Vector(InferenceContext::kUnknownDim);\n  } else {\n    auto dim0_value = dim0->scalar<int32>()();\n    if (dim0_value < 0) {\n      return errors::InvalidArgument(\n          \"Cannot specify a negative value for output_dim0\");\n    }\n    dim0_shape = c->Vector(dim0_value);\n  }\n\n  ShapeHandle out;\n  TF_RETURN_IF_ERROR(c->Concatenate(dim0_shape, subshape, &out));\n  c->set_output(0, out);\n  return Status::OK();\n}\n\nStatus SparseSegmentReductionWithNumSegmentsShapeFn(InferenceContext* c) {\n  ShapeHandle data_shape;\n  TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &data_shape));\n\n  ShapeHandle indices_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &indices_shape));\n\n  ShapeHandle segment_ids_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &segment_ids_shape));\n\n  ShapeHandle num_segments_shape;\n  TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &num_segments_shape));\n\n  // indices and segment_ids should merge cleanly.\n  ShapeHandle unused;\n  TF_RETURN_IF_ERROR(c->Merge(indices_shape, segment_ids_shape, &unused));\n\n  ShapeHandle subshape;\n  TF_RETURN_IF_ERROR(c->Subshape(data_shape, 1, &subshape));\n\n  ShapeHandle out;\n  const Tensor* dim0 = c->input_tensor(3);\n  if (dim0 == nullptr) {\n    // We don't have the value at inference time, so the output\n    // shape is unknown.\n    TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(InferenceContext::kUnknownDim),\n                                      subshape, &out));\n  } else {\n    auto dim0_value = dim0->scalar<int32>()();\n    if (dim0_value < 0) {\n      return errors::InvalidArgument(\n          \"Cannot specify a negative value for num_segments\");\n    }\n    TF_RETURN_IF_ERROR(c->Concatenate(c->Vector(dim0_value), subshape, &out));\n  }\n  c->set_output(0, out);\n  return Status::OK();\n}\n}  // namespace\n\nREGISTER_OP(\"SegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMean\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentProd\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMin\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"SegmentMax\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .SetShapeFn(SegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentMax\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentMin\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"UnsortedSegmentProd\")\n    .Input(\"data: T\")\n    .Input(\"segment_ids: Tindices\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tindices: {int32,int64}\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnsortedSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSum\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSumWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: realnumbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentSumGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"SparseSegmentMean\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentMeanWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentMeanGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtN\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtNWithNumSegments\")\n    .Input(\"data: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"num_segments: Tnumsegments\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tnumsegments: {int32,int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionWithNumSegmentsShapeFn);\n\nREGISTER_OP(\"SparseSegmentSqrtNGrad\")\n    .Input(\"grad: T\")\n    .Input(\"indices: Tidx\")\n    .Input(\"segment_ids: Tsegmentids\")\n    .Input(\"output_dim0: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .Attr(\"Tsegmentids: {int32, int64} = DT_INT32\")\n    .SetShapeFn(SparseSegmentReductionGradShapeFn);\n\nREGISTER_OP(\"All\")\n    .Input(\"input: bool\")\n    .Input(\"reduction_indices: Tidx\")\n    .Output(\"output: bool\")\n    .Attr(\"keep_dims: bool = false\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\nREGISTER_OP(\"Any\")\n    .Input(\"input: bool\")\n    .Input(\"reduction_indices: Tidx\")\n    .Attr(\"keep_dims: bool = false\")\n    .Output(\"output: bool\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::ReductionShape);\n\n// --------------------------------------------------------------------------\n\nnamespace {\n\ntemplate <typename T>\nStatus RangeSize(const Tensor* start_t, const Tensor* limit_t,\n                 const Tensor* delta_t, InferenceContext* const c) {\n  T start = start_t->scalar<T>()();\n  T limit = limit_t->scalar<T>()();\n  T delta = delta_t->scalar<T>()();\n  if (start > limit && delta > T(0)) {\n    return errors::InvalidArgument(\n        \"Requires start <= limit when delta > 0: \", start, \"/\", limit);\n  }\n  if (start < limit && delta < T(0)) {\n    return errors::InvalidArgument(\n        \"Requires start >= limit when delta < 0: \", start, \"/\", limit);\n  }\n  if (delta == T(0)) {\n    return errors::InvalidArgument(\"Requires delta != 0\");\n  }\n\n  auto size = (std::is_integral<T>::value\n                   ? ((Eigen::numext::abs(limit - start) +\n                       Eigen::numext::abs(delta) - T(1)) /\n                      Eigen::numext::abs(delta))\n                   : (Eigen::numext::ceil(\n                         Eigen::numext::abs((limit - start) / delta))));\n  c->set_output(0, c->Vector(static_cast<int64_t>(size)));\n  return Status::OK();\n}\n\n}  // namespace\n\nREGISTER_OP(\"Range\")\n    .Input(\"start: Tidx\")\n    .Input(\"limit: Tidx\")\n    .Input(\"delta: Tidx\")\n    .Output(\"output: Tidx\")\n    .Attr(\n        \"Tidx: \"\n        \"{bfloat16, half, float, double, int8, int16, int32, int64, uint32} = \"\n        \"DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(0), 0, &unused),\n                                      \" for 'start'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(1), 0, &unused),\n                                      \" for 'limit'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(2), 0, &unused),\n                                      \" for 'delta'\");\n      const Tensor* start_t = c->input_tensor(0);\n      const Tensor* limit_t = c->input_tensor(1);\n      const Tensor* delta_t = c->input_tensor(2);\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (start_t == nullptr || limit_t == nullptr || delta_t == nullptr) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return Status::OK();\n      }\n      if (dtype == DT_INT32) {\n        return RangeSize<int32>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT16) {\n        return RangeSize<int16>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT8) {\n        return RangeSize<int8>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_INT64) {\n        return RangeSize<int64_t>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_UINT32) {\n        return RangeSize<uint32>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_FLOAT) {\n        return RangeSize<float>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_DOUBLE) {\n        return RangeSize<double>(start_t, limit_t, delta_t, c);\n      } else if (dtype == DT_BFLOAT16) {\n        return RangeSize<bfloat16>(start_t, limit_t, delta_t, c);\n      } else {\n        return errors::InvalidArgument(\"Unsupported dtype\", dtype);\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"LinSpace\")\n    .Input(\"start: T\")\n    .Input(\"stop: T\")\n    .Input(\"num: Tidx\")\n    .Output(\"output: T\")\n    .Attr(\"T: {bfloat16, half, float, double}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(0), 0, &unused),\n                                      \" for 'start'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(1), 0, &unused),\n                                      \" for 'stop'\");\n      TF_RETURN_WITH_CONTEXT_IF_ERROR(c->WithRank(c->input(2), 0, &unused),\n                                      \" for 'num'\");\n      const Tensor* num_t = c->input_tensor(2);\n      if (num_t == nullptr) {\n        c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n        return Status::OK();\n      }\n\n      int64_t num;\n      if (num_t->dtype() == DT_INT32) {\n        num = num_t->scalar<int32>()();\n      } else {\n        num = num_t->scalar<int64_t>()();\n      }\n      if (num <= 0) return errors::InvalidArgument(\"Requires num > 0: \", num);\n      c->set_output(0, c->Vector(num));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Complex\")\n    .Input(\"real: T\")\n    .Input(\"imag: T\")\n    .Output(\"out: Tout\")\n    .Attr(\"T: {float, double} = DT_FLOAT\")\n    .Attr(\"Tout: {complex64, complex128} = DT_COMPLEX64\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"Real\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Imag\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Angle\")\n    .Input(\"input: T\")\n    .Output(\"output: Tout\")\n    .Attr(\"T: {complex64, complex128} = DT_COMPLEX64\")\n    .Attr(\"Tout: {float, double} = DT_FLOAT\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Conj\")\n    .Input(\"input: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: {complex64, complex128, variant} = DT_COMPLEX64\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->input(0));\n      auto* handle_data = c->input_handle_shapes_and_types(0);\n      if (handle_data != nullptr) {\n        c->set_output_handle_shapes_and_types(0, *handle_data);\n      }\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Cross\")\n    .Input(\"a: T\")\n    .Input(\"b: T\")\n    .Output(\"product: T\")\n    .Attr(\"T: realnumbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle a_shape;\n      ShapeHandle b_shape;\n      // * Input rank >= 1.\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &a_shape));\n      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(1), 1, &b_shape));\n\n      // * Both inputs have the same shape.\n      TF_RETURN_IF_ERROR(c->Merge(a_shape, b_shape, &a_shape));\n\n      // * input_shape[-1] == 3.\n      if (c->RankKnown(a_shape)) {\n        int rank = c->Rank(a_shape);\n        auto dim = c->Dim(a_shape, rank - 1);\n        TF_RETURN_IF_ERROR(c->WithValue(dim, 3, &dim));\n      }\n      c->set_output(0, a_shape);\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"HistogramFixedWidth\")\n    .Input(\"values: T\")\n    .Input(\"value_range: T\")\n    .Input(\"nbins: int32\")\n    .Output(\"out: dtype\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"dtype: {int32, int64} = DT_INT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      // value_range should be a vector.\n      ShapeHandle value_range_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &value_range_shape));\n      // value_range should have two elements.\n      DimensionHandle unused;\n      TF_RETURN_IF_ERROR(\n          c->WithValue(c->Dim(value_range_shape, 0), 2, &unused));\n      // nbins should be a scalar.\n      ShapeHandle nbins_shape;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &nbins_shape));\n\n      // If nbins is available, set the shape from nbins.\n      const Tensor* nbins_input = c->input_tensor(2);\n      if (nbins_input != nullptr) {\n        int64_t nbins;\n        TF_RETURN_IF_ERROR(c->GetScalarFromTensor(nbins_input, &nbins));\n        // nbins has to be positive.\n        if (nbins <= 0) {\n          return errors::InvalidArgument(\"Requires nbins > 0: \", nbins);\n        }\n        c->set_output(0, c->Vector(nbins));\n      } else {\n        c->set_output(0, c->UnknownShapeOfRank(1));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Bincount\")\n    .Input(\"arr: int32\")\n    .Input(\"size: int32\")\n    .Input(\"weights: T\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Output(\"bins: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // The input `size` must be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* size_tensor = c->input_tensor(1);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShapeOfRank(1));\n        return Status::OK();\n      }\n\n      if (size_tensor->dims() != 0) {\n        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                       size_tensor->dims());\n      }\n\n      // Return `[size]` shape if size is known.\n      int32_t size_val = size_tensor->scalar<int32>()();\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n      c->set_output(0, c->MakeShape({size_val}));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"DenseBincount\")\n    .Input(\"input: Tidx\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // The input `input` must be at most matrix.\n      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 2, &unused));\n      // The input `size` must be a scalar.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n\n      const Tensor* size_tensor = c->input_tensor(1);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n      if (size_tensor->dims() != 0) {\n        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                       size_tensor->dims());\n      }\n\n      int64_t size_val;\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (dtype == DT_INT32) {\n        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n      } else if (dtype == DT_INT64) {\n        size_val = size_tensor->scalar<int64_t>()();\n      } else {\n        return errors::InvalidArgument(\"size dtype must be int32 or int64\");\n      }\n      // Return `[size]` shape if size is known.\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n      if (c->Rank(c->input(0)) == 1) {\n        c->set_output(0, c->MakeShape({size_val}));\n      } else if (c->Rank(c->input(0)) == 2) {\n        c->set_output(0, c->MakeShape({c->Dim(c->input(0), 0), size_val}));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"SparseBincount\")\n    .Input(\"indices: int64\")\n    .Input(\"values: Tidx\")\n    .Input(\"dense_shape: int64\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      const Tensor* size_tensor = c->input_tensor(3);\n      if (size_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n      if (size_tensor->dims() != 0) {\n        return errors::InvalidArgument(\"Shape must be rank 0 but is rank \",\n                                       size_tensor->dims());\n      }\n\n      int64_t size_val;\n      DataType dtype;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"Tidx\", &dtype));\n      if (dtype == DT_INT32) {\n        size_val = static_cast<int64_t>(size_tensor->scalar<int32>()());\n      } else if (dtype == DT_INT64) {\n        size_val = size_tensor->scalar<int64_t>()();\n      } else {\n        return errors::InvalidArgument(\"size dtype must be int32 or int64\");\n      }\n      // Return `[size]` shape if size is known.\n      if (size_val < 0) {\n        return errors::InvalidArgument(\"size (\", size_val,\n                                       \") must be non-negative\");\n      }\n\n      const Tensor* shape_tensor = c->input_tensor(2);\n      if (shape_tensor == nullptr) {\n        // Return unknown shape if size is not known.\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n      if (shape_tensor->NumElements() == 1) {\n        c->set_output(0, c->MakeShape({size_val}));\n      } else if (shape_tensor->NumElements() == 2) {\n        c->set_output(\n            0, c->MakeShape({shape_tensor->flat<int64_t>()(0), size_val}));\n      } else {\n        return errors::InvalidArgument(\"Input must be less than rank 2\");\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"RaggedBincount\")\n    .Input(\"splits: int64\")\n    .Input(\"values: Tidx\")\n    .Input(\"size: Tidx\")\n    .Input(\"weights: T\")\n    .Attr(\"Tidx: {int32, int64}\")\n    .Attr(\"T: {int32, int64, float32, float64}\")\n    .Attr(\"binary_output: bool = false\")\n    .Output(\"output: T\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Cumsum\")\n    .Input(\"x: T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"Cumprod\")\n    .Input(\"x: T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: numbertype\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"CumulativeLogsumexp\")\n    .Input(\"x : T\")\n    .Input(\"axis: Tidx\")\n    .Attr(\"exclusive: bool = false\")\n    .Attr(\"reverse: bool = false\")\n    .Output(\"out: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"Tidx: {int32, int64} = DT_INT32\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"QuantizedMatMul\")\n    .Input(\"a: T1\")\n    .Input(\"b: T2\")\n    .Input(\"min_a: float\")\n    .Input(\"max_a: float\")\n    .Input(\"min_b: float\")\n    .Input(\"max_b: float\")\n    .Output(\"out: Toutput\")\n    .Output(\"min_out: float\")\n    .Output(\"max_out: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .Attr(\"transpose_a: bool = false\")\n    .Attr(\"transpose_b: bool = false\")\n    .Attr(\"Tactivation: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::MatMulShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"QuantizedMul\")\n    .Input(\"x: T1\")\n    .Input(\"y: T2\")\n    .Input(\"min_x: float\")\n    .Input(\"max_x: float\")\n    .Input(\"min_y: float\")\n    .Input(\"max_y: float\")\n    .Output(\"z: Toutput\")\n    .Output(\"min_z: float\")\n    .Output(\"max_z: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BroadcastBinaryOpShapeFn(c));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"QuantizedAdd\")\n    .Input(\"x: T1\")\n    .Input(\"y: T2\")\n    .Input(\"min_x: float\")\n    .Input(\"max_x: float\")\n    .Input(\"min_y: float\")\n    .Input(\"max_y: float\")\n    .Output(\"z: Toutput\")\n    .Output(\"min_z: float\")\n    .Output(\"max_z: float\")\n    .Attr(\"T1: quantizedtype\")\n    .Attr(\"T2: quantizedtype\")\n    .Attr(\"Toutput: quantizedtype = DT_QINT32\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::BroadcastBinaryOpShapeFn(c));\n      // min_x, max_x, min_y, max_y should be scalar.\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(5), 0, &unused));\n\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"QuantizeDownAndShrinkRange\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"Requantize\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Input(\"requested_output_min: float\")\n    .Input(\"requested_output_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .Attr(\"out_type: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"RequantizationRange\")\n    .Input(\"input: Tinput\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"Tinput: quantizedtype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n      c->set_output(0, c->Scalar());\n      c->set_output(1, c->Scalar());\n      return Status::OK();\n    });\n\n// --------------------------------------------------------------------------\n\nREGISTER_OP(\"Bucketize\")\n    .Input(\"input: T\")\n    .Output(\"output: int32\")\n    .Attr(\"T: {int32, int64, float, double}\")\n    .Attr(\"boundaries: list(float)\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"ClipByValue\")\n    .Input(\"t: T\")\n    .Input(\"clip_value_min: T\")\n    .Input(\"clip_value_max: T\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\n#ifdef INTEL_MKL\n// Note: This op is not commutative w.r.t. to all its inputs.\nREGISTER_OP(\"_MklAddN\")\n    .Input(\"inputs: N * T\")\n    .Input(\"mkl_input: N * uint8\")\n    .Output(\"sum: T\")\n    .Output(\"mkl_sum: uint8\")\n    .Attr(\"N: int >= 1\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle cur = c->input(c->num_inputs() - 1);\n      for (int i = c->num_inputs() - 2; i >= 0; --i) {\n        TF_RETURN_WITH_CONTEXT_IF_ERROR(c->Merge(c->input(i), cur, &cur),\n                                        \"From merging shape \", i,\n                                        \" with other shapes.\");\n      }\n      c->set_output(0, cur);\n      return Status::OK();\n    })\n    .Doc(R\"doc(\nAdd two input tensors element wise using mkl kernel sum.\ninputs: Must all be the same size and shape.\n)doc\");\n\n#endif  // INTEL_MKL\n\nREGISTER_OP(\"RequantizePerChannel\")\n    .Input(\"input: T\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Input(\"requested_output_min: float\")\n    .Input(\"requested_output_max: float\")\n    .Output(\"output: out_type\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype = DT_QINT32\")\n    .Attr(\"out_type: quantizedtype = DT_QUINT8\")\n    .SetShapeFn([](InferenceContext* c) {\n      TF_RETURN_IF_ERROR(shape_inference::UnchangedShape(c));\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n      c->set_output(1, c->Scalar());\n      c->set_output(2, c->Scalar());\n      return Status::OK();\n    });\nREGISTER_OP(\"RequantizationRangePerChannel\")\n    .Input(\"input: T\")\n    .Input(\"input_min: float\")\n    .Input(\"input_max: float\")\n    .Output(\"output_min: float\")\n    .Output(\"output_max: float\")\n    .Attr(\"T: quantizedtype = DT_QINT32\")\n    .Attr(\"clip_value_max: float\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n      c->set_output(0, c->Scalar());\n      c->set_output(1, c->Scalar());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"NextAfter\")\n    .Attr(\"T: {float64, float32} = DT_FLOAT\")\n    .Input(\"x1: T\")\n    .Input(\"x2: T\")\n    .Output(\"output: T\")\n    .SetShapeFn(shape_inference::BroadcastBinaryOpShapeFn);\n\nREGISTER_OP(\"SobolSample\")\n    .Input(\"dim: int32\")\n    .Input(\"num_results: int32\")\n    .Input(\"skip: int32\")\n    .Attr(\"dtype: {float, double} = DT_FLOAT\")\n    .Output(\"samples: dtype\")\n    .SetShapeFn([](shape_inference::InferenceContext* c) {\n      ShapeHandle unused;\n\n      // inputs must be scalars\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n\n      const Tensor* dim_t = c->input_tensor(0);\n      const Tensor* num_results_t = c->input_tensor(1);\n\n      int32_t dim = dim_t == nullptr ? InferenceContext::kUnknownDim\n                                     : dim_t->scalar<int32>()();\n\n      int32_t num_results = num_results_t == nullptr\n                                ? InferenceContext::kUnknownDim\n                                : num_results_t->scalar<int32>()();\n\n      c->set_output(0, c->Matrix(num_results, dim));\n      return Status::OK();\n    });\n\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for bincount_ops.bincount.\"\"\"\nfrom absl.testing import parameterized\nimport numpy as np\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import bincount_ops\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops.ragged import ragged_factory_ops\nfrom tensorflow.python.ops.ragged import ragged_tensor\nfrom tensorflow.python.platform import googletest\n\n\nclass BincountTest(test_util.TensorFlowTestCase):\n\n  def test_empty(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=5)),\n          [0, 0, 0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=1)), [0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], minlength=0)), [])\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=0, dtype=np.float32)).dtype,\n          np.float32)\n      self.assertEqual(\n          self.evaluate(\n              bincount_ops.bincount([], minlength=3, dtype=np.float64)).dtype,\n          np.float64)\n\n  def test_values(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1, 1, 1, 2, 2, 3])),\n          [0, 3, 2, 1])\n      arr = [1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [0, 5, 4, 3, 2, 1])\n      arr += [0, 0, 0, 0, 0, 0]\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(arr)), [6, 5, 4, 3, 2, 1])\n\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([])), [])\n      self.assertAllEqual(self.evaluate(bincount_ops.bincount([0, 0, 0])), [3])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5])), [0, 0, 0, 0, 0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(10000))),\n          np.ones(10000))\n\n  def test_maxlength(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([5], maxlength=3)), [0, 0, 0])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([1], maxlength=3)), [0, 1])\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount([], maxlength=3)), [])\n\n  def test_random_with_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [dtypes.int32, dtypes.int64, dtypes.float32, dtypes.float64]:\n        arr = np.random.randint(0, 1000, num_samples)\n        if dtype == dtypes.int32 or dtype == dtypes.int64:\n          weights = np.random.randint(-100, 100, num_samples)\n        else:\n          weights = np.random.random(num_samples)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, weights)),\n            np.bincount(arr, weights))\n\n  def test_random_without_weights(self):\n    num_samples = 10000\n    with self.session():\n      np.random.seed(42)\n      for dtype in [np.int32, np.float32]:\n        arr = np.random.randint(0, 1000, num_samples)\n        weights = np.ones(num_samples).astype(dtype)\n        self.assertAllClose(\n            self.evaluate(bincount_ops.bincount(arr, None)),\n            np.bincount(arr, weights))\n\n  @test_util.run_gpu_only\n  def test_bincount_determinism_error(self):\n    arr = np.random.randint(0, 1000, size=1000)\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of Bincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=None))\n    arr = np.random.randint(0, 1000, size=(100, 100))\n    with test_util.deterministic_ops(), self.assertRaisesRegex(\n        errors_impl.UnimplementedError,\n        \"Determinism is not yet supported in GPU implementation of \"\n        \"DenseBincount.\"):\n      self.evaluate(bincount_ops.bincount(arr, None, axis=-1))\n\n  def test_zero_weights(self):\n    with self.session():\n      self.assertAllEqual(\n          self.evaluate(bincount_ops.bincount(np.arange(1000), np.zeros(1000))),\n          np.zeros(1000))\n\n  def test_negative(self):\n    # unsorted_segment_sum will only report InvalidArgumentError on CPU\n    with self.cached_session(), ops.device(\"/CPU:0\"):\n      with self.assertRaises(errors.InvalidArgumentError):\n        self.evaluate(bincount_ops.bincount([1, 2, 3, -1, 6, 8]))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_shape_function(self):\n    # size must be scalar.\n    with self.assertRaisesRegex(\n        (ValueError, errors.InvalidArgumentError),\n        \"Shape must be rank 0 but is rank 1(?s).*Bincount\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], [1], [])\n    # size must be positive.\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be non-negative\"):\n      gen_math_ops.bincount([1, 2, 3, 1, 6, 8], -5, [])\n    # if size is a constant then the shape is known.\n    v1 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], 5, [])\n    self.assertAllEqual(v1.get_shape().as_list(), [5])\n    # if size is a placeholder then the shape is unknown.\n    with ops.Graph().as_default():\n      s = array_ops.placeholder(dtype=dtypes.int32)\n      v2 = gen_math_ops.bincount([1, 2, 3, 1, 6, 8], s, [])\n      self.assertAllEqual(v2.get_shape().as_list(), [None])\n\n\nclass BincountOpTest(test_util.TensorFlowTestCase, parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.bincount(inp, minlength=size)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 1000\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.bincount(inp, minlength=size, weights=np_weight)\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096), dtype=dtype)\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_bincount_all_binary_with_weights(self, dtype):\n    np.random.seed(42)\n    size = 10\n    inp = np.random.randint(0, size, (4096,), dtype=dtype)\n    np_weight = np.random.random((4096,))\n    np_out = np.ones((size,))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size, binary_output=True)))\n\n  def _test_bincount_col_count(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(input=inp, weights=[], size=size)))\n\n  def _test_bincount_col_binary(self, num_rows, num_cols, size, dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=[], size=size, binary_output=True)))\n\n  def _test_bincount_col_count_with_weights(self, num_rows, num_cols, size,\n                                            dtype):\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    with test_util.use_gpu():\n      self.assertAllEqual(\n          np_out,\n          self.evaluate(\n              gen_math_ops.dense_bincount(\n                  input=inp, weights=np_weight, size=size)))\n\n  def test_col_reduce_basic(self):\n    with test_util.use_gpu():\n      v = self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[[1, 2, 3], [0, 3, 2]], weights=[], size=4))\n    expected_out = [[0., 1., 1., 1.], [1., 0., 1., 1.]]\n    self.assertAllEqual(expected_out, v)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 10\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_shared_memory_with_weights(self, dtype):\n    # num_rows * num_bins less than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_global_memory_with_weights(self, dtype):\n    # num_rows * num_bins more than half of max shared memory.\n    num_rows = 128\n    num_cols = 27\n    size = 1024\n    self._test_bincount_col_count_with_weights(num_rows, num_cols, size, dtype)\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 7\n    size = 10\n    self._test_bincount_col_binary(num_rows, num_cols, size, dtype)\n\n  def test_invalid_rank(self):\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"at most rank 2\"):\n      with test_util.use_gpu():\n        self.evaluate(\n            gen_math_ops.dense_bincount(\n                input=[[[1, 2, 3], [0, 3, 2]]], weights=[], size=10))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.dense_bincount(\n              input=[0], size=[1, 1], weights=[3], binary_output=False))\n\n\nclass SparseBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.bincount(inp_vals, minlength=size)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_count_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 1000\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.bincount(inp_vals, minlength=size, weights=inp_weight)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_all_binary_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    size = 10\n    n_elems = 4096\n    inp_indices = np.random.randint(0, num_rows, (n_elems,))\n    inp_vals = np.random.randint(0, size, (n_elems,), dtype=dtype)\n    inp_weight = np.random.random((n_elems,))\n\n    np_out = np.ones((size,))\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_indices,\n                values=inp_vals,\n                dense_shape=[num_rows],\n                size=size,\n                weights=inp_weight,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_count(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[])))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_sparse_bincount_col_reduce_binary(self, dtype):\n    num_rows = 128\n    num_cols = 27\n    size = 100\n    np.random.seed(42)\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    # from_dense will filter out 0s.\n    inp = inp + 1\n    # from_dense will cause OOM in GPU.\n    with ops.device(\"/CPU:0\"):\n      inp_sparse = sparse_ops.from_dense(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.sparse_bincount(\n                indices=inp_sparse.indices,\n                values=inp_sparse.values - 1,\n                dense_shape=inp_sparse.dense_shape,\n                size=size,\n                weights=[],\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.sparse_bincount(\n              indices=[[0], [1]],\n              values=[0, 0],\n              dense_shape=[1, 1],\n              size=[1, 1],\n              weights=[0, 0],\n              binary_output=False))\n\n\nclass RaggedBincountOpTest(test_util.TensorFlowTestCase,\n                           parameterized.TestCase):\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 2, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0,\n                                            0], [1, 1, 0, 1, 0, 0],\n                       [0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1]]\n    self.assertAllEqual(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=6,\n                binary_output=True)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_with_weights(self, dtype):\n    x = ragged_factory_ops.constant([[], [], [3, 0, 1], [], [5, 0, 4, 4]])\n    weights = ragged_factory_ops.constant([[], [], [.1, .2, .3], [],\n                                           [.2, .5, .6, .3]])\n    expected_output = [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.2, .3, 0, .1, 0, 0], [0, 0, 0, 0, 0, 0],\n                       [.5, 0, 0, 0, .9, .2]]\n    self.assertAllClose(\n        expected_output,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=weights.values,\n                size=6)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate(\n            [np.bincount(inp[j, :], minlength=size) for j in range(num_rows)],\n            axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits, values=x.values, weights=[], size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_count_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_weight = np.random.random((num_rows, num_cols))\n    np_out = np.reshape(\n        np.concatenate([\n            np.bincount(inp[j, :], weights=np_weight[j, :], minlength=size)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=np_weight,\n                size=size)))\n\n  @parameterized.parameters([{\n      \"dtype\": np.int32,\n  }, {\n      \"dtype\": np.int64,\n  }])\n  def test_ragged_bincount_binary_np_with_weights(self, dtype):\n    np.random.seed(42)\n    num_rows = 128\n    num_cols = 27\n    size = 1000\n    inp = np.random.randint(0, size, (num_rows, num_cols), dtype=dtype)\n    np_out = np.reshape(\n        np.concatenate([\n            np.where(np.bincount(inp[j, :], minlength=size) > 0, 1, 0)\n            for j in range(num_rows)\n        ],\n                       axis=0), (num_rows, size))\n    x = ragged_tensor.RaggedTensor.from_tensor(inp)\n    self.assertAllEqual(\n        np_out,\n        self.evaluate(\n            gen_math_ops.ragged_bincount(\n                splits=x.row_splits,\n                values=x.values,\n                weights=[],\n                size=size,\n                binary_output=True)))\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_size_is_not_scalar(self):  # b/206619828\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Shape must be rank 0 but is rank 1\"):\n      self.evaluate(\n          gen_math_ops.ragged_bincount(\n              splits=[0, 0, 1],\n              values=[1],\n              size=[1, 1],\n              weights=[0, 0, 0],\n              binary_output=False,\n              name=None))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/bincount_op.cc", "tensorflow/core/ops/math_ops.cc", "tensorflow/python/kernel_tests/math_ops/bincount_op_test.py"], "buggy_code_start_loc": [278, 1701, 346], "buggy_code_end_loc": [464, 1772, 652], "fixing_code_start_loc": [279, 1702, 347], "fixing_code_end_loc": [474, 1786, 687], "type": "CWE-754", "message": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `*Bincount` operations allows malicious users to cause denial of service by passing in arguments which would trigger a `CHECK`-fail. There are several conditions that the input arguments must satisfy. Some are not caught during shape inference and others are not caught during kernel implementation. This results in `CHECK` failures later when the output tensors get allocated. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-21737", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-03T14:15:08.363", "lastModified": "2022-02-09T05:08:56.793", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Tensorflow is an Open Source Machine Learning Framework. The implementation of `*Bincount` operations allows malicious users to cause denial of service by passing in arguments which would trigger a `CHECK`-fail. There are several conditions that the input arguments must satisfy. Some are not caught during shape inference and others are not caught during kernel implementation. This results in `CHECK` failures later when the output tensors get allocated. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range."}, {"lang": "es", "value": "Tensorflow es un marco de aprendizaje autom\u00e1tico de c\u00f3digo abierto. La implementaci\u00f3n de las operaciones \"*Bincount\" permite a usuarios maliciosos causar una denegaci\u00f3n de servicio al pasar argumentos que desencadenen un fallo de \"CHECK\". Se presentan varias condiciones que los argumentos de entrada deben satisfacer. Algunas no son detectadas durante la inferencia de la forma y otras no son detectadas durante la implementaci\u00f3n del n\u00facleo. Esto resulta en fallos de \"CHECK\" m\u00e1s tarde, cuando son asignados los tensores de salida. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.8.0. Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.7.1, TensorFlow versi\u00f3n 2.6.3, y TensorFlow versi\u00f3n 2.5.3, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-754"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.5.2", "matchCriteriaId": "688150BF-477C-48FC-9AEF-A79AC57A6DDC"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndIncluding": "2.6.2", "matchCriteriaId": "C9E69B60-8C97-47E2-9027-9598B8392E5D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:*:*:*:*:*:*:*", "matchCriteriaId": "2EDFAAB8-799C-4259-9102-944D4760DA2C"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/5100e359aef5c8021f2e71c7b986420b85ce7b3d/tensorflow/core/kernels/bincount_op.cc", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/7019ce4f68925fd01cdafde26f8d8c938f47e6f9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-f2vv-v9cg-qhh7", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/7019ce4f68925fd01cdafde26f8d8c938f47e6f9"}}