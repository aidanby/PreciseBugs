{"buggy_code": ["/*\n * NETLINK      Kernel-user communication protocol.\n *\n * \t\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n * \t\t\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n * \t\t\t\tPatrick McHardy <kaber@trash.net>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n * Tue Jun 26 14:36:48 MEST 2001 Herbert \"herp\" Rosmanith\n *                               added netlink_proto_exit\n * Tue Jan 22 18:32:44 BRST 2002 Arnaldo C. de Melo <acme@conectiva.com.br>\n * \t\t\t\t use nlk_sk, as sk->protinfo is on a diet 8)\n * Fri Jul 22 19:51:12 MEST 2005 Harald Welte <laforge@gnumonks.org>\n * \t\t\t\t - inc module use count of module that owns\n * \t\t\t\t   the kernel socket in case userspace opens\n * \t\t\t\t   socket of same protocol\n * \t\t\t\t - remove all module support, since netlink is\n * \t\t\t\t   mandatory if CONFIG_NET=y these days\n */\n\n#include <linux/module.h>\n\n#include <linux/capability.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/stat.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/fcntl.h>\n#include <linux/termios.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/security.h>\n#include <linux/jhash.h>\n#include <linux/jiffies.h>\n#include <linux/random.h>\n#include <linux/bitops.h>\n#include <linux/mm.h>\n#include <linux/types.h>\n#include <linux/audit.h>\n#include <linux/mutex.h>\n#include <linux/vmalloc.h>\n#include <linux/if_arp.h>\n#include <linux/rhashtable.h>\n#include <asm/cacheflush.h>\n#include <linux/hash.h>\n#include <linux/genetlink.h>\n\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/scm.h>\n#include <net/netlink.h>\n\n#include \"af_netlink.h\"\n\nstruct listeners {\n\tstruct rcu_head\t\trcu;\n\tunsigned long\t\tmasks[0];\n};\n\n/* state bits */\n#define NETLINK_S_CONGESTED\t\t0x0\n\n/* flags */\n#define NETLINK_F_KERNEL_SOCKET\t\t0x1\n#define NETLINK_F_RECV_PKTINFO\t\t0x2\n#define NETLINK_F_BROADCAST_SEND_ERROR\t0x4\n#define NETLINK_F_RECV_NO_ENOBUFS\t0x8\n#define NETLINK_F_LISTEN_ALL_NSID\t0x10\n#define NETLINK_F_CAP_ACK\t\t0x20\n\nstatic inline int netlink_is_kernel(struct sock *sk)\n{\n\treturn nlk_sk(sk)->flags & NETLINK_F_KERNEL_SOCKET;\n}\n\nstruct netlink_table *nl_table __read_mostly;\nEXPORT_SYMBOL_GPL(nl_table);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(nl_table_wait);\n\nstatic int netlink_dump(struct sock *sk);\nstatic void netlink_skb_destructor(struct sk_buff *skb);\n\n/* nl_table locking explained:\n * Lookup and traversal are protected with an RCU read-side lock. Insertion\n * and removal are protected with per bucket lock while using RCU list\n * modification primitives and may run in parallel to RCU protected lookups.\n * Destruction of the Netlink socket may only occur *after* nl_table_lock has\n * been acquired * either during or after the socket has been removed from\n * the list and after an RCU grace period.\n */\nDEFINE_RWLOCK(nl_table_lock);\nEXPORT_SYMBOL_GPL(nl_table_lock);\nstatic atomic_t nl_table_users = ATOMIC_INIT(0);\n\n#define nl_deref_protected(X) rcu_dereference_protected(X, lockdep_is_held(&nl_table_lock));\n\nstatic ATOMIC_NOTIFIER_HEAD(netlink_chain);\n\nstatic DEFINE_SPINLOCK(netlink_tap_lock);\nstatic struct list_head netlink_tap_all __read_mostly;\n\nstatic const struct rhashtable_params netlink_rhashtable_params;\n\nstatic inline u32 netlink_group_mask(u32 group)\n{\n\treturn group ? 1 << (group - 1) : 0;\n}\n\nstatic struct sk_buff *netlink_to_full_skb(const struct sk_buff *skb,\n\t\t\t\t\t   gfp_t gfp_mask)\n{\n\tunsigned int len = skb_end_offset(skb);\n\tstruct sk_buff *new;\n\n\tnew = alloc_skb(len, gfp_mask);\n\tif (new == NULL)\n\t\treturn NULL;\n\n\tNETLINK_CB(new).portid = NETLINK_CB(skb).portid;\n\tNETLINK_CB(new).dst_group = NETLINK_CB(skb).dst_group;\n\tNETLINK_CB(new).creds = NETLINK_CB(skb).creds;\n\n\tmemcpy(skb_put(new, len), skb->data, len);\n\treturn new;\n}\n\nint netlink_add_tap(struct netlink_tap *nt)\n{\n\tif (unlikely(nt->dev->type != ARPHRD_NETLINK))\n\t\treturn -EINVAL;\n\n\tspin_lock(&netlink_tap_lock);\n\tlist_add_rcu(&nt->list, &netlink_tap_all);\n\tspin_unlock(&netlink_tap_lock);\n\n\t__module_get(nt->module);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netlink_add_tap);\n\nstatic int __netlink_remove_tap(struct netlink_tap *nt)\n{\n\tbool found = false;\n\tstruct netlink_tap *tmp;\n\n\tspin_lock(&netlink_tap_lock);\n\n\tlist_for_each_entry(tmp, &netlink_tap_all, list) {\n\t\tif (nt == tmp) {\n\t\t\tlist_del_rcu(&nt->list);\n\t\t\tfound = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"__netlink_remove_tap: %p not found\\n\", nt);\nout:\n\tspin_unlock(&netlink_tap_lock);\n\n\tif (found)\n\t\tmodule_put(nt->module);\n\n\treturn found ? 0 : -ENODEV;\n}\n\nint netlink_remove_tap(struct netlink_tap *nt)\n{\n\tint ret;\n\n\tret = __netlink_remove_tap(nt);\n\tsynchronize_net();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(netlink_remove_tap);\n\nstatic bool netlink_filter_tap(const struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\t/* We take the more conservative approach and\n\t * whitelist socket protocols that may pass.\n\t */\n\tswitch (sk->sk_protocol) {\n\tcase NETLINK_ROUTE:\n\tcase NETLINK_USERSOCK:\n\tcase NETLINK_SOCK_DIAG:\n\tcase NETLINK_NFLOG:\n\tcase NETLINK_XFRM:\n\tcase NETLINK_FIB_LOOKUP:\n\tcase NETLINK_NETFILTER:\n\tcase NETLINK_GENERIC:\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int __netlink_deliver_tap_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct sk_buff *nskb;\n\tstruct sock *sk = skb->sk;\n\tint ret = -ENOMEM;\n\n\tdev_hold(dev);\n\n\tif (is_vmalloc_addr(skb->head))\n\t\tnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (nskb) {\n\t\tnskb->dev = dev;\n\t\tnskb->protocol = htons((u16) sk->sk_protocol);\n\t\tnskb->pkt_type = netlink_is_kernel(sk) ?\n\t\t\t\t PACKET_KERNEL : PACKET_USER;\n\t\tskb_reset_network_header(nskb);\n\t\tret = dev_queue_xmit(nskb);\n\t\tif (unlikely(ret > 0))\n\t\t\tret = net_xmit_errno(ret);\n\t}\n\n\tdev_put(dev);\n\treturn ret;\n}\n\nstatic void __netlink_deliver_tap(struct sk_buff *skb)\n{\n\tint ret;\n\tstruct netlink_tap *tmp;\n\n\tif (!netlink_filter_tap(skb))\n\t\treturn;\n\n\tlist_for_each_entry_rcu(tmp, &netlink_tap_all, list) {\n\t\tret = __netlink_deliver_tap_skb(skb, tmp->dev);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t}\n}\n\nstatic void netlink_deliver_tap(struct sk_buff *skb)\n{\n\trcu_read_lock();\n\n\tif (unlikely(!list_empty(&netlink_tap_all)))\n\t\t__netlink_deliver_tap(skb);\n\n\trcu_read_unlock();\n}\n\nstatic void netlink_deliver_tap_kernel(struct sock *dst, struct sock *src,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tif (!(netlink_is_kernel(dst) && netlink_is_kernel(src)))\n\t\tnetlink_deliver_tap(skb);\n}\n\nstatic void netlink_overrun(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (!(nlk->flags & NETLINK_F_RECV_NO_ENOBUFS)) {\n\t\tif (!test_and_set_bit(NETLINK_S_CONGESTED,\n\t\t\t\t      &nlk_sk(sk)->state)) {\n\t\t\tsk->sk_err = ENOBUFS;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\tatomic_inc(&sk->sk_drops);\n}\n\nstatic void netlink_rcv_wake(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (skb_queue_empty(&sk->sk_receive_queue))\n\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\tif (!test_bit(NETLINK_S_CONGESTED, &nlk->state))\n\t\twake_up_interruptible(&nlk->wait);\n}\n\nstatic void netlink_skb_destructor(struct sk_buff *skb)\n{\n\tif (is_vmalloc_addr(skb->head)) {\n\t\tif (!skb->cloned ||\n\t\t    !atomic_dec_return(&(skb_shinfo(skb)->dataref)))\n\t\t\tvfree(skb->head);\n\n\t\tskb->head = NULL;\n\t}\n\tif (skb->sk != NULL)\n\t\tsock_rfree(skb);\n}\n\nstatic void netlink_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tWARN_ON(skb->sk != NULL);\n\tskb->sk = sk;\n\tskb->destructor = netlink_skb_destructor;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, skb->truesize);\n}\n\nstatic void netlink_sock_destruct(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->cb_running) {\n\t\tif (nlk->cb.done)\n\t\t\tnlk->cb.done(&nlk->cb);\n\n\t\tmodule_put(nlk->cb.module);\n\t\tkfree_skb(nlk->cb.skb);\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tprintk(KERN_ERR \"Freeing alive netlink socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(nlk_sk(sk)->groups);\n}\n\n/* This lock without WQ_FLAG_EXCLUSIVE is good on UP and it is _very_ bad on\n * SMP. Look, when several writers sleep and reader wakes them up, all but one\n * immediately hit write lock and grab all the cpus. Exclusive sleep solves\n * this, _but_ remember, it adds useless work on UP machines.\n */\n\nvoid netlink_table_grab(void)\n\t__acquires(nl_table_lock)\n{\n\tmight_sleep();\n\n\twrite_lock_irq(&nl_table_lock);\n\n\tif (atomic_read(&nl_table_users)) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\tadd_wait_queue_exclusive(&nl_table_wait, &wait);\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tif (atomic_read(&nl_table_users) == 0)\n\t\t\t\tbreak;\n\t\t\twrite_unlock_irq(&nl_table_lock);\n\t\t\tschedule();\n\t\t\twrite_lock_irq(&nl_table_lock);\n\t\t}\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nl_table_wait, &wait);\n\t}\n}\n\nvoid netlink_table_ungrab(void)\n\t__releases(nl_table_lock)\n{\n\twrite_unlock_irq(&nl_table_lock);\n\twake_up(&nl_table_wait);\n}\n\nstatic inline void\nnetlink_lock_table(void)\n{\n\t/* read_lock() synchronizes us to netlink_table_grab */\n\n\tread_lock(&nl_table_lock);\n\tatomic_inc(&nl_table_users);\n\tread_unlock(&nl_table_lock);\n}\n\nstatic inline void\nnetlink_unlock_table(void)\n{\n\tif (atomic_dec_and_test(&nl_table_users))\n\t\twake_up(&nl_table_wait);\n}\n\nstruct netlink_compare_arg\n{\n\tpossible_net_t pnet;\n\tu32 portid;\n};\n\n/* Doing sizeof directly may yield 4 extra bytes on 64-bit. */\n#define netlink_compare_arg_len \\\n\t(offsetof(struct netlink_compare_arg, portid) + sizeof(u32))\n\nstatic inline int netlink_compare(struct rhashtable_compare_arg *arg,\n\t\t\t\t  const void *ptr)\n{\n\tconst struct netlink_compare_arg *x = arg->key;\n\tconst struct netlink_sock *nlk = ptr;\n\n\treturn nlk->portid != x->portid ||\n\t       !net_eq(sock_net(&nlk->sk), read_pnet(&x->pnet));\n}\n\nstatic void netlink_compare_arg_init(struct netlink_compare_arg *arg,\n\t\t\t\t     struct net *net, u32 portid)\n{\n\tmemset(arg, 0, sizeof(*arg));\n\twrite_pnet(&arg->pnet, net);\n\targ->portid = portid;\n}\n\nstatic struct sock *__netlink_lookup(struct netlink_table *table, u32 portid,\n\t\t\t\t     struct net *net)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, net, portid);\n\treturn rhashtable_lookup_fast(&table->hash, &arg,\n\t\t\t\t      netlink_rhashtable_params);\n}\n\nstatic int __netlink_insert(struct netlink_table *table, struct sock *sk)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(sk), nlk_sk(sk)->portid);\n\treturn rhashtable_lookup_insert_key(&table->hash, &arg,\n\t\t\t\t\t    &nlk_sk(sk)->node,\n\t\t\t\t\t    netlink_rhashtable_params);\n}\n\nstatic struct sock *netlink_lookup(struct net *net, int protocol, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[protocol];\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = __netlink_lookup(table, portid, net);\n\tif (sk)\n\t\tsock_hold(sk);\n\trcu_read_unlock();\n\n\treturn sk;\n}\n\nstatic const struct proto_ops netlink_ops;\n\nstatic void\nnetlink_update_listeners(struct sock *sk)\n{\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\tunsigned long mask;\n\tunsigned int i;\n\tstruct listeners *listeners;\n\n\tlisteners = nl_deref_protected(tbl->listeners);\n\tif (!listeners)\n\t\treturn;\n\n\tfor (i = 0; i < NLGRPLONGS(tbl->groups); i++) {\n\t\tmask = 0;\n\t\tsk_for_each_bound(sk, &tbl->mc_list) {\n\t\t\tif (i < NLGRPLONGS(nlk_sk(sk)->ngroups))\n\t\t\t\tmask |= nlk_sk(sk)->groups[i];\n\t\t}\n\t\tlisteners->masks[i] = mask;\n\t}\n\t/* this function is only called with the netlink table \"grabbed\", which\n\t * makes sure updates are visible before bind or setsockopt return. */\n}\n\nstatic int netlink_insert(struct sock *sk, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = nlk_sk(sk)->portid == portid ? 0 : -EBUSY;\n\tif (nlk_sk(sk)->bound)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (BITS_PER_LONG > 32 &&\n\t    unlikely(atomic_read(&table->hash.nelems) >= UINT_MAX))\n\t\tgoto err;\n\n\tnlk_sk(sk)->portid = portid;\n\tsock_hold(sk);\n\n\terr = __netlink_insert(table, sk);\n\tif (err) {\n\t\t/* In case the hashtable backend returns with -EBUSY\n\t\t * from here, it must not escape to the caller.\n\t\t */\n\t\tif (unlikely(err == -EBUSY))\n\t\t\terr = -EOVERFLOW;\n\t\tif (err == -EEXIST)\n\t\t\terr = -EADDRINUSE;\n\t\tsock_put(sk);\n\t\tgoto err;\n\t}\n\n\t/* We need to ensure that the socket is hashed and visible. */\n\tsmp_wmb();\n\tnlk_sk(sk)->bound = portid;\n\nerr:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void netlink_remove(struct sock *sk)\n{\n\tstruct netlink_table *table;\n\n\ttable = &nl_table[sk->sk_protocol];\n\tif (!rhashtable_remove_fast(&table->hash, &nlk_sk(sk)->node,\n\t\t\t\t    netlink_rhashtable_params)) {\n\t\tWARN_ON(atomic_read(&sk->sk_refcnt) == 1);\n\t\t__sock_put(sk);\n\t}\n\n\tnetlink_table_grab();\n\tif (nlk_sk(sk)->subscriptions) {\n\t\t__sk_del_bind_node(sk);\n\t\tnetlink_update_listeners(sk);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC)\n\t\tatomic_inc(&genl_sk_destructing_cnt);\n\tnetlink_table_ungrab();\n}\n\nstatic struct proto netlink_proto = {\n\t.name\t  = \"NETLINK\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct netlink_sock),\n};\n\nstatic int __netlink_create(struct net *net, struct socket *sock,\n\t\t\t    struct mutex *cb_mutex, int protocol,\n\t\t\t    int kern)\n{\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\n\tsock->ops = &netlink_ops;\n\n\tsk = sk_alloc(net, PF_NETLINK, GFP_KERNEL, &netlink_proto, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n\n\tsock_init_data(sock, sk);\n\n\tnlk = nlk_sk(sk);\n\tif (cb_mutex) {\n\t\tnlk->cb_mutex = cb_mutex;\n\t} else {\n\t\tnlk->cb_mutex = &nlk->cb_def_mutex;\n\t\tmutex_init(nlk->cb_mutex);\n\t}\n\tinit_waitqueue_head(&nlk->wait);\n\n\tsk->sk_destruct = netlink_sock_destruct;\n\tsk->sk_protocol = protocol;\n\treturn 0;\n}\n\nstatic int netlink_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t  int kern)\n{\n\tstruct module *module = NULL;\n\tstruct mutex *cb_mutex;\n\tstruct netlink_sock *nlk;\n\tint (*bind)(struct net *net, int group);\n\tvoid (*unbind)(struct net *net, int group);\n\tint err = 0;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (protocol < 0 || protocol >= MAX_LINKS)\n\t\treturn -EPROTONOSUPPORT;\n\n\tnetlink_lock_table();\n#ifdef CONFIG_MODULES\n\tif (!nl_table[protocol].registered) {\n\t\tnetlink_unlock_table();\n\t\trequest_module(\"net-pf-%d-proto-%d\", PF_NETLINK, protocol);\n\t\tnetlink_lock_table();\n\t}\n#endif\n\tif (nl_table[protocol].registered &&\n\t    try_module_get(nl_table[protocol].module))\n\t\tmodule = nl_table[protocol].module;\n\telse\n\t\terr = -EPROTONOSUPPORT;\n\tcb_mutex = nl_table[protocol].cb_mutex;\n\tbind = nl_table[protocol].bind;\n\tunbind = nl_table[protocol].unbind;\n\tnetlink_unlock_table();\n\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = __netlink_create(net, sock, cb_mutex, protocol, kern);\n\tif (err < 0)\n\t\tgoto out_module;\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(net, &netlink_proto, 1);\n\tlocal_bh_enable();\n\n\tnlk = nlk_sk(sock->sk);\n\tnlk->module = module;\n\tnlk->netlink_bind = bind;\n\tnlk->netlink_unbind = unbind;\nout:\n\treturn err;\n\nout_module:\n\tmodule_put(module);\n\tgoto out;\n}\n\nstatic void deferred_put_nlk_sk(struct rcu_head *head)\n{\n\tstruct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu);\n\n\tsock_put(&nlk->sk);\n}\n\nstatic int netlink_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnetlink_remove(sk);\n\tsock_orphan(sk);\n\tnlk = nlk_sk(sk);\n\n\t/*\n\t * OK. Socket is unlinked, any packets that arrive now\n\t * will be purged.\n\t */\n\n\t/* must not acquire netlink_table_lock in any way again before unbind\n\t * and notifying genetlink is done as otherwise it might deadlock\n\t */\n\tif (nlk->netlink_unbind) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nlk->ngroups; i++)\n\t\t\tif (test_bit(i, nlk->groups))\n\t\t\t\tnlk->netlink_unbind(sock_net(sk), i + 1);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC &&\n\t    atomic_dec_return(&genl_sk_destructing_cnt) == 0)\n\t\twake_up(&genl_sk_destructing_waitq);\n\n\tsock->sk = NULL;\n\twake_up_interruptible_all(&nlk->wait);\n\n\tskb_queue_purge(&sk->sk_write_queue);\n\n\tif (nlk->portid && nlk->bound) {\n\t\tstruct netlink_notify n = {\n\t\t\t\t\t\t.net = sock_net(sk),\n\t\t\t\t\t\t.protocol = sk->sk_protocol,\n\t\t\t\t\t\t.portid = nlk->portid,\n\t\t\t\t\t  };\n\t\tatomic_notifier_call_chain(&netlink_chain,\n\t\t\t\tNETLINK_URELEASE, &n);\n\t}\n\n\tmodule_put(nlk->module);\n\n\tif (netlink_is_kernel(sk)) {\n\t\tnetlink_table_grab();\n\t\tBUG_ON(nl_table[sk->sk_protocol].registered == 0);\n\t\tif (--nl_table[sk->sk_protocol].registered == 0) {\n\t\t\tstruct listeners *old;\n\n\t\t\told = nl_deref_protected(nl_table[sk->sk_protocol].listeners);\n\t\t\tRCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL);\n\t\t\tkfree_rcu(old, rcu);\n\t\t\tnl_table[sk->sk_protocol].module = NULL;\n\t\t\tnl_table[sk->sk_protocol].bind = NULL;\n\t\t\tnl_table[sk->sk_protocol].unbind = NULL;\n\t\t\tnl_table[sk->sk_protocol].flags = 0;\n\t\t\tnl_table[sk->sk_protocol].registered = 0;\n\t\t}\n\t\tnetlink_table_ungrab();\n\t}\n\n\tkfree(nlk->groups);\n\tnlk->groups = NULL;\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), &netlink_proto, -1);\n\tlocal_bh_enable();\n\tcall_rcu(&nlk->rcu, deferred_put_nlk_sk);\n\treturn 0;\n}\n\nstatic int netlink_autobind(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\ts32 portid = task_tgid_vnr(current);\n\tint err;\n\ts32 rover = -4096;\n\tbool ok;\n\nretry:\n\tcond_resched();\n\trcu_read_lock();\n\tok = !__netlink_lookup(table, portid, net);\n\trcu_read_unlock();\n\tif (!ok) {\n\t\t/* Bind collision, search negative portid values. */\n\t\tif (rover == -4096)\n\t\t\t/* rover will be in range [S32_MIN, -4097] */\n\t\t\trover = S32_MIN + prandom_u32_max(-4096 - S32_MIN);\n\t\telse if (rover >= -4096)\n\t\t\trover = -4097;\n\t\tportid = rover--;\n\t\tgoto retry;\n\t}\n\n\terr = netlink_insert(sk, portid);\n\tif (err == -EADDRINUSE)\n\t\tgoto retry;\n\n\t/* If 2 threads race to autobind, that is fine.  */\n\tif (err == -EBUSY)\n\t\terr = 0;\n\n\treturn err;\n}\n\n/**\n * __netlink_ns_capable - General netlink message capability test\n * @nsp: NETLINK_CB of the socket buffer holding a netlink command from userspace.\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in the user namespace @user_ns.\n */\nbool __netlink_ns_capable(const struct netlink_skb_parms *nsp,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn ((nsp->flags & NETLINK_SKB_DST) ||\n\t\tfile_ns_capable(nsp->sk->sk_socket->file, user_ns, cap)) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(__netlink_ns_capable);\n\n/**\n * netlink_ns_capable - General netlink message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in the user namespace @user_ns.\n */\nbool netlink_ns_capable(const struct sk_buff *skb,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn __netlink_ns_capable(&NETLINK_CB(skb), user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_ns_capable);\n\n/**\n * netlink_capable - Netlink global message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in all user namespaces.\n */\nbool netlink_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_capable);\n\n/**\n * netlink_net_capable - Netlink network namespace message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap over the network namespace of\n * the socket we received the message from.\n */\nbool netlink_net_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, sock_net(skb->sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_net_capable);\n\nstatic inline int netlink_allowed(const struct socket *sock, unsigned int flag)\n{\n\treturn (nl_table[sock->sk->sk_protocol].flags & flag) ||\n\t\tns_capable(sock_net(sock->sk)->user_ns, CAP_NET_ADMIN);\n}\n\nstatic void\nnetlink_update_subscriptions(struct sock *sk, unsigned int subscriptions)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->subscriptions && !subscriptions)\n\t\t__sk_del_bind_node(sk);\n\telse if (!nlk->subscriptions && subscriptions)\n\t\tsk_add_bind_node(sk, &nl_table[sk->sk_protocol].mc_list);\n\tnlk->subscriptions = subscriptions;\n}\n\nstatic int netlink_realloc_groups(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int groups;\n\tunsigned long *new_groups;\n\tint err = 0;\n\n\tnetlink_table_grab();\n\n\tgroups = nl_table[sk->sk_protocol].groups;\n\tif (!nl_table[sk->sk_protocol].registered) {\n\t\terr = -ENOENT;\n\t\tgoto out_unlock;\n\t}\n\n\tif (nlk->ngroups >= groups)\n\t\tgoto out_unlock;\n\n\tnew_groups = krealloc(nlk->groups, NLGRPSZ(groups), GFP_ATOMIC);\n\tif (new_groups == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tmemset((char *)new_groups + NLGRPSZ(nlk->ngroups), 0,\n\t       NLGRPSZ(groups) - NLGRPSZ(nlk->ngroups));\n\n\tnlk->groups = new_groups;\n\tnlk->ngroups = groups;\n out_unlock:\n\tnetlink_table_ungrab();\n\treturn err;\n}\n\nstatic void netlink_undo_bind(int group, long unsigned int groups,\n\t\t\t      struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint undo;\n\n\tif (!nlk->netlink_unbind)\n\t\treturn;\n\n\tfor (undo = 0; undo < group; undo++)\n\t\tif (test_bit(undo, &groups))\n\t\t\tnlk->netlink_unbind(sock_net(sk), undo + 1);\n}\n\nstatic int netlink_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\tint addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\tint err;\n\tlong unsigned int groups = nladdr->nl_groups;\n\tbool bound;\n\n\tif (addr_len < sizeof(struct sockaddr_nl))\n\t\treturn -EINVAL;\n\n\tif (nladdr->nl_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\n\t/* Only superuser is allowed to listen multicasts */\n\tif (groups) {\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tbound = nlk->bound;\n\tif (bound) {\n\t\t/* Ensure nlk->portid is up-to-date. */\n\t\tsmp_rmb();\n\n\t\tif (nladdr->nl_pid != nlk->portid)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (nlk->netlink_bind && groups) {\n\t\tint group;\n\n\t\tfor (group = 0; group < nlk->ngroups; group++) {\n\t\t\tif (!test_bit(group, &groups))\n\t\t\t\tcontinue;\n\t\t\terr = nlk->netlink_bind(net, group + 1);\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\tnetlink_undo_bind(group, groups, sk);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t/* No need for barriers here as we return to user-space without\n\t * using any of the bound attributes.\n\t */\n\tif (!bound) {\n\t\terr = nladdr->nl_pid ?\n\t\t\tnetlink_insert(sk, nladdr->nl_pid) :\n\t\t\tnetlink_autobind(sock);\n\t\tif (err) {\n\t\t\tnetlink_undo_bind(nlk->ngroups, groups, sk);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (!groups && (nlk->groups == NULL || !(u32)nlk->groups[0]))\n\t\treturn 0;\n\n\tnetlink_table_grab();\n\tnetlink_update_subscriptions(sk, nlk->subscriptions +\n\t\t\t\t\t hweight32(groups) -\n\t\t\t\t\t hweight32(nlk->groups[0]));\n\tnlk->groups[0] = (nlk->groups[0] & ~0xffffffffUL) | groups;\n\tnetlink_update_listeners(sk);\n\tnetlink_table_ungrab();\n\n\treturn 0;\n}\n\nstatic int netlink_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int alen, int flags)\n{\n\tint err = 0;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\n\tif (alen < sizeof(addr->sa_family))\n\t\treturn -EINVAL;\n\n\tif (addr->sa_family == AF_UNSPEC) {\n\t\tsk->sk_state\t= NETLINK_UNCONNECTED;\n\t\tnlk->dst_portid\t= 0;\n\t\tnlk->dst_group  = 0;\n\t\treturn 0;\n\t}\n\tif (addr->sa_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\n\tif ((nladdr->nl_groups || nladdr->nl_pid) &&\n\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\treturn -EPERM;\n\n\t/* No need for barriers here as we return to user-space without\n\t * using any of the bound attributes.\n\t */\n\tif (!nlk->bound)\n\t\terr = netlink_autobind(sock);\n\n\tif (err == 0) {\n\t\tsk->sk_state\t= NETLINK_CONNECTED;\n\t\tnlk->dst_portid = nladdr->nl_pid;\n\t\tnlk->dst_group  = ffs(nladdr->nl_groups);\n\t}\n\n\treturn err;\n}\n\nstatic int netlink_getname(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int *addr_len, int peer)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, nladdr, addr);\n\n\tnladdr->nl_family = AF_NETLINK;\n\tnladdr->nl_pad = 0;\n\t*addr_len = sizeof(*nladdr);\n\n\tif (peer) {\n\t\tnladdr->nl_pid = nlk->dst_portid;\n\t\tnladdr->nl_groups = netlink_group_mask(nlk->dst_group);\n\t} else {\n\t\tnladdr->nl_pid = nlk->portid;\n\t\tnladdr->nl_groups = nlk->groups ? nlk->groups[0] : 0;\n\t}\n\treturn 0;\n}\n\nstatic int netlink_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\t unsigned long arg)\n{\n\t/* try to hand this ioctl down to the NIC drivers.\n\t */\n\treturn -ENOIOCTLCMD;\n}\n\nstatic struct sock *netlink_getsockbyportid(struct sock *ssk, u32 portid)\n{\n\tstruct sock *sock;\n\tstruct netlink_sock *nlk;\n\n\tsock = netlink_lookup(sock_net(ssk), ssk->sk_protocol, portid);\n\tif (!sock)\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\n\t/* Don't bother queuing skb if kernel socket has no input function */\n\tnlk = nlk_sk(sock);\n\tif (sock->sk_state == NETLINK_CONNECTED &&\n\t    nlk->dst_portid != nlk_sk(ssk)->portid) {\n\t\tsock_put(sock);\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\t}\n\treturn sock;\n}\n\nstruct sock *netlink_getsockbyfilp(struct file *filp)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct sock *sock;\n\n\tif (!S_ISSOCK(inode->i_mode))\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\tsock = SOCKET_I(inode)->sk;\n\tif (sock->sk_family != AF_NETLINK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsock_hold(sock);\n\treturn sock;\n}\n\nstatic struct sk_buff *netlink_alloc_large_skb(unsigned int size,\n\t\t\t\t\t       int broadcast)\n{\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tif (size <= NLMSG_GOODSIZE || broadcast)\n\t\treturn alloc_skb(size, GFP_KERNEL);\n\n\tsize = SKB_DATA_ALIGN(size) +\n\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tdata = vmalloc(size);\n\tif (data == NULL)\n\t\treturn NULL;\n\n\tskb = __build_skb(data, size);\n\tif (skb == NULL)\n\t\tvfree(data);\n\telse\n\t\tskb->destructor = netlink_skb_destructor;\n\n\treturn skb;\n}\n\n/*\n * Attach a skb to a netlink socket.\n * The caller must hold a reference to the destination socket. On error, the\n * reference is dropped. The skb is not send to the destination, just all\n * all error checks are performed and memory in the queue is reserved.\n * Return values:\n * < 0: error. skb freed, reference to sock dropped.\n * 0: continue\n * 1: repeat lookup - reference dropped while waiting for socket memory.\n */\nint netlink_attachskb(struct sock *sk, struct sk_buff *skb,\n\t\t      long *timeo, struct sock *ssk)\n{\n\tstruct netlink_sock *nlk;\n\n\tnlk = nlk_sk(sk);\n\n\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t     test_bit(NETLINK_S_CONGESTED, &nlk->state))) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\t\tif (!*timeo) {\n\t\t\tif (!ssk || netlink_is_kernel(ssk))\n\t\t\t\tnetlink_overrun(sk);\n\t\t\tsock_put(sk);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tadd_wait_queue(&nlk->wait, &wait);\n\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t\t     test_bit(NETLINK_S_CONGESTED, &nlk->state)) &&\n\t\t    !sock_flag(sk, SOCK_DEAD))\n\t\t\t*timeo = schedule_timeout(*timeo);\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nlk->wait, &wait);\n\t\tsock_put(sk);\n\n\t\tif (signal_pending(current)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn sock_intr_errno(*timeo);\n\t\t}\n\t\treturn 1;\n\t}\n\tnetlink_skb_set_owner_r(skb, sk);\n\treturn 0;\n}\n\nstatic int __netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = skb->len;\n\n\tnetlink_deliver_tap(skb);\n\n\tskb_queue_tail(&sk->sk_receive_queue, skb);\n\tsk->sk_data_ready(sk);\n\treturn len;\n}\n\nint netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = __netlink_sendskb(sk, skb);\n\n\tsock_put(sk);\n\treturn len;\n}\n\nvoid netlink_detachskb(struct sock *sk, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\tsock_put(sk);\n}\n\nstatic struct sk_buff *netlink_trim(struct sk_buff *skb, gfp_t allocation)\n{\n\tint delta;\n\n\tWARN_ON(skb->sk != NULL);\n\tdelta = skb->end - skb->tail;\n\tif (is_vmalloc_addr(skb->head) || delta * 2 < skb->truesize)\n\t\treturn skb;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, allocation);\n\t\tif (!nskb)\n\t\t\treturn skb;\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tif (!pskb_expand_head(skb, 0, -delta, allocation))\n\t\tskb->truesize -= delta;\n\n\treturn skb;\n}\n\nstatic int netlink_unicast_kernel(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct sock *ssk)\n{\n\tint ret;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tret = -ECONNREFUSED;\n\tif (nlk->netlink_rcv != NULL) {\n\t\tret = skb->len;\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\tNETLINK_CB(skb).sk = ssk;\n\t\tnetlink_deliver_tap_kernel(sk, ssk, skb);\n\t\tnlk->netlink_rcv(skb);\n\t\tconsume_skb(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\tsock_put(sk);\n\treturn ret;\n}\n\nint netlink_unicast(struct sock *ssk, struct sk_buff *skb,\n\t\t    u32 portid, int nonblock)\n{\n\tstruct sock *sk;\n\tint err;\n\tlong timeo;\n\n\tskb = netlink_trim(skb, gfp_any());\n\n\ttimeo = sock_sndtimeo(ssk, nonblock);\nretry:\n\tsk = netlink_getsockbyportid(ssk, portid);\n\tif (IS_ERR(sk)) {\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(sk);\n\t}\n\tif (netlink_is_kernel(sk))\n\t\treturn netlink_unicast_kernel(sk, skb, ssk);\n\n\tif (sk_filter(sk, skb)) {\n\t\terr = skb->len;\n\t\tkfree_skb(skb);\n\t\tsock_put(sk);\n\t\treturn err;\n\t}\n\n\terr = netlink_attachskb(sk, skb, &timeo, ssk);\n\tif (err == 1)\n\t\tgoto retry;\n\tif (err)\n\t\treturn err;\n\n\treturn netlink_sendskb(sk, skb);\n}\nEXPORT_SYMBOL(netlink_unicast);\n\nint netlink_has_listeners(struct sock *sk, unsigned int group)\n{\n\tint res = 0;\n\tstruct listeners *listeners;\n\n\tBUG_ON(!netlink_is_kernel(sk));\n\n\trcu_read_lock();\n\tlisteners = rcu_dereference(nl_table[sk->sk_protocol].listeners);\n\n\tif (listeners && group - 1 < nl_table[sk->sk_protocol].groups)\n\t\tres = test_bit(group - 1, listeners->masks);\n\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(netlink_has_listeners);\n\nstatic int netlink_broadcast_deliver(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t    !test_bit(NETLINK_S_CONGESTED, &nlk->state)) {\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\t__netlink_sendskb(sk, skb);\n\t\treturn atomic_read(&sk->sk_rmem_alloc) > (sk->sk_rcvbuf >> 1);\n\t}\n\treturn -1;\n}\n\nstruct netlink_broadcast_data {\n\tstruct sock *exclude_sk;\n\tstruct net *net;\n\tu32 portid;\n\tu32 group;\n\tint failure;\n\tint delivery_failure;\n\tint congested;\n\tint delivered;\n\tgfp_t allocation;\n\tstruct sk_buff *skb, *skb2;\n\tint (*tx_filter)(struct sock *dsk, struct sk_buff *skb, void *data);\n\tvoid *tx_data;\n};\n\nstatic void do_one_broadcast(struct sock *sk,\n\t\t\t\t    struct netlink_broadcast_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint val;\n\n\tif (p->exclude_sk == sk)\n\t\treturn;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\treturn;\n\n\tif (!net_eq(sock_net(sk), p->net)) {\n\t\tif (!(nlk->flags & NETLINK_F_LISTEN_ALL_NSID))\n\t\t\treturn;\n\n\t\tif (!peernet_has_id(sock_net(sk), p->net))\n\t\t\treturn;\n\n\t\tif (!file_ns_capable(sk->sk_socket->file, p->net->user_ns,\n\t\t\t\t     CAP_NET_BROADCAST))\n\t\t\treturn;\n\t}\n\n\tif (p->failure) {\n\t\tnetlink_overrun(sk);\n\t\treturn;\n\t}\n\n\tsock_hold(sk);\n\tif (p->skb2 == NULL) {\n\t\tif (skb_shared(p->skb)) {\n\t\t\tp->skb2 = skb_clone(p->skb, p->allocation);\n\t\t} else {\n\t\t\tp->skb2 = skb_get(p->skb);\n\t\t\t/*\n\t\t\t * skb ownership may have been set when\n\t\t\t * delivered to a previous socket.\n\t\t\t */\n\t\t\tskb_orphan(p->skb2);\n\t\t}\n\t}\n\tif (p->skb2 == NULL) {\n\t\tnetlink_overrun(sk);\n\t\t/* Clone failed. Notify ALL listeners. */\n\t\tp->failure = 1;\n\t\tif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\n\t\t\tp->delivery_failure = 1;\n\t\tgoto out;\n\t}\n\tif (p->tx_filter && p->tx_filter(sk, p->skb2, p->tx_data)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\tif (sk_filter(sk, p->skb2)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\tNETLINK_CB(p->skb2).nsid = peernet2id(sock_net(sk), p->net);\n\tNETLINK_CB(p->skb2).nsid_is_set = true;\n\tval = netlink_broadcast_deliver(sk, p->skb2);\n\tif (val < 0) {\n\t\tnetlink_overrun(sk);\n\t\tif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\n\t\t\tp->delivery_failure = 1;\n\t} else {\n\t\tp->congested |= val;\n\t\tp->delivered = 1;\n\t\tp->skb2 = NULL;\n\t}\nout:\n\tsock_put(sk);\n}\n\nint netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid,\n\tu32 group, gfp_t allocation,\n\tint (*filter)(struct sock *dsk, struct sk_buff *skb, void *data),\n\tvoid *filter_data)\n{\n\tstruct net *net = sock_net(ssk);\n\tstruct netlink_broadcast_data info;\n\tstruct sock *sk;\n\n\tskb = netlink_trim(skb, allocation);\n\n\tinfo.exclude_sk = ssk;\n\tinfo.net = net;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\tinfo.failure = 0;\n\tinfo.delivery_failure = 0;\n\tinfo.congested = 0;\n\tinfo.delivered = 0;\n\tinfo.allocation = allocation;\n\tinfo.skb = skb;\n\tinfo.skb2 = NULL;\n\tinfo.tx_filter = filter;\n\tinfo.tx_data = filter_data;\n\n\t/* While we sleep in clone, do not allow to change socket list */\n\n\tnetlink_lock_table();\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tdo_one_broadcast(sk, &info);\n\n\tconsume_skb(skb);\n\n\tnetlink_unlock_table();\n\n\tif (info.delivery_failure) {\n\t\tkfree_skb(info.skb2);\n\t\treturn -ENOBUFS;\n\t}\n\tconsume_skb(info.skb2);\n\n\tif (info.delivered) {\n\t\tif (info.congested && gfpflags_allow_blocking(allocation))\n\t\t\tyield();\n\t\treturn 0;\n\t}\n\treturn -ESRCH;\n}\nEXPORT_SYMBOL(netlink_broadcast_filtered);\n\nint netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 portid,\n\t\t      u32 group, gfp_t allocation)\n{\n\treturn netlink_broadcast_filtered(ssk, skb, portid, group, allocation,\n\t\tNULL, NULL);\n}\nEXPORT_SYMBOL(netlink_broadcast);\n\nstruct netlink_set_err_data {\n\tstruct sock *exclude_sk;\n\tu32 portid;\n\tu32 group;\n\tint code;\n};\n\nstatic int do_one_set_err(struct sock *sk, struct netlink_set_err_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint ret = 0;\n\n\tif (sk == p->exclude_sk)\n\t\tgoto out;\n\n\tif (!net_eq(sock_net(sk), sock_net(p->exclude_sk)))\n\t\tgoto out;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\tgoto out;\n\n\tif (p->code == ENOBUFS && nlk->flags & NETLINK_F_RECV_NO_ENOBUFS) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tsk->sk_err = p->code;\n\tsk->sk_error_report(sk);\nout:\n\treturn ret;\n}\n\n/**\n * netlink_set_err - report error to broadcast listeners\n * @ssk: the kernel netlink socket, as returned by netlink_kernel_create()\n * @portid: the PORTID of a process that we want to skip (if any)\n * @group: the broadcast group that will notice the error\n * @code: error code, must be negative (as usual in kernelspace)\n *\n * This function returns the number of broadcast listeners that have set the\n * NETLINK_NO_ENOBUFS socket option.\n */\nint netlink_set_err(struct sock *ssk, u32 portid, u32 group, int code)\n{\n\tstruct netlink_set_err_data info;\n\tstruct sock *sk;\n\tint ret = 0;\n\n\tinfo.exclude_sk = ssk;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\t/* sk->sk_err wants a positive error value */\n\tinfo.code = -code;\n\n\tread_lock(&nl_table_lock);\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tret += do_one_set_err(sk, &info);\n\n\tread_unlock(&nl_table_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(netlink_set_err);\n\n/* must be called with netlink table grabbed */\nstatic void netlink_update_socket_mc(struct netlink_sock *nlk,\n\t\t\t\t     unsigned int group,\n\t\t\t\t     int is_new)\n{\n\tint old, new = !!is_new, subscriptions;\n\n\told = test_bit(group - 1, nlk->groups);\n\tsubscriptions = nlk->subscriptions - old + new;\n\tif (new)\n\t\t__set_bit(group - 1, nlk->groups);\n\telse\n\t\t__clear_bit(group - 1, nlk->groups);\n\tnetlink_update_subscriptions(&nlk->sk, subscriptions);\n\tnetlink_update_listeners(&nlk->sk);\n}\n\nstatic int netlink_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t      char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int val = 0;\n\tint err;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optlen >= sizeof(int) &&\n\t    get_user(val, (unsigned int __user *)optval))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_RECV_PKTINFO;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_RECV_PKTINFO;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_ADD_MEMBERSHIP:\n\tcase NETLINK_DROP_MEMBERSHIP: {\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (!val || val - 1 >= nlk->ngroups)\n\t\t\treturn -EINVAL;\n\t\tif (optname == NETLINK_ADD_MEMBERSHIP && nlk->netlink_bind) {\n\t\t\terr = nlk->netlink_bind(sock_net(sk), val);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tnetlink_table_grab();\n\t\tnetlink_update_socket_mc(nlk, val,\n\t\t\t\t\t optname == NETLINK_ADD_MEMBERSHIP);\n\t\tnetlink_table_ungrab();\n\t\tif (optname == NETLINK_DROP_MEMBERSHIP && nlk->netlink_unbind)\n\t\t\tnlk->netlink_unbind(sock_net(sk), val);\n\n\t\terr = 0;\n\t\tbreak;\n\t}\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_BROADCAST_SEND_ERROR;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_BROADCAST_SEND_ERROR;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tif (val) {\n\t\t\tnlk->flags |= NETLINK_F_RECV_NO_ENOBUFS;\n\t\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\t\t\twake_up_interruptible(&nlk->wait);\n\t\t} else {\n\t\t\tnlk->flags &= ~NETLINK_F_RECV_NO_ENOBUFS;\n\t\t}\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_LISTEN_ALL_NSID:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_BROADCAST))\n\t\t\treturn -EPERM;\n\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_LISTEN_ALL_NSID;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_LISTEN_ALL_NSID;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_CAP_ACK:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_CAP_ACK;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_CAP_ACK;\n\t\terr = 0;\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t}\n\treturn err;\n}\n\nstatic int netlink_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t      char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint len, val, err;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_RECV_PKTINFO ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_RECV_NO_ENOBUFS ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_LIST_MEMBERSHIPS: {\n\t\tint pos, idx, shift;\n\n\t\terr = 0;\n\t\tnetlink_lock_table();\n\t\tfor (pos = 0; pos * 8 < nlk->ngroups; pos += sizeof(u32)) {\n\t\t\tif (len - pos < sizeof(u32))\n\t\t\t\tbreak;\n\n\t\t\tidx = pos / sizeof(unsigned long);\n\t\t\tshift = (pos % sizeof(unsigned long)) * 8;\n\t\t\tif (put_user((u32)(nlk->groups[idx] >> shift),\n\t\t\t\t     (u32 __user *)(optval + pos))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (put_user(ALIGN(nlk->ngroups / 8, sizeof(u32)), optlen))\n\t\t\terr = -EFAULT;\n\t\tnetlink_unlock_table();\n\t\tbreak;\n\t}\n\tcase NETLINK_CAP_ACK:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_CAP_ACK ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t}\n\treturn err;\n}\n\nstatic void netlink_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct nl_pktinfo info;\n\n\tinfo.group = NETLINK_CB(skb).dst_group;\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_PKTINFO, sizeof(info), &info);\n}\n\nstatic void netlink_cmsg_listen_all_nsid(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tif (!NETLINK_CB(skb).nsid_is_set)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_LISTEN_ALL_NSID, sizeof(int),\n\t\t &NETLINK_CB(skb).nsid);\n}\n\nstatic int netlink_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\tu32 dst_portid;\n\tu32 dst_group;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct scm_cookie scm;\n\tu32 netlink_skb_flags = 0;\n\n\tif (msg->msg_flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\terr = scm_send(sock, msg, &scm, true);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (msg->msg_namelen) {\n\t\terr = -EINVAL;\n\t\tif (addr->nl_family != AF_NETLINK)\n\t\t\tgoto out;\n\t\tdst_portid = addr->nl_pid;\n\t\tdst_group = ffs(addr->nl_groups);\n\t\terr =  -EPERM;\n\t\tif ((dst_group || dst_portid) &&\n\t\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\t\tgoto out;\n\t\tnetlink_skb_flags |= NETLINK_SKB_DST;\n\t} else {\n\t\tdst_portid = nlk->dst_portid;\n\t\tdst_group = nlk->dst_group;\n\t}\n\n\tif (!nlk->bound) {\n\t\terr = netlink_autobind(sock);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t/* Ensure nlk is hashed and visible. */\n\t\tsmp_rmb();\n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > sk->sk_sndbuf - 32)\n\t\tgoto out;\n\terr = -ENOBUFS;\n\tskb = netlink_alloc_large_skb(len, dst_group);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tNETLINK_CB(skb).portid\t= nlk->portid;\n\tNETLINK_CB(skb).dst_group = dst_group;\n\tNETLINK_CB(skb).creds\t= scm.creds;\n\tNETLINK_CB(skb).flags\t= netlink_skb_flags;\n\n\terr = -EFAULT;\n\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\terr = security_netlink_send(sk, skb);\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\tif (dst_group) {\n\t\tatomic_inc(&skb->users);\n\t\tnetlink_broadcast(sk, skb, dst_portid, dst_group, GFP_KERNEL);\n\t}\n\terr = netlink_unicast(sk, skb, dst_portid, msg->msg_flags&MSG_DONTWAIT);\n\nout:\n\tscm_destroy(&scm);\n\treturn err;\n}\n\nstatic int netlink_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t   int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint noblock = flags&MSG_DONTWAIT;\n\tsize_t copied;\n\tstruct sk_buff *skb, *data_skb;\n\tint err, ret;\n\n\tif (flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tcopied = 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tdata_skb = skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\t/*\n\t\t * If this skb has a frag_list, then here that means that we\n\t\t * will have to use the frag_list skb's data for compat tasks\n\t\t * and the regular skb's data for normal (non-compat) tasks.\n\t\t *\n\t\t * If we need to send the compat skb, assign it to the\n\t\t * 'data_skb' variable so that it will be used below for data\n\t\t * copying. We keep 'skb' for everything else, including\n\t\t * freeing both later.\n\t\t */\n\t\tif (flags & MSG_CMSG_COMPAT)\n\t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n\t}\n#endif\n\n\t/* Record the max length of recvmsg() calls for future allocations */\n\tnlk->max_recvmsg_len = max(nlk->max_recvmsg_len, len);\n\tnlk->max_recvmsg_len = min_t(size_t, nlk->max_recvmsg_len,\n\t\t\t\t     16384);\n\n\tcopied = data_skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(data_skb);\n\terr = skb_copy_datagram_msg(data_skb, 0, msg, copied);\n\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\t\taddr->nl_family = AF_NETLINK;\n\t\taddr->nl_pad    = 0;\n\t\taddr->nl_pid\t= NETLINK_CB(skb).portid;\n\t\taddr->nl_groups\t= netlink_group_mask(NETLINK_CB(skb).dst_group);\n\t\tmsg->msg_namelen = sizeof(*addr);\n\t}\n\n\tif (nlk->flags & NETLINK_F_RECV_PKTINFO)\n\t\tnetlink_cmsg_recv_pktinfo(msg, skb);\n\tif (nlk->flags & NETLINK_F_LISTEN_ALL_NSID)\n\t\tnetlink_cmsg_listen_all_nsid(sk, msg, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\tscm.creds = *NETLINK_CREDS(skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = data_skb->len;\n\n\tskb_free_datagram(sk, skb);\n\n\tif (nlk->cb_running &&\n\t    atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\n\t\tret = netlink_dump(sk);\n\t\tif (ret) {\n\t\t\tsk->sk_err = -ret;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\n\tscm_recv(sock, msg, &scm, flags);\nout:\n\tnetlink_rcv_wake(sk);\n\treturn err ? : copied;\n}\n\nstatic void netlink_data_ready(struct sock *sk)\n{\n\tBUG();\n}\n\n/*\n *\tWe export these functions to other modules. They provide a\n *\tcomplete set of kernel non-blocking support for message\n *\tqueueing.\n */\n\nstruct sock *\n__netlink_kernel_create(struct net *net, int unit, struct module *module,\n\t\t\tstruct netlink_kernel_cfg *cfg)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\tstruct listeners *listeners = NULL;\n\tstruct mutex *cb_mutex = cfg ? cfg->cb_mutex : NULL;\n\tunsigned int groups;\n\n\tBUG_ON(!nl_table);\n\n\tif (unit < 0 || unit >= MAX_LINKS)\n\t\treturn NULL;\n\n\tif (sock_create_lite(PF_NETLINK, SOCK_DGRAM, unit, &sock))\n\t\treturn NULL;\n\n\tif (__netlink_create(net, sock, cb_mutex, unit, 1) < 0)\n\t\tgoto out_sock_release_nosk;\n\n\tsk = sock->sk;\n\n\tif (!cfg || cfg->groups < 32)\n\t\tgroups = 32;\n\telse\n\t\tgroups = cfg->groups;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tgoto out_sock_release;\n\n\tsk->sk_data_ready = netlink_data_ready;\n\tif (cfg && cfg->input)\n\t\tnlk_sk(sk)->netlink_rcv = cfg->input;\n\n\tif (netlink_insert(sk, 0))\n\t\tgoto out_sock_release;\n\n\tnlk = nlk_sk(sk);\n\tnlk->flags |= NETLINK_F_KERNEL_SOCKET;\n\n\tnetlink_table_grab();\n\tif (!nl_table[unit].registered) {\n\t\tnl_table[unit].groups = groups;\n\t\trcu_assign_pointer(nl_table[unit].listeners, listeners);\n\t\tnl_table[unit].cb_mutex = cb_mutex;\n\t\tnl_table[unit].module = module;\n\t\tif (cfg) {\n\t\t\tnl_table[unit].bind = cfg->bind;\n\t\t\tnl_table[unit].unbind = cfg->unbind;\n\t\t\tnl_table[unit].flags = cfg->flags;\n\t\t\tif (cfg->compare)\n\t\t\t\tnl_table[unit].compare = cfg->compare;\n\t\t}\n\t\tnl_table[unit].registered = 1;\n\t} else {\n\t\tkfree(listeners);\n\t\tnl_table[unit].registered++;\n\t}\n\tnetlink_table_ungrab();\n\treturn sk;\n\nout_sock_release:\n\tkfree(listeners);\n\tnetlink_kernel_release(sk);\n\treturn NULL;\n\nout_sock_release_nosk:\n\tsock_release(sock);\n\treturn NULL;\n}\nEXPORT_SYMBOL(__netlink_kernel_create);\n\nvoid\nnetlink_kernel_release(struct sock *sk)\n{\n\tif (sk == NULL || sk->sk_socket == NULL)\n\t\treturn;\n\n\tsock_release(sk->sk_socket);\n}\nEXPORT_SYMBOL(netlink_kernel_release);\n\nint __netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tstruct listeners *new, *old;\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\n\tif (groups < 32)\n\t\tgroups = 32;\n\n\tif (NLGRPSZ(tbl->groups) < NLGRPSZ(groups)) {\n\t\tnew = kzalloc(sizeof(*new) + NLGRPSZ(groups), GFP_ATOMIC);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t\told = nl_deref_protected(tbl->listeners);\n\t\tmemcpy(new->masks, old->masks, NLGRPSZ(tbl->groups));\n\t\trcu_assign_pointer(tbl->listeners, new);\n\n\t\tkfree_rcu(old, rcu);\n\t}\n\ttbl->groups = groups;\n\n\treturn 0;\n}\n\n/**\n * netlink_change_ngroups - change number of multicast groups\n *\n * This changes the number of multicast groups that are available\n * on a certain netlink family. Note that it is not possible to\n * change the number of groups to below 32. Also note that it does\n * not implicitly call netlink_clear_multicast_users() when the\n * number of groups is reduced.\n *\n * @sk: The kernel netlink socket, as returned by netlink_kernel_create().\n * @groups: The new number of groups.\n */\nint netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tint err;\n\n\tnetlink_table_grab();\n\terr = __netlink_change_ngroups(sk, groups);\n\tnetlink_table_ungrab();\n\n\treturn err;\n}\n\nvoid __netlink_clear_multicast_users(struct sock *ksk, unsigned int group)\n{\n\tstruct sock *sk;\n\tstruct netlink_table *tbl = &nl_table[ksk->sk_protocol];\n\n\tsk_for_each_bound(sk, &tbl->mc_list)\n\t\tnetlink_update_socket_mc(nlk_sk(sk), group, 0);\n}\n\nstruct nlmsghdr *\n__nlmsg_put(struct sk_buff *skb, u32 portid, u32 seq, int type, int len, int flags)\n{\n\tstruct nlmsghdr *nlh;\n\tint size = nlmsg_msg_size(len);\n\n\tnlh = (struct nlmsghdr *)skb_put(skb, NLMSG_ALIGN(size));\n\tnlh->nlmsg_type = type;\n\tnlh->nlmsg_len = size;\n\tnlh->nlmsg_flags = flags;\n\tnlh->nlmsg_pid = portid;\n\tnlh->nlmsg_seq = seq;\n\tif (!__builtin_constant_p(size) || NLMSG_ALIGN(size) - size != 0)\n\t\tmemset(nlmsg_data(nlh) + len, 0, NLMSG_ALIGN(size) - size);\n\treturn nlh;\n}\nEXPORT_SYMBOL(__nlmsg_put);\n\n/*\n * It looks a bit ugly.\n * It would be better to create kernel thread.\n */\n\nstatic int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nint __netlink_dump_start(struct sock *ssk, struct sk_buff *skb,\n\t\t\t const struct nlmsghdr *nlh,\n\t\t\t struct netlink_dump_control *control)\n{\n\tstruct netlink_callback *cb;\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\tint ret;\n\n\tatomic_inc(&skb->users);\n\n\tsk = netlink_lookup(sock_net(ssk), ssk->sk_protocol, NETLINK_CB(skb).portid);\n\tif (sk == NULL) {\n\t\tret = -ECONNREFUSED;\n\t\tgoto error_free;\n\t}\n\n\tnlk = nlk_sk(sk);\n\tmutex_lock(nlk->cb_mutex);\n\t/* A dump is in progress... */\n\tif (nlk->cb_running) {\n\t\tret = -EBUSY;\n\t\tgoto error_unlock;\n\t}\n\t/* add reference of module which cb->dump belongs to */\n\tif (!try_module_get(control->module)) {\n\t\tret = -EPROTONOSUPPORT;\n\t\tgoto error_unlock;\n\t}\n\n\tcb = &nlk->cb;\n\tmemset(cb, 0, sizeof(*cb));\n\tcb->start = control->start;\n\tcb->dump = control->dump;\n\tcb->done = control->done;\n\tcb->nlh = nlh;\n\tcb->data = control->data;\n\tcb->module = control->module;\n\tcb->min_dump_alloc = control->min_dump_alloc;\n\tcb->skb = skb;\n\n\tnlk->cb_running = true;\n\n\tmutex_unlock(nlk->cb_mutex);\n\n\tif (cb->start)\n\t\tcb->start(cb);\n\n\tret = netlink_dump(sk);\n\tsock_put(sk);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* We successfully started a dump, by returning -EINTR we\n\t * signal not to send ACK even if it was requested.\n\t */\n\treturn -EINTR;\n\nerror_unlock:\n\tsock_put(sk);\n\tmutex_unlock(nlk->cb_mutex);\nerror_free:\n\tkfree_skb(skb);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netlink_dump_start);\n\nvoid netlink_ack(struct sk_buff *in_skb, struct nlmsghdr *nlh, int err)\n{\n\tstruct sk_buff *skb;\n\tstruct nlmsghdr *rep;\n\tstruct nlmsgerr *errmsg;\n\tsize_t payload = sizeof(*errmsg);\n\tstruct netlink_sock *nlk = nlk_sk(NETLINK_CB(in_skb).sk);\n\n\t/* Error messages get the original request appened, unless the user\n\t * requests to cap the error message.\n\t */\n\tif (!(nlk->flags & NETLINK_F_CAP_ACK) && err)\n\t\tpayload += nlmsg_len(nlh);\n\n\tskb = nlmsg_new(payload, GFP_KERNEL);\n\tif (!skb) {\n\t\tstruct sock *sk;\n\n\t\tsk = netlink_lookup(sock_net(in_skb->sk),\n\t\t\t\t    in_skb->sk->sk_protocol,\n\t\t\t\t    NETLINK_CB(in_skb).portid);\n\t\tif (sk) {\n\t\t\tsk->sk_err = ENOBUFS;\n\t\t\tsk->sk_error_report(sk);\n\t\t\tsock_put(sk);\n\t\t}\n\t\treturn;\n\t}\n\n\trep = __nlmsg_put(skb, NETLINK_CB(in_skb).portid, nlh->nlmsg_seq,\n\t\t\t  NLMSG_ERROR, payload, 0);\n\terrmsg = nlmsg_data(rep);\n\terrmsg->error = err;\n\tmemcpy(&errmsg->msg, nlh, payload > sizeof(*errmsg) ? nlh->nlmsg_len : sizeof(*nlh));\n\tnetlink_unicast(in_skb->sk, skb, NETLINK_CB(in_skb).portid, MSG_DONTWAIT);\n}\nEXPORT_SYMBOL(netlink_ack);\n\nint netlink_rcv_skb(struct sk_buff *skb, int (*cb)(struct sk_buff *,\n\t\t\t\t\t\t     struct nlmsghdr *))\n{\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\twhile (skb->len >= nlmsg_total_size(0)) {\n\t\tint msglen;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\terr = 0;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN || skb->len < nlh->nlmsg_len)\n\t\t\treturn 0;\n\n\t\t/* Only requests are handled by the kernel */\n\t\tif (!(nlh->nlmsg_flags & NLM_F_REQUEST))\n\t\t\tgoto ack;\n\n\t\t/* Skip control messages */\n\t\tif (nlh->nlmsg_type < NLMSG_MIN_TYPE)\n\t\t\tgoto ack;\n\n\t\terr = cb(skb, nlh);\n\t\tif (err == -EINTR)\n\t\t\tgoto skip;\n\nack:\n\t\tif (nlh->nlmsg_flags & NLM_F_ACK || err)\n\t\t\tnetlink_ack(skb, nlh, err);\n\nskip:\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\t\tskb_pull(skb, msglen);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netlink_rcv_skb);\n\n/**\n * nlmsg_notify - send a notification netlink message\n * @sk: netlink socket to use\n * @skb: notification message\n * @portid: destination netlink portid for reports or 0\n * @group: destination multicast group or 0\n * @report: 1 to report back, 0 to disable\n * @flags: allocation flags\n */\nint nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid,\n\t\t unsigned int group, int report, gfp_t flags)\n{\n\tint err = 0;\n\n\tif (group) {\n\t\tint exclude_portid = 0;\n\n\t\tif (report) {\n\t\t\tatomic_inc(&skb->users);\n\t\t\texclude_portid = portid;\n\t\t}\n\n\t\t/* errors reported via destination sk->sk_err, but propagate\n\t\t * delivery errors if NETLINK_BROADCAST_ERROR flag is set */\n\t\terr = nlmsg_multicast(sk, skb, exclude_portid, group, flags);\n\t}\n\n\tif (report) {\n\t\tint err2;\n\n\t\terr2 = nlmsg_unicast(sk, skb, portid);\n\t\tif (!err || err == -ESRCH)\n\t\t\terr = err2;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(nlmsg_notify);\n\n#ifdef CONFIG_PROC_FS\nstruct nl_seq_iter {\n\tstruct seq_net_private p;\n\tstruct rhashtable_iter hti;\n\tint link;\n};\n\nstatic int netlink_walk_start(struct nl_seq_iter *iter)\n{\n\tint err;\n\n\terr = rhashtable_walk_init(&nl_table[iter->link].hash, &iter->hti,\n\t\t\t\t   GFP_KERNEL);\n\tif (err) {\n\t\titer->link = MAX_LINKS;\n\t\treturn err;\n\t}\n\n\terr = rhashtable_walk_start(&iter->hti);\n\treturn err == -EAGAIN ? 0 : err;\n}\n\nstatic void netlink_walk_stop(struct nl_seq_iter *iter)\n{\n\trhashtable_walk_stop(&iter->hti);\n\trhashtable_walk_exit(&iter->hti);\n}\n\nstatic void *__netlink_seq_next(struct seq_file *seq)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tstruct netlink_sock *nlk;\n\n\tdo {\n\t\tfor (;;) {\n\t\t\tint err;\n\n\t\t\tnlk = rhashtable_walk_next(&iter->hti);\n\n\t\t\tif (IS_ERR(nlk)) {\n\t\t\t\tif (PTR_ERR(nlk) == -EAGAIN)\n\t\t\t\t\tcontinue;\n\n\t\t\t\treturn nlk;\n\t\t\t}\n\n\t\t\tif (nlk)\n\t\t\t\tbreak;\n\n\t\t\tnetlink_walk_stop(iter);\n\t\t\tif (++iter->link >= MAX_LINKS)\n\t\t\t\treturn NULL;\n\n\t\t\terr = netlink_walk_start(iter);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\t\t}\n\t} while (sock_net(&nlk->sk) != seq_file_net(seq));\n\n\treturn nlk;\n}\n\nstatic void *netlink_seq_start(struct seq_file *seq, loff_t *posp)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tvoid *obj = SEQ_START_TOKEN;\n\tloff_t pos;\n\tint err;\n\n\titer->link = 0;\n\n\terr = netlink_walk_start(iter);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tfor (pos = *posp; pos && obj && !IS_ERR(obj); pos--)\n\t\tobj = __netlink_seq_next(seq);\n\n\treturn obj;\n}\n\nstatic void *netlink_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn __netlink_seq_next(seq);\n}\n\nstatic void netlink_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\n\tif (iter->link >= MAX_LINKS)\n\t\treturn;\n\n\tnetlink_walk_stop(iter);\n}\n\n\nstatic int netlink_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"sk       Eth Pid    Groups   \"\n\t\t\t \"Rmem     Wmem     Dump     Locks     Drops     Inode\\n\");\n\t} else {\n\t\tstruct sock *s = v;\n\t\tstruct netlink_sock *nlk = nlk_sk(s);\n\n\t\tseq_printf(seq, \"%pK %-3d %-6u %08x %-8d %-8d %d %-8d %-8d %-8lu\\n\",\n\t\t\t   s,\n\t\t\t   s->sk_protocol,\n\t\t\t   nlk->portid,\n\t\t\t   nlk->groups ? (u32)nlk->groups[0] : 0,\n\t\t\t   sk_rmem_alloc_get(s),\n\t\t\t   sk_wmem_alloc_get(s),\n\t\t\t   nlk->cb_running,\n\t\t\t   atomic_read(&s->sk_refcnt),\n\t\t\t   atomic_read(&s->sk_drops),\n\t\t\t   sock_i_ino(s)\n\t\t\t);\n\n\t}\n\treturn 0;\n}\n\nstatic const struct seq_operations netlink_seq_ops = {\n\t.start  = netlink_seq_start,\n\t.next   = netlink_seq_next,\n\t.stop   = netlink_seq_stop,\n\t.show   = netlink_seq_show,\n};\n\n\nstatic int netlink_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &netlink_seq_ops,\n\t\t\t\tsizeof(struct nl_seq_iter));\n}\n\nstatic const struct file_operations netlink_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= netlink_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nint netlink_register_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_register(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_register_notifier);\n\nint netlink_unregister_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_unregister(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_unregister_notifier);\n\nstatic const struct proto_ops netlink_ops = {\n\t.family =\tPF_NETLINK,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tnetlink_release,\n\t.bind =\t\tnetlink_bind,\n\t.connect =\tnetlink_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tnetlink_getname,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tnetlink_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tnetlink_setsockopt,\n\t.getsockopt =\tnetlink_getsockopt,\n\t.sendmsg =\tnetlink_sendmsg,\n\t.recvmsg =\tnetlink_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family netlink_family_ops = {\n\t.family = PF_NETLINK,\n\t.create = netlink_create,\n\t.owner\t= THIS_MODULE,\t/* for consistency 8) */\n};\n\nstatic int __net_init netlink_net_init(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create(\"netlink\", 0, net->proc_net, &netlink_seq_fops))\n\t\treturn -ENOMEM;\n#endif\n\treturn 0;\n}\n\nstatic void __net_exit netlink_net_exit(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tremove_proc_entry(\"netlink\", net->proc_net);\n#endif\n}\n\nstatic void __init netlink_add_usersock_entry(void)\n{\n\tstruct listeners *listeners;\n\tint groups = 32;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tpanic(\"netlink_add_usersock_entry: Cannot allocate listeners\\n\");\n\n\tnetlink_table_grab();\n\n\tnl_table[NETLINK_USERSOCK].groups = groups;\n\trcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);\n\tnl_table[NETLINK_USERSOCK].module = THIS_MODULE;\n\tnl_table[NETLINK_USERSOCK].registered = 1;\n\tnl_table[NETLINK_USERSOCK].flags = NL_CFG_F_NONROOT_SEND;\n\n\tnetlink_table_ungrab();\n}\n\nstatic struct pernet_operations __net_initdata netlink_net_ops = {\n\t.init = netlink_net_init,\n\t.exit = netlink_net_exit,\n};\n\nstatic inline u32 netlink_hash(const void *data, u32 len, u32 seed)\n{\n\tconst struct netlink_sock *nlk = data;\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(&nlk->sk), nlk->portid);\n\treturn jhash2((u32 *)&arg, netlink_compare_arg_len / sizeof(u32), seed);\n}\n\nstatic const struct rhashtable_params netlink_rhashtable_params = {\n\t.head_offset = offsetof(struct netlink_sock, node),\n\t.key_len = netlink_compare_arg_len,\n\t.obj_hashfn = netlink_hash,\n\t.obj_cmpfn = netlink_compare,\n\t.automatic_shrinking = true,\n};\n\nstatic int __init netlink_proto_init(void)\n{\n\tint i;\n\tint err = proto_register(&netlink_proto, 0);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\tBUILD_BUG_ON(sizeof(struct netlink_skb_parms) > FIELD_SIZEOF(struct sk_buff, cb));\n\n\tnl_table = kcalloc(MAX_LINKS, sizeof(*nl_table), GFP_KERNEL);\n\tif (!nl_table)\n\t\tgoto panic;\n\n\tfor (i = 0; i < MAX_LINKS; i++) {\n\t\tif (rhashtable_init(&nl_table[i].hash,\n\t\t\t\t    &netlink_rhashtable_params) < 0) {\n\t\t\twhile (--i > 0)\n\t\t\t\trhashtable_destroy(&nl_table[i].hash);\n\t\t\tkfree(nl_table);\n\t\t\tgoto panic;\n\t\t}\n\t}\n\n\tINIT_LIST_HEAD(&netlink_tap_all);\n\n\tnetlink_add_usersock_entry();\n\n\tsock_register(&netlink_family_ops);\n\tregister_pernet_subsys(&netlink_net_ops);\n\t/* The netlink device handler may be needed early. */\n\trtnetlink_init();\nout:\n\treturn err;\npanic:\n\tpanic(\"netlink_init: Cannot allocate nl_table\\n\");\n}\n\ncore_initcall(netlink_proto_init);\n"], "fixing_code": ["/*\n * NETLINK      Kernel-user communication protocol.\n *\n * \t\tAuthors:\tAlan Cox <alan@lxorguk.ukuu.org.uk>\n * \t\t\t\tAlexey Kuznetsov <kuznet@ms2.inr.ac.ru>\n * \t\t\t\tPatrick McHardy <kaber@trash.net>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n * Tue Jun 26 14:36:48 MEST 2001 Herbert \"herp\" Rosmanith\n *                               added netlink_proto_exit\n * Tue Jan 22 18:32:44 BRST 2002 Arnaldo C. de Melo <acme@conectiva.com.br>\n * \t\t\t\t use nlk_sk, as sk->protinfo is on a diet 8)\n * Fri Jul 22 19:51:12 MEST 2005 Harald Welte <laforge@gnumonks.org>\n * \t\t\t\t - inc module use count of module that owns\n * \t\t\t\t   the kernel socket in case userspace opens\n * \t\t\t\t   socket of same protocol\n * \t\t\t\t - remove all module support, since netlink is\n * \t\t\t\t   mandatory if CONFIG_NET=y these days\n */\n\n#include <linux/module.h>\n\n#include <linux/capability.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/stat.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/fcntl.h>\n#include <linux/termios.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/rtnetlink.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/security.h>\n#include <linux/jhash.h>\n#include <linux/jiffies.h>\n#include <linux/random.h>\n#include <linux/bitops.h>\n#include <linux/mm.h>\n#include <linux/types.h>\n#include <linux/audit.h>\n#include <linux/mutex.h>\n#include <linux/vmalloc.h>\n#include <linux/if_arp.h>\n#include <linux/rhashtable.h>\n#include <asm/cacheflush.h>\n#include <linux/hash.h>\n#include <linux/genetlink.h>\n\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/scm.h>\n#include <net/netlink.h>\n\n#include \"af_netlink.h\"\n\nstruct listeners {\n\tstruct rcu_head\t\trcu;\n\tunsigned long\t\tmasks[0];\n};\n\n/* state bits */\n#define NETLINK_S_CONGESTED\t\t0x0\n\n/* flags */\n#define NETLINK_F_KERNEL_SOCKET\t\t0x1\n#define NETLINK_F_RECV_PKTINFO\t\t0x2\n#define NETLINK_F_BROADCAST_SEND_ERROR\t0x4\n#define NETLINK_F_RECV_NO_ENOBUFS\t0x8\n#define NETLINK_F_LISTEN_ALL_NSID\t0x10\n#define NETLINK_F_CAP_ACK\t\t0x20\n\nstatic inline int netlink_is_kernel(struct sock *sk)\n{\n\treturn nlk_sk(sk)->flags & NETLINK_F_KERNEL_SOCKET;\n}\n\nstruct netlink_table *nl_table __read_mostly;\nEXPORT_SYMBOL_GPL(nl_table);\n\nstatic DECLARE_WAIT_QUEUE_HEAD(nl_table_wait);\n\nstatic int netlink_dump(struct sock *sk);\nstatic void netlink_skb_destructor(struct sk_buff *skb);\n\n/* nl_table locking explained:\n * Lookup and traversal are protected with an RCU read-side lock. Insertion\n * and removal are protected with per bucket lock while using RCU list\n * modification primitives and may run in parallel to RCU protected lookups.\n * Destruction of the Netlink socket may only occur *after* nl_table_lock has\n * been acquired * either during or after the socket has been removed from\n * the list and after an RCU grace period.\n */\nDEFINE_RWLOCK(nl_table_lock);\nEXPORT_SYMBOL_GPL(nl_table_lock);\nstatic atomic_t nl_table_users = ATOMIC_INIT(0);\n\n#define nl_deref_protected(X) rcu_dereference_protected(X, lockdep_is_held(&nl_table_lock));\n\nstatic ATOMIC_NOTIFIER_HEAD(netlink_chain);\n\nstatic DEFINE_SPINLOCK(netlink_tap_lock);\nstatic struct list_head netlink_tap_all __read_mostly;\n\nstatic const struct rhashtable_params netlink_rhashtable_params;\n\nstatic inline u32 netlink_group_mask(u32 group)\n{\n\treturn group ? 1 << (group - 1) : 0;\n}\n\nstatic struct sk_buff *netlink_to_full_skb(const struct sk_buff *skb,\n\t\t\t\t\t   gfp_t gfp_mask)\n{\n\tunsigned int len = skb_end_offset(skb);\n\tstruct sk_buff *new;\n\n\tnew = alloc_skb(len, gfp_mask);\n\tif (new == NULL)\n\t\treturn NULL;\n\n\tNETLINK_CB(new).portid = NETLINK_CB(skb).portid;\n\tNETLINK_CB(new).dst_group = NETLINK_CB(skb).dst_group;\n\tNETLINK_CB(new).creds = NETLINK_CB(skb).creds;\n\n\tmemcpy(skb_put(new, len), skb->data, len);\n\treturn new;\n}\n\nint netlink_add_tap(struct netlink_tap *nt)\n{\n\tif (unlikely(nt->dev->type != ARPHRD_NETLINK))\n\t\treturn -EINVAL;\n\n\tspin_lock(&netlink_tap_lock);\n\tlist_add_rcu(&nt->list, &netlink_tap_all);\n\tspin_unlock(&netlink_tap_lock);\n\n\t__module_get(nt->module);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(netlink_add_tap);\n\nstatic int __netlink_remove_tap(struct netlink_tap *nt)\n{\n\tbool found = false;\n\tstruct netlink_tap *tmp;\n\n\tspin_lock(&netlink_tap_lock);\n\n\tlist_for_each_entry(tmp, &netlink_tap_all, list) {\n\t\tif (nt == tmp) {\n\t\t\tlist_del_rcu(&nt->list);\n\t\t\tfound = true;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpr_warn(\"__netlink_remove_tap: %p not found\\n\", nt);\nout:\n\tspin_unlock(&netlink_tap_lock);\n\n\tif (found)\n\t\tmodule_put(nt->module);\n\n\treturn found ? 0 : -ENODEV;\n}\n\nint netlink_remove_tap(struct netlink_tap *nt)\n{\n\tint ret;\n\n\tret = __netlink_remove_tap(nt);\n\tsynchronize_net();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(netlink_remove_tap);\n\nstatic bool netlink_filter_tap(const struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\n\t/* We take the more conservative approach and\n\t * whitelist socket protocols that may pass.\n\t */\n\tswitch (sk->sk_protocol) {\n\tcase NETLINK_ROUTE:\n\tcase NETLINK_USERSOCK:\n\tcase NETLINK_SOCK_DIAG:\n\tcase NETLINK_NFLOG:\n\tcase NETLINK_XFRM:\n\tcase NETLINK_FIB_LOOKUP:\n\tcase NETLINK_NETFILTER:\n\tcase NETLINK_GENERIC:\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int __netlink_deliver_tap_skb(struct sk_buff *skb,\n\t\t\t\t     struct net_device *dev)\n{\n\tstruct sk_buff *nskb;\n\tstruct sock *sk = skb->sk;\n\tint ret = -ENOMEM;\n\n\tdev_hold(dev);\n\n\tif (is_vmalloc_addr(skb->head))\n\t\tnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (nskb) {\n\t\tnskb->dev = dev;\n\t\tnskb->protocol = htons((u16) sk->sk_protocol);\n\t\tnskb->pkt_type = netlink_is_kernel(sk) ?\n\t\t\t\t PACKET_KERNEL : PACKET_USER;\n\t\tskb_reset_network_header(nskb);\n\t\tret = dev_queue_xmit(nskb);\n\t\tif (unlikely(ret > 0))\n\t\t\tret = net_xmit_errno(ret);\n\t}\n\n\tdev_put(dev);\n\treturn ret;\n}\n\nstatic void __netlink_deliver_tap(struct sk_buff *skb)\n{\n\tint ret;\n\tstruct netlink_tap *tmp;\n\n\tif (!netlink_filter_tap(skb))\n\t\treturn;\n\n\tlist_for_each_entry_rcu(tmp, &netlink_tap_all, list) {\n\t\tret = __netlink_deliver_tap_skb(skb, tmp->dev);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\t}\n}\n\nstatic void netlink_deliver_tap(struct sk_buff *skb)\n{\n\trcu_read_lock();\n\n\tif (unlikely(!list_empty(&netlink_tap_all)))\n\t\t__netlink_deliver_tap(skb);\n\n\trcu_read_unlock();\n}\n\nstatic void netlink_deliver_tap_kernel(struct sock *dst, struct sock *src,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tif (!(netlink_is_kernel(dst) && netlink_is_kernel(src)))\n\t\tnetlink_deliver_tap(skb);\n}\n\nstatic void netlink_overrun(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (!(nlk->flags & NETLINK_F_RECV_NO_ENOBUFS)) {\n\t\tif (!test_and_set_bit(NETLINK_S_CONGESTED,\n\t\t\t\t      &nlk_sk(sk)->state)) {\n\t\t\tsk->sk_err = ENOBUFS;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\tatomic_inc(&sk->sk_drops);\n}\n\nstatic void netlink_rcv_wake(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (skb_queue_empty(&sk->sk_receive_queue))\n\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\tif (!test_bit(NETLINK_S_CONGESTED, &nlk->state))\n\t\twake_up_interruptible(&nlk->wait);\n}\n\nstatic void netlink_skb_destructor(struct sk_buff *skb)\n{\n\tif (is_vmalloc_addr(skb->head)) {\n\t\tif (!skb->cloned ||\n\t\t    !atomic_dec_return(&(skb_shinfo(skb)->dataref)))\n\t\t\tvfree(skb->head);\n\n\t\tskb->head = NULL;\n\t}\n\tif (skb->sk != NULL)\n\t\tsock_rfree(skb);\n}\n\nstatic void netlink_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tWARN_ON(skb->sk != NULL);\n\tskb->sk = sk;\n\tskb->destructor = netlink_skb_destructor;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, skb->truesize);\n}\n\nstatic void netlink_sock_destruct(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->cb_running) {\n\t\tif (nlk->cb.done)\n\t\t\tnlk->cb.done(&nlk->cb);\n\n\t\tmodule_put(nlk->cb.module);\n\t\tkfree_skb(nlk->cb.skb);\n\t}\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tprintk(KERN_ERR \"Freeing alive netlink socket %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(nlk_sk(sk)->groups);\n}\n\n/* This lock without WQ_FLAG_EXCLUSIVE is good on UP and it is _very_ bad on\n * SMP. Look, when several writers sleep and reader wakes them up, all but one\n * immediately hit write lock and grab all the cpus. Exclusive sleep solves\n * this, _but_ remember, it adds useless work on UP machines.\n */\n\nvoid netlink_table_grab(void)\n\t__acquires(nl_table_lock)\n{\n\tmight_sleep();\n\n\twrite_lock_irq(&nl_table_lock);\n\n\tif (atomic_read(&nl_table_users)) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\tadd_wait_queue_exclusive(&nl_table_wait, &wait);\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tif (atomic_read(&nl_table_users) == 0)\n\t\t\t\tbreak;\n\t\t\twrite_unlock_irq(&nl_table_lock);\n\t\t\tschedule();\n\t\t\twrite_lock_irq(&nl_table_lock);\n\t\t}\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nl_table_wait, &wait);\n\t}\n}\n\nvoid netlink_table_ungrab(void)\n\t__releases(nl_table_lock)\n{\n\twrite_unlock_irq(&nl_table_lock);\n\twake_up(&nl_table_wait);\n}\n\nstatic inline void\nnetlink_lock_table(void)\n{\n\t/* read_lock() synchronizes us to netlink_table_grab */\n\n\tread_lock(&nl_table_lock);\n\tatomic_inc(&nl_table_users);\n\tread_unlock(&nl_table_lock);\n}\n\nstatic inline void\nnetlink_unlock_table(void)\n{\n\tif (atomic_dec_and_test(&nl_table_users))\n\t\twake_up(&nl_table_wait);\n}\n\nstruct netlink_compare_arg\n{\n\tpossible_net_t pnet;\n\tu32 portid;\n};\n\n/* Doing sizeof directly may yield 4 extra bytes on 64-bit. */\n#define netlink_compare_arg_len \\\n\t(offsetof(struct netlink_compare_arg, portid) + sizeof(u32))\n\nstatic inline int netlink_compare(struct rhashtable_compare_arg *arg,\n\t\t\t\t  const void *ptr)\n{\n\tconst struct netlink_compare_arg *x = arg->key;\n\tconst struct netlink_sock *nlk = ptr;\n\n\treturn nlk->portid != x->portid ||\n\t       !net_eq(sock_net(&nlk->sk), read_pnet(&x->pnet));\n}\n\nstatic void netlink_compare_arg_init(struct netlink_compare_arg *arg,\n\t\t\t\t     struct net *net, u32 portid)\n{\n\tmemset(arg, 0, sizeof(*arg));\n\twrite_pnet(&arg->pnet, net);\n\targ->portid = portid;\n}\n\nstatic struct sock *__netlink_lookup(struct netlink_table *table, u32 portid,\n\t\t\t\t     struct net *net)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, net, portid);\n\treturn rhashtable_lookup_fast(&table->hash, &arg,\n\t\t\t\t      netlink_rhashtable_params);\n}\n\nstatic int __netlink_insert(struct netlink_table *table, struct sock *sk)\n{\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(sk), nlk_sk(sk)->portid);\n\treturn rhashtable_lookup_insert_key(&table->hash, &arg,\n\t\t\t\t\t    &nlk_sk(sk)->node,\n\t\t\t\t\t    netlink_rhashtable_params);\n}\n\nstatic struct sock *netlink_lookup(struct net *net, int protocol, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[protocol];\n\tstruct sock *sk;\n\n\trcu_read_lock();\n\tsk = __netlink_lookup(table, portid, net);\n\tif (sk)\n\t\tsock_hold(sk);\n\trcu_read_unlock();\n\n\treturn sk;\n}\n\nstatic const struct proto_ops netlink_ops;\n\nstatic void\nnetlink_update_listeners(struct sock *sk)\n{\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\tunsigned long mask;\n\tunsigned int i;\n\tstruct listeners *listeners;\n\n\tlisteners = nl_deref_protected(tbl->listeners);\n\tif (!listeners)\n\t\treturn;\n\n\tfor (i = 0; i < NLGRPLONGS(tbl->groups); i++) {\n\t\tmask = 0;\n\t\tsk_for_each_bound(sk, &tbl->mc_list) {\n\t\t\tif (i < NLGRPLONGS(nlk_sk(sk)->ngroups))\n\t\t\t\tmask |= nlk_sk(sk)->groups[i];\n\t\t}\n\t\tlisteners->masks[i] = mask;\n\t}\n\t/* this function is only called with the netlink table \"grabbed\", which\n\t * makes sure updates are visible before bind or setsockopt return. */\n}\n\nstatic int netlink_insert(struct sock *sk, u32 portid)\n{\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\tint err;\n\n\tlock_sock(sk);\n\n\terr = nlk_sk(sk)->portid == portid ? 0 : -EBUSY;\n\tif (nlk_sk(sk)->bound)\n\t\tgoto err;\n\n\terr = -ENOMEM;\n\tif (BITS_PER_LONG > 32 &&\n\t    unlikely(atomic_read(&table->hash.nelems) >= UINT_MAX))\n\t\tgoto err;\n\n\tnlk_sk(sk)->portid = portid;\n\tsock_hold(sk);\n\n\terr = __netlink_insert(table, sk);\n\tif (err) {\n\t\t/* In case the hashtable backend returns with -EBUSY\n\t\t * from here, it must not escape to the caller.\n\t\t */\n\t\tif (unlikely(err == -EBUSY))\n\t\t\terr = -EOVERFLOW;\n\t\tif (err == -EEXIST)\n\t\t\terr = -EADDRINUSE;\n\t\tsock_put(sk);\n\t\tgoto err;\n\t}\n\n\t/* We need to ensure that the socket is hashed and visible. */\n\tsmp_wmb();\n\tnlk_sk(sk)->bound = portid;\n\nerr:\n\trelease_sock(sk);\n\treturn err;\n}\n\nstatic void netlink_remove(struct sock *sk)\n{\n\tstruct netlink_table *table;\n\n\ttable = &nl_table[sk->sk_protocol];\n\tif (!rhashtable_remove_fast(&table->hash, &nlk_sk(sk)->node,\n\t\t\t\t    netlink_rhashtable_params)) {\n\t\tWARN_ON(atomic_read(&sk->sk_refcnt) == 1);\n\t\t__sock_put(sk);\n\t}\n\n\tnetlink_table_grab();\n\tif (nlk_sk(sk)->subscriptions) {\n\t\t__sk_del_bind_node(sk);\n\t\tnetlink_update_listeners(sk);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC)\n\t\tatomic_inc(&genl_sk_destructing_cnt);\n\tnetlink_table_ungrab();\n}\n\nstatic struct proto netlink_proto = {\n\t.name\t  = \"NETLINK\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct netlink_sock),\n};\n\nstatic int __netlink_create(struct net *net, struct socket *sock,\n\t\t\t    struct mutex *cb_mutex, int protocol,\n\t\t\t    int kern)\n{\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\n\tsock->ops = &netlink_ops;\n\n\tsk = sk_alloc(net, PF_NETLINK, GFP_KERNEL, &netlink_proto, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n\n\tsock_init_data(sock, sk);\n\n\tnlk = nlk_sk(sk);\n\tif (cb_mutex) {\n\t\tnlk->cb_mutex = cb_mutex;\n\t} else {\n\t\tnlk->cb_mutex = &nlk->cb_def_mutex;\n\t\tmutex_init(nlk->cb_mutex);\n\t}\n\tinit_waitqueue_head(&nlk->wait);\n\n\tsk->sk_destruct = netlink_sock_destruct;\n\tsk->sk_protocol = protocol;\n\treturn 0;\n}\n\nstatic int netlink_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t  int kern)\n{\n\tstruct module *module = NULL;\n\tstruct mutex *cb_mutex;\n\tstruct netlink_sock *nlk;\n\tint (*bind)(struct net *net, int group);\n\tvoid (*unbind)(struct net *net, int group);\n\tint err = 0;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (protocol < 0 || protocol >= MAX_LINKS)\n\t\treturn -EPROTONOSUPPORT;\n\n\tnetlink_lock_table();\n#ifdef CONFIG_MODULES\n\tif (!nl_table[protocol].registered) {\n\t\tnetlink_unlock_table();\n\t\trequest_module(\"net-pf-%d-proto-%d\", PF_NETLINK, protocol);\n\t\tnetlink_lock_table();\n\t}\n#endif\n\tif (nl_table[protocol].registered &&\n\t    try_module_get(nl_table[protocol].module))\n\t\tmodule = nl_table[protocol].module;\n\telse\n\t\terr = -EPROTONOSUPPORT;\n\tcb_mutex = nl_table[protocol].cb_mutex;\n\tbind = nl_table[protocol].bind;\n\tunbind = nl_table[protocol].unbind;\n\tnetlink_unlock_table();\n\n\tif (err < 0)\n\t\tgoto out;\n\n\terr = __netlink_create(net, sock, cb_mutex, protocol, kern);\n\tif (err < 0)\n\t\tgoto out_module;\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(net, &netlink_proto, 1);\n\tlocal_bh_enable();\n\n\tnlk = nlk_sk(sock->sk);\n\tnlk->module = module;\n\tnlk->netlink_bind = bind;\n\tnlk->netlink_unbind = unbind;\nout:\n\treturn err;\n\nout_module:\n\tmodule_put(module);\n\tgoto out;\n}\n\nstatic void deferred_put_nlk_sk(struct rcu_head *head)\n{\n\tstruct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu);\n\n\tsock_put(&nlk->sk);\n}\n\nstatic int netlink_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnetlink_remove(sk);\n\tsock_orphan(sk);\n\tnlk = nlk_sk(sk);\n\n\t/*\n\t * OK. Socket is unlinked, any packets that arrive now\n\t * will be purged.\n\t */\n\n\t/* must not acquire netlink_table_lock in any way again before unbind\n\t * and notifying genetlink is done as otherwise it might deadlock\n\t */\n\tif (nlk->netlink_unbind) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nlk->ngroups; i++)\n\t\t\tif (test_bit(i, nlk->groups))\n\t\t\t\tnlk->netlink_unbind(sock_net(sk), i + 1);\n\t}\n\tif (sk->sk_protocol == NETLINK_GENERIC &&\n\t    atomic_dec_return(&genl_sk_destructing_cnt) == 0)\n\t\twake_up(&genl_sk_destructing_waitq);\n\n\tsock->sk = NULL;\n\twake_up_interruptible_all(&nlk->wait);\n\n\tskb_queue_purge(&sk->sk_write_queue);\n\n\tif (nlk->portid && nlk->bound) {\n\t\tstruct netlink_notify n = {\n\t\t\t\t\t\t.net = sock_net(sk),\n\t\t\t\t\t\t.protocol = sk->sk_protocol,\n\t\t\t\t\t\t.portid = nlk->portid,\n\t\t\t\t\t  };\n\t\tatomic_notifier_call_chain(&netlink_chain,\n\t\t\t\tNETLINK_URELEASE, &n);\n\t}\n\n\tmodule_put(nlk->module);\n\n\tif (netlink_is_kernel(sk)) {\n\t\tnetlink_table_grab();\n\t\tBUG_ON(nl_table[sk->sk_protocol].registered == 0);\n\t\tif (--nl_table[sk->sk_protocol].registered == 0) {\n\t\t\tstruct listeners *old;\n\n\t\t\told = nl_deref_protected(nl_table[sk->sk_protocol].listeners);\n\t\t\tRCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL);\n\t\t\tkfree_rcu(old, rcu);\n\t\t\tnl_table[sk->sk_protocol].module = NULL;\n\t\t\tnl_table[sk->sk_protocol].bind = NULL;\n\t\t\tnl_table[sk->sk_protocol].unbind = NULL;\n\t\t\tnl_table[sk->sk_protocol].flags = 0;\n\t\t\tnl_table[sk->sk_protocol].registered = 0;\n\t\t}\n\t\tnetlink_table_ungrab();\n\t}\n\n\tkfree(nlk->groups);\n\tnlk->groups = NULL;\n\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), &netlink_proto, -1);\n\tlocal_bh_enable();\n\tcall_rcu(&nlk->rcu, deferred_put_nlk_sk);\n\treturn 0;\n}\n\nstatic int netlink_autobind(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_table *table = &nl_table[sk->sk_protocol];\n\ts32 portid = task_tgid_vnr(current);\n\tint err;\n\ts32 rover = -4096;\n\tbool ok;\n\nretry:\n\tcond_resched();\n\trcu_read_lock();\n\tok = !__netlink_lookup(table, portid, net);\n\trcu_read_unlock();\n\tif (!ok) {\n\t\t/* Bind collision, search negative portid values. */\n\t\tif (rover == -4096)\n\t\t\t/* rover will be in range [S32_MIN, -4097] */\n\t\t\trover = S32_MIN + prandom_u32_max(-4096 - S32_MIN);\n\t\telse if (rover >= -4096)\n\t\t\trover = -4097;\n\t\tportid = rover--;\n\t\tgoto retry;\n\t}\n\n\terr = netlink_insert(sk, portid);\n\tif (err == -EADDRINUSE)\n\t\tgoto retry;\n\n\t/* If 2 threads race to autobind, that is fine.  */\n\tif (err == -EBUSY)\n\t\terr = 0;\n\n\treturn err;\n}\n\n/**\n * __netlink_ns_capable - General netlink message capability test\n * @nsp: NETLINK_CB of the socket buffer holding a netlink command from userspace.\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in the user namespace @user_ns.\n */\nbool __netlink_ns_capable(const struct netlink_skb_parms *nsp,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn ((nsp->flags & NETLINK_SKB_DST) ||\n\t\tfile_ns_capable(nsp->sk->sk_socket->file, user_ns, cap)) &&\n\t\tns_capable(user_ns, cap);\n}\nEXPORT_SYMBOL(__netlink_ns_capable);\n\n/**\n * netlink_ns_capable - General netlink message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @user_ns: The user namespace of the capability to use\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in the user namespace @user_ns.\n */\nbool netlink_ns_capable(const struct sk_buff *skb,\n\t\t\tstruct user_namespace *user_ns, int cap)\n{\n\treturn __netlink_ns_capable(&NETLINK_CB(skb), user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_ns_capable);\n\n/**\n * netlink_capable - Netlink global message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap in all user namespaces.\n */\nbool netlink_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, &init_user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_capable);\n\n/**\n * netlink_net_capable - Netlink network namespace message capability test\n * @skb: socket buffer holding a netlink command from userspace\n * @cap: The capability to use\n *\n * Test to see if the opener of the socket we received the message\n * from had when the netlink socket was created and the sender of the\n * message has has the capability @cap over the network namespace of\n * the socket we received the message from.\n */\nbool netlink_net_capable(const struct sk_buff *skb, int cap)\n{\n\treturn netlink_ns_capable(skb, sock_net(skb->sk)->user_ns, cap);\n}\nEXPORT_SYMBOL(netlink_net_capable);\n\nstatic inline int netlink_allowed(const struct socket *sock, unsigned int flag)\n{\n\treturn (nl_table[sock->sk->sk_protocol].flags & flag) ||\n\t\tns_capable(sock_net(sock->sk)->user_ns, CAP_NET_ADMIN);\n}\n\nstatic void\nnetlink_update_subscriptions(struct sock *sk, unsigned int subscriptions)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (nlk->subscriptions && !subscriptions)\n\t\t__sk_del_bind_node(sk);\n\telse if (!nlk->subscriptions && subscriptions)\n\t\tsk_add_bind_node(sk, &nl_table[sk->sk_protocol].mc_list);\n\tnlk->subscriptions = subscriptions;\n}\n\nstatic int netlink_realloc_groups(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int groups;\n\tunsigned long *new_groups;\n\tint err = 0;\n\n\tnetlink_table_grab();\n\n\tgroups = nl_table[sk->sk_protocol].groups;\n\tif (!nl_table[sk->sk_protocol].registered) {\n\t\terr = -ENOENT;\n\t\tgoto out_unlock;\n\t}\n\n\tif (nlk->ngroups >= groups)\n\t\tgoto out_unlock;\n\n\tnew_groups = krealloc(nlk->groups, NLGRPSZ(groups), GFP_ATOMIC);\n\tif (new_groups == NULL) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\tmemset((char *)new_groups + NLGRPSZ(nlk->ngroups), 0,\n\t       NLGRPSZ(groups) - NLGRPSZ(nlk->ngroups));\n\n\tnlk->groups = new_groups;\n\tnlk->ngroups = groups;\n out_unlock:\n\tnetlink_table_ungrab();\n\treturn err;\n}\n\nstatic void netlink_undo_bind(int group, long unsigned int groups,\n\t\t\t      struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint undo;\n\n\tif (!nlk->netlink_unbind)\n\t\treturn;\n\n\tfor (undo = 0; undo < group; undo++)\n\t\tif (test_bit(undo, &groups))\n\t\t\tnlk->netlink_unbind(sock_net(sk), undo + 1);\n}\n\nstatic int netlink_bind(struct socket *sock, struct sockaddr *addr,\n\t\t\tint addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\tint err;\n\tlong unsigned int groups = nladdr->nl_groups;\n\tbool bound;\n\n\tif (addr_len < sizeof(struct sockaddr_nl))\n\t\treturn -EINVAL;\n\n\tif (nladdr->nl_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\n\t/* Only superuser is allowed to listen multicasts */\n\tif (groups) {\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tbound = nlk->bound;\n\tif (bound) {\n\t\t/* Ensure nlk->portid is up-to-date. */\n\t\tsmp_rmb();\n\n\t\tif (nladdr->nl_pid != nlk->portid)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (nlk->netlink_bind && groups) {\n\t\tint group;\n\n\t\tfor (group = 0; group < nlk->ngroups; group++) {\n\t\t\tif (!test_bit(group, &groups))\n\t\t\t\tcontinue;\n\t\t\terr = nlk->netlink_bind(net, group + 1);\n\t\t\tif (!err)\n\t\t\t\tcontinue;\n\t\t\tnetlink_undo_bind(group, groups, sk);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t/* No need for barriers here as we return to user-space without\n\t * using any of the bound attributes.\n\t */\n\tif (!bound) {\n\t\terr = nladdr->nl_pid ?\n\t\t\tnetlink_insert(sk, nladdr->nl_pid) :\n\t\t\tnetlink_autobind(sock);\n\t\tif (err) {\n\t\t\tnetlink_undo_bind(nlk->ngroups, groups, sk);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (!groups && (nlk->groups == NULL || !(u32)nlk->groups[0]))\n\t\treturn 0;\n\n\tnetlink_table_grab();\n\tnetlink_update_subscriptions(sk, nlk->subscriptions +\n\t\t\t\t\t hweight32(groups) -\n\t\t\t\t\t hweight32(nlk->groups[0]));\n\tnlk->groups[0] = (nlk->groups[0] & ~0xffffffffUL) | groups;\n\tnetlink_update_listeners(sk);\n\tnetlink_table_ungrab();\n\n\treturn 0;\n}\n\nstatic int netlink_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int alen, int flags)\n{\n\tint err = 0;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\n\n\tif (alen < sizeof(addr->sa_family))\n\t\treturn -EINVAL;\n\n\tif (addr->sa_family == AF_UNSPEC) {\n\t\tsk->sk_state\t= NETLINK_UNCONNECTED;\n\t\tnlk->dst_portid\t= 0;\n\t\tnlk->dst_group  = 0;\n\t\treturn 0;\n\t}\n\tif (addr->sa_family != AF_NETLINK)\n\t\treturn -EINVAL;\n\n\tif ((nladdr->nl_groups || nladdr->nl_pid) &&\n\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\treturn -EPERM;\n\n\t/* No need for barriers here as we return to user-space without\n\t * using any of the bound attributes.\n\t */\n\tif (!nlk->bound)\n\t\terr = netlink_autobind(sock);\n\n\tif (err == 0) {\n\t\tsk->sk_state\t= NETLINK_CONNECTED;\n\t\tnlk->dst_portid = nladdr->nl_pid;\n\t\tnlk->dst_group  = ffs(nladdr->nl_groups);\n\t}\n\n\treturn err;\n}\n\nstatic int netlink_getname(struct socket *sock, struct sockaddr *addr,\n\t\t\t   int *addr_len, int peer)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, nladdr, addr);\n\n\tnladdr->nl_family = AF_NETLINK;\n\tnladdr->nl_pad = 0;\n\t*addr_len = sizeof(*nladdr);\n\n\tif (peer) {\n\t\tnladdr->nl_pid = nlk->dst_portid;\n\t\tnladdr->nl_groups = netlink_group_mask(nlk->dst_group);\n\t} else {\n\t\tnladdr->nl_pid = nlk->portid;\n\t\tnladdr->nl_groups = nlk->groups ? nlk->groups[0] : 0;\n\t}\n\treturn 0;\n}\n\nstatic int netlink_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\t unsigned long arg)\n{\n\t/* try to hand this ioctl down to the NIC drivers.\n\t */\n\treturn -ENOIOCTLCMD;\n}\n\nstatic struct sock *netlink_getsockbyportid(struct sock *ssk, u32 portid)\n{\n\tstruct sock *sock;\n\tstruct netlink_sock *nlk;\n\n\tsock = netlink_lookup(sock_net(ssk), ssk->sk_protocol, portid);\n\tif (!sock)\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\n\t/* Don't bother queuing skb if kernel socket has no input function */\n\tnlk = nlk_sk(sock);\n\tif (sock->sk_state == NETLINK_CONNECTED &&\n\t    nlk->dst_portid != nlk_sk(ssk)->portid) {\n\t\tsock_put(sock);\n\t\treturn ERR_PTR(-ECONNREFUSED);\n\t}\n\treturn sock;\n}\n\nstruct sock *netlink_getsockbyfilp(struct file *filp)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct sock *sock;\n\n\tif (!S_ISSOCK(inode->i_mode))\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\tsock = SOCKET_I(inode)->sk;\n\tif (sock->sk_family != AF_NETLINK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tsock_hold(sock);\n\treturn sock;\n}\n\nstatic struct sk_buff *netlink_alloc_large_skb(unsigned int size,\n\t\t\t\t\t       int broadcast)\n{\n\tstruct sk_buff *skb;\n\tvoid *data;\n\n\tif (size <= NLMSG_GOODSIZE || broadcast)\n\t\treturn alloc_skb(size, GFP_KERNEL);\n\n\tsize = SKB_DATA_ALIGN(size) +\n\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tdata = vmalloc(size);\n\tif (data == NULL)\n\t\treturn NULL;\n\n\tskb = __build_skb(data, size);\n\tif (skb == NULL)\n\t\tvfree(data);\n\telse\n\t\tskb->destructor = netlink_skb_destructor;\n\n\treturn skb;\n}\n\n/*\n * Attach a skb to a netlink socket.\n * The caller must hold a reference to the destination socket. On error, the\n * reference is dropped. The skb is not send to the destination, just all\n * all error checks are performed and memory in the queue is reserved.\n * Return values:\n * < 0: error. skb freed, reference to sock dropped.\n * 0: continue\n * 1: repeat lookup - reference dropped while waiting for socket memory.\n */\nint netlink_attachskb(struct sock *sk, struct sk_buff *skb,\n\t\t      long *timeo, struct sock *ssk)\n{\n\tstruct netlink_sock *nlk;\n\n\tnlk = nlk_sk(sk);\n\n\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t     test_bit(NETLINK_S_CONGESTED, &nlk->state))) {\n\t\tDECLARE_WAITQUEUE(wait, current);\n\t\tif (!*timeo) {\n\t\t\tif (!ssk || netlink_is_kernel(ssk))\n\t\t\t\tnetlink_overrun(sk);\n\t\t\tsock_put(sk);\n\t\t\tkfree_skb(skb);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\t\tadd_wait_queue(&nlk->wait, &wait);\n\n\t\tif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\n\t\t     test_bit(NETLINK_S_CONGESTED, &nlk->state)) &&\n\t\t    !sock_flag(sk, SOCK_DEAD))\n\t\t\t*timeo = schedule_timeout(*timeo);\n\n\t\t__set_current_state(TASK_RUNNING);\n\t\tremove_wait_queue(&nlk->wait, &wait);\n\t\tsock_put(sk);\n\n\t\tif (signal_pending(current)) {\n\t\t\tkfree_skb(skb);\n\t\t\treturn sock_intr_errno(*timeo);\n\t\t}\n\t\treturn 1;\n\t}\n\tnetlink_skb_set_owner_r(skb, sk);\n\treturn 0;\n}\n\nstatic int __netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = skb->len;\n\n\tnetlink_deliver_tap(skb);\n\n\tskb_queue_tail(&sk->sk_receive_queue, skb);\n\tsk->sk_data_ready(sk);\n\treturn len;\n}\n\nint netlink_sendskb(struct sock *sk, struct sk_buff *skb)\n{\n\tint len = __netlink_sendskb(sk, skb);\n\n\tsock_put(sk);\n\treturn len;\n}\n\nvoid netlink_detachskb(struct sock *sk, struct sk_buff *skb)\n{\n\tkfree_skb(skb);\n\tsock_put(sk);\n}\n\nstatic struct sk_buff *netlink_trim(struct sk_buff *skb, gfp_t allocation)\n{\n\tint delta;\n\n\tWARN_ON(skb->sk != NULL);\n\tdelta = skb->end - skb->tail;\n\tif (is_vmalloc_addr(skb->head) || delta * 2 < skb->truesize)\n\t\treturn skb;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, allocation);\n\t\tif (!nskb)\n\t\t\treturn skb;\n\t\tconsume_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tif (!pskb_expand_head(skb, 0, -delta, allocation))\n\t\tskb->truesize -= delta;\n\n\treturn skb;\n}\n\nstatic int netlink_unicast_kernel(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct sock *ssk)\n{\n\tint ret;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tret = -ECONNREFUSED;\n\tif (nlk->netlink_rcv != NULL) {\n\t\tret = skb->len;\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\tNETLINK_CB(skb).sk = ssk;\n\t\tnetlink_deliver_tap_kernel(sk, ssk, skb);\n\t\tnlk->netlink_rcv(skb);\n\t\tconsume_skb(skb);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\tsock_put(sk);\n\treturn ret;\n}\n\nint netlink_unicast(struct sock *ssk, struct sk_buff *skb,\n\t\t    u32 portid, int nonblock)\n{\n\tstruct sock *sk;\n\tint err;\n\tlong timeo;\n\n\tskb = netlink_trim(skb, gfp_any());\n\n\ttimeo = sock_sndtimeo(ssk, nonblock);\nretry:\n\tsk = netlink_getsockbyportid(ssk, portid);\n\tif (IS_ERR(sk)) {\n\t\tkfree_skb(skb);\n\t\treturn PTR_ERR(sk);\n\t}\n\tif (netlink_is_kernel(sk))\n\t\treturn netlink_unicast_kernel(sk, skb, ssk);\n\n\tif (sk_filter(sk, skb)) {\n\t\terr = skb->len;\n\t\tkfree_skb(skb);\n\t\tsock_put(sk);\n\t\treturn err;\n\t}\n\n\terr = netlink_attachskb(sk, skb, &timeo, ssk);\n\tif (err == 1)\n\t\tgoto retry;\n\tif (err)\n\t\treturn err;\n\n\treturn netlink_sendskb(sk, skb);\n}\nEXPORT_SYMBOL(netlink_unicast);\n\nint netlink_has_listeners(struct sock *sk, unsigned int group)\n{\n\tint res = 0;\n\tstruct listeners *listeners;\n\n\tBUG_ON(!netlink_is_kernel(sk));\n\n\trcu_read_lock();\n\tlisteners = rcu_dereference(nl_table[sk->sk_protocol].listeners);\n\n\tif (listeners && group - 1 < nl_table[sk->sk_protocol].groups)\n\t\tres = test_bit(group - 1, listeners->masks);\n\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL_GPL(netlink_has_listeners);\n\nstatic int netlink_broadcast_deliver(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\n\tif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\n\t    !test_bit(NETLINK_S_CONGESTED, &nlk->state)) {\n\t\tnetlink_skb_set_owner_r(skb, sk);\n\t\t__netlink_sendskb(sk, skb);\n\t\treturn atomic_read(&sk->sk_rmem_alloc) > (sk->sk_rcvbuf >> 1);\n\t}\n\treturn -1;\n}\n\nstruct netlink_broadcast_data {\n\tstruct sock *exclude_sk;\n\tstruct net *net;\n\tu32 portid;\n\tu32 group;\n\tint failure;\n\tint delivery_failure;\n\tint congested;\n\tint delivered;\n\tgfp_t allocation;\n\tstruct sk_buff *skb, *skb2;\n\tint (*tx_filter)(struct sock *dsk, struct sk_buff *skb, void *data);\n\tvoid *tx_data;\n};\n\nstatic void do_one_broadcast(struct sock *sk,\n\t\t\t\t    struct netlink_broadcast_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint val;\n\n\tif (p->exclude_sk == sk)\n\t\treturn;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\treturn;\n\n\tif (!net_eq(sock_net(sk), p->net)) {\n\t\tif (!(nlk->flags & NETLINK_F_LISTEN_ALL_NSID))\n\t\t\treturn;\n\n\t\tif (!peernet_has_id(sock_net(sk), p->net))\n\t\t\treturn;\n\n\t\tif (!file_ns_capable(sk->sk_socket->file, p->net->user_ns,\n\t\t\t\t     CAP_NET_BROADCAST))\n\t\t\treturn;\n\t}\n\n\tif (p->failure) {\n\t\tnetlink_overrun(sk);\n\t\treturn;\n\t}\n\n\tsock_hold(sk);\n\tif (p->skb2 == NULL) {\n\t\tif (skb_shared(p->skb)) {\n\t\t\tp->skb2 = skb_clone(p->skb, p->allocation);\n\t\t} else {\n\t\t\tp->skb2 = skb_get(p->skb);\n\t\t\t/*\n\t\t\t * skb ownership may have been set when\n\t\t\t * delivered to a previous socket.\n\t\t\t */\n\t\t\tskb_orphan(p->skb2);\n\t\t}\n\t}\n\tif (p->skb2 == NULL) {\n\t\tnetlink_overrun(sk);\n\t\t/* Clone failed. Notify ALL listeners. */\n\t\tp->failure = 1;\n\t\tif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\n\t\t\tp->delivery_failure = 1;\n\t\tgoto out;\n\t}\n\tif (p->tx_filter && p->tx_filter(sk, p->skb2, p->tx_data)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\tif (sk_filter(sk, p->skb2)) {\n\t\tkfree_skb(p->skb2);\n\t\tp->skb2 = NULL;\n\t\tgoto out;\n\t}\n\tNETLINK_CB(p->skb2).nsid = peernet2id(sock_net(sk), p->net);\n\tNETLINK_CB(p->skb2).nsid_is_set = true;\n\tval = netlink_broadcast_deliver(sk, p->skb2);\n\tif (val < 0) {\n\t\tnetlink_overrun(sk);\n\t\tif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\n\t\t\tp->delivery_failure = 1;\n\t} else {\n\t\tp->congested |= val;\n\t\tp->delivered = 1;\n\t\tp->skb2 = NULL;\n\t}\nout:\n\tsock_put(sk);\n}\n\nint netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid,\n\tu32 group, gfp_t allocation,\n\tint (*filter)(struct sock *dsk, struct sk_buff *skb, void *data),\n\tvoid *filter_data)\n{\n\tstruct net *net = sock_net(ssk);\n\tstruct netlink_broadcast_data info;\n\tstruct sock *sk;\n\n\tskb = netlink_trim(skb, allocation);\n\n\tinfo.exclude_sk = ssk;\n\tinfo.net = net;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\tinfo.failure = 0;\n\tinfo.delivery_failure = 0;\n\tinfo.congested = 0;\n\tinfo.delivered = 0;\n\tinfo.allocation = allocation;\n\tinfo.skb = skb;\n\tinfo.skb2 = NULL;\n\tinfo.tx_filter = filter;\n\tinfo.tx_data = filter_data;\n\n\t/* While we sleep in clone, do not allow to change socket list */\n\n\tnetlink_lock_table();\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tdo_one_broadcast(sk, &info);\n\n\tconsume_skb(skb);\n\n\tnetlink_unlock_table();\n\n\tif (info.delivery_failure) {\n\t\tkfree_skb(info.skb2);\n\t\treturn -ENOBUFS;\n\t}\n\tconsume_skb(info.skb2);\n\n\tif (info.delivered) {\n\t\tif (info.congested && gfpflags_allow_blocking(allocation))\n\t\t\tyield();\n\t\treturn 0;\n\t}\n\treturn -ESRCH;\n}\nEXPORT_SYMBOL(netlink_broadcast_filtered);\n\nint netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 portid,\n\t\t      u32 group, gfp_t allocation)\n{\n\treturn netlink_broadcast_filtered(ssk, skb, portid, group, allocation,\n\t\tNULL, NULL);\n}\nEXPORT_SYMBOL(netlink_broadcast);\n\nstruct netlink_set_err_data {\n\tstruct sock *exclude_sk;\n\tu32 portid;\n\tu32 group;\n\tint code;\n};\n\nstatic int do_one_set_err(struct sock *sk, struct netlink_set_err_data *p)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint ret = 0;\n\n\tif (sk == p->exclude_sk)\n\t\tgoto out;\n\n\tif (!net_eq(sock_net(sk), sock_net(p->exclude_sk)))\n\t\tgoto out;\n\n\tif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\n\t    !test_bit(p->group - 1, nlk->groups))\n\t\tgoto out;\n\n\tif (p->code == ENOBUFS && nlk->flags & NETLINK_F_RECV_NO_ENOBUFS) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tsk->sk_err = p->code;\n\tsk->sk_error_report(sk);\nout:\n\treturn ret;\n}\n\n/**\n * netlink_set_err - report error to broadcast listeners\n * @ssk: the kernel netlink socket, as returned by netlink_kernel_create()\n * @portid: the PORTID of a process that we want to skip (if any)\n * @group: the broadcast group that will notice the error\n * @code: error code, must be negative (as usual in kernelspace)\n *\n * This function returns the number of broadcast listeners that have set the\n * NETLINK_NO_ENOBUFS socket option.\n */\nint netlink_set_err(struct sock *ssk, u32 portid, u32 group, int code)\n{\n\tstruct netlink_set_err_data info;\n\tstruct sock *sk;\n\tint ret = 0;\n\n\tinfo.exclude_sk = ssk;\n\tinfo.portid = portid;\n\tinfo.group = group;\n\t/* sk->sk_err wants a positive error value */\n\tinfo.code = -code;\n\n\tread_lock(&nl_table_lock);\n\n\tsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\n\t\tret += do_one_set_err(sk, &info);\n\n\tread_unlock(&nl_table_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(netlink_set_err);\n\n/* must be called with netlink table grabbed */\nstatic void netlink_update_socket_mc(struct netlink_sock *nlk,\n\t\t\t\t     unsigned int group,\n\t\t\t\t     int is_new)\n{\n\tint old, new = !!is_new, subscriptions;\n\n\told = test_bit(group - 1, nlk->groups);\n\tsubscriptions = nlk->subscriptions - old + new;\n\tif (new)\n\t\t__set_bit(group - 1, nlk->groups);\n\telse\n\t\t__clear_bit(group - 1, nlk->groups);\n\tnetlink_update_subscriptions(&nlk->sk, subscriptions);\n\tnetlink_update_listeners(&nlk->sk);\n}\n\nstatic int netlink_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t      char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tunsigned int val = 0;\n\tint err;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (optlen >= sizeof(int) &&\n\t    get_user(val, (unsigned int __user *)optval))\n\t\treturn -EFAULT;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_RECV_PKTINFO;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_RECV_PKTINFO;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_ADD_MEMBERSHIP:\n\tcase NETLINK_DROP_MEMBERSHIP: {\n\t\tif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\n\t\t\treturn -EPERM;\n\t\terr = netlink_realloc_groups(sk);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (!val || val - 1 >= nlk->ngroups)\n\t\t\treturn -EINVAL;\n\t\tif (optname == NETLINK_ADD_MEMBERSHIP && nlk->netlink_bind) {\n\t\t\terr = nlk->netlink_bind(sock_net(sk), val);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tnetlink_table_grab();\n\t\tnetlink_update_socket_mc(nlk, val,\n\t\t\t\t\t optname == NETLINK_ADD_MEMBERSHIP);\n\t\tnetlink_table_ungrab();\n\t\tif (optname == NETLINK_DROP_MEMBERSHIP && nlk->netlink_unbind)\n\t\t\tnlk->netlink_unbind(sock_net(sk), val);\n\n\t\terr = 0;\n\t\tbreak;\n\t}\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_BROADCAST_SEND_ERROR;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_BROADCAST_SEND_ERROR;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tif (val) {\n\t\t\tnlk->flags |= NETLINK_F_RECV_NO_ENOBUFS;\n\t\t\tclear_bit(NETLINK_S_CONGESTED, &nlk->state);\n\t\t\twake_up_interruptible(&nlk->wait);\n\t\t} else {\n\t\t\tnlk->flags &= ~NETLINK_F_RECV_NO_ENOBUFS;\n\t\t}\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_LISTEN_ALL_NSID:\n\t\tif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_BROADCAST))\n\t\t\treturn -EPERM;\n\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_LISTEN_ALL_NSID;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_LISTEN_ALL_NSID;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_CAP_ACK:\n\t\tif (val)\n\t\t\tnlk->flags |= NETLINK_F_CAP_ACK;\n\t\telse\n\t\t\tnlk->flags &= ~NETLINK_F_CAP_ACK;\n\t\terr = 0;\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t}\n\treturn err;\n}\n\nstatic int netlink_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t      char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint len, val, err;\n\n\tif (level != SOL_NETLINK)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase NETLINK_PKTINFO:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_RECV_PKTINFO ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_BROADCAST_ERROR:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_NO_ENOBUFS:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_RECV_NO_ENOBUFS ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tcase NETLINK_LIST_MEMBERSHIPS: {\n\t\tint pos, idx, shift;\n\n\t\terr = 0;\n\t\tnetlink_lock_table();\n\t\tfor (pos = 0; pos * 8 < nlk->ngroups; pos += sizeof(u32)) {\n\t\t\tif (len - pos < sizeof(u32))\n\t\t\t\tbreak;\n\n\t\t\tidx = pos / sizeof(unsigned long);\n\t\t\tshift = (pos % sizeof(unsigned long)) * 8;\n\t\t\tif (put_user((u32)(nlk->groups[idx] >> shift),\n\t\t\t\t     (u32 __user *)(optval + pos))) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (put_user(ALIGN(nlk->ngroups / 8, sizeof(u32)), optlen))\n\t\t\terr = -EFAULT;\n\t\tnetlink_unlock_table();\n\t\tbreak;\n\t}\n\tcase NETLINK_CAP_ACK:\n\t\tif (len < sizeof(int))\n\t\t\treturn -EINVAL;\n\t\tlen = sizeof(int);\n\t\tval = nlk->flags & NETLINK_F_CAP_ACK ? 1 : 0;\n\t\tif (put_user(len, optlen) ||\n\t\t    put_user(val, optval))\n\t\t\treturn -EFAULT;\n\t\terr = 0;\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t}\n\treturn err;\n}\n\nstatic void netlink_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\n{\n\tstruct nl_pktinfo info;\n\n\tinfo.group = NETLINK_CB(skb).dst_group;\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_PKTINFO, sizeof(info), &info);\n}\n\nstatic void netlink_cmsg_listen_all_nsid(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t struct sk_buff *skb)\n{\n\tif (!NETLINK_CB(skb).nsid_is_set)\n\t\treturn;\n\n\tput_cmsg(msg, SOL_NETLINK, NETLINK_LISTEN_ALL_NSID, sizeof(int),\n\t\t &NETLINK_CB(skb).nsid);\n}\n\nstatic int netlink_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\tu32 dst_portid;\n\tu32 dst_group;\n\tstruct sk_buff *skb;\n\tint err;\n\tstruct scm_cookie scm;\n\tu32 netlink_skb_flags = 0;\n\n\tif (msg->msg_flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\terr = scm_send(sock, msg, &scm, true);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (msg->msg_namelen) {\n\t\terr = -EINVAL;\n\t\tif (addr->nl_family != AF_NETLINK)\n\t\t\tgoto out;\n\t\tdst_portid = addr->nl_pid;\n\t\tdst_group = ffs(addr->nl_groups);\n\t\terr =  -EPERM;\n\t\tif ((dst_group || dst_portid) &&\n\t\t    !netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\n\t\t\tgoto out;\n\t\tnetlink_skb_flags |= NETLINK_SKB_DST;\n\t} else {\n\t\tdst_portid = nlk->dst_portid;\n\t\tdst_group = nlk->dst_group;\n\t}\n\n\tif (!nlk->bound) {\n\t\terr = netlink_autobind(sock);\n\t\tif (err)\n\t\t\tgoto out;\n\t} else {\n\t\t/* Ensure nlk is hashed and visible. */\n\t\tsmp_rmb();\n\t}\n\n\terr = -EMSGSIZE;\n\tif (len > sk->sk_sndbuf - 32)\n\t\tgoto out;\n\terr = -ENOBUFS;\n\tskb = netlink_alloc_large_skb(len, dst_group);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tNETLINK_CB(skb).portid\t= nlk->portid;\n\tNETLINK_CB(skb).dst_group = dst_group;\n\tNETLINK_CB(skb).creds\t= scm.creds;\n\tNETLINK_CB(skb).flags\t= netlink_skb_flags;\n\n\terr = -EFAULT;\n\tif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\terr = security_netlink_send(sk, skb);\n\tif (err) {\n\t\tkfree_skb(skb);\n\t\tgoto out;\n\t}\n\n\tif (dst_group) {\n\t\tatomic_inc(&skb->users);\n\t\tnetlink_broadcast(sk, skb, dst_portid, dst_group, GFP_KERNEL);\n\t}\n\terr = netlink_unicast(sk, skb, dst_portid, msg->msg_flags&MSG_DONTWAIT);\n\nout:\n\tscm_destroy(&scm);\n\treturn err;\n}\n\nstatic int netlink_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\n\t\t\t   int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tint noblock = flags&MSG_DONTWAIT;\n\tsize_t copied;\n\tstruct sk_buff *skb, *data_skb;\n\tint err, ret;\n\n\tif (flags&MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tcopied = 0;\n\n\tskb = skb_recv_datagram(sk, flags, noblock, &err);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tdata_skb = skb;\n\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\n\tif (unlikely(skb_shinfo(skb)->frag_list)) {\n\t\t/*\n\t\t * If this skb has a frag_list, then here that means that we\n\t\t * will have to use the frag_list skb's data for compat tasks\n\t\t * and the regular skb's data for normal (non-compat) tasks.\n\t\t *\n\t\t * If we need to send the compat skb, assign it to the\n\t\t * 'data_skb' variable so that it will be used below for data\n\t\t * copying. We keep 'skb' for everything else, including\n\t\t * freeing both later.\n\t\t */\n\t\tif (flags & MSG_CMSG_COMPAT)\n\t\t\tdata_skb = skb_shinfo(skb)->frag_list;\n\t}\n#endif\n\n\t/* Record the max length of recvmsg() calls for future allocations */\n\tnlk->max_recvmsg_len = max(nlk->max_recvmsg_len, len);\n\tnlk->max_recvmsg_len = min_t(size_t, nlk->max_recvmsg_len,\n\t\t\t\t     16384);\n\n\tcopied = data_skb->len;\n\tif (len < copied) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\n\tskb_reset_transport_header(data_skb);\n\terr = skb_copy_datagram_msg(data_skb, 0, msg, copied);\n\n\tif (msg->msg_name) {\n\t\tDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\n\t\taddr->nl_family = AF_NETLINK;\n\t\taddr->nl_pad    = 0;\n\t\taddr->nl_pid\t= NETLINK_CB(skb).portid;\n\t\taddr->nl_groups\t= netlink_group_mask(NETLINK_CB(skb).dst_group);\n\t\tmsg->msg_namelen = sizeof(*addr);\n\t}\n\n\tif (nlk->flags & NETLINK_F_RECV_PKTINFO)\n\t\tnetlink_cmsg_recv_pktinfo(msg, skb);\n\tif (nlk->flags & NETLINK_F_LISTEN_ALL_NSID)\n\t\tnetlink_cmsg_listen_all_nsid(sk, msg, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\tscm.creds = *NETLINK_CREDS(skb);\n\tif (flags & MSG_TRUNC)\n\t\tcopied = data_skb->len;\n\n\tskb_free_datagram(sk, skb);\n\n\tif (nlk->cb_running &&\n\t    atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\n\t\tret = netlink_dump(sk);\n\t\tif (ret) {\n\t\t\tsk->sk_err = -ret;\n\t\t\tsk->sk_error_report(sk);\n\t\t}\n\t}\n\n\tscm_recv(sock, msg, &scm, flags);\nout:\n\tnetlink_rcv_wake(sk);\n\treturn err ? : copied;\n}\n\nstatic void netlink_data_ready(struct sock *sk)\n{\n\tBUG();\n}\n\n/*\n *\tWe export these functions to other modules. They provide a\n *\tcomplete set of kernel non-blocking support for message\n *\tqueueing.\n */\n\nstruct sock *\n__netlink_kernel_create(struct net *net, int unit, struct module *module,\n\t\t\tstruct netlink_kernel_cfg *cfg)\n{\n\tstruct socket *sock;\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\tstruct listeners *listeners = NULL;\n\tstruct mutex *cb_mutex = cfg ? cfg->cb_mutex : NULL;\n\tunsigned int groups;\n\n\tBUG_ON(!nl_table);\n\n\tif (unit < 0 || unit >= MAX_LINKS)\n\t\treturn NULL;\n\n\tif (sock_create_lite(PF_NETLINK, SOCK_DGRAM, unit, &sock))\n\t\treturn NULL;\n\n\tif (__netlink_create(net, sock, cb_mutex, unit, 1) < 0)\n\t\tgoto out_sock_release_nosk;\n\n\tsk = sock->sk;\n\n\tif (!cfg || cfg->groups < 32)\n\t\tgroups = 32;\n\telse\n\t\tgroups = cfg->groups;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tgoto out_sock_release;\n\n\tsk->sk_data_ready = netlink_data_ready;\n\tif (cfg && cfg->input)\n\t\tnlk_sk(sk)->netlink_rcv = cfg->input;\n\n\tif (netlink_insert(sk, 0))\n\t\tgoto out_sock_release;\n\n\tnlk = nlk_sk(sk);\n\tnlk->flags |= NETLINK_F_KERNEL_SOCKET;\n\n\tnetlink_table_grab();\n\tif (!nl_table[unit].registered) {\n\t\tnl_table[unit].groups = groups;\n\t\trcu_assign_pointer(nl_table[unit].listeners, listeners);\n\t\tnl_table[unit].cb_mutex = cb_mutex;\n\t\tnl_table[unit].module = module;\n\t\tif (cfg) {\n\t\t\tnl_table[unit].bind = cfg->bind;\n\t\t\tnl_table[unit].unbind = cfg->unbind;\n\t\t\tnl_table[unit].flags = cfg->flags;\n\t\t\tif (cfg->compare)\n\t\t\t\tnl_table[unit].compare = cfg->compare;\n\t\t}\n\t\tnl_table[unit].registered = 1;\n\t} else {\n\t\tkfree(listeners);\n\t\tnl_table[unit].registered++;\n\t}\n\tnetlink_table_ungrab();\n\treturn sk;\n\nout_sock_release:\n\tkfree(listeners);\n\tnetlink_kernel_release(sk);\n\treturn NULL;\n\nout_sock_release_nosk:\n\tsock_release(sock);\n\treturn NULL;\n}\nEXPORT_SYMBOL(__netlink_kernel_create);\n\nvoid\nnetlink_kernel_release(struct sock *sk)\n{\n\tif (sk == NULL || sk->sk_socket == NULL)\n\t\treturn;\n\n\tsock_release(sk->sk_socket);\n}\nEXPORT_SYMBOL(netlink_kernel_release);\n\nint __netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tstruct listeners *new, *old;\n\tstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\n\n\tif (groups < 32)\n\t\tgroups = 32;\n\n\tif (NLGRPSZ(tbl->groups) < NLGRPSZ(groups)) {\n\t\tnew = kzalloc(sizeof(*new) + NLGRPSZ(groups), GFP_ATOMIC);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t\told = nl_deref_protected(tbl->listeners);\n\t\tmemcpy(new->masks, old->masks, NLGRPSZ(tbl->groups));\n\t\trcu_assign_pointer(tbl->listeners, new);\n\n\t\tkfree_rcu(old, rcu);\n\t}\n\ttbl->groups = groups;\n\n\treturn 0;\n}\n\n/**\n * netlink_change_ngroups - change number of multicast groups\n *\n * This changes the number of multicast groups that are available\n * on a certain netlink family. Note that it is not possible to\n * change the number of groups to below 32. Also note that it does\n * not implicitly call netlink_clear_multicast_users() when the\n * number of groups is reduced.\n *\n * @sk: The kernel netlink socket, as returned by netlink_kernel_create().\n * @groups: The new number of groups.\n */\nint netlink_change_ngroups(struct sock *sk, unsigned int groups)\n{\n\tint err;\n\n\tnetlink_table_grab();\n\terr = __netlink_change_ngroups(sk, groups);\n\tnetlink_table_ungrab();\n\n\treturn err;\n}\n\nvoid __netlink_clear_multicast_users(struct sock *ksk, unsigned int group)\n{\n\tstruct sock *sk;\n\tstruct netlink_table *tbl = &nl_table[ksk->sk_protocol];\n\n\tsk_for_each_bound(sk, &tbl->mc_list)\n\t\tnetlink_update_socket_mc(nlk_sk(sk), group, 0);\n}\n\nstruct nlmsghdr *\n__nlmsg_put(struct sk_buff *skb, u32 portid, u32 seq, int type, int len, int flags)\n{\n\tstruct nlmsghdr *nlh;\n\tint size = nlmsg_msg_size(len);\n\n\tnlh = (struct nlmsghdr *)skb_put(skb, NLMSG_ALIGN(size));\n\tnlh->nlmsg_type = type;\n\tnlh->nlmsg_len = size;\n\tnlh->nlmsg_flags = flags;\n\tnlh->nlmsg_pid = portid;\n\tnlh->nlmsg_seq = seq;\n\tif (!__builtin_constant_p(size) || NLMSG_ALIGN(size) - size != 0)\n\t\tmemset(nlmsg_data(nlh) + len, 0, NLMSG_ALIGN(size) - size);\n\treturn nlh;\n}\nEXPORT_SYMBOL(__nlmsg_put);\n\n/*\n * It looks a bit ugly.\n * It would be better to create kernel thread.\n */\n\nstatic int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tstruct module *module;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL |\n\t\t\t\t\t    __GFP_NOWARN | __GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = alloc_skb(alloc_size, GFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmodule = cb->module;\n\tskb = cb->skb;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(module);\n\tconsume_skb(skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nint __netlink_dump_start(struct sock *ssk, struct sk_buff *skb,\n\t\t\t const struct nlmsghdr *nlh,\n\t\t\t struct netlink_dump_control *control)\n{\n\tstruct netlink_callback *cb;\n\tstruct sock *sk;\n\tstruct netlink_sock *nlk;\n\tint ret;\n\n\tatomic_inc(&skb->users);\n\n\tsk = netlink_lookup(sock_net(ssk), ssk->sk_protocol, NETLINK_CB(skb).portid);\n\tif (sk == NULL) {\n\t\tret = -ECONNREFUSED;\n\t\tgoto error_free;\n\t}\n\n\tnlk = nlk_sk(sk);\n\tmutex_lock(nlk->cb_mutex);\n\t/* A dump is in progress... */\n\tif (nlk->cb_running) {\n\t\tret = -EBUSY;\n\t\tgoto error_unlock;\n\t}\n\t/* add reference of module which cb->dump belongs to */\n\tif (!try_module_get(control->module)) {\n\t\tret = -EPROTONOSUPPORT;\n\t\tgoto error_unlock;\n\t}\n\n\tcb = &nlk->cb;\n\tmemset(cb, 0, sizeof(*cb));\n\tcb->start = control->start;\n\tcb->dump = control->dump;\n\tcb->done = control->done;\n\tcb->nlh = nlh;\n\tcb->data = control->data;\n\tcb->module = control->module;\n\tcb->min_dump_alloc = control->min_dump_alloc;\n\tcb->skb = skb;\n\n\tnlk->cb_running = true;\n\n\tmutex_unlock(nlk->cb_mutex);\n\n\tif (cb->start)\n\t\tcb->start(cb);\n\n\tret = netlink_dump(sk);\n\tsock_put(sk);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/* We successfully started a dump, by returning -EINTR we\n\t * signal not to send ACK even if it was requested.\n\t */\n\treturn -EINTR;\n\nerror_unlock:\n\tsock_put(sk);\n\tmutex_unlock(nlk->cb_mutex);\nerror_free:\n\tkfree_skb(skb);\n\treturn ret;\n}\nEXPORT_SYMBOL(__netlink_dump_start);\n\nvoid netlink_ack(struct sk_buff *in_skb, struct nlmsghdr *nlh, int err)\n{\n\tstruct sk_buff *skb;\n\tstruct nlmsghdr *rep;\n\tstruct nlmsgerr *errmsg;\n\tsize_t payload = sizeof(*errmsg);\n\tstruct netlink_sock *nlk = nlk_sk(NETLINK_CB(in_skb).sk);\n\n\t/* Error messages get the original request appened, unless the user\n\t * requests to cap the error message.\n\t */\n\tif (!(nlk->flags & NETLINK_F_CAP_ACK) && err)\n\t\tpayload += nlmsg_len(nlh);\n\n\tskb = nlmsg_new(payload, GFP_KERNEL);\n\tif (!skb) {\n\t\tstruct sock *sk;\n\n\t\tsk = netlink_lookup(sock_net(in_skb->sk),\n\t\t\t\t    in_skb->sk->sk_protocol,\n\t\t\t\t    NETLINK_CB(in_skb).portid);\n\t\tif (sk) {\n\t\t\tsk->sk_err = ENOBUFS;\n\t\t\tsk->sk_error_report(sk);\n\t\t\tsock_put(sk);\n\t\t}\n\t\treturn;\n\t}\n\n\trep = __nlmsg_put(skb, NETLINK_CB(in_skb).portid, nlh->nlmsg_seq,\n\t\t\t  NLMSG_ERROR, payload, 0);\n\terrmsg = nlmsg_data(rep);\n\terrmsg->error = err;\n\tmemcpy(&errmsg->msg, nlh, payload > sizeof(*errmsg) ? nlh->nlmsg_len : sizeof(*nlh));\n\tnetlink_unicast(in_skb->sk, skb, NETLINK_CB(in_skb).portid, MSG_DONTWAIT);\n}\nEXPORT_SYMBOL(netlink_ack);\n\nint netlink_rcv_skb(struct sk_buff *skb, int (*cb)(struct sk_buff *,\n\t\t\t\t\t\t     struct nlmsghdr *))\n{\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\twhile (skb->len >= nlmsg_total_size(0)) {\n\t\tint msglen;\n\n\t\tnlh = nlmsg_hdr(skb);\n\t\terr = 0;\n\n\t\tif (nlh->nlmsg_len < NLMSG_HDRLEN || skb->len < nlh->nlmsg_len)\n\t\t\treturn 0;\n\n\t\t/* Only requests are handled by the kernel */\n\t\tif (!(nlh->nlmsg_flags & NLM_F_REQUEST))\n\t\t\tgoto ack;\n\n\t\t/* Skip control messages */\n\t\tif (nlh->nlmsg_type < NLMSG_MIN_TYPE)\n\t\t\tgoto ack;\n\n\t\terr = cb(skb, nlh);\n\t\tif (err == -EINTR)\n\t\t\tgoto skip;\n\nack:\n\t\tif (nlh->nlmsg_flags & NLM_F_ACK || err)\n\t\t\tnetlink_ack(skb, nlh, err);\n\nskip:\n\t\tmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\n\t\tif (msglen > skb->len)\n\t\t\tmsglen = skb->len;\n\t\tskb_pull(skb, msglen);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(netlink_rcv_skb);\n\n/**\n * nlmsg_notify - send a notification netlink message\n * @sk: netlink socket to use\n * @skb: notification message\n * @portid: destination netlink portid for reports or 0\n * @group: destination multicast group or 0\n * @report: 1 to report back, 0 to disable\n * @flags: allocation flags\n */\nint nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid,\n\t\t unsigned int group, int report, gfp_t flags)\n{\n\tint err = 0;\n\n\tif (group) {\n\t\tint exclude_portid = 0;\n\n\t\tif (report) {\n\t\t\tatomic_inc(&skb->users);\n\t\t\texclude_portid = portid;\n\t\t}\n\n\t\t/* errors reported via destination sk->sk_err, but propagate\n\t\t * delivery errors if NETLINK_BROADCAST_ERROR flag is set */\n\t\terr = nlmsg_multicast(sk, skb, exclude_portid, group, flags);\n\t}\n\n\tif (report) {\n\t\tint err2;\n\n\t\terr2 = nlmsg_unicast(sk, skb, portid);\n\t\tif (!err || err == -ESRCH)\n\t\t\terr = err2;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(nlmsg_notify);\n\n#ifdef CONFIG_PROC_FS\nstruct nl_seq_iter {\n\tstruct seq_net_private p;\n\tstruct rhashtable_iter hti;\n\tint link;\n};\n\nstatic int netlink_walk_start(struct nl_seq_iter *iter)\n{\n\tint err;\n\n\terr = rhashtable_walk_init(&nl_table[iter->link].hash, &iter->hti,\n\t\t\t\t   GFP_KERNEL);\n\tif (err) {\n\t\titer->link = MAX_LINKS;\n\t\treturn err;\n\t}\n\n\terr = rhashtable_walk_start(&iter->hti);\n\treturn err == -EAGAIN ? 0 : err;\n}\n\nstatic void netlink_walk_stop(struct nl_seq_iter *iter)\n{\n\trhashtable_walk_stop(&iter->hti);\n\trhashtable_walk_exit(&iter->hti);\n}\n\nstatic void *__netlink_seq_next(struct seq_file *seq)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tstruct netlink_sock *nlk;\n\n\tdo {\n\t\tfor (;;) {\n\t\t\tint err;\n\n\t\t\tnlk = rhashtable_walk_next(&iter->hti);\n\n\t\t\tif (IS_ERR(nlk)) {\n\t\t\t\tif (PTR_ERR(nlk) == -EAGAIN)\n\t\t\t\t\tcontinue;\n\n\t\t\t\treturn nlk;\n\t\t\t}\n\n\t\t\tif (nlk)\n\t\t\t\tbreak;\n\n\t\t\tnetlink_walk_stop(iter);\n\t\t\tif (++iter->link >= MAX_LINKS)\n\t\t\t\treturn NULL;\n\n\t\t\terr = netlink_walk_start(iter);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\t\t}\n\t} while (sock_net(&nlk->sk) != seq_file_net(seq));\n\n\treturn nlk;\n}\n\nstatic void *netlink_seq_start(struct seq_file *seq, loff_t *posp)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\tvoid *obj = SEQ_START_TOKEN;\n\tloff_t pos;\n\tint err;\n\n\titer->link = 0;\n\n\terr = netlink_walk_start(iter);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\tfor (pos = *posp; pos && obj && !IS_ERR(obj); pos--)\n\t\tobj = __netlink_seq_next(seq);\n\n\treturn obj;\n}\n\nstatic void *netlink_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn __netlink_seq_next(seq);\n}\n\nstatic void netlink_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct nl_seq_iter *iter = seq->private;\n\n\tif (iter->link >= MAX_LINKS)\n\t\treturn;\n\n\tnetlink_walk_stop(iter);\n}\n\n\nstatic int netlink_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN) {\n\t\tseq_puts(seq,\n\t\t\t \"sk       Eth Pid    Groups   \"\n\t\t\t \"Rmem     Wmem     Dump     Locks     Drops     Inode\\n\");\n\t} else {\n\t\tstruct sock *s = v;\n\t\tstruct netlink_sock *nlk = nlk_sk(s);\n\n\t\tseq_printf(seq, \"%pK %-3d %-6u %08x %-8d %-8d %d %-8d %-8d %-8lu\\n\",\n\t\t\t   s,\n\t\t\t   s->sk_protocol,\n\t\t\t   nlk->portid,\n\t\t\t   nlk->groups ? (u32)nlk->groups[0] : 0,\n\t\t\t   sk_rmem_alloc_get(s),\n\t\t\t   sk_wmem_alloc_get(s),\n\t\t\t   nlk->cb_running,\n\t\t\t   atomic_read(&s->sk_refcnt),\n\t\t\t   atomic_read(&s->sk_drops),\n\t\t\t   sock_i_ino(s)\n\t\t\t);\n\n\t}\n\treturn 0;\n}\n\nstatic const struct seq_operations netlink_seq_ops = {\n\t.start  = netlink_seq_start,\n\t.next   = netlink_seq_next,\n\t.stop   = netlink_seq_stop,\n\t.show   = netlink_seq_show,\n};\n\n\nstatic int netlink_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &netlink_seq_ops,\n\t\t\t\tsizeof(struct nl_seq_iter));\n}\n\nstatic const struct file_operations netlink_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= netlink_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nint netlink_register_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_register(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_register_notifier);\n\nint netlink_unregister_notifier(struct notifier_block *nb)\n{\n\treturn atomic_notifier_chain_unregister(&netlink_chain, nb);\n}\nEXPORT_SYMBOL(netlink_unregister_notifier);\n\nstatic const struct proto_ops netlink_ops = {\n\t.family =\tPF_NETLINK,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tnetlink_release,\n\t.bind =\t\tnetlink_bind,\n\t.connect =\tnetlink_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tnetlink_getname,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tnetlink_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tnetlink_setsockopt,\n\t.getsockopt =\tnetlink_getsockopt,\n\t.sendmsg =\tnetlink_sendmsg,\n\t.recvmsg =\tnetlink_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family netlink_family_ops = {\n\t.family = PF_NETLINK,\n\t.create = netlink_create,\n\t.owner\t= THIS_MODULE,\t/* for consistency 8) */\n};\n\nstatic int __net_init netlink_net_init(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create(\"netlink\", 0, net->proc_net, &netlink_seq_fops))\n\t\treturn -ENOMEM;\n#endif\n\treturn 0;\n}\n\nstatic void __net_exit netlink_net_exit(struct net *net)\n{\n#ifdef CONFIG_PROC_FS\n\tremove_proc_entry(\"netlink\", net->proc_net);\n#endif\n}\n\nstatic void __init netlink_add_usersock_entry(void)\n{\n\tstruct listeners *listeners;\n\tint groups = 32;\n\n\tlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\n\tif (!listeners)\n\t\tpanic(\"netlink_add_usersock_entry: Cannot allocate listeners\\n\");\n\n\tnetlink_table_grab();\n\n\tnl_table[NETLINK_USERSOCK].groups = groups;\n\trcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);\n\tnl_table[NETLINK_USERSOCK].module = THIS_MODULE;\n\tnl_table[NETLINK_USERSOCK].registered = 1;\n\tnl_table[NETLINK_USERSOCK].flags = NL_CFG_F_NONROOT_SEND;\n\n\tnetlink_table_ungrab();\n}\n\nstatic struct pernet_operations __net_initdata netlink_net_ops = {\n\t.init = netlink_net_init,\n\t.exit = netlink_net_exit,\n};\n\nstatic inline u32 netlink_hash(const void *data, u32 len, u32 seed)\n{\n\tconst struct netlink_sock *nlk = data;\n\tstruct netlink_compare_arg arg;\n\n\tnetlink_compare_arg_init(&arg, sock_net(&nlk->sk), nlk->portid);\n\treturn jhash2((u32 *)&arg, netlink_compare_arg_len / sizeof(u32), seed);\n}\n\nstatic const struct rhashtable_params netlink_rhashtable_params = {\n\t.head_offset = offsetof(struct netlink_sock, node),\n\t.key_len = netlink_compare_arg_len,\n\t.obj_hashfn = netlink_hash,\n\t.obj_cmpfn = netlink_compare,\n\t.automatic_shrinking = true,\n};\n\nstatic int __init netlink_proto_init(void)\n{\n\tint i;\n\tint err = proto_register(&netlink_proto, 0);\n\n\tif (err != 0)\n\t\tgoto out;\n\n\tBUILD_BUG_ON(sizeof(struct netlink_skb_parms) > FIELD_SIZEOF(struct sk_buff, cb));\n\n\tnl_table = kcalloc(MAX_LINKS, sizeof(*nl_table), GFP_KERNEL);\n\tif (!nl_table)\n\t\tgoto panic;\n\n\tfor (i = 0; i < MAX_LINKS; i++) {\n\t\tif (rhashtable_init(&nl_table[i].hash,\n\t\t\t\t    &netlink_rhashtable_params) < 0) {\n\t\t\twhile (--i > 0)\n\t\t\t\trhashtable_destroy(&nl_table[i].hash);\n\t\t\tkfree(nl_table);\n\t\t\tgoto panic;\n\t\t}\n\t}\n\n\tINIT_LIST_HEAD(&netlink_tap_all);\n\n\tnetlink_add_usersock_entry();\n\n\tsock_register(&netlink_family_ops);\n\tregister_pernet_subsys(&netlink_net_ops);\n\t/* The netlink device handler may be needed early. */\n\trtnetlink_init();\nout:\n\treturn err;\npanic:\n\tpanic(\"netlink_init: Cannot allocate nl_table\\n\");\n}\n\ncore_initcall(netlink_proto_init);\n"], "filenames": ["net/netlink/af_netlink.c"], "buggy_code_start_loc": [2061], "buggy_code_end_loc": [2140], "fixing_code_start_loc": [2062], "fixing_code_end_loc": [2143], "type": "CWE-362", "message": "Race condition in the netlink_dump function in net/netlink/af_netlink.c in the Linux kernel before 4.6.3 allows local users to cause a denial of service (double free) or possibly have unspecified other impact via a crafted application that makes sendmsg system calls, leading to a free operation associated with a new dump that started earlier than anticipated.", "other": {"cve": {"id": "CVE-2016-9806", "sourceIdentifier": "cve@mitre.org", "published": "2016-12-28T07:59:00.667", "lastModified": "2023-01-17T21:05:07.660", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Race condition in the netlink_dump function in net/netlink/af_netlink.c in the Linux kernel before 4.6.3 allows local users to cause a denial of service (double free) or possibly have unspecified other impact via a crafted application that makes sendmsg system calls, leading to a free operation associated with a new dump that started earlier than anticipated."}, {"lang": "es", "value": "Condici\u00f3n de carrera en la funci\u00f3n netlink_dump en net/netlink/af_netlink.c en el kernel de Linux en versiones anteriores a 4.6.3 permite a usuarios locales provocar una denegaci\u00f3n de servicio (liberaci\u00f3n doble) o posiblemente tener otro impacto no especificado a trav\u00e9s de una aplicaci\u00f3n manipulada que realiza llamadas al sistema sendmsg, conduciendo a una operaci\u00f3n libre asociada con un nuevo volcado que comenz\u00f3 antes de lo anticipado."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}, {"lang": "en", "value": "CWE-415"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.12", "versionEndExcluding": "3.12.62", "matchCriteriaId": "E9A22B98-2E9C-4D17-B28B-DF092681AFCD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.73", "matchCriteriaId": "1346A01D-227D-4D11-8C7A-ADBAE630C87D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.37", "matchCriteriaId": "7DEF7E2D-A1AA-4733-A573-11EE52A2B419"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.37", "matchCriteriaId": "B55F09A2-F470-41BA-9585-40E8C1960ABA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.28", "matchCriteriaId": "2BACB680-D42D-4EFF-9B8B-121AA348DB7A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.14", "matchCriteriaId": "06B86F5B-ACB3-42F5-B15C-0EEB47DF8809"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.6.3", "matchCriteriaId": "0911A351-61CB-4070-A172-8AD9BC1871AE"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=92964c79b357efd980812c4de5c1fd2ec8bb5520", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://lists.openwall.net/netdev/2016/05/15/69", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.6.3", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/12/03/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/94653", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1037968", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2669", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1401502", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/92964c79b357efd980812c4de5c1fd2ec8bb5520", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://source.android.com/security/bulletin/2017-03-01.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/92964c79b357efd980812c4de5c1fd2ec8bb5520"}}