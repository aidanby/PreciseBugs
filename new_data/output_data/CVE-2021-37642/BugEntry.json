{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Our general strategy for preventing conflicts between concurrent\n// reads and writes of resource variables is to:\n// * For read operations, we:\n//   - acquire the variable's mutex (in \"shared\" mode);\n//   - make a (shallow) copy of the Tensor object, which increments\n//     the reference count on the variable's TensorBuffer;\n//   - release the variable's mutex;\n//   - use the copy of the Tensor object to do the read.\n// * For write operations, we:\n//   - acquire the variable's mutex (in \"exclusive\" mode);\n//   - check the reference count of variable's TensorBuffer and\n//     if it is >1, make a deep copy of the variable's Tensor;\n//   - mutate the variable's Tensor;\n//   - and release the variable's mutex.\n// This allows several read operations to all use the same\n// TensorBuffer without needing to copy. When it comes time to write\n// it will only make a copy if there is an outstanding read using the\n// buffer. Write operations are serialized by the variable's mutex.\n//\n// For sparse operations (scatter, gather, sparse optimizer updates),\n// we need to avoid copies, since there may not be enough memory for\n// to copies of the whole tensor. To support this, we make two\n// modifications to the above strategy:\n// * For sparse reads (gather), we hold the variable's mutex (still in\n//   \"shared\" mode) for the duration of the whole read. This means\n//   that as long as you only do sparse read operations no write will\n//   see the reference count >1.\n// * For sparse write operations where the user explicitly specifies\n//   that they want to perform the write without locks held\n//   (use_locking=false), we never copy even if the variable's\n//   reference count is >1.\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif\n\n#include \"tensorflow/core/kernels/resource_variable_ops.h\"\n\n#include <memory>\n#include <vector>\n\n#include \"absl/strings/str_join.h\"\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/dense_update_functor.h\"\n#include \"tensorflow/core/kernels/gather_functor.h\"\n#include \"tensorflow/core/kernels/gather_nd_op.h\"\n#include \"tensorflow/core/kernels/scatter_functor.h\"\n#include \"tensorflow/core/kernels/training_op_helpers.h\"\n#include \"tensorflow/core/kernels/variable_ops.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/platform/casts.h\"\n#include \"tensorflow/core/platform/mem.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/util.h\"\n\nnamespace tensorflow {\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\").Device(DEVICE_CPU),\n                        ResourceHandlesOp<Var>);\n\nReadVariableOp::ReadVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n  OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n}\n\nnamespace {\n\nStatus CopyVariable(int output_idx, OpKernelContext* ctx, const Tensor* t) {\n  Tensor* output;\n  Notification n;\n  Status status;\n  AllocatorAttributes attr;\n  if (t->dtype() == DT_VARIANT) {\n    attr.set_on_host(true);\n  }\n  TF_RETURN_IF_ERROR(\n      ctx->allocate_output(output_idx, t->shape(), &output, attr));\n  if (t->dtype() == DT_VARIANT) {\n    output->flat<Variant>() = t->flat<Variant>();\n  } else if (ctx->op_device_context() != nullptr) {\n    // TODO(apassos): remove the down_cast by just returning Device* from\n    // OpKernelContext\n    Device* device = down_cast<Device*>(ctx->device());\n    ctx->op_device_context()->CopyTensorInSameDevice(\n        t, device, output, [&n, &status](const Status& s) {\n          status = s;\n          n.Notify();\n        });\n    n.WaitForNotification();\n    return status;\n  } else {\n    switch (t->dtype()) {\n#define HANDLER(type)                       \\\n  case DataTypeToEnum<type>::value:         \\\n    output->flat<type>() = t->flat<type>(); \\\n    break;\n      TF_CALL_ALL_TYPES(HANDLER);\n#undef HANDLER\n      default:\n        return errors::Internal(\"Unsupported dtype\", t->dtype());\n    }\n  }\n  return Status::OK();\n}\n\n}  // namespace\n\nvoid ReadVariableOp::Compute(OpKernelContext* ctx) {\n  core::RefCountPtr<Var> variable;\n  const ResourceHandle& handle = HandleFromInput(ctx, 0);\n  const auto status = LookupResource(ctx, handle, &variable);\n  OP_REQUIRES(ctx, status.ok(),\n              errors::FailedPrecondition(\n                  \"Could not find variable \", handle.name(), \". \",\n                  \"This could mean that the variable has been deleted. \",\n                  \"In TF1, it can also mean the variable is uninitialized. \",\n                  \"Debug info: container=\", handle.container(),\n                  \", status=\", status.ToString()));\n\n  tf_shared_lock ml(*variable->mu());\n  // We're acquiring a reference to the underlying buffer while\n  // holding a shared lock to guarantee ordering of reads and\n  // writes when in copy-on-write mode.\n  const Tensor* t = variable->tensor();\n  if (!variable->copy_on_read_mode.load()) {\n    OP_REQUIRES(\n        ctx, dtype_ == t->dtype(),\n        errors::InvalidArgument(\n            \"Trying to read variable with wrong dtype. Expected \",\n            DataTypeString(dtype_), \" got \", DataTypeString(t->dtype())));\n    ctx->set_output(0, *t);\n  } else {\n    OP_REQUIRES_OK(ctx, CopyVariable(0, ctx, t));\n  }\n}\n\nReadVariablesOp::ReadVariablesOp(OpKernelConstruction* c) : OpKernel(c) {\n  int n;\n  OP_REQUIRES_OK(c, c->GetAttr(\"N\", &n));\n  OP_REQUIRES_OK(c, c->GetAttr(\"dtypes\", &dtypes_));\n  OP_REQUIRES(c, n == dtypes_.size(),\n              errors::InvalidArgument(\n                  \"Mismatched number of arguments to ReadVariablesOp (\", n,\n                  \" vs. \", dtypes_.size(), \")\"));\n}\n\nvoid ReadVariablesOp::Compute(OpKernelContext* ctx) {\n  std::vector<core::RefCountPtr<Var>> variables(dtypes_.size());\n  std::vector<const ResourceHandle*> handles(dtypes_.size());\n  for (size_t i = 0; i < dtypes_.size(); ++i) {\n    handles[i] = &HandleFromInput(ctx, i);\n  }\n\n  OP_REQUIRES_OK(ctx, LookupResources(ctx, handles, &variables));\n\n  std::vector<string> uninitialized_vars;\n  for (int64_t i = 0; i < variables.size(); i++) {\n    if (variables[i] == nullptr) {\n      uninitialized_vars.push_back(handles[i]->name());\n    }\n  }\n\n  OP_REQUIRES(ctx, uninitialized_vars.empty(),\n              errors::FailedPrecondition(\n                  \"In ReadVariablesOp the following variables were \"\n                  \"found uninitialized: \",\n                  absl::StrJoin(uninitialized_vars, \", \")));\n\n  for (size_t i = 0; i < dtypes_.size(); ++i) {\n    // We're acquiring a reference to the underlying buffer while\n    // holding a shared lock to guarantee ordering of reads and\n    // writes.\n    tf_shared_lock ml(*variables[i]->mu());\n    OP_REQUIRES(ctx, dtypes_[i] == variables[i]->tensor()->dtype(),\n                errors::InvalidArgument(\n                    \"Trying to read variable \", handles[i]->name(),\n                    \" from Container: \", handles[i]->container(),\n                    \" with wrong dtype. Expected \", DataTypeString(dtypes_[i]),\n                    \" got \", DataTypeString(variables[i]->tensor()->dtype())));\n    if (variables[i]->copy_on_read_mode.load()) {\n      OP_REQUIRES_OK(ctx, CopyVariable(i, ctx, variables[i]->tensor()));\n    } else {\n      const Tensor& t = *variables[i]->tensor();\n      ctx->set_output(i, t);\n    }\n  }\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"ReadVariableOp\").Device(DEVICE_CPU),\n                        ReadVariableOp);\nREGISTER_KERNEL_BUILDER(Name(\"_ReadVariablesOp\").Device(DEVICE_CPU),\n                        ReadVariablesOp);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"ReadVariableOp\").Device(DEVICE_DEFAULT).HostMemory(\"resource\"),\n    ReadVariableOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"_ReadVariablesOp\").Device(DEVICE_DEFAULT).HostMemory(\"resources\"),\n    ReadVariablesOp);\n\nVarHandleOp::VarHandleOp(OpKernelConstruction* context) : OpKernel(context) {\n  OP_REQUIRES_OK(context, context->GetAttr(\"container\", &container_));\n  OP_REQUIRES_OK(context, context->GetAttr(\"shared_name\", &name_));\n\n  OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_and_shape_.dtype));\n  OP_REQUIRES_OK(context, context->GetAttr(\"shape\", &dtype_and_shape_.shape));\n\n  is_anonymous_ = name_ == ResourceHandle::ANONYMOUS_NAME;\n\n  if (!is_anonymous_) {\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(context, context->allocate_temp(DT_RESOURCE, TensorShape({}),\n                                                   &resource_, attr));\n    resource_.scalar<ResourceHandle>()() = MakeResourceHandle<Var>(\n        context, container_, name_,\n        std::vector<DtypeAndPartialTensorShape>{dtype_and_shape_});\n  }\n}\n\nvoid VarHandleOp::Compute(OpKernelContext* ctx) {\n  if (is_anonymous_) {\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    Tensor handle;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DT_RESOURCE, TensorShape({}), &handle, attr));\n    handle.scalar<ResourceHandle>()() = MakeResourceHandle<Var>(\n        ctx, container_, name_,\n        std::vector<DtypeAndPartialTensorShape>{dtype_and_shape_},\n        ctx->stack_trace());\n    ctx->set_output(0, handle);\n  } else {\n    ctx->set_output(0, resource_);\n  }\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\").Device(DEVICE_CPU), VarHandleOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nREGISTER_KERNEL_BUILDER(\n    Name(\"ReadVariableOp\").Device(DEVICE_GPU).HostMemory(\"resource\"),\n    ReadVariableOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"_ReadVariablesOp\").Device(DEVICE_GPU).HostMemory(\"resources\"),\n    ReadVariablesOp);\n\n#define REGISTER_GPU_KERNELS(type)                             \\\n  namespace functor {                                          \\\n  template <>                                                  \\\n  void DenseUpdate<GPUDevice, type, ASSIGN>::operator()(       \\\n      const GPUDevice& d, typename TTypes<type>::Flat lhs,     \\\n      typename TTypes<type>::ConstFlat rhs);                   \\\n  extern template struct DenseUpdate<GPUDevice, type, ASSIGN>; \\\n  }                                                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\")                  \\\n                              .Device(DEVICE_GPU)              \\\n                              .HostMemory(\"resource\")          \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          VarHandleOp)\nTF_CALL_GPU_ALL_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\nTF_CALL_variant(REGISTER_GPU_KERNELS);\nTF_CALL_uint32(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resources\")\n                            .TypeConstraint(\"dtypes\",\n                                            {DT_INT64, DT_COMPLEX64,\n                                             DT_COMPLEX128, DT_HALF, DT_FLOAT,\n                                             DT_DOUBLE, DT_BOOL, DT_VARIANT}),\n                        ResourceHandlesOp<Var>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_DEFAULT_KERNELS(type)                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\")                 \\\n                              .Device(DEVICE_DEFAULT)         \\\n                              .HostMemory(\"resource\")         \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          VarHandleOp)\nTF_CALL_GPU_ALL_TYPES(REGISTER_DEFAULT_KERNELS);\nTF_CALL_int64(REGISTER_DEFAULT_KERNELS);\nTF_CALL_variant(REGISTER_DEFAULT_KERNELS);\nTF_CALL_uint32(REGISTER_DEFAULT_KERNELS);\n#undef REGISTER_DEFAULT_KERNELS\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"resources\")\n                            .TypeConstraint(\"dtypes\",\n                                            {DT_INT64, DT_COMPLEX64,\n                                             DT_COMPLEX128, DT_HALF, DT_FLOAT,\n                                             DT_DOUBLE, DT_BOOL, DT_VARIANT}),\n                        ResourceHandlesOp<Var>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"VariableShape\").Device(DEVICE_CPU).TypeConstraint<int32>(\"out_type\"),\n    VariableShapeOp<int32>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"VariableShape\").Device(DEVICE_CPU).TypeConstraint<int64>(\"out_type\"),\n    VariableShapeOp<int64>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"VariableShape\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"out_type\")\n                            .HostMemory(\"output\")\n                            .HostMemory(\"input\"),\n                        VariableShapeOp<int32>);\nREGISTER_KERNEL_BUILDER(Name(\"VariableShape\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int64>(\"out_type\")\n                            .HostMemory(\"output\")\n                            .HostMemory(\"input\"),\n                        VariableShapeOp<int64>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nDestroyResourceOp::DestroyResourceOp(OpKernelConstruction* ctx)\n    : OpKernel(ctx) {\n  OP_REQUIRES_OK(ctx,\n                 ctx->GetAttr(\"ignore_lookup_error\", &ignore_lookup_error_));\n}\n\nvoid DestroyResourceOp::Compute(OpKernelContext* ctx) {\n  const ResourceHandle& p = HandleFromInput(ctx, 0);\n  Status status = DeleteResource(ctx, p);\n  if (ignore_lookup_error_ && errors::IsNotFound(status)) {\n    return;\n  }\n  OP_REQUIRES_OK(ctx, status);\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"DestroyResourceOp\").Device(DEVICE_CPU),\n                        DestroyResourceOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"DestroyResourceOp\").Device(DEVICE_GPU).HostMemory(\"resource\"),\n    DestroyResourceOp);\n\ntemplate <typename Device, typename T>\nclass AssignVariableOp : public OpKernel {\n public:\n  explicit AssignVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n    if (!c->GetAttr(\"_grappler_relax_allocator_constraints\",\n                    &relax_constraints_)\n             .ok()) {\n      relax_constraints_ = false;\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    OP_REQUIRES(context, dtype_ == context->input(1).dtype(),\n                errors::InvalidArgument(\n                    \"Variable and value dtypes don't match; respectively, \",\n                    DataTypeString(dtype_), \" and \",\n                    DataTypeString(context->input(1).dtype())));\n    core::RefCountPtr<Var> variable;\n    const Tensor& value = context->input(1);\n    // Note: every resource-variable-manipulating op assumes copy-on-write\n    // semantics, and creates a copy of the variable's Tensor if its refcount is\n    // bigger than 1 when we try to modify it. This means we never need to copy\n    // the original tensor for AssignVariableOp; even if there are other live\n    // users of it we know none can modify it so this is always safe (even in\n    // esoteric cases where the same tensor is used to initialize multiple\n    // variables or the tensor is a constant this is safe, as future writes will\n    // trigger copies).\n    OP_REQUIRES_OK(context, LookupOrCreateResource<Var>(\n                                context, HandleFromInput(context, 0), &variable,\n                                [this, &value](Var** ptr) {\n                                  *ptr = new Var(dtype_);\n                                  *(*ptr)->tensor() = value;\n                                  (*ptr)->is_initialized = true;\n                                  return Status::OK();\n                                }));\n    mutex_lock ml(*variable->mu());\n    // (variable->tensor()->dtype() == DT_INVALID && !variable->is_initialized)\n    // check below is to allow an XLA specific situation wherein update can\n    // happen first by the AssignVariableOp,\n    // in which case the variable is still uninitialized.\n    // When using TF-XLA, this scenario is possible when the execution uses the\n    // 'fallback' path (which essentially invokes Tensorflow ops via\n    // partitioned_call).\n    OP_REQUIRES(context,\n                (variable->tensor()->dtype() == DT_INVALID &&\n                 !variable->is_initialized) ||\n                    variable->tensor()->dtype() == dtype_,\n                errors::InvalidArgument(\n                    \"Trying to assign variable with wrong dtype. Expected \",\n                    DataTypeString(variable->tensor()->dtype()), \" got \",\n                    DataTypeString(dtype_)));\n    if (variable->copy_on_read_mode.load()) {\n      AllocatorAttributes attr;\n      attr.set_gpu_compatible(true);\n      attr.set_nic_compatible(true);\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(value.dtype(), value.shape(),\n                                            variable->tensor(), attr));\n      functor::DenseUpdate<Device, T, ASSIGN> copy_functor;\n      copy_functor(context->eigen_device<Device>(),\n                   variable->tensor()->flat<T>(), value.flat<T>());\n    } else {\n      *variable->tensor() = value;\n    }\n    variable->is_initialized = true;\n  }\n\n private:\n  DataType dtype_;\n  bool relax_constraints_;\n};\n\ntemplate <typename Device>\nclass AssignVariableOp<Device, Variant> : public OpKernel {\n public:\n  explicit AssignVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES(c, dtype_ == DT_VARIANT,\n                errors::Internal(\"Variant kernel called with dtype: \",\n                                 DataTypeString(dtype_)));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& value = context->input(1);\n    core::RefCountPtr<Var> variable;\n    OP_REQUIRES_OK(context, LookupOrCreateResource<Var>(\n                                context, HandleFromInput(context, 0), &variable,\n                                [](Var** ptr) {\n                                  // Created on host.\n                                  *ptr = new Var(DT_VARIANT);\n                                  return Status::OK();\n                                }));\n\n    // For purposes of forwarding DT_VARIANT, we want the least\n    // restrictive attr; we already know the input is on host.\n    AllocatorAttributes attr;\n\n    // Copying is unnecessary if we are the last user of the value\n    // tensor, we can just adopt the input tensor's buffer instead.\n    // Note that Variant objects themselves always reside on host.\n    //\n    // We nevertheless want to signal to the runtime that the tensor\n    // should reside in memory of the associated device, as Variant\n    // tensors may be marked as sitting on either CPU or GPU.  This\n    // helps to elide one or more copies.\n    std::unique_ptr<Tensor> input_alias = context->forward_input(\n        1, OpKernelContext::Params::kNoReservation /*output_index*/, DT_VARIANT,\n        value.shape(),\n        DEVICE_MEMORY /* HOST_MEMORY is only reserved for special cases */,\n        attr);\n\n    mutex_lock ml(*variable->mu());\n    OP_REQUIRES(context, variable->tensor()->dtype() == DT_VARIANT,\n                errors::InvalidArgument(\n                    \"Trying to assign variable with wrong dtype. Expected \",\n                    DataTypeString(variable->tensor()->dtype()), \" got \",\n                    DataTypeString(DT_VARIANT)));\n    variable->is_initialized = true;\n    *variable->tensor() = Tensor(DT_VARIANT, value.shape());\n\n    if (input_alias) {\n      *variable->tensor() = *input_alias;\n      return;\n    }\n\n    // Need to copy, but maybe we can re-use variable's buffer?\n    if (!variable->tensor()->RefCountIsOne() ||\n        !variable->tensor()->shape().IsSameSize(value.shape())) {\n      // Allocation of DT_VARIANT is always on host.\n      attr.set_on_host(true);\n      OP_REQUIRES_OK(context, context->allocate_temp(DT_VARIANT, value.shape(),\n                                                     variable->tensor(), attr));\n    }\n\n    const auto elements_in = value.flat<Variant>();\n    auto elements_out = variable->tensor()->flat<Variant>();\n    for (int64_t i = 0; i < elements_in.size(); ++i) {\n      elements_out(i) = elements_in(i);\n    }\n  }\n\n private:\n  DataType dtype_;\n};\n\n#define REGISTER_KERNELS(type)                                \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignVariableOp\")            \\\n                              .Device(DEVICE_CPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          AssignVariableOp<Eigen::ThreadPoolDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNELS(type)                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignVariableOp\")           \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"resource\"),       \\\n                          AssignVariableOp<GPUDevice, type>);\n\nTF_CALL_GPU_ALL_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\nTF_CALL_variant(REGISTER_GPU_KERNELS);\nTF_CALL_uint32(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, DenseUpdateType Op>\nclass AssignUpdateVariableOp : public OpKernel {\n public:\n  explicit AssignUpdateVariableOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    core::RefCountPtr<Var> variable;\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &variable));\n\n    const Tensor& value = context->input(1);\n    // TODO(apassos): We could possibly avoid the copy done by\n    // PrepareToUpdateVariable() for commutative operations like Op ==\n    // ADD if value's refcount was 1.\n    mutex_lock ml(*variable->mu());\n    Tensor* var_tensor = variable->tensor();\n    OP_REQUIRES(context, var_tensor->shape().IsSameSize(value.shape()),\n                errors::InvalidArgument(\"Cannot update variable with shape \",\n                                        var_tensor->shape().DebugString(),\n                                        \" using a Tensor with shape \",\n                                        value.shape().DebugString(),\n                                        \", shapes must be equal.\"));\n    OP_REQUIRES_OK(\n        context, PrepareToUpdateVariable<Device, T>(\n                     context, var_tensor, variable->copy_on_read_mode.load()));\n    functor::DenseUpdate<Device, T, Op> update_functor;\n    update_functor(context->eigen_device<Device>(), var_tensor->flat<T>(),\n                   value.flat<T>());\n  }\n};\n\n#define REGISTER_KERNELS(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                         \\\n      Name(\"AssignAddVariableOp\")                                  \\\n          .Device(DEVICE_CPU)                                      \\\n          .TypeConstraint<type>(\"dtype\"),                          \\\n      AssignUpdateVariableOp<Eigen::ThreadPoolDevice, type, ADD>); \\\n  REGISTER_KERNEL_BUILDER(                                         \\\n      Name(\"AssignSubVariableOp\")                                  \\\n          .Device(DEVICE_CPU)                                      \\\n          .TypeConstraint<type>(\"dtype\"),                          \\\n      AssignUpdateVariableOp<Eigen::ThreadPoolDevice, type, SUB>);\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignAddVariableOp\")                    \\\n                              .Device(DEVICE_GPU)                        \\\n                              .HostMemory(\"resource\")                    \\\n                              .TypeConstraint<type>(\"dtype\"),            \\\n                          AssignUpdateVariableOp<GPUDevice, type, ADD>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignSubVariableOp\")                    \\\n                              .Device(DEVICE_GPU)                        \\\n                              .HostMemory(\"resource\")                    \\\n                              .TypeConstraint<type>(\"dtype\"),            \\\n                          AssignUpdateVariableOp<GPUDevice, type, SUB>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass VarIsInitializedOp : public OpKernel {\n public:\n  explicit VarIsInitializedOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({}), &output));\n    auto output_tensor = output->tensor<bool, 0>();\n    core::RefCountPtr<Var> variable;\n    Status s = LookupResource(context, HandleFromInput(context, 0), &variable);\n    if (!s.ok()) {\n      output_tensor() = false;\n      return;\n    }\n    mutex_lock ml(*variable->mu());\n    output_tensor() = variable->is_initialized;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\").Device(DEVICE_CPU),\n                        VarIsInitializedOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"is_initialized\"),\n                        IsResourceInitialized<Var>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"is_initialized\"),\n                        IsResourceInitialized<Var>);\n\ntemplate <typename Device, typename T, typename Index>\nclass ResourceGatherOp : public OpKernel {\n public:\n  explicit ResourceGatherOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"batch_dims\", &batch_dims_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n    OP_REQUIRES(\n        c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n    OP_REQUIRES(\n        c, params.shape().dims() >= batch_dims_,\n        errors::InvalidArgument(\"params must have at least \", batch_dims_,\n                                \" (batch_dims) dimensions but it has shape \",\n                                params.shape().DebugString()));\n\n    // Check that we have enough index space\n    const int64_t N = indices.NumElements();\n    OP_REQUIRES(\n        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params.dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // The result shape is params.shape[:batch_dims] +\n    // indices.shape[batch_dims:] + params.shape[batch_dims+1:].\n    TensorShape result_shape;\n    for (int i = 0; i < batch_dims_; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n    for (int i = batch_dims_; i < indices.dims(); ++i) {\n      result_shape.AddDim(indices.dim_size(i));\n    }\n    for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n\n    Tensor* out = nullptr;\n    Tensor tmp;\n    if (params.dtype() == DT_VARIANT) {\n      tmp = Tensor(DT_VARIANT, result_shape);\n      c->set_output(0, tmp);\n      out = &tmp;\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(0, result_shape, &out));\n    }\n\n    if (N > 0) {\n      Tensor tmp_indices;\n\n      // Points to the original or updated (if batch_dims is set) indices.\n      const Tensor* op_indices = &indices;\n      if (batch_dims_ > 0) {\n        OP_REQUIRES_OK(c, c->allocate_temp(indices.dtype(), indices.shape(),\n                                           &tmp_indices));\n        functor::DenseUpdate<Device, Index, ASSIGN> copy_functor;\n        copy_functor(c->eigen_device<Device>(), tmp_indices.flat<Index>(),\n                     indices.flat<Index>());\n\n        AddBatchOffsets(c, &tmp_indices, params);\n        if (!c->status().ok()) return;\n        op_indices = &tmp_indices;\n      }\n\n      int64_t gather_dim_size = 1;\n      for (int idx = 0; idx <= batch_dims_; ++idx) {\n        gather_dim_size *= params.dim_size(idx);\n      }\n      int64_t inner_size = 1;\n      for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n        inner_size *= params.dim_size(i);\n      }\n      auto params_flat = params.shaped<T, 3>({1, gather_dim_size, inner_size});\n      const auto indices_flat = op_indices->flat<Index>();\n      auto out_flat = out->shaped<T, 3>({1, N, out->NumElements() / N});\n\n      functor::GatherFunctor<Device, T, Index> functor;\n      int64_t bad_i = functor(c, params_flat, indices_flat, out_flat);\n\n      OP_REQUIRES(\n          c, bad_i < 0,\n          errors::InvalidArgument(\n              \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n              indices_flat(bad_i), \" is not in [0, \", params.dim_size(0), \")\"));\n    }\n  }\n\n private:\n  // Add the batch offset derived from params to each batch of indices.\n  // Example: batch_dims = 1, indices = [[0, 1, 2], [0, 1, 2]]\n  // If indexing into a params dimension of size 4, then the indices will become\n  // [0, 1, 2, 4, 5, 6]\n  void AddBatchOffsets(OpKernelContext* ctx, Tensor* indices,\n                       const Tensor& params) {\n    int64_t batch_size = 1;  // The size of all batch dimensions.\n    for (int idx = 0; idx < batch_dims_; ++idx) {\n      batch_size *= params.dim_size(idx);\n    }\n    OP_REQUIRES(\n        ctx, batch_size != 0,\n        errors::InvalidArgument(\n            \"Inner size of indices would result in batch_size of 0 and a \",\n            \"division by 0 in the implementation. This is illegal\"));\n\n    auto indices_flat = indices->flat<Index>();\n    int64_t const index_inner_size = indices->NumElements() / batch_size;\n    int64_t const batch_offset = params.dim_size(batch_dims_);\n    for (int64_t batch_idx = 0, dest_idx = 0; batch_idx < batch_size;\n         ++batch_idx) {\n      for (int64_t idx = 0; idx < index_inner_size; ++idx) {\n        indices_flat(dest_idx++) += batch_offset * batch_idx;\n      }\n    }\n  }\n\n  int32 batch_dims_ = 0;\n};\n\n#define REGISTER_GATHER_FULL(dev, type, index_type)                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")                       \\\n                              .Device(DEVICE_##dev)                    \\\n                              .HostMemory(\"resource\")                  \\\n                              .TypeConstraint<type>(\"dtype\")           \\\n                              .TypeConstraint<index_type>(\"Tindices\"), \\\n                          ResourceGatherOp<dev##Device, type, index_type>)\n\n#define REGISTER_GATHER_ALL_INDICES(dev, type) \\\n  REGISTER_GATHER_FULL(dev, type, int32);      \\\n  REGISTER_GATHER_FULL(dev, type, int64)\n\n#define REGISTER_GATHER_CPU(type) REGISTER_GATHER_ALL_INDICES(CPU, type)\n\n// Registration of the CPU implementations.\nTF_CALL_ALL_TYPES(REGISTER_GATHER_CPU);\nTF_CALL_QUANTIZED_TYPES(REGISTER_GATHER_CPU);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GATHER_GPU(type) REGISTER_GATHER_ALL_INDICES(GPU, type)\n\nTF_CALL_int64(REGISTER_GATHER_GPU);\nTF_CALL_GPU_ALL_TYPES(REGISTER_GATHER_GPU);\n\n// Variant objects themselves sit on CPU, even if they contain data\n// pointing to a device.\nREGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceGatherOp<GPUDevice, Variant, int32>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceGatherOp<GPUDevice, Variant, int64>)\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_GATHER_CPU\n#undef REGISTER_GATHER_GPU\n#undef REGISTER_GATHER_ALL_INDICES\n#undef REGISTER_GATHER_FULL\n\ntemplate <typename Device, typename T, typename Index>\nclass ResourceGatherNdOp : public OpKernel {\n public:\n  explicit ResourceGatherNdOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n\n    Tensor out;\n    OP_REQUIRES_OK(\n        c, functor::DoGatherNd<Device, T, Index>(c, params, indices, &out));\n    c->set_output(0, out);\n  }\n};\n\n#define REGISTER_GATHER_ND_FULL(dev, type, index_type)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceGatherNd\")                     \\\n                              .Device(DEVICE_##dev)                    \\\n                              .HostMemory(\"resource\")                  \\\n                              .TypeConstraint<type>(\"dtype\")           \\\n                              .TypeConstraint<index_type>(\"Tindices\"), \\\n                          ResourceGatherNdOp<dev##Device, type, index_type>)\n\n#define REGISTER_GATHER_ND_ALL_INDICES(dev, type) \\\n  REGISTER_GATHER_ND_FULL(dev, type, int32);      \\\n  REGISTER_GATHER_ND_FULL(dev, type, int64)\n\n#define REGISTER_GATHER_ND_CPU(type) REGISTER_GATHER_ND_ALL_INDICES(CPU, type)\n\n// Registration of the CPU implementations.\nTF_CALL_ALL_TYPES(REGISTER_GATHER_ND_CPU);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GATHER_ND_GPU(type) REGISTER_GATHER_ND_ALL_INDICES(GPU, type)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GATHER_ND_GPU);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_GATHER_ND_CPU\n#undef REGISTER_GATHER_ND_GPU\n#undef REGISTER_GATHER_ND_ALL_INDICES\n#undef REGISTER_GATHER_ND_FULL\n\ntemplate <typename Device, typename T, typename Index, scatter_op::UpdateOp op>\nclass ResourceScatterUpdateOp : public OpKernel {\n public:\n  explicit ResourceScatterUpdateOp(OpKernelConstruction* c) : OpKernel(c) {\n    // We use the same kernel for many operations.\n    // Each operation has a different set of attributes defined in its nodes.\n    Status s = c->GetAttr(\"use_locking\", &use_exclusive_lock_);\n    if (!s.ok()) {\n      use_exclusive_lock_ = false;\n    }\n  }\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    const bool is_non_pod_dtype = c->input_dtype(0) == DT_RESOURCE ||\n                                  c->input_dtype(0) == DT_STRING ||\n                                  c->input_dtype(0) == DT_VARIANT;\n    if (is_non_pod_dtype || use_exclusive_lock_) {\n      mutex_lock ml(*v->mu());\n      DoCompute(c);\n    } else {\n      // For POD dtypes, we can safely run the update without the mutex.\n      tf_shared_lock ml(*v->mu());\n      DoCompute(c);\n    }\n  }\n\n private:\n  bool use_exclusive_lock_;\n\n  void DoCompute(OpKernelContext* c) {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    Tensor* params = v->tensor();\n    const Tensor& indices = c->input(1);\n    const Tensor& updates = c->input(2);\n\n    // Check that rank(updates.shape) = rank(indices.shape + params.shape[1:])\n    OP_REQUIRES(c,\n                updates.dims() == 0 ||\n                    updates.dims() == indices.dims() + params->dims() - 1,\n                errors::InvalidArgument(\n                    \"Must have updates.shape = indices.shape + \"\n                    \"params.shape[1:] or updates.shape = [], got \",\n                    \"updates.shape \", updates.shape().DebugString(),\n                    \", indices.shape \", indices.shape().DebugString(),\n                    \", params.shape \", params->shape().DebugString()));\n\n    // Check that we have enough index space\n    const int64_t N_big = indices.NumElements();\n    OP_REQUIRES(\n        c, N_big <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"indices has too many elements for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", N_big, \" > \",\n                                std::numeric_limits<Index>::max()));\n    const Index N = static_cast<Index>(N_big);\n    OP_REQUIRES(\n        c, params->dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params->dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    if (N > 0) {\n      auto indices_flat = indices.flat<Index>();\n      auto params_flat = params->flat_outer_dims<T>();\n      if (TensorShapeUtils::IsScalar(updates.shape())) {\n        const auto update = updates.scalar<T>();\n\n        functor::ScatterScalarFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, update, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      } else {\n        int64_t num_updates = updates.NumElements();\n        OP_REQUIRES(\n            c, TensorShapeUtils::StartsWith(updates.shape(), indices.shape()),\n            errors::InvalidArgument(\n                \"The shape of indices (\", indices.shape().DebugString(),\n                \") must be a prefix of the shape of updates (\",\n                updates.shape().DebugString(), \")\"));\n        auto updates_flat = updates.shaped<T, 2>({N, num_updates / N});\n\n        functor::ScatterFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, updates_flat, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      }\n    }\n  }\n};\n\n#define REGISTER_SCATTER_KERNEL_INDEX(type, index_type, dev, name, op) \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(name)                                                       \\\n          .Device(DEVICE_##dev)                                        \\\n          .HostMemory(\"resource\")                                      \\\n          .TypeConstraint<type>(\"dtype\")                               \\\n          .TypeConstraint<index_type>(\"Tindices\"),                     \\\n      ResourceScatterUpdateOp<dev##Device, type, index_type, op>)\n\n#define REGISTER_SCATTER_KERNEL(type, dev, name, op)         \\\n  REGISTER_SCATTER_KERNEL_INDEX(type, int32, dev, name, op); \\\n  REGISTER_SCATTER_KERNEL_INDEX(type, int64, dev, name, op);\n\n#define REGISTER_SCATTER_ARITHMETIC(type, dev)                \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterAdd\",    \\\n                          scatter_op::UpdateOp::ADD);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterSub\",    \\\n                          scatter_op::UpdateOp::SUB);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMul\",    \\\n                          scatter_op::UpdateOp::MUL);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterDiv\",    \\\n                          scatter_op::UpdateOp::DIV);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterUpdate\", \\\n                          scatter_op::UpdateOp::ASSIGN);\n#define REGISTER_SCATTER_MINMAX(type, dev)                 \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMin\", \\\n                          scatter_op::UpdateOp::MIN);      \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMax\", \\\n                          scatter_op::UpdateOp::MAX);\n\n// Registers CPU kernels.\n#define REGISTER_SCATTER_ARITHMETIC_CPU(type) \\\n  REGISTER_SCATTER_ARITHMETIC(type, CPU);\n#define REGISTER_SCATTER_MINMAX_CPU(type) REGISTER_SCATTER_MINMAX(type, CPU);\n\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ARITHMETIC_CPU);\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_SCATTER_MINMAX_CPU);\n\nREGISTER_SCATTER_KERNEL(tstring, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\nREGISTER_SCATTER_KERNEL(bool, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\nREGISTER_SCATTER_KERNEL(Variant, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_SCATTER_ARITHMETIC_GPU(type) \\\n  REGISTER_SCATTER_ARITHMETIC(type, GPU);\n#define REGISTER_SCATTER_MINMAX_GPU(type) REGISTER_SCATTER_MINMAX(type, GPU);\n\n#define REGISTER_SCATTER_UPDATE_GPU(type) REGISTER_SCATTER_UPDATE(type, GPU);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_SCATTER_ARITHMETIC_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_SCATTER_MINMAX_GPU);\n\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, Variant, int32,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .TypeConstraint<bool>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, bool, int32,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, Variant, int64,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .TypeConstraint<int64>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, int64, int64,\n                                                scatter_op::UpdateOp::ASSIGN>)\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_SCATTER_ARITHMETIC\n#undef REGISTER_SCATTER_ARITHMETIC_CPU\n#undef REGISTER_SCATTER_MINMAX\n#undef REGISTER_SCATTER_MINMAX_CPU\n#undef REGISTER_SCATTER_KERNEL\n#undef REGISTER_SCATTER_KERNEL_INDEX\n\n}  // namespace tensorflow\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ShardedVariable.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.compat import v2_compat\nfrom tensorflow.python.distribute import sharded_variable\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.module import module\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import loader\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.training.tracking import tracking\nfrom tensorflow.python.training.tracking import util\nfrom tensorflow.python.util import nest\n\n\ndef _load_and_run(\n    model_dir,\n    inputs,\n    signature_key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY):\n  \"\"\"Load a SavedModel into a TF 1.x-style graph and run `signature_key`.\"\"\"\n  graph = ops.Graph()\n  with graph.as_default(), session_lib.Session() as session:\n    meta_graph_def = loader.load(session, [tag_constants.SERVING], model_dir)\n    signature = meta_graph_def.signature_def[signature_key]\n    feed_dict = {}\n    for arg_name in inputs.keys():\n      input_tensor = session.graph.get_tensor_by_name(\n          signature.inputs[arg_name].name)\n      feed_dict[input_tensor] = inputs[arg_name]\n    output_dict = {}\n    for output_name, output_tensor_info in signature.outputs.items():\n      output_dict[output_name] = session.graph.get_tensor_by_name(\n          output_tensor_info.name)\n    return session.run(output_dict, feed_dict=feed_dict)\n\n\nclass PartitionerTest(test.TestCase):\n\n  def test_fixed_shards_partitioner(self):\n    partitioner = sharded_variable.FixedShardsPartitioner(num_shards=2)\n    got = partitioner(tensor_shape.TensorShape([10, 3]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n  def test_min_size_partitioner(self):\n    partitioner = sharded_variable.MinSizePartitioner(\n        min_shard_bytes=4, max_shards=2)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n    partitioner = sharded_variable.MinSizePartitioner(\n        min_shard_bytes=4, max_shards=10)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [6, 1])\n\n  def test_max_size_partitioner(self):\n    partitioner = sharded_variable.MaxSizePartitioner(max_shard_bytes=4)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [6, 1])\n\n    partitioner = sharded_variable.MaxSizePartitioner(\n        max_shard_bytes=4, max_shards=2)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n    partitioner = sharded_variable.MaxSizePartitioner(max_shard_bytes=1024)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [1, 1])\n\n\nclass ShardedVariableTest(test.TestCase, parameterized.TestCase):\n\n  def test_sharded_variable_simple(self):\n    v0 = variables_lib.Variable([0])\n    v1 = variables_lib.Variable([1])\n    s = sharded_variable.ShardedVariable([v0, v1], name='s')\n    self.assertEqual(s.variables[0], v0)\n    self.assertEqual(s.variables[1], v1)\n    self.assertEqual(s.shape.as_list(), [2])\n    self.assertEqual(s.dtype, v0.dtype)\n    self.assertEqual(s.name, 's')\n\n  def test_assign(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign([[4, 4], [5, 5], [6, 6], [7, 7]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[4, 4]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[5, 5], [6, 6]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[7, 7]])\n    self.assertIs(ret, s)\n\n  def test_assign_add(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign_add([[1, 1], [1, 1], [2, 2], [2, 2]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[1, 1]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[2, 2], [4, 4]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[5, 5]])\n    self.assertIs(ret, s)\n\n  def test_assign_sub(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign_sub([[0, 0], [1, 1], [1, 1], [3, 3]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[0, 0]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[0, 0], [1, 1]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[0, 0]])\n    self.assertIs(ret, s)\n\n  def test_scatter_add_uneven_partition(self):\n    v = variables_lib.Variable(array_ops.zeros((32, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.], [5.]]),\n        indices=constant_op.constant([0, 10, 11, 12, 30, 31]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    v.scatter_add(sparse_delta)\n    sv.scatter_add(sparse_delta)\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      v.scatter_add(sparse_delta)\n      sv.scatter_add(sparse_delta)\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  @parameterized.parameters('scatter_add', 'scatter_div', 'scatter_max',\n                            'scatter_min', 'scatter_mul', 'scatter_sub',\n                            'scatter_update')\n  def test_scatter_ops_even_partition(self, op):\n    v = variables_lib.Variable(array_ops.zeros((30, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.]]),\n        indices=constant_op.constant([0, 10, 12, 21, 22]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    getattr(v, op)(sparse_delta, name='scatter_v')\n    getattr(sv, op)(sparse_delta, name='scatter_sv')\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      getattr(v, op)(sparse_delta, name='scatter_v')\n      getattr(sv, op)(sparse_delta, name='scatter_sv')\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  def test_batch_scatter_update(self):\n    v = variables_lib.Variable(array_ops.zeros((32, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.], [5.]]),\n        indices=constant_op.constant([10, 11, 12, 13, 14, 15]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    v.batch_scatter_update(sparse_delta)\n    sv.batch_scatter_update(sparse_delta)\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      v.batch_scatter_update(sparse_delta)\n      sv.batch_scatter_update(sparse_delta)\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  def test_sparse_read(self):\n    v = variables_lib.Variable(array_ops.zeros((30, 1)))\n    indices = constant_op.constant([0, 10, 12, 21, 22])\n\n    v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    self.assertAllEqual(v.sparse_read(indices), sv.sparse_read(indices))\n\n    @def_function.function\n    def func():\n      return v.sparse_read(indices), sv.sparse_read(indices)\n\n    got, expect = func()\n    self.assertAllEqual(got, expect)\n\n  def test_control_dep_on_assign(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    @def_function.function\n    def func():\n      ret = s.assign([[4, 4], [5, 5], [6, 6], [7, 7]])\n      with ops.control_dependencies([ret]):\n        a = array_ops.ones((1, 1))\n      with ops.control_dependencies([control_flow_ops.group(ret)]):\n        b = array_ops.ones((1, 1))\n      return a, b\n\n    func()\n\n  def test_convert_to_tensor(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    t = ops.convert_to_tensor(s)\n    self.assertAllEqual(t, [[0, 0], [1, 1], [2, 2], [3, 3]])\n\n  def test_save_restore(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n\n    cp = util.Checkpoint(s=s)\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [0])\n    cp.write(fname)\n\n    self.evaluate(cp.s.variables[0].assign([4]))\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [4])\n\n    cp.restore(fname)\n    # Tests that the original weights are restored.\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [0])\n\n  def test_save_restore_different_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n\n    cp = util.Checkpoint(s=s)\n    cp.write(fname)\n\n    variables2 = [variables_lib.Variable([0, 0, 0, 0])]\n    s2 = sharded_variable.ShardedVariable(variables2, name='s')\n\n    # Restore from 4 partitions into 1.\n    cp2 = util.Checkpoint(s=s2)\n    cp2.restore(fname)\n    self.assertAllEqual(self.evaluate(cp2.s.variables[0]), [0, 1, 2, 3])\n\n    self.evaluate(cp2.s.variables[0].assign([5, 10, 15, 20]))\n    cp2.write(fname)\n\n    # Restore 1 partition into 4.\n    cp.restore(fname)\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [5])\n    self.assertEqual(self.evaluate(cp.s.variables[1]), [10])\n    self.assertEqual(self.evaluate(cp.s.variables[2]), [15])\n    self.assertEqual(self.evaluate(cp.s.variables[3]), [20])\n\n  def test_save_restore_4_to_2_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n    cp = util.Checkpoint(s=s)\n    cp.write(fname)\n\n    variables2 = [\n        variables_lib.Variable([0, 0]),\n        variables_lib.Variable([0, 0])\n    ]\n    s2 = sharded_variable.ShardedVariable(variables2, name='s')\n    cp2 = util.Checkpoint(s=s2)\n    cp2.restore(fname)\n    # Assert that weights from the 4 partitions were loaded here.\n    self.assertLen(cp2.s.variables, 2)\n    self.assertAllEqual(self.evaluate(cp2.s.variables[0]), [0, 1])\n    self.assertAllEqual(self.evaluate(cp2.s.variables[1]), [2, 3])\n\n  def test_delayed_restore(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    model = tracking.AutoTrackable()\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    model.s = sharded_variable.ShardedVariable(variables)\n    cp = util.Checkpoint(model=model)\n    cp.write(fname)\n\n    model2 = tracking.AutoTrackable()\n    cp2 = util.Checkpoint(model=model2)\n    cp2.restore(fname)\n    variables2 = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0])\n    ]\n    model2.s = sharded_variable.ShardedVariable(variables2)\n    self.assertAllEqual(self.evaluate(model2.s.variables[0]), [0])\n    self.assertAllEqual(self.evaluate(model2.s.variables[1]), [1])\n    self.assertAllEqual(self.evaluate(model2.s.variables[2]), [2])\n    self.assertAllEqual(self.evaluate(model2.s.variables[3]), [3])\n\n  def test_delayed_restore_4_to_2_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    model = tracking.AutoTrackable()\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    model.s = sharded_variable.ShardedVariable(variables)\n    cp = util.Checkpoint(model=model)\n    cp.write(fname)\n\n    model2 = tracking.AutoTrackable()\n    cp2 = util.Checkpoint(model=model2)\n    cp2.restore(fname)\n    variables2 = [\n        variables_lib.Variable([0, 0]),\n        variables_lib.Variable([0, 0])\n    ]\n    model2.s = sharded_variable.ShardedVariable(variables2)\n    self.assertAllEqual(self.evaluate(model2.s.variables[0]), [0, 1])\n    self.assertAllEqual(self.evaluate(model2.s.variables[1]), [2, 3])\n\n  def test_save_graph_def(self):\n    root = tracking.AutoTrackable()\n    v1 = variables_lib.Variable([3.])\n    v2 = variables_lib.Variable([2.])\n    root.v = sharded_variable.ShardedVariable([v1, v2])\n    root.train = def_function.function(\n        lambda x: embedding_ops.embedding_lookup_v2(root.v.variables, x))\n    # TODO(b/144057383): Remove the necessity of root.serve once saving context\n    # is made to tf.function cache.\n    root.serve = def_function.function(\n        lambda x: embedding_ops.embedding_lookup_v2(root.v.variables[0], x),\n        input_signature=[tensor_spec.TensorSpec([2], dtypes.int32, name='x')])\n\n    # Trace and use root.train\n    self.assertAllEqual([3., 2.], root.train([0, 1]).numpy())\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save.save(root, save_dir, root.serve)\n    self.assertAllEqual([3., 2.],\n                        _load_and_run(save_dir, {'x': [0, 1]})['output_0'])\n\n    # Continue using root.train for training\n    self.assertAllEqual([3., 2.], root.train([0, 1]).numpy())\n\n  def test_load_raises_error(self):\n    root = tracking.AutoTrackable()\n    v1 = variables_lib.Variable([3.])\n    v2 = variables_lib.Variable([2.])\n    root.v = sharded_variable.ShardedVariable([v1, v2])\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save.save(root, save_dir)\n\n    with self.assertRaisesRegex(\n        ValueError, 'Loading a saved_model containing ShardedVariable'):\n      load.load(save_dir)\n\n  def test_validation_errors(self):\n    with self.assertRaisesRegex(ValueError, 'Expected a list of '):\n      sharded_variable.ShardedVariable(\n          [variables_lib.Variable([0]), 'not-a-variable'])\n\n    with self.assertRaisesRegex(ValueError, 'must have the same dtype'):\n      sharded_variable.ShardedVariable([\n          variables_lib.Variable([0], dtype='int64'),\n          variables_lib.Variable([1], dtype='int32')\n      ])\n\n    with self.assertRaisesRegex(ValueError, 'the same shapes except'):\n      sharded_variable.ShardedVariable([\n          variables_lib.Variable(array_ops.ones((5, 10))),\n          variables_lib.Variable(array_ops.ones((5, 20)))\n      ])\n\n    with self.assertRaisesRegex(ValueError, '`SaveSliceInfo` should not'):\n      v = variables_lib.Variable([0])\n      v._set_save_slice_info(\n          variables_lib.Variable.SaveSliceInfo(\n              full_name='s', full_shape=[2], var_offset=[0], var_shape=[1]))\n      sharded_variable.ShardedVariable([v])\n\n  def test_as_function_input(self):\n    variables1 = [\n        variables_lib.Variable([1]),\n        variables_lib.Variable([1]),\n    ]\n    s = sharded_variable.ShardedVariable(variables1)\n    variables2 = [\n        variables_lib.Variable([2]),\n        variables_lib.Variable([2]),\n    ]\n    s2 = sharded_variable.ShardedVariable(variables2)\n\n    trace_count = [0]\n\n    @def_function.function\n    def func(sharded_var):\n      trace_count[0] = trace_count[0] + 1\n      sharded_var.assign([0, 0])\n\n    func(s)\n    self.assertAllEqual(ops.convert_to_tensor(s), [0, 0])\n    self.assertEqual(trace_count[0], 1)\n    func(s2)\n    self.assertAllEqual(ops.convert_to_tensor(s2), [0, 0])\n    self.assertEqual(trace_count[0], 1)\n\n  def test_flatten(self):\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n    ]\n    s = sharded_variable.ShardedVariable(variables)\n\n    got = nest.flatten(s)\n    self.assertIs(s, got[0])\n\n    got = nest.flatten(s, expand_composites=True)\n    self.assertAllEqual(variables, got)\n\n  def test_tf_module(self):\n\n    class Model(module.Module):\n\n      def __init__(self):\n        super().__init__()\n        variables = [\n            variables_lib.Variable([0]),\n            variables_lib.Variable([1]),\n        ]\n        self.w = sharded_variable.ShardedVariable(variables)\n\n    model = Model()\n\n    self.assertLen(model.variables, 2)\n    self.assertEqual(model.variables[0], [0])\n    self.assertEqual(model.variables[1], [1])\n    self.assertAllEqual(model.variables, model.trainable_variables)\n\n    self.assertLen(model._checkpoint_dependencies, 1)\n    self.assertIs(model._checkpoint_dependencies[0].ref, model.w)\n\n  def test_embedding_lookup(self):\n    v = [\n        variables_lib.Variable([[1., 2.], [3., 4.]]),\n        variables_lib.Variable([[5., 6.], [7., 8.]]),\n        variables_lib.Variable([[9., 10.]])\n    ]\n    sv = sharded_variable.ShardedVariable(v)\n\n    @def_function.function\n    def lookup():\n      ids = constant_op.constant([0, 3, 4])\n      return embedding_ops.embedding_lookup_v2(sv, ids)\n\n    @def_function.function\n    def sparse_lookup():\n      sp_ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[0, 3, 4, 1],\n          dense_shape=[3, 3])\n      return embedding_ops.embedding_lookup_sparse_v2(sv, sp_ids, None)\n\n    @def_function.function\n    def safe_sparse_lookup():\n      sp_ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[0, -1, 4, 1],\n          dense_shape=[3, 3])\n      sp_weights = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[1., 1., -1., 1.],\n          dense_shape=[3, 3])\n      return embedding_ops.safe_embedding_lookup_sparse_v2(\n          sv, sp_ids, sp_weights)\n\n    # TODO(chenkai): Add safe_sparse_lookup to the list. Currently\n    # ShardedVariable is converted to a tensor in safe_sparse_lookup.\n    for func in [lookup, sparse_lookup]:\n      num_gather_ops = 0\n      for op in func.get_concrete_function().graph.get_operations():\n        if op.type == 'ResourceGather':\n          num_gather_ops += 1\n      self.assertEqual(\n          num_gather_ops, len(v), 'Number of ResourceGather op does not match'\n          ' expected, possibly due to ShardedVariable accidentally being'\n          ' converted to tensor in embedding_lookup ops.')\n\n    self.assertAllEqual(lookup(), [[1., 2.], [7., 8.], [9., 10.]])\n    self.assertAllClose(sparse_lookup(), [[4., 5.], [9., 10.], [3., 4.]])\n    self.assertAllClose(safe_sparse_lookup(), [[1., 2.], [0., 0.], [3., 4.]])\n\n  def test_slicing(self):\n    v = [\n        variables_lib.Variable([[1, 2], [3, 4], [5, 6]]),\n        variables_lib.Variable([[7, 8], [9, 10], [11, 12]]),\n        variables_lib.Variable([[13, 14], [15, 16]])\n    ]\n    sv = sharded_variable.ShardedVariable(v)\n    empty = v[0][0:0]\n\n    # Test cases: positive step\n    self.assertAllEqual(sv[:], array_ops.concat(v, axis=0))\n    self.assertAllEqual(sv[:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[-8:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[-10:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[5:], [[11, 12], [13, 14], [15, 16]])\n    self.assertAllEqual(sv[5:-1], [[11, 12], [13, 14]])\n    self.assertAllEqual(sv[::3], [[1, 2], [7, 8], [13, 14]])\n    self.assertAllEqual(sv[::5], [[1, 2], [11, 12]])\n    self.assertAllEqual(sv[1::6], [[3, 4], [15, 16]])\n    self.assertAllEqual(sv[1:5:6], [[3, 4]])\n    self.assertAllEqual(sv[1::7], [[3, 4]])\n    self.assertAllEqual(sv[2:7], [[5, 6], [7, 8], [9, 10], [11, 12], [13, 14]])\n    self.assertAllEqual(sv[2:7:2], [[5, 6], [9, 10], [13, 14]])\n    self.assertAllEqual(sv[2:7:3], [[5, 6], [11, 12]])\n\n    # Test cases: negative step\n    self.assertAllEqual(\n        sv[::-1], array_ops.reverse(array_ops.concat(v, axis=0), axis=[0]))\n    self.assertAllEqual(sv[2::-1], [[5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[2:-8:-1], [[5, 6], [3, 4]])\n    self.assertAllEqual(sv[2:-10:-1], [[5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[4::-1], [[9, 10], [7, 8], [5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[-1:-3:-1], [[15, 16], [13, 14]])\n    self.assertAllEqual(sv[::-5], [[15, 16], [5, 6]])\n    self.assertAllEqual(sv[6::-6], [[13, 14], [1, 2]])\n    self.assertAllEqual(sv[6:5:-6], [[13, 14]])\n    self.assertAllEqual(sv[6::-7], [[13, 14]])\n    self.assertAllEqual(sv[7:1:-1],\n                        [[15, 16], [13, 14], [11, 12], [9, 10], [7, 8], [5, 6]])\n    self.assertAllEqual(sv[7:1:-2], [[15, 16], [11, 12], [7, 8]])\n    self.assertAllEqual(sv[7:1:-4], [[15, 16], [7, 8]])\n\n    # Test cases: empty slice\n    self.assertAllEqual(sv[0:0], empty)\n    self.assertAllEqual(sv[5:3], empty)\n    self.assertAllEqual(sv[3:5:-1], empty)\n    self.assertAllEqual(sv[-1:0], empty)\n    self.assertAllEqual(sv[2:-1:-1], empty)\n\n    # Test cases: slicing other dimensions\n    self.assertAllEqual(sv[:, 0], [1, 3, 5, 7, 9, 11, 13, 15])\n    self.assertAllEqual(sv[:, 0:1], [[1], [3], [5], [7], [9], [11], [13], [15]])\n\n    # Test cases: normal indexing\n    self.assertAllEqual(sv[2], [5, 6])\n    self.assertAllEqual(sv[6], [13, 14])\n    self.assertAllEqual(sv[2, 1], 6)\n    self.assertAllEqual(sv[-2], [13, 14])\n    with self.assertRaisesRegex(IndexError, 'out of bounds'):\n      _ = sv[100]\n    with self.assertRaisesRegex(IndexError, 'out of bounds'):\n      _ = sv[-100]\n\n    # Test cases: Ellipsis\n    self.assertAllEqual(sv[...], array_ops.concat(v, axis=0))\n    self.assertAllEqual(sv[..., 0], [1, 3, 5, 7, 9, 11, 13, 15])\n    self.assertAllEqual(sv[0:1, ...], [[1, 2]])\n\n    # Test cases: newaxis\n    self.assertAllEqual(\n        sv[array_ops.newaxis, ...],\n        array_ops.expand_dims_v2(array_ops.concat(v, axis=0), axis=0))\n\n    # Test cases: boolean masks\n    self.assertAllEqual(sv[ops.convert_to_tensor(sv) > 10],\n                        [11, 12, 13, 14, 15, 16])\n\n    # Test cases: tensor input\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[constant_op.constant(1)::]\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[:constant_op.constant(1):]\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[constant_op.constant(1)]\n\n    # Test cases: inside tf.function\n    @def_function.function\n    def func():\n      a = sv[:, 0]\n      return a\n\n    self.assertAllEqual(func(), [1, 3, 5, 7, 9, 11, 13, 15])\n\n  def test_operator_overload(self):\n    v1 = [\n        variables_lib.Variable([1.]),\n        variables_lib.Variable([2.]),\n    ]\n    sv1 = sharded_variable.ShardedVariable(v1)\n\n    v2 = [\n        variables_lib.Variable([1.]),\n        variables_lib.Variable([2.]),\n    ]\n    sv2 = sharded_variable.ShardedVariable(v2)\n\n    equal = sv1 == sv2\n    self.assertAllEqual(equal, [True, True])\n    self.assertAllEqual(sv1 + sv2, [2.0, 4.0])\n\n\nif __name__ == '__main__':\n  v2_compat.enable_v2_behavior()\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// Our general strategy for preventing conflicts between concurrent\n// reads and writes of resource variables is to:\n// * For read operations, we:\n//   - acquire the variable's mutex (in \"shared\" mode);\n//   - make a (shallow) copy of the Tensor object, which increments\n//     the reference count on the variable's TensorBuffer;\n//   - release the variable's mutex;\n//   - use the copy of the Tensor object to do the read.\n// * For write operations, we:\n//   - acquire the variable's mutex (in \"exclusive\" mode);\n//   - check the reference count of variable's TensorBuffer and\n//     if it is >1, make a deep copy of the variable's Tensor;\n//   - mutate the variable's Tensor;\n//   - and release the variable's mutex.\n// This allows several read operations to all use the same\n// TensorBuffer without needing to copy. When it comes time to write\n// it will only make a copy if there is an outstanding read using the\n// buffer. Write operations are serialized by the variable's mutex.\n//\n// For sparse operations (scatter, gather, sparse optimizer updates),\n// we need to avoid copies, since there may not be enough memory for\n// to copies of the whole tensor. To support this, we make two\n// modifications to the above strategy:\n// * For sparse reads (gather), we hold the variable's mutex (still in\n//   \"shared\" mode) for the duration of the whole read. This means\n//   that as long as you only do sparse read operations no write will\n//   see the reference count >1.\n// * For sparse write operations where the user explicitly specifies\n//   that they want to perform the write without locks held\n//   (use_locking=false), we never copy even if the variable's\n//   reference count is >1.\n\n#define EIGEN_USE_THREADS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define EIGEN_USE_GPU\n#endif\n\n#include \"tensorflow/core/kernels/resource_variable_ops.h\"\n\n#include <memory>\n#include <vector>\n\n#include \"absl/strings/str_join.h\"\n#include \"tensorflow/core/common_runtime/device.h\"\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/variant_op_registry.h\"\n#include \"tensorflow/core/kernels/dense_update_functor.h\"\n#include \"tensorflow/core/kernels/gather_functor.h\"\n#include \"tensorflow/core/kernels/gather_nd_op.h\"\n#include \"tensorflow/core/kernels/scatter_functor.h\"\n#include \"tensorflow/core/kernels/training_op_helpers.h\"\n#include \"tensorflow/core/kernels/variable_ops.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/refcount.h\"\n#include \"tensorflow/core/platform/casts.h\"\n#include \"tensorflow/core/platform/mem.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/util.h\"\n\nnamespace tensorflow {\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\").Device(DEVICE_CPU),\n                        ResourceHandlesOp<Var>);\n\nReadVariableOp::ReadVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n  OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n}\n\nnamespace {\n\nStatus CopyVariable(int output_idx, OpKernelContext* ctx, const Tensor* t) {\n  Tensor* output;\n  Notification n;\n  Status status;\n  AllocatorAttributes attr;\n  if (t->dtype() == DT_VARIANT) {\n    attr.set_on_host(true);\n  }\n  TF_RETURN_IF_ERROR(\n      ctx->allocate_output(output_idx, t->shape(), &output, attr));\n  if (t->dtype() == DT_VARIANT) {\n    output->flat<Variant>() = t->flat<Variant>();\n  } else if (ctx->op_device_context() != nullptr) {\n    // TODO(apassos): remove the down_cast by just returning Device* from\n    // OpKernelContext\n    Device* device = down_cast<Device*>(ctx->device());\n    ctx->op_device_context()->CopyTensorInSameDevice(\n        t, device, output, [&n, &status](const Status& s) {\n          status = s;\n          n.Notify();\n        });\n    n.WaitForNotification();\n    return status;\n  } else {\n    switch (t->dtype()) {\n#define HANDLER(type)                       \\\n  case DataTypeToEnum<type>::value:         \\\n    output->flat<type>() = t->flat<type>(); \\\n    break;\n      TF_CALL_ALL_TYPES(HANDLER);\n#undef HANDLER\n      default:\n        return errors::Internal(\"Unsupported dtype\", t->dtype());\n    }\n  }\n  return Status::OK();\n}\n\n}  // namespace\n\nvoid ReadVariableOp::Compute(OpKernelContext* ctx) {\n  core::RefCountPtr<Var> variable;\n  const ResourceHandle& handle = HandleFromInput(ctx, 0);\n  const auto status = LookupResource(ctx, handle, &variable);\n  OP_REQUIRES(ctx, status.ok(),\n              errors::FailedPrecondition(\n                  \"Could not find variable \", handle.name(), \". \",\n                  \"This could mean that the variable has been deleted. \",\n                  \"In TF1, it can also mean the variable is uninitialized. \",\n                  \"Debug info: container=\", handle.container(),\n                  \", status=\", status.ToString()));\n\n  tf_shared_lock ml(*variable->mu());\n  // We're acquiring a reference to the underlying buffer while\n  // holding a shared lock to guarantee ordering of reads and\n  // writes when in copy-on-write mode.\n  const Tensor* t = variable->tensor();\n  if (!variable->copy_on_read_mode.load()) {\n    OP_REQUIRES(\n        ctx, dtype_ == t->dtype(),\n        errors::InvalidArgument(\n            \"Trying to read variable with wrong dtype. Expected \",\n            DataTypeString(dtype_), \" got \", DataTypeString(t->dtype())));\n    ctx->set_output(0, *t);\n  } else {\n    OP_REQUIRES_OK(ctx, CopyVariable(0, ctx, t));\n  }\n}\n\nReadVariablesOp::ReadVariablesOp(OpKernelConstruction* c) : OpKernel(c) {\n  int n;\n  OP_REQUIRES_OK(c, c->GetAttr(\"N\", &n));\n  OP_REQUIRES_OK(c, c->GetAttr(\"dtypes\", &dtypes_));\n  OP_REQUIRES(c, n == dtypes_.size(),\n              errors::InvalidArgument(\n                  \"Mismatched number of arguments to ReadVariablesOp (\", n,\n                  \" vs. \", dtypes_.size(), \")\"));\n}\n\nvoid ReadVariablesOp::Compute(OpKernelContext* ctx) {\n  std::vector<core::RefCountPtr<Var>> variables(dtypes_.size());\n  std::vector<const ResourceHandle*> handles(dtypes_.size());\n  for (size_t i = 0; i < dtypes_.size(); ++i) {\n    handles[i] = &HandleFromInput(ctx, i);\n  }\n\n  OP_REQUIRES_OK(ctx, LookupResources(ctx, handles, &variables));\n\n  std::vector<string> uninitialized_vars;\n  for (int64_t i = 0; i < variables.size(); i++) {\n    if (variables[i] == nullptr) {\n      uninitialized_vars.push_back(handles[i]->name());\n    }\n  }\n\n  OP_REQUIRES(ctx, uninitialized_vars.empty(),\n              errors::FailedPrecondition(\n                  \"In ReadVariablesOp the following variables were \"\n                  \"found uninitialized: \",\n                  absl::StrJoin(uninitialized_vars, \", \")));\n\n  for (size_t i = 0; i < dtypes_.size(); ++i) {\n    // We're acquiring a reference to the underlying buffer while\n    // holding a shared lock to guarantee ordering of reads and\n    // writes.\n    tf_shared_lock ml(*variables[i]->mu());\n    OP_REQUIRES(ctx, dtypes_[i] == variables[i]->tensor()->dtype(),\n                errors::InvalidArgument(\n                    \"Trying to read variable \", handles[i]->name(),\n                    \" from Container: \", handles[i]->container(),\n                    \" with wrong dtype. Expected \", DataTypeString(dtypes_[i]),\n                    \" got \", DataTypeString(variables[i]->tensor()->dtype())));\n    if (variables[i]->copy_on_read_mode.load()) {\n      OP_REQUIRES_OK(ctx, CopyVariable(i, ctx, variables[i]->tensor()));\n    } else {\n      const Tensor& t = *variables[i]->tensor();\n      ctx->set_output(i, t);\n    }\n  }\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"ReadVariableOp\").Device(DEVICE_CPU),\n                        ReadVariableOp);\nREGISTER_KERNEL_BUILDER(Name(\"_ReadVariablesOp\").Device(DEVICE_CPU),\n                        ReadVariablesOp);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"ReadVariableOp\").Device(DEVICE_DEFAULT).HostMemory(\"resource\"),\n    ReadVariableOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"_ReadVariablesOp\").Device(DEVICE_DEFAULT).HostMemory(\"resources\"),\n    ReadVariablesOp);\n\nVarHandleOp::VarHandleOp(OpKernelConstruction* context) : OpKernel(context) {\n  OP_REQUIRES_OK(context, context->GetAttr(\"container\", &container_));\n  OP_REQUIRES_OK(context, context->GetAttr(\"shared_name\", &name_));\n\n  OP_REQUIRES_OK(context, context->GetAttr(\"dtype\", &dtype_and_shape_.dtype));\n  OP_REQUIRES_OK(context, context->GetAttr(\"shape\", &dtype_and_shape_.shape));\n\n  is_anonymous_ = name_ == ResourceHandle::ANONYMOUS_NAME;\n\n  if (!is_anonymous_) {\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(context, context->allocate_temp(DT_RESOURCE, TensorShape({}),\n                                                   &resource_, attr));\n    resource_.scalar<ResourceHandle>()() = MakeResourceHandle<Var>(\n        context, container_, name_,\n        std::vector<DtypeAndPartialTensorShape>{dtype_and_shape_});\n  }\n}\n\nvoid VarHandleOp::Compute(OpKernelContext* ctx) {\n  if (is_anonymous_) {\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    Tensor handle;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DT_RESOURCE, TensorShape({}), &handle, attr));\n    handle.scalar<ResourceHandle>()() = MakeResourceHandle<Var>(\n        ctx, container_, name_,\n        std::vector<DtypeAndPartialTensorShape>{dtype_and_shape_},\n        ctx->stack_trace());\n    ctx->set_output(0, handle);\n  } else {\n    ctx->set_output(0, resource_);\n  }\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\").Device(DEVICE_CPU), VarHandleOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nREGISTER_KERNEL_BUILDER(\n    Name(\"ReadVariableOp\").Device(DEVICE_GPU).HostMemory(\"resource\"),\n    ReadVariableOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"_ReadVariablesOp\").Device(DEVICE_GPU).HostMemory(\"resources\"),\n    ReadVariablesOp);\n\n#define REGISTER_GPU_KERNELS(type)                             \\\n  namespace functor {                                          \\\n  template <>                                                  \\\n  void DenseUpdate<GPUDevice, type, ASSIGN>::operator()(       \\\n      const GPUDevice& d, typename TTypes<type>::Flat lhs,     \\\n      typename TTypes<type>::ConstFlat rhs);                   \\\n  extern template struct DenseUpdate<GPUDevice, type, ASSIGN>; \\\n  }                                                            \\\n  REGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\")                  \\\n                              .Device(DEVICE_GPU)              \\\n                              .HostMemory(\"resource\")          \\\n                              .TypeConstraint<type>(\"dtype\"),  \\\n                          VarHandleOp)\nTF_CALL_GPU_ALL_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\nTF_CALL_variant(REGISTER_GPU_KERNELS);\nTF_CALL_uint32(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resources\")\n                            .TypeConstraint(\"dtypes\",\n                                            {DT_INT64, DT_COMPLEX64,\n                                             DT_COMPLEX128, DT_HALF, DT_FLOAT,\n                                             DT_DOUBLE, DT_BOOL, DT_VARIANT}),\n                        ResourceHandlesOp<Var>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_DEFAULT_KERNELS(type)                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"VarHandleOp\")                 \\\n                              .Device(DEVICE_DEFAULT)         \\\n                              .HostMemory(\"resource\")         \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          VarHandleOp)\nTF_CALL_GPU_ALL_TYPES(REGISTER_DEFAULT_KERNELS);\nTF_CALL_int64(REGISTER_DEFAULT_KERNELS);\nTF_CALL_variant(REGISTER_DEFAULT_KERNELS);\nTF_CALL_uint32(REGISTER_DEFAULT_KERNELS);\n#undef REGISTER_DEFAULT_KERNELS\n\nREGISTER_KERNEL_BUILDER(Name(\"_VarHandlesOp\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"resources\")\n                            .TypeConstraint(\"dtypes\",\n                                            {DT_INT64, DT_COMPLEX64,\n                                             DT_COMPLEX128, DT_HALF, DT_FLOAT,\n                                             DT_DOUBLE, DT_BOOL, DT_VARIANT}),\n                        ResourceHandlesOp<Var>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"VariableShape\").Device(DEVICE_CPU).TypeConstraint<int32>(\"out_type\"),\n    VariableShapeOp<int32>);\nREGISTER_KERNEL_BUILDER(\n    Name(\"VariableShape\").Device(DEVICE_CPU).TypeConstraint<int64>(\"out_type\"),\n    VariableShapeOp<int64>);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"VariableShape\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int32>(\"out_type\")\n                            .HostMemory(\"output\")\n                            .HostMemory(\"input\"),\n                        VariableShapeOp<int32>);\nREGISTER_KERNEL_BUILDER(Name(\"VariableShape\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<int64>(\"out_type\")\n                            .HostMemory(\"output\")\n                            .HostMemory(\"input\"),\n                        VariableShapeOp<int64>);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nDestroyResourceOp::DestroyResourceOp(OpKernelConstruction* ctx)\n    : OpKernel(ctx) {\n  OP_REQUIRES_OK(ctx,\n                 ctx->GetAttr(\"ignore_lookup_error\", &ignore_lookup_error_));\n}\n\nvoid DestroyResourceOp::Compute(OpKernelContext* ctx) {\n  const ResourceHandle& p = HandleFromInput(ctx, 0);\n  Status status = DeleteResource(ctx, p);\n  if (ignore_lookup_error_ && errors::IsNotFound(status)) {\n    return;\n  }\n  OP_REQUIRES_OK(ctx, status);\n}\n\nREGISTER_KERNEL_BUILDER(Name(\"DestroyResourceOp\").Device(DEVICE_CPU),\n                        DestroyResourceOp);\nREGISTER_KERNEL_BUILDER(\n    Name(\"DestroyResourceOp\").Device(DEVICE_GPU).HostMemory(\"resource\"),\n    DestroyResourceOp);\n\ntemplate <typename Device, typename T>\nclass AssignVariableOp : public OpKernel {\n public:\n  explicit AssignVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n    if (!c->GetAttr(\"_grappler_relax_allocator_constraints\",\n                    &relax_constraints_)\n             .ok()) {\n      relax_constraints_ = false;\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    OP_REQUIRES(context, dtype_ == context->input(1).dtype(),\n                errors::InvalidArgument(\n                    \"Variable and value dtypes don't match; respectively, \",\n                    DataTypeString(dtype_), \" and \",\n                    DataTypeString(context->input(1).dtype())));\n    core::RefCountPtr<Var> variable;\n    const Tensor& value = context->input(1);\n    // Note: every resource-variable-manipulating op assumes copy-on-write\n    // semantics, and creates a copy of the variable's Tensor if its refcount is\n    // bigger than 1 when we try to modify it. This means we never need to copy\n    // the original tensor for AssignVariableOp; even if there are other live\n    // users of it we know none can modify it so this is always safe (even in\n    // esoteric cases where the same tensor is used to initialize multiple\n    // variables or the tensor is a constant this is safe, as future writes will\n    // trigger copies).\n    OP_REQUIRES_OK(context, LookupOrCreateResource<Var>(\n                                context, HandleFromInput(context, 0), &variable,\n                                [this, &value](Var** ptr) {\n                                  *ptr = new Var(dtype_);\n                                  *(*ptr)->tensor() = value;\n                                  (*ptr)->is_initialized = true;\n                                  return Status::OK();\n                                }));\n    mutex_lock ml(*variable->mu());\n    // (variable->tensor()->dtype() == DT_INVALID && !variable->is_initialized)\n    // check below is to allow an XLA specific situation wherein update can\n    // happen first by the AssignVariableOp,\n    // in which case the variable is still uninitialized.\n    // When using TF-XLA, this scenario is possible when the execution uses the\n    // 'fallback' path (which essentially invokes Tensorflow ops via\n    // partitioned_call).\n    OP_REQUIRES(context,\n                (variable->tensor()->dtype() == DT_INVALID &&\n                 !variable->is_initialized) ||\n                    variable->tensor()->dtype() == dtype_,\n                errors::InvalidArgument(\n                    \"Trying to assign variable with wrong dtype. Expected \",\n                    DataTypeString(variable->tensor()->dtype()), \" got \",\n                    DataTypeString(dtype_)));\n    if (variable->copy_on_read_mode.load()) {\n      AllocatorAttributes attr;\n      attr.set_gpu_compatible(true);\n      attr.set_nic_compatible(true);\n      OP_REQUIRES_OK(context,\n                     context->allocate_temp(value.dtype(), value.shape(),\n                                            variable->tensor(), attr));\n      functor::DenseUpdate<Device, T, ASSIGN> copy_functor;\n      copy_functor(context->eigen_device<Device>(),\n                   variable->tensor()->flat<T>(), value.flat<T>());\n    } else {\n      *variable->tensor() = value;\n    }\n    variable->is_initialized = true;\n  }\n\n private:\n  DataType dtype_;\n  bool relax_constraints_;\n};\n\ntemplate <typename Device>\nclass AssignVariableOp<Device, Variant> : public OpKernel {\n public:\n  explicit AssignVariableOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"dtype\", &dtype_));\n    OP_REQUIRES(c, dtype_ == DT_VARIANT,\n                errors::Internal(\"Variant kernel called with dtype: \",\n                                 DataTypeString(dtype_)));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& value = context->input(1);\n    core::RefCountPtr<Var> variable;\n    OP_REQUIRES_OK(context, LookupOrCreateResource<Var>(\n                                context, HandleFromInput(context, 0), &variable,\n                                [](Var** ptr) {\n                                  // Created on host.\n                                  *ptr = new Var(DT_VARIANT);\n                                  return Status::OK();\n                                }));\n\n    // For purposes of forwarding DT_VARIANT, we want the least\n    // restrictive attr; we already know the input is on host.\n    AllocatorAttributes attr;\n\n    // Copying is unnecessary if we are the last user of the value\n    // tensor, we can just adopt the input tensor's buffer instead.\n    // Note that Variant objects themselves always reside on host.\n    //\n    // We nevertheless want to signal to the runtime that the tensor\n    // should reside in memory of the associated device, as Variant\n    // tensors may be marked as sitting on either CPU or GPU.  This\n    // helps to elide one or more copies.\n    std::unique_ptr<Tensor> input_alias = context->forward_input(\n        1, OpKernelContext::Params::kNoReservation /*output_index*/, DT_VARIANT,\n        value.shape(),\n        DEVICE_MEMORY /* HOST_MEMORY is only reserved for special cases */,\n        attr);\n\n    mutex_lock ml(*variable->mu());\n    OP_REQUIRES(context, variable->tensor()->dtype() == DT_VARIANT,\n                errors::InvalidArgument(\n                    \"Trying to assign variable with wrong dtype. Expected \",\n                    DataTypeString(variable->tensor()->dtype()), \" got \",\n                    DataTypeString(DT_VARIANT)));\n    variable->is_initialized = true;\n    *variable->tensor() = Tensor(DT_VARIANT, value.shape());\n\n    if (input_alias) {\n      *variable->tensor() = *input_alias;\n      return;\n    }\n\n    // Need to copy, but maybe we can re-use variable's buffer?\n    if (!variable->tensor()->RefCountIsOne() ||\n        !variable->tensor()->shape().IsSameSize(value.shape())) {\n      // Allocation of DT_VARIANT is always on host.\n      attr.set_on_host(true);\n      OP_REQUIRES_OK(context, context->allocate_temp(DT_VARIANT, value.shape(),\n                                                     variable->tensor(), attr));\n    }\n\n    const auto elements_in = value.flat<Variant>();\n    auto elements_out = variable->tensor()->flat<Variant>();\n    for (int64_t i = 0; i < elements_in.size(); ++i) {\n      elements_out(i) = elements_in(i);\n    }\n  }\n\n private:\n  DataType dtype_;\n};\n\n#define REGISTER_KERNELS(type)                                \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignVariableOp\")            \\\n                              .Device(DEVICE_CPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          AssignVariableOp<Eigen::ThreadPoolDevice, type>);\n\nTF_CALL_ALL_TYPES(REGISTER_KERNELS);\nTF_CALL_QUANTIZED_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNELS(type)                           \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignVariableOp\")           \\\n                              .Device(DEVICE_GPU)            \\\n                              .TypeConstraint<type>(\"dtype\") \\\n                              .HostMemory(\"resource\"),       \\\n                          AssignVariableOp<GPUDevice, type>);\n\nTF_CALL_GPU_ALL_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\nTF_CALL_variant(REGISTER_GPU_KERNELS);\nTF_CALL_uint32(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntemplate <typename Device, typename T, DenseUpdateType Op>\nclass AssignUpdateVariableOp : public OpKernel {\n public:\n  explicit AssignUpdateVariableOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    core::RefCountPtr<Var> variable;\n    OP_REQUIRES_OK(context, LookupResource(context, HandleFromInput(context, 0),\n                                           &variable));\n\n    const Tensor& value = context->input(1);\n    // TODO(apassos): We could possibly avoid the copy done by\n    // PrepareToUpdateVariable() for commutative operations like Op ==\n    // ADD if value's refcount was 1.\n    mutex_lock ml(*variable->mu());\n    Tensor* var_tensor = variable->tensor();\n    OP_REQUIRES(context, var_tensor->shape().IsSameSize(value.shape()),\n                errors::InvalidArgument(\"Cannot update variable with shape \",\n                                        var_tensor->shape().DebugString(),\n                                        \" using a Tensor with shape \",\n                                        value.shape().DebugString(),\n                                        \", shapes must be equal.\"));\n    OP_REQUIRES_OK(\n        context, PrepareToUpdateVariable<Device, T>(\n                     context, var_tensor, variable->copy_on_read_mode.load()));\n    functor::DenseUpdate<Device, T, Op> update_functor;\n    update_functor(context->eigen_device<Device>(), var_tensor->flat<T>(),\n                   value.flat<T>());\n  }\n};\n\n#define REGISTER_KERNELS(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                         \\\n      Name(\"AssignAddVariableOp\")                                  \\\n          .Device(DEVICE_CPU)                                      \\\n          .TypeConstraint<type>(\"dtype\"),                          \\\n      AssignUpdateVariableOp<Eigen::ThreadPoolDevice, type, ADD>); \\\n  REGISTER_KERNEL_BUILDER(                                         \\\n      Name(\"AssignSubVariableOp\")                                  \\\n          .Device(DEVICE_CPU)                                      \\\n          .TypeConstraint<type>(\"dtype\"),                          \\\n      AssignUpdateVariableOp<Eigen::ThreadPoolDevice, type, SUB>);\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GPU_KERNELS(type)                                       \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignAddVariableOp\")                    \\\n                              .Device(DEVICE_GPU)                        \\\n                              .HostMemory(\"resource\")                    \\\n                              .TypeConstraint<type>(\"dtype\"),            \\\n                          AssignUpdateVariableOp<GPUDevice, type, ADD>); \\\n  REGISTER_KERNEL_BUILDER(Name(\"AssignSubVariableOp\")                    \\\n                              .Device(DEVICE_GPU)                        \\\n                              .HostMemory(\"resource\")                    \\\n                              .TypeConstraint<type>(\"dtype\"),            \\\n                          AssignUpdateVariableOp<GPUDevice, type, SUB>);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GPU_KERNELS);\nTF_CALL_int64(REGISTER_GPU_KERNELS);\n#undef REGISTER_GPU_KERNELS\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nclass VarIsInitializedOp : public OpKernel {\n public:\n  explicit VarIsInitializedOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, TensorShape({}), &output));\n    auto output_tensor = output->tensor<bool, 0>();\n    core::RefCountPtr<Var> variable;\n    Status s = LookupResource(context, HandleFromInput(context, 0), &variable);\n    if (!s.ok()) {\n      output_tensor() = false;\n      return;\n    }\n    mutex_lock ml(*variable->mu());\n    output_tensor() = variable->is_initialized;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\").Device(DEVICE_CPU),\n                        VarIsInitializedOp);\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"is_initialized\"),\n                        IsResourceInitialized<Var>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\nREGISTER_KERNEL_BUILDER(Name(\"VarIsInitializedOp\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"is_initialized\"),\n                        IsResourceInitialized<Var>);\n\ntemplate <typename Device, typename T, typename Index>\nclass ResourceGatherOp : public OpKernel {\n public:\n  explicit ResourceGatherOp(OpKernelConstruction* c) : OpKernel(c) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"batch_dims\", &batch_dims_));\n  }\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n    OP_REQUIRES(\n        c, TensorShapeUtils::IsVectorOrHigher(params.shape()),\n        errors::InvalidArgument(\"params must be at least 1 dimensional\"));\n    OP_REQUIRES(\n        c, params.shape().dims() >= batch_dims_,\n        errors::InvalidArgument(\"params must have at least \", batch_dims_,\n                                \" (batch_dims) dimensions but it has shape \",\n                                params.shape().DebugString()));\n\n    // Check that we have enough index space\n    const int64_t N = indices.NumElements();\n    OP_REQUIRES(\n        c, params.dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params.dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // The result shape is params.shape[:batch_dims] +\n    // indices.shape[batch_dims:] + params.shape[batch_dims+1:].\n    TensorShape result_shape;\n    for (int i = 0; i < batch_dims_; ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n    for (int i = batch_dims_; i < indices.dims(); ++i) {\n      result_shape.AddDim(indices.dim_size(i));\n    }\n    for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n      result_shape.AddDim(params.dim_size(i));\n    }\n\n    Tensor* out = nullptr;\n    Tensor tmp;\n    if (params.dtype() == DT_VARIANT) {\n      tmp = Tensor(DT_VARIANT, result_shape);\n      c->set_output(0, tmp);\n      out = &tmp;\n    } else {\n      OP_REQUIRES_OK(c, c->allocate_output(0, result_shape, &out));\n    }\n\n    if (N > 0) {\n      Tensor tmp_indices;\n\n      // Points to the original or updated (if batch_dims is set) indices.\n      const Tensor* op_indices = &indices;\n      if (batch_dims_ > 0) {\n        OP_REQUIRES_OK(c, c->allocate_temp(indices.dtype(), indices.shape(),\n                                           &tmp_indices));\n        functor::DenseUpdate<Device, Index, ASSIGN> copy_functor;\n        copy_functor(c->eigen_device<Device>(), tmp_indices.flat<Index>(),\n                     indices.flat<Index>());\n\n        AddBatchOffsets(c, &tmp_indices, params);\n        if (!c->status().ok()) return;\n        op_indices = &tmp_indices;\n      }\n\n      int64_t gather_dim_size = 1;\n      for (int idx = 0; idx <= batch_dims_; ++idx) {\n        gather_dim_size *= params.dim_size(idx);\n      }\n      int64_t inner_size = 1;\n      for (int i = batch_dims_ + 1; i < params.dims(); ++i) {\n        inner_size *= params.dim_size(i);\n      }\n      auto params_flat = params.shaped<T, 3>({1, gather_dim_size, inner_size});\n      const auto indices_flat = op_indices->flat<Index>();\n      auto out_flat = out->shaped<T, 3>({1, N, out->NumElements() / N});\n\n      functor::GatherFunctor<Device, T, Index> functor;\n      int64_t bad_i = functor(c, params_flat, indices_flat, out_flat);\n\n      OP_REQUIRES(\n          c, bad_i < 0,\n          errors::InvalidArgument(\n              \"indices\", SliceDebugString(indices.shape(), bad_i), \" = \",\n              indices_flat(bad_i), \" is not in [0, \", params.dim_size(0), \")\"));\n    }\n  }\n\n private:\n  // Add the batch offset derived from params to each batch of indices.\n  // Example: batch_dims = 1, indices = [[0, 1, 2], [0, 1, 2]]\n  // If indexing into a params dimension of size 4, then the indices will become\n  // [0, 1, 2, 4, 5, 6]\n  void AddBatchOffsets(OpKernelContext* ctx, Tensor* indices,\n                       const Tensor& params) {\n    int64_t batch_size = 1;  // The size of all batch dimensions.\n    for (int idx = 0; idx < batch_dims_; ++idx) {\n      batch_size *= params.dim_size(idx);\n    }\n    OP_REQUIRES(\n        ctx, batch_size != 0,\n        errors::InvalidArgument(\n            \"Inner size of indices would result in batch_size of 0 and a \",\n            \"division by 0 in the implementation. This is illegal\"));\n\n    auto indices_flat = indices->flat<Index>();\n    int64_t const index_inner_size = indices->NumElements() / batch_size;\n    int64_t const batch_offset = params.dim_size(batch_dims_);\n    for (int64_t batch_idx = 0, dest_idx = 0; batch_idx < batch_size;\n         ++batch_idx) {\n      for (int64_t idx = 0; idx < index_inner_size; ++idx) {\n        indices_flat(dest_idx++) += batch_offset * batch_idx;\n      }\n    }\n  }\n\n  int32 batch_dims_ = 0;\n};\n\n#define REGISTER_GATHER_FULL(dev, type, index_type)                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")                       \\\n                              .Device(DEVICE_##dev)                    \\\n                              .HostMemory(\"resource\")                  \\\n                              .TypeConstraint<type>(\"dtype\")           \\\n                              .TypeConstraint<index_type>(\"Tindices\"), \\\n                          ResourceGatherOp<dev##Device, type, index_type>)\n\n#define REGISTER_GATHER_ALL_INDICES(dev, type) \\\n  REGISTER_GATHER_FULL(dev, type, int32);      \\\n  REGISTER_GATHER_FULL(dev, type, int64)\n\n#define REGISTER_GATHER_CPU(type) REGISTER_GATHER_ALL_INDICES(CPU, type)\n\n// Registration of the CPU implementations.\nTF_CALL_ALL_TYPES(REGISTER_GATHER_CPU);\nTF_CALL_QUANTIZED_TYPES(REGISTER_GATHER_CPU);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GATHER_GPU(type) REGISTER_GATHER_ALL_INDICES(GPU, type)\n\nTF_CALL_int64(REGISTER_GATHER_GPU);\nTF_CALL_GPU_ALL_TYPES(REGISTER_GATHER_GPU);\n\n// Variant objects themselves sit on CPU, even if they contain data\n// pointing to a device.\nREGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceGatherOp<GPUDevice, Variant, int32>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceGather\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceGatherOp<GPUDevice, Variant, int64>)\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_GATHER_CPU\n#undef REGISTER_GATHER_GPU\n#undef REGISTER_GATHER_ALL_INDICES\n#undef REGISTER_GATHER_FULL\n\ntemplate <typename Device, typename T, typename Index>\nclass ResourceGatherNdOp : public OpKernel {\n public:\n  explicit ResourceGatherNdOp(OpKernelConstruction* c) : OpKernel(c) {}\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    // NOTE: We hold the lock for the whole gather operation instead\n    // of increasing the reference count of v->tensor() to avoid a\n    // situation where a write to the same variable will see a\n    // reference count greater than one and make a copy of the\n    // (potentially very large) tensor buffer.\n    tf_shared_lock ml(*v->mu());\n    const Tensor& params = *v->tensor();\n    const Tensor& indices = c->input(1);\n\n    Tensor out;\n    OP_REQUIRES_OK(\n        c, functor::DoGatherNd<Device, T, Index>(c, params, indices, &out));\n    c->set_output(0, out);\n  }\n};\n\n#define REGISTER_GATHER_ND_FULL(dev, type, index_type)                 \\\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceGatherNd\")                     \\\n                              .Device(DEVICE_##dev)                    \\\n                              .HostMemory(\"resource\")                  \\\n                              .TypeConstraint<type>(\"dtype\")           \\\n                              .TypeConstraint<index_type>(\"Tindices\"), \\\n                          ResourceGatherNdOp<dev##Device, type, index_type>)\n\n#define REGISTER_GATHER_ND_ALL_INDICES(dev, type) \\\n  REGISTER_GATHER_ND_FULL(dev, type, int32);      \\\n  REGISTER_GATHER_ND_FULL(dev, type, int64)\n\n#define REGISTER_GATHER_ND_CPU(type) REGISTER_GATHER_ND_ALL_INDICES(CPU, type)\n\n// Registration of the CPU implementations.\nTF_CALL_ALL_TYPES(REGISTER_GATHER_ND_CPU);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_GATHER_ND_GPU(type) REGISTER_GATHER_ND_ALL_INDICES(GPU, type)\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_GATHER_ND_GPU);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_GATHER_ND_CPU\n#undef REGISTER_GATHER_ND_GPU\n#undef REGISTER_GATHER_ND_ALL_INDICES\n#undef REGISTER_GATHER_ND_FULL\n\nnamespace {\n\ntemplate <typename Device>\nbool isCPUDevice() {\n  return false;\n}\n\ntemplate <>\nbool isCPUDevice<CPUDevice>() {\n  return true;\n}\n\ntemplate <typename T>\nbool ValidateInput(const Tensor& updates) {\n  const auto updates_flat = updates.flat<T>();\n  const T zero(0);\n  for (int i = 0; i < updates.NumElements(); i++) {\n    if (updates_flat(i) == zero) return false;\n  }\n  return true;\n}\n\ntemplate <>\nbool ValidateInput<Variant>(const Tensor& updates) {\n  return true;\n}\n\n}  // namespace\n\ntemplate <typename Device, typename T, typename Index, scatter_op::UpdateOp op>\nclass ResourceScatterUpdateOp : public OpKernel {\n public:\n  explicit ResourceScatterUpdateOp(OpKernelConstruction* c) : OpKernel(c) {\n    // We use the same kernel for many operations.\n    // Each operation has a different set of attributes defined in its nodes.\n    Status s = c->GetAttr(\"use_locking\", &use_exclusive_lock_);\n    if (!s.ok()) {\n      use_exclusive_lock_ = false;\n    }\n  }\n\n  void Compute(OpKernelContext* c) override {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    OP_REQUIRES_OK(c, EnsureSparseVariableAccess<Device, T>(c, v.get()));\n    const bool is_non_pod_dtype = c->input_dtype(0) == DT_RESOURCE ||\n                                  c->input_dtype(0) == DT_STRING ||\n                                  c->input_dtype(0) == DT_VARIANT;\n    if (is_non_pod_dtype || use_exclusive_lock_) {\n      mutex_lock ml(*v->mu());\n      DoCompute(c);\n    } else {\n      // For POD dtypes, we can safely run the update without the mutex.\n      tf_shared_lock ml(*v->mu());\n      DoCompute(c);\n    }\n  }\n\n private:\n  bool use_exclusive_lock_;\n\n  void DoCompute(OpKernelContext* c) {\n    core::RefCountPtr<Var> v;\n    OP_REQUIRES_OK(c, LookupResource(c, HandleFromInput(c, 0), &v));\n    Tensor* params = v->tensor();\n    const Tensor& indices = c->input(1);\n    const Tensor& updates = c->input(2);\n\n    // Check that rank(updates.shape) = rank(indices.shape + params.shape[1:])\n    OP_REQUIRES(c,\n                updates.dims() == 0 ||\n                    updates.dims() == indices.dims() + params->dims() - 1,\n                errors::InvalidArgument(\n                    \"Must have updates.shape = indices.shape + \"\n                    \"params.shape[1:] or updates.shape = [], got \",\n                    \"updates.shape \", updates.shape().DebugString(),\n                    \", indices.shape \", indices.shape().DebugString(),\n                    \", params.shape \", params->shape().DebugString()));\n\n    // Check that we have enough index space\n    const int64_t N_big = indices.NumElements();\n    OP_REQUIRES(\n        c, N_big <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"indices has too many elements for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", N_big, \" > \",\n                                std::numeric_limits<Index>::max()));\n    const Index N = static_cast<Index>(N_big);\n    OP_REQUIRES(\n        c, params->dim_size(0) <= std::numeric_limits<Index>::max(),\n        errors::InvalidArgument(\"params.shape[0] too large for \",\n                                DataTypeString(DataTypeToEnum<Index>::v()),\n                                \" indexing: \", params->dim_size(0), \" > \",\n                                std::numeric_limits<Index>::max()));\n\n    // Prevent division by 0\n    if (isCPUDevice<Device>() && op == tensorflow::scatter_op::UpdateOp::DIV) {\n      OP_REQUIRES(c, ValidateInput<T>(updates),\n                  errors::InvalidArgument(\"updates must not contain 0\"));\n    }\n\n    if (N > 0) {\n      auto indices_flat = indices.flat<Index>();\n      auto params_flat = params->flat_outer_dims<T>();\n      if (TensorShapeUtils::IsScalar(updates.shape())) {\n        const auto update = updates.scalar<T>();\n\n        functor::ScatterScalarFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, update, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      } else {\n        int64_t num_updates = updates.NumElements();\n        OP_REQUIRES(\n            c, TensorShapeUtils::StartsWith(updates.shape(), indices.shape()),\n            errors::InvalidArgument(\n                \"The shape of indices (\", indices.shape().DebugString(),\n                \") must be a prefix of the shape of updates (\",\n                updates.shape().DebugString(), \")\"));\n        auto updates_flat = updates.shaped<T, 2>({N, num_updates / N});\n\n        functor::ScatterFunctor<Device, T, Index, op> functor;\n        const Index bad_i = functor(c, c->template eigen_device<Device>(),\n                                    params_flat, updates_flat, indices_flat);\n        OP_REQUIRES(c, bad_i < 0,\n                    errors::InvalidArgument(\n                        \"indices\", SliceDebugString(indices.shape(), bad_i),\n                        \" = \", indices_flat(bad_i), \" is not in [0, \",\n                        params->dim_size(0), \")\"));\n      }\n    }\n  }\n};\n\n#define REGISTER_SCATTER_KERNEL_INDEX(type, index_type, dev, name, op) \\\n  REGISTER_KERNEL_BUILDER(                                             \\\n      Name(name)                                                       \\\n          .Device(DEVICE_##dev)                                        \\\n          .HostMemory(\"resource\")                                      \\\n          .TypeConstraint<type>(\"dtype\")                               \\\n          .TypeConstraint<index_type>(\"Tindices\"),                     \\\n      ResourceScatterUpdateOp<dev##Device, type, index_type, op>)\n\n#define REGISTER_SCATTER_KERNEL(type, dev, name, op)         \\\n  REGISTER_SCATTER_KERNEL_INDEX(type, int32, dev, name, op); \\\n  REGISTER_SCATTER_KERNEL_INDEX(type, int64, dev, name, op);\n\n#define REGISTER_SCATTER_ARITHMETIC(type, dev)                \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterAdd\",    \\\n                          scatter_op::UpdateOp::ADD);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterSub\",    \\\n                          scatter_op::UpdateOp::SUB);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMul\",    \\\n                          scatter_op::UpdateOp::MUL);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterDiv\",    \\\n                          scatter_op::UpdateOp::DIV);         \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterUpdate\", \\\n                          scatter_op::UpdateOp::ASSIGN);\n#define REGISTER_SCATTER_MINMAX(type, dev)                 \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMin\", \\\n                          scatter_op::UpdateOp::MIN);      \\\n  REGISTER_SCATTER_KERNEL(type, dev, \"ResourceScatterMax\", \\\n                          scatter_op::UpdateOp::MAX);\n\n// Registers CPU kernels.\n#define REGISTER_SCATTER_ARITHMETIC_CPU(type) \\\n  REGISTER_SCATTER_ARITHMETIC(type, CPU);\n#define REGISTER_SCATTER_MINMAX_CPU(type) REGISTER_SCATTER_MINMAX(type, CPU);\n\nTF_CALL_NUMBER_TYPES(REGISTER_SCATTER_ARITHMETIC_CPU);\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_SCATTER_MINMAX_CPU);\n\nREGISTER_SCATTER_KERNEL(tstring, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\nREGISTER_SCATTER_KERNEL(bool, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\nREGISTER_SCATTER_KERNEL(Variant, CPU, \"ResourceScatterUpdate\",\n                        scatter_op::UpdateOp::ASSIGN);\n\n// Registers GPU kernels.\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n#define REGISTER_SCATTER_ARITHMETIC_GPU(type) \\\n  REGISTER_SCATTER_ARITHMETIC(type, GPU);\n#define REGISTER_SCATTER_MINMAX_GPU(type) REGISTER_SCATTER_MINMAX(type, GPU);\n\n#define REGISTER_SCATTER_UPDATE_GPU(type) REGISTER_SCATTER_UPDATE(type, GPU);\n\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_SCATTER_ARITHMETIC_GPU);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_SCATTER_MINMAX_GPU);\n\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, Variant, int32,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .TypeConstraint<bool>(\"dtype\")\n                            .TypeConstraint<int32>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, bool, int32,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .HostMemory(\"indices\")\n                            .TypeConstraint<Variant>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, Variant, int64,\n                                                scatter_op::UpdateOp::ASSIGN>)\nREGISTER_KERNEL_BUILDER(Name(\"ResourceScatterUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"resource\")\n                            .TypeConstraint<int64>(\"dtype\")\n                            .TypeConstraint<int64>(\"Tindices\"),\n                        ResourceScatterUpdateOp<GPUDevice, int64, int64,\n                                                scatter_op::UpdateOp::ASSIGN>)\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#undef REGISTER_SCATTER_ARITHMETIC\n#undef REGISTER_SCATTER_ARITHMETIC_CPU\n#undef REGISTER_SCATTER_MINMAX\n#undef REGISTER_SCATTER_MINMAX_CPU\n#undef REGISTER_SCATTER_KERNEL\n#undef REGISTER_SCATTER_KERNEL_INDEX\n\n}  // namespace tensorflow\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for ShardedVariable.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import parameterized\n\nfrom tensorflow.python.client import session as session_lib\nfrom tensorflow.python.compat import v2_compat\nfrom tensorflow.python.distribute import sharded_variable\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import tensor_spec\nfrom tensorflow.python.module import module\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import embedding_ops\nfrom tensorflow.python.ops import variables as variables_lib\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import load\nfrom tensorflow.python.saved_model import loader\nfrom tensorflow.python.saved_model import save\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.training.tracking import tracking\nfrom tensorflow.python.training.tracking import util\nfrom tensorflow.python.util import nest\n\n\ndef _load_and_run(\n    model_dir,\n    inputs,\n    signature_key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY):\n  \"\"\"Load a SavedModel into a TF 1.x-style graph and run `signature_key`.\"\"\"\n  graph = ops.Graph()\n  with graph.as_default(), session_lib.Session() as session:\n    meta_graph_def = loader.load(session, [tag_constants.SERVING], model_dir)\n    signature = meta_graph_def.signature_def[signature_key]\n    feed_dict = {}\n    for arg_name in inputs.keys():\n      input_tensor = session.graph.get_tensor_by_name(\n          signature.inputs[arg_name].name)\n      feed_dict[input_tensor] = inputs[arg_name]\n    output_dict = {}\n    for output_name, output_tensor_info in signature.outputs.items():\n      output_dict[output_name] = session.graph.get_tensor_by_name(\n          output_tensor_info.name)\n    return session.run(output_dict, feed_dict=feed_dict)\n\n\nclass PartitionerTest(test.TestCase):\n\n  def test_fixed_shards_partitioner(self):\n    partitioner = sharded_variable.FixedShardsPartitioner(num_shards=2)\n    got = partitioner(tensor_shape.TensorShape([10, 3]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n  def test_min_size_partitioner(self):\n    partitioner = sharded_variable.MinSizePartitioner(\n        min_shard_bytes=4, max_shards=2)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n    partitioner = sharded_variable.MinSizePartitioner(\n        min_shard_bytes=4, max_shards=10)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [6, 1])\n\n  def test_max_size_partitioner(self):\n    partitioner = sharded_variable.MaxSizePartitioner(max_shard_bytes=4)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [6, 1])\n\n    partitioner = sharded_variable.MaxSizePartitioner(\n        max_shard_bytes=4, max_shards=2)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [2, 1])\n\n    partitioner = sharded_variable.MaxSizePartitioner(max_shard_bytes=1024)\n    got = partitioner(tensor_shape.TensorShape([6, 1]), dtypes.float32)\n    self.assertAllEqual(got, [1, 1])\n\n\nclass ShardedVariableTest(test.TestCase, parameterized.TestCase):\n\n  def test_sharded_variable_simple(self):\n    v0 = variables_lib.Variable([0])\n    v1 = variables_lib.Variable([1])\n    s = sharded_variable.ShardedVariable([v0, v1], name='s')\n    self.assertEqual(s.variables[0], v0)\n    self.assertEqual(s.variables[1], v1)\n    self.assertEqual(s.shape.as_list(), [2])\n    self.assertEqual(s.dtype, v0.dtype)\n    self.assertEqual(s.name, 's')\n\n  def test_assign(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign([[4, 4], [5, 5], [6, 6], [7, 7]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[4, 4]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[5, 5], [6, 6]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[7, 7]])\n    self.assertIs(ret, s)\n\n  def test_assign_add(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign_add([[1, 1], [1, 1], [2, 2], [2, 2]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[1, 1]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[2, 2], [4, 4]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[5, 5]])\n    self.assertIs(ret, s)\n\n  def test_assign_sub(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    ret = s.assign_sub([[0, 0], [1, 1], [1, 1], [3, 3]])\n    self.assertAllEqual(self.evaluate(s.variables[0]), [[0, 0]])\n    self.assertAllEqual(self.evaluate(s.variables[1]), [[0, 0], [1, 1]])\n    self.assertAllEqual(self.evaluate(s.variables[2]), [[0, 0]])\n    self.assertIs(ret, s)\n\n  def test_scatter_add_uneven_partition(self):\n    v = variables_lib.Variable(array_ops.zeros((32, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.], [5.]]),\n        indices=constant_op.constant([0, 10, 11, 12, 30, 31]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    v.scatter_add(sparse_delta)\n    sv.scatter_add(sparse_delta)\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      v.scatter_add(sparse_delta)\n      sv.scatter_add(sparse_delta)\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  @parameterized.parameters('scatter_add', 'scatter_div', 'scatter_max',\n                            'scatter_min', 'scatter_mul', 'scatter_sub',\n                            'scatter_update')\n  def test_scatter_ops_even_partition(self, op):\n    v = variables_lib.Variable(array_ops.zeros((30, 1)))\n    # Make sure values does not contain 0 due to testing `scatter_div`!\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[1.], [2.], [3.], [4.], [5.]]),\n        indices=constant_op.constant([0, 10, 12, 21, 22]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    getattr(v, op)(sparse_delta, name='scatter_v')\n    getattr(sv, op)(sparse_delta, name='scatter_sv')\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      getattr(v, op)(sparse_delta, name='scatter_v')\n      getattr(sv, op)(sparse_delta, name='scatter_sv')\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  def test_batch_scatter_update(self):\n    v = variables_lib.Variable(array_ops.zeros((32, 1)))\n    sparse_delta = ops.IndexedSlices(\n        values=constant_op.constant([[0.], [1.], [2.], [3.], [4.], [5.]]),\n        indices=constant_op.constant([10, 11, 12, 13, 14, 15]))\n\n    v0 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((11, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    v.batch_scatter_update(sparse_delta)\n    sv.batch_scatter_update(sparse_delta)\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n    @def_function.function\n    def func():\n      v.batch_scatter_update(sparse_delta)\n      sv.batch_scatter_update(sparse_delta)\n\n    func()\n    self.assertAllEqual(v, ops.convert_to_tensor(sv))\n\n  def test_sparse_read(self):\n    v = variables_lib.Variable(array_ops.zeros((30, 1)))\n    indices = constant_op.constant([0, 10, 12, 21, 22])\n\n    v0 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v1 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    v2 = variables_lib.Variable(array_ops.zeros((10, 1)))\n    sv = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    self.assertAllEqual(v.sparse_read(indices), sv.sparse_read(indices))\n\n    @def_function.function\n    def func():\n      return v.sparse_read(indices), sv.sparse_read(indices)\n\n    got, expect = func()\n    self.assertAllEqual(got, expect)\n\n  def test_control_dep_on_assign(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n\n    @def_function.function\n    def func():\n      ret = s.assign([[4, 4], [5, 5], [6, 6], [7, 7]])\n      with ops.control_dependencies([ret]):\n        a = array_ops.ones((1, 1))\n      with ops.control_dependencies([control_flow_ops.group(ret)]):\n        b = array_ops.ones((1, 1))\n      return a, b\n\n    func()\n\n  def test_convert_to_tensor(self):\n    v0 = variables_lib.Variable([[0, 0]])\n    v1 = variables_lib.Variable([[1, 1], [2, 2]])\n    v2 = variables_lib.Variable([[3, 3]])\n    s = sharded_variable.ShardedVariable([v0, v1, v2])\n    t = ops.convert_to_tensor(s)\n    self.assertAllEqual(t, [[0, 0], [1, 1], [2, 2], [3, 3]])\n\n  def test_save_restore(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n\n    cp = util.Checkpoint(s=s)\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [0])\n    cp.write(fname)\n\n    self.evaluate(cp.s.variables[0].assign([4]))\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [4])\n\n    cp.restore(fname)\n    # Tests that the original weights are restored.\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [0])\n\n  def test_save_restore_different_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n\n    cp = util.Checkpoint(s=s)\n    cp.write(fname)\n\n    variables2 = [variables_lib.Variable([0, 0, 0, 0])]\n    s2 = sharded_variable.ShardedVariable(variables2, name='s')\n\n    # Restore from 4 partitions into 1.\n    cp2 = util.Checkpoint(s=s2)\n    cp2.restore(fname)\n    self.assertAllEqual(self.evaluate(cp2.s.variables[0]), [0, 1, 2, 3])\n\n    self.evaluate(cp2.s.variables[0].assign([5, 10, 15, 20]))\n    cp2.write(fname)\n\n    # Restore 1 partition into 4.\n    cp.restore(fname)\n    self.assertEqual(self.evaluate(cp.s.variables[0]), [5])\n    self.assertEqual(self.evaluate(cp.s.variables[1]), [10])\n    self.assertEqual(self.evaluate(cp.s.variables[2]), [15])\n    self.assertEqual(self.evaluate(cp.s.variables[3]), [20])\n\n  def test_save_restore_4_to_2_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    s = sharded_variable.ShardedVariable(variables, name='s')\n    cp = util.Checkpoint(s=s)\n    cp.write(fname)\n\n    variables2 = [\n        variables_lib.Variable([0, 0]),\n        variables_lib.Variable([0, 0])\n    ]\n    s2 = sharded_variable.ShardedVariable(variables2, name='s')\n    cp2 = util.Checkpoint(s=s2)\n    cp2.restore(fname)\n    # Assert that weights from the 4 partitions were loaded here.\n    self.assertLen(cp2.s.variables, 2)\n    self.assertAllEqual(self.evaluate(cp2.s.variables[0]), [0, 1])\n    self.assertAllEqual(self.evaluate(cp2.s.variables[1]), [2, 3])\n\n  def test_delayed_restore(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    model = tracking.AutoTrackable()\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    model.s = sharded_variable.ShardedVariable(variables)\n    cp = util.Checkpoint(model=model)\n    cp.write(fname)\n\n    model2 = tracking.AutoTrackable()\n    cp2 = util.Checkpoint(model=model2)\n    cp2.restore(fname)\n    variables2 = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0]),\n        variables_lib.Variable([0])\n    ]\n    model2.s = sharded_variable.ShardedVariable(variables2)\n    self.assertAllEqual(self.evaluate(model2.s.variables[0]), [0])\n    self.assertAllEqual(self.evaluate(model2.s.variables[1]), [1])\n    self.assertAllEqual(self.evaluate(model2.s.variables[2]), [2])\n    self.assertAllEqual(self.evaluate(model2.s.variables[3]), [3])\n\n  def test_delayed_restore_4_to_2_partitions(self):\n    fname = os.path.join(self.get_temp_dir(), 'checkpoint')\n    model = tracking.AutoTrackable()\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n        variables_lib.Variable([2]),\n        variables_lib.Variable([3])\n    ]\n    model.s = sharded_variable.ShardedVariable(variables)\n    cp = util.Checkpoint(model=model)\n    cp.write(fname)\n\n    model2 = tracking.AutoTrackable()\n    cp2 = util.Checkpoint(model=model2)\n    cp2.restore(fname)\n    variables2 = [\n        variables_lib.Variable([0, 0]),\n        variables_lib.Variable([0, 0])\n    ]\n    model2.s = sharded_variable.ShardedVariable(variables2)\n    self.assertAllEqual(self.evaluate(model2.s.variables[0]), [0, 1])\n    self.assertAllEqual(self.evaluate(model2.s.variables[1]), [2, 3])\n\n  def test_save_graph_def(self):\n    root = tracking.AutoTrackable()\n    v1 = variables_lib.Variable([3.])\n    v2 = variables_lib.Variable([2.])\n    root.v = sharded_variable.ShardedVariable([v1, v2])\n    root.train = def_function.function(\n        lambda x: embedding_ops.embedding_lookup_v2(root.v.variables, x))\n    # TODO(b/144057383): Remove the necessity of root.serve once saving context\n    # is made to tf.function cache.\n    root.serve = def_function.function(\n        lambda x: embedding_ops.embedding_lookup_v2(root.v.variables[0], x),\n        input_signature=[tensor_spec.TensorSpec([2], dtypes.int32, name='x')])\n\n    # Trace and use root.train\n    self.assertAllEqual([3., 2.], root.train([0, 1]).numpy())\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save.save(root, save_dir, root.serve)\n    self.assertAllEqual([3., 2.],\n                        _load_and_run(save_dir, {'x': [0, 1]})['output_0'])\n\n    # Continue using root.train for training\n    self.assertAllEqual([3., 2.], root.train([0, 1]).numpy())\n\n  def test_load_raises_error(self):\n    root = tracking.AutoTrackable()\n    v1 = variables_lib.Variable([3.])\n    v2 = variables_lib.Variable([2.])\n    root.v = sharded_variable.ShardedVariable([v1, v2])\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save.save(root, save_dir)\n\n    with self.assertRaisesRegex(\n        ValueError, 'Loading a saved_model containing ShardedVariable'):\n      load.load(save_dir)\n\n  def test_validation_errors(self):\n    with self.assertRaisesRegex(ValueError, 'Expected a list of '):\n      sharded_variable.ShardedVariable(\n          [variables_lib.Variable([0]), 'not-a-variable'])\n\n    with self.assertRaisesRegex(ValueError, 'must have the same dtype'):\n      sharded_variable.ShardedVariable([\n          variables_lib.Variable([0], dtype='int64'),\n          variables_lib.Variable([1], dtype='int32')\n      ])\n\n    with self.assertRaisesRegex(ValueError, 'the same shapes except'):\n      sharded_variable.ShardedVariable([\n          variables_lib.Variable(array_ops.ones((5, 10))),\n          variables_lib.Variable(array_ops.ones((5, 20)))\n      ])\n\n    with self.assertRaisesRegex(ValueError, '`SaveSliceInfo` should not'):\n      v = variables_lib.Variable([0])\n      v._set_save_slice_info(\n          variables_lib.Variable.SaveSliceInfo(\n              full_name='s', full_shape=[2], var_offset=[0], var_shape=[1]))\n      sharded_variable.ShardedVariable([v])\n\n  def test_as_function_input(self):\n    variables1 = [\n        variables_lib.Variable([1]),\n        variables_lib.Variable([1]),\n    ]\n    s = sharded_variable.ShardedVariable(variables1)\n    variables2 = [\n        variables_lib.Variable([2]),\n        variables_lib.Variable([2]),\n    ]\n    s2 = sharded_variable.ShardedVariable(variables2)\n\n    trace_count = [0]\n\n    @def_function.function\n    def func(sharded_var):\n      trace_count[0] = trace_count[0] + 1\n      sharded_var.assign([0, 0])\n\n    func(s)\n    self.assertAllEqual(ops.convert_to_tensor(s), [0, 0])\n    self.assertEqual(trace_count[0], 1)\n    func(s2)\n    self.assertAllEqual(ops.convert_to_tensor(s2), [0, 0])\n    self.assertEqual(trace_count[0], 1)\n\n  def test_flatten(self):\n    variables = [\n        variables_lib.Variable([0]),\n        variables_lib.Variable([1]),\n    ]\n    s = sharded_variable.ShardedVariable(variables)\n\n    got = nest.flatten(s)\n    self.assertIs(s, got[0])\n\n    got = nest.flatten(s, expand_composites=True)\n    self.assertAllEqual(variables, got)\n\n  def test_tf_module(self):\n\n    class Model(module.Module):\n\n      def __init__(self):\n        super().__init__()\n        variables = [\n            variables_lib.Variable([0]),\n            variables_lib.Variable([1]),\n        ]\n        self.w = sharded_variable.ShardedVariable(variables)\n\n    model = Model()\n\n    self.assertLen(model.variables, 2)\n    self.assertEqual(model.variables[0], [0])\n    self.assertEqual(model.variables[1], [1])\n    self.assertAllEqual(model.variables, model.trainable_variables)\n\n    self.assertLen(model._checkpoint_dependencies, 1)\n    self.assertIs(model._checkpoint_dependencies[0].ref, model.w)\n\n  def test_embedding_lookup(self):\n    v = [\n        variables_lib.Variable([[1., 2.], [3., 4.]]),\n        variables_lib.Variable([[5., 6.], [7., 8.]]),\n        variables_lib.Variable([[9., 10.]])\n    ]\n    sv = sharded_variable.ShardedVariable(v)\n\n    @def_function.function\n    def lookup():\n      ids = constant_op.constant([0, 3, 4])\n      return embedding_ops.embedding_lookup_v2(sv, ids)\n\n    @def_function.function\n    def sparse_lookup():\n      sp_ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[0, 3, 4, 1],\n          dense_shape=[3, 3])\n      return embedding_ops.embedding_lookup_sparse_v2(sv, sp_ids, None)\n\n    @def_function.function\n    def safe_sparse_lookup():\n      sp_ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[0, -1, 4, 1],\n          dense_shape=[3, 3])\n      sp_weights = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1], [1, 0], [2, 2]],\n          values=[1., 1., -1., 1.],\n          dense_shape=[3, 3])\n      return embedding_ops.safe_embedding_lookup_sparse_v2(\n          sv, sp_ids, sp_weights)\n\n    # TODO(chenkai): Add safe_sparse_lookup to the list. Currently\n    # ShardedVariable is converted to a tensor in safe_sparse_lookup.\n    for func in [lookup, sparse_lookup]:\n      num_gather_ops = 0\n      for op in func.get_concrete_function().graph.get_operations():\n        if op.type == 'ResourceGather':\n          num_gather_ops += 1\n      self.assertEqual(\n          num_gather_ops, len(v), 'Number of ResourceGather op does not match'\n          ' expected, possibly due to ShardedVariable accidentally being'\n          ' converted to tensor in embedding_lookup ops.')\n\n    self.assertAllEqual(lookup(), [[1., 2.], [7., 8.], [9., 10.]])\n    self.assertAllClose(sparse_lookup(), [[4., 5.], [9., 10.], [3., 4.]])\n    self.assertAllClose(safe_sparse_lookup(), [[1., 2.], [0., 0.], [3., 4.]])\n\n  def test_slicing(self):\n    v = [\n        variables_lib.Variable([[1, 2], [3, 4], [5, 6]]),\n        variables_lib.Variable([[7, 8], [9, 10], [11, 12]]),\n        variables_lib.Variable([[13, 14], [15, 16]])\n    ]\n    sv = sharded_variable.ShardedVariable(v)\n    empty = v[0][0:0]\n\n    # Test cases: positive step\n    self.assertAllEqual(sv[:], array_ops.concat(v, axis=0))\n    self.assertAllEqual(sv[:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[-8:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[-10:2], [[1, 2], [3, 4]])\n    self.assertAllEqual(sv[5:], [[11, 12], [13, 14], [15, 16]])\n    self.assertAllEqual(sv[5:-1], [[11, 12], [13, 14]])\n    self.assertAllEqual(sv[::3], [[1, 2], [7, 8], [13, 14]])\n    self.assertAllEqual(sv[::5], [[1, 2], [11, 12]])\n    self.assertAllEqual(sv[1::6], [[3, 4], [15, 16]])\n    self.assertAllEqual(sv[1:5:6], [[3, 4]])\n    self.assertAllEqual(sv[1::7], [[3, 4]])\n    self.assertAllEqual(sv[2:7], [[5, 6], [7, 8], [9, 10], [11, 12], [13, 14]])\n    self.assertAllEqual(sv[2:7:2], [[5, 6], [9, 10], [13, 14]])\n    self.assertAllEqual(sv[2:7:3], [[5, 6], [11, 12]])\n\n    # Test cases: negative step\n    self.assertAllEqual(\n        sv[::-1], array_ops.reverse(array_ops.concat(v, axis=0), axis=[0]))\n    self.assertAllEqual(sv[2::-1], [[5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[2:-8:-1], [[5, 6], [3, 4]])\n    self.assertAllEqual(sv[2:-10:-1], [[5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[4::-1], [[9, 10], [7, 8], [5, 6], [3, 4], [1, 2]])\n    self.assertAllEqual(sv[-1:-3:-1], [[15, 16], [13, 14]])\n    self.assertAllEqual(sv[::-5], [[15, 16], [5, 6]])\n    self.assertAllEqual(sv[6::-6], [[13, 14], [1, 2]])\n    self.assertAllEqual(sv[6:5:-6], [[13, 14]])\n    self.assertAllEqual(sv[6::-7], [[13, 14]])\n    self.assertAllEqual(sv[7:1:-1],\n                        [[15, 16], [13, 14], [11, 12], [9, 10], [7, 8], [5, 6]])\n    self.assertAllEqual(sv[7:1:-2], [[15, 16], [11, 12], [7, 8]])\n    self.assertAllEqual(sv[7:1:-4], [[15, 16], [7, 8]])\n\n    # Test cases: empty slice\n    self.assertAllEqual(sv[0:0], empty)\n    self.assertAllEqual(sv[5:3], empty)\n    self.assertAllEqual(sv[3:5:-1], empty)\n    self.assertAllEqual(sv[-1:0], empty)\n    self.assertAllEqual(sv[2:-1:-1], empty)\n\n    # Test cases: slicing other dimensions\n    self.assertAllEqual(sv[:, 0], [1, 3, 5, 7, 9, 11, 13, 15])\n    self.assertAllEqual(sv[:, 0:1], [[1], [3], [5], [7], [9], [11], [13], [15]])\n\n    # Test cases: normal indexing\n    self.assertAllEqual(sv[2], [5, 6])\n    self.assertAllEqual(sv[6], [13, 14])\n    self.assertAllEqual(sv[2, 1], 6)\n    self.assertAllEqual(sv[-2], [13, 14])\n    with self.assertRaisesRegex(IndexError, 'out of bounds'):\n      _ = sv[100]\n    with self.assertRaisesRegex(IndexError, 'out of bounds'):\n      _ = sv[-100]\n\n    # Test cases: Ellipsis\n    self.assertAllEqual(sv[...], array_ops.concat(v, axis=0))\n    self.assertAllEqual(sv[..., 0], [1, 3, 5, 7, 9, 11, 13, 15])\n    self.assertAllEqual(sv[0:1, ...], [[1, 2]])\n\n    # Test cases: newaxis\n    self.assertAllEqual(\n        sv[array_ops.newaxis, ...],\n        array_ops.expand_dims_v2(array_ops.concat(v, axis=0), axis=0))\n\n    # Test cases: boolean masks\n    self.assertAllEqual(sv[ops.convert_to_tensor(sv) > 10],\n                        [11, 12, 13, 14, 15, 16])\n\n    # Test cases: tensor input\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[constant_op.constant(1)::]\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[:constant_op.constant(1):]\n    with self.assertRaisesRegex(TypeError, 'not allowed'):\n      _ = sv[constant_op.constant(1)]\n\n    # Test cases: inside tf.function\n    @def_function.function\n    def func():\n      a = sv[:, 0]\n      return a\n\n    self.assertAllEqual(func(), [1, 3, 5, 7, 9, 11, 13, 15])\n\n  def test_operator_overload(self):\n    v1 = [\n        variables_lib.Variable([1.]),\n        variables_lib.Variable([2.]),\n    ]\n    sv1 = sharded_variable.ShardedVariable(v1)\n\n    v2 = [\n        variables_lib.Variable([1.]),\n        variables_lib.Variable([2.]),\n    ]\n    sv2 = sharded_variable.ShardedVariable(v2)\n\n    equal = sv1 == sv2\n    self.assertAllEqual(equal, [True, True])\n    self.assertAllEqual(sv1 + sv2, [2.0, 4.0])\n\n\nif __name__ == '__main__':\n  v2_compat.enable_v2_behavior()\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/resource_variable_ops.cc", "tensorflow/python/distribute/sharded_variable_test.py"], "buggy_code_start_loc": [875, 177], "buggy_code_end_loc": [940, 180], "fixing_code_start_loc": [876, 178], "fixing_code_end_loc": [976, 181], "type": "CWE-369", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of `tf.raw_ops.ResourceScatterDiv` is vulnerable to a division by 0 error. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/resource_variable_ops.cc#L865) uses a common class for all binary operations but fails to treat the division by 0 case separately. We have patched the issue in GitHub commit 4aacb30888638da75023e6601149415b39763d76. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37642", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T18:15:10.633", "lastModified": "2021-08-18T17:34:35.453", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of `tf.raw_ops.ResourceScatterDiv` is vulnerable to a division by 0 error. The [implementation](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/resource_variable_ops.cc#L865) uses a common class for all binary operations but fails to treat the division by 0 case separately. We have patched the issue in GitHub commit 4aacb30888638da75023e6601149415b39763d76. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico. En las versiones afectadas la implementaci\u00f3n de \"tf.raw_ops.ResourceScatterDiv\" es vulnerable a un error de divisi\u00f3n por 0. La [implementaci\u00f3n](https://github.com/tensorflow/tensorflow/blob/8d72537c6abf5a44103b57b9c2e22c14f5f49698/tensorflow/core/kernels/resource_variable_ops.cc#L865) usa una clase com\u00fan para todas las operaciones binarias, pero no trata el caso de la divisi\u00f3n por 0 por separado. Hemos parcheado el problema en el commit 4aacb30888638da75023e6601149415b39763d76 de GitHub. La correcci\u00f3n se incluir\u00e1 en TensorFlow versi\u00f3n 2.6.0. Tambi\u00e9n se incluir\u00e1 este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3, y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango de soporte."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/4aacb30888638da75023e6601149415b39763d76", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-ch4f-829c-v5pw", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/4aacb30888638da75023e6601149415b39763d76"}}