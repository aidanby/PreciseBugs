{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io\n */\n#include <uapi/linux/btf.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/perf_event.h>\n#include <linux/ctype.h>\n#include <linux/error-injection.h>\n#include <linux/bpf_lsm.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are\n * four pointer types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n *\n * The following reference types represent a potential reference to a kernel\n * resource which, after first being allocated, must be checked and freed by\n * the BPF program:\n * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET\n *\n * When the verifier sees a helper call return a reference type, it allocates a\n * pointer id for the reference and stores it in the current function state.\n * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into\n * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type\n * passes through a NULL-check conditional. For the branch wherein the state is\n * changed to CONST_IMM, the verifier releases the reference.\n *\n * For each helper function that allocates a reference, such as\n * bpf_sk_lookup_tcp(), there is a corresponding release function, such as\n * bpf_sk_release(). When a reference type passes into the release function,\n * the verifier also releases the reference. If any unchecked or unreleased\n * reference remains at the end of the program, the verifier rejects it.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n\t/* length of verifier log at the time this state was pushed on stack */\n\tu32 log_pos;\n};\n\n#define BPF_COMPLEXITY_LIMIT_JMP_SEQ\t8192\n#define BPF_COMPLEXITY_LIMIT_STATES\t64\n\n#define BPF_MAP_KEY_POISON\t(1ULL << 63)\n#define BPF_MAP_KEY_SEEN\t(1ULL << 62)\n\n#define BPF_MAP_PTR_UNPRIV\t1UL\n#define BPF_MAP_PTR_POISON\t((void *)((0xeB9FUL << 1) +\t\\\n\t\t\t\t\t  POISON_POINTER_DELTA))\n#define BPF_MAP_PTR(X)\t\t((struct bpf_map *)((X) & ~BPF_MAP_PTR_UNPRIV))\n\nstatic bool bpf_map_ptr_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn BPF_MAP_PTR(aux->map_ptr_state) == BPF_MAP_PTR_POISON;\n}\n\nstatic bool bpf_map_ptr_unpriv(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_ptr_state & BPF_MAP_PTR_UNPRIV;\n}\n\nstatic void bpf_map_ptr_store(struct bpf_insn_aux_data *aux,\n\t\t\t      const struct bpf_map *map, bool unpriv)\n{\n\tBUILD_BUG_ON((unsigned long)BPF_MAP_PTR_POISON & BPF_MAP_PTR_UNPRIV);\n\tunpriv |= bpf_map_ptr_unpriv(aux);\n\taux->map_ptr_state = (unsigned long)map |\n\t\t\t     (unpriv ? BPF_MAP_PTR_UNPRIV : 0UL);\n}\n\nstatic bool bpf_map_key_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & BPF_MAP_KEY_POISON;\n}\n\nstatic bool bpf_map_key_unseen(const struct bpf_insn_aux_data *aux)\n{\n\treturn !(aux->map_key_state & BPF_MAP_KEY_SEEN);\n}\n\nstatic u64 bpf_map_key_immediate(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & ~(BPF_MAP_KEY_SEEN | BPF_MAP_KEY_POISON);\n}\n\nstatic void bpf_map_key_store(struct bpf_insn_aux_data *aux, u64 state)\n{\n\tbool poisoned = bpf_map_key_poisoned(aux);\n\n\taux->map_key_state = state | BPF_MAP_KEY_SEEN |\n\t\t\t     (poisoned ? BPF_MAP_KEY_POISON : 0ULL);\n}\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n\tint mem_size;\n\tu64 msize_max_value;\n\tint ref_obj_id;\n\tint func_id;\n\tu32 btf_id;\n};\n\nstruct btf *btf_vmlinux;\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\nstatic const struct bpf_line_info *\nfind_linfo(const struct bpf_verifier_env *env, u32 insn_off)\n{\n\tconst struct bpf_line_info *linfo;\n\tconst struct bpf_prog *prog;\n\tu32 i, nr_linfo;\n\n\tprog = env->prog;\n\tnr_linfo = prog->aux->nr_linfo;\n\n\tif (!nr_linfo || insn_off >= prog->len)\n\t\treturn NULL;\n\n\tlinfo = prog->aux->linfo;\n\tfor (i = 1; i < nr_linfo; i++)\n\t\tif (insn_off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\treturn &linfo[i - 1];\n}\n\nvoid bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,\n\t\t       va_list args)\n{\n\tunsigned int n;\n\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (log->level == BPF_LOG_KERNEL) {\n\t\tpr_err(\"BPF:%s\\n\", log->kbuf);\n\t\treturn;\n\t}\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\nstatic void bpf_vlog_reset(struct bpf_verifier_log *log, u32 new_pos)\n{\n\tchar zero = 0;\n\n\tif (!bpf_verifier_log_needed(log))\n\t\treturn;\n\n\tlog->len_used = new_pos;\n\tif (put_user(zero, log->ubuf + new_pos))\n\t\tlog->ubuf = NULL;\n}\n\n/* log_level controls verbosity level of eBPF verifier.\n * bpf_verifier_log_write() is used to dump the verification trace to the log,\n * so the user can figure out what's wrong with the program\n */\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\nEXPORT_SYMBOL_GPL(bpf_verifier_log_write);\n\n__printf(2, 3) static void verbose(void *private_data, const char *fmt, ...)\n{\n\tstruct bpf_verifier_env *env = private_data;\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\n\n__printf(2, 3) void bpf_log(struct bpf_verifier_log *log,\n\t\t\t    const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(log, fmt, args);\n\tva_end(args);\n}\n\nstatic const char *ltrim(const char *s)\n{\n\twhile (isspace(*s))\n\t\ts++;\n\n\treturn s;\n}\n\n__printf(3, 4) static void verbose_linfo(struct bpf_verifier_env *env,\n\t\t\t\t\t u32 insn_off,\n\t\t\t\t\t const char *prefix_fmt, ...)\n{\n\tconst struct bpf_line_info *linfo;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tlinfo = find_linfo(env, insn_off);\n\tif (!linfo || linfo == env->prev_linfo)\n\t\treturn;\n\n\tif (prefix_fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, prefix_fmt);\n\t\tbpf_verifier_vlog(&env->log, prefix_fmt, args);\n\t\tva_end(args);\n\t}\n\n\tverbose(env, \"%s\\n\",\n\t\tltrim(btf_name_by_offset(env->prog->aux->btf,\n\t\t\t\t\t linfo->line_off)));\n\n\tenv->prev_linfo = linfo;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\nstatic bool type_is_sk_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCK_COMMON ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_XDP_SOCK;\n}\n\nstatic bool reg_type_not_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_MAP_VALUE ||\n\t\ttype == PTR_TO_SOCK_COMMON;\n}\n\nstatic bool reg_type_may_be_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_MAP_VALUE_OR_NULL ||\n\t       type == PTR_TO_SOCKET_OR_NULL ||\n\t       type == PTR_TO_SOCK_COMMON_OR_NULL ||\n\t       type == PTR_TO_TCP_SOCK_OR_NULL ||\n\t       type == PTR_TO_BTF_ID_OR_NULL ||\n\t       type == PTR_TO_MEM_OR_NULL ||\n\t       type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t       type == PTR_TO_RDWR_BUF_OR_NULL;\n}\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}\n\nstatic bool reg_type_may_be_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCKET_OR_NULL ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_TCP_SOCK_OR_NULL ||\n\t\ttype == PTR_TO_MEM ||\n\t\ttype == PTR_TO_MEM_OR_NULL;\n}\n\nstatic bool arg_type_may_be_refcounted(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_SOCK_COMMON;\n}\n\n/* Determine whether the function releases some resources allocated by another\n * function call. The first reference type argument will be assumed to be\n * released by release_reference().\n */\nstatic bool is_release_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_release ||\n\t       func_id == BPF_FUNC_ringbuf_submit ||\n\t       func_id == BPF_FUNC_ringbuf_discard;\n}\n\nstatic bool may_be_acquire_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_lookup_tcp ||\n\t\tfunc_id == BPF_FUNC_sk_lookup_udp ||\n\t\tfunc_id == BPF_FUNC_skc_lookup_tcp ||\n\t\tfunc_id == BPF_FUNC_map_lookup_elem ||\n\t        func_id == BPF_FUNC_ringbuf_reserve;\n}\n\nstatic bool is_acquire_function(enum bpf_func_id func_id,\n\t\t\t\tconst struct bpf_map *map)\n{\n\tenum bpf_map_type map_type = map ? map->map_type : BPF_MAP_TYPE_UNSPEC;\n\n\tif (func_id == BPF_FUNC_sk_lookup_tcp ||\n\t    func_id == BPF_FUNC_sk_lookup_udp ||\n\t    func_id == BPF_FUNC_skc_lookup_tcp ||\n\t    func_id == BPF_FUNC_ringbuf_reserve)\n\t\treturn true;\n\n\tif (func_id == BPF_FUNC_map_lookup_elem &&\n\t    (map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t     map_type == BPF_MAP_TYPE_SOCKHASH))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool is_ptr_cast_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_tcp_sock ||\n\t\tfunc_id == BPF_FUNC_sk_fullsock;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n\t[PTR_TO_FLOW_KEYS]\t= \"flow_keys\",\n\t[PTR_TO_SOCKET]\t\t= \"sock\",\n\t[PTR_TO_SOCKET_OR_NULL] = \"sock_or_null\",\n\t[PTR_TO_SOCK_COMMON]\t= \"sock_common\",\n\t[PTR_TO_SOCK_COMMON_OR_NULL] = \"sock_common_or_null\",\n\t[PTR_TO_TCP_SOCK]\t= \"tcp_sock\",\n\t[PTR_TO_TCP_SOCK_OR_NULL] = \"tcp_sock_or_null\",\n\t[PTR_TO_TP_BUFFER]\t= \"tp_buffer\",\n\t[PTR_TO_XDP_SOCK]\t= \"xdp_sock\",\n\t[PTR_TO_BTF_ID]\t\t= \"ptr_\",\n\t[PTR_TO_BTF_ID_OR_NULL]\t= \"ptr_or_null_\",\n\t[PTR_TO_MEM]\t\t= \"mem\",\n\t[PTR_TO_MEM_OR_NULL]\t= \"mem_or_null\",\n\t[PTR_TO_RDONLY_BUF]\t= \"rdonly_buf\",\n\t[PTR_TO_RDONLY_BUF_OR_NULL] = \"rdonly_buf_or_null\",\n\t[PTR_TO_RDWR_BUF]\t= \"rdwr_buf\",\n\t[PTR_TO_RDWR_BUF_OR_NULL] = \"rdwr_buf_or_null\",\n};\n\nstatic char slot_type_char[] = {\n\t[STACK_INVALID]\t= '?',\n\t[STACK_SPILL]\t= 'r',\n\t[STACK_MISC]\t= 'm',\n\t[STACK_ZERO]\t= '0',\n};\n\nstatic void print_liveness(struct bpf_verifier_env *env,\n\t\t\t   enum bpf_reg_liveness live)\n{\n\tif (live & (REG_LIVE_READ | REG_LIVE_WRITTEN | REG_LIVE_DONE))\n\t    verbose(env, \"_\");\n\tif (live & REG_LIVE_READ)\n\t\tverbose(env, \"r\");\n\tif (live & REG_LIVE_WRITTEN)\n\t\tverbose(env, \"w\");\n\tif (live & REG_LIVE_DONE)\n\t\tverbose(env, \"D\");\n}\n\nstatic struct bpf_func_state *func(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[reg->frameno];\n}\n\nconst char *kernel_type_name(u32 id)\n{\n\treturn btf_name_by_offset(btf_vmlinux,\n\t\t\t\t  btf_type_by_id(btf_vmlinux, id)->name_off);\n}\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID || t == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tverbose(env, \"\\n\");\n}\n\n#define COPY_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\t\\\nstatic int copy_##NAME##_state(struct bpf_func_state *dst,\t\t\\\n\t\t\t       const struct bpf_func_state *src)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (!src->FIELD)\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\tif (WARN_ON_ONCE(dst->COUNT < src->COUNT)) {\t\t\t\\\n\t\t/* internal bug, make state invalid to reject the program */ \\\n\t\tmemset(dst, 0, sizeof(*dst));\t\t\t\t\\\n\t\treturn -EFAULT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tmemcpy(dst->FIELD, src->FIELD,\t\t\t\t\t\\\n\t       sizeof(*src->FIELD) * (src->COUNT / SIZE));\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* copy_reference_state() */\nCOPY_STATE_FN(reference, acquired_refs, refs, 1)\n/* copy_stack_state() */\nCOPY_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef COPY_STATE_FN\n\n#define REALLOC_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\\\nstatic int realloc_##NAME##_state(struct bpf_func_state *state, int size, \\\n\t\t\t\t  bool copy_old)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu32 old_size = state->COUNT;\t\t\t\t\t\\\n\tstruct bpf_##NAME##_state *new_##FIELD;\t\t\t\t\\\n\tint slot = size / SIZE;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (size <= old_size || !size) {\t\t\t\t\\\n\t\tif (copy_old)\t\t\t\t\t\t\\\n\t\t\treturn 0;\t\t\t\t\t\\\n\t\tstate->COUNT = slot * SIZE;\t\t\t\t\\\n\t\tif (!size && old_size) {\t\t\t\t\\\n\t\t\tkfree(state->FIELD);\t\t\t\t\\\n\t\t\tstate->FIELD = NULL;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tnew_##FIELD = kmalloc_array(slot, sizeof(struct bpf_##NAME##_state), \\\n\t\t\t\t    GFP_KERNEL);\t\t\t\\\n\tif (!new_##FIELD)\t\t\t\t\t\t\\\n\t\treturn -ENOMEM;\t\t\t\t\t\t\\\n\tif (copy_old) {\t\t\t\t\t\t\t\\\n\t\tif (state->FIELD)\t\t\t\t\t\\\n\t\t\tmemcpy(new_##FIELD, state->FIELD,\t\t\\\n\t\t\t       sizeof(*new_##FIELD) * (old_size / SIZE)); \\\n\t\tmemset(new_##FIELD + old_size / SIZE, 0,\t\t\\\n\t\t       sizeof(*new_##FIELD) * (size - old_size) / SIZE); \\\n\t}\t\t\t\t\t\t\t\t\\\n\tstate->COUNT = slot * SIZE;\t\t\t\t\t\\\n\tkfree(state->FIELD);\t\t\t\t\t\t\\\n\tstate->FIELD = new_##FIELD;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* realloc_reference_state() */\nREALLOC_STATE_FN(reference, acquired_refs, refs, 1)\n/* realloc_stack_state() */\nREALLOC_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef REALLOC_STATE_FN\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_func_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which realloc_stack_state() copies over. It points to previous\n * bpf_verifier_state which is never reallocated.\n */\nstatic int realloc_func_state(struct bpf_func_state *state, int stack_size,\n\t\t\t      int refs_size, bool copy_old)\n{\n\tint err = realloc_reference_state(state, refs_size, copy_old);\n\tif (err)\n\t\treturn err;\n\treturn realloc_stack_state(state, stack_size, copy_old);\n}\n\n/* Acquire a pointer id from the env and update the state->refs to include\n * this new pointer reference.\n * On success, returns a valid pointer id to associate with the register\n * On failure, returns a negative errno.\n */\nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint new_ofs = state->acquired_refs;\n\tint id, err;\n\n\terr = realloc_reference_state(state, state->acquired_refs + 1, true);\n\tif (err)\n\t\treturn err;\n\tid = ++env->id_gen;\n\tstate->refs[new_ofs].id = id;\n\tstate->refs[new_ofs].insn_idx = insn_idx;\n\n\treturn id;\n}\n\n/* release function corresponding to acquire_reference_state(). Idempotent. */\nstatic int release_reference_state(struct bpf_func_state *state, int ptr_id)\n{\n\tint i, last_idx;\n\n\tlast_idx = state->acquired_refs - 1;\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id == ptr_id) {\n\t\t\tif (last_idx && i != last_idx)\n\t\t\t\tmemcpy(&state->refs[i], &state->refs[last_idx],\n\t\t\t\t       sizeof(*state->refs));\n\t\t\tmemset(&state->refs[last_idx], 0, sizeof(*state->refs));\n\t\t\tstate->acquired_refs--;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic int transfer_reference_state(struct bpf_func_state *dst,\n\t\t\t\t    struct bpf_func_state *src)\n{\n\tint err = realloc_reference_state(dst, src->acquired_refs, false);\n\tif (err)\n\t\treturn err;\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic void free_func_state(struct bpf_func_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tkfree(state->refs);\n\tkfree(state->stack);\n\tkfree(state);\n}\n\nstatic void clear_jmp_history(struct bpf_verifier_state *state)\n{\n\tkfree(state->jmp_history);\n\tstate->jmp_history = NULL;\n\tstate->jmp_history_cnt = 0;\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tint i;\n\n\tfor (i = 0; i <= state->curframe; i++) {\n\t\tfree_func_state(state->frame[i]);\n\t\tstate->frame[i] = NULL;\n\t}\n\tclear_jmp_history(state);\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_func_state(struct bpf_func_state *dst,\n\t\t\t   const struct bpf_func_state *src)\n{\n\tint err;\n\n\terr = realloc_func_state(dst, src->allocated_stack, src->acquired_refs,\n\t\t\t\t false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tu32 jmp_sz = sizeof(struct bpf_idx_pair) * src->jmp_history_cnt;\n\tint i, err;\n\n\tif (dst_state->jmp_history_cnt < src->jmp_history_cnt) {\n\t\tkfree(dst_state->jmp_history);\n\t\tdst_state->jmp_history = kmalloc(jmp_sz, GFP_USER);\n\t\tif (!dst_state->jmp_history)\n\t\t\treturn -ENOMEM;\n\t}\n\tmemcpy(dst_state->jmp_history, src->jmp_history, jmp_sz);\n\tdst_state->jmp_history_cnt = src->jmp_history_cnt;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->curframe = src->curframe;\n\tdst_state->active_spin_lock = src->active_spin_lock;\n\tdst_state->branches = src->branches;\n\tdst_state->parent = src->parent;\n\tdst_state->first_insn_idx = src->first_insn_idx;\n\tdst_state->last_insn_idx = src->last_insn_idx;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic void update_branch_counts(struct bpf_verifier_env *env, struct bpf_verifier_state *st)\n{\n\twhile (st) {\n\t\tu32 br = --st->branches;\n\n\t\t/* WARN_ON(br > 1) technically makes sense here,\n\t\t * but see comment in push_stack(), hence:\n\t\t */\n\t\tWARN_ONCE((int)br < 0,\n\t\t\t  \"BUG update_branch_counts:branches_to_explore=%d\\n\",\n\t\t\t  br);\n\t\tif (br)\n\t\t\tbreak;\n\t\tst = st->parent;\n\t}\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx, bool pop_log)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (pop_log)\n\t\tbpf_vlog_reset(&env->log, head->log_pos);\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\telem->log_pos = env->log.len_used;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_JMP_SEQ) {\n\t\tverbose(env, \"The sequence of %d jumps is too complex.\\n\",\n\t\t\tenv->stack_size);\n\t\tgoto err;\n\t}\n\tif (elem->st.parent) {\n\t\t++elem->st.parent->branches;\n\t\t/* WARN_ON(branches > 2) technically makes sense here,\n\t\t * but\n\t\t * 1. speculative states will bump 'branches' for non-branch\n\t\t * instructions\n\t\t * 2. is_state_visited() heuristics may decide not to create\n\t\t * a new state for a sequence of branches and all such current\n\t\t * and cloned states will be pointing to a single parent state\n\t\t * which might have large 'branches' count.\n\t\t */\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL, false));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\t/* Clear id, off, and union(map_ptr, range) */\n\tmemset(((u8 *)reg) + sizeof(reg->type), 0,\n\t       offsetof(struct bpf_reg_state, var_off) - sizeof(reg->type));\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\nstatic void __mark_reg32_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->var_off = tnum_const_subreg(reg->var_off, imm);\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void __mark_reg_const_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n\treg->type = SCALAR_VALUE;\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __mark_reg64_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\nstatic void __mark_reg32_unbounded(struct bpf_reg_state *reg)\n{\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __update_reg32_bounds(struct bpf_reg_state *reg)\n{\n\tstruct tnum var32_off = tnum_subreg(reg->var_off);\n\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->s32_min_value = max_t(s32, reg->s32_min_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->s32_max_value = min_t(s32, reg->s32_max_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MAX));\n\treg->u32_min_value = max_t(u32, reg->u32_min_value, (u32)var32_off.value);\n\treg->u32_max_value = min(reg->u32_max_value,\n\t\t\t\t (u32)(var32_off.value | var32_off.mask));\n}\n\nstatic void __update_reg64_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t__update_reg32_bounds(reg);\n\t__update_reg64_bounds(reg);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg32_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->s32_min_value >= 0 || reg->s32_max_value < 0) {\n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s32)reg->u32_max_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->s32_min_value = reg->u32_min_value;\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t} else if ((s32)reg->u32_min_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value;\n\t}\n}\n\nstatic void __reg64_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t__reg32_deduce_bounds(reg);\n\t__reg64_deduce_bounds(reg);\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\tstruct tnum var64_off = tnum_intersect(reg->var_off,\n\t\t\t\t\t       tnum_range(reg->umin_value,\n\t\t\t\t\t\t\t  reg->umax_value));\n\tstruct tnum var32_off = tnum_intersect(tnum_subreg(reg->var_off),\n\t\t\t\t\t\ttnum_range(reg->u32_min_value,\n\t\t\t\t\t\t\t   reg->u32_max_value));\n\n\treg->var_off = tnum_or(tnum_clear_subreg(var64_off), var32_off);\n}\n\nstatic void __reg_assign_32_into_64(struct bpf_reg_state *reg)\n{\n\treg->umin_value = reg->u32_min_value;\n\treg->umax_value = reg->u32_max_value;\n\t/* Attempt to pull 32-bit signed bounds into 64-bit bounds\n\t * but must be positive otherwise set to worse case bounds\n\t * and refine later from tnum.\n\t */\n\tif (reg->s32_min_value >= 0 && reg->s32_max_value >= 0)\n\t\treg->smax_value = reg->s32_max_value;\n\telse\n\t\treg->smax_value = U32_MAX;\n\tif (reg->s32_min_value >= 0)\n\t\treg->smin_value = reg->s32_min_value;\n\telse\n\t\treg->smin_value = 0;\n}\n\nstatic void __reg_combine_32_into_64(struct bpf_reg_state *reg)\n{\n\t/* special case when 64-bit register has upper 32-bit register\n\t * zeroed. Typically happens after zext or <<32, >>32 sequence\n\t * allowing us to use 32-bit bounds directly,\n\t */\n\tif (tnum_equals_const(tnum_clear_subreg(reg->var_off), 0)) {\n\t\t__reg_assign_32_into_64(reg);\n\t} else {\n\t\t/* Otherwise the best we can do is push lower 32bit known and\n\t\t * unknown bits into register (var_off set from jmp logic)\n\t\t * then learn as much as possible from the 64-bit tnum\n\t\t * known and unknown bits. The previous smin/smax bounds are\n\t\t * invalid here because of jmp32 compare so mark them unknown\n\t\t * so they do not impact tnum bounds calculation.\n\t\t */\n\t\t__mark_reg64_unbounded(reg);\n\t\t__update_reg_bounds(reg);\n\t}\n\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__reg_deduce_bounds(reg);\n\t__reg_bound_offset(reg);\n\t__update_reg_bounds(reg);\n}\n\nstatic bool __reg64_bound_s32(s64 a)\n{\n\tif (a > S32_MIN && a < S32_MAX)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool __reg64_bound_u32(u64 a)\n{\n\tif (a > U32_MIN && a < U32_MAX)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void __reg_combine_64_into_32(struct bpf_reg_state *reg)\n{\n\t__mark_reg32_unbounded(reg);\n\n\tif (__reg64_bound_s32(reg->smin_value))\n\t\treg->s32_min_value = (s32)reg->smin_value;\n\tif (__reg64_bound_s32(reg->smax_value))\n\t\treg->s32_max_value = (s32)reg->smax_value;\n\tif (__reg64_bound_u32(reg->umin_value))\n\t\treg->u32_min_value = (u32)reg->umin_value;\n\tif (__reg64_bound_u32(reg->umax_value))\n\t\treg->u32_max_value = (u32)reg->umax_value;\n\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__reg_deduce_bounds(reg);\n\t__reg_bound_offset(reg);\n\t__update_reg_bounds(reg);\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(const struct bpf_verifier_env *env,\n\t\t\t       struct bpf_reg_state *reg)\n{\n\t/*\n\t * Clear type, id, off, and union(map_ptr, range) and\n\t * padding between 'type' and union\n\t */\n\tmemset(reg, 0, offsetof(struct bpf_reg_state, var_off));\n\treg->type = SCALAR_VALUE;\n\treg->var_off = tnum_unknown;\n\treg->frameno = 0;\n\treg->precise = env->subprog_cnt > 1 || !env->bpf_capable;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(env, regs + regno);\n}\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(env, reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(env, regs + regno);\n}\n\nstatic void mark_btf_ld_reg(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_reg_state *regs, u32 regno,\n\t\t\t    enum bpf_reg_type reg_type, u32 btf_id)\n{\n\tif (reg_type == SCALAR_VALUE) {\n\t\tmark_reg_unknown(env, regs, regno);\n\t\treturn;\n\t}\n\tmark_reg_known_zero(env, regs, regno);\n\tregs[regno].type = PTR_TO_BTF_ID;\n\tregs[regno].btf_id = btf_id;\n}\n\n#define DEF_NOT_SUBREG\t(0)\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t\tregs[i].parent = NULL;\n\t\tregs[i].subreg_def = DEF_NOT_SUBREG;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\tregs[BPF_REG_FP].frameno = state->frameno;\n}\n\n#define BPF_MAIN_FUNC (-1)\nstatic void init_func_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *state,\n\t\t\t    int callsite, int frameno, int subprogno)\n{\n\tstate->callsite = callsite;\n\tstate->frameno = frameno;\n\tstate->subprogno = subprogno;\n\tinit_reg_state(env, state);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic int cmp_subprogs(const void *a, const void *b)\n{\n\treturn ((struct bpf_subprog_info *)a)->start -\n\t       ((struct bpf_subprog_info *)b)->start;\n}\n\nstatic int find_subprog(struct bpf_verifier_env *env, int off)\n{\n\tstruct bpf_subprog_info *p;\n\n\tp = bsearch(&off, env->subprog_info, env->subprog_cnt,\n\t\t    sizeof(env->subprog_info[0]), cmp_subprogs);\n\tif (!p)\n\t\treturn -ENOENT;\n\treturn p - env->subprog_info;\n\n}\n\nstatic int add_subprog(struct bpf_verifier_env *env, int off)\n{\n\tint insn_cnt = env->prog->len;\n\tint ret;\n\n\tif (off >= insn_cnt || off < 0) {\n\t\tverbose(env, \"call to invalid destination\\n\");\n\t\treturn -EINVAL;\n\t}\n\tret = find_subprog(env, off);\n\tif (ret >= 0)\n\t\treturn 0;\n\tif (env->subprog_cnt >= BPF_MAX_SUBPROGS) {\n\t\tverbose(env, \"too many subprograms\\n\");\n\t\treturn -E2BIG;\n\t}\n\tenv->subprog_info[env->subprog_cnt++].start = off;\n\tsort(env->subprog_info, env->subprog_cnt,\n\t     sizeof(env->subprog_info[0]), cmp_subprogs, NULL);\n\treturn 0;\n}\n\nstatic int check_subprogs(struct bpf_verifier_env *env)\n{\n\tint i, ret, subprog_start, subprog_end, off, cur_subprog = 0;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\n\t/* Add entry function. */\n\tret = add_subprog(env, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* determine subprog starts. The end is one before the next starts */\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (!env->bpf_capable) {\n\t\t\tverbose(env,\n\t\t\t\t\"function calls to other bpf functions are allowed for CAP_BPF and CAP_SYS_ADMIN\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\t\tret = add_subprog(env, i + insn[i].imm + 1);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t/* Add a fake 'exit' subprog which could simplify subprog iteration\n\t * logic. 'subprog_cnt' should not be increased.\n\t */\n\tsubprog[env->subprog_cnt].start = insn_cnt;\n\n\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\t\tverbose(env, \"func#%d @%d\\n\", i, subprog[i].start);\n\n\t/* now check that all jumps are within the same subprog */\n\tsubprog_start = subprog[cur_subprog].start;\n\tsubprog_end = subprog[cur_subprog + 1].start;\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tu8 code = insn[i].code;\n\n\t\tif (BPF_CLASS(code) != BPF_JMP && BPF_CLASS(code) != BPF_JMP32)\n\t\t\tgoto next;\n\t\tif (BPF_OP(code) == BPF_EXIT || BPF_OP(code) == BPF_CALL)\n\t\t\tgoto next;\n\t\toff = i + insn[i].off + 1;\n\t\tif (off < subprog_start || off >= subprog_end) {\n\t\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", i, off);\n\t\t\treturn -EINVAL;\n\t\t}\nnext:\n\t\tif (i == subprog_end - 1) {\n\t\t\t/* to avoid fall-through from one subprog into another\n\t\t\t * the last insn of the subprog should be either exit\n\t\t\t * or unconditional jump back\n\t\t\t */\n\t\t\tif (code != (BPF_JMP | BPF_EXIT) &&\n\t\t\t    code != (BPF_JMP | BPF_JA)) {\n\t\t\t\tverbose(env, \"last insn is not an exit or jmp\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsubprog_start = subprog_end;\n\t\t\tcur_subprog++;\n\t\t\tif (cur_subprog < env->subprog_cnt)\n\t\t\t\tsubprog_end = subprog[cur_subprog + 1].start;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Parentage chain of this register (or stack slot) should take care of all\n * issues like callee-saved registers, stack slot allocation time, etc.\n */\nstatic int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent, u8 flag)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tint cnt = 0;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str[parent->type],\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* The first condition is more likely to be true than the\n\t\t * second, checked it first.\n\t\t */\n\t\tif ((parent->live & REG_LIVE_READ) == flag ||\n\t\t    parent->live & REG_LIVE_READ64)\n\t\t\t/* The parentage chain never changes and\n\t\t\t * this parent was already marked as LIVE_READ.\n\t\t\t * There is no need to keep walking the chain again and\n\t\t\t * keep re-marking all parents as LIVE_READ.\n\t\t\t * This case happens when the same register is read\n\t\t\t * multiple times without writes into it in-between.\n\t\t\t * Also, if parent has the stronger REG_LIVE_READ64 set,\n\t\t\t * then no need to set the weak REG_LIVE_READ32.\n\t\t\t */\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= flag;\n\t\t/* REG_LIVE_READ64 overrides REG_LIVE_READ32. */\n\t\tif (flag == REG_LIVE_READ64)\n\t\t\tparent->live &= ~REG_LIVE_READ32;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t\tcnt++;\n\t}\n\n\tif (env->longest_mark_read_walk < cnt)\n\t\tenv->longest_mark_read_walk = cnt;\n\treturn 0;\n}\n\n/* This function is supposed to be used by the following 32-bit optimization\n * code only. It returns TRUE if the source or destination register operates\n * on 64-bit, otherwise return FALSE.\n */\nstatic bool is_reg64(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t     u32 regno, struct bpf_reg_state *reg, enum reg_arg_type t)\n{\n\tu8 code, class, op;\n\n\tcode = insn->code;\n\tclass = BPF_CLASS(code);\n\top = BPF_OP(code);\n\tif (class == BPF_JMP) {\n\t\t/* BPF_EXIT for \"main\" will reach here. Return TRUE\n\t\t * conservatively.\n\t\t */\n\t\tif (op == BPF_EXIT)\n\t\t\treturn true;\n\t\tif (op == BPF_CALL) {\n\t\t\t/* BPF to BPF call will reach here because of marking\n\t\t\t * caller saved clobber with DST_OP_NO_MARK for which we\n\t\t\t * don't care the register def because they are anyway\n\t\t\t * marked as NOT_INIT already.\n\t\t\t */\n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\treturn false;\n\t\t\t/* Helper call will reach here because of arg type\n\t\t\t * check, conservatively return TRUE.\n\t\t\t */\n\t\t\tif (t == SRC_OP)\n\t\t\t\treturn true;\n\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (class == BPF_ALU64 || class == BPF_JMP ||\n\t    /* BPF_END always use BPF_ALU class. */\n\t    (class == BPF_ALU && op == BPF_END && insn->imm == 64))\n\t\treturn true;\n\n\tif (class == BPF_ALU || class == BPF_JMP32)\n\t\treturn false;\n\n\tif (class == BPF_LDX) {\n\t\tif (t != SRC_OP)\n\t\t\treturn BPF_SIZE(code) == BPF_DW;\n\t\t/* LDX source must be ptr. */\n\t\treturn true;\n\t}\n\n\tif (class == BPF_STX) {\n\t\tif (reg->type != SCALAR_VALUE)\n\t\t\treturn true;\n\t\treturn BPF_SIZE(code) == BPF_DW;\n\t}\n\n\tif (class == BPF_LD) {\n\t\tu8 mode = BPF_MODE(code);\n\n\t\t/* LD_IMM64 */\n\t\tif (mode == BPF_IMM)\n\t\t\treturn true;\n\n\t\t/* Both LD_IND and LD_ABS return 32-bit data. */\n\t\tif (t != SRC_OP)\n\t\t\treturn  false;\n\n\t\t/* Implicit ctx ptr. */\n\t\tif (regno == BPF_REG_6)\n\t\t\treturn true;\n\n\t\t/* Explicit source could be any width. */\n\t\treturn true;\n\t}\n\n\tif (class == BPF_ST)\n\t\t/* The only source register for BPF_ST is a ptr. */\n\t\treturn true;\n\n\t/* Conservatively return true at default. */\n\treturn true;\n}\n\n/* Return TRUE if INSN doesn't have explicit value define. */\nstatic bool insn_no_def(struct bpf_insn *insn)\n{\n\tu8 class = BPF_CLASS(insn->code);\n\n\treturn (class == BPF_JMP || class == BPF_JMP32 ||\n\t\tclass == BPF_STX || class == BPF_ST);\n}\n\n/* Return TRUE if INSN has defined any 32-bit value explicitly. */\nstatic bool insn_has_def32(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tif (insn_no_def(insn))\n\t\treturn false;\n\n\treturn !is_reg64(env, insn, insn->dst_reg, NULL, DST_OP);\n}\n\nstatic void mark_insn_zext(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_reg_state *reg)\n{\n\ts32 def_idx = reg->subreg_def;\n\n\tif (def_idx == DEF_NOT_SUBREG)\n\t\treturn;\n\n\tenv->insn_aux_data[def_idx - 1].zext_dst = true;\n\t/* The dst will be zero extended, so won't be sub-register anymore. */\n\treg->subreg_def = DEF_NOT_SUBREG;\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_insn *insn = env->prog->insnsi + env->insn_idx;\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tbool rw64;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\treg = &regs[regno];\n\trw64 = is_reg64(env, insn, regno, reg, t);\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (reg->type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\tif (regno == BPF_REG_FP)\n\t\t\treturn 0;\n\n\t\tif (rw64)\n\t\t\tmark_insn_zext(env, reg);\n\n\t\treturn mark_reg_read(env, reg, reg->parent,\n\t\t\t\t     rw64 ? REG_LIVE_READ64 : REG_LIVE_READ32);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\treg->live |= REG_LIVE_WRITTEN;\n\t\treg->subreg_def = rw64 ? DEF_NOT_SUBREG : env->insn_idx + 1;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\n/* for any branch, call, exit record the history of jmps in the given state */\nstatic int push_jmp_history(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_verifier_state *cur)\n{\n\tu32 cnt = cur->jmp_history_cnt;\n\tstruct bpf_idx_pair *p;\n\n\tcnt++;\n\tp = krealloc(cur->jmp_history, cnt * sizeof(*p), GFP_USER);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tp[cnt - 1].idx = env->insn_idx;\n\tp[cnt - 1].prev_idx = env->prev_insn_idx;\n\tcur->jmp_history = p;\n\tcur->jmp_history_cnt = cnt;\n\treturn 0;\n}\n\n/* Backtrack one insn at a time. If idx is not at the top of recorded\n * history then previous instruction came from straight line execution.\n */\nstatic int get_prev_insn_idx(struct bpf_verifier_state *st, int i,\n\t\t\t     u32 *history)\n{\n\tu32 cnt = *history;\n\n\tif (cnt && st->jmp_history[cnt - 1].idx == i) {\n\t\ti = st->jmp_history[cnt - 1].prev_idx;\n\t\t(*history)--;\n\t} else {\n\t\ti--;\n\t}\n\treturn i;\n}\n\n/* For given verifier state backtrack_insn() is called from the last insn to\n * the first insn. Its purpose is to compute a bitmask of registers and\n * stack slots that needs precision in the parent verifier state.\n */\nstatic int backtrack_insn(struct bpf_verifier_env *env, int idx,\n\t\t\t  u32 *reg_mask, u64 *stack_mask)\n{\n\tconst struct bpf_insn_cbs cbs = {\n\t\t.cb_print\t= verbose,\n\t\t.private_data\t= env,\n\t};\n\tstruct bpf_insn *insn = env->prog->insnsi + idx;\n\tu8 class = BPF_CLASS(insn->code);\n\tu8 opcode = BPF_OP(insn->code);\n\tu8 mode = BPF_MODE(insn->code);\n\tu32 dreg = 1u << insn->dst_reg;\n\tu32 sreg = 1u << insn->src_reg;\n\tu32 spi;\n\n\tif (insn->code == 0)\n\t\treturn 0;\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"regs=%x stack=%llx before \", *reg_mask, *stack_mask);\n\t\tverbose(env, \"%d: \", idx);\n\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t}\n\n\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\tif (opcode == BPF_MOV) {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t/* dreg = sreg\n\t\t\t\t * dreg needs precision after this insn\n\t\t\t\t * sreg needs precision before this insn\n\t\t\t\t */\n\t\t\t\t*reg_mask &= ~dreg;\n\t\t\t\t*reg_mask |= sreg;\n\t\t\t} else {\n\t\t\t\t/* dreg = K\n\t\t\t\t * dreg needs precision after this insn.\n\t\t\t\t * Corresponding register is already marked\n\t\t\t\t * as precise=true in this verifier state.\n\t\t\t\t * No further markings in parent are necessary\n\t\t\t\t */\n\t\t\t\t*reg_mask &= ~dreg;\n\t\t\t}\n\t\t} else {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t/* dreg += sreg\n\t\t\t\t * both dreg and sreg need precision\n\t\t\t\t * before this insn\n\t\t\t\t */\n\t\t\t\t*reg_mask |= sreg;\n\t\t\t} /* else dreg += K\n\t\t\t   * dreg still needs precision before this insn\n\t\t\t   */\n\t\t}\n\t} else if (class == BPF_LDX) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\t*reg_mask &= ~dreg;\n\n\t\t/* scalars can only be spilled into stack w/o losing precision.\n\t\t * Load from any other memory can be zero extended.\n\t\t * The desire to keep that precision is already indicated\n\t\t * by 'precise' mark in corresponding register of this state.\n\t\t * No further tracking necessary.\n\t\t */\n\t\tif (insn->src_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\t\tif (BPF_SIZE(insn->code) != BPF_DW)\n\t\t\treturn 0;\n\n\t\t/* dreg = *(u64 *)[fp - off] was a fill from the stack.\n\t\t * that [fp - off] slot contains scalar that needs to be\n\t\t * tracked with precision\n\t\t */\n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t*stack_mask |= 1ull << spi;\n\t} else if (class == BPF_STX || class == BPF_ST) {\n\t\tif (*reg_mask & dreg)\n\t\t\t/* stx & st shouldn't be using _scalar_ dst_reg\n\t\t\t * to access memory. It means backtracking\n\t\t\t * encountered a case of pointer subtraction.\n\t\t\t */\n\t\t\treturn -ENOTSUPP;\n\t\t/* scalars can only be spilled into stack */\n\t\tif (insn->dst_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\t\tif (BPF_SIZE(insn->code) != BPF_DW)\n\t\t\treturn 0;\n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (!(*stack_mask & (1ull << spi)))\n\t\t\treturn 0;\n\t\t*stack_mask &= ~(1ull << spi);\n\t\tif (class == BPF_STX)\n\t\t\t*reg_mask |= sreg;\n\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\tif (opcode == BPF_CALL) {\n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\treturn -ENOTSUPP;\n\t\t\t/* regular helper call sets R0 */\n\t\t\t*reg_mask &= ~1;\n\t\t\tif (*reg_mask & 0x3f) {\n\t\t\t\t/* if backtracing was looking for registers R1-R5\n\t\t\t\t * they should have been found already.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"BUG regs %x\\n\", *reg_mask);\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else if (opcode == BPF_EXIT) {\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t} else if (class == BPF_LD) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\t*reg_mask &= ~dreg;\n\t\t/* It's ld_imm64 or ld_abs or ld_ind.\n\t\t * For ld_imm64 no further tracking of precision\n\t\t * into parent is necessary\n\t\t */\n\t\tif (mode == BPF_IND || mode == BPF_ABS)\n\t\t\t/* to be analyzed */\n\t\t\treturn -ENOTSUPP;\n\t}\n\treturn 0;\n}\n\n/* the scalar precision tracking algorithm:\n * . at the start all registers have precise=false.\n * . scalar ranges are tracked as normal through alu and jmp insns.\n * . once precise value of the scalar register is used in:\n *   .  ptr + scalar alu\n *   . if (scalar cond K|scalar)\n *   .  helper_call(.., scalar, ...) where ARG_CONST is expected\n *   backtrack through the verifier states and mark all registers and\n *   stack slots with spilled constants that these scalar regisers\n *   should be precise.\n * . during state pruning two registers (or spilled stack slots)\n *   are equivalent if both are not precise.\n *\n * Note the verifier cannot simply walk register parentage chain,\n * since many different registers and stack slots could have been\n * used to compute single precise scalar.\n *\n * The approach of starting with precise=true for all registers and then\n * backtrack to mark a register as not precise when the verifier detects\n * that program doesn't care about specific value (e.g., when helper\n * takes register as ARG_ANYTHING parameter) is not safe.\n *\n * It's ok to walk single parentage chain of the verifier states.\n * It's possible that this backtracking will go all the way till 1st insn.\n * All other branches will be explored for needing precision later.\n *\n * The backtracking needs to deal with cases like:\n *   R8=map_value(id=0,off=0,ks=4,vs=1952,imm=0) R9_w=map_value(id=0,off=40,ks=4,vs=1952,imm=0)\n * r9 -= r8\n * r5 = r9\n * if r5 > 0x79f goto pc+7\n *    R5_w=inv(id=0,umax_value=1951,var_off=(0x0; 0x7ff))\n * r5 += 1\n * ...\n * call bpf_perf_event_output#25\n *   where .arg5_type = ARG_CONST_SIZE_OR_ZERO\n *\n * and this case:\n * r6 = 1\n * call foo // uses callee's r6 inside to compute r0\n * r0 += r6\n * if r0 == 0 goto\n *\n * to track above reg_mask/stack_mask needs to be independent for each frame.\n *\n * Also if parent's curframe > frame where backtracking started,\n * the verifier need to mark registers in both frames, otherwise callees\n * may incorrectly prune callers. This is similar to\n * commit 7640ead93924 (\"bpf: verifier: make sure callees don't prune with caller differences\")\n *\n * For now backtracking falls back into conservative marking.\n */\nstatic void mark_all_scalars_precise(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_verifier_state *st)\n{\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tint i, j;\n\n\t/* big hammer: mark all scalars precise in this path.\n\t * pop_stack may still get !precise scalars.\n\t */\n\tfor (; st; st = st->parent)\n\t\tfor (i = 0; i <= st->curframe; i++) {\n\t\t\tfunc = st->frame[i];\n\t\t\tfor (j = 0; j < BPF_REG_FP; j++) {\n\t\t\t\treg = &func->regs[j];\n\t\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t}\n\t\t\tfor (j = 0; j < func->allocated_stack / BPF_REG_SIZE; j++) {\n\t\t\t\tif (func->stack[j].slot_type[0] != STACK_SPILL)\n\t\t\t\t\tcontinue;\n\t\t\t\treg = &func->stack[j].spilled_ptr;\n\t\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t}\n\t\t}\n}\n\nstatic int __mark_chain_precision(struct bpf_verifier_env *env, int regno,\n\t\t\t\t  int spi)\n{\n\tstruct bpf_verifier_state *st = env->cur_state;\n\tint first_idx = st->first_insn_idx;\n\tint last_idx = env->insn_idx;\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tu32 reg_mask = regno >= 0 ? 1u << regno : 0;\n\tu64 stack_mask = spi >= 0 ? 1ull << spi : 0;\n\tbool skip_first = true;\n\tbool new_marks = false;\n\tint i, err;\n\n\tif (!env->bpf_capable)\n\t\treturn 0;\n\n\tfunc = st->frame[st->curframe];\n\tif (regno >= 0) {\n\t\treg = &func->regs[regno];\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tWARN_ONCE(1, \"backtracing misuse\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (!reg->precise)\n\t\t\tnew_marks = true;\n\t\telse\n\t\t\treg_mask = 0;\n\t\treg->precise = true;\n\t}\n\n\twhile (spi >= 0) {\n\t\tif (func->stack[spi].slot_type[0] != STACK_SPILL) {\n\t\t\tstack_mask = 0;\n\t\t\tbreak;\n\t\t}\n\t\treg = &func->stack[spi].spilled_ptr;\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tstack_mask = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (!reg->precise)\n\t\t\tnew_marks = true;\n\t\telse\n\t\t\tstack_mask = 0;\n\t\treg->precise = true;\n\t\tbreak;\n\t}\n\n\tif (!new_marks)\n\t\treturn 0;\n\tif (!reg_mask && !stack_mask)\n\t\treturn 0;\n\tfor (;;) {\n\t\tDECLARE_BITMAP(mask, 64);\n\t\tu32 history = st->jmp_history_cnt;\n\n\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\tverbose(env, \"last_idx %d first_idx %d\\n\", last_idx, first_idx);\n\t\tfor (i = last_idx;;) {\n\t\t\tif (skip_first) {\n\t\t\t\terr = 0;\n\t\t\t\tskip_first = false;\n\t\t\t} else {\n\t\t\t\terr = backtrack_insn(env, i, &reg_mask, &stack_mask);\n\t\t\t}\n\t\t\tif (err == -ENOTSUPP) {\n\t\t\t\tmark_all_scalars_precise(env, st);\n\t\t\t\treturn 0;\n\t\t\t} else if (err) {\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\tif (!reg_mask && !stack_mask)\n\t\t\t\t/* Found assignment(s) into tracked register in this state.\n\t\t\t\t * Since this state is already marked, just return.\n\t\t\t\t * Nothing to be tracked further in the parent state.\n\t\t\t\t */\n\t\t\t\treturn 0;\n\t\t\tif (i == first_idx)\n\t\t\t\tbreak;\n\t\t\ti = get_prev_insn_idx(st, i, &history);\n\t\t\tif (i >= env->prog->len) {\n\t\t\t\t/* This can happen if backtracking reached insn 0\n\t\t\t\t * and there are still reg_mask or stack_mask\n\t\t\t\t * to backtrack.\n\t\t\t\t * It means the backtracking missed the spot where\n\t\t\t\t * particular register was initialized with a constant.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"BUG backtracking idx %d\\n\", i);\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\t\tst = st->parent;\n\t\tif (!st)\n\t\t\tbreak;\n\n\t\tnew_marks = false;\n\t\tfunc = st->frame[st->curframe];\n\t\tbitmap_from_u64(mask, reg_mask);\n\t\tfor_each_set_bit(i, mask, 32) {\n\t\t\treg = &func->regs[i];\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\treg_mask &= ~(1u << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!reg->precise)\n\t\t\t\tnew_marks = true;\n\t\t\treg->precise = true;\n\t\t}\n\n\t\tbitmap_from_u64(mask, stack_mask);\n\t\tfor_each_set_bit(i, mask, 64) {\n\t\t\tif (i >= func->allocated_stack / BPF_REG_SIZE) {\n\t\t\t\t/* the sequence of instructions:\n\t\t\t\t * 2: (bf) r3 = r10\n\t\t\t\t * 3: (7b) *(u64 *)(r3 -8) = r0\n\t\t\t\t * 4: (79) r4 = *(u64 *)(r10 -8)\n\t\t\t\t * doesn't contain jmps. It's backtracked\n\t\t\t\t * as a single block.\n\t\t\t\t * During backtracking insn 3 is not recognized as\n\t\t\t\t * stack access, so at the end of backtracking\n\t\t\t\t * stack slot fp-8 is still marked in stack_mask.\n\t\t\t\t * However the parent state may not have accessed\n\t\t\t\t * fp-8 and it's \"unallocated\" stack space.\n\t\t\t\t * In such case fallback to conservative.\n\t\t\t\t */\n\t\t\t\tmark_all_scalars_precise(env, st);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tif (func->stack[i].slot_type[0] != STACK_SPILL) {\n\t\t\t\tstack_mask &= ~(1ull << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treg = &func->stack[i].spilled_ptr;\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tstack_mask &= ~(1ull << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!reg->precise)\n\t\t\t\tnew_marks = true;\n\t\t\treg->precise = true;\n\t\t}\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tprint_verifier_state(env, func);\n\t\t\tverbose(env, \"parent %s regs=%x stack=%llx marks\\n\",\n\t\t\t\tnew_marks ? \"didn't have\" : \"already had\",\n\t\t\t\treg_mask, stack_mask);\n\t\t}\n\n\t\tif (!reg_mask && !stack_mask)\n\t\t\tbreak;\n\t\tif (!new_marks)\n\t\t\tbreak;\n\n\t\tlast_idx = st->last_insn_idx;\n\t\tfirst_idx = st->first_insn_idx;\n\t}\n\treturn 0;\n}\n\nstatic int mark_chain_precision(struct bpf_verifier_env *env, int regno)\n{\n\treturn __mark_chain_precision(env, regno, -1);\n}\n\nstatic int mark_chain_precision_stack(struct bpf_verifier_env *env, int spi)\n{\n\treturn __mark_chain_precision(env, -1, spi);\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\tcase PTR_TO_RDONLY_BUF:\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\tcase PTR_TO_RDWR_BUF:\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_equals_const(reg->var_off, 0);\n}\n\nstatic bool register_is_const(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_is_const(reg->var_off);\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic void save_register_state(struct bpf_func_state *state,\n\t\t\t\tint spi, struct bpf_reg_state *reg)\n{\n\tint i;\n\n\tstate->stack[spi].spilled_ptr = *reg;\n\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *state, /* func where register points to */\n\t\t\t     int off, int size, int value_regno, int insn_idx)\n{\n\tstruct bpf_func_state *cur; /* state of the current function */\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\tu32 dst_reg = env->prog->insnsi[insn_idx].dst_reg;\n\tstruct bpf_reg_state *reg = NULL;\n\n\terr = realloc_func_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t state->acquired_refs, true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tif (value_regno >= 0)\n\t\treg = &cur->regs[value_regno];\n\n\tif (reg && size == BPF_REG_SIZE && register_is_const(reg) &&\n\t    !register_is_null(reg) && env->bpf_capable) {\n\t\tif (dst_reg != BPF_REG_FP) {\n\t\t\t/* The backtracking logic can only recognize explicit\n\t\t\t * stack slot address like [fp - 8]. Other spill of\n\t\t\t * scalar via different register has to be conervative.\n\t\t\t * Backtrack from here and mark all registers as precise\n\t\t\t * that contributed into 'reg' being a constant.\n\t\t\t */\n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tsave_register_state(state, spi, reg);\n\t} else if (reg && is_spillable_regtype(reg->type)) {\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (state != cur && reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"cannot spill pointers to stack into stack frame of the caller\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!env->bypass_spec_v4) {\n\t\t\tbool sanitize = false;\n\n\t\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t\t    register_is_const(&state->stack[spi].spilled_ptr))\n\t\t\t\tsanitize = true;\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tif (state->stack[spi].slot_type[i] == STACK_MISC) {\n\t\t\t\t\tsanitize = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tif (sanitize) {\n\t\t\t\tint *poff = &env->insn_aux_data[insn_idx].sanitize_stack_off;\n\t\t\t\tint soff = (-spi - 1) * BPF_REG_SIZE;\n\n\t\t\t\t/* detected reuse of integer stack slot with a pointer\n\t\t\t\t * which means either llvm is reusing stack slot or\n\t\t\t\t * an attacker is trying to exploit CVE-2018-3639\n\t\t\t\t * (speculative store bypass)\n\t\t\t\t * Have to sanitize that slot with preemptive\n\t\t\t\t * store of zero.\n\t\t\t\t */\n\t\t\t\tif (*poff && *poff != soff) {\n\t\t\t\t\t/* disallow programs where single insn stores\n\t\t\t\t\t * into two different stack slots, since verifier\n\t\t\t\t\t * cannot sanitize them\n\t\t\t\t\t */\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"insn %d cannot access two stack slots fp%d and fp%d\",\n\t\t\t\t\t\tinsn_idx, *poff, soff);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\t*poff = soff;\n\t\t\t}\n\t\t}\n\t\tsave_register_state(state, spi, reg);\n\t} else {\n\t\tu8 type = STACK_MISC;\n\n\t\t/* regular write of data into stack destroys any spilled ptr */\n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\t\t/* Mark slots as STACK_MISC if they belonged to spilled ptr. */\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL)\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tstate->stack[spi].slot_type[i] = STACK_MISC;\n\n\t\t/* only mark the slot as written if all 8 bytes were written\n\t\t * otherwise read propagation may incorrectly stop too soon\n\t\t * when stack slots are partially written.\n\t\t * This heuristic means that read propagation will be\n\t\t * conservative, since it will add reg_live_read marks\n\t\t * to stack slots all the way to first state when programs\n\t\t * writes+reads less than 8 bytes\n\t\t */\n\t\tif (size == BPF_REG_SIZE)\n\t\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\t/* when we zero initialize stack slots mark them as such */\n\t\tif (reg && register_is_null(reg)) {\n\t\t\t/* backtracking doesn't work for STACK_ZERO yet. */\n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\ttype = STACK_ZERO;\n\t\t}\n\n\t\t/* Mark slots affected by this stack write. */\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\ttype;\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *reg_state /* func where register points to */,\n\t\t\t    int off, int size, int value_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tstruct bpf_reg_state *reg;\n\tu8 *stype;\n\n\tif (reg_state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = reg_state->stack[spi].slot_type;\n\treg = &reg_state->stack[spi].spilled_ptr;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\t\tverbose(env, \"invalid size of register fill\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t\tif (value_regno >= 0) {\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t\t}\n\t\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\t\treturn 0;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = *reg;\n\t\t\t/* mark reg as written since spilled pointer state likely\n\t\t\t * has its liveness marks cleared by is_state_visited()\n\t\t\t * which resets stack/reg liveness for state transitions\n\t\t\t */\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t} else if (__is_pointer_value(env->allow_ptr_leaks, reg)) {\n\t\t\t/* If value_regno==-1, the caller is asking us whether\n\t\t\t * it is acceptable to use this value as a SCALAR_VALUE\n\t\t\t * (e.g. for XADD).\n\t\t\t * We must not allow unprivileged callers to do that\n\t\t\t * with spilled pointers.\n\t\t\t */\n\t\t\tverbose(env, \"leaking pointer from stack off %d\\n\",\n\t\t\t\toff);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t} else {\n\t\tint zeros = 0;\n\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_MISC)\n\t\t\t\tcontinue;\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_ZERO) {\n\t\t\t\tzeros++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\tif (value_regno >= 0) {\n\t\t\tif (zeros == size) {\n\t\t\t\t/* any size read into register is zero extended,\n\t\t\t\t * so the whole register == const_zero\n\t\t\t\t */\n\t\t\t\t__mark_reg_const_zero(&state->regs[value_regno]);\n\t\t\t\t/* backtracking doesn't support STACK_ZERO yet,\n\t\t\t\t * so mark it precise here, so that later\n\t\t\t\t * backtracking can stop here.\n\t\t\t\t * Backtracking may not need this if this register\n\t\t\t\t * doesn't participate in pointer adjustment.\n\t\t\t\t * Forward propagation of precise flag is not\n\t\t\t\t * necessary either. This mark is only to stop\n\t\t\t\t * backtracking. Any register that contributed\n\t\t\t\t * to const 0 was marked precise before spill.\n\t\t\t\t */\n\t\t\t\tstate->regs[value_regno].precise = true;\n\t\t\t} else {\n\t\t\t\t/* have read misc data from the stack */\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t}\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_access(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_reg_state *reg,\n\t\t\t      int off, int size)\n{\n\t/* Stack accesses must be at a fixed offset, so that we\n\t * can determine what type of data were returned. See\n\t * check_stack_read().\n\t */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\\n\",\n\t\t\ttn_buf, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_map_access_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, enum bpf_access_type type)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\tu32 cap = bpf_map_flags_to_cap(map);\n\n\tif (type == BPF_WRITE && !(cap & BPF_MAP_CAN_WRITE)) {\n\t\tverbose(env, \"write into map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (type == BPF_READ && !(cap & BPF_MAP_CAN_READ)) {\n\t\tverbose(env, \"read from map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into memory region (e.g., map value, ringbuf sample, etc) */\nstatic int __check_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t      int off, int size, u32 mem_size,\n\t\t\t      bool zero_size_allowed)\n{\n\tbool size_ok = size > 0 || (size == 0 && zero_size_allowed);\n\tstruct bpf_reg_state *reg;\n\n\tif (off >= 0 && size_ok && (u64)off + size <= mem_size)\n\t\treturn 0;\n\n\treg = &cur_regs(env)[regno];\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE:\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t\tbreak;\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, off, mem_size);\n\t\tbreak;\n\tcase PTR_TO_MEM:\n\tdefault:\n\t\tverbose(env, \"invalid access to memory, mem_size=%u off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t}\n\n\treturn -EACCES;\n}\n\n/* check read/write into a memory region with possible variable offset */\nstatic int check_mem_region_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t   int off, int size, u32 mem_size,\n\t\t\t\t   bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register pointing to memory region, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_verifier_state(env, state);\n\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0 &&\n\t    (reg->smin_value == S64_MIN ||\n\t     (off + reg->smin_value != (s64)(s32)(off + reg->smin_value)) ||\n\t      reg->smin_value + off < 0)) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any such access\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d max value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tstruct bpf_map *map = reg->map_ptr;\n\tint err;\n\n\terr = check_mem_region_access(env, regno, off, size, map->value_size,\n\t\t\t\t      zero_size_allowed);\n\tif (err)\n\t\treturn err;\n\n\tif (map_value_has_spin_lock(map)) {\n\t\tu32 lock = map->spin_lock_off;\n\n\t\t/* if any part of struct bpf_spin_lock can be touched by\n\t\t * load/store reject this program.\n\t\t * To check that [x1, x2) overlaps with [y1, y2)\n\t\t * it is sufficient to check x1 < y2 && y1 < x2.\n\t\t */\n\t\tif (reg->smin_value + off < lock + sizeof(struct bpf_spin_lock) &&\n\t\t     lock < reg->umax_value + off + size) {\n\t\t\tverbose(env, \"bpf_spin_lock cannot be accessed directly by load/store\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\t/* Program types only with direct read access go here! */\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\n\t/* Program types with direct read + write access go here! */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tif (t == BPF_WRITE)\n\t\t\tenv->seen_direct_write = true;\n\n\t\treturn true;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, off, size, reg->range,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\n\t/* __check_mem_access has made sure \"off + size - 1\" is within u16.\n\t * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff,\n\t * otherwise find_good_pkt_pointers would have refused to set range info\n\t * that __check_mem_access would have rejected this pkt access.\n\t * Therefore, \"off + reg->umax_value + size - 1\" won't overflow u32.\n\t */\n\tenv->prog->aux->max_pkt_offset =\n\t\tmax_t(u32, env->prog->aux->max_pkt_offset,\n\t\t      off + reg->umax_value + size - 1);\n\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t*btf_id = info.btf_id;\n\t\telse\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic int check_flow_keys_access(struct bpf_verifier_env *env, int off,\n\t\t\t\t  int size)\n{\n\tif (size < 0 || off < 0 ||\n\t    (u64)off + size > sizeof(struct bpf_flow_keys)) {\n\t\tverbose(env, \"invalid access to flow keys off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_sock_access(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t     u32 regno, int off, int size,\n\t\t\t     enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info = {};\n\tbool valid;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (reg->type) {\n\tcase PTR_TO_SOCK_COMMON:\n\t\tvalid = bpf_sock_common_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tvalid = bpf_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tvalid = bpf_tcp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tvalid = bpf_xdp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\n\tif (valid) {\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size =\n\t\t\tinfo.ctx_field_size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"R%d invalid %s access off=%d size=%d\\n\",\n\t\tregno, reg_type_str[reg->type], off, size);\n\n\treturn -EACCES;\n}\n\nstatic struct bpf_reg_state *reg_state(struct bpf_verifier_env *env, int regno)\n{\n\treturn cur_regs(env) + regno;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, reg_state(env, regno));\n}\n\nstatic bool is_ctx_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn reg->type == PTR_TO_CTX;\n}\n\nstatic bool is_sk_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_sk_pointer(reg->type);\n}\n\nstatic bool is_pkt_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool is_flow_key_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\t/* Separate to is_ctx_reg() since we still want to allow BPF_ST here. */\n\treturn reg->type == PTR_TO_FLOW_KEYS;\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int off,\n\t\t\t       int size, bool strict_alignment_once)\n{\n\tbool strict = env->strict_alignment || strict_alignment_once;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_FLOW_KEYS:\n\t\tpointer_desc = \"flow keys \";\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t/* The stack spill tracking logic in check_stack_write()\n\t\t * and check_stack_read() relies on stack accesses being\n\t\t * aligned.\n\t\t */\n\t\tstrict = true;\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tpointer_desc = \"sock \";\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON:\n\t\tpointer_desc = \"sock_common \";\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tpointer_desc = \"tcp_sock \";\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tpointer_desc = \"xdp_sock \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\nstatic int update_stack_depth(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_func_state *func,\n\t\t\t      int off)\n{\n\tu16 stack = env->subprog_info[func->subprogno].stack_depth;\n\n\tif (stack >= -off)\n\t\treturn 0;\n\n\t/* update known max for given subprogram */\n\tenv->subprog_info[func->subprogno].stack_depth = -off;\n\treturn 0;\n}\n\n/* starting from main bpf function walk all instructions of the function\n * and recursively walk all callees that given function can call.\n * Ignore jump and exit insns.\n * Since recursion is prevented by check_cfg() this algorithm\n * only needs a local stack of MAX_CALL_FRAMES to remember callsites\n */\nstatic int check_max_stack_depth(struct bpf_verifier_env *env)\n{\n\tint depth = 0, frame = 0, idx = 0, i = 0, subprog_end;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint ret_insn[MAX_CALL_FRAMES];\n\tint ret_prog[MAX_CALL_FRAMES];\n\nprocess_func:\n\t/* round up to 32-bytes, since this is granularity\n\t * of interpreter stack size\n\t */\n\tdepth += round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tif (depth > MAX_BPF_STACK) {\n\t\tverbose(env, \"combined stack size of %d calls is %d. Too large\\n\",\n\t\t\tframe + 1, depth);\n\t\treturn -EACCES;\n\t}\ncontinue_func:\n\tsubprog_end = subprog[idx + 1].start;\n\tfor (; i < subprog_end; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* remember insn and function to return to */\n\t\tret_insn[frame] = i + 1;\n\t\tret_prog[frame] = idx;\n\n\t\t/* find the callee */\n\t\ti = i + insn[i].imm + 1;\n\t\tidx = find_subprog(env, i);\n\t\tif (idx < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tframe++;\n\t\tif (frame >= MAX_CALL_FRAMES) {\n\t\t\tverbose(env, \"the call stack of %d frames is too deep !\\n\",\n\t\t\t\tframe);\n\t\t\treturn -E2BIG;\n\t\t}\n\t\tgoto process_func;\n\t}\n\t/* end of for() loop means the last insn of the 'subprog'\n\t * was reached. Doesn't matter whether it was JA or EXIT\n\t */\n\tif (frame == 0)\n\t\treturn 0;\n\tdepth -= round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tframe--;\n\ti = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_func;\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nstatic int get_callee_stack_depth(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_insn *insn, int idx)\n{\n\tint start = idx + insn->imm + 1, subprog;\n\n\tsubprog = find_subprog(env, start);\n\tif (subprog < 0) {\n\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t  start);\n\t\treturn -EFAULT;\n\t}\n\treturn env->subprog_info[subprog].stack_depth;\n}\n#endif\n\nint check_ctx_reg(struct bpf_verifier_env *env,\n\t\t  const struct bpf_reg_state *reg, int regno)\n{\n\t/* Access to ctx or passing it to a helper is only allowed in\n\t * its original, unmodified form.\n\t */\n\n\tif (reg->off) {\n\t\tverbose(env, \"dereference of modified ctx ptr R%d off=%d disallowed\\n\",\n\t\t\tregno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable ctx access var_off=%s disallowed\\n\", tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int __check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t const char *buf_info,\n\t\t\t\t const struct bpf_reg_state *reg,\n\t\t\t\t int regno, int off, int size)\n{\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d invalid %s buffer access: off=%d, size=%d\\n\",\n\t\t\tregno, buf_info, off, size);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d invalid variable buffer offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_tp_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  int regno, int off, int size)\n{\n\tint err;\n\n\terr = __check_buffer_access(env, \"tracepoint\", reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > env->prog->aux->max_tp_access)\n\t\tenv->prog->aux->max_tp_access = off + size;\n\n\treturn 0;\n}\n\nstatic int check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t       int regno, int off, int size,\n\t\t\t       bool zero_size_allowed,\n\t\t\t       const char *buf_info,\n\t\t\t       u32 *max_access)\n{\n\tint err;\n\n\terr = __check_buffer_access(env, buf_info, reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > *max_access)\n\t\t*max_access = off + size;\n\n\treturn 0;\n}\n\n/* BPF architecture zero extends alu32 ops into 64-bit registesr */\nstatic void zext_32_to_64(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_subreg(reg->var_off);\n\t__reg_assign_32_into_64(reg);\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n\n\t/* If size is smaller than 32bit register the 32bit register\n\t * values are also truncated so we push 64-bit bounds into\n\t * 32-bit bounds. Above were truncated < 32-bits already.\n\t */\n\tif (size >= 4)\n\t\treturn;\n\t__reg_combine_64_into_32(reg);\n}\n\nstatic bool bpf_map_is_rdonly(const struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_RDONLY_PROG) && map->frozen;\n}\n\nstatic int bpf_map_direct_read(struct bpf_map *map, int off, int size, u64 *val)\n{\n\tvoid *ptr;\n\tu64 addr;\n\tint err;\n\n\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\tif (err)\n\t\treturn err;\n\tptr = (void *)(long)addr + off;\n\n\tswitch (size) {\n\tcase sizeof(u8):\n\t\t*val = (u64)*(u8 *)ptr;\n\t\tbreak;\n\tcase sizeof(u16):\n\t\t*val = (u64)*(u16 *)ptr;\n\t\tbreak;\n\tcase sizeof(u32):\n\t\t*val = (u64)*(u32 *)ptr;\n\t\tbreak;\n\tcase sizeof(u64):\n\t\t*val = *(u64 *)ptr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int check_ptr_to_btf_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tconst struct btf_type *t = btf_type_by_id(btf_vmlinux, reg->btf_id);\n\tconst char *tname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\tu32 btf_id;\n\tint ret;\n\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid variable offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, tname, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\tif (env->ops->btf_struct_access) {\n\t\tret = env->ops->btf_struct_access(&env->log, t, off, size,\n\t\t\t\t\t\t  atype, &btf_id);\n\t} else {\n\t\tif (atype != BPF_READ) {\n\t\t\tverbose(env, \"only read is supported\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tret = btf_struct_access(&env->log, t, off, size, atype,\n\t\t\t\t\t&btf_id);\n\t}\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (atype == BPF_READ && value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, btf_id);\n\n\treturn 0;\n}\n\nstatic int check_ptr_to_map_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_map *map = reg->map_ptr;\n\tconst struct btf_type *t;\n\tconst char *tname;\n\tu32 btf_id;\n\tint ret;\n\n\tif (!btf_vmlinux) {\n\t\tverbose(env, \"map_ptr access not supported without CONFIG_DEBUG_INFO_BTF\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (!map->ops->map_btf_id || !*map->ops->map_btf_id) {\n\t\tverbose(env, \"map_ptr access not supported for map type %d\\n\",\n\t\t\tmap->map_type);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = btf_type_by_id(btf_vmlinux, *map->ops->map_btf_id);\n\ttname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\n\tif (!env->allow_ptr_to_map_access) {\n\t\tverbose(env,\n\t\t\t\"%s access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\\n\",\n\t\t\ttname);\n\t\treturn -EPERM;\n\t}\n\n\tif (off < 0) {\n\t\tverbose(env, \"R%d is %s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (atype != BPF_READ) {\n\t\tverbose(env, \"only read from %s is supported\\n\", tname);\n\t\treturn -EACCES;\n\t}\n\n\tret = btf_struct_access(&env->log, t, off, size, atype, &btf_id);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, btf_id);\n\n\treturn 0;\n}\n\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_map_access_type(env, regno, off, size, t);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\tstruct bpf_map *map = reg->map_ptr;\n\n\t\t\t/* if map is read-only, track its contents as scalars */\n\t\t\tif (tnum_is_const(reg->var_off) &&\n\t\t\t    bpf_map_is_rdonly(map) &&\n\t\t\t    map->ops->map_direct_value_addr) {\n\t\t\t\tint map_off = off + reg->var_off.value;\n\t\t\t\tu64 val = 0;\n\n\t\t\t\terr = bpf_map_direct_read(map, map_off, size,\n\t\t\t\t\t\t\t  &val);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tregs[value_regno].type = SCALAR_VALUE;\n\t\t\t\t__mark_reg_known(&regs[value_regno], val);\n\t\t\t} else {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t}\n\t\t}\n\t} else if (reg->type == PTR_TO_MEM) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->mem_size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\t\tu32 btf_id = 0;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type, &btf_id);\n\t\tif (err)\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE) {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t} else {\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\t\tif (reg_type_may_be_null(reg_type))\n\t\t\t\t\tregs[value_regno].id = ++env->id_gen;\n\t\t\t\t/* A load of ctx field could have different\n\t\t\t\t * actual load size with the one encoded in the\n\t\t\t\t * insn. When the dst is PTR, it is for sure not\n\t\t\t\t * a sub-register.\n\t\t\t\t */\n\t\t\t\tregs[value_regno].subreg_def = DEF_NOT_SUBREG;\n\t\t\t\tif (reg_type == PTR_TO_BTF_ID ||\n\t\t\t\t    reg_type == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t\t\tregs[value_regno].btf_id = btf_id;\n\t\t\t}\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_access(env, reg, off, size);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (type_is_sk_pointer(reg->type)) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, insn_idx, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_TP_BUFFER) {\n\t\terr = check_tp_buffer_access(env, reg, regno, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_BTF_ID) {\n\t\terr = check_ptr_to_btf_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == CONST_PTR_TO_MAP) {\n\t\terr = check_ptr_to_map_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == PTR_TO_RDONLY_BUF) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdonly\",\n\t\t\t\t\t  &env->prog->aux->max_rdonly_access);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_RDWR_BUF) {\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdwr\",\n\t\t\t\t\t  &env->prog->aux->max_rdwr_access);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\tif (is_ctx_reg(env, insn->dst_reg) ||\n\t    is_pkt_reg(env, insn->dst_reg) ||\n\t    is_flow_key_reg(env, insn->dst_reg) ||\n\t    is_sk_reg(env, insn->dst_reg)) {\n\t\tverbose(env, \"BPF_XADD stores into R%d %s is not allowed\\n\",\n\t\t\tinsn->dst_reg,\n\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1, true);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1, true);\n}\n\nstatic int __check_stack_boundary(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t  int off, int access_size,\n\t\t\t\t  bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\t\tregno, off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid stack type R%d var_off=%s access_size=%d\\n\",\n\t\t\t\tregno, tn_buf, access_size);\n\t\t}\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err, min_off, max_off, i, j, slot, spi;\n\n\tif (reg->type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tmin_off = max_off = reg->var_off.value + reg->off;\n\t\terr = __check_stack_boundary(env, regno, min_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t/* Variable offset is prohibited for unprivileged mode for\n\t\t * simplicity since it requires corresponding support in\n\t\t * Spectre masking for stack ALU.\n\t\t * See also retrieve_ptr_limit().\n\t\t */\n\t\tif (!env->bypass_spec_v1) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"R%d indirect variable offset stack access prohibited for !root, var_off=%s\\n\",\n\t\t\t\tregno, tn_buf);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* Only initialized buffer on stack is allowed to be accessed\n\t\t * with variable offset. With uninitialized buffer it's hard to\n\t\t * guarantee that whole memory is marked as initialized on\n\t\t * helper return since specific bounds are unknown what may\n\t\t * cause uninitialized stack leaking.\n\t\t */\n\t\tif (meta && meta->raw_mode)\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smax_value >= BPF_MAX_VAR_OFF ||\n\t\t    reg->smax_value <= -BPF_MAX_VAR_OFF) {\n\t\t\tverbose(env, \"R%d unbounded indirect variable offset stack access\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmin_off = reg->smin_value + reg->off;\n\t\tmax_off = reg->smax_value + reg->off;\n\t\terr = __check_stack_boundary(env, regno, min_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err) {\n\t\t\tverbose(env, \"R%d min value is outside of stack bound\\n\",\n\t\t\t\tregno);\n\t\t\treturn err;\n\t\t}\n\t\terr = __check_stack_boundary(env, regno, max_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err) {\n\t\t\tverbose(env, \"R%d max value is outside of stack bound\\n\",\n\t\t\t\tregno);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = min_off; i < max_off + access_size; i++) {\n\t\tu8 *stype;\n\n\t\tslot = -i - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot)\n\t\t\tgoto err;\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tif (*stype == STACK_MISC)\n\t\t\tgoto mark;\n\t\tif (*stype == STACK_ZERO) {\n\t\t\t/* helper can write anything into the stack */\n\t\t\t*stype = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\n\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t    state->stack[spi].spilled_ptr.type == PTR_TO_BTF_ID)\n\t\t\tgoto mark;\n\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t    state->stack[spi].spilled_ptr.type == SCALAR_VALUE) {\n\t\t\t__mark_reg_unknown(env, &state->stack[spi].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tstate->stack[spi].slot_type[j] = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\n\nerr:\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\t\tmin_off, i - min_off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid indirect read from stack var_off %s+%d size %d\\n\",\n\t\t\t\ttn_buf, i - min_off, access_size);\n\t\t}\n\t\treturn -EACCES;\nmark:\n\t\t/* reading any byte out of 8-byte 'spill_slot' will cause\n\t\t * the whole slot to be marked as 'read'\n\t\t */\n\t\tmark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t      state->stack[spi].spilled_ptr.parent,\n\t\t\t      REG_LIVE_READ64);\n\t}\n\treturn update_stack_depth(env, state, min_off);\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\n/* Implementation details:\n * bpf_map_lookup returns PTR_TO_MAP_VALUE_OR_NULL\n * Two bpf_map_lookups (even with the same key) will have different reg->id.\n * For traditional PTR_TO_MAP_VALUE the verifier clears reg->id after\n * value_or_null->value transition, since the verifier only cares about\n * the range of access to valid map value pointer and doesn't care about actual\n * address of the map element.\n * For maps with 'struct bpf_spin_lock' inside map value the verifier keeps\n * reg->id > 0 after value_or_null->value transition. By doing so\n * two bpf_map_lookups will be considered two different pointers that\n * point to different bpf_spin_locks.\n * The verifier allows taking only one bpf_spin_lock at a time to avoid\n * dead-locks.\n * Since only one bpf_spin_lock is allowed the checks are simpler than\n * reg_is_refcounted() logic. The verifier needs to remember only\n * one spin_lock instead of array of acquired_refs.\n * cur_state->active_spin_lock remembers which map value element got locked\n * and clears it after bpf_spin_unlock.\n */\nstatic int process_spin_lock(struct bpf_verifier_env *env, int regno,\n\t\t\t     bool is_lock)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tbool is_const = tnum_is_const(reg->var_off);\n\tstruct bpf_map *map = reg->map_ptr;\n\tu64 val = reg->var_off.value;\n\n\tif (reg->type != PTR_TO_MAP_VALUE) {\n\t\tverbose(env, \"R%d is not a pointer to map_value\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\tif (!is_const) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. bpf_spin_lock has to be at the constant offset\\n\",\n\t\t\tregno);\n\t\treturn -EINVAL;\n\t}\n\tif (!map->btf) {\n\t\tverbose(env,\n\t\t\t\"map '%s' has to have BTF in order to use bpf_spin_lock\\n\",\n\t\t\tmap->name);\n\t\treturn -EINVAL;\n\t}\n\tif (!map_value_has_spin_lock(map)) {\n\t\tif (map->spin_lock_off == -E2BIG)\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' has more than one 'struct bpf_spin_lock'\\n\",\n\t\t\t\tmap->name);\n\t\telse if (map->spin_lock_off == -ENOENT)\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' doesn't have 'struct bpf_spin_lock'\\n\",\n\t\t\t\tmap->name);\n\t\telse\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' is not a struct type or bpf_spin_lock is mangled\\n\",\n\t\t\t\tmap->name);\n\t\treturn -EINVAL;\n\t}\n\tif (map->spin_lock_off != val + reg->off) {\n\t\tverbose(env, \"off %lld doesn't point to 'struct bpf_spin_lock'\\n\",\n\t\t\tval + reg->off);\n\t\treturn -EINVAL;\n\t}\n\tif (is_lock) {\n\t\tif (cur->active_spin_lock) {\n\t\t\tverbose(env,\n\t\t\t\t\"Locking two bpf_spin_locks are not allowed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur->active_spin_lock = reg->id;\n\t} else {\n\t\tif (!cur->active_spin_lock) {\n\t\t\tverbose(env, \"bpf_spin_unlock without taking a lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (cur->active_spin_lock != reg->id) {\n\t\t\tverbose(env, \"bpf_spin_unlock of different lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur->active_spin_lock = 0;\n\t}\n\treturn 0;\n}\n\nstatic bool arg_type_is_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_MEM ||\n\t       type == ARG_PTR_TO_MEM_OR_NULL ||\n\t       type == ARG_PTR_TO_UNINIT_MEM;\n}\n\nstatic bool arg_type_is_mem_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_SIZE ||\n\t       type == ARG_CONST_SIZE_OR_ZERO;\n}\n\nstatic bool arg_type_is_alloc_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_ALLOC_MEM ||\n\t       type == ARG_PTR_TO_ALLOC_MEM_OR_NULL;\n}\n\nstatic bool arg_type_is_alloc_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_ALLOC_SIZE_OR_ZERO;\n}\n\nstatic bool arg_type_is_int_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_INT ||\n\t       type == ARG_PTR_TO_LONG;\n}\n\nstatic int int_ptr_type_to_size(enum bpf_arg_type type)\n{\n\tif (type == ARG_PTR_TO_INT)\n\t\treturn sizeof(u32);\n\telse if (type == ARG_PTR_TO_LONG)\n\t\treturn sizeof(u64);\n\n\treturn -EINVAL;\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 arg,\n\t\t\t  struct bpf_call_arg_meta *meta,\n\t\t\t  const struct bpf_func_proto *fn)\n{\n\tu32 regno = BPF_REG_1 + arg;\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tenum bpf_arg_type arg_type = fn->arg_type[arg];\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO ||\n\t\t   arg_type == ARG_CONST_ALLOC_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX ||\n\t\t   arg_type == ARG_PTR_TO_CTX_OR_NULL) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (!(register_is_null(reg) &&\n\t\t      arg_type == ARG_PTR_TO_CTX_OR_NULL)) {\n\t\t\tif (type != expected_type)\n\t\t\t\tgoto err_type;\n\t\t\terr = check_ctx_reg(env, reg, regno);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SOCK_COMMON) {\n\t\texpected_type = PTR_TO_SOCK_COMMON;\n\t\t/* Any sk pointer can be ARG_PTR_TO_SOCK_COMMON */\n\t\tif (!type_is_sk_pointer(type))\n\t\t\tgoto err_type;\n\t\tif (reg->ref_obj_id) {\n\t\t\tif (meta->ref_obj_id) {\n\t\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\t\tmeta->ref_obj_id);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SOCKET ||\n\t\t   arg_type == ARG_PTR_TO_SOCKET_OR_NULL) {\n\t\texpected_type = PTR_TO_SOCKET;\n\t\tif (!(register_is_null(reg) &&\n\t\t      arg_type == ARG_PTR_TO_SOCKET_OR_NULL)) {\n\t\t\tif (type != expected_type)\n\t\t\t\tgoto err_type;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_BTF_ID) {\n\t\texpected_type = PTR_TO_BTF_ID;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (!fn->check_btf_id) {\n\t\t\tif (reg->btf_id != meta->btf_id) {\n\t\t\t\tverbose(env, \"Helper has type %s got %s in R%d\\n\",\n\t\t\t\t\tkernel_type_name(meta->btf_id),\n\t\t\t\t\tkernel_type_name(reg->btf_id), regno);\n\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t} else if (!fn->check_btf_id(reg->btf_id, arg)) {\n\t\t\tverbose(env, \"Helper does not support %s in R%d\\n\",\n\t\t\t\tkernel_type_name(reg->btf_id), regno);\n\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value || reg->off) {\n\t\t\tverbose(env, \"R%d is a pointer to in-kernel struct with non-zero offset\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SPIN_LOCK) {\n\t\tif (meta->func_id == BPF_FUNC_spin_lock) {\n\t\t\tif (process_spin_lock(env, regno, true))\n\t\t\t\treturn -EACCES;\n\t\t} else if (meta->func_id == BPF_FUNC_spin_unlock) {\n\t\t\tif (process_spin_lock(env, regno, false))\n\t\t\t\treturn -EACCES;\n\t\t} else {\n\t\t\tverbose(env, \"verifier internal error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t} else if (arg_type_is_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(reg) &&\n\t\t    (arg_type == ARG_PTR_TO_MEM_OR_NULL ||\n\t\t     arg_type == ARG_PTR_TO_ALLOC_MEM_OR_NULL))\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != PTR_TO_MEM &&\n\t\t\t type != PTR_TO_RDONLY_BUF &&\n\t\t\t type != PTR_TO_RDWR_BUF &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else if (arg_type_is_alloc_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_MEM;\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_ALLOC_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (meta->ref_obj_id) {\n\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\tmeta->ref_obj_id);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t} else if (arg_type_is_int_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) &&\n\t\t    type != PTR_TO_MAP_VALUE &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->key_size, false,\n\t\t\t\t\t      NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t\t   (arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL &&\n\t\t    !register_is_null(reg)) ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->raw_mode = (arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE);\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->value_size, false,\n\t\t\t\t\t      meta);\n\t} else if (arg_type_is_mem_size(arg_type)) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* This is used to refine r0 return value bounds for helpers\n\t\t * that enforce this value as an upper bound on return values.\n\t\t * See do_refine_retval_range() for helpers that can refine\n\t\t * the return value. C type of helper is u32 so we pull register\n\t\t * bound from umax_value however, if negative verifier errors\n\t\t * out. Only upper bounds can be learned because retval is an\n\t\t * int type and negative retvals are allowed.\n\t\t */\n\t\tmeta->msize_max_value = reg->umax_value;\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t\tif (!err)\n\t\t\terr = mark_chain_precision(env, regno);\n\t} else if (arg_type_is_alloc_size(arg_type)) {\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"R%d unbounded size, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->mem_size = reg->var_off.value;\n\t} else if (arg_type_is_int_ptr(arg_type)) {\n\t\tint size = int_ptr_type_to_size(arg_type);\n\n\t\terr = check_helper_mem_access(env, regno, size, false, meta);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_ptr_alignment(env, reg, 0, size, true);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_skb_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value &&\n\t\t    func_id != BPF_FUNC_xdp_output)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_RINGBUF:\n\t\tif (func_id != BPF_FUNC_ringbuf_output &&\n\t\t    func_id != BPF_FUNC_ringbuf_reserve &&\n\t\t    func_id != BPF_FUNC_ringbuf_submit &&\n\t\t    func_id != BPF_FUNC_ringbuf_discard &&\n\t\t    func_id != BPF_FUNC_ringbuf_query)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\tif (func_id != BPF_FUNC_get_local_storage)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_DEVMAP:\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap and xskmap, open when use-cases\n\t * appear.\n\t */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_map &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (func_id != BPF_FUNC_sk_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sock_hash_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\t\tif (func_id != BPF_FUNC_sk_select_reuseport)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_pop_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SK_STORAGE:\n\t\tif (func_id != BPF_FUNC_sk_storage_get &&\n\t\t    func_id != BPF_FUNC_sk_storage_delete)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tif (env->subprog_cnt > 1) {\n\t\t\tverbose(env, \"tail_calls are not allowed in programs with bpf-to-bpf calls\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\tcase BPF_FUNC_skb_output:\n\tcase BPF_FUNC_xdp_output:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_DEVMAP_HASH &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_XSKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\tcase BPF_FUNC_msg_redirect_map:\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_hash:\n\tcase BPF_FUNC_msg_redirect_hash:\n\tcase BPF_FUNC_sock_hash_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\tif (map->map_type != BPF_MAP_TYPE_REUSEPORT_SOCKARRAY &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_peek_elem:\n\tcase BPF_FUNC_map_pop_elem:\n\tcase BPF_FUNC_map_push_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_storage_get:\n\tcase BPF_FUNC_sk_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_SK_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic bool check_raw_mode_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\t/* We only support one arg being in raw mode at the moment,\n\t * which is sufficient for the helper functions we have\n\t * right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic bool check_args_pair_invalid(enum bpf_arg_type arg_curr,\n\t\t\t\t    enum bpf_arg_type arg_next)\n{\n\treturn (arg_type_is_mem_ptr(arg_curr) &&\n\t        !arg_type_is_mem_size(arg_next)) ||\n\t       (!arg_type_is_mem_ptr(arg_curr) &&\n\t\targ_type_is_mem_size(arg_next));\n}\n\nstatic bool check_arg_pair_ok(const struct bpf_func_proto *fn)\n{\n\t/* bpf_xxx(..., buf, len) call will access 'len'\n\t * bytes from memory 'buf'. Both arg types need\n\t * to be paired, so make sure there's no buggy\n\t * helper function specification.\n\t */\n\tif (arg_type_is_mem_size(fn->arg1_type) ||\n\t    arg_type_is_mem_ptr(fn->arg5_type)  ||\n\t    check_args_pair_invalid(fn->arg1_type, fn->arg2_type) ||\n\t    check_args_pair_invalid(fn->arg2_type, fn->arg3_type) ||\n\t    check_args_pair_invalid(fn->arg3_type, fn->arg4_type) ||\n\t    check_args_pair_invalid(fn->arg4_type, fn->arg5_type))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool check_refcount_ok(const struct bpf_func_proto *fn, int func_id)\n{\n\tint count = 0;\n\n\tif (arg_type_may_be_refcounted(fn->arg1_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg2_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg3_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg4_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg5_type))\n\t\tcount++;\n\n\t/* A reference acquiring function cannot acquire\n\t * another refcounted ptr.\n\t */\n\tif (may_be_acquire_function(func_id) && count)\n\t\treturn false;\n\n\t/* We only support one arg being unreferenced at the moment,\n\t * which is sufficient for the helper functions we have right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic int check_func_proto(const struct bpf_func_proto *fn, int func_id)\n{\n\treturn check_raw_mode_ok(fn) &&\n\t       check_arg_pair_ok(fn) &&\n\t       check_refcount_ok(fn, func_id) ? 0 : -EINVAL;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void __clear_all_pkt_pointers(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(env, reg);\n\t}\n}\n\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__clear_all_pkt_pointers(env, vstate->frame[i]);\n}\n\nstatic void release_reg_references(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *state,\n\t\t\t\t   int ref_obj_id)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].ref_obj_id == ref_obj_id)\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg->ref_obj_id == ref_obj_id)\n\t\t\t__mark_reg_unknown(env, reg);\n\t}\n}\n\n/* The pointer with the specified id has released its reference to kernel\n * resources. Identify all copies of the same pointer and clear the reference.\n */\nstatic int release_reference(struct bpf_verifier_env *env,\n\t\t\t     int ref_obj_id)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint err;\n\tint i;\n\n\terr = release_reference_state(cur_func(env), ref_obj_id);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\trelease_reg_references(env, vstate->frame[i], ref_obj_id);\n\n\treturn 0;\n}\n\nstatic void clear_caller_saved_regs(struct bpf_verifier_env *env,\n\t\t\t\t    struct bpf_reg_state *regs)\n{\n\tint i;\n\n\t/* after the call registers r0 - r5 were scratched */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n}\n\nstatic int check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t   int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_info_aux *func_info_aux;\n\tstruct bpf_func_state *caller, *callee;\n\tint i, err, subprog, target_insn;\n\tbool is_global = false;\n\n\tif (state->curframe + 1 >= MAX_CALL_FRAMES) {\n\t\tverbose(env, \"the call stack of %d frames is too deep\\n\",\n\t\t\tstate->curframe + 2);\n\t\treturn -E2BIG;\n\t}\n\n\ttarget_insn = *insn_idx + insn->imm;\n\tsubprog = find_subprog(env, target_insn + 1);\n\tif (subprog < 0) {\n\t\tverbose(env, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\ttarget_insn + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcaller = state->frame[state->curframe];\n\tif (state->frame[state->curframe + 1]) {\n\t\tverbose(env, \"verifier bug. Frame %d already allocated\\n\",\n\t\t\tstate->curframe + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tfunc_info_aux = env->prog->aux->func_info_aux;\n\tif (func_info_aux)\n\t\tis_global = func_info_aux[subprog].linkage == BTF_FUNC_GLOBAL;\n\terr = btf_check_func_arg_match(env, subprog, caller->regs);\n\tif (err == -EFAULT)\n\t\treturn err;\n\tif (is_global) {\n\t\tif (err) {\n\t\t\tverbose(env, \"Caller passes invalid args into func#%d\\n\",\n\t\t\t\tsubprog);\n\t\t\treturn err;\n\t\t} else {\n\t\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Func#%d is global and valid. Skipping.\\n\",\n\t\t\t\t\tsubprog);\n\t\t\tclear_caller_saved_regs(env, caller->regs);\n\n\t\t\t/* All global functions return SCALAR_VALUE */\n\t\t\tmark_reg_unknown(env, caller->regs, BPF_REG_0);\n\n\t\t\t/* continue with next insn after call */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tcallee = kzalloc(sizeof(*callee), GFP_KERNEL);\n\tif (!callee)\n\t\treturn -ENOMEM;\n\tstate->frame[state->curframe + 1] = callee;\n\n\t/* callee cannot access r0, r6 - r9 for reading and has to write\n\t * into its own stack before reading from it.\n\t * callee can read/write into caller's stack\n\t */\n\tinit_func_state(env, callee,\n\t\t\t/* remember the callsite, it will be used by bpf_exit */\n\t\t\t*insn_idx /* callsite */,\n\t\t\tstate->curframe + 1 /* frameno within this callchain */,\n\t\t\tsubprog /* subprog number within this prog */);\n\n\t/* Transfer references to the callee */\n\terr = transfer_reference_state(callee, caller);\n\tif (err)\n\t\treturn err;\n\n\t/* copy r1 - r5 args that callee can access.  The copy includes parent\n\t * pointers, which connects us up to the liveness chain\n\t */\n\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\tcallee->regs[i] = caller->regs[i];\n\n\tclear_caller_saved_regs(env, caller->regs);\n\n\t/* only increment it after check_reg_arg() finished */\n\tstate->curframe++;\n\n\t/* and go analyze first insn of the callee */\n\t*insn_idx = target_insn;\n\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"caller:\\n\");\n\t\tprint_verifier_state(env, caller);\n\t\tverbose(env, \"callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t}\n\treturn 0;\n}\n\nstatic int prepare_func_exit(struct bpf_verifier_env *env, int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tstruct bpf_reg_state *r0;\n\tint err;\n\n\tcallee = state->frame[state->curframe];\n\tr0 = &callee->regs[BPF_REG_0];\n\tif (r0->type == PTR_TO_STACK) {\n\t\t/* technically it's ok to return caller's stack pointer\n\t\t * (or caller's caller's pointer) back to the caller,\n\t\t * since these pointers are valid. Only current stack\n\t\t * pointer will be invalid as soon as function exits,\n\t\t * but let's be conservative\n\t\t */\n\t\tverbose(env, \"cannot return stack pointer to the caller\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstate->curframe--;\n\tcaller = state->frame[state->curframe];\n\t/* return to the caller whatever r0 had in the callee */\n\tcaller->regs[BPF_REG_0] = *r0;\n\n\t/* Transfer references to the caller */\n\terr = transfer_reference_state(caller, callee);\n\tif (err)\n\t\treturn err;\n\n\t*insn_idx = callee->callsite + 1;\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"returning from callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t\tverbose(env, \"to caller at %d:\\n\", *insn_idx);\n\t\tprint_verifier_state(env, caller);\n\t}\n\t/* clear everything in the callee */\n\tfree_func_state(callee);\n\tstate->frame[state->curframe + 1] = NULL;\n\treturn 0;\n}\n\nstatic void do_refine_retval_range(struct bpf_reg_state *regs, int ret_type,\n\t\t\t\t   int func_id,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *ret_reg = &regs[BPF_REG_0];\n\n\tif (ret_type != RET_INTEGER ||\n\t    (func_id != BPF_FUNC_get_stack &&\n\t     func_id != BPF_FUNC_probe_read_str &&\n\t     func_id != BPF_FUNC_probe_read_kernel_str &&\n\t     func_id != BPF_FUNC_probe_read_user_str))\n\t\treturn;\n\n\tret_reg->smax_value = meta->msize_max_value;\n\tret_reg->s32_max_value = meta->msize_max_value;\n\t__reg_deduce_bounds(ret_reg);\n\t__reg_bound_offset(ret_reg);\n\t__update_reg_bounds(ret_reg);\n}\n\nstatic int\nrecord_func_map(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_map *map = meta->map_ptr;\n\n\tif (func_id != BPF_FUNC_tail_call &&\n\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t    func_id != BPF_FUNC_map_update_elem &&\n\t    func_id != BPF_FUNC_map_delete_elem &&\n\t    func_id != BPF_FUNC_map_push_elem &&\n\t    func_id != BPF_FUNC_map_pop_elem &&\n\t    func_id != BPF_FUNC_map_peek_elem)\n\t\treturn 0;\n\n\tif (map == NULL) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* In case of read-only, some additional restrictions\n\t * need to be applied in order to prevent altering the\n\t * state of the map from program side.\n\t */\n\tif ((map->map_flags & BPF_F_RDONLY_PROG) &&\n\t    (func_id == BPF_FUNC_map_delete_elem ||\n\t     func_id == BPF_FUNC_map_update_elem ||\n\t     func_id == BPF_FUNC_map_push_elem ||\n\t     func_id == BPF_FUNC_map_pop_elem)) {\n\t\tverbose(env, \"write into map forbidden\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (!BPF_MAP_PTR(aux->map_ptr_state))\n\t\tbpf_map_ptr_store(aux, meta->map_ptr,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\telse if (BPF_MAP_PTR(aux->map_ptr_state) != meta->map_ptr)\n\t\tbpf_map_ptr_store(aux, BPF_MAP_PTR_POISON,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\treturn 0;\n}\n\nstatic int\nrecord_func_key(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg;\n\tstruct bpf_map *map = meta->map_ptr;\n\tstruct tnum range;\n\tu64 val;\n\tint err;\n\n\tif (func_id != BPF_FUNC_tail_call)\n\t\treturn 0;\n\tif (!map || map->map_type != BPF_MAP_TYPE_PROG_ARRAY) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trange = tnum_range(0, map->max_entries - 1);\n\treg = &regs[BPF_REG_3];\n\n\tif (!register_is_const(reg) || !tnum_in(range, reg->var_off)) {\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\t\treturn 0;\n\t}\n\n\terr = mark_chain_precision(env, BPF_REG_3);\n\tif (err)\n\t\treturn err;\n\n\tval = reg->var_off.value;\n\tif (bpf_map_key_unseen(aux))\n\t\tbpf_map_key_store(aux, val);\n\telse if (!bpf_map_key_poisoned(aux) &&\n\t\t  bpf_map_key_immediate(aux) != val)\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\treturn 0;\n}\n\nstatic int check_reference_leak(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint i;\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tverbose(env, \"Unreleased reference id=%d alloc_insn=%d\\n\",\n\t\t\tstate->refs[i].id, state->refs[i].insn_idx);\n\t}\n\treturn state->acquired_refs ? -EINVAL : 0;\n}\n\nstatic int check_helper_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn, func_id);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\tmeta.func_id = func_id;\n\t/* check args */\n\tfor (i = 0; i < 5; i++) {\n\t\tif (!fn->check_btf_id) {\n\t\t\terr = btf_resolve_helper_id(&env->log, fn, i);\n\t\t\tif (err > 0)\n\t\t\t\tmeta.btf_id = err;\n\t\t}\n\t\terr = check_func_arg(env, i, &meta, fn);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_key(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (func_id == BPF_FUNC_tail_call) {\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t} else if (is_release_function(func_id)) {\n\t\terr = release_reference(env, meta.ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"func %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tregs = cur_regs(env);\n\n\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t * this is required because get_local_storage() can't return an error.\n\t */\n\tif (func_id == BPF_FUNC_get_local_storage &&\n\t    !register_is_null(&regs[BPF_REG_2])) {\n\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* helper call returns 64-bit value. */\n\tregs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL ||\n\t\t   fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tif (fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE;\n\t\t\tif (map_value_has_spin_lock(meta.map_ptr))\n\t\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (fn->ret_type == RET_PTR_TO_SOCKET_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_SOCK_COMMON_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCK_COMMON_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_TCP_SOCK_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_TCP_SOCK_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_ALLOC_MEM_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\tregs[BPF_REG_0].mem_size = meta.mem_size;\n\t} else if (fn->ret_type == RET_PTR_TO_BTF_ID_OR_NULL) {\n\t\tint ret_btf_id;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID_OR_NULL;\n\t\tret_btf_id = *fn->ret_btf_id;\n\t\tif (ret_btf_id == 0) {\n\t\t\tverbose(env, \"invalid return type %d of func %s#%d\\n\",\n\t\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_ptr_cast_function(func_id)) {\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t} else if (is_acquire_function(func_id, meta.map_ptr)) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\t/* For mark_ptr_or_null_reg() */\n\t\tregs[BPF_REG_0].id = id;\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif ((func_id == BPF_FUNC_get_stack ||\n\t     func_id == BPF_FUNC_get_task_stack) &&\n\t    !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (func_id == BPF_FUNC_get_stackid || func_id == BPF_FUNC_get_stack)\n\t\tenv->prog->call_get_stack = true;\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_add32_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u32, where overflow is well-defined */\n\ts32 res = (s32)((u32)a + (u32)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s32 a, s32 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool signed_sub32_overflows(s32 a, s32 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts32 res = (s32)((u32)a - (u32)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic struct bpf_insn_aux_data *cur_aux(struct bpf_verifier_env *env)\n{\n\treturn &env->insn_aux_data[env->insn_idx];\n}\n\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\t/* Indirect variable offset stack access is prohibited in\n\t\t * unprivileged mode so it's not handled here.\n\t\t */\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic bool can_skip_alu_sanitation(const struct bpf_verifier_env *env,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn env->bypass_spec_v1 || BPF_SRC(insn->code) == BPF_K;\n}\n\nstatic int update_alu_sanitation_state(struct bpf_insn_aux_data *aux,\n\t\t\t\t       u32 alu_state, u32 alu_limit)\n{\n\t/* If we arrived here from different branches with different\n\t * state or limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\treturn 0;\n}\n\nstatic int sanitize_val_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\treturn update_alu_sanitation_state(aux, BPF_ALU_NON_POINTER, 0);\n}\n\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->bypass_spec_v1) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void scalar32_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_add32_overflows(dst_reg->s32_min_value, smin_val) ||\n\t    signed_add32_overflows(dst_reg->s32_max_value, smax_val)) {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value += smin_val;\n\t\tdst_reg->s32_max_value += smax_val;\n\t}\n\tif (dst_reg->u32_min_value + umin_val < umin_val ||\n\t    dst_reg->u32_max_value + umax_val < umax_val) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value += umin_val;\n\t\tdst_reg->u32_max_value += umax_val;\n\t}\n}\n\nstatic void scalar_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value += smin_val;\n\t\tdst_reg->smax_value += smax_val;\n\t}\n\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value += umin_val;\n\t\tdst_reg->umax_value += umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_sub32_overflows(dst_reg->s32_min_value, smax_val) ||\n\t    signed_sub32_overflows(dst_reg->s32_max_value, smin_val)) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value -= smax_val;\n\t\tdst_reg->s32_max_value -= smin_val;\n\t}\n\tif (dst_reg->u32_min_value < umax_val) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\tdst_reg->u32_min_value -= umax_val;\n\t\tdst_reg->u32_max_value -= umin_val;\n\t}\n}\n\nstatic void scalar_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value -= smax_val;\n\t\tdst_reg->smax_value -= smin_val;\n\t}\n\tif (dst_reg->umin_value < umax_val) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\tdst_reg->umin_value -= umax_val;\n\t\tdst_reg->umax_value -= umin_val;\n\t}\n}\n\nstatic void scalar32_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (smin_val < 0 || dst_reg->s32_min_value < 0) {\n\t\t/* Ain't nobody got time to multiply that sign */\n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t/* Both values are positive, so we can work with unsigned and\n\t * copy the result to signed (unless it exceeds S32_MAX).\n\t */\n\tif (umax_val > U16_MAX || dst_reg->u32_max_value > U16_MAX) {\n\t\t/* Potential overflow, we know nothing */\n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->u32_min_value *= umin_val;\n\tdst_reg->u32_max_value *= umax_val;\n\tif (dst_reg->u32_max_value > S32_MAX) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t/* Ain't nobody got time to multiply that sign */\n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t/* Both values are positive, so we can work with unsigned and\n\t * copy the result to signed (unless it exceeds S64_MAX).\n\t */\n\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t/* Potential overflow, we know nothing */\n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->umin_value *= umin_val;\n\tdst_reg->umax_value *= umax_val;\n\tif (dst_reg->umax_value > S64_MAX) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n}\n\nstatic void scalar32_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\t/* Assuming scalar64_min_max_and will be called so its safe\n\t * to skip updating register for known 32-bit case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our minimum from the var_off, since that's inherently\n\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t */\n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = min(dst_reg->u32_max_value, umax_val);\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ANDing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n\n}\n\nstatic void scalar_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t  src_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our minimum from the var_off, since that's inherently\n\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t */\n\tdst_reg->umin_value = dst_reg->var_off.value;\n\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t/* ANDing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->smin_value;\n\tu32 umin_val = src_reg->umin_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->umin_value;\n\t\tdst_reg->s32_max_value = dst_reg->umax_value;\n\t}\n}\n\nstatic void scalar_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t      struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t  src_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\tdst_reg->umax_value = dst_reg->var_off.value | dst_reg->var_off.mask;\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void __scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t/* We lose all sign bit information (except what we can pick\n\t * up from var_off)\n\t */\n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\t/* If we might shift our top bit out, then we know nothing */\n\tif (umax_val > 31 || dst_reg->u32_max_value > 1ULL << (31 - umax_val)) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value <<= umin_val;\n\t\tdst_reg->u32_max_value <<= umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\t/* u32 alu operation will zext upper bits */\n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\tdst_reg->var_off = tnum_subreg(tnum_lshift(subreg, umin_val));\n\t/* Not required but being careful mark reg64 bounds as unknown so\n\t * that we are forced to pick them up from tnum and zext later and\n\t * if some path skips this step we are still safe.\n\t */\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void __scalar64_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t/* Special case <<32 because it is a common compiler pattern to sign\n\t * extend subreg by doing <<32 s>>32. In this case if 32bit bounds are\n\t * positive we know this shift will also be positive so we can track\n\t * bounds correctly. Otherwise we lose all sign bit information except\n\t * what we can pick up from var_off. Perhaps we can generalize this\n\t * later to shifts of any length.\n\t */\n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_max_value >= 0)\n\t\tdst_reg->smax_value = (s64)dst_reg->s32_max_value << 32;\n\telse\n\t\tdst_reg->smax_value = S64_MAX;\n\n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_min_value >= 0)\n\t\tdst_reg->smin_value = (s64)dst_reg->s32_min_value << 32;\n\telse\n\t\tdst_reg->smin_value = S64_MIN;\n\n\t/* If we might shift our top bit out, then we know nothing */\n\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value <<= umin_val;\n\t\tdst_reg->umax_value <<= umax_val;\n\t}\n}\n\nstatic void scalar_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* scalar64 calc uses 32bit unshifted bounds so must be called first */\n\t__scalar64_min_max_lsh(dst_reg, umin_val, umax_val);\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\n\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t * be negative, then either:\n\t * 1) src_reg might be zero, so the sign bit of the result is\n\t *    unknown, so we lose our signed bounds\n\t * 2) it's known negative, thus the unsigned bounds capture the\n\t *    signed bounds\n\t * 3) the signed bounds cross zero, so they tell us nothing\n\t *    about the result\n\t * If the value in dst_reg is known nonnegative, then again the\n\t * unsigned bounts capture the signed bounds.\n\t * Thus, in all cases it suffices to blow away our signed bounds\n\t * and rely on inferring new ones from the unsigned bounds and\n\t * var_off of the result.\n\t */\n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\n\tdst_reg->var_off = tnum_rshift(subreg, umin_val);\n\tdst_reg->u32_min_value >>= umax_val;\n\tdst_reg->u32_max_value >>= umin_val;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t * be negative, then either:\n\t * 1) src_reg might be zero, so the sign bit of the result is\n\t *    unknown, so we lose our signed bounds\n\t * 2) it's known negative, thus the unsigned bounds capture the\n\t *    signed bounds\n\t * 3) the signed bounds cross zero, so they tell us nothing\n\t *    about the result\n\t * If the value in dst_reg is known nonnegative, then again the\n\t * unsigned bounts capture the signed bounds.\n\t * Thus, in all cases it suffices to blow away our signed bounds\n\t * and rely on inferring new ones from the unsigned bounds and\n\t * var_off of the result.\n\t */\n\tdst_reg->smin_value = S64_MIN;\n\tdst_reg->smax_value = S64_MAX;\n\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\tdst_reg->umin_value >>= umax_val;\n\tdst_reg->umax_value >>= umin_val;\n\n\t/* Its not easy to operate on alu32 bounds here because it depends\n\t * on bits being shifted in. Take easy way out and mark unbounded\n\t * so we can recalculate later from tnum.\n\t */\n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t  struct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->u32_min_value;\n\n\t/* Upon reaching here, src_known is true and\n\t * umax_val is equal to umin_val.\n\t */\n\tdst_reg->s32_min_value = (u32)(((s32)dst_reg->s32_min_value) >> umin_val);\n\tdst_reg->s32_max_value = (u32)(((s32)dst_reg->s32_max_value) >> umin_val);\n\n\tdst_reg->var_off = tnum_arshift(tnum_subreg(dst_reg->var_off), umin_val, 32);\n\n\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t * dst_reg var_off to refine the result.\n\t */\n\tdst_reg->u32_min_value = 0;\n\tdst_reg->u32_max_value = U32_MAX;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* Upon reaching here, src_known is true and umax_val is equal\n\t * to umin_val.\n\t */\n\tdst_reg->smin_value >>= umin_val;\n\tdst_reg->smax_value >>= umin_val;\n\n\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val, 64);\n\n\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t * dst_reg var_off to refine the result.\n\t */\n\tdst_reg->umin_value = 0;\n\tdst_reg->umax_value = U64_MAX;\n\n\t/* Its not easy to operate on alu32 bounds here because it depends\n\t * on bits being shifted in from upper 32-bits. Take easy way out\n\t * and mark unbounded so we can recalculate later from tnum.\n\t */\n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\ts32 s32_min_val, s32_max_val;\n\tu32 u32_min_val, u32_max_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\tbool alu32 = (BPF_CLASS(insn->code) != BPF_ALU64);\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\n\ts32_min_val = src_reg.s32_min_value;\n\ts32_max_val = src_reg.s32_max_value;\n\tu32_min_val = src_reg.u32_min_value;\n\tu32_max_val = src_reg.u32_max_value;\n\n\tif (alu32) {\n\t\tsrc_known = tnum_subreg_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (s32_min_val != s32_max_val || u32_min_val != u32_max_val)) ||\n\t\t    s32_min_val > s32_max_val || u32_min_val > u32_max_val) {\n\t\t\t/* Taint dst register if offset had invalid bounds\n\t\t\t * derived from e.g. dead branches.\n\t\t\t */\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tsrc_known = tnum_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (smin_val != smax_val || umin_val != umax_val)) ||\n\t\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t\t/* Taint dst register if offset had invalid bounds\n\t\t\t * derived from e.g. dead branches.\n\t\t\t */\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\t/* Calculate sign/unsigned bounds and tnum for alu32 and alu64 bit ops.\n\t * There are two classes of instructions: The first class we track both\n\t * alu32 and alu64 sign/unsigned bounds independently this provides the\n\t * greatest amount of precision when alu operations are mixed with jmp32\n\t * operations. These operations are BPF_ADD, BPF_SUB, BPF_MUL, BPF_ADD,\n\t * and BPF_OR. This is possible because these ops have fairly easy to\n\t * understand and calculate behavior in both 32-bit and 64-bit alu ops.\n\t * See alu32 verifier tests for examples. The second class of\n\t * operations, BPF_LSH, BPF_RSH, and BPF_ARSH, however are not so easy\n\t * with regards to tracking sign/unsigned bounds because the bits may\n\t * cross subreg boundaries in the alu64 case. When this happens we mark\n\t * the reg unbounded in the subreg bound space and use the resulting\n\t * tnum to calculate an approximation of the sign/unsigned bounds.\n\t */\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tscalar32_min_max_add(dst_reg, &src_reg);\n\t\tscalar_min_max_add(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tscalar32_min_max_sub(dst_reg, &src_reg);\n\t\tscalar_min_max_sub(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_mul(dst_reg, &src_reg);\n\t\tscalar_min_max_mul(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_AND:\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_and(dst_reg, &src_reg);\n\t\tscalar_min_max_and(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_or(dst_reg, &src_reg);\n\t\tscalar_min_max_or(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_lsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_lsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_rsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_rsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_arsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_arsh(dst_reg, &src_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\t/* ALU32 ops are zero extended into 64bit register */\n\tif (alu32)\n\t\tzext_32_to_64(dst_reg);\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\tdst_reg->subreg_def = DEF_NOT_SUBREG;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\tdst_reg->subreg_def = env->insn_idx + 1;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tzext_32_to_64(dst_reg);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void __find_good_pkt_pointers(struct bpf_func_state *state,\n\t\t\t\t     struct bpf_reg_state *dst_reg,\n\t\t\t\t     enum bpf_reg_type type, u16 new_range)\n{\n\tstruct bpf_reg_state *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tu16 new_range;\n\tint i;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__find_good_pkt_pointers(vstate->frame[i], dst_reg, type,\n\t\t\t\t\t new_range);\n}\n\nstatic int is_branch32_taken(struct bpf_reg_state *reg, u32 val, u8 opcode)\n{\n\tstruct tnum subreg = tnum_subreg(reg->var_off);\n\ts32 sval = (s32)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !!tnum_equals_const(subreg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !tnum_equals_const(subreg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~subreg.mask & subreg.value) & val)\n\t\t\treturn 1;\n\t\tif (!((subreg.mask | subreg.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->u32_min_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->s32_min_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->u32_max_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->s32_max_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->u32_min_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->s32_min_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->u32_max_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->s32_max_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n\nstatic int is_branch64_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\ts64 sval = (s64)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value\n *      range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode,\n\t\t\t   bool is_jmp32)\n{\n\tif (__is_pointer_value(false, reg)) {\n\t\tif (!reg_type_not_null(reg->type))\n\t\t\treturn -1;\n\n\t\t/* If pointer is valid tests against zero will fail so we can\n\t\t * use this to direct branch taken.\n\t\t */\n\t\tif (val != 0)\n\t\t\treturn -1;\n\n\t\tswitch (opcode) {\n\t\tcase BPF_JEQ:\n\t\t\treturn 0;\n\t\tcase BPF_JNE:\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (is_jmp32)\n\t\treturn is_branch32_taken(reg, val, opcode);\n\treturn is_branch64_taken(reg, val, opcode);\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg,\n\t\t\t    u64 val, u32 val32,\n\t\t\t    u8 opcode, bool is_jmp32)\n{\n\tstruct tnum false_32off = tnum_subreg(false_reg->var_off);\n\tstruct tnum false_64off = false_reg->var_off;\n\tstruct tnum true_32off = tnum_subreg(true_reg->var_off);\n\tstruct tnum true_64off = true_reg->var_off;\n\ts64 sval = (s64)val;\n\ts32 sval32 = (s32)val32;\n\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\n\t\t/* For BPF_JEQ, if this is false we know nothing Jon Snow, but\n\t\t * if it is true we know the value for sure. Likewise for\n\t\t * BPF_JNE.\n\t\t */\n\t\tif (is_jmp32)\n\t\t\t__mark_reg32_known(reg, val32);\n\t\telse\n\t\t\t__mark_reg_known(reg, val);\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tif (is_jmp32) {\n\t\t\tfalse_32off = tnum_and(false_32off, tnum_const(~val32));\n\t\t\tif (is_power_of_2(val32))\n\t\t\t\ttrue_32off = tnum_or(true_32off,\n\t\t\t\t\t\t     tnum_const(val32));\n\t\t} else {\n\t\t\tfalse_64off = tnum_and(false_64off, tnum_const(~val));\n\t\t\tif (is_power_of_2(val))\n\t\t\t\ttrue_64off = tnum_or(true_64off,\n\t\t\t\t\t\t     tnum_const(val));\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umax = opcode == BPF_JGT ? val32  : val32 - 1;\n\t\t\tu32 true_umin = opcode == BPF_JGT ? val32 + 1 : val32;\n\n\t\t\tfalse_reg->u32_max_value = min(false_reg->u32_max_value,\n\t\t\t\t\t\t       false_umax);\n\t\t\ttrue_reg->u32_min_value = max(true_reg->u32_min_value,\n\t\t\t\t\t\t      true_umin);\n\t\t} else {\n\t\t\tu64 false_umax = opcode == BPF_JGT ? val    : val - 1;\n\t\t\tu64 true_umin = opcode == BPF_JGT ? val + 1 : val;\n\n\t\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smax = opcode == BPF_JSGT ? sval32    : sval32 - 1;\n\t\t\ts32 true_smin = opcode == BPF_JSGT ? sval32 + 1 : sval32;\n\n\t\t\tfalse_reg->s32_max_value = min(false_reg->s32_max_value, false_smax);\n\t\t\ttrue_reg->s32_min_value = max(true_reg->s32_min_value, true_smin);\n\t\t} else {\n\t\t\ts64 false_smax = opcode == BPF_JSGT ? sval    : sval - 1;\n\t\t\ts64 true_smin = opcode == BPF_JSGT ? sval + 1 : sval;\n\n\t\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umin = opcode == BPF_JLT ? val32  : val32 + 1;\n\t\t\tu32 true_umax = opcode == BPF_JLT ? val32 - 1 : val32;\n\n\t\t\tfalse_reg->u32_min_value = max(false_reg->u32_min_value,\n\t\t\t\t\t\t       false_umin);\n\t\t\ttrue_reg->u32_max_value = min(true_reg->u32_max_value,\n\t\t\t\t\t\t      true_umax);\n\t\t} else {\n\t\t\tu64 false_umin = opcode == BPF_JLT ? val    : val + 1;\n\t\t\tu64 true_umax = opcode == BPF_JLT ? val - 1 : val;\n\n\t\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smin = opcode == BPF_JSLT ? sval32    : sval32 + 1;\n\t\t\ts32 true_smax = opcode == BPF_JSLT ? sval32 - 1 : sval32;\n\n\t\t\tfalse_reg->s32_min_value = max(false_reg->s32_min_value, false_smin);\n\t\t\ttrue_reg->s32_max_value = min(true_reg->s32_max_value, true_smax);\n\t\t} else {\n\t\t\ts64 false_smin = opcode == BPF_JSLT ? sval    : sval + 1;\n\t\t\ts64 true_smax = opcode == BPF_JSLT ? sval - 1 : sval;\n\n\t\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (is_jmp32) {\n\t\tfalse_reg->var_off = tnum_or(tnum_clear_subreg(false_64off),\n\t\t\t\t\t     tnum_subreg(false_32off));\n\t\ttrue_reg->var_off = tnum_or(tnum_clear_subreg(true_64off),\n\t\t\t\t\t    tnum_subreg(true_32off));\n\t\t__reg_combine_32_into_64(false_reg);\n\t\t__reg_combine_32_into_64(true_reg);\n\t} else {\n\t\tfalse_reg->var_off = false_64off;\n\t\ttrue_reg->var_off = true_64off;\n\t\t__reg_combine_64_into_32(false_reg);\n\t\t__reg_combine_64_into_32(true_reg);\n\t}\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg,\n\t\t\t\tu64 val, u32 val32,\n\t\t\t\tu8 opcode, bool is_jmp32)\n{\n\t/* How can we transform \"a <op> b\" into \"b <op> a\"? */\n\tstatic const u8 opcode_flip[16] = {\n\t\t/* these stay the same */\n\t\t[BPF_JEQ  >> 4] = BPF_JEQ,\n\t\t[BPF_JNE  >> 4] = BPF_JNE,\n\t\t[BPF_JSET >> 4] = BPF_JSET,\n\t\t/* these swap \"lesser\" and \"greater\" (L and G in the opcodes) */\n\t\t[BPF_JGE  >> 4] = BPF_JLE,\n\t\t[BPF_JGT  >> 4] = BPF_JLT,\n\t\t[BPF_JLE  >> 4] = BPF_JGE,\n\t\t[BPF_JLT  >> 4] = BPF_JGT,\n\t\t[BPF_JSGE >> 4] = BPF_JSLE,\n\t\t[BPF_JSGT >> 4] = BPF_JSLT,\n\t\t[BPF_JSLE >> 4] = BPF_JSGE,\n\t\t[BPF_JSLT >> 4] = BPF_JSGT\n\t};\n\topcode = opcode_flip[opcode >> 4];\n\t/* This uses zero as \"not present in table\"; luckily the zero opcode,\n\t * BPF_JA, can't get here.\n\t */\n\tif (opcode)\n\t\treg_set_min_max(true_reg, false_reg, val, val32, opcode, is_jmp32);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\t\tif (map->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\t\treg->type = PTR_TO_SOCKET;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else if (reg->type == PTR_TO_SOCK_COMMON_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\t} else if (reg->type == PTR_TO_TCP_SOCK_OR_NULL) {\n\t\t\treg->type = PTR_TO_TCP_SOCK;\n\t\t} else if (reg->type == PTR_TO_BTF_ID_OR_NULL) {\n\t\t\treg->type = PTR_TO_BTF_ID;\n\t\t} else if (reg->type == PTR_TO_MEM_OR_NULL) {\n\t\t\treg->type = PTR_TO_MEM;\n\t\t} else if (reg->type == PTR_TO_RDONLY_BUF_OR_NULL) {\n\t\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\t} else if (reg->type == PTR_TO_RDWR_BUF_OR_NULL) {\n\t\t\treg->type = PTR_TO_RDWR_BUF;\n\t\t}\n\t\tif (is_null) {\n\t\t\t/* We don't need id and ref_obj_id from this point\n\t\t\t * onwards anymore, thus we should better reset it,\n\t\t\t * so that state pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t\treg->ref_obj_id = 0;\n\t\t} else if (!reg_may_point_to_spin_lock(reg)) {\n\t\t\t/* For not-NULL ptr, reg->ref_obj_id will be reset\n\t\t\t * in release_reg_references().\n\t\t\t *\n\t\t\t * reg->id is still used by spin_lock ptr. Other\n\t\t\t * than spin_lock ptr type, reg->id can be reset.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\nstatic void __mark_ptr_or_null_regs(struct bpf_func_state *state, u32 id,\n\t\t\t\t    bool is_null)\n{\n\tstruct bpf_reg_state *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &state->regs[i], id, is_null);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs;\n\tu32 ref_obj_id = regs[regno].ref_obj_id;\n\tu32 id = regs[regno].id;\n\tint i;\n\n\tif (ref_obj_id && ref_obj_id == id && is_null)\n\t\t/* regs[regno] is in the \" == NULL\" branch.\n\t\t * No one could have freed the reference state before\n\t\t * doing the NULL check.\n\t\t */\n\t\tWARN_ON_ONCE(release_reference_state(state, id));\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__mark_ptr_or_null_regs(vstate->frame[i], id, is_null);\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\t/* Pointers are always 64-bit. */\n\tif (BPF_CLASS(insn->code) == BPF_JMP32)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs, *src_reg = NULL;\n\tu8 opcode = BPF_OP(insn->code);\n\tbool is_jmp32;\n\tint pred = -1;\n\tint err;\n\n\t/* Only conditional jumps are expected to reach here. */\n\tif (opcode == BPF_JA || opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP/JMP32 opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tsrc_reg = &regs[insn->src_reg];\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tis_jmp32 = BPF_CLASS(insn->code) == BPF_JMP32;\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tpred = is_branch_taken(dst_reg, insn->imm, opcode, is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(src_reg->var_off))) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       tnum_subreg(src_reg->var_off).value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(src_reg->var_off)) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       src_reg->var_off.value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t}\n\n\tif (pred >= 0) {\n\t\t/* If we get here with a dst_reg pointer type it is because\n\t\t * above is_branch_taken() special cased the 0 comparison.\n\t\t */\n\t\tif (!__is_pointer_value(false, dst_reg))\n\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\tif (BPF_SRC(insn->code) == BPF_X && !err)\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (pred == 1) {\n\t\t/* only follow the goto, ignore fall-through */\n\t\t*insn_idx += insn->off;\n\t\treturn 0;\n\t} else if (pred == 0) {\n\t\t/* only follow fall-through branch, since\n\t\t * that's where the program will go\n\t\t */\n\t\treturn 0;\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tstruct bpf_reg_state *src_reg = &regs[insn->src_reg];\n\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    src_reg->type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(src_reg->var_off) ||\n\t\t\t    (is_jmp32 &&\n\t\t\t     tnum_is_const(tnum_subreg(src_reg->var_off))))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg,\n\t\t\t\t\t\tsrc_reg->var_off.value,\n\t\t\t\t\t\ttnum_subreg(src_reg->var_off).value,\n\t\t\t\t\t\topcode, is_jmp32);\n\t\t\telse if (tnum_is_const(dst_reg->var_off) ||\n\t\t\t\t (is_jmp32 &&\n\t\t\t\t  tnum_is_const(tnum_subreg(dst_reg->var_off))))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    src_reg,\n\t\t\t\t\t\t    dst_reg->var_off.value,\n\t\t\t\t\t\t    tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t\t\t    opcode, is_jmp32);\n\t\t\telse if (!is_jmp32 &&\n\t\t\t\t (opcode == BPF_JEQ || opcode == BPF_JNE))\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    src_reg, dst_reg, opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, (u32)insn->imm,\n\t\t\t\t\topcode, is_jmp32);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem().\n\t * NOTE: these optimizations below are related with pointer comparison\n\t *       which will never be JMP32.\n\t */\n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map;\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\tmap = env->used_maps[aux->map_index];\n\tmark_reg_known_zero(env, regs, insn->dst_reg);\n\tregs[insn->dst_reg].map_ptr = map;\n\n\tif (insn->src_reg == BPF_PSEUDO_MAP_VALUE) {\n\t\tregs[insn->dst_reg].type = PTR_TO_MAP_VALUE;\n\t\tregs[insn->dst_reg].off = aux->map_off;\n\t\tif (map_value_has_spin_lock(map))\n\t\t\tregs[insn->dst_reg].id = ++env->id_gen;\n\t} else if (insn->src_reg == BPF_PSEUDO_MAP_FD) {\n\t\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\t} else {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstatic const int ctx_reg = BPF_REG_6;\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, ctx_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (env->cur_state->active_spin_lock) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be used inside bpf_spin_lock-ed region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (regs[ctx_reg].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = check_ctx_reg(env, &regs[ctx_reg], ctx_reg);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t/* ld_abs load up to 32-bit skb data. */\n\tregs[BPF_REG_0].subreg_def = env->insn_idx + 1;\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tint err;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif ((env->prog->type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     env->prog->type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convetion is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), range);\n\t\tverbose(env, \" should have been in %s\\n\", tn_buf);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\nstatic u32 state_htab_size(struct bpf_verifier_env *env)\n{\n\treturn env->prog->len;\n}\n\nstatic struct bpf_verifier_state_list **explored_state(\n\t\t\t\t\tstruct bpf_verifier_env *env,\n\t\t\t\t\tint idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_func_state *state = cur->frame[cur->curframe];\n\n\treturn &env->explored_states[(idx ^ state->callsite) % state_htab_size(env)];\n}\n\nstatic void init_explored_state(struct bpf_verifier_env *env, int idx)\n{\n\tenv->insn_aux_data[idx].prune_point = true;\n}\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env,\n\t\t     bool loop_ok)\n{\n\tint *insn_stack = env->cfg.insn_stack;\n\tint *insn_state = env->cfg.insn_state;\n\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tinit_explored_state(env, w);\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (env->cfg.cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[env->cfg.cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tif (loop_ok && env->bpf_capable)\n\t\t\treturn 0;\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint *insn_stack, *insn_state;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = env->cfg.insn_state = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = env->cfg.insn_stack = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkvfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tenv->cfg.cur_stack = 1;\n\npeek_stack:\n\tif (env->cfg.cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[env->cfg.cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP ||\n\t    BPF_CLASS(insns[t].code) == BPF_JMP32) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, false);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tinit_explored_state(env, t + 1);\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tinit_explored_state(env, t);\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH,\n\t\t\t\t\t\tenv, false);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* unconditional jmp is not a good pruning point,\n\t\t\t * but it's marked, since backtracking needs\n\t\t\t * to record jmp history in is_state_visited().\n\t\t\t */\n\t\t\tinit_explored_state(env, t + insns[t].off + 1);\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tinit_explored_state(env, t + 1);\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tinit_explored_state(env, t);\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, false);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (env->cfg.cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkvfree(insn_state);\n\tkvfree(insn_stack);\n\tenv->cfg.insn_state = env->cfg.insn_stack = NULL;\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tstruct bpf_func_info_aux *info_aux = NULL;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tu32 prev_offset = 0;\n\tint ret = -ENOMEM;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\tinfo_aux = kcalloc(nfuncs, sizeof(*info_aux), GFP_KERNEL | __GFP_NOWARN);\n\tif (!info_aux)\n\t\tgoto err_free;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || !btf_type_is_func(type)) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t\tinfo_aux[i].linkage = BTF_INFO_VLEN(type->info);\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\tprog->aux->func_info_aux = info_aux;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\tkfree(info_aux);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i;\n\n\tif (!aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\taux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(env, &st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(env, &st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = *explored_state(env, insn);\n\twhile (sl) {\n\t\tif (sl->state.branches)\n\t\t\tgoto next;\n\t\tif (sl->state.insn_idx != insn ||\n\t\t    sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* Verification state from speculative execution simulation\n\t * must never prune a non-speculative execution one.\n\t */\n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\tif (old->active_spin_lock != cur->active_spin_lock)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* Return 0 if no propagation happened. Return negative error code if error\n * happened. Otherwise, return the propagated bit.\n */\nstatic int propagate_liveness_reg(struct bpf_verifier_env *env,\n\t\t\t\t  struct bpf_reg_state *reg,\n\t\t\t\t  struct bpf_reg_state *parent_reg)\n{\n\tu8 parent_flag = parent_reg->live & REG_LIVE_READ;\n\tu8 flag = reg->live & REG_LIVE_READ;\n\tint err;\n\n\t/* When comes here, read flags of PARENT_REG or REG could be any of\n\t * REG_LIVE_READ64, REG_LIVE_READ32, REG_LIVE_NONE. There is no need\n\t * of propagation if PARENT_REG has strongest REG_LIVE_READ64.\n\t */\n\tif (parent_flag == REG_LIVE_READ64 ||\n\t    /* Or if there is no read flag from REG. */\n\t    !flag ||\n\t    /* Or if the read flag from REG is the same as PARENT_REG. */\n\t    parent_flag == flag)\n\t\treturn 0;\n\n\terr = mark_reg_read(env, reg, parent_reg, flag);\n\tif (err)\n\t\treturn err;\n\n\treturn flag;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at an\n * equivalent state (jump target or such) we didn't arrive by the straight-line\n * code, so read marks in the state must propagate to the parent regardless\n * of the state's write marks. That's what 'parent == state->parent' comparison\n * in mark_reg_read() is for.\n */\nstatic int propagate_liveness(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_verifier_state *vstate,\n\t\t\t      struct bpf_verifier_state *vparent)\n{\n\tstruct bpf_reg_state *state_reg, *parent_reg;\n\tstruct bpf_func_state *state, *parent;\n\tint i, frame, err = 0;\n\n\tif (vparent->curframe != vstate->curframe) {\n\t\tWARN(1, \"propagate_live: parent frame %d current frame %d\\n\",\n\t\t     vparent->curframe, vstate->curframe);\n\t\treturn -EFAULT;\n\t}\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\tfor (frame = 0; frame <= vstate->curframe; frame++) {\n\t\tparent = vparent->frame[frame];\n\t\tstate = vstate->frame[frame];\n\t\tparent_reg = parent->regs;\n\t\tstate_reg = state->regs;\n\t\t/* We don't need to worry about FP liveness, it's read-only */\n\t\tfor (i = frame < vstate->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++) {\n\t\t\terr = propagate_liveness_reg(env, &state_reg[i],\n\t\t\t\t\t\t     &parent_reg[i]);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tif (err == REG_LIVE_READ64)\n\t\t\t\tmark_insn_zext(env, &parent_reg[i]);\n\t\t}\n\n\t\t/* Propagate stack slots. */\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tparent_reg = &parent->stack[i].spilled_ptr;\n\t\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\t\terr = propagate_liveness_reg(env, state_reg,\n\t\t\t\t\t\t     parent_reg);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* find precise scalars in the previous equivalent state and\n * propagate them into the current state\n */\nstatic int propagate_precision(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_verifier_state *old)\n{\n\tstruct bpf_reg_state *state_reg;\n\tstruct bpf_func_state *state;\n\tint i, err = 0;\n\n\tstate = old->frame[old->curframe];\n\tstate_reg = state->regs;\n\tfor (i = 0; i < BPF_REG_FP; i++, state_reg++) {\n\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t    !state_reg->precise)\n\t\t\tcontinue;\n\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\tverbose(env, \"propagating r%d\\n\", i);\n\t\terr = mark_chain_precision(env, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t    !state_reg->precise)\n\t\t\tcontinue;\n\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\tverbose(env, \"propagating fp%d\\n\",\n\t\t\t\t(-i - 1) * BPF_REG_SIZE);\n\t\terr = mark_chain_precision_stack(env, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic bool states_maybe_looping(struct bpf_verifier_state *old,\n\t\t\t\t struct bpf_verifier_state *cur)\n{\n\tstruct bpf_func_state *fold, *fcur;\n\tint i, fr = cur->curframe;\n\n\tif (old->curframe != fr)\n\t\treturn false;\n\n\tfold = old->frame[fr];\n\tfcur = cur->frame[fr];\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (memcmp(&fold->regs[i], &fcur->regs[i],\n\t\t\t   offsetof(struct bpf_reg_state, parent)))\n\t\t\treturn false;\n\treturn true;\n}\n\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl, **pprev;\n\tstruct bpf_verifier_state *cur = env->cur_state, *new;\n\tint i, j, err, states_cnt = 0;\n\tbool add_new_state = env->test_state_freq ? true : false;\n\n\tcur->last_insn_idx = env->prev_insn_idx;\n\tif (!env->insn_aux_data[insn_idx].prune_point)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\t/* bpf progs typically have pruning point every 4 instructions\n\t * http://vger.kernel.org/bpfconf2019.html#session-1\n\t * Do not add new state for future pruning if the verifier hasn't seen\n\t * at least 2 jumps and at least 8 instructions.\n\t * This heuristics helps decrease 'total_states' and 'peak_states' metric.\n\t * In tests that amounts to up to 50% reduction into total verifier\n\t * memory consumption and 20% verifier time speedup.\n\t */\n\tif (env->jmps_processed - env->prev_jmps_processed >= 2 &&\n\t    env->insn_processed - env->prev_insn_processed >= 8)\n\t\tadd_new_state = true;\n\n\tpprev = explored_state(env, insn_idx);\n\tsl = *pprev;\n\n\tclean_live_states(env, insn_idx, cur);\n\n\twhile (sl) {\n\t\tstates_cnt++;\n\t\tif (sl->state.insn_idx != insn_idx)\n\t\t\tgoto next;\n\t\tif (sl->state.branches) {\n\t\t\tif (states_maybe_looping(&sl->state, cur) &&\n\t\t\t    states_equal(env, &sl->state, cur)) {\n\t\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\t\tverbose(env, \"infinite loop detected at insn %d\\n\", insn_idx);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* if the verifier is processing a loop, avoid adding new state\n\t\t\t * too often, since different loop iterations have distinct\n\t\t\t * states and may not help future pruning.\n\t\t\t * This threshold shouldn't be too low to make sure that\n\t\t\t * a loop with large bound will be rejected quickly.\n\t\t\t * The most abusive loop will be:\n\t\t\t * r1 += 1\n\t\t\t * if r1 < 1000000 goto pc-2\n\t\t\t * 1M insn_procssed limit / 100 == 10k peak states.\n\t\t\t * This threshold shouldn't be too high either, since states\n\t\t\t * at the end of the loop are likely to be useful in pruning.\n\t\t\t */\n\t\t\tif (env->jmps_processed - env->prev_jmps_processed < 20 &&\n\t\t\t    env->insn_processed - env->prev_insn_processed < 100)\n\t\t\t\tadd_new_state = false;\n\t\t\tgoto miss;\n\t\t}\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\tsl->hit_cnt++;\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\terr = propagate_liveness(env, &sl->state, cur);\n\n\t\t\t/* if previous state reached the exit with precision and\n\t\t\t * current state is equivalent to it (except precsion marks)\n\t\t\t * the precision needs to be propagated back in\n\t\t\t * the current state.\n\t\t\t */\n\t\t\terr = err ? : push_jmp_history(env, cur);\n\t\t\terr = err ? : propagate_precision(env, &sl->state);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn 1;\n\t\t}\nmiss:\n\t\t/* when new state is not going to be added do not increase miss count.\n\t\t * Otherwise several loop iterations will remove the state\n\t\t * recorded earlier. The goal of these heuristics is to have\n\t\t * states from some iterations of the loop (some in the beginning\n\t\t * and some at the end) to help pruning.\n\t\t */\n\t\tif (add_new_state)\n\t\t\tsl->miss_cnt++;\n\t\t/* heuristic to determine whether this state is beneficial\n\t\t * to keep checking from state equivalence point of view.\n\t\t * Higher numbers increase max_states_per_insn and verification time,\n\t\t * but do not meaningfully decrease insn_processed.\n\t\t */\n\t\tif (sl->miss_cnt > sl->hit_cnt * 3 + 3) {\n\t\t\t/* the state is unlikely to be useful. Remove it to\n\t\t\t * speed up verification\n\t\t\t */\n\t\t\t*pprev = sl->next;\n\t\t\tif (sl->state.frame[0]->regs[0].live & REG_LIVE_DONE) {\n\t\t\t\tu32 br = sl->state.branches;\n\n\t\t\t\tWARN_ONCE(br,\n\t\t\t\t\t  \"BUG live_done but branches_to_explore %d\\n\",\n\t\t\t\t\t  br);\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tenv->peak_states--;\n\t\t\t} else {\n\t\t\t\t/* cannot free this state, since parentage chain may\n\t\t\t\t * walk it later. Add it for free_list instead to\n\t\t\t\t * be freed at the end of verification\n\t\t\t\t */\n\t\t\t\tsl->next = env->free_list;\n\t\t\t\tenv->free_list = sl;\n\t\t\t}\n\t\t\tsl = *pprev;\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tpprev = &sl->next;\n\t\tsl = *pprev;\n\t}\n\n\tif (env->max_states_per_insn < states_cnt)\n\t\tenv->max_states_per_insn = states_cnt;\n\n\tif (!env->bpf_capable && states_cnt > BPF_COMPLEXITY_LIMIT_STATES)\n\t\treturn push_jmp_history(env, cur);\n\n\tif (!add_new_state)\n\t\treturn push_jmp_history(env, cur);\n\n\t/* There were no equivalent states, remember the current one.\n\t * Technically the current state is not proven to be safe yet,\n\t * but it will either reach outer most bpf_exit (which means it's safe)\n\t * or it will be rejected. When there are no loops the verifier won't be\n\t * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx)\n\t * again on the way to bpf_exit.\n\t * When looping the sl->state.branches will be > 0 and this state\n\t * will not be considered for equivalence until branches == 0.\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\tenv->total_states++;\n\tenv->peak_states++;\n\tenv->prev_jmps_processed = env->jmps_processed;\n\tenv->prev_insn_processed = env->insn_processed;\n\n\t/* add new state to the head of linked list */\n\tnew = &new_sl->state;\n\terr = copy_verifier_state(new, cur);\n\tif (err) {\n\t\tfree_verifier_state(new, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew->insn_idx = insn_idx;\n\tWARN_ONCE(new->branches != 1,\n\t\t  \"BUG is_state_visited:branches_to_explore=%d insn %d\\n\", new->branches, insn_idx);\n\n\tcur->parent = new;\n\tcur->first_insn_idx = insn_idx;\n\tclear_jmp_history(cur);\n\tnew_sl->next = *explored_state(env, insn_idx);\n\t*explored_state(env, insn_idx) = new_sl;\n\t/* connect new state to parentage chain. Current frame needs all\n\t * registers connected. Only r6 - r9 of the callers are alive (pushed\n\t * to the stack implicitly by JITs) so in callers' frames connect just\n\t * r6 - r9 as an optimization. Callers will have r1 - r5 connected to\n\t * the state of the call instruction (with WRITTEN set), and r0 comes\n\t * from callee with its full parentage chain, anyway.\n\t */\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tfor (i = j < cur->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].parent = &new->frame[j]->regs[i];\n\t\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].live = REG_LIVE_NONE;\n\t}\n\n\t/* all stack frames are accessible from callee, clear them all */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tstruct bpf_func_state *frame = cur->frame[j];\n\t\tstruct bpf_func_state *newframe = new->frame[j];\n\n\t\tfor (i = 0; i < frame->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tframe->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\t\t\tframe->stack[i].spilled_ptr.parent =\n\t\t\t\t\t\t&newframe->stack[i].spilled_ptr;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Return true if it's OK to have the same insn return a different type. */\nstatic bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n/* If an instruction was previously used with particular pointer types, then we\n * need to be careful to avoid cases such as the below, where it may be ok\n * for one branch accessing the pointer, but not ok for the other branch:\n *\n * R1 = sock_ptr\n * goto X;\n * ...\n * R1 = some_other_valid_ptr;\n * goto X;\n * ...\n * R2 = *(u32 *)(R1 + 0);\n */\nstatic bool reg_type_mismatch(enum bpf_reg_type src, enum bpf_reg_type prev)\n{\n\treturn src != prev && (!reg_type_mismatch_ok(src) ||\n\t\t\t       !reg_type_mismatch_ok(prev));\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 ||\n\t\t    (env->log.level & BPF_LOG_LEVEL && do_print_state)) {\n\t\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe]);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[env->insn_idx].seen = env->pass_cnt;\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn->imm, env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tenv->insn_aux_data[env->insn_idx].seen = env->pass_cnt;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic bool is_tracing_prog_type(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_KPROBE:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool is_preallocated_map(struct bpf_map *map)\n{\n\tif (!check_map_prealloc(map))\n\t\treturn false;\n\tif (map->inner_map_meta && !check_map_prealloc(map->inner_map_meta))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/*\n\t * Validate that trace type programs use preallocated hash maps.\n\t *\n\t * For programs attached to PERF events this is mandatory as the\n\t * perf NMI can hit any arbitrary code sequence.\n\t *\n\t * All other trace types using preallocated hash maps are unsafe as\n\t * well because tracepoint or kprobes can be inside locked regions\n\t * of the memory allocator or at a place where a recursion into the\n\t * memory allocator would see inconsistent state.\n\t *\n\t * On RT enabled kernels run-time allocation of all trace type\n\t * programs is strictly prohibited due to lock type constraints. On\n\t * !RT kernels it is allowed for backwards compatibility reasons for\n\t * now, but warnings are emitted so developers are made aware of\n\t * the unsafety and can fix their programs before this is enforced.\n\t */\n\tif (is_tracing_prog_type(prog->type) && !is_preallocated_map(map)) {\n\t\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\tverbose(env, \"trace type programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tWARN_ONCE(1, \"trace type BPF program uses run-time allocation\\n\");\n\t\tverbose(env, \"trace type programs with run-time allocated hash maps are unsafe. Switch to preallocated hash maps.\\n\");\n\t}\n\n\tif ((is_tracing_prog_type(prog->type) ||\n\t     prog->type == BPF_PROG_TYPE_SOCKET_FILTER) &&\n\t    map_value_has_spin_lock(map)) {\n\t\tverbose(env, \"tracing progs cannot use bpf_spin_lock yet\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((bpf_prog_is_dev_bound(prog->aux) || bpf_map_is_dev_bound(map)) &&\n\t    !bpf_offload_prog_map_match(prog, map)) {\n\t\tverbose(env, \"offload device mismatch between prog and map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\tverbose(env, \"bpf_struct_ops map cannot be used in prog\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool bpf_map_is_cgroup_storage(struct bpf_map *map)\n{\n\treturn (map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||\n\t\tmap->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE);\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_insn_aux_data *aux;\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\t\t\tu64 addr;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn[0].src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\t/* In final convert_pseudo_ld_imm64() step, this is\n\t\t\t * converted into regular 64-bit imm load insn.\n\t\t\t */\n\t\t\tif ((insn[0].src_reg != BPF_PSEUDO_MAP_FD &&\n\t\t\t     insn[0].src_reg != BPF_PSEUDO_MAP_VALUE) ||\n\t\t\t    (insn[0].src_reg == BPF_PSEUDO_MAP_FD &&\n\t\t\t     insn[1].imm != 0)) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn[0].imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn[0].imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\taux = &env->insn_aux_data[i];\n\t\t\tif (insn->src_reg == BPF_PSEUDO_MAP_FD) {\n\t\t\t\taddr = (unsigned long)map;\n\t\t\t} else {\n\t\t\t\tu32 off = insn[1].imm;\n\n\t\t\t\tif (off >= BPF_MAX_VAR_OFF) {\n\t\t\t\t\tverbose(env, \"direct value offset of %u is not allowed\\n\", off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (!map->ops->map_direct_value_addr) {\n\t\t\t\t\tverbose(env, \"no direct value access support for this map type\\n\");\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\t\t\t\tif (err) {\n\t\t\t\t\tverbose(env, \"invalid access to map value pointer, value_size=%u off=%u\\n\",\n\t\t\t\t\t\tmap->value_size, off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\n\t\t\t\taux->map_off = off;\n\t\t\t\taddr += off;\n\t\t\t}\n\n\t\t\tinsn[0].imm = (u32)addr;\n\t\t\tinsn[1].imm = addr >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++) {\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\taux->map_index = j;\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_used_maps()\n\t\t\t */\n\t\t\tbpf_map_inc(map);\n\n\t\t\taux->map_index = env->used_map_cnt;\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tif (bpf_map_is_cgroup_storage(map) &&\n\t\t\t    bpf_cgroup_storage_assign(env->prog->aux, map)) {\n\t\t\t\tverbose(env, \"only one cgroup storage of each type is allowed\\n\");\n\t\t\t\tfdput(f);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Basic sanity check before we invest more work here. */\n\t\tif (!bpf_opcode_in_insntable(insn->code)) {\n\t\t\tverbose(env, \"unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\t__bpf_free_used_maps(env->prog->aux, env->used_maps,\n\t\t\t     env->used_map_cnt);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_prog *new_prog, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tstruct bpf_insn *insn = new_prog->insnsi;\n\tu32 prog_len;\n\tint i;\n\n\t/* aux info at OFF always needs adjustment, no matter fast path\n\t * (cnt == 1) is taken or not. There is no guarantee INSN at OFF is the\n\t * original insn at old prog.\n\t */\n\told_data[off].zext_dst = insn_has_def32(env, insn + off + cnt - 1);\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tprog_len = new_prog->len;\n\tnew_data = vzalloc(array_size(prog_len,\n\t\t\t\t      sizeof(struct bpf_insn_aux_data)));\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++) {\n\t\tnew_data[i].seen = env->pass_cnt;\n\t\tnew_data[i].zext_dst = insn_has_def32(env, insn + i);\n\t}\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic void adjust_subprog_starts(struct bpf_verifier_env *env, u32 off, u32 len)\n{\n\tint i;\n\n\tif (len == 1)\n\t\treturn;\n\t/* NOTE: fake 'exit' subprog should be updated as well. */\n\tfor (i = 0; i <= env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].start <= off)\n\t\t\tcontinue;\n\t\tenv->subprog_info[i].start += len - 1;\n\t}\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (IS_ERR(new_prog)) {\n\t\tif (PTR_ERR(new_prog) == -ERANGE)\n\t\t\tverbose(env,\n\t\t\t\t\"insn %d cannot be patched due to 16-bit range\\n\",\n\t\t\t\tenv->insn_aux_data[off].orig_idx);\n\t\treturn NULL;\n\t}\n\tif (adjust_insn_aux_data(env, new_prog, off, len))\n\t\treturn NULL;\n\tadjust_subprog_starts(env, off, len);\n\treturn new_prog;\n}\n\nstatic int adjust_subprog_starts_after_remove(struct bpf_verifier_env *env,\n\t\t\t\t\t      u32 off, u32 cnt)\n{\n\tint i, j;\n\n\t/* find first prog starting at or after off (first to remove) */\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].start >= off)\n\t\t\tbreak;\n\t/* find first prog starting at or after off + cnt (first to stay) */\n\tfor (j = i; j < env->subprog_cnt; j++)\n\t\tif (env->subprog_info[j].start >= off + cnt)\n\t\t\tbreak;\n\t/* if j doesn't start exactly at off + cnt, we are just removing\n\t * the front of previous prog\n\t */\n\tif (env->subprog_info[j].start != off + cnt)\n\t\tj--;\n\n\tif (j > i) {\n\t\tstruct bpf_prog_aux *aux = env->prog->aux;\n\t\tint move;\n\n\t\t/* move fake 'exit' subprog as well */\n\t\tmove = env->subprog_cnt + 1 - j;\n\n\t\tmemmove(env->subprog_info + i,\n\t\t\tenv->subprog_info + j,\n\t\t\tsizeof(*env->subprog_info) * move);\n\t\tenv->subprog_cnt -= j - i;\n\n\t\t/* remove func_info */\n\t\tif (aux->func_info) {\n\t\t\tmove = aux->func_info_cnt - j;\n\n\t\t\tmemmove(aux->func_info + i,\n\t\t\t\taux->func_info + j,\n\t\t\t\tsizeof(*aux->func_info) * move);\n\t\t\taux->func_info_cnt -= j - i;\n\t\t\t/* func_info->insn_off is set after all code rewrites,\n\t\t\t * in adjust_btf_func() - no need to adjust\n\t\t\t */\n\t\t}\n\t} else {\n\t\t/* convert i from \"first prog to remove\" to \"first to adjust\" */\n\t\tif (env->subprog_info[i].start == off)\n\t\t\ti++;\n\t}\n\n\t/* update fake 'exit' subprog as well */\n\tfor (; i <= env->subprog_cnt; i++)\n\t\tenv->subprog_info[i].start -= cnt;\n\n\treturn 0;\n}\n\nstatic int bpf_adj_linfo_after_remove(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t      u32 cnt)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tu32 i, l_off, l_cnt, nr_linfo;\n\tstruct bpf_line_info *linfo;\n\n\tnr_linfo = prog->aux->nr_linfo;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\tlinfo = prog->aux->linfo;\n\n\t/* find first line info to remove, count lines to be removed */\n\tfor (i = 0; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off >= off)\n\t\t\tbreak;\n\n\tl_off = i;\n\tl_cnt = 0;\n\tfor (; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off < off + cnt)\n\t\t\tl_cnt++;\n\t\telse\n\t\t\tbreak;\n\n\t/* First live insn doesn't match first live linfo, it needs to \"inherit\"\n\t * last removed linfo.  prog is already modified, so prog->len == off\n\t * means no live instructions after (tail of the program was removed).\n\t */\n\tif (prog->len != off && l_cnt &&\n\t    (i == nr_linfo || linfo[i].insn_off != off + cnt)) {\n\t\tl_cnt--;\n\t\tlinfo[--i].insn_off = off + cnt;\n\t}\n\n\t/* remove the line info which refer to the removed instructions */\n\tif (l_cnt) {\n\t\tmemmove(linfo + l_off, linfo + i,\n\t\t\tsizeof(*linfo) * (nr_linfo - i));\n\n\t\tprog->aux->nr_linfo -= l_cnt;\n\t\tnr_linfo = prog->aux->nr_linfo;\n\t}\n\n\t/* pull all linfo[i].insn_off >= off + cnt in by cnt */\n\tfor (i = l_off; i < nr_linfo; i++)\n\t\tlinfo[i].insn_off -= cnt;\n\n\t/* fix up all subprogs (incl. 'exit') which start >= off */\n\tfor (i = 0; i <= env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].linfo_idx > l_off) {\n\t\t\t/* program may have started in the removed region but\n\t\t\t * may not be fully removed\n\t\t\t */\n\t\t\tif (env->subprog_info[i].linfo_idx >= l_off + l_cnt)\n\t\t\t\tenv->subprog_info[i].linfo_idx -= l_cnt;\n\t\t\telse\n\t\t\t\tenv->subprog_info[i].linfo_idx = l_off;\n\t\t}\n\n\treturn 0;\n}\n\nstatic int verifier_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tunsigned int orig_prog_len = env->prog->len;\n\tint err;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\tbpf_prog_offload_remove_insns(env, off, cnt);\n\n\terr = bpf_remove_insns(env->prog, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = adjust_subprog_starts_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = bpf_adj_linfo_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\tmemmove(aux_data + off,\taux_data + off + cnt,\n\t\tsizeof(*aux_data) * (orig_prog_len - off - cnt));\n\n\treturn 0;\n}\n\n/* The verifier does more data flow analysis than llvm and will not\n * explore branches that are dead at run time. Malicious programs can\n * have dead code too. Therefore replace all dead at-run-time code\n * with 'ja -1'.\n *\n * Just nops are not optimal, e.g. if they would sit at the end of the\n * program and through another bug we would manage to jump there, then\n * we'd execute beyond program memory otherwise. Returning exception\n * code also wouldn't work since we can have subprogs where the dead\n * code could be located.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn trap = BPF_JMP_IMM(BPF_JA, 0, 0, -1);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &trap, sizeof(trap));\n\t}\n}\n\nstatic bool insn_is_cond_jump(u8 code)\n{\n\tu8 op;\n\n\tif (BPF_CLASS(code) == BPF_JMP32)\n\t\treturn true;\n\n\tif (BPF_CLASS(code) != BPF_JMP)\n\t\treturn false;\n\n\top = BPF_OP(code);\n\treturn op != BPF_JA && op != BPF_EXIT && op != BPF_CALL;\n}\n\nstatic void opt_hard_wire_dead_code_branches(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (!insn_is_cond_jump(insn->code))\n\t\t\tcontinue;\n\n\t\tif (!aux_data[i + 1].seen)\n\t\t\tja.off = insn->off;\n\t\telse if (!aux_data[i + 1 + insn->off].seen)\n\t\t\tja.off = 0;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\t\tbpf_prog_offload_replace_insn(env, i, &ja);\n\n\t\tmemcpy(insn, &ja, sizeof(ja));\n\t}\n}\n\nstatic int opt_remove_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tint j;\n\n\t\tj = 0;\n\t\twhile (i + j < insn_cnt && !aux_data[i + j].seen)\n\t\t\tj++;\n\t\tif (!j)\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, j);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt = env->prog->len;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_remove_nops(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (memcmp(&insn[i], &ja, sizeof(ja)))\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt--;\n\t\ti--;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_subreg_zext_lo32_rnd_hi32(struct bpf_verifier_env *env,\n\t\t\t\t\t const union bpf_attr *attr)\n{\n\tstruct bpf_insn *patch, zext_patch[2], rnd_hi32_patch[4];\n\tstruct bpf_insn_aux_data *aux = env->insn_aux_data;\n\tint i, patch_len, delta = 0, len = env->prog->len;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_prog *new_prog;\n\tbool rnd_hi32;\n\n\trnd_hi32 = attr->prog_flags & BPF_F_TEST_RND_HI32;\n\tzext_patch[1] = BPF_ZEXT_REG(0);\n\trnd_hi32_patch[1] = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, 0);\n\trnd_hi32_patch[2] = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\trnd_hi32_patch[3] = BPF_ALU64_REG(BPF_OR, 0, BPF_REG_AX);\n\tfor (i = 0; i < len; i++) {\n\t\tint adj_idx = i + delta;\n\t\tstruct bpf_insn insn;\n\n\t\tinsn = insns[adj_idx];\n\t\tif (!aux[adj_idx].zext_dst) {\n\t\t\tu8 code, class;\n\t\t\tu32 imm_rnd;\n\n\t\t\tif (!rnd_hi32)\n\t\t\t\tcontinue;\n\n\t\t\tcode = insn.code;\n\t\t\tclass = BPF_CLASS(code);\n\t\t\tif (insn_no_def(&insn))\n\t\t\t\tcontinue;\n\n\t\t\t/* NOTE: arg \"reg\" (the fourth one) is only used for\n\t\t\t *       BPF_STX which has been ruled out in above\n\t\t\t *       check, it is safe to pass NULL here.\n\t\t\t */\n\t\t\tif (is_reg64(env, &insn, insn.dst_reg, NULL, DST_OP)) {\n\t\t\t\tif (class == BPF_LD &&\n\t\t\t\t    BPF_MODE(code) == BPF_IMM)\n\t\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* ctx load could be transformed into wider load. */\n\t\t\tif (class == BPF_LDX &&\n\t\t\t    aux[adj_idx].ptr_type == PTR_TO_CTX)\n\t\t\t\tcontinue;\n\n\t\t\timm_rnd = get_random_int();\n\t\t\trnd_hi32_patch[0] = insn;\n\t\t\trnd_hi32_patch[1].imm = imm_rnd;\n\t\t\trnd_hi32_patch[3].dst_reg = insn.dst_reg;\n\t\t\tpatch = rnd_hi32_patch;\n\t\t\tpatch_len = 4;\n\t\t\tgoto apply_patch_buffer;\n\t\t}\n\n\t\tif (!bpf_jit_needs_zext())\n\t\t\tcontinue;\n\n\t\tzext_patch[0] = insn;\n\t\tzext_patch[1].dst_reg = insn.dst_reg;\n\t\tzext_patch[1].src_reg = insn.dst_reg;\n\t\tpatch = zext_patch;\n\t\tpatch_len = 2;\napply_patch_buffer:\n\t\tnew_prog = bpf_patch_insn_data(env, adj_idx, patch, patch_len);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\t\tenv->prog = new_prog;\n\t\tinsns = new_prog->insnsi;\n\t\taux = env->insn_aux_data;\n\t\tdelta += patch_len - 1;\n\t}\n\n\treturn 0;\n}\n\n/* convert load instructions that access fields of a context type into a\n * sequence of instructions that access fields of the underlying structure:\n *     struct __sk_buff    -> struct sk_buff\n *     struct bpf_sock_ops -> struct sock\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tu32 target_size, size_default, off;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\n\tif (ops->gen_prologue || env->seen_direct_write) {\n\t\tif (!ops->gen_prologue) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tbpf_convert_ctx_access_t convert_ctx_access;\n\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (type == BPF_WRITE &&\n\t\t    env->insn_aux_data[i + delta].sanitize_stack_off) {\n\t\t\tstruct bpf_insn patch[] = {\n\t\t\t\t/* Sanitize suspicious stack slot with zero.\n\t\t\t\t * There are no memory dependencies for this store,\n\t\t\t\t * since it's only using frame pointer and immediate\n\t\t\t\t * constant of zero\n\t\t\t\t */\n\t\t\t\tBPF_ST_MEM(BPF_DW, BPF_REG_FP,\n\t\t\t\t\t   env->insn_aux_data[i + delta].sanitize_stack_off,\n\t\t\t\t\t   0),\n\t\t\t\t/* the original STX instruction will immediately\n\t\t\t\t * overwrite the same stack slot with appropriate value\n\t\t\t\t */\n\t\t\t\t*insn,\n\t\t\t};\n\n\t\t\tcnt = ARRAY_SIZE(patch);\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patch, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (env->insn_aux_data[i + delta].ptr_type) {\n\t\tcase PTR_TO_CTX:\n\t\t\tif (!ops->convert_ctx_access)\n\t\t\t\tcontinue;\n\t\t\tconvert_ctx_access = ops->convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_SOCKET:\n\t\tcase PTR_TO_SOCK_COMMON:\n\t\t\tconvert_ctx_access = bpf_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_TCP_SOCK:\n\t\t\tconvert_ctx_access = bpf_tcp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_XDP_SOCK:\n\t\t\tconvert_ctx_access = bpf_xdp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_BTF_ID:\n\t\t\tif (type == BPF_READ) {\n\t\t\t\tinsn->code = BPF_LDX | BPF_PROBE_MEM |\n\t\t\t\t\tBPF_SIZE((insn)->code);\n\t\t\t\tenv->prog->aux->num_exentries++;\n\t\t\t} else if (env->prog->type != BPF_PROG_TYPE_STRUCT_OPS) {\n\t\t\t\tverbose(env, \"Writes through BTF pointers are not allowed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tcontinue;\n\t\t}\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tsize_default = bpf_ctx_off_adjust_machine(ctx_field_size);\n\t\toff = insn->off;\n\t\tif (is_narrower_load) {\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(size_default - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tu8 shift = bpf_ctx_narrow_access_offset(\n\t\t\t\toff, size, size_default) * 8;\n\t\t\tif (ctx_field_size <= 4) {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t} else {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1ULL << size * 8) - 1);\n\t\t\t}\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\nstatic int jit_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog, **func, *tmp;\n\tint i, j, subprog_start, subprog_end = 0, len, subprog;\n\tstruct bpf_insn *insn;\n\tvoid *old_bpf_func;\n\tint err, num_exentries;\n\n\tif (env->subprog_cnt <= 1)\n\t\treturn 0;\n\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* Upon error here we cannot fall back to interpreter but\n\t\t * need a hard reject of the program. Thus -EFAULT is\n\t\t * propagated in any case.\n\t\t */\n\t\tsubprog = find_subprog(env, i + insn->imm + 1);\n\t\tif (subprog < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i + insn->imm + 1);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* temporarily remember subprog id inside insn instead of\n\t\t * aux_data, since next loop will split up all insns into funcs\n\t\t */\n\t\tinsn->off = subprog;\n\t\t/* remember original imm in case JIT fails and fallback\n\t\t * to interpreter will be needed\n\t\t */\n\t\tenv->insn_aux_data[i].call_imm = insn->imm;\n\t\t/* point imm to __bpf_call_base+1 from JITs point of view */\n\t\tinsn->imm = 1;\n\t}\n\n\terr = bpf_prog_alloc_jited_linfo(prog);\n\tif (err)\n\t\tgoto out_undo_insn;\n\n\terr = -ENOMEM;\n\tfunc = kcalloc(env->subprog_cnt, sizeof(prog), GFP_KERNEL);\n\tif (!func)\n\t\tgoto out_undo_insn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tsubprog_start = subprog_end;\n\t\tsubprog_end = env->subprog_info[i + 1].start;\n\n\t\tlen = subprog_end - subprog_start;\n\t\t/* BPF_PROG_RUN doesn't call subprogs directly,\n\t\t * hence main prog stats include the runtime of subprogs.\n\t\t * subprogs don't have IDs and not reachable via prog_get_next_id\n\t\t * func[i]->aux->stats will never be accessed and stays NULL\n\t\t */\n\t\tfunc[i] = bpf_prog_alloc_no_stats(bpf_prog_size(len), GFP_USER);\n\t\tif (!func[i])\n\t\t\tgoto out_free;\n\t\tmemcpy(func[i]->insnsi, &prog->insnsi[subprog_start],\n\t\t       len * sizeof(struct bpf_insn));\n\t\tfunc[i]->type = prog->type;\n\t\tfunc[i]->len = len;\n\t\tif (bpf_prog_calc_tag(func[i]))\n\t\t\tgoto out_free;\n\t\tfunc[i]->is_func = 1;\n\t\tfunc[i]->aux->func_idx = i;\n\t\t/* the btf and func_info will be freed only at prog->aux */\n\t\tfunc[i]->aux->btf = prog->aux->btf;\n\t\tfunc[i]->aux->func_info = prog->aux->func_info;\n\n\t\t/* Use bpf_prog_F_tag to indicate functions in stack traces.\n\t\t * Long term would need debug info to populate names\n\t\t */\n\t\tfunc[i]->aux->name[0] = 'F';\n\t\tfunc[i]->aux->stack_depth = env->subprog_info[i].stack_depth;\n\t\tfunc[i]->jit_requested = 1;\n\t\tfunc[i]->aux->linfo = prog->aux->linfo;\n\t\tfunc[i]->aux->nr_linfo = prog->aux->nr_linfo;\n\t\tfunc[i]->aux->jited_linfo = prog->aux->jited_linfo;\n\t\tfunc[i]->aux->linfo_idx = env->subprog_info[i].linfo_idx;\n\t\tnum_exentries = 0;\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t\t    BPF_MODE(insn->code) == BPF_PROBE_MEM)\n\t\t\t\tnum_exentries++;\n\t\t}\n\t\tfunc[i]->aux->num_exentries = num_exentries;\n\t\tfunc[i] = bpf_int_jit_compile(func[i]);\n\t\tif (!func[i]->jited) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\t/* at this point all bpf functions were successfully JITed\n\t * now populate all bpf_calls with correct addresses and\n\t * run last pass of JIT\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tsubprog = insn->off;\n\t\t\tinsn->imm = BPF_CAST_CALL(func[subprog]->bpf_func) -\n\t\t\t\t    __bpf_call_base;\n\t\t}\n\n\t\t/* we use the aux data to keep a list of the start addresses\n\t\t * of the JITed images for each function in the program\n\t\t *\n\t\t * for some architectures, such as powerpc64, the imm field\n\t\t * might not be large enough to hold the offset of the start\n\t\t * address of the callee's JITed image from __bpf_call_base\n\t\t *\n\t\t * in such cases, we can lookup the start address of a callee\n\t\t * by using its subprog id, available from the off field of\n\t\t * the call instruction, as an index for this list\n\t\t */\n\t\tfunc[i]->aux->func = func;\n\t\tfunc[i]->aux->func_cnt = env->subprog_cnt;\n\t}\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\told_bpf_func = func[i]->bpf_func;\n\t\ttmp = bpf_int_jit_compile(func[i]);\n\t\tif (tmp != func[i] || func[i]->bpf_func != old_bpf_func) {\n\t\t\tverbose(env, \"JIT doesn't support bpf-to-bpf calls\\n\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t/* finally lock prog and jit images for all functions and\n\t * populate kallsysm\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tbpf_prog_lock_ro(func[i]);\n\t\tbpf_prog_kallsyms_add(func[i]);\n\t}\n\n\t/* Last step: make now unused interpreter insns from main\n\t * prog consistent for later dump requests, so they can\n\t * later look the same as if they were interpreted only.\n\t */\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = env->insn_aux_data[i].call_imm;\n\t\tsubprog = find_subprog(env, i + insn->off + 1);\n\t\tinsn->imm = subprog;\n\t}\n\n\tprog->jited = 1;\n\tprog->bpf_func = func[0]->bpf_func;\n\tprog->aux->func = func;\n\tprog->aux->func_cnt = env->subprog_cnt;\n\tbpf_prog_free_unused_jited_linfo(prog);\n\treturn 0;\nout_free:\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (func[i])\n\t\t\tbpf_jit_free(func[i]);\n\tkfree(func);\nout_undo_insn:\n\t/* cleanup main prog to be interpreted */\n\tprog->jit_requested = 0;\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = 0;\n\t\tinsn->imm = env->insn_aux_data[i].call_imm;\n\t}\n\tbpf_prog_free_jited_linfo(prog);\n\treturn err;\n}\n\nstatic int fixup_call_args(struct bpf_verifier_env *env)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint i, depth;\n#endif\n\tint err = 0;\n\n\tif (env->prog->jit_requested &&\n\t    !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\terr = jit_subprogs(env);\n\t\tif (err == 0)\n\t\t\treturn 0;\n\t\tif (err == -EFAULT)\n\t\t\treturn err;\n\t}\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tfor (i = 0; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tdepth = get_callee_stack_depth(env, insn, i);\n\t\tif (depth < 0)\n\t\t\treturn depth;\n\t\tbpf_patch_call_args(insn, depth);\n\t}\n\terr = 0;\n#endif\n\treturn err;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn insn_buf[16];\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\tif (issrc) {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,\n\t\t\t\t\t\t\t off_reg);\n\t\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\t} else {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,\n\t\t\t\t\t\t\t BPF_REG_AX);\n\t\t\t}\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tenv->prog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tsl = env->free_list;\n\twhile (sl) {\n\t\tsln = sl->next;\n\t\tfree_verifier_state(&sl->state, false);\n\t\tkfree(sl);\n\t\tsl = sln;\n\t}\n\tenv->free_list = NULL;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < state_htab_size(env); i++) {\n\t\tsl = env->explored_states[i];\n\n\t\twhile (sl) {\n\t\t\tsln = sl->next;\n\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\tkfree(sl);\n\t\t\tsl = sln;\n\t\t}\n\t\tenv->explored_states[i] = NULL;\n\t}\n}\n\n/* The verifier is using insn_aux_data[] to store temporary data during\n * verification and to store information for passes that run after the\n * verification like dead code sanitization. do_check_common() for subprogram N\n * may analyze many other subprograms. sanitize_insn_aux_data() clears all\n * temporary data after do_check_common() finds that subprogram N cannot be\n * verified independently. pass_cnt counts the number of times\n * do_check_common() was run and insn->aux->seen tells the pass number\n * insn_aux_data was touched. These variables are compared to clear temporary\n * data from failed pass. For testing and experiments do_check_common() can be\n * run multiple times even when prior attempt to verify is unsuccessful.\n */\nstatic void sanitize_insn_aux_data(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tstruct bpf_insn_aux_data *aux;\n\tint i, class;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tclass = BPF_CLASS(insn[i].code);\n\t\tif (class != BPF_LDX && class != BPF_STX)\n\t\t\tcontinue;\n\t\taux = &env->insn_aux_data[i];\n\t\tif (aux->seen != env->pass_cnt)\n\t\t\tcontinue;\n\t\tmemset(aux, 0, offsetof(typeof(*aux), orig_idx));\n\t}\n}\n\nstatic int do_check_common(struct bpf_verifier_env *env, int subprog)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_reg_state *regs;\n\tint ret, i;\n\n\tenv->prev_linfo = NULL;\n\tenv->pass_cnt++;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->branches = 1;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\tsubprog);\n\n\tregs = state->frame[state->curframe]->regs;\n\tif (subprog || env->prog->type == BPF_PROG_TYPE_EXT) {\n\t\tret = btf_prepare_func_args(env, subprog, regs);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++) {\n\t\t\tif (regs[i].type == PTR_TO_CTX)\n\t\t\t\tmark_reg_known_zero(env, regs, i);\n\t\t\telse if (regs[i].type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, i);\n\t\t}\n\t} else {\n\t\t/* 1st arg to a function */\n\t\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\t\tmark_reg_known_zero(env, regs, BPF_REG_1);\n\t\tret = btf_check_func_arg_match(env, subprog, regs);\n\t\tif (ret == -EFAULT)\n\t\t\t/* unlikely verifier bug. abort.\n\t\t\t * ret == 0 and ret < 0 are sadly acceptable for\n\t\t\t * main() function due to backward compatibility.\n\t\t\t * Like socket filter program may be written as:\n\t\t\t * int bpf_prog(struct pt_regs *ctx)\n\t\t\t * and never dereference that ctx in the program.\n\t\t\t * 'struct pt_regs' is a type mismatch for socket\n\t\t\t * filter that should be using 'struct __sk_buff'.\n\t\t\t */\n\t\t\tgoto out;\n\t}\n\n\tret = do_check(env);\nout:\n\t/* check for NULL is necessary, since cur_state can be freed inside\n\t * do_check() under memory pressure.\n\t */\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\twhile (!pop_stack(env, NULL, NULL, false));\n\tif (!ret && pop_log)\n\t\tbpf_vlog_reset(&env->log, 0);\n\tfree_states(env);\n\tif (ret)\n\t\t/* clean aux data in case subprog was rejected */\n\t\tsanitize_insn_aux_data(env);\n\treturn ret;\n}\n\n/* Verify all global functions in a BPF program one by one based on their BTF.\n * All global functions must pass verification. Otherwise the whole program is rejected.\n * Consider:\n * int bar(int);\n * int foo(int f)\n * {\n *    return bar(f);\n * }\n * int bar(int b)\n * {\n *    ...\n * }\n * foo() will be verified first for R1=any_scalar_value. During verification it\n * will be assumed that bar() already verified successfully and call to bar()\n * from foo() will be checked for type match only. Later bar() will be verified\n * independently to check that it's safe for R1=any_scalar_value.\n */\nstatic int do_check_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i, ret;\n\n\tif (!aux->func_info)\n\t\treturn 0;\n\n\tfor (i = 1; i < env->subprog_cnt; i++) {\n\t\tif (aux->func_info_aux[i].linkage != BTF_FUNC_GLOBAL)\n\t\t\tcontinue;\n\t\tenv->insn_idx = env->subprog_info[i].start;\n\t\tWARN_ON_ONCE(env->insn_idx == 0);\n\t\tret = do_check_common(env, i);\n\t\tif (ret) {\n\t\t\treturn ret;\n\t\t} else if (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tverbose(env,\n\t\t\t\t\"Func#%d is safe for any args that match its prototype\\n\",\n\t\t\t\ti);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int do_check_main(struct bpf_verifier_env *env)\n{\n\tint ret;\n\n\tenv->insn_idx = 0;\n\tret = do_check_common(env, 0);\n\tif (!ret)\n\t\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\treturn ret;\n}\n\n\nstatic void print_verification_stats(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (env->log.level & BPF_LOG_STATS) {\n\t\tverbose(env, \"verification time %lld usec\\n\",\n\t\t\tdiv_u64(env->verification_time, 1000));\n\t\tverbose(env, \"stack depth \");\n\t\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\t\tu32 depth = env->subprog_info[i].stack_depth;\n\n\t\t\tverbose(env, \"%d\", depth);\n\t\t\tif (i + 1 < env->subprog_cnt)\n\t\t\t\tverbose(env, \"+\");\n\t\t}\n\t\tverbose(env, \"\\n\");\n\t}\n\tverbose(env, \"processed %d insns (limit %d) max_states_per_insn %d \"\n\t\t\"total_states %d peak_states %d mark_read %d\\n\",\n\t\tenv->insn_processed, BPF_COMPLEXITY_LIMIT_INSNS,\n\t\tenv->max_states_per_insn, env->total_states,\n\t\tenv->peak_states, env->longest_mark_read_walk);\n}\n\nstatic int check_struct_ops_btf_id(struct bpf_verifier_env *env)\n{\n\tconst struct btf_type *t, *func_proto;\n\tconst struct bpf_struct_ops *st_ops;\n\tconst struct btf_member *member;\n\tstruct bpf_prog *prog = env->prog;\n\tu32 btf_id, member_idx;\n\tconst char *mname;\n\n\tbtf_id = prog->aux->attach_btf_id;\n\tst_ops = bpf_struct_ops_find(btf_id);\n\tif (!st_ops) {\n\t\tverbose(env, \"attach_btf_id %u is not a supported struct\\n\",\n\t\t\tbtf_id);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = st_ops->type;\n\tmember_idx = prog->expected_attach_type;\n\tif (member_idx >= btf_type_vlen(t)) {\n\t\tverbose(env, \"attach to invalid member idx %u of struct %s\\n\",\n\t\t\tmember_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tmember = &btf_type_member(t)[member_idx];\n\tmname = btf_name_by_offset(btf_vmlinux, member->name_off);\n\tfunc_proto = btf_type_resolve_func_ptr(btf_vmlinux, member->type,\n\t\t\t\t\t       NULL);\n\tif (!func_proto) {\n\t\tverbose(env, \"attach to invalid member %s(@idx %u) of struct %s\\n\",\n\t\t\tmname, member_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (st_ops->check_member) {\n\t\tint err = st_ops->check_member(t, member);\n\n\t\tif (err) {\n\t\t\tverbose(env, \"attach to unsupported member %s of struct %s\\n\",\n\t\t\t\tmname, st_ops->name);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tprog->aux->attach_func_proto = func_proto;\n\tprog->aux->attach_func_name = mname;\n\tenv->ops = st_ops->verifier_ops;\n\n\treturn 0;\n}\n#define SECURITY_PREFIX \"security_\"\n\nstatic int check_attach_modify_return(struct bpf_prog *prog, unsigned long addr)\n{\n\tif (within_error_injection_list(addr) ||\n\t    !strncmp(SECURITY_PREFIX, prog->aux->attach_func_name,\n\t\t     sizeof(SECURITY_PREFIX) - 1))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic int check_attach_btf_id(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool prog_extension = prog->type == BPF_PROG_TYPE_EXT;\n\tstruct bpf_prog *tgt_prog = prog->aux->linked_prog;\n\tu32 btf_id = prog->aux->attach_btf_id;\n\tconst char prefix[] = \"btf_trace_\";\n\tstruct btf_func_model fmodel;\n\tint ret = 0, subprog = -1, i;\n\tstruct bpf_trampoline *tr;\n\tconst struct btf_type *t;\n\tbool conservative = true;\n\tconst char *tname;\n\tstruct btf *btf;\n\tlong addr;\n\tu64 key;\n\n\tif (prog->type == BPF_PROG_TYPE_STRUCT_OPS)\n\t\treturn check_struct_ops_btf_id(env);\n\n\tif (prog->type != BPF_PROG_TYPE_TRACING &&\n\t    prog->type != BPF_PROG_TYPE_LSM &&\n\t    !prog_extension)\n\t\treturn 0;\n\n\tif (!btf_id) {\n\t\tverbose(env, \"Tracing programs must provide btf_id\\n\");\n\t\treturn -EINVAL;\n\t}\n\tbtf = bpf_prog_get_target_btf(prog);\n\tif (!btf) {\n\t\tverbose(env,\n\t\t\t\"FENTRY/FEXIT program can only be attached to another program annotated with BTF\\n\");\n\t\treturn -EINVAL;\n\t}\n\tt = btf_type_by_id(btf, btf_id);\n\tif (!t) {\n\t\tverbose(env, \"attach_btf_id %u is invalid\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\ttname = btf_name_by_offset(btf, t->name_off);\n\tif (!tname) {\n\t\tverbose(env, \"attach_btf_id %u doesn't have a name\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\tif (tgt_prog) {\n\t\tstruct bpf_prog_aux *aux = tgt_prog->aux;\n\n\t\tfor (i = 0; i < aux->func_info_cnt; i++)\n\t\t\tif (aux->func_info[i].type_id == btf_id) {\n\t\t\t\tsubprog = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (subprog == -1) {\n\t\t\tverbose(env, \"Subprog %s doesn't exist\\n\", tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tconservative = aux->func_info_aux[subprog].unreliable;\n\t\tif (prog_extension) {\n\t\t\tif (conservative) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Cannot replace static functions\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!prog->jit_requested) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Extension programs should be JITed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tenv->ops = bpf_verifier_ops[tgt_prog->type];\n\t\t\tprog->expected_attach_type = tgt_prog->expected_attach_type;\n\t\t}\n\t\tif (!tgt_prog->jited) {\n\t\t\tverbose(env, \"Can attach to only JITed progs\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == prog->type) {\n\t\t\t/* Cannot fentry/fexit another fentry/fexit program.\n\t\t\t * Cannot attach program extension to another extension.\n\t\t\t * It's ok to attach fentry/fexit to extension program.\n\t\t\t */\n\t\t\tverbose(env, \"Cannot recursively attach\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_TRACING &&\n\t\t    prog_extension &&\n\t\t    (tgt_prog->expected_attach_type == BPF_TRACE_FENTRY ||\n\t\t     tgt_prog->expected_attach_type == BPF_TRACE_FEXIT)) {\n\t\t\t/* Program extensions can extend all program types\n\t\t\t * except fentry/fexit. The reason is the following.\n\t\t\t * The fentry/fexit programs are used for performance\n\t\t\t * analysis, stats and can be attached to any program\n\t\t\t * type except themselves. When extension program is\n\t\t\t * replacing XDP function it is necessary to allow\n\t\t\t * performance analysis of all functions. Both original\n\t\t\t * XDP program and its program extension. Hence\n\t\t\t * attaching fentry/fexit to BPF_PROG_TYPE_EXT is\n\t\t\t * allowed. If extending of fentry/fexit was allowed it\n\t\t\t * would be possible to create long call chain\n\t\t\t * fentry->extension->fentry->extension beyond\n\t\t\t * reasonable stack size. Hence extending fentry is not\n\t\t\t * allowed.\n\t\t\t */\n\t\t\tverbose(env, \"Cannot extend fentry/fexit\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tkey = ((u64)aux->id) << 32 | btf_id;\n\t} else {\n\t\tif (prog_extension) {\n\t\t\tverbose(env, \"Cannot replace kernel functions\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tkey = btf_id;\n\t}\n\n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_RAW_TP:\n\t\tif (tgt_prog) {\n\t\t\tverbose(env,\n\t\t\t\t\"Only FENTRY/FEXIT progs are attachable to another BPF prog\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!btf_type_is_typedef(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a typedef\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (strncmp(prefix, tname, sizeof(prefix) - 1)) {\n\t\t\tverbose(env, \"attach_btf_id %u points to wrong type name %s\\n\",\n\t\t\t\tbtf_id, tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttname += sizeof(prefix) - 1;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_ptr(t))\n\t\t\t/* should never happen in valid vmlinux build */\n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\t/* should never happen in valid vmlinux build */\n\t\t\treturn -EINVAL;\n\n\t\t/* remember two read only pointers that are valid for\n\t\t * the life time of the kernel\n\t\t */\n\t\tprog->aux->attach_func_name = tname;\n\t\tprog->aux->attach_func_proto = t;\n\t\tprog->aux->attach_btf_trace = true;\n\t\treturn 0;\n\tcase BPF_TRACE_ITER:\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\t\tprog->aux->attach_func_name = tname;\n\t\tprog->aux->attach_func_proto = t;\n\t\tif (!bpf_iter_prog_supported(prog))\n\t\t\treturn -EINVAL;\n\t\tret = btf_distill_func_proto(&env->log, btf, t,\n\t\t\t\t\t     tname, &fmodel);\n\t\treturn ret;\n\tdefault:\n\t\tif (!prog_extension)\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\tcase BPF_MODIFY_RETURN:\n\tcase BPF_LSM_MAC:\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\t\tprog->aux->attach_func_name = tname;\n\t\tif (prog->type == BPF_PROG_TYPE_LSM) {\n\t\t\tret = bpf_lsm_verify_prog(&env->log, prog);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (prog_extension &&\n\t\t    btf_check_type_match(env, prog, btf, t))\n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\t\ttr = bpf_trampoline_lookup(key);\n\t\tif (!tr)\n\t\t\treturn -ENOMEM;\n\t\t/* t is either vmlinux type or another program's type */\n\t\tprog->aux->attach_func_proto = t;\n\t\tmutex_lock(&tr->mutex);\n\t\tif (tr->func.addr) {\n\t\t\tprog->aux->trampoline = tr;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tgt_prog && conservative) {\n\t\t\tprog->aux->attach_func_proto = NULL;\n\t\t\tt = NULL;\n\t\t}\n\t\tret = btf_distill_func_proto(&env->log, btf, t,\n\t\t\t\t\t     tname, &tr->func.model);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (tgt_prog) {\n\t\t\tif (subprog == 0)\n\t\t\t\taddr = (long) tgt_prog->bpf_func;\n\t\t\telse\n\t\t\t\taddr = (long) tgt_prog->aux->func[subprog]->bpf_func;\n\t\t} else {\n\t\t\taddr = kallsyms_lookup_name(tname);\n\t\t\tif (!addr) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"The address of function %s cannot be found\\n\",\n\t\t\t\t\ttname);\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (prog->expected_attach_type == BPF_MODIFY_RETURN) {\n\t\t\tret = check_attach_modify_return(prog, addr);\n\t\t\tif (ret)\n\t\t\t\tverbose(env, \"%s() is not modifiable\\n\",\n\t\t\t\t\tprog->aux->attach_func_name);\n\t\t}\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t\ttr->func.addr = (void *)addr;\n\t\tprog->aux->trampoline = tr;\nout:\n\t\tmutex_unlock(&tr->mutex);\n\t\tif (ret)\n\t\t\tbpf_trampoline_put(tr);\n\t\treturn ret;\n\t}\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr,\n\t      union bpf_attr __user *uattr)\n{\n\tu64 start_time = ktime_get_ns();\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifier_log *log;\n\tint i, len, ret = -EINVAL;\n\tbool is_priv;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tlen = (*prog)->len;\n\tenv->insn_aux_data =\n\t\tvzalloc(array_size(sizeof(struct bpf_insn_aux_data), len));\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tfor (i = 0; i < len; i++)\n\t\tenv->insn_aux_data[i].orig_idx = i;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\tis_priv = bpf_capable();\n\n\tif (!btf_vmlinux && IS_ENABLED(CONFIG_DEBUG_INFO_BTF)) {\n\t\tmutex_lock(&bpf_verifier_lock);\n\t\tif (!btf_vmlinux)\n\t\t\tbtf_vmlinux = btf_parse_vmlinux();\n\t\tmutex_unlock(&bpf_verifier_lock);\n\t}\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tif (!is_priv)\n\t\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 2 ||\n\t\t    !log->level || !log->ubuf || log->level & ~BPF_LOG_MASK)\n\t\t\tgoto err_unlock;\n\t}\n\n\tif (IS_ERR(btf_vmlinux)) {\n\t\t/* Either gcc or pahole or kernel are broken. */\n\t\tverbose(env, \"in-kernel BTF is malformed\\n\");\n\t\tret = PTR_ERR(btf_vmlinux);\n\t\tgoto skip_full_check;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\tif (attr->prog_flags & BPF_F_ANY_ALIGNMENT)\n\t\tenv->strict_alignment = false;\n\n\tenv->allow_ptr_leaks = bpf_allow_ptr_leaks();\n\tenv->allow_ptr_to_map_access = bpf_allow_ptr_to_map_access();\n\tenv->bypass_spec_v1 = bpf_bypass_spec_v1();\n\tenv->bypass_spec_v4 = bpf_bypass_spec_v4();\n\tenv->bpf_capable = bpf_capable();\n\n\tif (is_priv)\n\t\tenv->test_state_freq = attr->prog_flags & BPF_F_TEST_STATE_FREQ;\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = bpf_prog_offload_verifier_prep(env->prog);\n\t\tif (ret)\n\t\t\tgoto skip_full_check;\n\t}\n\n\tenv->explored_states = kvcalloc(state_htab_size(env),\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_subprogs(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_btf_info(env, attr, uattr);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_attach_btf_id(env);\n\tif (ret)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = do_check_subprogs(env);\n\tret = ret ?: do_check_main(env);\n\n\tif (ret == 0 && bpf_prog_is_dev_bound(env->prog->aux))\n\t\tret = bpf_prog_offload_finalize(env);\n\nskip_full_check:\n\tkvfree(env->explored_states);\n\n\tif (ret == 0)\n\t\tret = check_max_stack_depth(env);\n\n\t/* instruction rewrites happen after this point */\n\tif (is_priv) {\n\t\tif (ret == 0)\n\t\t\topt_hard_wire_dead_code_branches(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_dead_code(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_nops(env);\n\t} else {\n\t\tif (ret == 0)\n\t\t\tsanitize_dead_code(env);\n\t}\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\t/* do 32-bit optimization after insn patching has done so those patched\n\t * insns could be handled correctly.\n\t */\n\tif (ret == 0 && !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = opt_subreg_zext_lo32_rnd_hi32(env, attr);\n\t\tenv->prog->aux->verifier_zext = bpf_jit_needs_zext() ? !ret\n\t\t\t\t\t\t\t\t     : false;\n\t}\n\n\tif (ret == 0)\n\t\tret = fixup_call_args(env);\n\n\tenv->verification_time = ktime_get_ns() - start_time;\n\tprint_verification_stats(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\n\tif (ret == 0)\n\t\tadjust_btf_func(env);\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_used_maps() will release them.\n\t\t */\n\t\trelease_maps(env);\n\n\t/* extension progs temporarily inherit the attach_type of their targets\n\t   for verification purposes, so set it back to zero before returning\n\t */\n\tif (env->prog->type == BPF_PROG_TYPE_EXT)\n\t\tenv->prog->expected_attach_type = 0;\n\n\t*prog = env->prog;\nerr_unlock:\n\tif (!is_priv)\n\t\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com\n * Copyright (c) 2016 Facebook\n * Copyright (c) 2018 Covalent IO, Inc. http://covalent.io\n */\n#include <uapi/linux/btf.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/perf_event.h>\n#include <linux/ctype.h>\n#include <linux/error-injection.h>\n#include <linux/bpf_lsm.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n};\n\n/* bpf_check() is a static code analyzer that walks eBPF program\n * instruction by instruction and updates register/stack state.\n * All paths of conditional branches are analyzed until 'bpf_exit' insn.\n *\n * The first pass is depth-first-search to check that the program is a DAG.\n * It rejects the following programs:\n * - larger than BPF_MAXINSNS insns\n * - if loop is present (detected via back-edge)\n * - unreachable insns exist (shouldn't be a forest. program = one function)\n * - out of bounds or malformed jumps\n * The second pass is all possible path descent from the 1st insn.\n * Since it's analyzing all pathes through the program, the length of the\n * analysis is limited to 64k insn, which may be hit even if total number of\n * insn is less then 4K, but there are too many branches that change stack/regs.\n * Number of 'branches to be analyzed' is limited to 1k\n *\n * On entry to each instruction, each register has a type, and the instruction\n * changes the types of the registers depending on instruction semantics.\n * If instruction is BPF_MOV64_REG(BPF_REG_1, BPF_REG_5), then type of R5 is\n * copied to R1.\n *\n * All registers are 64-bit.\n * R0 - return register\n * R1-R5 argument passing registers\n * R6-R9 callee saved registers\n * R10 - frame pointer read-only\n *\n * At the start of BPF program the register R1 contains a pointer to bpf_context\n * and has type PTR_TO_CTX.\n *\n * Verifier tracks arithmetic operations on pointers in case:\n *    BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -20),\n * 1st insn copies R10 (which has FRAME_PTR) type into R1\n * and 2nd arithmetic instruction is pattern matched to recognize\n * that it wants to construct a pointer to some element within stack.\n * So after 2nd insn, the register R1 has type PTR_TO_STACK\n * (and -20 constant is saved for further stack bounds checking).\n * Meaning that this reg is a pointer to stack plus known immediate constant.\n *\n * Most of the time the registers have SCALAR_VALUE type, which\n * means the register has some value, but it's not a valid pointer.\n * (like pointer plus pointer becomes SCALAR_VALUE type)\n *\n * When verifier sees load or store instructions the type of base register\n * can be: PTR_TO_MAP_VALUE, PTR_TO_CTX, PTR_TO_STACK, PTR_TO_SOCKET. These are\n * four pointer types recognized by check_mem_access() function.\n *\n * PTR_TO_MAP_VALUE means that this register is pointing to 'map element value'\n * and the range of [ptr, ptr + map's value_size) is accessible.\n *\n * registers used to pass values to function calls are checked against\n * function argument constraints.\n *\n * ARG_PTR_TO_MAP_KEY is one of such argument constraints.\n * It means that the register type passed to this function must be\n * PTR_TO_STACK and it will be used inside the function as\n * 'pointer to map element key'\n *\n * For example the argument constraints for bpf_map_lookup_elem():\n *   .ret_type = RET_PTR_TO_MAP_VALUE_OR_NULL,\n *   .arg1_type = ARG_CONST_MAP_PTR,\n *   .arg2_type = ARG_PTR_TO_MAP_KEY,\n *\n * ret_type says that this function returns 'pointer to map elem value or null'\n * function expects 1st argument to be a const pointer to 'struct bpf_map' and\n * 2nd argument should be a pointer to stack, which will be used inside\n * the helper function as a pointer to map element key.\n *\n * On the kernel side the helper function looks like:\n * u64 bpf_map_lookup_elem(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n * {\n *    struct bpf_map *map = (struct bpf_map *) (unsigned long) r1;\n *    void *key = (void *) (unsigned long) r2;\n *    void *value;\n *\n *    here kernel can access 'key' and 'map' pointers safely, knowing that\n *    [key, key + map->key_size) bytes are valid and were initialized on\n *    the stack of eBPF program.\n * }\n *\n * Corresponding eBPF program may look like:\n *    BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),  // after this insn R2 type is FRAME_PTR\n *    BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4), // after this insn R2 type is PTR_TO_STACK\n *    BPF_LD_MAP_FD(BPF_REG_1, map_fd),      // after this insn R1 type is CONST_PTR_TO_MAP\n *    BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n * here verifier looks at prototype of map_lookup_elem() and sees:\n * .arg1_type == ARG_CONST_MAP_PTR and R1->type == CONST_PTR_TO_MAP, which is ok,\n * Now verifier knows that this map has key of R1->map_ptr->key_size bytes\n *\n * Then .arg2_type == ARG_PTR_TO_MAP_KEY and R2->type == PTR_TO_STACK, ok so far,\n * Now verifier checks that [R2, R2 + map's key_size) are within stack limits\n * and were initialized prior to this call.\n * If it's ok, then verifier allows this BPF_CALL insn and looks at\n * .ret_type which is RET_PTR_TO_MAP_VALUE_OR_NULL, so it sets\n * R0->type = PTR_TO_MAP_VALUE_OR_NULL which means bpf_map_lookup_elem() function\n * returns ether pointer to map value or NULL.\n *\n * When type PTR_TO_MAP_VALUE_OR_NULL passes through 'if (reg != 0) goto +off'\n * insn, the register holding that pointer in the true branch changes state to\n * PTR_TO_MAP_VALUE and the same register changes state to CONST_IMM in the false\n * branch. See check_cond_jmp_op().\n *\n * After the call R0 is set to return type of the function and registers R1-R5\n * are set to NOT_INIT to indicate that they are no longer readable.\n *\n * The following reference types represent a potential reference to a kernel\n * resource which, after first being allocated, must be checked and freed by\n * the BPF program:\n * - PTR_TO_SOCKET_OR_NULL, PTR_TO_SOCKET\n *\n * When the verifier sees a helper call return a reference type, it allocates a\n * pointer id for the reference and stores it in the current function state.\n * Similar to the way that PTR_TO_MAP_VALUE_OR_NULL is converted into\n * PTR_TO_MAP_VALUE, PTR_TO_SOCKET_OR_NULL becomes PTR_TO_SOCKET when the type\n * passes through a NULL-check conditional. For the branch wherein the state is\n * changed to CONST_IMM, the verifier releases the reference.\n *\n * For each helper function that allocates a reference, such as\n * bpf_sk_lookup_tcp(), there is a corresponding release function, such as\n * bpf_sk_release(). When a reference type passes into the release function,\n * the verifier also releases the reference. If any unchecked or unreleased\n * reference remains at the end of the program, the verifier rejects it.\n */\n\n/* verifier_state + insn_idx are pushed to stack when branch is encountered */\nstruct bpf_verifier_stack_elem {\n\t/* verifer state is 'st'\n\t * before processing instruction 'insn_idx'\n\t * and after processing instruction 'prev_insn_idx'\n\t */\n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n\t/* length of verifier log at the time this state was pushed on stack */\n\tu32 log_pos;\n};\n\n#define BPF_COMPLEXITY_LIMIT_JMP_SEQ\t8192\n#define BPF_COMPLEXITY_LIMIT_STATES\t64\n\n#define BPF_MAP_KEY_POISON\t(1ULL << 63)\n#define BPF_MAP_KEY_SEEN\t(1ULL << 62)\n\n#define BPF_MAP_PTR_UNPRIV\t1UL\n#define BPF_MAP_PTR_POISON\t((void *)((0xeB9FUL << 1) +\t\\\n\t\t\t\t\t  POISON_POINTER_DELTA))\n#define BPF_MAP_PTR(X)\t\t((struct bpf_map *)((X) & ~BPF_MAP_PTR_UNPRIV))\n\nstatic bool bpf_map_ptr_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn BPF_MAP_PTR(aux->map_ptr_state) == BPF_MAP_PTR_POISON;\n}\n\nstatic bool bpf_map_ptr_unpriv(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_ptr_state & BPF_MAP_PTR_UNPRIV;\n}\n\nstatic void bpf_map_ptr_store(struct bpf_insn_aux_data *aux,\n\t\t\t      const struct bpf_map *map, bool unpriv)\n{\n\tBUILD_BUG_ON((unsigned long)BPF_MAP_PTR_POISON & BPF_MAP_PTR_UNPRIV);\n\tunpriv |= bpf_map_ptr_unpriv(aux);\n\taux->map_ptr_state = (unsigned long)map |\n\t\t\t     (unpriv ? BPF_MAP_PTR_UNPRIV : 0UL);\n}\n\nstatic bool bpf_map_key_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & BPF_MAP_KEY_POISON;\n}\n\nstatic bool bpf_map_key_unseen(const struct bpf_insn_aux_data *aux)\n{\n\treturn !(aux->map_key_state & BPF_MAP_KEY_SEEN);\n}\n\nstatic u64 bpf_map_key_immediate(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & ~(BPF_MAP_KEY_SEEN | BPF_MAP_KEY_POISON);\n}\n\nstatic void bpf_map_key_store(struct bpf_insn_aux_data *aux, u64 state)\n{\n\tbool poisoned = bpf_map_key_poisoned(aux);\n\n\taux->map_key_state = state | BPF_MAP_KEY_SEEN |\n\t\t\t     (poisoned ? BPF_MAP_KEY_POISON : 0ULL);\n}\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tint regno;\n\tint access_size;\n\tint mem_size;\n\tu64 msize_max_value;\n\tint ref_obj_id;\n\tint func_id;\n\tu32 btf_id;\n};\n\nstruct btf *btf_vmlinux;\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\nstatic const struct bpf_line_info *\nfind_linfo(const struct bpf_verifier_env *env, u32 insn_off)\n{\n\tconst struct bpf_line_info *linfo;\n\tconst struct bpf_prog *prog;\n\tu32 i, nr_linfo;\n\n\tprog = env->prog;\n\tnr_linfo = prog->aux->nr_linfo;\n\n\tif (!nr_linfo || insn_off >= prog->len)\n\t\treturn NULL;\n\n\tlinfo = prog->aux->linfo;\n\tfor (i = 1; i < nr_linfo; i++)\n\t\tif (insn_off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\treturn &linfo[i - 1];\n}\n\nvoid bpf_verifier_vlog(struct bpf_verifier_log *log, const char *fmt,\n\t\t       va_list args)\n{\n\tunsigned int n;\n\n\tn = vscnprintf(log->kbuf, BPF_VERIFIER_TMP_LOG_SIZE, fmt, args);\n\n\tWARN_ONCE(n >= BPF_VERIFIER_TMP_LOG_SIZE - 1,\n\t\t  \"verifier log line truncated - local buffer too short\\n\");\n\n\tn = min(log->len_total - log->len_used - 1, n);\n\tlog->kbuf[n] = '\\0';\n\n\tif (log->level == BPF_LOG_KERNEL) {\n\t\tpr_err(\"BPF:%s\\n\", log->kbuf);\n\t\treturn;\n\t}\n\tif (!copy_to_user(log->ubuf + log->len_used, log->kbuf, n + 1))\n\t\tlog->len_used += n;\n\telse\n\t\tlog->ubuf = NULL;\n}\n\nstatic void bpf_vlog_reset(struct bpf_verifier_log *log, u32 new_pos)\n{\n\tchar zero = 0;\n\n\tif (!bpf_verifier_log_needed(log))\n\t\treturn;\n\n\tlog->len_used = new_pos;\n\tif (put_user(zero, log->ubuf + new_pos))\n\t\tlog->ubuf = NULL;\n}\n\n/* log_level controls verbosity level of eBPF verifier.\n * bpf_verifier_log_write() is used to dump the verification trace to the log,\n * so the user can figure out what's wrong with the program\n */\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\nEXPORT_SYMBOL_GPL(bpf_verifier_log_write);\n\n__printf(2, 3) static void verbose(void *private_data, const char *fmt, ...)\n{\n\tstruct bpf_verifier_env *env = private_data;\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\n\n__printf(2, 3) void bpf_log(struct bpf_verifier_log *log,\n\t\t\t    const char *fmt, ...)\n{\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(log, fmt, args);\n\tva_end(args);\n}\n\nstatic const char *ltrim(const char *s)\n{\n\twhile (isspace(*s))\n\t\ts++;\n\n\treturn s;\n}\n\n__printf(3, 4) static void verbose_linfo(struct bpf_verifier_env *env,\n\t\t\t\t\t u32 insn_off,\n\t\t\t\t\t const char *prefix_fmt, ...)\n{\n\tconst struct bpf_line_info *linfo;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tlinfo = find_linfo(env, insn_off);\n\tif (!linfo || linfo == env->prev_linfo)\n\t\treturn;\n\n\tif (prefix_fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, prefix_fmt);\n\t\tbpf_verifier_vlog(&env->log, prefix_fmt, args);\n\t\tva_end(args);\n\t}\n\n\tverbose(env, \"%s\\n\",\n\t\tltrim(btf_name_by_offset(env->prog->aux->btf,\n\t\t\t\t\t linfo->line_off)));\n\n\tenv->prev_linfo = linfo;\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\nstatic bool type_is_sk_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCK_COMMON ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_XDP_SOCK;\n}\n\nstatic bool reg_type_not_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_MAP_VALUE ||\n\t\ttype == PTR_TO_SOCK_COMMON;\n}\n\nstatic bool reg_type_may_be_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_MAP_VALUE_OR_NULL ||\n\t       type == PTR_TO_SOCKET_OR_NULL ||\n\t       type == PTR_TO_SOCK_COMMON_OR_NULL ||\n\t       type == PTR_TO_TCP_SOCK_OR_NULL ||\n\t       type == PTR_TO_BTF_ID_OR_NULL ||\n\t       type == PTR_TO_MEM_OR_NULL ||\n\t       type == PTR_TO_RDONLY_BUF_OR_NULL ||\n\t       type == PTR_TO_RDWR_BUF_OR_NULL;\n}\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn reg->type == PTR_TO_MAP_VALUE &&\n\t\tmap_value_has_spin_lock(reg->map_ptr);\n}\n\nstatic bool reg_type_may_be_refcounted_or_null(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCKET_OR_NULL ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_TCP_SOCK_OR_NULL ||\n\t\ttype == PTR_TO_MEM ||\n\t\ttype == PTR_TO_MEM_OR_NULL;\n}\n\nstatic bool arg_type_may_be_refcounted(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_SOCK_COMMON;\n}\n\n/* Determine whether the function releases some resources allocated by another\n * function call. The first reference type argument will be assumed to be\n * released by release_reference().\n */\nstatic bool is_release_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_release ||\n\t       func_id == BPF_FUNC_ringbuf_submit ||\n\t       func_id == BPF_FUNC_ringbuf_discard;\n}\n\nstatic bool may_be_acquire_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_lookup_tcp ||\n\t\tfunc_id == BPF_FUNC_sk_lookup_udp ||\n\t\tfunc_id == BPF_FUNC_skc_lookup_tcp ||\n\t\tfunc_id == BPF_FUNC_map_lookup_elem ||\n\t        func_id == BPF_FUNC_ringbuf_reserve;\n}\n\nstatic bool is_acquire_function(enum bpf_func_id func_id,\n\t\t\t\tconst struct bpf_map *map)\n{\n\tenum bpf_map_type map_type = map ? map->map_type : BPF_MAP_TYPE_UNSPEC;\n\n\tif (func_id == BPF_FUNC_sk_lookup_tcp ||\n\t    func_id == BPF_FUNC_sk_lookup_udp ||\n\t    func_id == BPF_FUNC_skc_lookup_tcp ||\n\t    func_id == BPF_FUNC_ringbuf_reserve)\n\t\treturn true;\n\n\tif (func_id == BPF_FUNC_map_lookup_elem &&\n\t    (map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t     map_type == BPF_MAP_TYPE_SOCKHASH))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool is_ptr_cast_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_tcp_sock ||\n\t\tfunc_id == BPF_FUNC_sk_fullsock;\n}\n\n/* string representation of 'enum bpf_reg_type' */\nstatic const char * const reg_type_str[] = {\n\t[NOT_INIT]\t\t= \"?\",\n\t[SCALAR_VALUE]\t\t= \"inv\",\n\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t[PTR_TO_MAP_VALUE_OR_NULL] = \"map_value_or_null\",\n\t[PTR_TO_STACK]\t\t= \"fp\",\n\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n\t[PTR_TO_FLOW_KEYS]\t= \"flow_keys\",\n\t[PTR_TO_SOCKET]\t\t= \"sock\",\n\t[PTR_TO_SOCKET_OR_NULL] = \"sock_or_null\",\n\t[PTR_TO_SOCK_COMMON]\t= \"sock_common\",\n\t[PTR_TO_SOCK_COMMON_OR_NULL] = \"sock_common_or_null\",\n\t[PTR_TO_TCP_SOCK]\t= \"tcp_sock\",\n\t[PTR_TO_TCP_SOCK_OR_NULL] = \"tcp_sock_or_null\",\n\t[PTR_TO_TP_BUFFER]\t= \"tp_buffer\",\n\t[PTR_TO_XDP_SOCK]\t= \"xdp_sock\",\n\t[PTR_TO_BTF_ID]\t\t= \"ptr_\",\n\t[PTR_TO_BTF_ID_OR_NULL]\t= \"ptr_or_null_\",\n\t[PTR_TO_MEM]\t\t= \"mem\",\n\t[PTR_TO_MEM_OR_NULL]\t= \"mem_or_null\",\n\t[PTR_TO_RDONLY_BUF]\t= \"rdonly_buf\",\n\t[PTR_TO_RDONLY_BUF_OR_NULL] = \"rdonly_buf_or_null\",\n\t[PTR_TO_RDWR_BUF]\t= \"rdwr_buf\",\n\t[PTR_TO_RDWR_BUF_OR_NULL] = \"rdwr_buf_or_null\",\n};\n\nstatic char slot_type_char[] = {\n\t[STACK_INVALID]\t= '?',\n\t[STACK_SPILL]\t= 'r',\n\t[STACK_MISC]\t= 'm',\n\t[STACK_ZERO]\t= '0',\n};\n\nstatic void print_liveness(struct bpf_verifier_env *env,\n\t\t\t   enum bpf_reg_liveness live)\n{\n\tif (live & (REG_LIVE_READ | REG_LIVE_WRITTEN | REG_LIVE_DONE))\n\t    verbose(env, \"_\");\n\tif (live & REG_LIVE_READ)\n\t\tverbose(env, \"r\");\n\tif (live & REG_LIVE_WRITTEN)\n\t\tverbose(env, \"w\");\n\tif (live & REG_LIVE_DONE)\n\t\tverbose(env, \"D\");\n}\n\nstatic struct bpf_func_state *func(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[reg->frameno];\n}\n\nconst char *kernel_type_name(u32 id)\n{\n\treturn btf_name_by_offset(btf_vmlinux,\n\t\t\t\t  btf_type_by_id(btf_vmlinux, id)->name_off);\n}\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t/* reg->off should be 0 for SCALAR_VALUE */\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tif (t == PTR_TO_BTF_ID || t == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t\tverbose(env, \"%s\", kernel_type_name(reg->btf_id));\n\t\t\tverbose(env, \"(id=%d\", reg->id);\n\t\t\tif (reg_type_may_be_refcounted_or_null(t))\n\t\t\t\tverbose(env, \",ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose(env, \",off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose(env, \",r=%d\", reg->range);\n\t\t\telse if (t == CONST_PTR_TO_MAP ||\n\t\t\t\t t == PTR_TO_MAP_VALUE ||\n\t\t\t\t t == PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t\tverbose(env, \",ks=%d,vs=%d\",\n\t\t\t\t\treg->map_ptr->key_size,\n\t\t\t\t\treg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t/* Typically an immediate SCALAR_VALUE, but\n\t\t\t\t * could be a pointer whose offset is too big\n\t\t\t\t * for reg->off\n\t\t\t\t */\n\t\t\t\tverbose(env, \",imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose(env, \",smin_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose(env, \",smax_value=%lld\",\n\t\t\t\t\t\t(long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose(env, \",umin_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose(env, \",umax_value=%llu\",\n\t\t\t\t\t\t(unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose(env, \",var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose(env, \",s32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose(env, \",s32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose(env, \",u32_min_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose(env, \",u32_max_value=%d\",\n\t\t\t\t\t\t(int)(reg->u32_max_value));\n\t\t\t}\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[\n\t\t\t\t\tstate->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\tprint_liveness(env, state->stack[i].spilled_ptr.live);\n\t\tif (state->stack[i].slot_type[0] == STACK_SPILL) {\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\t\t\tverbose(env, \"=%s\", reg_type_str[t]);\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tverbose(env, \"\\n\");\n}\n\n#define COPY_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\t\\\nstatic int copy_##NAME##_state(struct bpf_func_state *dst,\t\t\\\n\t\t\t       const struct bpf_func_state *src)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (!src->FIELD)\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\tif (WARN_ON_ONCE(dst->COUNT < src->COUNT)) {\t\t\t\\\n\t\t/* internal bug, make state invalid to reject the program */ \\\n\t\tmemset(dst, 0, sizeof(*dst));\t\t\t\t\\\n\t\treturn -EFAULT;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tmemcpy(dst->FIELD, src->FIELD,\t\t\t\t\t\\\n\t       sizeof(*src->FIELD) * (src->COUNT / SIZE));\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* copy_reference_state() */\nCOPY_STATE_FN(reference, acquired_refs, refs, 1)\n/* copy_stack_state() */\nCOPY_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef COPY_STATE_FN\n\n#define REALLOC_STATE_FN(NAME, COUNT, FIELD, SIZE)\t\t\t\\\nstatic int realloc_##NAME##_state(struct bpf_func_state *state, int size, \\\n\t\t\t\t  bool copy_old)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tu32 old_size = state->COUNT;\t\t\t\t\t\\\n\tstruct bpf_##NAME##_state *new_##FIELD;\t\t\t\t\\\n\tint slot = size / SIZE;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (size <= old_size || !size) {\t\t\t\t\\\n\t\tif (copy_old)\t\t\t\t\t\t\\\n\t\t\treturn 0;\t\t\t\t\t\\\n\t\tstate->COUNT = slot * SIZE;\t\t\t\t\\\n\t\tif (!size && old_size) {\t\t\t\t\\\n\t\t\tkfree(state->FIELD);\t\t\t\t\\\n\t\t\tstate->FIELD = NULL;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tnew_##FIELD = kmalloc_array(slot, sizeof(struct bpf_##NAME##_state), \\\n\t\t\t\t    GFP_KERNEL);\t\t\t\\\n\tif (!new_##FIELD)\t\t\t\t\t\t\\\n\t\treturn -ENOMEM;\t\t\t\t\t\t\\\n\tif (copy_old) {\t\t\t\t\t\t\t\\\n\t\tif (state->FIELD)\t\t\t\t\t\\\n\t\t\tmemcpy(new_##FIELD, state->FIELD,\t\t\\\n\t\t\t       sizeof(*new_##FIELD) * (old_size / SIZE)); \\\n\t\tmemset(new_##FIELD + old_size / SIZE, 0,\t\t\\\n\t\t       sizeof(*new_##FIELD) * (size - old_size) / SIZE); \\\n\t}\t\t\t\t\t\t\t\t\\\n\tstate->COUNT = slot * SIZE;\t\t\t\t\t\\\n\tkfree(state->FIELD);\t\t\t\t\t\t\\\n\tstate->FIELD = new_##FIELD;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n/* realloc_reference_state() */\nREALLOC_STATE_FN(reference, acquired_refs, refs, 1)\n/* realloc_stack_state() */\nREALLOC_STATE_FN(stack, allocated_stack, stack, BPF_REG_SIZE)\n#undef REALLOC_STATE_FN\n\n/* do_check() starts with zero-sized stack in struct bpf_verifier_state to\n * make it consume minimal amount of memory. check_stack_write() access from\n * the program calls into realloc_func_state() to grow the stack size.\n * Note there is a non-zero 'parent' pointer inside bpf_verifier_state\n * which realloc_stack_state() copies over. It points to previous\n * bpf_verifier_state which is never reallocated.\n */\nstatic int realloc_func_state(struct bpf_func_state *state, int stack_size,\n\t\t\t      int refs_size, bool copy_old)\n{\n\tint err = realloc_reference_state(state, refs_size, copy_old);\n\tif (err)\n\t\treturn err;\n\treturn realloc_stack_state(state, stack_size, copy_old);\n}\n\n/* Acquire a pointer id from the env and update the state->refs to include\n * this new pointer reference.\n * On success, returns a valid pointer id to associate with the register\n * On failure, returns a negative errno.\n */\nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint new_ofs = state->acquired_refs;\n\tint id, err;\n\n\terr = realloc_reference_state(state, state->acquired_refs + 1, true);\n\tif (err)\n\t\treturn err;\n\tid = ++env->id_gen;\n\tstate->refs[new_ofs].id = id;\n\tstate->refs[new_ofs].insn_idx = insn_idx;\n\n\treturn id;\n}\n\n/* release function corresponding to acquire_reference_state(). Idempotent. */\nstatic int release_reference_state(struct bpf_func_state *state, int ptr_id)\n{\n\tint i, last_idx;\n\n\tlast_idx = state->acquired_refs - 1;\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id == ptr_id) {\n\t\t\tif (last_idx && i != last_idx)\n\t\t\t\tmemcpy(&state->refs[i], &state->refs[last_idx],\n\t\t\t\t       sizeof(*state->refs));\n\t\t\tmemset(&state->refs[last_idx], 0, sizeof(*state->refs));\n\t\t\tstate->acquired_refs--;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic int transfer_reference_state(struct bpf_func_state *dst,\n\t\t\t\t    struct bpf_func_state *src)\n{\n\tint err = realloc_reference_state(dst, src->acquired_refs, false);\n\tif (err)\n\t\treturn err;\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic void free_func_state(struct bpf_func_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tkfree(state->refs);\n\tkfree(state->stack);\n\tkfree(state);\n}\n\nstatic void clear_jmp_history(struct bpf_verifier_state *state)\n{\n\tkfree(state->jmp_history);\n\tstate->jmp_history = NULL;\n\tstate->jmp_history_cnt = 0;\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tint i;\n\n\tfor (i = 0; i <= state->curframe; i++) {\n\t\tfree_func_state(state->frame[i]);\n\t\tstate->frame[i] = NULL;\n\t}\n\tclear_jmp_history(state);\n\tif (free_self)\n\t\tkfree(state);\n}\n\n/* copy verifier state from src to dst growing dst stack space\n * when necessary to accommodate larger src stack\n */\nstatic int copy_func_state(struct bpf_func_state *dst,\n\t\t\t   const struct bpf_func_state *src)\n{\n\tint err;\n\n\terr = realloc_func_state(dst, src->allocated_stack, src->acquired_refs,\n\t\t\t\t false);\n\tif (err)\n\t\treturn err;\n\tmemcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tu32 jmp_sz = sizeof(struct bpf_idx_pair) * src->jmp_history_cnt;\n\tint i, err;\n\n\tif (dst_state->jmp_history_cnt < src->jmp_history_cnt) {\n\t\tkfree(dst_state->jmp_history);\n\t\tdst_state->jmp_history = kmalloc(jmp_sz, GFP_USER);\n\t\tif (!dst_state->jmp_history)\n\t\t\treturn -ENOMEM;\n\t}\n\tmemcpy(dst_state->jmp_history, src->jmp_history, jmp_sz);\n\tdst_state->jmp_history_cnt = src->jmp_history_cnt;\n\n\t/* if dst has more stack frames then src frame, free them */\n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->curframe = src->curframe;\n\tdst_state->active_spin_lock = src->active_spin_lock;\n\tdst_state->branches = src->branches;\n\tdst_state->parent = src->parent;\n\tdst_state->first_insn_idx = src->first_insn_idx;\n\tdst_state->last_insn_idx = src->last_insn_idx;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic void update_branch_counts(struct bpf_verifier_env *env, struct bpf_verifier_state *st)\n{\n\twhile (st) {\n\t\tu32 br = --st->branches;\n\n\t\t/* WARN_ON(br > 1) technically makes sense here,\n\t\t * but see comment in push_stack(), hence:\n\t\t */\n\t\tWARN_ONCE((int)br < 0,\n\t\t\t  \"BUG update_branch_counts:branches_to_explore=%d\\n\",\n\t\t\t  br);\n\t\tif (br)\n\t\t\tbreak;\n\t\tst = st->parent;\n\t}\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx, bool pop_log)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (pop_log)\n\t\tbpf_vlog_reset(&env->log, head->log_pos);\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\telem->log_pos = env->log.len_used;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_JMP_SEQ) {\n\t\tverbose(env, \"The sequence of %d jumps is too complex.\\n\",\n\t\t\tenv->stack_size);\n\t\tgoto err;\n\t}\n\tif (elem->st.parent) {\n\t\t++elem->st.parent->branches;\n\t\t/* WARN_ON(branches > 2) technically makes sense here,\n\t\t * but\n\t\t * 1. speculative states will bump 'branches' for non-branch\n\t\t * instructions\n\t\t * 2. is_state_visited() heuristics may decide not to create\n\t\t * a new state for a sequence of branches and all such current\n\t\t * and cloned states will be pointing to a single parent state\n\t\t * which might have large 'branches' count.\n\t\t */\n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t/* pop all elements and return */\n\twhile (!pop_stack(env, NULL, NULL, false));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\n/* Mark the unknown part of a register (variable offset or scalar value) as\n * known to have the value @imm.\n */\nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\t/* Clear id, off, and union(map_ptr, range) */\n\tmemset(((u8 *)reg) + sizeof(reg->type), 0,\n\t       offsetof(struct bpf_reg_state, var_off) - sizeof(reg->type));\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\nstatic void __mark_reg32_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->var_off = tnum_const_subreg(reg->var_off, imm);\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\n/* Mark the 'variable offset' part of a register as zero.  This should be\n * used only on registers holding a pointer type.\n */\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void __mark_reg_const_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n\treg->type = SCALAR_VALUE;\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\n/* Unmodified PTR_TO_PACKET[_META,_END] register from ctx access. */\nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t/* The register can already have a range from prior markings.\n\t * This is fine as long as it hasn't been advanced from its\n\t * origin.\n\t */\n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n/* Reset the min/max bounds of a register */\nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __mark_reg64_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\nstatic void __mark_reg32_unbounded(struct bpf_reg_state *reg)\n{\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __update_reg32_bounds(struct bpf_reg_state *reg)\n{\n\tstruct tnum var32_off = tnum_subreg(reg->var_off);\n\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->s32_min_value = max_t(s32, reg->s32_min_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->s32_max_value = min_t(s32, reg->s32_max_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MAX));\n\treg->u32_min_value = max_t(u32, reg->u32_min_value, (u32)var32_off.value);\n\treg->u32_max_value = min(reg->u32_max_value,\n\t\t\t\t (u32)(var32_off.value | var32_off.mask));\n}\n\nstatic void __update_reg64_bounds(struct bpf_reg_state *reg)\n{\n\t/* min signed is max(sign bit) | min(other bits) */\n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t/* max signed is min(sign bit) | max(other bits) */\n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t__update_reg32_bounds(reg);\n\t__update_reg64_bounds(reg);\n}\n\n/* Uses signed min/max values to inform unsigned, and vice-versa */\nstatic void __reg32_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->s32_min_value >= 0 || reg->s32_max_value < 0) {\n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s32)reg->u32_max_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->s32_min_value = reg->u32_min_value;\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t} else if ((s32)reg->u32_min_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value;\n\t}\n}\n\nstatic void __reg64_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t/* Learn sign from signed bounds.\n\t * If we cannot cross the sign boundary, then signed and unsigned bounds\n\t * are the same, so combine.  This works even in the negative case, e.g.\n\t * -3 s<= x s<= -1 implies 0xf...fd u<= x u<= 0xf...ff.\n\t */\n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t/* Learn sign from unsigned bounds.  Signed bounds cross the sign\n\t * boundary, so we must be careful.\n\t */\n\tif ((s64)reg->umax_value >= 0) {\n\t\t/* Positive.  We can't learn anything from the smin, but smax\n\t\t * is positive, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t/* Negative.  We can't learn anything from the smax, but smin\n\t\t * is negative, hence safe.\n\t\t */\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t__reg32_deduce_bounds(reg);\n\t__reg64_deduce_bounds(reg);\n}\n\n/* Attempts to improve var_off based on unsigned min/max information */\nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\tstruct tnum var64_off = tnum_intersect(reg->var_off,\n\t\t\t\t\t       tnum_range(reg->umin_value,\n\t\t\t\t\t\t\t  reg->umax_value));\n\tstruct tnum var32_off = tnum_intersect(tnum_subreg(reg->var_off),\n\t\t\t\t\t\ttnum_range(reg->u32_min_value,\n\t\t\t\t\t\t\t   reg->u32_max_value));\n\n\treg->var_off = tnum_or(tnum_clear_subreg(var64_off), var32_off);\n}\n\nstatic void __reg_assign_32_into_64(struct bpf_reg_state *reg)\n{\n\treg->umin_value = reg->u32_min_value;\n\treg->umax_value = reg->u32_max_value;\n\t/* Attempt to pull 32-bit signed bounds into 64-bit bounds\n\t * but must be positive otherwise set to worse case bounds\n\t * and refine later from tnum.\n\t */\n\tif (reg->s32_min_value >= 0 && reg->s32_max_value >= 0)\n\t\treg->smax_value = reg->s32_max_value;\n\telse\n\t\treg->smax_value = U32_MAX;\n\tif (reg->s32_min_value >= 0)\n\t\treg->smin_value = reg->s32_min_value;\n\telse\n\t\treg->smin_value = 0;\n}\n\nstatic void __reg_combine_32_into_64(struct bpf_reg_state *reg)\n{\n\t/* special case when 64-bit register has upper 32-bit register\n\t * zeroed. Typically happens after zext or <<32, >>32 sequence\n\t * allowing us to use 32-bit bounds directly,\n\t */\n\tif (tnum_equals_const(tnum_clear_subreg(reg->var_off), 0)) {\n\t\t__reg_assign_32_into_64(reg);\n\t} else {\n\t\t/* Otherwise the best we can do is push lower 32bit known and\n\t\t * unknown bits into register (var_off set from jmp logic)\n\t\t * then learn as much as possible from the 64-bit tnum\n\t\t * known and unknown bits. The previous smin/smax bounds are\n\t\t * invalid here because of jmp32 compare so mark them unknown\n\t\t * so they do not impact tnum bounds calculation.\n\t\t */\n\t\t__mark_reg64_unbounded(reg);\n\t\t__update_reg_bounds(reg);\n\t}\n\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__reg_deduce_bounds(reg);\n\t__reg_bound_offset(reg);\n\t__update_reg_bounds(reg);\n}\n\nstatic bool __reg64_bound_s32(s64 a)\n{\n\tif (a > S32_MIN && a < S32_MAX)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool __reg64_bound_u32(u64 a)\n{\n\tif (a > U32_MIN && a < U32_MAX)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void __reg_combine_64_into_32(struct bpf_reg_state *reg)\n{\n\t__mark_reg32_unbounded(reg);\n\n\tif (__reg64_bound_s32(reg->smin_value))\n\t\treg->s32_min_value = (s32)reg->smin_value;\n\tif (__reg64_bound_s32(reg->smax_value))\n\t\treg->s32_max_value = (s32)reg->smax_value;\n\tif (__reg64_bound_u32(reg->umin_value))\n\t\treg->u32_min_value = (u32)reg->umin_value;\n\tif (__reg64_bound_u32(reg->umax_value))\n\t\treg->u32_max_value = (u32)reg->umax_value;\n\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__reg_deduce_bounds(reg);\n\t__reg_bound_offset(reg);\n\t__update_reg_bounds(reg);\n}\n\n/* Mark a register as having a completely unknown (scalar) value. */\nstatic void __mark_reg_unknown(const struct bpf_verifier_env *env,\n\t\t\t       struct bpf_reg_state *reg)\n{\n\t/*\n\t * Clear type, id, off, and union(map_ptr, range) and\n\t * padding between 'type' and union\n\t */\n\tmemset(reg, 0, offsetof(struct bpf_reg_state, var_off));\n\treg->type = SCALAR_VALUE;\n\treg->var_off = tnum_unknown;\n\treg->frameno = 0;\n\treg->precise = env->subprog_cnt > 1 || !env->bpf_capable;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(env, regs + regno);\n}\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(env, reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs except FP */\n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(env, regs + regno);\n}\n\nstatic void mark_btf_ld_reg(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_reg_state *regs, u32 regno,\n\t\t\t    enum bpf_reg_type reg_type, u32 btf_id)\n{\n\tif (reg_type == SCALAR_VALUE) {\n\t\tmark_reg_unknown(env, regs, regno);\n\t\treturn;\n\t}\n\tmark_reg_known_zero(env, regs, regno);\n\tregs[regno].type = PTR_TO_BTF_ID;\n\tregs[regno].btf_id = btf_id;\n}\n\n#define DEF_NOT_SUBREG\t(0)\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t\tregs[i].parent = NULL;\n\t\tregs[i].subreg_def = DEF_NOT_SUBREG;\n\t}\n\n\t/* frame pointer */\n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\tregs[BPF_REG_FP].frameno = state->frameno;\n}\n\n#define BPF_MAIN_FUNC (-1)\nstatic void init_func_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *state,\n\t\t\t    int callsite, int frameno, int subprogno)\n{\n\tstate->callsite = callsite;\n\tstate->frameno = frameno;\n\tstate->subprogno = subprogno;\n\tinit_reg_state(env, state);\n}\n\nenum reg_arg_type {\n\tSRC_OP,\t\t/* register is used as source operand */\n\tDST_OP,\t\t/* register is used as destination operand */\n\tDST_OP_NO_MARK\t/* same as above, check only, don't mark */\n};\n\nstatic int cmp_subprogs(const void *a, const void *b)\n{\n\treturn ((struct bpf_subprog_info *)a)->start -\n\t       ((struct bpf_subprog_info *)b)->start;\n}\n\nstatic int find_subprog(struct bpf_verifier_env *env, int off)\n{\n\tstruct bpf_subprog_info *p;\n\n\tp = bsearch(&off, env->subprog_info, env->subprog_cnt,\n\t\t    sizeof(env->subprog_info[0]), cmp_subprogs);\n\tif (!p)\n\t\treturn -ENOENT;\n\treturn p - env->subprog_info;\n\n}\n\nstatic int add_subprog(struct bpf_verifier_env *env, int off)\n{\n\tint insn_cnt = env->prog->len;\n\tint ret;\n\n\tif (off >= insn_cnt || off < 0) {\n\t\tverbose(env, \"call to invalid destination\\n\");\n\t\treturn -EINVAL;\n\t}\n\tret = find_subprog(env, off);\n\tif (ret >= 0)\n\t\treturn 0;\n\tif (env->subprog_cnt >= BPF_MAX_SUBPROGS) {\n\t\tverbose(env, \"too many subprograms\\n\");\n\t\treturn -E2BIG;\n\t}\n\tenv->subprog_info[env->subprog_cnt++].start = off;\n\tsort(env->subprog_info, env->subprog_cnt,\n\t     sizeof(env->subprog_info[0]), cmp_subprogs, NULL);\n\treturn 0;\n}\n\nstatic int check_subprogs(struct bpf_verifier_env *env)\n{\n\tint i, ret, subprog_start, subprog_end, off, cur_subprog = 0;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\n\t/* Add entry function. */\n\tret = add_subprog(env, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* determine subprog starts. The end is one before the next starts */\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (!env->bpf_capable) {\n\t\t\tverbose(env,\n\t\t\t\t\"function calls to other bpf functions are allowed for CAP_BPF and CAP_SYS_ADMIN\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\t\tret = add_subprog(env, i + insn[i].imm + 1);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t/* Add a fake 'exit' subprog which could simplify subprog iteration\n\t * logic. 'subprog_cnt' should not be increased.\n\t */\n\tsubprog[env->subprog_cnt].start = insn_cnt;\n\n\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\t\tverbose(env, \"func#%d @%d\\n\", i, subprog[i].start);\n\n\t/* now check that all jumps are within the same subprog */\n\tsubprog_start = subprog[cur_subprog].start;\n\tsubprog_end = subprog[cur_subprog + 1].start;\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tu8 code = insn[i].code;\n\n\t\tif (BPF_CLASS(code) != BPF_JMP && BPF_CLASS(code) != BPF_JMP32)\n\t\t\tgoto next;\n\t\tif (BPF_OP(code) == BPF_EXIT || BPF_OP(code) == BPF_CALL)\n\t\t\tgoto next;\n\t\toff = i + insn[i].off + 1;\n\t\tif (off < subprog_start || off >= subprog_end) {\n\t\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", i, off);\n\t\t\treturn -EINVAL;\n\t\t}\nnext:\n\t\tif (i == subprog_end - 1) {\n\t\t\t/* to avoid fall-through from one subprog into another\n\t\t\t * the last insn of the subprog should be either exit\n\t\t\t * or unconditional jump back\n\t\t\t */\n\t\t\tif (code != (BPF_JMP | BPF_EXIT) &&\n\t\t\t    code != (BPF_JMP | BPF_JA)) {\n\t\t\t\tverbose(env, \"last insn is not an exit or jmp\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsubprog_start = subprog_end;\n\t\t\tcur_subprog++;\n\t\t\tif (cur_subprog < env->subprog_cnt)\n\t\t\t\tsubprog_end = subprog[cur_subprog + 1].start;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Parentage chain of this register (or stack slot) should take care of all\n * issues like callee-saved registers, stack slot allocation time, etc.\n */\nstatic int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent, u8 flag)\n{\n\tbool writes = parent == state->parent; /* Observe write marks */\n\tint cnt = 0;\n\n\twhile (parent) {\n\t\t/* if read wasn't screened by an earlier write ... */\n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str[parent->type],\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* The first condition is more likely to be true than the\n\t\t * second, checked it first.\n\t\t */\n\t\tif ((parent->live & REG_LIVE_READ) == flag ||\n\t\t    parent->live & REG_LIVE_READ64)\n\t\t\t/* The parentage chain never changes and\n\t\t\t * this parent was already marked as LIVE_READ.\n\t\t\t * There is no need to keep walking the chain again and\n\t\t\t * keep re-marking all parents as LIVE_READ.\n\t\t\t * This case happens when the same register is read\n\t\t\t * multiple times without writes into it in-between.\n\t\t\t * Also, if parent has the stronger REG_LIVE_READ64 set,\n\t\t\t * then no need to set the weak REG_LIVE_READ32.\n\t\t\t */\n\t\t\tbreak;\n\t\t/* ... then we depend on parent's value */\n\t\tparent->live |= flag;\n\t\t/* REG_LIVE_READ64 overrides REG_LIVE_READ32. */\n\t\tif (flag == REG_LIVE_READ64)\n\t\t\tparent->live &= ~REG_LIVE_READ32;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t\tcnt++;\n\t}\n\n\tif (env->longest_mark_read_walk < cnt)\n\t\tenv->longest_mark_read_walk = cnt;\n\treturn 0;\n}\n\n/* This function is supposed to be used by the following 32-bit optimization\n * code only. It returns TRUE if the source or destination register operates\n * on 64-bit, otherwise return FALSE.\n */\nstatic bool is_reg64(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t     u32 regno, struct bpf_reg_state *reg, enum reg_arg_type t)\n{\n\tu8 code, class, op;\n\n\tcode = insn->code;\n\tclass = BPF_CLASS(code);\n\top = BPF_OP(code);\n\tif (class == BPF_JMP) {\n\t\t/* BPF_EXIT for \"main\" will reach here. Return TRUE\n\t\t * conservatively.\n\t\t */\n\t\tif (op == BPF_EXIT)\n\t\t\treturn true;\n\t\tif (op == BPF_CALL) {\n\t\t\t/* BPF to BPF call will reach here because of marking\n\t\t\t * caller saved clobber with DST_OP_NO_MARK for which we\n\t\t\t * don't care the register def because they are anyway\n\t\t\t * marked as NOT_INIT already.\n\t\t\t */\n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\treturn false;\n\t\t\t/* Helper call will reach here because of arg type\n\t\t\t * check, conservatively return TRUE.\n\t\t\t */\n\t\t\tif (t == SRC_OP)\n\t\t\t\treturn true;\n\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (class == BPF_ALU64 || class == BPF_JMP ||\n\t    /* BPF_END always use BPF_ALU class. */\n\t    (class == BPF_ALU && op == BPF_END && insn->imm == 64))\n\t\treturn true;\n\n\tif (class == BPF_ALU || class == BPF_JMP32)\n\t\treturn false;\n\n\tif (class == BPF_LDX) {\n\t\tif (t != SRC_OP)\n\t\t\treturn BPF_SIZE(code) == BPF_DW;\n\t\t/* LDX source must be ptr. */\n\t\treturn true;\n\t}\n\n\tif (class == BPF_STX) {\n\t\tif (reg->type != SCALAR_VALUE)\n\t\t\treturn true;\n\t\treturn BPF_SIZE(code) == BPF_DW;\n\t}\n\n\tif (class == BPF_LD) {\n\t\tu8 mode = BPF_MODE(code);\n\n\t\t/* LD_IMM64 */\n\t\tif (mode == BPF_IMM)\n\t\t\treturn true;\n\n\t\t/* Both LD_IND and LD_ABS return 32-bit data. */\n\t\tif (t != SRC_OP)\n\t\t\treturn  false;\n\n\t\t/* Implicit ctx ptr. */\n\t\tif (regno == BPF_REG_6)\n\t\t\treturn true;\n\n\t\t/* Explicit source could be any width. */\n\t\treturn true;\n\t}\n\n\tif (class == BPF_ST)\n\t\t/* The only source register for BPF_ST is a ptr. */\n\t\treturn true;\n\n\t/* Conservatively return true at default. */\n\treturn true;\n}\n\n/* Return TRUE if INSN doesn't have explicit value define. */\nstatic bool insn_no_def(struct bpf_insn *insn)\n{\n\tu8 class = BPF_CLASS(insn->code);\n\n\treturn (class == BPF_JMP || class == BPF_JMP32 ||\n\t\tclass == BPF_STX || class == BPF_ST);\n}\n\n/* Return TRUE if INSN has defined any 32-bit value explicitly. */\nstatic bool insn_has_def32(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tif (insn_no_def(insn))\n\t\treturn false;\n\n\treturn !is_reg64(env, insn, insn->dst_reg, NULL, DST_OP);\n}\n\nstatic void mark_insn_zext(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_reg_state *reg)\n{\n\ts32 def_idx = reg->subreg_def;\n\n\tif (def_idx == DEF_NOT_SUBREG)\n\t\treturn;\n\n\tenv->insn_aux_data[def_idx - 1].zext_dst = true;\n\t/* The dst will be zero extended, so won't be sub-register anymore. */\n\treg->subreg_def = DEF_NOT_SUBREG;\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_insn *insn = env->prog->insnsi + env->insn_idx;\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tbool rw64;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\treg = &regs[regno];\n\trw64 = is_reg64(env, insn, regno, reg, t);\n\tif (t == SRC_OP) {\n\t\t/* check whether register used as source operand can be read */\n\t\tif (reg->type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't need to worry about FP liveness because it's read-only */\n\t\tif (regno == BPF_REG_FP)\n\t\t\treturn 0;\n\n\t\tif (rw64)\n\t\t\tmark_insn_zext(env, reg);\n\n\t\treturn mark_reg_read(env, reg, reg->parent,\n\t\t\t\t     rw64 ? REG_LIVE_READ64 : REG_LIVE_READ32);\n\t} else {\n\t\t/* check whether register used as dest operand can be written to */\n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\treg->live |= REG_LIVE_WRITTEN;\n\t\treg->subreg_def = rw64 ? DEF_NOT_SUBREG : env->insn_idx + 1;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\n/* for any branch, call, exit record the history of jmps in the given state */\nstatic int push_jmp_history(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_verifier_state *cur)\n{\n\tu32 cnt = cur->jmp_history_cnt;\n\tstruct bpf_idx_pair *p;\n\n\tcnt++;\n\tp = krealloc(cur->jmp_history, cnt * sizeof(*p), GFP_USER);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tp[cnt - 1].idx = env->insn_idx;\n\tp[cnt - 1].prev_idx = env->prev_insn_idx;\n\tcur->jmp_history = p;\n\tcur->jmp_history_cnt = cnt;\n\treturn 0;\n}\n\n/* Backtrack one insn at a time. If idx is not at the top of recorded\n * history then previous instruction came from straight line execution.\n */\nstatic int get_prev_insn_idx(struct bpf_verifier_state *st, int i,\n\t\t\t     u32 *history)\n{\n\tu32 cnt = *history;\n\n\tif (cnt && st->jmp_history[cnt - 1].idx == i) {\n\t\ti = st->jmp_history[cnt - 1].prev_idx;\n\t\t(*history)--;\n\t} else {\n\t\ti--;\n\t}\n\treturn i;\n}\n\n/* For given verifier state backtrack_insn() is called from the last insn to\n * the first insn. Its purpose is to compute a bitmask of registers and\n * stack slots that needs precision in the parent verifier state.\n */\nstatic int backtrack_insn(struct bpf_verifier_env *env, int idx,\n\t\t\t  u32 *reg_mask, u64 *stack_mask)\n{\n\tconst struct bpf_insn_cbs cbs = {\n\t\t.cb_print\t= verbose,\n\t\t.private_data\t= env,\n\t};\n\tstruct bpf_insn *insn = env->prog->insnsi + idx;\n\tu8 class = BPF_CLASS(insn->code);\n\tu8 opcode = BPF_OP(insn->code);\n\tu8 mode = BPF_MODE(insn->code);\n\tu32 dreg = 1u << insn->dst_reg;\n\tu32 sreg = 1u << insn->src_reg;\n\tu32 spi;\n\n\tif (insn->code == 0)\n\t\treturn 0;\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"regs=%x stack=%llx before \", *reg_mask, *stack_mask);\n\t\tverbose(env, \"%d: \", idx);\n\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t}\n\n\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\tif (opcode == BPF_MOV) {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t/* dreg = sreg\n\t\t\t\t * dreg needs precision after this insn\n\t\t\t\t * sreg needs precision before this insn\n\t\t\t\t */\n\t\t\t\t*reg_mask &= ~dreg;\n\t\t\t\t*reg_mask |= sreg;\n\t\t\t} else {\n\t\t\t\t/* dreg = K\n\t\t\t\t * dreg needs precision after this insn.\n\t\t\t\t * Corresponding register is already marked\n\t\t\t\t * as precise=true in this verifier state.\n\t\t\t\t * No further markings in parent are necessary\n\t\t\t\t */\n\t\t\t\t*reg_mask &= ~dreg;\n\t\t\t}\n\t\t} else {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t/* dreg += sreg\n\t\t\t\t * both dreg and sreg need precision\n\t\t\t\t * before this insn\n\t\t\t\t */\n\t\t\t\t*reg_mask |= sreg;\n\t\t\t} /* else dreg += K\n\t\t\t   * dreg still needs precision before this insn\n\t\t\t   */\n\t\t}\n\t} else if (class == BPF_LDX) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\t*reg_mask &= ~dreg;\n\n\t\t/* scalars can only be spilled into stack w/o losing precision.\n\t\t * Load from any other memory can be zero extended.\n\t\t * The desire to keep that precision is already indicated\n\t\t * by 'precise' mark in corresponding register of this state.\n\t\t * No further tracking necessary.\n\t\t */\n\t\tif (insn->src_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\t\tif (BPF_SIZE(insn->code) != BPF_DW)\n\t\t\treturn 0;\n\n\t\t/* dreg = *(u64 *)[fp - off] was a fill from the stack.\n\t\t * that [fp - off] slot contains scalar that needs to be\n\t\t * tracked with precision\n\t\t */\n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t*stack_mask |= 1ull << spi;\n\t} else if (class == BPF_STX || class == BPF_ST) {\n\t\tif (*reg_mask & dreg)\n\t\t\t/* stx & st shouldn't be using _scalar_ dst_reg\n\t\t\t * to access memory. It means backtracking\n\t\t\t * encountered a case of pointer subtraction.\n\t\t\t */\n\t\t\treturn -ENOTSUPP;\n\t\t/* scalars can only be spilled into stack */\n\t\tif (insn->dst_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\t\tif (BPF_SIZE(insn->code) != BPF_DW)\n\t\t\treturn 0;\n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (!(*stack_mask & (1ull << spi)))\n\t\t\treturn 0;\n\t\t*stack_mask &= ~(1ull << spi);\n\t\tif (class == BPF_STX)\n\t\t\t*reg_mask |= sreg;\n\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\tif (opcode == BPF_CALL) {\n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\treturn -ENOTSUPP;\n\t\t\t/* regular helper call sets R0 */\n\t\t\t*reg_mask &= ~1;\n\t\t\tif (*reg_mask & 0x3f) {\n\t\t\t\t/* if backtracing was looking for registers R1-R5\n\t\t\t\t * they should have been found already.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"BUG regs %x\\n\", *reg_mask);\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else if (opcode == BPF_EXIT) {\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t} else if (class == BPF_LD) {\n\t\tif (!(*reg_mask & dreg))\n\t\t\treturn 0;\n\t\t*reg_mask &= ~dreg;\n\t\t/* It's ld_imm64 or ld_abs or ld_ind.\n\t\t * For ld_imm64 no further tracking of precision\n\t\t * into parent is necessary\n\t\t */\n\t\tif (mode == BPF_IND || mode == BPF_ABS)\n\t\t\t/* to be analyzed */\n\t\t\treturn -ENOTSUPP;\n\t}\n\treturn 0;\n}\n\n/* the scalar precision tracking algorithm:\n * . at the start all registers have precise=false.\n * . scalar ranges are tracked as normal through alu and jmp insns.\n * . once precise value of the scalar register is used in:\n *   .  ptr + scalar alu\n *   . if (scalar cond K|scalar)\n *   .  helper_call(.., scalar, ...) where ARG_CONST is expected\n *   backtrack through the verifier states and mark all registers and\n *   stack slots with spilled constants that these scalar regisers\n *   should be precise.\n * . during state pruning two registers (or spilled stack slots)\n *   are equivalent if both are not precise.\n *\n * Note the verifier cannot simply walk register parentage chain,\n * since many different registers and stack slots could have been\n * used to compute single precise scalar.\n *\n * The approach of starting with precise=true for all registers and then\n * backtrack to mark a register as not precise when the verifier detects\n * that program doesn't care about specific value (e.g., when helper\n * takes register as ARG_ANYTHING parameter) is not safe.\n *\n * It's ok to walk single parentage chain of the verifier states.\n * It's possible that this backtracking will go all the way till 1st insn.\n * All other branches will be explored for needing precision later.\n *\n * The backtracking needs to deal with cases like:\n *   R8=map_value(id=0,off=0,ks=4,vs=1952,imm=0) R9_w=map_value(id=0,off=40,ks=4,vs=1952,imm=0)\n * r9 -= r8\n * r5 = r9\n * if r5 > 0x79f goto pc+7\n *    R5_w=inv(id=0,umax_value=1951,var_off=(0x0; 0x7ff))\n * r5 += 1\n * ...\n * call bpf_perf_event_output#25\n *   where .arg5_type = ARG_CONST_SIZE_OR_ZERO\n *\n * and this case:\n * r6 = 1\n * call foo // uses callee's r6 inside to compute r0\n * r0 += r6\n * if r0 == 0 goto\n *\n * to track above reg_mask/stack_mask needs to be independent for each frame.\n *\n * Also if parent's curframe > frame where backtracking started,\n * the verifier need to mark registers in both frames, otherwise callees\n * may incorrectly prune callers. This is similar to\n * commit 7640ead93924 (\"bpf: verifier: make sure callees don't prune with caller differences\")\n *\n * For now backtracking falls back into conservative marking.\n */\nstatic void mark_all_scalars_precise(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_verifier_state *st)\n{\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tint i, j;\n\n\t/* big hammer: mark all scalars precise in this path.\n\t * pop_stack may still get !precise scalars.\n\t */\n\tfor (; st; st = st->parent)\n\t\tfor (i = 0; i <= st->curframe; i++) {\n\t\t\tfunc = st->frame[i];\n\t\t\tfor (j = 0; j < BPF_REG_FP; j++) {\n\t\t\t\treg = &func->regs[j];\n\t\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t}\n\t\t\tfor (j = 0; j < func->allocated_stack / BPF_REG_SIZE; j++) {\n\t\t\t\tif (func->stack[j].slot_type[0] != STACK_SPILL)\n\t\t\t\t\tcontinue;\n\t\t\t\treg = &func->stack[j].spilled_ptr;\n\t\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t}\n\t\t}\n}\n\nstatic int __mark_chain_precision(struct bpf_verifier_env *env, int regno,\n\t\t\t\t  int spi)\n{\n\tstruct bpf_verifier_state *st = env->cur_state;\n\tint first_idx = st->first_insn_idx;\n\tint last_idx = env->insn_idx;\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tu32 reg_mask = regno >= 0 ? 1u << regno : 0;\n\tu64 stack_mask = spi >= 0 ? 1ull << spi : 0;\n\tbool skip_first = true;\n\tbool new_marks = false;\n\tint i, err;\n\n\tif (!env->bpf_capable)\n\t\treturn 0;\n\n\tfunc = st->frame[st->curframe];\n\tif (regno >= 0) {\n\t\treg = &func->regs[regno];\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tWARN_ONCE(1, \"backtracing misuse\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (!reg->precise)\n\t\t\tnew_marks = true;\n\t\telse\n\t\t\treg_mask = 0;\n\t\treg->precise = true;\n\t}\n\n\twhile (spi >= 0) {\n\t\tif (func->stack[spi].slot_type[0] != STACK_SPILL) {\n\t\t\tstack_mask = 0;\n\t\t\tbreak;\n\t\t}\n\t\treg = &func->stack[spi].spilled_ptr;\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tstack_mask = 0;\n\t\t\tbreak;\n\t\t}\n\t\tif (!reg->precise)\n\t\t\tnew_marks = true;\n\t\telse\n\t\t\tstack_mask = 0;\n\t\treg->precise = true;\n\t\tbreak;\n\t}\n\n\tif (!new_marks)\n\t\treturn 0;\n\tif (!reg_mask && !stack_mask)\n\t\treturn 0;\n\tfor (;;) {\n\t\tDECLARE_BITMAP(mask, 64);\n\t\tu32 history = st->jmp_history_cnt;\n\n\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\tverbose(env, \"last_idx %d first_idx %d\\n\", last_idx, first_idx);\n\t\tfor (i = last_idx;;) {\n\t\t\tif (skip_first) {\n\t\t\t\terr = 0;\n\t\t\t\tskip_first = false;\n\t\t\t} else {\n\t\t\t\terr = backtrack_insn(env, i, &reg_mask, &stack_mask);\n\t\t\t}\n\t\t\tif (err == -ENOTSUPP) {\n\t\t\t\tmark_all_scalars_precise(env, st);\n\t\t\t\treturn 0;\n\t\t\t} else if (err) {\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\tif (!reg_mask && !stack_mask)\n\t\t\t\t/* Found assignment(s) into tracked register in this state.\n\t\t\t\t * Since this state is already marked, just return.\n\t\t\t\t * Nothing to be tracked further in the parent state.\n\t\t\t\t */\n\t\t\t\treturn 0;\n\t\t\tif (i == first_idx)\n\t\t\t\tbreak;\n\t\t\ti = get_prev_insn_idx(st, i, &history);\n\t\t\tif (i >= env->prog->len) {\n\t\t\t\t/* This can happen if backtracking reached insn 0\n\t\t\t\t * and there are still reg_mask or stack_mask\n\t\t\t\t * to backtrack.\n\t\t\t\t * It means the backtracking missed the spot where\n\t\t\t\t * particular register was initialized with a constant.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"BUG backtracking idx %d\\n\", i);\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\t\tst = st->parent;\n\t\tif (!st)\n\t\t\tbreak;\n\n\t\tnew_marks = false;\n\t\tfunc = st->frame[st->curframe];\n\t\tbitmap_from_u64(mask, reg_mask);\n\t\tfor_each_set_bit(i, mask, 32) {\n\t\t\treg = &func->regs[i];\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\treg_mask &= ~(1u << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!reg->precise)\n\t\t\t\tnew_marks = true;\n\t\t\treg->precise = true;\n\t\t}\n\n\t\tbitmap_from_u64(mask, stack_mask);\n\t\tfor_each_set_bit(i, mask, 64) {\n\t\t\tif (i >= func->allocated_stack / BPF_REG_SIZE) {\n\t\t\t\t/* the sequence of instructions:\n\t\t\t\t * 2: (bf) r3 = r10\n\t\t\t\t * 3: (7b) *(u64 *)(r3 -8) = r0\n\t\t\t\t * 4: (79) r4 = *(u64 *)(r10 -8)\n\t\t\t\t * doesn't contain jmps. It's backtracked\n\t\t\t\t * as a single block.\n\t\t\t\t * During backtracking insn 3 is not recognized as\n\t\t\t\t * stack access, so at the end of backtracking\n\t\t\t\t * stack slot fp-8 is still marked in stack_mask.\n\t\t\t\t * However the parent state may not have accessed\n\t\t\t\t * fp-8 and it's \"unallocated\" stack space.\n\t\t\t\t * In such case fallback to conservative.\n\t\t\t\t */\n\t\t\t\tmark_all_scalars_precise(env, st);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tif (func->stack[i].slot_type[0] != STACK_SPILL) {\n\t\t\t\tstack_mask &= ~(1ull << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\treg = &func->stack[i].spilled_ptr;\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tstack_mask &= ~(1ull << i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!reg->precise)\n\t\t\t\tnew_marks = true;\n\t\t\treg->precise = true;\n\t\t}\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tprint_verifier_state(env, func);\n\t\t\tverbose(env, \"parent %s regs=%x stack=%llx marks\\n\",\n\t\t\t\tnew_marks ? \"didn't have\" : \"already had\",\n\t\t\t\treg_mask, stack_mask);\n\t\t}\n\n\t\tif (!reg_mask && !stack_mask)\n\t\t\tbreak;\n\t\tif (!new_marks)\n\t\t\tbreak;\n\n\t\tlast_idx = st->last_insn_idx;\n\t\tfirst_idx = st->first_insn_idx;\n\t}\n\treturn 0;\n}\n\nstatic int mark_chain_precision(struct bpf_verifier_env *env, int regno)\n{\n\treturn __mark_chain_precision(env, regno, -1);\n}\n\nstatic int mark_chain_precision_stack(struct bpf_verifier_env *env, int spi)\n{\n\treturn __mark_chain_precision(env, -1, spi);\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\tcase PTR_TO_RDONLY_BUF:\n\tcase PTR_TO_RDONLY_BUF_OR_NULL:\n\tcase PTR_TO_RDWR_BUF:\n\tcase PTR_TO_RDWR_BUF_OR_NULL:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* Does this register contain a constant zero? */\nstatic bool register_is_null(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_equals_const(reg->var_off, 0);\n}\n\nstatic bool register_is_const(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_is_const(reg->var_off);\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\nstatic void save_register_state(struct bpf_func_state *state,\n\t\t\t\tint spi, struct bpf_reg_state *reg)\n{\n\tint i;\n\n\tstate->stack[spi].spilled_ptr = *reg;\n\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\tstate->stack[spi].slot_type[i] = STACK_SPILL;\n}\n\n/* check_stack_read/write functions track spill/fill of registers,\n * stack boundary and alignment are checked in check_mem_access()\n */\nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *state, /* func where register points to */\n\t\t\t     int off, int size, int value_regno, int insn_idx)\n{\n\tstruct bpf_func_state *cur; /* state of the current function */\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\tu32 dst_reg = env->prog->insnsi[insn_idx].dst_reg;\n\tstruct bpf_reg_state *reg = NULL;\n\n\terr = realloc_func_state(state, round_up(slot + 1, BPF_REG_SIZE),\n\t\t\t\t state->acquired_refs, true);\n\tif (err)\n\t\treturn err;\n\t/* caller checked that off % size == 0 and -MAX_BPF_STACK <= off < 0,\n\t * so it's aligned access and [off, off + size) are within stack limits\n\t */\n\tif (!env->allow_ptr_leaks &&\n\t    state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tif (value_regno >= 0)\n\t\treg = &cur->regs[value_regno];\n\n\tif (reg && size == BPF_REG_SIZE && register_is_const(reg) &&\n\t    !register_is_null(reg) && env->bpf_capable) {\n\t\tif (dst_reg != BPF_REG_FP) {\n\t\t\t/* The backtracking logic can only recognize explicit\n\t\t\t * stack slot address like [fp - 8]. Other spill of\n\t\t\t * scalar via different register has to be conervative.\n\t\t\t * Backtrack from here and mark all registers as precise\n\t\t\t * that contributed into 'reg' being a constant.\n\t\t\t */\n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tsave_register_state(state, spi, reg);\n\t} else if (reg && is_spillable_regtype(reg->type)) {\n\t\t/* register containing pointer is being spilled into stack */\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (state != cur && reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"cannot spill pointers to stack into stack frame of the caller\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!env->bypass_spec_v4) {\n\t\t\tbool sanitize = false;\n\n\t\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t\t    register_is_const(&state->stack[spi].spilled_ptr))\n\t\t\t\tsanitize = true;\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tif (state->stack[spi].slot_type[i] == STACK_MISC) {\n\t\t\t\t\tsanitize = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tif (sanitize) {\n\t\t\t\tint *poff = &env->insn_aux_data[insn_idx].sanitize_stack_off;\n\t\t\t\tint soff = (-spi - 1) * BPF_REG_SIZE;\n\n\t\t\t\t/* detected reuse of integer stack slot with a pointer\n\t\t\t\t * which means either llvm is reusing stack slot or\n\t\t\t\t * an attacker is trying to exploit CVE-2018-3639\n\t\t\t\t * (speculative store bypass)\n\t\t\t\t * Have to sanitize that slot with preemptive\n\t\t\t\t * store of zero.\n\t\t\t\t */\n\t\t\t\tif (*poff && *poff != soff) {\n\t\t\t\t\t/* disallow programs where single insn stores\n\t\t\t\t\t * into two different stack slots, since verifier\n\t\t\t\t\t * cannot sanitize them\n\t\t\t\t\t */\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"insn %d cannot access two stack slots fp%d and fp%d\",\n\t\t\t\t\t\tinsn_idx, *poff, soff);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\t*poff = soff;\n\t\t\t}\n\t\t}\n\t\tsave_register_state(state, spi, reg);\n\t} else {\n\t\tu8 type = STACK_MISC;\n\n\t\t/* regular write of data into stack destroys any spilled ptr */\n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\t\t/* Mark slots as STACK_MISC if they belonged to spilled ptr. */\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL)\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tstate->stack[spi].slot_type[i] = STACK_MISC;\n\n\t\t/* only mark the slot as written if all 8 bytes were written\n\t\t * otherwise read propagation may incorrectly stop too soon\n\t\t * when stack slots are partially written.\n\t\t * This heuristic means that read propagation will be\n\t\t * conservative, since it will add reg_live_read marks\n\t\t * to stack slots all the way to first state when programs\n\t\t * writes+reads less than 8 bytes\n\t\t */\n\t\tif (size == BPF_REG_SIZE)\n\t\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\t/* when we zero initialize stack slots mark them as such */\n\t\tif (reg && register_is_null(reg)) {\n\t\t\t/* backtracking doesn't work for STACK_ZERO yet. */\n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\ttype = STACK_ZERO;\n\t\t}\n\n\t\t/* Mark slots affected by this stack write. */\n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\ttype;\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *reg_state /* func where register points to */,\n\t\t\t    int off, int size, int value_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tstruct bpf_reg_state *reg;\n\tu8 *stype;\n\n\tif (reg_state->allocated_stack <= slot) {\n\t\tverbose(env, \"invalid read from stack off %d+0 size %d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\tstype = reg_state->stack[spi].slot_type;\n\treg = &reg_state->stack[spi].spilled_ptr;\n\n\tif (stype[0] == STACK_SPILL) {\n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\t\tverbose(env, \"invalid size of register fill\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t\tif (value_regno >= 0) {\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t\t}\n\t\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\t\treturn 0;\n\t\t}\n\t\tfor (i = 1; i < BPF_REG_SIZE; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] != STACK_SPILL) {\n\t\t\t\tverbose(env, \"corrupted spill memory\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\n\t\tif (value_regno >= 0) {\n\t\t\t/* restore register state from stack */\n\t\t\tstate->regs[value_regno] = *reg;\n\t\t\t/* mark reg as written since spilled pointer state likely\n\t\t\t * has its liveness marks cleared by is_state_visited()\n\t\t\t * which resets stack/reg liveness for state transitions\n\t\t\t */\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t} else if (__is_pointer_value(env->allow_ptr_leaks, reg)) {\n\t\t\t/* If value_regno==-1, the caller is asking us whether\n\t\t\t * it is acceptable to use this value as a SCALAR_VALUE\n\t\t\t * (e.g. for XADD).\n\t\t\t * We must not allow unprivileged callers to do that\n\t\t\t * with spilled pointers.\n\t\t\t */\n\t\t\tverbose(env, \"leaking pointer from stack off %d\\n\",\n\t\t\t\toff);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t} else {\n\t\tint zeros = 0;\n\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_MISC)\n\t\t\t\tcontinue;\n\t\t\tif (stype[(slot - i) % BPF_REG_SIZE] == STACK_ZERO) {\n\t\t\t\tzeros++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\tif (value_regno >= 0) {\n\t\t\tif (zeros == size) {\n\t\t\t\t/* any size read into register is zero extended,\n\t\t\t\t * so the whole register == const_zero\n\t\t\t\t */\n\t\t\t\t__mark_reg_const_zero(&state->regs[value_regno]);\n\t\t\t\t/* backtracking doesn't support STACK_ZERO yet,\n\t\t\t\t * so mark it precise here, so that later\n\t\t\t\t * backtracking can stop here.\n\t\t\t\t * Backtracking may not need this if this register\n\t\t\t\t * doesn't participate in pointer adjustment.\n\t\t\t\t * Forward propagation of precise flag is not\n\t\t\t\t * necessary either. This mark is only to stop\n\t\t\t\t * backtracking. Any register that contributed\n\t\t\t\t * to const 0 was marked precise before spill.\n\t\t\t\t */\n\t\t\t\tstate->regs[value_regno].precise = true;\n\t\t\t} else {\n\t\t\t\t/* have read misc data from the stack */\n\t\t\t\tmark_reg_unknown(env, state->regs, value_regno);\n\t\t\t}\n\t\t\tstate->regs[value_regno].live |= REG_LIVE_WRITTEN;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int check_stack_access(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_reg_state *reg,\n\t\t\t      int off, int size)\n{\n\t/* Stack accesses must be at a fixed offset, so that we\n\t * can determine what type of data were returned. See\n\t * check_stack_read().\n\t */\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable stack access var_off=%s off=%d size=%d\\n\",\n\t\t\ttn_buf, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\tverbose(env, \"invalid stack off=%d size=%d\\n\", off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_map_access_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, enum bpf_access_type type)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\tu32 cap = bpf_map_flags_to_cap(map);\n\n\tif (type == BPF_WRITE && !(cap & BPF_MAP_CAN_WRITE)) {\n\t\tverbose(env, \"write into map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (type == BPF_READ && !(cap & BPF_MAP_CAN_READ)) {\n\t\tverbose(env, \"read from map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into memory region (e.g., map value, ringbuf sample, etc) */\nstatic int __check_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t      int off, int size, u32 mem_size,\n\t\t\t      bool zero_size_allowed)\n{\n\tbool size_ok = size > 0 || (size == 0 && zero_size_allowed);\n\tstruct bpf_reg_state *reg;\n\n\tif (off >= 0 && size_ok && (u64)off + size <= mem_size)\n\t\treturn 0;\n\n\treg = &cur_regs(env)[regno];\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_VALUE:\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t\tbreak;\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, off, mem_size);\n\t\tbreak;\n\tcase PTR_TO_MEM:\n\tdefault:\n\t\tverbose(env, \"invalid access to memory, mem_size=%u off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t}\n\n\treturn -EACCES;\n}\n\n/* check read/write into a memory region with possible variable offset */\nstatic int check_mem_region_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t   int off, int size, u32 mem_size,\n\t\t\t\t   bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t/* We may have adjusted the register pointing to memory region, so we\n\t * need to try adding each of min_value and max_value to off\n\t * to make sure our theoretical access will be safe.\n\t */\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_verifier_state(env, state);\n\n\t/* The minimum value is only important with signed\n\t * comparisons where we can't assume the floor of a\n\t * value is 0.  If we are using signed variables for our\n\t * index'es we need to make sure that whatever we use\n\t * will have a set floor within our range.\n\t */\n\tif (reg->smin_value < 0 &&\n\t    (reg->smin_value == S64_MIN ||\n\t     (off + reg->smin_value != (s64)(s32)(off + reg->smin_value)) ||\n\t      reg->smin_value + off < 0)) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t/* If we haven't set a max value then we need to bail since we can't be\n\t * sure we won't do bad things.\n\t * If reg->umax_value + off could overflow, treat that as unbounded too.\n\t */\n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any such access\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d max value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n/* check read/write into a map element with possible variable offset */\nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tstruct bpf_map *map = reg->map_ptr;\n\tint err;\n\n\terr = check_mem_region_access(env, regno, off, size, map->value_size,\n\t\t\t\t      zero_size_allowed);\n\tif (err)\n\t\treturn err;\n\n\tif (map_value_has_spin_lock(map)) {\n\t\tu32 lock = map->spin_lock_off;\n\n\t\t/* if any part of struct bpf_spin_lock can be touched by\n\t\t * load/store reject this program.\n\t\t * To check that [x1, x2) overlaps with [y1, y2)\n\t\t * it is sufficient to check x1 < y2 && y1 < x2.\n\t\t */\n\t\tif (reg->smin_value + off < lock + sizeof(struct bpf_spin_lock) &&\n\t\t     lock < reg->umax_value + off + size) {\n\t\t\tverbose(env, \"bpf_spin_lock cannot be accessed directly by load/store\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\treturn err;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tswitch (env->prog->type) {\n\t/* Program types only with direct read access go here! */\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\t/* fallthrough */\n\n\t/* Program types with direct read + write access go here! */\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tif (t == BPF_WRITE)\n\t\t\tenv->seen_direct_write = true;\n\n\t\treturn true;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, off, size, reg->range,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\n\t/* __check_mem_access has made sure \"off + size - 1\" is within u16.\n\t * reg->umax_value can't be bigger than MAX_PACKET_OFF which is 0xffff,\n\t * otherwise find_good_pkt_pointers would have refused to set range info\n\t * that __check_mem_access would have rejected this pkt access.\n\t * Therefore, \"off + reg->umax_value + size - 1\" won't overflow u32.\n\t */\n\tenv->prog->aux->max_pkt_offset =\n\t\tmax_t(u32, env->prog->aux->max_pkt_offset,\n\t\t      off + reg->umax_value + size - 1);\n\n\treturn err;\n}\n\n/* check access to 'struct bpf_context' fields.  Supports fixed offsets only */\nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t/* A non zero info.ctx_field_size indicates that this field is a\n\t\t * candidate for later verifier transformation to load the whole\n\t\t * field and then apply a mask when accessed with a narrower\n\t\t * access than actual ctx access size. A zero info.ctx_field_size\n\t\t * will only allow for whole field access and rejects any other\n\t\t * type of narrower access.\n\t\t */\n\t\t*reg_type = info.reg_type;\n\n\t\tif (*reg_type == PTR_TO_BTF_ID || *reg_type == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t*btf_id = info.btf_id;\n\t\telse\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t/* remember the offset of last byte accessed in ctx */\n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic int check_flow_keys_access(struct bpf_verifier_env *env, int off,\n\t\t\t\t  int size)\n{\n\tif (size < 0 || off < 0 ||\n\t    (u64)off + size > sizeof(struct bpf_flow_keys)) {\n\t\tverbose(env, \"invalid access to flow keys off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_sock_access(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t     u32 regno, int off, int size,\n\t\t\t     enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info = {};\n\tbool valid;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (reg->type) {\n\tcase PTR_TO_SOCK_COMMON:\n\t\tvalid = bpf_sock_common_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tvalid = bpf_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tvalid = bpf_tcp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tvalid = bpf_xdp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\n\tif (valid) {\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size =\n\t\t\tinfo.ctx_field_size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"R%d invalid %s access off=%d size=%d\\n\",\n\t\tregno, reg_type_str[reg->type], off, size);\n\n\treturn -EACCES;\n}\n\nstatic struct bpf_reg_state *reg_state(struct bpf_verifier_env *env, int regno)\n{\n\treturn cur_regs(env) + regno;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, reg_state(env, regno));\n}\n\nstatic bool is_ctx_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn reg->type == PTR_TO_CTX;\n}\n\nstatic bool is_sk_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_sk_pointer(reg->type);\n}\n\nstatic bool is_pkt_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool is_flow_key_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\t/* Separate to is_ctx_reg() since we still want to allow BPF_ST here. */\n\treturn reg->type == PTR_TO_FLOW_KEYS;\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t/* For platforms that do not have a Kconfig enabling\n\t * CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS the value of\n\t * NET_IP_ALIGN is universally set to '2'.  And on platforms\n\t * that do set CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS, we get\n\t * to this code only in strict mode where we want to emulate\n\t * the NET_IP_ALIGN==2 checking.  Therefore use an\n\t * unconditional IP align value of '2'.\n\t */\n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t/* Byte size accesses are always allowed. */\n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int off,\n\t\t\t       int size, bool strict_alignment_once)\n{\n\tbool strict = env->strict_alignment || strict_alignment_once;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t/* Special case, because of NET_IP_ALIGN. Given metadata sits\n\t\t * right in front, treat it the very same way.\n\t\t */\n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_FLOW_KEYS:\n\t\tpointer_desc = \"flow keys \";\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t/* The stack spill tracking logic in check_stack_write()\n\t\t * and check_stack_read() relies on stack accesses being\n\t\t * aligned.\n\t\t */\n\t\tstrict = true;\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tpointer_desc = \"sock \";\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON:\n\t\tpointer_desc = \"sock_common \";\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tpointer_desc = \"tcp_sock \";\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tpointer_desc = \"xdp_sock \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\nstatic int update_stack_depth(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_func_state *func,\n\t\t\t      int off)\n{\n\tu16 stack = env->subprog_info[func->subprogno].stack_depth;\n\n\tif (stack >= -off)\n\t\treturn 0;\n\n\t/* update known max for given subprogram */\n\tenv->subprog_info[func->subprogno].stack_depth = -off;\n\treturn 0;\n}\n\n/* starting from main bpf function walk all instructions of the function\n * and recursively walk all callees that given function can call.\n * Ignore jump and exit insns.\n * Since recursion is prevented by check_cfg() this algorithm\n * only needs a local stack of MAX_CALL_FRAMES to remember callsites\n */\nstatic int check_max_stack_depth(struct bpf_verifier_env *env)\n{\n\tint depth = 0, frame = 0, idx = 0, i = 0, subprog_end;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint ret_insn[MAX_CALL_FRAMES];\n\tint ret_prog[MAX_CALL_FRAMES];\n\nprocess_func:\n\t/* round up to 32-bytes, since this is granularity\n\t * of interpreter stack size\n\t */\n\tdepth += round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tif (depth > MAX_BPF_STACK) {\n\t\tverbose(env, \"combined stack size of %d calls is %d. Too large\\n\",\n\t\t\tframe + 1, depth);\n\t\treturn -EACCES;\n\t}\ncontinue_func:\n\tsubprog_end = subprog[idx + 1].start;\n\tfor (; i < subprog_end; i++) {\n\t\tif (insn[i].code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn[i].src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* remember insn and function to return to */\n\t\tret_insn[frame] = i + 1;\n\t\tret_prog[frame] = idx;\n\n\t\t/* find the callee */\n\t\ti = i + insn[i].imm + 1;\n\t\tidx = find_subprog(env, i);\n\t\tif (idx < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tframe++;\n\t\tif (frame >= MAX_CALL_FRAMES) {\n\t\t\tverbose(env, \"the call stack of %d frames is too deep !\\n\",\n\t\t\t\tframe);\n\t\t\treturn -E2BIG;\n\t\t}\n\t\tgoto process_func;\n\t}\n\t/* end of for() loop means the last insn of the 'subprog'\n\t * was reached. Doesn't matter whether it was JA or EXIT\n\t */\n\tif (frame == 0)\n\t\treturn 0;\n\tdepth -= round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tframe--;\n\ti = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_func;\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nstatic int get_callee_stack_depth(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_insn *insn, int idx)\n{\n\tint start = idx + insn->imm + 1, subprog;\n\n\tsubprog = find_subprog(env, start);\n\tif (subprog < 0) {\n\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t  start);\n\t\treturn -EFAULT;\n\t}\n\treturn env->subprog_info[subprog].stack_depth;\n}\n#endif\n\nint check_ctx_reg(struct bpf_verifier_env *env,\n\t\t  const struct bpf_reg_state *reg, int regno)\n{\n\t/* Access to ctx or passing it to a helper is only allowed in\n\t * its original, unmodified form.\n\t */\n\n\tif (reg->off) {\n\t\tverbose(env, \"dereference of modified ctx ptr R%d off=%d disallowed\\n\",\n\t\t\tregno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable ctx access var_off=%s disallowed\\n\", tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int __check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t const char *buf_info,\n\t\t\t\t const struct bpf_reg_state *reg,\n\t\t\t\t int regno, int off, int size)\n{\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d invalid %s buffer access: off=%d, size=%d\\n\",\n\t\t\tregno, buf_info, off, size);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d invalid variable buffer offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_tp_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  int regno, int off, int size)\n{\n\tint err;\n\n\terr = __check_buffer_access(env, \"tracepoint\", reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > env->prog->aux->max_tp_access)\n\t\tenv->prog->aux->max_tp_access = off + size;\n\n\treturn 0;\n}\n\nstatic int check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t       int regno, int off, int size,\n\t\t\t       bool zero_size_allowed,\n\t\t\t       const char *buf_info,\n\t\t\t       u32 *max_access)\n{\n\tint err;\n\n\terr = __check_buffer_access(env, buf_info, reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > *max_access)\n\t\t*max_access = off + size;\n\n\treturn 0;\n}\n\n/* BPF architecture zero extends alu32 ops into 64-bit registesr */\nstatic void zext_32_to_64(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_subreg(reg->var_off);\n\t__reg_assign_32_into_64(reg);\n}\n\n/* truncate register to smaller size (in bytes)\n * must be called with size < BPF_REG_SIZE\n */\nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t/* clear high bits in bit representation */\n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t/* fix arithmetic bounds */\n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n\n\t/* If size is smaller than 32bit register the 32bit register\n\t * values are also truncated so we push 64-bit bounds into\n\t * 32-bit bounds. Above were truncated < 32-bits already.\n\t */\n\tif (size >= 4)\n\t\treturn;\n\t__reg_combine_64_into_32(reg);\n}\n\nstatic bool bpf_map_is_rdonly(const struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_RDONLY_PROG) && map->frozen;\n}\n\nstatic int bpf_map_direct_read(struct bpf_map *map, int off, int size, u64 *val)\n{\n\tvoid *ptr;\n\tu64 addr;\n\tint err;\n\n\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\tif (err)\n\t\treturn err;\n\tptr = (void *)(long)addr + off;\n\n\tswitch (size) {\n\tcase sizeof(u8):\n\t\t*val = (u64)*(u8 *)ptr;\n\t\tbreak;\n\tcase sizeof(u16):\n\t\t*val = (u64)*(u16 *)ptr;\n\t\tbreak;\n\tcase sizeof(u32):\n\t\t*val = (u64)*(u32 *)ptr;\n\t\tbreak;\n\tcase sizeof(u64):\n\t\t*val = *(u64 *)ptr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int check_ptr_to_btf_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tconst struct btf_type *t = btf_type_by_id(btf_vmlinux, reg->btf_id);\n\tconst char *tname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\tu32 btf_id;\n\tint ret;\n\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid variable offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, tname, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\tif (env->ops->btf_struct_access) {\n\t\tret = env->ops->btf_struct_access(&env->log, t, off, size,\n\t\t\t\t\t\t  atype, &btf_id);\n\t} else {\n\t\tif (atype != BPF_READ) {\n\t\t\tverbose(env, \"only read is supported\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tret = btf_struct_access(&env->log, t, off, size, atype,\n\t\t\t\t\t&btf_id);\n\t}\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (atype == BPF_READ && value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, btf_id);\n\n\treturn 0;\n}\n\nstatic int check_ptr_to_map_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_map *map = reg->map_ptr;\n\tconst struct btf_type *t;\n\tconst char *tname;\n\tu32 btf_id;\n\tint ret;\n\n\tif (!btf_vmlinux) {\n\t\tverbose(env, \"map_ptr access not supported without CONFIG_DEBUG_INFO_BTF\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (!map->ops->map_btf_id || !*map->ops->map_btf_id) {\n\t\tverbose(env, \"map_ptr access not supported for map type %d\\n\",\n\t\t\tmap->map_type);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = btf_type_by_id(btf_vmlinux, *map->ops->map_btf_id);\n\ttname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\n\tif (!env->allow_ptr_to_map_access) {\n\t\tverbose(env,\n\t\t\t\"%s access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\\n\",\n\t\t\ttname);\n\t\treturn -EPERM;\n\t}\n\n\tif (off < 0) {\n\t\tverbose(env, \"R%d is %s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (atype != BPF_READ) {\n\t\tverbose(env, \"only read from %s is supported\\n\", tname);\n\t\treturn -EACCES;\n\t}\n\n\tret = btf_struct_access(&env->log, t, off, size, atype, &btf_id);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, btf_id);\n\n\treturn 0;\n}\n\n\n/* check whether memory at (regno + off) is accessible for t = (read | write)\n * if t==write, value_regno is a register which value is stored into memory\n * if t==read, value_regno is a register which will receive the value from memory\n * if t==write && value_regno==-1, some unknown value is stored into memory\n * if t==read && value_regno==-1, don't care what we read from memory\n */\nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t/* alignment checks will add in reg->off themselves */\n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t/* for access checks, reg->off is just part of off */\n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_map_access_type(env, regno, off, size, t);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_map_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\tstruct bpf_map *map = reg->map_ptr;\n\n\t\t\t/* if map is read-only, track its contents as scalars */\n\t\t\tif (tnum_is_const(reg->var_off) &&\n\t\t\t    bpf_map_is_rdonly(map) &&\n\t\t\t    map->ops->map_direct_value_addr) {\n\t\t\t\tint map_off = off + reg->var_off.value;\n\t\t\t\tu64 val = 0;\n\n\t\t\t\terr = bpf_map_direct_read(map, map_off, size,\n\t\t\t\t\t\t\t  &val);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tregs[value_regno].type = SCALAR_VALUE;\n\t\t\t\t__mark_reg_known(&regs[value_regno], val);\n\t\t\t} else {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t}\n\t\t}\n\t} else if (reg->type == PTR_TO_MEM) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->mem_size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\t\tu32 btf_id = 0;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ctx_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type, &btf_id);\n\t\tif (err)\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t/* ctx access returns either a scalar, or a\n\t\t\t * PTR_TO_PACKET[_META,_END]. In the latter\n\t\t\t * case, we know the offset is zero.\n\t\t\t */\n\t\t\tif (reg_type == SCALAR_VALUE) {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t} else {\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\t\tif (reg_type_may_be_null(reg_type))\n\t\t\t\t\tregs[value_regno].id = ++env->id_gen;\n\t\t\t\t/* A load of ctx field could have different\n\t\t\t\t * actual load size with the one encoded in the\n\t\t\t\t * insn. When the dst is PTR, it is for sure not\n\t\t\t\t * a sub-register.\n\t\t\t\t */\n\t\t\t\tregs[value_regno].subreg_def = DEF_NOT_SUBREG;\n\t\t\t\tif (reg_type == PTR_TO_BTF_ID ||\n\t\t\t\t    reg_type == PTR_TO_BTF_ID_OR_NULL)\n\t\t\t\t\tregs[value_regno].btf_id = btf_id;\n\t\t\t}\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_access(env, reg, off, size);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tstate = func(env, reg);\n\t\terr = update_stack_depth(env, state, off);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_WRITE)\n\t\t\terr = check_stack_write(env, state, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t\telse\n\t\t\terr = check_stack_read(env, state, off, size,\n\t\t\t\t\t       value_regno);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (type_is_sk_pointer(reg->type)) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, insn_idx, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_TP_BUFFER) {\n\t\terr = check_tp_buffer_access(env, reg, regno, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_BTF_ID) {\n\t\terr = check_ptr_to_btf_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == CONST_PTR_TO_MAP) {\n\t\terr = check_ptr_to_map_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == PTR_TO_RDONLY_BUF) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str[reg->type]);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdonly\",\n\t\t\t\t\t  &env->prog->aux->max_rdonly_access);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_RDWR_BUF) {\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  \"rdwr\",\n\t\t\t\t\t  &env->prog->aux->max_rdwr_access);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\t/* b/h/w load zero-extends, mark upper bits as known 0 */\n\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_xadd(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint err;\n\n\tif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\n\t    insn->imm != 0) {\n\t\tverbose(env, \"BPF_XADD uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check src1 operand */\n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\tif (is_ctx_reg(env, insn->dst_reg) ||\n\t    is_pkt_reg(env, insn->dst_reg) ||\n\t    is_flow_key_reg(env, insn->dst_reg) ||\n\t    is_sk_reg(env, insn->dst_reg)) {\n\t\tverbose(env, \"BPF_XADD stores into R%d %s is not allowed\\n\",\n\t\t\tinsn->dst_reg,\n\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\treturn -EACCES;\n\t}\n\n\t/* check whether atomic_add can read the memory */\n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1, true);\n\tif (err)\n\t\treturn err;\n\n\t/* check whether atomic_add can write into the same memory */\n\treturn check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\tBPF_SIZE(insn->code), BPF_WRITE, -1, true);\n}\n\nstatic int __check_stack_boundary(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t  int off, int access_size,\n\t\t\t\t  bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\n\tif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\n\t    access_size < 0 || (access_size == 0 && !zero_size_allowed)) {\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid stack type R%d off=%d access_size=%d\\n\",\n\t\t\t\tregno, off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid stack type R%d var_off=%s access_size=%d\\n\",\n\t\t\t\tregno, tn_buf, access_size);\n\t\t}\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n/* when register 'regno' is passed into function that will read 'access_size'\n * bytes from that pointer, make sure that it's within stack boundary\n * and all elements of stack are initialized.\n * Unlike most pointer bounds-checking functions, this one doesn't take an\n * 'off' argument, so it has to add in reg->off itself.\n */\nstatic int check_stack_boundary(struct bpf_verifier_env *env, int regno,\n\t\t\t\tint access_size, bool zero_size_allowed,\n\t\t\t\tstruct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err, min_off, max_off, i, j, slot, spi;\n\n\tif (reg->type != PTR_TO_STACK) {\n\t\t/* Allow zero-byte read from NULL, regardless of pointer type */\n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\t\treg_type_str[reg->type],\n\t\t\treg_type_str[PTR_TO_STACK]);\n\t\treturn -EACCES;\n\t}\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tmin_off = max_off = reg->var_off.value + reg->off;\n\t\terr = __check_stack_boundary(env, regno, min_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t/* Variable offset is prohibited for unprivileged mode for\n\t\t * simplicity since it requires corresponding support in\n\t\t * Spectre masking for stack ALU.\n\t\t * See also retrieve_ptr_limit().\n\t\t */\n\t\tif (!env->bypass_spec_v1) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"R%d indirect variable offset stack access prohibited for !root, var_off=%s\\n\",\n\t\t\t\tregno, tn_buf);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* Only initialized buffer on stack is allowed to be accessed\n\t\t * with variable offset. With uninitialized buffer it's hard to\n\t\t * guarantee that whole memory is marked as initialized on\n\t\t * helper return since specific bounds are unknown what may\n\t\t * cause uninitialized stack leaking.\n\t\t */\n\t\tif (meta && meta->raw_mode)\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smax_value >= BPF_MAX_VAR_OFF ||\n\t\t    reg->smax_value <= -BPF_MAX_VAR_OFF) {\n\t\t\tverbose(env, \"R%d unbounded indirect variable offset stack access\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmin_off = reg->smin_value + reg->off;\n\t\tmax_off = reg->smax_value + reg->off;\n\t\terr = __check_stack_boundary(env, regno, min_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err) {\n\t\t\tverbose(env, \"R%d min value is outside of stack bound\\n\",\n\t\t\t\tregno);\n\t\t\treturn err;\n\t\t}\n\t\terr = __check_stack_boundary(env, regno, max_off, access_size,\n\t\t\t\t\t     zero_size_allowed);\n\t\tif (err) {\n\t\t\tverbose(env, \"R%d max value is outside of stack bound\\n\",\n\t\t\t\tregno);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (meta && meta->raw_mode) {\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = min_off; i < max_off + access_size; i++) {\n\t\tu8 *stype;\n\n\t\tslot = -i - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot)\n\t\t\tgoto err;\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tif (*stype == STACK_MISC)\n\t\t\tgoto mark;\n\t\tif (*stype == STACK_ZERO) {\n\t\t\t/* helper can write anything into the stack */\n\t\t\t*stype = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\n\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t    state->stack[spi].spilled_ptr.type == PTR_TO_BTF_ID)\n\t\t\tgoto mark;\n\n\t\tif (state->stack[spi].slot_type[0] == STACK_SPILL &&\n\t\t    state->stack[spi].spilled_ptr.type == SCALAR_VALUE) {\n\t\t\t__mark_reg_unknown(env, &state->stack[spi].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tstate->stack[spi].slot_type[j] = STACK_MISC;\n\t\t\tgoto mark;\n\t\t}\n\nerr:\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid indirect read from stack off %d+%d size %d\\n\",\n\t\t\t\tmin_off, i - min_off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid indirect read from stack var_off %s+%d size %d\\n\",\n\t\t\t\ttn_buf, i - min_off, access_size);\n\t\t}\n\t\treturn -EACCES;\nmark:\n\t\t/* reading any byte out of 8-byte 'spill_slot' will cause\n\t\t * the whole slot to be marked as 'read'\n\t\t */\n\t\tmark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t      state->stack[spi].spilled_ptr.parent,\n\t\t\t      REG_LIVE_READ64);\n\t}\n\treturn update_stack_depth(env, state, min_off);\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed);\n\tcase PTR_TO_MEM:\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_RDONLY_BUF:\n\t\tif (meta && meta->raw_mode)\n\t\t\treturn -EACCES;\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdonly\",\n\t\t\t\t\t   &env->prog->aux->max_rdonly_access);\n\tcase PTR_TO_RDWR_BUF:\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   \"rdwr\",\n\t\t\t\t\t   &env->prog->aux->max_rdwr_access);\n\tdefault: /* scalar_value|ptr_to_stack or invalid ptr */\n\t\treturn check_stack_boundary(env, regno, access_size,\n\t\t\t\t\t    zero_size_allowed, meta);\n\t}\n}\n\n/* Implementation details:\n * bpf_map_lookup returns PTR_TO_MAP_VALUE_OR_NULL\n * Two bpf_map_lookups (even with the same key) will have different reg->id.\n * For traditional PTR_TO_MAP_VALUE the verifier clears reg->id after\n * value_or_null->value transition, since the verifier only cares about\n * the range of access to valid map value pointer and doesn't care about actual\n * address of the map element.\n * For maps with 'struct bpf_spin_lock' inside map value the verifier keeps\n * reg->id > 0 after value_or_null->value transition. By doing so\n * two bpf_map_lookups will be considered two different pointers that\n * point to different bpf_spin_locks.\n * The verifier allows taking only one bpf_spin_lock at a time to avoid\n * dead-locks.\n * Since only one bpf_spin_lock is allowed the checks are simpler than\n * reg_is_refcounted() logic. The verifier needs to remember only\n * one spin_lock instead of array of acquired_refs.\n * cur_state->active_spin_lock remembers which map value element got locked\n * and clears it after bpf_spin_unlock.\n */\nstatic int process_spin_lock(struct bpf_verifier_env *env, int regno,\n\t\t\t     bool is_lock)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tbool is_const = tnum_is_const(reg->var_off);\n\tstruct bpf_map *map = reg->map_ptr;\n\tu64 val = reg->var_off.value;\n\n\tif (reg->type != PTR_TO_MAP_VALUE) {\n\t\tverbose(env, \"R%d is not a pointer to map_value\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\tif (!is_const) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. bpf_spin_lock has to be at the constant offset\\n\",\n\t\t\tregno);\n\t\treturn -EINVAL;\n\t}\n\tif (!map->btf) {\n\t\tverbose(env,\n\t\t\t\"map '%s' has to have BTF in order to use bpf_spin_lock\\n\",\n\t\t\tmap->name);\n\t\treturn -EINVAL;\n\t}\n\tif (!map_value_has_spin_lock(map)) {\n\t\tif (map->spin_lock_off == -E2BIG)\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' has more than one 'struct bpf_spin_lock'\\n\",\n\t\t\t\tmap->name);\n\t\telse if (map->spin_lock_off == -ENOENT)\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' doesn't have 'struct bpf_spin_lock'\\n\",\n\t\t\t\tmap->name);\n\t\telse\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' is not a struct type or bpf_spin_lock is mangled\\n\",\n\t\t\t\tmap->name);\n\t\treturn -EINVAL;\n\t}\n\tif (map->spin_lock_off != val + reg->off) {\n\t\tverbose(env, \"off %lld doesn't point to 'struct bpf_spin_lock'\\n\",\n\t\t\tval + reg->off);\n\t\treturn -EINVAL;\n\t}\n\tif (is_lock) {\n\t\tif (cur->active_spin_lock) {\n\t\t\tverbose(env,\n\t\t\t\t\"Locking two bpf_spin_locks are not allowed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur->active_spin_lock = reg->id;\n\t} else {\n\t\tif (!cur->active_spin_lock) {\n\t\t\tverbose(env, \"bpf_spin_unlock without taking a lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (cur->active_spin_lock != reg->id) {\n\t\t\tverbose(env, \"bpf_spin_unlock of different lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcur->active_spin_lock = 0;\n\t}\n\treturn 0;\n}\n\nstatic bool arg_type_is_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_MEM ||\n\t       type == ARG_PTR_TO_MEM_OR_NULL ||\n\t       type == ARG_PTR_TO_UNINIT_MEM;\n}\n\nstatic bool arg_type_is_mem_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_SIZE ||\n\t       type == ARG_CONST_SIZE_OR_ZERO;\n}\n\nstatic bool arg_type_is_alloc_mem_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_ALLOC_MEM ||\n\t       type == ARG_PTR_TO_ALLOC_MEM_OR_NULL;\n}\n\nstatic bool arg_type_is_alloc_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_ALLOC_SIZE_OR_ZERO;\n}\n\nstatic bool arg_type_is_int_ptr(enum bpf_arg_type type)\n{\n\treturn type == ARG_PTR_TO_INT ||\n\t       type == ARG_PTR_TO_LONG;\n}\n\nstatic int int_ptr_type_to_size(enum bpf_arg_type type)\n{\n\tif (type == ARG_PTR_TO_INT)\n\t\treturn sizeof(u32);\n\telse if (type == ARG_PTR_TO_LONG)\n\t\treturn sizeof(u64);\n\n\treturn -EINVAL;\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 arg,\n\t\t\t  struct bpf_call_arg_meta *meta,\n\t\t\t  const struct bpf_func_proto *fn)\n{\n\tu32 regno = BPF_REG_1 + arg;\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected_type, type = reg->type;\n\tenum bpf_arg_type arg_type = fn->arg_type[arg];\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (arg_type == ARG_PTR_TO_MAP_KEY ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE ||\n\t    arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_SIZE ||\n\t\t   arg_type == ARG_CONST_SIZE_OR_ZERO ||\n\t\t   arg_type == ARG_CONST_ALLOC_SIZE_OR_ZERO) {\n\t\texpected_type = SCALAR_VALUE;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_CONST_MAP_PTR) {\n\t\texpected_type = CONST_PTR_TO_MAP;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t} else if (arg_type == ARG_PTR_TO_CTX ||\n\t\t   arg_type == ARG_PTR_TO_CTX_OR_NULL) {\n\t\texpected_type = PTR_TO_CTX;\n\t\tif (!(register_is_null(reg) &&\n\t\t      arg_type == ARG_PTR_TO_CTX_OR_NULL)) {\n\t\t\tif (type != expected_type)\n\t\t\t\tgoto err_type;\n\t\t\terr = check_ctx_reg(env, reg, regno);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SOCK_COMMON) {\n\t\texpected_type = PTR_TO_SOCK_COMMON;\n\t\t/* Any sk pointer can be ARG_PTR_TO_SOCK_COMMON */\n\t\tif (!type_is_sk_pointer(type))\n\t\t\tgoto err_type;\n\t\tif (reg->ref_obj_id) {\n\t\t\tif (meta->ref_obj_id) {\n\t\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\t\tmeta->ref_obj_id);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SOCKET ||\n\t\t   arg_type == ARG_PTR_TO_SOCKET_OR_NULL) {\n\t\texpected_type = PTR_TO_SOCKET;\n\t\tif (!(register_is_null(reg) &&\n\t\t      arg_type == ARG_PTR_TO_SOCKET_OR_NULL)) {\n\t\t\tif (type != expected_type)\n\t\t\t\tgoto err_type;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_BTF_ID) {\n\t\texpected_type = PTR_TO_BTF_ID;\n\t\tif (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (!fn->check_btf_id) {\n\t\t\tif (reg->btf_id != meta->btf_id) {\n\t\t\t\tverbose(env, \"Helper has type %s got %s in R%d\\n\",\n\t\t\t\t\tkernel_type_name(meta->btf_id),\n\t\t\t\t\tkernel_type_name(reg->btf_id), regno);\n\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t} else if (!fn->check_btf_id(reg->btf_id, arg)) {\n\t\t\tverbose(env, \"Helper does not support %s in R%d\\n\",\n\t\t\t\tkernel_type_name(reg->btf_id), regno);\n\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (!tnum_is_const(reg->var_off) || reg->var_off.value || reg->off) {\n\t\t\tverbose(env, \"R%d is a pointer to in-kernel struct with non-zero offset\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else if (arg_type == ARG_PTR_TO_SPIN_LOCK) {\n\t\tif (meta->func_id == BPF_FUNC_spin_lock) {\n\t\t\tif (process_spin_lock(env, regno, true))\n\t\t\t\treturn -EACCES;\n\t\t} else if (meta->func_id == BPF_FUNC_spin_unlock) {\n\t\t\tif (process_spin_lock(env, regno, false))\n\t\t\t\treturn -EACCES;\n\t\t} else {\n\t\t\tverbose(env, \"verifier internal error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t} else if (arg_type_is_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\t/* One exception here. In case function allows for NULL to be\n\t\t * passed in as argument, it's a SCALAR_VALUE type. Final test\n\t\t * happens during stack boundary checking.\n\t\t */\n\t\tif (register_is_null(reg) &&\n\t\t    (arg_type == ARG_PTR_TO_MEM_OR_NULL ||\n\t\t     arg_type == ARG_PTR_TO_ALLOC_MEM_OR_NULL))\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (!type_is_pkt_pointer(type) &&\n\t\t\t type != PTR_TO_MAP_VALUE &&\n\t\t\t type != PTR_TO_MEM &&\n\t\t\t type != PTR_TO_RDONLY_BUF &&\n\t\t\t type != PTR_TO_RDWR_BUF &&\n\t\t\t type != expected_type)\n\t\t\tgoto err_type;\n\t\tmeta->raw_mode = arg_type == ARG_PTR_TO_UNINIT_MEM;\n\t} else if (arg_type_is_alloc_mem_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_MEM;\n\t\tif (register_is_null(reg) &&\n\t\t    arg_type == ARG_PTR_TO_ALLOC_MEM_OR_NULL)\n\t\t\t/* final test in check_stack_boundary() */;\n\t\telse if (type != expected_type)\n\t\t\tgoto err_type;\n\t\tif (meta->ref_obj_id) {\n\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\tmeta->ref_obj_id);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t} else if (arg_type_is_int_ptr(arg_type)) {\n\t\texpected_type = PTR_TO_STACK;\n\t\tif (!type_is_pkt_pointer(type) &&\n\t\t    type != PTR_TO_MAP_VALUE &&\n\t\t    type != expected_type)\n\t\t\tgoto err_type;\n\t} else {\n\t\tverbose(env, \"unsupported arg_type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\tif (arg_type == ARG_CONST_MAP_PTR) {\n\t\t/* bpf_map_xxx(map_ptr) call: remember that map_ptr */\n\t\tmeta->map_ptr = reg->map_ptr;\n\t} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., key) call:\n\t\t * check that [key, key + map->key_size) are within\n\t\t * stack limits and initialized\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* in function declaration map_ptr must come before\n\t\t\t * map_key, so that it's verified and known before\n\t\t\t * we have to check map_key here. Otherwise it means\n\t\t\t * that kernel subsystem misconfigured verifier\n\t\t\t */\n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->key_size, false,\n\t\t\t\t\t      NULL);\n\t} else if (arg_type == ARG_PTR_TO_MAP_VALUE ||\n\t\t   (arg_type == ARG_PTR_TO_MAP_VALUE_OR_NULL &&\n\t\t    !register_is_null(reg)) ||\n\t\t   arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE) {\n\t\t/* bpf_map_xxx(..., map_ptr, ..., value) call:\n\t\t * check [value, value + map->value_size) validity\n\t\t */\n\t\tif (!meta->map_ptr) {\n\t\t\t/* kernel subsystem misconfigured verifier */\n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->raw_mode = (arg_type == ARG_PTR_TO_UNINIT_MAP_VALUE);\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->value_size, false,\n\t\t\t\t\t      meta);\n\t} else if (arg_type_is_mem_size(arg_type)) {\n\t\tbool zero_size_allowed = (arg_type == ARG_CONST_SIZE_OR_ZERO);\n\n\t\t/* This is used to refine r0 return value bounds for helpers\n\t\t * that enforce this value as an upper bound on return values.\n\t\t * See do_refine_retval_range() for helpers that can refine\n\t\t * the return value. C type of helper is u32 so we pull register\n\t\t * bound from umax_value however, if negative verifier errors\n\t\t * out. Only upper bounds can be learned because retval is an\n\t\t * int type and negative retvals are allowed.\n\t\t */\n\t\tmeta->msize_max_value = reg->umax_value;\n\n\t\t/* The register is SCALAR_VALUE; the access check\n\t\t * happens using its boundaries.\n\t\t */\n\t\tif (!tnum_is_const(reg->var_off))\n\t\t\t/* For unprivileged variable accesses, disable raw\n\t\t\t * mode so that the program is required to\n\t\t\t * initialize all the memory that the helper could\n\t\t\t * just partially fill up.\n\t\t\t */\n\t\t\tmeta = NULL;\n\n\t\tif (reg->smin_value < 0) {\n\t\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->umin_value == 0) {\n\t\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t\t      meta);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t\t      reg->umax_value,\n\t\t\t\t\t      zero_size_allowed, meta);\n\t\tif (!err)\n\t\t\terr = mark_chain_precision(env, regno);\n\t} else if (arg_type_is_alloc_size(arg_type)) {\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"R%d unbounded size, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->mem_size = reg->var_off.value;\n\t} else if (arg_type_is_int_ptr(arg_type)) {\n\t\tint size = int_ptr_type_to_size(arg_type);\n\n\t\terr = check_helper_mem_access(env, regno, size, false, meta);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_ptr_alignment(env, reg, 0, size, true);\n\t}\n\n\treturn err;\nerr_type:\n\tverbose(env, \"R%d type=%s expected=%s\\n\", regno,\n\t\treg_type_str[type], reg_type_str[expected_type]);\n\treturn -EACCES;\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t/* We need a two way check, first is from map perspective ... */\n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_skb_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value &&\n\t\t    func_id != BPF_FUNC_xdp_output)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_RINGBUF:\n\t\tif (func_id != BPF_FUNC_ringbuf_output &&\n\t\t    func_id != BPF_FUNC_ringbuf_reserve &&\n\t\t    func_id != BPF_FUNC_ringbuf_submit &&\n\t\t    func_id != BPF_FUNC_ringbuf_discard &&\n\t\t    func_id != BPF_FUNC_ringbuf_query)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\tif (func_id != BPF_FUNC_get_local_storage)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_DEVMAP:\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\t/* Restrict bpf side of cpumap and xskmap, open when use-cases\n\t * appear.\n\t */\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_map &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (func_id != BPF_FUNC_sk_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sock_hash_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\t\tif (func_id != BPF_FUNC_sk_select_reuseport)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_pop_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SK_STORAGE:\n\t\tif (func_id != BPF_FUNC_sk_storage_get &&\n\t\t    func_id != BPF_FUNC_sk_storage_delete)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* ... and second from the function itself. */\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tif (env->subprog_cnt > 1) {\n\t\t\tverbose(env, \"tail_calls are not allowed in programs with bpf-to-bpf calls\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\tcase BPF_FUNC_skb_output:\n\tcase BPF_FUNC_xdp_output:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_DEVMAP_HASH &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_XSKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\tcase BPF_FUNC_msg_redirect_map:\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_hash:\n\tcase BPF_FUNC_msg_redirect_hash:\n\tcase BPF_FUNC_sock_hash_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\tif (map->map_type != BPF_MAP_TYPE_REUSEPORT_SOCKARRAY &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_peek_elem:\n\tcase BPF_FUNC_map_pop_elem:\n\tcase BPF_FUNC_map_push_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_storage_get:\n\tcase BPF_FUNC_sk_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_SK_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic bool check_raw_mode_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\t/* We only support one arg being in raw mode at the moment,\n\t * which is sufficient for the helper functions we have\n\t * right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic bool check_args_pair_invalid(enum bpf_arg_type arg_curr,\n\t\t\t\t    enum bpf_arg_type arg_next)\n{\n\treturn (arg_type_is_mem_ptr(arg_curr) &&\n\t        !arg_type_is_mem_size(arg_next)) ||\n\t       (!arg_type_is_mem_ptr(arg_curr) &&\n\t\targ_type_is_mem_size(arg_next));\n}\n\nstatic bool check_arg_pair_ok(const struct bpf_func_proto *fn)\n{\n\t/* bpf_xxx(..., buf, len) call will access 'len'\n\t * bytes from memory 'buf'. Both arg types need\n\t * to be paired, so make sure there's no buggy\n\t * helper function specification.\n\t */\n\tif (arg_type_is_mem_size(fn->arg1_type) ||\n\t    arg_type_is_mem_ptr(fn->arg5_type)  ||\n\t    check_args_pair_invalid(fn->arg1_type, fn->arg2_type) ||\n\t    check_args_pair_invalid(fn->arg2_type, fn->arg3_type) ||\n\t    check_args_pair_invalid(fn->arg3_type, fn->arg4_type) ||\n\t    check_args_pair_invalid(fn->arg4_type, fn->arg5_type))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool check_refcount_ok(const struct bpf_func_proto *fn, int func_id)\n{\n\tint count = 0;\n\n\tif (arg_type_may_be_refcounted(fn->arg1_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg2_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg3_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg4_type))\n\t\tcount++;\n\tif (arg_type_may_be_refcounted(fn->arg5_type))\n\t\tcount++;\n\n\t/* A reference acquiring function cannot acquire\n\t * another refcounted ptr.\n\t */\n\tif (may_be_acquire_function(func_id) && count)\n\t\treturn false;\n\n\t/* We only support one arg being unreferenced at the moment,\n\t * which is sufficient for the helper functions we have right now.\n\t */\n\treturn count <= 1;\n}\n\nstatic int check_func_proto(const struct bpf_func_proto *fn, int func_id)\n{\n\treturn check_raw_mode_ok(fn) &&\n\t       check_arg_pair_ok(fn) &&\n\t       check_refcount_ok(fn, func_id) ? 0 : -EINVAL;\n}\n\n/* Packet data might have moved, any old PTR_TO_PACKET[_META,_END]\n * are now invalid, so turn them into unknown SCALAR_VALUE.\n */\nstatic void __clear_all_pkt_pointers(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(env, reg);\n\t}\n}\n\nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint i;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__clear_all_pkt_pointers(env, vstate->frame[i]);\n}\n\nstatic void release_reg_references(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *state,\n\t\t\t\t   int ref_obj_id)\n{\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (regs[i].ref_obj_id == ref_obj_id)\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg->ref_obj_id == ref_obj_id)\n\t\t\t__mark_reg_unknown(env, reg);\n\t}\n}\n\n/* The pointer with the specified id has released its reference to kernel\n * resources. Identify all copies of the same pointer and clear the reference.\n */\nstatic int release_reference(struct bpf_verifier_env *env,\n\t\t\t     int ref_obj_id)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tint err;\n\tint i;\n\n\terr = release_reference_state(cur_func(env), ref_obj_id);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\trelease_reg_references(env, vstate->frame[i], ref_obj_id);\n\n\treturn 0;\n}\n\nstatic void clear_caller_saved_regs(struct bpf_verifier_env *env,\n\t\t\t\t    struct bpf_reg_state *regs)\n{\n\tint i;\n\n\t/* after the call registers r0 - r5 were scratched */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n}\n\nstatic int check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t   int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_info_aux *func_info_aux;\n\tstruct bpf_func_state *caller, *callee;\n\tint i, err, subprog, target_insn;\n\tbool is_global = false;\n\n\tif (state->curframe + 1 >= MAX_CALL_FRAMES) {\n\t\tverbose(env, \"the call stack of %d frames is too deep\\n\",\n\t\t\tstate->curframe + 2);\n\t\treturn -E2BIG;\n\t}\n\n\ttarget_insn = *insn_idx + insn->imm;\n\tsubprog = find_subprog(env, target_insn + 1);\n\tif (subprog < 0) {\n\t\tverbose(env, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\ttarget_insn + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tcaller = state->frame[state->curframe];\n\tif (state->frame[state->curframe + 1]) {\n\t\tverbose(env, \"verifier bug. Frame %d already allocated\\n\",\n\t\t\tstate->curframe + 1);\n\t\treturn -EFAULT;\n\t}\n\n\tfunc_info_aux = env->prog->aux->func_info_aux;\n\tif (func_info_aux)\n\t\tis_global = func_info_aux[subprog].linkage == BTF_FUNC_GLOBAL;\n\terr = btf_check_func_arg_match(env, subprog, caller->regs);\n\tif (err == -EFAULT)\n\t\treturn err;\n\tif (is_global) {\n\t\tif (err) {\n\t\t\tverbose(env, \"Caller passes invalid args into func#%d\\n\",\n\t\t\t\tsubprog);\n\t\t\treturn err;\n\t\t} else {\n\t\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Func#%d is global and valid. Skipping.\\n\",\n\t\t\t\t\tsubprog);\n\t\t\tclear_caller_saved_regs(env, caller->regs);\n\n\t\t\t/* All global functions return SCALAR_VALUE */\n\t\t\tmark_reg_unknown(env, caller->regs, BPF_REG_0);\n\n\t\t\t/* continue with next insn after call */\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tcallee = kzalloc(sizeof(*callee), GFP_KERNEL);\n\tif (!callee)\n\t\treturn -ENOMEM;\n\tstate->frame[state->curframe + 1] = callee;\n\n\t/* callee cannot access r0, r6 - r9 for reading and has to write\n\t * into its own stack before reading from it.\n\t * callee can read/write into caller's stack\n\t */\n\tinit_func_state(env, callee,\n\t\t\t/* remember the callsite, it will be used by bpf_exit */\n\t\t\t*insn_idx /* callsite */,\n\t\t\tstate->curframe + 1 /* frameno within this callchain */,\n\t\t\tsubprog /* subprog number within this prog */);\n\n\t/* Transfer references to the callee */\n\terr = transfer_reference_state(callee, caller);\n\tif (err)\n\t\treturn err;\n\n\t/* copy r1 - r5 args that callee can access.  The copy includes parent\n\t * pointers, which connects us up to the liveness chain\n\t */\n\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\tcallee->regs[i] = caller->regs[i];\n\n\tclear_caller_saved_regs(env, caller->regs);\n\n\t/* only increment it after check_reg_arg() finished */\n\tstate->curframe++;\n\n\t/* and go analyze first insn of the callee */\n\t*insn_idx = target_insn;\n\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"caller:\\n\");\n\t\tprint_verifier_state(env, caller);\n\t\tverbose(env, \"callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t}\n\treturn 0;\n}\n\nstatic int prepare_func_exit(struct bpf_verifier_env *env, int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tstruct bpf_reg_state *r0;\n\tint err;\n\n\tcallee = state->frame[state->curframe];\n\tr0 = &callee->regs[BPF_REG_0];\n\tif (r0->type == PTR_TO_STACK) {\n\t\t/* technically it's ok to return caller's stack pointer\n\t\t * (or caller's caller's pointer) back to the caller,\n\t\t * since these pointers are valid. Only current stack\n\t\t * pointer will be invalid as soon as function exits,\n\t\t * but let's be conservative\n\t\t */\n\t\tverbose(env, \"cannot return stack pointer to the caller\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tstate->curframe--;\n\tcaller = state->frame[state->curframe];\n\t/* return to the caller whatever r0 had in the callee */\n\tcaller->regs[BPF_REG_0] = *r0;\n\n\t/* Transfer references to the caller */\n\terr = transfer_reference_state(caller, callee);\n\tif (err)\n\t\treturn err;\n\n\t*insn_idx = callee->callsite + 1;\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"returning from callee:\\n\");\n\t\tprint_verifier_state(env, callee);\n\t\tverbose(env, \"to caller at %d:\\n\", *insn_idx);\n\t\tprint_verifier_state(env, caller);\n\t}\n\t/* clear everything in the callee */\n\tfree_func_state(callee);\n\tstate->frame[state->curframe + 1] = NULL;\n\treturn 0;\n}\n\nstatic void do_refine_retval_range(struct bpf_reg_state *regs, int ret_type,\n\t\t\t\t   int func_id,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *ret_reg = &regs[BPF_REG_0];\n\n\tif (ret_type != RET_INTEGER ||\n\t    (func_id != BPF_FUNC_get_stack &&\n\t     func_id != BPF_FUNC_probe_read_str &&\n\t     func_id != BPF_FUNC_probe_read_kernel_str &&\n\t     func_id != BPF_FUNC_probe_read_user_str))\n\t\treturn;\n\n\tret_reg->smax_value = meta->msize_max_value;\n\tret_reg->s32_max_value = meta->msize_max_value;\n\t__reg_deduce_bounds(ret_reg);\n\t__reg_bound_offset(ret_reg);\n\t__update_reg_bounds(ret_reg);\n}\n\nstatic int\nrecord_func_map(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_map *map = meta->map_ptr;\n\n\tif (func_id != BPF_FUNC_tail_call &&\n\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t    func_id != BPF_FUNC_map_update_elem &&\n\t    func_id != BPF_FUNC_map_delete_elem &&\n\t    func_id != BPF_FUNC_map_push_elem &&\n\t    func_id != BPF_FUNC_map_pop_elem &&\n\t    func_id != BPF_FUNC_map_peek_elem)\n\t\treturn 0;\n\n\tif (map == NULL) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* In case of read-only, some additional restrictions\n\t * need to be applied in order to prevent altering the\n\t * state of the map from program side.\n\t */\n\tif ((map->map_flags & BPF_F_RDONLY_PROG) &&\n\t    (func_id == BPF_FUNC_map_delete_elem ||\n\t     func_id == BPF_FUNC_map_update_elem ||\n\t     func_id == BPF_FUNC_map_push_elem ||\n\t     func_id == BPF_FUNC_map_pop_elem)) {\n\t\tverbose(env, \"write into map forbidden\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (!BPF_MAP_PTR(aux->map_ptr_state))\n\t\tbpf_map_ptr_store(aux, meta->map_ptr,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\telse if (BPF_MAP_PTR(aux->map_ptr_state) != meta->map_ptr)\n\t\tbpf_map_ptr_store(aux, BPF_MAP_PTR_POISON,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\treturn 0;\n}\n\nstatic int\nrecord_func_key(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg;\n\tstruct bpf_map *map = meta->map_ptr;\n\tstruct tnum range;\n\tu64 val;\n\tint err;\n\n\tif (func_id != BPF_FUNC_tail_call)\n\t\treturn 0;\n\tif (!map || map->map_type != BPF_MAP_TYPE_PROG_ARRAY) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\trange = tnum_range(0, map->max_entries - 1);\n\treg = &regs[BPF_REG_3];\n\n\tif (!register_is_const(reg) || !tnum_in(range, reg->var_off)) {\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\t\treturn 0;\n\t}\n\n\terr = mark_chain_precision(env, BPF_REG_3);\n\tif (err)\n\t\treturn err;\n\n\tval = reg->var_off.value;\n\tif (bpf_map_key_unseen(aux))\n\t\tbpf_map_key_store(aux, val);\n\telse if (!bpf_map_key_poisoned(aux) &&\n\t\t  bpf_map_key_immediate(aux) != val)\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\treturn 0;\n}\n\nstatic int check_reference_leak(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint i;\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tverbose(env, \"Unreleased reference id=%d alloc_insn=%d\\n\",\n\t\t\tstate->refs[i].id, state->refs[i].insn_idx);\n\t}\n\treturn state->acquired_refs ? -EINVAL : 0;\n}\n\nstatic int check_helper_call(struct bpf_verifier_env *env, int func_id, int insn_idx)\n{\n\tconst struct bpf_func_proto *fn = NULL;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tbool changes_data;\n\tint i, err;\n\n\t/* find function prototype */\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t/* eBPF programs must be GPL compatible to use GPL-ed functions */\n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* With LD_ABS/IND some JITs save/restore skb from r1. */\n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn, func_id);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\tmeta.func_id = func_id;\n\t/* check args */\n\tfor (i = 0; i < 5; i++) {\n\t\tif (!fn->check_btf_id) {\n\t\t\terr = btf_resolve_helper_id(&env->log, fn, i);\n\t\t\tif (err > 0)\n\t\t\t\tmeta.btf_id = err;\n\t\t}\n\t\terr = check_func_arg(env, i, &meta, fn);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_key(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t/* Mark slots with STACK_MISC in case of raw mode, stack offset\n\t * is inferred from register state.\n\t */\n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (func_id == BPF_FUNC_tail_call) {\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t} else if (is_release_function(func_id)) {\n\t\terr = release_reference(env, meta.ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"func %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tregs = cur_regs(env);\n\n\t/* check that flags argument in get_local_storage(map, flags) is 0,\n\t * this is required because get_local_storage() can't return an error.\n\t */\n\tif (func_id == BPF_FUNC_get_local_storage &&\n\t    !register_is_null(&regs[BPF_REG_2])) {\n\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* reset caller saved regs */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* helper call returns 64-bit value. */\n\tregs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t/* update return register (already marked as written above) */\n\tif (fn->ret_type == RET_INTEGER) {\n\t\t/* sets type to SCALAR_VALUE */\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t} else if (fn->ret_type == RET_VOID) {\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL ||\n\t\t   fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t/* There is no offset yet applied, variable or fixed */\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t/* remember map_ptr, so that check_map_access()\n\t\t * can check 'value_size' boundary of memory access\n\t\t * to map element returned from bpf_map_lookup_elem()\n\t\t */\n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tif (fn->ret_type == RET_PTR_TO_MAP_VALUE) {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE;\n\t\t\tif (map_value_has_spin_lock(meta.map_ptr))\n\t\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t} else {\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t} else if (fn->ret_type == RET_PTR_TO_SOCKET_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_SOCK_COMMON_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCK_COMMON_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_TCP_SOCK_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_TCP_SOCK_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (fn->ret_type == RET_PTR_TO_ALLOC_MEM_OR_NULL) {\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_MEM_OR_NULL;\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\tregs[BPF_REG_0].mem_size = meta.mem_size;\n\t} else if (fn->ret_type == RET_PTR_TO_BTF_ID_OR_NULL) {\n\t\tint ret_btf_id;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID_OR_NULL;\n\t\tret_btf_id = *fn->ret_btf_id;\n\t\tif (ret_btf_id == 0) {\n\t\t\tverbose(env, \"invalid return type %d of func %s#%d\\n\",\n\t\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\t} else {\n\t\tverbose(env, \"unknown return type %d of func %s#%d\\n\",\n\t\t\tfn->ret_type, func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_ptr_cast_function(func_id)) {\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t} else if (is_acquire_function(func_id, meta.map_ptr)) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\t/* For mark_ptr_or_null_reg() */\n\t\tregs[BPF_REG_0].id = id;\n\t\t/* For release_reference() */\n\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif ((func_id == BPF_FUNC_get_stack ||\n\t     func_id == BPF_FUNC_get_task_stack) &&\n\t    !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (func_id == BPF_FUNC_get_stackid || func_id == BPF_FUNC_get_stack)\n\t\tenv->prog->call_get_stack = true;\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_add32_overflows(s64 a, s64 b)\n{\n\t/* Do the add in u32, where overflow is well-defined */\n\ts32 res = (s32)((u32)a + (u32)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s32 a, s32 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool signed_sub32_overflows(s32 a, s32 b)\n{\n\t/* Do the sub in u64, where overflow is well-defined */\n\ts32 res = (s32)((u32)a - (u32)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str[type], val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str[type], reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str[type]);\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str[type]);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic struct bpf_insn_aux_data *cur_aux(struct bpf_verifier_env *env)\n{\n\treturn &env->insn_aux_data[env->insn_idx];\n}\n\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *ptr_limit, u8 opcode, bool off_is_neg)\n{\n\tbool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t    (opcode == BPF_SUB && !off_is_neg);\n\tu32 off;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\t/* Indirect variable offset stack access is prohibited in\n\t\t * unprivileged mode so it's not handled here.\n\t\t */\n\t\toff = ptr_reg->off + ptr_reg->var_off.value;\n\t\tif (mask_to_left)\n\t\t\t*ptr_limit = MAX_BPF_STACK + off;\n\t\telse\n\t\t\t*ptr_limit = -off;\n\t\treturn 0;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (mask_to_left) {\n\t\t\t*ptr_limit = ptr_reg->umax_value + ptr_reg->off;\n\t\t} else {\n\t\t\toff = ptr_reg->smin_value + ptr_reg->off;\n\t\t\t*ptr_limit = ptr_reg->map_ptr->value_size - off;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic bool can_skip_alu_sanitation(const struct bpf_verifier_env *env,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn env->bypass_spec_v1 || BPF_SRC(insn->code) == BPF_K;\n}\n\nstatic int update_alu_sanitation_state(struct bpf_insn_aux_data *aux,\n\t\t\t\t       u32 alu_state, u32 alu_limit)\n{\n\t/* If we arrived here from different branches with different\n\t * state or limits to sanitize, then this won't work.\n\t */\n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn -EACCES;\n\n\t/* Corresponding fixup done in fixup_bpf_calls(). */\n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\treturn 0;\n}\n\nstatic int sanitize_val_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\treturn update_alu_sanitation_state(aux, BPF_ALU_NON_POINTER, 0);\n}\n\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    bool off_is_neg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t/* We already marked aux for masking from non-speculative\n\t * paths, thus we got here in the first place. We only care\n\t * to explore bad access from here.\n\t */\n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\talu_state |= ptr_is_dst_reg ?\n\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\tif (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))\n\t\treturn 0;\n\tif (update_alu_sanitation_state(aux, alu_state, alu_limit))\n\t\treturn -EACCES;\ndo_sim:\n\t/* Simulate and find potential out-of-bounds access under\n\t * speculative execution from truncation as a result of\n\t * masking when off was not within expected range. If off\n\t * sits in dst, then we temporarily need to move ptr there\n\t * to simulate dst (== 0) +/-= ptr. Needed, for example,\n\t * for cases where we use K-based arithmetic in one direction\n\t * and truncated reg-based in the other in order to explore\n\t * bad access.\n\t */\n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\t*dst_reg = *ptr_reg;\n\t}\n\tret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? -EFAULT : 0;\n}\n\n/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.\n * Caller should also handle BPF_MOV case separately.\n * If we return -EACCES, caller may want to try again treating pointer as a\n * scalar.  So we only emit a diagnostic if !env->allow_ptr_leaks.\n */\nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t/* Taint dst register if offset had invalid bounds derived from\n\t\t * e.g. dead branches.\n\t\t */\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops on pointers produce (meaningless) scalars */\n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str[ptr_reg->type]);\n\t\treturn -EACCES;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (!env->allow_ptr_leaks && !known && (smin_val < 0) != (smax_val < 0)) {\n\t\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, pointer arithmetic with it prohibited for !root\\n\",\n\t\t\t\toff_reg == dst_reg ? dst : src);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In case of 'scalar += pointer', dst_reg inherits pointer type and id.\n\t * The id may be overwritten later if we create a new variable offset.\n\t */\n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t/* pointer types do not carry 32-bit bounds at the moment. */\n\t__mark_reg32_unbounded(dst_reg);\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\t/* We can take a fixed offset as long as it doesn't overflow\n\t\t * the s32 'off' field\n\t\t */\n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t/* pointer += K.  Accumulate it into fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  Note that off_reg->off\n\t\t * == 0, since it's a scalar.\n\t\t * dst_reg gets the pointer type and since some positive\n\t\t * integer value was added to the pointer, give it a new 'id'\n\t\t * if it's a PTR_TO_PACKET.\n\t\t * this creates a new 'base' pointer, off_reg (variable) gets\n\t\t * added into the variable offset, and we copy the fixed offset\n\t\t * from ptr_reg.\n\t\t */\n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different maps or paths\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tif (dst_reg == off_reg) {\n\t\t\t/* scalar -= pointer.  Creates an unknown scalar */\n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t/* We don't allow subtraction from FP, because (according to\n\t\t * test_verifier.c test \"invalid fp arithmetic\", JITs might not\n\t\t * be able to deal with it.\n\t\t */\n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t/* pointer -= K.  Subtract it from fixed offset */\n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t/* A new variable offset is created.  If the subtrahend is known\n\t\t * nonnegative, then any reg->range we had before is still good.\n\t\t */\n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t/* Overflow possible, we know nothing */\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t/* something was added to pkt_ptr, set range to zero */\n\t\t\tif (smin_val < 0)\n\t\t\t\tdst_reg->raw = 0;\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t/* bitwise ops on pointers are troublesome, prohibit. */\n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t/* other operators (e.g. MUL,LSH) produce non-pointer results */\n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\n\t/* For unprivileged we require that resulting offset must be in bounds\n\t * in order to be able to sanitize access later on.\n\t */\n\tif (!env->bypass_spec_v1) {\n\t\tif (dst_reg->type == PTR_TO_MAP_VALUE &&\n\t\t    check_map_access(env, dst, dst_reg->off, 1, false)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t} else if (dst_reg->type == PTR_TO_STACK &&\n\t\t\t   check_stack_access(env, dst_reg, dst_reg->off +\n\t\t\t\t\t      dst_reg->var_off.value, 1)) {\n\t\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void scalar32_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_add32_overflows(dst_reg->s32_min_value, smin_val) ||\n\t    signed_add32_overflows(dst_reg->s32_max_value, smax_val)) {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value += smin_val;\n\t\tdst_reg->s32_max_value += smax_val;\n\t}\n\tif (dst_reg->u32_min_value + umin_val < umin_val ||\n\t    dst_reg->u32_max_value + umax_val < umax_val) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value += umin_val;\n\t\tdst_reg->u32_max_value += umax_val;\n\t}\n}\n\nstatic void scalar_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value += smin_val;\n\t\tdst_reg->smax_value += smax_val;\n\t}\n\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value += umin_val;\n\t\tdst_reg->umax_value += umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_sub32_overflows(dst_reg->s32_min_value, smax_val) ||\n\t    signed_sub32_overflows(dst_reg->s32_max_value, smin_val)) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value -= smax_val;\n\t\tdst_reg->s32_max_value -= smin_val;\n\t}\n\tif (dst_reg->u32_min_value < umax_val) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\tdst_reg->u32_min_value -= umax_val;\n\t\tdst_reg->u32_max_value -= umin_val;\n\t}\n}\n\nstatic void scalar_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value -= smax_val;\n\t\tdst_reg->smax_value -= smin_val;\n\t}\n\tif (dst_reg->umin_value < umax_val) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\t/* Cannot overflow (as long as bounds are consistent) */\n\t\tdst_reg->umin_value -= umax_val;\n\t\tdst_reg->umax_value -= umin_val;\n\t}\n}\n\nstatic void scalar32_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (smin_val < 0 || dst_reg->s32_min_value < 0) {\n\t\t/* Ain't nobody got time to multiply that sign */\n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t/* Both values are positive, so we can work with unsigned and\n\t * copy the result to signed (unless it exceeds S32_MAX).\n\t */\n\tif (umax_val > U16_MAX || dst_reg->u32_max_value > U16_MAX) {\n\t\t/* Potential overflow, we know nothing */\n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->u32_min_value *= umin_val;\n\tdst_reg->u32_max_value *= umax_val;\n\tif (dst_reg->u32_max_value > S32_MAX) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t/* Ain't nobody got time to multiply that sign */\n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t/* Both values are positive, so we can work with unsigned and\n\t * copy the result to signed (unless it exceeds S64_MAX).\n\t */\n\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t/* Potential overflow, we know nothing */\n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->umin_value *= umin_val;\n\tdst_reg->umax_value *= umax_val;\n\tif (dst_reg->umax_value > S64_MAX) {\n\t\t/* Overflow possible, we know nothing */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n}\n\nstatic void scalar32_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\t/* Assuming scalar64_min_max_and will be called so its safe\n\t * to skip updating register for known 32-bit case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our minimum from the var_off, since that's inherently\n\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t */\n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = min(dst_reg->u32_max_value, umax_val);\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ANDing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n\n}\n\nstatic void scalar_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value &\n\t\t\t\t\t  src_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our minimum from the var_off, since that's inherently\n\t * bitwise.  Our maximum is the minimum of the operands' maxima.\n\t */\n\tdst_reg->umin_value = dst_reg->var_off.value;\n\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ANDing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t/* ANDing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t      struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t  src_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\tdst_reg->umax_value = dst_reg->var_off.value | dst_reg->var_off.mask;\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void __scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t/* We lose all sign bit information (except what we can pick\n\t * up from var_off)\n\t */\n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\t/* If we might shift our top bit out, then we know nothing */\n\tif (umax_val > 31 || dst_reg->u32_max_value > 1ULL << (31 - umax_val)) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value <<= umin_val;\n\t\tdst_reg->u32_max_value <<= umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\t/* u32 alu operation will zext upper bits */\n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\tdst_reg->var_off = tnum_subreg(tnum_lshift(subreg, umin_val));\n\t/* Not required but being careful mark reg64 bounds as unknown so\n\t * that we are forced to pick them up from tnum and zext later and\n\t * if some path skips this step we are still safe.\n\t */\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void __scalar64_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t/* Special case <<32 because it is a common compiler pattern to sign\n\t * extend subreg by doing <<32 s>>32. In this case if 32bit bounds are\n\t * positive we know this shift will also be positive so we can track\n\t * bounds correctly. Otherwise we lose all sign bit information except\n\t * what we can pick up from var_off. Perhaps we can generalize this\n\t * later to shifts of any length.\n\t */\n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_max_value >= 0)\n\t\tdst_reg->smax_value = (s64)dst_reg->s32_max_value << 32;\n\telse\n\t\tdst_reg->smax_value = S64_MAX;\n\n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_min_value >= 0)\n\t\tdst_reg->smin_value = (s64)dst_reg->s32_min_value << 32;\n\telse\n\t\tdst_reg->smin_value = S64_MIN;\n\n\t/* If we might shift our top bit out, then we know nothing */\n\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value <<= umin_val;\n\t\tdst_reg->umax_value <<= umax_val;\n\t}\n}\n\nstatic void scalar_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* scalar64 calc uses 32bit unshifted bounds so must be called first */\n\t__scalar64_min_max_lsh(dst_reg, umin_val, umax_val);\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\n\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t * be negative, then either:\n\t * 1) src_reg might be zero, so the sign bit of the result is\n\t *    unknown, so we lose our signed bounds\n\t * 2) it's known negative, thus the unsigned bounds capture the\n\t *    signed bounds\n\t * 3) the signed bounds cross zero, so they tell us nothing\n\t *    about the result\n\t * If the value in dst_reg is known nonnegative, then again the\n\t * unsigned bounts capture the signed bounds.\n\t * Thus, in all cases it suffices to blow away our signed bounds\n\t * and rely on inferring new ones from the unsigned bounds and\n\t * var_off of the result.\n\t */\n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\n\tdst_reg->var_off = tnum_rshift(subreg, umin_val);\n\tdst_reg->u32_min_value >>= umax_val;\n\tdst_reg->u32_max_value >>= umin_val;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* BPF_RSH is an unsigned shift.  If the value in dst_reg might\n\t * be negative, then either:\n\t * 1) src_reg might be zero, so the sign bit of the result is\n\t *    unknown, so we lose our signed bounds\n\t * 2) it's known negative, thus the unsigned bounds capture the\n\t *    signed bounds\n\t * 3) the signed bounds cross zero, so they tell us nothing\n\t *    about the result\n\t * If the value in dst_reg is known nonnegative, then again the\n\t * unsigned bounts capture the signed bounds.\n\t * Thus, in all cases it suffices to blow away our signed bounds\n\t * and rely on inferring new ones from the unsigned bounds and\n\t * var_off of the result.\n\t */\n\tdst_reg->smin_value = S64_MIN;\n\tdst_reg->smax_value = S64_MAX;\n\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\tdst_reg->umin_value >>= umax_val;\n\tdst_reg->umax_value >>= umin_val;\n\n\t/* Its not easy to operate on alu32 bounds here because it depends\n\t * on bits being shifted in. Take easy way out and mark unbounded\n\t * so we can recalculate later from tnum.\n\t */\n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t  struct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->u32_min_value;\n\n\t/* Upon reaching here, src_known is true and\n\t * umax_val is equal to umin_val.\n\t */\n\tdst_reg->s32_min_value = (u32)(((s32)dst_reg->s32_min_value) >> umin_val);\n\tdst_reg->s32_max_value = (u32)(((s32)dst_reg->s32_max_value) >> umin_val);\n\n\tdst_reg->var_off = tnum_arshift(tnum_subreg(dst_reg->var_off), umin_val, 32);\n\n\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t * dst_reg var_off to refine the result.\n\t */\n\tdst_reg->u32_min_value = 0;\n\tdst_reg->u32_max_value = U32_MAX;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->umin_value;\n\n\t/* Upon reaching here, src_known is true and umax_val is equal\n\t * to umin_val.\n\t */\n\tdst_reg->smin_value >>= umin_val;\n\tdst_reg->smax_value >>= umin_val;\n\n\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val, 64);\n\n\t/* blow away the dst_reg umin_value/umax_value and rely on\n\t * dst_reg var_off to refine the result.\n\t */\n\tdst_reg->umin_value = 0;\n\tdst_reg->umax_value = U64_MAX;\n\n\t/* Its not easy to operate on alu32 bounds here because it depends\n\t * on bits being shifted in from upper 32-bits. Take easy way out\n\t * and mark unbounded so we can recalculate later from tnum.\n\t */\n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\n/* WARNING: This function does calculations on 64-bit values, but the actual\n * execution may occur on 32-bit values. Therefore, things like bitshifts\n * need extra checks in the 32-bit case.\n */\nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\ts32 s32_min_val, s32_max_val;\n\tu32 u32_min_val, u32_max_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\tbool alu32 = (BPF_CLASS(insn->code) != BPF_ALU64);\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\n\ts32_min_val = src_reg.s32_min_value;\n\ts32_max_val = src_reg.s32_max_value;\n\tu32_min_val = src_reg.u32_min_value;\n\tu32_max_val = src_reg.u32_max_value;\n\n\tif (alu32) {\n\t\tsrc_known = tnum_subreg_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (s32_min_val != s32_max_val || u32_min_val != u32_max_val)) ||\n\t\t    s32_min_val > s32_max_val || u32_min_val > u32_max_val) {\n\t\t\t/* Taint dst register if offset had invalid bounds\n\t\t\t * derived from e.g. dead branches.\n\t\t\t */\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tsrc_known = tnum_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (smin_val != smax_val || umin_val != umax_val)) ||\n\t\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t\t/* Taint dst register if offset had invalid bounds\n\t\t\t * derived from e.g. dead branches.\n\t\t\t */\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\t/* Calculate sign/unsigned bounds and tnum for alu32 and alu64 bit ops.\n\t * There are two classes of instructions: The first class we track both\n\t * alu32 and alu64 sign/unsigned bounds independently this provides the\n\t * greatest amount of precision when alu operations are mixed with jmp32\n\t * operations. These operations are BPF_ADD, BPF_SUB, BPF_MUL, BPF_ADD,\n\t * and BPF_OR. This is possible because these ops have fairly easy to\n\t * understand and calculate behavior in both 32-bit and 64-bit alu ops.\n\t * See alu32 verifier tests for examples. The second class of\n\t * operations, BPF_LSH, BPF_RSH, and BPF_ARSH, however are not so easy\n\t * with regards to tracking sign/unsigned bounds because the bits may\n\t * cross subreg boundaries in the alu64 case. When this happens we mark\n\t * the reg unbounded in the subreg bound space and use the resulting\n\t * tnum to calculate an approximation of the sign/unsigned bounds.\n\t */\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to add from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tscalar32_min_max_add(dst_reg, &src_reg);\n\t\tscalar_min_max_add(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"R%d tried to sub from different pointers or scalars\\n\", dst);\n\t\t\treturn ret;\n\t\t}\n\t\tscalar32_min_max_sub(dst_reg, &src_reg);\n\t\tscalar_min_max_sub(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_mul(dst_reg, &src_reg);\n\t\tscalar_min_max_mul(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_AND:\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_and(dst_reg, &src_reg);\n\t\tscalar_min_max_and(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_or(dst_reg, &src_reg);\n\t\tscalar_min_max_or(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_lsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_lsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_rsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_rsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t/* Shifts greater than 31 or 63 are undefined.\n\t\t\t * This includes shifts by a negative number.\n\t\t\t */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_arsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_arsh(dst_reg, &src_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\t/* ALU32 ops are zero extended into 64bit register */\n\tif (alu32)\n\t\tzext_32_to_64(dst_reg);\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}\n\n/* Handles ALU ops other than BPF_END, BPF_NEG and BPF_MOV: computes new min/max\n * and var_off.\n */\nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar. Disallow all math except\n\t\t\t\t * pointer subtraction\n\t\t\t\t */\n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) /* pointer += K */\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n/* check validity of 32-bit and 64-bit arithmetic operations */\nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != 0 ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand, mark as required later */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t/* case: R1 = R2\n\t\t\t\t * copy register state to dest reg\n\t\t\t\t */\n\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\tdst_reg->subreg_def = DEF_NOT_SUBREG;\n\t\t\t} else {\n\t\t\t\t/* R1 = (u32) R2 */\n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t*dst_reg = *src_reg;\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\tdst_reg->subreg_def = env->insn_idx + 1;\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tzext_32_to_64(dst_reg);\n\t\t\t}\n\t\t} else {\n\t\t\t/* case: R = imm\n\t\t\t * remember the value we stored into this reg\n\t\t\t */\n\t\t\t/* clear any state __mark_reg_known doesn't set */\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t/* all other ALU ops: and, sub, xor, add, ... */\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check src2 operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t/* check dest operand */\n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void __find_good_pkt_pointers(struct bpf_func_state *state,\n\t\t\t\t     struct bpf_reg_state *dst_reg,\n\t\t\t\t     enum bpf_reg_type type, u16 new_range)\n{\n\tstruct bpf_reg_state *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t/* keep the maximum range already checked */\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\treg->range = max(reg->range, new_range);\n\t}\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tu16 new_range;\n\tint i;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t/* This doesn't give us any range */\n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t/* Risk of overflow.  For instance, ptr + (1<<63) may be less\n\t\t * than pkt_end, but that's because it's also less than pkt.\n\t\t */\n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range--;\n\n\t/* Examples for register markings:\n\t *\n\t * pkt_data in dst register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 > pkt_end) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (r2 < pkt_end) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   Where:\n\t *     r2 == dst_reg, pkt_end == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * pkt_data in src register:\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end >= r2) goto <access okay>\n\t *   <handle exception>\n\t *\n\t *   r2 = r3;\n\t *   r2 += 8;\n\t *   if (pkt_end <= r2) goto <handle exception>\n\t *   <access okay>\n\t *\n\t *   Where:\n\t *     pkt_end == dst_reg, r2 == src_reg\n\t *     r2=pkt(id=n,off=8,r=0)\n\t *     r3=pkt(id=n,off=0,r=0)\n\t *\n\t * Find register r3 and mark its range as r3=pkt(id=n,off=0,r=8)\n\t * or r3=pkt(id=n,off=0,r=8-1), so that range of bytes [r3, r3 + 8)\n\t * and [r3, r3 + 8-1) respectively is safe to access depending on\n\t * the check.\n\t */\n\n\t/* If our ids match, then we must have the same max_value.  And we\n\t * don't care about the other reg's fixed offset, since if it's too big\n\t * the range won't allow anything.\n\t * dst_reg->off is known < MAX_PACKET_OFF, therefore it fits in a u16.\n\t */\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__find_good_pkt_pointers(vstate->frame[i], dst_reg, type,\n\t\t\t\t\t new_range);\n}\n\nstatic int is_branch32_taken(struct bpf_reg_state *reg, u32 val, u8 opcode)\n{\n\tstruct tnum subreg = tnum_subreg(reg->var_off);\n\ts32 sval = (s32)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !!tnum_equals_const(subreg, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !tnum_equals_const(subreg, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~subreg.mask & subreg.value) & val)\n\t\t\treturn 1;\n\t\tif (!((subreg.mask | subreg.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->u32_min_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->s32_min_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->u32_max_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->s32_max_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->u32_min_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->s32_min_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->u32_max_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->s32_max_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n\nstatic int is_branch64_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\ts64 sval = (s64)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n/* compute branch direction of the expression \"if (reg opcode val) goto target;\"\n * and return:\n *  1 - branch will be taken and \"goto target\" will be executed\n *  0 - branch will not be taken and fall-through to next insn\n * -1 - unknown. Example: \"if (reg < 5)\" is unknown when register value\n *      range [0,10]\n */\nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode,\n\t\t\t   bool is_jmp32)\n{\n\tif (__is_pointer_value(false, reg)) {\n\t\tif (!reg_type_not_null(reg->type))\n\t\t\treturn -1;\n\n\t\t/* If pointer is valid tests against zero will fail so we can\n\t\t * use this to direct branch taken.\n\t\t */\n\t\tif (val != 0)\n\t\t\treturn -1;\n\n\t\tswitch (opcode) {\n\t\tcase BPF_JEQ:\n\t\t\treturn 0;\n\t\tcase BPF_JNE:\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (is_jmp32)\n\t\treturn is_branch32_taken(reg, val, opcode);\n\treturn is_branch64_taken(reg, val, opcode);\n}\n\n/* Adjusts the register min/max values in the case that the dst_reg is the\n * variable register that we are working on, and src_reg is a constant or we're\n * simply doing a BPF_K check.\n * In JEQ/JNE cases we also adjust the var_off values.\n */\nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg,\n\t\t\t    u64 val, u32 val32,\n\t\t\t    u8 opcode, bool is_jmp32)\n{\n\tstruct tnum false_32off = tnum_subreg(false_reg->var_off);\n\tstruct tnum false_64off = false_reg->var_off;\n\tstruct tnum true_32off = tnum_subreg(true_reg->var_off);\n\tstruct tnum true_64off = true_reg->var_off;\n\ts64 sval = (s64)val;\n\ts32 sval32 = (s32)val32;\n\n\t/* If the dst_reg is a pointer, we can't learn anything about its\n\t * variable offset from the compare (unless src_reg were a pointer into\n\t * the same object, but we don't bother with that.\n\t * Since false_reg and true_reg have the same type by construction, we\n\t * only need to check one of them for pointerness.\n\t */\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\n\t\t/* For BPF_JEQ, if this is false we know nothing Jon Snow, but\n\t\t * if it is true we know the value for sure. Likewise for\n\t\t * BPF_JNE.\n\t\t */\n\t\tif (is_jmp32)\n\t\t\t__mark_reg32_known(reg, val32);\n\t\telse\n\t\t\t__mark_reg_known(reg, val);\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tif (is_jmp32) {\n\t\t\tfalse_32off = tnum_and(false_32off, tnum_const(~val32));\n\t\t\tif (is_power_of_2(val32))\n\t\t\t\ttrue_32off = tnum_or(true_32off,\n\t\t\t\t\t\t     tnum_const(val32));\n\t\t} else {\n\t\t\tfalse_64off = tnum_and(false_64off, tnum_const(~val));\n\t\t\tif (is_power_of_2(val))\n\t\t\t\ttrue_64off = tnum_or(true_64off,\n\t\t\t\t\t\t     tnum_const(val));\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umax = opcode == BPF_JGT ? val32  : val32 - 1;\n\t\t\tu32 true_umin = opcode == BPF_JGT ? val32 + 1 : val32;\n\n\t\t\tfalse_reg->u32_max_value = min(false_reg->u32_max_value,\n\t\t\t\t\t\t       false_umax);\n\t\t\ttrue_reg->u32_min_value = max(true_reg->u32_min_value,\n\t\t\t\t\t\t      true_umin);\n\t\t} else {\n\t\t\tu64 false_umax = opcode == BPF_JGT ? val    : val - 1;\n\t\t\tu64 true_umin = opcode == BPF_JGT ? val + 1 : val;\n\n\t\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smax = opcode == BPF_JSGT ? sval32    : sval32 - 1;\n\t\t\ts32 true_smin = opcode == BPF_JSGT ? sval32 + 1 : sval32;\n\n\t\t\tfalse_reg->s32_max_value = min(false_reg->s32_max_value, false_smax);\n\t\t\ttrue_reg->s32_min_value = max(true_reg->s32_min_value, true_smin);\n\t\t} else {\n\t\t\ts64 false_smax = opcode == BPF_JSGT ? sval    : sval - 1;\n\t\t\ts64 true_smin = opcode == BPF_JSGT ? sval + 1 : sval;\n\n\t\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umin = opcode == BPF_JLT ? val32  : val32 + 1;\n\t\t\tu32 true_umax = opcode == BPF_JLT ? val32 - 1 : val32;\n\n\t\t\tfalse_reg->u32_min_value = max(false_reg->u32_min_value,\n\t\t\t\t\t\t       false_umin);\n\t\t\ttrue_reg->u32_max_value = min(true_reg->u32_max_value,\n\t\t\t\t\t\t      true_umax);\n\t\t} else {\n\t\t\tu64 false_umin = opcode == BPF_JLT ? val    : val + 1;\n\t\t\tu64 true_umax = opcode == BPF_JLT ? val - 1 : val;\n\n\t\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smin = opcode == BPF_JSLT ? sval32    : sval32 + 1;\n\t\t\ts32 true_smax = opcode == BPF_JSLT ? sval32 - 1 : sval32;\n\n\t\t\tfalse_reg->s32_min_value = max(false_reg->s32_min_value, false_smin);\n\t\t\ttrue_reg->s32_max_value = min(true_reg->s32_max_value, true_smax);\n\t\t} else {\n\t\t\ts64 false_smin = opcode == BPF_JSLT ? sval    : sval + 1;\n\t\t\ts64 true_smax = opcode == BPF_JSLT ? sval - 1 : sval;\n\n\t\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (is_jmp32) {\n\t\tfalse_reg->var_off = tnum_or(tnum_clear_subreg(false_64off),\n\t\t\t\t\t     tnum_subreg(false_32off));\n\t\ttrue_reg->var_off = tnum_or(tnum_clear_subreg(true_64off),\n\t\t\t\t\t    tnum_subreg(true_32off));\n\t\t__reg_combine_32_into_64(false_reg);\n\t\t__reg_combine_32_into_64(true_reg);\n\t} else {\n\t\tfalse_reg->var_off = false_64off;\n\t\ttrue_reg->var_off = true_64off;\n\t\t__reg_combine_64_into_32(false_reg);\n\t\t__reg_combine_64_into_32(true_reg);\n\t}\n}\n\n/* Same as above, but for the case that dst_reg holds a constant and src_reg is\n * the variable reg.\n */\nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg,\n\t\t\t\tu64 val, u32 val32,\n\t\t\t\tu8 opcode, bool is_jmp32)\n{\n\t/* How can we transform \"a <op> b\" into \"b <op> a\"? */\n\tstatic const u8 opcode_flip[16] = {\n\t\t/* these stay the same */\n\t\t[BPF_JEQ  >> 4] = BPF_JEQ,\n\t\t[BPF_JNE  >> 4] = BPF_JNE,\n\t\t[BPF_JSET >> 4] = BPF_JSET,\n\t\t/* these swap \"lesser\" and \"greater\" (L and G in the opcodes) */\n\t\t[BPF_JGE  >> 4] = BPF_JLE,\n\t\t[BPF_JGT  >> 4] = BPF_JLT,\n\t\t[BPF_JLE  >> 4] = BPF_JGE,\n\t\t[BPF_JLT  >> 4] = BPF_JGT,\n\t\t[BPF_JSGE >> 4] = BPF_JSLE,\n\t\t[BPF_JSGT >> 4] = BPF_JSLT,\n\t\t[BPF_JSLE >> 4] = BPF_JSGE,\n\t\t[BPF_JSLT >> 4] = BPF_JSGT\n\t};\n\topcode = opcode_flip[opcode >> 4];\n\t/* This uses zero as \"not present in table\"; luckily the zero opcode,\n\t * BPF_JA, can't get here.\n\t */\n\tif (opcode)\n\t\treg_set_min_max(true_reg, false_reg, val, val32, opcode, is_jmp32);\n}\n\n/* Regs are known to be equal, so intersect their min/max/var_off */\nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\t/* We might have learned new bounds from the var_off. */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n\t/* We might have learned something about the sign bit. */\n\t__reg_deduce_bounds(src_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(src_reg);\n\t__reg_bound_offset(dst_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(src_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (reg_type_may_be_null(reg->type) && reg->id == id) {\n\t\t/* Old offset (both fixed and variable parts) should\n\t\t * have been known-zero, because we don't allow pointer\n\t\t * arithmetic on pointers that might be NULL.\n\t\t */\n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value ||\n\t\t\t\t !tnum_equals_const(reg->var_off, 0) ||\n\t\t\t\t reg->off)) {\n\t\t\t__mark_reg_known_zero(reg);\n\t\t\treg->off = 0;\n\t\t}\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t} else if (reg->type == PTR_TO_MAP_VALUE_OR_NULL) {\n\t\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\t\tif (map->inner_map_meta) {\n\t\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\t\treg->type = PTR_TO_SOCKET;\n\t\t\t} else {\n\t\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t\t}\n\t\t} else if (reg->type == PTR_TO_SOCKET_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else if (reg->type == PTR_TO_SOCK_COMMON_OR_NULL) {\n\t\t\treg->type = PTR_TO_SOCK_COMMON;\n\t\t} else if (reg->type == PTR_TO_TCP_SOCK_OR_NULL) {\n\t\t\treg->type = PTR_TO_TCP_SOCK;\n\t\t} else if (reg->type == PTR_TO_BTF_ID_OR_NULL) {\n\t\t\treg->type = PTR_TO_BTF_ID;\n\t\t} else if (reg->type == PTR_TO_MEM_OR_NULL) {\n\t\t\treg->type = PTR_TO_MEM;\n\t\t} else if (reg->type == PTR_TO_RDONLY_BUF_OR_NULL) {\n\t\t\treg->type = PTR_TO_RDONLY_BUF;\n\t\t} else if (reg->type == PTR_TO_RDWR_BUF_OR_NULL) {\n\t\t\treg->type = PTR_TO_RDWR_BUF;\n\t\t}\n\t\tif (is_null) {\n\t\t\t/* We don't need id and ref_obj_id from this point\n\t\t\t * onwards anymore, thus we should better reset it,\n\t\t\t * so that state pruning has chances to take effect.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t\treg->ref_obj_id = 0;\n\t\t} else if (!reg_may_point_to_spin_lock(reg)) {\n\t\t\t/* For not-NULL ptr, reg->ref_obj_id will be reset\n\t\t\t * in release_reg_references().\n\t\t\t *\n\t\t\t * reg->id is still used by spin_lock ptr. Other\n\t\t\t * than spin_lock ptr type, reg->id can be reset.\n\t\t\t */\n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\nstatic void __mark_ptr_or_null_regs(struct bpf_func_state *state, u32 id,\n\t\t\t\t    bool is_null)\n{\n\tstruct bpf_reg_state *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tmark_ptr_or_null_reg(state, &state->regs[i], id, is_null);\n\n\tbpf_for_each_spilled_reg(i, state, reg) {\n\t\tif (!reg)\n\t\t\tcontinue;\n\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t}\n}\n\n/* The logic is similar to find_good_pkt_pointers(), both could eventually\n * be folded together at some point.\n */\nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs;\n\tu32 ref_obj_id = regs[regno].ref_obj_id;\n\tu32 id = regs[regno].id;\n\tint i;\n\n\tif (ref_obj_id && ref_obj_id == id && is_null)\n\t\t/* regs[regno] is in the \" == NULL\" branch.\n\t\t * No one could have freed the reference state before\n\t\t * doing the NULL check.\n\t\t */\n\t\tWARN_ON_ONCE(release_reference_state(state, id));\n\n\tfor (i = 0; i <= vstate->curframe; i++)\n\t\t__mark_ptr_or_null_regs(vstate->frame[i], id, is_null);\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\t/* Pointers are always 64-bit. */\n\tif (BPF_CLASS(insn->code) == BPF_JMP32)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' > pkt_end, pkt_meta' > pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end > pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' < pkt_end, pkt_meta' < pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end < pkt_data', pkt_data > pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' >= pkt_end, pkt_meta' >= pkt_data */\n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end >= pkt_data', pkt_data >= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t/* pkt_data' <= pkt_end, pkt_meta' <= pkt_data */\n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t/* pkt_end <= pkt_data', pkt_data <= pkt_meta' */\n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs, *src_reg = NULL;\n\tu8 opcode = BPF_OP(insn->code);\n\tbool is_jmp32;\n\tint pred = -1;\n\tint err;\n\n\t/* Only conditional jumps are expected to reach here. */\n\tif (opcode == BPF_JA || opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP/JMP32 opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* check src1 operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tsrc_reg = &regs[insn->src_reg];\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* check src2 operand */\n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tis_jmp32 = BPF_CLASS(insn->code) == BPF_JMP32;\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tpred = is_branch_taken(dst_reg, insn->imm, opcode, is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(src_reg->var_off))) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       tnum_subreg(src_reg->var_off).value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(src_reg->var_off)) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       src_reg->var_off.value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t}\n\n\tif (pred >= 0) {\n\t\t/* If we get here with a dst_reg pointer type it is because\n\t\t * above is_branch_taken() special cased the 0 comparison.\n\t\t */\n\t\tif (!__is_pointer_value(false, dst_reg))\n\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\tif (BPF_SRC(insn->code) == BPF_X && !err)\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (pred == 1) {\n\t\t/* only follow the goto, ignore fall-through */\n\t\t*insn_idx += insn->off;\n\t\treturn 0;\n\t} else if (pred == 0) {\n\t\t/* only follow fall-through branch, since\n\t\t * that's where the program will go\n\t\t */\n\t\treturn 0;\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t/* detect if we are comparing against a constant value so we can adjust\n\t * our min/max values for our dst register.\n\t * this is only legit if both are scalars (or pointers to the same\n\t * object, I suppose, but we don't support that right now), because\n\t * otherwise the different base pointers mean the offsets aren't\n\t * comparable.\n\t */\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tstruct bpf_reg_state *src_reg = &regs[insn->src_reg];\n\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    src_reg->type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(src_reg->var_off) ||\n\t\t\t    (is_jmp32 &&\n\t\t\t     tnum_is_const(tnum_subreg(src_reg->var_off))))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg,\n\t\t\t\t\t\tsrc_reg->var_off.value,\n\t\t\t\t\t\ttnum_subreg(src_reg->var_off).value,\n\t\t\t\t\t\topcode, is_jmp32);\n\t\t\telse if (tnum_is_const(dst_reg->var_off) ||\n\t\t\t\t (is_jmp32 &&\n\t\t\t\t  tnum_is_const(tnum_subreg(dst_reg->var_off))))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    src_reg,\n\t\t\t\t\t\t    dst_reg->var_off.value,\n\t\t\t\t\t\t    tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t\t\t    opcode, is_jmp32);\n\t\t\telse if (!is_jmp32 &&\n\t\t\t\t (opcode == BPF_JEQ || opcode == BPF_JNE))\n\t\t\t\t/* Comparing for equality, we can combine knowledge */\n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    src_reg, dst_reg, opcode);\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, (u32)insn->imm,\n\t\t\t\t\topcode, is_jmp32);\n\t}\n\n\t/* detect if R == 0 where R is returned from bpf_map_lookup_elem().\n\t * NOTE: these optimizations below are related with pointer comparison\n\t *       which will never be JMP32.\n\t */\n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    reg_type_may_be_null(dst_reg->type)) {\n\t\t/* Mark all identical registers in each branch as either\n\t\t * safe or unknown depending R == 0 or R != 0 conditional.\n\t\t */\n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_verifier_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n/* verify BPF_LD_IMM64 instruction */\nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map;\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\tmap = env->used_maps[aux->map_index];\n\tmark_reg_known_zero(env, regs, insn->dst_reg);\n\tregs[insn->dst_reg].map_ptr = map;\n\n\tif (insn->src_reg == BPF_PSEUDO_MAP_VALUE) {\n\t\tregs[insn->dst_reg].type = PTR_TO_MAP_VALUE;\n\t\tregs[insn->dst_reg].off = aux->map_off;\n\t\tif (map_value_has_spin_lock(map))\n\t\t\tregs[insn->dst_reg].id = ++env->id_gen;\n\t} else if (insn->src_reg == BPF_PSEUDO_MAP_FD) {\n\t\tregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\n\t} else {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n/* verify safety of LD_ABS|LD_IND instructions:\n * - they can only appear in the programs where ctx == skb\n * - since they are wrappers of function calls, they scratch R1-R5 registers,\n *   preserve R6-R9, and store return value into R0\n *\n * Implicit input:\n *   ctx == skb == R6 == CTX\n *\n * Explicit input:\n *   SRC == any register\n *   IMM == 32-bit immediate\n *\n * Output:\n *   R0 - 8/16/32-bit skb data converted to cpu endianness\n */\nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstatic const int ctx_reg = BPF_REG_6;\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(env->prog->type)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->subprog_cnt > 1) {\n\t\t/* when program has LD_ABS insn JITs and interpreter assume\n\t\t * that r1 == ctx == skb which is not the case for callees\n\t\t * that can have arbitrary arguments. It's problematic\n\t\t * for main prog as well since JITs would need to analyze\n\t\t * all functions in order to make proper register save/restore\n\t\t * decisions in the main prog. Hence disallow LD_ABS with calls\n\t\t */\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions cannot be mixed with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* check whether implicit source operand (register R6) is readable */\n\terr = check_reg_arg(env, ctx_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t/* Disallow usage of BPF_LD_[ABS|IND] with reference tracking, as\n\t * gen_ld_abs() may terminate the program at runtime, leading to\n\t * reference leak.\n\t */\n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (env->cur_state->active_spin_lock) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be used inside bpf_spin_lock-ed region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (regs[ctx_reg].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t/* check explicit source operand */\n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = check_ctx_reg(env, &regs[ctx_reg], ctx_reg);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* reset caller saved regs to unreadable */\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t/* mark destination R0 register as readable, since it contains\n\t * the value fetched from the packet.\n\t * Already marked as written above.\n\t */\n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t/* ld_abs load up to 32-bit skb data. */\n\tregs[BPF_REG_0].subreg_def = env->insn_idx + 1;\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1);\n\tint err;\n\n\t/* LSM and struct_ops func-ptr's return type could be \"void\" */\n\tif ((env->prog->type == BPF_PROG_TYPE_STRUCT_OPS ||\n\t     env->prog->type == BPF_PROG_TYPE_LSM) &&\n\t    !prog->aux->attach_func_proto->type)\n\t\treturn 0;\n\n\t/* eBPF calling convetion is such that R0 is used\n\t * to return the value from eBPF program.\n\t * Make sure that it's readable at this time\n\t * of bpf_exit, which means that program wrote\n\t * something into it earlier\n\t */\n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tswitch (env->prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t/* freplace program can return anything as its return value\n\t\t * depends on the to-be-replaced kernel func or bpf program.\n\t\t */\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str[reg->type]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\tverbose(env, \"At program exit the register R0 \");\n\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"has value %s\", tn_buf);\n\t\t} else {\n\t\t\tverbose(env, \"has unknown scalar value\");\n\t\t}\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), range);\n\t\tverbose(env, \" should have been in %s\\n\", tn_buf);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}\n\n/* non-recursive DFS pseudo code\n * 1  procedure DFS-iterative(G,v):\n * 2      label v as discovered\n * 3      let S be a stack\n * 4      S.push(v)\n * 5      while S is not empty\n * 6            t <- S.pop()\n * 7            if t is what we're looking for:\n * 8                return t\n * 9            for all edges e in G.adjacentEdges(t) do\n * 10               if edge e is already labelled\n * 11                   continue with the next edge\n * 12               w <- G.adjacentVertex(t,e)\n * 13               if vertex w is not discovered and not explored\n * 14                   label e as tree-edge\n * 15                   label w as discovered\n * 16                   S.push(w)\n * 17                   continue at 5\n * 18               else if vertex w is discovered\n * 19                   label e as back-edge\n * 20               else\n * 21                   // vertex w is explored\n * 22                   label e as forward- or cross-edge\n * 23           label t as explored\n * 24           S.pop()\n *\n * convention:\n * 0x10 - discovered\n * 0x11 - discovered and fall-through edge labelled\n * 0x12 - discovered and fall-through and branch edges labelled\n * 0x20 - explored\n */\n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\nstatic u32 state_htab_size(struct bpf_verifier_env *env)\n{\n\treturn env->prog->len;\n}\n\nstatic struct bpf_verifier_state_list **explored_state(\n\t\t\t\t\tstruct bpf_verifier_env *env,\n\t\t\t\t\tint idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_func_state *state = cur->frame[cur->curframe];\n\n\treturn &env->explored_states[(idx ^ state->callsite) % state_htab_size(env)];\n}\n\nstatic void init_explored_state(struct bpf_verifier_env *env, int idx)\n{\n\tenv->insn_aux_data[idx].prune_point = true;\n}\n\n/* t, w, e - match pseudo-code above:\n * t - index of current instruction\n * w - next instruction\n * e - edge\n */\nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env,\n\t\t     bool loop_ok)\n{\n\tint *insn_stack = env->cfg.insn_stack;\n\tint *insn_state = env->cfg.insn_state;\n\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn 0;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn 0;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH)\n\t\t/* mark branch target for state pruning */\n\t\tinit_explored_state(env, w);\n\n\tif (insn_state[w] == 0) {\n\t\t/* tree-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (env->cfg.cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[env->cfg.cur_stack++] = w;\n\t\treturn 1;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tif (loop_ok && env->bpf_capable)\n\t\t\treturn 0;\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t/* forward- or cross-edge */\n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n/* non-recursive depth-first-search to detect loops in BPF program\n * loop == back-edge in directed graph\n */\nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint *insn_stack, *insn_state;\n\tint ret = 0;\n\tint i, t;\n\n\tinsn_state = env->cfg.insn_state = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = env->cfg.insn_stack = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkvfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED; /* mark 1st insn as discovered */\n\tinsn_stack[0] = 0; /* 0 is the first instruction */\n\tenv->cfg.cur_stack = 1;\n\npeek_stack:\n\tif (env->cfg.cur_stack == 0)\n\t\tgoto check_state;\n\tt = insn_stack[env->cfg.cur_stack - 1];\n\n\tif (BPF_CLASS(insns[t].code) == BPF_JMP ||\n\t    BPF_CLASS(insns[t].code) == BPF_JMP32) {\n\t\tu8 opcode = BPF_OP(insns[t].code);\n\n\t\tif (opcode == BPF_EXIT) {\n\t\t\tgoto mark_explored;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, false);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tinit_explored_state(env, t + 1);\n\t\t\tif (insns[t].src_reg == BPF_PSEUDO_CALL) {\n\t\t\t\tinit_explored_state(env, t);\n\t\t\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH,\n\t\t\t\t\t\tenv, false);\n\t\t\t\tif (ret == 1)\n\t\t\t\t\tgoto peek_stack;\n\t\t\t\telse if (ret < 0)\n\t\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (opcode == BPF_JA) {\n\t\t\tif (BPF_SRC(insns[t].code) != BPF_K) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\t/* unconditional jump with single edge */\n\t\t\tret = push_insn(t, t + insns[t].off + 1,\n\t\t\t\t\tFALLTHROUGH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t\t/* unconditional jmp is not a good pruning point,\n\t\t\t * but it's marked, since backtracking needs\n\t\t\t * to record jmp history in is_state_visited().\n\t\t\t */\n\t\t\tinit_explored_state(env, t + insns[t].off + 1);\n\t\t\t/* tell verifier to check for equivalent states\n\t\t\t * after every call and jump\n\t\t\t */\n\t\t\tif (t + 1 < insn_cnt)\n\t\t\t\tinit_explored_state(env, t + 1);\n\t\t} else {\n\t\t\t/* conditional jump with two edges */\n\t\t\tinit_explored_state(env, t);\n\t\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\n\t\t\tret = push_insn(t, t + insns[t].off + 1, BRANCH, env, true);\n\t\t\tif (ret == 1)\n\t\t\t\tgoto peek_stack;\n\t\t\telse if (ret < 0)\n\t\t\t\tgoto err_free;\n\t\t}\n\t} else {\n\t\t/* all other non-branch instructions with single\n\t\t * fall-through edge\n\t\t */\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env, false);\n\t\tif (ret == 1)\n\t\t\tgoto peek_stack;\n\t\telse if (ret < 0)\n\t\t\tgoto err_free;\n\t}\n\nmark_explored:\n\tinsn_state[t] = EXPLORED;\n\tif (env->cfg.cur_stack-- <= 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\tgoto peek_stack;\n\ncheck_state:\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\tret = 0; /* cfg looks good */\n\nerr_free:\n\tkvfree(insn_state);\n\tkvfree(insn_stack);\n\tenv->cfg.insn_state = env->cfg.insn_stack = NULL;\n\treturn ret;\n}\n\n/* The minimum supported BTF func info size */\n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, nfuncs, urec_size, min_size;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tstruct bpf_func_info_aux *info_aux = NULL;\n\tconst struct btf_type *type;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *urecord;\n\tu32 prev_offset = 0;\n\tint ret = -ENOMEM;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs)\n\t\treturn 0;\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = u64_to_user_ptr(attr->func_info);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\tinfo_aux = kcalloc(nfuncs, sizeof(*info_aux), GFP_KERNEL | __GFP_NOWARN);\n\tif (!info_aux)\n\t\tgoto err_free;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t/* set the size kernel expects so loader can zero\n\t\t\t\t * out the rest of the record.\n\t\t\t\t */\n\t\t\t\tif (put_user(min_size, &uattr->func_info_rec_size))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check insn_off */\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/* check type_id */\n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || !btf_type_is_func(type)) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t\tinfo_aux[i].linkage = BTF_INFO_VLEN(type->info);\n\t\tprev_offset = krecord[i].insn_off;\n\t\turecord += urec_size;\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\tprog->aux->func_info_aux = info_aux;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\tkfree(info_aux);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i;\n\n\tif (!aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\taux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\t(offsetof(struct bpf_line_info, line_col) + \\\n\t\tsizeof(((struct bpf_line_info *)(0))->line_col))\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tvoid __user *ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t/* Need to zero it in case the userspace may\n\t * pass in a smaller bpf_line_info object.\n\t */\n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = u64_to_user_ptr(attr->line_info);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (put_user(expected_size,\n\t\t\t\t\t     &uattr->line_info_rec_size))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_user(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t/*\n\t\t * Check insn_off to ensure\n\t\t * 1) strictly increasing AND\n\t\t * 2) bounded by prog->len\n\t\t *\n\t\t * The linfo[0].insn_off == 0 check logically falls into\n\t\t * the later \"missing bpf_line_info for func...\" case\n\t\t * because the first linfo[0].insn_off must be the\n\t\t * first sub also and the first sub must have\n\t\t * subprog_info[0].start == 0.\n\t\t */\n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tulinfo += rec_size;\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt)\n\t\treturn 0;\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n/* check %cur's range satisfies %old's */\nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value;\n}\n\n/* Maximum number of register states that can exist at once */\n#define ID_MAP_SIZE\t(MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE)\nstruct idpair {\n\tu32 old;\n\tu32 cur;\n};\n\n/* If in the old state two registers had the same id, then they need to have\n * the same id in the new state as well.  But that id could be different from\n * the old state, so we need to track the mapping from old to new ids.\n * Once we have seen that, say, a reg with old id 5 had new id 9, any subsequent\n * regs with old id 5 must also have new id 9 for the new state to be safe.  But\n * regs with a different old id could still have new id 9, we don't care about\n * that.\n * So we look through our idmap to see if this old id has been seen before.  If\n * so, we require the new id to match; otherwise, we add the id pair to the map.\n */\nstatic bool check_ids(u32 old_id, u32 cur_id, struct idpair *idmap)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t/* liveness must not touch this register anymore */\n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t/* since the register is unused, clear its state\n\t\t\t * to make further comparison simpler\n\t\t\t */\n\t\t\t__mark_reg_not_init(env, &st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t/* liveness must not touch this stack slot anymore */\n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(env, &st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t/* all regs in this state in all frames were already marked */\n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n/* the parentage chains form a tree.\n * the verifier states are added to state lists at given insn and\n * pushed into state stack for future exploration.\n * when the verifier reaches bpf_exit insn some of the verifer states\n * stored in the state lists have their final liveness state already,\n * but a lot of states will get revised from liveness point of view when\n * the verifier explores other branches.\n * Example:\n * 1: r0 = 1\n * 2: if r1 == 100 goto pc+1\n * 3: r0 = 2\n * 4: exit\n * when the verifier reaches exit insn the register r0 in the state list of\n * insn 2 will be seen as !REG_LIVE_READ. Then the verifier pops the other_branch\n * of insn 2 and goes exploring further. At the insn 4 it will walk the\n * parentage chain from insn 4 into insn 2 and will mark r0 as REG_LIVE_READ.\n *\n * Since the verifier pushes the branch states as it sees them while exploring\n * the program the condition of walking the branch instruction for the second\n * time means that all states below this branch were already explored and\n * their final liveness markes are already propagated.\n * Hence when the verifier completes the search of state list in is_state_visited()\n * we can call this clean_live_states() function to mark all liveness states\n * as REG_LIVE_DONE to indicate that 'parent' pointers of 'struct bpf_reg_state'\n * will not be used.\n * This function also clears the registers and stack for states that !READ\n * to simplify state merging.\n *\n * Important note here that walking the same branch instruction in the callee\n * doesn't meant that the states are DONE. The verifier has to compare\n * the callsites\n */\nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = *explored_state(env, insn);\n\twhile (sl) {\n\t\tif (sl->state.branches)\n\t\t\tgoto next;\n\t\tif (sl->state.insn_idx != insn ||\n\t\t    sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\n/* Returns true if (rold safe implies rcur safe) */\nstatic bool regsafe(struct bpf_reg_state *rold, struct bpf_reg_state *rcur,\n\t\t    struct idpair *idmap)\n{\n\tbool equal;\n\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t/* explored state didn't use this */\n\t\treturn true;\n\n\tequal = memcmp(rold, rcur, offsetof(struct bpf_reg_state, parent)) == 0;\n\n\tif (rold->type == PTR_TO_STACK)\n\t\t/* two stack pointers are equal only if they're pointing to\n\t\t * the same stack frame, since fp-8 in foo != fp-8 in bar\n\t\t */\n\t\treturn equal && rold->frameno == rcur->frameno;\n\n\tif (equal)\n\t\treturn true;\n\n\tif (rold->type == NOT_INIT)\n\t\t/* explored state can't have used this */\n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\tswitch (rold->type) {\n\tcase SCALAR_VALUE:\n\t\tif (rcur->type == SCALAR_VALUE) {\n\t\t\tif (!rold->precise && !rcur->precise)\n\t\t\t\treturn true;\n\t\t\t/* new val must satisfy old val knowledge */\n\t\t\treturn range_within(rold, rcur) &&\n\t\t\t       tnum_in(rold->var_off, rcur->var_off);\n\t\t} else {\n\t\t\t/* We're trying to use a pointer in place of a scalar.\n\t\t\t * Even if the scalar was unbounded, this could lead to\n\t\t\t * pointer leaks because scalars are allowed to leak\n\t\t\t * while pointers are not. We could make this safe in\n\t\t\t * special cases if root is calling us, but it's\n\t\t\t * probably not worth the hassle.\n\t\t\t */\n\t\t\treturn false;\n\t\t}\n\tcase PTR_TO_MAP_VALUE:\n\t\t/* If the new min/max/var_off satisfy the old ones and\n\t\t * everything else matches, we are OK.\n\t\t * 'id' is not compared, since it's only used for maps with\n\t\t * bpf_spin_lock inside map element and in such cases if\n\t\t * the rest of the prog is valid for one map element then\n\t\t * it's valid for all map elements regardless of the key\n\t\t * used in bpf_map_lookup()\n\t\t */\n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_MAP_VALUE_OR_NULL:\n\t\t/* a PTR_TO_MAP_VALUE could be safe to use as a\n\t\t * PTR_TO_MAP_VALUE_OR_NULL into the same map.\n\t\t * However, if the old PTR_TO_MAP_VALUE_OR_NULL then got NULL-\n\t\t * checked, doing so could have affected others with the same\n\t\t * id, and we can't check for that because we lost the id when\n\t\t * we converted to a PTR_TO_MAP_VALUE.\n\t\t */\n\t\tif (rcur->type != PTR_TO_MAP_VALUE_OR_NULL)\n\t\t\treturn false;\n\t\tif (memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)))\n\t\t\treturn false;\n\t\t/* Check our ids match any regs they're supposed to */\n\t\treturn check_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\tif (rcur->type != rold->type)\n\t\t\treturn false;\n\t\t/* We must have at least as much range as the old ptr\n\t\t * did, so that any accesses which were safe before are\n\t\t * still safe.  This is true even if old range < old off,\n\t\t * since someone could have accessed through (ptr - k), or\n\t\t * even done ptr -= k in a register, to get a safe access.\n\t\t */\n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t/* If the offsets don't match, we can't trust our alignment;\n\t\t * nor can we be sure that we won't fall out of range.\n\t\t */\n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t/* id relations must be preserved */\n\t\tif (rold->id && !check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t/* new val must satisfy old val knowledge */\n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_CTX:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\t\t/* Only valid matches are exact, which memcmp() above\n\t\t * would have accepted\n\t\t */\n\tdefault:\n\t\t/* Don't know what's going on, just say it's not safe */\n\t\treturn false;\n\t}\n\n\t/* Shouldn't get here; if we do, say it's not safe */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\nstatic bool stacksafe(struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur,\n\t\t      struct idpair *idmap)\n{\n\tint i, spi;\n\n\t/* walk slots of the explored stack and ignore any additional\n\t * slots in the current stack, since explored(safe) state\n\t * didn't use them\n\t */\n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t/* explored state didn't use this */\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\t/* explored stack has more populated slots than current stack\n\t\t * and these slots were used\n\t\t */\n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t/* if old state was safe with misc data in the stack\n\t\t * it will be safe with zero-initialized stack.\n\t\t * The opposite is not true\n\t\t */\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t/* Ex: old explored (safe) state has STACK_SPILL in\n\t\t\t * this stack slot, but current has STACK_MISC ->\n\t\t\t * this verifier states are not equivalent,\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tif (!regsafe(&old->stack[spi].spilled_ptr,\n\t\t\t     &cur->stack[spi].spilled_ptr,\n\t\t\t     idmap))\n\t\t\t/* when explored and current stack slot are both storing\n\t\t\t * spilled registers, check that stored pointers types\n\t\t\t * are the same as well.\n\t\t\t * Ex: explored safe path could have stored\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -8}\n\t\t\t * but current path has stored:\n\t\t\t * (bpf_reg_state) {.type = PTR_TO_STACK, .off = -16}\n\t\t\t * such verifier states are not equivalent.\n\t\t\t * return false to continue verification of this path\n\t\t\t */\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur)\n{\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\treturn !memcmp(old->refs, cur->refs,\n\t\t       sizeof(*old->refs) * old->acquired_refs);\n}\n\n/* compare two verifier states\n *\n * all states stored in state_list are known to be valid, since\n * verifier reached 'bpf_exit' instruction through them\n *\n * this function is called when verifier exploring different branches of\n * execution popped from the state stack. If it sees an old state that has\n * more strict register state and more strict stack state then this execution\n * branch doesn't need to be explored further, since verifier already\n * concluded that more strict state leads to valid finish.\n *\n * Therefore two states are equivalent if register state is more conservative\n * and explored stack state is more conservative than the current one.\n * Example:\n *       explored                   current\n * (slot1=INV slot2=MISC) == (slot1=MISC slot2=MISC)\n * (slot1=MISC slot2=MISC) != (slot1=INV slot2=MISC)\n *\n * In other words if current stack state (one being explored) has more\n * valid slots than old one that already passed validation, it means\n * the verifier can stop exploring and conclude that current state is valid too\n *\n * Similarly with registers. If explored state has register type as invalid\n * whereas register type in current state is meaningful, it means that\n * the current state will reach 'bpf_exit' instruction safely\n */\nstatic bool func_states_equal(struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\n\tif (!refsafe(old, cur))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}\n\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\t/* Verification state from speculative execution simulation\n\t * must never prune a non-speculative execution one.\n\t */\n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\tif (old->active_spin_lock != cur->active_spin_lock)\n\t\treturn false;\n\n\t/* for states to be equal callsites have to be the same\n\t * and all frame states need to be equivalent\n\t */\n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n/* Return 0 if no propagation happened. Return negative error code if error\n * happened. Otherwise, return the propagated bit.\n */\nstatic int propagate_liveness_reg(struct bpf_verifier_env *env,\n\t\t\t\t  struct bpf_reg_state *reg,\n\t\t\t\t  struct bpf_reg_state *parent_reg)\n{\n\tu8 parent_flag = parent_reg->live & REG_LIVE_READ;\n\tu8 flag = reg->live & REG_LIVE_READ;\n\tint err;\n\n\t/* When comes here, read flags of PARENT_REG or REG could be any of\n\t * REG_LIVE_READ64, REG_LIVE_READ32, REG_LIVE_NONE. There is no need\n\t * of propagation if PARENT_REG has strongest REG_LIVE_READ64.\n\t */\n\tif (parent_flag == REG_LIVE_READ64 ||\n\t    /* Or if there is no read flag from REG. */\n\t    !flag ||\n\t    /* Or if the read flag from REG is the same as PARENT_REG. */\n\t    parent_flag == flag)\n\t\treturn 0;\n\n\terr = mark_reg_read(env, reg, parent_reg, flag);\n\tif (err)\n\t\treturn err;\n\n\treturn flag;\n}\n\n/* A write screens off any subsequent reads; but write marks come from the\n * straight-line code between a state and its parent.  When we arrive at an\n * equivalent state (jump target or such) we didn't arrive by the straight-line\n * code, so read marks in the state must propagate to the parent regardless\n * of the state's write marks. That's what 'parent == state->parent' comparison\n * in mark_reg_read() is for.\n */\nstatic int propagate_liveness(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_verifier_state *vstate,\n\t\t\t      struct bpf_verifier_state *vparent)\n{\n\tstruct bpf_reg_state *state_reg, *parent_reg;\n\tstruct bpf_func_state *state, *parent;\n\tint i, frame, err = 0;\n\n\tif (vparent->curframe != vstate->curframe) {\n\t\tWARN(1, \"propagate_live: parent frame %d current frame %d\\n\",\n\t\t     vparent->curframe, vstate->curframe);\n\t\treturn -EFAULT;\n\t}\n\t/* Propagate read liveness of registers... */\n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\tfor (frame = 0; frame <= vstate->curframe; frame++) {\n\t\tparent = vparent->frame[frame];\n\t\tstate = vstate->frame[frame];\n\t\tparent_reg = parent->regs;\n\t\tstate_reg = state->regs;\n\t\t/* We don't need to worry about FP liveness, it's read-only */\n\t\tfor (i = frame < vstate->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++) {\n\t\t\terr = propagate_liveness_reg(env, &state_reg[i],\n\t\t\t\t\t\t     &parent_reg[i]);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tif (err == REG_LIVE_READ64)\n\t\t\t\tmark_insn_zext(env, &parent_reg[i]);\n\t\t}\n\n\t\t/* Propagate stack slots. */\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tparent_reg = &parent->stack[i].spilled_ptr;\n\t\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\t\terr = propagate_liveness_reg(env, state_reg,\n\t\t\t\t\t\t     parent_reg);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* find precise scalars in the previous equivalent state and\n * propagate them into the current state\n */\nstatic int propagate_precision(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_verifier_state *old)\n{\n\tstruct bpf_reg_state *state_reg;\n\tstruct bpf_func_state *state;\n\tint i, err = 0;\n\n\tstate = old->frame[old->curframe];\n\tstate_reg = state->regs;\n\tfor (i = 0; i < BPF_REG_FP; i++, state_reg++) {\n\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t    !state_reg->precise)\n\t\t\tcontinue;\n\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\tverbose(env, \"propagating r%d\\n\", i);\n\t\terr = mark_chain_precision(env, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t    !state_reg->precise)\n\t\t\tcontinue;\n\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\tverbose(env, \"propagating fp%d\\n\",\n\t\t\t\t(-i - 1) * BPF_REG_SIZE);\n\t\terr = mark_chain_precision_stack(env, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic bool states_maybe_looping(struct bpf_verifier_state *old,\n\t\t\t\t struct bpf_verifier_state *cur)\n{\n\tstruct bpf_func_state *fold, *fcur;\n\tint i, fr = cur->curframe;\n\n\tif (old->curframe != fr)\n\t\treturn false;\n\n\tfold = old->frame[fr];\n\tfcur = cur->frame[fr];\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (memcmp(&fold->regs[i], &fcur->regs[i],\n\t\t\t   offsetof(struct bpf_reg_state, parent)))\n\t\t\treturn false;\n\treturn true;\n}\n\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl, **pprev;\n\tstruct bpf_verifier_state *cur = env->cur_state, *new;\n\tint i, j, err, states_cnt = 0;\n\tbool add_new_state = env->test_state_freq ? true : false;\n\n\tcur->last_insn_idx = env->prev_insn_idx;\n\tif (!env->insn_aux_data[insn_idx].prune_point)\n\t\t/* this 'insn_idx' instruction wasn't marked, so we will not\n\t\t * be doing state search here\n\t\t */\n\t\treturn 0;\n\n\t/* bpf progs typically have pruning point every 4 instructions\n\t * http://vger.kernel.org/bpfconf2019.html#session-1\n\t * Do not add new state for future pruning if the verifier hasn't seen\n\t * at least 2 jumps and at least 8 instructions.\n\t * This heuristics helps decrease 'total_states' and 'peak_states' metric.\n\t * In tests that amounts to up to 50% reduction into total verifier\n\t * memory consumption and 20% verifier time speedup.\n\t */\n\tif (env->jmps_processed - env->prev_jmps_processed >= 2 &&\n\t    env->insn_processed - env->prev_insn_processed >= 8)\n\t\tadd_new_state = true;\n\n\tpprev = explored_state(env, insn_idx);\n\tsl = *pprev;\n\n\tclean_live_states(env, insn_idx, cur);\n\n\twhile (sl) {\n\t\tstates_cnt++;\n\t\tif (sl->state.insn_idx != insn_idx)\n\t\t\tgoto next;\n\t\tif (sl->state.branches) {\n\t\t\tif (states_maybe_looping(&sl->state, cur) &&\n\t\t\t    states_equal(env, &sl->state, cur)) {\n\t\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\t\tverbose(env, \"infinite loop detected at insn %d\\n\", insn_idx);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* if the verifier is processing a loop, avoid adding new state\n\t\t\t * too often, since different loop iterations have distinct\n\t\t\t * states and may not help future pruning.\n\t\t\t * This threshold shouldn't be too low to make sure that\n\t\t\t * a loop with large bound will be rejected quickly.\n\t\t\t * The most abusive loop will be:\n\t\t\t * r1 += 1\n\t\t\t * if r1 < 1000000 goto pc-2\n\t\t\t * 1M insn_procssed limit / 100 == 10k peak states.\n\t\t\t * This threshold shouldn't be too high either, since states\n\t\t\t * at the end of the loop are likely to be useful in pruning.\n\t\t\t */\n\t\t\tif (env->jmps_processed - env->prev_jmps_processed < 20 &&\n\t\t\t    env->insn_processed - env->prev_insn_processed < 100)\n\t\t\t\tadd_new_state = false;\n\t\t\tgoto miss;\n\t\t}\n\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\tsl->hit_cnt++;\n\t\t\t/* reached equivalent register/stack state,\n\t\t\t * prune the search.\n\t\t\t * Registers read by the continuation are read by us.\n\t\t\t * If we have any write marks in env->cur_state, they\n\t\t\t * will prevent corresponding reads in the continuation\n\t\t\t * from reaching our parent (an explored_state).  Our\n\t\t\t * own state will get the read marks recorded, but\n\t\t\t * they'll be immediately forgotten as we're pruning\n\t\t\t * this state and will pop a new one.\n\t\t\t */\n\t\t\terr = propagate_liveness(env, &sl->state, cur);\n\n\t\t\t/* if previous state reached the exit with precision and\n\t\t\t * current state is equivalent to it (except precsion marks)\n\t\t\t * the precision needs to be propagated back in\n\t\t\t * the current state.\n\t\t\t */\n\t\t\terr = err ? : push_jmp_history(env, cur);\n\t\t\terr = err ? : propagate_precision(env, &sl->state);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn 1;\n\t\t}\nmiss:\n\t\t/* when new state is not going to be added do not increase miss count.\n\t\t * Otherwise several loop iterations will remove the state\n\t\t * recorded earlier. The goal of these heuristics is to have\n\t\t * states from some iterations of the loop (some in the beginning\n\t\t * and some at the end) to help pruning.\n\t\t */\n\t\tif (add_new_state)\n\t\t\tsl->miss_cnt++;\n\t\t/* heuristic to determine whether this state is beneficial\n\t\t * to keep checking from state equivalence point of view.\n\t\t * Higher numbers increase max_states_per_insn and verification time,\n\t\t * but do not meaningfully decrease insn_processed.\n\t\t */\n\t\tif (sl->miss_cnt > sl->hit_cnt * 3 + 3) {\n\t\t\t/* the state is unlikely to be useful. Remove it to\n\t\t\t * speed up verification\n\t\t\t */\n\t\t\t*pprev = sl->next;\n\t\t\tif (sl->state.frame[0]->regs[0].live & REG_LIVE_DONE) {\n\t\t\t\tu32 br = sl->state.branches;\n\n\t\t\t\tWARN_ONCE(br,\n\t\t\t\t\t  \"BUG live_done but branches_to_explore %d\\n\",\n\t\t\t\t\t  br);\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tenv->peak_states--;\n\t\t\t} else {\n\t\t\t\t/* cannot free this state, since parentage chain may\n\t\t\t\t * walk it later. Add it for free_list instead to\n\t\t\t\t * be freed at the end of verification\n\t\t\t\t */\n\t\t\t\tsl->next = env->free_list;\n\t\t\t\tenv->free_list = sl;\n\t\t\t}\n\t\t\tsl = *pprev;\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tpprev = &sl->next;\n\t\tsl = *pprev;\n\t}\n\n\tif (env->max_states_per_insn < states_cnt)\n\t\tenv->max_states_per_insn = states_cnt;\n\n\tif (!env->bpf_capable && states_cnt > BPF_COMPLEXITY_LIMIT_STATES)\n\t\treturn push_jmp_history(env, cur);\n\n\tif (!add_new_state)\n\t\treturn push_jmp_history(env, cur);\n\n\t/* There were no equivalent states, remember the current one.\n\t * Technically the current state is not proven to be safe yet,\n\t * but it will either reach outer most bpf_exit (which means it's safe)\n\t * or it will be rejected. When there are no loops the verifier won't be\n\t * seeing this tuple (frame[0].callsite, frame[1].callsite, .. insn_idx)\n\t * again on the way to bpf_exit.\n\t * When looping the sl->state.branches will be > 0 and this state\n\t * will not be considered for equivalence until branches == 0.\n\t */\n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\tenv->total_states++;\n\tenv->peak_states++;\n\tenv->prev_jmps_processed = env->jmps_processed;\n\tenv->prev_insn_processed = env->insn_processed;\n\n\t/* add new state to the head of linked list */\n\tnew = &new_sl->state;\n\terr = copy_verifier_state(new, cur);\n\tif (err) {\n\t\tfree_verifier_state(new, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew->insn_idx = insn_idx;\n\tWARN_ONCE(new->branches != 1,\n\t\t  \"BUG is_state_visited:branches_to_explore=%d insn %d\\n\", new->branches, insn_idx);\n\n\tcur->parent = new;\n\tcur->first_insn_idx = insn_idx;\n\tclear_jmp_history(cur);\n\tnew_sl->next = *explored_state(env, insn_idx);\n\t*explored_state(env, insn_idx) = new_sl;\n\t/* connect new state to parentage chain. Current frame needs all\n\t * registers connected. Only r6 - r9 of the callers are alive (pushed\n\t * to the stack implicitly by JITs) so in callers' frames connect just\n\t * r6 - r9 as an optimization. Callers will have r1 - r5 connected to\n\t * the state of the call instruction (with WRITTEN set), and r0 comes\n\t * from callee with its full parentage chain, anyway.\n\t */\n\t/* clear write marks in current state: the writes we did are not writes\n\t * our child did, so they don't screen off its reads from us.\n\t * (There are no read marks in current state, because reads always mark\n\t * their parent and current state never has children yet.  Only\n\t * explored_states can get read marks.)\n\t */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tfor (i = j < cur->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].parent = &new->frame[j]->regs[i];\n\t\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].live = REG_LIVE_NONE;\n\t}\n\n\t/* all stack frames are accessible from callee, clear them all */\n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tstruct bpf_func_state *frame = cur->frame[j];\n\t\tstruct bpf_func_state *newframe = new->frame[j];\n\n\t\tfor (i = 0; i < frame->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tframe->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\t\t\tframe->stack[i].spilled_ptr.parent =\n\t\t\t\t\t\t&newframe->stack[i].spilled_ptr;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/* Return true if it's OK to have the same insn return a different type. */\nstatic bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (type) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCKET_OR_NULL:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_SOCK_COMMON_OR_NULL:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_TCP_SOCK_OR_NULL:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID_OR_NULL:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n/* If an instruction was previously used with particular pointer types, then we\n * need to be careful to avoid cases such as the below, where it may be ok\n * for one branch accessing the pointer, but not ok for the other branch:\n *\n * R1 = sock_ptr\n * goto X;\n * ...\n * R1 = some_other_valid_ptr;\n * goto X;\n * ...\n * R2 = *(u32 *)(R1 + 0);\n */\nstatic bool reg_type_mismatch(enum bpf_reg_type src, enum bpf_reg_type prev)\n{\n\treturn src != prev && (!reg_type_mismatch_ok(src) ||\n\t\t\t       !reg_type_mismatch_ok(prev));\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\terr = is_state_visited(env, env->insn_idx);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (err == 1) {\n\t\t\t/* found equivalent state, can prune the search */\n\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\tif (do_print_state)\n\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t}\n\t\t\tgoto process_bpf_exit;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 ||\n\t\t    (env->log.level & BPF_LOG_LEVEL && do_print_state)) {\n\t\t\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\t\t\tverbose(env, \"%d:\", env->insn_idx);\n\t\t\telse\n\t\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe]);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t}\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tenv->insn_aux_data[env->insn_idx].seen = env->pass_cnt;\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type *prev_src_type, src_reg_type;\n\n\t\t\t/* check for reserved fields is already done */\n\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t/* check that memory (src_reg + off) is readable,\n\t\t\t * the state of dst_reg will be updated by this func\n\t\t\t */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_src_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_src_type == NOT_INIT) {\n\t\t\t\t/* saw a valid insn\n\t\t\t\t * dst_reg = *(u32 *)(src_reg + off)\n\t\t\t\t * save type to validate intersecting paths\n\t\t\t\t */\n\t\t\t\t*prev_src_type = src_reg_type;\n\n\t\t\t} else if (reg_type_mismatch(src_reg_type, *prev_src_type)) {\n\t\t\t\t/* ABuser program is trying to use the same insn\n\t\t\t\t * dst_reg = *(u32*) (src_reg + off)\n\t\t\t\t * with different pointer types:\n\t\t\t\t * src_reg == ctx in one branch and\n\t\t\t\t * src_reg == stack|map in some other branch.\n\t\t\t\t * Reject it.\n\t\t\t\t */\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type *prev_dst_type, dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_XADD) {\n\t\t\t\terr = check_xadd(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* check src1 operand */\n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t/* check src2 operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tprev_dst_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\t\t\tif (*prev_dst_type == NOT_INIT) {\n\t\t\t\t*prev_dst_type = dst_reg_type;\n\t\t\t} else if (reg_type_mismatch(dst_reg_type, *prev_dst_type)) {\n\t\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t} else if (class == BPF_ST) {\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t/* check src operand */\n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tif (is_ctx_reg(env, insn->dst_reg)) {\n\t\t\t\tverbose(env, \"BPF_ST stores into R%d %s is not allowed\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\treg_type_str[reg_state(env, insn->dst_reg)->type]);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\t/* check that memory (dst_reg + off) is writeable */\n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->off != 0 ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock &&\n\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL ||\n\t\t\t\t     insn->imm != BPF_FUNC_spin_unlock)) {\n\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn->imm, env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_spin_lock) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t/* exit from nested function */\n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tenv->insn_aux_data[env->insn_idx].seen = env->pass_cnt;\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_map_prealloc(struct bpf_map *map)\n{\n\treturn (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\tmap->map_type != BPF_MAP_TYPE_HASH_OF_MAPS) ||\n\t\t!(map->map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic bool is_tracing_prog_type(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_KPROBE:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool is_preallocated_map(struct bpf_map *map)\n{\n\tif (!check_map_prealloc(map))\n\t\treturn false;\n\tif (map->inner_map_meta && !check_map_prealloc(map->inner_map_meta))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\t/*\n\t * Validate that trace type programs use preallocated hash maps.\n\t *\n\t * For programs attached to PERF events this is mandatory as the\n\t * perf NMI can hit any arbitrary code sequence.\n\t *\n\t * All other trace types using preallocated hash maps are unsafe as\n\t * well because tracepoint or kprobes can be inside locked regions\n\t * of the memory allocator or at a place where a recursion into the\n\t * memory allocator would see inconsistent state.\n\t *\n\t * On RT enabled kernels run-time allocation of all trace type\n\t * programs is strictly prohibited due to lock type constraints. On\n\t * !RT kernels it is allowed for backwards compatibility reasons for\n\t * now, but warnings are emitted so developers are made aware of\n\t * the unsafety and can fix their programs before this is enforced.\n\t */\n\tif (is_tracing_prog_type(prog->type) && !is_preallocated_map(map)) {\n\t\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\tverbose(env, \"trace type programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tWARN_ONCE(1, \"trace type BPF program uses run-time allocation\\n\");\n\t\tverbose(env, \"trace type programs with run-time allocated hash maps are unsafe. Switch to preallocated hash maps.\\n\");\n\t}\n\n\tif ((is_tracing_prog_type(prog->type) ||\n\t     prog->type == BPF_PROG_TYPE_SOCKET_FILTER) &&\n\t    map_value_has_spin_lock(map)) {\n\t\tverbose(env, \"tracing progs cannot use bpf_spin_lock yet\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((bpf_prog_is_dev_bound(prog->aux) || bpf_map_is_dev_bound(map)) &&\n\t    !bpf_offload_prog_map_match(prog, map)) {\n\t\tverbose(env, \"offload device mismatch between prog and map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\tverbose(env, \"bpf_struct_ops map cannot be used in prog\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool bpf_map_is_cgroup_storage(struct bpf_map *map)\n{\n\treturn (map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||\n\t\tmap->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE);\n}\n\n/* look for pseudo eBPF instructions that access map FDs and\n * replace them with actual map pointers\n */\nstatic int replace_map_fd_with_map_ptr(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_STX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM &&\n\t\t      BPF_MODE(insn->code) != BPF_XADD) || insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_insn_aux_data *aux;\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\t\t\tu64 addr;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn[0].src_reg == 0)\n\t\t\t\t/* valid generic load 64-bit imm */\n\t\t\t\tgoto next_insn;\n\n\t\t\t/* In final convert_pseudo_ld_imm64() step, this is\n\t\t\t * converted into regular 64-bit imm load insn.\n\t\t\t */\n\t\t\tif ((insn[0].src_reg != BPF_PSEUDO_MAP_FD &&\n\t\t\t     insn[0].src_reg != BPF_PSEUDO_MAP_VALUE) ||\n\t\t\t    (insn[0].src_reg == BPF_PSEUDO_MAP_FD &&\n\t\t\t     insn[1].imm != 0)) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf = fdget(insn[0].imm);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn[0].imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\taux = &env->insn_aux_data[i];\n\t\t\tif (insn->src_reg == BPF_PSEUDO_MAP_FD) {\n\t\t\t\taddr = (unsigned long)map;\n\t\t\t} else {\n\t\t\t\tu32 off = insn[1].imm;\n\n\t\t\t\tif (off >= BPF_MAX_VAR_OFF) {\n\t\t\t\t\tverbose(env, \"direct value offset of %u is not allowed\\n\", off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (!map->ops->map_direct_value_addr) {\n\t\t\t\t\tverbose(env, \"no direct value access support for this map type\\n\");\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\t\t\t\tif (err) {\n\t\t\t\t\tverbose(env, \"invalid access to map value pointer, value_size=%u off=%u\\n\",\n\t\t\t\t\t\tmap->value_size, off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\n\t\t\t\taux->map_off = off;\n\t\t\t\taddr += off;\n\t\t\t}\n\n\t\t\tinsn[0].imm = (u32)addr;\n\t\t\tinsn[1].imm = addr >> 32;\n\n\t\t\t/* check whether we recorded this map already */\n\t\t\tfor (j = 0; j < env->used_map_cnt; j++) {\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\taux->map_index = j;\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t/* hold the map. If the program is rejected by verifier,\n\t\t\t * the map will be released by release_maps() or it\n\t\t\t * will be used by the valid program until it's unloaded\n\t\t\t * and all maps are released in free_used_maps()\n\t\t\t */\n\t\t\tbpf_map_inc(map);\n\n\t\t\taux->map_index = env->used_map_cnt;\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tif (bpf_map_is_cgroup_storage(map) &&\n\t\t\t    bpf_cgroup_storage_assign(env->prog->aux, map)) {\n\t\t\t\tverbose(env, \"only one cgroup storage of each type is allowed\\n\");\n\t\t\t\tfdput(f);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Basic sanity check before we invest more work here. */\n\t\tif (!bpf_opcode_in_insntable(insn->code)) {\n\t\t\tverbose(env, \"unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* now all pseudo BPF_LD_IMM64 instructions load valid\n\t * 'struct bpf_map *' into a register instead of user map_fd.\n\t * These pointers will be used later by verifier to validate map access.\n\t */\n\treturn 0;\n}\n\n/* drop refcnt of maps used by the rejected program */\nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\t__bpf_free_used_maps(env->prog->aux, env->used_maps,\n\t\t\t     env->used_map_cnt);\n}\n\n/* convert pseudo BPF_LD_IMM64 into generic BPF_LD_IMM64 */\nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++)\n\t\tif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tinsn->src_reg = 0;\n}\n\n/* single env->prog->insni[off] instruction was replaced with the range\n * insni[off, off + cnt).  Adjust corresponding insn_aux_data by copying\n * [0, off) and [off, end) to new locations, so the patched range stays zero\n */\nstatic int adjust_insn_aux_data(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_prog *new_prog, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;\n\tstruct bpf_insn *insn = new_prog->insnsi;\n\tu32 prog_len;\n\tint i;\n\n\t/* aux info at OFF always needs adjustment, no matter fast path\n\t * (cnt == 1) is taken or not. There is no guarantee INSN at OFF is the\n\t * original insn at old prog.\n\t */\n\told_data[off].zext_dst = insn_has_def32(env, insn + off + cnt - 1);\n\n\tif (cnt == 1)\n\t\treturn 0;\n\tprog_len = new_prog->len;\n\tnew_data = vzalloc(array_size(prog_len,\n\t\t\t\t      sizeof(struct bpf_insn_aux_data)));\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++) {\n\t\tnew_data[i].seen = env->pass_cnt;\n\t\tnew_data[i].zext_dst = insn_has_def32(env, insn + i);\n\t}\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n\treturn 0;\n}\n\nstatic void adjust_subprog_starts(struct bpf_verifier_env *env, u32 off, u32 len)\n{\n\tint i;\n\n\tif (len == 1)\n\t\treturn;\n\t/* NOTE: fake 'exit' subprog should be updated as well. */\n\tfor (i = 0; i <= env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].start <= off)\n\t\t\tcontinue;\n\t\tenv->subprog_info[i].start += len - 1;\n\t}\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (IS_ERR(new_prog)) {\n\t\tif (PTR_ERR(new_prog) == -ERANGE)\n\t\t\tverbose(env,\n\t\t\t\t\"insn %d cannot be patched due to 16-bit range\\n\",\n\t\t\t\tenv->insn_aux_data[off].orig_idx);\n\t\treturn NULL;\n\t}\n\tif (adjust_insn_aux_data(env, new_prog, off, len))\n\t\treturn NULL;\n\tadjust_subprog_starts(env, off, len);\n\treturn new_prog;\n}\n\nstatic int adjust_subprog_starts_after_remove(struct bpf_verifier_env *env,\n\t\t\t\t\t      u32 off, u32 cnt)\n{\n\tint i, j;\n\n\t/* find first prog starting at or after off (first to remove) */\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].start >= off)\n\t\t\tbreak;\n\t/* find first prog starting at or after off + cnt (first to stay) */\n\tfor (j = i; j < env->subprog_cnt; j++)\n\t\tif (env->subprog_info[j].start >= off + cnt)\n\t\t\tbreak;\n\t/* if j doesn't start exactly at off + cnt, we are just removing\n\t * the front of previous prog\n\t */\n\tif (env->subprog_info[j].start != off + cnt)\n\t\tj--;\n\n\tif (j > i) {\n\t\tstruct bpf_prog_aux *aux = env->prog->aux;\n\t\tint move;\n\n\t\t/* move fake 'exit' subprog as well */\n\t\tmove = env->subprog_cnt + 1 - j;\n\n\t\tmemmove(env->subprog_info + i,\n\t\t\tenv->subprog_info + j,\n\t\t\tsizeof(*env->subprog_info) * move);\n\t\tenv->subprog_cnt -= j - i;\n\n\t\t/* remove func_info */\n\t\tif (aux->func_info) {\n\t\t\tmove = aux->func_info_cnt - j;\n\n\t\t\tmemmove(aux->func_info + i,\n\t\t\t\taux->func_info + j,\n\t\t\t\tsizeof(*aux->func_info) * move);\n\t\t\taux->func_info_cnt -= j - i;\n\t\t\t/* func_info->insn_off is set after all code rewrites,\n\t\t\t * in adjust_btf_func() - no need to adjust\n\t\t\t */\n\t\t}\n\t} else {\n\t\t/* convert i from \"first prog to remove\" to \"first to adjust\" */\n\t\tif (env->subprog_info[i].start == off)\n\t\t\ti++;\n\t}\n\n\t/* update fake 'exit' subprog as well */\n\tfor (; i <= env->subprog_cnt; i++)\n\t\tenv->subprog_info[i].start -= cnt;\n\n\treturn 0;\n}\n\nstatic int bpf_adj_linfo_after_remove(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t      u32 cnt)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tu32 i, l_off, l_cnt, nr_linfo;\n\tstruct bpf_line_info *linfo;\n\n\tnr_linfo = prog->aux->nr_linfo;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\tlinfo = prog->aux->linfo;\n\n\t/* find first line info to remove, count lines to be removed */\n\tfor (i = 0; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off >= off)\n\t\t\tbreak;\n\n\tl_off = i;\n\tl_cnt = 0;\n\tfor (; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off < off + cnt)\n\t\t\tl_cnt++;\n\t\telse\n\t\t\tbreak;\n\n\t/* First live insn doesn't match first live linfo, it needs to \"inherit\"\n\t * last removed linfo.  prog is already modified, so prog->len == off\n\t * means no live instructions after (tail of the program was removed).\n\t */\n\tif (prog->len != off && l_cnt &&\n\t    (i == nr_linfo || linfo[i].insn_off != off + cnt)) {\n\t\tl_cnt--;\n\t\tlinfo[--i].insn_off = off + cnt;\n\t}\n\n\t/* remove the line info which refer to the removed instructions */\n\tif (l_cnt) {\n\t\tmemmove(linfo + l_off, linfo + i,\n\t\t\tsizeof(*linfo) * (nr_linfo - i));\n\n\t\tprog->aux->nr_linfo -= l_cnt;\n\t\tnr_linfo = prog->aux->nr_linfo;\n\t}\n\n\t/* pull all linfo[i].insn_off >= off + cnt in by cnt */\n\tfor (i = l_off; i < nr_linfo; i++)\n\t\tlinfo[i].insn_off -= cnt;\n\n\t/* fix up all subprogs (incl. 'exit') which start >= off */\n\tfor (i = 0; i <= env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].linfo_idx > l_off) {\n\t\t\t/* program may have started in the removed region but\n\t\t\t * may not be fully removed\n\t\t\t */\n\t\t\tif (env->subprog_info[i].linfo_idx >= l_off + l_cnt)\n\t\t\t\tenv->subprog_info[i].linfo_idx -= l_cnt;\n\t\t\telse\n\t\t\t\tenv->subprog_info[i].linfo_idx = l_off;\n\t\t}\n\n\treturn 0;\n}\n\nstatic int verifier_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tunsigned int orig_prog_len = env->prog->len;\n\tint err;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\tbpf_prog_offload_remove_insns(env, off, cnt);\n\n\terr = bpf_remove_insns(env->prog, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = adjust_subprog_starts_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = bpf_adj_linfo_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\tmemmove(aux_data + off,\taux_data + off + cnt,\n\t\tsizeof(*aux_data) * (orig_prog_len - off - cnt));\n\n\treturn 0;\n}\n\n/* The verifier does more data flow analysis than llvm and will not\n * explore branches that are dead at run time. Malicious programs can\n * have dead code too. Therefore replace all dead at-run-time code\n * with 'ja -1'.\n *\n * Just nops are not optimal, e.g. if they would sit at the end of the\n * program and through another bug we would manage to jump there, then\n * we'd execute beyond program memory otherwise. Returning exception\n * code also wouldn't work since we can have subprogs where the dead\n * code could be located.\n */\nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn trap = BPF_JMP_IMM(BPF_JA, 0, 0, -1);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &trap, sizeof(trap));\n\t}\n}\n\nstatic bool insn_is_cond_jump(u8 code)\n{\n\tu8 op;\n\n\tif (BPF_CLASS(code) == BPF_JMP32)\n\t\treturn true;\n\n\tif (BPF_CLASS(code) != BPF_JMP)\n\t\treturn false;\n\n\top = BPF_OP(code);\n\treturn op != BPF_JA && op != BPF_EXIT && op != BPF_CALL;\n}\n\nstatic void opt_hard_wire_dead_code_branches(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (!insn_is_cond_jump(insn->code))\n\t\t\tcontinue;\n\n\t\tif (!aux_data[i + 1].seen)\n\t\t\tja.off = insn->off;\n\t\telse if (!aux_data[i + 1 + insn->off].seen)\n\t\t\tja.off = 0;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\t\tbpf_prog_offload_replace_insn(env, i, &ja);\n\n\t\tmemcpy(insn, &ja, sizeof(ja));\n\t}\n}\n\nstatic int opt_remove_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tint j;\n\n\t\tj = 0;\n\t\twhile (i + j < insn_cnt && !aux_data[i + j].seen)\n\t\t\tj++;\n\t\tif (!j)\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, j);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt = env->prog->len;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_remove_nops(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (memcmp(&insn[i], &ja, sizeof(ja)))\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt--;\n\t\ti--;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_subreg_zext_lo32_rnd_hi32(struct bpf_verifier_env *env,\n\t\t\t\t\t const union bpf_attr *attr)\n{\n\tstruct bpf_insn *patch, zext_patch[2], rnd_hi32_patch[4];\n\tstruct bpf_insn_aux_data *aux = env->insn_aux_data;\n\tint i, patch_len, delta = 0, len = env->prog->len;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_prog *new_prog;\n\tbool rnd_hi32;\n\n\trnd_hi32 = attr->prog_flags & BPF_F_TEST_RND_HI32;\n\tzext_patch[1] = BPF_ZEXT_REG(0);\n\trnd_hi32_patch[1] = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, 0);\n\trnd_hi32_patch[2] = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\trnd_hi32_patch[3] = BPF_ALU64_REG(BPF_OR, 0, BPF_REG_AX);\n\tfor (i = 0; i < len; i++) {\n\t\tint adj_idx = i + delta;\n\t\tstruct bpf_insn insn;\n\n\t\tinsn = insns[adj_idx];\n\t\tif (!aux[adj_idx].zext_dst) {\n\t\t\tu8 code, class;\n\t\t\tu32 imm_rnd;\n\n\t\t\tif (!rnd_hi32)\n\t\t\t\tcontinue;\n\n\t\t\tcode = insn.code;\n\t\t\tclass = BPF_CLASS(code);\n\t\t\tif (insn_no_def(&insn))\n\t\t\t\tcontinue;\n\n\t\t\t/* NOTE: arg \"reg\" (the fourth one) is only used for\n\t\t\t *       BPF_STX which has been ruled out in above\n\t\t\t *       check, it is safe to pass NULL here.\n\t\t\t */\n\t\t\tif (is_reg64(env, &insn, insn.dst_reg, NULL, DST_OP)) {\n\t\t\t\tif (class == BPF_LD &&\n\t\t\t\t    BPF_MODE(code) == BPF_IMM)\n\t\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t/* ctx load could be transformed into wider load. */\n\t\t\tif (class == BPF_LDX &&\n\t\t\t    aux[adj_idx].ptr_type == PTR_TO_CTX)\n\t\t\t\tcontinue;\n\n\t\t\timm_rnd = get_random_int();\n\t\t\trnd_hi32_patch[0] = insn;\n\t\t\trnd_hi32_patch[1].imm = imm_rnd;\n\t\t\trnd_hi32_patch[3].dst_reg = insn.dst_reg;\n\t\t\tpatch = rnd_hi32_patch;\n\t\t\tpatch_len = 4;\n\t\t\tgoto apply_patch_buffer;\n\t\t}\n\n\t\tif (!bpf_jit_needs_zext())\n\t\t\tcontinue;\n\n\t\tzext_patch[0] = insn;\n\t\tzext_patch[1].dst_reg = insn.dst_reg;\n\t\tzext_patch[1].src_reg = insn.dst_reg;\n\t\tpatch = zext_patch;\n\t\tpatch_len = 2;\napply_patch_buffer:\n\t\tnew_prog = bpf_patch_insn_data(env, adj_idx, patch, patch_len);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\t\tenv->prog = new_prog;\n\t\tinsns = new_prog->insnsi;\n\t\taux = env->insn_aux_data;\n\t\tdelta += patch_len - 1;\n\t}\n\n\treturn 0;\n}\n\n/* convert load instructions that access fields of a context type into a\n * sequence of instructions that access fields of the underlying structure:\n *     struct __sk_buff    -> struct sk_buff\n *     struct bpf_sock_ops -> struct sock\n */\nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tu32 target_size, size_default, off;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\n\tif (ops->gen_prologue || env->seen_direct_write) {\n\t\tif (!ops->gen_prologue) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux))\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tbpf_convert_ctx_access_t convert_ctx_access;\n\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (type == BPF_WRITE &&\n\t\t    env->insn_aux_data[i + delta].sanitize_stack_off) {\n\t\t\tstruct bpf_insn patch[] = {\n\t\t\t\t/* Sanitize suspicious stack slot with zero.\n\t\t\t\t * There are no memory dependencies for this store,\n\t\t\t\t * since it's only using frame pointer and immediate\n\t\t\t\t * constant of zero\n\t\t\t\t */\n\t\t\t\tBPF_ST_MEM(BPF_DW, BPF_REG_FP,\n\t\t\t\t\t   env->insn_aux_data[i + delta].sanitize_stack_off,\n\t\t\t\t\t   0),\n\t\t\t\t/* the original STX instruction will immediately\n\t\t\t\t * overwrite the same stack slot with appropriate value\n\t\t\t\t */\n\t\t\t\t*insn,\n\t\t\t};\n\n\t\t\tcnt = ARRAY_SIZE(patch);\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patch, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch (env->insn_aux_data[i + delta].ptr_type) {\n\t\tcase PTR_TO_CTX:\n\t\t\tif (!ops->convert_ctx_access)\n\t\t\t\tcontinue;\n\t\t\tconvert_ctx_access = ops->convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_SOCKET:\n\t\tcase PTR_TO_SOCK_COMMON:\n\t\t\tconvert_ctx_access = bpf_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_TCP_SOCK:\n\t\t\tconvert_ctx_access = bpf_tcp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_XDP_SOCK:\n\t\t\tconvert_ctx_access = bpf_xdp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_BTF_ID:\n\t\t\tif (type == BPF_READ) {\n\t\t\t\tinsn->code = BPF_LDX | BPF_PROBE_MEM |\n\t\t\t\t\tBPF_SIZE((insn)->code);\n\t\t\t\tenv->prog->aux->num_exentries++;\n\t\t\t} else if (env->prog->type != BPF_PROG_TYPE_STRUCT_OPS) {\n\t\t\t\tverbose(env, \"Writes through BTF pointers are not allowed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tcontinue;\n\t\t}\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tsize_default = bpf_ctx_off_adjust_machine(ctx_field_size);\n\t\toff = insn->off;\n\t\tif (is_narrower_load) {\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(size_default - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tu8 shift = bpf_ctx_narrow_access_offset(\n\t\t\t\toff, size, size_default) * 8;\n\t\t\tif (ctx_field_size <= 4) {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t} else {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1ULL << size * 8) - 1);\n\t\t\t}\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\nstatic int jit_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog, **func, *tmp;\n\tint i, j, subprog_start, subprog_end = 0, len, subprog;\n\tstruct bpf_insn *insn;\n\tvoid *old_bpf_func;\n\tint err, num_exentries;\n\n\tif (env->subprog_cnt <= 1)\n\t\treturn 0;\n\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\t/* Upon error here we cannot fall back to interpreter but\n\t\t * need a hard reject of the program. Thus -EFAULT is\n\t\t * propagated in any case.\n\t\t */\n\t\tsubprog = find_subprog(env, i + insn->imm + 1);\n\t\tif (subprog < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i + insn->imm + 1);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t/* temporarily remember subprog id inside insn instead of\n\t\t * aux_data, since next loop will split up all insns into funcs\n\t\t */\n\t\tinsn->off = subprog;\n\t\t/* remember original imm in case JIT fails and fallback\n\t\t * to interpreter will be needed\n\t\t */\n\t\tenv->insn_aux_data[i].call_imm = insn->imm;\n\t\t/* point imm to __bpf_call_base+1 from JITs point of view */\n\t\tinsn->imm = 1;\n\t}\n\n\terr = bpf_prog_alloc_jited_linfo(prog);\n\tif (err)\n\t\tgoto out_undo_insn;\n\n\terr = -ENOMEM;\n\tfunc = kcalloc(env->subprog_cnt, sizeof(prog), GFP_KERNEL);\n\tif (!func)\n\t\tgoto out_undo_insn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tsubprog_start = subprog_end;\n\t\tsubprog_end = env->subprog_info[i + 1].start;\n\n\t\tlen = subprog_end - subprog_start;\n\t\t/* BPF_PROG_RUN doesn't call subprogs directly,\n\t\t * hence main prog stats include the runtime of subprogs.\n\t\t * subprogs don't have IDs and not reachable via prog_get_next_id\n\t\t * func[i]->aux->stats will never be accessed and stays NULL\n\t\t */\n\t\tfunc[i] = bpf_prog_alloc_no_stats(bpf_prog_size(len), GFP_USER);\n\t\tif (!func[i])\n\t\t\tgoto out_free;\n\t\tmemcpy(func[i]->insnsi, &prog->insnsi[subprog_start],\n\t\t       len * sizeof(struct bpf_insn));\n\t\tfunc[i]->type = prog->type;\n\t\tfunc[i]->len = len;\n\t\tif (bpf_prog_calc_tag(func[i]))\n\t\t\tgoto out_free;\n\t\tfunc[i]->is_func = 1;\n\t\tfunc[i]->aux->func_idx = i;\n\t\t/* the btf and func_info will be freed only at prog->aux */\n\t\tfunc[i]->aux->btf = prog->aux->btf;\n\t\tfunc[i]->aux->func_info = prog->aux->func_info;\n\n\t\t/* Use bpf_prog_F_tag to indicate functions in stack traces.\n\t\t * Long term would need debug info to populate names\n\t\t */\n\t\tfunc[i]->aux->name[0] = 'F';\n\t\tfunc[i]->aux->stack_depth = env->subprog_info[i].stack_depth;\n\t\tfunc[i]->jit_requested = 1;\n\t\tfunc[i]->aux->linfo = prog->aux->linfo;\n\t\tfunc[i]->aux->nr_linfo = prog->aux->nr_linfo;\n\t\tfunc[i]->aux->jited_linfo = prog->aux->jited_linfo;\n\t\tfunc[i]->aux->linfo_idx = env->subprog_info[i].linfo_idx;\n\t\tnum_exentries = 0;\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t\t    BPF_MODE(insn->code) == BPF_PROBE_MEM)\n\t\t\t\tnum_exentries++;\n\t\t}\n\t\tfunc[i]->aux->num_exentries = num_exentries;\n\t\tfunc[i] = bpf_int_jit_compile(func[i]);\n\t\tif (!func[i]->jited) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\t/* at this point all bpf functions were successfully JITed\n\t * now populate all bpf_calls with correct addresses and\n\t * run last pass of JIT\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tsubprog = insn->off;\n\t\t\tinsn->imm = BPF_CAST_CALL(func[subprog]->bpf_func) -\n\t\t\t\t    __bpf_call_base;\n\t\t}\n\n\t\t/* we use the aux data to keep a list of the start addresses\n\t\t * of the JITed images for each function in the program\n\t\t *\n\t\t * for some architectures, such as powerpc64, the imm field\n\t\t * might not be large enough to hold the offset of the start\n\t\t * address of the callee's JITed image from __bpf_call_base\n\t\t *\n\t\t * in such cases, we can lookup the start address of a callee\n\t\t * by using its subprog id, available from the off field of\n\t\t * the call instruction, as an index for this list\n\t\t */\n\t\tfunc[i]->aux->func = func;\n\t\tfunc[i]->aux->func_cnt = env->subprog_cnt;\n\t}\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\told_bpf_func = func[i]->bpf_func;\n\t\ttmp = bpf_int_jit_compile(func[i]);\n\t\tif (tmp != func[i] || func[i]->bpf_func != old_bpf_func) {\n\t\t\tverbose(env, \"JIT doesn't support bpf-to-bpf calls\\n\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t/* finally lock prog and jit images for all functions and\n\t * populate kallsysm\n\t */\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tbpf_prog_lock_ro(func[i]);\n\t\tbpf_prog_kallsyms_add(func[i]);\n\t}\n\n\t/* Last step: make now unused interpreter insns from main\n\t * prog consistent for later dump requests, so they can\n\t * later look the same as if they were interpreted only.\n\t */\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = env->insn_aux_data[i].call_imm;\n\t\tsubprog = find_subprog(env, i + insn->off + 1);\n\t\tinsn->imm = subprog;\n\t}\n\n\tprog->jited = 1;\n\tprog->bpf_func = func[0]->bpf_func;\n\tprog->aux->func = func;\n\tprog->aux->func_cnt = env->subprog_cnt;\n\tbpf_prog_free_unused_jited_linfo(prog);\n\treturn 0;\nout_free:\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (func[i])\n\t\t\tbpf_jit_free(func[i]);\n\tkfree(func);\nout_undo_insn:\n\t/* cleanup main prog to be interpreted */\n\tprog->jit_requested = 0;\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tinsn->off = 0;\n\t\tinsn->imm = env->insn_aux_data[i].call_imm;\n\t}\n\tbpf_prog_free_jited_linfo(prog);\n\treturn err;\n}\n\nstatic int fixup_call_args(struct bpf_verifier_env *env)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint i, depth;\n#endif\n\tint err = 0;\n\n\tif (env->prog->jit_requested &&\n\t    !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\terr = jit_subprogs(env);\n\t\tif (err == 0)\n\t\t\treturn 0;\n\t\tif (err == -EFAULT)\n\t\t\treturn err;\n\t}\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tfor (i = 0; i < prog->len; i++, insn++) {\n\t\tif (insn->code != (BPF_JMP | BPF_CALL) ||\n\t\t    insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tdepth = get_callee_stack_depth(env, insn, i);\n\t\tif (depth < 0)\n\t\t\treturn depth;\n\t\tbpf_patch_call_args(insn, depth);\n\t}\n\terr = 0;\n#endif\n\treturn err;\n}\n\n/* fixup insn->imm field of bpf_call instructions\n * and inline eligible helpers as explicit sequence of BPF instructions\n *\n * this function is called after eBPF program passed verification\n */\nstatic int fixup_bpf_calls(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool expect_blinding = bpf_jit_blinding_enabled(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tstruct bpf_insn mask_and_div[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx div 0 -> 0 */\n\t\t\t\tBPF_JMP_IMM(BPF_JNE, insn->src_reg, 0, 2),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn mask_and_mod[] = {\n\t\t\t\tBPF_MOV32_REG(insn->src_reg, insn->src_reg),\n\t\t\t\t/* Rx mod 0 -> Rx */\n\t\t\t\tBPF_JMP_IMM(BPF_JEQ, insn->src_reg, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn *patchlet;\n\n\t\t\tif (insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\t\tpatchlet = mask_and_div + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_div) - (is64 ? 1 : 0);\n\t\t\t} else {\n\t\t\t\tpatchlet = mask_and_mod + (is64 ? 1 : 0);\n\t\t\t\tcnt = ARRAY_SIZE(mask_and_mod) - (is64 ? 1 : 0);\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn insn_buf[16];\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\tif (issrc) {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,\n\t\t\t\t\t\t\t off_reg);\n\t\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\t} else {\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,\n\t\t\t\t\t\t\t BPF_REG_AX);\n\t\t\t}\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t/* If we tail call into other programs, we\n\t\t\t * cannot make any assumptions since they can\n\t\t\t * be replaced dynamically during runtime in\n\t\t\t * the program array.\n\t\t\t */\n\t\t\tprog->cb_access = 1;\n\t\t\tenv->prog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tenv->prog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t/* mark bpf_tail_call as different opcode to avoid\n\t\t\t * conditional branch in the interpeter for every normal\n\t\t\t * call and to prevent accidental JITing by JIT compiler\n\t\t\t * that doesn't support bpf_tail_call yet\n\t\t\t */\n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !expect_blinding &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t/* instead of changing every JIT dealing with tail_call\n\t\t\t * emit two extra insns:\n\t\t\t * if (index >= max_entries) goto out;\n\t\t\t * index &= array->index_mask;\n\t\t\t * to avoid out-of-bounds cpu speculation\n\t\t\t */\n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* BPF_EMIT_CALL() assumptions in some of the map_gen_lookup\n\t\t * and other inlining handlers are currently limited to 64 bit\n\t\t * only.\n\t\t */\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (int (*)(struct bpf_map *map, void *value))NULL));\n\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_lookup_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_update_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_delete_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_push_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_pop_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CAST_CALL(ops->map_peek_elem) -\n\t\t\t\t\t    __bpf_call_base;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t/* all functions that have prototype and verifier allowed\n\t\t * programs to call them, must be real in-kernel functions\n\t\t */\n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t/* Since poke tab is now finalized, publish aux to tracker. */\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tsl = env->free_list;\n\twhile (sl) {\n\t\tsln = sl->next;\n\t\tfree_verifier_state(&sl->state, false);\n\t\tkfree(sl);\n\t\tsl = sln;\n\t}\n\tenv->free_list = NULL;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < state_htab_size(env); i++) {\n\t\tsl = env->explored_states[i];\n\n\t\twhile (sl) {\n\t\t\tsln = sl->next;\n\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\tkfree(sl);\n\t\t\tsl = sln;\n\t\t}\n\t\tenv->explored_states[i] = NULL;\n\t}\n}\n\n/* The verifier is using insn_aux_data[] to store temporary data during\n * verification and to store information for passes that run after the\n * verification like dead code sanitization. do_check_common() for subprogram N\n * may analyze many other subprograms. sanitize_insn_aux_data() clears all\n * temporary data after do_check_common() finds that subprogram N cannot be\n * verified independently. pass_cnt counts the number of times\n * do_check_common() was run and insn->aux->seen tells the pass number\n * insn_aux_data was touched. These variables are compared to clear temporary\n * data from failed pass. For testing and experiments do_check_common() can be\n * run multiple times even when prior attempt to verify is unsuccessful.\n */\nstatic void sanitize_insn_aux_data(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tstruct bpf_insn_aux_data *aux;\n\tint i, class;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tclass = BPF_CLASS(insn[i].code);\n\t\tif (class != BPF_LDX && class != BPF_STX)\n\t\t\tcontinue;\n\t\taux = &env->insn_aux_data[i];\n\t\tif (aux->seen != env->pass_cnt)\n\t\t\tcontinue;\n\t\tmemset(aux, 0, offsetof(typeof(*aux), orig_idx));\n\t}\n}\n\nstatic int do_check_common(struct bpf_verifier_env *env, int subprog)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_reg_state *regs;\n\tint ret, i;\n\n\tenv->prev_linfo = NULL;\n\tenv->pass_cnt++;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->branches = 1;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC /* callsite */,\n\t\t\t0 /* frameno */,\n\t\t\tsubprog);\n\n\tregs = state->frame[state->curframe]->regs;\n\tif (subprog || env->prog->type == BPF_PROG_TYPE_EXT) {\n\t\tret = btf_prepare_func_args(env, subprog, regs);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++) {\n\t\t\tif (regs[i].type == PTR_TO_CTX)\n\t\t\t\tmark_reg_known_zero(env, regs, i);\n\t\t\telse if (regs[i].type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, i);\n\t\t}\n\t} else {\n\t\t/* 1st arg to a function */\n\t\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\t\tmark_reg_known_zero(env, regs, BPF_REG_1);\n\t\tret = btf_check_func_arg_match(env, subprog, regs);\n\t\tif (ret == -EFAULT)\n\t\t\t/* unlikely verifier bug. abort.\n\t\t\t * ret == 0 and ret < 0 are sadly acceptable for\n\t\t\t * main() function due to backward compatibility.\n\t\t\t * Like socket filter program may be written as:\n\t\t\t * int bpf_prog(struct pt_regs *ctx)\n\t\t\t * and never dereference that ctx in the program.\n\t\t\t * 'struct pt_regs' is a type mismatch for socket\n\t\t\t * filter that should be using 'struct __sk_buff'.\n\t\t\t */\n\t\t\tgoto out;\n\t}\n\n\tret = do_check(env);\nout:\n\t/* check for NULL is necessary, since cur_state can be freed inside\n\t * do_check() under memory pressure.\n\t */\n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\twhile (!pop_stack(env, NULL, NULL, false));\n\tif (!ret && pop_log)\n\t\tbpf_vlog_reset(&env->log, 0);\n\tfree_states(env);\n\tif (ret)\n\t\t/* clean aux data in case subprog was rejected */\n\t\tsanitize_insn_aux_data(env);\n\treturn ret;\n}\n\n/* Verify all global functions in a BPF program one by one based on their BTF.\n * All global functions must pass verification. Otherwise the whole program is rejected.\n * Consider:\n * int bar(int);\n * int foo(int f)\n * {\n *    return bar(f);\n * }\n * int bar(int b)\n * {\n *    ...\n * }\n * foo() will be verified first for R1=any_scalar_value. During verification it\n * will be assumed that bar() already verified successfully and call to bar()\n * from foo() will be checked for type match only. Later bar() will be verified\n * independently to check that it's safe for R1=any_scalar_value.\n */\nstatic int do_check_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i, ret;\n\n\tif (!aux->func_info)\n\t\treturn 0;\n\n\tfor (i = 1; i < env->subprog_cnt; i++) {\n\t\tif (aux->func_info_aux[i].linkage != BTF_FUNC_GLOBAL)\n\t\t\tcontinue;\n\t\tenv->insn_idx = env->subprog_info[i].start;\n\t\tWARN_ON_ONCE(env->insn_idx == 0);\n\t\tret = do_check_common(env, i);\n\t\tif (ret) {\n\t\t\treturn ret;\n\t\t} else if (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tverbose(env,\n\t\t\t\t\"Func#%d is safe for any args that match its prototype\\n\",\n\t\t\t\ti);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int do_check_main(struct bpf_verifier_env *env)\n{\n\tint ret;\n\n\tenv->insn_idx = 0;\n\tret = do_check_common(env, 0);\n\tif (!ret)\n\t\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\treturn ret;\n}\n\n\nstatic void print_verification_stats(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (env->log.level & BPF_LOG_STATS) {\n\t\tverbose(env, \"verification time %lld usec\\n\",\n\t\t\tdiv_u64(env->verification_time, 1000));\n\t\tverbose(env, \"stack depth \");\n\t\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\t\tu32 depth = env->subprog_info[i].stack_depth;\n\n\t\t\tverbose(env, \"%d\", depth);\n\t\t\tif (i + 1 < env->subprog_cnt)\n\t\t\t\tverbose(env, \"+\");\n\t\t}\n\t\tverbose(env, \"\\n\");\n\t}\n\tverbose(env, \"processed %d insns (limit %d) max_states_per_insn %d \"\n\t\t\"total_states %d peak_states %d mark_read %d\\n\",\n\t\tenv->insn_processed, BPF_COMPLEXITY_LIMIT_INSNS,\n\t\tenv->max_states_per_insn, env->total_states,\n\t\tenv->peak_states, env->longest_mark_read_walk);\n}\n\nstatic int check_struct_ops_btf_id(struct bpf_verifier_env *env)\n{\n\tconst struct btf_type *t, *func_proto;\n\tconst struct bpf_struct_ops *st_ops;\n\tconst struct btf_member *member;\n\tstruct bpf_prog *prog = env->prog;\n\tu32 btf_id, member_idx;\n\tconst char *mname;\n\n\tbtf_id = prog->aux->attach_btf_id;\n\tst_ops = bpf_struct_ops_find(btf_id);\n\tif (!st_ops) {\n\t\tverbose(env, \"attach_btf_id %u is not a supported struct\\n\",\n\t\t\tbtf_id);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = st_ops->type;\n\tmember_idx = prog->expected_attach_type;\n\tif (member_idx >= btf_type_vlen(t)) {\n\t\tverbose(env, \"attach to invalid member idx %u of struct %s\\n\",\n\t\t\tmember_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tmember = &btf_type_member(t)[member_idx];\n\tmname = btf_name_by_offset(btf_vmlinux, member->name_off);\n\tfunc_proto = btf_type_resolve_func_ptr(btf_vmlinux, member->type,\n\t\t\t\t\t       NULL);\n\tif (!func_proto) {\n\t\tverbose(env, \"attach to invalid member %s(@idx %u) of struct %s\\n\",\n\t\t\tmname, member_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (st_ops->check_member) {\n\t\tint err = st_ops->check_member(t, member);\n\n\t\tif (err) {\n\t\t\tverbose(env, \"attach to unsupported member %s of struct %s\\n\",\n\t\t\t\tmname, st_ops->name);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tprog->aux->attach_func_proto = func_proto;\n\tprog->aux->attach_func_name = mname;\n\tenv->ops = st_ops->verifier_ops;\n\n\treturn 0;\n}\n#define SECURITY_PREFIX \"security_\"\n\nstatic int check_attach_modify_return(struct bpf_prog *prog, unsigned long addr)\n{\n\tif (within_error_injection_list(addr) ||\n\t    !strncmp(SECURITY_PREFIX, prog->aux->attach_func_name,\n\t\t     sizeof(SECURITY_PREFIX) - 1))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic int check_attach_btf_id(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool prog_extension = prog->type == BPF_PROG_TYPE_EXT;\n\tstruct bpf_prog *tgt_prog = prog->aux->linked_prog;\n\tu32 btf_id = prog->aux->attach_btf_id;\n\tconst char prefix[] = \"btf_trace_\";\n\tstruct btf_func_model fmodel;\n\tint ret = 0, subprog = -1, i;\n\tstruct bpf_trampoline *tr;\n\tconst struct btf_type *t;\n\tbool conservative = true;\n\tconst char *tname;\n\tstruct btf *btf;\n\tlong addr;\n\tu64 key;\n\n\tif (prog->type == BPF_PROG_TYPE_STRUCT_OPS)\n\t\treturn check_struct_ops_btf_id(env);\n\n\tif (prog->type != BPF_PROG_TYPE_TRACING &&\n\t    prog->type != BPF_PROG_TYPE_LSM &&\n\t    !prog_extension)\n\t\treturn 0;\n\n\tif (!btf_id) {\n\t\tverbose(env, \"Tracing programs must provide btf_id\\n\");\n\t\treturn -EINVAL;\n\t}\n\tbtf = bpf_prog_get_target_btf(prog);\n\tif (!btf) {\n\t\tverbose(env,\n\t\t\t\"FENTRY/FEXIT program can only be attached to another program annotated with BTF\\n\");\n\t\treturn -EINVAL;\n\t}\n\tt = btf_type_by_id(btf, btf_id);\n\tif (!t) {\n\t\tverbose(env, \"attach_btf_id %u is invalid\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\ttname = btf_name_by_offset(btf, t->name_off);\n\tif (!tname) {\n\t\tverbose(env, \"attach_btf_id %u doesn't have a name\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\tif (tgt_prog) {\n\t\tstruct bpf_prog_aux *aux = tgt_prog->aux;\n\n\t\tfor (i = 0; i < aux->func_info_cnt; i++)\n\t\t\tif (aux->func_info[i].type_id == btf_id) {\n\t\t\t\tsubprog = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (subprog == -1) {\n\t\t\tverbose(env, \"Subprog %s doesn't exist\\n\", tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tconservative = aux->func_info_aux[subprog].unreliable;\n\t\tif (prog_extension) {\n\t\t\tif (conservative) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Cannot replace static functions\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!prog->jit_requested) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Extension programs should be JITed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tenv->ops = bpf_verifier_ops[tgt_prog->type];\n\t\t\tprog->expected_attach_type = tgt_prog->expected_attach_type;\n\t\t}\n\t\tif (!tgt_prog->jited) {\n\t\t\tverbose(env, \"Can attach to only JITed progs\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == prog->type) {\n\t\t\t/* Cannot fentry/fexit another fentry/fexit program.\n\t\t\t * Cannot attach program extension to another extension.\n\t\t\t * It's ok to attach fentry/fexit to extension program.\n\t\t\t */\n\t\t\tverbose(env, \"Cannot recursively attach\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_TRACING &&\n\t\t    prog_extension &&\n\t\t    (tgt_prog->expected_attach_type == BPF_TRACE_FENTRY ||\n\t\t     tgt_prog->expected_attach_type == BPF_TRACE_FEXIT)) {\n\t\t\t/* Program extensions can extend all program types\n\t\t\t * except fentry/fexit. The reason is the following.\n\t\t\t * The fentry/fexit programs are used for performance\n\t\t\t * analysis, stats and can be attached to any program\n\t\t\t * type except themselves. When extension program is\n\t\t\t * replacing XDP function it is necessary to allow\n\t\t\t * performance analysis of all functions. Both original\n\t\t\t * XDP program and its program extension. Hence\n\t\t\t * attaching fentry/fexit to BPF_PROG_TYPE_EXT is\n\t\t\t * allowed. If extending of fentry/fexit was allowed it\n\t\t\t * would be possible to create long call chain\n\t\t\t * fentry->extension->fentry->extension beyond\n\t\t\t * reasonable stack size. Hence extending fentry is not\n\t\t\t * allowed.\n\t\t\t */\n\t\t\tverbose(env, \"Cannot extend fentry/fexit\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tkey = ((u64)aux->id) << 32 | btf_id;\n\t} else {\n\t\tif (prog_extension) {\n\t\t\tverbose(env, \"Cannot replace kernel functions\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tkey = btf_id;\n\t}\n\n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_RAW_TP:\n\t\tif (tgt_prog) {\n\t\t\tverbose(env,\n\t\t\t\t\"Only FENTRY/FEXIT progs are attachable to another BPF prog\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!btf_type_is_typedef(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a typedef\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (strncmp(prefix, tname, sizeof(prefix) - 1)) {\n\t\t\tverbose(env, \"attach_btf_id %u points to wrong type name %s\\n\",\n\t\t\t\tbtf_id, tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttname += sizeof(prefix) - 1;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_ptr(t))\n\t\t\t/* should never happen in valid vmlinux build */\n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\t/* should never happen in valid vmlinux build */\n\t\t\treturn -EINVAL;\n\n\t\t/* remember two read only pointers that are valid for\n\t\t * the life time of the kernel\n\t\t */\n\t\tprog->aux->attach_func_name = tname;\n\t\tprog->aux->attach_func_proto = t;\n\t\tprog->aux->attach_btf_trace = true;\n\t\treturn 0;\n\tcase BPF_TRACE_ITER:\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\t\tprog->aux->attach_func_name = tname;\n\t\tprog->aux->attach_func_proto = t;\n\t\tif (!bpf_iter_prog_supported(prog))\n\t\t\treturn -EINVAL;\n\t\tret = btf_distill_func_proto(&env->log, btf, t,\n\t\t\t\t\t     tname, &fmodel);\n\t\treturn ret;\n\tdefault:\n\t\tif (!prog_extension)\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\tcase BPF_MODIFY_RETURN:\n\tcase BPF_LSM_MAC:\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\t\tprog->aux->attach_func_name = tname;\n\t\tif (prog->type == BPF_PROG_TYPE_LSM) {\n\t\t\tret = bpf_lsm_verify_prog(&env->log, prog);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tverbose(env, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (prog_extension &&\n\t\t    btf_check_type_match(env, prog, btf, t))\n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\t\ttr = bpf_trampoline_lookup(key);\n\t\tif (!tr)\n\t\t\treturn -ENOMEM;\n\t\t/* t is either vmlinux type or another program's type */\n\t\tprog->aux->attach_func_proto = t;\n\t\tmutex_lock(&tr->mutex);\n\t\tif (tr->func.addr) {\n\t\t\tprog->aux->trampoline = tr;\n\t\t\tgoto out;\n\t\t}\n\t\tif (tgt_prog && conservative) {\n\t\t\tprog->aux->attach_func_proto = NULL;\n\t\t\tt = NULL;\n\t\t}\n\t\tret = btf_distill_func_proto(&env->log, btf, t,\n\t\t\t\t\t     tname, &tr->func.model);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tif (tgt_prog) {\n\t\t\tif (subprog == 0)\n\t\t\t\taddr = (long) tgt_prog->bpf_func;\n\t\t\telse\n\t\t\t\taddr = (long) tgt_prog->aux->func[subprog]->bpf_func;\n\t\t} else {\n\t\t\taddr = kallsyms_lookup_name(tname);\n\t\t\tif (!addr) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"The address of function %s cannot be found\\n\",\n\t\t\t\t\ttname);\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (prog->expected_attach_type == BPF_MODIFY_RETURN) {\n\t\t\tret = check_attach_modify_return(prog, addr);\n\t\t\tif (ret)\n\t\t\t\tverbose(env, \"%s() is not modifiable\\n\",\n\t\t\t\t\tprog->aux->attach_func_name);\n\t\t}\n\n\t\tif (ret)\n\t\t\tgoto out;\n\t\ttr->func.addr = (void *)addr;\n\t\tprog->aux->trampoline = tr;\nout:\n\t\tmutex_unlock(&tr->mutex);\n\t\tif (ret)\n\t\t\tbpf_trampoline_put(tr);\n\t\treturn ret;\n\t}\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr,\n\t      union bpf_attr __user *uattr)\n{\n\tu64 start_time = ktime_get_ns();\n\tstruct bpf_verifier_env *env;\n\tstruct bpf_verifier_log *log;\n\tint i, len, ret = -EINVAL;\n\tbool is_priv;\n\n\t/* no program is valid */\n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t/* 'struct bpf_verifier_env' can be global, but since it's not small,\n\t * allocate/free it every time bpf_check() is called\n\t */\n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\tlog = &env->log;\n\n\tlen = (*prog)->len;\n\tenv->insn_aux_data =\n\t\tvzalloc(array_size(sizeof(struct bpf_insn_aux_data), len));\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tfor (i = 0; i < len; i++)\n\t\tenv->insn_aux_data[i].orig_idx = i;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\tis_priv = bpf_capable();\n\n\tif (!btf_vmlinux && IS_ENABLED(CONFIG_DEBUG_INFO_BTF)) {\n\t\tmutex_lock(&bpf_verifier_lock);\n\t\tif (!btf_vmlinux)\n\t\t\tbtf_vmlinux = btf_parse_vmlinux();\n\t\tmutex_unlock(&bpf_verifier_lock);\n\t}\n\n\t/* grab the mutex to protect few globals used by verifier */\n\tif (!is_priv)\n\t\tmutex_lock(&bpf_verifier_lock);\n\n\tif (attr->log_level || attr->log_buf || attr->log_size) {\n\t\t/* user requested verbose verifier output\n\t\t * and supplied buffer to store the verification trace\n\t\t */\n\t\tlog->level = attr->log_level;\n\t\tlog->ubuf = (char __user *) (unsigned long) attr->log_buf;\n\t\tlog->len_total = attr->log_size;\n\n\t\tret = -EINVAL;\n\t\t/* log attributes have to be sane */\n\t\tif (log->len_total < 128 || log->len_total > UINT_MAX >> 2 ||\n\t\t    !log->level || !log->ubuf || log->level & ~BPF_LOG_MASK)\n\t\t\tgoto err_unlock;\n\t}\n\n\tif (IS_ERR(btf_vmlinux)) {\n\t\t/* Either gcc or pahole or kernel are broken. */\n\t\tverbose(env, \"in-kernel BTF is malformed\\n\");\n\t\tret = PTR_ERR(btf_vmlinux);\n\t\tgoto skip_full_check;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\tif (attr->prog_flags & BPF_F_ANY_ALIGNMENT)\n\t\tenv->strict_alignment = false;\n\n\tenv->allow_ptr_leaks = bpf_allow_ptr_leaks();\n\tenv->allow_ptr_to_map_access = bpf_allow_ptr_to_map_access();\n\tenv->bypass_spec_v1 = bpf_bypass_spec_v1();\n\tenv->bypass_spec_v4 = bpf_bypass_spec_v4();\n\tenv->bpf_capable = bpf_capable();\n\n\tif (is_priv)\n\t\tenv->test_state_freq = attr->prog_flags & BPF_F_TEST_STATE_FREQ;\n\n\tret = replace_map_fd_with_map_ptr(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tif (bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = bpf_prog_offload_verifier_prep(env->prog);\n\t\tif (ret)\n\t\t\tgoto skip_full_check;\n\t}\n\n\tenv->explored_states = kvcalloc(state_htab_size(env),\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = check_subprogs(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_btf_info(env, attr, uattr);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_attach_btf_id(env);\n\tif (ret)\n\t\tgoto skip_full_check;\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = do_check_subprogs(env);\n\tret = ret ?: do_check_main(env);\n\n\tif (ret == 0 && bpf_prog_is_dev_bound(env->prog->aux))\n\t\tret = bpf_prog_offload_finalize(env);\n\nskip_full_check:\n\tkvfree(env->explored_states);\n\n\tif (ret == 0)\n\t\tret = check_max_stack_depth(env);\n\n\t/* instruction rewrites happen after this point */\n\tif (is_priv) {\n\t\tif (ret == 0)\n\t\t\topt_hard_wire_dead_code_branches(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_dead_code(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_nops(env);\n\t} else {\n\t\tif (ret == 0)\n\t\t\tsanitize_dead_code(env);\n\t}\n\n\tif (ret == 0)\n\t\t/* program is valid, convert *(u32*)(ctx + off) accesses */\n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = fixup_bpf_calls(env);\n\n\t/* do 32-bit optimization after insn patching has done so those patched\n\t * insns could be handled correctly.\n\t */\n\tif (ret == 0 && !bpf_prog_is_dev_bound(env->prog->aux)) {\n\t\tret = opt_subreg_zext_lo32_rnd_hi32(env, attr);\n\t\tenv->prog->aux->verifier_zext = bpf_jit_needs_zext() ? !ret\n\t\t\t\t\t\t\t\t     : false;\n\t}\n\n\tif (ret == 0)\n\t\tret = fixup_call_args(env);\n\n\tenv->verification_time = ktime_get_ns() - start_time;\n\tprint_verification_stats(env);\n\n\tif (log->level && bpf_verifier_log_full(log))\n\t\tret = -ENOSPC;\n\tif (log->level && !log->ubuf) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret == 0 && env->used_map_cnt) {\n\t\t/* if program passed verifier, update used_maps in bpf_prog_info */\n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\n\t\t/* program is valid. Convert pseudo bpf_ld_imm64 into generic\n\t\t * bpf_ld_imm64 instructions\n\t\t */\n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\n\tif (ret == 0)\n\t\tadjust_btf_func(env);\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t/* if we didn't copy map pointers into bpf_prog_info, release\n\t\t * them now. Otherwise free_used_maps() will release them.\n\t\t */\n\t\trelease_maps(env);\n\n\t/* extension progs temporarily inherit the attach_type of their targets\n\t   for verification purposes, so set it back to zero before returning\n\t */\n\tif (env->prog->type == BPF_PROG_TYPE_EXT)\n\t\tenv->prog->expected_attach_type = 0;\n\n\t*prog = env->prog;\nerr_unlock:\n\tif (!is_priv)\n\t\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n"], "filenames": ["kernel/bpf/verifier.c"], "buggy_code_start_loc": [5670], "buggy_code_end_loc": [5696], "fixing_code_start_loc": [5670], "fixing_code_end_loc": [5696], "type": "CWE-681", "message": "An issue was discovered in the Linux kernel before 5.8.15. scalar32_min_max_or in kernel/bpf/verifier.c mishandles bounds tracking during use of 64-bit values, aka CID-5b9fbeb75b6a.", "other": {"cve": {"id": "CVE-2020-27194", "sourceIdentifier": "cve@mitre.org", "published": "2020-10-16T21:15:14.567", "lastModified": "2022-06-28T14:11:45.273", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in the Linux kernel before 5.8.15. scalar32_min_max_or in kernel/bpf/verifier.c mishandles bounds tracking during use of 64-bit values, aka CID-5b9fbeb75b6a."}, {"lang": "es", "value": "Se detect\u00f3 un problema en el kernel de Linux versiones anteriores a 5.8.15.&#xa0;La funci\u00f3n scalar32_min_max_or en el archivo kernel/bpf/verifier.c, maneja inapropiadamente el seguimiento de l\u00edmites durante el uso de valores de 64 bits, tambi\u00e9n se conoce como CID-5b9fbeb75b6a"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-681"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.8.15", "matchCriteriaId": "9BF3F4F2-5281-41EC-9471-4C35FCDDC7AA"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.8.15", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/5b9fbeb75b6a98955f628e205ac26689bcb1383e", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/5b9fbeb75b6a98955f628e205ac26689bcb1383e"}}