{"buggy_code": ["Changes\n=======\n\ndev (master)\n------------\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* ... [Short description of non-trivial change.] (Issue #)\n\n\n1.24.1 (2018-11-02)\n-------------------\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of `ciphers` parameter for `create_urllib3_context()`. (Issue #1462)\n\n\n1.24 (2018-10-16)\n-----------------\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n-----------------\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n-----------------\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n-------------------\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n-----------------\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n-----------------\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n-------------------\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n-----------------\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n-------------------\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in PR #1010.\n\n\n1.18 (2016-09-26)\n-----------------\n\n* Fixed incorrect message for IncompleteRead exception. (PR #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n-----------------\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (PR #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (PR #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (PR #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (PR #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (PR #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)\n\n\n1.16 (2016-06-11)\n-----------------\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n-------------------\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n-----------------\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n-----------------\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n-------------------\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n-----------------\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n-----------------\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n-----------------\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n-------------------\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n-------------------\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n-------------------\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n-------------------\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n-----------------\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n------------------\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n----------------\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n------------------\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n------------------\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n------------------\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n----------------\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n------------------\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n----------------\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n----------------\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n----------------\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n----------------\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n----------------\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n------------------\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n------------------\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n----------------\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n----------------\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n------------------\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n------------------\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n----------------\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n------------------\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n----------------\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n------------------\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n----------------\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n----------------\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n----------------\n\n* First release.\n", "# Contributions to the urllib3 project\n\n## Creator & Maintainer\n\n* Andrey Petrov <andrey.petrov@shazow.net>\n\n\n## Contributors\n\nIn chronological order:\n\n* victor.vde <http://code.google.com/u/victor.vde/>\n  * HTTPS patch (which inspired HTTPSConnectionPool)\n\n* erikcederstrand <http://code.google.com/u/erikcederstrand/>\n  * NTLM-authenticated HTTPSConnectionPool\n  * Basic-authenticated HTTPSConnectionPool (merged into make_headers)\n\n* niphlod <niphlod@gmail.com>\n  * Client-verified SSL certificates for HTTPSConnectionPool\n  * Response gzip and deflate encoding support\n  * Better unicode support for filepost using StringIO buffers\n\n* btoconnor <brian@btoconnor.net>\n  * Non-multipart encoding for POST requests\n\n* p.dobrogost <http://code.google.com/u/@WBRSRlBZDhBFXQB6/>\n  * Code review, PEP8 compliance, benchmark fix\n\n* kennethreitz <me@kennethreitz.com>\n  * Bugfixes, suggestions, Requests integration\n\n* georgemarshall <https://github.com/georgemarshall>\n  * Bugfixes, Improvements and Test coverage\n\n* Thomas Kluyver <thomas@kluyver.me.uk>\n  * Python 3 support\n\n* brandon-rhodes <http://rhodesmill.org/brandon>\n  * Design review, bugfixes, test coverage.\n\n* studer <theo.studer@gmail.com>\n  * IPv6 url support and test coverage\n\n* Shivaram Lingamneni <slingamn@cs.stanford.edu>\n  * Support for explicitly closing pooled connections\n\n* hartator <hartator@gmail.com>\n  * Corrected multipart behavior for params\n\n* Thomas Wei\u00dfschuh <thomas@t-8ch.de>\n  * Support for TLS SNI\n  * API unification of ssl_version/cert_reqs\n  * SSL fingerprint and alternative hostname verification\n  * Bugfixes in testsuite\n\n* Sune Kirkeby <mig@ibofobi.dk>\n  * Optional SNI-support for Python 2 via PyOpenSSL.\n\n* Marc Schlaich <marc.schlaich@gmail.com>\n  * Various bugfixes and test improvements.\n\n* Bryce Boe <bbzbryce@gmail.com>\n  * Correct six.moves conflict\n  * Fixed pickle support of some exceptions\n\n* Boris Figovsky <boris.figovsky@ravellosystems.com>\n  * Allowed to skip SSL hostname verification\n\n* Cory Benfield <https://lukasa.co.uk/about/>\n  * Stream method for Response objects.\n  * Return native strings in header values.\n  * Generate 'Host' header when using proxies.\n\n* Jason Robinson <jaywink@basshero.org>\n  * Add missing WrappedSocket.fileno method in PyOpenSSL\n\n* Audrius Butkevicius <audrius.butkevicius@elastichosts.com>\n  * Fixed a race condition\n\n* Stanislav Vitkovskiy <stas.vitkovsky@gmail.com>\n  * Added HTTPS (CONNECT) proxy support\n\n* Stephen Holsapple <sholsapp@gmail.com>\n  * Added abstraction for granular control of request fields\n\n* Martin von Gagern <Martin.vGagern@gmx.net>\n  * Support for non-ASCII header parameters\n\n* Kevin Burke <kev@inburke.com> and Pavel Kirichenko <juanych@yandex-team.ru>\n  * Support for separate connect and request timeouts\n\n* Peter Waller <p@pwaller.net>\n  * HTTPResponse.tell() for determining amount received over the wire\n\n* Nipunn Koorapati <nipunn1313@gmail.com>\n  * Ignore default ports when comparing hosts for equality\n\n* Danilo @dbrgn <https://dbrgn.ch/>\n  * Disabled TLS compression by default on Python 3.2+\n  * Disabled TLS compression in pyopenssl contrib module\n  * Configurable cipher suites in pyopenssl contrib module\n\n* Roman Bogorodskiy <roman.bogorodskiy@ericsson.com>\n  * Account retries on proxy errors\n\n* Nicolas Delaby <nicolas.delaby@ezeep.com>\n  * Use the platform-specific CA certificate locations\n\n* Josh Schneier <https://github.com/jschneier>\n  * HTTPHeaderDict and associated tests and docs\n  * Bugfixes, docs, test coverage\n\n* Tahia Khan <http://tahia.tk/>\n  * Added Timeout examples in docs\n\n* Arthur Grunseid <https://grunseid.com>\n  * source_address support and tests (with https://github.com/bui)\n\n* Ian Cordasco <graffatcolmingov@gmail.com>\n  * PEP8 Compliance and Linting\n  * Add ability to pass socket options to an HTTP Connection\n\n* Erik Tollerud <erik.tollerud@gmail.com>\n  * Support for standard library io module.\n\n* Krishna Prasad <kprasad.iitd@gmail.com>\n  * Google App Engine documentation\n\n* Aaron Meurer <asmeurer@gmail.com>\n  * Added Url.url, which unparses a Url\n\n* Evgeny Kapun <abacabadabacaba@gmail.com>\n  * Bugfixes\n\n* Benjamen Meyer <bm_witness@yahoo.com>\n  * Security Warning Documentation update for proper capture\n\n* Shivan Sornarajah <github@sornars.com>\n  * Support for using ConnectionPool and PoolManager as context managers.\n\n* Alex Gaynor <alex.gaynor@gmail.com>\n  * Updates to the default SSL configuration\n\n* Tomas Tomecek <ttomecek@redhat.com>\n  * Implemented generator for getting chunks from chunked responses.\n\n* tlynn <https://github.com/tlynn>\n  * Respect the warning preferences at import.\n\n* David D. Riddle <ddriddle@illinois.edu>\n  * IPv6 bugfixes in testsuite\n\n* Thea Flowers <magicalgirl@google.com>\n  * App Engine environment tests.\n  * Documentation re-write.\n\n* John Krauss <https://github.com/talos>\n  * Clues to debugging problems with `cryptography` dependency in docs\n\n* Disassem <https://github.com/Disassem>\n  * Fix pool-default headers not applying for url-encoded requests like GET.\n\n* James Atherfold <jlatherfold@hotmail.com>\n  * Bugfixes relating to cleanup of connections during errors.\n\n* Christian Pedersen <https://github.com/chripede>\n  * IPv6 HTTPS proxy bugfix\n\n* Jordan Moldow <https://github.com/jmoldow>\n  * Fix low-level exceptions leaking from ``HTTPResponse.stream()``.\n  * Bugfix for ``ConnectionPool.urlopen(release_conn=False)``.\n  * Creation of ``HTTPConnectionPool.ResponseCls``.\n\n* Predrag Gruevski <https://github.com/obi1kenobi>\n  * Made cert digest comparison use a constant-time algorithm.\n\n* Adam Talsma <https://github.com/a-tal>\n  * Bugfix to ca_cert file paths.\n\n* Evan Meagher <https://evanmeagher.net>\n  * Bugfix related to `memoryview` usage in PyOpenSSL adapter\n\n* John Vandenberg <jayvdb@gmail.com>\n  * Python 2.6 fixes; pyflakes and pep8 compliance\n\n* Andy Caldwell <andy.m.caldwell@googlemail.com>\n  * Bugfix related to reusing connections in indeterminate states.\n\n* Ville Skytt\u00e4 <ville.skytta@iki.fi>\n  * Logging efficiency improvements, spelling fixes, Travis config.\n\n* Shige Takeda <smtakeda@gmail.com>\n  * Started Recipes documentation and added a recipe about handling concatenated gzip data in HTTP response\n\n* Jesse Shapiro <jesse@jesseshapiro.net>\n  * Various character-encoding fixes/tweaks\n  * Disabling IPv6 DNS when IPv6 connections not supported\n\n* David Foster <http://dafoster.net/>\n  * Ensure order of request and response headers are preserved.\n\n* Jeremy Cline <jeremy@jcline.org>\n  * Added connection pool keys by scheme\n\n* Aviv Palivoda <palaviv@gmail.com>\n  * History list to Retry object.\n  * HTTPResponse contains the last Retry object.\n\n* Nate Prewitt <nate.prewitt@gmail.com>\n  * Ensure timeouts are not booleans and greater than zero.\n  * Fixed infinite loop in ``stream`` when amt=None.\n  * Added length_remaining to determine remaining data to be read.\n  * Added enforce_content_length to raise exception when incorrect content-length received.\n\n* Seth Michael Larson <sethmichaellarson@protonmail.com>\n  * Created selectors backport that supports PEP 475.\n\n* Alexandre Dias <alex.dias@smarkets.com>\n  * Don't retry on timeout if method not in whitelist\n\n* Moinuddin Quadri <moin18@gmail.com>\n  * Lazily load idna package\n\n* Tom White <s6yg1ez3@mail2tor.com>\n  * Made SOCKS handler differentiate socks5h from socks5 and socks4a from socks4.\n\n* Tim Burke <tim.burke@gmail.com>\n  * Stop buffering entire deflate-encoded responses.\n\n* Tuukka Mustonen <tuukka.mustonen@gmail.com>\n  * Add counter for status_forcelist retries.\n\n* Erik Rose <erik@mozilla.com>\n  * Bugfix to pyopenssl vendoring\n\n* Wolfgang Richter <wolfgang.richter@gmail.com>\n  * Bugfix related to loading full certificate chains with PyOpenSSL backend.\n\n* Mike Miller <github@mikeage.net>\n  * Logging improvements to include the HTTP(S) port when opening a new connection\n\n* Ioannis Tziakos <mail@itziakos.gr>\n  * Fix ``util.selectors._fileobj_to_fd`` to accept ``long``.\n  * Update appveyor tox setup to use the 64bit python.\n\n* Akamai (through Jesse Shapiro) <jshapiro@akamai.com>\n  * Ongoing maintenance\n\n* Dominique Leuenberger <dimstar@opensuse.org>\n  * Minor fixes in the test suite\n\n* Will Bond <will@wbond.net>\n  * Add Python 2.6 support to ``contrib.securetransport``\n\n* Aleksei Alekseev <alekseev.yeskela@gmail.com>\n  * using auth info for socks proxy\n\n* Chris Wilcox <git@crwilcox.com>\n  * Improve contribution guide\n  * Add ``HTTPResponse.geturl`` method to provide ``urllib2.urlopen().geturl()`` behavior\n\n* Bruce Merry <https://www.brucemerry.org.za>\n  * Fix leaking exceptions when system calls are interrupted with zero timeout\n\n* Hugo van Kemenade <https://github.com/hugovk>\n  * Drop support for EOL Python 2.6\n\n* Tim Bell <https://github.com/timb07>\n  * Bugfix for responses with Content-Type: message/* logging warnings\n\n* Justin Bramley <https://github.com/jbramleycl>\n  * Add ability to handle multiple Content-Encodings\n\n* [Your name or handle] <[email or website]>\n  * [Brief summary of your changes]\n", "from __future__ import absolute_import\nimport collections\nimport functools\nimport logging\n\nfrom ._collections import RecentlyUsedContainer\nfrom .connectionpool import HTTPConnectionPool, HTTPSConnectionPool\nfrom .connectionpool import port_by_scheme\nfrom .exceptions import LocationValueError, MaxRetryError, ProxySchemeUnknown\nfrom .packages.six.moves.urllib.parse import urljoin\nfrom .request import RequestMethods\nfrom .util.url import parse_url\nfrom .util.retry import Retry\n\n\n__all__ = ['PoolManager', 'ProxyManager', 'proxy_from_url']\n\n\nlog = logging.getLogger(__name__)\n\nSSL_KEYWORDS = ('key_file', 'cert_file', 'cert_reqs', 'ca_certs',\n                'ssl_version', 'ca_cert_dir', 'ssl_context')\n\n# All known keyword arguments that could be provided to the pool manager, its\n# pools, or the underlying connections. This is used to construct a pool key.\n_key_fields = (\n    'key_scheme',  # str\n    'key_host',  # str\n    'key_port',  # int\n    'key_timeout',  # int or float or Timeout\n    'key_retries',  # int or Retry\n    'key_strict',  # bool\n    'key_block',  # bool\n    'key_source_address',  # str\n    'key_key_file',  # str\n    'key_cert_file',  # str\n    'key_cert_reqs',  # str\n    'key_ca_certs',  # str\n    'key_ssl_version',  # str\n    'key_ca_cert_dir',  # str\n    'key_ssl_context',  # instance of ssl.SSLContext or urllib3.util.ssl_.SSLContext\n    'key_maxsize',  # int\n    'key_headers',  # dict\n    'key__proxy',  # parsed proxy url\n    'key__proxy_headers',  # dict\n    'key_socket_options',  # list of (level (int), optname (int), value (int or str)) tuples\n    'key__socks_options',  # dict\n    'key_assert_hostname',  # bool or string\n    'key_assert_fingerprint',  # str\n    'key_server_hostname',  # str\n)\n\n#: The namedtuple class used to construct keys for the connection pool.\n#: All custom key schemes should include the fields in this key at a minimum.\nPoolKey = collections.namedtuple('PoolKey', _key_fields)\n\n\ndef _default_key_normalizer(key_class, request_context):\n    \"\"\"\n    Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey\n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    return key_class(**context)\n\n\n#: A dictionary that maps a scheme to a callable that creates a pool key.\n#: This can be used to alter the way pool keys are constructed, if desired.\n#: Each PoolManager makes a copy of this dictionary so they can be configured\n#: globally here, or individually on the instance.\nkey_fn_by_scheme = {\n    'http': functools.partial(_default_key_normalizer, PoolKey),\n    'https': functools.partial(_default_key_normalizer, PoolKey),\n}\n\npool_classes_by_scheme = {\n    'http': HTTPConnectionPool,\n    'https': HTTPSConnectionPool,\n}\n\n\nclass PoolManager(RequestMethods):\n    \"\"\"\n    Allows for arbitrary requests while transparently keeping track of\n    necessary connection pools for you.\n\n    :param num_pools:\n        Number of connection pools to cache before discarding the least\n        recently used pool.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param \\\\**connection_pool_kw:\n        Additional parameters are used to create fresh\n        :class:`urllib3.connectionpool.ConnectionPool` instances.\n\n    Example::\n\n        >>> manager = PoolManager(num_pools=2)\n        >>> r = manager.request('GET', 'http://google.com/')\n        >>> r = manager.request('GET', 'http://google.com/mail')\n        >>> r = manager.request('GET', 'http://yahoo.com/')\n        >>> len(manager.pools)\n        2\n\n    \"\"\"\n\n    proxy = None\n\n    def __init__(self, num_pools=10, headers=None, **connection_pool_kw):\n        RequestMethods.__init__(self, headers)\n        self.connection_pool_kw = connection_pool_kw\n        self.pools = RecentlyUsedContainer(num_pools,\n                                           dispose_func=lambda p: p.close())\n\n        # Locally set the pool classes and keys so other PoolManagers can\n        # override them.\n        self.pool_classes_by_scheme = pool_classes_by_scheme\n        self.key_fn_by_scheme = key_fn_by_scheme.copy()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.clear()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def _new_pool(self, scheme, host, port, request_context=None):\n        \"\"\"\n        Create a new :class:`ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.\n        \"\"\"\n        pool_cls = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in ('scheme', 'host', 'port'):\n            request_context.pop(key, None)\n\n        if scheme == 'http':\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)\n\n    def clear(self):\n        \"\"\"\n        Empty our store of pools and direct them all to close.\n\n        This will not affect in-flight connections, but they will not be\n        re-used after completion.\n        \"\"\"\n        self.pools.clear()\n\n    def connection_from_host(self, host, port=None, scheme='http', pool_kwargs=None):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the host, port, and scheme.\n\n        If ``port`` isn't given, it will be derived from the ``scheme`` using\n        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is\n        provided, it is merged with the instance's ``connection_pool_kw``\n        variable and used to create the new connection pool, if one is\n        needed.\n        \"\"\"\n\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        request_context = self._merge_pool_kwargs(pool_kwargs)\n        request_context['scheme'] = scheme or 'http'\n        if not port:\n            port = port_by_scheme.get(request_context['scheme'].lower(), 80)\n        request_context['port'] = port\n        request_context['host'] = host\n\n        return self.connection_from_context(request_context)\n\n    def connection_from_context(self, request_context):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the request context.\n\n        ``request_context`` must at least contain the ``scheme`` key and its\n        value must be a key in ``key_fn_by_scheme`` instance variable.\n        \"\"\"\n        scheme = request_context['scheme'].lower()\n        pool_key_constructor = self.key_fn_by_scheme[scheme]\n        pool_key = pool_key_constructor(request_context)\n\n        return self.connection_from_pool_key(pool_key, request_context=request_context)\n\n    def connection_from_pool_key(self, pool_key, request_context=None):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the provided pool key.\n\n        ``pool_key`` should be a namedtuple that only contains immutable\n        objects. At a minimum it must have the ``scheme``, ``host``, and\n        ``port`` fields.\n        \"\"\"\n        with self.pools.lock:\n            # If the scheme, host, or port doesn't match existing open\n            # connections, open a new ConnectionPool.\n            pool = self.pools.get(pool_key)\n            if pool:\n                return pool\n\n            # Make a fresh ConnectionPool of the desired type\n            scheme = request_context['scheme']\n            host = request_context['host']\n            port = request_context['port']\n            pool = self._new_pool(scheme, host, port, request_context=request_context)\n            self.pools[pool_key] = pool\n\n        return pool\n\n    def connection_from_url(self, url, pool_kwargs=None):\n        \"\"\"\n        Similar to :func:`urllib3.connectionpool.connection_from_url`.\n\n        If ``pool_kwargs`` is not provided and a new pool needs to be\n        constructed, ``self.connection_pool_kw`` is used to initialize\n        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``\n        is provided, it is used instead. Note that if a new pool does not\n        need to be created for the request, the provided ``pool_kwargs`` are\n        not used.\n        \"\"\"\n        u = parse_url(url)\n        return self.connection_from_host(u.host, port=u.port, scheme=u.scheme,\n                                         pool_kwargs=pool_kwargs)\n\n    def _merge_pool_kwargs(self, override):\n        \"\"\"\n        Merge a dictionary of override values for self.connection_pool_kw.\n\n        This does not modify self.connection_pool_kw and returns a new dict.\n        Any keys in the override dictionary with a value of ``None`` are\n        removed from the merged dictionary.\n        \"\"\"\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs\n\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            for header in retries.remove_headers_on_redirect:\n                kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\n\n\nclass ProxyManager(PoolManager):\n    \"\"\"\n    Behaves just like :class:`PoolManager`, but sends all requests through\n    the defined proxy, using the CONNECT method for HTTPS URLs.\n\n    :param proxy_url:\n        The URL of the proxy to be used.\n\n    :param proxy_headers:\n        A dictionary containing headers that will be sent to the proxy. In case\n        of HTTP they are being sent with each request, while in the\n        HTTPS/CONNECT case they are sent only once. Could be used for proxy\n        authentication.\n\n    Example:\n        >>> proxy = urllib3.ProxyManager('http://localhost:3128/')\n        >>> r1 = proxy.request('GET', 'http://google.com/')\n        >>> r2 = proxy.request('GET', 'http://httpbin.org/')\n        >>> len(proxy.pools)\n        1\n        >>> r3 = proxy.request('GET', 'https://httpbin.org/')\n        >>> r4 = proxy.request('GET', 'https://twitter.com/')\n        >>> len(proxy.pools)\n        3\n\n    \"\"\"\n\n    def __init__(self, proxy_url, num_pools=10, headers=None,\n                 proxy_headers=None, **connection_pool_kw):\n\n        if isinstance(proxy_url, HTTPConnectionPool):\n            proxy_url = '%s://%s:%i' % (proxy_url.scheme, proxy_url.host,\n                                        proxy_url.port)\n        proxy = parse_url(proxy_url)\n        if not proxy.port:\n            port = port_by_scheme.get(proxy.scheme, 80)\n            proxy = proxy._replace(port=port)\n\n        if proxy.scheme not in (\"http\", \"https\"):\n            raise ProxySchemeUnknown(proxy.scheme)\n\n        self.proxy = proxy\n        self.proxy_headers = proxy_headers or {}\n\n        connection_pool_kw['_proxy'] = self.proxy\n        connection_pool_kw['_proxy_headers'] = self.proxy_headers\n\n        super(ProxyManager, self).__init__(\n            num_pools, headers, **connection_pool_kw)\n\n    def connection_from_host(self, host, port=None, scheme='http', pool_kwargs=None):\n        if scheme == \"https\":\n            return super(ProxyManager, self).connection_from_host(\n                host, port, scheme, pool_kwargs=pool_kwargs)\n\n        return super(ProxyManager, self).connection_from_host(\n            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs)\n\n    def _set_proxy_headers(self, url, headers=None):\n        \"\"\"\n        Sets headers needed by proxies: specifically, the Accept and Host\n        headers. Only sets headers not provided by the user.\n        \"\"\"\n        headers_ = {'Accept': '*/*'}\n\n        netloc = parse_url(url).netloc\n        if netloc:\n            headers_['Host'] = netloc\n\n        if headers:\n            headers_.update(headers)\n        return headers_\n\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        u = parse_url(url)\n\n        if u.scheme == \"http\":\n            # For proxied HTTPS requests, httplib sets the necessary headers\n            # on the CONNECT to the proxy. For HTTP, we'll definitely\n            # need to set 'Host' at the very least.\n            headers = kw.get('headers', self.headers)\n            kw['headers'] = self._set_proxy_headers(url, headers)\n\n        return super(ProxyManager, self).urlopen(method, url, redirect=redirect, **kw)\n\n\ndef proxy_from_url(url, **kw):\n    return ProxyManager(proxy_url=url, **kw)\n", "from __future__ import absolute_import\nimport time\nimport logging\nfrom collections import namedtuple\nfrom itertools import takewhile\nimport email\nimport re\n\nfrom ..exceptions import (\n    ConnectTimeoutError,\n    MaxRetryError,\n    ProtocolError,\n    ReadTimeoutError,\n    ResponseError,\n    InvalidHeader,\n)\nfrom ..packages import six\n\n\nlog = logging.getLogger(__name__)\n\n\n# Data structure for representing the metadata of requests that result in a retry.\nRequestHistory = namedtuple('RequestHistory', [\"method\", \"url\", \"error\",\n                                               \"status\", \"redirect_location\"])\n\n\nclass Retry(object):\n    \"\"\" Retry configuration.\n\n    Each retry attempt will create a new Retry object with updated values, so\n    they can be safely reused.\n\n    Retries can be defined as a default for a pool::\n\n        retries = Retry(connect=5, read=2, redirect=5)\n        http = PoolManager(retries=retries)\n        response = http.request('GET', 'http://example.com/')\n\n    Or per-request (which overrides the default for the pool)::\n\n        response = http.request('GET', 'http://example.com/', retries=Retry(10))\n\n    Retries can be disabled by passing ``False``::\n\n        response = http.request('GET', 'http://example.com/', retries=False)\n\n    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless\n    retries are disabled, in which case the causing exception will be raised.\n\n    :param int total:\n        Total number of retries to allow. Takes precedence over other counts.\n\n        Set to ``None`` to remove this constraint and fall back on other\n        counts. It's a good idea to set this to some sensibly-high value to\n        account for unexpected edge cases and avoid infinite retry loops.\n\n        Set to ``0`` to fail on the first retry.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int connect:\n        How many connection-related errors to retry on.\n\n        These are errors raised before the request is sent to the remote server,\n        which we assume has not triggered the server to process the request.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int read:\n        How many times to retry on read errors.\n\n        These errors are raised after the request was sent to the server, so the\n        request may have side-effects.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int redirect:\n        How many redirects to perform. Limit this to avoid infinite redirect\n        loops.\n\n        A redirect is a HTTP response with a status code 301, 302, 303, 307 or\n        308.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int status:\n        How many times to retry on bad status codes.\n\n        These are retries made on responses, where status code matches\n        ``status_forcelist``.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param iterable method_whitelist:\n        Set of uppercased HTTP method verbs that we should retry on.\n\n        By default, we only retry on methods which are considered to be\n        idempotent (multiple requests with the same parameters end with the\n        same state). See :attr:`Retry.DEFAULT_METHOD_WHITELIST`.\n\n        Set to a ``False`` value to retry on any verb.\n\n    :param iterable status_forcelist:\n        A set of integer HTTP status codes that we should force a retry on.\n        A retry is initiated if the request method is in ``method_whitelist``\n        and the response status code is in ``status_forcelist``.\n\n        By default, this is disabled with ``None``.\n\n    :param float backoff_factor:\n        A backoff factor to apply between attempts after the second try\n        (most errors are resolved immediately by a second try without a\n        delay). urllib3 will sleep for::\n\n            {backoff factor} * (2 ** ({number of total retries} - 1))\n\n        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep\n        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer\n        than :attr:`Retry.BACKOFF_MAX`.\n\n        By default, backoff is disabled (set to 0).\n\n    :param bool raise_on_redirect: Whether, if the number of redirects is\n        exhausted, to raise a MaxRetryError, or to return a response with a\n        response code in the 3xx range.\n\n    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:\n        whether we should raise an exception, or return a response,\n        if status falls in ``status_forcelist`` range and retries have\n        been exhausted.\n\n    :param tuple history: The history of the request encountered during\n        each call to :meth:`~Retry.increment`. The list is in the order\n        the requests occurred. Each list item is of class :class:`RequestHistory`.\n\n    :param bool respect_retry_after_header:\n        Whether to respect Retry-After header on status codes defined as\n        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.\n\n    :param iterable remove_headers_on_redirect:\n        Sequence of headers to remove from the request when a response\n        indicating a redirect is returned before firing off the redirected\n        request.\n    \"\"\"\n\n    DEFAULT_METHOD_WHITELIST = frozenset([\n        'HEAD', 'GET', 'PUT', 'DELETE', 'OPTIONS', 'TRACE'])\n\n    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])\n\n    DEFAULT_REDIRECT_HEADERS_BLACKLIST = frozenset(['Authorization'])\n\n    #: Maximum backoff time.\n    BACKOFF_MAX = 120\n\n    def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,\n                 method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                 backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n                 history=None, respect_retry_after_header=True,\n                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):\n\n        self.total = total\n        self.connect = connect\n        self.read = read\n        self.status = status\n\n        if redirect is False or total is False:\n            redirect = 0\n            raise_on_redirect = False\n\n        self.redirect = redirect\n        self.status_forcelist = status_forcelist or set()\n        self.method_whitelist = method_whitelist\n        self.backoff_factor = backoff_factor\n        self.raise_on_redirect = raise_on_redirect\n        self.raise_on_status = raise_on_status\n        self.history = history or tuple()\n        self.respect_retry_after_header = respect_retry_after_header\n        self.remove_headers_on_redirect = remove_headers_on_redirect\n\n    def new(self, **kw):\n        params = dict(\n            total=self.total,\n            connect=self.connect, read=self.read, redirect=self.redirect, status=self.status,\n            method_whitelist=self.method_whitelist,\n            status_forcelist=self.status_forcelist,\n            backoff_factor=self.backoff_factor,\n            raise_on_redirect=self.raise_on_redirect,\n            raise_on_status=self.raise_on_status,\n            history=self.history,\n            remove_headers_on_redirect=self.remove_headers_on_redirect\n        )\n        params.update(kw)\n        return type(self)(**params)\n\n    @classmethod\n    def from_int(cls, retries, redirect=True, default=None):\n        \"\"\" Backwards-compatibility for the old retries format.\"\"\"\n        if retries is None:\n            retries = default if default is not None else cls.DEFAULT\n\n        if isinstance(retries, Retry):\n            return retries\n\n        redirect = bool(redirect) and None\n        new_retries = cls(retries, redirect=redirect)\n        log.debug(\"Converted retries value: %r -> %r\", retries, new_retries)\n        return new_retries\n\n    def get_backoff_time(self):\n        \"\"\" Formula for computing the current backoff\n\n        :rtype: float\n        \"\"\"\n        # We want to consider only the last consecutive errors sequence (Ignore redirects).\n        consecutive_errors_len = len(list(takewhile(lambda x: x.redirect_location is None,\n                                                    reversed(self.history))))\n        if consecutive_errors_len <= 1:\n            return 0\n\n        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))\n        return min(self.BACKOFF_MAX, backoff_value)\n\n    def parse_retry_after(self, retry_after):\n        # Whitespace: https://tools.ietf.org/html/rfc7230#section-3.2.4\n        if re.match(r\"^\\s*[0-9]+\\s*$\", retry_after):\n            seconds = int(retry_after)\n        else:\n            retry_date_tuple = email.utils.parsedate(retry_after)\n            if retry_date_tuple is None:\n                raise InvalidHeader(\"Invalid Retry-After header: %s\" % retry_after)\n            retry_date = time.mktime(retry_date_tuple)\n            seconds = retry_date - time.time()\n\n        if seconds < 0:\n            seconds = 0\n\n        return seconds\n\n    def get_retry_after(self, response):\n        \"\"\" Get the value of Retry-After in seconds. \"\"\"\n\n        retry_after = response.getheader(\"Retry-After\")\n\n        if retry_after is None:\n            return None\n\n        return self.parse_retry_after(retry_after)\n\n    def sleep_for_retry(self, response=None):\n        retry_after = self.get_retry_after(response)\n        if retry_after:\n            time.sleep(retry_after)\n            return True\n\n        return False\n\n    def _sleep_backoff(self):\n        backoff = self.get_backoff_time()\n        if backoff <= 0:\n            return\n        time.sleep(backoff)\n\n    def sleep(self, response=None):\n        \"\"\" Sleep between retry attempts.\n\n        This method will respect a server's ``Retry-After`` response header\n        and sleep the duration of the time requested. If that is not present, it\n        will use an exponential backoff. By default, the backoff factor is 0 and\n        this method will return immediately.\n        \"\"\"\n\n        if response:\n            slept = self.sleep_for_retry(response)\n            if slept:\n                return\n\n        self._sleep_backoff()\n\n    def _is_connection_error(self, err):\n        \"\"\" Errors when we're fairly sure that the server did not receive the\n        request, so it should be safe to retry.\n        \"\"\"\n        return isinstance(err, ConnectTimeoutError)\n\n    def _is_read_error(self, err):\n        \"\"\" Errors that occur after the request has been started, so we should\n        assume that the server began processing it.\n        \"\"\"\n        return isinstance(err, (ReadTimeoutError, ProtocolError))\n\n    def _is_method_retryable(self, method):\n        \"\"\" Checks if a given HTTP method should be retried upon, depending if\n        it is included on the method whitelist.\n        \"\"\"\n        if self.method_whitelist and method.upper() not in self.method_whitelist:\n            return False\n\n        return True\n\n    def is_retry(self, method, status_code, has_retry_after=False):\n        \"\"\" Is this method/status code retryable? (Based on whitelists and control\n        variables such as the number of total retries to allow, whether to\n        respect the Retry-After header, whether this header is present, and\n        whether the returned status code is on the list of status codes to\n        be retried upon on the presence of the aforementioned header)\n        \"\"\"\n        if not self._is_method_retryable(method):\n            return False\n\n        if self.status_forcelist and status_code in self.status_forcelist:\n            return True\n\n        return (self.total and self.respect_retry_after_header and\n                has_retry_after and (status_code in self.RETRY_AFTER_STATUS_CODES))\n\n    def is_exhausted(self):\n        \"\"\" Are we out of retries? \"\"\"\n        retry_counts = (self.total, self.connect, self.read, self.redirect, self.status)\n        retry_counts = list(filter(None, retry_counts))\n        if not retry_counts:\n            return False\n\n        return min(retry_counts) < 0\n\n    def increment(self, method=None, url=None, response=None, error=None,\n                  _pool=None, _stacktrace=None):\n        \"\"\" Return a new Retry object with incremented retry counters.\n\n        :param response: A response object, or None, if the server did not\n            return a response.\n        :type response: :class:`~urllib3.response.HTTPResponse`\n        :param Exception error: An error encountered during the request, or\n            None if the response was received successfully.\n\n        :return: A new ``Retry`` object.\n        \"\"\"\n        if self.total is False and error:\n            # Disabled, indicate to re-raise the error.\n            raise six.reraise(type(error), error, _stacktrace)\n\n        total = self.total\n        if total is not None:\n            total -= 1\n\n        connect = self.connect\n        read = self.read\n        redirect = self.redirect\n        status_count = self.status\n        cause = 'unknown'\n        status = None\n        redirect_location = None\n\n        if error and self._is_connection_error(error):\n            # Connect retry?\n            if connect is False:\n                raise six.reraise(type(error), error, _stacktrace)\n            elif connect is not None:\n                connect -= 1\n\n        elif error and self._is_read_error(error):\n            # Read retry?\n            if read is False or not self._is_method_retryable(method):\n                raise six.reraise(type(error), error, _stacktrace)\n            elif read is not None:\n                read -= 1\n\n        elif response and response.get_redirect_location():\n            # Redirect retry?\n            if redirect is not None:\n                redirect -= 1\n            cause = 'too many redirects'\n            redirect_location = response.get_redirect_location()\n            status = response.status\n\n        else:\n            # Incrementing because of a server error like a 500 in\n            # status_forcelist and a the given method is in the whitelist\n            cause = ResponseError.GENERIC_ERROR\n            if response and response.status:\n                if status_count is not None:\n                    status_count -= 1\n                cause = ResponseError.SPECIFIC_ERROR.format(\n                    status_code=response.status)\n                status = response.status\n\n        history = self.history + (RequestHistory(method, url, error, status, redirect_location),)\n\n        new_retry = self.new(\n            total=total,\n            connect=connect, read=read, redirect=redirect, status=status_count,\n            history=history)\n\n        if new_retry.is_exhausted():\n            raise MaxRetryError(_pool, url, error or ResponseError(cause))\n\n        log.debug(\"Incremented Retry for (url='%s'): %r\", url, new_retry)\n\n        return new_retry\n\n    def __repr__(self):\n        return ('{cls.__name__}(total={self.total}, connect={self.connect}, '\n                'read={self.read}, redirect={self.redirect}, status={self.status})').format(\n                    cls=type(self), self=self)\n\n\n# For backwards compatibility (equivalent to pre-v1.9):\nRetry.DEFAULT = Retry(3)\n", "import pytest\n\nfrom urllib3.response import HTTPResponse\nfrom urllib3.packages.six.moves import xrange\nfrom urllib3.util.retry import Retry, RequestHistory\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    MaxRetryError,\n    ReadTimeoutError,\n    ResponseError,\n)\n\n\nclass TestRetry(object):\n\n    def test_string(self):\n        \"\"\" Retry string representation looks the way we expect \"\"\"\n        retry = Retry()\n        assert str(retry) == 'Retry(total=10, connect=None, read=None, redirect=None, status=None)'\n        for _ in range(3):\n            retry = retry.increment(method='GET')\n        assert str(retry) == 'Retry(total=7, connect=None, read=None, redirect=None, status=None)'\n\n    def test_retry_both_specified(self):\n        \"\"\"Total can win if it's lower than the connect value\"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=3, total=2)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(error=error)\n        assert e.value.reason == error\n\n    def test_retry_higher_total_loses(self):\n        \"\"\" A lower connect timeout than the total is honored \"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=2, total=3)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(error=error)\n\n    def test_retry_higher_total_loses_vs_read(self):\n        \"\"\" A lower read timeout than the total is honored \"\"\"\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(read=2, total=3)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(method='GET', error=error)\n\n    def test_retry_total_none(self):\n        \"\"\" if Total is none, connect error should take precedence \"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=2, total=None)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(error=error)\n        assert e.value.reason == error\n\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(connect=2, total=None)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        assert not retry.is_exhausted()\n\n    def test_retry_default(self):\n        \"\"\" If no value is specified, should retry connects 3 times \"\"\"\n        retry = Retry()\n        assert retry.total == 10\n        assert retry.connect is None\n        assert retry.read is None\n        assert retry.redirect is None\n\n        error = ConnectTimeoutError()\n        retry = Retry(connect=1)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(error=error)\n\n        retry = Retry(connect=1)\n        retry = retry.increment(error=error)\n        assert not retry.is_exhausted()\n\n        assert Retry(0).raise_on_redirect\n        assert not Retry(False).raise_on_redirect\n\n    def test_retry_read_zero(self):\n        \"\"\" No second chances on read timeouts, by default \"\"\"\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(read=0)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(method='GET', error=error)\n        assert e.value.reason == error\n\n    def test_status_counter(self):\n        resp = HTTPResponse(status=400)\n        retry = Retry(status=2)\n        retry = retry.increment(response=resp)\n        retry = retry.increment(response=resp)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(response=resp)\n        assert str(e.value.reason) == ResponseError.SPECIFIC_ERROR.format(status_code=400)\n\n    def test_backoff(self):\n        \"\"\" Backoff is computed correctly \"\"\"\n        max_backoff = Retry.BACKOFF_MAX\n\n        retry = Retry(total=100, backoff_factor=0.2)\n        assert retry.get_backoff_time() == 0  # First request\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0  # First retry\n\n        retry = retry.increment(method='GET')\n        assert retry.backoff_factor == 0.2\n        assert retry.total == 98\n        assert retry.get_backoff_time() == 0.4  # Start backoff\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.8\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 1.6\n\n        for _ in xrange(10):\n            retry = retry.increment(method='GET')\n\n        assert retry.get_backoff_time() == max_backoff\n\n    def test_zero_backoff(self):\n        retry = Retry()\n        assert retry.get_backoff_time() == 0\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0\n\n    def test_backoff_reset_after_redirect(self):\n        retry = Retry(total=100, redirect=5, backoff_factor=0.2)\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.4\n        redirect_response = HTTPResponse(status=302, headers={'location': 'test'})\n        retry = retry.increment(method='GET', response=redirect_response)\n        assert retry.get_backoff_time() == 0\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.4\n\n    def test_sleep(self):\n        # sleep a very small amount of time so our code coverage is happy\n        retry = Retry(backoff_factor=0.0001)\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        retry.sleep()\n\n    def test_status_forcelist(self):\n        retry = Retry(status_forcelist=xrange(500, 600))\n        assert not retry.is_retry('GET', status_code=200)\n        assert not retry.is_retry('GET', status_code=400)\n        assert retry.is_retry('GET', status_code=500)\n\n        retry = Retry(total=1, status_forcelist=[418])\n        assert not retry.is_retry('GET', status_code=400)\n        assert retry.is_retry('GET', status_code=418)\n\n        # String status codes are not matched.\n        retry = Retry(total=1, status_forcelist=['418'])\n        assert not retry.is_retry('GET', status_code=418)\n\n    def test_method_whitelist_with_status_forcelist(self):\n        # Falsey method_whitelist means to retry on any method.\n        retry = Retry(status_forcelist=[500], method_whitelist=None)\n        assert retry.is_retry('GET', status_code=500)\n        assert retry.is_retry('POST', status_code=500)\n\n        # Criteria of method_whitelist and status_forcelist are ANDed.\n        retry = Retry(status_forcelist=[500], method_whitelist=['POST'])\n        assert not retry.is_retry('GET', status_code=500)\n        assert retry.is_retry('POST', status_code=500)\n\n    def test_exhausted(self):\n        assert not Retry(0).is_exhausted()\n        assert Retry(-1).is_exhausted()\n        assert Retry(1).increment(method='GET').total == 0\n\n    @pytest.mark.parametrize('total', [-1, 0])\n    def test_disabled(self, total):\n        with pytest.raises(MaxRetryError):\n            Retry(total).increment(method='GET')\n\n    def test_error_message(self):\n        retry = Retry(total=0)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment(method='GET',\n                                    error=ReadTimeoutError(None, \"/\", \"read timed out\"))\n        assert 'Caused by redirect' not in str(e.value)\n        assert str(e.value.reason) == 'None: read timed out'\n\n        retry = Retry(total=1)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment('POST', '/')\n            retry = retry.increment('POST', '/')\n        assert 'Caused by redirect' not in str(e.value)\n        assert isinstance(e.value.reason, ResponseError)\n        assert str(e.value.reason) == ResponseError.GENERIC_ERROR\n\n        retry = Retry(total=1)\n        response = HTTPResponse(status=500)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment('POST', '/', response=response)\n            retry = retry.increment('POST', '/', response=response)\n        assert 'Caused by redirect' not in str(e.value)\n        msg = ResponseError.SPECIFIC_ERROR.format(status_code=500)\n        assert str(e.value.reason) == msg\n\n        retry = Retry(connect=1)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment(error=ConnectTimeoutError('conntimeout'))\n            retry = retry.increment(error=ConnectTimeoutError('conntimeout'))\n        assert 'Caused by redirect' not in str(e.value)\n        assert str(e.value.reason) == 'conntimeout'\n\n    def test_history(self):\n        retry = Retry(total=10, method_whitelist=frozenset(['GET', 'POST']))\n        assert retry.history == tuple()\n        connection_error = ConnectTimeoutError('conntimeout')\n        retry = retry.increment('GET', '/test1', None, connection_error)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),)\n        assert retry.history == history\n\n        read_error = ReadTimeoutError(None, \"/test2\", \"read timed out\")\n        retry = retry.increment('POST', '/test2', None, read_error)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),\n                   RequestHistory('POST', '/test2', read_error, None, None))\n        assert retry.history == history\n\n        response = HTTPResponse(status=500)\n        retry = retry.increment('GET', '/test3', response, None)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),\n                   RequestHistory('POST', '/test2', read_error, None, None),\n                   RequestHistory('GET', '/test3', None, 500, None))\n        assert retry.history == history\n\n    def test_retry_method_not_in_whitelist(self):\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry()\n        with pytest.raises(ReadTimeoutError):\n            retry.increment(method='POST', error=error)\n\n    def test_retry_default_remove_headers_on_redirect(self):\n        retry = Retry()\n\n        assert list(retry.remove_headers_on_redirect) == ['Authorization']\n\n    def test_retry_set_remove_headers_on_redirect(self):\n        retry = Retry(remove_headers_on_redirect=['X-API-Secret'])\n\n        assert list(retry.remove_headers_on_redirect) == ['X-API-Secret']\n", "import unittest\nimport json\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6\nfrom dummyserver.testcase import (HTTPDummyServerTestCase,\n                                  IPv6HTTPDummyServerTestCase)\nfrom urllib3.poolmanager import PoolManager\nfrom urllib3.connectionpool import port_by_scheme\nfrom urllib3.exceptions import MaxRetryError\nfrom urllib3.util.retry import Retry\n\n\nclass TestPoolManager(HTTPDummyServerTestCase):\n\n    def setUp(self):\n        self.base_url = 'http://%s:%d' % (self.host, self.port)\n        self.base_url_alt = 'http://%s:%d' % (self.host_alt, self.port)\n\n    def test_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/' % self.base_url},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/' % self.base_url})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_redirect_twice(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect' % self.base_url},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                       self.base_url)})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_redirect_to_relative_url(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '/redirect'},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '/redirect'})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_cross_host_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        cross_host_location = '%s/echo?a=b' % self.base_url_alt\n        try:\n            http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': cross_host_location},\n                         timeout=1, retries=0)\n            self.fail(\"Request succeeded instead of raising an exception like it should.\")\n\n        except MaxRetryError:\n            pass\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/echo?a=b' % self.base_url_alt},\n                         timeout=1, retries=1)\n\n        self.assertEqual(r._pool.host, self.host_alt)\n\n    def test_too_many_redirects(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        try:\n            r = http.request('GET', '%s/redirect' % self.base_url,\n                             fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                           self.base_url)},\n                             retries=1)\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        try:\n            r = http.request('GET', '%s/redirect' % self.base_url,\n                             fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                           self.base_url)},\n                             retries=Retry(total=None, redirect=1))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n    def test_redirect_cross_host_remove_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'Authorization': 'foo'})\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('Authorization', data)\n\n    def test_redirect_cross_host_no_remove_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'Authorization': 'foo'},\n                         retries=Retry(remove_headers_on_redirect=[]))\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertEqual(data['Authorization'], 'foo')\n\n    def test_redirect_cross_host_set_removed_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'X-API-Secret': 'foo',\n                                  'Authorization': 'bar'},\n                         retries=Retry(remove_headers_on_redirect=['X-API-Secret']))\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('X-API-Secret', data)\n        self.assertEqual(data['Authorization'], 'bar')\n\n    def test_raise_on_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                       self.base_url)},\n                         retries=Retry(total=None, redirect=1, raise_on_redirect=False))\n\n        self.assertEqual(r.status, 303)\n\n    def test_raise_on_status(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        try:\n            # the default is to raise\n            r = http.request('GET', '%s/status' % self.base_url,\n                             fields={'status': '500 Internal Server Error'},\n                             retries=Retry(total=1, status_forcelist=range(500, 600)))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        try:\n            # raise explicitly\n            r = http.request('GET', '%s/status' % self.base_url,\n                             fields={'status': '500 Internal Server Error'},\n                             retries=Retry(total=1,\n                                           status_forcelist=range(500, 600),\n                                           raise_on_status=True))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        # don't raise\n        r = http.request('GET', '%s/status' % self.base_url,\n                         fields={'status': '500 Internal Server Error'},\n                         retries=Retry(total=1,\n                                       status_forcelist=range(500, 600),\n                                       raise_on_status=False))\n\n        self.assertEqual(r.status, 500)\n\n    def test_missing_port(self):\n        # Can a URL that lacks an explicit port like ':80' succeed, or\n        # will all such URLs fail with an error?\n\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        # By globally adjusting `port_by_scheme` we pretend for a moment\n        # that HTTP's default port is not 80, but is the port at which\n        # our test server happens to be listening.\n        port_by_scheme['http'] = self.port\n        try:\n            r = http.request('GET', 'http://%s/' % self.host, retries=0)\n        finally:\n            port_by_scheme['http'] = 80\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_headers(self):\n        http = PoolManager(headers={'Foo': 'bar'})\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request('POST', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_url('GET', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_body('POST', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_url('GET', '%s/headers' % self.base_url, headers={'Baz': 'quux'})\n        returned_headers = json.loads(r.data.decode())\n        self.assertIsNone(returned_headers.get('Foo'))\n        self.assertEqual(returned_headers.get('Baz'), 'quux')\n\n        r = http.request_encode_body('GET', '%s/headers' % self.base_url, headers={'Baz': 'quux'})\n        returned_headers = json.loads(r.data.decode())\n        self.assertIsNone(returned_headers.get('Foo'))\n        self.assertEqual(returned_headers.get('Baz'), 'quux')\n\n    def test_http_with_ssl_keywords(self):\n        http = PoolManager(ca_certs='REQUIRED')\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', 'http://%s:%s/' % (self.host, self.port))\n        self.assertEqual(r.status, 200)\n\n    def test_http_with_ca_cert_dir(self):\n        http = PoolManager(ca_certs='REQUIRED', ca_cert_dir='/nosuchdir')\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', 'http://%s:%s/' % (self.host, self.port))\n        self.assertEqual(r.status, 200)\n\n\n@pytest.mark.skipif(\n    not HAS_IPV6,\n    reason='IPv6 is not supported on this system'\n)\nclass TestIPv6PoolManager(IPv6HTTPDummyServerTestCase):\n\n    def setUp(self):\n        self.base_url = 'http://[%s]:%d' % (self.host, self.port)\n\n    def test_ipv6(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n        http.request('GET', self.base_url)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"], "fixing_code": ["Changes\n=======\n\ndev (master)\n------------\n\n* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)\n\n* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)\n\n* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)\n\n* ... [Short description of non-trivial change.] (Issue #)\n\n\n1.24.1 (2018-11-02)\n-------------------\n\n* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)\n\n* Restored functionality of `ciphers` parameter for `create_urllib3_context()`. (Issue #1462)\n\n\n1.24 (2018-10-16)\n-----------------\n\n* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)\n\n* Test against Python 3.7 on AppVeyor. (Pull #1453)\n\n* Early-out ipv6 checks when running on App Engine. (Pull #1450)\n\n* Change ambiguous description of backoff_factor (Pull #1436)\n\n* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)\n\n* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).\n\n* Add a server_hostname parameter to HTTPSConnection which allows for\n  overriding the SNI hostname sent in the handshake. (Pull #1397)\n\n* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)\n\n* Fixed bug where responses with header Content-Type: message/* erroneously\n  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)\n\n* Move urllib3 to src/urllib3 (Pull #1409)\n\n\n1.23 (2018-06-04)\n-----------------\n\n* Allow providing a list of headers to strip from requests when redirecting\n  to a different host. Defaults to the ``Authorization`` header. Different\n  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)\n\n* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).\n\n* Dropped Python 3.3 support. (Pull #1242)\n\n* Put the connection back in the pool when calling stream() or read_chunked() on\n  a chunked HEAD response. (Issue #1234)\n\n* Fixed pyOpenSSL-specific ssl client authentication issue when clients\n  attempted to auth via certificate + chain (Issue #1060)\n\n* Add the port to the connectionpool connect print (Pull #1251)\n\n* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)\n\n* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)\n\n* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)\n\n* Added support for auth info in url for SOCKS proxy (Pull #1363)\n\n\n1.22 (2017-07-20)\n-----------------\n\n* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via\n  IPv6 proxy. (Issue #1222)\n\n* Made the connection pool retry on ``SSLError``.  The original ``SSLError``\n  is available on ``MaxRetryError.reason``. (Issue #1112)\n\n* Drain and release connection before recursing on retry/redirect.  Fixes\n  deadlocks with a blocking connectionpool. (Issue #1167)\n\n* Fixed compatibility for cookiejar. (Issue #1229)\n\n* pyopenssl: Use vendored version of ``six``. (Issue #1231)\n\n\n1.21.1 (2017-05-02)\n-------------------\n\n* Fixed SecureTransport issue that would cause long delays in response body\n  delivery. (Pull #1154)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)\n\n* Fixed regression in 1.21 that threw exceptions when users passed the\n  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.\n  (Pull #1157)\n\n\n1.21 (2017-04-25)\n-----------------\n\n* Improved performance of certain selector system calls on Python 3.5 and\n  later. (Pull #1095)\n\n* Resolved issue where the PyOpenSSL backend would not wrap SysCallError\n  exceptions appropriately when sending data. (Pull #1125)\n\n* Selectors now detects a monkey-patched select module after import for modules\n  that patch the select module like eventlet, greenlet. (Pull #1128)\n\n* Reduced memory consumption when streaming zlib-compressed responses\n  (as opposed to raw deflate streams). (Pull #1129)\n\n* Connection pools now use the entire request context when constructing the\n  pool key. (Pull #1016)\n\n* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,\n  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.\n  (Pull #1016)\n\n* Add retry counter for ``status_forcelist``. (Issue #1147)\n\n* Added ``contrib`` module for using SecureTransport on macOS:\n  ``urllib3.contrib.securetransport``.  (Pull #1122)\n\n* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:\n  for schemes it does not recognise, it assumes they are case-sensitive and\n  leaves them unchanged.\n  (Issue #1080)\n\n\n1.20 (2017-01-19)\n-----------------\n\n* Added support for waiting for I/O using selectors other than select,\n  improving urllib3's behaviour with large numbers of concurrent connections.\n  (Pull #1001)\n\n* Updated the date for the system clock check. (Issue #1005)\n\n* ConnectionPools now correctly consider hostnames to be case-insensitive.\n  (Issue #1032)\n\n* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Pull #1063)\n\n* Outdated versions of cryptography now cause the PyOpenSSL contrib module\n  to fail when it is injected, rather than at first use. (Issue #1044)\n\n* Automatically attempt to rewind a file-like body object when a request is\n  retried or redirected. (Pull #1039)\n\n* Fix some bugs that occur when modules incautiously patch the queue module.\n  (Pull #1061)\n\n* Prevent retries from occurring on read timeouts for which the request method\n  was not in the method whitelist. (Issue #1059)\n\n* Changed the PyOpenSSL contrib module to lazily load idna to avoid\n  unnecessarily bloating the memory of programs that don't need it. (Pull\n  #1076)\n\n* Add support for IPv6 literals with zone identifiers. (Pull #1013)\n\n* Added support for socks5h:// and socks4a:// schemes when working with SOCKS\n  proxies, and controlled remote DNS appropriately. (Issue #1035)\n\n\n1.19.1 (2016-11-16)\n-------------------\n\n* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)\n\n\n1.19 (2016-11-03)\n-----------------\n\n* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when\n  using the default retry logic. (Pull #955)\n\n* Remove markers from setup.py to assist ancient setuptools versions. (Issue\n  #986)\n\n* Disallow superscripts and other integerish things in URL ports. (Issue #989)\n\n* Allow urllib3's HTTPResponse.stream() method to continue to work with\n  non-httplib underlying FPs. (Pull #990)\n\n* Empty filenames in multipart headers are now emitted as such, rather than\n  being suppressed. (Issue #1015)\n\n* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)\n\n\n1.18.1 (2016-10-27)\n-------------------\n\n* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with\n  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This\n  release fixes a vulnerability whereby urllib3 in the above configuration\n  would silently fail to validate TLS certificates due to erroneously setting\n  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous\n  flags do not cause a problem in OpenSSL versions before 1.1.0, which\n  interprets the presence of any flag as requesting certificate validation.\n\n  There is no PR for this patch, as it was prepared for simultaneous disclosure\n  and release. The master branch received the same fix in PR #1010.\n\n\n1.18 (2016-09-26)\n-----------------\n\n* Fixed incorrect message for IncompleteRead exception. (PR #973)\n\n* Accept ``iPAddress`` subject alternative name fields in TLS certificates.\n  (Issue #258)\n\n* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.\n  (Issue #977)\n\n* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)\n\n\n1.17 (2016-09-06)\n-----------------\n\n* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)\n\n* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)\n\n* Substantially refactored documentation. (Issue #887)\n\n* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.\n  (Issue #858)\n\n* Normalize the scheme and host in the URL parser (Issue #833)\n\n* ``HTTPResponse`` contains the last ``Retry`` object, which now also\n  contains retries history. (Issue #848)\n\n* Timeout can no longer be set as boolean, and must be greater than zero.\n  (PR #924)\n\n* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We\n  now use cryptography and idna, both of which are already dependencies of\n  PyOpenSSL. (PR #930)\n\n* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)\n\n* Try to use the operating system's certificates when we are using an\n  ``SSLContext``. (PR #941)\n\n* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to\n  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)\n\n* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)\n\n* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)\n\n* Implemented ``length_remaining`` to determine remaining content\n  to be read. (PR #949)\n\n* Implemented ``enforce_content_length`` to enable exceptions when\n  incomplete data chunks are received. (PR #949)\n\n* Dropped connection start, dropped connection reset, redirect, forced retry,\n  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)\n\n\n1.16 (2016-06-11)\n-----------------\n\n* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)\n\n* Provide ``key_fn_by_scheme`` pool keying mechanism that can be\n  overridden. (Issue #830)\n\n* Normalize scheme and host to lowercase for pool keys, and include\n  ``source_address``. (Issue #830)\n\n* Cleaner exception chain in Python 3 for ``_make_request``.\n  (Issue #861)\n\n* Fixed installing ``urllib3[socks]`` extra. (Issue #864)\n\n* Fixed signature of ``ConnectionPool.close`` so it can actually safely be\n  called by subclasses. (Issue #873)\n\n* Retain ``release_conn`` state across retries. (Issues #651, #866)\n\n* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to\n  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)\n\n\n1.15.1 (2016-04-11)\n-------------------\n\n* Fix packaging to include backports module. (Issue #841)\n\n\n1.15 (2016-04-06)\n-----------------\n\n* Added Retry(raise_on_status=False). (Issue #720)\n\n* Always use setuptools, no more distutils fallback. (Issue #785)\n\n* Dropped support for Python 3.2. (Issue #786)\n\n* Chunked transfer encoding when requesting with ``chunked=True``.\n  (Issue #790)\n\n* Fixed regression with IPv6 port parsing. (Issue #801)\n\n* Append SNIMissingWarning messages to allow users to specify it in\n  the PYTHONWARNINGS environment variable. (Issue #816)\n\n* Handle unicode headers in Py2. (Issue #818)\n\n* Log certificate when there is a hostname mismatch. (Issue #820)\n\n* Preserve order of request/response headers. (Issue #821)\n\n\n1.14 (2015-12-29)\n-----------------\n\n* contrib: SOCKS proxy support! (Issue #762)\n\n* Fixed AppEngine handling of transfer-encoding header and bug\n  in Timeout defaults checking. (Issue #763)\n\n\n1.13.1 (2015-12-18)\n-------------------\n\n* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)\n\n\n1.13 (2015-12-14)\n-----------------\n\n* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)\n\n* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)\n\n* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)\n\n* Close connections more defensively on exception. (Issue #734)\n\n* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without\n  repeatedly flushing the decoder, to function better on Jython. (Issue #743)\n\n* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)\n\n\n1.12 (2015-09-03)\n-----------------\n\n* Rely on ``six`` for importing ``httplib`` to work around\n  conflicts with other Python 3 shims. (Issue #688)\n\n* Add support for directories of certificate authorities, as supported by\n  OpenSSL. (Issue #701)\n\n* New exception: ``NewConnectionError``, raised when we fail to establish\n  a new connection, usually ``ECONNREFUSED`` socket error.\n\n\n1.11 (2015-07-21)\n-----------------\n\n* When ``ca_certs`` is given, ``cert_reqs`` defaults to\n  ``'CERT_REQUIRED'``. (Issue #650)\n\n* ``pip install urllib3[secure]`` will install Certifi and\n  PyOpenSSL as dependencies. (Issue #678)\n\n* Made ``HTTPHeaderDict`` usable as a ``headers`` input value\n  (Issues #632, #679)\n\n* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_\n  which has an ``AppEngineManager`` for using ``URLFetch`` in a\n  Google AppEngine environment. (Issue #664)\n\n* Dev: Added test suite for AppEngine. (Issue #631)\n\n* Fix performance regression when using PyOpenSSL. (Issue #626)\n\n* Passing incorrect scheme (e.g. ``foo://``) will raise\n  ``ValueError`` instead of ``AssertionError`` (backwards\n  compatible for now, but please migrate). (Issue #640)\n\n* Fix pools not getting replenished when an error occurs during a\n  request using ``release_conn=False``. (Issue #644)\n\n* Fix pool-default headers not applying for url-encoded requests\n  like GET. (Issue #657)\n\n* log.warning in Python 3 when headers are skipped due to parsing\n  errors. (Issue #642)\n\n* Close and discard connections if an error occurs during read.\n  (Issue #660)\n\n* Fix host parsing for IPv6 proxies. (Issue #668)\n\n* Separate warning type SubjectAltNameWarning, now issued once\n  per host. (Issue #671)\n\n* Fix ``httplib.IncompleteRead`` not getting converted to\n  ``ProtocolError`` when using ``HTTPResponse.stream()``\n  (Issue #674)\n\n1.10.4 (2015-05-03)\n-------------------\n\n* Migrate tests to Tornado 4. (Issue #594)\n\n* Append default warning configuration rather than overwrite.\n  (Issue #603)\n\n* Fix streaming decoding regression. (Issue #595)\n\n* Fix chunked requests losing state across keep-alive connections.\n  (Issue #599)\n\n* Fix hanging when chunked HEAD response has no body. (Issue #605)\n\n\n1.10.3 (2015-04-21)\n-------------------\n\n* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.\n  (Issue #558)\n\n* Fix regression of duplicate header keys being discarded.\n  (Issue #563)\n\n* ``Response.stream()`` returns a generator for chunked responses.\n  (Issue #560)\n\n* Set upper-bound timeout when waiting for a socket in PyOpenSSL.\n  (Issue #585)\n\n* Work on platforms without `ssl` module for plain HTTP requests.\n  (Issue #587)\n\n* Stop relying on the stdlib's default cipher list. (Issue #588)\n\n\n1.10.2 (2015-02-25)\n-------------------\n\n* Fix file descriptor leakage on retries. (Issue #548)\n\n* Removed RC4 from default cipher list. (Issue #551)\n\n* Header performance improvements. (Issue #544)\n\n* Fix PoolManager not obeying redirect retry settings. (Issue #553)\n\n\n1.10.1 (2015-02-10)\n-------------------\n\n* Pools can be used as context managers. (Issue #545)\n\n* Don't re-use connections which experienced an SSLError. (Issue #529)\n\n* Don't fail when gzip decoding an empty stream. (Issue #535)\n\n* Add sha256 support for fingerprint verification. (Issue #540)\n\n* Fixed handling of header values containing commas. (Issue #533)\n\n\n1.10 (2014-12-14)\n-----------------\n\n* Disabled SSLv3. (Issue #473)\n\n* Add ``Url.url`` property to return the composed url string. (Issue #394)\n\n* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)\n\n* ``MaxRetryError.reason`` will always be an exception, not string.\n  (Issue #481)\n\n* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)\n\n* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)\n\n* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.\n  (Issue #496)\n\n* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.\n  (Issue #499)\n\n* Close and discard sockets which experienced SSL-related errors.\n  (Issue #501)\n\n* Handle ``body`` param in ``.request(...)``. (Issue #513)\n\n* Respect timeout with HTTPS proxy. (Issue #505)\n\n* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)\n\n\n1.9.1 (2014-09-13)\n------------------\n\n* Apply socket arguments before binding. (Issue #427)\n\n* More careful checks if fp-like object is closed. (Issue #435)\n\n* Fixed packaging issues of some development-related files not\n  getting included. (Issue #440)\n\n* Allow performing *only* fingerprint verification. (Issue #444)\n\n* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)\n\n* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)\n\n* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.\n  (Issue #443)\n\n\n\n1.9 (2014-07-04)\n----------------\n\n* Shuffled around development-related files. If you're maintaining a distro\n  package of urllib3, you may need to tweak things. (Issue #415)\n\n* Unverified HTTPS requests will trigger a warning on the first request. See\n  our new `security documentation\n  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.\n  (Issue #426)\n\n* New retry logic and ``urllib3.util.retry.Retry`` configuration object.\n  (Issue #326)\n\n* All raised exceptions should now wrapped in a\n  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)\n\n* All errors during a retry-enabled request should be wrapped in\n  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions\n  which were previously exempt. Underlying error is accessible from the\n  ``.reason`` property. (Issue #326)\n\n* ``urllib3.exceptions.ConnectionError`` renamed to\n  ``urllib3.exceptions.ProtocolError``. (Issue #326)\n\n* Errors during response read (such as IncompleteRead) are now wrapped in\n  ``urllib3.exceptions.ProtocolError``. (Issue #418)\n\n* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.\n  (Issue #417)\n\n* Catch read timeouts over SSL connections as\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)\n\n* Apply socket arguments before connecting. (Issue #427)\n\n\n1.8.3 (2014-06-23)\n------------------\n\n* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)\n\n* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)\n\n* Wrap ``socket.timeout`` exception with\n  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)\n\n* Fixed proxy-related bug where connections were being reused incorrectly.\n  (Issues #366, #369)\n\n* Added ``socket_options`` keyword parameter which allows to define\n  ``setsockopt`` configuration of new sockets. (Issue #397)\n\n* Removed ``HTTPConnection.tcp_nodelay`` in favor of\n  ``HTTPConnection.default_socket_options``. (Issue #397)\n\n* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)\n\n\n1.8.2 (2014-04-17)\n------------------\n\n* Fix ``urllib3.util`` not being included in the package.\n\n\n1.8.1 (2014-04-17)\n------------------\n\n* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)\n\n* Don't install ``dummyserver`` into ``site-packages`` as it's only needed\n  for the test suite. (Issue #362)\n\n* Added support for specifying ``source_address``. (Issue #352)\n\n\n1.8 (2014-03-04)\n----------------\n\n* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in\n  username, and blank ports like 'hostname:').\n\n* New ``urllib3.connection`` module which contains all the HTTPConnection\n  objects.\n\n* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor\n  signature to a more sensible order. [Backwards incompatible]\n  (Issues #252, #262, #263)\n\n* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)\n\n* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which\n  returns the number of bytes read so far. (Issue #277)\n\n* Support for platforms without threading. (Issue #289)\n\n* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``\n  to allow a pool with no specified port to be considered equal to to an\n  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)\n\n* Improved default SSL/TLS settings to avoid vulnerabilities.\n  (Issue #309)\n\n* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.\n  (Issue #310)\n\n* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests\n  will send the entire HTTP request ~200 milliseconds faster; however, some of\n  the resulting TCP packets will be smaller. (Issue #254)\n\n* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``\n  from the default 64 to 1024 in a single certificate. (Issue #318)\n\n* Headers are now passed and stored as a custom\n  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.\n  (Issue #329, #333)\n\n* Headers no longer lose their case on Python 3. (Issue #236)\n\n* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA\n  certificates on inject. (Issue #332)\n\n* Requests with ``retries=False`` will immediately raise any exceptions without\n  wrapping them in ``MaxRetryError``. (Issue #348)\n\n* Fixed open socket leak with SSL-related failures. (Issue #344, #348)\n\n\n1.7.1 (2013-09-25)\n------------------\n\n* Added granular timeout support with new ``urllib3.util.Timeout`` class.\n  (Issue #231)\n\n* Fixed Python 3.4 support. (Issue #238)\n\n\n1.7 (2013-08-14)\n----------------\n\n* More exceptions are now pickle-able, with tests. (Issue #174)\n\n* Fixed redirecting with relative URLs in Location header. (Issue #178)\n\n* Support for relative urls in ``Location: ...`` header. (Issue #179)\n\n* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus\n  file-like functionality. (Issue #187)\n\n* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will\n  skip hostname verification for SSL connections. (Issue #194)\n\n* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a\n  generator wrapped around ``.read(...)``. (Issue #198)\n\n* IPv6 url parsing enforces brackets around the hostname. (Issue #199)\n\n* Fixed thread race condition in\n  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)\n\n* ``ProxyManager`` requests now include non-default port in ``Host: ...``\n  header. (Issue #217)\n\n* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)\n\n* New ``RequestField`` object can be passed to the ``fields=...`` param which\n  can specify headers. (Issue #220)\n\n* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.\n  (Issue #221)\n\n* Use international headers when posting file names. (Issue #119)\n\n* Improved IPv6 support. (Issue #203)\n\n\n1.6 (2013-04-25)\n----------------\n\n* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)\n\n* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.\n\n* Improved SSL-related code. ``cert_req`` now optionally takes a string like\n  \"REQUIRED\" or \"NONE\". Same with ``ssl_version`` takes strings like \"SSLv23\"\n  The string values reflect the suffix of the respective constant variable.\n  (Issue #130)\n\n* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly\n  closed proxy connections and larger read buffers. (Issue #135)\n\n* Ensure the connection is closed if no data is received, fixes connection leak\n  on some platforms. (Issue #133)\n\n* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)\n\n* Tests fixed to be compatible with Py26 again. (Issue #125)\n\n* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant\n  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)\n\n* Allow an explicit content type to be specified when encoding file fields.\n  (Issue #126)\n\n* Exceptions are now pickleable, with tests. (Issue #101)\n\n* Fixed default headers not getting passed in some cases. (Issue #99)\n\n* Treat \"content-encoding\" header value as case-insensitive, per RFC 2616\n  Section 3.5. (Issue #110)\n\n* \"Connection Refused\" SocketErrors will get retried rather than raised.\n  (Issue #92)\n\n* Updated vendored ``six``, no longer overrides the global ``six`` module\n  namespace. (Issue #113)\n\n* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding\n  the exception that prompted the final retry. If ``reason is None`` then it\n  was due to a redirect. (Issue #92, #114)\n\n* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.\n  (Issue #149)\n\n* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters\n  that are not files. (Issue #111)\n\n* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)\n\n* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or\n  against an arbitrary hostname (when connecting by IP or for misconfigured\n  servers). (Issue #140)\n\n* Streaming decompression support. (Issue #159)\n\n\n1.5 (2012-08-02)\n----------------\n\n* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug\n  logging in urllib3.\n\n* Native full URL parsing (including auth, path, query, fragment) available in\n  ``urllib3.util.parse_url(url)``.\n\n* Built-in redirect will switch method to 'GET' if status code is 303.\n  (Issue #11)\n\n* ``urllib3.PoolManager`` strips the scheme and host before sending the request\n  uri. (Issue #8)\n\n* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,\n  based on the Content-Type header, fails.\n\n* Fixed bug with pool depletion and leaking connections (Issue #76). Added\n  explicit connection closing on pool eviction. Added\n  ``urllib3.PoolManager.clear()``.\n\n* 99% -> 100% unit test coverage.\n\n\n1.4 (2012-06-16)\n----------------\n\n* Minor AppEngine-related fixes.\n\n* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.\n\n* Improved url parsing. (Issue #73)\n\n* IPv6 url support. (Issue #72)\n\n\n1.3 (2012-03-25)\n----------------\n\n* Removed pre-1.0 deprecated API.\n\n* Refactored helpers into a ``urllib3.util`` submodule.\n\n* Fixed multipart encoding to support list-of-tuples for keys with multiple\n  values. (Issue #48)\n\n* Fixed multiple Set-Cookie headers in response not getting merged properly in\n  Python 3. (Issue #53)\n\n* AppEngine support with Py27. (Issue #61)\n\n* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs\n  bytes.\n\n\n1.2.2 (2012-02-06)\n------------------\n\n* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)\n\n\n1.2.1 (2012-02-05)\n------------------\n\n* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)\n\n* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``\n  which inherits from ``ValueError``.\n\n\n1.2 (2012-01-29)\n----------------\n\n* Added Python 3 support (tested on 3.2.2)\n\n* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)\n\n* Use ``select.poll`` instead of ``select.select`` for platforms that support\n  it.\n\n* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive\n  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.\n\n* Fixed ``ImportError`` during install when ``ssl`` module is not available.\n  (Issue #41)\n\n* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not\n  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)\n\n* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +\n  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.\n  Added socket-level tests.\n\n* More tests. Achievement Unlocked: 99% Coverage.\n\n\n1.1 (2012-01-07)\n----------------\n\n* Refactored ``dummyserver`` to its own root namespace module (used for\n  testing).\n\n* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in\n  Py32's ``ssl_match_hostname``. (Issue #25)\n\n* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)\n\n* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue\n  #27)\n\n* Fixed timeout-related bugs. (Issues #17, #23)\n\n\n1.0.2 (2011-11-04)\n------------------\n\n* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if\n  you're using the object manually. (Thanks pyos)\n\n* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by\n  wrapping the access log in a mutex. (Thanks @christer)\n\n* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and\n  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.\n\n\n1.0.1 (2011-10-10)\n------------------\n\n* Fixed a bug where the same connection would get returned into the pool twice,\n  causing extraneous \"HttpConnectionPool is full\" log warnings.\n\n\n1.0 (2011-10-08)\n----------------\n\n* Added ``PoolManager`` with LRU expiration of connections (tested and\n  documented).\n* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works\n  with HTTPS proxies).\n* Added optional partial-read support for responses when\n  ``preload_content=False``. You can now make requests and just read the headers\n  without loading the content.\n* Made response decoding optional (default on, same as before).\n* Added optional explicit boundary string for ``encode_multipart_formdata``.\n* Convenience request methods are now inherited from ``RequestMethods``. Old\n  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of\n  the new ``request(method, url, ...)``.\n* Refactored code to be even more decoupled, reusable, and extendable.\n* License header added to ``.py`` files.\n* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code\n  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.\n* Embettered all the things!\n* Started writing this file.\n\n\n0.4.1 (2011-07-17)\n------------------\n\n* Minor bug fixes, code cleanup.\n\n\n0.4 (2011-03-01)\n----------------\n\n* Better unicode support.\n* Added ``VerifiedHTTPSConnection``.\n* Added ``NTLMConnectionPool`` in contrib.\n* Minor improvements.\n\n\n0.3.1 (2010-07-13)\n------------------\n\n* Added ``assert_host_name`` optional parameter. Now compatible with proxies.\n\n\n0.3 (2009-12-10)\n----------------\n\n* Added HTTPS support.\n* Minor bug fixes.\n* Refactored, broken backwards compatibility with 0.2.\n* API to be treated as stable from this version forward.\n\n\n0.2 (2008-11-17)\n----------------\n\n* Added unit tests.\n* Bug fixes.\n\n\n0.1 (2008-11-16)\n----------------\n\n* First release.\n", "# Contributions to the urllib3 project\n\n## Creator & Maintainer\n\n* Andrey Petrov <andrey.petrov@shazow.net>\n\n\n## Contributors\n\nIn chronological order:\n\n* victor.vde <http://code.google.com/u/victor.vde/>\n  * HTTPS patch (which inspired HTTPSConnectionPool)\n\n* erikcederstrand <http://code.google.com/u/erikcederstrand/>\n  * NTLM-authenticated HTTPSConnectionPool\n  * Basic-authenticated HTTPSConnectionPool (merged into make_headers)\n\n* niphlod <niphlod@gmail.com>\n  * Client-verified SSL certificates for HTTPSConnectionPool\n  * Response gzip and deflate encoding support\n  * Better unicode support for filepost using StringIO buffers\n\n* btoconnor <brian@btoconnor.net>\n  * Non-multipart encoding for POST requests\n\n* p.dobrogost <http://code.google.com/u/@WBRSRlBZDhBFXQB6/>\n  * Code review, PEP8 compliance, benchmark fix\n\n* kennethreitz <me@kennethreitz.com>\n  * Bugfixes, suggestions, Requests integration\n\n* georgemarshall <https://github.com/georgemarshall>\n  * Bugfixes, Improvements and Test coverage\n\n* Thomas Kluyver <thomas@kluyver.me.uk>\n  * Python 3 support\n\n* brandon-rhodes <http://rhodesmill.org/brandon>\n  * Design review, bugfixes, test coverage.\n\n* studer <theo.studer@gmail.com>\n  * IPv6 url support and test coverage\n\n* Shivaram Lingamneni <slingamn@cs.stanford.edu>\n  * Support for explicitly closing pooled connections\n\n* hartator <hartator@gmail.com>\n  * Corrected multipart behavior for params\n\n* Thomas Wei\u00dfschuh <thomas@t-8ch.de>\n  * Support for TLS SNI\n  * API unification of ssl_version/cert_reqs\n  * SSL fingerprint and alternative hostname verification\n  * Bugfixes in testsuite\n\n* Sune Kirkeby <mig@ibofobi.dk>\n  * Optional SNI-support for Python 2 via PyOpenSSL.\n\n* Marc Schlaich <marc.schlaich@gmail.com>\n  * Various bugfixes and test improvements.\n\n* Bryce Boe <bbzbryce@gmail.com>\n  * Correct six.moves conflict\n  * Fixed pickle support of some exceptions\n\n* Boris Figovsky <boris.figovsky@ravellosystems.com>\n  * Allowed to skip SSL hostname verification\n\n* Cory Benfield <https://lukasa.co.uk/about/>\n  * Stream method for Response objects.\n  * Return native strings in header values.\n  * Generate 'Host' header when using proxies.\n\n* Jason Robinson <jaywink@basshero.org>\n  * Add missing WrappedSocket.fileno method in PyOpenSSL\n\n* Audrius Butkevicius <audrius.butkevicius@elastichosts.com>\n  * Fixed a race condition\n\n* Stanislav Vitkovskiy <stas.vitkovsky@gmail.com>\n  * Added HTTPS (CONNECT) proxy support\n\n* Stephen Holsapple <sholsapp@gmail.com>\n  * Added abstraction for granular control of request fields\n\n* Martin von Gagern <Martin.vGagern@gmx.net>\n  * Support for non-ASCII header parameters\n\n* Kevin Burke <kev@inburke.com> and Pavel Kirichenko <juanych@yandex-team.ru>\n  * Support for separate connect and request timeouts\n\n* Peter Waller <p@pwaller.net>\n  * HTTPResponse.tell() for determining amount received over the wire\n\n* Nipunn Koorapati <nipunn1313@gmail.com>\n  * Ignore default ports when comparing hosts for equality\n\n* Danilo @dbrgn <https://dbrgn.ch/>\n  * Disabled TLS compression by default on Python 3.2+\n  * Disabled TLS compression in pyopenssl contrib module\n  * Configurable cipher suites in pyopenssl contrib module\n\n* Roman Bogorodskiy <roman.bogorodskiy@ericsson.com>\n  * Account retries on proxy errors\n\n* Nicolas Delaby <nicolas.delaby@ezeep.com>\n  * Use the platform-specific CA certificate locations\n\n* Josh Schneier <https://github.com/jschneier>\n  * HTTPHeaderDict and associated tests and docs\n  * Bugfixes, docs, test coverage\n\n* Tahia Khan <http://tahia.tk/>\n  * Added Timeout examples in docs\n\n* Arthur Grunseid <https://grunseid.com>\n  * source_address support and tests (with https://github.com/bui)\n\n* Ian Cordasco <graffatcolmingov@gmail.com>\n  * PEP8 Compliance and Linting\n  * Add ability to pass socket options to an HTTP Connection\n\n* Erik Tollerud <erik.tollerud@gmail.com>\n  * Support for standard library io module.\n\n* Krishna Prasad <kprasad.iitd@gmail.com>\n  * Google App Engine documentation\n\n* Aaron Meurer <asmeurer@gmail.com>\n  * Added Url.url, which unparses a Url\n\n* Evgeny Kapun <abacabadabacaba@gmail.com>\n  * Bugfixes\n\n* Benjamen Meyer <bm_witness@yahoo.com>\n  * Security Warning Documentation update for proper capture\n\n* Shivan Sornarajah <github@sornars.com>\n  * Support for using ConnectionPool and PoolManager as context managers.\n\n* Alex Gaynor <alex.gaynor@gmail.com>\n  * Updates to the default SSL configuration\n\n* Tomas Tomecek <ttomecek@redhat.com>\n  * Implemented generator for getting chunks from chunked responses.\n\n* tlynn <https://github.com/tlynn>\n  * Respect the warning preferences at import.\n\n* David D. Riddle <ddriddle@illinois.edu>\n  * IPv6 bugfixes in testsuite\n\n* Thea Flowers <magicalgirl@google.com>\n  * App Engine environment tests.\n  * Documentation re-write.\n\n* John Krauss <https://github.com/talos>\n  * Clues to debugging problems with `cryptography` dependency in docs\n\n* Disassem <https://github.com/Disassem>\n  * Fix pool-default headers not applying for url-encoded requests like GET.\n\n* James Atherfold <jlatherfold@hotmail.com>\n  * Bugfixes relating to cleanup of connections during errors.\n\n* Christian Pedersen <https://github.com/chripede>\n  * IPv6 HTTPS proxy bugfix\n\n* Jordan Moldow <https://github.com/jmoldow>\n  * Fix low-level exceptions leaking from ``HTTPResponse.stream()``.\n  * Bugfix for ``ConnectionPool.urlopen(release_conn=False)``.\n  * Creation of ``HTTPConnectionPool.ResponseCls``.\n\n* Predrag Gruevski <https://github.com/obi1kenobi>\n  * Made cert digest comparison use a constant-time algorithm.\n\n* Adam Talsma <https://github.com/a-tal>\n  * Bugfix to ca_cert file paths.\n\n* Evan Meagher <https://evanmeagher.net>\n  * Bugfix related to `memoryview` usage in PyOpenSSL adapter\n\n* John Vandenberg <jayvdb@gmail.com>\n  * Python 2.6 fixes; pyflakes and pep8 compliance\n\n* Andy Caldwell <andy.m.caldwell@googlemail.com>\n  * Bugfix related to reusing connections in indeterminate states.\n\n* Ville Skytt\u00e4 <ville.skytta@iki.fi>\n  * Logging efficiency improvements, spelling fixes, Travis config.\n\n* Shige Takeda <smtakeda@gmail.com>\n  * Started Recipes documentation and added a recipe about handling concatenated gzip data in HTTP response\n\n* Jesse Shapiro <jesse@jesseshapiro.net>\n  * Various character-encoding fixes/tweaks\n  * Disabling IPv6 DNS when IPv6 connections not supported\n\n* David Foster <http://dafoster.net/>\n  * Ensure order of request and response headers are preserved.\n\n* Jeremy Cline <jeremy@jcline.org>\n  * Added connection pool keys by scheme\n\n* Aviv Palivoda <palaviv@gmail.com>\n  * History list to Retry object.\n  * HTTPResponse contains the last Retry object.\n\n* Nate Prewitt <nate.prewitt@gmail.com>\n  * Ensure timeouts are not booleans and greater than zero.\n  * Fixed infinite loop in ``stream`` when amt=None.\n  * Added length_remaining to determine remaining data to be read.\n  * Added enforce_content_length to raise exception when incorrect content-length received.\n\n* Seth Michael Larson <sethmichaellarson@protonmail.com>\n  * Created selectors backport that supports PEP 475.\n\n* Alexandre Dias <alex.dias@smarkets.com>\n  * Don't retry on timeout if method not in whitelist\n\n* Moinuddin Quadri <moin18@gmail.com>\n  * Lazily load idna package\n\n* Tom White <s6yg1ez3@mail2tor.com>\n  * Made SOCKS handler differentiate socks5h from socks5 and socks4a from socks4.\n\n* Tim Burke <tim.burke@gmail.com>\n  * Stop buffering entire deflate-encoded responses.\n\n* Tuukka Mustonen <tuukka.mustonen@gmail.com>\n  * Add counter for status_forcelist retries.\n\n* Erik Rose <erik@mozilla.com>\n  * Bugfix to pyopenssl vendoring\n\n* Wolfgang Richter <wolfgang.richter@gmail.com>\n  * Bugfix related to loading full certificate chains with PyOpenSSL backend.\n\n* Mike Miller <github@mikeage.net>\n  * Logging improvements to include the HTTP(S) port when opening a new connection\n\n* Ioannis Tziakos <mail@itziakos.gr>\n  * Fix ``util.selectors._fileobj_to_fd`` to accept ``long``.\n  * Update appveyor tox setup to use the 64bit python.\n\n* Akamai (through Jesse Shapiro) <jshapiro@akamai.com>\n  * Ongoing maintenance\n\n* Dominique Leuenberger <dimstar@opensuse.org>\n  * Minor fixes in the test suite\n\n* Will Bond <will@wbond.net>\n  * Add Python 2.6 support to ``contrib.securetransport``\n\n* Aleksei Alekseev <alekseev.yeskela@gmail.com>\n  * using auth info for socks proxy\n\n* Chris Wilcox <git@crwilcox.com>\n  * Improve contribution guide\n  * Add ``HTTPResponse.geturl`` method to provide ``urllib2.urlopen().geturl()`` behavior\n\n* Bruce Merry <https://www.brucemerry.org.za>\n  * Fix leaking exceptions when system calls are interrupted with zero timeout\n\n* Hugo van Kemenade <https://github.com/hugovk>\n  * Drop support for EOL Python 2.6\n\n* Tim Bell <https://github.com/timb07>\n  * Bugfix for responses with Content-Type: message/* logging warnings\n\n* Justin Bramley <https://github.com/jbramleycl>\n  * Add ability to handle multiple Content-Encodings\n\n* Katsuhiko YOSHIDA <https://github.com/kyoshidajp>\n  * Remove Authorization header regardless of case when redirecting to cross-site\n\n* [Your name or handle] <[email or website]>\n  * [Brief summary of your changes]\n", "from __future__ import absolute_import\nimport collections\nimport functools\nimport logging\n\nfrom ._collections import RecentlyUsedContainer\nfrom .connectionpool import HTTPConnectionPool, HTTPSConnectionPool\nfrom .connectionpool import port_by_scheme\nfrom .exceptions import LocationValueError, MaxRetryError, ProxySchemeUnknown\nfrom .packages import six\nfrom .packages.six.moves.urllib.parse import urljoin\nfrom .request import RequestMethods\nfrom .util.url import parse_url\nfrom .util.retry import Retry\n\n\n__all__ = ['PoolManager', 'ProxyManager', 'proxy_from_url']\n\n\nlog = logging.getLogger(__name__)\n\nSSL_KEYWORDS = ('key_file', 'cert_file', 'cert_reqs', 'ca_certs',\n                'ssl_version', 'ca_cert_dir', 'ssl_context')\n\n# All known keyword arguments that could be provided to the pool manager, its\n# pools, or the underlying connections. This is used to construct a pool key.\n_key_fields = (\n    'key_scheme',  # str\n    'key_host',  # str\n    'key_port',  # int\n    'key_timeout',  # int or float or Timeout\n    'key_retries',  # int or Retry\n    'key_strict',  # bool\n    'key_block',  # bool\n    'key_source_address',  # str\n    'key_key_file',  # str\n    'key_cert_file',  # str\n    'key_cert_reqs',  # str\n    'key_ca_certs',  # str\n    'key_ssl_version',  # str\n    'key_ca_cert_dir',  # str\n    'key_ssl_context',  # instance of ssl.SSLContext or urllib3.util.ssl_.SSLContext\n    'key_maxsize',  # int\n    'key_headers',  # dict\n    'key__proxy',  # parsed proxy url\n    'key__proxy_headers',  # dict\n    'key_socket_options',  # list of (level (int), optname (int), value (int or str)) tuples\n    'key__socks_options',  # dict\n    'key_assert_hostname',  # bool or string\n    'key_assert_fingerprint',  # str\n    'key_server_hostname',  # str\n)\n\n#: The namedtuple class used to construct keys for the connection pool.\n#: All custom key schemes should include the fields in this key at a minimum.\nPoolKey = collections.namedtuple('PoolKey', _key_fields)\n\n\ndef _default_key_normalizer(key_class, request_context):\n    \"\"\"\n    Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey\n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    return key_class(**context)\n\n\n#: A dictionary that maps a scheme to a callable that creates a pool key.\n#: This can be used to alter the way pool keys are constructed, if desired.\n#: Each PoolManager makes a copy of this dictionary so they can be configured\n#: globally here, or individually on the instance.\nkey_fn_by_scheme = {\n    'http': functools.partial(_default_key_normalizer, PoolKey),\n    'https': functools.partial(_default_key_normalizer, PoolKey),\n}\n\npool_classes_by_scheme = {\n    'http': HTTPConnectionPool,\n    'https': HTTPSConnectionPool,\n}\n\n\nclass PoolManager(RequestMethods):\n    \"\"\"\n    Allows for arbitrary requests while transparently keeping track of\n    necessary connection pools for you.\n\n    :param num_pools:\n        Number of connection pools to cache before discarding the least\n        recently used pool.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param \\\\**connection_pool_kw:\n        Additional parameters are used to create fresh\n        :class:`urllib3.connectionpool.ConnectionPool` instances.\n\n    Example::\n\n        >>> manager = PoolManager(num_pools=2)\n        >>> r = manager.request('GET', 'http://google.com/')\n        >>> r = manager.request('GET', 'http://google.com/mail')\n        >>> r = manager.request('GET', 'http://yahoo.com/')\n        >>> len(manager.pools)\n        2\n\n    \"\"\"\n\n    proxy = None\n\n    def __init__(self, num_pools=10, headers=None, **connection_pool_kw):\n        RequestMethods.__init__(self, headers)\n        self.connection_pool_kw = connection_pool_kw\n        self.pools = RecentlyUsedContainer(num_pools,\n                                           dispose_func=lambda p: p.close())\n\n        # Locally set the pool classes and keys so other PoolManagers can\n        # override them.\n        self.pool_classes_by_scheme = pool_classes_by_scheme\n        self.key_fn_by_scheme = key_fn_by_scheme.copy()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.clear()\n        # Return False to re-raise any potential exceptions\n        return False\n\n    def _new_pool(self, scheme, host, port, request_context=None):\n        \"\"\"\n        Create a new :class:`ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.\n        \"\"\"\n        pool_cls = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in ('scheme', 'host', 'port'):\n            request_context.pop(key, None)\n\n        if scheme == 'http':\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)\n\n    def clear(self):\n        \"\"\"\n        Empty our store of pools and direct them all to close.\n\n        This will not affect in-flight connections, but they will not be\n        re-used after completion.\n        \"\"\"\n        self.pools.clear()\n\n    def connection_from_host(self, host, port=None, scheme='http', pool_kwargs=None):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the host, port, and scheme.\n\n        If ``port`` isn't given, it will be derived from the ``scheme`` using\n        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is\n        provided, it is merged with the instance's ``connection_pool_kw``\n        variable and used to create the new connection pool, if one is\n        needed.\n        \"\"\"\n\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        request_context = self._merge_pool_kwargs(pool_kwargs)\n        request_context['scheme'] = scheme or 'http'\n        if not port:\n            port = port_by_scheme.get(request_context['scheme'].lower(), 80)\n        request_context['port'] = port\n        request_context['host'] = host\n\n        return self.connection_from_context(request_context)\n\n    def connection_from_context(self, request_context):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the request context.\n\n        ``request_context`` must at least contain the ``scheme`` key and its\n        value must be a key in ``key_fn_by_scheme`` instance variable.\n        \"\"\"\n        scheme = request_context['scheme'].lower()\n        pool_key_constructor = self.key_fn_by_scheme[scheme]\n        pool_key = pool_key_constructor(request_context)\n\n        return self.connection_from_pool_key(pool_key, request_context=request_context)\n\n    def connection_from_pool_key(self, pool_key, request_context=None):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the provided pool key.\n\n        ``pool_key`` should be a namedtuple that only contains immutable\n        objects. At a minimum it must have the ``scheme``, ``host``, and\n        ``port`` fields.\n        \"\"\"\n        with self.pools.lock:\n            # If the scheme, host, or port doesn't match existing open\n            # connections, open a new ConnectionPool.\n            pool = self.pools.get(pool_key)\n            if pool:\n                return pool\n\n            # Make a fresh ConnectionPool of the desired type\n            scheme = request_context['scheme']\n            host = request_context['host']\n            port = request_context['port']\n            pool = self._new_pool(scheme, host, port, request_context=request_context)\n            self.pools[pool_key] = pool\n\n        return pool\n\n    def connection_from_url(self, url, pool_kwargs=None):\n        \"\"\"\n        Similar to :func:`urllib3.connectionpool.connection_from_url`.\n\n        If ``pool_kwargs`` is not provided and a new pool needs to be\n        constructed, ``self.connection_pool_kw`` is used to initialize\n        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``\n        is provided, it is used instead. Note that if a new pool does not\n        need to be created for the request, the provided ``pool_kwargs`` are\n        not used.\n        \"\"\"\n        u = parse_url(url)\n        return self.connection_from_host(u.host, port=u.port, scheme=u.scheme,\n                                         pool_kwargs=pool_kwargs)\n\n    def _merge_pool_kwargs(self, override):\n        \"\"\"\n        Merge a dictionary of override values for self.connection_pool_kw.\n\n        This does not modify self.connection_pool_kw and returns a new dict.\n        Any keys in the override dictionary with a value of ``None`` are\n        removed from the merged dictionary.\n        \"\"\"\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs\n\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"\"\"\n        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`\n        with custom cross-host redirect logic and only sends the request-uri\n        portion of the ``url``.\n\n        The given ``url`` parameter must be absolute, such that an appropriate\n        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.\n        \"\"\"\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            headers = list(six.iterkeys(kw['headers']))\n            for header in headers:\n                if header.lower() in retries.remove_headers_on_redirect:\n                    kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)\n\n\nclass ProxyManager(PoolManager):\n    \"\"\"\n    Behaves just like :class:`PoolManager`, but sends all requests through\n    the defined proxy, using the CONNECT method for HTTPS URLs.\n\n    :param proxy_url:\n        The URL of the proxy to be used.\n\n    :param proxy_headers:\n        A dictionary containing headers that will be sent to the proxy. In case\n        of HTTP they are being sent with each request, while in the\n        HTTPS/CONNECT case they are sent only once. Could be used for proxy\n        authentication.\n\n    Example:\n        >>> proxy = urllib3.ProxyManager('http://localhost:3128/')\n        >>> r1 = proxy.request('GET', 'http://google.com/')\n        >>> r2 = proxy.request('GET', 'http://httpbin.org/')\n        >>> len(proxy.pools)\n        1\n        >>> r3 = proxy.request('GET', 'https://httpbin.org/')\n        >>> r4 = proxy.request('GET', 'https://twitter.com/')\n        >>> len(proxy.pools)\n        3\n\n    \"\"\"\n\n    def __init__(self, proxy_url, num_pools=10, headers=None,\n                 proxy_headers=None, **connection_pool_kw):\n\n        if isinstance(proxy_url, HTTPConnectionPool):\n            proxy_url = '%s://%s:%i' % (proxy_url.scheme, proxy_url.host,\n                                        proxy_url.port)\n        proxy = parse_url(proxy_url)\n        if not proxy.port:\n            port = port_by_scheme.get(proxy.scheme, 80)\n            proxy = proxy._replace(port=port)\n\n        if proxy.scheme not in (\"http\", \"https\"):\n            raise ProxySchemeUnknown(proxy.scheme)\n\n        self.proxy = proxy\n        self.proxy_headers = proxy_headers or {}\n\n        connection_pool_kw['_proxy'] = self.proxy\n        connection_pool_kw['_proxy_headers'] = self.proxy_headers\n\n        super(ProxyManager, self).__init__(\n            num_pools, headers, **connection_pool_kw)\n\n    def connection_from_host(self, host, port=None, scheme='http', pool_kwargs=None):\n        if scheme == \"https\":\n            return super(ProxyManager, self).connection_from_host(\n                host, port, scheme, pool_kwargs=pool_kwargs)\n\n        return super(ProxyManager, self).connection_from_host(\n            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs)\n\n    def _set_proxy_headers(self, url, headers=None):\n        \"\"\"\n        Sets headers needed by proxies: specifically, the Accept and Host\n        headers. Only sets headers not provided by the user.\n        \"\"\"\n        headers_ = {'Accept': '*/*'}\n\n        netloc = parse_url(url).netloc\n        if netloc:\n            headers_['Host'] = netloc\n\n        if headers:\n            headers_.update(headers)\n        return headers_\n\n    def urlopen(self, method, url, redirect=True, **kw):\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        u = parse_url(url)\n\n        if u.scheme == \"http\":\n            # For proxied HTTPS requests, httplib sets the necessary headers\n            # on the CONNECT to the proxy. For HTTP, we'll definitely\n            # need to set 'Host' at the very least.\n            headers = kw.get('headers', self.headers)\n            kw['headers'] = self._set_proxy_headers(url, headers)\n\n        return super(ProxyManager, self).urlopen(method, url, redirect=redirect, **kw)\n\n\ndef proxy_from_url(url, **kw):\n    return ProxyManager(proxy_url=url, **kw)\n", "from __future__ import absolute_import\nimport time\nimport logging\nfrom collections import namedtuple\nfrom itertools import takewhile\nimport email\nimport re\n\nfrom ..exceptions import (\n    ConnectTimeoutError,\n    MaxRetryError,\n    ProtocolError,\n    ReadTimeoutError,\n    ResponseError,\n    InvalidHeader,\n)\nfrom ..packages import six\n\n\nlog = logging.getLogger(__name__)\n\n\n# Data structure for representing the metadata of requests that result in a retry.\nRequestHistory = namedtuple('RequestHistory', [\"method\", \"url\", \"error\",\n                                               \"status\", \"redirect_location\"])\n\n\nclass Retry(object):\n    \"\"\" Retry configuration.\n\n    Each retry attempt will create a new Retry object with updated values, so\n    they can be safely reused.\n\n    Retries can be defined as a default for a pool::\n\n        retries = Retry(connect=5, read=2, redirect=5)\n        http = PoolManager(retries=retries)\n        response = http.request('GET', 'http://example.com/')\n\n    Or per-request (which overrides the default for the pool)::\n\n        response = http.request('GET', 'http://example.com/', retries=Retry(10))\n\n    Retries can be disabled by passing ``False``::\n\n        response = http.request('GET', 'http://example.com/', retries=False)\n\n    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless\n    retries are disabled, in which case the causing exception will be raised.\n\n    :param int total:\n        Total number of retries to allow. Takes precedence over other counts.\n\n        Set to ``None`` to remove this constraint and fall back on other\n        counts. It's a good idea to set this to some sensibly-high value to\n        account for unexpected edge cases and avoid infinite retry loops.\n\n        Set to ``0`` to fail on the first retry.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int connect:\n        How many connection-related errors to retry on.\n\n        These are errors raised before the request is sent to the remote server,\n        which we assume has not triggered the server to process the request.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int read:\n        How many times to retry on read errors.\n\n        These errors are raised after the request was sent to the server, so the\n        request may have side-effects.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param int redirect:\n        How many redirects to perform. Limit this to avoid infinite redirect\n        loops.\n\n        A redirect is a HTTP response with a status code 301, 302, 303, 307 or\n        308.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n        Set to ``False`` to disable and imply ``raise_on_redirect=False``.\n\n    :param int status:\n        How many times to retry on bad status codes.\n\n        These are retries made on responses, where status code matches\n        ``status_forcelist``.\n\n        Set to ``0`` to fail on the first retry of this type.\n\n    :param iterable method_whitelist:\n        Set of uppercased HTTP method verbs that we should retry on.\n\n        By default, we only retry on methods which are considered to be\n        idempotent (multiple requests with the same parameters end with the\n        same state). See :attr:`Retry.DEFAULT_METHOD_WHITELIST`.\n\n        Set to a ``False`` value to retry on any verb.\n\n    :param iterable status_forcelist:\n        A set of integer HTTP status codes that we should force a retry on.\n        A retry is initiated if the request method is in ``method_whitelist``\n        and the response status code is in ``status_forcelist``.\n\n        By default, this is disabled with ``None``.\n\n    :param float backoff_factor:\n        A backoff factor to apply between attempts after the second try\n        (most errors are resolved immediately by a second try without a\n        delay). urllib3 will sleep for::\n\n            {backoff factor} * (2 ** ({number of total retries} - 1))\n\n        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep\n        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer\n        than :attr:`Retry.BACKOFF_MAX`.\n\n        By default, backoff is disabled (set to 0).\n\n    :param bool raise_on_redirect: Whether, if the number of redirects is\n        exhausted, to raise a MaxRetryError, or to return a response with a\n        response code in the 3xx range.\n\n    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:\n        whether we should raise an exception, or return a response,\n        if status falls in ``status_forcelist`` range and retries have\n        been exhausted.\n\n    :param tuple history: The history of the request encountered during\n        each call to :meth:`~Retry.increment`. The list is in the order\n        the requests occurred. Each list item is of class :class:`RequestHistory`.\n\n    :param bool respect_retry_after_header:\n        Whether to respect Retry-After header on status codes defined as\n        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.\n\n    :param iterable remove_headers_on_redirect:\n        Sequence of headers to remove from the request when a response\n        indicating a redirect is returned before firing off the redirected\n        request.\n    \"\"\"\n\n    DEFAULT_METHOD_WHITELIST = frozenset([\n        'HEAD', 'GET', 'PUT', 'DELETE', 'OPTIONS', 'TRACE'])\n\n    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])\n\n    DEFAULT_REDIRECT_HEADERS_BLACKLIST = frozenset(['Authorization'])\n\n    #: Maximum backoff time.\n    BACKOFF_MAX = 120\n\n    def __init__(self, total=10, connect=None, read=None, redirect=None, status=None,\n                 method_whitelist=DEFAULT_METHOD_WHITELIST, status_forcelist=None,\n                 backoff_factor=0, raise_on_redirect=True, raise_on_status=True,\n                 history=None, respect_retry_after_header=True,\n                 remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST):\n\n        self.total = total\n        self.connect = connect\n        self.read = read\n        self.status = status\n\n        if redirect is False or total is False:\n            redirect = 0\n            raise_on_redirect = False\n\n        self.redirect = redirect\n        self.status_forcelist = status_forcelist or set()\n        self.method_whitelist = method_whitelist\n        self.backoff_factor = backoff_factor\n        self.raise_on_redirect = raise_on_redirect\n        self.raise_on_status = raise_on_status\n        self.history = history or tuple()\n        self.respect_retry_after_header = respect_retry_after_header\n        self.remove_headers_on_redirect = frozenset([\n            h.lower() for h in remove_headers_on_redirect])\n\n    def new(self, **kw):\n        params = dict(\n            total=self.total,\n            connect=self.connect, read=self.read, redirect=self.redirect, status=self.status,\n            method_whitelist=self.method_whitelist,\n            status_forcelist=self.status_forcelist,\n            backoff_factor=self.backoff_factor,\n            raise_on_redirect=self.raise_on_redirect,\n            raise_on_status=self.raise_on_status,\n            history=self.history,\n            remove_headers_on_redirect=self.remove_headers_on_redirect\n        )\n        params.update(kw)\n        return type(self)(**params)\n\n    @classmethod\n    def from_int(cls, retries, redirect=True, default=None):\n        \"\"\" Backwards-compatibility for the old retries format.\"\"\"\n        if retries is None:\n            retries = default if default is not None else cls.DEFAULT\n\n        if isinstance(retries, Retry):\n            return retries\n\n        redirect = bool(redirect) and None\n        new_retries = cls(retries, redirect=redirect)\n        log.debug(\"Converted retries value: %r -> %r\", retries, new_retries)\n        return new_retries\n\n    def get_backoff_time(self):\n        \"\"\" Formula for computing the current backoff\n\n        :rtype: float\n        \"\"\"\n        # We want to consider only the last consecutive errors sequence (Ignore redirects).\n        consecutive_errors_len = len(list(takewhile(lambda x: x.redirect_location is None,\n                                                    reversed(self.history))))\n        if consecutive_errors_len <= 1:\n            return 0\n\n        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))\n        return min(self.BACKOFF_MAX, backoff_value)\n\n    def parse_retry_after(self, retry_after):\n        # Whitespace: https://tools.ietf.org/html/rfc7230#section-3.2.4\n        if re.match(r\"^\\s*[0-9]+\\s*$\", retry_after):\n            seconds = int(retry_after)\n        else:\n            retry_date_tuple = email.utils.parsedate(retry_after)\n            if retry_date_tuple is None:\n                raise InvalidHeader(\"Invalid Retry-After header: %s\" % retry_after)\n            retry_date = time.mktime(retry_date_tuple)\n            seconds = retry_date - time.time()\n\n        if seconds < 0:\n            seconds = 0\n\n        return seconds\n\n    def get_retry_after(self, response):\n        \"\"\" Get the value of Retry-After in seconds. \"\"\"\n\n        retry_after = response.getheader(\"Retry-After\")\n\n        if retry_after is None:\n            return None\n\n        return self.parse_retry_after(retry_after)\n\n    def sleep_for_retry(self, response=None):\n        retry_after = self.get_retry_after(response)\n        if retry_after:\n            time.sleep(retry_after)\n            return True\n\n        return False\n\n    def _sleep_backoff(self):\n        backoff = self.get_backoff_time()\n        if backoff <= 0:\n            return\n        time.sleep(backoff)\n\n    def sleep(self, response=None):\n        \"\"\" Sleep between retry attempts.\n\n        This method will respect a server's ``Retry-After`` response header\n        and sleep the duration of the time requested. If that is not present, it\n        will use an exponential backoff. By default, the backoff factor is 0 and\n        this method will return immediately.\n        \"\"\"\n\n        if response:\n            slept = self.sleep_for_retry(response)\n            if slept:\n                return\n\n        self._sleep_backoff()\n\n    def _is_connection_error(self, err):\n        \"\"\" Errors when we're fairly sure that the server did not receive the\n        request, so it should be safe to retry.\n        \"\"\"\n        return isinstance(err, ConnectTimeoutError)\n\n    def _is_read_error(self, err):\n        \"\"\" Errors that occur after the request has been started, so we should\n        assume that the server began processing it.\n        \"\"\"\n        return isinstance(err, (ReadTimeoutError, ProtocolError))\n\n    def _is_method_retryable(self, method):\n        \"\"\" Checks if a given HTTP method should be retried upon, depending if\n        it is included on the method whitelist.\n        \"\"\"\n        if self.method_whitelist and method.upper() not in self.method_whitelist:\n            return False\n\n        return True\n\n    def is_retry(self, method, status_code, has_retry_after=False):\n        \"\"\" Is this method/status code retryable? (Based on whitelists and control\n        variables such as the number of total retries to allow, whether to\n        respect the Retry-After header, whether this header is present, and\n        whether the returned status code is on the list of status codes to\n        be retried upon on the presence of the aforementioned header)\n        \"\"\"\n        if not self._is_method_retryable(method):\n            return False\n\n        if self.status_forcelist and status_code in self.status_forcelist:\n            return True\n\n        return (self.total and self.respect_retry_after_header and\n                has_retry_after and (status_code in self.RETRY_AFTER_STATUS_CODES))\n\n    def is_exhausted(self):\n        \"\"\" Are we out of retries? \"\"\"\n        retry_counts = (self.total, self.connect, self.read, self.redirect, self.status)\n        retry_counts = list(filter(None, retry_counts))\n        if not retry_counts:\n            return False\n\n        return min(retry_counts) < 0\n\n    def increment(self, method=None, url=None, response=None, error=None,\n                  _pool=None, _stacktrace=None):\n        \"\"\" Return a new Retry object with incremented retry counters.\n\n        :param response: A response object, or None, if the server did not\n            return a response.\n        :type response: :class:`~urllib3.response.HTTPResponse`\n        :param Exception error: An error encountered during the request, or\n            None if the response was received successfully.\n\n        :return: A new ``Retry`` object.\n        \"\"\"\n        if self.total is False and error:\n            # Disabled, indicate to re-raise the error.\n            raise six.reraise(type(error), error, _stacktrace)\n\n        total = self.total\n        if total is not None:\n            total -= 1\n\n        connect = self.connect\n        read = self.read\n        redirect = self.redirect\n        status_count = self.status\n        cause = 'unknown'\n        status = None\n        redirect_location = None\n\n        if error and self._is_connection_error(error):\n            # Connect retry?\n            if connect is False:\n                raise six.reraise(type(error), error, _stacktrace)\n            elif connect is not None:\n                connect -= 1\n\n        elif error and self._is_read_error(error):\n            # Read retry?\n            if read is False or not self._is_method_retryable(method):\n                raise six.reraise(type(error), error, _stacktrace)\n            elif read is not None:\n                read -= 1\n\n        elif response and response.get_redirect_location():\n            # Redirect retry?\n            if redirect is not None:\n                redirect -= 1\n            cause = 'too many redirects'\n            redirect_location = response.get_redirect_location()\n            status = response.status\n\n        else:\n            # Incrementing because of a server error like a 500 in\n            # status_forcelist and a the given method is in the whitelist\n            cause = ResponseError.GENERIC_ERROR\n            if response and response.status:\n                if status_count is not None:\n                    status_count -= 1\n                cause = ResponseError.SPECIFIC_ERROR.format(\n                    status_code=response.status)\n                status = response.status\n\n        history = self.history + (RequestHistory(method, url, error, status, redirect_location),)\n\n        new_retry = self.new(\n            total=total,\n            connect=connect, read=read, redirect=redirect, status=status_count,\n            history=history)\n\n        if new_retry.is_exhausted():\n            raise MaxRetryError(_pool, url, error or ResponseError(cause))\n\n        log.debug(\"Incremented Retry for (url='%s'): %r\", url, new_retry)\n\n        return new_retry\n\n    def __repr__(self):\n        return ('{cls.__name__}(total={self.total}, connect={self.connect}, '\n                'read={self.read}, redirect={self.redirect}, status={self.status})').format(\n                    cls=type(self), self=self)\n\n\n# For backwards compatibility (equivalent to pre-v1.9):\nRetry.DEFAULT = Retry(3)\n", "import pytest\n\nfrom urllib3.response import HTTPResponse\nfrom urllib3.packages.six.moves import xrange\nfrom urllib3.util.retry import Retry, RequestHistory\nfrom urllib3.exceptions import (\n    ConnectTimeoutError,\n    MaxRetryError,\n    ReadTimeoutError,\n    ResponseError,\n)\n\n\nclass TestRetry(object):\n\n    def test_string(self):\n        \"\"\" Retry string representation looks the way we expect \"\"\"\n        retry = Retry()\n        assert str(retry) == 'Retry(total=10, connect=None, read=None, redirect=None, status=None)'\n        for _ in range(3):\n            retry = retry.increment(method='GET')\n        assert str(retry) == 'Retry(total=7, connect=None, read=None, redirect=None, status=None)'\n\n    def test_retry_both_specified(self):\n        \"\"\"Total can win if it's lower than the connect value\"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=3, total=2)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(error=error)\n        assert e.value.reason == error\n\n    def test_retry_higher_total_loses(self):\n        \"\"\" A lower connect timeout than the total is honored \"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=2, total=3)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(error=error)\n\n    def test_retry_higher_total_loses_vs_read(self):\n        \"\"\" A lower read timeout than the total is honored \"\"\"\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(read=2, total=3)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(method='GET', error=error)\n\n    def test_retry_total_none(self):\n        \"\"\" if Total is none, connect error should take precedence \"\"\"\n        error = ConnectTimeoutError()\n        retry = Retry(connect=2, total=None)\n        retry = retry.increment(error=error)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(error=error)\n        assert e.value.reason == error\n\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(connect=2, total=None)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        retry = retry.increment(method='GET', error=error)\n        assert not retry.is_exhausted()\n\n    def test_retry_default(self):\n        \"\"\" If no value is specified, should retry connects 3 times \"\"\"\n        retry = Retry()\n        assert retry.total == 10\n        assert retry.connect is None\n        assert retry.read is None\n        assert retry.redirect is None\n\n        error = ConnectTimeoutError()\n        retry = Retry(connect=1)\n        retry = retry.increment(error=error)\n        with pytest.raises(MaxRetryError):\n            retry.increment(error=error)\n\n        retry = Retry(connect=1)\n        retry = retry.increment(error=error)\n        assert not retry.is_exhausted()\n\n        assert Retry(0).raise_on_redirect\n        assert not Retry(False).raise_on_redirect\n\n    def test_retry_read_zero(self):\n        \"\"\" No second chances on read timeouts, by default \"\"\"\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry(read=0)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(method='GET', error=error)\n        assert e.value.reason == error\n\n    def test_status_counter(self):\n        resp = HTTPResponse(status=400)\n        retry = Retry(status=2)\n        retry = retry.increment(response=resp)\n        retry = retry.increment(response=resp)\n        with pytest.raises(MaxRetryError) as e:\n            retry.increment(response=resp)\n        assert str(e.value.reason) == ResponseError.SPECIFIC_ERROR.format(status_code=400)\n\n    def test_backoff(self):\n        \"\"\" Backoff is computed correctly \"\"\"\n        max_backoff = Retry.BACKOFF_MAX\n\n        retry = Retry(total=100, backoff_factor=0.2)\n        assert retry.get_backoff_time() == 0  # First request\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0  # First retry\n\n        retry = retry.increment(method='GET')\n        assert retry.backoff_factor == 0.2\n        assert retry.total == 98\n        assert retry.get_backoff_time() == 0.4  # Start backoff\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.8\n\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 1.6\n\n        for _ in xrange(10):\n            retry = retry.increment(method='GET')\n\n        assert retry.get_backoff_time() == max_backoff\n\n    def test_zero_backoff(self):\n        retry = Retry()\n        assert retry.get_backoff_time() == 0\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0\n\n    def test_backoff_reset_after_redirect(self):\n        retry = Retry(total=100, redirect=5, backoff_factor=0.2)\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.4\n        redirect_response = HTTPResponse(status=302, headers={'location': 'test'})\n        retry = retry.increment(method='GET', response=redirect_response)\n        assert retry.get_backoff_time() == 0\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        assert retry.get_backoff_time() == 0.4\n\n    def test_sleep(self):\n        # sleep a very small amount of time so our code coverage is happy\n        retry = Retry(backoff_factor=0.0001)\n        retry = retry.increment(method='GET')\n        retry = retry.increment(method='GET')\n        retry.sleep()\n\n    def test_status_forcelist(self):\n        retry = Retry(status_forcelist=xrange(500, 600))\n        assert not retry.is_retry('GET', status_code=200)\n        assert not retry.is_retry('GET', status_code=400)\n        assert retry.is_retry('GET', status_code=500)\n\n        retry = Retry(total=1, status_forcelist=[418])\n        assert not retry.is_retry('GET', status_code=400)\n        assert retry.is_retry('GET', status_code=418)\n\n        # String status codes are not matched.\n        retry = Retry(total=1, status_forcelist=['418'])\n        assert not retry.is_retry('GET', status_code=418)\n\n    def test_method_whitelist_with_status_forcelist(self):\n        # Falsey method_whitelist means to retry on any method.\n        retry = Retry(status_forcelist=[500], method_whitelist=None)\n        assert retry.is_retry('GET', status_code=500)\n        assert retry.is_retry('POST', status_code=500)\n\n        # Criteria of method_whitelist and status_forcelist are ANDed.\n        retry = Retry(status_forcelist=[500], method_whitelist=['POST'])\n        assert not retry.is_retry('GET', status_code=500)\n        assert retry.is_retry('POST', status_code=500)\n\n    def test_exhausted(self):\n        assert not Retry(0).is_exhausted()\n        assert Retry(-1).is_exhausted()\n        assert Retry(1).increment(method='GET').total == 0\n\n    @pytest.mark.parametrize('total', [-1, 0])\n    def test_disabled(self, total):\n        with pytest.raises(MaxRetryError):\n            Retry(total).increment(method='GET')\n\n    def test_error_message(self):\n        retry = Retry(total=0)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment(method='GET',\n                                    error=ReadTimeoutError(None, \"/\", \"read timed out\"))\n        assert 'Caused by redirect' not in str(e.value)\n        assert str(e.value.reason) == 'None: read timed out'\n\n        retry = Retry(total=1)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment('POST', '/')\n            retry = retry.increment('POST', '/')\n        assert 'Caused by redirect' not in str(e.value)\n        assert isinstance(e.value.reason, ResponseError)\n        assert str(e.value.reason) == ResponseError.GENERIC_ERROR\n\n        retry = Retry(total=1)\n        response = HTTPResponse(status=500)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment('POST', '/', response=response)\n            retry = retry.increment('POST', '/', response=response)\n        assert 'Caused by redirect' not in str(e.value)\n        msg = ResponseError.SPECIFIC_ERROR.format(status_code=500)\n        assert str(e.value.reason) == msg\n\n        retry = Retry(connect=1)\n        with pytest.raises(MaxRetryError) as e:\n            retry = retry.increment(error=ConnectTimeoutError('conntimeout'))\n            retry = retry.increment(error=ConnectTimeoutError('conntimeout'))\n        assert 'Caused by redirect' not in str(e.value)\n        assert str(e.value.reason) == 'conntimeout'\n\n    def test_history(self):\n        retry = Retry(total=10, method_whitelist=frozenset(['GET', 'POST']))\n        assert retry.history == tuple()\n        connection_error = ConnectTimeoutError('conntimeout')\n        retry = retry.increment('GET', '/test1', None, connection_error)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),)\n        assert retry.history == history\n\n        read_error = ReadTimeoutError(None, \"/test2\", \"read timed out\")\n        retry = retry.increment('POST', '/test2', None, read_error)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),\n                   RequestHistory('POST', '/test2', read_error, None, None))\n        assert retry.history == history\n\n        response = HTTPResponse(status=500)\n        retry = retry.increment('GET', '/test3', response, None)\n        history = (RequestHistory('GET', '/test1', connection_error, None, None),\n                   RequestHistory('POST', '/test2', read_error, None, None),\n                   RequestHistory('GET', '/test3', None, 500, None))\n        assert retry.history == history\n\n    def test_retry_method_not_in_whitelist(self):\n        error = ReadTimeoutError(None, \"/\", \"read timed out\")\n        retry = Retry()\n        with pytest.raises(ReadTimeoutError):\n            retry.increment(method='POST', error=error)\n\n    def test_retry_default_remove_headers_on_redirect(self):\n        retry = Retry()\n\n        assert list(retry.remove_headers_on_redirect) == ['authorization']\n\n    def test_retry_set_remove_headers_on_redirect(self):\n        retry = Retry(remove_headers_on_redirect=['X-API-Secret'])\n\n        assert list(retry.remove_headers_on_redirect) == ['x-api-secret']\n", "import unittest\nimport json\n\nimport pytest\n\nfrom dummyserver.server import HAS_IPV6\nfrom dummyserver.testcase import (HTTPDummyServerTestCase,\n                                  IPv6HTTPDummyServerTestCase)\nfrom urllib3.poolmanager import PoolManager\nfrom urllib3.connectionpool import port_by_scheme\nfrom urllib3.exceptions import MaxRetryError\nfrom urllib3.util.retry import Retry\n\n\nclass TestPoolManager(HTTPDummyServerTestCase):\n\n    def setUp(self):\n        self.base_url = 'http://%s:%d' % (self.host, self.port)\n        self.base_url_alt = 'http://%s:%d' % (self.host_alt, self.port)\n\n    def test_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/' % self.base_url},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/' % self.base_url})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_redirect_twice(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect' % self.base_url},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                       self.base_url)})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_redirect_to_relative_url(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '/redirect'},\n                         redirect=False)\n\n        self.assertEqual(r.status, 303)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '/redirect'})\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_cross_host_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        cross_host_location = '%s/echo?a=b' % self.base_url_alt\n        try:\n            http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': cross_host_location},\n                         timeout=1, retries=0)\n            self.fail(\"Request succeeded instead of raising an exception like it should.\")\n\n        except MaxRetryError:\n            pass\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/echo?a=b' % self.base_url_alt},\n                         timeout=1, retries=1)\n\n        self.assertEqual(r._pool.host, self.host_alt)\n\n    def test_too_many_redirects(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        try:\n            r = http.request('GET', '%s/redirect' % self.base_url,\n                             fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                           self.base_url)},\n                             retries=1)\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        try:\n            r = http.request('GET', '%s/redirect' % self.base_url,\n                             fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                           self.base_url)},\n                             retries=Retry(total=None, redirect=1))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n    def test_redirect_cross_host_remove_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'Authorization': 'foo'})\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('Authorization', data)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'authorization': 'foo'})\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('authorization', data)\n        self.assertNotIn('Authorization', data)\n\n    def test_redirect_cross_host_no_remove_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'Authorization': 'foo'},\n                         retries=Retry(remove_headers_on_redirect=[]))\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertEqual(data['Authorization'], 'foo')\n\n    def test_redirect_cross_host_set_removed_headers(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'X-API-Secret': 'foo',\n                                  'Authorization': 'bar'},\n                         retries=Retry(remove_headers_on_redirect=['X-API-Secret']))\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('X-API-Secret', data)\n        self.assertEqual(data['Authorization'], 'bar')\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/headers' % self.base_url_alt},\n                         headers={'x-api-secret': 'foo',\n                                  'authorization': 'bar'},\n                         retries=Retry(remove_headers_on_redirect=['X-API-Secret']))\n\n        self.assertEqual(r.status, 200)\n\n        data = json.loads(r.data.decode('utf-8'))\n\n        self.assertNotIn('x-api-secret', data)\n        self.assertNotIn('X-API-Secret', data)\n        self.assertEqual(data['Authorization'], 'bar')\n\n    def test_raise_on_redirect(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/redirect' % self.base_url,\n                         fields={'target': '%s/redirect?target=%s/' % (self.base_url,\n                                                                       self.base_url)},\n                         retries=Retry(total=None, redirect=1, raise_on_redirect=False))\n\n        self.assertEqual(r.status, 303)\n\n    def test_raise_on_status(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        try:\n            # the default is to raise\n            r = http.request('GET', '%s/status' % self.base_url,\n                             fields={'status': '500 Internal Server Error'},\n                             retries=Retry(total=1, status_forcelist=range(500, 600)))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        try:\n            # raise explicitly\n            r = http.request('GET', '%s/status' % self.base_url,\n                             fields={'status': '500 Internal Server Error'},\n                             retries=Retry(total=1,\n                                           status_forcelist=range(500, 600),\n                                           raise_on_status=True))\n            self.fail(\"Failed to raise MaxRetryError exception, returned %r\" % r.status)\n        except MaxRetryError:\n            pass\n\n        # don't raise\n        r = http.request('GET', '%s/status' % self.base_url,\n                         fields={'status': '500 Internal Server Error'},\n                         retries=Retry(total=1,\n                                       status_forcelist=range(500, 600),\n                                       raise_on_status=False))\n\n        self.assertEqual(r.status, 500)\n\n    def test_missing_port(self):\n        # Can a URL that lacks an explicit port like ':80' succeed, or\n        # will all such URLs fail with an error?\n\n        http = PoolManager()\n        self.addCleanup(http.clear)\n\n        # By globally adjusting `port_by_scheme` we pretend for a moment\n        # that HTTP's default port is not 80, but is the port at which\n        # our test server happens to be listening.\n        port_by_scheme['http'] = self.port\n        try:\n            r = http.request('GET', 'http://%s/' % self.host, retries=0)\n        finally:\n            port_by_scheme['http'] = 80\n\n        self.assertEqual(r.status, 200)\n        self.assertEqual(r.data, b'Dummy server!')\n\n    def test_headers(self):\n        http = PoolManager(headers={'Foo': 'bar'})\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request('POST', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_url('GET', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_body('POST', '%s/headers' % self.base_url)\n        returned_headers = json.loads(r.data.decode())\n        self.assertEqual(returned_headers.get('Foo'), 'bar')\n\n        r = http.request_encode_url('GET', '%s/headers' % self.base_url, headers={'Baz': 'quux'})\n        returned_headers = json.loads(r.data.decode())\n        self.assertIsNone(returned_headers.get('Foo'))\n        self.assertEqual(returned_headers.get('Baz'), 'quux')\n\n        r = http.request_encode_body('GET', '%s/headers' % self.base_url, headers={'Baz': 'quux'})\n        returned_headers = json.loads(r.data.decode())\n        self.assertIsNone(returned_headers.get('Foo'))\n        self.assertEqual(returned_headers.get('Baz'), 'quux')\n\n    def test_http_with_ssl_keywords(self):\n        http = PoolManager(ca_certs='REQUIRED')\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', 'http://%s:%s/' % (self.host, self.port))\n        self.assertEqual(r.status, 200)\n\n    def test_http_with_ca_cert_dir(self):\n        http = PoolManager(ca_certs='REQUIRED', ca_cert_dir='/nosuchdir')\n        self.addCleanup(http.clear)\n\n        r = http.request('GET', 'http://%s:%s/' % (self.host, self.port))\n        self.assertEqual(r.status, 200)\n\n\n@pytest.mark.skipif(\n    not HAS_IPV6,\n    reason='IPv6 is not supported on this system'\n)\nclass TestIPv6PoolManager(IPv6HTTPDummyServerTestCase):\n\n    def setUp(self):\n        self.base_url = 'http://[%s]:%d' % (self.host, self.port)\n\n    def test_ipv6(self):\n        http = PoolManager()\n        self.addCleanup(http.clear)\n        http.request('GET', self.base_url)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"], "filenames": ["CHANGES.rst", "CONTRIBUTORS.txt", "src/urllib3/poolmanager.py", "src/urllib3/util/retry.py", "test/test_retry.py", "test/with_dummyserver/test_poolmanager.py"], "buggy_code_start_loc": [9, 274, 9, 182, 256, 125], "buggy_code_end_loc": [9, 274, 347, 183, 262, 154], "fixing_code_start_loc": [10, 275, 10, 182, 256, 126], "fixing_code_end_loc": [12, 278, 350, 184, 262, 180], "type": "CWE-601", "message": "urllib3 before 1.24.2 does not remove the authorization HTTP header when following a cross-origin redirect (i.e., a redirect that differs in host, port, or scheme). This can allow for credentials in the authorization header to be exposed to unintended hosts or transmitted in cleartext. NOTE: this issue exists because of an incomplete fix for CVE-2018-20060 (which was case-sensitive).", "other": {"cve": {"id": "CVE-2018-25091", "sourceIdentifier": "cve@mitre.org", "published": "2023-10-15T19:15:09.213", "lastModified": "2023-10-19T14:01:05.073", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "urllib3 before 1.24.2 does not remove the authorization HTTP header when following a cross-origin redirect (i.e., a redirect that differs in host, port, or scheme). This can allow for credentials in the authorization header to be exposed to unintended hosts or transmitted in cleartext. NOTE: this issue exists because of an incomplete fix for CVE-2018-20060 (which was case-sensitive)."}, {"lang": "es", "value": "urllib3 anterior a 1.24.2 no elimina el encabezado HTTP de autorizaci\u00f3n cuando se sigue una redirecci\u00f3n de origen cruzado (es decir, una redirecci\u00f3n que difiere en host, puerto o esquema). Esto puede permitir que las credenciales en el encabezado de autorizaci\u00f3n se expongan a hosts no deseados o se transmitan en texto plano. NOTA: este problema existe debido a una soluci\u00f3n incompleta para CVE-2018-20060 (que distingu\u00eda entre may\u00fasculas y min\u00fasculas)."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 6.1, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 2.7}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-601"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:python:urllib3:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.24.2", "matchCriteriaId": "D0B5613A-F0A4-438A-A01E-4E2DAB4FAB8B"}]}]}], "references": [{"url": "https://github.com/urllib3/urllib3/commit/adb358f8e06865406d1f05e581a16cbea2136fbc", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/urllib3/urllib3/compare/1.24.1...1.24.2", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/urllib3/urllib3/issues/1510", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/urllib3/urllib3/commit/adb358f8e06865406d1f05e581a16cbea2136fbc"}}