{"buggy_code": ["import { AggregationOperator, Field, Relation, RootEntityType } from '../../model';\nimport { FieldSegment, RelationSegment } from '../../model/implementation/collect-path';\nimport { IDENTITY_ANALYZER } from '../../model/implementation/flex-search';\nimport {\n    AddEdgesQueryNode,\n    AggregationQueryNode,\n    BasicType,\n    BinaryOperationQueryNode,\n    BinaryOperator,\n    BinaryOperatorWithAnalyzer,\n    ConcatListsQueryNode,\n    ConditionalQueryNode,\n    ConfirmForBillingQueryNode,\n    ConstBoolQueryNode,\n    ConstIntQueryNode,\n    CountQueryNode,\n    CreateBillingEntityQueryNode,\n    CreateEntitiesQueryNode,\n    CreateEntityQueryNode,\n    DeleteEntitiesQueryNode,\n    DeleteEntitiesResultValue,\n    DynamicPropertyAccessQueryNode,\n    EdgeIdentifier,\n    EntitiesIdentifierKind,\n    EntitiesQueryNode,\n    EntityFromIdQueryNode,\n    FieldPathQueryNode,\n    FieldQueryNode,\n    FirstOfListQueryNode,\n    FollowEdgeQueryNode,\n    ListItemQueryNode,\n    ListQueryNode,\n    LiteralQueryNode,\n    MergeObjectsQueryNode,\n    NullQueryNode,\n    ObjectEntriesQueryNode,\n    ObjectQueryNode,\n    OperatorWithAnalyzerQueryNode,\n    OrderDirection,\n    OrderSpecification,\n    PartialEdgeIdentifier,\n    PropertyAccessQueryNode,\n    QueryNode,\n    QueryResultValidator,\n    RemoveEdgesQueryNode,\n    RevisionQueryNode,\n    RootEntityIDQueryNode,\n    RUNTIME_ERROR_CODE_PROPERTY,\n    RUNTIME_ERROR_TOKEN,\n    RuntimeErrorQueryNode,\n    SafeListQueryNode,\n    SetEdgeQueryNode,\n    SetFieldQueryNode,\n    TransformListQueryNode,\n    TraversalQueryNode,\n    TypeCheckQueryNode,\n    UnaryOperationQueryNode,\n    UnaryOperator,\n    UpdateEntitiesQueryNode,\n    VariableAssignmentQueryNode,\n    VariableQueryNode,\n    WithPreExecutionQueryNode\n} from '../../query-tree';\nimport {\n    FlexSearchComplexOperatorQueryNode,\n    FlexSearchFieldExistsQueryNode,\n    FlexSearchQueryNode,\n    FlexSearchStartsWithQueryNode\n} from '../../query-tree/flex-search';\nimport { Quantifier, QuantifierFilterNode } from '../../query-tree/quantifiers';\nimport { extractVariableAssignments, simplifyBooleans } from '../../query-tree/utils';\nimport { not } from '../../schema-generation/utils/input-types';\nimport { Constructor, decapitalize } from '../../utils/utils';\nimport { FlexSearchTokenizable } from '../database-adapter';\nimport { analyzeLikePatternPrefix } from '../like-helpers';\nimport { aql, AQLCompoundQuery, AQLFragment, AQLQueryResultVariable, AQLVariable } from './aql';\nimport { billingCollectionName, getCollectionNameForRelation, getCollectionNameForRootEntity } from './arango-basics';\nimport { getFlexSearchViewNameForRootEntity } from './schema-migration/arango-search-helpers';\n\nenum AccessType {\n    READ,\n    WRITE\n}\n\nclass QueryContext {\n    private variableMap = new Map<VariableQueryNode, AQLFragment>();\n    private preExecQueries: AQLCompoundQuery[] = [];\n    private readAccessedCollections = new Set<string>();\n    private writeAccessedCollections = new Set<string>();\n    private extensions: Map<unknown, unknown> | undefined;\n\n    /**\n     * Creates a new QueryContext with an independent variable map except that all query result variables of this\n     * context are available.\n     */\n    private newPreExecContext(): QueryContext {\n        const newContext = new QueryContext();\n        this.variableMap.forEach((aqlVar, varNode) => {\n            if (aqlVar instanceof AQLQueryResultVariable) {\n                newContext.variableMap.set(varNode, aqlVar);\n            }\n        });\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        return newContext;\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     * @param variableNode the variable token as it is referenced in the query tree\n     * @param aqlVariable the variable token as it will be available within the AQL fragment\n     */\n    private newNestedContextWithVariableMapping(\n        variableNode: VariableQueryNode,\n        aqlVariable: AQLFragment\n    ): QueryContext {\n        const newContext = new QueryContext();\n        newContext.variableMap = new Map(this.variableMap);\n        newContext.variableMap.set(variableNode, aqlVariable);\n        newContext.preExecQueries = this.preExecQueries;\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        return newContext;\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     *\n     * The AQLFragment for the variable will be available via getVariable().\n     *\n     * @param {VariableQueryNode} variableNode the variable as referenced in the query tree\n     * @returns {QueryContext} the nested context\n     */\n    introduceVariable(variableNode: VariableQueryNode): QueryContext {\n        if (this.variableMap.has(variableNode)) {\n            throw new Error(`Variable ${variableNode} is introduced twice`);\n        }\n        const variable = new AQLVariable(variableNode.label);\n        return this.newNestedContextWithVariableMapping(variableNode, variable);\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     *\n     * @param variableNode the variable as referenced in the query tree\n     * @param existingVariable a variable that has been previously introduced with introduceVariable() and fetched by getVariable\n     * @returns {QueryContext} the nested context\n     */\n    introduceVariableAlias(variableNode: VariableQueryNode, existingVariable: AQLFragment): QueryContext {\n        return this.newNestedContextWithVariableMapping(variableNode, existingVariable);\n    }\n\n    /**\n     * Creates a new QueryContext that includes an additional transaction step and adds resultVariable to the scope\n     * which will contain the result of the query\n     *\n     * The preExecQuery is evaluated in an independent context that has access to all previous preExecQuery result\n     * variables.\n     *\n     * @param preExecQuery the query to execute as transaction step\n     * @param resultVariable the variable to store the query result\n     * @param resultValidator an optional validator for the query result\n     */\n    addPreExecuteQuery(\n        preExecQuery: QueryNode,\n        resultVariable?: VariableQueryNode,\n        resultValidator?: QueryResultValidator\n    ): QueryContext {\n        let resultVar: AQLQueryResultVariable | undefined;\n        let newContext: QueryContext;\n        if (resultVariable) {\n            resultVar = new AQLQueryResultVariable(resultVariable.label);\n            newContext = this.newNestedContextWithVariableMapping(resultVariable, resultVar);\n        } else {\n            resultVar = undefined;\n            newContext = this;\n        }\n\n        const aqlQuery = createAQLCompoundQuery(preExecQuery, resultVar, resultValidator, this.newPreExecContext());\n\n        this.preExecQueries.push(aqlQuery);\n        return newContext;\n    }\n\n    /**\n     * Adds the information (in-place) that a collection is accessed\n     */\n    addCollectionAccess(collection: string, accessType: AccessType): void {\n        switch (accessType) {\n            case AccessType.READ:\n                this.readAccessedCollections.add(collection);\n                break;\n            case AccessType.WRITE:\n                this.writeAccessedCollections.add(collection);\n                break;\n        }\n    }\n\n    withExtension(key: unknown, value: unknown): QueryContext {\n        const newContext = new QueryContext();\n        newContext.variableMap = this.variableMap;\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        newContext.extensions = new Map([...(this.extensions ? this.extensions.entries() : []), [key, value]]);\n        return newContext;\n    }\n\n    getExtension(key: unknown): unknown {\n        if (!this.extensions) {\n            return undefined;\n        }\n        return this.extensions.get(key);\n    }\n\n    /**\n     * Gets an AQLFragment that evaluates to the value of a variable in the current scope\n     */\n    getVariable(variableNode: VariableQueryNode): AQLVariable {\n        const variable = this.variableMap.get(variableNode);\n        if (!variable) {\n            throw new Error(`Variable ${variableNode.toString()} is used but not introduced`);\n        }\n        return variable;\n    }\n\n    getPreExecuteQueries(): AQLCompoundQuery[] {\n        return this.preExecQueries;\n    }\n\n    getReadAccessedCollections(): string[] {\n        return Array.from(this.readAccessedCollections);\n    }\n\n    getWriteAccessedCollections(): string[] {\n        return Array.from(this.writeAccessedCollections);\n    }\n}\n\nfunction createAQLCompoundQuery(\n    node: QueryNode,\n    resultVariable: AQLQueryResultVariable | undefined,\n    resultValidator: QueryResultValidator | undefined,\n    context: QueryContext\n): AQLCompoundQuery {\n    // move LET statements up\n    // they often occur for value objects / entity extensions\n    // this avoids the FIRST() and the subquery which reduces load on the AQL query optimizer\n    let variableAssignments: AQLFragment[] = [];\n    const variableAssignmentNodes: VariableAssignmentQueryNode[] = [];\n    node = extractVariableAssignments(node, variableAssignmentNodes);\n    for (const assignmentNode of variableAssignmentNodes) {\n        context = context.introduceVariable(assignmentNode.variableNode);\n        const tmpVar = context.getVariable(assignmentNode.variableNode);\n        variableAssignments.push(aql`LET ${tmpVar} = ${processNode(assignmentNode.variableValueNode, context)}`);\n    }\n\n    const aqlQuery = aql.lines(...variableAssignments, aql`RETURN ${processNode(node, context)}`);\n    const preExecQueries = context.getPreExecuteQueries();\n    const readAccessedCollections = context.getReadAccessedCollections();\n    const writeAccessedCollections = context.getWriteAccessedCollections();\n\n    return new AQLCompoundQuery(\n        preExecQueries,\n        aqlQuery,\n        resultVariable,\n        resultValidator,\n        readAccessedCollections,\n        writeAccessedCollections\n    );\n}\n\ntype NodeProcessor<T extends QueryNode> = (node: T, context: QueryContext) => AQLFragment;\n\nconst inFlexSearchFilterSymbol = Symbol('inFlexSearchFilter');\n\nnamespace aqlExt {\n    export function safeJSONKey(key: string): AQLFragment {\n        if (aql.isSafeIdentifier(key)) {\n            // we could always collide with a (future) keyword, so use \"name\" syntax instead of identifier\n            // (\"\" looks more natural than `` in json keys)\n            return aql`${aql.string(key)}`;\n        } else {\n            return aql`${key}`; // fall back to bound values\n        }\n    }\n\n    export function parenthesizeList(...content: AQLFragment[]): AQLFragment {\n        return aql.lines(aql`(`, aql.indent(aql.lines(...content)), aql`)`);\n    }\n\n    export function parenthesizeObject(...content: AQLFragment[]): AQLFragment {\n        return aql`FIRST${parenthesizeList(...content)}`;\n    }\n}\n\nconst processors = new Map<Constructor<QueryNode>, NodeProcessor<QueryNode>>();\n\nfunction register<T extends QueryNode>(type: Constructor<T>, processor: NodeProcessor<T>) {\n    processors.set(type, processor as NodeProcessor<QueryNode>); // probably some bivariancy issue\n}\n\nregister(LiteralQueryNode, node => {\n    return aql.value(node.value);\n});\n\nregister(NullQueryNode, () => {\n    return aql`null`;\n});\n\nregister(RuntimeErrorQueryNode, node => {\n    const runtimeErrorToken = aql.code(RUNTIME_ERROR_TOKEN);\n    if (node.code) {\n        const codeProp = aql.code(RUNTIME_ERROR_CODE_PROPERTY);\n        return aql`{ ${codeProp}: ${node.code}, ${runtimeErrorToken}: ${node.message} }`;\n    }\n    return aql`{ ${runtimeErrorToken}: ${node.message} }`;\n});\n\nregister(ConstBoolQueryNode, node => {\n    return node.value ? aql`true` : aql`false`;\n});\n\nregister(ConstIntQueryNode, node => {\n    return aql.integer(node.value);\n});\n\nregister(ObjectQueryNode, (node, context) => {\n    if (!node.properties.length) {\n        return aql`{}`;\n    }\n\n    const properties = node.properties.map(\n        p => aql`${aqlExt.safeJSONKey(p.propertyName)}: ${processNode(p.valueNode, context)}`\n    );\n    return aql.lines(aql`{`, aql.indent(aql.join(properties, aql`,\\n`)), aql`}`);\n});\n\nregister(ListQueryNode, (node, context) => {\n    if (!node.itemNodes.length) {\n        return aql`[]`;\n    }\n\n    return aql.lines(\n        aql`[`,\n        aql.indent(\n            aql.join(\n                node.itemNodes.map(itemNode => processNode(itemNode, context)),\n                aql`,\\n`\n            )\n        ),\n        aql`]`\n    );\n});\n\nregister(ConcatListsQueryNode, (node, context) => {\n    const listNodes = node.listNodes.map(node => processNode(node, context));\n    const listNodeStr = aql.join(listNodes, aql`, `);\n    // note: UNION just appends, there is a special UNION_DISTINCT to filter out duplicates\n    return aql`UNION(${listNodeStr})`;\n});\n\nregister(VariableQueryNode, (node, context) => {\n    return context.getVariable(node);\n});\n\nregister(VariableAssignmentQueryNode, (node, context) => {\n    const newContext = context.introduceVariable(node.variableNode);\n    const tmpVar = newContext.getVariable(node.variableNode);\n\n    // note that we have to know statically if the context var is a list or an object\n    // assuming object here because lists are not needed currently\n    return aqlExt.parenthesizeObject(\n        aql`LET ${tmpVar} = ${processNode(node.variableValueNode, newContext)}`,\n        aql`RETURN ${processNode(node.resultNode, newContext)}`\n    );\n});\n\nregister(WithPreExecutionQueryNode, (node, context) => {\n    let currentContext = context;\n    for (const preExecParm of node.preExecQueries) {\n        currentContext = currentContext.addPreExecuteQuery(\n            preExecParm.query,\n            preExecParm.resultVariable,\n            preExecParm.resultValidator\n        );\n    }\n\n    return aql`${processNode(node.resultNode, currentContext)}`;\n});\n\nregister(EntityFromIdQueryNode, (node, context) => {\n    const collection = getCollectionForType(node.rootEntityType, AccessType.READ, context);\n    return aql`DOCUMENT(${collection}, ${processNode(node.idNode, context)})`;\n});\n\nregister(PropertyAccessQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getPropertyAccessFragment(node.propertyName)}`;\n});\n\nregister(FieldQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getPropertyAccessFragment(node.field.name)}`;\n});\n\nregister(DynamicPropertyAccessQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}[${processNode(node.propertyNode, context)}]`;\n});\n\nregister(FieldPathQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getFieldPathAccessFragment(node.path)}`;\n});\n\nfunction getPropertyAccessFragment(propertyName: string) {\n    if (aql.isSafeIdentifier(propertyName)) {\n        return aql`.${aql.identifier(propertyName)}`;\n    }\n    // fall back to bound values. do not attempt aql.string for security reasons - should not be the case normally, anyway.\n    return aql`[${propertyName}]`;\n}\n\nfunction getFieldPathAccessFragment(path: ReadonlyArray<Field>): AQLFragment {\n    if (path.length > 0) {\n        const [head, ...tail] = path;\n        return aql`${getPropertyAccessFragment(head.name)}${getFieldPathAccessFragment(tail)}`;\n    } else {\n        return aql``;\n    }\n}\n\nregister(RootEntityIDQueryNode, (node, context) => {\n    return aql`${processNode(node.objectNode, context)}._key`; // ids are stored in _key field\n});\n\nregister(RevisionQueryNode, (node, context) => {\n    return aql`${processNode(node.objectNode, context)}._rev`;\n});\n\nregister(FlexSearchQueryNode, (node, context) => {\n    let itemContext = context.introduceVariable(node.itemVariable).withExtension(inFlexSearchFilterSymbol, true);\n    const viewName = getFlexSearchViewNameForRootEntity(node.rootEntityType!);\n    context.addCollectionAccess(viewName, AccessType.READ);\n    return aqlExt.parenthesizeList(\n        aql`FOR ${itemContext.getVariable(node.itemVariable)}`,\n        aql`IN ${aql.collection(viewName)}`,\n        aql`SEARCH ${processNode(node.flexFilterNode, itemContext)}`,\n        node.isOptimisationsDisabled ? aql`OPTIONS { conditionOptimization: 'none' }` : aql``,\n        aql`RETURN ${itemContext.getVariable(node.itemVariable)}`\n    );\n});\n\nregister(TransformListQueryNode, (node, context) => {\n    let itemContext = context.introduceVariable(node.itemVariable);\n    const itemVar = itemContext.getVariable(node.itemVariable);\n    let itemProjectionContext = itemContext;\n\n    // move LET statements up\n    // they often occur for value objects / entity extensions\n    // this avoids the FIRST() and the subquery which reduces load on the AQL query optimizer\n    let variableAssignments: AQLFragment[] = [];\n    let innerNode = node.innerNode;\n    const variableAssignmentNodes: VariableAssignmentQueryNode[] = [];\n    innerNode = extractVariableAssignments(innerNode, variableAssignmentNodes);\n    for (const assignmentNode of variableAssignmentNodes) {\n        itemProjectionContext = itemProjectionContext.introduceVariable(assignmentNode.variableNode);\n        const tmpVar = itemProjectionContext.getVariable(assignmentNode.variableNode);\n        variableAssignments.push(\n            aql`LET ${tmpVar} = ${processNode(assignmentNode.variableValueNode, itemProjectionContext)}`\n        );\n    }\n\n    return aqlExt.parenthesizeList(\n        aql`FOR ${itemVar}`,\n        generateInClauseWithFilterAndOrderAndLimit({ node, context, itemContext, itemVar }),\n        ...variableAssignments,\n        aql`RETURN ${processNode(innerNode, itemProjectionContext)}`\n    );\n});\n\n/**\n * Generates an IN... clause for a TransformListQueryNode to be used within a query / subquery (FOR ... IN ...)\n */\nfunction generateInClauseWithFilterAndOrderAndLimit({\n    node,\n    context,\n    itemVar,\n    itemContext\n}: {\n    node: TransformListQueryNode;\n    context: QueryContext;\n    itemVar: AQLVariable;\n    itemContext: QueryContext;\n}) {\n    let list: AQLFragment;\n    let filterDanglingEdges = aql``;\n    if (node.listNode instanceof FollowEdgeQueryNode) {\n        list = getSimpleFollowEdgeFragment(node.listNode, context);\n        filterDanglingEdges = aql`FILTER ${itemVar} != null`;\n    } else {\n        list = processNode(node.listNode, context);\n    }\n    let filter = simplifyBooleans(node.filterNode);\n\n    let limitClause;\n    if (node.maxCount != undefined) {\n        if (node.skip === 0) {\n            limitClause = aql`LIMIT ${node.maxCount}`;\n        } else {\n            limitClause = aql`LIMIT ${node.skip}, ${node.maxCount}`;\n        }\n    } else if (node.skip > 0) {\n        limitClause = aql`LIMIT ${node.skip}, ${Number.MAX_SAFE_INTEGER}`;\n    } else {\n        limitClause = aql``;\n    }\n\n    return aql.lines(\n        aql`IN ${list}`,\n        filter instanceof ConstBoolQueryNode && filter.value ? aql`` : aql`FILTER ${processNode(filter, itemContext)}`,\n        filterDanglingEdges,\n        generateSortAQL(node.orderBy, itemContext),\n        limitClause\n    );\n}\n\n/**\n * Generates an IN... clause for a list to be used within a query / subquery (FOR ... IN ...)\n */\nfunction generateInClause(node: QueryNode, context: QueryContext, entityVar: AQLFragment) {\n    if (node instanceof TransformListQueryNode && node.innerNode === node.itemVariable) {\n        const itemContext = context.introduceVariableAlias(node.itemVariable, entityVar);\n        return generateInClauseWithFilterAndOrderAndLimit({ node, itemContext, itemVar: entityVar, context });\n    }\n\n    return aql`IN ${processNode(node, context)}`;\n}\n\nregister(CountQueryNode, (node, context) => {\n    if (node.listNode instanceof FieldQueryNode || node.listNode instanceof EntitiesQueryNode) {\n        // These cases are known to be optimized\n        // TODO this does not catch the safe-list case (list ? list : []), where we could optimize to (list ? LENGTH(list) : 0)\n        // so we probably need to add an optimization to the query tree builder\n        return aql`LENGTH(${processNode(node.listNode, context)})`;\n    }\n\n    // in the general case (mostly a TransformListQueryNode), it is better to use the COLLeCT WITH COUNT syntax\n    // because it avoids building the whole collection temporarily in memory\n    // however, https://docs.arangodb.com/3.2/AQL/Examples/Counting.html does not really mention this case, so we\n    // should evaluate it again\n    // note that ArangoDB's inline-subqueries rule optimizes for the case where listNode is a TransformList again.\n    const itemVar = aql.variable('item');\n    const countVar = aql.variable('count');\n    return aqlExt.parenthesizeObject(\n        aql`FOR ${itemVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        aql`COLLECT WITH COUNT INTO ${countVar}`,\n        aql`RETURN ${countVar}`\n    );\n});\n\nregister(AggregationQueryNode, (node, context) => {\n    const itemVar = aql.variable('item');\n    const aggregationVar = aql.variable(node.operator.toLowerCase());\n    let aggregationFunction: AQLFragment | undefined;\n    let filterFrag: AQLFragment | undefined;\n    let itemFrag = itemVar;\n    let resultFragment = aggregationVar;\n    let isList = false;\n    let distinct = false;\n    let sort = false;\n    switch (node.operator) {\n        case AggregationOperator.MIN:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`MIN`;\n            break;\n        case AggregationOperator.MAX:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`MAX`;\n            break;\n        case AggregationOperator.SUM:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`SUM`;\n            resultFragment = aql`${resultFragment} != null ? ${resultFragment} : 0`; // SUM([]) === 0\n            break;\n        case AggregationOperator.AVERAGE:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`AVERAGE`;\n            break;\n\n        case AggregationOperator.COUNT:\n            aggregationFunction = aql`COUNT`;\n            break;\n        case AggregationOperator.SOME:\n            aggregationFunction = aql`COUNT`;\n            resultFragment = aql`${resultFragment} > 0`;\n            break;\n        case AggregationOperator.NONE:\n            aggregationFunction = aql`COUNT`;\n            resultFragment = aql`${resultFragment} == 0`;\n            break;\n\n        // using MAX >= true in place of SOME\n        //   and MAX <  true in place of NONE\n        // (basically, MAX is similar to SOME, and NONE is !SOME. Can't use MIN for EVERY because MIN([]) = null.)\n        case AggregationOperator.SOME_NULL:\n            itemFrag = aql`${itemFrag} == null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.SOME_NOT_NULL:\n            itemFrag = aql`${itemFrag} != null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.NONE_NULL:\n            itemFrag = aql`${itemFrag} == null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.EVERY_NULL:\n            // -> NONE_NOT_NULL\n            itemFrag = aql`${itemFrag} != null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.COUNT_NULL:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} == null`;\n            break;\n        case AggregationOperator.COUNT_NOT_NULL:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} != null`;\n            break;\n\n        // these treat NULL like FALSE, so don't filter them away\n        // using MAX >= true in place of SOME\n        //   and MAX <  true in place of NONE\n        // (basically, MAX is similar to SOME, and NONE is !SOME. Can't use MIN for EVERY because MIN([]) = null.)\n        case AggregationOperator.SOME_TRUE:\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.SOME_NOT_TRUE:\n            itemFrag = aql`!${itemFrag}`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.EVERY_TRUE:\n            // -> NONE_NOT_TRUE\n            itemFrag = aql`!${itemFrag}`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.NONE_TRUE:\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.COUNT_TRUE:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} >= true`;\n            break;\n        case AggregationOperator.COUNT_NOT_TRUE:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} < true`;\n            break;\n\n        // these should also remove NULL values by definition\n        case AggregationOperator.DISTINCT:\n            // use COLLECT a = a instead of RETURN DISTINCT to be able to sort\n            distinct = true;\n            filterFrag = aql`${itemVar} != null`;\n            isList = true;\n            sort = node.sort;\n            break;\n\n        case AggregationOperator.COUNT_DISTINCT:\n            aggregationFunction = aql`COUNT_DISTINCT`;\n            filterFrag = aql`${itemVar} != null`;\n            break;\n\n        default:\n            throw new Error(`Unsupported aggregator: ${(node as any).aggregationOperator}`);\n    }\n    return aqlExt[isList ? 'parenthesizeList' : 'parenthesizeObject'](\n        aql`FOR ${itemVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        filterFrag ? aql`FILTER ${filterFrag}` : aql``,\n        sort ? aql`SORT ${itemVar}` : aql``,\n        aggregationFunction\n            ? aql`COLLECT AGGREGATE ${aggregationVar} = ${aggregationFunction}(${itemFrag})`\n            : distinct\n            ? aql`COLLECT ${aggregationVar} = ${itemFrag}`\n            : aql``,\n        aql`RETURN ${resultFragment}`\n    );\n});\n\nregister(MergeObjectsQueryNode, (node, context) => {\n    const objectList = node.objectNodes.map(node => processNode(node, context));\n    const objectsFragment = aql.join(objectList, aql`, `);\n    return aql`MERGE(${objectsFragment})`;\n});\n\nregister(ObjectEntriesQueryNode, (node, context) => {\n    const objectVar = aql.variable('object');\n    const keyVar = aql.variable('key');\n    return aqlExt.parenthesizeList(\n        aql`LET ${objectVar} = ${processNode(node.objectNode, context)}`,\n        aql`FOR ${keyVar} IN IS_DOCUMENT(${objectVar}) ? ATTRIBUTES(${objectVar}) : []`,\n        aql`RETURN [ ${keyVar}, ${objectVar}[${keyVar}] ]`\n    );\n});\n\nregister(FirstOfListQueryNode, (node, context) => {\n    return aql`FIRST(${processNode(node.listNode, context)})`;\n});\n\nregister(ListItemQueryNode, (node, context) => {\n    return aql`(${processNode(node.listNode, context)})[${node.index}]`;\n});\n\nregister(BinaryOperationQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n\n    // a > NULL is equivalent to a != NULL, and it can use indices better\n    // (but don't do it in flexsearch, there > NULL is something different from != NULL\n    if (\n        node.operator === BinaryOperator.UNEQUAL &&\n        (node.rhs instanceof NullQueryNode || (node.rhs instanceof LiteralQueryNode && node.rhs.value == undefined)) &&\n        !context.getExtension(inFlexSearchFilterSymbol)\n    ) {\n        return aql`(${lhs} > NULL)`;\n    }\n\n    const rhs = processNode(node.rhs, context);\n    const op = getAQLOperator(node.operator);\n    if (op) {\n        return aql`(${lhs} ${op} ${rhs})`;\n    }\n\n    switch (node.operator) {\n        case BinaryOperator.CONTAINS:\n            return aql`(${lhs} LIKE CONCAT(\"%\", ${rhs}, \"%\"))`;\n        case BinaryOperator.STARTS_WITH:\n            const slowFrag = aql`(LEFT(${lhs}, LENGTH(${rhs})) == ${rhs})`;\n            if (node.rhs instanceof LiteralQueryNode && typeof node.rhs.value === 'string') {\n                const fastFrag = getFastStartsWithQuery(lhs, node.rhs.value);\n                // still ned to use the slow frag to get case sensitiveness\n                // this is really bad for performance, see explanation in LIKE branch below\n                return aql`${fastFrag} && ${slowFrag}`;\n            }\n            return slowFrag;\n        case BinaryOperator.ENDS_WITH:\n            return aql`(RIGHT(${lhs}, LENGTH(${rhs})) == ${rhs})`;\n        case BinaryOperator.LIKE:\n            const slowLikeFrag = aql`LIKE(${lhs}, ${rhs}, true)`; // true: caseInsensitive\n            if (node.rhs instanceof LiteralQueryNode && typeof node.rhs.value === 'string') {\n                const { literalPrefix, isSimplePrefixPattern, isLiteralPattern } = analyzeLikePatternPrefix(\n                    node.rhs.value\n                );\n\n                if (isLiteralPattern) {\n                    return getEqualsIgnoreCaseQuery(lhs, literalPrefix);\n                }\n\n                const fastFrag = getFastStartsWithQuery(lhs, literalPrefix);\n                if (isSimplePrefixPattern) {\n                    // we can optimize the whole LIKE away and use a skiplist-index-optimizable range select\n                    return fastFrag;\n                }\n                // we can at least use the prefix search to narrow down the results\n                // however, this is way worse because we lose the ability to sort-and-then-limit using the same index\n                // -> queries with a \"first\" argument suddenly have the time complexity of the pre-limited\n                // (or even pre-filtered if the database decides to use the index for sorting) result size instead of\n                // being in O(first).\n                return aql`(${fastFrag} && ${slowLikeFrag})`;\n            }\n            return slowLikeFrag;\n        case BinaryOperator.APPEND:\n            return aql`CONCAT(${lhs}, ${rhs})`;\n        case BinaryOperator.PREPEND:\n            return aql`CONCAT(${rhs}, ${lhs})`;\n        case BinaryOperator.SUBTRACT_LISTS:\n            return aql`MINUS(${lhs}, ${rhs})`;\n        default:\n            throw new Error(`Unsupported binary operator: ${op}`);\n    }\n});\n\nregister(OperatorWithAnalyzerQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n    const rhs = processNode(node.rhs, context);\n    const analyzer = node.analyzer;\n\n    const isIdentityAnalyzer = !node.analyzer || node.analyzer === IDENTITY_ANALYZER;\n    // some operators support case-converting analyzers (like norm_ci) which only generate one token\n    const normalizedRhs = isIdentityAnalyzer ? rhs : aql`TOKENS(${rhs}, ${analyzer})[0]`;\n\n    switch (node.operator) {\n        case BinaryOperatorWithAnalyzer.EQUAL:\n            return aql`ANALYZER( ${lhs} == ${normalizedRhs},${analyzer})`;\n        case BinaryOperatorWithAnalyzer.UNEQUAL:\n            return aql`ANALYZER( ${lhs} != ${normalizedRhs},${analyzer})`;\n        case BinaryOperatorWithAnalyzer.IN:\n            if (isIdentityAnalyzer) {\n                return aql`(${lhs} IN ${rhs})`;\n            }\n            const loopVar = aql.variable(`token`);\n            return aql`ANALYZER( ${lhs} IN ( FOR ${loopVar} IN TOKENS(${rhs} , ${analyzer}) RETURN ${loopVar}[0] ), ${analyzer} )`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_ANY_WORD:\n            return aql`ANALYZER( ${lhs} IN TOKENS(${rhs}, ${analyzer}),${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_PREFIX:\n            // can't pass NULL to STARTS_WITH (generates an error)\n            // if an expression does not have a token, nothing can contain a prefix thereof, so we don't find anything\n            // this is also good behavior in case of searching because you just find nothing if you type special chars\n            // instead of finding everything\n            return aql`(LENGTH(TOKENS(${rhs},${analyzer})) ? ANALYZER( STARTS_WITH( ${lhs}, TOKENS(${rhs},${analyzer})[0]), ${analyzer}) : false)`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_PHRASE:\n            return aql`ANALYZER( PHRASE( ${lhs}, ${rhs}), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_LESS_THAN:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${''} , ${normalizedRhs}, true, false), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_LESS_THAN_OR_EQUAL:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${''} , ${normalizedRhs}, true, true), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_GREATER_THAN:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${normalizedRhs}, ${String.fromCodePoint(\n                0x10ffff\n            )}, false, true), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_GREATER_THAN_OR_EQUAL:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${normalizedRhs}, ${String.fromCodePoint(\n                0x10ffff\n            )}, true, true), ${analyzer})`;\n        default:\n            throw new Error(`Unsupported operator: ${node.operator}`);\n    }\n});\n\nregister(FlexSearchStartsWithQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n    const rhs = processNode(node.rhs, context);\n\n    if (!node.analyzer || node.analyzer === IDENTITY_ANALYZER) {\n        return aql`STARTS_WITH(${lhs}, ${rhs})`;\n    }\n\n    // This query node can be used with simple case-converting analyzers\n    // These case-converting analyzers will only ever result in one token, so we can use the first one, which\n    // is the input value case-converted.\n    return aql`ANALYZER(STARTS_WITH(${lhs}, TOKENS(${rhs},${node.analyzer})[0]), ${node.analyzer})`;\n});\n\nregister(FlexSearchFieldExistsQueryNode, (node, context) => {\n    const sourceNode = processNode(node.sourceNode, context);\n    if (node.analyzer) {\n        return aql`EXISTS(${sourceNode}, \"analyzer\", ${node.analyzer})`;\n    } else {\n        return aql`EXISTS(${sourceNode})`;\n    }\n});\n\nregister(FlexSearchComplexOperatorQueryNode, (node, context) => {\n    throw new Error(`Internal Error: FlexSearchComplexOperatorQueryNode must be expanded before generating the query.`);\n});\n\nfunction getBillingInput(\n    node: ConfirmForBillingQueryNode | CreateBillingEntityQueryNode,\n    key: string | number | AQLFragment,\n    context: QueryContext,\n    currentTimestamp: string\n) {\n    return aql`\n        key: ${key},\n        type: ${node.rootEntityTypeName},\n        category: ${processNode(node.categoryNode, context)},\n        quantity: ${processNode(node.quantityNode, context)},\n        isExported: false,\n        createdAt: ${currentTimestamp},\n        updatedAt: ${currentTimestamp}`;\n}\n\nregister(CreateBillingEntityQueryNode, (node, context) => {\n    const currentTimestamp = new Date().toISOString();\n    return aqlExt.parenthesizeList(\n        aql`UPSERT {\n            key: ${node.key},\n            type: ${node.rootEntityTypeName}\n        }`,\n        aql`INSERT {\n            ${getBillingInput(node, node.key, context, currentTimestamp)},\n            isConfirmedForExport: false\n         }`,\n        aql`UPDATE (OLD.isConfirmedForExport ? {} : {\n            updatedAt: ${currentTimestamp},\n            category: ${processNode(node.categoryNode, context)},\n            quantity: ${processNode(node.quantityNode, context)}\n        })`,\n        aql`IN ${getCollectionForBilling(AccessType.WRITE, context)}`,\n        aql`RETURN ${node.key}`\n    );\n});\n\nregister(ConfirmForBillingQueryNode, (node, context) => {\n    const key = processNode(node.keyNode, context);\n    const currentTimestamp = new Date().toISOString();\n    return aqlExt.parenthesizeList(\n        aql`UPSERT {\n            key: ${key},\n            type: ${node.rootEntityTypeName}\n        }`,\n        aql`INSERT {\n            ${getBillingInput(node, key, context, currentTimestamp)},\n            isConfirmedForExport: true,\n            confirmedForExportAt: ${currentTimestamp}\n         }`,\n        aql`UPDATE (OLD.isConfirmedForExport ? {} : {\n            isConfirmedForExport: true,\n            updatedAt: ${currentTimestamp},\n            confirmedForExportAt: ${currentTimestamp},\n            category: ${processNode(node.categoryNode, context)},\n            quantity: ${processNode(node.quantityNode, context)}\n        })`,\n        aql`IN ${getCollectionForBilling(AccessType.WRITE, context)}`,\n        aql`RETURN true`\n    );\n});\n\nfunction getFastStartsWithQuery(lhs: AQLFragment, rhsValue: string): AQLFragment {\n    if (!rhsValue.length) {\n        return aql`IS_STRING(${lhs})`;\n    }\n\n    // this works as long as the highest possible code point is also the last one in the collation\n    const maxChar = String.fromCodePoint(0x10ffff);\n    const maxStr = rhsValue + maxChar;\n\n    // UPPER is used to get the \"smallest\" representation of the value case-sensitive, LOWER for the \"largest\".\n    // the ordering looks like this:\n    // [\n    //   \"A\",\n    //   \"a\",\n    //   \"AA\",\n    //   \"Aa\",\n    //   \"aA\",\n    //   \"aa\",\n    //   \"AB\",\n    //   \"Ab\",\n    //   \"aB\",\n    //   \"ab\",\n    //   \"B\",\n    //   \"b\"\n    // ]\n    // This means that if the actual value is longer than the given prefix (i.e. it's a real prefix and not the whole\n    // string), the match will be case-insensitive. However, if the remaining suffix if empty, the search would\n    // sometimes be case-sensitive: If you search for the prefix a, A will not be found (because A < a), but a will\n    // match the prefix filter A. In order to avoid this, one needs to convert the given string to the lowest value\n    // within its case-sensitivity category. For ASCII characters, that's simply UPPER(), but that will not always be\n    // the case. The same thing applies to the upper bound.\n    return aql`(${lhs} >= UPPER(${rhsValue}) && ${lhs} < LOWER(${maxStr}))`;\n\n    // the following does not work because string sorting depends on the DB's collator\n    // which does not necessarily sort the characters by code points\n    // charCodeAt / fromCharCode works on code units, and so does the string indexer / substr / length\n    /*const lastCharCode = rhsValue.charCodeAt(rhsValue.length - 1);\n    const nextCharCode = lastCharCode + 1;\n    if (nextCharCode >= 0xD800) {\n        // don't mess with surrogate pairs\n        return undefined;\n    }\n\n    const nextValue = rhsValue.substring(0, rhsValue.length - 1) + String.fromCharCode(nextCharCode);\n    return aql`(${lhs} >= ${rhsValue} && ${lhs} < ${nextValue})`;*/\n}\n\nfunction getEqualsIgnoreCaseQuery(lhs: AQLFragment, rhsValue: string): AQLFragment {\n    // if the string e.g. only consists of digits, no need for special case sensitivity checking\n    if (isStringCaseInsensitive(rhsValue)) {\n        return aql`(${lhs} == ${aql.value(rhsValue)})`;\n    }\n\n    // w.r.t. UPPER/LOWER, see the comment in getFastStartsWithQuery\n    const lowerBoundFrag = aql`UPPER(${rhsValue})`;\n    const upperBoundFrag = aql`LOWER(${rhsValue})`;\n    return aql`(${lhs} >= ${lowerBoundFrag} && ${lhs} <= ${upperBoundFrag})`;\n}\n\nregister(UnaryOperationQueryNode, (node, context) => {\n    switch (node.operator) {\n        case UnaryOperator.NOT:\n            return aql`!(${processNode(node.valueNode, context)})`;\n        case UnaryOperator.JSON_STRINGIFY:\n            return aql`JSON_STRINGIFY(${processNode(node.valueNode, context)})`;\n        case UnaryOperator.ROUND:\n            return aql`ROUND(${processNode(node.valueNode, context)})`;\n        default:\n            throw new Error(`Unsupported unary operator: ${node.operator}`);\n    }\n});\n\nregister(ConditionalQueryNode, (node, context) => {\n    const cond = processNode(node.condition, context);\n    const expr1 = processNode(node.expr1, context);\n    const expr2 = processNode(node.expr2, context);\n    return aql`(${cond} ? ${expr1} : ${expr2})`;\n});\n\nregister(TypeCheckQueryNode, (node, context) => {\n    const value = processNode(node.valueNode, context);\n\n    switch (node.type) {\n        case BasicType.SCALAR:\n            return aql`(IS_BOOL(${value}) || IS_NUMBER(${value}) || IS_STRING(${value}))`;\n        case BasicType.LIST:\n            return aql`IS_LIST(${value})`;\n        case BasicType.OBJECT:\n            return aql`IS_OBJECT(${value})`;\n        case BasicType.NULL:\n            return aql`IS_NULL(${value})`;\n    }\n});\n\nregister(SafeListQueryNode, (node, context) => {\n    const reducedNode = new ConditionalQueryNode(\n        new TypeCheckQueryNode(node.sourceNode, BasicType.LIST),\n        node.sourceNode,\n        ListQueryNode.EMPTY\n    );\n    return processNode(reducedNode, context);\n});\n\nregister(QuantifierFilterNode, (node, context) => {\n    let { quantifier, conditionNode, listNode, itemVariable } = node;\n    conditionNode = simplifyBooleans(conditionNode);\n\n    const fastFragment = getQuantifierFilterUsingArrayComparisonOperator(\n        { quantifier, conditionNode, listNode, itemVariable },\n        context\n    );\n    if (fastFragment) {\n        return fastFragment;\n    }\n\n    // reduce 'every' to 'none' so that count-based evaluation is possible\n    if (quantifier === 'every') {\n        quantifier = 'none';\n        conditionNode = not(conditionNode);\n    }\n\n    const filteredListNode = new TransformListQueryNode({\n        listNode,\n        filterNode: conditionNode,\n        itemVariable\n    });\n\n    const finalNode = new BinaryOperationQueryNode(\n        new CountQueryNode(filteredListNode),\n        quantifier === 'none' ? BinaryOperator.EQUAL : BinaryOperator.GREATER_THAN,\n        new LiteralQueryNode(0)\n    );\n    return processNode(finalNode, context);\n});\n\n// uses the array expansion operator (https://docs.arangodb.com/3.0/AQL/Advanced/ArrayOperators.html#array-expansion)\n// that can utilize an index like \"items[*].itemNumber\" if possible\n// (specifically for something like items_some: {itemNumber: \"abc\"})\nfunction getQuantifierFilterUsingArrayComparisonOperator(\n    {\n        quantifier,\n        conditionNode,\n        listNode,\n        itemVariable\n    }: {\n        quantifier: Quantifier;\n        conditionNode: QueryNode;\n        listNode: QueryNode;\n        itemVariable: VariableQueryNode;\n    },\n    context: QueryContext\n): AQLFragment | undefined {\n    // ArangoDB supports array comparison operators (e.g. field ALL > 5)\n    // https://www.arangodb.com/docs/stable/aql/operators.html#array-comparison-operators\n    // it can be combined with the array expansion operator (e.g. items[*].field)\n    // https://docs.arangodb.com/3.0/AQL/Advanced/ArrayOperators.html#array-expansion\n    // quantifier filters with exactly one filter field that uses a comparison operator can be optimized with this\n    // this simplifies the AQL expression a lot (no filtering-then-checking-length), and also enables early pruning\n\n    // only possible on lists that are field accesses (but we can handle safe lists below)\n    let isSafeList = false;\n    if (listNode instanceof SafeListQueryNode) {\n        isSafeList = true;\n        listNode = listNode.sourceNode;\n    }\n    if (!(listNode instanceof FieldQueryNode)) {\n        return undefined;\n    }\n\n    if (!(conditionNode instanceof BinaryOperationQueryNode)) {\n        return undefined;\n    }\n\n    let operator: BinaryOperator;\n    switch (conditionNode.operator) {\n        case BinaryOperator.EQUAL:\n        case BinaryOperator.IN:\n        case BinaryOperator.UNEQUAL:\n        case BinaryOperator.LESS_THAN:\n        case BinaryOperator.LESS_THAN_OR_EQUAL:\n        case BinaryOperator.GREATER_THAN:\n        case BinaryOperator.GREATER_THAN_OR_EQUAL:\n            operator = conditionNode.operator;\n            break;\n\n        case BinaryOperator.LIKE:\n            // see if this really is a equals search so we can optimize it (only possible as long as it does not contain any case-specific characters)\n            if (!(conditionNode.rhs instanceof LiteralQueryNode) || typeof conditionNode.rhs.value !== 'string') {\n                return undefined;\n            }\n            const likePattern: string = conditionNode.rhs.value;\n            const { isLiteralPattern } = analyzeLikePatternPrefix(likePattern);\n            if (!isLiteralPattern || !isStringCaseInsensitive(likePattern)) {\n                return undefined;\n            }\n            operator = BinaryOperator.EQUAL;\n            break;\n\n        default:\n            return undefined;\n    }\n\n    let fields: Field[] = [];\n    let currentFieldNode = conditionNode.lhs;\n    while (currentFieldNode !== itemVariable) {\n        if (!(currentFieldNode instanceof FieldQueryNode)) {\n            return undefined;\n        }\n        fields.unshift(currentFieldNode.field); // we're traversing from back to front\n        currentFieldNode = currentFieldNode.objectNode;\n    }\n\n    const valueFrag = processNode(conditionNode.rhs, context);\n\n    let fieldValueFrag: AQLFragment;\n    if (fields.length) {\n        const fieldAccessFrag = aql.concat(fields.map(f => getPropertyAccessFragment(f.name)));\n        fieldValueFrag = aql`${processNode(listNode, context)}[*]${fieldAccessFrag}`;\n        // no need to use the SafeListQueryNode here because [*] already expands NULL to []\n    } else {\n        // special case: scalar list - no array expansion\n        if (isSafeList) {\n            // re-wrap in SafeListQueryNode to support generic case at the bottom\n            // \"something\" ANY IN NULL would not work (yields false instead of true)\n            // the shortcut with EQUAL / some would work though as \"something\" IN NULL is false\n            fieldValueFrag = processNode(new SafeListQueryNode(listNode), context);\n        } else {\n            fieldValueFrag = processNode(listNode, context);\n        }\n    }\n\n    // The case of \"field ANY == value\" can further be optimized into \"value IN field\" which can use an array index\n    // https://www.arangodb.com/docs/stable/indexing-index-basics.html#indexing-array-values\n    if (operator === BinaryOperator.EQUAL && quantifier === 'some') {\n        return aql`(${valueFrag} IN ${fieldValueFrag})`;\n    }\n\n    let quantifierFrag: AQLFragment;\n    switch (quantifier) {\n        case 'some':\n            quantifierFrag = aql`ANY`;\n            break;\n        case 'every':\n            quantifierFrag = aql`ALL`;\n            break;\n        case 'none':\n            quantifierFrag = aql`NONE`;\n            break;\n        default:\n            throw new Error(`Unexpected quantifier: ${quantifier}`);\n    }\n\n    const operatorFrag = getAQLOperator(operator);\n    if (!operatorFrag) {\n        throw new Error(`Unable to get AQL fragment for operator ${operator}`);\n    }\n    return aql`(${fieldValueFrag} ${quantifierFrag} ${operatorFrag} ${valueFrag})`;\n}\n\nregister(EntitiesQueryNode, (node, context) => {\n    return getCollectionForType(node.rootEntityType, AccessType.READ, context);\n});\n\nregister(FollowEdgeQueryNode, (node, context) => {\n    const tmpVar = aql.variable('node');\n    // need to wrap this in a subquery because ANY is not possible as first token of an expression node in AQL\n    return aqlExt.parenthesizeList(\n        aql`FOR ${tmpVar}`,\n        aql`IN ${getSimpleFollowEdgeFragment(node, context)}`,\n        aql`FILTER ${tmpVar} != null`,\n        aql`RETURN ${tmpVar}`\n    );\n});\n\nregister(TraversalQueryNode, (node, context) => {\n    const sourceFrag = processNode(node.sourceEntityNode, context);\n    const fieldDepth = node.fieldSegments.filter(s => s.isListSegment).length;\n\n    if (node.relationSegments.length) {\n        let mapFrag: ((itemFrag: AQLFragment) => AQLFragment) | undefined;\n\n        let remainingDepth = fieldDepth;\n        if (node.fieldSegments.length) {\n            // if we have both, it might be beneficial to do the field traversal within the mapping node\n            // because it may allow ArangoDB to figure out that only one particular field is of interest, and e.g.\n            // discard the root entities earlier\n\n            if (node.captureRootEntity) {\n                if (fieldDepth === 0) {\n                    // fieldSegments.length && fieldDepth === 0 means we only traverse through entity extensions\n                    // actually, shouldn't really occur because a collect path can't end with an entity extension and\n                    // value objects don't capture root entities\n                    // however, we can easily implement this so let's do it\n                    mapFrag = nodeFrag =>\n                        aql`{ obj: ${getFieldTraversalFragmentWithoutFlattening(\n                            node.fieldSegments,\n                            nodeFrag\n                        )}, root: ${nodeFrag}) }`;\n                } else {\n                    // the result of getFieldTraversalFragmentWithoutFlattening() now is a list, so we need to iterate\n                    // over it. if the depth is > 1, we need to flatten the deeper ones so we can do one FOR loop over them\n                    // we still return a list, so we just reduce the depth to 1 and not to 0\n                    const entityVar = aql.variable('entity');\n                    mapFrag = rootEntityFrag =>\n                        aqlExt.parenthesizeList(\n                            aql`FOR ${entityVar} IN ${getFlattenFrag(\n                                getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, rootEntityFrag),\n                                fieldDepth - 1\n                            )}`,\n                            aql`RETURN { obj: ${entityVar}, root: ${rootEntityFrag} }`\n                        );\n                    remainingDepth = 1;\n                }\n            } else {\n                mapFrag = nodeFrag => getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, nodeFrag);\n            }\n        } else {\n            if (node.captureRootEntity) {\n                // doesn't make sense to capture the root entity if we're returning the root entities\n                throw new Error(`captureRootEntity without fieldSegments detected`);\n            }\n        }\n\n        // traversal requires real ids\n        let fixedSourceFrag = sourceFrag;\n        if (node.entitiesIdentifierKind === EntitiesIdentifierKind.ID) {\n            if (node.sourceIsList) {\n                fixedSourceFrag = getFullIDFromKeysFragment(\n                    sourceFrag,\n                    node.relationSegments[0].relationSide.sourceType\n                );\n            } else {\n                fixedSourceFrag = getFullIDFromKeyFragment(\n                    sourceFrag,\n                    node.relationSegments[0].relationSide.sourceType\n                );\n            }\n        }\n\n        const frag = getRelationTraversalFragment({\n            segments: node.relationSegments,\n            sourceFrag: fixedSourceFrag,\n            sourceIsList: node.sourceIsList,\n            alwaysProduceList: node.alwaysProduceList,\n            mapFrag,\n            context\n        });\n        if (node.relationSegments.some(s => s.isListSegment) || node.sourceIsList) {\n            // if the relation contains a list segment, getRelationTraversalFragment will return a list\n            // if we already returned lists within the mapFrag (-> current value of remainingDepth), we need to add that\n            remainingDepth++;\n        }\n        // flatten 1 less than the depth, see below\n        return getFlattenFrag(frag, remainingDepth - 1);\n    }\n\n    if (node.captureRootEntity) {\n        // doesn't make sense (and isn't possible) to capture the root entity if we're not even crossing root entities\n        throw new Error(`captureRootEntity without relationSegments detected`);\n    }\n\n    if (node.sourceIsList) {\n        // don't need, don't bother\n        throw new Error(`sourceIsList without relationSegments detected`);\n    }\n\n    if (node.alwaysProduceList) {\n        // don't need, don't bother\n        throw new Error(`alwaysProduceList without relationSegments detected`);\n    }\n\n    if (node.entitiesIdentifierKind !== EntitiesIdentifierKind.ENTITY) {\n        throw new Error(`Only ENTITY identifiers supported without relationSegments`);\n    }\n\n    if (!node.fieldSegments.length) {\n        // should normally not occur\n        return sourceFrag;\n    }\n\n    // flatten 1 less than the fieldDepth:\n    // - no list segments -> evaluate to the object\n    // - one list segment -> evaluate to the list, so no flattening\n    // - two list segments -> needs flattening once to get one list\n    return getFlattenFrag(getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, sourceFrag), fieldDepth - 1);\n});\n\nfunction getRelationTraversalFragment({\n    segments,\n    sourceFrag,\n    sourceIsList,\n    alwaysProduceList,\n    mapFrag,\n    context\n}: {\n    readonly segments: ReadonlyArray<RelationSegment>;\n    readonly sourceFrag: AQLFragment;\n    readonly sourceIsList: boolean;\n    readonly alwaysProduceList: boolean;\n    readonly mapFrag?: (itemFrag: AQLFragment) => AQLFragment;\n    readonly context: QueryContext;\n}) {\n    if (!segments.length) {\n        return sourceFrag;\n    }\n\n    // ArangoDB 3.4.5 introduced PRUNE which also supports IS_SAME_COLLECTION so we may be able to use just one\n    // traversal which lists all affected edge collections and prunes on the path in the future.\n\n    const forFragments: AQLFragment[] = [];\n    const sourceEntityVar = aql.variable(`sourceEntity`);\n    let currentObjectFrag = sourceIsList ? sourceEntityVar : sourceFrag;\n    let segmentIndex = 0;\n    for (const segment of segments) {\n        const nodeVar = aql.variable(`node`);\n        const edgeVar = aql.variable(`edge`);\n        const pathVar = aql.variable(`path`);\n        const dir = segment.relationSide.isFromSide ? aql`OUTBOUND` : aql`INBOUND`;\n        let filterFrag = aql``;\n        let pruneFrag = aql``;\n        if (segment.vertexFilter) {\n            if (!segment.vertexFilterVariable) {\n                throw new Error(`vertexFilter is set, but vertexFilterVariable is not`);\n            }\n            const filterContext = context.introduceVariableAlias(segment.vertexFilterVariable, nodeVar);\n            // PRUNE to stop on a node that has to be filtered out (only necessary for traversals > 1 path length)\n            // however, PRUNE only seems to be a performance feature and is not reliably evaluated\n            // (e.g. it's not when using COLLECT with distinct for some reason), so we need to add a path filter\n            if (segment.maxDepth > 1) {\n                if (\n                    !(\n                        segment.vertexFilter instanceof BinaryOperationQueryNode &&\n                        segment.vertexFilter.lhs instanceof FieldQueryNode &&\n                        segment.vertexFilter.lhs.objectNode === segment.vertexFilterVariable\n                    )\n                ) {\n                    throw new Error(`Unsupported filter pattern for graph traversal`);\n                }\n                const vertexInPathFrag = aql`${pathVar}.vertices[*]`;\n                const pathFilterContext = context.introduceVariableAlias(\n                    segment.vertexFilterVariable,\n                    vertexInPathFrag\n                );\n                const lhsFrag = processNode(segment.vertexFilter.lhs, pathFilterContext);\n                const opFrag = getAQLOperator(segment.vertexFilter.operator);\n                if (!opFrag) {\n                    throw new Error(`Unsupported filter pattern for graph traversal`);\n                }\n                const pathFilterFrag = aql`${lhsFrag} ALL ${opFrag} ${processNode(\n                    segment.vertexFilter.rhs,\n                    pathFilterContext\n                )}`;\n                filterFrag = aql`\\nFILTER ${pathFilterFrag}`;\n                pruneFrag = aql`\\nPRUNE !(${processNode(segment.vertexFilter, filterContext)})`;\n            } else {\n                // FILTER to filter out result nodes\n                filterFrag = aql`\\nFILTER ${processNode(segment.vertexFilter, filterContext)}`;\n            }\n        }\n        const traversalFrag = aql`FOR ${nodeVar}, ${edgeVar}, ${pathVar} IN ${segment.minDepth}..${\n            segment.maxDepth\n        } ${dir} ${currentObjectFrag} ${getCollectionForRelation(\n            segment.relationSide.relation,\n            AccessType.READ,\n            context\n        )}${pruneFrag}${filterFrag}`;\n        if (segment.isListSegment || (alwaysProduceList && segmentIndex === segments.length - 1)) {\n            // this is simple - we can just push one FOR statement after the other\n            forFragments.push(traversalFrag);\n            currentObjectFrag = nodeVar;\n        } else {\n            // if this is not a list, we need to preserve NULL values\n            // (actually, we don't in some cases, but we need to figure out when)\n            // to preserve null values, we need to use FIRST\n            // to ignore dangling edges, add a FILTER though (if there was one dangling edge and one real edge collected, we should use the real one)\n            const nullableVar = aql.variable(`nullableNode`);\n            forFragments.push(\n                aql`LET ${nullableVar} = FIRST(${traversalFrag} FILTER ${nodeVar} != null RETURN ${nodeVar})`\n            );\n            currentObjectFrag = nullableVar;\n        }\n\n        segmentIndex++;\n    }\n\n    const lastSegment = segments[segments.length - 1];\n\n    // remove dangling edges, unless we already did because the last segment wasn't a list segment (see above, we add the FILTER there)\n    if (lastSegment.isListSegment) {\n        forFragments.push(aql`FILTER ${currentObjectFrag} != null`);\n    }\n\n    const returnFrag = mapFrag ? mapFrag(currentObjectFrag) : currentObjectFrag;\n    const returnList = lastSegment.resultIsList || sourceIsList || alwaysProduceList;\n    // make sure we don't return a list with one element\n    return aqlExt[returnList ? 'parenthesizeList' : 'parenthesizeObject'](\n        sourceIsList ? aql`FOR ${sourceEntityVar} IN ${sourceFrag}` : aql``,\n        ...forFragments,\n        aql`RETURN ${returnFrag}`\n    );\n}\n\nfunction getFlattenFrag(listFrag: AQLFragment, depth: number) {\n    if (depth <= 0) {\n        return listFrag;\n    }\n    if (depth === 1) {\n        return aql`${listFrag}[**]`;\n    }\n    return aql`FLATTEN(${listFrag}, ${aql.integer(depth)})`;\n}\n\nfunction getFieldTraversalFragmentWithoutFlattening(segments: ReadonlyArray<FieldSegment>, sourceFrag: AQLFragment) {\n    if (!segments.length) {\n        return sourceFrag;\n    }\n\n    let frag = sourceFrag;\n    for (const segment of segments) {\n        frag = aql`${frag}${getPropertyAccessFragment(segment.field.name)}`;\n        if (segment.isListSegment) {\n            // the array expansion operator [*] does two useful things:\n            // - it performs the next field access basically as .map(o => o.fieldName).\n            // - it converts non-lists to lists (important so that if we flatten afterwards, we don't include NULL lists\n            // the latter is why we also add the [*] at the end of the expression, which might look strange in the AQL.\n            frag = aql`${frag}[*]`;\n        }\n    }\n\n    return frag;\n}\n\nregister(CreateEntityQueryNode, (node, context) => {\n    return aqlExt.parenthesizeObject(\n        aql`INSERT ${processNode(node.objectNode, context)} IN ${getCollectionForType(\n            node.rootEntityType,\n            AccessType.WRITE,\n            context\n        )}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(CreateEntitiesQueryNode, (node, context) => {\n    const entityVar = aql.variable('entity');\n    return aqlExt.parenthesizeList(\n        aql`FOR ${entityVar} IN ${processNode(node.objectsNode, context)}`,\n        aql`INSERT ${entityVar} IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(UpdateEntitiesQueryNode, (node, context) => {\n    const newContext = context.introduceVariable(node.currentEntityVariable);\n    const entityVar = newContext.getVariable(node.currentEntityVariable);\n    let entityFrag: AQLFragment;\n    let options: AQLFragment;\n    let updateFrag = processNode(new ObjectQueryNode(node.updates), newContext);\n    let additionalUpdates: ReadonlyArray<SetFieldQueryNode> = [];\n\n    if (node.revision) {\n        entityFrag = aql`MERGE(${entityVar}, { _rev: ${aql.value(node.revision)} })`;\n        options = aql`{ mergeObjects: false, ignoreRevs: false }`;\n        // to guarantee that the _rev changes, we need to set a property to a new value\n        updateFrag = aql`MERGE(${updateFrag}, { _revDummy: ${entityVar}._rev })`;\n    } else {\n        entityFrag = entityVar;\n        options = aql`{ mergeObjects: false }`;\n    }\n\n    return aqlExt.parenthesizeList(\n        aql`FOR ${entityVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        aql`UPDATE ${entityFrag}`,\n        aql`WITH ${updateFrag}`,\n        aql`IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        aql`OPTIONS ${options}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(DeleteEntitiesQueryNode, (node, context) => {\n    const entityVar = aql.variable(decapitalize(node.rootEntityType.name));\n    let entityFrag: AQLFragment;\n    let optionsFrag: AQLFragment;\n\n    if (node.revision) {\n        if (node.entitiesIdentifierKind === EntitiesIdentifierKind.ID) {\n            entityFrag = aql`{ _key: ${entityVar}, _rev: ${aql.value(node.revision)} }`;\n        } else {\n            entityFrag = aql`MERGE(${entityVar}, { _rev: ${aql.value(node.revision)} })`;\n        }\n        optionsFrag = aql`OPTIONS { ignoreRevs: false }`;\n    } else {\n        entityFrag = entityVar;\n        optionsFrag = aql``;\n    }\n\n    const countVar = aql.variable(`count`);\n    return aqlExt[\n        node.resultValue === DeleteEntitiesResultValue.OLD_ENTITIES ? 'parenthesizeList' : 'parenthesizeObject'\n    ](\n        aql`FOR ${entityVar}`,\n        aql`${generateInClause(node.listNode, context, entityVar)}`,\n        aql`REMOVE ${entityFrag}`,\n        aql`IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        optionsFrag,\n        node.resultValue === DeleteEntitiesResultValue.OLD_ENTITIES\n            ? aql`RETURN OLD`\n            : aql.lines(aql`COLLECT WITH COUNT INTO ${countVar}`, aql`RETURN ${countVar}`)\n    );\n});\n\nregister(AddEdgesQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    return aqlExt.parenthesizeList(\n        aql`FOR ${edgeVar}`,\n        aql`IN [ ${aql.join(\n            node.edges.map(edge => formatEdge(node.relation, edge, context)),\n            aql`, `\n        )} ]`,\n        aql`UPSERT { _from: ${edgeVar}._from, _to: ${edgeVar}._to }`, // need to unpack avoid dynamic property names in UPSERT example filter\n        aql`INSERT ${edgeVar}`,\n        aql`UPDATE {}`,\n        aql`IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\nregister(RemoveEdgesQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    const fromVar = aql.variable('from');\n    const toVar = aql.variable('to');\n    let edgeFilter: AQLFragment;\n    if (node.edgeFilter.fromIDsNode && node.edgeFilter.toIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._from == ${fromVar} && ${edgeVar}._to == ${toVar}`;\n    } else if (node.edgeFilter.fromIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._from == ${fromVar}`;\n    } else if (node.edgeFilter.toIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._to == ${toVar}`;\n    } else {\n        edgeFilter = aql``;\n    }\n    return aqlExt.parenthesizeList(\n        node.edgeFilter.fromIDsNode\n            ? aql`FOR ${fromVar} IN ${getFullIDsFromKeysNode(\n                  node.edgeFilter.fromIDsNode!,\n                  node.relation.fromType,\n                  context\n              )}`\n            : aql``,\n        node.edgeFilter.toIDsNode\n            ? aql`FOR ${toVar} IN ${getFullIDsFromKeysNode(node.edgeFilter.toIDsNode!, node.relation.toType, context)}`\n            : aql``,\n        aql`FOR ${edgeVar} IN ${getCollectionForRelation(node.relation, AccessType.READ, context)}`,\n        edgeFilter,\n        aql`REMOVE ${edgeVar} IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\nregister(SetEdgeQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    return aqlExt.parenthesizeList(\n        aql`UPSERT ${formatEdge(node.relation, node.existingEdge, context)}`,\n        aql`INSERT ${formatEdge(node.relation, node.newEdge, context)}`,\n        aql`UPDATE ${formatEdge(node.relation, node.newEdge, context)}`,\n        aql`IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\n/**\n * Gets an aql fragment that evaluates to a string of the format \"collectionName/objectKey\", given a query node that\n * evaluates to the \"object id\", which is, in arango terms, the _key.\n */\nfunction getFullIDFromKeyNode(node: QueryNode, rootEntityType: RootEntityType, context: QueryContext): AQLFragment {\n    // special handling to avoid concat if possible - do not alter the behavior\n    if (node instanceof LiteralQueryNode && typeof node.value == 'string') {\n        // just append the node to the literal key in JavaScript and bind it as a string\n        return aql`${getCollectionNameForRootEntity(rootEntityType) + '/' + node.value}`;\n    }\n    if (node instanceof RootEntityIDQueryNode) {\n        // access the _id field. processNode(node) would access the _key field instead.\n        return aql`${processNode(node.objectNode, context)}._id`;\n    }\n\n    // fall back to general case\n    return getFullIDFromKeyFragment(processNode(node, context), rootEntityType);\n}\n\nfunction getFullIDsFromKeysNode(\n    idsNode: QueryNode,\n    rootEntityType: RootEntityType,\n    context: QueryContext\n): AQLFragment {\n    if (idsNode instanceof ListQueryNode) {\n        // this probably generates cleaner AQL without dynamic concat\n        const idFragments = idsNode.itemNodes.map(idNode => getFullIDFromKeyNode(idNode, rootEntityType, context));\n        return aql`[${aql.join(idFragments, aql`, `)}]`;\n    }\n    if (\n        idsNode instanceof LiteralQueryNode &&\n        Array.isArray(idsNode.value) &&\n        idsNode.value.every(v => typeof v === 'string')\n    ) {\n        const collName = getCollectionNameForRootEntity(rootEntityType);\n        const ids = idsNode.value.map(val => collName + '/' + val);\n        return aql.value(ids);\n    }\n\n    return getFullIDFromKeysFragment(processNode(idsNode, context), rootEntityType);\n}\n\nfunction getFullIDFromKeyFragment(keyFragment: AQLFragment, rootEntityType: RootEntityType): AQLFragment {\n    return aql`CONCAT(${getCollectionNameForRootEntity(rootEntityType) + '/'}, ${keyFragment})`;\n}\n\nfunction getFullIDFromKeysFragment(keysFragment: AQLFragment, rootEntityType: RootEntityType): AQLFragment {\n    const idVar = aql.variable('id');\n    return aql`(FOR ${idVar} IN ${keysFragment} RETURN ${getFullIDFromKeyFragment(idVar, rootEntityType)})`;\n}\n\nfunction formatEdge(\n    relation: Relation,\n    edge: PartialEdgeIdentifier | EdgeIdentifier,\n    context: QueryContext\n): AQLFragment {\n    const conditions = [];\n    if (edge.fromIDNode) {\n        conditions.push(aql`_from: ${getFullIDFromKeyNode(edge.fromIDNode, relation.fromType, context)}`);\n    }\n    if (edge.toIDNode) {\n        conditions.push(aql`_to: ${getFullIDFromKeyNode(edge.toIDNode, relation.toType, context)}`);\n    }\n\n    return aql`{${aql.join(conditions, aql`, `)}}`;\n}\n\nfunction getAQLOperator(op: BinaryOperator): AQLFragment | undefined {\n    switch (op) {\n        case BinaryOperator.AND:\n            return aql`&&`;\n        case BinaryOperator.OR:\n            return aql`||`;\n        case BinaryOperator.EQUAL:\n            return aql`==`;\n        case BinaryOperator.UNEQUAL:\n            return aql`!=`;\n        case BinaryOperator.LESS_THAN:\n            return aql`<`;\n        case BinaryOperator.LESS_THAN_OR_EQUAL:\n            return aql`<=`;\n        case BinaryOperator.GREATER_THAN:\n            return aql`>`;\n        case BinaryOperator.GREATER_THAN_OR_EQUAL:\n            return aql`>=`;\n        case BinaryOperator.IN:\n            return aql`IN`;\n        case BinaryOperator.ADD:\n            return aql`+`;\n        case BinaryOperator.SUBTRACT:\n            return aql`-`;\n        case BinaryOperator.MULTIPLY:\n            return aql`*`;\n        case BinaryOperator.DIVIDE:\n            return aql`/`;\n        case BinaryOperator.MODULO:\n            return aql`%`;\n        default:\n            return undefined;\n    }\n}\n\nfunction generateSortAQL(orderBy: OrderSpecification, context: QueryContext): AQLFragment {\n    if (orderBy.isUnordered()) {\n        return aql``;\n    }\n\n    function dirAQL(dir: OrderDirection) {\n        if (dir == OrderDirection.DESCENDING) {\n            return aql` DESC`;\n        }\n        return aql``;\n    }\n\n    const clauses = orderBy.clauses.map(cl => aql`(${processNode(cl.valueNode, context)}) ${dirAQL(cl.direction)}`);\n\n    return aql`SORT ${aql.join(clauses, aql`, `)}`;\n}\n\nfunction processNode(node: QueryNode, context: QueryContext): AQLFragment {\n    const processor = processors.get(node.constructor as Constructor<QueryNode>);\n    if (!processor) {\n        throw new Error(`Unsupported query type: ${node.constructor.name}`);\n    }\n    return processor(node, context);\n}\n\n// TODO I think AQLCompoundQuery (AQL transaction node) should not be the exported type\n// we should rather export AQLExecutableQuery[] (as AQL transaction) directly.\nexport function getAQLQuery(node: QueryNode): AQLCompoundQuery {\n    return createAQLCompoundQuery(node, aql.queryResultVariable('result'), undefined, new QueryContext());\n}\n\nfunction getCollectionForBilling(accessType: AccessType, context: QueryContext) {\n    const name = billingCollectionName;\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\nfunction getCollectionForType(type: RootEntityType, accessType: AccessType, context: QueryContext) {\n    const name = getCollectionNameForRootEntity(type);\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\nfunction getCollectionForRelation(relation: Relation, accessType: AccessType, context: QueryContext) {\n    const name = getCollectionNameForRelation(relation);\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\n/**\n * Processes a FollowEdgeQueryNode into a fragment to be used within `IN ...` (as opposed to be used in a general\n * expression context)\n */\nfunction getSimpleFollowEdgeFragment(node: FollowEdgeQueryNode, context: QueryContext): AQLFragment {\n    const dir = node.relationSide.isFromSide ? aql`OUTBOUND` : aql`INBOUND`;\n    return aql`${dir} ${processNode(node.sourceEntityNode, context)} ${getCollectionForRelation(\n        node.relationSide.relation,\n        AccessType.READ,\n        context\n    )}`;\n}\n\nfunction isStringCaseInsensitive(str: string) {\n    return str.toLowerCase() === str.toUpperCase();\n}\n\nexport function generateTokenizationQuery(tokensFiltered: ReadonlyArray<FlexSearchTokenizable>) {\n    const fragments: string[] = [];\n    for (let i = 0; i < tokensFiltered.length; i++) {\n        const value = tokensFiltered[i];\n        fragments.push(`token_${i}: TOKENS(\"${value.expression}\", \"${value.analyzer}\")`);\n    }\n    const query = `RETURN { ${fragments.join(',\\n')} }`;\n    return query;\n}\n", "import { Database } from 'arangojs';\nimport { globalContext } from '../../config/global';\nimport { ProjectOptions } from '../../config/interfaces';\nimport { Logger } from '../../config/logging';\nimport { ExecutionOptions } from '../../execution/execution-options';\nimport {\n    ConflictRetriesExhaustedError,\n    TransactionCancelledError,\n    TransactionTimeoutError\n} from '../../execution/runtime-errors';\nimport { Model } from '../../model';\nimport { ALL_QUERY_RESULT_VALIDATOR_FUNCTION_PROVIDERS, QueryNode } from '../../query-tree';\nimport { FlexSearchTokenization } from '../../query-tree/flex-search';\nimport { Mutable } from '../../utils/util-types';\nimport { objectValues, sleep, sleepInterruptible } from '../../utils/utils';\nimport { getPreciseTime, Watch } from '../../utils/watch';\nimport {\n    DatabaseAdapter,\n    DatabaseAdapterTimings,\n    ExecutionArgs,\n    ExecutionPlan,\n    ExecutionResult,\n    FlexSearchTokenizable,\n    TransactionStats\n} from '../database-adapter';\nimport { AQLCompoundQuery, aqlConfig, AQLExecutableQuery } from './aql';\nimport { generateTokenizationQuery, getAQLQuery } from './aql-generator';\nimport { RequestInstrumentation, RequestInstrumentationPhase } from './arangojs-instrumentation/config';\nimport { CancellationManager } from './cancellation-manager';\nimport {\n    ArangoDBConfig,\n    DEFAULT_RETRY_DELAY_BASE_MS,\n    getArangoDBLogger,\n    initDatabase,\n    RETRY_DELAY_RANDOM_FRACTION\n} from './config';\nimport { ERROR_ARANGO_CONFLICT, ERROR_QUERY_KILLED } from './error-codes';\nimport { hasRevisionAssertions } from './revision-helper';\nimport { SchemaAnalyzer } from './schema-migration/analyzer';\nimport { SchemaMigration } from './schema-migration/migrations';\nimport { MigrationPerformer } from './schema-migration/performer';\nimport { TransactionError } from '../../execution/transaction-error';\nimport { ArangoDBVersion, ArangoDBVersionHelper } from './version-helper';\nimport uuid = require('uuid');\n\nconst requestInstrumentationBodyKey = 'cruddlRequestInstrumentation';\n\ninterface ArangoExecutionOptions {\n    readonly queries: ReadonlyArray<AQLExecutableQuery>;\n    readonly options: ExecutionOptions;\n    /**\n     * An ID that will be prepended to all queries in this transaction so they can be aborted on cancellation\n     */\n    readonly transactionID: string;\n}\n\ninterface ArangoError extends Error {\n    readonly errorNum?: number;\n    readonly errorMessage?: string;\n}\n\nfunction isArangoError(error: Error): error is ArangoError {\n    return 'errorNum' in error;\n}\n\ninterface ArangoTransactionResult {\n    readonly data?: any;\n    readonly error?: ArangoError;\n    readonly timings?: { readonly [key: string]: number };\n    readonly plans?: ReadonlyArray<any>;\n    readonly stats: TransactionStats;\n}\n\ninterface TransactionResult {\n    readonly data?: any;\n    readonly timings?: Pick<DatabaseAdapterTimings, 'database' | 'dbConnection'>;\n    readonly plans?: ReadonlyArray<any>;\n    readonly databaseError?: Error;\n    readonly stats: TransactionStats;\n\n    /**\n     * True if the transactionTimeoutMs has taken effect. Does not necessarily mean that the query has been killed,\n     * you should check databaseError for his.\n     */\n    readonly hasTimedOut: boolean;\n\n    /**\n     * True if the cancellationToken has taken effect. Does not necessarily mean that the query has been killed,\n     * you should check databaseError for his.\n     */\n    readonly wasCancelled: boolean;\n}\n\nexport class ArangoDBAdapter implements DatabaseAdapter {\n    private readonly db: Database;\n    private readonly logger: Logger;\n    private readonly analyzer: SchemaAnalyzer;\n    private readonly migrationPerformer: MigrationPerformer;\n    private readonly cancellationManager: CancellationManager;\n    private readonly versionHelper: ArangoDBVersionHelper;\n    private readonly doNonMandatoryMigrations: boolean;\n    private readonly arangoExecutionFunction: string;\n\n    constructor(private readonly config: ArangoDBConfig, private schemaContext?: ProjectOptions) {\n        this.logger = getArangoDBLogger(schemaContext);\n        this.db = initDatabase(config);\n        this.analyzer = new SchemaAnalyzer(config, schemaContext);\n        this.migrationPerformer = new MigrationPerformer(config);\n        this.versionHelper = new ArangoDBVersionHelper(this.db);\n        this.arangoExecutionFunction = this.buildUpArangoExecutionFunction();\n        // the cancellation manager gets its own database instance so its cancellation requests are not queued\n        this.cancellationManager = new CancellationManager({ database: initDatabase(config) });\n        this.doNonMandatoryMigrations = config.doNonMandatoryMigrations !== false; // defaults to true\n    }\n\n    /**\n     * Gets the javascript source code for a function that executes a transaction\n     * @returns {string}\n     */\n    private buildUpArangoExecutionFunction(): string {\n        // The following function will be translated to a string and executed (as one transaction) within the\n        // ArangoDB server itself. Therefore the next comment is necessary to instruct our test coverage tool\n        // (https://github.com/istanbuljs/nyc) not to instrument the code with coverage instructions.\n\n        /* istanbul ignore next */\n        const arangoExecutionFunction = function({ queries, options, transactionID }: ArangoExecutionOptions) {\n            const db = require('@arangodb').db;\n            const enableProfiling = options.recordTimings;\n            const internal = enableProfiling ? require('internal') : undefined;\n\n            function getPreciseTime() {\n                return internal.time();\n            }\n\n            const startTime = enableProfiling ? getPreciseTime() : 0;\n\n            let validators: { [name: string]: (validationData: any, result: any) => void } = {};\n            //inject_validators_here\n\n            let timings: { [key: string]: number } | undefined = enableProfiling ? {} : undefined;\n            let timingsTotal = 0;\n\n            let plans: any[] = [];\n\n            let transactionStats: Mutable<TransactionStats> = {};\n\n            /**\n             * Throws an error so that the transaction is rolled back and returns the given value as transaction result\n             */\n            function rollbackWithResult(transactionResult: any): never {\n                const error = new Error(`${JSON.stringify(transactionResult)}`);\n                error.name = 'RolledBackTransactionError';\n                throw error;\n            }\n\n            function rollbackWithError(error: any): never {\n                if (enableProfiling && timings) {\n                    timings.js = getPreciseTime() - startTime - timingsTotal;\n                }\n\n                // the return is here to please typescript, it actually returns *never* (it throws)\n                return rollbackWithResult({\n                    error,\n                    timings,\n                    plans,\n                    stats: transactionStats\n                });\n            }\n\n            let resultHolder: { [p: string]: any } = {};\n            for (const query of queries) {\n                const bindVars = query.boundValues;\n                for (const key in query.usedPreExecResultNames) {\n                    bindVars[query.usedPreExecResultNames[key]] = resultHolder[key];\n                }\n\n                let explainResult;\n                // Execute the AQL query\n                let executionResult;\n                try {\n                    // the explain statement also can cause errors which should be caught\n                    if (options.recordPlan) {\n                        const stmt = db._createStatement({\n                            query: query.code,\n                            bindVars\n                        });\n                        explainResult = stmt.explain({ allPlans: true });\n                    }\n\n                    executionResult = db._query({\n                        query: `/*id:${transactionID}*/\\n${query.code}`,\n                        bindVars,\n                        options: {\n                            profile: options.recordPlan ? 2 : options.recordTimings ? 1 : 0,\n                            memoryLimit: options.queryMemoryLimit\n                        }\n                    });\n                } catch (error) {\n                    if (explainResult) {\n                        plans.push({\n                            plan: explainResult.plans[0],\n                            discardedPlans: explainResult.plans.slice(1),\n                            warnings: explainResult.warnings\n                        });\n                    }\n\n                    rollbackWithError(error);\n                }\n\n                const resultData = executionResult.next();\n\n                if (timings) {\n                    let profile = executionResult.getExtra().profile;\n                    for (let key in profile) {\n                        if (profile.hasOwnProperty(key)) {\n                            timings[key] = (timings[key] || 0) + profile[key];\n                            timingsTotal += profile[key];\n                        }\n                    }\n                }\n\n                if (options.recordPlan) {\n                    const extra = executionResult.getExtra();\n                    plans.push({\n                        plan: extra.plan,\n                        discardedPlans: explainResult ? explainResult.plans.slice(1) : [],\n                        stats: extra.stats,\n                        warnings: extra.warnings,\n                        profile: extra.profile\n                    });\n                }\n\n                if (executionResult.getExtra().stats && executionResult.getExtra().stats.peakMemoryUsage) {\n                    const usage = executionResult.getExtra().stats.peakMemoryUsage;\n                    if (!transactionStats.peakQueryMemoryUsage || transactionStats.peakQueryMemoryUsage < usage) {\n                        transactionStats.peakQueryMemoryUsage = usage;\n                    }\n                }\n\n                if (query.resultName) {\n                    resultHolder[query.resultName] = resultData;\n                }\n\n                try {\n                    if (query.resultValidator) {\n                        for (const key in query.resultValidator) {\n                            if (key in validators) {\n                                validators[key](query.resultValidator[key], resultData);\n                            }\n                        }\n                    }\n                } catch (e) {\n                    rollbackWithError({\n                        message: e.message,\n                        code: e.code\n                    });\n                }\n            }\n\n            // the last query is always the main query, use its result as result of the transaction\n            const lastQueryResultName = queries[queries.length - 1].resultName;\n            let data;\n            if (lastQueryResultName) {\n                data = resultHolder[lastQueryResultName];\n            } else {\n                data = undefined;\n            }\n\n            if (enableProfiling && timings) {\n                timings.js = getPreciseTime() - startTime - timingsTotal;\n            }\n\n            const transactionResult = {\n                data,\n                timings,\n                plans,\n                stats: transactionStats\n            };\n\n            if (options.mutationMode === 'rollback') {\n                rollbackWithResult(transactionResult);\n            }\n\n            return transactionResult;\n        };\n\n        const validatorProviders = ALL_QUERY_RESULT_VALIDATOR_FUNCTION_PROVIDERS.map(\n            provider => `[${JSON.stringify(provider.getValidatorName())}]: ${String(provider.getValidatorFunction())}`\n        );\n\n        const allValidatorFunctionsObjectString = `validators = {${validatorProviders.join(',\\n')}}`;\n\n        return String(arangoExecutionFunction).replace('//inject_validators_here', allValidatorFunctionsObjectString);\n    }\n\n    async execute(queryTree: QueryNode) {\n        const result = await this.executeExt({ queryTree });\n        if (result.error) {\n            throw result.error;\n        }\n        return result.data;\n    }\n\n    async executeExt({ queryTree, ...options }: ExecutionArgs): Promise<ExecutionResult> {\n        const prepStartTime = getPreciseTime();\n        globalContext.registerContext(this.schemaContext);\n        let executableQueries: AQLExecutableQuery[];\n        let aqlQuery: AQLCompoundQuery;\n        const oldEnableIndentationForCode = aqlConfig.enableIndentationForCode;\n        aqlConfig.enableIndentationForCode = !!options.recordPlan;\n        try {\n            //TODO Execute single statement AQL queries directly without \"db.transaction\"?\n            aqlQuery = getAQLQuery(queryTree);\n            executableQueries = aqlQuery.getExecutableQueries();\n        } finally {\n            globalContext.unregisterContext();\n            aqlConfig.enableIndentationForCode = oldEnableIndentationForCode;\n        }\n        if (this.logger.isTraceEnabled()) {\n            this.logger.trace(`Executing AQL: ${aqlQuery.toColoredString()}`);\n        }\n        const aqlPreparationTime = getPreciseTime() - prepStartTime;\n\n        // if the query contains revision assertions (_revision is used in updates / deletes), CONFLICT errors are\n        // expected and retrying the mutation won't help. The caller needs to handle the conflicts then.\n        // otherwise, conflicts can still occur because of how arangodb internally works, but those can be solved\n        // by retrying the query.\n        let executionResult;\n        if (hasRevisionAssertions(queryTree)) {\n            executionResult = await this.executeTransactionOnce(executableQueries, options, aqlQuery);\n        } else {\n            executionResult = await this.executeTransactionWithRetries(executableQueries, options, aqlQuery);\n        }\n        const {\n            databaseError,\n            timings: transactionTimings,\n            data,\n            plans,\n            stats,\n            hasTimedOut,\n            wasCancelled\n        } = executionResult;\n\n        let timings;\n        if (options.recordTimings && transactionTimings) {\n            timings = {\n                ...transactionTimings,\n                preparation: {\n                    total: aqlPreparationTime,\n                    aql: aqlPreparationTime\n                }\n            };\n        }\n\n        let plan: ExecutionPlan | undefined;\n        if (options.recordPlan && plans) {\n            plan = {\n                queryTree,\n                transactionSteps: executableQueries.map((q, index) => ({\n                    query: q.code,\n                    boundValues: q.boundValues,\n                    plan: plans[index] && plans[index].plan,\n                    discardedPlans: plans[index] && plans[index].discardedPlans,\n                    stats: plans[index] && plans[index].stats,\n                    warnings: plans[index] && plans[index].warnings,\n                    profile: plans[index] && plans[index].profile\n                }))\n            };\n        }\n\n        let error;\n        if (databaseError) {\n            error = this.processDatabaseError(databaseError, {\n                wasCancelled,\n                hasTimedOut,\n                transactionTimeoutMs: options.transactionTimeoutMs\n            });\n        }\n\n        return {\n            error,\n            data,\n            timings,\n            plan,\n            stats\n        };\n    }\n\n    private processDatabaseError(\n        error: Error,\n        {\n            hasTimedOut,\n            wasCancelled,\n            transactionTimeoutMs\n        }: { hasTimedOut: boolean; wasCancelled: boolean; transactionTimeoutMs: number | undefined }\n    ): Error {\n        // might be just something like a TypeError\n        if (!isArangoError(error)) {\n            return new TransactionError(error.message, error);\n        }\n\n        // some errors need to be translated because we only can differentiate with the context here\n        if (error.errorNum === ERROR_QUERY_KILLED) {\n            // only check these flags if a QUERY_KILLED error is thrown because we might have initiated a query\n            // kill due to timeout / cancellation, but it might have completed or errored for some other reason\n            // before the kill is executed\n            if (hasTimedOut) {\n                return new TransactionTimeoutError({ timeoutMs: transactionTimeoutMs });\n            } else if (wasCancelled) {\n                return new TransactionCancelledError();\n            }\n        }\n\n        // the arango errors are weird and have their message in \"errorMessage\"...\n        return new TransactionError(error.errorMessage || error.message, error);\n    }\n\n    private async executeTransactionWithRetries(\n        executableQueries: ReadonlyArray<AQLExecutableQuery>,\n        options: ExecutionOptions,\n        aqlQuery: AQLCompoundQuery\n    ): Promise<TransactionResult> {\n        const maxRetries = this.config.retriesOnConflict || 0;\n        let nextRetryDelay = 0;\n        let retries = 0;\n        let result;\n        // timings need to be added up\n        let timings: TransactionResult['timings'] | undefined;\n\n        while (true) {\n            result = await this.executeTransactionOnce(executableQueries, options, aqlQuery);\n\n            if (options.recordTimings && result.timings) {\n                timings = {\n                    database: sumUpValues([timings ? timings.database : {}, result.timings.database]),\n                    dbConnection: sumUpValues([timings ? timings.dbConnection : {}, result.timings.dbConnection])\n                } as TransactionResult['timings'];\n            }\n\n            const stats = {\n                ...result.stats,\n                retries\n            };\n\n            if (!result.databaseError || !this.isRetryableError(result.databaseError) || !maxRetries) {\n                return {\n                    ...result,\n                    timings,\n                    stats\n                };\n            }\n\n            if (retries >= maxRetries) {\n                // retries exhausted\n                return {\n                    ...result,\n                    timings,\n                    stats,\n                    databaseError: new ConflictRetriesExhaustedError({ causedBy: result.databaseError, retries })\n                };\n            }\n\n            const sleepStart = getPreciseTime();\n            const randomFactor = 1 + RETRY_DELAY_RANDOM_FRACTION * (2 * Math.random() - 1);\n            const delayWithRandomComponent = nextRetryDelay * randomFactor;\n            const shouldContinue = await sleepInterruptible(delayWithRandomComponent, options.cancellationToken);\n            if (options.recordTimings && timings) {\n                const sleepLength = getPreciseTime() - sleepStart;\n                timings = {\n                    ...timings,\n                    dbConnection: sumUpValues([timings.dbConnection, { retryDelay: sleepLength, total: sleepLength }])\n                } as TransactionResult['timings'];\n            }\n            if (!shouldContinue) {\n                // cancellation token fired before the sleep time was over\n                // we already have a result with an error, so it's probably better to return that instead of a generic \"cancelled\"\n                // probably doesn't matter anyway because the caller probably is no longer interested in the result\n                return {\n                    ...result,\n                    timings,\n                    stats\n                };\n            }\n\n            if (nextRetryDelay) {\n                nextRetryDelay *= 2;\n            } else {\n                nextRetryDelay = this.config.retryDelayBaseMs || DEFAULT_RETRY_DELAY_BASE_MS;\n            }\n            retries++;\n        }\n    }\n\n    private isRetryableError(error: ArangoError): boolean {\n        return error.errorNum === ERROR_ARANGO_CONFLICT;\n    }\n\n    private async executeTransactionOnce(\n        executableQueries: ReadonlyArray<AQLExecutableQuery>,\n        options: ExecutionOptions,\n        aqlQuery: AQLCompoundQuery\n    ): Promise<TransactionResult> {\n        const transactionID = uuid();\n        const args: ArangoExecutionOptions = {\n            queries: executableQueries,\n            options: {\n                ...options,\n                queryMemoryLimit: options.queryMemoryLimit || this.config.queryMemoryLimit\n            },\n            transactionID\n        };\n        let isTransactionFinished = false;\n        const watch = new Watch();\n\n        let hasTimedOut = false;\n        let wasCancelled = false;\n\n        let cancellationToken = options.cancellationToken;\n        if (cancellationToken) {\n            cancellationToken.then(() => {\n                wasCancelled = true;\n            });\n        }\n        let requestSentCallback: (() => void) | undefined;\n        let requestSentPromise = new Promise<void>(resolve => (requestSentCallback = resolve));\n        let timeout: any | undefined;\n        if (options.transactionTimeoutMs != undefined) {\n            const ms = options.transactionTimeoutMs;\n            // transactionTimeout is a timeout that should only be started when the request is actually sent to ArangoDB\n            const timeoutPromise = requestSentPromise\n                .then(\n                    () =>\n                        new Promise<void>(resolve => {\n                            timeout = setTimeout(resolve, ms);\n                        })\n                )\n                .then(() => {\n                    hasTimedOut = true;\n                });\n            if (cancellationToken) {\n                cancellationToken.then(() => {\n                    if (timeout) {\n                        clearTimeout(timeout);\n                        timeout = undefined;\n                    }\n                });\n                cancellationToken = Promise.race([cancellationToken, timeoutPromise]);\n            } else {\n                cancellationToken = timeoutPromise;\n            }\n        }\n\n        // we pass the cancellationToken to the call to Database.transaction(). This will remove the request from the\n        // http agent's queue. However, it won't cancel the request if already sent because ArangoDB does NOT abort a\n        // query in this case, so this would not help. In the contrary, it would free up the connection in the arangojs\n        // http agent so that more queries can be run in parallel than configured (via maxSockets). This would be\n        // dangerous because it might exhaust ArangoDB threads so that ArangoDB no longer responds, and it might even\n        // cause too much memory to be allocated. For this reason, we only kill the query (see below) and let that\n        // killed query also abort the transaction.\n        // Note: this only works because we use our own version of the arangojs database (CustomDatbase)\n        (args as any)[requestInstrumentationBodyKey] = {\n            onPhaseEnded: (phase: RequestInstrumentationPhase) => {\n                watch.stop(phase);\n\n                if (phase === 'socketInit') {\n                    // start the timeout promise if needed\n                    if (requestSentCallback) {\n                        requestSentCallback();\n                    }\n\n                    if (cancellationToken) {\n                        // delay cancellation a bit for two reasons\n                        // - don't take the effort of finding and killing a query if it's fast anyway\n                        // - the cancellation might occur before the transaction script starts the query\n                        // we only really need this to cancel long-running queries\n                        cancellationToken\n                            .then(() => sleep(30))\n                            .then(() => {\n                                // don't try to kill the query if the transaction() call finished already - this would mean that it\n                                // either was faster than the delay above, or the request was removed from the request queue\n                                if (!isTransactionFinished) {\n                                    this.logger.debug(`Cancelling query ${transactionID}`);\n                                    this.cancellationManager.cancelQuery(transactionID).catch(e => {\n                                        this.logger.warn(`Error cancelling query ${transactionID}: ${e.stack}`);\n                                    });\n                                }\n                            });\n                    }\n                }\n            },\n            cancellationToken\n        } as RequestInstrumentation;\n\n        const dbStartTime = getPreciseTime();\n        let transactionResult: ArangoTransactionResult;\n        try {\n            transactionResult = await this.db.executeTransaction(\n                {\n                    read: aqlQuery.readAccessedCollections,\n                    write: aqlQuery.writeAccessedCollections\n                },\n                this.arangoExecutionFunction,\n                {\n                    params: args,\n                    waitForSync: true\n                }\n            );\n        } catch (e) {\n            isTransactionFinished = true;\n            if (e.message.startsWith('RolledBackTransactionError: ')) {\n                const valStr = e.message.substr('RolledBackTransactionError: '.length);\n                try {\n                    transactionResult = JSON.parse(valStr);\n                } catch (eParse) {\n                    throw new Error(`Error parsing result of rolled back transaction`);\n                }\n            } else {\n                throw e;\n            }\n        } finally {\n            if (timeout) {\n                clearTimeout(timeout);\n                timeout = undefined;\n            }\n        }\n        const { timings: databaseReportedTimings, data, plans, error: databaseError } = transactionResult;\n        isTransactionFinished = true;\n\n        let timings;\n        if (options.recordTimings && databaseReportedTimings) {\n            const dbConnectionTotal = getPreciseTime() - dbStartTime;\n            const queuing = watch.timings.queuing;\n            const socketInit = watch.timings.socketInit || 0;\n            const lookup = watch.timings.lookup || 0;\n            const connecting = watch.timings.connecting || 0;\n            const receiving = watch.timings.receiving;\n            const waiting = watch.timings.waiting;\n            const other = watch.timings.total - queuing - socketInit - lookup - connecting - receiving - waiting;\n            const dbInternalTotal = objectValues<number>(databaseReportedTimings).reduce((a, b) => a + b, 0);\n            timings = {\n                dbConnection: {\n                    queuing,\n                    socketInit,\n                    lookup,\n                    connecting,\n                    waiting,\n                    receiving,\n                    other,\n                    total: dbConnectionTotal\n                },\n                database: {\n                    ...databaseReportedTimings,\n                    total: dbInternalTotal\n                }\n            };\n        }\n\n        return {\n            timings,\n            data,\n            plans,\n            databaseError,\n            stats: transactionResult.stats,\n            hasTimedOut,\n            wasCancelled\n        };\n    }\n\n    /**\n     * Compares the model with the database and determines migrations to do\n     */\n    async getOutstandingMigrations(model: Model): Promise<ReadonlyArray<SchemaMigration>> {\n        return this.analyzer.getOutstandingMigrations(model);\n    }\n\n    /**\n     * Performs a single mutation\n     */\n    async performMigration(migration: SchemaMigration): Promise<void> {\n        await this.migrationPerformer.performMigration(migration);\n    }\n\n    /**\n     * Performs schema migration as configured with autocreateIndices/autoremoveIndices\n     */\n    async updateSchema(model: Model): Promise<void> {\n        const migrations = await this.getOutstandingMigrations(model);\n        const skippedMigrations: SchemaMigration[] = [];\n        for (const migration of migrations) {\n            if (!migration.isMandatory && !this.doNonMandatoryMigrations) {\n                this.logger.debug(`Skipping migration \"${migration.description}\" because of configuration`);\n                skippedMigrations.push(migration);\n                continue;\n            }\n            try {\n                this.logger.info(`Performing migration \"${migration.description}\"`);\n                await this.performMigration(migration);\n                this.logger.info(`Successfully performed migration \"${migration.description}\"`);\n            } catch (e) {\n                this.logger.error(`Error performing migration \"${migration.description}\": ${e.stack}`);\n                throw e;\n            }\n        }\n    }\n\n    async getArangoDBVersion(): Promise<ArangoDBVersion | undefined> {\n        return this.versionHelper.getArangoDBVersion();\n    }\n\n    async tokenizeExpressions(\n        tokenizations: ReadonlyArray<FlexSearchTokenizable>\n    ): Promise<ReadonlyArray<FlexSearchTokenization>> {\n        const tokenizationsFiltered = tokenizations.filter(\n            (value, index) =>\n                !tokenizations.some(\n                    (value2, index2) =>\n                        value.expression === value2.expression && value.analyzer === value2.analyzer && index > index2\n                )\n        );\n\n        const cursor = await this.db.query(generateTokenizationQuery(tokenizationsFiltered));\n\n        const result = await cursor.next();\n        const resultArray: FlexSearchTokenization[] = [];\n        for (let i = 0; i < tokenizationsFiltered.length; i++) {\n            resultArray.push({\n                expression: tokenizationsFiltered[i].expression,\n                analyzer: tokenizationsFiltered[i].analyzer,\n                tokens: result['token_' + i]\n            });\n        }\n\n        return resultArray;\n    }\n}\n\nfunction sumUpValues(objects: ReadonlyArray<{ readonly [key: string]: number }>): { readonly [key: string]: number } {\n    const result: { [key: string]: number } = {};\n    for (const obj of objects) {\n        for (const key of Object.keys(obj)) {\n            if (Number.isFinite(obj[key])) {\n                if (key in result && Number.isFinite(result[key])) {\n                    result[key] += obj[key];\n                } else {\n                    result[key] = obj[key];\n                }\n            }\n        }\n    }\n    return result;\n}\n"], "fixing_code": ["import { AggregationOperator, Field, Relation, RootEntityType } from '../../model';\nimport { FieldSegment, RelationSegment } from '../../model/implementation/collect-path';\nimport { IDENTITY_ANALYZER } from '../../model/implementation/flex-search';\nimport {\n    AddEdgesQueryNode,\n    AggregationQueryNode,\n    BasicType,\n    BinaryOperationQueryNode,\n    BinaryOperator,\n    BinaryOperatorWithAnalyzer,\n    ConcatListsQueryNode,\n    ConditionalQueryNode,\n    ConfirmForBillingQueryNode,\n    ConstBoolQueryNode,\n    ConstIntQueryNode,\n    CountQueryNode,\n    CreateBillingEntityQueryNode,\n    CreateEntitiesQueryNode,\n    CreateEntityQueryNode,\n    DeleteEntitiesQueryNode,\n    DeleteEntitiesResultValue,\n    DynamicPropertyAccessQueryNode,\n    EdgeIdentifier,\n    EntitiesIdentifierKind,\n    EntitiesQueryNode,\n    EntityFromIdQueryNode,\n    FieldPathQueryNode,\n    FieldQueryNode,\n    FirstOfListQueryNode,\n    FollowEdgeQueryNode,\n    ListItemQueryNode,\n    ListQueryNode,\n    LiteralQueryNode,\n    MergeObjectsQueryNode,\n    NullQueryNode,\n    ObjectEntriesQueryNode,\n    ObjectQueryNode,\n    OperatorWithAnalyzerQueryNode,\n    OrderDirection,\n    OrderSpecification,\n    PartialEdgeIdentifier,\n    PropertyAccessQueryNode,\n    QueryNode,\n    QueryResultValidator,\n    RemoveEdgesQueryNode,\n    RevisionQueryNode,\n    RootEntityIDQueryNode,\n    RUNTIME_ERROR_CODE_PROPERTY,\n    RUNTIME_ERROR_TOKEN,\n    RuntimeErrorQueryNode,\n    SafeListQueryNode,\n    SetEdgeQueryNode,\n    SetFieldQueryNode,\n    TransformListQueryNode,\n    TraversalQueryNode,\n    TypeCheckQueryNode,\n    UnaryOperationQueryNode,\n    UnaryOperator,\n    UpdateEntitiesQueryNode,\n    VariableAssignmentQueryNode,\n    VariableQueryNode,\n    WithPreExecutionQueryNode\n} from '../../query-tree';\nimport {\n    FlexSearchComplexOperatorQueryNode,\n    FlexSearchFieldExistsQueryNode,\n    FlexSearchQueryNode,\n    FlexSearchStartsWithQueryNode\n} from '../../query-tree/flex-search';\nimport { Quantifier, QuantifierFilterNode } from '../../query-tree/quantifiers';\nimport { extractVariableAssignments, simplifyBooleans } from '../../query-tree/utils';\nimport { not } from '../../schema-generation/utils/input-types';\nimport { Constructor, decapitalize } from '../../utils/utils';\nimport { FlexSearchTokenizable } from '../database-adapter';\nimport { analyzeLikePatternPrefix } from '../like-helpers';\nimport { aql, AQLCompoundQuery, AQLFragment, AQLQueryResultVariable, AQLVariable } from './aql';\nimport { billingCollectionName, getCollectionNameForRelation, getCollectionNameForRootEntity } from './arango-basics';\nimport { getFlexSearchViewNameForRootEntity } from './schema-migration/arango-search-helpers';\n\nenum AccessType {\n    READ,\n    WRITE\n}\n\nclass QueryContext {\n    private variableMap = new Map<VariableQueryNode, AQLFragment>();\n    private preExecQueries: AQLCompoundQuery[] = [];\n    private readAccessedCollections = new Set<string>();\n    private writeAccessedCollections = new Set<string>();\n    private extensions: Map<unknown, unknown> | undefined;\n\n    /**\n     * Creates a new QueryContext with an independent variable map except that all query result variables of this\n     * context are available.\n     */\n    private newPreExecContext(): QueryContext {\n        const newContext = new QueryContext();\n        this.variableMap.forEach((aqlVar, varNode) => {\n            if (aqlVar instanceof AQLQueryResultVariable) {\n                newContext.variableMap.set(varNode, aqlVar);\n            }\n        });\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        return newContext;\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     * @param variableNode the variable token as it is referenced in the query tree\n     * @param aqlVariable the variable token as it will be available within the AQL fragment\n     */\n    private newNestedContextWithVariableMapping(\n        variableNode: VariableQueryNode,\n        aqlVariable: AQLFragment\n    ): QueryContext {\n        const newContext = new QueryContext();\n        newContext.variableMap = new Map(this.variableMap);\n        newContext.variableMap.set(variableNode, aqlVariable);\n        newContext.preExecQueries = this.preExecQueries;\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        return newContext;\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     *\n     * The AQLFragment for the variable will be available via getVariable().\n     *\n     * @param {VariableQueryNode} variableNode the variable as referenced in the query tree\n     * @returns {QueryContext} the nested context\n     */\n    introduceVariable(variableNode: VariableQueryNode): QueryContext {\n        if (this.variableMap.has(variableNode)) {\n            throw new Error(`Variable ${variableNode} is introduced twice`);\n        }\n        const variable = new AQLVariable(variableNode.label);\n        return this.newNestedContextWithVariableMapping(variableNode, variable);\n    }\n\n    /**\n     * Creates a new QueryContext that is identical to this one but has one additional variable binding\n     *\n     * @param variableNode the variable as referenced in the query tree\n     * @param existingVariable a variable that has been previously introduced with introduceVariable() and fetched by getVariable\n     * @returns {QueryContext} the nested context\n     */\n    introduceVariableAlias(variableNode: VariableQueryNode, existingVariable: AQLFragment): QueryContext {\n        return this.newNestedContextWithVariableMapping(variableNode, existingVariable);\n    }\n\n    /**\n     * Creates a new QueryContext that includes an additional transaction step and adds resultVariable to the scope\n     * which will contain the result of the query\n     *\n     * The preExecQuery is evaluated in an independent context that has access to all previous preExecQuery result\n     * variables.\n     *\n     * @param preExecQuery the query to execute as transaction step\n     * @param resultVariable the variable to store the query result\n     * @param resultValidator an optional validator for the query result\n     */\n    addPreExecuteQuery(\n        preExecQuery: QueryNode,\n        resultVariable?: VariableQueryNode,\n        resultValidator?: QueryResultValidator\n    ): QueryContext {\n        let resultVar: AQLQueryResultVariable | undefined;\n        let newContext: QueryContext;\n        if (resultVariable) {\n            resultVar = new AQLQueryResultVariable(resultVariable.label);\n            newContext = this.newNestedContextWithVariableMapping(resultVariable, resultVar);\n        } else {\n            resultVar = undefined;\n            newContext = this;\n        }\n\n        const aqlQuery = createAQLCompoundQuery(preExecQuery, resultVar, resultValidator, this.newPreExecContext());\n\n        this.preExecQueries.push(aqlQuery);\n        return newContext;\n    }\n\n    /**\n     * Adds the information (in-place) that a collection is accessed\n     */\n    addCollectionAccess(collection: string, accessType: AccessType): void {\n        switch (accessType) {\n            case AccessType.READ:\n                this.readAccessedCollections.add(collection);\n                break;\n            case AccessType.WRITE:\n                this.writeAccessedCollections.add(collection);\n                break;\n        }\n    }\n\n    withExtension(key: unknown, value: unknown): QueryContext {\n        const newContext = new QueryContext();\n        newContext.variableMap = this.variableMap;\n        newContext.readAccessedCollections = this.readAccessedCollections;\n        newContext.writeAccessedCollections = this.writeAccessedCollections;\n        newContext.extensions = new Map([...(this.extensions ? this.extensions.entries() : []), [key, value]]);\n        return newContext;\n    }\n\n    getExtension(key: unknown): unknown {\n        if (!this.extensions) {\n            return undefined;\n        }\n        return this.extensions.get(key);\n    }\n\n    /**\n     * Gets an AQLFragment that evaluates to the value of a variable in the current scope\n     */\n    getVariable(variableNode: VariableQueryNode): AQLVariable {\n        const variable = this.variableMap.get(variableNode);\n        if (!variable) {\n            throw new Error(`Variable ${variableNode.toString()} is used but not introduced`);\n        }\n        return variable;\n    }\n\n    getPreExecuteQueries(): AQLCompoundQuery[] {\n        return this.preExecQueries;\n    }\n\n    getReadAccessedCollections(): string[] {\n        return Array.from(this.readAccessedCollections);\n    }\n\n    getWriteAccessedCollections(): string[] {\n        return Array.from(this.writeAccessedCollections);\n    }\n}\n\nfunction createAQLCompoundQuery(\n    node: QueryNode,\n    resultVariable: AQLQueryResultVariable | undefined,\n    resultValidator: QueryResultValidator | undefined,\n    context: QueryContext\n): AQLCompoundQuery {\n    // move LET statements up\n    // they often occur for value objects / entity extensions\n    // this avoids the FIRST() and the subquery which reduces load on the AQL query optimizer\n    let variableAssignments: AQLFragment[] = [];\n    const variableAssignmentNodes: VariableAssignmentQueryNode[] = [];\n    node = extractVariableAssignments(node, variableAssignmentNodes);\n    for (const assignmentNode of variableAssignmentNodes) {\n        context = context.introduceVariable(assignmentNode.variableNode);\n        const tmpVar = context.getVariable(assignmentNode.variableNode);\n        variableAssignments.push(aql`LET ${tmpVar} = ${processNode(assignmentNode.variableValueNode, context)}`);\n    }\n\n    const aqlQuery = aql.lines(...variableAssignments, aql`RETURN ${processNode(node, context)}`);\n    const preExecQueries = context.getPreExecuteQueries();\n    const readAccessedCollections = context.getReadAccessedCollections();\n    const writeAccessedCollections = context.getWriteAccessedCollections();\n\n    return new AQLCompoundQuery(\n        preExecQueries,\n        aqlQuery,\n        resultVariable,\n        resultValidator,\n        readAccessedCollections,\n        writeAccessedCollections\n    );\n}\n\ntype NodeProcessor<T extends QueryNode> = (node: T, context: QueryContext) => AQLFragment;\n\nconst inFlexSearchFilterSymbol = Symbol('inFlexSearchFilter');\n\nnamespace aqlExt {\n    export function safeJSONKey(key: string): AQLFragment {\n        if (aql.isSafeIdentifier(key)) {\n            // we could always collide with a (future) keyword, so use \"name\" syntax instead of identifier\n            // (\"\" looks more natural than `` in json keys)\n            return aql`${aql.string(key)}`;\n        } else {\n            return aql`${key}`; // fall back to bound values\n        }\n    }\n\n    export function parenthesizeList(...content: AQLFragment[]): AQLFragment {\n        return aql.lines(aql`(`, aql.indent(aql.lines(...content)), aql`)`);\n    }\n\n    export function parenthesizeObject(...content: AQLFragment[]): AQLFragment {\n        return aql`FIRST${parenthesizeList(...content)}`;\n    }\n}\n\nconst processors = new Map<Constructor<QueryNode>, NodeProcessor<QueryNode>>();\n\nfunction register<T extends QueryNode>(type: Constructor<T>, processor: NodeProcessor<T>) {\n    processors.set(type, processor as NodeProcessor<QueryNode>); // probably some bivariancy issue\n}\n\nregister(LiteralQueryNode, node => {\n    return aql.value(node.value);\n});\n\nregister(NullQueryNode, () => {\n    return aql`null`;\n});\n\nregister(RuntimeErrorQueryNode, node => {\n    const runtimeErrorToken = aql.code(RUNTIME_ERROR_TOKEN);\n    if (node.code) {\n        const codeProp = aql.code(RUNTIME_ERROR_CODE_PROPERTY);\n        return aql`{ ${codeProp}: ${node.code}, ${runtimeErrorToken}: ${node.message} }`;\n    }\n    return aql`{ ${runtimeErrorToken}: ${node.message} }`;\n});\n\nregister(ConstBoolQueryNode, node => {\n    return node.value ? aql`true` : aql`false`;\n});\n\nregister(ConstIntQueryNode, node => {\n    return aql.integer(node.value);\n});\n\nregister(ObjectQueryNode, (node, context) => {\n    if (!node.properties.length) {\n        return aql`{}`;\n    }\n\n    const properties = node.properties.map(\n        p => aql`${aqlExt.safeJSONKey(p.propertyName)}: ${processNode(p.valueNode, context)}`\n    );\n    return aql.lines(aql`{`, aql.indent(aql.join(properties, aql`,\\n`)), aql`}`);\n});\n\nregister(ListQueryNode, (node, context) => {\n    if (!node.itemNodes.length) {\n        return aql`[]`;\n    }\n\n    return aql.lines(\n        aql`[`,\n        aql.indent(\n            aql.join(\n                node.itemNodes.map(itemNode => processNode(itemNode, context)),\n                aql`,\\n`\n            )\n        ),\n        aql`]`\n    );\n});\n\nregister(ConcatListsQueryNode, (node, context) => {\n    const listNodes = node.listNodes.map(node => processNode(node, context));\n    const listNodeStr = aql.join(listNodes, aql`, `);\n    // note: UNION just appends, there is a special UNION_DISTINCT to filter out duplicates\n    return aql`UNION(${listNodeStr})`;\n});\n\nregister(VariableQueryNode, (node, context) => {\n    return context.getVariable(node);\n});\n\nregister(VariableAssignmentQueryNode, (node, context) => {\n    const newContext = context.introduceVariable(node.variableNode);\n    const tmpVar = newContext.getVariable(node.variableNode);\n\n    // note that we have to know statically if the context var is a list or an object\n    // assuming object here because lists are not needed currently\n    return aqlExt.parenthesizeObject(\n        aql`LET ${tmpVar} = ${processNode(node.variableValueNode, newContext)}`,\n        aql`RETURN ${processNode(node.resultNode, newContext)}`\n    );\n});\n\nregister(WithPreExecutionQueryNode, (node, context) => {\n    let currentContext = context;\n    for (const preExecParm of node.preExecQueries) {\n        currentContext = currentContext.addPreExecuteQuery(\n            preExecParm.query,\n            preExecParm.resultVariable,\n            preExecParm.resultValidator\n        );\n    }\n\n    return aql`${processNode(node.resultNode, currentContext)}`;\n});\n\nregister(EntityFromIdQueryNode, (node, context) => {\n    const collection = getCollectionForType(node.rootEntityType, AccessType.READ, context);\n    return aql`DOCUMENT(${collection}, ${processNode(node.idNode, context)})`;\n});\n\nregister(PropertyAccessQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getPropertyAccessFragment(node.propertyName)}`;\n});\n\nregister(FieldQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getPropertyAccessFragment(node.field.name)}`;\n});\n\nregister(DynamicPropertyAccessQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}[${processNode(node.propertyNode, context)}]`;\n});\n\nregister(FieldPathQueryNode, (node, context) => {\n    const object = processNode(node.objectNode, context);\n    return aql`${object}${getFieldPathAccessFragment(node.path)}`;\n});\n\nfunction getPropertyAccessFragment(propertyName: string) {\n    if (aql.isSafeIdentifier(propertyName)) {\n        return aql`.${aql.identifier(propertyName)}`;\n    }\n    // fall back to bound values. do not attempt aql.string for security reasons - should not be the case normally, anyway.\n    return aql`[${propertyName}]`;\n}\n\nfunction getFieldPathAccessFragment(path: ReadonlyArray<Field>): AQLFragment {\n    if (path.length > 0) {\n        const [head, ...tail] = path;\n        return aql`${getPropertyAccessFragment(head.name)}${getFieldPathAccessFragment(tail)}`;\n    } else {\n        return aql``;\n    }\n}\n\nregister(RootEntityIDQueryNode, (node, context) => {\n    return aql`${processNode(node.objectNode, context)}._key`; // ids are stored in _key field\n});\n\nregister(RevisionQueryNode, (node, context) => {\n    return aql`${processNode(node.objectNode, context)}._rev`;\n});\n\nregister(FlexSearchQueryNode, (node, context) => {\n    let itemContext = context.introduceVariable(node.itemVariable).withExtension(inFlexSearchFilterSymbol, true);\n    const viewName = getFlexSearchViewNameForRootEntity(node.rootEntityType!);\n    context.addCollectionAccess(viewName, AccessType.READ);\n    return aqlExt.parenthesizeList(\n        aql`FOR ${itemContext.getVariable(node.itemVariable)}`,\n        aql`IN ${aql.collection(viewName)}`,\n        aql`SEARCH ${processNode(node.flexFilterNode, itemContext)}`,\n        node.isOptimisationsDisabled ? aql`OPTIONS { conditionOptimization: 'none' }` : aql``,\n        aql`RETURN ${itemContext.getVariable(node.itemVariable)}`\n    );\n});\n\nregister(TransformListQueryNode, (node, context) => {\n    let itemContext = context.introduceVariable(node.itemVariable);\n    const itemVar = itemContext.getVariable(node.itemVariable);\n    let itemProjectionContext = itemContext;\n\n    // move LET statements up\n    // they often occur for value objects / entity extensions\n    // this avoids the FIRST() and the subquery which reduces load on the AQL query optimizer\n    let variableAssignments: AQLFragment[] = [];\n    let innerNode = node.innerNode;\n    const variableAssignmentNodes: VariableAssignmentQueryNode[] = [];\n    innerNode = extractVariableAssignments(innerNode, variableAssignmentNodes);\n    for (const assignmentNode of variableAssignmentNodes) {\n        itemProjectionContext = itemProjectionContext.introduceVariable(assignmentNode.variableNode);\n        const tmpVar = itemProjectionContext.getVariable(assignmentNode.variableNode);\n        variableAssignments.push(\n            aql`LET ${tmpVar} = ${processNode(assignmentNode.variableValueNode, itemProjectionContext)}`\n        );\n    }\n\n    return aqlExt.parenthesizeList(\n        aql`FOR ${itemVar}`,\n        generateInClauseWithFilterAndOrderAndLimit({ node, context, itemContext, itemVar }),\n        ...variableAssignments,\n        aql`RETURN ${processNode(innerNode, itemProjectionContext)}`\n    );\n});\n\n/**\n * Generates an IN... clause for a TransformListQueryNode to be used within a query / subquery (FOR ... IN ...)\n */\nfunction generateInClauseWithFilterAndOrderAndLimit({\n    node,\n    context,\n    itemVar,\n    itemContext\n}: {\n    node: TransformListQueryNode;\n    context: QueryContext;\n    itemVar: AQLVariable;\n    itemContext: QueryContext;\n}) {\n    let list: AQLFragment;\n    let filterDanglingEdges = aql``;\n    if (node.listNode instanceof FollowEdgeQueryNode) {\n        list = getSimpleFollowEdgeFragment(node.listNode, context);\n        filterDanglingEdges = aql`FILTER ${itemVar} != null`;\n    } else {\n        list = processNode(node.listNode, context);\n    }\n    let filter = simplifyBooleans(node.filterNode);\n\n    let limitClause;\n    if (node.maxCount != undefined) {\n        if (node.skip === 0) {\n            limitClause = aql`LIMIT ${node.maxCount}`;\n        } else {\n            limitClause = aql`LIMIT ${node.skip}, ${node.maxCount}`;\n        }\n    } else if (node.skip > 0) {\n        limitClause = aql`LIMIT ${node.skip}, ${Number.MAX_SAFE_INTEGER}`;\n    } else {\n        limitClause = aql``;\n    }\n\n    return aql.lines(\n        aql`IN ${list}`,\n        filter instanceof ConstBoolQueryNode && filter.value ? aql`` : aql`FILTER ${processNode(filter, itemContext)}`,\n        filterDanglingEdges,\n        generateSortAQL(node.orderBy, itemContext),\n        limitClause\n    );\n}\n\n/**\n * Generates an IN... clause for a list to be used within a query / subquery (FOR ... IN ...)\n */\nfunction generateInClause(node: QueryNode, context: QueryContext, entityVar: AQLFragment) {\n    if (node instanceof TransformListQueryNode && node.innerNode === node.itemVariable) {\n        const itemContext = context.introduceVariableAlias(node.itemVariable, entityVar);\n        return generateInClauseWithFilterAndOrderAndLimit({ node, itemContext, itemVar: entityVar, context });\n    }\n\n    return aql`IN ${processNode(node, context)}`;\n}\n\nregister(CountQueryNode, (node, context) => {\n    if (node.listNode instanceof FieldQueryNode || node.listNode instanceof EntitiesQueryNode) {\n        // These cases are known to be optimized\n        // TODO this does not catch the safe-list case (list ? list : []), where we could optimize to (list ? LENGTH(list) : 0)\n        // so we probably need to add an optimization to the query tree builder\n        return aql`LENGTH(${processNode(node.listNode, context)})`;\n    }\n\n    // in the general case (mostly a TransformListQueryNode), it is better to use the COLLeCT WITH COUNT syntax\n    // because it avoids building the whole collection temporarily in memory\n    // however, https://docs.arangodb.com/3.2/AQL/Examples/Counting.html does not really mention this case, so we\n    // should evaluate it again\n    // note that ArangoDB's inline-subqueries rule optimizes for the case where listNode is a TransformList again.\n    const itemVar = aql.variable('item');\n    const countVar = aql.variable('count');\n    return aqlExt.parenthesizeObject(\n        aql`FOR ${itemVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        aql`COLLECT WITH COUNT INTO ${countVar}`,\n        aql`RETURN ${countVar}`\n    );\n});\n\nregister(AggregationQueryNode, (node, context) => {\n    const itemVar = aql.variable('item');\n    const aggregationVar = aql.variable(node.operator.toLowerCase());\n    let aggregationFunction: AQLFragment | undefined;\n    let filterFrag: AQLFragment | undefined;\n    let itemFrag = itemVar;\n    let resultFragment = aggregationVar;\n    let isList = false;\n    let distinct = false;\n    let sort = false;\n    switch (node.operator) {\n        case AggregationOperator.MIN:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`MIN`;\n            break;\n        case AggregationOperator.MAX:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`MAX`;\n            break;\n        case AggregationOperator.SUM:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`SUM`;\n            resultFragment = aql`${resultFragment} != null ? ${resultFragment} : 0`; // SUM([]) === 0\n            break;\n        case AggregationOperator.AVERAGE:\n            filterFrag = aql`${itemVar} != null`;\n            aggregationFunction = aql`AVERAGE`;\n            break;\n\n        case AggregationOperator.COUNT:\n            aggregationFunction = aql`COUNT`;\n            break;\n        case AggregationOperator.SOME:\n            aggregationFunction = aql`COUNT`;\n            resultFragment = aql`${resultFragment} > 0`;\n            break;\n        case AggregationOperator.NONE:\n            aggregationFunction = aql`COUNT`;\n            resultFragment = aql`${resultFragment} == 0`;\n            break;\n\n        // using MAX >= true in place of SOME\n        //   and MAX <  true in place of NONE\n        // (basically, MAX is similar to SOME, and NONE is !SOME. Can't use MIN for EVERY because MIN([]) = null.)\n        case AggregationOperator.SOME_NULL:\n            itemFrag = aql`${itemFrag} == null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.SOME_NOT_NULL:\n            itemFrag = aql`${itemFrag} != null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.NONE_NULL:\n            itemFrag = aql`${itemFrag} == null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.EVERY_NULL:\n            // -> NONE_NOT_NULL\n            itemFrag = aql`${itemFrag} != null`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.COUNT_NULL:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} == null`;\n            break;\n        case AggregationOperator.COUNT_NOT_NULL:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} != null`;\n            break;\n\n        // these treat NULL like FALSE, so don't filter them away\n        // using MAX >= true in place of SOME\n        //   and MAX <  true in place of NONE\n        // (basically, MAX is similar to SOME, and NONE is !SOME. Can't use MIN for EVERY because MIN([]) = null.)\n        case AggregationOperator.SOME_TRUE:\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.SOME_NOT_TRUE:\n            itemFrag = aql`!${itemFrag}`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} >= true`;\n            break;\n        case AggregationOperator.EVERY_TRUE:\n            // -> NONE_NOT_TRUE\n            itemFrag = aql`!${itemFrag}`;\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.NONE_TRUE:\n            aggregationFunction = aql`MAX`;\n            resultFragment = aql`${resultFragment} < true`;\n            break;\n        case AggregationOperator.COUNT_TRUE:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} >= true`;\n            break;\n        case AggregationOperator.COUNT_NOT_TRUE:\n            aggregationFunction = aql`COUNT`;\n            filterFrag = aql`${itemVar} < true`;\n            break;\n\n        // these should also remove NULL values by definition\n        case AggregationOperator.DISTINCT:\n            // use COLLECT a = a instead of RETURN DISTINCT to be able to sort\n            distinct = true;\n            filterFrag = aql`${itemVar} != null`;\n            isList = true;\n            sort = node.sort;\n            break;\n\n        case AggregationOperator.COUNT_DISTINCT:\n            aggregationFunction = aql`COUNT_DISTINCT`;\n            filterFrag = aql`${itemVar} != null`;\n            break;\n\n        default:\n            throw new Error(`Unsupported aggregator: ${(node as any).aggregationOperator}`);\n    }\n    return aqlExt[isList ? 'parenthesizeList' : 'parenthesizeObject'](\n        aql`FOR ${itemVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        filterFrag ? aql`FILTER ${filterFrag}` : aql``,\n        sort ? aql`SORT ${itemVar}` : aql``,\n        aggregationFunction\n            ? aql`COLLECT AGGREGATE ${aggregationVar} = ${aggregationFunction}(${itemFrag})`\n            : distinct\n            ? aql`COLLECT ${aggregationVar} = ${itemFrag}`\n            : aql``,\n        aql`RETURN ${resultFragment}`\n    );\n});\n\nregister(MergeObjectsQueryNode, (node, context) => {\n    const objectList = node.objectNodes.map(node => processNode(node, context));\n    const objectsFragment = aql.join(objectList, aql`, `);\n    return aql`MERGE(${objectsFragment})`;\n});\n\nregister(ObjectEntriesQueryNode, (node, context) => {\n    const objectVar = aql.variable('object');\n    const keyVar = aql.variable('key');\n    return aqlExt.parenthesizeList(\n        aql`LET ${objectVar} = ${processNode(node.objectNode, context)}`,\n        aql`FOR ${keyVar} IN IS_DOCUMENT(${objectVar}) ? ATTRIBUTES(${objectVar}) : []`,\n        aql`RETURN [ ${keyVar}, ${objectVar}[${keyVar}] ]`\n    );\n});\n\nregister(FirstOfListQueryNode, (node, context) => {\n    return aql`FIRST(${processNode(node.listNode, context)})`;\n});\n\nregister(ListItemQueryNode, (node, context) => {\n    return aql`(${processNode(node.listNode, context)})[${node.index}]`;\n});\n\nregister(BinaryOperationQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n\n    // a > NULL is equivalent to a != NULL, and it can use indices better\n    // (but don't do it in flexsearch, there > NULL is something different from != NULL\n    if (\n        node.operator === BinaryOperator.UNEQUAL &&\n        (node.rhs instanceof NullQueryNode || (node.rhs instanceof LiteralQueryNode && node.rhs.value == undefined)) &&\n        !context.getExtension(inFlexSearchFilterSymbol)\n    ) {\n        return aql`(${lhs} > NULL)`;\n    }\n\n    const rhs = processNode(node.rhs, context);\n    const op = getAQLOperator(node.operator);\n    if (op) {\n        return aql`(${lhs} ${op} ${rhs})`;\n    }\n\n    switch (node.operator) {\n        case BinaryOperator.CONTAINS:\n            return aql`(${lhs} LIKE CONCAT(\"%\", ${rhs}, \"%\"))`;\n        case BinaryOperator.STARTS_WITH:\n            const slowFrag = aql`(LEFT(${lhs}, LENGTH(${rhs})) == ${rhs})`;\n            if (node.rhs instanceof LiteralQueryNode && typeof node.rhs.value === 'string') {\n                const fastFrag = getFastStartsWithQuery(lhs, node.rhs.value);\n                // still ned to use the slow frag to get case sensitiveness\n                // this is really bad for performance, see explanation in LIKE branch below\n                return aql`${fastFrag} && ${slowFrag}`;\n            }\n            return slowFrag;\n        case BinaryOperator.ENDS_WITH:\n            return aql`(RIGHT(${lhs}, LENGTH(${rhs})) == ${rhs})`;\n        case BinaryOperator.LIKE:\n            const slowLikeFrag = aql`LIKE(${lhs}, ${rhs}, true)`; // true: caseInsensitive\n            if (node.rhs instanceof LiteralQueryNode && typeof node.rhs.value === 'string') {\n                const { literalPrefix, isSimplePrefixPattern, isLiteralPattern } = analyzeLikePatternPrefix(\n                    node.rhs.value\n                );\n\n                if (isLiteralPattern) {\n                    return getEqualsIgnoreCaseQuery(lhs, literalPrefix);\n                }\n\n                const fastFrag = getFastStartsWithQuery(lhs, literalPrefix);\n                if (isSimplePrefixPattern) {\n                    // we can optimize the whole LIKE away and use a skiplist-index-optimizable range select\n                    return fastFrag;\n                }\n                // we can at least use the prefix search to narrow down the results\n                // however, this is way worse because we lose the ability to sort-and-then-limit using the same index\n                // -> queries with a \"first\" argument suddenly have the time complexity of the pre-limited\n                // (or even pre-filtered if the database decides to use the index for sorting) result size instead of\n                // being in O(first).\n                return aql`(${fastFrag} && ${slowLikeFrag})`;\n            }\n            return slowLikeFrag;\n        case BinaryOperator.APPEND:\n            return aql`CONCAT(${lhs}, ${rhs})`;\n        case BinaryOperator.PREPEND:\n            return aql`CONCAT(${rhs}, ${lhs})`;\n        case BinaryOperator.SUBTRACT_LISTS:\n            return aql`MINUS(${lhs}, ${rhs})`;\n        default:\n            throw new Error(`Unsupported binary operator: ${op}`);\n    }\n});\n\nregister(OperatorWithAnalyzerQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n    const rhs = processNode(node.rhs, context);\n    const analyzer = node.analyzer;\n\n    const isIdentityAnalyzer = !node.analyzer || node.analyzer === IDENTITY_ANALYZER;\n    // some operators support case-converting analyzers (like norm_ci) which only generate one token\n    const normalizedRhs = isIdentityAnalyzer ? rhs : aql`TOKENS(${rhs}, ${analyzer})[0]`;\n\n    switch (node.operator) {\n        case BinaryOperatorWithAnalyzer.EQUAL:\n            return aql`ANALYZER( ${lhs} == ${normalizedRhs},${analyzer})`;\n        case BinaryOperatorWithAnalyzer.UNEQUAL:\n            return aql`ANALYZER( ${lhs} != ${normalizedRhs},${analyzer})`;\n        case BinaryOperatorWithAnalyzer.IN:\n            if (isIdentityAnalyzer) {\n                return aql`(${lhs} IN ${rhs})`;\n            }\n            const loopVar = aql.variable(`token`);\n            return aql`ANALYZER( ${lhs} IN ( FOR ${loopVar} IN TOKENS(${rhs} , ${analyzer}) RETURN ${loopVar}[0] ), ${analyzer} )`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_ANY_WORD:\n            return aql`ANALYZER( ${lhs} IN TOKENS(${rhs}, ${analyzer}),${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_PREFIX:\n            // can't pass NULL to STARTS_WITH (generates an error)\n            // if an expression does not have a token, nothing can contain a prefix thereof, so we don't find anything\n            // this is also good behavior in case of searching because you just find nothing if you type special chars\n            // instead of finding everything\n            return aql`(LENGTH(TOKENS(${rhs},${analyzer})) ? ANALYZER( STARTS_WITH( ${lhs}, TOKENS(${rhs},${analyzer})[0]), ${analyzer}) : false)`;\n        case BinaryOperatorWithAnalyzer.FLEX_SEARCH_CONTAINS_PHRASE:\n            return aql`ANALYZER( PHRASE( ${lhs}, ${rhs}), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_LESS_THAN:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${''} , ${normalizedRhs}, true, false), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_LESS_THAN_OR_EQUAL:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${''} , ${normalizedRhs}, true, true), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_GREATER_THAN:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${normalizedRhs}, ${String.fromCodePoint(\n                0x10ffff\n            )}, false, true), ${analyzer})`;\n        case BinaryOperatorWithAnalyzer.FLEX_STRING_GREATER_THAN_OR_EQUAL:\n            return aql`ANALYZER( IN_RANGE(${lhs}, ${normalizedRhs}, ${String.fromCodePoint(\n                0x10ffff\n            )}, true, true), ${analyzer})`;\n        default:\n            throw new Error(`Unsupported operator: ${node.operator}`);\n    }\n});\n\nregister(FlexSearchStartsWithQueryNode, (node, context) => {\n    const lhs = processNode(node.lhs, context);\n    const rhs = processNode(node.rhs, context);\n\n    if (!node.analyzer || node.analyzer === IDENTITY_ANALYZER) {\n        return aql`STARTS_WITH(${lhs}, ${rhs})`;\n    }\n\n    // This query node can be used with simple case-converting analyzers\n    // These case-converting analyzers will only ever result in one token, so we can use the first one, which\n    // is the input value case-converted.\n    return aql`ANALYZER(STARTS_WITH(${lhs}, TOKENS(${rhs},${node.analyzer})[0]), ${node.analyzer})`;\n});\n\nregister(FlexSearchFieldExistsQueryNode, (node, context) => {\n    const sourceNode = processNode(node.sourceNode, context);\n    if (node.analyzer) {\n        return aql`EXISTS(${sourceNode}, \"analyzer\", ${node.analyzer})`;\n    } else {\n        return aql`EXISTS(${sourceNode})`;\n    }\n});\n\nregister(FlexSearchComplexOperatorQueryNode, (node, context) => {\n    throw new Error(`Internal Error: FlexSearchComplexOperatorQueryNode must be expanded before generating the query.`);\n});\n\nfunction getBillingInput(\n    node: ConfirmForBillingQueryNode | CreateBillingEntityQueryNode,\n    key: string | number | AQLFragment,\n    context: QueryContext,\n    currentTimestamp: string\n) {\n    return aql`\n        key: ${key},\n        type: ${node.rootEntityTypeName},\n        category: ${processNode(node.categoryNode, context)},\n        quantity: ${processNode(node.quantityNode, context)},\n        isExported: false,\n        createdAt: ${currentTimestamp},\n        updatedAt: ${currentTimestamp}`;\n}\n\nregister(CreateBillingEntityQueryNode, (node, context) => {\n    const currentTimestamp = new Date().toISOString();\n    return aqlExt.parenthesizeList(\n        aql`UPSERT {\n            key: ${node.key},\n            type: ${node.rootEntityTypeName}\n        }`,\n        aql`INSERT {\n            ${getBillingInput(node, node.key, context, currentTimestamp)},\n            isConfirmedForExport: false\n         }`,\n        aql`UPDATE (OLD.isConfirmedForExport ? {} : {\n            updatedAt: ${currentTimestamp},\n            category: ${processNode(node.categoryNode, context)},\n            quantity: ${processNode(node.quantityNode, context)}\n        })`,\n        aql`IN ${getCollectionForBilling(AccessType.WRITE, context)}`,\n        aql`RETURN ${node.key}`\n    );\n});\n\nregister(ConfirmForBillingQueryNode, (node, context) => {\n    const key = processNode(node.keyNode, context);\n    const currentTimestamp = new Date().toISOString();\n    return aqlExt.parenthesizeList(\n        aql`UPSERT {\n            key: ${key},\n            type: ${node.rootEntityTypeName}\n        }`,\n        aql`INSERT {\n            ${getBillingInput(node, key, context, currentTimestamp)},\n            isConfirmedForExport: true,\n            confirmedForExportAt: ${currentTimestamp}\n         }`,\n        aql`UPDATE (OLD.isConfirmedForExport ? {} : {\n            isConfirmedForExport: true,\n            updatedAt: ${currentTimestamp},\n            confirmedForExportAt: ${currentTimestamp},\n            category: ${processNode(node.categoryNode, context)},\n            quantity: ${processNode(node.quantityNode, context)}\n        })`,\n        aql`IN ${getCollectionForBilling(AccessType.WRITE, context)}`,\n        aql`RETURN true`\n    );\n});\n\nfunction getFastStartsWithQuery(lhs: AQLFragment, rhsValue: string): AQLFragment {\n    if (!rhsValue.length) {\n        return aql`IS_STRING(${lhs})`;\n    }\n\n    // this works as long as the highest possible code point is also the last one in the collation\n    const maxChar = String.fromCodePoint(0x10ffff);\n    const maxStr = rhsValue + maxChar;\n\n    // UPPER is used to get the \"smallest\" representation of the value case-sensitive, LOWER for the \"largest\".\n    // the ordering looks like this:\n    // [\n    //   \"A\",\n    //   \"a\",\n    //   \"AA\",\n    //   \"Aa\",\n    //   \"aA\",\n    //   \"aa\",\n    //   \"AB\",\n    //   \"Ab\",\n    //   \"aB\",\n    //   \"ab\",\n    //   \"B\",\n    //   \"b\"\n    // ]\n    // This means that if the actual value is longer than the given prefix (i.e. it's a real prefix and not the whole\n    // string), the match will be case-insensitive. However, if the remaining suffix if empty, the search would\n    // sometimes be case-sensitive: If you search for the prefix a, A will not be found (because A < a), but a will\n    // match the prefix filter A. In order to avoid this, one needs to convert the given string to the lowest value\n    // within its case-sensitivity category. For ASCII characters, that's simply UPPER(), but that will not always be\n    // the case. The same thing applies to the upper bound.\n    return aql`(${lhs} >= UPPER(${rhsValue}) && ${lhs} < LOWER(${maxStr}))`;\n\n    // the following does not work because string sorting depends on the DB's collator\n    // which does not necessarily sort the characters by code points\n    // charCodeAt / fromCharCode works on code units, and so does the string indexer / substr / length\n    /*const lastCharCode = rhsValue.charCodeAt(rhsValue.length - 1);\n    const nextCharCode = lastCharCode + 1;\n    if (nextCharCode >= 0xD800) {\n        // don't mess with surrogate pairs\n        return undefined;\n    }\n\n    const nextValue = rhsValue.substring(0, rhsValue.length - 1) + String.fromCharCode(nextCharCode);\n    return aql`(${lhs} >= ${rhsValue} && ${lhs} < ${nextValue})`;*/\n}\n\nfunction getEqualsIgnoreCaseQuery(lhs: AQLFragment, rhsValue: string): AQLFragment {\n    // if the string e.g. only consists of digits, no need for special case sensitivity checking\n    if (isStringCaseInsensitive(rhsValue)) {\n        return aql`(${lhs} == ${aql.value(rhsValue)})`;\n    }\n\n    // w.r.t. UPPER/LOWER, see the comment in getFastStartsWithQuery\n    const lowerBoundFrag = aql`UPPER(${rhsValue})`;\n    const upperBoundFrag = aql`LOWER(${rhsValue})`;\n    return aql`(${lhs} >= ${lowerBoundFrag} && ${lhs} <= ${upperBoundFrag})`;\n}\n\nregister(UnaryOperationQueryNode, (node, context) => {\n    switch (node.operator) {\n        case UnaryOperator.NOT:\n            return aql`!(${processNode(node.valueNode, context)})`;\n        case UnaryOperator.JSON_STRINGIFY:\n            return aql`JSON_STRINGIFY(${processNode(node.valueNode, context)})`;\n        case UnaryOperator.ROUND:\n            return aql`ROUND(${processNode(node.valueNode, context)})`;\n        default:\n            throw new Error(`Unsupported unary operator: ${node.operator}`);\n    }\n});\n\nregister(ConditionalQueryNode, (node, context) => {\n    const cond = processNode(node.condition, context);\n    const expr1 = processNode(node.expr1, context);\n    const expr2 = processNode(node.expr2, context);\n    return aql`(${cond} ? ${expr1} : ${expr2})`;\n});\n\nregister(TypeCheckQueryNode, (node, context) => {\n    const value = processNode(node.valueNode, context);\n\n    switch (node.type) {\n        case BasicType.SCALAR:\n            return aql`(IS_BOOL(${value}) || IS_NUMBER(${value}) || IS_STRING(${value}))`;\n        case BasicType.LIST:\n            return aql`IS_LIST(${value})`;\n        case BasicType.OBJECT:\n            return aql`IS_OBJECT(${value})`;\n        case BasicType.NULL:\n            return aql`IS_NULL(${value})`;\n    }\n});\n\nregister(SafeListQueryNode, (node, context) => {\n    const reducedNode = new ConditionalQueryNode(\n        new TypeCheckQueryNode(node.sourceNode, BasicType.LIST),\n        node.sourceNode,\n        ListQueryNode.EMPTY\n    );\n    return processNode(reducedNode, context);\n});\n\nregister(QuantifierFilterNode, (node, context) => {\n    let { quantifier, conditionNode, listNode, itemVariable } = node;\n    conditionNode = simplifyBooleans(conditionNode);\n\n    const fastFragment = getQuantifierFilterUsingArrayComparisonOperator(\n        { quantifier, conditionNode, listNode, itemVariable },\n        context\n    );\n    if (fastFragment) {\n        return fastFragment;\n    }\n\n    // reduce 'every' to 'none' so that count-based evaluation is possible\n    if (quantifier === 'every') {\n        quantifier = 'none';\n        conditionNode = not(conditionNode);\n    }\n\n    const filteredListNode = new TransformListQueryNode({\n        listNode,\n        filterNode: conditionNode,\n        itemVariable\n    });\n\n    const finalNode = new BinaryOperationQueryNode(\n        new CountQueryNode(filteredListNode),\n        quantifier === 'none' ? BinaryOperator.EQUAL : BinaryOperator.GREATER_THAN,\n        new LiteralQueryNode(0)\n    );\n    return processNode(finalNode, context);\n});\n\n// uses the array expansion operator (https://docs.arangodb.com/3.0/AQL/Advanced/ArrayOperators.html#array-expansion)\n// that can utilize an index like \"items[*].itemNumber\" if possible\n// (specifically for something like items_some: {itemNumber: \"abc\"})\nfunction getQuantifierFilterUsingArrayComparisonOperator(\n    {\n        quantifier,\n        conditionNode,\n        listNode,\n        itemVariable\n    }: {\n        quantifier: Quantifier;\n        conditionNode: QueryNode;\n        listNode: QueryNode;\n        itemVariable: VariableQueryNode;\n    },\n    context: QueryContext\n): AQLFragment | undefined {\n    // ArangoDB supports array comparison operators (e.g. field ALL > 5)\n    // https://www.arangodb.com/docs/stable/aql/operators.html#array-comparison-operators\n    // it can be combined with the array expansion operator (e.g. items[*].field)\n    // https://docs.arangodb.com/3.0/AQL/Advanced/ArrayOperators.html#array-expansion\n    // quantifier filters with exactly one filter field that uses a comparison operator can be optimized with this\n    // this simplifies the AQL expression a lot (no filtering-then-checking-length), and also enables early pruning\n\n    // only possible on lists that are field accesses (but we can handle safe lists below)\n    let isSafeList = false;\n    if (listNode instanceof SafeListQueryNode) {\n        isSafeList = true;\n        listNode = listNode.sourceNode;\n    }\n    if (!(listNode instanceof FieldQueryNode)) {\n        return undefined;\n    }\n\n    if (!(conditionNode instanceof BinaryOperationQueryNode)) {\n        return undefined;\n    }\n\n    let operator: BinaryOperator;\n    switch (conditionNode.operator) {\n        case BinaryOperator.EQUAL:\n        case BinaryOperator.IN:\n        case BinaryOperator.UNEQUAL:\n        case BinaryOperator.LESS_THAN:\n        case BinaryOperator.LESS_THAN_OR_EQUAL:\n        case BinaryOperator.GREATER_THAN:\n        case BinaryOperator.GREATER_THAN_OR_EQUAL:\n            operator = conditionNode.operator;\n            break;\n\n        case BinaryOperator.LIKE:\n            // see if this really is a equals search so we can optimize it (only possible as long as it does not contain any case-specific characters)\n            if (!(conditionNode.rhs instanceof LiteralQueryNode) || typeof conditionNode.rhs.value !== 'string') {\n                return undefined;\n            }\n            const likePattern: string = conditionNode.rhs.value;\n            const { isLiteralPattern } = analyzeLikePatternPrefix(likePattern);\n            if (!isLiteralPattern || !isStringCaseInsensitive(likePattern)) {\n                return undefined;\n            }\n            operator = BinaryOperator.EQUAL;\n            break;\n\n        default:\n            return undefined;\n    }\n\n    let fields: Field[] = [];\n    let currentFieldNode = conditionNode.lhs;\n    while (currentFieldNode !== itemVariable) {\n        if (!(currentFieldNode instanceof FieldQueryNode)) {\n            return undefined;\n        }\n        fields.unshift(currentFieldNode.field); // we're traversing from back to front\n        currentFieldNode = currentFieldNode.objectNode;\n    }\n\n    const valueFrag = processNode(conditionNode.rhs, context);\n\n    let fieldValueFrag: AQLFragment;\n    if (fields.length) {\n        const fieldAccessFrag = aql.concat(fields.map(f => getPropertyAccessFragment(f.name)));\n        fieldValueFrag = aql`${processNode(listNode, context)}[*]${fieldAccessFrag}`;\n        // no need to use the SafeListQueryNode here because [*] already expands NULL to []\n    } else {\n        // special case: scalar list - no array expansion\n        if (isSafeList) {\n            // re-wrap in SafeListQueryNode to support generic case at the bottom\n            // \"something\" ANY IN NULL would not work (yields false instead of true)\n            // the shortcut with EQUAL / some would work though as \"something\" IN NULL is false\n            fieldValueFrag = processNode(new SafeListQueryNode(listNode), context);\n        } else {\n            fieldValueFrag = processNode(listNode, context);\n        }\n    }\n\n    // The case of \"field ANY == value\" can further be optimized into \"value IN field\" which can use an array index\n    // https://www.arangodb.com/docs/stable/indexing-index-basics.html#indexing-array-values\n    if (operator === BinaryOperator.EQUAL && quantifier === 'some') {\n        return aql`(${valueFrag} IN ${fieldValueFrag})`;\n    }\n\n    let quantifierFrag: AQLFragment;\n    switch (quantifier) {\n        case 'some':\n            quantifierFrag = aql`ANY`;\n            break;\n        case 'every':\n            quantifierFrag = aql`ALL`;\n            break;\n        case 'none':\n            quantifierFrag = aql`NONE`;\n            break;\n        default:\n            throw new Error(`Unexpected quantifier: ${quantifier}`);\n    }\n\n    const operatorFrag = getAQLOperator(operator);\n    if (!operatorFrag) {\n        throw new Error(`Unable to get AQL fragment for operator ${operator}`);\n    }\n    return aql`(${fieldValueFrag} ${quantifierFrag} ${operatorFrag} ${valueFrag})`;\n}\n\nregister(EntitiesQueryNode, (node, context) => {\n    return getCollectionForType(node.rootEntityType, AccessType.READ, context);\n});\n\nregister(FollowEdgeQueryNode, (node, context) => {\n    const tmpVar = aql.variable('node');\n    // need to wrap this in a subquery because ANY is not possible as first token of an expression node in AQL\n    return aqlExt.parenthesizeList(\n        aql`FOR ${tmpVar}`,\n        aql`IN ${getSimpleFollowEdgeFragment(node, context)}`,\n        aql`FILTER ${tmpVar} != null`,\n        aql`RETURN ${tmpVar}`\n    );\n});\n\nregister(TraversalQueryNode, (node, context) => {\n    const sourceFrag = processNode(node.sourceEntityNode, context);\n    const fieldDepth = node.fieldSegments.filter(s => s.isListSegment).length;\n\n    if (node.relationSegments.length) {\n        let mapFrag: ((itemFrag: AQLFragment) => AQLFragment) | undefined;\n\n        let remainingDepth = fieldDepth;\n        if (node.fieldSegments.length) {\n            // if we have both, it might be beneficial to do the field traversal within the mapping node\n            // because it may allow ArangoDB to figure out that only one particular field is of interest, and e.g.\n            // discard the root entities earlier\n\n            if (node.captureRootEntity) {\n                if (fieldDepth === 0) {\n                    // fieldSegments.length && fieldDepth === 0 means we only traverse through entity extensions\n                    // actually, shouldn't really occur because a collect path can't end with an entity extension and\n                    // value objects don't capture root entities\n                    // however, we can easily implement this so let's do it\n                    mapFrag = nodeFrag =>\n                        aql`{ obj: ${getFieldTraversalFragmentWithoutFlattening(\n                            node.fieldSegments,\n                            nodeFrag\n                        )}, root: ${nodeFrag}) }`;\n                } else {\n                    // the result of getFieldTraversalFragmentWithoutFlattening() now is a list, so we need to iterate\n                    // over it. if the depth is > 1, we need to flatten the deeper ones so we can do one FOR loop over them\n                    // we still return a list, so we just reduce the depth to 1 and not to 0\n                    const entityVar = aql.variable('entity');\n                    mapFrag = rootEntityFrag =>\n                        aqlExt.parenthesizeList(\n                            aql`FOR ${entityVar} IN ${getFlattenFrag(\n                                getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, rootEntityFrag),\n                                fieldDepth - 1\n                            )}`,\n                            aql`RETURN { obj: ${entityVar}, root: ${rootEntityFrag} }`\n                        );\n                    remainingDepth = 1;\n                }\n            } else {\n                mapFrag = nodeFrag => getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, nodeFrag);\n            }\n        } else {\n            if (node.captureRootEntity) {\n                // doesn't make sense to capture the root entity if we're returning the root entities\n                throw new Error(`captureRootEntity without fieldSegments detected`);\n            }\n        }\n\n        // traversal requires real ids\n        let fixedSourceFrag = sourceFrag;\n        if (node.entitiesIdentifierKind === EntitiesIdentifierKind.ID) {\n            if (node.sourceIsList) {\n                fixedSourceFrag = getFullIDFromKeysFragment(\n                    sourceFrag,\n                    node.relationSegments[0].relationSide.sourceType\n                );\n            } else {\n                fixedSourceFrag = getFullIDFromKeyFragment(\n                    sourceFrag,\n                    node.relationSegments[0].relationSide.sourceType\n                );\n            }\n        }\n\n        const frag = getRelationTraversalFragment({\n            segments: node.relationSegments,\n            sourceFrag: fixedSourceFrag,\n            sourceIsList: node.sourceIsList,\n            alwaysProduceList: node.alwaysProduceList,\n            mapFrag,\n            context\n        });\n        if (node.relationSegments.some(s => s.isListSegment) || node.sourceIsList) {\n            // if the relation contains a list segment, getRelationTraversalFragment will return a list\n            // if we already returned lists within the mapFrag (-> current value of remainingDepth), we need to add that\n            remainingDepth++;\n        }\n        // flatten 1 less than the depth, see below\n        return getFlattenFrag(frag, remainingDepth - 1);\n    }\n\n    if (node.captureRootEntity) {\n        // doesn't make sense (and isn't possible) to capture the root entity if we're not even crossing root entities\n        throw new Error(`captureRootEntity without relationSegments detected`);\n    }\n\n    if (node.sourceIsList) {\n        // don't need, don't bother\n        throw new Error(`sourceIsList without relationSegments detected`);\n    }\n\n    if (node.alwaysProduceList) {\n        // don't need, don't bother\n        throw new Error(`alwaysProduceList without relationSegments detected`);\n    }\n\n    if (node.entitiesIdentifierKind !== EntitiesIdentifierKind.ENTITY) {\n        throw new Error(`Only ENTITY identifiers supported without relationSegments`);\n    }\n\n    if (!node.fieldSegments.length) {\n        // should normally not occur\n        return sourceFrag;\n    }\n\n    // flatten 1 less than the fieldDepth:\n    // - no list segments -> evaluate to the object\n    // - one list segment -> evaluate to the list, so no flattening\n    // - two list segments -> needs flattening once to get one list\n    return getFlattenFrag(getFieldTraversalFragmentWithoutFlattening(node.fieldSegments, sourceFrag), fieldDepth - 1);\n});\n\nfunction getRelationTraversalFragment({\n    segments,\n    sourceFrag,\n    sourceIsList,\n    alwaysProduceList,\n    mapFrag,\n    context\n}: {\n    readonly segments: ReadonlyArray<RelationSegment>;\n    readonly sourceFrag: AQLFragment;\n    readonly sourceIsList: boolean;\n    readonly alwaysProduceList: boolean;\n    readonly mapFrag?: (itemFrag: AQLFragment) => AQLFragment;\n    readonly context: QueryContext;\n}) {\n    if (!segments.length) {\n        return sourceFrag;\n    }\n\n    // ArangoDB 3.4.5 introduced PRUNE which also supports IS_SAME_COLLECTION so we may be able to use just one\n    // traversal which lists all affected edge collections and prunes on the path in the future.\n\n    const forFragments: AQLFragment[] = [];\n    const sourceEntityVar = aql.variable(`sourceEntity`);\n    let currentObjectFrag = sourceIsList ? sourceEntityVar : sourceFrag;\n    let segmentIndex = 0;\n    for (const segment of segments) {\n        const nodeVar = aql.variable(`node`);\n        const edgeVar = aql.variable(`edge`);\n        const pathVar = aql.variable(`path`);\n        const dir = segment.relationSide.isFromSide ? aql`OUTBOUND` : aql`INBOUND`;\n        let filterFrag = aql``;\n        let pruneFrag = aql``;\n        if (segment.vertexFilter) {\n            if (!segment.vertexFilterVariable) {\n                throw new Error(`vertexFilter is set, but vertexFilterVariable is not`);\n            }\n            const filterContext = context.introduceVariableAlias(segment.vertexFilterVariable, nodeVar);\n            // PRUNE to stop on a node that has to be filtered out (only necessary for traversals > 1 path length)\n            // however, PRUNE only seems to be a performance feature and is not reliably evaluated\n            // (e.g. it's not when using COLLECT with distinct for some reason), so we need to add a path filter\n            if (segment.maxDepth > 1) {\n                if (\n                    !(\n                        segment.vertexFilter instanceof BinaryOperationQueryNode &&\n                        segment.vertexFilter.lhs instanceof FieldQueryNode &&\n                        segment.vertexFilter.lhs.objectNode === segment.vertexFilterVariable\n                    )\n                ) {\n                    throw new Error(`Unsupported filter pattern for graph traversal`);\n                }\n                const vertexInPathFrag = aql`${pathVar}.vertices[*]`;\n                const pathFilterContext = context.introduceVariableAlias(\n                    segment.vertexFilterVariable,\n                    vertexInPathFrag\n                );\n                const lhsFrag = processNode(segment.vertexFilter.lhs, pathFilterContext);\n                const opFrag = getAQLOperator(segment.vertexFilter.operator);\n                if (!opFrag) {\n                    throw new Error(`Unsupported filter pattern for graph traversal`);\n                }\n                const pathFilterFrag = aql`${lhsFrag} ALL ${opFrag} ${processNode(\n                    segment.vertexFilter.rhs,\n                    pathFilterContext\n                )}`;\n                filterFrag = aql`\\nFILTER ${pathFilterFrag}`;\n                pruneFrag = aql`\\nPRUNE !(${processNode(segment.vertexFilter, filterContext)})`;\n            } else {\n                // FILTER to filter out result nodes\n                filterFrag = aql`\\nFILTER ${processNode(segment.vertexFilter, filterContext)}`;\n            }\n        }\n        const traversalFrag = aql`FOR ${nodeVar}, ${edgeVar}, ${pathVar} IN ${segment.minDepth}..${\n            segment.maxDepth\n        } ${dir} ${currentObjectFrag} ${getCollectionForRelation(\n            segment.relationSide.relation,\n            AccessType.READ,\n            context\n        )}${pruneFrag}${filterFrag}`;\n        if (segment.isListSegment || (alwaysProduceList && segmentIndex === segments.length - 1)) {\n            // this is simple - we can just push one FOR statement after the other\n            forFragments.push(traversalFrag);\n            currentObjectFrag = nodeVar;\n        } else {\n            // if this is not a list, we need to preserve NULL values\n            // (actually, we don't in some cases, but we need to figure out when)\n            // to preserve null values, we need to use FIRST\n            // to ignore dangling edges, add a FILTER though (if there was one dangling edge and one real edge collected, we should use the real one)\n            const nullableVar = aql.variable(`nullableNode`);\n            forFragments.push(\n                aql`LET ${nullableVar} = FIRST(${traversalFrag} FILTER ${nodeVar} != null RETURN ${nodeVar})`\n            );\n            currentObjectFrag = nullableVar;\n        }\n\n        segmentIndex++;\n    }\n\n    const lastSegment = segments[segments.length - 1];\n\n    // remove dangling edges, unless we already did because the last segment wasn't a list segment (see above, we add the FILTER there)\n    if (lastSegment.isListSegment) {\n        forFragments.push(aql`FILTER ${currentObjectFrag} != null`);\n    }\n\n    const returnFrag = mapFrag ? mapFrag(currentObjectFrag) : currentObjectFrag;\n    const returnList = lastSegment.resultIsList || sourceIsList || alwaysProduceList;\n    // make sure we don't return a list with one element\n    return aqlExt[returnList ? 'parenthesizeList' : 'parenthesizeObject'](\n        sourceIsList ? aql`FOR ${sourceEntityVar} IN ${sourceFrag}` : aql``,\n        ...forFragments,\n        aql`RETURN ${returnFrag}`\n    );\n}\n\nfunction getFlattenFrag(listFrag: AQLFragment, depth: number) {\n    if (depth <= 0) {\n        return listFrag;\n    }\n    if (depth === 1) {\n        return aql`${listFrag}[**]`;\n    }\n    return aql`FLATTEN(${listFrag}, ${aql.integer(depth)})`;\n}\n\nfunction getFieldTraversalFragmentWithoutFlattening(segments: ReadonlyArray<FieldSegment>, sourceFrag: AQLFragment) {\n    if (!segments.length) {\n        return sourceFrag;\n    }\n\n    let frag = sourceFrag;\n    for (const segment of segments) {\n        frag = aql`${frag}${getPropertyAccessFragment(segment.field.name)}`;\n        if (segment.isListSegment) {\n            // the array expansion operator [*] does two useful things:\n            // - it performs the next field access basically as .map(o => o.fieldName).\n            // - it converts non-lists to lists (important so that if we flatten afterwards, we don't include NULL lists\n            // the latter is why we also add the [*] at the end of the expression, which might look strange in the AQL.\n            frag = aql`${frag}[*]`;\n        }\n    }\n\n    return frag;\n}\n\nregister(CreateEntityQueryNode, (node, context) => {\n    return aqlExt.parenthesizeObject(\n        aql`INSERT ${processNode(node.objectNode, context)} IN ${getCollectionForType(\n            node.rootEntityType,\n            AccessType.WRITE,\n            context\n        )}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(CreateEntitiesQueryNode, (node, context) => {\n    const entityVar = aql.variable('entity');\n    return aqlExt.parenthesizeList(\n        aql`FOR ${entityVar} IN ${processNode(node.objectsNode, context)}`,\n        aql`INSERT ${entityVar} IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(UpdateEntitiesQueryNode, (node, context) => {\n    const newContext = context.introduceVariable(node.currentEntityVariable);\n    const entityVar = newContext.getVariable(node.currentEntityVariable);\n    let entityFrag: AQLFragment;\n    let options: AQLFragment;\n    let updateFrag = processNode(new ObjectQueryNode(node.updates), newContext);\n    let additionalUpdates: ReadonlyArray<SetFieldQueryNode> = [];\n\n    if (node.revision) {\n        entityFrag = aql`MERGE(${entityVar}, { _rev: ${aql.value(node.revision)} })`;\n        options = aql`{ mergeObjects: false, ignoreRevs: false }`;\n        // to guarantee that the _rev changes, we need to set a property to a new value\n        updateFrag = aql`MERGE(${updateFrag}, { _revDummy: ${entityVar}._rev })`;\n    } else {\n        entityFrag = entityVar;\n        options = aql`{ mergeObjects: false }`;\n    }\n\n    return aqlExt.parenthesizeList(\n        aql`FOR ${entityVar}`,\n        aql`IN ${processNode(node.listNode, context)}`,\n        aql`UPDATE ${entityFrag}`,\n        aql`WITH ${updateFrag}`,\n        aql`IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        aql`OPTIONS ${options}`,\n        aql`RETURN NEW._key`\n    );\n});\n\nregister(DeleteEntitiesQueryNode, (node, context) => {\n    const entityVar = aql.variable(decapitalize(node.rootEntityType.name));\n    let entityFrag: AQLFragment;\n    let optionsFrag: AQLFragment;\n\n    if (node.revision) {\n        if (node.entitiesIdentifierKind === EntitiesIdentifierKind.ID) {\n            entityFrag = aql`{ _key: ${entityVar}, _rev: ${aql.value(node.revision)} }`;\n        } else {\n            entityFrag = aql`MERGE(${entityVar}, { _rev: ${aql.value(node.revision)} })`;\n        }\n        optionsFrag = aql`OPTIONS { ignoreRevs: false }`;\n    } else {\n        entityFrag = entityVar;\n        optionsFrag = aql``;\n    }\n\n    const countVar = aql.variable(`count`);\n    return aqlExt[\n        node.resultValue === DeleteEntitiesResultValue.OLD_ENTITIES ? 'parenthesizeList' : 'parenthesizeObject'\n    ](\n        aql`FOR ${entityVar}`,\n        aql`${generateInClause(node.listNode, context, entityVar)}`,\n        aql`REMOVE ${entityFrag}`,\n        aql`IN ${getCollectionForType(node.rootEntityType, AccessType.WRITE, context)}`,\n        optionsFrag,\n        node.resultValue === DeleteEntitiesResultValue.OLD_ENTITIES\n            ? aql`RETURN OLD`\n            : aql.lines(aql`COLLECT WITH COUNT INTO ${countVar}`, aql`RETURN ${countVar}`)\n    );\n});\n\nregister(AddEdgesQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    return aqlExt.parenthesizeList(\n        aql`FOR ${edgeVar}`,\n        aql`IN [ ${aql.join(\n            node.edges.map(edge => formatEdge(node.relation, edge, context)),\n            aql`, `\n        )} ]`,\n        aql`UPSERT { _from: ${edgeVar}._from, _to: ${edgeVar}._to }`, // need to unpack avoid dynamic property names in UPSERT example filter\n        aql`INSERT ${edgeVar}`,\n        aql`UPDATE {}`,\n        aql`IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\nregister(RemoveEdgesQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    const fromVar = aql.variable('from');\n    const toVar = aql.variable('to');\n    let edgeFilter: AQLFragment;\n    if (node.edgeFilter.fromIDsNode && node.edgeFilter.toIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._from == ${fromVar} && ${edgeVar}._to == ${toVar}`;\n    } else if (node.edgeFilter.fromIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._from == ${fromVar}`;\n    } else if (node.edgeFilter.toIDsNode) {\n        edgeFilter = aql`FILTER ${edgeVar}._to == ${toVar}`;\n    } else {\n        edgeFilter = aql``;\n    }\n    return aqlExt.parenthesizeList(\n        node.edgeFilter.fromIDsNode\n            ? aql`FOR ${fromVar} IN ${getFullIDsFromKeysNode(\n                  node.edgeFilter.fromIDsNode!,\n                  node.relation.fromType,\n                  context\n              )}`\n            : aql``,\n        node.edgeFilter.toIDsNode\n            ? aql`FOR ${toVar} IN ${getFullIDsFromKeysNode(node.edgeFilter.toIDsNode!, node.relation.toType, context)}`\n            : aql``,\n        aql`FOR ${edgeVar} IN ${getCollectionForRelation(node.relation, AccessType.READ, context)}`,\n        edgeFilter,\n        aql`REMOVE ${edgeVar} IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\nregister(SetEdgeQueryNode, (node, context) => {\n    const edgeVar = aql.variable('edge');\n    return aqlExt.parenthesizeList(\n        aql`UPSERT ${formatEdge(node.relation, node.existingEdge, context)}`,\n        aql`INSERT ${formatEdge(node.relation, node.newEdge, context)}`,\n        aql`UPDATE ${formatEdge(node.relation, node.newEdge, context)}`,\n        aql`IN ${getCollectionForRelation(node.relation, AccessType.WRITE, context)}`\n    );\n});\n\n/**\n * Gets an aql fragment that evaluates to a string of the format \"collectionName/objectKey\", given a query node that\n * evaluates to the \"object id\", which is, in arango terms, the _key.\n */\nfunction getFullIDFromKeyNode(node: QueryNode, rootEntityType: RootEntityType, context: QueryContext): AQLFragment {\n    // special handling to avoid concat if possible - do not alter the behavior\n    if (node instanceof LiteralQueryNode && typeof node.value == 'string') {\n        // just append the node to the literal key in JavaScript and bind it as a string\n        return aql`${getCollectionNameForRootEntity(rootEntityType) + '/' + node.value}`;\n    }\n    if (node instanceof RootEntityIDQueryNode) {\n        // access the _id field. processNode(node) would access the _key field instead.\n        return aql`${processNode(node.objectNode, context)}._id`;\n    }\n\n    // fall back to general case\n    return getFullIDFromKeyFragment(processNode(node, context), rootEntityType);\n}\n\nfunction getFullIDsFromKeysNode(\n    idsNode: QueryNode,\n    rootEntityType: RootEntityType,\n    context: QueryContext\n): AQLFragment {\n    if (idsNode instanceof ListQueryNode) {\n        // this probably generates cleaner AQL without dynamic concat\n        const idFragments = idsNode.itemNodes.map(idNode => getFullIDFromKeyNode(idNode, rootEntityType, context));\n        return aql`[${aql.join(idFragments, aql`, `)}]`;\n    }\n    if (\n        idsNode instanceof LiteralQueryNode &&\n        Array.isArray(idsNode.value) &&\n        idsNode.value.every(v => typeof v === 'string')\n    ) {\n        const collName = getCollectionNameForRootEntity(rootEntityType);\n        const ids = idsNode.value.map(val => collName + '/' + val);\n        return aql.value(ids);\n    }\n\n    return getFullIDFromKeysFragment(processNode(idsNode, context), rootEntityType);\n}\n\nfunction getFullIDFromKeyFragment(keyFragment: AQLFragment, rootEntityType: RootEntityType): AQLFragment {\n    return aql`CONCAT(${getCollectionNameForRootEntity(rootEntityType) + '/'}, ${keyFragment})`;\n}\n\nfunction getFullIDFromKeysFragment(keysFragment: AQLFragment, rootEntityType: RootEntityType): AQLFragment {\n    const idVar = aql.variable('id');\n    return aql`(FOR ${idVar} IN ${keysFragment} RETURN ${getFullIDFromKeyFragment(idVar, rootEntityType)})`;\n}\n\nfunction formatEdge(\n    relation: Relation,\n    edge: PartialEdgeIdentifier | EdgeIdentifier,\n    context: QueryContext\n): AQLFragment {\n    const conditions = [];\n    if (edge.fromIDNode) {\n        conditions.push(aql`_from: ${getFullIDFromKeyNode(edge.fromIDNode, relation.fromType, context)}`);\n    }\n    if (edge.toIDNode) {\n        conditions.push(aql`_to: ${getFullIDFromKeyNode(edge.toIDNode, relation.toType, context)}`);\n    }\n\n    return aql`{${aql.join(conditions, aql`, `)}}`;\n}\n\nfunction getAQLOperator(op: BinaryOperator): AQLFragment | undefined {\n    switch (op) {\n        case BinaryOperator.AND:\n            return aql`&&`;\n        case BinaryOperator.OR:\n            return aql`||`;\n        case BinaryOperator.EQUAL:\n            return aql`==`;\n        case BinaryOperator.UNEQUAL:\n            return aql`!=`;\n        case BinaryOperator.LESS_THAN:\n            return aql`<`;\n        case BinaryOperator.LESS_THAN_OR_EQUAL:\n            return aql`<=`;\n        case BinaryOperator.GREATER_THAN:\n            return aql`>`;\n        case BinaryOperator.GREATER_THAN_OR_EQUAL:\n            return aql`>=`;\n        case BinaryOperator.IN:\n            return aql`IN`;\n        case BinaryOperator.ADD:\n            return aql`+`;\n        case BinaryOperator.SUBTRACT:\n            return aql`-`;\n        case BinaryOperator.MULTIPLY:\n            return aql`*`;\n        case BinaryOperator.DIVIDE:\n            return aql`/`;\n        case BinaryOperator.MODULO:\n            return aql`%`;\n        default:\n            return undefined;\n    }\n}\n\nfunction generateSortAQL(orderBy: OrderSpecification, context: QueryContext): AQLFragment {\n    if (orderBy.isUnordered()) {\n        return aql``;\n    }\n\n    function dirAQL(dir: OrderDirection) {\n        if (dir == OrderDirection.DESCENDING) {\n            return aql` DESC`;\n        }\n        return aql``;\n    }\n\n    const clauses = orderBy.clauses.map(cl => aql`(${processNode(cl.valueNode, context)}) ${dirAQL(cl.direction)}`);\n\n    return aql`SORT ${aql.join(clauses, aql`, `)}`;\n}\n\nfunction processNode(node: QueryNode, context: QueryContext): AQLFragment {\n    const processor = processors.get(node.constructor as Constructor<QueryNode>);\n    if (!processor) {\n        throw new Error(`Unsupported query type: ${node.constructor.name}`);\n    }\n    return processor(node, context);\n}\n\n// TODO I think AQLCompoundQuery (AQL transaction node) should not be the exported type\n// we should rather export AQLExecutableQuery[] (as AQL transaction) directly.\nexport function getAQLQuery(node: QueryNode): AQLCompoundQuery {\n    return createAQLCompoundQuery(node, aql.queryResultVariable('result'), undefined, new QueryContext());\n}\n\nfunction getCollectionForBilling(accessType: AccessType, context: QueryContext) {\n    const name = billingCollectionName;\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\nfunction getCollectionForType(type: RootEntityType, accessType: AccessType, context: QueryContext) {\n    const name = getCollectionNameForRootEntity(type);\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\nfunction getCollectionForRelation(relation: Relation, accessType: AccessType, context: QueryContext) {\n    const name = getCollectionNameForRelation(relation);\n    context.addCollectionAccess(name, accessType);\n    return aql.collection(name);\n}\n\n/**\n * Processes a FollowEdgeQueryNode into a fragment to be used within `IN ...` (as opposed to be used in a general\n * expression context)\n */\nfunction getSimpleFollowEdgeFragment(node: FollowEdgeQueryNode, context: QueryContext): AQLFragment {\n    const dir = node.relationSide.isFromSide ? aql`OUTBOUND` : aql`INBOUND`;\n    return aql`${dir} ${processNode(node.sourceEntityNode, context)} ${getCollectionForRelation(\n        node.relationSide.relation,\n        AccessType.READ,\n        context\n    )}`;\n}\n\nfunction isStringCaseInsensitive(str: string) {\n    return str.toLowerCase() === str.toUpperCase();\n}\n\nexport function generateTokenizationQuery(tokensFiltered: ReadonlyArray<FlexSearchTokenizable>): AQLFragment {\n    const fragments: AQLFragment[] = [];\n    for (let i = 0; i < tokensFiltered.length; i++) {\n        const value = tokensFiltered[i];\n        fragments.push(aql`${aql.identifier('token_' + i)}: TOKENS(${value.expression}, ${value.analyzer})`);\n    }\n    return aql`RETURN { ${aql.join(fragments, aql`',\\n`)} }`;\n}\n", "import { Database } from 'arangojs';\nimport { globalContext } from '../../config/global';\nimport { ProjectOptions } from '../../config/interfaces';\nimport { Logger } from '../../config/logging';\nimport { ExecutionOptions } from '../../execution/execution-options';\nimport {\n    ConflictRetriesExhaustedError,\n    TransactionCancelledError,\n    TransactionTimeoutError\n} from '../../execution/runtime-errors';\nimport { Model } from '../../model';\nimport { ALL_QUERY_RESULT_VALIDATOR_FUNCTION_PROVIDERS, QueryNode } from '../../query-tree';\nimport { FlexSearchTokenization } from '../../query-tree/flex-search';\nimport { Mutable } from '../../utils/util-types';\nimport { objectValues, sleep, sleepInterruptible } from '../../utils/utils';\nimport { getPreciseTime, Watch } from '../../utils/watch';\nimport {\n    DatabaseAdapter,\n    DatabaseAdapterTimings,\n    ExecutionArgs,\n    ExecutionPlan,\n    ExecutionResult,\n    FlexSearchTokenizable,\n    TransactionStats\n} from '../database-adapter';\nimport { AQLCompoundQuery, aqlConfig, AQLExecutableQuery } from './aql';\nimport { generateTokenizationQuery, getAQLQuery } from './aql-generator';\nimport { RequestInstrumentation, RequestInstrumentationPhase } from './arangojs-instrumentation/config';\nimport { CancellationManager } from './cancellation-manager';\nimport {\n    ArangoDBConfig,\n    DEFAULT_RETRY_DELAY_BASE_MS,\n    getArangoDBLogger,\n    initDatabase,\n    RETRY_DELAY_RANDOM_FRACTION\n} from './config';\nimport { ERROR_ARANGO_CONFLICT, ERROR_QUERY_KILLED } from './error-codes';\nimport { hasRevisionAssertions } from './revision-helper';\nimport { SchemaAnalyzer } from './schema-migration/analyzer';\nimport { SchemaMigration } from './schema-migration/migrations';\nimport { MigrationPerformer } from './schema-migration/performer';\nimport { TransactionError } from '../../execution/transaction-error';\nimport { ArangoDBVersion, ArangoDBVersionHelper } from './version-helper';\nimport uuid = require('uuid');\n\nconst requestInstrumentationBodyKey = 'cruddlRequestInstrumentation';\n\ninterface ArangoExecutionOptions {\n    readonly queries: ReadonlyArray<AQLExecutableQuery>;\n    readonly options: ExecutionOptions;\n    /**\n     * An ID that will be prepended to all queries in this transaction so they can be aborted on cancellation\n     */\n    readonly transactionID: string;\n}\n\ninterface ArangoError extends Error {\n    readonly errorNum?: number;\n    readonly errorMessage?: string;\n}\n\nfunction isArangoError(error: Error): error is ArangoError {\n    return 'errorNum' in error;\n}\n\ninterface ArangoTransactionResult {\n    readonly data?: any;\n    readonly error?: ArangoError;\n    readonly timings?: { readonly [key: string]: number };\n    readonly plans?: ReadonlyArray<any>;\n    readonly stats: TransactionStats;\n}\n\ninterface TransactionResult {\n    readonly data?: any;\n    readonly timings?: Pick<DatabaseAdapterTimings, 'database' | 'dbConnection'>;\n    readonly plans?: ReadonlyArray<any>;\n    readonly databaseError?: Error;\n    readonly stats: TransactionStats;\n\n    /**\n     * True if the transactionTimeoutMs has taken effect. Does not necessarily mean that the query has been killed,\n     * you should check databaseError for his.\n     */\n    readonly hasTimedOut: boolean;\n\n    /**\n     * True if the cancellationToken has taken effect. Does not necessarily mean that the query has been killed,\n     * you should check databaseError for his.\n     */\n    readonly wasCancelled: boolean;\n}\n\nexport class ArangoDBAdapter implements DatabaseAdapter {\n    private readonly db: Database;\n    private readonly logger: Logger;\n    private readonly analyzer: SchemaAnalyzer;\n    private readonly migrationPerformer: MigrationPerformer;\n    private readonly cancellationManager: CancellationManager;\n    private readonly versionHelper: ArangoDBVersionHelper;\n    private readonly doNonMandatoryMigrations: boolean;\n    private readonly arangoExecutionFunction: string;\n\n    constructor(private readonly config: ArangoDBConfig, private schemaContext?: ProjectOptions) {\n        this.logger = getArangoDBLogger(schemaContext);\n        this.db = initDatabase(config);\n        this.analyzer = new SchemaAnalyzer(config, schemaContext);\n        this.migrationPerformer = new MigrationPerformer(config);\n        this.versionHelper = new ArangoDBVersionHelper(this.db);\n        this.arangoExecutionFunction = this.buildUpArangoExecutionFunction();\n        // the cancellation manager gets its own database instance so its cancellation requests are not queued\n        this.cancellationManager = new CancellationManager({ database: initDatabase(config) });\n        this.doNonMandatoryMigrations = config.doNonMandatoryMigrations !== false; // defaults to true\n    }\n\n    /**\n     * Gets the javascript source code for a function that executes a transaction\n     * @returns {string}\n     */\n    private buildUpArangoExecutionFunction(): string {\n        // The following function will be translated to a string and executed (as one transaction) within the\n        // ArangoDB server itself. Therefore the next comment is necessary to instruct our test coverage tool\n        // (https://github.com/istanbuljs/nyc) not to instrument the code with coverage instructions.\n\n        /* istanbul ignore next */\n        const arangoExecutionFunction = function({ queries, options, transactionID }: ArangoExecutionOptions) {\n            const db = require('@arangodb').db;\n            const enableProfiling = options.recordTimings;\n            const internal = enableProfiling ? require('internal') : undefined;\n\n            function getPreciseTime() {\n                return internal.time();\n            }\n\n            const startTime = enableProfiling ? getPreciseTime() : 0;\n\n            let validators: { [name: string]: (validationData: any, result: any) => void } = {};\n            //inject_validators_here\n\n            let timings: { [key: string]: number } | undefined = enableProfiling ? {} : undefined;\n            let timingsTotal = 0;\n\n            let plans: any[] = [];\n\n            let transactionStats: Mutable<TransactionStats> = {};\n\n            /**\n             * Throws an error so that the transaction is rolled back and returns the given value as transaction result\n             */\n            function rollbackWithResult(transactionResult: any): never {\n                const error = new Error(`${JSON.stringify(transactionResult)}`);\n                error.name = 'RolledBackTransactionError';\n                throw error;\n            }\n\n            function rollbackWithError(error: any): never {\n                if (enableProfiling && timings) {\n                    timings.js = getPreciseTime() - startTime - timingsTotal;\n                }\n\n                // the return is here to please typescript, it actually returns *never* (it throws)\n                return rollbackWithResult({\n                    error,\n                    timings,\n                    plans,\n                    stats: transactionStats\n                });\n            }\n\n            let resultHolder: { [p: string]: any } = {};\n            for (const query of queries) {\n                const bindVars = query.boundValues;\n                for (const key in query.usedPreExecResultNames) {\n                    bindVars[query.usedPreExecResultNames[key]] = resultHolder[key];\n                }\n\n                let explainResult;\n                // Execute the AQL query\n                let executionResult;\n                try {\n                    // the explain statement also can cause errors which should be caught\n                    if (options.recordPlan) {\n                        const stmt = db._createStatement({\n                            query: query.code,\n                            bindVars\n                        });\n                        explainResult = stmt.explain({ allPlans: true });\n                    }\n\n                    executionResult = db._query({\n                        query: `/*id:${transactionID}*/\\n${query.code}`,\n                        bindVars,\n                        options: {\n                            profile: options.recordPlan ? 2 : options.recordTimings ? 1 : 0,\n                            memoryLimit: options.queryMemoryLimit\n                        }\n                    });\n                } catch (error) {\n                    if (explainResult) {\n                        plans.push({\n                            plan: explainResult.plans[0],\n                            discardedPlans: explainResult.plans.slice(1),\n                            warnings: explainResult.warnings\n                        });\n                    }\n\n                    rollbackWithError(error);\n                }\n\n                const resultData = executionResult.next();\n\n                if (timings) {\n                    let profile = executionResult.getExtra().profile;\n                    for (let key in profile) {\n                        if (profile.hasOwnProperty(key)) {\n                            timings[key] = (timings[key] || 0) + profile[key];\n                            timingsTotal += profile[key];\n                        }\n                    }\n                }\n\n                if (options.recordPlan) {\n                    const extra = executionResult.getExtra();\n                    plans.push({\n                        plan: extra.plan,\n                        discardedPlans: explainResult ? explainResult.plans.slice(1) : [],\n                        stats: extra.stats,\n                        warnings: extra.warnings,\n                        profile: extra.profile\n                    });\n                }\n\n                if (executionResult.getExtra().stats && executionResult.getExtra().stats.peakMemoryUsage) {\n                    const usage = executionResult.getExtra().stats.peakMemoryUsage;\n                    if (!transactionStats.peakQueryMemoryUsage || transactionStats.peakQueryMemoryUsage < usage) {\n                        transactionStats.peakQueryMemoryUsage = usage;\n                    }\n                }\n\n                if (query.resultName) {\n                    resultHolder[query.resultName] = resultData;\n                }\n\n                try {\n                    if (query.resultValidator) {\n                        for (const key in query.resultValidator) {\n                            if (key in validators) {\n                                validators[key](query.resultValidator[key], resultData);\n                            }\n                        }\n                    }\n                } catch (e) {\n                    rollbackWithError({\n                        message: e.message,\n                        code: e.code\n                    });\n                }\n            }\n\n            // the last query is always the main query, use its result as result of the transaction\n            const lastQueryResultName = queries[queries.length - 1].resultName;\n            let data;\n            if (lastQueryResultName) {\n                data = resultHolder[lastQueryResultName];\n            } else {\n                data = undefined;\n            }\n\n            if (enableProfiling && timings) {\n                timings.js = getPreciseTime() - startTime - timingsTotal;\n            }\n\n            const transactionResult = {\n                data,\n                timings,\n                plans,\n                stats: transactionStats\n            };\n\n            if (options.mutationMode === 'rollback') {\n                rollbackWithResult(transactionResult);\n            }\n\n            return transactionResult;\n        };\n\n        const validatorProviders = ALL_QUERY_RESULT_VALIDATOR_FUNCTION_PROVIDERS.map(\n            provider => `[${JSON.stringify(provider.getValidatorName())}]: ${String(provider.getValidatorFunction())}`\n        );\n\n        const allValidatorFunctionsObjectString = `validators = {${validatorProviders.join(',\\n')}}`;\n\n        return String(arangoExecutionFunction).replace('//inject_validators_here', allValidatorFunctionsObjectString);\n    }\n\n    async execute(queryTree: QueryNode) {\n        const result = await this.executeExt({ queryTree });\n        if (result.error) {\n            throw result.error;\n        }\n        return result.data;\n    }\n\n    async executeExt({ queryTree, ...options }: ExecutionArgs): Promise<ExecutionResult> {\n        const prepStartTime = getPreciseTime();\n        globalContext.registerContext(this.schemaContext);\n        let executableQueries: AQLExecutableQuery[];\n        let aqlQuery: AQLCompoundQuery;\n        const oldEnableIndentationForCode = aqlConfig.enableIndentationForCode;\n        aqlConfig.enableIndentationForCode = !!options.recordPlan;\n        try {\n            //TODO Execute single statement AQL queries directly without \"db.transaction\"?\n            aqlQuery = getAQLQuery(queryTree);\n            executableQueries = aqlQuery.getExecutableQueries();\n        } finally {\n            globalContext.unregisterContext();\n            aqlConfig.enableIndentationForCode = oldEnableIndentationForCode;\n        }\n        if (this.logger.isTraceEnabled()) {\n            this.logger.trace(`Executing AQL: ${aqlQuery.toColoredString()}`);\n        }\n        const aqlPreparationTime = getPreciseTime() - prepStartTime;\n\n        // if the query contains revision assertions (_revision is used in updates / deletes), CONFLICT errors are\n        // expected and retrying the mutation won't help. The caller needs to handle the conflicts then.\n        // otherwise, conflicts can still occur because of how arangodb internally works, but those can be solved\n        // by retrying the query.\n        let executionResult;\n        if (hasRevisionAssertions(queryTree)) {\n            executionResult = await this.executeTransactionOnce(executableQueries, options, aqlQuery);\n        } else {\n            executionResult = await this.executeTransactionWithRetries(executableQueries, options, aqlQuery);\n        }\n        const {\n            databaseError,\n            timings: transactionTimings,\n            data,\n            plans,\n            stats,\n            hasTimedOut,\n            wasCancelled\n        } = executionResult;\n\n        let timings;\n        if (options.recordTimings && transactionTimings) {\n            timings = {\n                ...transactionTimings,\n                preparation: {\n                    total: aqlPreparationTime,\n                    aql: aqlPreparationTime\n                }\n            };\n        }\n\n        let plan: ExecutionPlan | undefined;\n        if (options.recordPlan && plans) {\n            plan = {\n                queryTree,\n                transactionSteps: executableQueries.map((q, index) => ({\n                    query: q.code,\n                    boundValues: q.boundValues,\n                    plan: plans[index] && plans[index].plan,\n                    discardedPlans: plans[index] && plans[index].discardedPlans,\n                    stats: plans[index] && plans[index].stats,\n                    warnings: plans[index] && plans[index].warnings,\n                    profile: plans[index] && plans[index].profile\n                }))\n            };\n        }\n\n        let error;\n        if (databaseError) {\n            error = this.processDatabaseError(databaseError, {\n                wasCancelled,\n                hasTimedOut,\n                transactionTimeoutMs: options.transactionTimeoutMs\n            });\n        }\n\n        return {\n            error,\n            data,\n            timings,\n            plan,\n            stats\n        };\n    }\n\n    private processDatabaseError(\n        error: Error,\n        {\n            hasTimedOut,\n            wasCancelled,\n            transactionTimeoutMs\n        }: { hasTimedOut: boolean; wasCancelled: boolean; transactionTimeoutMs: number | undefined }\n    ): Error {\n        // might be just something like a TypeError\n        if (!isArangoError(error)) {\n            return new TransactionError(error.message, error);\n        }\n\n        // some errors need to be translated because we only can differentiate with the context here\n        if (error.errorNum === ERROR_QUERY_KILLED) {\n            // only check these flags if a QUERY_KILLED error is thrown because we might have initiated a query\n            // kill due to timeout / cancellation, but it might have completed or errored for some other reason\n            // before the kill is executed\n            if (hasTimedOut) {\n                return new TransactionTimeoutError({ timeoutMs: transactionTimeoutMs });\n            } else if (wasCancelled) {\n                return new TransactionCancelledError();\n            }\n        }\n\n        // the arango errors are weird and have their message in \"errorMessage\"...\n        return new TransactionError(error.errorMessage || error.message, error);\n    }\n\n    private async executeTransactionWithRetries(\n        executableQueries: ReadonlyArray<AQLExecutableQuery>,\n        options: ExecutionOptions,\n        aqlQuery: AQLCompoundQuery\n    ): Promise<TransactionResult> {\n        const maxRetries = this.config.retriesOnConflict || 0;\n        let nextRetryDelay = 0;\n        let retries = 0;\n        let result;\n        // timings need to be added up\n        let timings: TransactionResult['timings'] | undefined;\n\n        while (true) {\n            result = await this.executeTransactionOnce(executableQueries, options, aqlQuery);\n\n            if (options.recordTimings && result.timings) {\n                timings = {\n                    database: sumUpValues([timings ? timings.database : {}, result.timings.database]),\n                    dbConnection: sumUpValues([timings ? timings.dbConnection : {}, result.timings.dbConnection])\n                } as TransactionResult['timings'];\n            }\n\n            const stats = {\n                ...result.stats,\n                retries\n            };\n\n            if (!result.databaseError || !this.isRetryableError(result.databaseError) || !maxRetries) {\n                return {\n                    ...result,\n                    timings,\n                    stats\n                };\n            }\n\n            if (retries >= maxRetries) {\n                // retries exhausted\n                return {\n                    ...result,\n                    timings,\n                    stats,\n                    databaseError: new ConflictRetriesExhaustedError({ causedBy: result.databaseError, retries })\n                };\n            }\n\n            const sleepStart = getPreciseTime();\n            const randomFactor = 1 + RETRY_DELAY_RANDOM_FRACTION * (2 * Math.random() - 1);\n            const delayWithRandomComponent = nextRetryDelay * randomFactor;\n            const shouldContinue = await sleepInterruptible(delayWithRandomComponent, options.cancellationToken);\n            if (options.recordTimings && timings) {\n                const sleepLength = getPreciseTime() - sleepStart;\n                timings = {\n                    ...timings,\n                    dbConnection: sumUpValues([timings.dbConnection, { retryDelay: sleepLength, total: sleepLength }])\n                } as TransactionResult['timings'];\n            }\n            if (!shouldContinue) {\n                // cancellation token fired before the sleep time was over\n                // we already have a result with an error, so it's probably better to return that instead of a generic \"cancelled\"\n                // probably doesn't matter anyway because the caller probably is no longer interested in the result\n                return {\n                    ...result,\n                    timings,\n                    stats\n                };\n            }\n\n            if (nextRetryDelay) {\n                nextRetryDelay *= 2;\n            } else {\n                nextRetryDelay = this.config.retryDelayBaseMs || DEFAULT_RETRY_DELAY_BASE_MS;\n            }\n            retries++;\n        }\n    }\n\n    private isRetryableError(error: ArangoError): boolean {\n        return error.errorNum === ERROR_ARANGO_CONFLICT;\n    }\n\n    private async executeTransactionOnce(\n        executableQueries: ReadonlyArray<AQLExecutableQuery>,\n        options: ExecutionOptions,\n        aqlQuery: AQLCompoundQuery\n    ): Promise<TransactionResult> {\n        const transactionID = uuid();\n        const args: ArangoExecutionOptions = {\n            queries: executableQueries,\n            options: {\n                ...options,\n                queryMemoryLimit: options.queryMemoryLimit || this.config.queryMemoryLimit\n            },\n            transactionID\n        };\n        let isTransactionFinished = false;\n        const watch = new Watch();\n\n        let hasTimedOut = false;\n        let wasCancelled = false;\n\n        let cancellationToken = options.cancellationToken;\n        if (cancellationToken) {\n            cancellationToken.then(() => {\n                wasCancelled = true;\n            });\n        }\n        let requestSentCallback: (() => void) | undefined;\n        let requestSentPromise = new Promise<void>(resolve => (requestSentCallback = resolve));\n        let timeout: any | undefined;\n        if (options.transactionTimeoutMs != undefined) {\n            const ms = options.transactionTimeoutMs;\n            // transactionTimeout is a timeout that should only be started when the request is actually sent to ArangoDB\n            const timeoutPromise = requestSentPromise\n                .then(\n                    () =>\n                        new Promise<void>(resolve => {\n                            timeout = setTimeout(resolve, ms);\n                        })\n                )\n                .then(() => {\n                    hasTimedOut = true;\n                });\n            if (cancellationToken) {\n                cancellationToken.then(() => {\n                    if (timeout) {\n                        clearTimeout(timeout);\n                        timeout = undefined;\n                    }\n                });\n                cancellationToken = Promise.race([cancellationToken, timeoutPromise]);\n            } else {\n                cancellationToken = timeoutPromise;\n            }\n        }\n\n        // we pass the cancellationToken to the call to Database.transaction(). This will remove the request from the\n        // http agent's queue. However, it won't cancel the request if already sent because ArangoDB does NOT abort a\n        // query in this case, so this would not help. In the contrary, it would free up the connection in the arangojs\n        // http agent so that more queries can be run in parallel than configured (via maxSockets). This would be\n        // dangerous because it might exhaust ArangoDB threads so that ArangoDB no longer responds, and it might even\n        // cause too much memory to be allocated. For this reason, we only kill the query (see below) and let that\n        // killed query also abort the transaction.\n        // Note: this only works because we use our own version of the arangojs database (CustomDatbase)\n        (args as any)[requestInstrumentationBodyKey] = {\n            onPhaseEnded: (phase: RequestInstrumentationPhase) => {\n                watch.stop(phase);\n\n                if (phase === 'socketInit') {\n                    // start the timeout promise if needed\n                    if (requestSentCallback) {\n                        requestSentCallback();\n                    }\n\n                    if (cancellationToken) {\n                        // delay cancellation a bit for two reasons\n                        // - don't take the effort of finding and killing a query if it's fast anyway\n                        // - the cancellation might occur before the transaction script starts the query\n                        // we only really need this to cancel long-running queries\n                        cancellationToken\n                            .then(() => sleep(30))\n                            .then(() => {\n                                // don't try to kill the query if the transaction() call finished already - this would mean that it\n                                // either was faster than the delay above, or the request was removed from the request queue\n                                if (!isTransactionFinished) {\n                                    this.logger.debug(`Cancelling query ${transactionID}`);\n                                    this.cancellationManager.cancelQuery(transactionID).catch(e => {\n                                        this.logger.warn(`Error cancelling query ${transactionID}: ${e.stack}`);\n                                    });\n                                }\n                            });\n                    }\n                }\n            },\n            cancellationToken\n        } as RequestInstrumentation;\n\n        const dbStartTime = getPreciseTime();\n        let transactionResult: ArangoTransactionResult;\n        try {\n            transactionResult = await this.db.executeTransaction(\n                {\n                    read: aqlQuery.readAccessedCollections,\n                    write: aqlQuery.writeAccessedCollections\n                },\n                this.arangoExecutionFunction,\n                {\n                    params: args,\n                    waitForSync: true\n                }\n            );\n        } catch (e) {\n            isTransactionFinished = true;\n            if (e.message.startsWith('RolledBackTransactionError: ')) {\n                const valStr = e.message.substr('RolledBackTransactionError: '.length);\n                try {\n                    transactionResult = JSON.parse(valStr);\n                } catch (eParse) {\n                    throw new Error(`Error parsing result of rolled back transaction`);\n                }\n            } else {\n                throw e;\n            }\n        } finally {\n            if (timeout) {\n                clearTimeout(timeout);\n                timeout = undefined;\n            }\n        }\n        const { timings: databaseReportedTimings, data, plans, error: databaseError } = transactionResult;\n        isTransactionFinished = true;\n\n        let timings;\n        if (options.recordTimings && databaseReportedTimings) {\n            const dbConnectionTotal = getPreciseTime() - dbStartTime;\n            const queuing = watch.timings.queuing;\n            const socketInit = watch.timings.socketInit || 0;\n            const lookup = watch.timings.lookup || 0;\n            const connecting = watch.timings.connecting || 0;\n            const receiving = watch.timings.receiving;\n            const waiting = watch.timings.waiting;\n            const other = watch.timings.total - queuing - socketInit - lookup - connecting - receiving - waiting;\n            const dbInternalTotal = objectValues<number>(databaseReportedTimings).reduce((a, b) => a + b, 0);\n            timings = {\n                dbConnection: {\n                    queuing,\n                    socketInit,\n                    lookup,\n                    connecting,\n                    waiting,\n                    receiving,\n                    other,\n                    total: dbConnectionTotal\n                },\n                database: {\n                    ...databaseReportedTimings,\n                    total: dbInternalTotal\n                }\n            };\n        }\n\n        return {\n            timings,\n            data,\n            plans,\n            databaseError,\n            stats: transactionResult.stats,\n            hasTimedOut,\n            wasCancelled\n        };\n    }\n\n    /**\n     * Compares the model with the database and determines migrations to do\n     */\n    async getOutstandingMigrations(model: Model): Promise<ReadonlyArray<SchemaMigration>> {\n        return this.analyzer.getOutstandingMigrations(model);\n    }\n\n    /**\n     * Performs a single mutation\n     */\n    async performMigration(migration: SchemaMigration): Promise<void> {\n        await this.migrationPerformer.performMigration(migration);\n    }\n\n    /**\n     * Performs schema migration as configured with autocreateIndices/autoremoveIndices\n     */\n    async updateSchema(model: Model): Promise<void> {\n        const migrations = await this.getOutstandingMigrations(model);\n        const skippedMigrations: SchemaMigration[] = [];\n        for (const migration of migrations) {\n            if (!migration.isMandatory && !this.doNonMandatoryMigrations) {\n                this.logger.debug(`Skipping migration \"${migration.description}\" because of configuration`);\n                skippedMigrations.push(migration);\n                continue;\n            }\n            try {\n                this.logger.info(`Performing migration \"${migration.description}\"`);\n                await this.performMigration(migration);\n                this.logger.info(`Successfully performed migration \"${migration.description}\"`);\n            } catch (e) {\n                this.logger.error(`Error performing migration \"${migration.description}\": ${e.stack}`);\n                throw e;\n            }\n        }\n    }\n\n    async getArangoDBVersion(): Promise<ArangoDBVersion | undefined> {\n        return this.versionHelper.getArangoDBVersion();\n    }\n\n    async tokenizeExpressions(\n        tokenizations: ReadonlyArray<FlexSearchTokenizable>\n    ): Promise<ReadonlyArray<FlexSearchTokenization>> {\n        const tokenizationsFiltered = tokenizations.filter(\n            (value, index) =>\n                !tokenizations.some(\n                    (value2, index2) =>\n                        value.expression === value2.expression && value.analyzer === value2.analyzer && index > index2\n                )\n        );\n\n        const aqlFragment = generateTokenizationQuery(tokenizationsFiltered);\n        const queryCode = aqlFragment.getCode();\n        const cursor = await this.db.query(queryCode.code, queryCode.boundValues);\n\n        const result = await cursor.next();\n        const resultArray: FlexSearchTokenization[] = [];\n        for (let i = 0; i < tokenizationsFiltered.length; i++) {\n            resultArray.push({\n                expression: tokenizationsFiltered[i].expression,\n                analyzer: tokenizationsFiltered[i].analyzer,\n                tokens: result['token_' + i]\n            });\n        }\n\n        return resultArray;\n    }\n}\n\nfunction sumUpValues(objects: ReadonlyArray<{ readonly [key: string]: number }>): { readonly [key: string]: number } {\n    const result: { [key: string]: number } = {};\n    for (const obj of objects) {\n        for (const key of Object.keys(obj)) {\n            if (Number.isFinite(obj[key])) {\n                if (key in result && Number.isFinite(result[key])) {\n                    result[key] += obj[key];\n                } else {\n                    result[key] = obj[key];\n                }\n            }\n        }\n    }\n    return result;\n}\n"], "filenames": ["src/database/arangodb/aql-generator.ts", "src/database/arangodb/arangodb-adapter.ts"], "buggy_code_start_loc": [1765, 721], "buggy_code_end_loc": [1774, 722], "fixing_code_start_loc": [1765, 721], "fixing_code_end_loc": [1773, 724], "type": "CWE-74", "message": "cruddl is software for creating a GraphQL API for a database, using the GraphQL SDL to model a schema. If cruddl starting with version 1.1.0 and prior to versions 2.7.0 and 3.0.2 is used to generate a schema that uses `@flexSearchFulltext`, users of that schema may be able to inject arbitrary AQL queries that will be forwarded to and executed by ArangoDB. Schemas that do not use `@flexSearchFulltext` are not affected. The attacker needs to have `READ` permission to at least one root entity type that has `@flexSearchFulltext` enabled. The issue has been fixed in version 3.0.2 and in version 2.7.0 of cruddl. As a workaround, users can temporarily remove `@flexSearchFulltext` from their schemas.", "other": {"cve": {"id": "CVE-2022-36084", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-08T22:15:08.713", "lastModified": "2022-09-13T19:54:39.943", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "cruddl is software for creating a GraphQL API for a database, using the GraphQL SDL to model a schema. If cruddl starting with version 1.1.0 and prior to versions 2.7.0 and 3.0.2 is used to generate a schema that uses `@flexSearchFulltext`, users of that schema may be able to inject arbitrary AQL queries that will be forwarded to and executed by ArangoDB. Schemas that do not use `@flexSearchFulltext` are not affected. The attacker needs to have `READ` permission to at least one root entity type that has `@flexSearchFulltext` enabled. The issue has been fixed in version 3.0.2 and in version 2.7.0 of cruddl. As a workaround, users can temporarily remove `@flexSearchFulltext` from their schemas."}, {"lang": "es", "value": "cruddl es un software para crear una API GraphQL para una base de datos, usando GraphQL SDL para modelar un esquema.&#xa0;Si es usado cruddl a partir de la versi\u00f3n 1.1.0 y anteriores a 2.7.0 y 3.0.2, para generar un esquema que usa \"@flexSearchFulltext\", los usuarios de ese esquema pueden inyectar consultas AQL arbitrarias que ser\u00e1n reenviadas a y ejecutadas por ArangoDB.&#xa0;Los esquemas que no usan \"@flexSearchFulltext\" no est\u00e1n afectados.&#xa0;El atacante debe tener permiso \"READ\" para al menos un tipo de entidad root que tenga habilitado \"@flexSearchFulltext\".&#xa0;El problema ha sido corregido en versi\u00f3n 3.0.2 y en versi\u00f3n 2.7.0 de cruddl.&#xa0;Como mitigaci\u00f3n, los usuarios pueden eliminar temporalmente \"@flexSearchFulltext\" de sus esquemas"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.9, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.1, "impactScore": 6.0}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-74"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-74"}, {"lang": "en", "value": "CWE-943"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:aeb:cruddl:*:*:*:*:*:node.js:*:*", "versionStartIncluding": "1.1.0", "versionEndExcluding": "2.7.0", "matchCriteriaId": "B819657F-CBA8-42E1-A658-5DF74F6C2103"}, {"vulnerable": true, "criteria": "cpe:2.3:a:aeb:cruddl:*:*:*:*:*:node.js:*:*", "versionStartIncluding": "3.0.0", "versionEndExcluding": "3.0.2", "matchCriteriaId": "A34E8018-8A03-44A6-8DD0-99BD8A879FAC"}]}]}], "references": [{"url": "https://github.com/AEB-labs/cruddl/commit/13b9233733ed6fc822718a07bc90a80cd3492698", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/AEB-labs/cruddl/pull/253", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/AEB-labs/cruddl/security/advisories/GHSA-qm4w-4995-vg7f", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/AEB-labs/cruddl/commit/13b9233733ed6fc822718a07bc90a80cd3492698"}}