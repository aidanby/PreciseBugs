{"buggy_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n\n#include <trace/events/ext4.h>\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nstatic int ext4_ext_dirty(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\terr = ext4_handle_dirty_metadata(handle, inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\tint depth;\n\n\tif (path) {\n\t\tstruct ext4_extent *ex;\n\t\tdepth = path->p_depth;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\t/*\n\t\t * If there are at least EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\n\t\t * block groups per flexgroup, reserve the first block\n\t\t * group for directories and special files.  Regular\n\t\t * files will start at the second block group.  This\n\t\t * tends to speed up directory access and improves\n\t\t * fsck times.\n\t\t */\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = ext4_group_first_block_no(inode->i_sb, block_group);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour + block;\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, NULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 6)\n\t\t\tsize = 6;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 5)\n\t\t\tsize = 5;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 3)\n\t\t\tsize = 3;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 4)\n\t\t\tsize = 4;\n#endif\n\t}\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs, num = 0;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tstruct ext4_extent *ext;\n\tstruct ext4_extent_idx *ext_idx;\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\text = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\text_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t\"bad header/extent: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\terror_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, __LINE__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %d->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\tint need_to_validate = 0;\n\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\ttrace_ext4_ext_load_extent(inode, block,\n\t\t\t\t\t\tpath[ppos].p_block);\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t/* validate the extent entries */\n\t\t\tneed_to_validate = 1;\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (need_to_validate && ext4_ext_check(inode, eh, i))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\tlen = EXT_MAX_INDEX(curp->p_hdr) - curp->p_idx;\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\tif (curp->p_idx != EXT_LAST_INDEX(curp->p_hdr)) {\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent_idx);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert new index %d after: %llu. \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tlogical, ptr, len,\n\t\t\t\t\t(curp->p_idx + 1), (curp->p_idx + 2));\n\t\t\tmemmove(curp->p_idx + 2, curp->p_idx + 1, len);\n\t\t}\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\tlen = len * sizeof(struct ext4_extent_idx);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert new index %d before: %llu. \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, ptr, len,\n\t\t\t\tcurp->p_idx, (curp->p_idx + 1));\n\t\tmemmove(curp->p_idx + 1, curp->p_idx, len);\n\t\tix = curp->p_idx;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     > le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct ext4_extent *ex;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\tex = EXT_FIRST_EXTENT(neh);\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\t/* TODO: we could do it by single memmove */\n\tm = 0;\n\tpath[depth].p_ext++;\n\twhile (path[depth].p_ext <=\n\t\t\tEXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(path[depth].p_ext->ee_block),\n\t\t\t\text4_ext_pblock(path[depth].p_ext),\n\t\t\t\text4_ext_is_uninitialized(path[depth].p_ext),\n\t\t\t\text4_ext_get_actual_len(path[depth].p_ext),\n\t\t\t\tnewblock);\n\t\t/*memmove(ex++, path[depth].p_ext++,\n\t\t\t\tsizeof(struct ext4_extent));\n\t\tneh->eh_entries++;*/\n\t\tpath[depth].p_ext++;\n\t\tm++;\n\t}\n\tif (m) {\n\t\tmemmove(ex, path[depth].p_ext-m, sizeof(struct ext4_extent)*m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\t\t/* copy indexes */\n\t\tm = 0;\n\t\tpath[i].p_idx++;\n\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\twhile (path[i].p_idx <= EXT_MAX_INDEX(path[i].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", i,\n\t\t\t\t\tle32_to_cpu(path[i].p_idx->ei_block),\n\t\t\t\t\text4_idx_pblock(path[i].p_idx),\n\t\t\t\t\tnewblock);\n\t\t\t/*memmove(++fidx, path[i].p_idx++,\n\t\t\t\t\tsizeof(struct ext4_extent_idx));\n\t\t\tneh->eh_entries++;\n\t\t\tBUG_ON(neh->eh_entries > neh->eh_max);*/\n\t\t\tpath[i].p_idx++;\n\t\t\tm++;\n\t\t}\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx - m,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp = path;\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, path, newext, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, curp->p_hdr, sizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* create index in new top-level index: num,max,pointer */\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\tgoto out;\n\n\tcurp->p_hdr->eh_magic = EXT4_EXT_MAGIC;\n\tcurp->p_hdr->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\tcurp->p_hdr->eh_entries = cpu_to_le16(1);\n\tcurp->p_idx = EXT_FIRST_INDEX(curp->p_hdr);\n\n\tif (path[0].p_hdr->eh_depth)\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_INDEX(path[0].p_hdr)->ei_block;\n\telse\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_EXTENT(path[0].p_hdr)->ee_block;\n\text4_idx_store_pblock(curp->p_idx, newblock);\n\n\tneh = ext_inode_hdr(inode);\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tneh->eh_depth = cpu_to_le16(path->p_depth + 1);\n\terr = ext4_ext_dirty(handle, inode, curp);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, path, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? ix->ei_block : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\t\t\t    EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext4_ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext4_ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCK.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCK;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCK\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCK;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic int ext4_ext_try_to_merge(struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\tret = ext4_ext_try_to_merge_right(inode, path, ex);\n\n\treturn ret;\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCK)\n\t\t\tgoto out;\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCK - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCK) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\terr = ext4_ext_create_new_leaf(handle, inode, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nstatic int ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\t       ext4_lblk_t num, ext_prepare_callback func,\n\t\t\t       void *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCK) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext4_ext_pblock(ex);\n\t\t}\n\n\t\tif (unlikely(cbex.ec_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"cbex.ec_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\terr = func(inode, path, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCK;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0);\n}\n\n/*\n * Return 0 if cache is invalid; 1 if the cache is valid\n */\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\tstruct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tint ret = 0;\n\n\t/*\n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\n\t/* has cache valid data? */\n\tif (cex->ec_len == 0)\n\t\tgoto errout;\n\n\tif (in_range(block, cex->ec_block, cex->ec_len)) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = 1;\n\t}\nerrout:\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n * It's used in truncate case only, thus all requests are for\n * last index in the block only.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadat blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_extent *ex,\n\t\t\t\text4_lblk_t from, ext4_lblk_t to)\n{\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tstart = ext4_ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, NULL, start, num, flags);\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\tprintk(KERN_INFO \"strange request: removal %u-%u from %u:%u\\n\",\n\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t start)\n{\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b, block;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf\\n\", start);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block + ex_ee_len - 1 < EXT_MAX_BLOCK ?\n\t\t\tex_ee_block + ex_ee_len - 1 : EXT_MAX_BLOCK;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\tif (a != ex_ee_block && b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tblock = 0;\n\t\t\tnum = 0;\n\t\t\tBUG();\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = a - block;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\t/* remove head of the extent */\n\t\t\tblock = a;\n\t\t\tnum = b - a;\n\t\t\t/* there is no \"make a hole\" API yet */\n\t\t\tBUG();\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = 0;\n\t\t\tBUG_ON(a != ex_ee_block);\n\t\t\tBUG_ON(b != ex_ee_block + ex_ee_len - 1);\n\t\t}\n\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0) {\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\tex->ee_block = cpu_to_le32(block);\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path;\n\thandle_t *handle;\n\tint i, err;\n\n\text_debug(\"truncate since %u\\n\", start);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\text4_ext_invalidate_cache(inode);\n\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1), GFP_NOFS);\n\tif (path == NULL) {\n\t\text4_journal_stop(handle);\n\t\treturn -ENOMEM;\n\t}\n\tpath[0].p_depth = depth;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\ti = err = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path, start);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, ext4_idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\");\n#ifdef AGGRESSIVE_TEST\n\t\tprintk(\", aggressive tests\");\n#endif\n#ifdef CHECK_BINSEARCH\n\t\tprintk(\", check binsearch\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tprintk(\", stats\");\n#endif\n\t\tprintk(\"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNINIT1\t0x2  /* mark first half uninitialized */\n#define EXT4_EXT_MARK_UNINIT2\t0x4  /* mark second half uninitialized */\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or uninit) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le32(ee_len);\n\t\text4_ext_try_to_merge(inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (upto three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}\n\n#define EXT4_EXT_ZERO_LEN 7\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (up to three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\tint ret = 0;\n\tint may_zeroout;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (map->m_lblk - ee_block);\n\tnewblock = map->m_lblk - ee_block + ext4_ext_pblock(ex);\n\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext4_ext_pblock(ex));\n\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tmay_zeroout = ee_block + ee_len <= eof_block;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* If extent has less than 2*EXT4_EXT_ZERO_LEN zerout directly */\n\tif (ee_len <= 2*EXT4_EXT_ZERO_LEN && may_zeroout) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zeroed the full extent */\n\t\treturn allocated;\n\t}\n\n\t/* ex1: ee_block to map->m_lblk - 1 : uninitialized */\n\tif (map->m_lblk > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(map->m_lblk - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > map->m_len)\n\t\tex2->ee_len = cpu_to_le16(map->m_len);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > map->m_len) {\n\t\tunsigned int newdepth;\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN zerout directly */\n\t\tif (allocated <= EXT4_EXT_ZERO_LEN && may_zeroout) {\n\t\t\t/*\n\t\t\t * map->m_lblk == ee_block is handled by the zerouout\n\t\t\t * at the beginning.\n\t\t\t * Mark first half uninitialized.\n\t\t\t * Mark second half initialized and zero out the\n\t\t\t * initialized extent\n\t\t\t */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = cpu_to_le16(ee_len - allocated);\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\n\t\t\tex3 = &newex;\n\t\t\tex3->ee_block = cpu_to_le32(map->m_lblk);\n\t\t\text4_ext_store_pblock(ex3, newblock);\n\t\t\tex3->ee_len = cpu_to_le16(allocated);\n\t\t\terr = ext4_ext_insert_extent(handle, inode, path,\n\t\t\t\t\t\t\tex3, 0);\n\t\t\tif (err == -ENOSPC) {\n\t\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto fix_extent_len;\n\t\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\t\text4_ext_store_pblock(ex,\n\t\t\t\t\text4_ext_pblock(&orig_ex));\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\t/* blocks available from map->m_lblk */\n\t\t\t\treturn allocated;\n\n\t\t\t} else if (err)\n\t\t\t\tgoto fix_extent_len;\n\n\t\t\t/*\n\t\t\t * We need to zero out the second half because\n\t\t\t * an fallocate request can update file size and\n\t\t\t * converting the second half to initialized extent\n\t\t\t * implies that we can leak some junk data to user\n\t\t\t * space.\n\t\t\t */\n\t\t\terr =  ext4_ext_zeroout(inode, ex3);\n\t\t\tif (err) {\n\t\t\t\t/*\n\t\t\t\t * We should actually mark the\n\t\t\t\t * second half as uninit and return error\n\t\t\t\t * Insert would have changed the extent\n\t\t\t\t */\n\t\t\t\tdepth = ext_depth(inode);\n\t\t\t\text4_ext_drop_refs(path);\n\t\t\t\tpath = ext4_ext_find_extent(inode, map->m_lblk,\n\t\t\t\t\t\t\t    path);\n\t\t\t\tif (IS_ERR(path)) {\n\t\t\t\t\terr = PTR_ERR(path);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t\t/* get the second half extent details */\n\t\t\t\tex = path[depth].p_ext;\n\t\t\t\terr = ext4_ext_get_access(handle, inode,\n\t\t\t\t\t\t\t\tpath + depth);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\text4_ext_mark_uninitialized(ex);\n\t\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\t/* zeroed the second half */\n\t\t\treturn allocated;\n\t\t}\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(map->m_lblk + map->m_len);\n\t\text4_ext_store_pblock(ex3, newblock + map->m_len);\n\t\tex3->ee_len = cpu_to_le16(allocated - map->m_len);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, 0);\n\t\tif (err == -ENOSPC && may_zeroout) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from map->m_lblk */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\tee_len -= ext4_ext_get_actual_len(ex3);\n\t\torig_ex.ee_len = cpu_to_le16(ee_len);\n\t\tmay_zeroout = ee_block + ee_len <= eof_block;\n\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\teh = path[depth].p_hdr;\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = map->m_len;\n\n\t\t/* If extent has less than EXT4_EXT_ZERO_LEN and we are trying\n\t\t * to insert a extent in the middle zerout directly\n\t\t * otherwise give the extent a chance to merge to left\n\t\t */\n\t\tif (le16_to_cpu(orig_ex.ee_len) <= EXT4_EXT_ZERO_LEN &&\n\t\t\tmap->m_lblk != ee_block && may_zeroout) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zero out the first half */\n\t\t\t/* blocks available from map->m_lblk */\n\t\t\treturn allocated;\n\t\t}\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(map->m_lblk - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/* ex2: map->m_lblk to map->m_lblk + maxblocks-1 : initialised */\n\tex2->ee_block = cpu_to_le32(map->m_lblk);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/*\n\t * New (initialized) extent starts from the first block\n\t * in the current extent. i.e., ex2 == ex\n\t * We have to see if it can be merged with the extent\n\t * on the left.\n\t */\n\tif (ex2 > EXT_FIRST_EXTENT(eh)) {\n\t\t/*\n\t\t * To merge left, pass \"ex2 - 1\" to try_to_merge(),\n\t\t * since it merges towards right _only_.\n\t\t */\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2 - 1);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tdepth = ext_depth(inode);\n\t\t\tex2--;\n\t\t}\n\t}\n\t/*\n\t * Try to Merge towards right. This might be required\n\t * only when the whole extent is being written to.\n\t * i.e. ex2 == ex and ex3 == NULL.\n\t */\n\tif (!ex3) {\n\t\tret = ext4_ext_try_to_merge(inode, path, ex2);\n\t\tif (ret) {\n\t\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, 0);\n\tif (err == -ENOSPC && may_zeroout) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitialized extent may result in splitting the uninitialized\n * extent into multiple /initialized uninitialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the uninitialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitialized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex1 = NULL;\n\tstruct ext4_extent *ex2 = NULL;\n\tstruct ext4_extent *ex3 = NULL;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int allocated, ee_len, depth;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\tint may_zeroout;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (map->m_lblk - ee_block);\n\tnewblock = map->m_lblk - ee_block + ext4_ext_pblock(ex);\n\n\tex2 = ex;\n\torig_ex.ee_block = ex->ee_block;\n\torig_ex.ee_len   = cpu_to_le16(ee_len);\n\text4_ext_store_pblock(&orig_ex, ext4_ext_pblock(ex));\n\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tmay_zeroout = ee_block + ee_len <= eof_block;\n\n\t/*\n \t * If the uninitialized extent begins at the same logical\n \t * block where the write begins, and the write completely\n \t * covers the extent, then we don't need to split it.\n \t */\n\tif ((map->m_lblk == ee_block) && (allocated <= map->m_len))\n\t\treturn allocated;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* ex1: ee_block to map->m_lblk - 1 : uninitialized */\n\tif (map->m_lblk > ee_block) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(map->m_lblk - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * for sanity, update the length of the ex2 extent before\n\t * we insert ex3, if ex1 is NULL. This is to avoid temporary\n\t * overlap of blocks.\n\t */\n\tif (!ex1 && allocated > map->m_len)\n\t\tex2->ee_len = cpu_to_le16(map->m_len);\n\t/* ex3: to ee_block + ee_len : uninitialised */\n\tif (allocated > map->m_len) {\n\t\tunsigned int newdepth;\n\t\tex3 = &newex;\n\t\tex3->ee_block = cpu_to_le32(map->m_lblk + map->m_len);\n\t\text4_ext_store_pblock(ex3, newblock + map->m_len);\n\t\tex3->ee_len = cpu_to_le16(allocated - map->m_len);\n\t\text4_ext_mark_uninitialized(ex3);\n\t\terr = ext4_ext_insert_extent(handle, inode, path, ex3, flags);\n\t\tif (err == -ENOSPC && may_zeroout) {\n\t\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\t\tif (err)\n\t\t\t\tgoto fix_extent_len;\n\t\t\t/* update the extent length and mark as initialized */\n\t\t\tex->ee_block = orig_ex.ee_block;\n\t\t\tex->ee_len   = orig_ex.ee_len;\n\t\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t\t/* zeroed the full extent */\n\t\t\t/* blocks available from map->m_lblk */\n\t\t\treturn allocated;\n\n\t\t} else if (err)\n\t\t\tgoto fix_extent_len;\n\t\t/*\n\t\t * The depth, and hence eh & ex might change\n\t\t * as part of the insert above.\n\t\t */\n\t\tnewdepth = ext_depth(inode);\n\t\t/*\n\t\t * update the extent length after successful insert of the\n\t\t * split extent\n\t\t */\n\t\tee_len -= ext4_ext_get_actual_len(ex3);\n\t\torig_ex.ee_len = cpu_to_le16(ee_len);\n\t\tmay_zeroout = ee_block + ee_len <= eof_block;\n\n\t\tdepth = newdepth;\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tif (ex2 != &newex)\n\t\t\tex2 = ex;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tallocated = map->m_len;\n\t}\n\t/*\n\t * If there was a change of depth as part of the\n\t * insertion of ex3 above, we need to update the length\n\t * of the ex1 extent again here\n\t */\n\tif (ex1 && ex1 != ex) {\n\t\tex1 = ex;\n\t\tex1->ee_len = cpu_to_le16(map->m_lblk - ee_block);\n\t\text4_ext_mark_uninitialized(ex1);\n\t\tex2 = &newex;\n\t}\n\t/*\n\t * ex2: map->m_lblk to map->m_lblk + map->m_len-1 : to be written\n\t * using direct I/O, uninitialised still.\n\t */\n\tex2->ee_block = cpu_to_le32(map->m_lblk);\n\text4_ext_store_pblock(ex2, newblock);\n\tex2->ee_len = cpu_to_le16(allocated);\n\text4_ext_mark_uninitialized(ex2);\n\tif (ex2 != ex)\n\t\tgoto insert;\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\text_debug(\"out here\\n\");\n\tgoto out;\ninsert:\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && may_zeroout) {\n\t\terr =  ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_block = orig_ex.ee_block;\n\t\tex->ee_len   = orig_ex.ee_len;\n\t\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\t\text4_ext_dirty(handle, inode, path + depth);\n\t\t/* zero out the first half */\n\t\treturn allocated;\n\t} else if (err)\n\t\tgoto fix_extent_len;\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err ? err : allocated;\n\nfix_extent_len:\n\tex->ee_block = orig_ex.ee_block;\n\tex->ee_len   = orig_ex.ee_len;\n\text4_ext_store_pblock(ex, ext4_ext_pblock(&orig_ex));\n\text4_ext_mark_uninitialized(ex);\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tstruct ext4_extent_header *eh;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)le32_to_cpu(ex->ee_block),\n\t\text4_ext_get_actual_len(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\tif (unlikely(!eh->eh_entries)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries == 0 and \"\n\t\t\t\t \"EOFBLOCKS_FL set\");\n\t\treturn -EIO;\n\t}\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical\"\n\t\t  \"block %llu, max_blocks %u, flags %d, allocated %u\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io && !(io->flag & EXT4_IO_END_UNWRITTEN)) {\n\t\t\tio->flag = EXT4_IO_END_UNWRITTEN;\n\t\t\tatomic_inc(&EXT4_I(inode)->i_aiodio_unwritten);\n\t\t} else\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0) {\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\n\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 0);\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex;\n\text4_fsblk_t newblock = 0;\n\tint err = 0, depth, ret;\n\tunsigned int allocated = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* check in cache */\n\tif (ext4_ext_in_cache(inode, map->m_lblk, &newex)) {\n\t\tif (!newex.ee_start_lo && !newex.ee_start_hi) {\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else {\n\t\t\t/* block is already allocated */\n\t\t\tnewblock = map->m_lblk\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext4_ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t(map->m_lblk - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/* Do not put uninitialized extent in the cache */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\t\t\tee_len, ee_start);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(handle,\n\t\t\t\t\tinode, map, path, flags, allocated,\n\t\t\t\t\tnewblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright);\n\tif (err)\n\t\tgoto out2;\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\tar.len = allocated;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * uninitialized extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\t\tif (io && !(io->flag & EXT4_IO_END_UNWRITTEN)) {\n\t\t\t\tio->flag = EXT4_IO_END_UNWRITTEN;\n\t\t\t\tatomic_inc(&EXT4_I(inode)->i_aiodio_unwritten);\n\t\t\t} else\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t}\n\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, ar.len);\n\tif (err)\n\t\tgoto out2;\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err) {\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, ext4_ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), 0);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 1);\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, map->m_lblk, allocated, newblock);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\ttrace_ext4_ext_map_blocks_exit(inode, map->m_lblk,\n\t\tnewblock, map->m_len, err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tint err = 0;\n\n\t/*\n\t * finish any pending end_io work so we won't run the risk of\n\t * converting any truncated blocks to initialized later\n\t */\n\text4_flush_completed_IO(inode);\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size & (sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\thandle_t *handle;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/* We only support the FALLOC_FL_KEEP_SIZE mode */\n\tif (mode & ~FALLOC_FL_KEEP_SIZE)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- map.m_lblk;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\n\tret = inode_newsize_ok(inode, (len + offset));\n\tif (ret) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\t\treturn ret;\n\t}\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk = map.m_lblk + ret;\n\t\tmap.m_len = max_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_CREATE_UNINIT_EXT);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((map.m_lblk + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = (map.m_lblk + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t (map.m_flags & EXT4_MAP_NEW));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks,\n\t\t\t\tret > 0 ? ret2 : ret);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, map.m_len);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, struct ext4_ext_path *path,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\tloff_t\tsize;\n\t__u32\tflags = 0;\n\tint\t\tret = 0;\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits;\n\n\tblksize_bits = inode->i_sb->s_blocksize_bits;\n\tlogical = (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_start == 0) {\n\t\t/*\n\t\t * No extent in extent-tree contains block @newex->ec_start,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t *\n\t\t * Holes or delayed-extents are processed as follows.\n\t\t * 1. lookup dirty pages with specified range in pagecache.\n\t\t *    If no page is got, then there is no delayed-extent and\n\t\t *    return with EXT_CONTINUE.\n\t\t * 2. find the 1st mapped buffer,\n\t\t * 3. check if the mapped buffer is both in the request range\n\t\t *    and a delayed buffer. If not, there is no delayed-extent,\n\t\t *    then return.\n\t\t * 4. a delayed-extent is found, the extent will be collected.\n\t\t */\n\t\text4_lblk_t\tend = 0;\n\t\tpgoff_t\t\tlast_offset;\n\t\tpgoff_t\t\toffset;\n\t\tpgoff_t\t\tindex;\n\t\tstruct page\t**pages = NULL;\n\t\tstruct buffer_head *bh = NULL;\n\t\tstruct buffer_head *head = NULL;\n\t\tunsigned int nr_pages = PAGE_SIZE / sizeof(struct page *);\n\n\t\tpages = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (pages == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\toffset = logical >> PAGE_SHIFT;\nrepeat:\n\t\tlast_offset = offset;\n\t\thead = NULL;\n\t\tret = find_get_pages_tag(inode->i_mapping, &offset,\n\t\t\t\t\tPAGECACHE_TAG_DIRTY, nr_pages, pages);\n\n\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t/* First time, try to find a mapped buffer. */\n\t\t\tif (ret == 0) {\nout:\n\t\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\t\tpage_cache_release(pages[index]);\n\t\t\t\t/* just a hole. */\n\t\t\t\tkfree(pages);\n\t\t\t\treturn EXT_CONTINUE;\n\t\t\t}\n\n\t\t\t/* Try to find the 1st mapped buffer. */\n\t\t\tend = ((__u64)pages[0]->index << PAGE_SHIFT) >>\n\t\t\t\t  blksize_bits;\n\t\t\tif (!page_has_buffers(pages[0]))\n\t\t\t\tgoto out;\n\t\t\thead = page_buffers(pages[0]);\n\t\t\tif (!head)\n\t\t\t\tgoto out;\n\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (buffer_mapped(bh)) {\n\t\t\t\t\t/* get the 1st mapped buffer. */\n\t\t\t\t\tif (end > newex->ec_block +\n\t\t\t\t\t\tnewex->ec_len)\n\t\t\t\t\t\t/* The buffer is out of\n\t\t\t\t\t\t * the request range.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tgoto out;\n\t\t\t\t\tgoto found_mapped_buffer;\n\t\t\t\t}\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\t/* No mapped buffer found. */\n\t\t\tgoto out;\n\t\t} else {\n\t\t\t/*Find contiguous delayed buffers. */\n\t\t\tif (ret > 0 && pages[0]->index == last_offset)\n\t\t\t\thead = page_buffers(pages[0]);\n\t\t\tbh = head;\n\t\t}\n\nfound_mapped_buffer:\n\t\tif (bh != NULL && buffer_delay(bh)) {\n\t\t\t/* 1st or contiguous delayed buffer found. */\n\t\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t\t/*\n\t\t\t\t * 1st delayed buffer found, record\n\t\t\t\t * the start of extent.\n\t\t\t\t */\n\t\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\t\tnewex->ec_block = end;\n\t\t\t\tlogical = (__u64)end << blksize_bits;\n\t\t\t}\n\t\t\t/* Find contiguous delayed buffers. */\n\t\t\tdo {\n\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\tfor (index = 1; index < ret; index++) {\n\t\t\t\tif (!page_has_buffers(pages[index])) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thead = page_buffers(pages[index]);\n\t\t\t\tif (!head) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (pages[index]->index !=\n\t\t\t\t\tpages[0]->index + index) {\n\t\t\t\t\t/* Blocks are not contiguous. */\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbh = head;\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\t\t/* Delayed-extent ends. */\n\t\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t\tend++;\n\t\t\t\t} while (bh != head);\n\t\t\t}\n\t\t} else if (!(flags & FIEMAP_EXTENT_DELALLOC))\n\t\t\t/* a hole found. */\n\t\t\tgoto out;\n\nfound_delayed_extent:\n\t\tnewex->ec_len = min(end - newex->ec_block,\n\t\t\t\t\t\t(ext4_lblk_t)EXT_INIT_MAX_LEN);\n\t\tif (ret == nr_pages && bh != NULL &&\n\t\t\tnewex->ec_len < EXT_INIT_MAX_LEN &&\n\t\t\tbuffer_delay(bh)) {\n\t\t\t/* Have not collected an extent and continue. */\n\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\tpage_cache_release(pages[index]);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tfor (index = 0; index < ret; index++)\n\t\t\tpage_cache_release(pages[index]);\n\t\tkfree(pages);\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\tsize = i_size_read(inode);\n\tif (logical + length >= size)\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\tret = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 1)\n\t\treturn EXT_BREAK;\n\treturn EXT_CONTINUE;\n}\n\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCK)\n\t\t\tlast_blk = EXT_MAX_BLOCK-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n\n"], "fixing_code": ["/*\n * Copyright (c) 2003-2006, Cluster File Systems, Inc, info@clusterfs.com\n * Written by Alex Tomas <alex@clusterfs.com>\n *\n * Architecture independence:\n *   Copyright (c) 2005, Bull S.A.\n *   Written by Pierre Peiffer <pierre.peiffer@bull.net>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n */\n\n/*\n * Extents support for EXT4\n *\n * TODO:\n *   - ext4*_error() should be used in some situations\n *   - analyze all BUG()/BUG_ON(), use -EIO where appropriate\n *   - smart tree reduction\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/jbd2.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/falloc.h>\n#include <asm/uaccess.h>\n#include <linux/fiemap.h>\n#include \"ext4_jbd2.h\"\n#include \"ext4_extents.h\"\n\n#include <trace/events/ext4.h>\n\nstatic int ext4_ext_truncate_extend_restart(handle_t *handle,\n\t\t\t\t\t    struct inode *inode,\n\t\t\t\t\t    int needed)\n{\n\tint err;\n\n\tif (!ext4_handle_valid(handle))\n\t\treturn 0;\n\tif (handle->h_buffer_credits > needed)\n\t\treturn 0;\n\terr = ext4_journal_extend(handle, needed);\n\tif (err <= 0)\n\t\treturn err;\n\terr = ext4_truncate_restart_trans(handle, inode, needed);\n\tif (err == 0)\n\t\terr = -EAGAIN;\n\n\treturn err;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n */\nstatic int ext4_ext_get_access(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\treturn ext4_journal_get_write_access(handle, path->p_bh);\n\t}\n\t/* path points to leaf/index in inode body */\n\t/* we use in-core data, no need to protect them */\n\treturn 0;\n}\n\n/*\n * could return:\n *  - EROFS\n *  - ENOMEM\n *  - EIO\n */\nstatic int ext4_ext_dirty(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\tif (path->p_bh) {\n\t\t/* path points to block */\n\t\terr = ext4_handle_dirty_metadata(handle, inode, path->p_bh);\n\t} else {\n\t\t/* path points to leaf/index in inode body */\n\t\terr = ext4_mark_inode_dirty(handle, inode);\n\t}\n\treturn err;\n}\n\nstatic ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      ext4_lblk_t block)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\text4_fsblk_t bg_start;\n\text4_fsblk_t last_block;\n\text4_grpblk_t colour;\n\text4_group_t block_group;\n\tint flex_size = ext4_flex_bg_size(EXT4_SB(inode->i_sb));\n\tint depth;\n\n\tif (path) {\n\t\tstruct ext4_extent *ex;\n\t\tdepth = path->p_depth;\n\n\t\t/*\n\t\t * Try to predict block placement assuming that we are\n\t\t * filling in a file which will eventually be\n\t\t * non-sparse --- i.e., in the case of libbfd writing\n\t\t * an ELF object sections out-of-order but in a way\n\t\t * the eventually results in a contiguous object or\n\t\t * executable file, or some database extending a table\n\t\t * space file.  However, this is actually somewhat\n\t\t * non-ideal if we are writing a sparse file such as\n\t\t * qemu or KVM writing a raw image file that is going\n\t\t * to stay fairly sparse, since it will end up\n\t\t * fragmenting the file system's free space.  Maybe we\n\t\t * should have some hueristics or some way to allow\n\t\t * userspace to pass a hint to file system,\n\t\t * especially if the latter case turns out to be\n\t\t * common.\n\t\t */\n\t\tex = path[depth].p_ext;\n\t\tif (ex) {\n\t\t\text4_fsblk_t ext_pblk = ext4_ext_pblock(ex);\n\t\t\text4_lblk_t ext_block = le32_to_cpu(ex->ee_block);\n\n\t\t\tif (block > ext_block)\n\t\t\t\treturn ext_pblk + (block - ext_block);\n\t\t\telse\n\t\t\t\treturn ext_pblk - (ext_block - block);\n\t\t}\n\n\t\t/* it looks like index is empty;\n\t\t * try to find starting block from index itself */\n\t\tif (path[depth].p_bh)\n\t\t\treturn path[depth].p_bh->b_blocknr;\n\t}\n\n\t/* OK. use inode's group */\n\tblock_group = ei->i_block_group;\n\tif (flex_size >= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {\n\t\t/*\n\t\t * If there are at least EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME\n\t\t * block groups per flexgroup, reserve the first block\n\t\t * group for directories and special files.  Regular\n\t\t * files will start at the second block group.  This\n\t\t * tends to speed up directory access and improves\n\t\t * fsck times.\n\t\t */\n\t\tblock_group &= ~(flex_size-1);\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tblock_group++;\n\t}\n\tbg_start = ext4_group_first_block_no(inode->i_sb, block_group);\n\tlast_block = ext4_blocks_count(EXT4_SB(inode->i_sb)->s_es) - 1;\n\n\t/*\n\t * If we are doing delayed allocation, we don't need take\n\t * colour into account.\n\t */\n\tif (test_opt(inode->i_sb, DELALLOC))\n\t\treturn bg_start;\n\n\tif (bg_start + EXT4_BLOCKS_PER_GROUP(inode->i_sb) <= last_block)\n\t\tcolour = (current->pid % 16) *\n\t\t\t(EXT4_BLOCKS_PER_GROUP(inode->i_sb) / 16);\n\telse\n\t\tcolour = (current->pid % 16) * ((last_block - bg_start) / 16);\n\treturn bg_start + colour + block;\n}\n\n/*\n * Allocation for a meta data block\n */\nstatic ext4_fsblk_t\next4_ext_new_meta_block(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path,\n\t\t\tstruct ext4_extent *ex, int *err)\n{\n\text4_fsblk_t goal, newblock;\n\n\tgoal = ext4_ext_find_goal(inode, path, le32_to_cpu(ex->ee_block));\n\tnewblock = ext4_new_meta_blocks(handle, inode, goal, NULL, err);\n\treturn newblock;\n}\n\nstatic inline int ext4_ext_space_block(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 6)\n\t\t\tsize = 6;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_block_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = (inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t\t/ sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 5)\n\t\t\tsize = 5;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 3)\n\t\t\tsize = 3;\n#endif\n\t}\n\treturn size;\n}\n\nstatic inline int ext4_ext_space_root_idx(struct inode *inode, int check)\n{\n\tint size;\n\n\tsize = sizeof(EXT4_I(inode)->i_data);\n\tsize -= sizeof(struct ext4_extent_header);\n\tsize /= sizeof(struct ext4_extent_idx);\n\tif (!check) {\n#ifdef AGGRESSIVE_TEST\n\t\tif (size > 4)\n\t\t\tsize = 4;\n#endif\n\t}\n\treturn size;\n}\n\n/*\n * Calculate the number of metadata blocks needed\n * to allocate @blocks\n * Worse case is one block per extent\n */\nint ext4_ext_calc_metadata_amount(struct inode *inode, ext4_lblk_t lblock)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint idxs, num = 0;\n\n\tidxs = ((inode->i_sb->s_blocksize - sizeof(struct ext4_extent_header))\n\t\t/ sizeof(struct ext4_extent_idx));\n\n\t/*\n\t * If the new delayed allocation block is contiguous with the\n\t * previous da block, it can share index blocks with the\n\t * previous block, so we only need to allocate a new index\n\t * block every idxs leaf blocks.  At ldxs**2 blocks, we need\n\t * an additional index block, and at ldxs**3 blocks, yet\n\t * another index blocks.\n\t */\n\tif (ei->i_da_metadata_calc_len &&\n\t    ei->i_da_metadata_calc_last_lblock+1 == lblock) {\n\t\tif ((ei->i_da_metadata_calc_len % idxs) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs)) == 0)\n\t\t\tnum++;\n\t\tif ((ei->i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {\n\t\t\tnum++;\n\t\t\tei->i_da_metadata_calc_len = 0;\n\t\t} else\n\t\t\tei->i_da_metadata_calc_len++;\n\t\tei->i_da_metadata_calc_last_lblock++;\n\t\treturn num;\n\t}\n\n\t/*\n\t * In the worst case we need a new set of index blocks at\n\t * every level of the inode's extent tree.\n\t */\n\tei->i_da_metadata_calc_len = 1;\n\tei->i_da_metadata_calc_last_lblock = lblock;\n\treturn ext_depth(inode) + 1;\n}\n\nstatic int\next4_ext_max_entries(struct inode *inode, int depth)\n{\n\tint max;\n\n\tif (depth == ext_depth(inode)) {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_root(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_root_idx(inode, 1);\n\t} else {\n\t\tif (depth == 0)\n\t\t\tmax = ext4_ext_space_block(inode, 1);\n\t\telse\n\t\t\tmax = ext4_ext_space_block_idx(inode, 1);\n\t}\n\n\treturn max;\n}\n\nstatic int ext4_valid_extent(struct inode *inode, struct ext4_extent *ext)\n{\n\text4_fsblk_t block = ext4_ext_pblock(ext);\n\tint len = ext4_ext_get_actual_len(ext);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, len);\n}\n\nstatic int ext4_valid_extent_idx(struct inode *inode,\n\t\t\t\tstruct ext4_extent_idx *ext_idx)\n{\n\text4_fsblk_t block = ext4_idx_pblock(ext_idx);\n\n\treturn ext4_data_block_valid(EXT4_SB(inode->i_sb), block, 1);\n}\n\nstatic int ext4_valid_extent_entries(struct inode *inode,\n\t\t\t\tstruct ext4_extent_header *eh,\n\t\t\t\tint depth)\n{\n\tstruct ext4_extent *ext;\n\tstruct ext4_extent_idx *ext_idx;\n\tunsigned short entries;\n\tif (eh->eh_entries == 0)\n\t\treturn 1;\n\n\tentries = le16_to_cpu(eh->eh_entries);\n\n\tif (depth == 0) {\n\t\t/* leaf entries */\n\t\text = EXT_FIRST_EXTENT(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent(inode, ext))\n\t\t\t\treturn 0;\n\t\t\text++;\n\t\t\tentries--;\n\t\t}\n\t} else {\n\t\text_idx = EXT_FIRST_INDEX(eh);\n\t\twhile (entries) {\n\t\t\tif (!ext4_valid_extent_idx(inode, ext_idx))\n\t\t\t\treturn 0;\n\t\t\text_idx++;\n\t\t\tentries--;\n\t\t}\n\t}\n\treturn 1;\n}\n\nstatic int __ext4_ext_check(const char *function, unsigned int line,\n\t\t\t    struct inode *inode, struct ext4_extent_header *eh,\n\t\t\t    int depth)\n{\n\tconst char *error_msg;\n\tint max = 0;\n\n\tif (unlikely(eh->eh_magic != EXT4_EXT_MAGIC)) {\n\t\terror_msg = \"invalid magic\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_depth) != depth)) {\n\t\terror_msg = \"unexpected eh_depth\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(eh->eh_max == 0)) {\n\t\terror_msg = \"invalid eh_max\";\n\t\tgoto corrupted;\n\t}\n\tmax = ext4_ext_max_entries(inode, depth);\n\tif (unlikely(le16_to_cpu(eh->eh_max) > max)) {\n\t\terror_msg = \"too large eh_max\";\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(le16_to_cpu(eh->eh_entries) > le16_to_cpu(eh->eh_max))) {\n\t\terror_msg = \"invalid eh_entries\";\n\t\tgoto corrupted;\n\t}\n\tif (!ext4_valid_extent_entries(inode, eh, depth)) {\n\t\terror_msg = \"invalid extent entries\";\n\t\tgoto corrupted;\n\t}\n\treturn 0;\n\ncorrupted:\n\text4_error_inode(inode, function, line, 0,\n\t\t\t\"bad header/extent: %s - magic %x, \"\n\t\t\t\"entries %u, max %u(%u), depth %u(%u)\",\n\t\t\terror_msg, le16_to_cpu(eh->eh_magic),\n\t\t\tle16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max),\n\t\t\tmax, le16_to_cpu(eh->eh_depth), depth);\n\n\treturn -EIO;\n}\n\n#define ext4_ext_check(inode, eh, depth)\t\\\n\t__ext4_ext_check(__func__, __LINE__, inode, eh, depth)\n\nint ext4_ext_check_inode(struct inode *inode)\n{\n\treturn ext4_ext_check(inode, ext_inode_hdr(inode), ext_depth(inode));\n}\n\n#ifdef EXT_DEBUG\nstatic void ext4_ext_show_path(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint k, l = path->p_depth;\n\n\text_debug(\"path:\");\n\tfor (k = 0; k <= l; k++, path++) {\n\t\tif (path->p_idx) {\n\t\t  ext_debug(\"  %d->%llu\", le32_to_cpu(path->p_idx->ei_block),\n\t\t\t    ext4_idx_pblock(path->p_idx));\n\t\t} else if (path->p_ext) {\n\t\t\text_debug(\"  %d:[%d]%d:%llu \",\n\t\t\t\t  le32_to_cpu(path->p_ext->ee_block),\n\t\t\t\t  ext4_ext_is_uninitialized(path->p_ext),\n\t\t\t\t  ext4_ext_get_actual_len(path->p_ext),\n\t\t\t\t  ext4_ext_pblock(path->p_ext));\n\t\t} else\n\t\t\text_debug(\"  []\");\n\t}\n\text_debug(\"\\n\");\n}\n\nstatic void ext4_ext_show_leaf(struct inode *inode, struct ext4_ext_path *path)\n{\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex;\n\tint i;\n\n\tif (!path)\n\t\treturn;\n\n\teh = path[depth].p_hdr;\n\tex = EXT_FIRST_EXTENT(eh);\n\n\text_debug(\"Displaying leaf extents for inode %lu\\n\", inode->i_ino);\n\n\tfor (i = 0; i < le16_to_cpu(eh->eh_entries); i++, ex++) {\n\t\text_debug(\"%d:[%d]%d:%llu \", le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex), ext4_ext_pblock(ex));\n\t}\n\text_debug(\"\\n\");\n}\n#else\n#define ext4_ext_show_path(inode, path)\n#define ext4_ext_show_leaf(inode, path)\n#endif\n\nvoid ext4_ext_drop_refs(struct ext4_ext_path *path)\n{\n\tint depth = path->p_depth;\n\tint i;\n\n\tfor (i = 0; i <= depth; i++, path++)\n\t\tif (path->p_bh) {\n\t\t\tbrelse(path->p_bh);\n\t\t\tpath->p_bh = NULL;\n\t\t}\n}\n\n/*\n * ext4_ext_binsearch_idx:\n * binary search for the closest index of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch_idx(struct inode *inode,\n\t\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent_idx *r, *l, *m;\n\n\n\text_debug(\"binsearch for %u(idx):  \", block);\n\n\tl = EXT_FIRST_INDEX(eh) + 1;\n\tr = EXT_LAST_INDEX(eh);\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ei_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ei_block),\n\t\t\t\tm, le32_to_cpu(m->ei_block),\n\t\t\t\tr, le32_to_cpu(r->ei_block));\n\t}\n\n\tpath->p_idx = l - 1;\n\text_debug(\"  -> %d->%lld \", le32_to_cpu(path->p_idx->ei_block),\n\t\t  ext4_idx_pblock(path->p_idx));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent_idx *chix, *ix;\n\t\tint k;\n\n\t\tchix = ix = EXT_FIRST_INDEX(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ix++) {\n\t\t  if (k != 0 &&\n\t\t      le32_to_cpu(ix->ei_block) <= le32_to_cpu(ix[-1].ei_block)) {\n\t\t\t\tprintk(KERN_DEBUG \"k=%d, ix=0x%p, \"\n\t\t\t\t       \"first=0x%p\\n\", k,\n\t\t\t\t       ix, EXT_FIRST_INDEX(eh));\n\t\t\t\tprintk(KERN_DEBUG \"%u <= %u\\n\",\n\t\t\t\t       le32_to_cpu(ix->ei_block),\n\t\t\t\t       le32_to_cpu(ix[-1].ei_block));\n\t\t\t}\n\t\t\tBUG_ON(k && le32_to_cpu(ix->ei_block)\n\t\t\t\t\t   <= le32_to_cpu(ix[-1].ei_block));\n\t\t\tif (block < le32_to_cpu(ix->ei_block))\n\t\t\t\tbreak;\n\t\t\tchix = ix;\n\t\t}\n\t\tBUG_ON(chix != path->p_idx);\n\t}\n#endif\n\n}\n\n/*\n * ext4_ext_binsearch:\n * binary search for closest extent of the given block\n * the header must be checked before calling this\n */\nstatic void\next4_ext_binsearch(struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t block)\n{\n\tstruct ext4_extent_header *eh = path->p_hdr;\n\tstruct ext4_extent *r, *l, *m;\n\n\tif (eh->eh_entries == 0) {\n\t\t/*\n\t\t * this leaf is empty:\n\t\t * we get such a leaf in split/add case\n\t\t */\n\t\treturn;\n\t}\n\n\text_debug(\"binsearch for %u:  \", block);\n\n\tl = EXT_FIRST_EXTENT(eh) + 1;\n\tr = EXT_LAST_EXTENT(eh);\n\n\twhile (l <= r) {\n\t\tm = l + (r - l) / 2;\n\t\tif (block < le32_to_cpu(m->ee_block))\n\t\t\tr = m - 1;\n\t\telse\n\t\t\tl = m + 1;\n\t\text_debug(\"%p(%u):%p(%u):%p(%u) \", l, le32_to_cpu(l->ee_block),\n\t\t\t\tm, le32_to_cpu(m->ee_block),\n\t\t\t\tr, le32_to_cpu(r->ee_block));\n\t}\n\n\tpath->p_ext = l - 1;\n\text_debug(\"  -> %d:%llu:[%d]%d \",\n\t\t\tle32_to_cpu(path->p_ext->ee_block),\n\t\t\text4_ext_pblock(path->p_ext),\n\t\t\text4_ext_is_uninitialized(path->p_ext),\n\t\t\text4_ext_get_actual_len(path->p_ext));\n\n#ifdef CHECK_BINSEARCH\n\t{\n\t\tstruct ext4_extent *chex, *ex;\n\t\tint k;\n\n\t\tchex = ex = EXT_FIRST_EXTENT(eh);\n\t\tfor (k = 0; k < le16_to_cpu(eh->eh_entries); k++, ex++) {\n\t\t\tBUG_ON(k && le32_to_cpu(ex->ee_block)\n\t\t\t\t\t  <= le32_to_cpu(ex[-1].ee_block));\n\t\t\tif (block < le32_to_cpu(ex->ee_block))\n\t\t\t\tbreak;\n\t\t\tchex = ex;\n\t\t}\n\t\tBUG_ON(chex != path->p_ext);\n\t}\n#endif\n\n}\n\nint ext4_ext_tree_init(handle_t *handle, struct inode *inode)\n{\n\tstruct ext4_extent_header *eh;\n\n\teh = ext_inode_hdr(inode);\n\teh->eh_depth = 0;\n\teh->eh_entries = 0;\n\teh->eh_magic = EXT4_EXT_MAGIC;\n\teh->eh_max = cpu_to_le16(ext4_ext_space_root(inode, 0));\n\text4_mark_inode_dirty(handle, inode);\n\text4_ext_invalidate_cache(inode);\n\treturn 0;\n}\n\nstruct ext4_ext_path *\next4_ext_find_extent(struct inode *inode, ext4_lblk_t block,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct buffer_head *bh;\n\tshort int depth, i, ppos = 0, alloc = 0;\n\n\teh = ext_inode_hdr(inode);\n\tdepth = ext_depth(inode);\n\n\t/* account possible depth increase */\n\tif (!path) {\n\t\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 2),\n\t\t\t\tGFP_NOFS);\n\t\tif (!path)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\talloc = 1;\n\t}\n\tpath[0].p_hdr = eh;\n\tpath[0].p_bh = NULL;\n\n\ti = depth;\n\t/* walk through the tree */\n\twhile (i) {\n\t\tint need_to_validate = 0;\n\n\t\text_debug(\"depth %d: num %d, max %d\\n\",\n\t\t\t  ppos, le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\n\t\text4_ext_binsearch_idx(inode, path + ppos, block);\n\t\tpath[ppos].p_block = ext4_idx_pblock(path[ppos].p_idx);\n\t\tpath[ppos].p_depth = i;\n\t\tpath[ppos].p_ext = NULL;\n\n\t\tbh = sb_getblk(inode->i_sb, path[ppos].p_block);\n\t\tif (unlikely(!bh))\n\t\t\tgoto err;\n\t\tif (!bh_uptodate_or_lock(bh)) {\n\t\t\ttrace_ext4_ext_load_extent(inode, block,\n\t\t\t\t\t\tpath[ppos].p_block);\n\t\t\tif (bh_submit_read(bh) < 0) {\n\t\t\t\tput_bh(bh);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t/* validate the extent entries */\n\t\t\tneed_to_validate = 1;\n\t\t}\n\t\teh = ext_block_hdr(bh);\n\t\tppos++;\n\t\tif (unlikely(ppos > depth)) {\n\t\t\tput_bh(bh);\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"ppos %d > depth %d\", ppos, depth);\n\t\t\tgoto err;\n\t\t}\n\t\tpath[ppos].p_bh = bh;\n\t\tpath[ppos].p_hdr = eh;\n\t\ti--;\n\n\t\tif (need_to_validate && ext4_ext_check(inode, eh, i))\n\t\t\tgoto err;\n\t}\n\n\tpath[ppos].p_depth = i;\n\tpath[ppos].p_ext = NULL;\n\tpath[ppos].p_idx = NULL;\n\n\t/* find extent */\n\text4_ext_binsearch(inode, path + ppos, block);\n\t/* if not an empty leaf */\n\tif (path[ppos].p_ext)\n\t\tpath[ppos].p_block = ext4_ext_pblock(path[ppos].p_ext);\n\n\text4_ext_show_path(inode, path);\n\n\treturn path;\n\nerr:\n\text4_ext_drop_refs(path);\n\tif (alloc)\n\t\tkfree(path);\n\treturn ERR_PTR(-EIO);\n}\n\n/*\n * ext4_ext_insert_index:\n * insert new index [@logical;@ptr] into the block at @curp;\n * check where to insert: before @curp or after @curp\n */\nstatic int ext4_ext_insert_index(handle_t *handle, struct inode *inode,\n\t\t\t\t struct ext4_ext_path *curp,\n\t\t\t\t int logical, ext4_fsblk_t ptr)\n{\n\tstruct ext4_extent_idx *ix;\n\tint len, err;\n\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\treturn err;\n\n\tif (unlikely(logical == le32_to_cpu(curp->p_idx->ei_block))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\tlen = EXT_MAX_INDEX(curp->p_hdr) - curp->p_idx;\n\tif (logical > le32_to_cpu(curp->p_idx->ei_block)) {\n\t\t/* insert after */\n\t\tif (curp->p_idx != EXT_LAST_INDEX(curp->p_hdr)) {\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent_idx);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert new index %d after: %llu. \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tlogical, ptr, len,\n\t\t\t\t\t(curp->p_idx + 1), (curp->p_idx + 2));\n\t\t\tmemmove(curp->p_idx + 2, curp->p_idx + 1, len);\n\t\t}\n\t\tix = curp->p_idx + 1;\n\t} else {\n\t\t/* insert before */\n\t\tlen = len * sizeof(struct ext4_extent_idx);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert new index %d before: %llu. \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tlogical, ptr, len,\n\t\t\t\tcurp->p_idx, (curp->p_idx + 1));\n\t\tmemmove(curp->p_idx + 1, curp->p_idx, len);\n\t\tix = curp->p_idx;\n\t}\n\n\tix->ei_block = cpu_to_le32(logical);\n\text4_idx_store_pblock(ix, ptr);\n\tle16_add_cpu(&curp->p_hdr->eh_entries, 1);\n\n\tif (unlikely(le16_to_cpu(curp->p_hdr->eh_entries)\n\t\t\t     > le16_to_cpu(curp->p_hdr->eh_max))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d == ei_block %d!\",\n\t\t\t\t logical, le32_to_cpu(curp->p_idx->ei_block));\n\t\treturn -EIO;\n\t}\n\tif (unlikely(ix > EXT_LAST_INDEX(curp->p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"ix > EXT_LAST_INDEX!\");\n\t\treturn -EIO;\n\t}\n\n\terr = ext4_ext_dirty(handle, inode, curp);\n\text4_std_error(inode->i_sb, err);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_split:\n * inserts new subtree into the path, using free index entry\n * at depth @at:\n * - allocates all needed blocks (new leaf and all intermediate index blocks)\n * - makes decision where to split\n * - moves remaining extents and index entries (right to the split point)\n *   into the newly allocated blocks\n * - initializes subtree\n */\nstatic int ext4_ext_split(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int at)\n{\n\tstruct buffer_head *bh = NULL;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent_header *neh;\n\tstruct ext4_extent_idx *fidx;\n\tstruct ext4_extent *ex;\n\tint i = at, k, m, a;\n\text4_fsblk_t newblock, oldblock;\n\t__le32 border;\n\text4_fsblk_t *ablocks = NULL; /* array of allocated blocks */\n\tint err = 0;\n\n\t/* make decision: where to split? */\n\t/* FIXME: now decision is simplest: at current extent */\n\n\t/* if current leaf will be split, then we should use\n\t * border from split point */\n\tif (unlikely(path[depth].p_ext > EXT_MAX_EXTENT(path[depth].p_hdr))) {\n\t\tEXT4_ERROR_INODE(inode, \"p_ext > EXT_MAX_EXTENT!\");\n\t\treturn -EIO;\n\t}\n\tif (path[depth].p_ext != EXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\tborder = path[depth].p_ext[1].ee_block;\n\t\text_debug(\"leaf will be split.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\t  le32_to_cpu(border));\n\t} else {\n\t\tborder = newext->ee_block;\n\t\text_debug(\"leaf will be added.\"\n\t\t\t\t\" next leaf starts at %d\\n\",\n\t\t\t\tle32_to_cpu(border));\n\t}\n\n\t/*\n\t * If error occurs, then we break processing\n\t * and mark filesystem read-only. index won't\n\t * be inserted and tree will be in consistent\n\t * state. Next mount will repair buffers too.\n\t */\n\n\t/*\n\t * Get array to track all allocated blocks.\n\t * We need this to handle errors and free blocks\n\t * upon them.\n\t */\n\tablocks = kzalloc(sizeof(ext4_fsblk_t) * depth, GFP_NOFS);\n\tif (!ablocks)\n\t\treturn -ENOMEM;\n\n\t/* allocate all needed blocks */\n\text_debug(\"allocate %d blocks for indexes/leaf\\n\", depth - at);\n\tfor (a = 0; a < depth - at; a++) {\n\t\tnewblock = ext4_ext_new_meta_block(handle, inode, path,\n\t\t\t\t\t\t   newext, &err);\n\t\tif (newblock == 0)\n\t\t\tgoto cleanup;\n\t\tablocks[a] = newblock;\n\t}\n\n\t/* initialize new leaf */\n\tnewblock = ablocks[--a];\n\tif (unlikely(newblock == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"newblock == 0!\");\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err)\n\t\tgoto cleanup;\n\n\tneh = ext_block_hdr(bh);\n\tneh->eh_entries = 0;\n\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tneh->eh_depth = 0;\n\tex = EXT_FIRST_EXTENT(neh);\n\n\t/* move remainder of path[depth] to the new leaf */\n\tif (unlikely(path[depth].p_hdr->eh_entries !=\n\t\t     path[depth].p_hdr->eh_max)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh_entries %d != eh_max %d!\",\n\t\t\t\t path[depth].p_hdr->eh_entries,\n\t\t\t\t path[depth].p_hdr->eh_max);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\t/* start copy from next extent */\n\t/* TODO: we could do it by single memmove */\n\tm = 0;\n\tpath[depth].p_ext++;\n\twhile (path[depth].p_ext <=\n\t\t\tEXT_MAX_EXTENT(path[depth].p_hdr)) {\n\t\text_debug(\"move %d:%llu:[%d]%d in new leaf %llu\\n\",\n\t\t\t\tle32_to_cpu(path[depth].p_ext->ee_block),\n\t\t\t\text4_ext_pblock(path[depth].p_ext),\n\t\t\t\text4_ext_is_uninitialized(path[depth].p_ext),\n\t\t\t\text4_ext_get_actual_len(path[depth].p_ext),\n\t\t\t\tnewblock);\n\t\t/*memmove(ex++, path[depth].p_ext++,\n\t\t\t\tsizeof(struct ext4_extent));\n\t\tneh->eh_entries++;*/\n\t\tpath[depth].p_ext++;\n\t\tm++;\n\t}\n\tif (m) {\n\t\tmemmove(ex, path[depth].p_ext-m, sizeof(struct ext4_extent)*m);\n\t\tle16_add_cpu(&neh->eh_entries, m);\n\t}\n\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto cleanup;\n\tbrelse(bh);\n\tbh = NULL;\n\n\t/* correct old leaf */\n\tif (m) {\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tle16_add_cpu(&path[depth].p_hdr->eh_entries, -m);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t}\n\n\t/* create intermediate indexes */\n\tk = depth - at - 1;\n\tif (unlikely(k < 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"k %d < 0!\", k);\n\t\terr = -EIO;\n\t\tgoto cleanup;\n\t}\n\tif (k)\n\t\text_debug(\"create %d intermediate indices\\n\", k);\n\t/* insert new index into current index block */\n\t/* current depth stored in i var */\n\ti = depth - 1;\n\twhile (k--) {\n\t\toldblock = newblock;\n\t\tnewblock = ablocks[--a];\n\t\tbh = sb_getblk(inode->i_sb, newblock);\n\t\tif (!bh) {\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\tlock_buffer(bh);\n\n\t\terr = ext4_journal_get_create_access(handle, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\n\t\tneh = ext_block_hdr(bh);\n\t\tneh->eh_entries = cpu_to_le16(1);\n\t\tneh->eh_magic = EXT4_EXT_MAGIC;\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\t\tneh->eh_depth = cpu_to_le16(depth - i);\n\t\tfidx = EXT_FIRST_INDEX(neh);\n\t\tfidx->ei_block = border;\n\t\text4_idx_store_pblock(fidx, oldblock);\n\n\t\text_debug(\"int.index at %d (block %llu): %u -> %llu\\n\",\n\t\t\t\ti, newblock, le32_to_cpu(border), oldblock);\n\t\t/* copy indexes */\n\t\tm = 0;\n\t\tpath[i].p_idx++;\n\n\t\text_debug(\"cur 0x%p, last 0x%p\\n\", path[i].p_idx,\n\t\t\t\tEXT_MAX_INDEX(path[i].p_hdr));\n\t\tif (unlikely(EXT_MAX_INDEX(path[i].p_hdr) !=\n\t\t\t\t\tEXT_LAST_INDEX(path[i].p_hdr))) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_MAX_INDEX != EXT_LAST_INDEX ee_block %d!\",\n\t\t\t\t\t le32_to_cpu(path[i].p_ext->ee_block));\n\t\t\terr = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\twhile (path[i].p_idx <= EXT_MAX_INDEX(path[i].p_hdr)) {\n\t\t\text_debug(\"%d: move %d:%llu in new index %llu\\n\", i,\n\t\t\t\t\tle32_to_cpu(path[i].p_idx->ei_block),\n\t\t\t\t\text4_idx_pblock(path[i].p_idx),\n\t\t\t\t\tnewblock);\n\t\t\t/*memmove(++fidx, path[i].p_idx++,\n\t\t\t\t\tsizeof(struct ext4_extent_idx));\n\t\t\tneh->eh_entries++;\n\t\t\tBUG_ON(neh->eh_entries > neh->eh_max);*/\n\t\t\tpath[i].p_idx++;\n\t\t\tm++;\n\t\t}\n\t\tif (m) {\n\t\t\tmemmove(++fidx, path[i].p_idx - m,\n\t\t\t\tsizeof(struct ext4_extent_idx) * m);\n\t\t\tle16_add_cpu(&neh->eh_entries, m);\n\t\t}\n\t\tset_buffer_uptodate(bh);\n\t\tunlock_buffer(bh);\n\n\t\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\tbrelse(bh);\n\t\tbh = NULL;\n\n\t\t/* correct old index */\n\t\tif (m) {\n\t\t\terr = ext4_ext_get_access(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t\tle16_add_cpu(&path[i].p_hdr->eh_entries, -m);\n\t\t\terr = ext4_ext_dirty(handle, inode, path + i);\n\t\t\tif (err)\n\t\t\t\tgoto cleanup;\n\t\t}\n\n\t\ti--;\n\t}\n\n\t/* insert new index */\n\terr = ext4_ext_insert_index(handle, inode, path + at,\n\t\t\t\t    le32_to_cpu(border), newblock);\n\ncleanup:\n\tif (bh) {\n\t\tif (buffer_locked(bh))\n\t\t\tunlock_buffer(bh);\n\t\tbrelse(bh);\n\t}\n\n\tif (err) {\n\t\t/* free all allocated blocks in error case */\n\t\tfor (i = 0; i < depth; i++) {\n\t\t\tif (!ablocks[i])\n\t\t\t\tcontinue;\n\t\t\text4_free_blocks(handle, inode, NULL, ablocks[i], 1,\n\t\t\t\t\t EXT4_FREE_BLOCKS_METADATA);\n\t\t}\n\t}\n\tkfree(ablocks);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_grow_indepth:\n * implements tree growing procedure:\n * - allocates new block\n * - moves top-level data (index block or leaf) into the new block\n * - initializes new top-level, creating index that points to the\n *   just created block\n */\nstatic int ext4_ext_grow_indepth(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp = path;\n\tstruct ext4_extent_header *neh;\n\tstruct buffer_head *bh;\n\text4_fsblk_t newblock;\n\tint err = 0;\n\n\tnewblock = ext4_ext_new_meta_block(handle, inode, path, newext, &err);\n\tif (newblock == 0)\n\t\treturn err;\n\n\tbh = sb_getblk(inode->i_sb, newblock);\n\tif (!bh) {\n\t\terr = -EIO;\n\t\text4_std_error(inode->i_sb, err);\n\t\treturn err;\n\t}\n\tlock_buffer(bh);\n\n\terr = ext4_journal_get_create_access(handle, bh);\n\tif (err) {\n\t\tunlock_buffer(bh);\n\t\tgoto out;\n\t}\n\n\t/* move top-level index/leaf into new block */\n\tmemmove(bh->b_data, curp->p_hdr, sizeof(EXT4_I(inode)->i_data));\n\n\t/* set size of new block */\n\tneh = ext_block_hdr(bh);\n\t/* old root could have indexes or leaves\n\t * so calculate e_max right way */\n\tif (ext_depth(inode))\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block_idx(inode, 0));\n\telse\n\t\tneh->eh_max = cpu_to_le16(ext4_ext_space_block(inode, 0));\n\tneh->eh_magic = EXT4_EXT_MAGIC;\n\tset_buffer_uptodate(bh);\n\tunlock_buffer(bh);\n\n\terr = ext4_handle_dirty_metadata(handle, inode, bh);\n\tif (err)\n\t\tgoto out;\n\n\t/* create index in new top-level index: num,max,pointer */\n\terr = ext4_ext_get_access(handle, inode, curp);\n\tif (err)\n\t\tgoto out;\n\n\tcurp->p_hdr->eh_magic = EXT4_EXT_MAGIC;\n\tcurp->p_hdr->eh_max = cpu_to_le16(ext4_ext_space_root_idx(inode, 0));\n\tcurp->p_hdr->eh_entries = cpu_to_le16(1);\n\tcurp->p_idx = EXT_FIRST_INDEX(curp->p_hdr);\n\n\tif (path[0].p_hdr->eh_depth)\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_INDEX(path[0].p_hdr)->ei_block;\n\telse\n\t\tcurp->p_idx->ei_block =\n\t\t\tEXT_FIRST_EXTENT(path[0].p_hdr)->ee_block;\n\text4_idx_store_pblock(curp->p_idx, newblock);\n\n\tneh = ext_inode_hdr(inode);\n\text_debug(\"new root: num %d(%d), lblock %d, ptr %llu\\n\",\n\t\t  le16_to_cpu(neh->eh_entries), le16_to_cpu(neh->eh_max),\n\t\t  le32_to_cpu(EXT_FIRST_INDEX(neh)->ei_block),\n\t\t  ext4_idx_pblock(EXT_FIRST_INDEX(neh)));\n\n\tneh->eh_depth = cpu_to_le16(path->p_depth + 1);\n\terr = ext4_ext_dirty(handle, inode, curp);\nout:\n\tbrelse(bh);\n\n\treturn err;\n}\n\n/*\n * ext4_ext_create_new_leaf:\n * finds empty index and adds new leaf.\n * if no free index is found, then it requests in-depth growing.\n */\nstatic int ext4_ext_create_new_leaf(handle_t *handle, struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tstruct ext4_extent *newext)\n{\n\tstruct ext4_ext_path *curp;\n\tint depth, i, err = 0;\n\nrepeat:\n\ti = depth = ext_depth(inode);\n\n\t/* walk up to the tree and look for free index entry */\n\tcurp = path + depth;\n\twhile (i > 0 && !EXT_HAS_FREE_INDEX(curp)) {\n\t\ti--;\n\t\tcurp--;\n\t}\n\n\t/* we use already allocated block for index block,\n\t * so subsequent data blocks should be contiguous */\n\tif (EXT_HAS_FREE_INDEX(curp)) {\n\t\t/* if we found index with free entry, then use that\n\t\t * entry: create all needed subtree and add new leaf */\n\t\terr = ext4_ext_split(handle, inode, path, newext, i);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t    (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path))\n\t\t\terr = PTR_ERR(path);\n\t} else {\n\t\t/* tree is full, time to grow in depth */\n\t\terr = ext4_ext_grow_indepth(handle, inode, path, newext);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t/* refill path */\n\t\text4_ext_drop_refs(path);\n\t\tpath = ext4_ext_find_extent(inode,\n\t\t\t\t   (ext4_lblk_t)le32_to_cpu(newext->ee_block),\n\t\t\t\t    path);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * only first (depth 0 -> 1) produces free space;\n\t\t * in all other cases we have to split the grown tree\n\t\t */\n\t\tdepth = ext_depth(inode);\n\t\tif (path[depth].p_hdr->eh_entries == path[depth].p_hdr->eh_max) {\n\t\t\t/* now we need to split */\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/*\n * search the closest allocated block to the left for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_left(struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\text4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\tint depth, ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"EXT_FIRST_EXTENT != ex *logical %d ee_block %d!\",\n\t\t\t\t\t *logical, le32_to_cpu(ex->ee_block));\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t  \"ix (%d) != EXT_FIRST_INDEX (%d) (depth %d)!\",\n\t\t\t\t  ix != NULL ? ix->ei_block : 0,\n\t\t\t\t  EXT_FIRST_INDEX(path[depth].p_hdr) != NULL ?\n\t\t\t\t    EXT_FIRST_INDEX(path[depth].p_hdr)->ei_block : 0,\n\t\t\t\t  depth);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\t*logical = le32_to_cpu(ex->ee_block) + ee_len - 1;\n\t*phys = ext4_ext_pblock(ex) + ee_len - 1;\n\treturn 0;\n}\n\n/*\n * search the closest allocated block to the right for *logical\n * and returns it at @logical + it's physical address at @phys\n * if *logical is the smallest allocated block, the function\n * returns 0 at @phys\n * return value contains 0 (success) or error code\n */\nstatic int ext4_ext_search_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t ext4_lblk_t *logical, ext4_fsblk_t *phys)\n{\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent_idx *ix;\n\tstruct ext4_extent *ex;\n\text4_fsblk_t block;\n\tint depth;\t/* Note, NOT eh_depth; depth from top of tree */\n\tint ee_len;\n\n\tif (unlikely(path == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path == NULL *logical %d!\", *logical);\n\t\treturn -EIO;\n\t}\n\tdepth = path->p_depth;\n\t*phys = 0;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn 0;\n\n\t/* usually extent in the path covers blocks smaller\n\t * then *logical, but it can be that extent is the\n\t * first one in the file */\n\n\tex = path[depth].p_ext;\n\tee_len = ext4_ext_get_actual_len(ex);\n\tif (*logical < le32_to_cpu(ex->ee_block)) {\n\t\tif (unlikely(EXT_FIRST_EXTENT(path[depth].p_hdr) != ex)) {\n\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t \"first_extent(path[%d].p_hdr) != ex\",\n\t\t\t\t\t depth);\n\t\t\treturn -EIO;\n\t\t}\n\t\twhile (--depth >= 0) {\n\t\t\tix = path[depth].p_idx;\n\t\t\tif (unlikely(ix != EXT_FIRST_INDEX(path[depth].p_hdr))) {\n\t\t\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t\t\t \"ix != EXT_FIRST_INDEX *logical %d!\",\n\t\t\t\t\t\t *logical);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext4_ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\tif (unlikely(*logical < (le32_to_cpu(ex->ee_block) + ee_len))) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"logical %d < ee_block %d + ee_len %d!\",\n\t\t\t\t *logical, le32_to_cpu(ex->ee_block), ee_len);\n\t\treturn -EIO;\n\t}\n\n\tif (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {\n\t\t/* next allocated block in this leaf */\n\t\tex++;\n\t\t*logical = le32_to_cpu(ex->ee_block);\n\t\t*phys = ext4_ext_pblock(ex);\n\t\treturn 0;\n\t}\n\n\t/* go up and search for index to the right */\n\twhile (--depth >= 0) {\n\t\tix = path[depth].p_idx;\n\t\tif (ix != EXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\tgoto got_index;\n\t}\n\n\t/* we've gone up to the root and found no index to the right */\n\treturn 0;\n\ngot_index:\n\t/* we've found index to the right, let's\n\t * follow it and find the closest allocated\n\t * block to the right */\n\tix++;\n\tblock = ext4_idx_pblock(ix);\n\twhile (++depth < path->p_depth) {\n\t\tbh = sb_bread(inode->i_sb, block);\n\t\tif (bh == NULL)\n\t\t\treturn -EIO;\n\t\teh = ext_block_hdr(bh);\n\t\t/* subtract from p_depth to get proper eh_depth */\n\t\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\t\tput_bh(bh);\n\t\t\treturn -EIO;\n\t\t}\n\t\tix = EXT_FIRST_INDEX(eh);\n\t\tblock = ext4_idx_pblock(ix);\n\t\tput_bh(bh);\n\t}\n\n\tbh = sb_bread(inode->i_sb, block);\n\tif (bh == NULL)\n\t\treturn -EIO;\n\teh = ext_block_hdr(bh);\n\tif (ext4_ext_check(inode, eh, path->p_depth - depth)) {\n\t\tput_bh(bh);\n\t\treturn -EIO;\n\t}\n\tex = EXT_FIRST_EXTENT(eh);\n\t*logical = le32_to_cpu(ex->ee_block);\n\t*phys = ext4_ext_pblock(ex);\n\tput_bh(bh);\n\treturn 0;\n}\n\n/*\n * ext4_ext_next_allocated_block:\n * returns allocated block in subsequent extent or EXT_MAX_BLOCK.\n * NOTE: it considers block number from index entry as\n * allocated block. Thus, index entries have to be consistent\n * with leaves.\n */\nstatic ext4_lblk_t\next4_ext_next_allocated_block(struct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\tif (depth == 0 && path->p_ext == NULL)\n\t\treturn EXT_MAX_BLOCK;\n\n\twhile (depth >= 0) {\n\t\tif (depth == path->p_depth) {\n\t\t\t/* leaf */\n\t\t\tif (path[depth].p_ext !=\n\t\t\t\t\tEXT_LAST_EXTENT(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_ext[1].ee_block);\n\t\t} else {\n\t\t\t/* index */\n\t\t\tif (path[depth].p_idx !=\n\t\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\t  return le32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\t}\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_next_leaf_block:\n * returns first allocated block from next leaf or EXT_MAX_BLOCK\n */\nstatic ext4_lblk_t ext4_ext_next_leaf_block(struct inode *inode,\n\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tint depth;\n\n\tBUG_ON(path == NULL);\n\tdepth = path->p_depth;\n\n\t/* zero-tree has no leaf blocks at all */\n\tif (depth == 0)\n\t\treturn EXT_MAX_BLOCK;\n\n\t/* go to index block */\n\tdepth--;\n\n\twhile (depth >= 0) {\n\t\tif (path[depth].p_idx !=\n\t\t\t\tEXT_LAST_INDEX(path[depth].p_hdr))\n\t\t\treturn (ext4_lblk_t)\n\t\t\t\tle32_to_cpu(path[depth].p_idx[1].ei_block);\n\t\tdepth--;\n\t}\n\n\treturn EXT_MAX_BLOCK;\n}\n\n/*\n * ext4_ext_correct_indexes:\n * if leaf gets modified and modified extent is first in the leaf,\n * then we have to correct all indexes above.\n * TODO: do we need to correct tree in all cases?\n */\nstatic int ext4_ext_correct_indexes(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path)\n{\n\tstruct ext4_extent_header *eh;\n\tint depth = ext_depth(inode);\n\tstruct ext4_extent *ex;\n\t__le32 border;\n\tint k, err = 0;\n\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\tif (unlikely(ex == NULL || eh == NULL)) {\n\t\tEXT4_ERROR_INODE(inode,\n\t\t\t\t \"ex %p == NULL or eh %p == NULL\", ex, eh);\n\t\treturn -EIO;\n\t}\n\n\tif (depth == 0) {\n\t\t/* there is no tree at all */\n\t\treturn 0;\n\t}\n\n\tif (ex != EXT_FIRST_EXTENT(eh)) {\n\t\t/* we correct tree if first leaf got modified only */\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: we need correction if border is smaller than current one\n\t */\n\tk = depth - 1;\n\tborder = path[depth].p_ext->ee_block;\n\terr = ext4_ext_get_access(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\tpath[k].p_idx->ei_block = border;\n\terr = ext4_ext_dirty(handle, inode, path + k);\n\tif (err)\n\t\treturn err;\n\n\twhile (k--) {\n\t\t/* change all left-side indexes */\n\t\tif (path[k+1].p_idx != EXT_FIRST_INDEX(path[k+1].p_hdr))\n\t\t\tbreak;\n\t\terr = ext4_ext_get_access(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t\tpath[k].p_idx->ei_block = border;\n\t\terr = ext4_ext_dirty(handle, inode, path + k);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nint\next4_can_extents_be_merged(struct inode *inode, struct ext4_extent *ex1,\n\t\t\t\tstruct ext4_extent *ex2)\n{\n\tunsigned short ext1_ee_len, ext2_ee_len, max_len;\n\n\t/*\n\t * Make sure that either both extents are uninitialized, or\n\t * both are _not_.\n\t */\n\tif (ext4_ext_is_uninitialized(ex1) ^ ext4_ext_is_uninitialized(ex2))\n\t\treturn 0;\n\n\tif (ext4_ext_is_uninitialized(ex1))\n\t\tmax_len = EXT_UNINIT_MAX_LEN;\n\telse\n\t\tmax_len = EXT_INIT_MAX_LEN;\n\n\text1_ee_len = ext4_ext_get_actual_len(ex1);\n\text2_ee_len = ext4_ext_get_actual_len(ex2);\n\n\tif (le32_to_cpu(ex1->ee_block) + ext1_ee_len !=\n\t\t\tle32_to_cpu(ex2->ee_block))\n\t\treturn 0;\n\n\t/*\n\t * To allow future support for preallocated extents to be added\n\t * as an RO_COMPAT feature, refuse to merge to extents if\n\t * this can result in the top bit of ee_len being set.\n\t */\n\tif (ext1_ee_len + ext2_ee_len > max_len)\n\t\treturn 0;\n#ifdef AGGRESSIVE_TEST\n\tif (ext1_ee_len >= 4)\n\t\treturn 0;\n#endif\n\n\tif (ext4_ext_pblock(ex1) + ext1_ee_len == ext4_ext_pblock(ex2))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function tries to merge the \"ex\" extent to the next extent in the tree.\n * It always tries to merge towards right. If you want to merge towards\n * left, pass \"ex - 1\" as argument instead of \"ex\".\n * Returns 0 if the extents (ex and ex+1) were _not_ merged and returns\n * 1 if they got merged.\n */\nstatic int ext4_ext_try_to_merge_right(struct inode *inode,\n\t\t\t\t struct ext4_ext_path *path,\n\t\t\t\t struct ext4_extent *ex)\n{\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth, len;\n\tint merge_done = 0;\n\tint uninitialized = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\twhile (ex < EXT_LAST_EXTENT(eh)) {\n\t\tif (!ext4_can_extents_be_merged(inode, ex, ex + 1))\n\t\t\tbreak;\n\t\t/* merge with next extent! */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t+ ext4_ext_get_actual_len(ex + 1));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\tif (ex + 1 < EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = (EXT_LAST_EXTENT(eh) - ex - 1)\n\t\t\t\t* sizeof(struct ext4_extent);\n\t\t\tmemmove(ex + 1, ex + 2, len);\n\t\t}\n\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\tmerge_done = 1;\n\t\tWARN_ON(eh->eh_entries == 0);\n\t\tif (!eh->eh_entries)\n\t\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries = 0!\");\n\t}\n\n\treturn merge_done;\n}\n\n/*\n * This function tries to merge the @ex extent to neighbours in the tree.\n * return 1 if merge left else 0.\n */\nstatic int ext4_ext_try_to_merge(struct inode *inode,\n\t\t\t\t  struct ext4_ext_path *path,\n\t\t\t\t  struct ext4_extent *ex) {\n\tstruct ext4_extent_header *eh;\n\tunsigned int depth;\n\tint merge_done = 0;\n\tint ret = 0;\n\n\tdepth = ext_depth(inode);\n\tBUG_ON(path[depth].p_hdr == NULL);\n\teh = path[depth].p_hdr;\n\n\tif (ex > EXT_FIRST_EXTENT(eh))\n\t\tmerge_done = ext4_ext_try_to_merge_right(inode, path, ex - 1);\n\n\tif (!merge_done)\n\t\tret = ext4_ext_try_to_merge_right(inode, path, ex);\n\n\treturn ret;\n}\n\n/*\n * check if a portion of the \"newext\" extent overlaps with an\n * existing extent.\n *\n * If there is an overlap discovered, it updates the length of the newext\n * such that there will be no overlap, and then returns 1.\n * If there is no overlap found, it returns 0.\n */\nstatic unsigned int ext4_ext_check_overlap(struct inode *inode,\n\t\t\t\t\t   struct ext4_extent *newext,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\text4_lblk_t b1, b2;\n\tunsigned int depth, len1;\n\tunsigned int ret = 0;\n\n\tb1 = le32_to_cpu(newext->ee_block);\n\tlen1 = ext4_ext_get_actual_len(newext);\n\tdepth = ext_depth(inode);\n\tif (!path[depth].p_ext)\n\t\tgoto out;\n\tb2 = le32_to_cpu(path[depth].p_ext->ee_block);\n\n\t/*\n\t * get the next allocated block if the extent in the path\n\t * is before the requested block(s)\n\t */\n\tif (b2 < b1) {\n\t\tb2 = ext4_ext_next_allocated_block(path);\n\t\tif (b2 == EXT_MAX_BLOCK)\n\t\t\tgoto out;\n\t}\n\n\t/* check for wrap through zero on extent logical start block*/\n\tif (b1 + len1 < b1) {\n\t\tlen1 = EXT_MAX_BLOCK - b1;\n\t\tnewext->ee_len = cpu_to_le16(len1);\n\t\tret = 1;\n\t}\n\n\t/* check for overlap */\n\tif (b1 + len1 > b2) {\n\t\tnewext->ee_len = cpu_to_le16(b2 - b1);\n\t\tret = 1;\n\t}\nout:\n\treturn ret;\n}\n\n/*\n * ext4_ext_insert_extent:\n * tries to merge requsted extent into the existing extent or\n * inserts requested extent as new one into the tree,\n * creating new leaf in the no-space case.\n */\nint ext4_ext_insert_extent(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\tstruct ext4_extent *newext, int flag)\n{\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *ex, *fex;\n\tstruct ext4_extent *nearex; /* nearest extent */\n\tstruct ext4_ext_path *npath = NULL;\n\tint depth, len, err;\n\text4_lblk_t next;\n\tunsigned uninitialized = 0;\n\n\tif (unlikely(ext4_ext_get_actual_len(newext) == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"ext4_ext_get_actual_len(newext) == 0\");\n\t\treturn -EIO;\n\t}\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\n\t/* try to insert block into found extent and return */\n\tif (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)\n\t\t&& ext4_can_extents_be_merged(inode, ex, newext)) {\n\t\text_debug(\"append [%d]%d block to %d:[%d]%d (from %llu)\\n\",\n\t\t\t  ext4_ext_is_uninitialized(newext),\n\t\t\t  ext4_ext_get_actual_len(newext),\n\t\t\t  le32_to_cpu(ex->ee_block),\n\t\t\t  ext4_ext_is_uninitialized(ex),\n\t\t\t  ext4_ext_get_actual_len(ex),\n\t\t\t  ext4_ext_pblock(ex));\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t/*\n\t\t * ext4_can_extents_be_merged should have checked that either\n\t\t * both extents are uninitialized, or both aren't. Thus we\n\t\t * need to check only one of them here.\n\t\t */\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\tex->ee_len = cpu_to_le16(ext4_ext_get_actual_len(ex)\n\t\t\t\t\t+ ext4_ext_get_actual_len(newext));\n\t\tif (uninitialized)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\teh = path[depth].p_hdr;\n\t\tnearex = ex;\n\t\tgoto merge;\n\t}\n\nrepeat:\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max))\n\t\tgoto has_space;\n\n\t/* probably next leaf has space for us? */\n\tfex = EXT_LAST_EXTENT(eh);\n\tnext = ext4_ext_next_leaf_block(inode, path);\n\tif (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)\n\t    && next != EXT_MAX_BLOCK) {\n\t\text_debug(\"next leaf block - %d\\n\", next);\n\t\tBUG_ON(npath != NULL);\n\t\tnpath = ext4_ext_find_extent(inode, next, NULL);\n\t\tif (IS_ERR(npath))\n\t\t\treturn PTR_ERR(npath);\n\t\tBUG_ON(npath->p_depth != path->p_depth);\n\t\teh = npath[depth].p_hdr;\n\t\tif (le16_to_cpu(eh->eh_entries) < le16_to_cpu(eh->eh_max)) {\n\t\t\text_debug(\"next leaf isn't full(%d)\\n\",\n\t\t\t\t  le16_to_cpu(eh->eh_entries));\n\t\t\tpath = npath;\n\t\t\tgoto repeat;\n\t\t}\n\t\text_debug(\"next leaf has no free space(%d,%d)\\n\",\n\t\t\t  le16_to_cpu(eh->eh_entries), le16_to_cpu(eh->eh_max));\n\t}\n\n\t/*\n\t * There is no free space in the found leaf.\n\t * We're gonna add a new leaf in the tree.\n\t */\n\terr = ext4_ext_create_new_leaf(handle, inode, path, newext);\n\tif (err)\n\t\tgoto cleanup;\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\nhas_space:\n\tnearex = path[depth].p_ext;\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto cleanup;\n\n\tif (!nearex) {\n\t\t/* there is no extent in this leaf, create first one */\n\t\text_debug(\"first extent in the leaf: %d:%llu:[%d]%d\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext));\n\t\tpath[depth].p_ext = EXT_FIRST_EXTENT(eh);\n\t} else if (le32_to_cpu(newext->ee_block)\n\t\t\t   > le32_to_cpu(nearex->ee_block)) {\n/*\t\tBUG_ON(newext->ee_block == nearex->ee_block); */\n\t\tif (nearex != EXT_LAST_EXTENT(eh)) {\n\t\t\tlen = EXT_MAX_EXTENT(eh) - nearex;\n\t\t\tlen = (len - 1) * sizeof(struct ext4_extent);\n\t\t\tlen = len < 0 ? 0 : len;\n\t\t\text_debug(\"insert %d:%llu:[%d]%d after: nearest 0x%p, \"\n\t\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\t\text4_ext_pblock(newext),\n\t\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\t\tmemmove(nearex + 2, nearex + 1, len);\n\t\t}\n\t\tpath[depth].p_ext = nearex + 1;\n\t} else {\n\t\tBUG_ON(newext->ee_block == nearex->ee_block);\n\t\tlen = (EXT_MAX_EXTENT(eh) - nearex) * sizeof(struct ext4_extent);\n\t\tlen = len < 0 ? 0 : len;\n\t\text_debug(\"insert %d:%llu:[%d]%d before: nearest 0x%p, \"\n\t\t\t\t\"move %d from 0x%p to 0x%p\\n\",\n\t\t\t\tle32_to_cpu(newext->ee_block),\n\t\t\t\text4_ext_pblock(newext),\n\t\t\t\text4_ext_is_uninitialized(newext),\n\t\t\t\text4_ext_get_actual_len(newext),\n\t\t\t\tnearex, len, nearex + 1, nearex + 2);\n\t\tmemmove(nearex + 1, nearex, len);\n\t\tpath[depth].p_ext = nearex;\n\t}\n\n\tle16_add_cpu(&eh->eh_entries, 1);\n\tnearex = path[depth].p_ext;\n\tnearex->ee_block = newext->ee_block;\n\text4_ext_store_pblock(nearex, ext4_ext_pblock(newext));\n\tnearex->ee_len = newext->ee_len;\n\nmerge:\n\t/* try to merge extents to the right */\n\tif (!(flag & EXT4_GET_BLOCKS_PRE_IO))\n\t\text4_ext_try_to_merge(inode, path, nearex);\n\n\t/* try to merge extents to the left */\n\n\t/* time to correct all indexes above */\n\terr = ext4_ext_correct_indexes(handle, inode, path);\n\tif (err)\n\t\tgoto cleanup;\n\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\ncleanup:\n\tif (npath) {\n\t\text4_ext_drop_refs(npath);\n\t\tkfree(npath);\n\t}\n\text4_ext_invalidate_cache(inode);\n\treturn err;\n}\n\nstatic int ext4_ext_walk_space(struct inode *inode, ext4_lblk_t block,\n\t\t\t       ext4_lblk_t num, ext_prepare_callback func,\n\t\t\t       void *cbdata)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_ext_cache cbex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t next, start = 0, end = 0;\n\text4_lblk_t last = block + num;\n\tint depth, exists, err = 0;\n\n\tBUG_ON(func == NULL);\n\tBUG_ON(inode == NULL);\n\n\twhile (block < last && block != EXT_MAX_BLOCK) {\n\t\tnum = last - block;\n\t\t/* find extent for this block */\n\t\tdown_read(&EXT4_I(inode)->i_data_sem);\n\t\tpath = ext4_ext_find_extent(inode, block, path);\n\t\tup_read(&EXT4_I(inode)->i_data_sem);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tpath = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tdepth = ext_depth(inode);\n\t\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\tex = path[depth].p_ext;\n\t\tnext = ext4_ext_next_allocated_block(path);\n\n\t\texists = 0;\n\t\tif (!ex) {\n\t\t\t/* there is no extent yet, so try to allocate\n\t\t\t * all requested space */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t} else if (le32_to_cpu(ex->ee_block) > block) {\n\t\t\t/* need to allocate space before found extent */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\t\t/* need to allocate space after found extent */\n\t\t\tstart = block;\n\t\t\tend = block + num;\n\t\t\tif (end >= next)\n\t\t\t\tend = next;\n\t\t} else if (block >= le32_to_cpu(ex->ee_block)) {\n\t\t\t/*\n\t\t\t * some part of requested space is covered\n\t\t\t * by found extent\n\t\t\t */\n\t\t\tstart = block;\n\t\t\tend = le32_to_cpu(ex->ee_block)\n\t\t\t\t+ ext4_ext_get_actual_len(ex);\n\t\t\tif (block + num < end)\n\t\t\t\tend = block + num;\n\t\t\texists = 1;\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\t\tBUG_ON(end <= start);\n\n\t\tif (!exists) {\n\t\t\tcbex.ec_block = start;\n\t\t\tcbex.ec_len = end - start;\n\t\t\tcbex.ec_start = 0;\n\t\t} else {\n\t\t\tcbex.ec_block = le32_to_cpu(ex->ee_block);\n\t\t\tcbex.ec_len = ext4_ext_get_actual_len(ex);\n\t\t\tcbex.ec_start = ext4_ext_pblock(ex);\n\t\t}\n\n\t\tif (unlikely(cbex.ec_len == 0)) {\n\t\t\tEXT4_ERROR_INODE(inode, \"cbex.ec_len == 0\");\n\t\t\terr = -EIO;\n\t\t\tbreak;\n\t\t}\n\t\terr = func(inode, path, &cbex, ex, cbdata);\n\t\text4_ext_drop_refs(path);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tif (err == EXT_REPEAT)\n\t\t\tcontinue;\n\t\telse if (err == EXT_BREAK) {\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ext_depth(inode) != depth) {\n\t\t\t/* depth was changed. we have to realloc path */\n\t\t\tkfree(path);\n\t\t\tpath = NULL;\n\t\t}\n\n\t\tblock = cbex.ec_block + cbex.ec_len;\n\t}\n\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\n\treturn err;\n}\n\nstatic void\next4_ext_put_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\t__u32 len, ext4_fsblk_t start)\n{\n\tstruct ext4_ext_cache *cex;\n\tBUG_ON(len == 0);\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\tcex->ec_block = block;\n\tcex->ec_len = len;\n\tcex->ec_start = start;\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n}\n\n/*\n * ext4_ext_put_gap_in_cache:\n * calculate boundaries of the gap that the requested block fits into\n * and cache this gap\n */\nstatic void\next4_ext_put_gap_in_cache(struct inode *inode, struct ext4_ext_path *path,\n\t\t\t\text4_lblk_t block)\n{\n\tint depth = ext_depth(inode);\n\tunsigned long len;\n\text4_lblk_t lblock;\n\tstruct ext4_extent *ex;\n\n\tex = path[depth].p_ext;\n\tif (ex == NULL) {\n\t\t/* there is no extent yet, so gap is [0;-] */\n\t\tlblock = 0;\n\t\tlen = EXT_MAX_BLOCK;\n\t\text_debug(\"cache gap(whole file):\");\n\t} else if (block < le32_to_cpu(ex->ee_block)) {\n\t\tlblock = block;\n\t\tlen = le32_to_cpu(ex->ee_block) - block;\n\t\text_debug(\"cache gap(before): %u [%u:%u]\",\n\t\t\t\tblock,\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\t ext4_ext_get_actual_len(ex));\n\t} else if (block >= le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex)) {\n\t\text4_lblk_t next;\n\t\tlblock = le32_to_cpu(ex->ee_block)\n\t\t\t+ ext4_ext_get_actual_len(ex);\n\n\t\tnext = ext4_ext_next_allocated_block(path);\n\t\text_debug(\"cache gap(after): [%u:%u] %u\",\n\t\t\t\tle32_to_cpu(ex->ee_block),\n\t\t\t\text4_ext_get_actual_len(ex),\n\t\t\t\tblock);\n\t\tBUG_ON(next == lblock);\n\t\tlen = next - lblock;\n\t} else {\n\t\tlblock = len = 0;\n\t\tBUG();\n\t}\n\n\text_debug(\" -> %u:%lu\\n\", lblock, len);\n\text4_ext_put_in_cache(inode, lblock, len, 0);\n}\n\n/*\n * Return 0 if cache is invalid; 1 if the cache is valid\n */\nstatic int\next4_ext_in_cache(struct inode *inode, ext4_lblk_t block,\n\t\t\tstruct ext4_extent *ex)\n{\n\tstruct ext4_ext_cache *cex;\n\tint ret = 0;\n\n\t/*\n\t * We borrow i_block_reservation_lock to protect i_cached_extent\n\t */\n\tspin_lock(&EXT4_I(inode)->i_block_reservation_lock);\n\tcex = &EXT4_I(inode)->i_cached_extent;\n\n\t/* has cache valid data? */\n\tif (cex->ec_len == 0)\n\t\tgoto errout;\n\n\tif (in_range(block, cex->ec_block, cex->ec_len)) {\n\t\tex->ee_block = cpu_to_le32(cex->ec_block);\n\t\text4_ext_store_pblock(ex, cex->ec_start);\n\t\tex->ee_len = cpu_to_le16(cex->ec_len);\n\t\text_debug(\"%u cached by %u:%u:%llu\\n\",\n\t\t\t\tblock,\n\t\t\t\tcex->ec_block, cex->ec_len, cex->ec_start);\n\t\tret = 1;\n\t}\nerrout:\n\tspin_unlock(&EXT4_I(inode)->i_block_reservation_lock);\n\treturn ret;\n}\n\n/*\n * ext4_ext_rm_idx:\n * removes index from the index block.\n * It's used in truncate case only, thus all requests are for\n * last index in the block only.\n */\nstatic int ext4_ext_rm_idx(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_ext_path *path)\n{\n\tint err;\n\text4_fsblk_t leaf;\n\n\t/* free index block */\n\tpath--;\n\tleaf = ext4_idx_pblock(path->p_idx);\n\tif (unlikely(path->p_hdr->eh_entries == 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"path->p_hdr->eh_entries == 0\");\n\t\treturn -EIO;\n\t}\n\terr = ext4_ext_get_access(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\tle16_add_cpu(&path->p_hdr->eh_entries, -1);\n\terr = ext4_ext_dirty(handle, inode, path);\n\tif (err)\n\t\treturn err;\n\text_debug(\"index is empty, remove it, free block %llu\\n\", leaf);\n\text4_free_blocks(handle, inode, NULL, leaf, 1,\n\t\t\t EXT4_FREE_BLOCKS_METADATA | EXT4_FREE_BLOCKS_FORGET);\n\treturn err;\n}\n\n/*\n * ext4_ext_calc_credits_for_single_extent:\n * This routine returns max. credits that needed to insert an extent\n * to the extent tree.\n * When pass the actual path, the caller should calculate credits\n * under i_data_sem.\n */\nint ext4_ext_calc_credits_for_single_extent(struct inode *inode, int nrblocks,\n\t\t\t\t\t\tstruct ext4_ext_path *path)\n{\n\tif (path) {\n\t\tint depth = ext_depth(inode);\n\t\tint ret = 0;\n\n\t\t/* probably there is space in leaf? */\n\t\tif (le16_to_cpu(path[depth].p_hdr->eh_entries)\n\t\t\t\t< le16_to_cpu(path[depth].p_hdr->eh_max)) {\n\n\t\t\t/*\n\t\t\t *  There are some space in the leaf tree, no\n\t\t\t *  need to account for leaf block credit\n\t\t\t *\n\t\t\t *  bitmaps and block group descriptor blocks\n\t\t\t *  and other metadat blocks still need to be\n\t\t\t *  accounted.\n\t\t\t */\n\t\t\t/* 1 bitmap, 1 block group descriptor */\n\t\t\tret = 2 + EXT4_META_TRANS_BLOCKS(inode->i_sb);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ext4_chunk_trans_blocks(inode, nrblocks);\n}\n\n/*\n * How many index/leaf blocks need to change/allocate to modify nrblocks?\n *\n * if nrblocks are fit in a single extent (chunk flag is 1), then\n * in the worse case, each tree level index/leaf need to be changed\n * if the tree split due to insert a new extent, then the old tree\n * index/leaf need to be updated too\n *\n * If the nrblocks are discontiguous, they could cause\n * the whole tree split more than once, but this is really rare.\n */\nint ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)\n{\n\tint index;\n\tint depth = ext_depth(inode);\n\n\tif (chunk)\n\t\tindex = depth * 2;\n\telse\n\t\tindex = depth * 3;\n\n\treturn index;\n}\n\nstatic int ext4_remove_blocks(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_extent *ex,\n\t\t\t\text4_lblk_t from, ext4_lblk_t to)\n{\n\tunsigned short ee_len =  ext4_ext_get_actual_len(ex);\n\tint flags = EXT4_FREE_BLOCKS_FORGET;\n\n\tif (S_ISDIR(inode->i_mode) || S_ISLNK(inode->i_mode))\n\t\tflags |= EXT4_FREE_BLOCKS_METADATA;\n#ifdef EXTENTS_STATS\n\t{\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t\tspin_lock(&sbi->s_ext_stats_lock);\n\t\tsbi->s_ext_blocks += ee_len;\n\t\tsbi->s_ext_extents++;\n\t\tif (ee_len < sbi->s_ext_min)\n\t\t\tsbi->s_ext_min = ee_len;\n\t\tif (ee_len > sbi->s_ext_max)\n\t\t\tsbi->s_ext_max = ee_len;\n\t\tif (ext_depth(inode) > sbi->s_depth_max)\n\t\t\tsbi->s_depth_max = ext_depth(inode);\n\t\tspin_unlock(&sbi->s_ext_stats_lock);\n\t}\n#endif\n\tif (from >= le32_to_cpu(ex->ee_block)\n\t    && to == le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\t/* tail removal */\n\t\text4_lblk_t num;\n\t\text4_fsblk_t start;\n\n\t\tnum = le32_to_cpu(ex->ee_block) + ee_len - from;\n\t\tstart = ext4_ext_pblock(ex) + ee_len - num;\n\t\text_debug(\"free last %u blocks starting %llu\\n\", num, start);\n\t\text4_free_blocks(handle, inode, NULL, start, num, flags);\n\t} else if (from == le32_to_cpu(ex->ee_block)\n\t\t   && to <= le32_to_cpu(ex->ee_block) + ee_len - 1) {\n\t\tprintk(KERN_INFO \"strange request: removal %u-%u from %u:%u\\n\",\n\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t} else {\n\t\tprintk(KERN_INFO \"strange request: removal(2) \"\n\t\t\t\t\"%u-%u from %u:%u\\n\",\n\t\t\t\tfrom, to, le32_to_cpu(ex->ee_block), ee_len);\n\t}\n\treturn 0;\n}\n\nstatic int\next4_ext_rm_leaf(handle_t *handle, struct inode *inode,\n\t\tstruct ext4_ext_path *path, ext4_lblk_t start)\n{\n\tint err = 0, correct_index = 0;\n\tint depth = ext_depth(inode), credits;\n\tstruct ext4_extent_header *eh;\n\text4_lblk_t a, b, block;\n\tunsigned num;\n\text4_lblk_t ex_ee_block;\n\tunsigned short ex_ee_len;\n\tunsigned uninitialized = 0;\n\tstruct ext4_extent *ex;\n\n\t/* the header must be checked already in ext4_ext_remove_space() */\n\text_debug(\"truncate since %u in leaf\\n\", start);\n\tif (!path[depth].p_hdr)\n\t\tpath[depth].p_hdr = ext_block_hdr(path[depth].p_bh);\n\teh = path[depth].p_hdr;\n\tif (unlikely(path[depth].p_hdr == NULL)) {\n\t\tEXT4_ERROR_INODE(inode, \"path[%d].p_hdr == NULL\", depth);\n\t\treturn -EIO;\n\t}\n\t/* find where to start removing */\n\tex = EXT_LAST_EXTENT(eh);\n\n\tex_ee_block = le32_to_cpu(ex->ee_block);\n\tex_ee_len = ext4_ext_get_actual_len(ex);\n\n\twhile (ex >= EXT_FIRST_EXTENT(eh) &&\n\t\t\tex_ee_block + ex_ee_len > start) {\n\n\t\tif (ext4_ext_is_uninitialized(ex))\n\t\t\tuninitialized = 1;\n\t\telse\n\t\t\tuninitialized = 0;\n\n\t\text_debug(\"remove ext %u:[%d]%d\\n\", ex_ee_block,\n\t\t\t uninitialized, ex_ee_len);\n\t\tpath[depth].p_ext = ex;\n\n\t\ta = ex_ee_block > start ? ex_ee_block : start;\n\t\tb = ex_ee_block + ex_ee_len - 1 < EXT_MAX_BLOCK ?\n\t\t\tex_ee_block + ex_ee_len - 1 : EXT_MAX_BLOCK;\n\n\t\text_debug(\"  border %u:%u\\n\", a, b);\n\n\t\tif (a != ex_ee_block && b != ex_ee_block + ex_ee_len - 1) {\n\t\t\tblock = 0;\n\t\t\tnum = 0;\n\t\t\tBUG();\n\t\t} else if (a != ex_ee_block) {\n\t\t\t/* remove tail of the extent */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = a - block;\n\t\t} else if (b != ex_ee_block + ex_ee_len - 1) {\n\t\t\t/* remove head of the extent */\n\t\t\tblock = a;\n\t\t\tnum = b - a;\n\t\t\t/* there is no \"make a hole\" API yet */\n\t\t\tBUG();\n\t\t} else {\n\t\t\t/* remove whole extent: excellent! */\n\t\t\tblock = ex_ee_block;\n\t\t\tnum = 0;\n\t\t\tBUG_ON(a != ex_ee_block);\n\t\t\tBUG_ON(b != ex_ee_block + ex_ee_len - 1);\n\t\t}\n\n\t\t/*\n\t\t * 3 for leaf, sb, and inode plus 2 (bmap and group\n\t\t * descriptor) for each block group; assume two block\n\t\t * groups plus ex_ee_len/blocks_per_block_group for\n\t\t * the worst case\n\t\t */\n\t\tcredits = 7 + 2*(ex_ee_len/EXT4_BLOCKS_PER_GROUP(inode->i_sb));\n\t\tif (ex == EXT_FIRST_EXTENT(eh)) {\n\t\t\tcorrect_index = 1;\n\t\t\tcredits += (ext_depth(inode)) + 1;\n\t\t}\n\t\tcredits += EXT4_MAXQUOTAS_TRANS_BLOCKS(inode->i_sb);\n\n\t\terr = ext4_ext_truncate_extend_restart(handle, inode, credits);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_remove_blocks(handle, inode, ex, a, b);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (num == 0) {\n\t\t\t/* this extent is removed; mark slot entirely unused */\n\t\t\text4_ext_store_pblock(ex, 0);\n\t\t\tle16_add_cpu(&eh->eh_entries, -1);\n\t\t}\n\n\t\tex->ee_block = cpu_to_le32(block);\n\t\tex->ee_len = cpu_to_le16(num);\n\t\t/*\n\t\t * Do not mark uninitialized if all the blocks in the\n\t\t * extent have been removed.\n\t\t */\n\t\tif (uninitialized && num)\n\t\t\text4_ext_mark_uninitialized(ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\text_debug(\"new extent: %u:%u:%llu\\n\", block, num,\n\t\t\t\text4_ext_pblock(ex));\n\t\tex--;\n\t\tex_ee_block = le32_to_cpu(ex->ee_block);\n\t\tex_ee_len = ext4_ext_get_actual_len(ex);\n\t}\n\n\tif (correct_index && eh->eh_entries)\n\t\terr = ext4_ext_correct_indexes(handle, inode, path);\n\n\t/* if this leaf is free, then we should\n\t * remove it from index block above */\n\tif (err == 0 && eh->eh_entries == 0 && path[depth].p_bh != NULL)\n\t\terr = ext4_ext_rm_idx(handle, inode, path + depth);\n\nout:\n\treturn err;\n}\n\n/*\n * ext4_ext_more_to_rm:\n * returns 1 if current index has to be freed (even partial)\n */\nstatic int\next4_ext_more_to_rm(struct ext4_ext_path *path)\n{\n\tBUG_ON(path->p_idx == NULL);\n\n\tif (path->p_idx < EXT_FIRST_INDEX(path->p_hdr))\n\t\treturn 0;\n\n\t/*\n\t * if truncate on deeper level happened, it wasn't partial,\n\t * so we have to consider current index for truncation\n\t */\n\tif (le16_to_cpu(path->p_hdr->eh_entries) == path->p_block)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint depth = ext_depth(inode);\n\tstruct ext4_ext_path *path;\n\thandle_t *handle;\n\tint i, err;\n\n\text_debug(\"truncate since %u\\n\", start);\n\n\t/* probably first extent we're gonna free will be last in block */\n\thandle = ext4_journal_start(inode, depth + 1);\n\tif (IS_ERR(handle))\n\t\treturn PTR_ERR(handle);\n\nagain:\n\text4_ext_invalidate_cache(inode);\n\n\t/*\n\t * We start scanning from right side, freeing all the blocks\n\t * after i_size and walking into the tree depth-wise.\n\t */\n\tdepth = ext_depth(inode);\n\tpath = kzalloc(sizeof(struct ext4_ext_path) * (depth + 1), GFP_NOFS);\n\tif (path == NULL) {\n\t\text4_journal_stop(handle);\n\t\treturn -ENOMEM;\n\t}\n\tpath[0].p_depth = depth;\n\tpath[0].p_hdr = ext_inode_hdr(inode);\n\tif (ext4_ext_check(inode, path[0].p_hdr, depth)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\ti = err = 0;\n\n\twhile (i >= 0 && err == 0) {\n\t\tif (i == depth) {\n\t\t\t/* this is leaf block */\n\t\t\terr = ext4_ext_rm_leaf(handle, inode, path, start);\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* this is index block */\n\t\tif (!path[i].p_hdr) {\n\t\t\text_debug(\"initialize header\\n\");\n\t\t\tpath[i].p_hdr = ext_block_hdr(path[i].p_bh);\n\t\t}\n\n\t\tif (!path[i].p_idx) {\n\t\t\t/* this level hasn't been touched yet */\n\t\t\tpath[i].p_idx = EXT_LAST_INDEX(path[i].p_hdr);\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries)+1;\n\t\t\text_debug(\"init index ptr: hdr 0x%p, num %d\\n\",\n\t\t\t\t  path[i].p_hdr,\n\t\t\t\t  le16_to_cpu(path[i].p_hdr->eh_entries));\n\t\t} else {\n\t\t\t/* we were already here, see at next index */\n\t\t\tpath[i].p_idx--;\n\t\t}\n\n\t\text_debug(\"level %d - index, first 0x%p, cur 0x%p\\n\",\n\t\t\t\ti, EXT_FIRST_INDEX(path[i].p_hdr),\n\t\t\t\tpath[i].p_idx);\n\t\tif (ext4_ext_more_to_rm(path + i)) {\n\t\t\tstruct buffer_head *bh;\n\t\t\t/* go to the next level */\n\t\t\text_debug(\"move to level %d (block %llu)\\n\",\n\t\t\t\t  i + 1, ext4_idx_pblock(path[i].p_idx));\n\t\t\tmemset(path + i + 1, 0, sizeof(*path));\n\t\t\tbh = sb_bread(sb, ext4_idx_pblock(path[i].p_idx));\n\t\t\tif (!bh) {\n\t\t\t\t/* should we reset i_size? */\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (WARN_ON(i + 1 > depth)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ext4_ext_check(inode, ext_block_hdr(bh),\n\t\t\t\t\t\t\tdepth - i - 1)) {\n\t\t\t\terr = -EIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpath[i + 1].p_bh = bh;\n\n\t\t\t/* save actual number of indexes since this\n\t\t\t * number is changed at the next iteration */\n\t\t\tpath[i].p_block = le16_to_cpu(path[i].p_hdr->eh_entries);\n\t\t\ti++;\n\t\t} else {\n\t\t\t/* we finished processing this index, go up */\n\t\t\tif (path[i].p_hdr->eh_entries == 0 && i > 0) {\n\t\t\t\t/* index is empty, remove it;\n\t\t\t\t * handle must be already prepared by the\n\t\t\t\t * truncatei_leaf() */\n\t\t\t\terr = ext4_ext_rm_idx(handle, inode, path + i);\n\t\t\t}\n\t\t\t/* root level has p_bh == NULL, brelse() eats this */\n\t\t\tbrelse(path[i].p_bh);\n\t\t\tpath[i].p_bh = NULL;\n\t\t\ti--;\n\t\t\text_debug(\"return to level %d\\n\", i);\n\t\t}\n\t}\n\n\t/* TODO: flexible tree reduction should be here */\n\tif (path->p_hdr->eh_entries == 0) {\n\t\t/*\n\t\t * truncate to zero freed all the tree,\n\t\t * so we need to correct eh_depth\n\t\t */\n\t\terr = ext4_ext_get_access(handle, inode, path);\n\t\tif (err == 0) {\n\t\t\text_inode_hdr(inode)->eh_depth = 0;\n\t\t\text_inode_hdr(inode)->eh_max =\n\t\t\t\tcpu_to_le16(ext4_ext_space_root(inode, 0));\n\t\t\terr = ext4_ext_dirty(handle, inode, path);\n\t\t}\n\t}\nout:\n\text4_ext_drop_refs(path);\n\tkfree(path);\n\tif (err == -EAGAIN)\n\t\tgoto again;\n\text4_journal_stop(handle);\n\n\treturn err;\n}\n\n/*\n * called at mount time\n */\nvoid ext4_ext_init(struct super_block *sb)\n{\n\t/*\n\t * possible initialization would be here\n\t */\n\n\tif (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS)) {\n#if defined(AGGRESSIVE_TEST) || defined(CHECK_BINSEARCH) || defined(EXTENTS_STATS)\n\t\tprintk(KERN_INFO \"EXT4-fs: file extents enabled\");\n#ifdef AGGRESSIVE_TEST\n\t\tprintk(\", aggressive tests\");\n#endif\n#ifdef CHECK_BINSEARCH\n\t\tprintk(\", check binsearch\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tprintk(\", stats\");\n#endif\n\t\tprintk(\"\\n\");\n#endif\n#ifdef EXTENTS_STATS\n\t\tspin_lock_init(&EXT4_SB(sb)->s_ext_stats_lock);\n\t\tEXT4_SB(sb)->s_ext_min = 1 << 30;\n\t\tEXT4_SB(sb)->s_ext_max = 0;\n#endif\n\t}\n}\n\n/*\n * called at umount time\n */\nvoid ext4_ext_release(struct super_block *sb)\n{\n\tif (!EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_EXTENTS))\n\t\treturn;\n\n#ifdef EXTENTS_STATS\n\tif (EXT4_SB(sb)->s_ext_blocks && EXT4_SB(sb)->s_ext_extents) {\n\t\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\t\tprintk(KERN_ERR \"EXT4-fs: %lu blocks in %lu extents (%lu ave)\\n\",\n\t\t\tsbi->s_ext_blocks, sbi->s_ext_extents,\n\t\t\tsbi->s_ext_blocks / sbi->s_ext_extents);\n\t\tprintk(KERN_ERR \"EXT4-fs: extents: %lu min, %lu max, max depth %lu\\n\",\n\t\t\tsbi->s_ext_min, sbi->s_ext_max, sbi->s_depth_max);\n\t}\n#endif\n}\n\n/* FIXME!! we need to try to merge to left or right after zero-out  */\nstatic int ext4_ext_zeroout(struct inode *inode, struct ext4_extent *ex)\n{\n\text4_fsblk_t ee_pblock;\n\tunsigned int ee_len;\n\tint ret;\n\n\tee_len    = ext4_ext_get_actual_len(ex);\n\tee_pblock = ext4_ext_pblock(ex);\n\n\tret = sb_issue_zeroout(inode->i_sb, ee_pblock, ee_len, GFP_NOFS);\n\tif (ret > 0)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n/*\n * used by extent splitting.\n */\n#define EXT4_EXT_MAY_ZEROOUT\t0x1  /* safe to zeroout if split fails \\\n\t\t\t\t\tdue to ENOSPC */\n#define EXT4_EXT_MARK_UNINIT1\t0x2  /* mark first half uninitialized */\n#define EXT4_EXT_MARK_UNINIT2\t0x4  /* mark second half uninitialized */\n\n/*\n * ext4_split_extent_at() splits an extent at given block.\n *\n * @handle: the journal handle\n * @inode: the file inode\n * @path: the path to the extent\n * @split: the logical block where the extent is splitted.\n * @split_flags: indicates if the extent could be zeroout if split fails, and\n *\t\t the states(init or uninit) of new extents.\n * @flags: flags used to insert new extent to extent tree.\n *\n *\n * Splits extent [a, b] into two extents [a, @split) and [@split, b], states\n * of which are deterimined by split_flag.\n *\n * There are two cases:\n *  a> the extent are splitted into two extent.\n *  b> split is not needed, and just mark the extent.\n *\n * return 0 on success.\n */\nstatic int ext4_split_extent_at(handle_t *handle,\n\t\t\t     struct inode *inode,\n\t\t\t     struct ext4_ext_path *path,\n\t\t\t     ext4_lblk_t split,\n\t\t\t     int split_flag,\n\t\t\t     int flags)\n{\n\text4_fsblk_t newblock;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex, newex, orig_ex;\n\tstruct ext4_extent *ex2 = NULL;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\n\text_debug(\"ext4_split_extents_at: inode %lu, logical\"\n\t\t\"block %llu\\n\", inode->i_ino, (unsigned long long)split);\n\n\text4_ext_show_leaf(inode, path);\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tnewblock = split - ee_block + ext4_ext_pblock(ex);\n\n\tBUG_ON(split < ee_block || split >= (ee_block + ee_len));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\n\tif (split == ee_block) {\n\t\t/*\n\t\t * case b: block @split is the block that the extent begins with\n\t\t * then we just change the state of the extent, and splitting\n\t\t * is not needed.\n\t\t */\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\text4_ext_mark_uninitialized(ex);\n\t\telse\n\t\t\text4_ext_mark_initialized(ex);\n\n\t\tif (!(flags & EXT4_GET_BLOCKS_PRE_IO))\n\t\t\text4_ext_try_to_merge(inode, path, ex);\n\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tgoto out;\n\t}\n\n\t/* case a */\n\tmemcpy(&orig_ex, ex, sizeof(orig_ex));\n\tex->ee_len = cpu_to_le16(split - ee_block);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT1)\n\t\text4_ext_mark_uninitialized(ex);\n\n\t/*\n\t * path may lead to new leaf, not to original leaf any more\n\t * after ext4_ext_insert_extent() returns,\n\t */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\n\tif (err)\n\t\tgoto fix_extent_len;\n\n\tex2 = &newex;\n\tex2->ee_block = cpu_to_le32(split);\n\tex2->ee_len   = cpu_to_le16(ee_len - (split - ee_block));\n\text4_ext_store_pblock(ex2, newblock);\n\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\text4_ext_mark_uninitialized(ex2);\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, &orig_ex);\n\t\tif (err)\n\t\t\tgoto fix_extent_len;\n\t\t/* update the extent length and mark as initialized */\n\t\tex->ee_len = cpu_to_le32(ee_len);\n\t\text4_ext_try_to_merge(inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tgoto out;\n\t} else if (err)\n\t\tgoto fix_extent_len;\n\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n\nfix_extent_len:\n\tex->ee_len = orig_ex.ee_len;\n\text4_ext_dirty(handle, inode, path + depth);\n\treturn err;\n}\n\n/*\n * ext4_split_extents() splits an extent and mark extent which is covered\n * by @map as split_flags indicates\n *\n * It may result in splitting the extent into multiple extents (upto three)\n * There are three possibilities:\n *   a> There is no split required\n *   b> Splits in two extents: Split is happening at either end of the extent\n *   c> Splits in three extents: Somone is splitting in middle of the extent\n *\n */\nstatic int ext4_split_extent(handle_t *handle,\n\t\t\t      struct inode *inode,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      struct ext4_map_blocks *map,\n\t\t\t      int split_flag,\n\t\t\t      int flags)\n{\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len, depth;\n\tint err = 0;\n\tint uninitialized;\n\tint split_flag1, flags1;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tuninitialized = ext4_ext_is_uninitialized(ex);\n\n\tif (map->m_lblk + map->m_len < ee_block + ee_len) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tflags1 = flags | EXT4_GET_BLOCKS_PRE_IO;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1 |\n\t\t\t\t       EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk + map->m_len, split_flag1, flags1);\n\t}\n\n\text4_ext_drop_refs(path);\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, path);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\n\tif (map->m_lblk >= ee_block) {\n\t\tsplit_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?\n\t\t\t      EXT4_EXT_MAY_ZEROOUT : 0;\n\t\tif (uninitialized)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT1;\n\t\tif (split_flag & EXT4_EXT_MARK_UNINIT2)\n\t\t\tsplit_flag1 |= EXT4_EXT_MARK_UNINIT2;\n\t\terr = ext4_split_extent_at(handle, inode, path,\n\t\t\t\tmap->m_lblk, split_flag1, flags);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\text4_ext_show_leaf(inode, path);\nout:\n\treturn err ? err : map->m_len;\n}\n\n#define EXT4_EXT_ZERO_LEN 7\n/*\n * This function is called by ext4_ext_map_blocks() if someone tries to write\n * to an uninitialized extent. It may result in splitting the uninitialized\n * extent into multiple extents (up to three - one initialized and two\n * uninitialized).\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be initialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n */\nstatic int ext4_ext_convert_to_initialized(handle_t *handle,\n\t\t\t\t\t   struct inode *inode,\n\t\t\t\t\t   struct ext4_map_blocks *map,\n\t\t\t\t\t   struct ext4_ext_path *path)\n{\n\tstruct ext4_map_blocks split_map;\n\tstruct ext4_extent zero_ex;\n\tstruct ext4_extent *ex;\n\text4_lblk_t ee_block, eof_block;\n\tunsigned int allocated, ee_len, depth;\n\tint err = 0;\n\tint split_flag = 0;\n\n\text_debug(\"ext4_ext_convert_to_initialized: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\tallocated = ee_len - (map->m_lblk - ee_block);\n\n\tWARN_ON(map->m_lblk < ee_block);\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\n\t/* If extent has less than 2*EXT4_EXT_ZERO_LEN zerout directly */\n\tif (ee_len <= 2*EXT4_EXT_ZERO_LEN &&\n\t    (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\terr = ext4_ext_zeroout(inode, ex);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = ext4_ext_get_access(handle, inode, path + depth);\n\t\tif (err)\n\t\t\tgoto out;\n\t\text4_ext_mark_initialized(ex);\n\t\text4_ext_try_to_merge(inode, path, ex);\n\t\terr = ext4_ext_dirty(handle, inode, path + depth);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * four cases:\n\t * 1. split the extent into three extents.\n\t * 2. split the extent into two extents, zeroout the first half.\n\t * 3. split the extent into two extents, zeroout the second half.\n\t * 4. split the extent into two extents with out zeroout.\n\t */\n\tsplit_map.m_lblk = map->m_lblk;\n\tsplit_map.m_len = map->m_len;\n\n\tif (allocated > map->m_len) {\n\t\tif (allocated <= EXT4_EXT_ZERO_LEN &&\n\t\t    (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\t\t/* case 3 */\n\t\t\tzero_ex.ee_block =\n\t\t\t\t\t cpu_to_le32(map->m_lblk + map->m_len);\n\t\t\tzero_ex.ee_len = cpu_to_le16(allocated - map->m_len);\n\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\text4_ext_pblock(ex) + map->m_lblk - ee_block);\n\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tsplit_map.m_lblk = map->m_lblk;\n\t\t\tsplit_map.m_len = allocated;\n\t\t} else if ((map->m_lblk - ee_block + map->m_len <\n\t\t\t   EXT4_EXT_ZERO_LEN) &&\n\t\t\t   (EXT4_EXT_MAY_ZEROOUT & split_flag)) {\n\t\t\t/* case 2 */\n\t\t\tif (map->m_lblk != ee_block) {\n\t\t\t\tzero_ex.ee_block = ex->ee_block;\n\t\t\t\tzero_ex.ee_len = cpu_to_le16(map->m_lblk -\n\t\t\t\t\t\t\tee_block);\n\t\t\t\text4_ext_store_pblock(&zero_ex,\n\t\t\t\t\t\t      ext4_ext_pblock(ex));\n\t\t\t\terr = ext4_ext_zeroout(inode, &zero_ex);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tallocated = map->m_lblk - ee_block + map->m_len;\n\n\t\t\tsplit_map.m_lblk = ee_block;\n\t\t\tsplit_map.m_len = allocated;\n\t\t}\n\t}\n\n\tallocated = ext4_split_extent(handle, inode, path,\n\t\t\t\t       &split_map, split_flag, 0);\n\tif (allocated < 0)\n\t\terr = allocated;\n\nout:\n\treturn err ? err : allocated;\n}\n\n/*\n * This function is called by ext4_ext_map_blocks() from\n * ext4_get_blocks_dio_write() when DIO to write\n * to an uninitialized extent.\n *\n * Writing to an uninitialized extent may result in splitting the uninitialized\n * extent into multiple /initialized uninitialized extents (up to three)\n * There are three possibilities:\n *   a> There is no split required: Entire extent should be uninitialized\n *   b> Splits in two extents: Write is happening at either end of the extent\n *   c> Splits in three extents: Somone is writing in middle of the extent\n *\n * One of more index blocks maybe needed if the extent tree grow after\n * the uninitialized extent split. To prevent ENOSPC occur at the IO\n * complete, we need to split the uninitialized extent before DIO submit\n * the IO. The uninitialized extent called at this time will be split\n * into three uninitialized extent(at most). After IO complete, the part\n * being filled will be convert to initialized by the end_io callback function\n * via ext4_convert_unwritten_extents().\n *\n * Returns the size of uninitialized extent to be written on success.\n */\nstatic int ext4_split_unwritten_extents(handle_t *handle,\n\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\tstruct ext4_map_blocks *map,\n\t\t\t\t\tstruct ext4_ext_path *path,\n\t\t\t\t\tint flags)\n{\n\text4_lblk_t eof_block;\n\text4_lblk_t ee_block;\n\tstruct ext4_extent *ex;\n\tunsigned int ee_len;\n\tint split_flag = 0, depth;\n\n\text_debug(\"ext4_split_unwritten_extents: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)map->m_lblk, map->m_len);\n\n\teof_block = (inode->i_size + inode->i_sb->s_blocksize - 1) >>\n\t\tinode->i_sb->s_blocksize_bits;\n\tif (eof_block < map->m_lblk + map->m_len)\n\t\teof_block = map->m_lblk + map->m_len;\n\t/*\n\t * It is safe to convert extent to initialized via explicit\n\t * zeroout only if extent is fully insde i_size or new_size.\n\t */\n\tdepth = ext_depth(inode);\n\tex = path[depth].p_ext;\n\tee_block = le32_to_cpu(ex->ee_block);\n\tee_len = ext4_ext_get_actual_len(ex);\n\n\tsplit_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;\n\tsplit_flag |= EXT4_EXT_MARK_UNINIT2;\n\n\tflags |= EXT4_GET_BLOCKS_PRE_IO;\n\treturn ext4_split_extent(handle, inode, path, map, split_flag, flags);\n}\n\nstatic int ext4_convert_unwritten_extents_endio(handle_t *handle,\n\t\t\t\t\t      struct inode *inode,\n\t\t\t\t\t      struct ext4_ext_path *path)\n{\n\tstruct ext4_extent *ex;\n\tstruct ext4_extent_header *eh;\n\tint depth;\n\tint err = 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\tex = path[depth].p_ext;\n\n\text_debug(\"ext4_convert_unwritten_extents_endio: inode %lu, logical\"\n\t\t\"block %llu, max_blocks %u\\n\", inode->i_ino,\n\t\t(unsigned long long)le32_to_cpu(ex->ee_block),\n\t\text4_ext_get_actual_len(ex));\n\n\terr = ext4_ext_get_access(handle, inode, path + depth);\n\tif (err)\n\t\tgoto out;\n\t/* first mark the extent as initialized */\n\text4_ext_mark_initialized(ex);\n\n\t/* note: ext4_ext_correct_indexes() isn't needed here because\n\t * borders are not changed\n\t */\n\text4_ext_try_to_merge(inode, path, ex);\n\n\t/* Mark modified extent as dirty */\n\terr = ext4_ext_dirty(handle, inode, path + depth);\nout:\n\text4_ext_show_leaf(inode, path);\n\treturn err;\n}\n\nstatic void unmap_underlying_metadata_blocks(struct block_device *bdev,\n\t\t\tsector_t block, int count)\n{\n\tint i;\n\tfor (i = 0; i < count; i++)\n                unmap_underlying_metadata(bdev, block + i);\n}\n\n/*\n * Handle EOFBLOCKS_FL flag, clearing it if necessary\n */\nstatic int check_eofblocks_fl(handle_t *handle, struct inode *inode,\n\t\t\t      ext4_lblk_t lblk,\n\t\t\t      struct ext4_ext_path *path,\n\t\t\t      unsigned int len)\n{\n\tint i, depth;\n\tstruct ext4_extent_header *eh;\n\tstruct ext4_extent *last_ex;\n\n\tif (!ext4_test_inode_flag(inode, EXT4_INODE_EOFBLOCKS))\n\t\treturn 0;\n\n\tdepth = ext_depth(inode);\n\teh = path[depth].p_hdr;\n\n\tif (unlikely(!eh->eh_entries)) {\n\t\tEXT4_ERROR_INODE(inode, \"eh->eh_entries == 0 and \"\n\t\t\t\t \"EOFBLOCKS_FL set\");\n\t\treturn -EIO;\n\t}\n\tlast_ex = EXT_LAST_EXTENT(eh);\n\t/*\n\t * We should clear the EOFBLOCKS_FL flag if we are writing the\n\t * last block in the last extent in the file.  We test this by\n\t * first checking to see if the caller to\n\t * ext4_ext_get_blocks() was interested in the last block (or\n\t * a block beyond the last block) in the current extent.  If\n\t * this turns out to be false, we can bail out from this\n\t * function immediately.\n\t */\n\tif (lblk + len < le32_to_cpu(last_ex->ee_block) +\n\t    ext4_ext_get_actual_len(last_ex))\n\t\treturn 0;\n\t/*\n\t * If the caller does appear to be planning to write at or\n\t * beyond the end of the current extent, we then test to see\n\t * if the current extent is the last extent in the file, by\n\t * checking to make sure it was reached via the rightmost node\n\t * at each level of the tree.\n\t */\n\tfor (i = depth-1; i >= 0; i--)\n\t\tif (path[i].p_idx != EXT_LAST_INDEX(path[i].p_hdr))\n\t\t\treturn 0;\n\text4_clear_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\treturn ext4_mark_inode_dirty(handle, inode);\n}\n\nstatic int\next4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map,\n\t\t\tstruct ext4_ext_path *path, int flags,\n\t\t\tunsigned int allocated, ext4_fsblk_t newblock)\n{\n\tint ret = 0;\n\tint err = 0;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"ext4_ext_handle_uninitialized_extents: inode %lu, logical\"\n\t\t  \"block %llu, max_blocks %u, flags %d, allocated %u\",\n\t\t  inode->i_ino, (unsigned long long)map->m_lblk, map->m_len,\n\t\t  flags, allocated);\n\text4_ext_show_leaf(inode, path);\n\n\t/* get_block() before submit the IO, split the extent */\n\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\tret = ext4_split_unwritten_extents(handle, inode, map,\n\t\t\t\t\t\t   path, flags);\n\t\t/*\n\t\t * Flag the inode(non aio case) or end_io struct (aio case)\n\t\t * that this IO needs to conversion to written when IO is\n\t\t * completed\n\t\t */\n\t\tif (io && !(io->flag & EXT4_IO_END_UNWRITTEN)) {\n\t\t\tio->flag = EXT4_IO_END_UNWRITTEN;\n\t\t\tatomic_inc(&EXT4_I(inode)->i_aiodio_unwritten);\n\t\t} else\n\t\t\text4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t\tgoto out;\n\t}\n\t/* IO end_io complete, convert the filled extent to written */\n\tif ((flags & EXT4_GET_BLOCKS_CONVERT)) {\n\t\tret = ext4_convert_unwritten_extents_endio(handle, inode,\n\t\t\t\t\t\t\tpath);\n\t\tif (ret >= 0) {\n\t\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk,\n\t\t\t\t\t\t path, map->m_len);\n\t\t} else\n\t\t\terr = ret;\n\t\tgoto out2;\n\t}\n\t/* buffered IO case */\n\t/*\n\t * repeat fallocate creation request\n\t * we already have an unwritten extent\n\t */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT)\n\t\tgoto map_out;\n\n\t/* buffered READ or buffered write_begin() lookup */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * We have blocks reserved already.  We\n\t\t * return allocated blocks so that delalloc\n\t\t * won't do block reservation for us.  But\n\t\t * the buffer head will be unmapped so that\n\t\t * a read from the block returns 0s.\n\t\t */\n\t\tmap->m_flags |= EXT4_MAP_UNWRITTEN;\n\t\tgoto out1;\n\t}\n\n\t/* buffered write, writepage time, convert*/\n\tret = ext4_ext_convert_to_initialized(handle, inode, map, path);\n\tif (ret >= 0) {\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path,\n\t\t\t\t\t map->m_len);\n\t\tif (err < 0)\n\t\t\tgoto out2;\n\t}\n\nout:\n\tif (ret <= 0) {\n\t\terr = ret;\n\t\tgoto out2;\n\t} else\n\t\tallocated = ret;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\t/*\n\t * if we allocated more blocks than requested\n\t * we need to make sure we unmap the extra block\n\t * allocated. The actual needed block will get\n\t * unmapped later when we find the buffer_head marked\n\t * new.\n\t */\n\tif (allocated > map->m_len) {\n\t\tunmap_underlying_metadata_blocks(inode->i_sb->s_bdev,\n\t\t\t\t\tnewblock + map->m_len,\n\t\t\t\t\tallocated - map->m_len);\n\t\tallocated = map->m_len;\n\t}\n\n\t/*\n\t * If we have done fallocate with the offset that is already\n\t * delayed allocated, we would have block reservation\n\t * and quota reservation done in the delayed write path.\n\t * But fallocate would have already updated quota and block\n\t * count for this offset. So cancel these reservation\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 0);\n\nmap_out:\n\tmap->m_flags |= EXT4_MAP_MAPPED;\nout1:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\treturn err ? err : allocated;\n}\n\n/*\n * Block allocation/map/preallocation routine for extents based files\n *\n *\n * Need to be called with\n * down_read(&EXT4_I(inode)->i_data_sem) if not allocating file system block\n * (ie, create is zero). Otherwise down_write(&EXT4_I(inode)->i_data_sem)\n *\n * return > 0, number of of blocks already mapped/allocated\n *          if create == 0 and these are pre-allocated blocks\n *          \tbuffer head is unmapped\n *          otherwise blocks are mapped\n *\n * return = 0, if plain look up failed (blocks have not been allocated)\n *          buffer head is unmapped\n *\n * return < 0, error case.\n */\nint ext4_ext_map_blocks(handle_t *handle, struct inode *inode,\n\t\t\tstruct ext4_map_blocks *map, int flags)\n{\n\tstruct ext4_ext_path *path = NULL;\n\tstruct ext4_extent newex, *ex;\n\text4_fsblk_t newblock = 0;\n\tint err = 0, depth, ret;\n\tunsigned int allocated = 0;\n\tstruct ext4_allocation_request ar;\n\text4_io_end_t *io = EXT4_I(inode)->cur_aio_dio;\n\n\text_debug(\"blocks %u/%u requested for inode %lu\\n\",\n\t\t  map->m_lblk, map->m_len, inode->i_ino);\n\ttrace_ext4_ext_map_blocks_enter(inode, map->m_lblk, map->m_len, flags);\n\n\t/* check in cache */\n\tif (ext4_ext_in_cache(inode, map->m_lblk, &newex)) {\n\t\tif (!newex.ee_start_lo && !newex.ee_start_hi) {\n\t\t\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t\t\t/*\n\t\t\t\t * block isn't allocated yet and\n\t\t\t\t * user doesn't want to allocate it\n\t\t\t\t */\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\t/* we should allocate requested block */\n\t\t} else {\n\t\t\t/* block is already allocated */\n\t\t\tnewblock = map->m_lblk\n\t\t\t\t   - le32_to_cpu(newex.ee_block)\n\t\t\t\t   + ext4_ext_pblock(&newex);\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ext4_ext_get_actual_len(&newex) -\n\t\t\t\t(map->m_lblk - le32_to_cpu(newex.ee_block));\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* find extent for this block */\n\tpath = ext4_ext_find_extent(inode, map->m_lblk, NULL);\n\tif (IS_ERR(path)) {\n\t\terr = PTR_ERR(path);\n\t\tpath = NULL;\n\t\tgoto out2;\n\t}\n\n\tdepth = ext_depth(inode);\n\n\t/*\n\t * consistent leaf must not be empty;\n\t * this situation is possible, though, _during_ tree modification;\n\t * this is why assert can't be put in ext4_ext_find_extent()\n\t */\n\tif (unlikely(path[depth].p_ext == NULL && depth != 0)) {\n\t\tEXT4_ERROR_INODE(inode, \"bad extent address \"\n\t\t\t\t \"lblock: %lu, depth: %d pblock %lld\",\n\t\t\t\t (unsigned long) map->m_lblk, depth,\n\t\t\t\t path[depth].p_block);\n\t\terr = -EIO;\n\t\tgoto out2;\n\t}\n\n\tex = path[depth].p_ext;\n\tif (ex) {\n\t\text4_lblk_t ee_block = le32_to_cpu(ex->ee_block);\n\t\text4_fsblk_t ee_start = ext4_ext_pblock(ex);\n\t\tunsigned short ee_len;\n\n\t\t/*\n\t\t * Uninitialized extents are treated as holes, except that\n\t\t * we split out initialized portions during a write.\n\t\t */\n\t\tee_len = ext4_ext_get_actual_len(ex);\n\t\t/* if found extent covers block, simply return it */\n\t\tif (in_range(map->m_lblk, ee_block, ee_len)) {\n\t\t\tnewblock = map->m_lblk - ee_block + ee_start;\n\t\t\t/* number of remaining blocks in the extent */\n\t\t\tallocated = ee_len - (map->m_lblk - ee_block);\n\t\t\text_debug(\"%u fit into %u:%d -> %llu\\n\", map->m_lblk,\n\t\t\t\t  ee_block, ee_len, newblock);\n\n\t\t\t/* Do not put uninitialized extent in the cache */\n\t\t\tif (!ext4_ext_is_uninitialized(ex)) {\n\t\t\t\text4_ext_put_in_cache(inode, ee_block,\n\t\t\t\t\t\t\tee_len, ee_start);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tret = ext4_ext_handle_uninitialized_extents(handle,\n\t\t\t\t\tinode, map, path, flags, allocated,\n\t\t\t\t\tnewblock);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t/*\n\t * requested block isn't allocated yet;\n\t * we couldn't try to create block if create flag is zero\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_CREATE) == 0) {\n\t\t/*\n\t\t * put just found gap into cache to speed up\n\t\t * subsequent requests\n\t\t */\n\t\text4_ext_put_gap_in_cache(inode, path, map->m_lblk);\n\t\tgoto out2;\n\t}\n\t/*\n\t * Okay, we need to do block allocation.\n\t */\n\n\t/* find neighbour allocated blocks */\n\tar.lleft = map->m_lblk;\n\terr = ext4_ext_search_left(inode, path, &ar.lleft, &ar.pleft);\n\tif (err)\n\t\tgoto out2;\n\tar.lright = map->m_lblk;\n\terr = ext4_ext_search_right(inode, path, &ar.lright, &ar.pright);\n\tif (err)\n\t\tgoto out2;\n\n\t/*\n\t * See if request is beyond maximum number of blocks we can have in\n\t * a single extent. For an initialized extent this limit is\n\t * EXT_INIT_MAX_LEN and for an uninitialized extent this limit is\n\t * EXT_UNINIT_MAX_LEN.\n\t */\n\tif (map->m_len > EXT_INIT_MAX_LEN &&\n\t    !(flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_INIT_MAX_LEN;\n\telse if (map->m_len > EXT_UNINIT_MAX_LEN &&\n\t\t (flags & EXT4_GET_BLOCKS_UNINIT_EXT))\n\t\tmap->m_len = EXT_UNINIT_MAX_LEN;\n\n\t/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */\n\tnewex.ee_block = cpu_to_le32(map->m_lblk);\n\tnewex.ee_len = cpu_to_le16(map->m_len);\n\terr = ext4_ext_check_overlap(inode, &newex, path);\n\tif (err)\n\t\tallocated = ext4_ext_get_actual_len(&newex);\n\telse\n\t\tallocated = map->m_len;\n\n\t/* allocate new block */\n\tar.inode = inode;\n\tar.goal = ext4_ext_find_goal(inode, path, map->m_lblk);\n\tar.logical = map->m_lblk;\n\tar.len = allocated;\n\tif (S_ISREG(inode->i_mode))\n\t\tar.flags = EXT4_MB_HINT_DATA;\n\telse\n\t\t/* disable in-core preallocation for non-regular files */\n\t\tar.flags = 0;\n\tnewblock = ext4_mb_new_blocks(handle, &ar, &err);\n\tif (!newblock)\n\t\tgoto out2;\n\text_debug(\"allocate new block: goal %llu, found %llu/%u\\n\",\n\t\t  ar.goal, newblock, allocated);\n\n\t/* try to insert new extent into found leaf and return */\n\text4_ext_store_pblock(&newex, newblock);\n\tnewex.ee_len = cpu_to_le16(ar.len);\n\t/* Mark uninitialized */\n\tif (flags & EXT4_GET_BLOCKS_UNINIT_EXT){\n\t\text4_ext_mark_uninitialized(&newex);\n\t\t/*\n\t\t * io_end structure was created for every IO write to an\n\t\t * uninitialized extent. To avoid unnecessary conversion,\n\t\t * here we flag the IO that really needs the conversion.\n\t\t * For non asycn direct IO case, flag the inode state\n\t\t * that we need to perform conversion when IO is done.\n\t\t */\n\t\tif ((flags & EXT4_GET_BLOCKS_PRE_IO)) {\n\t\t\tif (io && !(io->flag & EXT4_IO_END_UNWRITTEN)) {\n\t\t\t\tio->flag = EXT4_IO_END_UNWRITTEN;\n\t\t\t\tatomic_inc(&EXT4_I(inode)->i_aiodio_unwritten);\n\t\t\t} else\n\t\t\t\text4_set_inode_state(inode,\n\t\t\t\t\t\t     EXT4_STATE_DIO_UNWRITTEN);\n\t\t}\n\t\tif (ext4_should_dioread_nolock(inode))\n\t\t\tmap->m_flags |= EXT4_MAP_UNINIT;\n\t}\n\n\terr = check_eofblocks_fl(handle, inode, map->m_lblk, path, ar.len);\n\tif (err)\n\t\tgoto out2;\n\n\terr = ext4_ext_insert_extent(handle, inode, path, &newex, flags);\n\tif (err) {\n\t\t/* free data blocks we just allocated */\n\t\t/* not a good idea to call discard here directly,\n\t\t * but otherwise we'd need to call it every free() */\n\t\text4_discard_preallocations(inode);\n\t\text4_free_blocks(handle, inode, NULL, ext4_ext_pblock(&newex),\n\t\t\t\t ext4_ext_get_actual_len(&newex), 0);\n\t\tgoto out2;\n\t}\n\n\t/* previous routine could use block we allocated */\n\tnewblock = ext4_ext_pblock(&newex);\n\tallocated = ext4_ext_get_actual_len(&newex);\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\tmap->m_flags |= EXT4_MAP_NEW;\n\n\t/*\n\t * Update reserved blocks/metadata blocks after successful\n\t * block allocation which had been deferred till now.\n\t */\n\tif (flags & EXT4_GET_BLOCKS_DELALLOC_RESERVE)\n\t\text4_da_update_reserve_space(inode, allocated, 1);\n\n\t/*\n\t * Cache the extent and update transaction to commit on fdatasync only\n\t * when it is _not_ an uninitialized extent.\n\t */\n\tif ((flags & EXT4_GET_BLOCKS_UNINIT_EXT) == 0) {\n\t\text4_ext_put_in_cache(inode, map->m_lblk, allocated, newblock);\n\t\text4_update_inode_fsync_trans(handle, inode, 1);\n\t} else\n\t\text4_update_inode_fsync_trans(handle, inode, 0);\nout:\n\tif (allocated > map->m_len)\n\t\tallocated = map->m_len;\n\text4_ext_show_leaf(inode, path);\n\tmap->m_flags |= EXT4_MAP_MAPPED;\n\tmap->m_pblk = newblock;\n\tmap->m_len = allocated;\nout2:\n\tif (path) {\n\t\text4_ext_drop_refs(path);\n\t\tkfree(path);\n\t}\n\ttrace_ext4_ext_map_blocks_exit(inode, map->m_lblk,\n\t\tnewblock, map->m_len, err ? err : allocated);\n\treturn err ? err : allocated;\n}\n\nvoid ext4_ext_truncate(struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct super_block *sb = inode->i_sb;\n\text4_lblk_t last_block;\n\thandle_t *handle;\n\tint err = 0;\n\n\t/*\n\t * finish any pending end_io work so we won't run the risk of\n\t * converting any truncated blocks to initialized later\n\t */\n\text4_flush_completed_IO(inode);\n\n\t/*\n\t * probably first extent we're gonna free will be last in block\n\t */\n\terr = ext4_writepage_trans_blocks(inode);\n\thandle = ext4_journal_start(inode, err);\n\tif (IS_ERR(handle))\n\t\treturn;\n\n\tif (inode->i_size & (sb->s_blocksize - 1))\n\t\text4_block_truncate_page(handle, mapping, inode->i_size);\n\n\tif (ext4_orphan_add(handle, inode))\n\t\tgoto out_stop;\n\n\tdown_write(&EXT4_I(inode)->i_data_sem);\n\text4_ext_invalidate_cache(inode);\n\n\text4_discard_preallocations(inode);\n\n\t/*\n\t * TODO: optimization is possible here.\n\t * Probably we need not scan at all,\n\t * because page truncation is enough.\n\t */\n\n\t/* we have to know where to truncate from in crash case */\n\tEXT4_I(inode)->i_disksize = inode->i_size;\n\text4_mark_inode_dirty(handle, inode);\n\n\tlast_block = (inode->i_size + sb->s_blocksize - 1)\n\t\t\t>> EXT4_BLOCK_SIZE_BITS(sb);\n\terr = ext4_ext_remove_space(inode, last_block);\n\n\t/* In a multi-transaction truncate, we only make the final\n\t * transaction synchronous.\n\t */\n\tif (IS_SYNC(inode))\n\t\text4_handle_sync(handle);\n\nout_stop:\n\tup_write(&EXT4_I(inode)->i_data_sem);\n\t/*\n\t * If this was a simple ftruncate() and the file will remain alive,\n\t * then we need to clear up the orphan record which we created above.\n\t * However, if this was a real unlink then we were called by\n\t * ext4_delete_inode(), and we allow that function to clean up the\n\t * orphan info for us.\n\t */\n\tif (inode->i_nlink)\n\t\text4_orphan_del(handle, inode);\n\n\tinode->i_mtime = inode->i_ctime = ext4_current_time(inode);\n\text4_mark_inode_dirty(handle, inode);\n\text4_journal_stop(handle);\n}\n\nstatic void ext4_falloc_update_inode(struct inode *inode,\n\t\t\t\tint mode, loff_t new_size, int update_ctime)\n{\n\tstruct timespec now;\n\n\tif (update_ctime) {\n\t\tnow = current_fs_time(inode->i_sb);\n\t\tif (!timespec_equal(&inode->i_ctime, &now))\n\t\t\tinode->i_ctime = now;\n\t}\n\t/*\n\t * Update only when preallocation was requested beyond\n\t * the file size.\n\t */\n\tif (!(mode & FALLOC_FL_KEEP_SIZE)) {\n\t\tif (new_size > i_size_read(inode))\n\t\t\ti_size_write(inode, new_size);\n\t\tif (new_size > EXT4_I(inode)->i_disksize)\n\t\t\text4_update_i_disksize(inode, new_size);\n\t} else {\n\t\t/*\n\t\t * Mark that we allocate beyond EOF so the subsequent truncate\n\t\t * can proceed even if the new size is the same as i_size.\n\t\t */\n\t\tif (new_size > i_size_read(inode))\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EOFBLOCKS);\n\t}\n\n}\n\n/*\n * preallocate space for a file. This implements ext4's fallocate file\n * operation, which gets called from sys_fallocate system call.\n * For block-mapped files, posix_fallocate should fall back to the method\n * of writing zeroes to the required new blocks (the same behavior which is\n * expected for file systems which do not support fallocate() system call).\n */\nlong ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)\n{\n\tstruct inode *inode = file->f_path.dentry->d_inode;\n\thandle_t *handle;\n\tloff_t new_size;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tint retries = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\t/* We only support the FALLOC_FL_KEEP_SIZE mode */\n\tif (mode & ~FALLOC_FL_KEEP_SIZE)\n\t\treturn -EOPNOTSUPP;\n\n\t/*\n\t * currently supporting (pre)allocate mode for extent-based\n\t * files _only_\n\t */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn -EOPNOTSUPP;\n\n\ttrace_ext4_fallocate_enter(inode, offset, len, mode);\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = (EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits)\n\t\t- map.m_lblk;\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\tmutex_lock(&inode->i_mutex);\n\tret = inode_newsize_ok(inode, (len + offset));\n\tif (ret) {\n\t\tmutex_unlock(&inode->i_mutex);\n\t\ttrace_ext4_fallocate_exit(inode, offset, max_blocks, ret);\n\t\treturn ret;\n\t}\nretry:\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk = map.m_lblk + ret;\n\t\tmap.m_len = max_blocks = max_blocks - ret;\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_CREATE_UNINIT_EXT);\n\t\tif (ret <= 0) {\n#ifdef EXT4FS_DEBUG\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, max_blocks);\n#endif\n\t\t\text4_mark_inode_dirty(handle, inode);\n\t\t\tret2 = ext4_journal_stop(handle);\n\t\t\tbreak;\n\t\t}\n\t\tif ((map.m_lblk + ret) >= (EXT4_BLOCK_ALIGN(offset + len,\n\t\t\t\t\t\tblkbits) >> blkbits))\n\t\t\tnew_size = offset + len;\n\t\telse\n\t\t\tnew_size = (map.m_lblk + ret) << blkbits;\n\n\t\text4_falloc_update_inode(inode, mode, new_size,\n\t\t\t\t\t (map.m_flags & EXT4_MAP_NEW));\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret2)\n\t\t\tbreak;\n\t}\n\tif (ret == -ENOSPC &&\n\t\t\text4_should_retry_alloc(inode->i_sb, &retries)) {\n\t\tret = 0;\n\t\tgoto retry;\n\t}\n\tmutex_unlock(&inode->i_mutex);\n\ttrace_ext4_fallocate_exit(inode, offset, max_blocks,\n\t\t\t\tret > 0 ? ret2 : ret);\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * This function convert a range of blocks to written extents\n * The caller of this function will pass the start offset and the size.\n * all unwritten extents within this range will be converted to\n * written extents.\n *\n * This function is called from the direct IO end io call back\n * function, to convert the fallocated extents after IO is completed.\n * Returns 0 on success.\n */\nint ext4_convert_unwritten_extents(struct inode *inode, loff_t offset,\n\t\t\t\t    ssize_t len)\n{\n\thandle_t *handle;\n\tunsigned int max_blocks;\n\tint ret = 0;\n\tint ret2 = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int credits, blkbits = inode->i_blkbits;\n\n\tmap.m_lblk = offset >> blkbits;\n\t/*\n\t * We can't just convert len to max_blocks because\n\t * If blocksize = 4096 offset = 3072 and len = 2048\n\t */\n\tmax_blocks = ((EXT4_BLOCK_ALIGN(len + offset, blkbits) >> blkbits) -\n\t\t      map.m_lblk);\n\t/*\n\t * credits to insert 1 extent into extent tree\n\t */\n\tcredits = ext4_chunk_trans_blocks(inode, max_blocks);\n\twhile (ret >= 0 && ret < max_blocks) {\n\t\tmap.m_lblk += ret;\n\t\tmap.m_len = (max_blocks -= ret);\n\t\thandle = ext4_journal_start(inode, credits);\n\t\tif (IS_ERR(handle)) {\n\t\t\tret = PTR_ERR(handle);\n\t\t\tbreak;\n\t\t}\n\t\tret = ext4_map_blocks(handle, inode, &map,\n\t\t\t\t      EXT4_GET_BLOCKS_IO_CONVERT_EXT);\n\t\tif (ret <= 0) {\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tprintk(KERN_ERR \"%s: ext4_ext_map_blocks \"\n\t\t\t\t    \"returned error inode#%lu, block=%u, \"\n\t\t\t\t    \"max_blocks=%u\", __func__,\n\t\t\t\t    inode->i_ino, map.m_lblk, map.m_len);\n\t\t}\n\t\text4_mark_inode_dirty(handle, inode);\n\t\tret2 = ext4_journal_stop(handle);\n\t\tif (ret <= 0 || ret2 )\n\t\t\tbreak;\n\t}\n\treturn ret > 0 ? ret2 : ret;\n}\n\n/*\n * Callback function called for each extent to gather FIEMAP information.\n */\nstatic int ext4_ext_fiemap_cb(struct inode *inode, struct ext4_ext_path *path,\n\t\t       struct ext4_ext_cache *newex, struct ext4_extent *ex,\n\t\t       void *data)\n{\n\t__u64\tlogical;\n\t__u64\tphysical;\n\t__u64\tlength;\n\tloff_t\tsize;\n\t__u32\tflags = 0;\n\tint\t\tret = 0;\n\tstruct fiemap_extent_info *fieinfo = data;\n\tunsigned char blksize_bits;\n\n\tblksize_bits = inode->i_sb->s_blocksize_bits;\n\tlogical = (__u64)newex->ec_block << blksize_bits;\n\n\tif (newex->ec_start == 0) {\n\t\t/*\n\t\t * No extent in extent-tree contains block @newex->ec_start,\n\t\t * then the block may stay in 1)a hole or 2)delayed-extent.\n\t\t *\n\t\t * Holes or delayed-extents are processed as follows.\n\t\t * 1. lookup dirty pages with specified range in pagecache.\n\t\t *    If no page is got, then there is no delayed-extent and\n\t\t *    return with EXT_CONTINUE.\n\t\t * 2. find the 1st mapped buffer,\n\t\t * 3. check if the mapped buffer is both in the request range\n\t\t *    and a delayed buffer. If not, there is no delayed-extent,\n\t\t *    then return.\n\t\t * 4. a delayed-extent is found, the extent will be collected.\n\t\t */\n\t\text4_lblk_t\tend = 0;\n\t\tpgoff_t\t\tlast_offset;\n\t\tpgoff_t\t\toffset;\n\t\tpgoff_t\t\tindex;\n\t\tstruct page\t**pages = NULL;\n\t\tstruct buffer_head *bh = NULL;\n\t\tstruct buffer_head *head = NULL;\n\t\tunsigned int nr_pages = PAGE_SIZE / sizeof(struct page *);\n\n\t\tpages = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\t\tif (pages == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\toffset = logical >> PAGE_SHIFT;\nrepeat:\n\t\tlast_offset = offset;\n\t\thead = NULL;\n\t\tret = find_get_pages_tag(inode->i_mapping, &offset,\n\t\t\t\t\tPAGECACHE_TAG_DIRTY, nr_pages, pages);\n\n\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t/* First time, try to find a mapped buffer. */\n\t\t\tif (ret == 0) {\nout:\n\t\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\t\tpage_cache_release(pages[index]);\n\t\t\t\t/* just a hole. */\n\t\t\t\tkfree(pages);\n\t\t\t\treturn EXT_CONTINUE;\n\t\t\t}\n\n\t\t\t/* Try to find the 1st mapped buffer. */\n\t\t\tend = ((__u64)pages[0]->index << PAGE_SHIFT) >>\n\t\t\t\t  blksize_bits;\n\t\t\tif (!page_has_buffers(pages[0]))\n\t\t\t\tgoto out;\n\t\t\thead = page_buffers(pages[0]);\n\t\t\tif (!head)\n\t\t\t\tgoto out;\n\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (buffer_mapped(bh)) {\n\t\t\t\t\t/* get the 1st mapped buffer. */\n\t\t\t\t\tif (end > newex->ec_block +\n\t\t\t\t\t\tnewex->ec_len)\n\t\t\t\t\t\t/* The buffer is out of\n\t\t\t\t\t\t * the request range.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tgoto out;\n\t\t\t\t\tgoto found_mapped_buffer;\n\t\t\t\t}\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\t/* No mapped buffer found. */\n\t\t\tgoto out;\n\t\t} else {\n\t\t\t/*Find contiguous delayed buffers. */\n\t\t\tif (ret > 0 && pages[0]->index == last_offset)\n\t\t\t\thead = page_buffers(pages[0]);\n\t\t\tbh = head;\n\t\t}\n\nfound_mapped_buffer:\n\t\tif (bh != NULL && buffer_delay(bh)) {\n\t\t\t/* 1st or contiguous delayed buffer found. */\n\t\t\tif (!(flags & FIEMAP_EXTENT_DELALLOC)) {\n\t\t\t\t/*\n\t\t\t\t * 1st delayed buffer found, record\n\t\t\t\t * the start of extent.\n\t\t\t\t */\n\t\t\t\tflags |= FIEMAP_EXTENT_DELALLOC;\n\t\t\t\tnewex->ec_block = end;\n\t\t\t\tlogical = (__u64)end << blksize_bits;\n\t\t\t}\n\t\t\t/* Find contiguous delayed buffers. */\n\t\t\tdo {\n\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t\tend++;\n\t\t\t} while (bh != head);\n\n\t\t\tfor (index = 1; index < ret; index++) {\n\t\t\t\tif (!page_has_buffers(pages[index])) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\thead = page_buffers(pages[index]);\n\t\t\t\tif (!head) {\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (pages[index]->index !=\n\t\t\t\t\tpages[0]->index + index) {\n\t\t\t\t\t/* Blocks are not contiguous. */\n\t\t\t\t\tbh = NULL;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbh = head;\n\t\t\t\tdo {\n\t\t\t\t\tif (!buffer_delay(bh))\n\t\t\t\t\t\t/* Delayed-extent ends. */\n\t\t\t\t\t\tgoto found_delayed_extent;\n\t\t\t\t\tbh = bh->b_this_page;\n\t\t\t\t\tend++;\n\t\t\t\t} while (bh != head);\n\t\t\t}\n\t\t} else if (!(flags & FIEMAP_EXTENT_DELALLOC))\n\t\t\t/* a hole found. */\n\t\t\tgoto out;\n\nfound_delayed_extent:\n\t\tnewex->ec_len = min(end - newex->ec_block,\n\t\t\t\t\t\t(ext4_lblk_t)EXT_INIT_MAX_LEN);\n\t\tif (ret == nr_pages && bh != NULL &&\n\t\t\tnewex->ec_len < EXT_INIT_MAX_LEN &&\n\t\t\tbuffer_delay(bh)) {\n\t\t\t/* Have not collected an extent and continue. */\n\t\t\tfor (index = 0; index < ret; index++)\n\t\t\t\tpage_cache_release(pages[index]);\n\t\t\tgoto repeat;\n\t\t}\n\n\t\tfor (index = 0; index < ret; index++)\n\t\t\tpage_cache_release(pages[index]);\n\t\tkfree(pages);\n\t}\n\n\tphysical = (__u64)newex->ec_start << blksize_bits;\n\tlength =   (__u64)newex->ec_len << blksize_bits;\n\n\tif (ex && ext4_ext_is_uninitialized(ex))\n\t\tflags |= FIEMAP_EXTENT_UNWRITTEN;\n\n\tsize = i_size_read(inode);\n\tif (logical + length >= size)\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\tret = fiemap_fill_next_extent(fieinfo, logical, physical,\n\t\t\t\t\tlength, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret == 1)\n\t\treturn EXT_BREAK;\n\treturn EXT_CONTINUE;\n}\n\n/* fiemap flags we can handle specified here */\n#define EXT4_FIEMAP_FLAGS\t(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)\n\nstatic int ext4_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\t__u64 physical = 0;\n\t__u64 length;\n\t__u32 flags = FIEMAP_EXTENT_LAST;\n\tint blockbits = inode->i_sb->s_blocksize_bits;\n\tint error = 0;\n\n\t/* in-inode? */\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tstruct ext4_iloc iloc;\n\t\tint offset;\t/* offset of xattr in inode */\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error)\n\t\t\treturn error;\n\t\tphysical = iloc.bh->b_blocknr << blockbits;\n\t\toffset = EXT4_GOOD_OLD_INODE_SIZE +\n\t\t\t\tEXT4_I(inode)->i_extra_isize;\n\t\tphysical += offset;\n\t\tlength = EXT4_SB(inode->i_sb)->s_inode_size - offset;\n\t\tflags |= FIEMAP_EXTENT_DATA_INLINE;\n\t\tbrelse(iloc.bh);\n\t} else { /* external block */\n\t\tphysical = EXT4_I(inode)->i_file_acl << blockbits;\n\t\tlength = inode->i_sb->s_blocksize;\n\t}\n\n\tif (physical)\n\t\terror = fiemap_fill_next_extent(fieinfo, 0, physical,\n\t\t\t\t\t\tlength, flags);\n\treturn (error < 0 ? error : 0);\n}\n\nint ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\t__u64 start, __u64 len)\n{\n\text4_lblk_t start_blk;\n\tint error = 0;\n\n\t/* fallback to generic here if not in extents fmt */\n\tif (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))\n\t\treturn generic_block_fiemap(inode, fieinfo, start, len,\n\t\t\text4_get_block);\n\n\tif (fiemap_check_flags(fieinfo, EXT4_FIEMAP_FLAGS))\n\t\treturn -EBADR;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\terror = ext4_xattr_fiemap(inode, fieinfo);\n\t} else {\n\t\text4_lblk_t len_blks;\n\t\t__u64 last_blk;\n\n\t\tstart_blk = start >> inode->i_sb->s_blocksize_bits;\n\t\tlast_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;\n\t\tif (last_blk >= EXT_MAX_BLOCK)\n\t\t\tlast_blk = EXT_MAX_BLOCK-1;\n\t\tlen_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;\n\n\t\t/*\n\t\t * Walk the extent tree gathering extent information.\n\t\t * ext4_ext_fiemap_cb will push extents back to user.\n\t\t */\n\t\terror = ext4_ext_walk_space(inode, start_blk, len_blks,\n\t\t\t\t\t  ext4_ext_fiemap_cb, fieinfo);\n\t}\n\n\treturn error;\n}\n\n"], "filenames": ["fs/ext4/extents.c"], "buggy_code_start_loc": [2760], "buggy_code_end_loc": [3253], "fixing_code_start_loc": [2760], "fixing_code_end_loc": [2917], "type": "NVD-CWE-noinfo", "message": "fs/ext4/extents.c in the Linux kernel before 3.0 does not mark a modified extent as dirty in certain cases of extent splitting, which allows local users to cause a denial of service (system crash) via vectors involving ext4 umount and mount operations.", "other": {"cve": {"id": "CVE-2011-3638", "sourceIdentifier": "secalert@redhat.com", "published": "2013-03-01T12:37:54.053", "lastModified": "2023-02-13T04:32:40.607", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "fs/ext4/extents.c in the Linux kernel before 3.0 does not mark a modified extent as dirty in certain cases of extent splitting, which allows local users to cause a denial of service (system crash) via vectors involving ext4 umount and mount operations."}, {"lang": "es", "value": "fs/ext4/extents.c en el kernel de Linux anterior a v3.0 no marca una medida de lo modificado como sucio (\"dirty\") en determinados casos de \"extent splitting\", permitiendo a usuarios locales provocar una denegaci\u00f3n de servicio (ca\u00edda del sistema) a trav\u00e9s de vectores relacionados con ext4 umount y operaciones de montaje."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:H/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "HIGH", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 1.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.0", "matchCriteriaId": "E0135A6D-9FB7-4E1B-B471-914E37494942"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v3.0/ChangeLog-3.0", "source": "secalert@redhat.com", "tags": ["Broken Link"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=667eff35a1f56fa74ce98a0c7c29a40adc1ba4e3", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/10/24/2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=747942", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/667eff35a1f56fa74ce98a0c7c29a40adc1ba4e3", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/667eff35a1f56fa74ce98a0c7c29a40adc1ba4e3"}}