{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_H\n#define _ASM_X86_UACCESS_H\n/*\n * User space memory access functions\n */\n#include <linux/compiler.h>\n#include <linux/kasan-checks.h>\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n#include <asm/smap.h>\n#include <asm/extable.h>\n\n/*\n * The fs value determines whether argument validity checking should be\n * performed or not.  If get_fs() == USER_DS, checking is performed, with\n * get_fs() == KERNEL_DS, checking is bypassed.\n *\n * For historical reasons, these macros are grossly misnamed.\n */\n\n#define MAKE_MM_SEG(s)\t((mm_segment_t) { (s) })\n\n#define KERNEL_DS\tMAKE_MM_SEG(-1UL)\n#define USER_DS \tMAKE_MM_SEG(TASK_SIZE_MAX)\n\n#define get_fs()\t(current->thread.addr_limit)\nstatic inline void set_fs(mm_segment_t fs)\n{\n\tcurrent->thread.addr_limit = fs;\n\t/* On user-mode return, check fs is correct */\n\tset_thread_flag(TIF_FSCHECK);\n}\n\n#define segment_eq(a, b)\t((a).seg == (b).seg)\n#define user_addr_max() (current->thread.addr_limit.seg)\n\n/*\n * Test whether a block of memory is a valid user space address.\n * Returns 0 if the range is valid, nonzero otherwise.\n */\nstatic inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, unsigned long limit)\n{\n\t/*\n\t * If we have used \"sizeof()\" for the size,\n\t * we know it won't overflow the limit (but\n\t * it might overflow the 'addr', so it's\n\t * important to subtract the size from the\n\t * limit, not add it to the address).\n\t */\n\tif (__builtin_constant_p(size))\n\t\treturn unlikely(addr > limit - size);\n\n\t/* Arbitrary sizes? Be careful about overflow */\n\taddr += size;\n\tif (unlikely(addr < size))\n\t\treturn true;\n\treturn unlikely(addr > limit);\n}\n\n#define __range_not_ok(addr, size, limit)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(addr);\t\t\t\t\t\t\\\n\t__chk_range_not_ok((unsigned long __force)(addr), size, limit); \\\n})\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nstatic inline bool pagefault_disabled(void);\n# define WARN_ON_IN_IRQ()\t\\\n\tWARN_ON_ONCE(!in_task() && !pagefault_disabled())\n#else\n# define WARN_ON_IN_IRQ()\n#endif\n\n/**\n * access_ok - Checks if a user space pointer is valid\n * @addr: User space pointer to start of block to check\n * @size: Size of block to check\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * Checks if a pointer to a block of memory in user space is valid.\n *\n * Note that, depending on architecture, this function probably just\n * checks that the pointer is in the user space range - after calling\n * this function, memory access functions may still return -EFAULT.\n *\n * Return: true (nonzero) if the memory block may be valid, false (zero)\n * if it is definitely invalid.\n */\n#define access_ok(addr, size)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_IN_IRQ();\t\t\t\t\t\t\\\n\tlikely(!__range_not_ok(addr, size, user_addr_max()));\t\t\\\n})\n\n/*\n * These are the main single-value transfer routines.  They automatically\n * use the right size if we just have the right pointer type.\n *\n * This gets kind of ugly. We want to return _two_ values in \"get_user()\"\n * and yet we don't want to do any pointers, because that is too much\n * of a performance impact. Thus we have a few rather ugly macros here,\n * and hide all the ugliness from the user.\n *\n * The \"__xxx\" versions of the user access functions are versions that\n * do not verify the address space, that must have been done previously\n * with a separate \"access_ok()\" call (this is used when we do multiple\n * accesses to the same area of user memory).\n */\n\nextern int __get_user_1(void);\nextern int __get_user_2(void);\nextern int __get_user_4(void);\nextern int __get_user_8(void);\nextern int __get_user_bad(void);\n\n#define __uaccess_begin() stac()\n#define __uaccess_end()   clac()\n#define __uaccess_begin_nospec()\t\\\n({\t\t\t\t\t\\\n\tstac();\t\t\t\t\\\n\tbarrier_nospec();\t\t\\\n})\n\n/*\n * This is a type: either unsigned long, if the argument fits into\n * that type, or otherwise unsigned long long.\n */\n#define __inttype(x) \\\n__typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))\n\n/**\n * get_user - Get a simple variable from user space.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Return: zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n/*\n * Careful: we have to cast the result to the type of the pointer\n * for sign reasons.\n *\n * The use of _ASM_DX as the register specifier is a bit of a\n * simplification, as gcc only cares about it as the starting point\n * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits\n * (%ecx being the next register in gcc's x86 register sequence), and\n * %rdx on 64 bits.\n *\n * Clang/LLVM cares about the size of the register, but still wants\n * the base register for something that ends up being a pair.\n */\n#define get_user(x, ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret_gu;\t\t\t\t\t\t\t\\\n\tregister __inttype(*(ptr)) __val_gu asm(\"%\"_ASM_DX);\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\t\\\n\tasm volatile(\"call __get_user_%P4\"\t\t\t\t\\\n\t\t     : \"=a\" (__ret_gu), \"=r\" (__val_gu),\t\t\\\n\t\t\tASM_CALL_CONSTRAINT\t\t\t\t\\\n\t\t     : \"0\" (ptr), \"i\" (sizeof(*(ptr))));\t\t\\\n\t(x) = (__force __typeof__(*(ptr))) __val_gu;\t\t\t\\\n\t__builtin_expect(__ret_gu, 0);\t\t\t\t\t\\\n})\n\n#define __put_user_x(size, x, ptr, __ret_pu)\t\t\t\\\n\tasm volatile(\"call __put_user_\" #size : \"=a\" (__ret_pu)\t\\\n\t\t     : \"0\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n\n\n\n#ifdef CONFIG_X86_32\n#define __put_user_goto_u64(x, addr, label)\t\t\t\\\n\tasm_volatile_goto(\"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, %l2)\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(2b, %l2)\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr)\t\t\t\\\n\t\t     : : label)\n\n#define __put_user_asm_ex_u64(x, addr)\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(2b, 3b)\t\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr))\n\n#define __put_user_x8(x, ptr, __ret_pu)\t\t\t\t\\\n\tasm volatile(\"call __put_user_8\" : \"=a\" (__ret_pu)\t\\\n\t\t     : \"A\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n#else\n#define __put_user_goto_u64(x, ptr, label) \\\n\t__put_user_goto(x, ptr, \"q\", \"\", \"er\", label)\n#define __put_user_asm_ex_u64(x, addr)\t\\\n\t__put_user_asm_ex(x, addr, \"q\", \"\", \"er\")\n#define __put_user_x8(x, ptr, __ret_pu) __put_user_x(8, x, ptr, __ret_pu)\n#endif\n\nextern void __put_user_bad(void);\n\n/*\n * Strange magic calling convention: pointer in %ecx,\n * value in %eax(:%edx), return value in %eax. clobbers %rbx\n */\nextern void __put_user_1(void);\nextern void __put_user_2(void);\nextern void __put_user_4(void);\nextern void __put_user_8(void);\n\n/**\n * put_user - Write a simple value into user space.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Return: zero on success, or -EFAULT on error.\n */\n#define put_user(x, ptr)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __ret_pu;\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val;\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\\\n\t__pu_val = x;\t\t\t\t\t\t\\\n\tswitch (sizeof(*(ptr))) {\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(1, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(2, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(4, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\\\n\t\t__put_user_x8(__pu_val, ptr, __ret_pu);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\\\n\t\t__put_user_x(X, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\t__builtin_expect(__ret_pu, 0);\t\t\t\t\\\n})\n\n#define __put_user_size(x, ptr, size, label)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"b\", \"b\", \"iq\", label);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"w\", \"w\", \"ir\", label);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"l\", \"k\", \"ir\", label);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto_u64(x, ptr, label);\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __put_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"b\", \"b\", \"iq\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"w\", \"w\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"l\", \"k\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex_u64((__typeof__(*ptr))(x), ptr);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_X86_32\n#define __get_user_asm_u64(x, ptr, retval, errret)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __ptr = (ptr);\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %2,%%eax\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %3,%%edx\\n\"\t\t\t\\\n\t\t     \"3:\\n\"\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmov %4,%0\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%eax,%%eax\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%edx,%%edx\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (retval), \"=&A\"(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(__ptr)), \"m\" __m(((u32 __user *)(__ptr)) + 1),\t\\\n\t\t       \"i\" (errret), \"0\" (retval));\t\t\t\\\n})\n\n#define __get_user_asm_ex_u64(x, ptr)\t\t\t(x) = __get_user_bad()\n#else\n#define __get_user_asm_u64(x, ptr, retval, errret) \\\n\t __get_user_asm(x, ptr, retval, \"q\", \"\", \"=r\", errret)\n#define __get_user_asm_ex_u64(x, ptr) \\\n\t __get_user_asm_ex(x, ptr, \"q\", \"\", \"=r\")\n#endif\n\n#define __get_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"b\", \"b\", \"=q\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"w\", \"w\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"l\", \"k\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_u64(x, ptr, retval, errret);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %2,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\txor\"itype\" %\"rtype\"1,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\" (err), ltype(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n#define __get_user_asm_nozero(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %2,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\" (err), ltype(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __get_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"b\", \"b\", \"=q\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"w\", \"w\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"l\", \"k\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex_u64(x, ptr);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %1,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n                     \"3:xor\"itype\" %\"rtype\"0,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"  jmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 3b)\t\t\t\t\\\n\t\t     : ltype(x) : \"m\" (__m(addr)))\n\n#define __put_user_nocheck(x, ptr, size)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\t__label__ __pu_label;\t\t\t\t\t\\\n\tint __pu_err = -EFAULT;\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val = (x);\t\t\t\\\n\t__typeof__(ptr) __pu_ptr = (ptr);\t\t\t\\\n\t__typeof__(size) __pu_size = (size);\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\\\n\t__put_user_size(__pu_val, __pu_ptr, __pu_size, __pu_label);\t\\\n\t__pu_err = 0;\t\t\t\t\t\t\\\n__pu_label:\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\\\n\t__builtin_expect(__pu_err, 0);\t\t\t\t\\\n})\n\n#define __get_user_nocheck(x, ptr, size)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\\\n\t__typeof__(ptr) __gu_ptr = (ptr);\t\t\t\t\\\n\t__typeof__(size) __gu_size = (size);\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\t__get_user_size(__gu_val, __gu_ptr, __gu_size, __gu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\\\n\t__builtin_expect(__gu_err, 0);\t\t\t\t\t\\\n})\n\n/* FIXME: this hack is definitely wrong -AK */\nstruct __large_struct { unsigned long buf[100]; };\n#define __m(x) (*(struct __large_struct __user *)(x))\n\n/*\n * Tell gcc we read from memory instead of writing: this is because\n * we do not write to any memory gcc knows about, so there are no\n * aliasing issues.\n */\n#define __put_user_goto(x, addr, itype, rtype, ltype, label)\t\\\n\tasm_volatile_goto(\"\\n\"\t\t\t\t\t\t\\\n\t\t\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\t\\\n\t\t_ASM_EXTABLE_UA(1b, %l2)\t\t\t\t\t\\\n\t\t: : ltype(x), \"m\" (__m(addr))\t\t\t\t\\\n\t\t: : label)\n\n#define __put_user_failed(x, addr, itype, rtype, ltype, errret)\t\t\\\n\t({\t__label__ __puflab;\t\t\t\t\t\\\n\t\tint __pufret = errret;\t\t\t\t\t\\\n\t\t__put_user_goto(x,addr,itype,rtype,ltype,__puflab);\t\\\n\t\t__pufret = 0;\t\t\t\t\t\t\\\n\t__puflab: __pufret; })\n\n#define __put_user_asm(x, addr, retval, itype, rtype, ltype, errret)\tdo {\t\\\n\tretval = __put_user_failed(x, addr, itype, rtype, ltype, errret);\t\\\n} while (0)\n\n#define __put_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     : : ltype(x), \"m\" (__m(addr)))\n\n/*\n * uaccess_try and catch\n */\n#define uaccess_try\tdo {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tbarrier();\n\n#define uaccess_try_nospec do {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\n#define uaccess_catch(err)\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(err) |= (current->thread.uaccess_err ? -EFAULT : 0);\t\t\\\n} while (0)\n\n/**\n * __get_user - Get a simple variable from user space, with less checking.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Return: zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n\n#define __get_user(x, ptr)\t\t\t\t\t\t\\\n\t__get_user_nocheck((x), (ptr), sizeof(*(ptr)))\n\n/**\n * __put_user - Write a simple value into user space, with less checking.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Return: zero on success, or -EFAULT on error.\n */\n\n#define __put_user(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_nocheck((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\n/*\n * {get|put}_user_try and catch\n *\n * get_user_try {\n *\tget_user_ex(...);\n * } get_user_catch(err)\n */\n#define get_user_try\t\tuaccess_try_nospec\n#define get_user_catch(err)\tuaccess_catch(err)\n\n#define get_user_ex(x, ptr)\tdo {\t\t\t\t\t\\\n\tunsigned long __gue_val;\t\t\t\t\t\\\n\t__get_user_size_ex((__gue_val), (ptr), (sizeof(*(ptr))));\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gue_val;\t\t\t\\\n} while (0)\n\n#define put_user_try\t\tuaccess_try\n#define put_user_catch(err)\tuaccess_catch(err)\n\n#define put_user_ex(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_size_ex((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\nextern unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n);\nextern __must_check long\nstrncpy_from_user(char *dst, const char __user *src, long count);\n\nextern __must_check long strnlen_user(const char __user *str, long n);\n\nunsigned long __must_check clear_user(void __user *mem, unsigned long len);\nunsigned long __must_check __clear_user(void __user *mem, unsigned long len);\n\nextern void __cmpxchg_wrong_size(void)\n\t__compiletime_error(\"Bad argument size for cmpxchg\");\n\n#define __user_atomic_cmpxchg_inatomic(uval, ptr, old, new, size)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret = 0;\t\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __old = (old);\t\t\t\t\\\n\t__typeof__(*(ptr)) __new = (new);\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgb %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"q\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgw %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgl %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tif (!IS_ENABLED(CONFIG_X86_64))\t\t\t\t\\\n\t\t\t__cmpxchg_wrong_size();\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgq %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__cmpxchg_wrong_size();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t*(uval) = __old;\t\t\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define user_atomic_cmpxchg_inatomic(uval, ptr, old, new)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\taccess_ok((ptr), sizeof(*(ptr))) ?\t\t\\\n\t\t__user_atomic_cmpxchg_inatomic((uval), (ptr),\t\t\\\n\t\t\t\t(old), (new), sizeof(*(ptr))) :\t\t\\\n\t\t-EFAULT;\t\t\t\t\t\t\\\n})\n\n/*\n * movsl can be slow when source and dest are not both 8-byte aligned\n */\n#ifdef CONFIG_X86_INTEL_USERCOPY\nextern struct movsl_mask {\n\tint mask;\n} ____cacheline_aligned_in_smp movsl_mask;\n#endif\n\n#define ARCH_HAS_NOCACHE_UACCESS 1\n\n#ifdef CONFIG_X86_32\n# include <asm/uaccess_32.h>\n#else\n# include <asm/uaccess_64.h>\n#endif\n\n/*\n * The \"unsafe\" user accesses aren't really \"unsafe\", but the naming\n * is a big fat warning: you have to not only do the access_ok()\n * checking before using them, but you have to surround them with the\n * user_access_begin/end() pair.\n */\nstatic __must_check __always_inline bool user_access_begin(const void __user *ptr, size_t len)\n{\n\tif (unlikely(!access_ok(ptr,len)))\n\t\treturn 0;\n\t__uaccess_begin_nospec();\n\treturn 1;\n}\n#define user_access_begin(a,b)\tuser_access_begin(a,b)\n#define user_access_end()\t__uaccess_end()\n\n#define user_access_save()\tsmap_save()\n#define user_access_restore(x)\tsmap_restore(x)\n\n#define unsafe_put_user(x, ptr, label)\t\\\n\t__put_user_size((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)), label)\n\n#define unsafe_get_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), sizeof(*(ptr)), __gu_err, -EFAULT);\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\t\\\n\tif (unlikely(__gu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n/*\n * We want the unsafe accessors to always be inlined and use\n * the error labels - thus the macro games.\n */\n#define unsafe_copy_loop(dst, src, len, type, label)\t\t\t\\\n\twhile (len >= sizeof(type)) {\t\t\t\t\t\\\n\t\tunsafe_put_user(*(type *)src,(type __user *)dst,label);\t\\\n\t\tdst += sizeof(type);\t\t\t\t\t\\\n\t\tsrc += sizeof(type);\t\t\t\t\t\\\n\t\tlen -= sizeof(type);\t\t\t\t\t\\\n\t}\n\n#define unsafe_copy_to_user(_dst,_src,_len,label)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tchar __user *__ucu_dst = (_dst);\t\t\t\t\\\n\tconst char *__ucu_src = (_src);\t\t\t\t\t\\\n\tsize_t __ucu_len = (_len);\t\t\t\t\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u64, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u32, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u16, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u8, label);\t\\\n} while (0)\n\n#endif /* _ASM_X86_UACCESS_H */\n\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_32_H\n#define _ASM_X86_UACCESS_32_H\n\n/*\n * User space memory access functions\n */\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n\nunsigned long __must_check __copy_user_ll\n\t\t(void *to, const void *from, unsigned long n);\nunsigned long __must_check __copy_from_user_ll_nocache_nozero\n\t\t(void *to, const void __user *from, unsigned long n);\n\nstatic __always_inline unsigned long __must_check\nraw_copy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\treturn __copy_user_ll((__force void *)to, from, n);\n}\n\nstatic __always_inline unsigned long\nraw_copy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tif (__builtin_constant_p(n)) {\n\t\tunsigned long ret;\n\n\t\tswitch (n) {\n\t\tcase 1:\n\t\t\tret = 0;\n\t\t\t__uaccess_begin_nospec();\n\t\t\t__get_user_asm_nozero(*(u8 *)to, from, ret,\n\t\t\t\t\t      \"b\", \"b\", \"=q\", 1);\n\t\t\t__uaccess_end();\n\t\t\treturn ret;\n\t\tcase 2:\n\t\t\tret = 0;\n\t\t\t__uaccess_begin_nospec();\n\t\t\t__get_user_asm_nozero(*(u16 *)to, from, ret,\n\t\t\t\t\t      \"w\", \"w\", \"=r\", 2);\n\t\t\t__uaccess_end();\n\t\t\treturn ret;\n\t\tcase 4:\n\t\t\tret = 0;\n\t\t\t__uaccess_begin_nospec();\n\t\t\t__get_user_asm_nozero(*(u32 *)to, from, ret,\n\t\t\t\t\t      \"l\", \"k\", \"=r\", 4);\n\t\t\t__uaccess_end();\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn __copy_user_ll(to, (__force const void *)from, n);\n}\n\nstatic __always_inline unsigned long\n__copy_from_user_inatomic_nocache(void *to, const void __user *from,\n\t\t\t\t  unsigned long n)\n{\n       return __copy_from_user_ll_nocache_nozero(to, from, n);\n}\n\n#endif /* _ASM_X86_UACCESS_32_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_64_H\n#define _ASM_X86_UACCESS_64_H\n\n/*\n * User space memory access functions\n */\n#include <linux/compiler.h>\n#include <linux/lockdep.h>\n#include <linux/kasan-checks.h>\n#include <asm/alternative.h>\n#include <asm/cpufeatures.h>\n#include <asm/page.h>\n\n/*\n * Copy To/From Userspace\n */\n\n/* Handles exceptions in both to and from, but doesn't do access_ok */\n__must_check unsigned long\ncopy_user_enhanced_fast_string(void *to, const void *from, unsigned len);\n__must_check unsigned long\ncopy_user_generic_string(void *to, const void *from, unsigned len);\n__must_check unsigned long\ncopy_user_generic_unrolled(void *to, const void *from, unsigned len);\n\nstatic __always_inline __must_check unsigned long\ncopy_user_generic(void *to, const void *from, unsigned len)\n{\n\tunsigned ret;\n\n\t/*\n\t * If CPU has ERMS feature, use copy_user_enhanced_fast_string.\n\t * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.\n\t * Otherwise, use copy_user_generic_unrolled.\n\t */\n\talternative_call_2(copy_user_generic_unrolled,\n\t\t\t copy_user_generic_string,\n\t\t\t X86_FEATURE_REP_GOOD,\n\t\t\t copy_user_enhanced_fast_string,\n\t\t\t X86_FEATURE_ERMS,\n\t\t\t ASM_OUTPUT2(\"=a\" (ret), \"=D\" (to), \"=S\" (from),\n\t\t\t\t     \"=d\" (len)),\n\t\t\t \"1\" (to), \"2\" (from), \"3\" (len)\n\t\t\t : \"memory\", \"rcx\", \"r8\", \"r9\", \"r10\", \"r11\");\n\treturn ret;\n}\n\nstatic __always_inline __must_check unsigned long\ncopy_to_user_mcsafe(void *to, const void *from, unsigned len)\n{\n\tunsigned long ret;\n\n\t__uaccess_begin();\n\t/*\n\t * Note, __memcpy_mcsafe() is explicitly used since it can\n\t * handle exceptions / faults.  memcpy_mcsafe() may fall back to\n\t * memcpy() which lacks this handling.\n\t */\n\tret = __memcpy_mcsafe(to, from, len);\n\t__uaccess_end();\n\treturn ret;\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_from_user(void *dst, const void __user *src, unsigned long size)\n{\n\tint ret = 0;\n\n\tif (!__builtin_constant_p(size))\n\t\treturn copy_user_generic(dst, (__force void *)src, size);\n\tswitch (size) {\n\tcase 1:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u8 *)dst, (u8 __user *)src,\n\t\t\t      ret, \"b\", \"b\", \"=q\", 1);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 2:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u16 *)dst, (u16 __user *)src,\n\t\t\t      ret, \"w\", \"w\", \"=r\", 2);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 4:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u32 *)dst, (u32 __user *)src,\n\t\t\t      ret, \"l\", \"k\", \"=r\", 4);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 8:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,\n\t\t\t      ret, \"q\", \"\", \"=r\", 8);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 10:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,\n\t\t\t       ret, \"q\", \"\", \"=r\", 10);\n\t\tif (likely(!ret))\n\t\t\t__get_user_asm_nozero(*(u16 *)(8 + (char *)dst),\n\t\t\t\t       (u16 __user *)(8 + (char __user *)src),\n\t\t\t\t       ret, \"w\", \"w\", \"=r\", 2);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 16:\n\t\t__uaccess_begin_nospec();\n\t\t__get_user_asm_nozero(*(u64 *)dst, (u64 __user *)src,\n\t\t\t       ret, \"q\", \"\", \"=r\", 16);\n\t\tif (likely(!ret))\n\t\t\t__get_user_asm_nozero(*(u64 *)(8 + (char *)dst),\n\t\t\t\t       (u64 __user *)(8 + (char __user *)src),\n\t\t\t\t       ret, \"q\", \"\", \"=r\", 8);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tdefault:\n\t\treturn copy_user_generic(dst, (__force void *)src, size);\n\t}\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_to_user(void __user *dst, const void *src, unsigned long size)\n{\n\tint ret = 0;\n\n\tif (!__builtin_constant_p(size))\n\t\treturn copy_user_generic((__force void *)dst, src, size);\n\tswitch (size) {\n\tcase 1:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u8 *)src, (u8 __user *)dst,\n\t\t\t      ret, \"b\", \"b\", \"iq\", 1);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 2:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u16 *)src, (u16 __user *)dst,\n\t\t\t      ret, \"w\", \"w\", \"ir\", 2);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 4:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u32 *)src, (u32 __user *)dst,\n\t\t\t      ret, \"l\", \"k\", \"ir\", 4);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 8:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u64 *)src, (u64 __user *)dst,\n\t\t\t      ret, \"q\", \"\", \"er\", 8);\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 10:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u64 *)src, (u64 __user *)dst,\n\t\t\t       ret, \"q\", \"\", \"er\", 10);\n\t\tif (likely(!ret)) {\n\t\t\tasm(\"\":::\"memory\");\n\t\t\t__put_user_asm(4[(u16 *)src], 4 + (u16 __user *)dst,\n\t\t\t\t       ret, \"w\", \"w\", \"ir\", 2);\n\t\t}\n\t\t__uaccess_end();\n\t\treturn ret;\n\tcase 16:\n\t\t__uaccess_begin();\n\t\t__put_user_asm(*(u64 *)src, (u64 __user *)dst,\n\t\t\t       ret, \"q\", \"\", \"er\", 16);\n\t\tif (likely(!ret)) {\n\t\t\tasm(\"\":::\"memory\");\n\t\t\t__put_user_asm(1[(u64 *)src], 1 + (u64 __user *)dst,\n\t\t\t\t       ret, \"q\", \"\", \"er\", 8);\n\t\t}\n\t\t__uaccess_end();\n\t\treturn ret;\n\tdefault:\n\t\treturn copy_user_generic((__force void *)dst, src, size);\n\t}\n}\n\nstatic __always_inline __must_check\nunsigned long raw_copy_in_user(void __user *dst, const void __user *src, unsigned long size)\n{\n\treturn copy_user_generic((__force void *)dst,\n\t\t\t\t (__force void *)src, size);\n}\n\nextern long __copy_user_nocache(void *dst, const void __user *src,\n\t\t\t\tunsigned size, int zerorest);\n\nextern long __copy_user_flushcache(void *dst, const void __user *src, unsigned size);\nextern void memcpy_page_flushcache(char *to, struct page *page, size_t offset,\n\t\t\t   size_t len);\n\nstatic inline int\n__copy_from_user_inatomic_nocache(void *dst, const void __user *src,\n\t\t\t\t  unsigned size)\n{\n\tkasan_check_write(dst, size);\n\treturn __copy_user_nocache(dst, src, size, 0);\n}\n\nstatic inline int\n__copy_from_user_flushcache(void *dst, const void __user *src, unsigned size)\n{\n\tkasan_check_write(dst, size);\n\treturn __copy_user_flushcache(dst, src, size);\n}\n\nunsigned long\nmcsafe_handle_tail(char *to, char *from, unsigned len);\n\n#endif /* _ASM_X86_UACCESS_64_H */\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_H\n#define _ASM_X86_UACCESS_H\n/*\n * User space memory access functions\n */\n#include <linux/compiler.h>\n#include <linux/kasan-checks.h>\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n#include <asm/smap.h>\n#include <asm/extable.h>\n\n/*\n * The fs value determines whether argument validity checking should be\n * performed or not.  If get_fs() == USER_DS, checking is performed, with\n * get_fs() == KERNEL_DS, checking is bypassed.\n *\n * For historical reasons, these macros are grossly misnamed.\n */\n\n#define MAKE_MM_SEG(s)\t((mm_segment_t) { (s) })\n\n#define KERNEL_DS\tMAKE_MM_SEG(-1UL)\n#define USER_DS \tMAKE_MM_SEG(TASK_SIZE_MAX)\n\n#define get_fs()\t(current->thread.addr_limit)\nstatic inline void set_fs(mm_segment_t fs)\n{\n\tcurrent->thread.addr_limit = fs;\n\t/* On user-mode return, check fs is correct */\n\tset_thread_flag(TIF_FSCHECK);\n}\n\n#define segment_eq(a, b)\t((a).seg == (b).seg)\n#define user_addr_max() (current->thread.addr_limit.seg)\n\n/*\n * Test whether a block of memory is a valid user space address.\n * Returns 0 if the range is valid, nonzero otherwise.\n */\nstatic inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, unsigned long limit)\n{\n\t/*\n\t * If we have used \"sizeof()\" for the size,\n\t * we know it won't overflow the limit (but\n\t * it might overflow the 'addr', so it's\n\t * important to subtract the size from the\n\t * limit, not add it to the address).\n\t */\n\tif (__builtin_constant_p(size))\n\t\treturn unlikely(addr > limit - size);\n\n\t/* Arbitrary sizes? Be careful about overflow */\n\taddr += size;\n\tif (unlikely(addr < size))\n\t\treturn true;\n\treturn unlikely(addr > limit);\n}\n\n#define __range_not_ok(addr, size, limit)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(addr);\t\t\t\t\t\t\\\n\t__chk_range_not_ok((unsigned long __force)(addr), size, limit); \\\n})\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\nstatic inline bool pagefault_disabled(void);\n# define WARN_ON_IN_IRQ()\t\\\n\tWARN_ON_ONCE(!in_task() && !pagefault_disabled())\n#else\n# define WARN_ON_IN_IRQ()\n#endif\n\n/**\n * access_ok - Checks if a user space pointer is valid\n * @addr: User space pointer to start of block to check\n * @size: Size of block to check\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * Checks if a pointer to a block of memory in user space is valid.\n *\n * Note that, depending on architecture, this function probably just\n * checks that the pointer is in the user space range - after calling\n * this function, memory access functions may still return -EFAULT.\n *\n * Return: true (nonzero) if the memory block may be valid, false (zero)\n * if it is definitely invalid.\n */\n#define access_ok(addr, size)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_IN_IRQ();\t\t\t\t\t\t\\\n\tlikely(!__range_not_ok(addr, size, user_addr_max()));\t\t\\\n})\n\n/*\n * These are the main single-value transfer routines.  They automatically\n * use the right size if we just have the right pointer type.\n *\n * This gets kind of ugly. We want to return _two_ values in \"get_user()\"\n * and yet we don't want to do any pointers, because that is too much\n * of a performance impact. Thus we have a few rather ugly macros here,\n * and hide all the ugliness from the user.\n *\n * The \"__xxx\" versions of the user access functions are versions that\n * do not verify the address space, that must have been done previously\n * with a separate \"access_ok()\" call (this is used when we do multiple\n * accesses to the same area of user memory).\n */\n\nextern int __get_user_1(void);\nextern int __get_user_2(void);\nextern int __get_user_4(void);\nextern int __get_user_8(void);\nextern int __get_user_bad(void);\n\n#define __uaccess_begin() stac()\n#define __uaccess_end()   clac()\n#define __uaccess_begin_nospec()\t\\\n({\t\t\t\t\t\\\n\tstac();\t\t\t\t\\\n\tbarrier_nospec();\t\t\\\n})\n\n/*\n * This is a type: either unsigned long, if the argument fits into\n * that type, or otherwise unsigned long long.\n */\n#define __inttype(x) \\\n__typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))\n\n/**\n * get_user - Get a simple variable from user space.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Return: zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n/*\n * Careful: we have to cast the result to the type of the pointer\n * for sign reasons.\n *\n * The use of _ASM_DX as the register specifier is a bit of a\n * simplification, as gcc only cares about it as the starting point\n * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits\n * (%ecx being the next register in gcc's x86 register sequence), and\n * %rdx on 64 bits.\n *\n * Clang/LLVM cares about the size of the register, but still wants\n * the base register for something that ends up being a pair.\n */\n#define get_user(x, ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret_gu;\t\t\t\t\t\t\t\\\n\tregister __inttype(*(ptr)) __val_gu asm(\"%\"_ASM_DX);\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\t\\\n\tasm volatile(\"call __get_user_%P4\"\t\t\t\t\\\n\t\t     : \"=a\" (__ret_gu), \"=r\" (__val_gu),\t\t\\\n\t\t\tASM_CALL_CONSTRAINT\t\t\t\t\\\n\t\t     : \"0\" (ptr), \"i\" (sizeof(*(ptr))));\t\t\\\n\t(x) = (__force __typeof__(*(ptr))) __val_gu;\t\t\t\\\n\t__builtin_expect(__ret_gu, 0);\t\t\t\t\t\\\n})\n\n#define __put_user_x(size, x, ptr, __ret_pu)\t\t\t\\\n\tasm volatile(\"call __put_user_\" #size : \"=a\" (__ret_pu)\t\\\n\t\t     : \"0\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n\n\n\n#ifdef CONFIG_X86_32\n#define __put_user_goto_u64(x, addr, label)\t\t\t\\\n\tasm_volatile_goto(\"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, %l2)\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(2b, %l2)\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr)\t\t\t\\\n\t\t     : : label)\n\n#define __put_user_asm_ex_u64(x, addr)\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(2b, 3b)\t\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr))\n\n#define __put_user_x8(x, ptr, __ret_pu)\t\t\t\t\\\n\tasm volatile(\"call __put_user_8\" : \"=a\" (__ret_pu)\t\\\n\t\t     : \"A\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n#else\n#define __put_user_goto_u64(x, ptr, label) \\\n\t__put_user_goto(x, ptr, \"q\", \"\", \"er\", label)\n#define __put_user_asm_ex_u64(x, addr)\t\\\n\t__put_user_asm_ex(x, addr, \"q\", \"\", \"er\")\n#define __put_user_x8(x, ptr, __ret_pu) __put_user_x(8, x, ptr, __ret_pu)\n#endif\n\nextern void __put_user_bad(void);\n\n/*\n * Strange magic calling convention: pointer in %ecx,\n * value in %eax(:%edx), return value in %eax. clobbers %rbx\n */\nextern void __put_user_1(void);\nextern void __put_user_2(void);\nextern void __put_user_4(void);\nextern void __put_user_8(void);\n\n/**\n * put_user - Write a simple value into user space.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Return: zero on success, or -EFAULT on error.\n */\n#define put_user(x, ptr)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __ret_pu;\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val;\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\\\n\t__pu_val = x;\t\t\t\t\t\t\\\n\tswitch (sizeof(*(ptr))) {\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(1, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(2, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(4, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\\\n\t\t__put_user_x8(__pu_val, ptr, __ret_pu);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\\\n\t\t__put_user_x(X, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\t__builtin_expect(__ret_pu, 0);\t\t\t\t\\\n})\n\n#define __put_user_size(x, ptr, size, label)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"b\", \"b\", \"iq\", label);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"w\", \"w\", \"ir\", label);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto(x, ptr, \"l\", \"k\", \"ir\", label);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_goto_u64(x, ptr, label);\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __put_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"b\", \"b\", \"iq\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"w\", \"w\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"l\", \"k\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex_u64((__typeof__(*ptr))(x), ptr);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_X86_32\n#define __get_user_asm_u64(x, ptr, retval, errret)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __ptr = (ptr);\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %2,%%eax\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %3,%%edx\\n\"\t\t\t\\\n\t\t     \"3:\\n\"\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmov %4,%0\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%eax,%%eax\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%edx,%%edx\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (retval), \"=&A\"(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(__ptr)), \"m\" __m(((u32 __user *)(__ptr)) + 1),\t\\\n\t\t       \"i\" (errret), \"0\" (retval));\t\t\t\\\n})\n\n#define __get_user_asm_ex_u64(x, ptr)\t\t\t(x) = __get_user_bad()\n#else\n#define __get_user_asm_u64(x, ptr, retval, errret) \\\n\t __get_user_asm(x, ptr, retval, \"q\", \"\", \"=r\", errret)\n#define __get_user_asm_ex_u64(x, ptr) \\\n\t __get_user_asm_ex(x, ptr, \"q\", \"\", \"=r\")\n#endif\n\n#define __get_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"b\", \"b\", \"=q\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"w\", \"w\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"l\", \"k\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_u64(x, ptr, retval, errret);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %2,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\txor\"itype\" %\"rtype\"1,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\" (err), ltype(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __get_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"b\", \"b\", \"=q\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"w\", \"w\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"l\", \"k\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex_u64(x, ptr);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %1,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n                     \"3:xor\"itype\" %\"rtype\"0,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"  jmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 3b)\t\t\t\t\\\n\t\t     : ltype(x) : \"m\" (__m(addr)))\n\n#define __put_user_nocheck(x, ptr, size)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\t__label__ __pu_label;\t\t\t\t\t\\\n\tint __pu_err = -EFAULT;\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val = (x);\t\t\t\\\n\t__typeof__(ptr) __pu_ptr = (ptr);\t\t\t\\\n\t__typeof__(size) __pu_size = (size);\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\\\n\t__put_user_size(__pu_val, __pu_ptr, __pu_size, __pu_label);\t\\\n\t__pu_err = 0;\t\t\t\t\t\t\\\n__pu_label:\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\\\n\t__builtin_expect(__pu_err, 0);\t\t\t\t\\\n})\n\n#define __get_user_nocheck(x, ptr, size)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\\\n\t__typeof__(ptr) __gu_ptr = (ptr);\t\t\t\t\\\n\t__typeof__(size) __gu_size = (size);\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\t__get_user_size(__gu_val, __gu_ptr, __gu_size, __gu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\\\n\t__builtin_expect(__gu_err, 0);\t\t\t\t\t\\\n})\n\n/* FIXME: this hack is definitely wrong -AK */\nstruct __large_struct { unsigned long buf[100]; };\n#define __m(x) (*(struct __large_struct __user *)(x))\n\n/*\n * Tell gcc we read from memory instead of writing: this is because\n * we do not write to any memory gcc knows about, so there are no\n * aliasing issues.\n */\n#define __put_user_goto(x, addr, itype, rtype, ltype, label)\t\\\n\tasm_volatile_goto(\"\\n\"\t\t\t\t\t\t\\\n\t\t\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\t\\\n\t\t_ASM_EXTABLE_UA(1b, %l2)\t\t\t\t\t\\\n\t\t: : ltype(x), \"m\" (__m(addr))\t\t\t\t\\\n\t\t: : label)\n\n#define __put_user_failed(x, addr, itype, rtype, ltype, errret)\t\t\\\n\t({\t__label__ __puflab;\t\t\t\t\t\\\n\t\tint __pufret = errret;\t\t\t\t\t\\\n\t\t__put_user_goto(x,addr,itype,rtype,ltype,__puflab);\t\\\n\t\t__pufret = 0;\t\t\t\t\t\t\\\n\t__puflab: __pufret; })\n\n#define __put_user_asm(x, addr, retval, itype, rtype, ltype, errret)\tdo {\t\\\n\tretval = __put_user_failed(x, addr, itype, rtype, ltype, errret);\t\\\n} while (0)\n\n#define __put_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     : : ltype(x), \"m\" (__m(addr)))\n\n/*\n * uaccess_try and catch\n */\n#define uaccess_try\tdo {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tbarrier();\n\n#define uaccess_try_nospec do {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\n#define uaccess_catch(err)\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(err) |= (current->thread.uaccess_err ? -EFAULT : 0);\t\t\\\n} while (0)\n\n/**\n * __get_user - Get a simple variable from user space, with less checking.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Return: zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n\n#define __get_user(x, ptr)\t\t\t\t\t\t\\\n\t__get_user_nocheck((x), (ptr), sizeof(*(ptr)))\n\n/**\n * __put_user - Write a simple value into user space, with less checking.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Return: zero on success, or -EFAULT on error.\n */\n\n#define __put_user(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_nocheck((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\n/*\n * {get|put}_user_try and catch\n *\n * get_user_try {\n *\tget_user_ex(...);\n * } get_user_catch(err)\n */\n#define get_user_try\t\tuaccess_try_nospec\n#define get_user_catch(err)\tuaccess_catch(err)\n\n#define get_user_ex(x, ptr)\tdo {\t\t\t\t\t\\\n\tunsigned long __gue_val;\t\t\t\t\t\\\n\t__get_user_size_ex((__gue_val), (ptr), (sizeof(*(ptr))));\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gue_val;\t\t\t\\\n} while (0)\n\n#define put_user_try\t\tuaccess_try\n#define put_user_catch(err)\tuaccess_catch(err)\n\n#define put_user_ex(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_size_ex((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\nextern unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n);\nextern __must_check long\nstrncpy_from_user(char *dst, const char __user *src, long count);\n\nextern __must_check long strnlen_user(const char __user *str, long n);\n\nunsigned long __must_check clear_user(void __user *mem, unsigned long len);\nunsigned long __must_check __clear_user(void __user *mem, unsigned long len);\n\nextern void __cmpxchg_wrong_size(void)\n\t__compiletime_error(\"Bad argument size for cmpxchg\");\n\n#define __user_atomic_cmpxchg_inatomic(uval, ptr, old, new, size)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret = 0;\t\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __old = (old);\t\t\t\t\\\n\t__typeof__(*(ptr)) __new = (new);\t\t\t\t\\\n\t__uaccess_begin_nospec();\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgb %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"q\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgw %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgl %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tif (!IS_ENABLED(CONFIG_X86_64))\t\t\t\t\\\n\t\t\t__cmpxchg_wrong_size();\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgq %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE_UA(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__cmpxchg_wrong_size();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t*(uval) = __old;\t\t\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define user_atomic_cmpxchg_inatomic(uval, ptr, old, new)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\taccess_ok((ptr), sizeof(*(ptr))) ?\t\t\\\n\t\t__user_atomic_cmpxchg_inatomic((uval), (ptr),\t\t\\\n\t\t\t\t(old), (new), sizeof(*(ptr))) :\t\t\\\n\t\t-EFAULT;\t\t\t\t\t\t\\\n})\n\n/*\n * movsl can be slow when source and dest are not both 8-byte aligned\n */\n#ifdef CONFIG_X86_INTEL_USERCOPY\nextern struct movsl_mask {\n\tint mask;\n} ____cacheline_aligned_in_smp movsl_mask;\n#endif\n\n#define ARCH_HAS_NOCACHE_UACCESS 1\n\n#ifdef CONFIG_X86_32\n# include <asm/uaccess_32.h>\n#else\n# include <asm/uaccess_64.h>\n#endif\n\n/*\n * The \"unsafe\" user accesses aren't really \"unsafe\", but the naming\n * is a big fat warning: you have to not only do the access_ok()\n * checking before using them, but you have to surround them with the\n * user_access_begin/end() pair.\n */\nstatic __must_check __always_inline bool user_access_begin(const void __user *ptr, size_t len)\n{\n\tif (unlikely(!access_ok(ptr,len)))\n\t\treturn 0;\n\t__uaccess_begin_nospec();\n\treturn 1;\n}\n#define user_access_begin(a,b)\tuser_access_begin(a,b)\n#define user_access_end()\t__uaccess_end()\n\n#define user_access_save()\tsmap_save()\n#define user_access_restore(x)\tsmap_restore(x)\n\n#define unsafe_put_user(x, ptr, label)\t\\\n\t__put_user_size((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)), label)\n\n#define unsafe_get_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), sizeof(*(ptr)), __gu_err, -EFAULT);\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\t\\\n\tif (unlikely(__gu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n/*\n * We want the unsafe accessors to always be inlined and use\n * the error labels - thus the macro games.\n */\n#define unsafe_copy_loop(dst, src, len, type, label)\t\t\t\\\n\twhile (len >= sizeof(type)) {\t\t\t\t\t\\\n\t\tunsafe_put_user(*(type *)src,(type __user *)dst,label);\t\\\n\t\tdst += sizeof(type);\t\t\t\t\t\\\n\t\tsrc += sizeof(type);\t\t\t\t\t\\\n\t\tlen -= sizeof(type);\t\t\t\t\t\\\n\t}\n\n#define unsafe_copy_to_user(_dst,_src,_len,label)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tchar __user *__ucu_dst = (_dst);\t\t\t\t\\\n\tconst char *__ucu_src = (_src);\t\t\t\t\t\\\n\tsize_t __ucu_len = (_len);\t\t\t\t\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u64, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u32, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u16, label);\t\\\n\tunsafe_copy_loop(__ucu_dst, __ucu_src, __ucu_len, u8, label);\t\\\n} while (0)\n\n#endif /* _ASM_X86_UACCESS_H */\n\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_32_H\n#define _ASM_X86_UACCESS_32_H\n\n/*\n * User space memory access functions\n */\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n\nunsigned long __must_check __copy_user_ll\n\t\t(void *to, const void *from, unsigned long n);\nunsigned long __must_check __copy_from_user_ll_nocache_nozero\n\t\t(void *to, const void __user *from, unsigned long n);\n\nstatic __always_inline unsigned long __must_check\nraw_copy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\treturn __copy_user_ll((__force void *)to, from, n);\n}\n\nstatic __always_inline unsigned long\nraw_copy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\treturn __copy_user_ll(to, (__force const void *)from, n);\n}\n\nstatic __always_inline unsigned long\n__copy_from_user_inatomic_nocache(void *to, const void __user *from,\n\t\t\t\t  unsigned long n)\n{\n       return __copy_from_user_ll_nocache_nozero(to, from, n);\n}\n\n#endif /* _ASM_X86_UACCESS_32_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _ASM_X86_UACCESS_64_H\n#define _ASM_X86_UACCESS_64_H\n\n/*\n * User space memory access functions\n */\n#include <linux/compiler.h>\n#include <linux/lockdep.h>\n#include <linux/kasan-checks.h>\n#include <asm/alternative.h>\n#include <asm/cpufeatures.h>\n#include <asm/page.h>\n\n/*\n * Copy To/From Userspace\n */\n\n/* Handles exceptions in both to and from, but doesn't do access_ok */\n__must_check unsigned long\ncopy_user_enhanced_fast_string(void *to, const void *from, unsigned len);\n__must_check unsigned long\ncopy_user_generic_string(void *to, const void *from, unsigned len);\n__must_check unsigned long\ncopy_user_generic_unrolled(void *to, const void *from, unsigned len);\n\nstatic __always_inline __must_check unsigned long\ncopy_user_generic(void *to, const void *from, unsigned len)\n{\n\tunsigned ret;\n\n\t/*\n\t * If CPU has ERMS feature, use copy_user_enhanced_fast_string.\n\t * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.\n\t * Otherwise, use copy_user_generic_unrolled.\n\t */\n\talternative_call_2(copy_user_generic_unrolled,\n\t\t\t copy_user_generic_string,\n\t\t\t X86_FEATURE_REP_GOOD,\n\t\t\t copy_user_enhanced_fast_string,\n\t\t\t X86_FEATURE_ERMS,\n\t\t\t ASM_OUTPUT2(\"=a\" (ret), \"=D\" (to), \"=S\" (from),\n\t\t\t\t     \"=d\" (len)),\n\t\t\t \"1\" (to), \"2\" (from), \"3\" (len)\n\t\t\t : \"memory\", \"rcx\", \"r8\", \"r9\", \"r10\", \"r11\");\n\treturn ret;\n}\n\nstatic __always_inline __must_check unsigned long\ncopy_to_user_mcsafe(void *to, const void *from, unsigned len)\n{\n\tunsigned long ret;\n\n\t__uaccess_begin();\n\t/*\n\t * Note, __memcpy_mcsafe() is explicitly used since it can\n\t * handle exceptions / faults.  memcpy_mcsafe() may fall back to\n\t * memcpy() which lacks this handling.\n\t */\n\tret = __memcpy_mcsafe(to, from, len);\n\t__uaccess_end();\n\treturn ret;\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_from_user(void *dst, const void __user *src, unsigned long size)\n{\n\treturn copy_user_generic(dst, (__force void *)src, size);\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_to_user(void __user *dst, const void *src, unsigned long size)\n{\n\treturn copy_user_generic((__force void *)dst, src, size);\n}\n\nstatic __always_inline __must_check\nunsigned long raw_copy_in_user(void __user *dst, const void __user *src, unsigned long size)\n{\n\treturn copy_user_generic((__force void *)dst,\n\t\t\t\t (__force void *)src, size);\n}\n\nextern long __copy_user_nocache(void *dst, const void __user *src,\n\t\t\t\tunsigned size, int zerorest);\n\nextern long __copy_user_flushcache(void *dst, const void __user *src, unsigned size);\nextern void memcpy_page_flushcache(char *to, struct page *page, size_t offset,\n\t\t\t   size_t len);\n\nstatic inline int\n__copy_from_user_inatomic_nocache(void *dst, const void __user *src,\n\t\t\t\t  unsigned size)\n{\n\tkasan_check_write(dst, size);\n\treturn __copy_user_nocache(dst, src, size, 0);\n}\n\nstatic inline int\n__copy_from_user_flushcache(void *dst, const void __user *src, unsigned size)\n{\n\tkasan_check_write(dst, size);\n\treturn __copy_user_flushcache(dst, src, size);\n}\n\nunsigned long\nmcsafe_handle_tail(char *to, char *from, unsigned len);\n\n#endif /* _ASM_X86_UACCESS_64_H */\n"], "filenames": ["arch/x86/include/asm/uaccess.h", "arch/x86/include/asm/uaccess_32.h", "arch/x86/include/asm/uaccess_64.h"], "buggy_code_start_loc": [381, 26, 68], "buggy_code_end_loc": [393, 53, 179], "fixing_code_start_loc": [380, 25, 68], "fixing_code_end_loc": [380, 25, 75], "type": "CWE-763", "message": "Copy_from_user on 64-bit versions of the Linux kernel does not implement the __uaccess_begin_nospec allowing a user to bypass the \"access_ok\" check and pass a kernel pointer to copy_from_user(). This would allow an attacker to leak information. We recommend upgrading beyond commit\u00a074e19ef0ff8061ef55957c3abd71614ef0f42f47", "other": {"cve": {"id": "CVE-2023-0459", "sourceIdentifier": "cve-coordination@google.com", "published": "2023-05-25T14:15:09.603", "lastModified": "2023-06-06T13:47:53.550", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Copy_from_user on 64-bit versions of the Linux kernel does not implement the __uaccess_begin_nospec allowing a user to bypass the \"access_ok\" check and pass a kernel pointer to copy_from_user(). This would allow an attacker to leak information. We recommend upgrading beyond commit\u00a074e19ef0ff8061ef55957c3abd71614ef0f42f47"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "cve-coordination@google.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-763"}]}, {"source": "cve-coordination@google.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-763"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.307", "matchCriteriaId": "891AEAFE-6900-48E0-A0EA-B80FCE04588D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.19.0", "versionEndExcluding": "4.19.274", "matchCriteriaId": "2513393F-113F-40C1-828E-542A55286974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.4.0", "versionEndExcluding": "5.4.233", "matchCriteriaId": "01D7B24C-ACBF-43E4-BA77-9FAF9ECD6941"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.10.0", "versionEndExcluding": "5.10.170", "matchCriteriaId": "A69105CF-22D5-4FE7-A3C6-C80D6CCD0E38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.15.0", "versionEndExcluding": "5.15.96", "matchCriteriaId": "B5CC1A01-8B90-47FC-A28A-9F870D9F407E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.1.0", "versionEndExcluding": "6.1.14", "matchCriteriaId": "ACB5D831-9547-4DFD-866F-DFFC1E72C13E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "6.2.0", "versionEndExcluding": "6.2.1", "matchCriteriaId": "876A1437-FED6-4A5F-A74C-42B5ACC15BC7"}]}]}], "references": [{"url": "https://github.com/torvalds/linux/commit/4b842e4e25b12951fa10dedb4bc16bc47e3b850c", "source": "cve-coordination@google.com", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/74e19ef0ff8061ef55957c3abd71614ef0f42f47", "source": "cve-coordination@google.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4b842e4e25b12951fa10dedb4bc16bc47e3b850c"}}