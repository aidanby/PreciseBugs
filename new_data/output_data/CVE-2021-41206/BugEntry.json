{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/xent_op.h\"\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/util/bcast.h\"\n#include \"tensorflow/core/util/determinism.h\"\n#include \"tensorflow/core/util/env_var.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nclass SoftmaxXentWithLogitsOp : public OpKernel {\n public:\n  explicit SoftmaxXentWithLogitsOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& logits_in = context->input(0);\n    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()));\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n                      \" labels_size=\", labels_in.shape().DebugString()));\n      shape_in = BCast::ToShape(bcast.output_shape());\n    }\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(shape_in),\n                errors::InvalidArgument(\"logits and labels must be either \"\n                                        \"2-dimensional, or broadcasted to be \"\n                                        \"2-dimensional\"));\n\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(context, !OpDeterminismRequired(),\n                  errors::Unimplemented(\n                      \"The GPU implementation of SoftmaxCrossEntropyWithLogits\"\n                      \" that would have been executed is not deterministic.\"\n                      \" Note that the Python API uses an alternative,\"\n                      \" deterministic, GPU-accelerated path when determinism is\"\n                      \" enabled.\"));\n    }\n\n    // loss is 1-D (one per example), and size is batch_size.\n\n    Tensor scratch;\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                        TensorShape({shape_in.dim_size(0), 1}),\n                                        &scratch));\n\n    Tensor* loss_out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({shape_in.dim_size(0)}), &loss_out));\n    Tensor* back_out = nullptr;\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      if (logits_in.IsSameSize(labels_in)) {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1},\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1}, logits_in.matrix<T>(),\n                labels_in.matrix<T>(), scratch.matrix<T>(), loss_out->vec<T>(),\n                back_out->matrix<T>());\n      } else {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                BCast::ToIndexArray<2>(bcast.x_bcast()),\n                BCast::ToIndexArray<2>(bcast.y_bcast()),\n                logits_in.template shaped<T, 2>(bcast.x_reshape()),\n                labels_in.template shaped<T, 2>(bcast.y_reshape()),\n                scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n      }\n    }\n  }\n};\n\n// Partial specialization for a CPUDevice, that uses the Eigen implementation\n// from XentEigenImpl.\nnamespace functor {\ntemplate <typename Device, typename T>\nstruct XentFunctorBase {\n  void operator()(const Device& d,\n                  const Eigen::DSizes<Eigen::DenseIndex, 2>& shape,\n                  const Eigen::array<Eigen::DenseIndex, 2>& logits_bcast,\n                  const Eigen::array<Eigen::DenseIndex, 2>& labels_bcast,\n                  typename TTypes<T>::ConstMatrix logits,\n                  typename TTypes<T>::ConstMatrix labels,\n                  typename TTypes<T>::Matrix scratch,\n                  typename TTypes<T>::Vec loss,\n                  typename TTypes<T>::Matrix backprop) {\n    XentEigenImpl<Device, T>::Compute(d, shape, logits_bcast, labels_bcast,\n                                      logits, labels, scratch, loss, backprop);\n  }\n};\n\ntemplate <typename T>\nstruct XentFunctor<CPUDevice, T> : XentFunctorBase<CPUDevice, T> {};\n\n}  // namespace functor\n\n#define REGISTER_CPU(T)                                         \\\n  REGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\") \\\n                              .Device(DEVICE_CPU)               \\\n                              .TypeConstraint<T>(\"T\"),          \\\n                          SoftmaxXentWithLogitsOp<CPUDevice, T>);\nTF_CALL_half(REGISTER_CPU);\nTF_CALL_float(REGISTER_CPU);\nTF_CALL_double(REGISTER_CPU);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, Eigen::half>);\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<double>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, double>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.kernel_tests import xent_op_test_base\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTest(xent_op_test_base.XentOpTestBase):\n\n  @test_util.run_deprecated_v1\n  def testRankTooLarge(self):\n    for dtype in np.float16, np.float32:\n      np_features = np.array([[[1., 1., 1., 1.]], [[1., 2., 3.,\n                                                    4.]]]).astype(dtype)\n      np_labels = np.array([[[0., 0., 0., 1.]], [[0., .5, .5,\n                                                  0.]]]).astype(dtype)\n      self.assertRaisesRegex(ValueError, \"rank 2, but is rank 3\",\n                             gen_nn_ops.softmax_cross_entropy_with_logits,\n                             np_features, np_labels)\n\n  def testFeaturesBroadcast(self):\n    np_f = np.array([[1., 2., 3., 4.],\n                     [1., 2., 3., 4.]]).astype(np.float32)\n    np_l = np.array([[0., 0., 0., 1.],\n                     [0., .5, .5, 0.]]).astype(np.float32)\n    np_loss, np_gradient = self._npXent(labels=np_l, logits=np_f)\n    tf_f = constant_op.constant(\n        np.array([[1., 2., 3., 4.]]).astype(np.float32))\n    tf_l = constant_op.constant(\n        np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32))\n    tf_loss, tf_gradient = gen_nn_ops.softmax_cross_entropy_with_logits(\n        tf_f, tf_l)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n  @test_util.run_deprecated_v1\n  def testNotMatrix(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        gen_nn_ops.softmax_cross_entropy_with_logits([0., 1., 2., 3.],\n                                                     [0., 1., 0., 1.])\n\n\nclass XentBenchmark(test.Benchmark):\n\n  def benchmarkZeroDimension(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"zero_dimension_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          logits = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n  def benchmarkSingleClass(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"single_class_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = constant_op.constant([[1.], [-1.], [0.]],\n                                        dtype=dtypes.float32)\n          logits = constant_op.constant([[-1.], [0.], [1.]],\n                                        dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2015-2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n# The following import is required to register the gradient function.\nfrom tensorflow.python.ops.nn_grad import _SoftmaxCrossEntropyWithLogitsGrad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTestBase(test.TestCase):\n\n  def _opDeterminismEnabled(self):\n    deterministic_ops = os.getenv(\"TF_DETERMINISTIC_OPS\", \"0\")\n    return deterministic_ops in (\"1\", \"true\")\n\n  def _opFwdBwd(self, labels, logits, axis=-1):\n    \"\"\" Runs the op-under-test both forwards and backwards.\"\"\"\n    logits = ops.convert_to_tensor(logits)  # needed for the gradient tape\n    with backprop.GradientTape() as tape:\n      tape.watch(logits)\n      loss = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, dim=axis)\n    return loss, tape.gradient(loss, logits)\n\n  def _npXent(self, labels, logits, dim=-1):\n    if dim == -1:\n      dim = len(logits.shape) - 1\n    one_only_on_dim = list(logits.shape)\n    one_only_on_dim[dim] = 1\n    e = np.exp(logits - np.reshape(np.amax(logits, axis=dim), one_only_on_dim))\n    probs = e / np.reshape(np.sum(e, axis=dim), one_only_on_dim)\n    bp = (probs - labels)\n    l = -np.sum(labels * np.log(probs + 1.0e-20), axis=dim)\n    return l, bp\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  def _testXent2D(self,\n                  np_labels,\n                  np_logits,\n                  with_placeholders=False,\n                  expected_gradient=None):\n    np_loss, np_gradient = self._npXent(labels=np_labels, logits=np_logits)\n    if expected_gradient is not None:\n      np_gradient = expected_gradient\n    with self.cached_session() as sess:\n      if with_placeholders:\n        logits_placeholder = array_ops.placeholder(np_logits.dtype)\n        labels_placeholder = array_ops.placeholder(np_labels.dtype)\n        loss, gradient = self._opFwdBwd(labels_placeholder, logits_placeholder)\n        tf_loss, tf_gradient = sess.run([loss, gradient],\n                                        feed_dict={\n                                            labels_placeholder: np_labels,\n                                            logits_placeholder: np_logits\n                                        })\n      else:\n        loss, gradient = self._opFwdBwd(np_labels, np_logits)\n        tf_loss, tf_gradient = self.evaluate([loss, gradient])\n    self.assertAllCloseAccordingToType(np_loss, tf_loss, half_rtol=1e-2)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n  def _testXentND(self, np_labels, np_logits, dim=-1):\n    np_loss, _ = self._npXent(np_labels, np_logits, dim=dim)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=np_labels, logits=np_logits, dim=dim)\n    tf_loss = self.evaluate(loss)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n\n  def _testSingleClass(self, expected_gradient=[[2.0], [1.0], [0.0], [0.0]]):\n    for dtype in np.float16, np.float32:\n      loss, gradient = self._opFwdBwd(\n          labels=np.array([[-1.], [0.], [1.], [1.]]).astype(dtype),\n          logits=np.array([[1.], [-1.], [0.], [1.]]).astype(dtype))\n      self.assertAllClose([0.0, 0.0, 0.0, 0.0], loss)\n      self.assertAllClose(expected_gradient, gradient)\n\n  def testSingleClass(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testSingleClass()\n\n  def testNpXent(self):\n    # We create 2 batches of logits for testing.\n    # batch 0 is the boring uniform distribution: 1, 1, 1, 1, with target 3.\n    # batch 1 has a bit of difference: 1, 2, 3, 4, with soft targets (1, 2).\n    logits = [[1., 1., 1., 1.], [1., 2., 3., 4.]]\n    labels = [[0., 0., 0., 1.], [0., .5, .5, 0.]]\n\n    # For batch 0, we expect the uniform distribution: 0.25, 0.25, 0.25, 0.25\n    # With a hard target 3, the gradient is [0.25, 0.25, 0.25, -0.75]\n    # The loss for this batch is -log(0.25) = 1.386\n    #\n    # For batch 1, we have:\n    # exp(0) = 1\n    # exp(1) = 2.718\n    # exp(2) = 7.389\n    # exp(3) = 20.085\n    # SUM = 31.192\n    # So we have as probabilities:\n    # exp(0) / SUM = 0.032\n    # exp(1) / SUM = 0.087\n    # exp(2) / SUM = 0.237\n    # exp(3) / SUM = 0.644\n    # With a soft target (1, 2), the gradient is\n    # [0.032, 0.087 - 0.5 = -0.413, 0.237 - 0.5 = -0.263, 0.644]\n    # The loss for this batch is [0.5 * -log(0.087), 0.5 * -log(0.237)]\n    # = [1.3862, 1.9401]\n    np_loss, np_gradient = self._npXent(np.array(labels), np.array(logits))\n    self.assertAllClose(\n        np.array([[0.25, 0.25, 0.25, -0.75], [0.0321, -0.4129, -0.2632,\n                                              0.6439]]),\n        np_gradient,\n        rtol=1.e-3,\n        atol=1.e-3)\n    self.assertAllClose(\n        np.array([1.3862, 1.9401]), np_loss, rtol=1.e-3, atol=1.e-3)\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  @test_util.run_deprecated_v1\n  def _testLabelsBroadcast(self, uniform_labels_gradient):\n    labels = np.array([[0., 0., 0., 1.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits, with_placeholders=True)\n    labels = np.array([[0.], [2.], [0.25]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.],\n                       [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(\n        labels,\n        logits,\n        with_placeholders=True,\n        expected_gradient=uniform_labels_gradient)\n\n  def testLabelsBroadcast(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testLabelsBroadcast(uniform_labels_gradient=[[\n        0.25, 0.25, 0.25, 0.25\n    ], [-1.968, -1.913, -1.763, -1.355], [-0.218, -0.163, -0.013, 0.394]])\n\n  @test_util.run_deprecated_v1\n  def testShapeMismatch(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        self._opFwdBwd(\n            labels=[[0., 1., 0.], [1., 0., 0.]], logits=[[0., 1.], [2., 3.]])\n\n  def testHalf(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits)\n\n  def testFloat(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float32)\n    self._testXent2D(labels, logits)\n\n  def testDouble(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float64)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float64)\n    self._testXent2D(labels, logits)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(logits, [3, 4], x, [3])\n\n      # Check that no extra computation gets performed. When only the first\n      # derivative is requested, the second derivative must not be computed.\n      # So when there is no second derivative, there is no `BatchMatMul` op\n      # in the graph.\n      op_names = [\n          op.op_def.name for op in sess.graph.get_operations() if op.op_def\n      ]\n      self.assertNotIn(\"BatchMatMul\", op_names)\n      self.assertNotIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testGradientLabelWithV2(self):\n    with self.cached_session():\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits_v2(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(labels, [3, 4], x, [3])\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testSecondGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant([\n          0.0, 0.0, 1.0 / 3, 0.0, 1.0 / 3, 0.0, 0.0, 0.0, 0.0, 0.5 / 3, 0.0,\n          0.5 / 3\n      ],\n                                    shape=[12],\n                                    dtype=dtypes.float64,\n                                    name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[12],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      loss = math_ops.reduce_sum(x)\n\n      gradients = gradients_impl.gradients(loss, [logits])[0]\n\n      err = gradient_checker.compute_gradient_error(logits, [12], gradients,\n                                                    [12])\n\n      if not self._opDeterminismEnabled():\n        # Check how second derivative is calculated.\n        # (it is equivalent to a `BatchMatMul` op being in the graph because of\n        # the implementation in SoftmaxCrossEntropyWithLogitsGrad)\n        op_names = [\n            op.op_def.name for op in sess.graph.get_operations() if op.op_def\n        ]\n        self.assertIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  def test3D(self):\n    labels = np.array([[[0., 0., 0., 1.], [0., 1., 0., 0.]],\n                       [[0., 0.5, 0.5, 0.], [0.5, 0.5, 0., 0.]],\n                       [[0., 1., 0., 0.], [0., 0., 1., 0.]]]).astype(np.float32)\n    logits = np.array([[[1., 1., 1., 1.], [1., 2., 3., 4.]],\n                       [[2., 3., 4., 5.], [6., 7., 8., 9.]],\n                       [[5., 4., 3., 2.], [1., 2., 3., 4.]]]).astype(np.float32)\n    self._testXentND(labels, logits, dim=0)\n    self._testXentND(labels, logits, dim=1)\n    self._testXentND(labels, logits, dim=-1)\n\n  def testZeroDimension(self):\n    labels = np.zeros([0, 2, 4]).astype(np.float32)\n    logits = np.zeros([0, 2, 4]).astype(np.float32)\n    np_loss, _ = self._npXent(labels=labels, logits=logits)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n    tf_loss = self.evaluate(loss)\n    self.assertAllEqual(np_loss, tf_loss)\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/xent_op.h\"\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/util/bcast.h\"\n#include \"tensorflow/core/util/determinism.h\"\n#include \"tensorflow/core/util/env_var.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nclass SoftmaxXentWithLogitsOp : public OpKernel {\n public:\n  explicit SoftmaxXentWithLogitsOp(OpKernelConstruction* context)\n      : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& logits_in = context->input(0);\n    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()),\n                /*fewer_dims_optimization=*/false);\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n                      \" labels_size=\", labels_in.shape().DebugString()));\n      shape_in = BCast::ToShape(bcast.output_shape());\n    }\n    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(shape_in),\n                errors::InvalidArgument(\"logits and labels must be either \"\n                                        \"2-dimensional, or broadcasted to be \"\n                                        \"2-dimensional\"));\n\n    if (std::is_same<Device, GPUDevice>::value) {\n      OP_REQUIRES(context, !OpDeterminismRequired(),\n                  errors::Unimplemented(\n                      \"The GPU implementation of SoftmaxCrossEntropyWithLogits\"\n                      \" that would have been executed is not deterministic.\"\n                      \" Note that the Python API uses an alternative,\"\n                      \" deterministic, GPU-accelerated path when determinism is\"\n                      \" enabled.\"));\n    }\n\n    // loss is 1-D (one per example), and size is batch_size.\n\n    Tensor scratch;\n    OP_REQUIRES_OK(\n        context, context->allocate_temp(DataTypeToEnum<T>::value,\n                                        TensorShape({shape_in.dim_size(0), 1}),\n                                        &scratch));\n\n    Tensor* loss_out = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(\n                       0, TensorShape({shape_in.dim_size(0)}), &loss_out));\n    Tensor* back_out = nullptr;\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n              BCast::ToIndexArray<2>(bcast.x_bcast()),\n              BCast::ToIndexArray<2>(bcast.y_bcast()),\n              logits_in.template shaped<T, 2>(bcast.x_reshape()),\n              labels_in.template shaped<T, 2>(bcast.y_reshape()),\n              scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n    }\n  }\n};\n\n// Partial specialization for a CPUDevice, that uses the Eigen implementation\n// from XentEigenImpl.\nnamespace functor {\ntemplate <typename Device, typename T>\nstruct XentFunctorBase {\n  void operator()(const Device& d,\n                  const Eigen::DSizes<Eigen::DenseIndex, 2>& shape,\n                  const Eigen::array<Eigen::DenseIndex, 2>& logits_bcast,\n                  const Eigen::array<Eigen::DenseIndex, 2>& labels_bcast,\n                  typename TTypes<T>::ConstMatrix logits,\n                  typename TTypes<T>::ConstMatrix labels,\n                  typename TTypes<T>::Matrix scratch,\n                  typename TTypes<T>::Vec loss,\n                  typename TTypes<T>::Matrix backprop) {\n    XentEigenImpl<Device, T>::Compute(d, shape, logits_bcast, labels_bcast,\n                                      logits, labels, scratch, loss, backprop);\n  }\n};\n\ntemplate <typename T>\nstruct XentFunctor<CPUDevice, T> : XentFunctorBase<CPUDevice, T> {};\n\n}  // namespace functor\n\n#define REGISTER_CPU(T)                                         \\\n  REGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\") \\\n                              .Device(DEVICE_CPU)               \\\n                              .TypeConstraint<T>(\"T\"),          \\\n                          SoftmaxXentWithLogitsOp<CPUDevice, T>);\nTF_CALL_half(REGISTER_CPU);\nTF_CALL_float(REGISTER_CPU);\nTF_CALL_double(REGISTER_CPU);\n\n#if (defined(GOOGLE_CUDA) && GOOGLE_CUDA) || \\\n    (defined(TENSORFLOW_USE_ROCM) && TENSORFLOW_USE_ROCM)\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<Eigen::half>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, Eigen::half>);\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<float>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, float>);\nREGISTER_KERNEL_BUILDER(Name(\"SoftmaxCrossEntropyWithLogits\")\n                            .Device(DEVICE_GPU)\n                            .TypeConstraint<double>(\"T\"),\n                        SoftmaxXentWithLogitsOp<GPUDevice, double>);\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport sys\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.kernel_tests import xent_op_test_base\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTest(xent_op_test_base.XentOpTestBase):\n\n  @test_util.run_deprecated_v1\n  def testRankTooLarge(self):\n    for dtype in np.float16, np.float32:\n      np_features = np.array([[[1., 1., 1., 1.]], [[1., 2., 3.,\n                                                    4.]]]).astype(dtype)\n      np_labels = np.array([[[0., 0., 0., 1.]], [[0., .5, .5,\n                                                  0.]]]).astype(dtype)\n      self.assertRaisesRegex(ValueError, \"rank 2, but is rank 3\",\n                             gen_nn_ops.softmax_cross_entropy_with_logits,\n                             np_features, np_labels)\n\n  def testFeaturesBroadcast(self):\n    np_f = np.array([[1., 2., 3., 4.],\n                     [1., 2., 3., 4.]]).astype(np.float32)\n    np_l = np.array([[0., 0., 0., 1.],\n                     [0., .5, .5, 0.]]).astype(np.float32)\n    np_loss, np_gradient = self._npXent(labels=np_l, logits=np_f)\n    tf_f = constant_op.constant(\n        np.array([[1., 2., 3., 4.]]).astype(np.float32))\n    tf_l = constant_op.constant(\n        np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32))\n    tf_loss, tf_gradient = gen_nn_ops.softmax_cross_entropy_with_logits(\n        tf_f, tf_l)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n    tf_f = constant_op.constant(np.array([[1.]]).astype(np.float32))\n    tf_l = constant_op.constant(np.array([[1.], [1.]]).astype(np.float32))\n    tf_loss, tf_gradient = gen_nn_ops.softmax_cross_entropy_with_logits(\n        tf_f, tf_l)\n    self.assertAllClose([0, 0], tf_loss)\n    self.assertAllCloseAccordingToType([[0], [0]], tf_gradient)\n\n  @test_util.run_deprecated_v1\n  def testNotMatrix(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        gen_nn_ops.softmax_cross_entropy_with_logits([0., 1., 2., 3.],\n                                                     [0., 1., 0., 1.])\n\n\nclass XentBenchmark(test.Benchmark):\n\n  def benchmarkZeroDimension(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"zero_dimension_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          logits = array_ops.zeros([0, 2, 4], dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n  def benchmarkSingleClass(self):\n    for (m, n, p, use_gpu) in itertools.product(\n        [128],\n        [10, 100, 1000, 10000, 100000],\n        [0.001, 0.01, 0.5, 0.99, 1.0],\n        [False]):\n      k = int(p * n)\n      if k == 0:\n        continue\n      name = \"single_class_m_%d_n_%d_k_%g_use_gpu_%s\" % (m, n, k, use_gpu)\n      device = \"/%s:0\" % (\"gpu\" if use_gpu else \"cpu\")\n      with ops.Graph().as_default():\n        with ops.device(device):\n          labels = constant_op.constant([[1.], [-1.], [0.]],\n                                        dtype=dtypes.float32)\n          logits = constant_op.constant([[-1.], [0.], [1.]],\n                                        dtype=dtypes.float32)\n          op = nn_ops.softmax_cross_entropy_with_logits(\n              labels=labels, logits=logits)\n        with session.Session() as sess:\n          r = self.run_op_benchmark(sess, op, min_iters=100, name=name)\n          gb_processed_input = m * n / 1.0e9\n          throughput = gb_processed_input / r[\"wall_time\"]\n          print(\"Benchmark: %s \\t wall_time: %0.03g s \\t \"\n                \"Throughput: %0.03g GB/s\" % (name, r[\"wall_time\"], throughput))\n          sys.stdout.flush()\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2015-2021 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SoftmaxCrossEntropyWithLogits op.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nimport numpy as np\n\nfrom tensorflow.python.eager import backprop\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\n# The following import is required to register the gradient function.\nfrom tensorflow.python.ops.nn_grad import _SoftmaxCrossEntropyWithLogitsGrad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass XentOpTestBase(test.TestCase):\n\n  def _opDeterminismEnabled(self):\n    deterministic_ops = os.getenv(\"TF_DETERMINISTIC_OPS\", \"0\")\n    return deterministic_ops in (\"1\", \"true\")\n\n  def _opFwdBwd(self, labels, logits, axis=-1):\n    \"\"\" Runs the op-under-test both forwards and backwards.\"\"\"\n    logits = ops.convert_to_tensor(logits)  # needed for the gradient tape\n    with backprop.GradientTape() as tape:\n      tape.watch(logits)\n      loss = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, dim=axis)\n    return loss, tape.gradient(loss, logits)\n\n  def _npXent(self, labels, logits, dim=-1):\n    if dim == -1:\n      dim = len(logits.shape) - 1\n    one_only_on_dim = list(logits.shape)\n    one_only_on_dim[dim] = 1\n    e = np.exp(logits - np.reshape(np.amax(logits, axis=dim), one_only_on_dim))\n    probs = e / np.reshape(np.sum(e, axis=dim), one_only_on_dim)\n    bp = (probs - labels)\n    l = -np.sum(labels * np.log(probs + 1.0e-20), axis=dim)\n    return l, bp\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  def _testXent2D(self,\n                  np_labels,\n                  np_logits,\n                  with_placeholders=False,\n                  expected_gradient=None):\n    np_loss, np_gradient = self._npXent(labels=np_labels, logits=np_logits)\n    if expected_gradient is not None:\n      np_gradient = expected_gradient\n    with self.cached_session() as sess:\n      if with_placeholders:\n        logits_placeholder = array_ops.placeholder(np_logits.dtype)\n        labels_placeholder = array_ops.placeholder(np_labels.dtype)\n        loss, gradient = self._opFwdBwd(labels_placeholder, logits_placeholder)\n        tf_loss, tf_gradient = sess.run([loss, gradient],\n                                        feed_dict={\n                                            labels_placeholder: np_labels,\n                                            logits_placeholder: np_logits\n                                        })\n      else:\n        loss, gradient = self._opFwdBwd(np_labels, np_logits)\n        tf_loss, tf_gradient = self.evaluate([loss, gradient])\n    self.assertAllCloseAccordingToType(np_loss, tf_loss, half_rtol=1e-2)\n    self.assertAllCloseAccordingToType(np_gradient, tf_gradient)\n\n  def _testXentND(self, np_labels, np_logits, dim=-1):\n    np_loss, _ = self._npXent(np_labels, np_logits, dim=dim)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=np_labels, logits=np_logits, dim=dim)\n    tf_loss = self.evaluate(loss)\n    self.assertAllCloseAccordingToType(np_loss, tf_loss)\n\n  def _testSingleClass(self, expected_gradient=[[2.0], [1.0], [0.0], [0.0]]):\n    for dtype in np.float16, np.float32:\n      loss, gradient = self._opFwdBwd(\n          labels=np.array([[-1.], [0.], [1.], [1.]]).astype(dtype),\n          logits=np.array([[1.], [-1.], [0.], [1.]]).astype(dtype))\n      self.assertAllClose([0.0, 0.0, 0.0, 0.0], loss)\n      self.assertAllClose(expected_gradient, gradient)\n\n  def testSingleClass(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testSingleClass()\n\n  def testNpXent(self):\n    # We create 2 batches of logits for testing.\n    # batch 0 is the boring uniform distribution: 1, 1, 1, 1, with target 3.\n    # batch 1 has a bit of difference: 1, 2, 3, 4, with soft targets (1, 2).\n    logits = [[1., 1., 1., 1.], [1., 2., 3., 4.]]\n    labels = [[0., 0., 0., 1.], [0., .5, .5, 0.]]\n\n    # For batch 0, we expect the uniform distribution: 0.25, 0.25, 0.25, 0.25\n    # With a hard target 3, the gradient is [0.25, 0.25, 0.25, -0.75]\n    # The loss for this batch is -log(0.25) = 1.386\n    #\n    # For batch 1, we have:\n    # exp(0) = 1\n    # exp(1) = 2.718\n    # exp(2) = 7.389\n    # exp(3) = 20.085\n    # SUM = 31.192\n    # So we have as probabilities:\n    # exp(0) / SUM = 0.032\n    # exp(1) / SUM = 0.087\n    # exp(2) / SUM = 0.237\n    # exp(3) / SUM = 0.644\n    # With a soft target (1, 2), the gradient is\n    # [0.032, 0.087 - 0.5 = -0.413, 0.237 - 0.5 = -0.263, 0.644]\n    # The loss for this batch is [0.5 * -log(0.087), 0.5 * -log(0.237)]\n    # = [1.3862, 1.9401]\n    np_loss, np_gradient = self._npXent(np.array(labels), np.array(logits))\n    self.assertAllClose(\n        np.array([[0.25, 0.25, 0.25, -0.75], [0.0321, -0.4129, -0.2632,\n                                              0.6439]]),\n        np_gradient,\n        rtol=1.e-3,\n        atol=1.e-3)\n    self.assertAllClose(\n        np.array([1.3862, 1.9401]), np_loss, rtol=1.e-3, atol=1.e-3)\n\n  # TODO(b/123860949): The values are constant folded for XLA, so placeholders\n  # are needed.\n  @test_util.run_deprecated_v1\n  def _testLabelsBroadcast(self, uniform_labels_gradient):\n    labels = np.array([[0., 0., 0., 1.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits, with_placeholders=True)\n    labels = np.array([[1.]]).astype(np.float16)\n    logits = np.array([[1.], [2.]]).astype(np.float16)\n    self._testXent2D(labels, logits, with_placeholders=True)\n    labels = np.array([[0.], [2.], [0.25]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.],\n                       [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(\n        labels,\n        logits,\n        with_placeholders=True,\n        expected_gradient=uniform_labels_gradient)\n\n  def testLabelsBroadcast(self):\n    \"\"\"This method is structured to be easily overridden by a child class.\"\"\"\n    self._testLabelsBroadcast(uniform_labels_gradient=[[\n        0.25, 0.25, 0.25, 0.25\n    ], [-1.968, -1.913, -1.763, -1.355], [-0.218, -0.163, -0.013, 0.394]])\n\n  @test_util.run_deprecated_v1\n  def testShapeMismatch(self):\n    with self.cached_session():\n      with self.assertRaises(ValueError):\n        self._opFwdBwd(\n            labels=[[0., 1., 0.], [1., 0., 0.]], logits=[[0., 1.], [2., 3.]])\n\n  def testHalf(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float16)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float16)\n    self._testXent2D(labels, logits)\n\n  def testFloat(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float32)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float32)\n    self._testXent2D(labels, logits)\n\n  def testDouble(self):\n    labels = np.array([[0., 0., 0., 1.], [0., .5, .5, 0.]]).astype(np.float64)\n    logits = np.array([[1., 1., 1., 1.], [1., 2., 3., 4.]]).astype(np.float64)\n    self._testXent2D(labels, logits)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(logits, [3, 4], x, [3])\n\n      # Check that no extra computation gets performed. When only the first\n      # derivative is requested, the second derivative must not be computed.\n      # So when there is no second derivative, there is no `BatchMatMul` op\n      # in the graph.\n      op_names = [\n          op.op_def.name for op in sess.graph.get_operations() if op.op_def\n      ]\n      self.assertNotIn(\"BatchMatMul\", op_names)\n      self.assertNotIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testGradientLabelWithV2(self):\n    with self.cached_session():\n      labels = constant_op.constant(\n          [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[3, 4],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits_v2(\n          labels=labels, logits=logits, name=\"xent\")\n      err = gradient_checker.compute_gradient_error(labels, [3, 4], x, [3])\n\n    self.assertLess(err, 5e-8)\n\n  @test_util.run_deprecated_v1\n  def testSecondGradient(self):\n    with self.cached_session() as sess:\n      labels = constant_op.constant([\n          0.0, 0.0, 1.0 / 3, 0.0, 1.0 / 3, 0.0, 0.0, 0.0, 0.0, 0.5 / 3, 0.0,\n          0.5 / 3\n      ],\n                                    shape=[12],\n                                    dtype=dtypes.float64,\n                                    name=\"labels\")\n      logits = constant_op.constant(\n          [0.1, 0.2, 0.3, 0.4, 0.1, 0.4, 0.9, 1.6, 0.1, 0.8, 2.7, 6.4],\n          shape=[12],\n          dtype=dtypes.float64,\n          name=\"logits\")\n      x = nn_ops.softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits, name=\"xent\")\n      loss = math_ops.reduce_sum(x)\n\n      gradients = gradients_impl.gradients(loss, [logits])[0]\n\n      err = gradient_checker.compute_gradient_error(logits, [12], gradients,\n                                                    [12])\n\n      if not self._opDeterminismEnabled():\n        # Check how second derivative is calculated.\n        # (it is equivalent to a `BatchMatMul` op being in the graph because of\n        # the implementation in SoftmaxCrossEntropyWithLogitsGrad)\n        op_names = [\n            op.op_def.name for op in sess.graph.get_operations() if op.op_def\n        ]\n        self.assertIn(\"BatchMatMulV2\", op_names)\n\n    self.assertLess(err, 5e-8)\n\n  def test3D(self):\n    labels = np.array([[[0., 0., 0., 1.], [0., 1., 0., 0.]],\n                       [[0., 0.5, 0.5, 0.], [0.5, 0.5, 0., 0.]],\n                       [[0., 1., 0., 0.], [0., 0., 1., 0.]]]).astype(np.float32)\n    logits = np.array([[[1., 1., 1., 1.], [1., 2., 3., 4.]],\n                       [[2., 3., 4., 5.], [6., 7., 8., 9.]],\n                       [[5., 4., 3., 2.], [1., 2., 3., 4.]]]).astype(np.float32)\n    self._testXentND(labels, logits, dim=0)\n    self._testXentND(labels, logits, dim=1)\n    self._testXentND(labels, logits, dim=-1)\n\n  def testZeroDimension(self):\n    labels = np.zeros([0, 2, 4]).astype(np.float32)\n    logits = np.zeros([0, 2, 4]).astype(np.float32)\n    np_loss, _ = self._npXent(labels=labels, logits=logits)\n    loss = nn_ops.softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n    tf_loss = self.evaluate(loss)\n    self.assertAllEqual(np_loss, tf_loss)\n"], "filenames": ["tensorflow/core/kernels/xent_op.cc", "tensorflow/python/kernel_tests/xent_op_test.py", "tensorflow/python/kernel_tests/xent_op_test_base.py"], "buggy_code_start_loc": [49, 64, 153], "buggy_code_end_loc": [105, 64, 153], "fixing_code_start_loc": [49, 65, 154], "fixing_code_end_loc": [98, 72, 157], "type": "CWE-354", "message": "TensorFlow is an open source platform for machine learning. In affected versions several TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible. We have discovered these issues internally via tooling while working on improving/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues. These fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41206", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T22:15:08.397", "lastModified": "2021-11-09T17:56:13.823", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions several TensorFlow operations are missing validation for the shapes of the tensor arguments involved in the call. Depending on the API, this can result in undefined behavior and segfault or `CHECK`-fail related crashes but in some scenarios writes and reads from heap populated arrays are also possible. We have discovered these issues internally via tooling while working on improving/testing GPU op determinism. As such, we don't have reproducers and there will be multiple fixes for these issues. These fixes will be included in TensorFlow 2.7.0. We will also cherrypick these commits on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, varias operaciones de TensorFlow no comprueban las formas de los argumentos del tensor involucrados en la llamada. Dependiendo de la API, esto puede resultar en un comportamiento indefinido y a ca\u00eddas relacionadas con segfault o \"CHECK\"-fail, pero en algunos escenarios tambi\u00e9n son posibles las escrituras y lecturas de arrays poblados por la pila. Hemos detectado estos problemas internamente por medio de herramientas mientras trabaj\u00e1bamos en mejorar/probar el determinismo de las operaciones de la GPU. Por lo tanto, no tenemos reproductores y habr\u00e1 m\u00faltiples correcciones para estos problemas. Estas correcciones ser\u00e1n incluidas en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n vamos a recoger estos commits en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-354"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.4", "matchCriteriaId": "0E596567-6F67-4880-8EC4-CB262BF02E0D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/4d74d8a00b07441cba090a02e0dd9ed385145bf4", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/4dddb2fd0b01cdd196101afbba6518658a2c9e07", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/579261dcd446385831fe4f7457d802a59685121d", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/68422b215e618df5ad375bcdc6d2052e9fd3080a", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/da4aad5946be30e5f049920fa076e1f7ef021261", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/e7f497570abb6b4ae5af4970620cd880e4c0c904", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-pgcq-h79j-2f69", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/4d74d8a00b07441cba090a02e0dd9ed385145bf4"}}