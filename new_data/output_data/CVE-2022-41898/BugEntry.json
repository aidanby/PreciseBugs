{"buggy_code": ["/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define EIGEN_USE_GPU\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/gpu_prim.h\"\n#include \"tensorflow/core/kernels/gpu_prim_helpers.h\"\n#include \"tensorflow/core/kernels/sparse_fill_empty_rows_op.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/gpu_device_functions.h\"\n#include \"tensorflow/core/util/gpu_kernel_helper.h\"\n#include \"tensorflow/core/util/gpu_solvers.h\"  // For ScratchSpace\n\n#if GOOGLE_CUDA\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_activation.h\"\nusing stream_executor::cuda::ScopedActivateExecutorContext;\n#elif TENSORFLOW_USE_ROCM\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_activation.h\"\nusing stream_executor::rocm::ScopedActivateExecutorContext;\n#endif\n\nnamespace tensorflow {\n\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace kernel_forward {\nbool to_pointers(bool x) { return x; }\nint32 to_pointers(int32 x) { return x; }\nint64 to_pointers(int64 x) { return x; }\ntemplate <class T>\nT* to_pointers(T* x) {\n  return x;\n}\ntemplate <class T>\ntypename T::PointerType to_pointers(T& x) {\n  return x.data();\n}\ntemplate <class T>\ntypename T::ConstPointerType to_pointers(const T& x) {\n  return x.data();\n}\n\ntemplate <typename Tindex, typename... CallerArgs, typename... KernelArgs>\nStatus wrap_kernel_call(void (*func)(KernelArgs...), const GPUDevice& device,\n                        Tindex size, CallerArgs... args) {\n  auto config = GetGpuLaunchConfig(size, device);\n  return GpuLaunchKernel(func, config.block_count, config.thread_per_block, 0,\n                         device.stream(), config, to_pointers(args)...);\n}\n};  // namespace kernel_forward\n\nusing kernel_forward::wrap_kernel_call;\n\nnamespace functor {\n\nnamespace {\ntemplate <typename To>\nstruct CastFunctor {\n  template <typename From>\n  __host__ __device__ To operator()(const From& value) const {\n    return static_cast<To>(value);\n  }\n};\n\n// Computes elements_per_row[0..dense_rows] and sets *rows_are_not_ordered to\n// true if the indices are not ordered by row.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CountElementsPerRowKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank, const Tindex* indices,\n    Tindex* elements_per_row, int* rows_are_not_ordered,\n    int* first_invalid_index) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex row = indices[i * rank];\n    if (row < 0 || row >= dense_rows) {\n      GpuAtomicMin(first_invalid_index, i);\n      continue;\n    }\n    GpuAtomicAdd(&elements_per_row[row], 1);\n    // Note that this only needs to compare rows, not columns, to satisfy the\n    // row-major order invariant.\n    if (i > 0 && row < indices[(i - 1) * rank]) {\n      // TODO(benbarsdell): Replace this with atomic_ref::store when available.\n      GpuAtomicMax(rows_are_not_ordered, 1);\n    }\n  }\n}\n\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CopyRowIndicesKernel(\n    GpuLaunchConfig cfg, int rank, const Tindex* indices, Tindex* row_indices) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    row_indices[i] = indices[i * rank];\n  }\n}\n\n// Sets empty_row_indicator[row] to whether the row is empty.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void ComputeEmptyRowIndicatorKernel(\n    GpuLaunchConfig cfg, const Tindex* elements_per_row,\n    bool* empty_row_indicator) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    empty_row_indicator[row] = elements_per_row[row] == 0;\n  }\n}\n\n// Copies indices and values to output_indices and output_values, leaving space\n// in the output for the new elements that will be inserted wherever there is an\n// empty row.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterInputElementsKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank,\n    const Tindex* input_index_map, const Tindex* indices, const T* values,\n    const Tindex* num_new_rows_before, Tindex* output_indices, T* output_values,\n    Tindex* reverse_index_map) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex input_i = input_index_map ? input_index_map[i] : i;\n    Tindex row = indices[input_i * rank];\n    Tindex output_i = i + num_new_rows_before[row];\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = indices[input_i * rank + dim];\n    }\n    output_values[output_i] = values[input_i];\n    if (reverse_index_map) {\n      reverse_index_map[input_i] = output_i;\n    }\n  }\n}\n\n// Sets the new elements (which correspond to the empty rows in the\n// input) in output_indices and output_values.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterNewElementsKernel(\n    GpuLaunchConfig cfg, int rank, const T* default_value,\n    const Tindex* num_new_rows_through, const Tindex* input_row_ends,\n    const bool* empty_row_indicator, Tindex* output_indices, T* output_values) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    if (!empty_row_indicator[row]) continue;  // Only process empty rows\n    Tindex input_i = (row == 0 ? 0 : input_row_ends[row - 1]);\n    Tindex output_i = input_i + (row == 0 ? 0 : num_new_rows_through[row - 1]);\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = (dim == 0) ? row : 0;\n    }\n    output_values[output_i] = *default_value;\n  }\n}\n\n}  // namespace\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context, const Tensor& default_value_t,\n                    const Tensor& indices_t, const Tensor& values_t,\n                    const Tensor& dense_shape_t,\n                    typename AsyncOpKernel::DoneCallback done) {\n    const int kEmptyRowIndicatorOutput = 2;\n\n    const auto default_value = default_value_t.scalar<T>();\n    const auto indices = indices_t.matrix<Tindex>();\n    const auto values = values_t.vec<T>();\n    const auto dense_shape = dense_shape_t.vec<Tindex>();\n\n    const Tindex N = indices_t.shape().dim_size(0);\n    const int rank = indices_t.shape().dim_size(1);\n    const Tindex dense_rows = dense_shape(0);  // Must be on the host\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    se::Stream* stream = context->op_device_context()->stream();\n    if (!stream) return errors::Internal(\"No GPU stream available.\");\n\n    if (dense_rows == 0) {\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          context, N, rank, /*num_empty_rows=*/0, &output_indices,\n          &output_values, &reverse_index_map));\n      if (context->output_required(kEmptyRowIndicatorOutput)) {\n        Tensor* unused = nullptr;\n        TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                    TensorShape({0}), &unused));\n      }\n      done();\n      return OkStatus();\n    }\n\n    // The algorithm as currently implemented is summarized as follows:\n    // 1) Compute elements_per_row (using GpuAtomicAdd).\n    // 2) Compute input_row_ends (the end index of each row) by computing the\n    //    prefix sum of elements_per_row.\n    // 3) Compute empty_row_indicator = (elements_per_row == 0).\n    // 4) Compute num_empty_rows_through (the no. empty rows up to and including\n    //    each row) by computing the prefix sum of empty_row_indicator.\n    // 5) Synchronize and allocate outputs (the sync is done implicitly by\n    //    enqueueing the remainder of the computation onto the stream as a host\n    //    callback).\n    // 6) If rows are not ordered:\n    //      Compute input_index_map by argsorting row indices.\n    // 7) Scatter input elements into output_indices and output_values using\n    //    input_index_map and num_empty_rows_through, leaving spaces for the\n    //    new values that will be inserted.\n    // 8) Scatter new default values into output_indices and output_values using\n    //    num_new_rows_through, input_row_ends, and empty_row_indicator.\n\n    // Summary of temporary allocations:\n    //   Tindex elements_per_row[dense_rows]\n    //   int rows_are_not_ordered[1]\n    //   Tindex row_indices[N]      (if rows_are_not_ordered)\n    //   Tindex input_index_map[N]  (if rows_are_not_ordered)\n    //   Tindex input_row_ends[dense_rows]\n    //   bool empty_row_indicator[dense_rows]\n    //   Tindex num_empty_rows_through[dense_rows]\n    //   Workspace for inclusive sums.\n    //   Workspace for radix sort.\n\n    Tensor elements_per_row_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &elements_per_row_t));\n    auto elements_per_row = elements_per_row_t.flat<Tindex>();\n    se::DeviceMemoryBase elements_per_row_gpu_memory(\n        elements_per_row.data(), dense_rows * sizeof(Tindex));\n    if (!stream\n             ->ThenMemZero(&elements_per_row_gpu_memory,\n                           dense_rows * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor rows_are_not_ordered_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &rows_are_not_ordered_t));\n    auto rows_are_not_ordered_gpu = rows_are_not_ordered_t.flat<int>();\n    se::DeviceMemoryBase rows_are_not_ordered_gpu_memory(\n        rows_are_not_ordered_gpu.data(), sizeof(int));\n    if (!stream->ThenMemZero(&rows_are_not_ordered_gpu_memory, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor first_invalid_index_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &first_invalid_index_t));\n    auto first_invalid_index_gpu = first_invalid_index_t.flat<int>();\n    constexpr const int kAllIndicesValid = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase first_invalid_index_gpu_memory(\n        first_invalid_index_gpu.data(), sizeof(int));\n    if (!stream\n             ->ThenMemset32(&first_invalid_index_gpu_memory, kAllIndicesValid,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          CountElementsPerRowKernel<Tindex>, /*device=*/device, /*size=*/N,\n          dense_rows, rank, indices, elements_per_row, rows_are_not_ordered_gpu,\n          first_invalid_index_gpu));\n    }\n\n    Tensor input_row_ends_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &input_row_ends_t));\n    auto input_row_ends = input_row_ends_t.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                                             /*input=*/elements_per_row.data(),\n                                             /*output=*/input_row_ends.data()));\n\n    Tensor empty_row_indicator_t;\n    bool* empty_row_indicator;\n    if (context->output_required(kEmptyRowIndicatorOutput)) {\n      Tensor* empty_row_indicator_t_ptr = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                  TensorShape({dense_rows}),\n                                                  &empty_row_indicator_t_ptr));\n      empty_row_indicator = empty_row_indicator_t_ptr->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DT_BOOL, TensorShape({dense_rows}), &empty_row_indicator_t));\n      empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n    }\n\n    TF_RETURN_IF_ERROR(wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n                                        /*device=*/device, /*size=*/dense_rows,\n                                        elements_per_row, empty_row_indicator));\n\n    // For each row, the number of empty rows up to and including that row.\n    Tensor num_empty_rows_through_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &num_empty_rows_through_t));\n    auto num_empty_rows_through = num_empty_rows_through_t.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        empty_row_indicator_cast(empty_row_indicator, {});\n\n    // The inclusive sum in CUB does not work do the right thing if\n    // `empty_row_indicator` is passed in as a `bool *`.\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                              /*input=*/empty_row_indicator_cast,\n                              /*output=*/num_empty_rows_through.data()));\n\n    ScratchSpace<Tindex> num_empty_rows_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(num_empty_rows_host.mutable_data(),\n                          se::DeviceMemoryBase(\n                              num_empty_rows_through.data() + (dense_rows - 1),\n                              sizeof(*num_empty_rows_host.data())),\n                          sizeof(*num_empty_rows_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> rows_are_not_ordered_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(rows_are_not_ordered_host.mutable_data(),\n                          rows_are_not_ordered_gpu_memory,\n                          sizeof(*rows_are_not_ordered_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> first_invalid_index_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(first_invalid_index_host.mutable_data(),\n                          first_invalid_index_gpu_memory,\n                          sizeof(*first_invalid_index_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    // We must wait for num_empty_rows and rows_are_not_ordered to be copied to\n    // the host, so we enqueue the remainder of the computation onto the stream\n    // asynchronously to avoid stalling execution.\n    auto async_finish_computation =\n        [this, context, kAllIndicesValid, index_type, N, rank, indices, values,\n         default_value, dense_rows, num_empty_rows_host,\n         rows_are_not_ordered_host, first_invalid_index_host,\n         num_empty_rows_through_t, num_empty_rows_through, input_row_ends_t,\n         input_row_ends, empty_row_indicator_t, empty_row_indicator,\n         done]() -> void {\n      DCHECK(done);  // Crash OK\n\n      // Ensure that within the callback, the proper GPU settings are\n      // configured.\n      auto stream = context->op_device_context()->stream();\n      ScopedActivateExecutorContext scoped_activation{stream->parent()};\n\n      int first_invalid_index = *first_invalid_index_host.data();\n      OP_REQUIRES_ASYNC(context, first_invalid_index == kAllIndicesValid,\n                        errors::InvalidArgument(\"indices(\", first_invalid_index,\n                                                \", 0) is invalid.\"),\n                        done);\n\n      Tindex num_empty_rows = *num_empty_rows_host.data();\n\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          AllocateOutputsExceptEmptyRowIndicator(\n              context, N, rank, num_empty_rows, &output_indices, &output_values,\n              &reverse_index_map),\n          done);\n\n      const GPUDevice& device = context->eigen_device<GPUDevice>();\n\n      Tindex* input_index_map = nullptr;\n      Tensor input_index_map_t;\n      int rows_are_not_ordered = *rows_are_not_ordered_host.data();\n      if (rows_are_not_ordered) {\n        OP_REQUIRES_OK_ASYNC(context,\n                             ArgSortByRows(context, device, N, rank, dense_rows,\n                                           indices, &input_index_map_t),\n                             done);\n        input_index_map = input_index_map_t.vec<Tindex>().data();\n      }\n\n      if (N > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterInputElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/N, dense_rows, rank,\n                             input_index_map, indices, values,\n                             num_empty_rows_through, output_indices,\n                             output_values, reverse_index_map),\n            done);\n      }\n\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n                           /*device=*/device, /*size=*/dense_rows, rank,\n                           default_value, num_empty_rows_through,\n                           input_row_ends, empty_row_indicator, output_indices,\n                           output_values),\n          done);\n\n      done();\n    };\n\n    context->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(stream, async_finish_computation);\n    return OkStatus();\n  }\n\n private:\n  Status AllocateOutputsExceptEmptyRowIndicator(\n      OpKernelContext* context, Tindex N, int rank, Tindex num_empty_rows,\n      Tindex** output_indices, T** output_values, Tindex** reverse_index_map) {\n    Tensor* output_indices_t;\n    const Tindex N_full = N + num_empty_rows;\n    TensorShape output_indices_shape({N_full, rank});\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_indices\", output_indices_shape, &output_indices_t));\n    *output_indices = output_indices_t->matrix<Tindex>().data();\n\n    Tensor* output_values_t;\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_values\", TensorShape({N_full}), &output_values_t));\n    *output_values = output_values_t->vec<T>().data();\n\n    const int kReverseIndexMapOutput = 3;\n    if (context->output_required(kReverseIndexMapOutput)) {\n      Tensor* reverse_index_map_t = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(\n          kReverseIndexMapOutput, TensorShape({N}), &reverse_index_map_t));\n      *reverse_index_map = reverse_index_map_t->vec<Tindex>().data();\n    } else {\n      *reverse_index_map = nullptr;\n    }\n    return OkStatus();\n  }\n\n  Status ArgSortByRows(OpKernelContext* context, const GPUDevice& device,\n                       Tindex N, int rank, Tindex dense_rows,\n                       typename TTypes<Tindex>::ConstMatrix indices,\n                       Tensor* input_index_map_t) {\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    // Extract row indices into separate array for use as keys for sorting.\n    Tensor row_indices_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(index_type, TensorShape({N}), &row_indices_t));\n    auto row_indices = row_indices_t.flat<Tindex>();\n    TF_RETURN_IF_ERROR(wrap_kernel_call(CopyRowIndicesKernel<Tindex>,\n                                        /*device=*/device, /*size=*/N, rank,\n                                        indices, row_indices));\n    // Allocate input_index_map.\n    TF_RETURN_IF_ERROR(context->allocate_temp(index_type, TensorShape({N}),\n                                              input_index_map_t));\n    Tindex* input_index_map = input_index_map_t->flat<Tindex>().data();\n    return GpuRadixSort(context, /*size=*/N, /*keys_in=*/row_indices.data(),\n                        /*keys_out=*/static_cast<Tindex*>(nullptr),\n                        /*indices_in=*/static_cast<Tindex*>(nullptr),\n                        /*indices_out=*/input_index_map,\n                        /*num_bits=*/Log2Ceiling64(dense_rows));\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRows<GPUDevice, T, int64>;\nTF_CALL_POD_TYPES(DEFINE_INT64)\n#undef DEFINE_INT64\n\nnamespace {\n\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void GatherOriginalGradValuesKernel(\n    GpuLaunchConfig cfg, const Tindex* reverse_index_map, const T* grad_values,\n    T* d_values, bool* visited) {\n  GPU_1D_KERNEL_LOOP(input_i, cfg.virtual_thread_count) {\n    Tindex output_i = reverse_index_map[input_i];\n    d_values[input_i] = grad_values[output_i];\n    visited[output_i] = true;\n  }\n}\n\ntemplate <typename T, typename Tindex>\nstruct ZeroMaskedValues {\n  ZeroMaskedValues(const bool* _mask, const T* _values)\n      : mask(_mask), values(_values) {}\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(Tindex i) const {\n    return mask[i] ? T(0) : values[i];\n  }\n  const bool* mask;  // true means return zero instead of value\n  const T* values;\n};\n\n}  // namespace\n\nnamespace functor {\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRowsGrad<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context,\n                    typename TTypes<Tindex>::ConstVec reverse_index_map,\n                    typename TTypes<T>::ConstVec grad_values,\n                    typename TTypes<T>::Vec d_values,\n                    typename TTypes<T>::Scalar d_default_value) {\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    const Tindex N = reverse_index_map.dimension(0);\n    const Tindex N_full = grad_values.dimension(0);\n\n    Tensor visited_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(DT_BOOL, TensorShape({N_full}), &visited_t));\n    auto visited = visited_t.vec<bool>();\n    visited.device(device) = visited.constant(false);\n\n    TF_RETURN_IF_ERROR(wrap_kernel_call(\n        GatherOriginalGradValuesKernel<T, Tindex>, /*device=*/device,\n        /*size=*/N, reverse_index_map, grad_values, d_values, visited));\n\n    // Now we mask out the visited values and sum the remaining ones (which\n    // correspond to the empty rows in the forward input) to compute\n    // d_default_value.\n\n    gpuprim::CountingInputIterator<Tindex, Tindex> counting_iterator(Tindex(0));\n    ZeroMaskedValues<T, Tindex> mask_values_fn(visited.data(),\n                                               grad_values.data());\n    gpuprim::TransformInputIterator<T, decltype(mask_values_fn),\n                                    decltype(counting_iterator), Tindex>\n        transform_iterator(counting_iterator, mask_values_fn);\n\n    std::size_t temp_storage_bytes = 0;\n    auto gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/nullptr, temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to calculate temp_storage_bytes, \"\n          \"status: \",\n          GpuGetErrorString(gpuprim_status));\n    }\n\n    Tensor temp_storage;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_INT8, TensorShape({static_cast<int64_t>(temp_storage_bytes)}),\n        &temp_storage));\n\n    gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/temp_storage.flat<int8>().data(), temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to sum values from originally-empty \"\n          \"rows. temp_storage_bytes: \",\n          temp_storage_bytes, \", status: \", GpuGetErrorString(gpuprim_status));\n    }\n\n    return OkStatus();\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRowsGrad<GPUDevice, T, int64>;\nTF_CALL_REAL_NUMBER_TYPES(DEFINE_INT64);\n#undef DEFINE_INT64\n\n}  // namespace tensorflow\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllCloseAccordingToType(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define EIGEN_USE_GPU\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/common_runtime/gpu/gpu_event_mgr.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/kernels/gpu_prim.h\"\n#include \"tensorflow/core/kernels/gpu_prim_helpers.h\"\n#include \"tensorflow/core/kernels/sparse_fill_empty_rows_op.h\"\n#include \"tensorflow/core/lib/core/bits.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/gpu_device_functions.h\"\n#include \"tensorflow/core/util/gpu_kernel_helper.h\"\n#include \"tensorflow/core/util/gpu_solvers.h\"  // For ScratchSpace\n\n#if GOOGLE_CUDA\n#include \"tensorflow/compiler/xla/stream_executor/cuda/cuda_activation.h\"\nusing stream_executor::cuda::ScopedActivateExecutorContext;\n#elif TENSORFLOW_USE_ROCM\n#include \"tensorflow/compiler/xla/stream_executor/rocm/rocm_activation.h\"\nusing stream_executor::rocm::ScopedActivateExecutorContext;\n#endif\n\nnamespace tensorflow {\n\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace kernel_forward {\nbool to_pointers(bool x) { return x; }\nint32 to_pointers(int32 x) { return x; }\nint64 to_pointers(int64 x) { return x; }\ntemplate <class T>\nT* to_pointers(T* x) {\n  return x;\n}\ntemplate <class T>\ntypename T::PointerType to_pointers(T& x) {\n  return x.data();\n}\ntemplate <class T>\ntypename T::ConstPointerType to_pointers(const T& x) {\n  return x.data();\n}\n\ntemplate <typename Tindex, typename... CallerArgs, typename... KernelArgs>\nStatus wrap_kernel_call(void (*func)(KernelArgs...), const GPUDevice& device,\n                        Tindex size, CallerArgs... args) {\n  auto config = GetGpuLaunchConfig(size, device);\n  return GpuLaunchKernel(func, config.block_count, config.thread_per_block, 0,\n                         device.stream(), config, to_pointers(args)...);\n}\n};  // namespace kernel_forward\n\nusing kernel_forward::wrap_kernel_call;\n\nnamespace functor {\n\nnamespace {\ntemplate <typename To>\nstruct CastFunctor {\n  template <typename From>\n  __host__ __device__ To operator()(const From& value) const {\n    return static_cast<To>(value);\n  }\n};\n\n// Computes elements_per_row[0..dense_rows] and sets *rows_are_not_ordered to\n// true if the indices are not ordered by row.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CountElementsPerRowKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank, const Tindex* indices,\n    Tindex* elements_per_row, int* rows_are_not_ordered,\n    int* first_invalid_index) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex row = indices[i * rank];\n    if (row < 0 || row >= dense_rows) {\n      GpuAtomicMin(first_invalid_index, i);\n      continue;\n    }\n    GpuAtomicAdd(&elements_per_row[row], 1);\n    // Note that this only needs to compare rows, not columns, to satisfy the\n    // row-major order invariant.\n    if (i > 0 && row < indices[(i - 1) * rank]) {\n      // TODO(benbarsdell): Replace this with atomic_ref::store when available.\n      GpuAtomicMax(rows_are_not_ordered, 1);\n    }\n  }\n}\n\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void CopyRowIndicesKernel(\n    GpuLaunchConfig cfg, int rank, const Tindex* indices, Tindex* row_indices) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    row_indices[i] = indices[i * rank];\n  }\n}\n\n// Sets empty_row_indicator[row] to whether the row is empty.\ntemplate <typename Tindex>\n__global__ __launch_bounds__(1024) void ComputeEmptyRowIndicatorKernel(\n    GpuLaunchConfig cfg, const Tindex* elements_per_row,\n    bool* empty_row_indicator) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    empty_row_indicator[row] = elements_per_row[row] == 0;\n  }\n}\n\n// Copies indices and values to output_indices and output_values, leaving space\n// in the output for the new elements that will be inserted wherever there is an\n// empty row.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterInputElementsKernel(\n    GpuLaunchConfig cfg, Tindex dense_rows, int rank,\n    const Tindex* input_index_map, const Tindex* indices, const T* values,\n    const Tindex* num_new_rows_before, Tindex* output_indices, T* output_values,\n    Tindex* reverse_index_map) {\n  GPU_1D_KERNEL_LOOP(i, cfg.virtual_thread_count) {\n    Tindex input_i = input_index_map ? input_index_map[i] : i;\n    Tindex row = indices[input_i * rank];\n    Tindex output_i = i + num_new_rows_before[row];\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = indices[input_i * rank + dim];\n    }\n    output_values[output_i] = values[input_i];\n    if (reverse_index_map) {\n      reverse_index_map[input_i] = output_i;\n    }\n  }\n}\n\n// Sets the new elements (which correspond to the empty rows in the\n// input) in output_indices and output_values.\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void ScatterNewElementsKernel(\n    GpuLaunchConfig cfg, int rank, const T* default_value,\n    const Tindex* num_new_rows_through, const Tindex* input_row_ends,\n    const bool* empty_row_indicator, Tindex* output_indices, T* output_values) {\n  GPU_1D_KERNEL_LOOP(row, cfg.virtual_thread_count) {\n    if (!empty_row_indicator[row]) continue;  // Only process empty rows\n    Tindex input_i = (row == 0 ? 0 : input_row_ends[row - 1]);\n    Tindex output_i = input_i + (row == 0 ? 0 : num_new_rows_through[row - 1]);\n    for (int dim = 0; dim < rank; ++dim) {\n      output_indices[output_i * rank + dim] = (dim == 0) ? row : 0;\n    }\n    output_values[output_i] = *default_value;\n  }\n}\n\n}  // namespace\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRows<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context, const Tensor& default_value_t,\n                    const Tensor& indices_t, const Tensor& values_t,\n                    const Tensor& dense_shape_t,\n                    typename AsyncOpKernel::DoneCallback done) {\n    const int kEmptyRowIndicatorOutput = 2;\n\n    const auto default_value = default_value_t.scalar<T>();\n    const auto indices = indices_t.matrix<Tindex>();\n    const auto values = values_t.vec<T>();\n    const auto dense_shape = dense_shape_t.vec<Tindex>();\n\n    const Tindex N = indices_t.shape().dim_size(0);\n    const int rank = indices_t.shape().dim_size(1);\n    const Tindex dense_rows = dense_shape(0);  // Must be on the host\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    se::Stream* stream = context->op_device_context()->stream();\n    if (!stream) return errors::Internal(\"No GPU stream available.\");\n\n    if (dense_rows == 0) {\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      TF_RETURN_IF_ERROR(AllocateOutputsExceptEmptyRowIndicator(\n          context, N, rank, /*num_empty_rows=*/0, &output_indices,\n          &output_values, &reverse_index_map));\n      if (context->output_required(kEmptyRowIndicatorOutput)) {\n        Tensor* unused = nullptr;\n        TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                    TensorShape({0}), &unused));\n      }\n      done();\n      return OkStatus();\n    }\n\n    // The algorithm as currently implemented is summarized as follows:\n    // 1) Compute elements_per_row (using GpuAtomicAdd).\n    // 2) Compute input_row_ends (the end index of each row) by computing the\n    //    prefix sum of elements_per_row.\n    // 3) Compute empty_row_indicator = (elements_per_row == 0).\n    // 4) Compute num_empty_rows_through (the no. empty rows up to and including\n    //    each row) by computing the prefix sum of empty_row_indicator.\n    // 5) Synchronize and allocate outputs (the sync is done implicitly by\n    //    enqueueing the remainder of the computation onto the stream as a host\n    //    callback).\n    // 6) If rows are not ordered:\n    //      Compute input_index_map by argsorting row indices.\n    // 7) Scatter input elements into output_indices and output_values using\n    //    input_index_map and num_empty_rows_through, leaving spaces for the\n    //    new values that will be inserted.\n    // 8) Scatter new default values into output_indices and output_values using\n    //    num_new_rows_through, input_row_ends, and empty_row_indicator.\n\n    // Summary of temporary allocations:\n    //   Tindex elements_per_row[dense_rows]\n    //   int rows_are_not_ordered[1]\n    //   Tindex row_indices[N]      (if rows_are_not_ordered)\n    //   Tindex input_index_map[N]  (if rows_are_not_ordered)\n    //   Tindex input_row_ends[dense_rows]\n    //   bool empty_row_indicator[dense_rows]\n    //   Tindex num_empty_rows_through[dense_rows]\n    //   Workspace for inclusive sums.\n    //   Workspace for radix sort.\n\n    Tensor elements_per_row_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &elements_per_row_t));\n    auto elements_per_row = elements_per_row_t.flat<Tindex>();\n    se::DeviceMemoryBase elements_per_row_gpu_memory(\n        elements_per_row.data(), dense_rows * sizeof(Tindex));\n    if (!stream\n             ->ThenMemZero(&elements_per_row_gpu_memory,\n                           dense_rows * sizeof(Tindex))\n             .ok()) {\n      return errors::Internal(\"Failed to zero elements_per_row\");\n    }\n    Tensor rows_are_not_ordered_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &rows_are_not_ordered_t));\n    auto rows_are_not_ordered_gpu = rows_are_not_ordered_t.flat<int>();\n    se::DeviceMemoryBase rows_are_not_ordered_gpu_memory(\n        rows_are_not_ordered_gpu.data(), sizeof(int));\n    if (!stream->ThenMemZero(&rows_are_not_ordered_gpu_memory, sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to zero rows_are_not_ordered\");\n    }\n    Tensor first_invalid_index_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(DT_INT32, TensorShape({1}),\n                                              &first_invalid_index_t));\n    auto first_invalid_index_gpu = first_invalid_index_t.flat<int>();\n    constexpr const int kAllIndicesValid = std::numeric_limits<int>::max();\n    se::DeviceMemoryBase first_invalid_index_gpu_memory(\n        first_invalid_index_gpu.data(), sizeof(int));\n    if (!stream\n             ->ThenMemset32(&first_invalid_index_gpu_memory, kAllIndicesValid,\n                            sizeof(int))\n             .ok()) {\n      return errors::Internal(\"Failed to initialize first_invalid_index\");\n    }\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          CountElementsPerRowKernel<Tindex>, /*device=*/device, /*size=*/N,\n          dense_rows, rank, indices, elements_per_row, rows_are_not_ordered_gpu,\n          first_invalid_index_gpu));\n    }\n\n    Tensor input_row_ends_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &input_row_ends_t));\n    auto input_row_ends = input_row_ends_t.flat<Tindex>();\n\n    TF_RETURN_IF_ERROR(GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                                             /*input=*/elements_per_row.data(),\n                                             /*output=*/input_row_ends.data()));\n\n    Tensor empty_row_indicator_t;\n    bool* empty_row_indicator;\n    if (context->output_required(kEmptyRowIndicatorOutput)) {\n      Tensor* empty_row_indicator_t_ptr = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(kEmptyRowIndicatorOutput,\n                                                  TensorShape({dense_rows}),\n                                                  &empty_row_indicator_t_ptr));\n      empty_row_indicator = empty_row_indicator_t_ptr->vec<bool>().data();\n    } else {\n      TF_RETURN_IF_ERROR(context->allocate_temp(\n          DT_BOOL, TensorShape({dense_rows}), &empty_row_indicator_t));\n      empty_row_indicator = empty_row_indicator_t.vec<bool>().data();\n    }\n\n    if (dense_rows > 0) {\n      TF_RETURN_IF_ERROR(\n          wrap_kernel_call(ComputeEmptyRowIndicatorKernel<Tindex>,\n                           /*device=*/device, /*size=*/dense_rows,\n                           elements_per_row, empty_row_indicator));\n    }\n\n    // For each row, the number of empty rows up to and including that row.\n    Tensor num_empty_rows_through_t;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        index_type, TensorShape({dense_rows}), &num_empty_rows_through_t));\n    auto num_empty_rows_through = num_empty_rows_through_t.flat<Tindex>();\n\n    gpuprim::TransformInputIterator<Tindex, CastFunctor<Tindex>, bool*>\n        empty_row_indicator_cast(empty_row_indicator, {});\n\n    // The inclusive sum in CUB does not work do the right thing if\n    // `empty_row_indicator` is passed in as a `bool *`.\n    TF_RETURN_IF_ERROR(\n        GpuInclusivePrefixSum(context, /*size=*/dense_rows,\n                              /*input=*/empty_row_indicator_cast,\n                              /*output=*/num_empty_rows_through.data()));\n\n    ScratchSpace<Tindex> num_empty_rows_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(num_empty_rows_host.mutable_data(),\n                          se::DeviceMemoryBase(\n                              num_empty_rows_through.data() + (dense_rows - 1),\n                              sizeof(*num_empty_rows_host.data())),\n                          sizeof(*num_empty_rows_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy num_empty_rows to host\");\n    }\n\n    ScratchSpace<int> rows_are_not_ordered_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(rows_are_not_ordered_host.mutable_data(),\n                          rows_are_not_ordered_gpu_memory,\n                          sizeof(*rows_are_not_ordered_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy rows_are_not_ordered to host\");\n    }\n\n    ScratchSpace<int> first_invalid_index_host(context, 1, /*on_host=*/true);\n    if (!stream\n             ->ThenMemcpy(first_invalid_index_host.mutable_data(),\n                          first_invalid_index_gpu_memory,\n                          sizeof(*first_invalid_index_host.data()))\n             .ok()) {\n      return errors::Internal(\"Failed to copy first_invalid_index to host\");\n    }\n\n    // We must wait for num_empty_rows and rows_are_not_ordered to be copied to\n    // the host, so we enqueue the remainder of the computation onto the stream\n    // asynchronously to avoid stalling execution.\n    auto async_finish_computation =\n        [this, context, kAllIndicesValid, index_type, N, rank, indices, values,\n         default_value, dense_rows, num_empty_rows_host,\n         rows_are_not_ordered_host, first_invalid_index_host,\n         num_empty_rows_through_t, num_empty_rows_through, input_row_ends_t,\n         input_row_ends, empty_row_indicator_t, empty_row_indicator,\n         done]() -> void {\n      DCHECK(done);  // Crash OK\n\n      // Ensure that within the callback, the proper GPU settings are\n      // configured.\n      auto stream = context->op_device_context()->stream();\n      ScopedActivateExecutorContext scoped_activation{stream->parent()};\n\n      int first_invalid_index = *first_invalid_index_host.data();\n      OP_REQUIRES_ASYNC(context, first_invalid_index == kAllIndicesValid,\n                        errors::InvalidArgument(\"indices(\", first_invalid_index,\n                                                \", 0) is invalid.\"),\n                        done);\n\n      Tindex num_empty_rows = *num_empty_rows_host.data();\n\n      Tindex* output_indices;\n      T* output_values;\n      Tindex* reverse_index_map;\n      OP_REQUIRES_OK_ASYNC(\n          context,\n          AllocateOutputsExceptEmptyRowIndicator(\n              context, N, rank, num_empty_rows, &output_indices, &output_values,\n              &reverse_index_map),\n          done);\n\n      const GPUDevice& device = context->eigen_device<GPUDevice>();\n\n      Tindex* input_index_map = nullptr;\n      Tensor input_index_map_t;\n      int rows_are_not_ordered = *rows_are_not_ordered_host.data();\n      if (rows_are_not_ordered) {\n        OP_REQUIRES_OK_ASYNC(context,\n                             ArgSortByRows(context, device, N, rank, dense_rows,\n                                           indices, &input_index_map_t),\n                             done);\n        input_index_map = input_index_map_t.vec<Tindex>().data();\n      }\n\n      if (N > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterInputElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/N, dense_rows, rank,\n                             input_index_map, indices, values,\n                             num_empty_rows_through, output_indices,\n                             output_values, reverse_index_map),\n            done);\n      }\n\n      if (dense_rows > 0) {\n        OP_REQUIRES_OK_ASYNC(\n            context,\n            wrap_kernel_call(ScatterNewElementsKernel<T, Tindex>,\n                             /*device=*/device, /*size=*/dense_rows, rank,\n                             default_value, num_empty_rows_through,\n                             input_row_ends, empty_row_indicator,\n                             output_indices, output_values),\n            done);\n      }\n\n      done();\n    };\n\n    context->device()\n        ->tensorflow_accelerator_device_info()\n        ->event_mgr->ThenExecute(stream, async_finish_computation);\n    return OkStatus();\n  }\n\n private:\n  Status AllocateOutputsExceptEmptyRowIndicator(\n      OpKernelContext* context, Tindex N, int rank, Tindex num_empty_rows,\n      Tindex** output_indices, T** output_values, Tindex** reverse_index_map) {\n    Tensor* output_indices_t;\n    const Tindex N_full = N + num_empty_rows;\n    TensorShape output_indices_shape({N_full, rank});\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_indices\", output_indices_shape, &output_indices_t));\n    *output_indices = output_indices_t->matrix<Tindex>().data();\n\n    Tensor* output_values_t;\n    TF_RETURN_IF_ERROR(context->allocate_output(\n        \"output_values\", TensorShape({N_full}), &output_values_t));\n    *output_values = output_values_t->vec<T>().data();\n\n    const int kReverseIndexMapOutput = 3;\n    if (context->output_required(kReverseIndexMapOutput)) {\n      Tensor* reverse_index_map_t = nullptr;\n      TF_RETURN_IF_ERROR(context->allocate_output(\n          kReverseIndexMapOutput, TensorShape({N}), &reverse_index_map_t));\n      *reverse_index_map = reverse_index_map_t->vec<Tindex>().data();\n    } else {\n      *reverse_index_map = nullptr;\n    }\n    return OkStatus();\n  }\n\n  Status ArgSortByRows(OpKernelContext* context, const GPUDevice& device,\n                       Tindex N, int rank, Tindex dense_rows,\n                       typename TTypes<Tindex>::ConstMatrix indices,\n                       Tensor* input_index_map_t) {\n    DataType index_type = DataTypeToEnum<Tindex>::value;\n    // Extract row indices into separate array for use as keys for sorting.\n    Tensor row_indices_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(index_type, TensorShape({N}), &row_indices_t));\n    auto row_indices = row_indices_t.flat<Tindex>();\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(CopyRowIndicesKernel<Tindex>,\n                                          /*device=*/device, /*size=*/N, rank,\n                                          indices, row_indices));\n    }\n    // Allocate input_index_map.\n    TF_RETURN_IF_ERROR(context->allocate_temp(index_type, TensorShape({N}),\n                                              input_index_map_t));\n    Tindex* input_index_map = input_index_map_t->flat<Tindex>().data();\n    return GpuRadixSort(context, /*size=*/N, /*keys_in=*/row_indices.data(),\n                        /*keys_out=*/static_cast<Tindex*>(nullptr),\n                        /*indices_in=*/static_cast<Tindex*>(nullptr),\n                        /*indices_out=*/input_index_map,\n                        /*num_bits=*/Log2Ceiling64(dense_rows));\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRows<GPUDevice, T, int64>;\nTF_CALL_POD_TYPES(DEFINE_INT64)\n#undef DEFINE_INT64\n\nnamespace {\n\ntemplate <typename T, typename Tindex>\n__global__ __launch_bounds__(1024) void GatherOriginalGradValuesKernel(\n    GpuLaunchConfig cfg, const Tindex* reverse_index_map, const T* grad_values,\n    T* d_values, bool* visited) {\n  GPU_1D_KERNEL_LOOP(input_i, cfg.virtual_thread_count) {\n    Tindex output_i = reverse_index_map[input_i];\n    d_values[input_i] = grad_values[output_i];\n    visited[output_i] = true;\n  }\n}\n\ntemplate <typename T, typename Tindex>\nstruct ZeroMaskedValues {\n  ZeroMaskedValues(const bool* _mask, const T* _values)\n      : mask(_mask), values(_values) {}\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE T operator()(Tindex i) const {\n    return mask[i] ? T(0) : values[i];\n  }\n  const bool* mask;  // true means return zero instead of value\n  const T* values;\n};\n\n}  // namespace\n\nnamespace functor {\n\ntemplate <typename T, typename Tindex>\nstruct SparseFillEmptyRowsGrad<GPUDevice, T, Tindex> {\n  Status operator()(OpKernelContext* context,\n                    typename TTypes<Tindex>::ConstVec reverse_index_map,\n                    typename TTypes<T>::ConstVec grad_values,\n                    typename TTypes<T>::Vec d_values,\n                    typename TTypes<T>::Scalar d_default_value) {\n    const GPUDevice& device = context->eigen_device<GPUDevice>();\n    const Tindex N = reverse_index_map.dimension(0);\n    const Tindex N_full = grad_values.dimension(0);\n\n    Tensor visited_t;\n    TF_RETURN_IF_ERROR(\n        context->allocate_temp(DT_BOOL, TensorShape({N_full}), &visited_t));\n    auto visited = visited_t.vec<bool>();\n    visited.device(device) = visited.constant(false);\n\n    if (N > 0) {\n      TF_RETURN_IF_ERROR(wrap_kernel_call(\n          GatherOriginalGradValuesKernel<T, Tindex>, /*device=*/device,\n          /*size=*/N, reverse_index_map, grad_values, d_values, visited));\n    }\n\n    // Now we mask out the visited values and sum the remaining ones (which\n    // correspond to the empty rows in the forward input) to compute\n    // d_default_value.\n\n    gpuprim::CountingInputIterator<Tindex, Tindex> counting_iterator(Tindex(0));\n    ZeroMaskedValues<T, Tindex> mask_values_fn(visited.data(),\n                                               grad_values.data());\n    gpuprim::TransformInputIterator<T, decltype(mask_values_fn),\n                                    decltype(counting_iterator), Tindex>\n        transform_iterator(counting_iterator, mask_values_fn);\n\n    std::size_t temp_storage_bytes = 0;\n    auto gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/nullptr, temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to calculate temp_storage_bytes, \"\n          \"status: \",\n          GpuGetErrorString(gpuprim_status));\n    }\n\n    Tensor temp_storage;\n    TF_RETURN_IF_ERROR(context->allocate_temp(\n        DT_INT8, TensorShape({static_cast<int64_t>(temp_storage_bytes)}),\n        &temp_storage));\n\n    gpuprim_status = gpuprim::DeviceReduce::Sum(\n        /*temp_storage=*/temp_storage.flat<int8>().data(), temp_storage_bytes,\n        /*d_in=*/transform_iterator,\n        /*d_out=*/d_default_value.data(),\n        /*num_items=*/N_full,\n        /*stream=*/device.stream());\n\n    if (gpuprim_status != gpuSuccess) {\n      return errors::Internal(\n          \"SparseFillEmptyRowsGrad: Could not launch \"\n          \"gpuprim::DeviceReduce::Sum to sum values from originally-empty \"\n          \"rows. temp_storage_bytes: \",\n          temp_storage_bytes, \", status: \", GpuGetErrorString(gpuprim_status));\n    }\n\n    return OkStatus();\n  }\n};\n\n}  // namespace functor\n\n#define DEFINE_INT64(T) \\\n  template struct functor::SparseFillEmptyRowsGrad<GPUDevice, T, int64>;\nTF_CALL_REAL_NUMBER_TYPES(DEFINE_INT64);\n#undef DEFINE_INT64\n\n}  // namespace tensorflow\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testSparseFillEmptyRowsGradEmpty(self):\n    with test_util.use_gpu():\n      grad, _ = self.evaluate(\n          sparse_ops.sparse_fill_empty_rows_grad(\n              reverse_index_map=[], grad_values=[]))\n      self.assertAllEqual(grad, [])\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float16, np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllCloseAccordingToType(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/sparse_fill_empty_rows_op_gpu.cu.cc", "tensorflow/python/kernel_tests/sparse_ops/sparse_ops_test.py"], "buggy_code_start_loc": [300, 516], "buggy_code_end_loc": [534, 516], "fixing_code_start_loc": [300, 517], "fixing_code_end_loc": [543, 524], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. If `SparseFillEmptyRowsGrad` is given empty inputs, TensorFlow will crash. We have patched the issue in GitHub commit af4a6a3c8b95022c351edae94560acc61253a1b8. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2022-41898", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-18T22:15:19.420", "lastModified": "2022-11-22T21:11:52.687", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. If `SparseFillEmptyRowsGrad` is given empty inputs, TensorFlow will crash. We have patched the issue in GitHub commit af4a6a3c8b95022c351edae94560acc61253a1b8. The fix will be included in TensorFlow 2.11. We will also cherrypick this commit on TensorFlow 2.10.1, 2.9.3, and TensorFlow 2.8.4, as these are also affected and still in supported range."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.8, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.8.4", "matchCriteriaId": "A694EEE1-BFB9-4E6C-B275-02DC2731961C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.3", "matchCriteriaId": "9057B403-719C-4F10-BAB6-67F84786A89E"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.10.0", "versionEndExcluding": "2.10.1", "matchCriteriaId": "793BC396-7686-47FA-A107-DA6FC90704A2"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_fill_empty_rows_op_gpu.cu.cc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/af4a6a3c8b95022c351edae94560acc61253a1b8", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-hq7g-wwwp-q46h", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/af4a6a3c8b95022c351edae94560acc61253a1b8"}}