{"buggy_code": ["// This file is part of Eigen, a lightweight C++ template library\n// for linear algebra.\n//\n// Copyright (C) 2015 Benoit Steiner <benoit.steiner.goog@gmail.com>\n//\n// This Source Code Form is subject to the terms of the Mozilla\n// Public License v. 2.0. If a copy of the MPL was not distributed\n// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n#ifndef CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n#define CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n\n#include <cmath>\n#include <iostream>\n\nnamespace Eigen {\n\n// The mantissa part of the fixed point representation. See\n// go/tensorfixedpoint for details\nstruct QInt8;\nstruct QUInt8;\nstruct QInt16;\nstruct QUInt16;\nstruct QInt32;\n\ntemplate <>\nstruct NumTraits<QInt8> : GenericNumTraits<int8_t> {};\ntemplate <>\nstruct NumTraits<QUInt8> : GenericNumTraits<uint8_t> {};\ntemplate <>\nstruct NumTraits<QInt16> : GenericNumTraits<int16_t> {};\ntemplate <>\nstruct NumTraits<QUInt16> : GenericNumTraits<uint16_t> {};\ntemplate <>\nstruct NumTraits<QInt32> : GenericNumTraits<int32_t> {};\n\nnamespace internal {\ntemplate <>\nstruct scalar_product_traits<QInt32, double> {\n  enum {\n    // Cost = NumTraits<T>::MulCost,\n    Defined = 1\n  };\n  typedef QInt32 ReturnType;\n};\n}\n\n// Wrap the 8bit int into a QInt8 struct instead of using a typedef to prevent\n// the compiler from silently type cast the mantissa into a bigger or a smaller\n// representation.\nstruct QInt8 {\n  QInt8() {}\n  QInt8(const int8_t v) : value(v) {}\n  QInt8(const QInt32 v);\n\n  operator int() const { return static_cast<int>(value); }\n\n  int8_t value;\n};\n\nstruct QUInt8 {\n  QUInt8() {}\n  QUInt8(const uint8_t v) : value(v) {}\n  QUInt8(const QInt32 v);\n\n  operator int() const { return static_cast<int>(value); }\n\n  uint8_t value;\n};\n\nstruct QInt16 {\n  QInt16() {}\n  QInt16(const int16_t v) : value(v) {}\n  QInt16(const QInt32 v);\n  operator int() const { return static_cast<int>(value); }\n\n  int16_t value;\n};\n\nstruct QUInt16 {\n  QUInt16() {}\n  QUInt16(const uint16_t v) : value(v) {}\n  QUInt16(const QInt32 v);\n  operator int() const { return static_cast<int>(value); }\n\n  uint16_t value;\n};\n\nstruct QInt32 {\n  QInt32() {}\n  QInt32(const int8_t v) : value(v) {}\n  QInt32(const int32_t v) : value(v) {}\n  QInt32(const uint32_t v) : value(static_cast<int32_t>(v)) {}\n  QInt32(const QInt8 v) : value(v.value) {}\n  QInt32(const float v) : value(static_cast<int32_t>(lrint(v))) {}\n#ifdef EIGEN_MAKING_DOCS\n  // Workaround to fix build on PPC.\n  QInt32(unsigned long v) : value(v) {}\n#endif\n\n  operator float() const { return static_cast<float>(value); }\n\n  int32_t value;\n};\n\nEIGEN_STRONG_INLINE QInt8::QInt8(const QInt32 v)\n    : value(v.value > 127 ? 127 : (v.value < -128 ? -128 : v.value)) {}\nEIGEN_STRONG_INLINE QUInt8::QUInt8(const QInt32 v)\n    : value(v.value > 255 ? 255 : (v.value < 0 ? 0 : v.value)) {}\nEIGEN_STRONG_INLINE QInt16::QInt16(const QInt32 v)\n    : value(v.value > 32767 ? 32767 : (v.value < -32768 ? -32768 : v.value)) {}\nEIGEN_STRONG_INLINE QUInt16::QUInt16(const QInt32 v)\n    : value(v.value > 65535 ? 65535 : (v.value < 0 ? 0 : v.value)) {}\n\n// Basic widening 8-bit operations: This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QUInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) - static_cast<int32_t>(b.value));\n}\n\n// Basic widening 16-bit operations: This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QUInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) - static_cast<int32_t>(b.value));\n}\n\n// Mixed QInt32 op QInt8 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QInt16 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QUInt8 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QUInt16 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Basic arithmetic operations on QInt32, which behaves like a int32_t.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt32 b) {\n  return a.value + b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt32 b) {\n  return a.value - b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt32 b) {\n  return a.value * b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator/(const QInt32 a, const QInt32 b) {\n  return a.value / b.value;\n}\nEIGEN_STRONG_INLINE QInt32& operator+=(QInt32& a, const QInt32 b) {\n  a.value += b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator-=(QInt32& a, const QInt32 b) {\n  a.value -= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator*=(QInt32& a, const QInt32 b) {\n  a.value *= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator/=(QInt32& a, const QInt32 b) {\n  a.value /= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a) { return -a.value; }\n\n// Scaling QInt32 by double. We do the arithmetic in double because\n// float only has 23 bits of mantissa, so casting QInt32 to float might reduce\n// accuracy by discarding up to 7 (least significant) bits.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const double b) {\n  return static_cast<int32_t>(lrint(static_cast<double>(a.value) * b));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const double a, const QInt32 b) {\n  return static_cast<int32_t>(lrint(a * static_cast<double>(b.value)));\n}\nEIGEN_STRONG_INLINE QInt32& operator*=(QInt32& a, const double b) {\n  a.value = static_cast<int32_t>(lrint(static_cast<double>(a.value) * b));\n  return a;\n}\n\n// Comparisons\nEIGEN_STRONG_INLINE bool operator==(const QInt8 a, const QInt8 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QUInt8 a, const QUInt8 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QInt16 a, const QInt16 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QUInt16 a, const QUInt16 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QInt32 a, const QInt32 b) {\n  return a.value == b.value;\n}\n\nEIGEN_STRONG_INLINE bool operator<(const QInt8 a, const QInt8 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QUInt8 a, const QUInt8 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QInt16 a, const QInt16 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QUInt16 a, const QUInt16 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QInt32 a, const QInt32 b) {\n  return a.value < b.value;\n}\n\nEIGEN_STRONG_INLINE bool operator>(const QInt8 a, const QInt8 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QUInt8 a, const QUInt8 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QInt16 a, const QInt16 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QUInt16 a, const QUInt16 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QInt32 a, const QInt32 b) {\n  return a.value > b.value;\n}\n\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt8 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QUInt8 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt16 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QUInt16 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt32 a) {\n  os << a.value;\n  return os;\n}\n\n}  // namespace Eigen\n\n#endif  // CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n"], "fixing_code": ["// This file is part of Eigen, a lightweight C++ template library\n// for linear algebra.\n//\n// Copyright (C) 2015 Benoit Steiner <benoit.steiner.goog@gmail.com>\n//\n// This Source Code Form is subject to the terms of the Mozilla\n// Public License v. 2.0. If a copy of the MPL was not distributed\n// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n#ifndef CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n#define CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n\n#include <cmath>\n#include <iostream>\n\nnamespace Eigen {\n\n// The mantissa part of the fixed point representation. See\n// go/tensorfixedpoint for details\nstruct QInt8;\nstruct QUInt8;\nstruct QInt16;\nstruct QUInt16;\nstruct QInt32;\n\ntemplate <>\nstruct NumTraits<QInt8> : GenericNumTraits<int8_t> {};\ntemplate <>\nstruct NumTraits<QUInt8> : GenericNumTraits<uint8_t> {};\ntemplate <>\nstruct NumTraits<QInt16> : GenericNumTraits<int16_t> {};\ntemplate <>\nstruct NumTraits<QUInt16> : GenericNumTraits<uint16_t> {};\ntemplate <>\nstruct NumTraits<QInt32> : GenericNumTraits<int32_t> {};\n\nnamespace internal {\ntemplate <>\nstruct scalar_product_traits<QInt32, double> {\n  enum {\n    // Cost = NumTraits<T>::MulCost,\n    Defined = 1\n  };\n  typedef QInt32 ReturnType;\n};\n}\n\n// Wrap the 8bit int into a QInt8 struct instead of using a typedef to prevent\n// the compiler from silently type cast the mantissa into a bigger or a smaller\n// representation.\nstruct QInt8 {\n  QInt8() : value(0) {}\n  QInt8(const int8_t v) : value(v) {}\n  QInt8(const QInt32 v);\n\n  operator int() const { return static_cast<int>(value); }\n\n  int8_t value;\n};\n\nstruct QUInt8 {\n  QUInt8() : value(0) {}\n  QUInt8(const uint8_t v) : value(v) {}\n  QUInt8(const QInt32 v);\n\n  operator int() const { return static_cast<int>(value); }\n\n  uint8_t value;\n};\n\nstruct QInt16 {\n  QInt16() : value(0) {}\n  QInt16(const int16_t v) : value(v) {}\n  QInt16(const QInt32 v);\n  operator int() const { return static_cast<int>(value); }\n\n  int16_t value;\n};\n\nstruct QUInt16 {\n  QUInt16() : value(0) {}\n  QUInt16(const uint16_t v) : value(v) {}\n  QUInt16(const QInt32 v);\n  operator int() const { return static_cast<int>(value); }\n\n  uint16_t value;\n};\n\nstruct QInt32 {\n  QInt32() : value(0) {}\n  QInt32(const int8_t v) : value(v) {}\n  QInt32(const int32_t v) : value(v) {}\n  QInt32(const uint32_t v) : value(static_cast<int32_t>(v)) {}\n  QInt32(const QInt8 v) : value(v.value) {}\n  QInt32(const float v) : value(static_cast<int32_t>(lrint(v))) {}\n#ifdef EIGEN_MAKING_DOCS\n  // Workaround to fix build on PPC.\n  QInt32(unsigned long v) : value(v) {}\n#endif\n\n  operator float() const { return static_cast<float>(value); }\n\n  int32_t value;\n};\n\nEIGEN_STRONG_INLINE QInt8::QInt8(const QInt32 v)\n    : value(v.value > 127 ? 127 : (v.value < -128 ? -128 : v.value)) {}\nEIGEN_STRONG_INLINE QUInt8::QUInt8(const QInt32 v)\n    : value(v.value > 255 ? 255 : (v.value < 0 ? 0 : v.value)) {}\nEIGEN_STRONG_INLINE QInt16::QInt16(const QInt32 v)\n    : value(v.value > 32767 ? 32767 : (v.value < -32768 ? -32768 : v.value)) {}\nEIGEN_STRONG_INLINE QUInt16::QUInt16(const QInt32 v)\n    : value(v.value > 65535 ? 65535 : (v.value < 0 ? 0 : v.value)) {}\n\n// Basic widening 8-bit operations: This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QUInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt8 a, const QInt8 b) {\n  return QInt32(static_cast<int32_t>(a.value) - static_cast<int32_t>(b.value));\n}\n\n// Basic widening 16-bit operations: This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QUInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt16 a, const QInt16 b) {\n  return QInt32(static_cast<int32_t>(a.value) - static_cast<int32_t>(b.value));\n}\n\n// Mixed QInt32 op QInt8 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt8 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QInt16 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt16 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QUInt8 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QUInt8 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QUInt8 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Mixed QInt32 op QUInt16 operations. This will be vectorized in future CLs.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value + static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator+(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) + b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value - static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) - b.value);\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QUInt16 b) {\n  return QInt32(a.value * static_cast<int32_t>(b.value));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QUInt16 a, const QInt32 b) {\n  return QInt32(static_cast<int32_t>(a.value) * b.value);\n}\n\n// Basic arithmetic operations on QInt32, which behaves like a int32_t.\nEIGEN_STRONG_INLINE QInt32 operator+(const QInt32 a, const QInt32 b) {\n  return a.value + b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a, const QInt32 b) {\n  return a.value - b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const QInt32 b) {\n  return a.value * b.value;\n}\nEIGEN_STRONG_INLINE QInt32 operator/(const QInt32 a, const QInt32 b) {\n  return a.value / b.value;\n}\nEIGEN_STRONG_INLINE QInt32& operator+=(QInt32& a, const QInt32 b) {\n  a.value += b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator-=(QInt32& a, const QInt32 b) {\n  a.value -= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator*=(QInt32& a, const QInt32 b) {\n  a.value *= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32& operator/=(QInt32& a, const QInt32 b) {\n  a.value /= b.value;\n  return a;\n}\nEIGEN_STRONG_INLINE QInt32 operator-(const QInt32 a) { return -a.value; }\n\n// Scaling QInt32 by double. We do the arithmetic in double because\n// float only has 23 bits of mantissa, so casting QInt32 to float might reduce\n// accuracy by discarding up to 7 (least significant) bits.\nEIGEN_STRONG_INLINE QInt32 operator*(const QInt32 a, const double b) {\n  return static_cast<int32_t>(lrint(static_cast<double>(a.value) * b));\n}\nEIGEN_STRONG_INLINE QInt32 operator*(const double a, const QInt32 b) {\n  return static_cast<int32_t>(lrint(a * static_cast<double>(b.value)));\n}\nEIGEN_STRONG_INLINE QInt32& operator*=(QInt32& a, const double b) {\n  a.value = static_cast<int32_t>(lrint(static_cast<double>(a.value) * b));\n  return a;\n}\n\n// Comparisons\nEIGEN_STRONG_INLINE bool operator==(const QInt8 a, const QInt8 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QUInt8 a, const QUInt8 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QInt16 a, const QInt16 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QUInt16 a, const QUInt16 b) {\n  return a.value == b.value;\n}\nEIGEN_STRONG_INLINE bool operator==(const QInt32 a, const QInt32 b) {\n  return a.value == b.value;\n}\n\nEIGEN_STRONG_INLINE bool operator<(const QInt8 a, const QInt8 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QUInt8 a, const QUInt8 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QInt16 a, const QInt16 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QUInt16 a, const QUInt16 b) {\n  return a.value < b.value;\n}\nEIGEN_STRONG_INLINE bool operator<(const QInt32 a, const QInt32 b) {\n  return a.value < b.value;\n}\n\nEIGEN_STRONG_INLINE bool operator>(const QInt8 a, const QInt8 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QUInt8 a, const QUInt8 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QInt16 a, const QInt16 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QUInt16 a, const QUInt16 b) {\n  return a.value > b.value;\n}\nEIGEN_STRONG_INLINE bool operator>(const QInt32 a, const QInt32 b) {\n  return a.value > b.value;\n}\n\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt8 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QUInt8 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt16 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QUInt16 a) {\n  os << static_cast<int>(a.value);\n  return os;\n}\nEIGEN_STRONG_INLINE std::ostream& operator<<(std::ostream& os, QInt32 a) {\n  os << a.value;\n  return os;\n}\n\n}  // namespace Eigen\n\n#endif  // CXX11_SRC_FIXEDPOINT_FIXEDPOINTTYPES_H_\n"], "filenames": ["third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/FixedPointTypes.h"], "buggy_code_start_loc": [52], "buggy_code_end_loc": [91], "fixing_code_start_loc": [52], "fixing_code_end_loc": [91], "type": "CWE-908", "message": "In affected versions of TensorFlow under certain cases a saved model can trigger use of uninitialized values during code execution. This is caused by having tensor buffers be filled with the default value of the type but forgetting to default initialize the quantized floating point types in Eigen. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0.", "other": {"cve": {"id": "CVE-2020-26266", "sourceIdentifier": "security-advisories@github.com", "published": "2020-12-10T23:15:12.647", "lastModified": "2020-12-14T17:54:09.583", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In affected versions of TensorFlow under certain cases a saved model can trigger use of uninitialized values during code execution. This is caused by having tensor buffers be filled with the default value of the type but forgetting to default initialize the quantized floating point types in Eigen. This is fixed in versions 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2, and 2.4.0."}, {"lang": "es", "value": "En las versiones afectadas de TensorFlow, en determinados casos, un modelo guardado puede activar el uso de valores no inicializados durante la ejecuci\u00f3n del c\u00f3digo.&#xa0;Esto es debido a que los b\u00faferes de tensor se llenan con el valor predeterminado del tipo, pero se olvidan de inicializar por defecto los tipos de punto flotante cuantificados en Eigen.&#xa0;Esto es corregido en las versiones 1.15.5, 2.0.4, 2.1.3, 2.2.2, 2.3.2 y 2.4.0."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:L/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 5.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:L", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 4.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 2.5}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-908"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-908"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.15.5", "matchCriteriaId": "CA3A54AC-E0F8-4741-8A80-04EEF746B14B"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.0.0", "versionEndExcluding": "2.0.4", "matchCriteriaId": "989E4548-7823-436F-A9FE-04158ED41C48"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.1.0", "versionEndExcluding": "2.1.3", "matchCriteriaId": "46417CA8-E666-4E12-B2A8-BB0E97D49BF4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.2.0", "versionEndExcluding": "2.2.2", "matchCriteriaId": "57B24744-0D81-41E9-9ED0-7296368DEF00"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.2", "matchCriteriaId": "DBEA56AF-3495-4883-9721-0FA9F08E7F6D"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/ace0c15a22f7f054abcc1f53eabbcb0a1239a9e2", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-qhxx-j73r-qpm2", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/ace0c15a22f7f054abcc1f53eabbcb0a1239a9e2"}}