{"buggy_code": ["// Copyright (c) 2013-2016 Sandstorm Development Group, Inc. and contributors\n// Licensed under the MIT License:\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n#define CAPNP_PRIVATE\n#include \"layout.h\"\n#include <kj/debug.h>\n#include \"arena.h\"\n#include <string.h>\n#include <stdlib.h>\n\n#if !CAPNP_LITE\n#include \"capability.h\"\n#endif  // !CAPNP_LITE\n\nnamespace capnp {\nnamespace _ {  // private\n\n#if !CAPNP_LITE\nstatic BrokenCapFactory* globalBrokenCapFactory = nullptr;\n// Horrible hack:  We need to be able to construct broken caps without any capability context,\n// but we can't have a link-time dependency on libcapnp-rpc.\n\nvoid setGlobalBrokenCapFactoryForLayoutCpp(BrokenCapFactory& factory) {\n  // Called from capability.c++ when the capability API is used, to make sure that layout.c++\n  // is ready for it.  May be called multiple times but always with the same value.\n#if __GNUC__ || defined(__clang__)\n  __atomic_store_n(&globalBrokenCapFactory, &factory, __ATOMIC_RELAXED);\n#elif _MSC_VER\n  *static_cast<BrokenCapFactory* volatile*>(&globalBrokenCapFactory) = &factory;\n#else\n#error \"Platform not supported\"\n#endif\n}\n\nstatic BrokenCapFactory* readGlobalBrokenCapFactoryForLayoutCpp() {\n#if __GNUC__ || defined(__clang__)\n  // Thread-sanitizer doesn't have the right information to know this is safe without doing an\n  // atomic read. https://groups.google.com/g/capnproto/c/634juhn5ap0/m/pyRiwWl1AAAJ\n  return __atomic_load_n(&globalBrokenCapFactory, __ATOMIC_RELAXED);\n#else\n  return globalBrokenCapFactory;\n#endif\n}\n\n}  // namespace _ (private)\n\nconst uint ClientHook::NULL_CAPABILITY_BRAND = 0;\nconst uint ClientHook::BROKEN_CAPABILITY_BRAND = 0;\n// Defined here rather than capability.c++ so that we can safely call isNull() in this file.\n\nnamespace _ {  // private\n\n#endif  // !CAPNP_LITE\n\n#if CAPNP_DEBUG_TYPES\n#define G(n) bounded<n>()\n#else\n#define G(n) n\n#endif\n\n// =======================================================================================\n\n#if __GNUC__ >= 8 && !__clang__\n// GCC 8 introduced a warning which complains whenever we try to memset() or memcpy() a\n// WirePointer, because we deleted the regular copy constructor / assignment operator. Weirdly, if\n// I remove those deletions, GCC *still* complains that WirePointer is non-trivial. I don't\n// understand why -- maybe because WireValue has private members? We don't want to make WireValue's\n// member public, but memset() and memcpy() on it are certainly valid and desirable, so we'll just\n// have to disable the warning I guess.\n#pragma GCC diagnostic ignored \"-Wclass-memaccess\"\n#endif\n\nstruct WirePointer {\n  // A pointer, in exactly the format in which it appears on the wire.\n\n  // Copying and moving is not allowed because the offset would become wrong.\n  WirePointer(const WirePointer& other) = delete;\n  WirePointer(WirePointer&& other) = delete;\n  WirePointer& operator=(const WirePointer& other) = delete;\n  WirePointer& operator=(WirePointer&& other) = delete;\n\n  // -----------------------------------------------------------------\n  // Common part of all pointers:  kind + offset\n  //\n  // Actually this is not terribly common.  The \"offset\" could actually be different things\n  // depending on the context:\n  // - For a regular (e.g. struct/list) pointer, a signed word offset from the word immediately\n  //   following the pointer pointer.  (The off-by-one means the offset is more often zero, saving\n  //   bytes on the wire when packed.)\n  // - For an inline composite list tag (not really a pointer, but structured similarly), an\n  //   element count.\n  // - For a FAR pointer, an unsigned offset into the target segment.\n  // - For a FAR landing pad, zero indicates that the target value immediately follows the pad while\n  //   1 indicates that the pad is followed by another FAR pointer that actually points at the\n  //   value.\n\n  enum Kind {\n    STRUCT = 0,\n    // Reference points at / describes a struct.\n\n    LIST = 1,\n    // Reference points at / describes a list.\n\n    FAR = 2,\n    // Reference is a \"far pointer\", which points at data located in a different segment.  The\n    // eventual target is one of the other kinds.\n\n    OTHER = 3\n    // Reference has type \"other\".  If the next 30 bits are all zero (i.e. the lower 32 bits contain\n    // only the kind OTHER) then the pointer is a capability.  All other values are reserved.\n  };\n\n  WireValue<uint32_t> offsetAndKind;\n\n  KJ_ALWAYS_INLINE(Kind kind() const) {\n    return static_cast<Kind>(offsetAndKind.get() & 3);\n  }\n  KJ_ALWAYS_INLINE(bool isPositional() const) {\n    return (offsetAndKind.get() & 2) == 0;  // match STRUCT and LIST but not FAR or OTHER\n  }\n  KJ_ALWAYS_INLINE(bool isCapability() const) {\n    return offsetAndKind.get() == OTHER;\n  }\n\n  KJ_ALWAYS_INLINE(word* target()) {\n    return reinterpret_cast<word*>(this) + 1 + (static_cast<int32_t>(offsetAndKind.get()) >> 2);\n  }\n  KJ_ALWAYS_INLINE(const word* target(SegmentReader* segment) const) {\n    if (segment == nullptr) {\n      return reinterpret_cast<const word*>(this + 1) +\n          (static_cast<int32_t>(offsetAndKind.get()) >> 2);\n    } else {\n      return segment->checkOffset(reinterpret_cast<const word*>(this + 1),\n                                  static_cast<int32_t>(offsetAndKind.get()) >> 2);\n    }\n  }\n  KJ_ALWAYS_INLINE(void setKindAndTarget(Kind kind, word* target, SegmentBuilder* segment)) {\n    // Check that the target is really in the same segment, otherwise subtracting pointers is\n    // undefined behavior.  As it turns out, it's undefined behavior that actually produces\n    // unexpected results in a real-world situation that actually happened:  At one time,\n    // OrphanBuilder's \"tag\" (a WirePointer) was allowed to be initialized as if it lived in\n    // a particular segment when in fact it does not.  On 32-bit systems, where words might\n    // only be 32-bit aligned, it's possible that the difference between `this` and `target` is\n    // not a whole number of words.  But clang optimizes:\n    //     (target - (word*)this - 1) << 2\n    // to:\n    //     (((ptrdiff_t)target - (ptrdiff_t)this - 8) >> 1)\n    // So now when the pointers are not aligned the same, we can end up corrupting the bottom\n    // two bits, where `kind` is stored.  For example, this turns a struct into a far pointer.\n    // Ouch!\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(this) >=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(this) <\n                reinterpret_cast<uintptr_t>(segment->getStartPtr() + segment->getSize()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(target) >=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(target) <=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr() + segment->getSize()));\n    offsetAndKind.set((static_cast<uint32_t>(target - reinterpret_cast<word*>(this) - 1) << 2) | kind);\n  }\n  KJ_ALWAYS_INLINE(void setKindWithZeroOffset(Kind kind)) {\n    offsetAndKind.set(kind);\n  }\n  KJ_ALWAYS_INLINE(void setKindAndTargetForEmptyStruct()) {\n    // This pointer points at an empty struct.  Assuming the WirePointer itself is in-bounds, we\n    // can set the target to point either at the WirePointer itself or immediately after it.  The\n    // latter would cause the WirePointer to be \"null\" (since for an empty struct the upper 32\n    // bits are going to be zero).  So we set an offset of -1, as if the struct were allocated\n    // immediately before this pointer, to distinguish it from null.\n    offsetAndKind.set(0xfffffffc);\n  }\n  KJ_ALWAYS_INLINE(void setKindForOrphan(Kind kind)) {\n    // OrphanBuilder contains a WirePointer, but since it isn't located in a segment, it should\n    // not have a valid offset (unless it is a FAR or OTHER pointer).  We set its offset to -1\n    // because setting it to zero would mean a pointer to an empty struct would appear to be a null\n    // pointer.\n    KJ_DREQUIRE(isPositional());\n    offsetAndKind.set(kind | 0xfffffffc);\n  }\n\n  KJ_ALWAYS_INLINE(ListElementCount inlineCompositeListElementCount() const) {\n    return ((bounded(offsetAndKind.get()) >> G(2))\n            & G(kj::maxValueForBits<LIST_ELEMENT_COUNT_BITS>())) * ELEMENTS;\n  }\n  KJ_ALWAYS_INLINE(void setKindAndInlineCompositeListElementCount(\n      Kind kind, ListElementCount elementCount)) {\n    offsetAndKind.set(unboundAs<uint32_t>((elementCount / ELEMENTS) << G(2)) | kind);\n  }\n\n  KJ_ALWAYS_INLINE(const word* farTarget(SegmentReader* segment) const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"farTarget() should only be called on FAR pointers.\");\n    return segment->checkOffset(segment->getStartPtr(), offsetAndKind.get() >> 3);\n  }\n  KJ_ALWAYS_INLINE(word* farTarget(SegmentBuilder* segment) const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"farTarget() should only be called on FAR pointers.\");\n    return segment->getPtrUnchecked((bounded(offsetAndKind.get()) >> G(3)) * WORDS);\n  }\n  KJ_ALWAYS_INLINE(bool isDoubleFar() const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"isDoubleFar() should only be called on FAR pointers.\");\n    return (offsetAndKind.get() >> 2) & 1;\n  }\n  KJ_ALWAYS_INLINE(void setFar(bool isDoubleFar, WordCountN<29> pos)) {\n    offsetAndKind.set(unboundAs<uint32_t>((pos / WORDS) << G(3)) |\n                      (static_cast<uint32_t>(isDoubleFar) << 2) |\n                      static_cast<uint32_t>(Kind::FAR));\n  }\n  KJ_ALWAYS_INLINE(void setCap(uint index)) {\n    offsetAndKind.set(static_cast<uint32_t>(Kind::OTHER));\n    capRef.index.set(index);\n  }\n\n  // -----------------------------------------------------------------\n  // Part of pointer that depends on the kind.\n\n  // Note:  Originally StructRef, ListRef, and FarRef were unnamed types, but this somehow\n  //   tickled a bug in GCC:\n  //     http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58192\n  struct StructRef {\n    WireValue<WordCount16> dataSize;\n    WireValue<WirePointerCount16> ptrCount;\n\n    inline WordCountN<17> wordSize() const {\n      return upgradeBound<uint32_t>(dataSize.get()) + ptrCount.get() * WORDS_PER_POINTER;\n    }\n\n    KJ_ALWAYS_INLINE(void set(WordCount16 ds, WirePointerCount16 rc)) {\n      dataSize.set(ds);\n      ptrCount.set(rc);\n    }\n    KJ_ALWAYS_INLINE(void set(StructSize size)) {\n      dataSize.set(size.data);\n      ptrCount.set(size.pointers);\n    }\n  };\n\n  struct ListRef {\n    WireValue<uint32_t> elementSizeAndCount;\n\n    KJ_ALWAYS_INLINE(ElementSize elementSize() const) {\n      return static_cast<ElementSize>(elementSizeAndCount.get() & 7);\n    }\n    KJ_ALWAYS_INLINE(ElementCountN<29> elementCount() const) {\n      return (bounded(elementSizeAndCount.get()) >> G(3)) * ELEMENTS;\n    }\n    KJ_ALWAYS_INLINE(WordCountN<29> inlineCompositeWordCount() const) {\n      return elementCount() * (ONE * WORDS / ELEMENTS);\n    }\n\n    KJ_ALWAYS_INLINE(void set(ElementSize es, ElementCountN<29> ec)) {\n      elementSizeAndCount.set(unboundAs<uint32_t>((ec / ELEMENTS) << G(3)) |\n                              static_cast<int>(es));\n    }\n\n    KJ_ALWAYS_INLINE(void setInlineComposite(WordCountN<29> wc)) {\n      elementSizeAndCount.set(unboundAs<uint32_t>((wc / WORDS) << G(3)) |\n                              static_cast<int>(ElementSize::INLINE_COMPOSITE));\n    }\n  };\n\n  struct FarRef {\n    WireValue<SegmentId> segmentId;\n\n    KJ_ALWAYS_INLINE(void set(SegmentId si)) {\n      segmentId.set(si);\n    }\n  };\n\n  struct CapRef {\n    WireValue<uint32_t> index;\n    // Index into the message's capability table.\n  };\n\n  union {\n    uint32_t upper32Bits;\n\n    StructRef structRef;\n\n    ListRef listRef;\n\n    FarRef farRef;\n\n    CapRef capRef;\n  };\n\n  KJ_ALWAYS_INLINE(bool isNull() const) {\n    // If the upper 32 bits are zero, this is a pointer to an empty struct.  We consider that to be\n    // our \"null\" value.\n    return (offsetAndKind.get() == 0) & (upper32Bits == 0);\n  }\n\n};\nstatic_assert(sizeof(WirePointer) == sizeof(word),\n    \"capnp::WirePointer is not exactly one word.  This will probably break everything.\");\nstatic_assert(unboundAs<size_t>(POINTERS * WORDS_PER_POINTER * BYTES_PER_WORD / BYTES) ==\n              sizeof(WirePointer),\n    \"WORDS_PER_POINTER is wrong.\");\nstatic_assert(unboundAs<size_t>(POINTERS * BYTES_PER_POINTER / BYTES) == sizeof(WirePointer),\n    \"BYTES_PER_POINTER is wrong.\");\nstatic_assert(unboundAs<size_t>(POINTERS * BITS_PER_POINTER / BITS_PER_BYTE / BYTES) ==\n              sizeof(WirePointer),\n    \"BITS_PER_POINTER is wrong.\");\n\n#define OUT_OF_BOUNDS_ERROR_DETAIL \\\n    \"This usually indicates that \" \\\n    \"the input data was corrupted, used a different encoding than specified (e.g. \" \\\n    \"packed vs. non-packed), or was not a Cap'n Proto message to begin with. Note \" \\\n    \"that this error is NOT due to a schema mismatch; the input is invalid \" \\\n    \"regardless of schema.\"\n\nnamespace {\n\nstatic const union {\n  AlignedData<unbound(POINTER_SIZE_IN_WORDS / WORDS)> word;\n  WirePointer pointer;\n} zero = {{{0}}};\n\n}  // namespace\n\n// =======================================================================================\n\nnamespace {\n\ntemplate <typename T>\nstruct SegmentAnd {\n  SegmentBuilder* segment;\n  T value;\n};\n\n}  // namespace\n\nstruct WireHelpers {\n#if CAPNP_DEBUG_TYPES\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 7) / 8, T>, word> roundBytesUpToWords(\n          kj::Quantity<kj::Bounded<maxN, T>, byte> bytes)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bytes + G(7) * BYTES) / BYTES_PER_WORD;\n  }\n\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 7) / 8, T>, byte> roundBitsUpToBytes(\n          kj::Quantity<kj::Bounded<maxN, T>, BitLabel> bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 63) / 64, T>, word> roundBitsUpToWords(\n          kj::Quantity<kj::Bounded<maxN, T>, BitLabel> bits)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bits + G(63) * BITS) / BITS_PER_WORD;\n  }\n#else\n  static KJ_ALWAYS_INLINE(WordCount roundBytesUpToWords(ByteCount bytes)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bytes + G(7) * BYTES) / BYTES_PER_WORD;\n  }\n\n  static KJ_ALWAYS_INLINE(ByteCount roundBitsUpToBytes(BitCount bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n\n  static KJ_ALWAYS_INLINE(WordCount64 roundBitsUpToWords(BitCount64 bits)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bits + G(63) * BITS) / BITS_PER_WORD;\n  }\n\n  static KJ_ALWAYS_INLINE(ByteCount64 roundBitsUpToBytes(BitCount64 bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n#endif\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(byte* ptr, ByteCount32 count)) {\n    if (count != ZERO * BYTES) memset(ptr, 0, unbound(count / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(word* ptr, WordCountN<29> count)) {\n    if (count != ZERO * WORDS) memset(ptr, 0, unbound(count * BYTES_PER_WORD / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(WirePointer* ptr, WirePointerCountN<29> count)) {\n    if (count != ZERO * POINTERS) memset(ptr, 0, unbound(count * BYTES_PER_POINTER / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(WirePointer* ptr)) {\n    memset(ptr, 0, sizeof(*ptr));\n  }\n\n  template <typename T>\n  static inline void zeroMemory(kj::ArrayPtr<T> array) {\n    if (array.size() != 0u) memset(array.begin(), 0, array.size() * sizeof(array[0]));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(byte* to, const byte* from, ByteCount32 count)) {\n    if (count != ZERO * BYTES) memcpy(to, from, unbound(count / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(word* to, const word* from, WordCountN<29> count)) {\n    if (count != ZERO * WORDS) memcpy(to, from, unbound(count * BYTES_PER_WORD / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(WirePointer* to, const WirePointer* from,\n                                          WirePointerCountN<29> count)) {\n    if (count != ZERO * POINTERS) memcpy(to, from, unbound(count * BYTES_PER_POINTER  / BYTES));\n  }\n\n  template <typename T>\n  static inline void copyMemory(T* to, const T* from) {\n    memcpy(to, from, sizeof(*from));\n  }\n\n  // TODO(cleanup): Turn these into a .copyTo() method of ArrayPtr?\n  template <typename T>\n  static inline void copyMemory(T* to, kj::ArrayPtr<T> from) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n  template <typename T>\n  static inline void copyMemory(T* to, kj::ArrayPtr<const T> from) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n  static KJ_ALWAYS_INLINE(void copyMemory(char* to, kj::StringPtr from)) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n\n  static KJ_ALWAYS_INLINE(bool boundsCheck(\n      SegmentReader* segment, const word* start, WordCountN<31> size)) {\n    // If segment is null, this is an unchecked message, so we don't do bounds checks.\n    return segment == nullptr || segment->checkObject(start, size);\n  }\n\n  static KJ_ALWAYS_INLINE(bool amplifiedRead(SegmentReader* segment, WordCount virtualAmount)) {\n    // If segment is null, this is an unchecked message, so we don't do read limiter checks.\n    return segment == nullptr || segment->amplifiedRead(virtualAmount);\n  }\n\n  static KJ_ALWAYS_INLINE(word* allocate(\n      WirePointer*& ref, SegmentBuilder*& segment, CapTableBuilder* capTable,\n      SegmentWordCount amount, WirePointer::Kind kind, BuilderArena* orphanArena)) {\n    // Allocate space in the message for a new object, creating far pointers if necessary. The\n    // space is guaranteed to be zero'd (because MessageBuilder implementations are required to\n    // return zero'd memory).\n    //\n    // * `ref` starts out being a reference to the pointer which shall be assigned to point at the\n    //   new object.  On return, `ref` points to a pointer which needs to be initialized with\n    //   the object's type information.  Normally this is the same pointer, but it can change if\n    //   a far pointer was allocated -- in this case, `ref` will end up pointing to the far\n    //   pointer's tag.  Either way, `allocate()` takes care of making sure that the original\n    //   pointer ends up leading to the new object.  On return, only the upper 32 bit of `*ref`\n    //   need to be filled in by the caller.\n    // * `segment` starts out pointing to the segment containing `ref`.  On return, it points to\n    //   the segment containing the allocated object, which is usually the same segment but could\n    //   be a different one if the original segment was out of space.\n    // * `amount` is the number of words to allocate.\n    // * `kind` is the kind of object to allocate.  It is used to initialize the pointer.  It\n    //   cannot be `FAR` -- far pointers are allocated automatically as needed.\n    // * `orphanArena` is usually null.  If it is non-null, then we're allocating an orphan object.\n    //   In this case, `segment` starts out null; the allocation takes place in an arbitrary\n    //   segment belonging to the arena.  `ref` will be initialized as a non-far pointer, but its\n    //   target offset will be set to zero.\n\n    if (orphanArena == nullptr) {\n      if (!ref->isNull()) zeroObject(segment, capTable, ref);\n\n      if (amount == ZERO * WORDS && kind == WirePointer::STRUCT) {\n        // Note that the check for kind == WirePointer::STRUCT will hopefully cause this whole\n        // branch to be optimized away from all the call sites that are allocating non-structs.\n        ref->setKindAndTargetForEmptyStruct();\n        return reinterpret_cast<word*>(ref);\n      }\n\n      KJ_ASSUME(segment != nullptr);\n      word* ptr = segment->allocate(amount);\n\n      if (ptr == nullptr) {\n\n        // Need to allocate in a new segment.  We'll need to allocate an extra pointer worth of\n        // space to act as the landing pad for a far pointer.\n\n        WordCount amountPlusRef = amount + POINTER_SIZE_IN_WORDS;\n        auto allocation = segment->getArena()->allocate(\n            assertMaxBits<SEGMENT_WORD_COUNT_BITS>(amountPlusRef, []() {\n              KJ_FAIL_REQUIRE(\"requested object size exceeds maximum segment size\");\n            }));\n        segment = allocation.segment;\n        ptr = allocation.words;\n\n        // Set up the original pointer to be a far pointer to the new segment.\n        ref->setFar(false, segment->getOffsetTo(ptr));\n        ref->farRef.set(segment->getSegmentId());\n\n        // Initialize the landing pad to indicate that the data immediately follows the pad.\n        ref = reinterpret_cast<WirePointer*>(ptr);\n        ref->setKindAndTarget(kind, ptr + POINTER_SIZE_IN_WORDS, segment);\n\n        // Allocated space follows new pointer.\n        return ptr + POINTER_SIZE_IN_WORDS;\n      } else {\n        ref->setKindAndTarget(kind, ptr, segment);\n        return ptr;\n      }\n    } else {\n      // orphanArena is non-null.  Allocate an orphan.\n      KJ_DASSERT(ref->isNull());\n      auto allocation = orphanArena->allocate(amount);\n      segment = allocation.segment;\n      ref->setKindForOrphan(kind);\n      return allocation.words;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(word* followFarsNoWritableCheck(\n      WirePointer*& ref, word* refTarget, SegmentBuilder*& segment)) {\n    // If `ref` is a far pointer, follow it.  On return, `ref` will have been updated to point at\n    // a WirePointer that contains the type information about the target object, and a pointer to\n    // the object contents is returned.  The caller must NOT use `ref->target()` as this may or may\n    // not actually return a valid pointer.  `segment` is also updated to point at the segment which\n    // actually contains the object.\n    //\n    // If `ref` is not a far pointer, this simply returns `refTarget`.  Usually, `refTarget` should\n    // be the same as `ref->target()`, but may not be in cases where `ref` is only a tag.\n\n    if (ref->kind() == WirePointer::FAR) {\n      segment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n      WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(segment));\n      if (!ref->isDoubleFar()) {\n        ref = pad;\n        return pad->target();\n      }\n\n      // Landing pad is another far pointer.  It is followed by a tag describing the pointed-to\n      // object.\n      ref = pad + 1;\n\n      segment = segment->getArena()->getSegment(pad->farRef.segmentId.get());\n      return pad->farTarget(segment);\n    } else {\n      return refTarget;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(word* followFars(\n      WirePointer*& ref, word* refTarget, SegmentBuilder*& segment)) {\n    auto result = followFarsNoWritableCheck(ref, refTarget, segment);\n    segment->checkWritable();\n    return result;\n  }\n\n  static KJ_ALWAYS_INLINE(kj::Maybe<const word&> followFars(\n      const WirePointer*& ref, const word* refTarget, SegmentReader*& segment))\n      KJ_WARN_UNUSED_RESULT {\n    // Like the other followFars() but operates on readers.\n\n    // If the segment is null, this is an unchecked message, so there are no FAR pointers.\n    if (segment != nullptr && ref->kind() == WirePointer::FAR) {\n      // Look up the segment containing the landing pad.\n      segment = segment->getArena()->tryGetSegment(ref->farRef.segmentId.get());\n      KJ_REQUIRE(segment != nullptr, \"Message contains far pointer to unknown segment.\") {\n        return nullptr;\n      }\n\n      // Find the landing pad and check that it is within bounds.\n      const word* ptr = ref->farTarget(segment);\n      auto padWords = (ONE + bounded(ref->isDoubleFar())) * POINTER_SIZE_IN_WORDS;\n      KJ_REQUIRE(boundsCheck(segment, ptr, padWords),\n                 \"Message contains out-of-bounds far pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        return nullptr;\n      }\n\n      const WirePointer* pad = reinterpret_cast<const WirePointer*>(ptr);\n\n      // If this is not a double-far then the landing pad is our final pointer.\n      if (!ref->isDoubleFar()) {\n        ref = pad;\n        return pad->target(segment);\n      }\n\n      // Landing pad is another far pointer.  It is followed by a tag describing the pointed-to\n      // object.\n      ref = pad + 1;\n\n      SegmentReader* newSegment = segment->getArena()->tryGetSegment(pad->farRef.segmentId.get());\n      KJ_REQUIRE(newSegment != nullptr,\n          \"Message contains double-far pointer to unknown segment.\") {\n        return nullptr;\n      }\n      KJ_REQUIRE(pad->kind() == WirePointer::FAR,\n          \"Second word of double-far pad must be far pointer.\") {\n        return nullptr;\n      }\n\n      segment = newSegment;\n      return pad->farTarget(segment);\n    } else {\n      KJ_DASSERT(refTarget != nullptr);\n      return refTarget;\n    }\n  }\n\n  // -----------------------------------------------------------------\n\n  static void zeroObject(SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref) {\n    // Zero out the pointed-to object.  Use when the pointer is about to be overwritten making the\n    // target object no longer reachable.\n\n    // We shouldn't zero out external data linked into the message.\n    if (!segment->isWritable()) return;\n\n    switch (ref->kind()) {\n      case WirePointer::STRUCT:\n      case WirePointer::LIST:\n        zeroObject(segment, capTable, ref, ref->target());\n        break;\n      case WirePointer::FAR: {\n        segment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n        if (segment->isWritable()) {  // Don't zero external data.\n          WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(segment));\n\n          if (ref->isDoubleFar()) {\n            segment = segment->getArena()->getSegment(pad->farRef.segmentId.get());\n            if (segment->isWritable()) {\n              zeroObject(segment, capTable, pad + 1, pad->farTarget(segment));\n            }\n            zeroMemory(pad, G(2) * POINTERS);\n          } else {\n            zeroObject(segment, capTable, pad);\n            zeroMemory(pad);\n          }\n        }\n        break;\n      }\n      case WirePointer::OTHER:\n        if (ref->isCapability()) {\n#if CAPNP_LITE\n          KJ_FAIL_ASSERT(\"Capability encountered in builder in lite mode?\") { break; }\n#else  // CAPNP_LINE\n          capTable->dropCap(ref->capRef.index.get());\n#endif  // CAPNP_LITE, else\n        } else {\n          KJ_FAIL_REQUIRE(\"Unknown pointer type.\") { break; }\n        }\n        break;\n    }\n  }\n\n  static void zeroObject(SegmentBuilder* segment, CapTableBuilder* capTable,\n                         WirePointer* tag, word* ptr) {\n    // We shouldn't zero out external data linked into the message.\n    if (!segment->isWritable()) return;\n\n    switch (tag->kind()) {\n      case WirePointer::STRUCT: {\n        WirePointer* pointerSection =\n            reinterpret_cast<WirePointer*>(ptr + tag->structRef.dataSize.get());\n        for (auto i: kj::zeroTo(tag->structRef.ptrCount.get())) {\n          zeroObject(segment, capTable, pointerSection + i);\n        }\n        zeroMemory(ptr, tag->structRef.wordSize());\n        break;\n      }\n      case WirePointer::LIST: {\n        switch (tag->listRef.elementSize()) {\n          case ElementSize::VOID:\n            // Nothing.\n            break;\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            zeroMemory(ptr, roundBitsUpToWords(\n                upgradeBound<uint64_t>(tag->listRef.elementCount()) *\n                dataBitsPerElement(tag->listRef.elementSize())));\n            break;\n          }\n          case ElementSize::POINTER: {\n            WirePointer* typedPtr = reinterpret_cast<WirePointer*>(ptr);\n            auto count = tag->listRef.elementCount() * (ONE * POINTERS / ELEMENTS);\n            for (auto i: kj::zeroTo(count)) {\n              zeroObject(segment, capTable, typedPtr + i);\n            }\n            zeroMemory(typedPtr, count);\n            break;\n          }\n          case ElementSize::INLINE_COMPOSITE: {\n            WirePointer* elementTag = reinterpret_cast<WirePointer*>(ptr);\n\n            KJ_ASSERT(elementTag->kind() == WirePointer::STRUCT,\n                  \"Don't know how to handle non-STRUCT inline composite.\");\n            WordCount dataSize = elementTag->structRef.dataSize.get();\n            WirePointerCount pointerCount = elementTag->structRef.ptrCount.get();\n\n            auto count = elementTag->inlineCompositeListElementCount();\n            if (pointerCount > ZERO * POINTERS) {\n              word* pos = ptr + POINTER_SIZE_IN_WORDS;\n              for (auto i KJ_UNUSED: kj::zeroTo(count)) {\n                pos += dataSize;\n\n                for (auto j KJ_UNUSED: kj::zeroTo(pointerCount)) {\n                  zeroObject(segment, capTable, reinterpret_cast<WirePointer*>(pos));\n                  pos += POINTER_SIZE_IN_WORDS;\n                }\n              }\n            }\n\n            auto wordsPerElement = elementTag->structRef.wordSize() / ELEMENTS;\n            zeroMemory(ptr, assertMaxBits<SEGMENT_WORD_COUNT_BITS>(POINTER_SIZE_IN_WORDS +\n                upgradeBound<uint64_t>(count) * wordsPerElement, []() {\n                  KJ_FAIL_ASSERT(\"encountered list pointer in builder which is too large to \"\n                      \"possibly fit in a segment. Bug in builder code?\");\n                }));\n            break;\n          }\n        }\n        break;\n      }\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"Unexpected FAR pointer.\") {\n          break;\n        }\n        break;\n      case WirePointer::OTHER:\n        KJ_FAIL_ASSERT(\"Unexpected OTHER pointer.\") {\n          break;\n        }\n        break;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(\n      void zeroPointerAndFars(SegmentBuilder* segment, WirePointer* ref)) {\n    // Zero out the pointer itself and, if it is a far pointer, zero the landing pad as well, but\n    // do not zero the object body.  Used when upgrading.\n\n    if (ref->kind() == WirePointer::FAR) {\n      SegmentBuilder* padSegment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n      if (padSegment->isWritable()) {  // Don't zero external data.\n        WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(padSegment));\n        if (ref->isDoubleFar()) {\n          zeroMemory(pad, G(2) * POINTERS);\n        } else {\n          zeroMemory(pad);\n        }\n      }\n    }\n\n    zeroMemory(ref);\n  }\n\n\n  // -----------------------------------------------------------------\n\n  static MessageSizeCounts totalSize(\n      SegmentReader* segment, const WirePointer* ref, int nestingLimit) {\n    // Compute the total size of the object pointed to, not counting far pointer overhead.\n\n    MessageSizeCounts result = { ZERO * WORDS, 0 };\n\n    if (ref->isNull()) {\n      return result;\n    }\n\n    KJ_REQUIRE(nestingLimit > 0, \"Message is too deeply-nested.\") {\n      return result;\n    }\n    --nestingLimit;\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, ref->target(segment), segment)) {\n      ptr = p;\n    } else {\n      return result;\n    }\n\n    switch (ref->kind()) {\n      case WirePointer::STRUCT: {\n        KJ_REQUIRE(boundsCheck(segment, ptr, ref->structRef.wordSize()),\n                   \"Message contained out-of-bounds struct pointer. \"\n                   OUT_OF_BOUNDS_ERROR_DETAIL) {\n          return result;\n        }\n        result.addWords(ref->structRef.wordSize());\n\n        const WirePointer* pointerSection =\n            reinterpret_cast<const WirePointer*>(ptr + ref->structRef.dataSize.get());\n        for (auto i: kj::zeroTo(ref->structRef.ptrCount.get())) {\n          result += totalSize(segment, pointerSection + i, nestingLimit);\n        }\n        break;\n      }\n      case WirePointer::LIST: {\n        switch (ref->listRef.elementSize()) {\n          case ElementSize::VOID:\n            // Nothing.\n            break;\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            auto totalWords = roundBitsUpToWords(\n                upgradeBound<uint64_t>(ref->listRef.elementCount()) *\n                dataBitsPerElement(ref->listRef.elementSize()));\n            KJ_REQUIRE(boundsCheck(segment, ptr, totalWords),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n            result.addWords(totalWords);\n            break;\n          }\n          case ElementSize::POINTER: {\n            auto count = ref->listRef.elementCount() * (POINTERS / ELEMENTS);\n\n            KJ_REQUIRE(boundsCheck(segment, ptr, count * WORDS_PER_POINTER),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            result.addWords(count * WORDS_PER_POINTER);\n\n            for (auto i: kj::zeroTo(count)) {\n              result += totalSize(segment, reinterpret_cast<const WirePointer*>(ptr) + i,\n                                  nestingLimit);\n            }\n            break;\n          }\n          case ElementSize::INLINE_COMPOSITE: {\n            auto wordCount = ref->listRef.inlineCompositeWordCount();\n            KJ_REQUIRE(boundsCheck(segment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            const WirePointer* elementTag = reinterpret_cast<const WirePointer*>(ptr);\n            auto count = elementTag->inlineCompositeListElementCount();\n\n            KJ_REQUIRE(elementTag->kind() == WirePointer::STRUCT,\n                       \"Don't know how to handle non-STRUCT inline composite.\") {\n              return result;\n            }\n\n            auto actualSize = elementTag->structRef.wordSize() / ELEMENTS *\n                              upgradeBound<uint64_t>(count);\n            KJ_REQUIRE(actualSize <= wordCount,\n                       \"Struct list pointer's elements overran size. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            // We count the actual size rather than the claimed word count because that's what\n            // we'll end up with if we make a copy.\n            result.addWords(actualSize + POINTER_SIZE_IN_WORDS);\n\n            WordCount dataSize = elementTag->structRef.dataSize.get();\n            WirePointerCount pointerCount = elementTag->structRef.ptrCount.get();\n\n            if (pointerCount > ZERO * POINTERS) {\n              const word* pos = ptr + POINTER_SIZE_IN_WORDS;\n              for (auto i KJ_UNUSED: kj::zeroTo(count)) {\n                pos += dataSize;\n\n                for (auto j KJ_UNUSED: kj::zeroTo(pointerCount)) {\n                  result += totalSize(segment, reinterpret_cast<const WirePointer*>(pos),\n                                      nestingLimit);\n                  pos += POINTER_SIZE_IN_WORDS;\n                }\n              }\n            }\n            break;\n          }\n        }\n        break;\n      }\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unexpected FAR pointer.\") {\n          break;\n        }\n        break;\n      case WirePointer::OTHER:\n        if (ref->isCapability()) {\n          result.capCount++;\n        } else {\n          KJ_FAIL_REQUIRE(\"Unknown pointer type.\") { break; }\n        }\n        break;\n    }\n\n    return result;\n  }\n\n  // -----------------------------------------------------------------\n  // Copy from an unchecked message.\n\n  static KJ_ALWAYS_INLINE(\n      void copyStruct(SegmentBuilder* segment, CapTableBuilder* capTable,\n                      word* dst, const word* src,\n                      StructDataWordCount dataSize, StructPointerCount pointerCount)) {\n    copyMemory(dst, src, dataSize);\n\n    const WirePointer* srcRefs = reinterpret_cast<const WirePointer*>(src + dataSize);\n    WirePointer* dstRefs = reinterpret_cast<WirePointer*>(dst + dataSize);\n\n    for (auto i: kj::zeroTo(pointerCount)) {\n      SegmentBuilder* subSegment = segment;\n      WirePointer* dstRef = dstRefs + i;\n      copyMessage(subSegment, capTable, dstRef, srcRefs + i);\n    }\n  }\n\n  static word* copyMessage(\n      SegmentBuilder*& segment, CapTableBuilder* capTable,\n      WirePointer*& dst, const WirePointer* src) {\n    // Not always-inline because it's recursive.\n\n    switch (src->kind()) {\n      case WirePointer::STRUCT: {\n        if (src->isNull()) {\n          zeroMemory(dst);\n          return nullptr;\n        } else {\n          const word* srcPtr = src->target(nullptr);\n          word* dstPtr = allocate(\n              dst, segment, capTable, src->structRef.wordSize(), WirePointer::STRUCT, nullptr);\n\n          copyStruct(segment, capTable, dstPtr, srcPtr, src->structRef.dataSize.get(),\n                     src->structRef.ptrCount.get());\n\n          dst->structRef.set(src->structRef.dataSize.get(), src->structRef.ptrCount.get());\n          return dstPtr;\n        }\n      }\n      case WirePointer::LIST: {\n        switch (src->listRef.elementSize()) {\n          case ElementSize::VOID:\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            auto wordCount = roundBitsUpToWords(\n                upgradeBound<uint64_t>(src->listRef.elementCount()) *\n                dataBitsPerElement(src->listRef.elementSize()));\n            const word* srcPtr = src->target(nullptr);\n            word* dstPtr = allocate(dst, segment, capTable, wordCount, WirePointer::LIST, nullptr);\n            copyMemory(dstPtr, srcPtr, wordCount);\n\n            dst->listRef.set(src->listRef.elementSize(), src->listRef.elementCount());\n            return dstPtr;\n          }\n\n          case ElementSize::POINTER: {\n            const WirePointer* srcRefs = reinterpret_cast<const WirePointer*>(src->target(nullptr));\n            WirePointer* dstRefs = reinterpret_cast<WirePointer*>(\n                allocate(dst, segment, capTable, src->listRef.elementCount() *\n                    (ONE * POINTERS / ELEMENTS) * WORDS_PER_POINTER,\n                    WirePointer::LIST, nullptr));\n\n            for (auto i: kj::zeroTo(src->listRef.elementCount() * (ONE * POINTERS / ELEMENTS))) {\n              SegmentBuilder* subSegment = segment;\n              WirePointer* dstRef = dstRefs + i;\n              copyMessage(subSegment, capTable, dstRef, srcRefs + i);\n            }\n\n            dst->listRef.set(ElementSize::POINTER, src->listRef.elementCount());\n            return reinterpret_cast<word*>(dstRefs);\n          }\n\n          case ElementSize::INLINE_COMPOSITE: {\n            const word* srcPtr = src->target(nullptr);\n            word* dstPtr = allocate(dst, segment, capTable,\n                assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n                    src->listRef.inlineCompositeWordCount() + POINTER_SIZE_IN_WORDS,\n                    []() { KJ_FAIL_ASSERT(\"list too big to fit in a segment\"); }),\n                WirePointer::LIST, nullptr);\n\n            dst->listRef.setInlineComposite(src->listRef.inlineCompositeWordCount());\n\n            const WirePointer* srcTag = reinterpret_cast<const WirePointer*>(srcPtr);\n            copyMemory(reinterpret_cast<WirePointer*>(dstPtr), srcTag);\n\n            const word* srcElement = srcPtr + POINTER_SIZE_IN_WORDS;\n            word* dstElement = dstPtr + POINTER_SIZE_IN_WORDS;\n\n            KJ_ASSERT(srcTag->kind() == WirePointer::STRUCT,\n                \"INLINE_COMPOSITE of lists is not yet supported.\");\n\n            for (auto i KJ_UNUSED: kj::zeroTo(srcTag->inlineCompositeListElementCount())) {\n              copyStruct(segment, capTable, dstElement, srcElement,\n                  srcTag->structRef.dataSize.get(), srcTag->structRef.ptrCount.get());\n              srcElement += srcTag->structRef.wordSize();\n              dstElement += srcTag->structRef.wordSize();\n            }\n            return dstPtr;\n          }\n        }\n        break;\n      }\n      case WirePointer::OTHER:\n        KJ_FAIL_REQUIRE(\"Unchecked messages cannot contain OTHER pointers (e.g. capabilities).\");\n        break;\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unchecked messages cannot contain far pointers.\");\n        break;\n    }\n\n    return nullptr;\n  }\n\n  static void transferPointer(SegmentBuilder* dstSegment, WirePointer* dst,\n                              SegmentBuilder* srcSegment, WirePointer* src) {\n    // Make *dst point to the same object as *src.  Both must reside in the same message, but can\n    // be in different segments.  Not always-inline because this is rarely used.\n    //\n    // Caller MUST zero out the source pointer after calling this, to make sure no later code\n    // mistakenly thinks the source location still owns the object.  transferPointer() doesn't do\n    // this zeroing itself because many callers transfer several pointers in a loop then zero out\n    // the whole section.\n\n    KJ_DASSERT(dst->isNull());\n    // We expect the caller to ensure the target is already null so won't leak.\n\n    if (src->isNull()) {\n      zeroMemory(dst);\n    } else if (src->isPositional()) {\n      transferPointer(dstSegment, dst, srcSegment, src, src->target());\n    } else {\n      // Far and other pointers are position-independent, so we can just copy.\n      copyMemory(dst, src);\n    }\n  }\n\n  static void transferPointer(SegmentBuilder* dstSegment, WirePointer* dst,\n                              SegmentBuilder* srcSegment, const WirePointer* srcTag,\n                              word* srcPtr) {\n    // Like the other overload, but splits src into a tag and a target.  Particularly useful for\n    // OrphanBuilder.\n\n    if (dstSegment == srcSegment) {\n      // Same segment, so create a direct pointer.\n\n      if (srcTag->kind() == WirePointer::STRUCT && srcTag->structRef.wordSize() == ZERO * WORDS) {\n        dst->setKindAndTargetForEmptyStruct();\n      } else {\n        dst->setKindAndTarget(srcTag->kind(), srcPtr, dstSegment);\n      }\n\n      // We can just copy the upper 32 bits.  (Use memcpy() to comply with aliasing rules.)\n      copyMemory(&dst->upper32Bits, &srcTag->upper32Bits);\n    } else {\n      // Need to create a far pointer.  Try to allocate it in the same segment as the source, so\n      // that it doesn't need to be a double-far.\n\n      WirePointer* landingPad =\n          reinterpret_cast<WirePointer*>(srcSegment->allocate(G(1) * WORDS));\n      if (landingPad == nullptr) {\n        // Darn, need a double-far.\n        auto allocation = srcSegment->getArena()->allocate(G(2) * WORDS);\n        SegmentBuilder* farSegment = allocation.segment;\n        landingPad = reinterpret_cast<WirePointer*>(allocation.words);\n\n        landingPad[0].setFar(false, srcSegment->getOffsetTo(srcPtr));\n        landingPad[0].farRef.segmentId.set(srcSegment->getSegmentId());\n\n        landingPad[1].setKindWithZeroOffset(srcTag->kind());\n        copyMemory(&landingPad[1].upper32Bits, &srcTag->upper32Bits);\n\n        dst->setFar(true, farSegment->getOffsetTo(reinterpret_cast<word*>(landingPad)));\n        dst->farRef.set(farSegment->getSegmentId());\n      } else {\n        // Simple landing pad is just a pointer.\n        landingPad->setKindAndTarget(srcTag->kind(), srcPtr, srcSegment);\n        copyMemory(&landingPad->upper32Bits, &srcTag->upper32Bits);\n\n        dst->setFar(false, srcSegment->getOffsetTo(reinterpret_cast<word*>(landingPad)));\n        dst->farRef.set(srcSegment->getSegmentId());\n      }\n    }\n  }\n\n  // -----------------------------------------------------------------\n\n  static KJ_ALWAYS_INLINE(StructBuilder initStructPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, StructSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // Allocate space for the new struct.  Newly-allocated space is automatically zeroed.\n    word* ptr = allocate(ref, segment, capTable, size.total(), WirePointer::STRUCT, orphanArena);\n\n    // Initialize the pointer.\n    ref->structRef.set(size);\n\n    // Build the StructBuilder.\n    return StructBuilder(segment, capTable, ptr, reinterpret_cast<WirePointer*>(ptr + size.data),\n                         size.data * BITS_PER_WORD, size.pointers);\n  }\n\n  static KJ_ALWAYS_INLINE(StructBuilder getWritableStructPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, StructSize size,\n      const word* defaultValue)) {\n    return getWritableStructPointer(ref, ref->target(), segment, capTable, size, defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(StructBuilder getWritableStructPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      StructSize size, const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return initStructPointer(ref, segment, capTable, size, orphanArena);\n      }\n      refTarget = copyMessage(segment, capTable, ref,\n          reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    WirePointer* oldRef = ref;\n    SegmentBuilder* oldSegment = segment;\n    word* oldPtr = followFars(oldRef, refTarget, oldSegment);\n\n    KJ_REQUIRE(oldRef->kind() == WirePointer::STRUCT,\n        \"Schema mismatch: Message contains non-struct pointer where struct pointer was expected.\") {\n      goto useDefault;\n    }\n\n    auto oldDataSize = oldRef->structRef.dataSize.get();\n    auto oldPointerCount = oldRef->structRef.ptrCount.get();\n    WirePointer* oldPointerSection =\n        reinterpret_cast<WirePointer*>(oldPtr + oldDataSize);\n\n    if (oldDataSize < size.data || oldPointerCount < size.pointers) {\n      // The space allocated for this struct is too small.  Unlike with readers, we can't just\n      // run with it and do bounds checks at access time, because how would we handle writes?\n      // Instead, we have to copy the struct to a new space now.\n\n      auto newDataSize = kj::max(oldDataSize, size.data);\n      auto newPointerCount = kj::max(oldPointerCount, size.pointers);\n      auto totalSize = newDataSize + newPointerCount * WORDS_PER_POINTER;\n\n      // Don't let allocate() zero out the object just yet.\n      zeroPointerAndFars(segment, ref);\n\n      word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::STRUCT, orphanArena);\n      ref->structRef.set(newDataSize, newPointerCount);\n\n      // Copy data section.\n      copyMemory(ptr, oldPtr, oldDataSize);\n\n      // Copy pointer section.\n      WirePointer* newPointerSection = reinterpret_cast<WirePointer*>(ptr + newDataSize);\n      for (auto i: kj::zeroTo(oldPointerCount)) {\n        transferPointer(segment, newPointerSection + i, oldSegment, oldPointerSection + i);\n      }\n\n      // Zero out old location.  This has two purposes:\n      // 1) We don't want to leak the original contents of the struct when the message is written\n      //    out as it may contain secrets that the caller intends to remove from the new copy.\n      // 2) Zeros will be deflated by packing, making this dead memory almost-free if it ever\n      //    hits the wire.\n      zeroMemory(oldPtr, oldDataSize + oldPointerCount * WORDS_PER_POINTER);\n\n      return StructBuilder(segment, capTable, ptr, newPointerSection, newDataSize * BITS_PER_WORD,\n                           newPointerCount);\n    } else {\n      return StructBuilder(oldSegment, capTable, oldPtr, oldPointerSection,\n                           oldDataSize * BITS_PER_WORD, oldPointerCount);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder initListPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      ElementCount elementCount, ElementSize elementSize, BuilderArena* orphanArena = nullptr)) {\n    KJ_DREQUIRE(elementSize != ElementSize::INLINE_COMPOSITE,\n        \"Should have called initStructListPointer() instead.\");\n\n    auto checkedElementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount,\n        []() { KJ_FAIL_REQUIRE(\"tried to allocate list with too many elements\"); });\n\n    auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n    auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n    auto step = bitsPerElementIncludingPointers(elementSize);\n    KJ_DASSERT(step * ELEMENTS == (dataSize + pointerCount * BITS_PER_POINTER));\n\n    // Calculate size of the list.\n    auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(checkedElementCount) * step);\n\n    // Allocate the list.\n    word* ptr = allocate(ref, segment, capTable, wordCount, WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(elementSize, checkedElementCount);\n\n    // Build the ListBuilder.\n    return ListBuilder(segment, capTable, ptr, step, checkedElementCount,\n                       dataSize, pointerCount, elementSize);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder initStructListPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      ElementCount elementCount, StructSize elementSize, BuilderArena* orphanArena = nullptr)) {\n    auto checkedElementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount,\n        []() { KJ_FAIL_REQUIRE(\"tried to allocate list with too many elements\"); });\n\n    WordsPerElementN<17> wordsPerElement = elementSize.total() / ELEMENTS;\n\n    // Allocate the list, prefixed by a single WirePointer.\n    auto wordCount = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n        upgradeBound<uint64_t>(checkedElementCount) * wordsPerElement,\n        []() { KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\"); });\n    word* ptr = allocate(ref, segment, capTable, POINTER_SIZE_IN_WORDS + wordCount,\n                         WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    // INLINE_COMPOSITE lists replace the element count with the word count.\n    ref->listRef.setInlineComposite(wordCount);\n\n    // Initialize the list tag.\n    reinterpret_cast<WirePointer*>(ptr)->setKindAndInlineCompositeListElementCount(\n        WirePointer::STRUCT, checkedElementCount);\n    reinterpret_cast<WirePointer*>(ptr)->structRef.set(elementSize);\n    ptr += POINTER_SIZE_IN_WORDS;\n\n    // Build the ListBuilder.\n    return ListBuilder(segment, capTable, ptr, wordsPerElement * BITS_PER_WORD, checkedElementCount,\n                       elementSize.data * BITS_PER_WORD, elementSize.pointers,\n                       ElementSize::INLINE_COMPOSITE);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointer(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      ElementSize elementSize, const word* defaultValue)) {\n    return getWritableListPointer(origRef, origRef->target(), origSegment, capTable, elementSize,\n                                  defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointer(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable, ElementSize elementSize,\n      const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    KJ_DREQUIRE(elementSize != ElementSize::INLINE_COMPOSITE,\n             \"Use getWritableStructListPointer() for struct lists.\");\n\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(elementSize);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    // We must verify that the pointer has the right size.  Unlike in\n    // getWritableStructListPointer(), we never need to \"upgrade\" the data, because this\n    // method is called only for non-struct lists, and there is no allowed upgrade path *to*\n    // a non-struct list, only *from* them.\n\n    WirePointer* ref = origRef;\n    SegmentBuilder* segment = origSegment;\n    word* ptr = followFars(ref, origRefTarget, segment);\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n        \"Schema mismatch: Called getWritableListPointer() but existing pointer is not a list.\") {\n      goto useDefault;\n    }\n\n    ElementSize oldSize = ref->listRef.elementSize();\n\n    if (oldSize == ElementSize::INLINE_COMPOSITE) {\n      // The existing element size is INLINE_COMPOSITE, though we expected a list of primitives.\n      // The existing data must have been written with a newer version of the protocol.  We\n      // therefore never need to upgrade the data in this case, but we do need to validate that it\n      // is a valid upgrade from what we expected.\n\n      // Read the tag to get the actual element count.\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n          \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\");\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      auto dataSize = tag->structRef.dataSize.get();\n      auto pointerCount = tag->structRef.ptrCount.get();\n\n      switch (elementSize) {\n        case ElementSize::VOID:\n          // Anything is a valid upgrade from Void.\n          break;\n\n        case ElementSize::BIT:\n          KJ_FAIL_REQUIRE(\n              \"Schema mismatch: Found struct list where bit list was expected; upgrading boolean \"\n              \"lists to structs is no longer supported.\") {\n            goto useDefault;\n          }\n          break;\n\n        case ElementSize::BYTE:\n        case ElementSize::TWO_BYTES:\n        case ElementSize::FOUR_BYTES:\n        case ElementSize::EIGHT_BYTES:\n          KJ_REQUIRE(dataSize >= ONE * WORDS,\n                     \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n            goto useDefault;\n          }\n          break;\n\n        case ElementSize::POINTER:\n          KJ_REQUIRE(pointerCount >= ONE * POINTERS,\n                     \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n            goto useDefault;\n          }\n          // Adjust the pointer to point at the reference segment.\n          ptr += dataSize;\n          break;\n\n        case ElementSize::INLINE_COMPOSITE:\n          KJ_UNREACHABLE;\n      }\n\n      // OK, looks valid.\n\n      return ListBuilder(segment, capTable, ptr,\n                         tag->structRef.wordSize() * BITS_PER_WORD / ELEMENTS,\n                         tag->inlineCompositeListElementCount(),\n                         dataSize * BITS_PER_WORD, pointerCount, ElementSize::INLINE_COMPOSITE);\n    } else {\n      auto dataSize = dataBitsPerElement(oldSize) * ELEMENTS;\n      auto pointerCount = pointersPerElement(oldSize) * ELEMENTS;\n\n      if (elementSize == ElementSize::BIT) {\n        KJ_REQUIRE(oldSize == ElementSize::BIT,\n            \"Schema mismatch: Found non-bit list where bit list was expected.\") {\n          goto useDefault;\n        }\n      } else {\n        KJ_REQUIRE(oldSize != ElementSize::BIT,\n            \"Schema mismatch: Found bit list where non-bit list was expected.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(dataSize >= dataBitsPerElement(elementSize) * ELEMENTS,\n                   \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(pointerCount >= pointersPerElement(elementSize) * ELEMENTS,\n                   \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n          goto useDefault;\n        }\n      }\n\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n      return ListBuilder(segment, capTable, ptr, step, ref->listRef.elementCount(),\n                         dataSize, pointerCount, oldSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointerAnySize(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      const word* defaultValue)) {\n    return getWritableListPointerAnySize(origRef, origRef->target(), origSegment,\n                                         capTable, defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointerAnySize(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(ElementSize::VOID);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    WirePointer* ref = origRef;\n    SegmentBuilder* segment = origSegment;\n    word* ptr = followFars(ref, origRefTarget, segment);\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n        \"Schema mismatch: Called getWritableListPointerAnySize() but existing pointer is not a \"\n        \"list.\") {\n      goto useDefault;\n    }\n\n    ElementSize elementSize = ref->listRef.elementSize();\n\n    if (elementSize == ElementSize::INLINE_COMPOSITE) {\n      // Read the tag to get the actual element count.\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n          \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\");\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      return ListBuilder(segment, capTable, ptr,\n                         tag->structRef.wordSize() * BITS_PER_WORD / ELEMENTS,\n                         tag->inlineCompositeListElementCount(),\n                         tag->structRef.dataSize.get() * BITS_PER_WORD,\n                         tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE);\n    } else {\n      auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n      auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n      return ListBuilder(segment, capTable, ptr, step, ref->listRef.elementCount(),\n                         dataSize, pointerCount, elementSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableStructListPointer(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      StructSize elementSize, const word* defaultValue)) {\n    return getWritableStructListPointer(origRef, origRef->target(), origSegment, capTable,\n                                        elementSize, defaultValue);\n  }\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableStructListPointer(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      StructSize elementSize, const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(ElementSize::INLINE_COMPOSITE);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    // We must verify that the pointer has the right size and potentially upgrade it if not.\n\n    WirePointer* oldRef = origRef;\n    SegmentBuilder* oldSegment = origSegment;\n    word* oldPtr = followFars(oldRef, origRefTarget, oldSegment);\n\n    KJ_REQUIRE(oldRef->kind() == WirePointer::LIST,\n               \"Schema mismatch: Called getList{Field,Element}() but existing pointer is not a \"\n               \"list.\") {\n      goto useDefault;\n    }\n\n    ElementSize oldSize = oldRef->listRef.elementSize();\n\n    if (oldSize == ElementSize::INLINE_COMPOSITE) {\n      // Existing list is INLINE_COMPOSITE, but we need to verify that the sizes match.\n\n      WirePointer* oldTag = reinterpret_cast<WirePointer*>(oldPtr);\n      oldPtr += POINTER_SIZE_IN_WORDS;\n      KJ_REQUIRE(oldTag->kind() == WirePointer::STRUCT,\n                 \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\") {\n        goto useDefault;\n      }\n\n      auto oldDataSize = oldTag->structRef.dataSize.get();\n      auto oldPointerCount = oldTag->structRef.ptrCount.get();\n      auto oldStep = (oldDataSize + oldPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n\n      auto elementCount = oldTag->inlineCompositeListElementCount();\n\n      if (oldDataSize >= elementSize.data && oldPointerCount >= elementSize.pointers) {\n        // Old size is at least as large as we need.  Ship it.\n        return ListBuilder(oldSegment, capTable, oldPtr, oldStep * BITS_PER_WORD, elementCount,\n                           oldDataSize * BITS_PER_WORD, oldPointerCount,\n                           ElementSize::INLINE_COMPOSITE);\n      }\n\n      // The structs in this list are smaller than expected, probably written using an older\n      // version of the protocol.  We need to make a copy and expand them.\n\n      auto newDataSize = kj::max(oldDataSize, elementSize.data);\n      auto newPointerCount = kj::max(oldPointerCount, elementSize.pointers);\n      auto newStep = (newDataSize + newPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n\n      auto totalSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n            newStep * upgradeBound<uint64_t>(elementCount),\n            []() { KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\"); });\n\n      // Don't let allocate() zero out the object just yet.\n      zeroPointerAndFars(origSegment, origRef);\n\n      word* newPtr = allocate(origRef, origSegment, capTable, totalSize + POINTER_SIZE_IN_WORDS,\n                              WirePointer::LIST, orphanArena);\n      origRef->listRef.setInlineComposite(totalSize);\n\n      WirePointer* newTag = reinterpret_cast<WirePointer*>(newPtr);\n      newTag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, elementCount);\n      newTag->structRef.set(newDataSize, newPointerCount);\n      newPtr += POINTER_SIZE_IN_WORDS;\n\n      word* src = oldPtr;\n      word* dst = newPtr;\n      for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n        // Copy data section.\n        copyMemory(dst, src, oldDataSize);\n\n        // Copy pointer section.\n        WirePointer* newPointerSection = reinterpret_cast<WirePointer*>(dst + newDataSize);\n        WirePointer* oldPointerSection = reinterpret_cast<WirePointer*>(src + oldDataSize);\n        for (auto j: kj::zeroTo(oldPointerCount)) {\n          transferPointer(origSegment, newPointerSection + j, oldSegment, oldPointerSection + j);\n        }\n\n        dst += newStep * (ONE * ELEMENTS);\n        src += oldStep * (ONE * ELEMENTS);\n      }\n\n      auto oldSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n            oldStep * upgradeBound<uint64_t>(elementCount),\n            []() { KJ_FAIL_ASSERT(\"old size overflows but new size doesn't?\"); });\n\n      // Zero out old location.  See explanation in getWritableStructPointer().\n      // Make sure to include the tag word.\n      zeroMemory(oldPtr - POINTER_SIZE_IN_WORDS, oldSize + POINTER_SIZE_IN_WORDS);\n\n      return ListBuilder(origSegment, capTable, newPtr, newStep * BITS_PER_WORD, elementCount,\n                         newDataSize * BITS_PER_WORD, newPointerCount,\n                         ElementSize::INLINE_COMPOSITE);\n    } else {\n      // We're upgrading from a non-struct list.\n\n      auto oldDataSize = dataBitsPerElement(oldSize) * ELEMENTS;\n      auto oldPointerCount = pointersPerElement(oldSize) * ELEMENTS;\n      auto oldStep = (oldDataSize + oldPointerCount * BITS_PER_POINTER) / ELEMENTS;\n      auto elementCount = oldRef->listRef.elementCount();\n\n      if (oldSize == ElementSize::VOID) {\n        // Nothing to copy, just allocate a new list.\n        return initStructListPointer(origRef, origSegment, capTable, elementCount, elementSize);\n      } else {\n        // Upgrading to an inline composite list.\n\n        KJ_REQUIRE(oldSize != ElementSize::BIT,\n            \"Schema mismatch: Found bit list where struct list was expected; upgrading boolean \"\n            \"lists to structs is no longer supported.\") {\n          goto useDefault;\n        }\n\n        auto newDataSize = elementSize.data;\n        auto newPointerCount = elementSize.pointers;\n\n        if (oldSize == ElementSize::POINTER) {\n          newPointerCount = kj::max(newPointerCount, ONE * POINTERS);\n        } else {\n          // Old list contains data elements, so we need at least 1 word of data.\n          newDataSize = kj::max(newDataSize, ONE * WORDS);\n        }\n\n        auto newStep = (newDataSize + newPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n        auto totalWords = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n              newStep * upgradeBound<uint64_t>(elementCount),\n              []() {KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\");});\n\n        // Don't let allocate() zero out the object just yet.\n        zeroPointerAndFars(origSegment, origRef);\n\n        word* newPtr = allocate(origRef, origSegment, capTable, totalWords + POINTER_SIZE_IN_WORDS,\n                                WirePointer::LIST, orphanArena);\n        origRef->listRef.setInlineComposite(totalWords);\n\n        WirePointer* tag = reinterpret_cast<WirePointer*>(newPtr);\n        tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, elementCount);\n        tag->structRef.set(newDataSize, newPointerCount);\n        newPtr += POINTER_SIZE_IN_WORDS;\n\n        if (oldSize == ElementSize::POINTER) {\n          WirePointer* dst = reinterpret_cast<WirePointer*>(newPtr + newDataSize);\n          WirePointer* src = reinterpret_cast<WirePointer*>(oldPtr);\n          for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n            transferPointer(origSegment, dst, oldSegment, src);\n            dst += newStep / WORDS_PER_POINTER * (ONE * ELEMENTS);\n            ++src;\n          }\n        } else {\n          byte* dst = reinterpret_cast<byte*>(newPtr);\n          byte* src = reinterpret_cast<byte*>(oldPtr);\n          auto newByteStep = newStep * (ONE * ELEMENTS) * BYTES_PER_WORD;\n          auto oldByteStep = oldDataSize / BITS_PER_BYTE;\n          for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n            copyMemory(dst, src, oldByteStep);\n            src += oldByteStep;\n            dst += newByteStep;\n          }\n        }\n\n        auto oldSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n              roundBitsUpToWords(oldStep * upgradeBound<uint64_t>(elementCount)),\n              []() { KJ_FAIL_ASSERT(\"old size overflows but new size doesn't?\"); });\n\n        // Zero out old location.  See explanation in getWritableStructPointer().\n        zeroMemory(oldPtr, oldSize);\n\n        return ListBuilder(origSegment, capTable, newPtr, newStep * BITS_PER_WORD, elementCount,\n                           newDataSize * BITS_PER_WORD, newPointerCount,\n                           ElementSize::INLINE_COMPOSITE);\n      }\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Text::Builder> initTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, TextSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // The byte list must include a NUL terminator.\n    auto byteSize = size + ONE * BYTES;\n\n    // Allocate the space.\n    word* ptr = allocate(\n        ref, segment, capTable, roundBytesUpToWords(byteSize), WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(ElementSize::BYTE, byteSize * (ONE * ELEMENTS / BYTES));\n\n    // Build the Text::Builder. Note that since allocate()ed memory is pre-zero'd, we don't need\n    // to initialize the NUL terminator.\n    return { segment, Text::Builder(reinterpret_cast<char*>(ptr), unbound(size / BYTES)) };\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Text::Builder> setTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, Text::Reader value,\n      BuilderArena* orphanArena = nullptr)) {\n    TextSize size = assertMax<MAX_TEXT_SIZE>(bounded(value.size()),\n        []() { KJ_FAIL_REQUIRE(\"text blob too big\"); }) * BYTES;\n\n    auto allocation = initTextPointer(ref, segment, capTable, size, orphanArena);\n    copyMemory(allocation.value.begin(), value);\n    return allocation;\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Builder getWritableTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, TextSize defaultSize)) {\n    return getWritableTextPointer(ref, ref->target(), segment,capTable,  defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Builder getWritableTextPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, TextSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultSize == ZERO * BYTES) {\n        return nullptr;\n      } else {\n        Text::Builder builder = initTextPointer(ref, segment, capTable, defaultSize).value;\n        copyMemory(builder.asBytes().begin(), reinterpret_cast<const byte*>(defaultValue),\n                   defaultSize);\n        return builder;\n      }\n    } else {\n      word* ptr = followFars(ref, refTarget, segment);\n      byte* bptr = reinterpret_cast<byte*>(ptr);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n          \"Schema mismatch: Called getText{Field,Element}() but existing pointer is not a list.\") {\n        goto useDefault;\n      }\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n          \"Schema mismatch: Called getText{Field,Element}() but existing list pointer is not \"\n          \"byte-sized.\") {\n        goto useDefault;\n      }\n\n      auto maybeSize = trySubtract(ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS),\n                                   ONE * BYTES);\n      KJ_IF_MAYBE(size, maybeSize) {\n        KJ_REQUIRE(*(bptr + *size) == '\\0', \"Text blob missing NUL terminator.\") {\n          goto useDefault;\n        }\n\n        return Text::Builder(reinterpret_cast<char*>(bptr), unbound(*size / BYTES));\n      } else {\n        KJ_FAIL_REQUIRE(\"zero-size blob can't be text (need NUL terminator)\") {\n          goto useDefault;\n        };\n      }\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Data::Builder> initDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, BlobSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // Allocate the space.\n    word* ptr = allocate(ref, segment, capTable, roundBytesUpToWords(size),\n                         WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(ElementSize::BYTE, size * (ONE * ELEMENTS / BYTES));\n\n    // Build the Data::Builder.\n    return { segment, Data::Builder(reinterpret_cast<byte*>(ptr), unbound(size / BYTES)) };\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Data::Builder> setDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, Data::Reader value,\n      BuilderArena* orphanArena = nullptr)) {\n    BlobSize size = assertMaxBits<BLOB_SIZE_BITS>(bounded(value.size()),\n        []() { KJ_FAIL_REQUIRE(\"text blob too big\"); }) * BYTES;\n\n    auto allocation = initDataPointer(ref, segment, capTable, size, orphanArena);\n    copyMemory(allocation.value.begin(), value);\n    return allocation;\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Builder getWritableDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, BlobSize defaultSize)) {\n    return getWritableDataPointer(ref, ref->target(), segment, capTable, defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Builder getWritableDataPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, BlobSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultSize == ZERO * BYTES) {\n        return nullptr;\n      } else {\n        Data::Builder builder = initDataPointer(ref, segment, capTable, defaultSize).value;\n        copyMemory(builder.begin(), reinterpret_cast<const byte*>(defaultValue), defaultSize);\n        return builder;\n      }\n    } else {\n      word* ptr = followFars(ref, refTarget, segment);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n          \"Schema mismatch: Called getData{Field,Element}() but existing pointer is not a list.\") {\n        goto useDefault;\n      }\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n          \"Schema mismatch: Called getData{Field,Element}() but existing list pointer is not \"\n          \"byte-sized.\") {\n        goto useDefault;\n      }\n\n      return Data::Builder(reinterpret_cast<byte*>(ptr),\n          unbound(ref->listRef.elementCount() / ELEMENTS));\n    }\n  }\n\n  static SegmentAnd<word*> setStructPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref, StructReader value,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    auto dataSize = roundBitsUpToBytes(value.dataSize);\n    auto ptrCount = value.pointerCount;\n\n    if (canonical) {\n      // StructReaders should not have bitwidths other than 1, but let's be safe\n      KJ_REQUIRE((value.dataSize == ONE * BITS)\n                 || (value.dataSize % BITS_PER_BYTE == ZERO * BITS));\n\n      if (value.dataSize == ONE * BITS) {\n        // Handle the truncation case where it's a false in a 1-bit struct\n        if (!value.getDataField<bool>(ZERO * ELEMENTS)) {\n          dataSize = ZERO * BYTES;\n        }\n      } else {\n        // Truncate the data section\n        auto data = value.getDataSectionAsBlob();\n        auto end = data.end();\n        while (end > data.begin() && end[-1] == 0) --end;\n        dataSize = intervalLength(data.begin(), end, MAX_STUCT_DATA_WORDS * BYTES_PER_WORD);\n      }\n\n      // Truncate pointer section\n      const WirePointer* ptr = value.pointers + ptrCount;\n      while (ptr > value.pointers && ptr[-1].isNull()) --ptr;\n      ptrCount = intervalLength(value.pointers, ptr, MAX_STRUCT_POINTER_COUNT);\n    }\n\n    auto dataWords = roundBytesUpToWords(dataSize);\n\n    auto totalSize = dataWords + ptrCount * WORDS_PER_POINTER;\n\n    word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::STRUCT, orphanArena);\n    ref->structRef.set(dataWords, ptrCount);\n\n    if (value.dataSize == ONE * BITS) {\n      // Data size could be made 0 by truncation\n      if (dataSize != ZERO * BYTES) {\n        *reinterpret_cast<char*>(ptr) = value.getDataField<bool>(ZERO * ELEMENTS);\n      }\n    } else {\n      copyMemory(reinterpret_cast<byte*>(ptr),\n                 reinterpret_cast<const byte*>(value.data),\n                 dataSize);\n    }\n\n    WirePointer* pointerSection = reinterpret_cast<WirePointer*>(ptr + dataWords);\n    for (auto i: kj::zeroTo(ptrCount)) {\n      copyPointer(segment, capTable, pointerSection + i,\n                  value.segment, value.capTable, value.pointers + i,\n                  value.nestingLimit, nullptr, canonical);\n    }\n\n    return { segment, ptr };\n  }\n\n#if !CAPNP_LITE\n  static void setCapabilityPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref,\n      kj::Own<ClientHook>&& cap) {\n    if (!ref->isNull()) {\n      zeroObject(segment, capTable, ref);\n    }\n    if (cap->isNull()) {\n      zeroMemory(ref);\n    } else {\n      ref->setCap(capTable->injectCap(kj::mv(cap)));\n    }\n  }\n#endif  // !CAPNP_LITE\n\n  static SegmentAnd<word*> setListPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref, ListReader value,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    auto totalSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n        roundBitsUpToWords(upgradeBound<uint64_t>(value.elementCount) * value.step),\n        []() { KJ_FAIL_ASSERT(\"encountered impossibly long struct list ListReader\"); });\n\n    if (value.elementSize != ElementSize::INLINE_COMPOSITE) {\n      // List of non-structs.\n      word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::LIST, orphanArena);\n\n      if (value.elementSize == ElementSize::POINTER) {\n        // List of pointers.\n        ref->listRef.set(ElementSize::POINTER, value.elementCount);\n        for (auto i: kj::zeroTo(value.elementCount * (ONE * POINTERS / ELEMENTS))) {\n          copyPointer(segment, capTable, reinterpret_cast<WirePointer*>(ptr) + i,\n                      value.segment, value.capTable,\n                      reinterpret_cast<const WirePointer*>(value.ptr) + i,\n                      value.nestingLimit, nullptr, canonical);\n        }\n      } else {\n        // List of data.\n        ref->listRef.set(value.elementSize, value.elementCount);\n\n        auto wholeByteSize =\n          assertMax(MAX_SEGMENT_WORDS * BYTES_PER_WORD,\n            upgradeBound<uint64_t>(value.elementCount) * value.step / BITS_PER_BYTE,\n            []() { KJ_FAIL_ASSERT(\"encountered impossibly long data ListReader\"); });\n        copyMemory(reinterpret_cast<byte*>(ptr), value.ptr, wholeByteSize);\n        auto leftoverBits =\n          (upgradeBound<uint64_t>(value.elementCount) * value.step) % BITS_PER_BYTE;\n        if (leftoverBits > ZERO * BITS) {\n          // We need to copy a partial byte.\n          uint8_t mask = (1 << unbound(leftoverBits / BITS)) - 1;\n          *((reinterpret_cast<byte*>(ptr)) + wholeByteSize) = mask & *(value.ptr + wholeByteSize);\n        }\n      }\n\n      return { segment, ptr };\n    } else {\n      // List of structs.\n      StructDataWordCount declDataSize = value.structDataSize / BITS_PER_WORD;\n      StructPointerCount declPointerCount = value.structPointerCount;\n\n      StructDataWordCount dataSize = ZERO * WORDS;\n      StructPointerCount ptrCount = ZERO * POINTERS;\n\n      if (canonical) {\n        for (auto i: kj::zeroTo(value.elementCount)) {\n          auto element = value.getStructElement(i);\n\n          // Truncate the data section\n          auto data = element.getDataSectionAsBlob();\n          auto end = data.end();\n          while (end > data.begin() && end[-1] == 0) --end;\n          dataSize = kj::max(dataSize, roundBytesUpToWords(\n              intervalLength(data.begin(), end, MAX_STUCT_DATA_WORDS * BYTES_PER_WORD)));\n\n          // Truncate pointer section\n          const WirePointer* ptr = element.pointers + element.pointerCount;\n          while (ptr > element.pointers && ptr[-1].isNull()) --ptr;\n          ptrCount = kj::max(ptrCount,\n              intervalLength(element.pointers, ptr, MAX_STRUCT_POINTER_COUNT));\n        }\n        auto newTotalSize = (dataSize + upgradeBound<uint64_t>(ptrCount) * WORDS_PER_POINTER)\n            / ELEMENTS * value.elementCount;\n        KJ_ASSERT(newTotalSize <= totalSize);  // we've only removed data!\n        totalSize = assumeMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(newTotalSize);\n      } else {\n        dataSize = declDataSize;\n        ptrCount = declPointerCount;\n      }\n\n      KJ_DASSERT(value.structDataSize % BITS_PER_WORD == ZERO * BITS);\n      word* ptr = allocate(ref, segment, capTable, totalSize + POINTER_SIZE_IN_WORDS,\n                           WirePointer::LIST, orphanArena);\n      ref->listRef.setInlineComposite(totalSize);\n\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, value.elementCount);\n      tag->structRef.set(dataSize, ptrCount);\n      word* dst = ptr + POINTER_SIZE_IN_WORDS;\n\n      const word* src = reinterpret_cast<const word*>(value.ptr);\n      for (auto i KJ_UNUSED: kj::zeroTo(value.elementCount)) {\n        copyMemory(dst, src, dataSize);\n        dst += dataSize;\n        src += declDataSize;\n\n        for (auto j: kj::zeroTo(ptrCount)) {\n          copyPointer(segment, capTable, reinterpret_cast<WirePointer*>(dst) + j,\n              value.segment, value.capTable, reinterpret_cast<const WirePointer*>(src) + j,\n              value.nestingLimit, nullptr, canonical);\n        }\n        dst += ptrCount * WORDS_PER_POINTER;\n        src += declPointerCount * WORDS_PER_POINTER;\n      }\n\n      return { segment, ptr };\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<word*> copyPointer(\n      SegmentBuilder* dstSegment, CapTableBuilder* dstCapTable, WirePointer* dst,\n      SegmentReader* srcSegment, CapTableReader* srcCapTable, const WirePointer* src,\n      int nestingLimit, BuilderArena* orphanArena = nullptr,\n      bool canonical = false)) {\n    return copyPointer(dstSegment, dstCapTable, dst,\n                       srcSegment, srcCapTable, src, src->target(srcSegment),\n                       nestingLimit, orphanArena, canonical);\n  }\n\n  static SegmentAnd<word*> copyPointer(\n      SegmentBuilder* dstSegment, CapTableBuilder* dstCapTable, WirePointer* dst,\n      SegmentReader* srcSegment, CapTableReader* srcCapTable, const WirePointer* src,\n      const word* srcTarget, int nestingLimit,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    // Deep-copy the object pointed to by src into dst.  It turns out we can't reuse\n    // readStructPointer(), etc. because they do type checking whereas here we want to accept any\n    // valid pointer.\n\n    if (src->isNull()) {\n    useDefault:\n      if (!dst->isNull()) {\n        zeroObject(dstSegment, dstCapTable, dst);\n        zeroMemory(dst);\n      }\n      return { dstSegment, nullptr };\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, WireHelpers::followFars(src, srcTarget, srcSegment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    switch (src->kind()) {\n      case WirePointer::STRUCT:\n        KJ_REQUIRE(nestingLimit > 0,\n              \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n          goto useDefault;\n        }\n\n        KJ_REQUIRE(boundsCheck(srcSegment, ptr, src->structRef.wordSize()),\n                   \"Message contained out-of-bounds struct pointer. \"\n                   OUT_OF_BOUNDS_ERROR_DETAIL) {\n          goto useDefault;\n        }\n        return setStructPointer(dstSegment, dstCapTable, dst,\n            StructReader(srcSegment, srcCapTable, ptr,\n                         reinterpret_cast<const WirePointer*>(ptr + src->structRef.dataSize.get()),\n                         src->structRef.dataSize.get() * BITS_PER_WORD,\n                         src->structRef.ptrCount.get(),\n                         nestingLimit - 1),\n            orphanArena, canonical);\n\n      case WirePointer::LIST: {\n        ElementSize elementSize = src->listRef.elementSize();\n\n        KJ_REQUIRE(nestingLimit > 0,\n              \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n          goto useDefault;\n        }\n\n        if (elementSize == ElementSize::INLINE_COMPOSITE) {\n          auto wordCount = src->listRef.inlineCompositeWordCount();\n          const WirePointer* tag = reinterpret_cast<const WirePointer*>(ptr);\n\n          KJ_REQUIRE(boundsCheck(srcSegment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                     \"Message contains out-of-bounds list pointer. \"\n                     OUT_OF_BOUNDS_ERROR_DETAIL) {\n            goto useDefault;\n          }\n\n          ptr += POINTER_SIZE_IN_WORDS;\n\n          KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n                     \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n            goto useDefault;\n          }\n\n          auto elementCount = tag->inlineCompositeListElementCount();\n          auto wordsPerElement = tag->structRef.wordSize() / ELEMENTS;\n\n          KJ_REQUIRE(wordsPerElement * upgradeBound<uint64_t>(elementCount) <= wordCount,\n                     \"INLINE_COMPOSITE list's elements overrun its word count.\") {\n            goto useDefault;\n          }\n\n          if (wordsPerElement * (ONE * ELEMENTS) == ZERO * WORDS) {\n            // Watch out for lists of zero-sized structs, which can claim to be arbitrarily large\n            // without having sent actual data.\n            KJ_REQUIRE(amplifiedRead(srcSegment, elementCount * (ONE * WORDS / ELEMENTS)),\n                       \"Message contains amplified list pointer.\") {\n              goto useDefault;\n            }\n          }\n\n          return setListPointer(dstSegment, dstCapTable, dst,\n              ListReader(srcSegment, srcCapTable, ptr,\n                         elementCount, wordsPerElement * BITS_PER_WORD,\n                         tag->structRef.dataSize.get() * BITS_PER_WORD,\n                         tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE,\n                         nestingLimit - 1),\n              orphanArena, canonical);\n        } else {\n          auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n          auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n          auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n          auto elementCount = src->listRef.elementCount();\n          auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(elementCount) * step);\n\n          KJ_REQUIRE(boundsCheck(srcSegment, ptr, wordCount),\n                     \"Message contains out-of-bounds list pointer. \"\n                     OUT_OF_BOUNDS_ERROR_DETAIL) {\n            goto useDefault;\n          }\n\n          if (elementSize == ElementSize::VOID) {\n            // Watch out for lists of void, which can claim to be arbitrarily large without having\n            // sent actual data.\n            KJ_REQUIRE(amplifiedRead(srcSegment, elementCount * (ONE * WORDS / ELEMENTS)),\n                       \"Message contains amplified list pointer.\") {\n              goto useDefault;\n            }\n          }\n\n          return setListPointer(dstSegment, dstCapTable, dst,\n              ListReader(srcSegment, srcCapTable, ptr, elementCount, step, dataSize, pointerCount,\n                         elementSize, nestingLimit - 1),\n              orphanArena, canonical);\n        }\n      }\n\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unexpected FAR pointer.\") {\n          goto useDefault;\n        }\n\n      case WirePointer::OTHER: {\n        KJ_REQUIRE(src->isCapability(), \"Unknown pointer type.\") {\n          goto useDefault;\n        }\n\n        if (canonical) {\n          KJ_FAIL_REQUIRE(\"Cannot create a canonical message with a capability\") {\n            break;\n          }\n        }\n#if !CAPNP_LITE\n        KJ_IF_MAYBE(cap, srcCapTable->extractCap(src->capRef.index.get())) {\n          setCapabilityPointer(dstSegment, dstCapTable, dst, kj::mv(*cap));\n          // Return dummy non-null pointer so OrphanBuilder doesn't end up null.\n          return { dstSegment, reinterpret_cast<word*>(1) };\n        } else {\n#endif  // !CAPNP_LITE\n          KJ_FAIL_REQUIRE(\"Message contained invalid capability pointer.\") {\n            goto useDefault;\n          }\n#if !CAPNP_LITE\n        }\n#endif  // !CAPNP_LITE\n      }\n    }\n\n    KJ_UNREACHABLE;\n  }\n\n  static void adopt(SegmentBuilder* segment, CapTableBuilder* capTable,\n                    WirePointer* ref, OrphanBuilder&& value) {\n    KJ_REQUIRE(value.segment == nullptr || value.segment->getArena() == segment->getArena(),\n               \"Adopted object must live in the same message.\");\n\n    if (!ref->isNull()) {\n      zeroObject(segment, capTable, ref);\n    }\n\n    if (value == nullptr) {\n      // Set null.\n      zeroMemory(ref);\n    } else if (value.tagAsPtr()->isPositional()) {\n      WireHelpers::transferPointer(segment, ref, value.segment, value.tagAsPtr(), value.location);\n    } else {\n      // FAR and OTHER pointers are position-independent, so we can just copy.\n      copyMemory(ref, value.tagAsPtr());\n    }\n\n    // Take ownership away from the OrphanBuilder.\n    zeroMemory(value.tagAsPtr());\n    value.location = nullptr;\n    value.segment = nullptr;\n  }\n\n  static OrphanBuilder disown(SegmentBuilder* segment, CapTableBuilder* capTable,\n                              WirePointer* ref) {\n    word* location;\n\n    if (ref->isNull()) {\n      location = nullptr;\n    } else if (ref->kind() == WirePointer::OTHER) {\n      KJ_REQUIRE(ref->isCapability(), \"Unknown pointer type.\") { break; }\n      location = reinterpret_cast<word*>(1);  // dummy so that it is non-null\n    } else {\n      WirePointer* refCopy = ref;\n      location = followFarsNoWritableCheck(refCopy, ref->target(), segment);\n    }\n\n    OrphanBuilder result(ref, segment, capTable, location);\n\n    if (!ref->isNull() && ref->isPositional()) {\n      result.tagAsPtr()->setKindForOrphan(ref->kind());\n    }\n\n    // Zero out the pointer that was disowned.\n    zeroMemory(ref);\n\n    return result;\n  }\n\n  // -----------------------------------------------------------------\n\n  static KJ_ALWAYS_INLINE(StructReader readStructPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* defaultValue,\n      int nestingLimit)) {\n    return readStructPointer(segment, capTable, ref, ref->target(segment),\n                             defaultValue, nestingLimit);\n  }\n\n  static KJ_ALWAYS_INLINE(StructReader readStructPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* refTarget,\n      const word* defaultValue, int nestingLimit)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return StructReader();\n      }\n      segment = nullptr;\n      ref = reinterpret_cast<const WirePointer*>(defaultValue);\n      refTarget = ref->target(segment);\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    KJ_REQUIRE(nestingLimit > 0,\n               \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n      goto useDefault;\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(ref->kind() == WirePointer::STRUCT,\n               \"Schema mismatch: Message contains non-struct pointer where struct pointer\"\n               \"was expected.\") {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(boundsCheck(segment, ptr, ref->structRef.wordSize()),\n               \"Message contained out-of-bounds struct pointer. \"\n               OUT_OF_BOUNDS_ERROR_DETAIL) {\n      goto useDefault;\n    }\n\n    return StructReader(\n        segment, capTable,\n        ptr, reinterpret_cast<const WirePointer*>(ptr + ref->structRef.dataSize.get()),\n        ref->structRef.dataSize.get() * BITS_PER_WORD,\n        ref->structRef.ptrCount.get(),\n        nestingLimit - 1);\n  }\n\n#if !CAPNP_LITE\n  static KJ_ALWAYS_INLINE(kj::Own<ClientHook> readCapabilityPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, int nestingLimit)) {\n    kj::Maybe<kj::Own<ClientHook>> maybeCap;\n\n    auto brokenCapFactory = readGlobalBrokenCapFactoryForLayoutCpp();\n\n    KJ_REQUIRE(brokenCapFactory != nullptr,\n               \"Trying to read capabilities without ever having created a capability context.  \"\n               \"To read capabilities from a message, you must imbue it with CapReaderContext, or \"\n               \"use the Cap'n Proto RPC system.\");\n\n    if (ref->isNull()) {\n      return brokenCapFactory->newNullCap();\n    } else if (!ref->isCapability()) {\n      KJ_FAIL_REQUIRE(\n          \"Schema mismatch: Message contains non-capability pointer where capability pointer was \"\n          \"expected.\") {\n        break;\n      }\n      return brokenCapFactory->newBrokenCap(\n          \"Calling capability extracted from a non-capability pointer.\");\n    } else KJ_IF_MAYBE(cap, capTable->extractCap(ref->capRef.index.get())) {\n      return kj::mv(*cap);\n    } else {\n      KJ_FAIL_REQUIRE(\"Message contains invalid capability pointer.\") {\n        break;\n      }\n      return brokenCapFactory->newBrokenCap(\"Calling invalid capability pointer.\");\n    }\n  }\n#endif  // !CAPNP_LITE\n\n  static KJ_ALWAYS_INLINE(ListReader readListPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* defaultValue,\n      ElementSize expectedElementSize, int nestingLimit, bool checkElementSize = true)) {\n    return readListPointer(segment, capTable, ref, ref->target(segment), defaultValue,\n                           expectedElementSize, nestingLimit, checkElementSize);\n  }\n\n  static KJ_ALWAYS_INLINE(ListReader readListPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* refTarget,\n      const word* defaultValue, ElementSize expectedElementSize, int nestingLimit,\n      bool checkElementSize = true)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListReader(expectedElementSize);\n      }\n      segment = nullptr;\n      ref = reinterpret_cast<const WirePointer*>(defaultValue);\n      refTarget = ref->target(segment);\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    KJ_REQUIRE(nestingLimit > 0,\n               \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n      goto useDefault;\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n               \"Schema mismatch: Message contains non-list pointer where list pointer was \"\n               \"expected.\") {\n      goto useDefault;\n    }\n\n    ElementSize elementSize = ref->listRef.elementSize();\n    if (elementSize == ElementSize::INLINE_COMPOSITE) {\n      auto wordCount = ref->listRef.inlineCompositeWordCount();\n\n      // An INLINE_COMPOSITE list points to a tag, which is formatted like a pointer.\n      const WirePointer* tag = reinterpret_cast<const WirePointer*>(ptr);\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                 \"Message contains out-of-bounds list pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n                 \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n        goto useDefault;\n      }\n\n      auto size = tag->inlineCompositeListElementCount();\n      auto wordsPerElement = tag->structRef.wordSize() / ELEMENTS;\n\n      KJ_REQUIRE(upgradeBound<uint64_t>(size) * wordsPerElement <= wordCount,\n                 \"INLINE_COMPOSITE list's elements overrun its word count.\") {\n        goto useDefault;\n      }\n\n      if (wordsPerElement * (ONE * ELEMENTS) == ZERO * WORDS) {\n        // Watch out for lists of zero-sized structs, which can claim to be arbitrarily large\n        // without having sent actual data.\n        KJ_REQUIRE(amplifiedRead(segment, size * (ONE * WORDS / ELEMENTS)),\n                   \"Message contains amplified list pointer.\") {\n          goto useDefault;\n        }\n      }\n\n      if (checkElementSize) {\n        // If a struct list was not expected, then presumably a non-struct list was upgraded to a\n        // struct list. We need to manipulate the pointer to point at the first field of the\n        // struct. Together with the `step` field, this will allow the struct list to be accessed\n        // as if it were a primitive list without branching.\n\n        // Check whether the size is compatible.\n        switch (expectedElementSize) {\n          case ElementSize::VOID:\n            break;\n\n          case ElementSize::BIT:\n            KJ_FAIL_REQUIRE(\n                \"Found struct list where bit list was expected; upgrading boolean lists to structs \"\n                \"is no longer supported.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES:\n            KJ_REQUIRE(tag->structRef.dataSize.get() > ZERO * WORDS,\n                       \"Schema mismatch: Expected a primitive list, but got a list of pointer-only \"\n                       \"structs.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::POINTER:\n            // We expected a list of pointers but got a list of structs.  Assuming the first field\n            // in the struct is the pointer we were looking for, we want to munge the pointer to\n            // point at the first element's pointer section.\n            ptr += tag->structRef.dataSize.get();\n            KJ_REQUIRE(tag->structRef.ptrCount.get() > ZERO * POINTERS,\n                       \"Schema mismatch: Expected a pointer list, but got a list of data-only \"\n                       \"structs.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::INLINE_COMPOSITE:\n            break;\n        }\n      }\n\n      return ListReader(\n          segment, capTable, ptr, size, wordsPerElement * BITS_PER_WORD,\n          tag->structRef.dataSize.get() * BITS_PER_WORD,\n          tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE,\n          nestingLimit - 1);\n\n    } else {\n      // This is a primitive or pointer list, but all such lists can also be interpreted as struct\n      // lists.  We need to compute the data size and pointer count for such structs.\n      auto dataSize = dataBitsPerElement(ref->listRef.elementSize()) * ELEMENTS;\n      auto pointerCount = pointersPerElement(ref->listRef.elementSize()) * ELEMENTS;\n      auto elementCount = ref->listRef.elementCount();\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n\n      auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(elementCount) * step);\n      KJ_REQUIRE(boundsCheck(segment, ptr, wordCount),\n            \"Message contains out-of-bounds list pointer. \"\n            OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      if (elementSize == ElementSize::VOID) {\n        // Watch out for lists of void, which can claim to be arbitrarily large without having sent\n        // actual data.\n        KJ_REQUIRE(amplifiedRead(segment, elementCount * (ONE * WORDS / ELEMENTS)),\n                   \"Message contains amplified list pointer.\") {\n          goto useDefault;\n        }\n      }\n\n      if (checkElementSize) {\n        if (elementSize == ElementSize::BIT && expectedElementSize != ElementSize::BIT) {\n          KJ_FAIL_REQUIRE(\n              \"Found bit list where struct list was expected; upgrading boolean lists to structs \"\n              \"is no longer supported.\") {\n            goto useDefault;\n          }\n        }\n\n        // Verify that the elements are at least as large as the expected type.  Note that if we\n        // expected INLINE_COMPOSITE, the expected sizes here will be zero, because bounds checking\n        // will be performed at field access time.  So this check here is for the case where we\n        // expected a list of some primitive or pointer type.\n\n        BitCount expectedDataBitsPerElement =\n            dataBitsPerElement(expectedElementSize) * ELEMENTS;\n        WirePointerCount expectedPointersPerElement =\n            pointersPerElement(expectedElementSize) * ELEMENTS;\n\n        KJ_REQUIRE(expectedDataBitsPerElement <= dataSize,\n                   \"Schema mismatch: Message contained list with incompatible element type.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(expectedPointersPerElement <= pointerCount,\n                   \"Schema mismatch: Message contained list with incompatible element type.\") {\n          goto useDefault;\n        }\n      }\n\n      return ListReader(segment, capTable, ptr, elementCount, step,\n                        dataSize, pointerCount, elementSize, nestingLimit - 1);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Reader readTextPointer(\n      SegmentReader* segment, const WirePointer* ref,\n      const void* defaultValue, ByteCount defaultSize)) {\n    return readTextPointer(segment, ref, ref->target(segment), defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Reader readTextPointer(\n      SegmentReader* segment, const WirePointer* ref, const word* refTarget,\n      const void* defaultValue, ByteCount defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr) defaultValue = \"\";\n      return Text::Reader(reinterpret_cast<const char*>(defaultValue),\n          unbound(defaultSize / BYTES));\n    } else {\n      const word* ptr;\n      KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n        ptr = p;\n      } else {\n        goto useDefault;\n      }\n\n      auto size = ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n                 \"Schema mismatch: Message contains non-list pointer where text was expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n                 \"Schema mismatch: Message contains list pointer of non-bytes where text was \"\n                 \"expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, roundBytesUpToWords(size)),\n                 \"Message contained out-of-bounds text pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(size > ZERO * BYTES, \"Message contains text that is not NUL-terminated.\") {\n        goto useDefault;\n      }\n\n      const char* cptr = reinterpret_cast<const char*>(ptr);\n      uint unboundedSize = unbound(size / BYTES) - 1;\n\n      KJ_REQUIRE(cptr[unboundedSize] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n        goto useDefault;\n      }\n\n      return Text::Reader(cptr, unboundedSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Reader readDataPointer(\n      SegmentReader* segment, const WirePointer* ref,\n      const void* defaultValue, BlobSize defaultSize)) {\n    return readDataPointer(segment, ref, ref->target(segment), defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Reader readDataPointer(\n      SegmentReader* segment, const WirePointer* ref, const word* refTarget,\n      const void* defaultValue, BlobSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      return Data::Reader(reinterpret_cast<const byte*>(defaultValue),\n          unbound(defaultSize / BYTES));\n    } else {\n      const word* ptr;\n      KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n        ptr = p;\n      } else {\n        goto useDefault;\n      }\n\n      if (KJ_UNLIKELY(ptr == nullptr)) {\n        // Already reported error.\n        goto useDefault;\n      }\n\n      auto size = ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n                 \"Schema mismatch: Message contains non-list pointer where data was expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n                 \"Schema mismatch: Message contains list pointer of non-bytes where data was \"\n                 \"expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, roundBytesUpToWords(size)),\n                 \"Message contained out-of-bounds data pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      return Data::Reader(reinterpret_cast<const byte*>(ptr), unbound(size / BYTES));\n    }\n  }\n};\n\n// =======================================================================================\n// PointerBuilder\n\nStructBuilder PointerBuilder::initStruct(StructSize size) {\n  return WireHelpers::initStructPointer(pointer, segment, capTable, size);\n}\n\nStructBuilder PointerBuilder::getStruct(StructSize size, const word* defaultValue) {\n  return WireHelpers::getWritableStructPointer(pointer, segment, capTable, size, defaultValue);\n}\n\nListBuilder PointerBuilder::initList(ElementSize elementSize, ElementCount elementCount) {\n  return WireHelpers::initListPointer(pointer, segment, capTable, elementCount, elementSize);\n}\n\nListBuilder PointerBuilder::initStructList(ElementCount elementCount, StructSize elementSize) {\n  return WireHelpers::initStructListPointer(pointer, segment, capTable, elementCount, elementSize);\n}\n\nListBuilder PointerBuilder::getList(ElementSize elementSize, const word* defaultValue) {\n  return WireHelpers::getWritableListPointer(pointer, segment, capTable, elementSize, defaultValue);\n}\n\nListBuilder PointerBuilder::getStructList(StructSize elementSize, const word* defaultValue) {\n  return WireHelpers::getWritableStructListPointer(\n      pointer, segment, capTable, elementSize, defaultValue);\n}\n\nListBuilder PointerBuilder::getListAnySize(const word* defaultValue) {\n  return WireHelpers::getWritableListPointerAnySize(pointer, segment, capTable, defaultValue);\n}\n\ntemplate <>\nText::Builder PointerBuilder::initBlob<Text>(ByteCount size) {\n  return WireHelpers::initTextPointer(pointer, segment, capTable,\n      assertMax<MAX_TEXT_SIZE>(size, ThrowOverflow())).value;\n}\ntemplate <>\nvoid PointerBuilder::setBlob<Text>(Text::Reader value) {\n  WireHelpers::setTextPointer(pointer, segment, capTable, value);\n}\ntemplate <>\nText::Builder PointerBuilder::getBlob<Text>(const void* defaultValue, ByteCount defaultSize) {\n  return WireHelpers::getWritableTextPointer(pointer, segment, capTable, defaultValue,\n      assertMax<MAX_TEXT_SIZE>(defaultSize, ThrowOverflow()));\n}\n\ntemplate <>\nData::Builder PointerBuilder::initBlob<Data>(ByteCount size) {\n  return WireHelpers::initDataPointer(pointer, segment, capTable,\n      assertMaxBits<BLOB_SIZE_BITS>(size, ThrowOverflow())).value;\n}\ntemplate <>\nvoid PointerBuilder::setBlob<Data>(Data::Reader value) {\n  WireHelpers::setDataPointer(pointer, segment, capTable, value);\n}\ntemplate <>\nData::Builder PointerBuilder::getBlob<Data>(const void* defaultValue, ByteCount defaultSize) {\n  return WireHelpers::getWritableDataPointer(pointer, segment, capTable, defaultValue,\n      assertMaxBits<BLOB_SIZE_BITS>(defaultSize, ThrowOverflow()));\n}\n\nvoid PointerBuilder::setStruct(const StructReader& value, bool canonical) {\n  WireHelpers::setStructPointer(segment, capTable, pointer, value, nullptr, canonical);\n}\n\nvoid PointerBuilder::setList(const ListReader& value, bool canonical) {\n  WireHelpers::setListPointer(segment, capTable, pointer, value, nullptr, canonical);\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> PointerBuilder::getCapability() {\n  return WireHelpers::readCapabilityPointer(\n      segment, capTable, pointer, kj::maxValue);\n}\n\nvoid PointerBuilder::setCapability(kj::Own<ClientHook>&& cap) {\n  WireHelpers::setCapabilityPointer(segment, capTable, pointer, kj::mv(cap));\n}\n#endif  // !CAPNP_LITE\n\nvoid PointerBuilder::adopt(OrphanBuilder&& value) {\n  WireHelpers::adopt(segment, capTable, pointer, kj::mv(value));\n}\n\nOrphanBuilder PointerBuilder::disown() {\n  return WireHelpers::disown(segment, capTable, pointer);\n}\n\nvoid PointerBuilder::clear() {\n  WireHelpers::zeroObject(segment, capTable, pointer);\n  WireHelpers::zeroMemory(pointer);\n}\n\nPointerType PointerBuilder::getPointerType() const {\n  if(pointer->isNull()) {\n    return PointerType::NULL_;\n  } else {\n    WirePointer* ptr = pointer;\n    SegmentBuilder* sgmt = segment;\n    WireHelpers::followFars(ptr, ptr->target(), sgmt);\n    switch(ptr->kind()) {\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"far pointer not followed?\");\n      case WirePointer::STRUCT:\n        return PointerType::STRUCT;\n      case WirePointer::LIST:\n        return PointerType::LIST;\n      case WirePointer::OTHER:\n        KJ_REQUIRE(ptr->isCapability(), \"unknown pointer type\");\n        return PointerType::CAPABILITY;\n    }\n    KJ_UNREACHABLE;\n  }\n}\n\nvoid PointerBuilder::transferFrom(PointerBuilder other) {\n  if (!pointer->isNull()) {\n    WireHelpers::zeroObject(segment, capTable, pointer);\n    WireHelpers::zeroMemory(pointer);\n  }\n  WireHelpers::transferPointer(segment, pointer, other.segment, other.pointer);\n  WireHelpers::zeroMemory(other.pointer);\n}\n\nvoid PointerBuilder::copyFrom(PointerReader other, bool canonical) {\n  if (other.pointer == nullptr) {\n    if (!pointer->isNull()) {\n      WireHelpers::zeroObject(segment, capTable, pointer);\n      WireHelpers::zeroMemory(pointer);\n    }\n  } else {\n    WireHelpers::copyPointer(segment, capTable, pointer,\n                             other.segment, other.capTable, other.pointer, other.nestingLimit,\n                             nullptr,\n                             canonical);\n  }\n}\n\nPointerReader PointerBuilder::asReader() const {\n  return PointerReader(segment, capTable, pointer, kj::maxValue);\n}\n\nBuilderArena* PointerBuilder::getArena() const {\n  return segment->getArena();\n}\n\nCapTableBuilder* PointerBuilder::getCapTable() {\n  return capTable;\n}\n\nPointerBuilder PointerBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// PointerReader\n\nPointerReader PointerReader::getRoot(SegmentReader* segment, CapTableReader* capTable,\n                                     const word* location, int nestingLimit) {\n  KJ_REQUIRE(WireHelpers::boundsCheck(segment, location, POINTER_SIZE_IN_WORDS),\n             \"Root location out-of-bounds.\") {\n    location = nullptr;\n  }\n\n  return PointerReader(segment, capTable,\n      reinterpret_cast<const WirePointer*>(location), nestingLimit);\n}\n\nStructReader PointerReader::getStruct(const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readStructPointer(segment, capTable, ref, defaultValue, nestingLimit);\n}\n\nListReader PointerReader::getList(ElementSize expectedElementSize, const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readListPointer(\n      segment, capTable, ref, defaultValue, expectedElementSize, nestingLimit);\n}\n\nListReader PointerReader::getListAnySize(const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readListPointer(\n      segment, capTable, ref, defaultValue, ElementSize::VOID /* dummy */, nestingLimit, false);\n}\n\ntemplate <>\nText::Reader PointerReader::getBlob<Text>(const void* defaultValue, ByteCount defaultSize) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readTextPointer(segment, ref, defaultValue, defaultSize);\n}\n\ntemplate <>\nData::Reader PointerReader::getBlob<Data>(const void* defaultValue, ByteCount defaultSize) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readDataPointer(segment, ref, defaultValue,\n      assertMaxBits<BLOB_SIZE_BITS>(defaultSize, ThrowOverflow()));\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> PointerReader::getCapability() const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readCapabilityPointer(segment, capTable, ref, nestingLimit);\n}\n#endif  // !CAPNP_LITE\n\nconst word* PointerReader::getUnchecked() const {\n  KJ_REQUIRE(segment == nullptr, \"getUncheckedPointer() only allowed on unchecked messages.\");\n  return reinterpret_cast<const word*>(pointer);\n}\n\nMessageSizeCounts PointerReader::targetSize() const {\n  return pointer == nullptr ? MessageSizeCounts { ZERO * WORDS, 0 }\n                            : WireHelpers::totalSize(segment, pointer, nestingLimit);\n}\n\nPointerType PointerReader::getPointerType() const {\n  if(pointer == nullptr || pointer->isNull()) {\n    return PointerType::NULL_;\n  } else {\n    const WirePointer* ptr = pointer;\n    const word* refTarget = ptr->target(segment);\n    SegmentReader* sgmt = segment;\n    if (WireHelpers::followFars(ptr, refTarget, sgmt) == nullptr) return PointerType::NULL_;\n    switch(ptr->kind()) {\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"far pointer not followed?\") { return PointerType::NULL_; }\n      case WirePointer::STRUCT:\n        return PointerType::STRUCT;\n      case WirePointer::LIST:\n        return PointerType::LIST;\n      case WirePointer::OTHER:\n        KJ_REQUIRE(ptr->isCapability(), \"unknown pointer type\") { return PointerType::NULL_; }\n        return PointerType::CAPABILITY;\n    }\n    KJ_UNREACHABLE;\n  }\n}\n\nkj::Maybe<Arena&> PointerReader::getArena() const {\n  return segment == nullptr ? nullptr : segment->getArena();\n}\n\nCapTableReader* PointerReader::getCapTable() {\n  return capTable;\n}\n\nPointerReader PointerReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool PointerReader::isCanonical(const word **readHead) {\n  if (!this->pointer) {\n    // The pointer is null, so we are canonical and do not read\n    return true;\n  }\n\n  if (!this->pointer->isPositional()) {\n    // The pointer is a FAR or OTHER pointer, and is non-canonical\n    return false;\n  }\n\n  switch (this->getPointerType()) {\n    case PointerType::NULL_:\n      // The pointer is null, we are canonical and do not read\n      return true;\n    case PointerType::STRUCT: {\n      bool dataTrunc = false, ptrTrunc = false;\n      auto structReader = this->getStruct(nullptr);\n      if (structReader.getDataSectionSize() == ZERO * BITS &&\n          structReader.getPointerSectionSize() == ZERO * POINTERS) {\n        return reinterpret_cast<const word*>(this->pointer) == structReader.getLocation();\n      } else {\n        // Fun fact: Once this call to isCanonical() returns, Clang may re-order the evaluation of\n        //   the && operators. In theory this is wrong because && is short-circuiting, but Clang\n        //   apparently sees that there are no side effects to the right of &&, so decides it is\n        //   safe to skip short-circuiting. It turns out, though, this is observable under\n        //   valgrind: if we don't initialize `dataTrunc` when declaring it above, then valgrind\n        //   reports \"Conditional jump or move depends on uninitialised value(s)\". Specifically\n        //   this happens in cases where structReader.isCanonical() returns false -- it is allowed\n        //   to skip initializing `dataTrunc` in that case. The short-circuiting && should mean\n        //   that we don't read `dataTrunc` in that case, except Clang's optimizations. Ultimately\n        //   the uninitialized read is fine because eventually the whole expression evaluates false\n        //   either way. But, to make valgrind happy, we initialize the bools above...\n        return structReader.isCanonical(readHead, readHead, &dataTrunc, &ptrTrunc) && dataTrunc && ptrTrunc;\n      }\n    }\n    case PointerType::LIST:\n      return this->getListAnySize(nullptr).isCanonical(readHead, pointer);\n    case PointerType::CAPABILITY:\n      KJ_FAIL_ASSERT(\"Capabilities are not positional\");\n  }\n  KJ_UNREACHABLE;\n}\n\n// =======================================================================================\n// StructBuilder\n\nvoid StructBuilder::clearAll() {\n  if (dataSize == ONE * BITS) {\n    setDataField<bool>(ONE * ELEMENTS, false);\n  } else {\n    WireHelpers::zeroMemory(reinterpret_cast<byte*>(data), dataSize / BITS_PER_BYTE);\n  }\n\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n}\n\nvoid StructBuilder::transferContentFrom(StructBuilder other) {\n  // Determine the amount of data the builders have in common.\n  auto sharedDataSize = kj::min(dataSize, other.dataSize);\n\n  if (dataSize > sharedDataSize) {\n    // Since the target is larger than the source, make sure to zero out the extra bits that the\n    // source doesn't have.\n    if (dataSize == ONE * BITS) {\n      setDataField<bool>(ZERO * ELEMENTS, false);\n    } else {\n      byte* unshared = reinterpret_cast<byte*>(data) + sharedDataSize / BITS_PER_BYTE;\n      // Note: this subtraction can't fail due to the if() above\n      WireHelpers::zeroMemory(unshared,\n          subtractChecked(dataSize, sharedDataSize, []() {}) / BITS_PER_BYTE);\n    }\n  }\n\n  // Copy over the shared part.\n  if (sharedDataSize == ONE * BITS) {\n    setDataField<bool>(ZERO * ELEMENTS, other.getDataField<bool>(ZERO * ELEMENTS));\n  } else {\n    WireHelpers::copyMemory(reinterpret_cast<byte*>(data),\n                            reinterpret_cast<byte*>(other.data),\n                            sharedDataSize / BITS_PER_BYTE);\n  }\n\n  // Zero out all pointers in the target.\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n\n  // Transfer the pointers.\n  auto sharedPointerCount = kj::min(pointerCount, other.pointerCount);\n  for (auto i: kj::zeroTo(sharedPointerCount)) {\n    WireHelpers::transferPointer(segment, pointers + i, other.segment, other.pointers + i);\n  }\n\n  // Zero out the pointers that were transferred in the source because it no longer has ownership.\n  // If the source had any extra pointers that the destination didn't have space for, we\n  // intentionally leave them be, so that they'll be cleaned up later.\n  WireHelpers::zeroMemory(other.pointers, sharedPointerCount);\n}\n\nvoid StructBuilder::copyContentFrom(StructReader other) {\n  // Determine the amount of data the builders have in common.\n  auto sharedDataSize = kj::min(dataSize, other.dataSize);\n  auto sharedPointerCount = kj::min(pointerCount, other.pointerCount);\n\n  if ((sharedDataSize > ZERO * BITS && other.data == data) ||\n      (sharedPointerCount > ZERO * POINTERS && other.pointers == pointers)) {\n    // At least one of the section pointers is pointing to ourself. Verify that the other is two\n    // (but ignore empty sections).\n    KJ_ASSERT((sharedDataSize == ZERO * BITS || other.data == data) &&\n              (sharedPointerCount == ZERO * POINTERS || other.pointers == pointers));\n    // So `other` appears to be a reader for this same struct. No coping is needed.\n    return;\n  }\n\n  if (dataSize > sharedDataSize) {\n    // Since the target is larger than the source, make sure to zero out the extra bits that the\n    // source doesn't have.\n    if (dataSize == ONE * BITS) {\n      setDataField<bool>(ZERO * ELEMENTS, false);\n    } else {\n      byte* unshared = reinterpret_cast<byte*>(data) + sharedDataSize / BITS_PER_BYTE;\n      WireHelpers::zeroMemory(unshared,\n          subtractChecked(dataSize, sharedDataSize, []() {}) / BITS_PER_BYTE);\n    }\n  }\n\n  // Copy over the shared part.\n  if (sharedDataSize == ONE * BITS) {\n    setDataField<bool>(ZERO * ELEMENTS, other.getDataField<bool>(ZERO * ELEMENTS));\n  } else {\n    WireHelpers::copyMemory(reinterpret_cast<byte*>(data),\n                            reinterpret_cast<const byte*>(other.data),\n                            sharedDataSize / BITS_PER_BYTE);\n  }\n\n  // Zero out all pointers in the target.\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n\n  // Copy the pointers.\n  for (auto i: kj::zeroTo(sharedPointerCount)) {\n    WireHelpers::copyPointer(segment, capTable, pointers + i,\n        other.segment, other.capTable, other.pointers + i, other.nestingLimit);\n  }\n}\n\nStructReader StructBuilder::asReader() const {\n  return StructReader(segment, capTable, data, pointers,\n      dataSize, pointerCount, kj::maxValue);\n}\n\nBuilderArena* StructBuilder::getArena() {\n  return segment->getArena();\n}\n\nCapTableBuilder* StructBuilder::getCapTable() {\n  return capTable;\n}\n\nStructBuilder StructBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// StructReader\n\nMessageSizeCounts StructReader::totalSize() const {\n  MessageSizeCounts result = {\n    WireHelpers::roundBitsUpToWords(dataSize) + pointerCount * WORDS_PER_POINTER, 0 };\n\n  for (auto i: kj::zeroTo(pointerCount)) {\n    result += WireHelpers::totalSize(segment, pointers + i, nestingLimit);\n  }\n\n  if (segment != nullptr) {\n    // This traversal should not count against the read limit, because it's highly likely that\n    // the caller is going to traverse the object again, e.g. to copy it.\n    segment->unread(result.wordCount);\n  }\n\n  return result;\n}\n\nkj::Array<word> StructReader::canonicalize() {\n  auto size = totalSize().wordCount + POINTER_SIZE_IN_WORDS;\n  kj::Array<word> backing = kj::heapArray<word>(unbound(size / WORDS));\n  WireHelpers::zeroMemory(backing.asPtr());\n  FlatMessageBuilder builder(backing);\n  _::PointerHelpers<AnyPointer>::getInternalBuilder(builder.initRoot<AnyPointer>()).setStruct(*this, true);\n  KJ_ASSERT(builder.isCanonical());\n  auto output = builder.getSegmentsForOutput()[0];\n  kj::Array<word> trunc = kj::heapArray<word>(output.size());\n  WireHelpers::copyMemory(trunc.begin(), output);\n  return trunc;\n}\n\nCapTableReader* StructReader::getCapTable() {\n  return capTable;\n}\n\nStructReader StructReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool StructReader::isCanonical(const word **readHead,\n                               const word **ptrHead,\n                               bool *dataTrunc,\n                               bool *ptrTrunc) {\n  if (this->getLocation() != *readHead) {\n    // Our target area is not at the readHead, preorder fails\n    return false;\n  }\n\n  if (this->getDataSectionSize() % BITS_PER_WORD != ZERO * BITS) {\n    // Using legacy non-word-size structs, reject\n    return false;\n  }\n  auto dataSize = this->getDataSectionSize() / BITS_PER_WORD;\n\n  // Mark whether the struct is properly truncated\n  KJ_IF_MAYBE(diff, trySubtract(dataSize, ONE * WORDS)) {\n    *dataTrunc = this->getDataField<uint64_t>(*diff / WORDS * ELEMENTS) != 0;\n  } else {\n    // Data segment empty.\n    *dataTrunc = true;\n  }\n\n  KJ_IF_MAYBE(diff, trySubtract(this->pointerCount, ONE * POINTERS)) {\n    *ptrTrunc  = !this->getPointerField(*diff).isNull();\n  } else {\n    *ptrTrunc = true;\n  }\n\n  // Advance the read head\n  *readHead += (dataSize + (this->pointerCount * WORDS_PER_POINTER));\n\n  // Check each pointer field for canonicity\n  for (auto ptrIndex: kj::zeroTo(this->pointerCount)) {\n    if (!this->getPointerField(ptrIndex).isCanonical(ptrHead)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n// =======================================================================================\n// ListBuilder\n\nText::Builder ListBuilder::asText() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Expected Text, got list of non-bytes.\") {\n    return Text::Builder();\n  }\n\n  size_t size = unbound(elementCount / ELEMENTS);\n\n  KJ_REQUIRE(size > 0, \"Message contains text that is not NUL-terminated.\") {\n    return Text::Builder();\n  }\n\n  char* cptr = reinterpret_cast<char*>(ptr);\n  --size;  // NUL terminator\n\n  KJ_REQUIRE(cptr[size] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n    return Text::Builder();\n  }\n\n  return Text::Builder(cptr, size);\n}\n\nData::Builder ListBuilder::asData() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Expected Text, got list of non-bytes.\") {\n    return Data::Builder();\n  }\n\n  return Data::Builder(reinterpret_cast<byte*>(ptr), unbound(elementCount / ELEMENTS));\n}\n\nStructBuilder ListBuilder::getStructElement(ElementCount index) {\n  auto indexBit = upgradeBound<uint64_t>(index) * step;\n  byte* structData = ptr + indexBit / BITS_PER_BYTE;\n  KJ_DASSERT(indexBit % BITS_PER_BYTE == ZERO * BITS);\n  return StructBuilder(segment, capTable, structData,\n      reinterpret_cast<WirePointer*>(structData + structDataSize / BITS_PER_BYTE),\n      structDataSize, structPointerCount);\n}\n\nListReader ListBuilder::asReader() const {\n  return ListReader(segment, capTable, ptr, elementCount, step, structDataSize, structPointerCount,\n                    elementSize, kj::maxValue);\n}\n\nBuilderArena* ListBuilder::getArena() {\n  return segment->getArena();\n}\n\nCapTableBuilder* ListBuilder::getCapTable() {\n  return capTable;\n}\n\nListBuilder ListBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// ListReader\n\nText::Reader ListReader::asText() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected Text, got list of non-bytes.\") {\n    return Text::Reader();\n  }\n\n  size_t size = unbound(elementCount / ELEMENTS);\n\n  KJ_REQUIRE(size > 0, \"Message contains text that is not NUL-terminated.\") {\n    return Text::Reader();\n  }\n\n  const char* cptr = reinterpret_cast<const char*>(ptr);\n  --size;  // NUL terminator\n\n  KJ_REQUIRE(cptr[size] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n    return Text::Reader();\n  }\n\n  return Text::Reader(cptr, size);\n}\n\nData::Reader ListReader::asData() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected Text, got list of non-bytes.\") {\n    return Data::Reader();\n  }\n\n  return Data::Reader(reinterpret_cast<const byte*>(ptr), unbound(elementCount / ELEMENTS));\n}\n\nkj::ArrayPtr<const byte> ListReader::asRawBytes() const {\n  KJ_REQUIRE(structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected data only, got pointers.\") {\n    return kj::ArrayPtr<const byte>();\n  }\n\n  return arrayPtr(reinterpret_cast<const byte*>(ptr),\n      WireHelpers::roundBitsUpToBytes(\n          upgradeBound<uint64_t>(elementCount) * (structDataSize / ELEMENTS)));\n}\n\nStructReader ListReader::getStructElement(ElementCount index) const {\n  KJ_REQUIRE(nestingLimit > 0,\n             \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n    return StructReader();\n  }\n\n  auto indexBit = upgradeBound<uint64_t>(index) * step;\n  const byte* structData = ptr + indexBit / BITS_PER_BYTE;\n  const WirePointer* structPointers =\n      reinterpret_cast<const WirePointer*>(structData + structDataSize / BITS_PER_BYTE);\n\n  KJ_DASSERT(indexBit % BITS_PER_BYTE == ZERO * BITS);\n  return StructReader(\n      segment, capTable, structData, structPointers,\n      structDataSize, structPointerCount,\n      nestingLimit - 1);\n}\n\nMessageSizeCounts ListReader::totalSize() const {\n  // TODO(cleanup): This is kind of a lot of logic duplicated from WireHelpers::totalSize(), but\n  //   it's unclear how to share it effectively.\n\n  MessageSizeCounts result = { ZERO * WORDS, 0 };\n\n  switch (elementSize) {\n    case ElementSize::VOID:\n      // Nothing.\n      break;\n    case ElementSize::BIT:\n    case ElementSize::BYTE:\n    case ElementSize::TWO_BYTES:\n    case ElementSize::FOUR_BYTES:\n    case ElementSize::EIGHT_BYTES:\n      result.addWords(WireHelpers::roundBitsUpToWords(\n          upgradeBound<uint64_t>(elementCount) * dataBitsPerElement(elementSize)));\n      break;\n    case ElementSize::POINTER: {\n      auto count = elementCount * (POINTERS / ELEMENTS);\n      result.addWords(count * WORDS_PER_POINTER);\n\n      for (auto i: kj::zeroTo(count)) {\n        result += WireHelpers::totalSize(segment, reinterpret_cast<const WirePointer*>(ptr) + i,\n                                         nestingLimit);\n      }\n      break;\n    }\n    case ElementSize::INLINE_COMPOSITE: {\n      // Don't forget to count the tag word.\n      auto wordSize = upgradeBound<uint64_t>(elementCount) * step / BITS_PER_WORD;\n      result.addWords(wordSize + POINTER_SIZE_IN_WORDS);\n\n      if (structPointerCount > ZERO * POINTERS) {\n        const word* pos = reinterpret_cast<const word*>(ptr);\n        for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n          pos += structDataSize / BITS_PER_WORD;\n\n          for (auto j KJ_UNUSED: kj::zeroTo(structPointerCount)) {\n            result += WireHelpers::totalSize(segment, reinterpret_cast<const WirePointer*>(pos),\n                                             nestingLimit);\n            pos += POINTER_SIZE_IN_WORDS;\n          }\n        }\n      }\n      break;\n    }\n  }\n\n  if (segment != nullptr) {\n    // This traversal should not count against the read limit, because it's highly likely that\n    // the caller is going to traverse the object again, e.g. to copy it.\n    segment->unread(result.wordCount);\n  }\n\n  return result;\n}\n\nCapTableReader* ListReader::getCapTable() {\n  return capTable;\n}\n\nListReader ListReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool ListReader::isCanonical(const word **readHead, const WirePointer *ref) {\n  switch (this->getElementSize()) {\n    case ElementSize::INLINE_COMPOSITE: {\n      *readHead += 1;\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        // The next word to read is the tag word, but the pointer is in\n        // front of it, so our check is slightly different\n        return false;\n      }\n      if (this->structDataSize % BITS_PER_WORD != ZERO * BITS) {\n        return false;\n      }\n      auto elementSize = StructSize(this->structDataSize / BITS_PER_WORD,\n                                    this->structPointerCount).total() / ELEMENTS;\n      auto totalSize = upgradeBound<uint64_t>(this->elementCount) * elementSize;\n      if (totalSize != ref->listRef.inlineCompositeWordCount()) {\n        return false;\n      }\n      if (elementSize == ZERO * WORDS / ELEMENTS) {\n        return true;\n      }\n      auto listEnd = *readHead + totalSize;\n      auto pointerHead = listEnd;\n      bool listDataTrunc = false;\n      bool listPtrTrunc = false;\n      for (auto ec: kj::zeroTo(this->elementCount)) {\n        bool dataTrunc, ptrTrunc;\n        if (!this->getStructElement(ec).isCanonical(readHead,\n                                                    &pointerHead,\n                                                    &dataTrunc,\n                                                    &ptrTrunc)) {\n          return false;\n        }\n        listDataTrunc |= dataTrunc;\n        listPtrTrunc  |= ptrTrunc;\n      }\n      KJ_REQUIRE(*readHead == listEnd, *readHead, listEnd);\n      *readHead = pointerHead;\n      return listDataTrunc && listPtrTrunc;\n    }\n    case ElementSize::POINTER: {\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        return false;\n      }\n      *readHead += this->elementCount * (POINTERS / ELEMENTS) * WORDS_PER_POINTER;\n      for (auto ec: kj::zeroTo(this->elementCount)) {\n        if (!this->getPointerElement(ec).isCanonical(readHead)) {\n          return false;\n        }\n      }\n      return true;\n    }\n    default: {\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        return false;\n      }\n\n      auto bitSize = upgradeBound<uint64_t>(this->elementCount) *\n                     dataBitsPerElement(this->elementSize);\n      auto truncatedByteSize = bitSize / BITS_PER_BYTE;\n      auto byteReadHead = reinterpret_cast<const uint8_t*>(*readHead) + truncatedByteSize;\n      auto readHeadEnd = *readHead + WireHelpers::roundBitsUpToWords(bitSize);\n\n      auto leftoverBits = bitSize % BITS_PER_BYTE;\n      if (leftoverBits > ZERO * BITS) {\n        auto mask = ~((1 << unbound(leftoverBits / BITS)) - 1);\n\n        if (mask & *byteReadHead) {\n          return false;\n        }\n        byteReadHead += 1;\n      }\n\n      while (byteReadHead != reinterpret_cast<const uint8_t*>(readHeadEnd)) {\n        if (*byteReadHead != 0) {\n          return false;\n        }\n        byteReadHead += 1;\n      }\n\n      *readHead = readHeadEnd;\n      return true;\n    }\n  }\n  KJ_UNREACHABLE;\n}\n\n// =======================================================================================\n// OrphanBuilder\n\nOrphanBuilder OrphanBuilder::initStruct(\n    BuilderArena* arena, CapTableBuilder* capTable, StructSize size) {\n  OrphanBuilder result;\n  StructBuilder builder = WireHelpers::initStructPointer(\n      result.tagAsPtr(), nullptr, capTable, size, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initList(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementCount elementCount, ElementSize elementSize) {\n  OrphanBuilder result;\n  ListBuilder builder = WireHelpers::initListPointer(\n      result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initStructList(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementCount elementCount, StructSize elementSize) {\n  OrphanBuilder result;\n  ListBuilder builder = WireHelpers::initStructListPointer(\n      result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initText(\n    BuilderArena* arena, CapTableBuilder* capTable, ByteCount size) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::initTextPointer(result.tagAsPtr(), nullptr, capTable,\n      assertMax<MAX_TEXT_SIZE>(size, ThrowOverflow()), arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initData(\n    BuilderArena* arena, CapTableBuilder* capTable, ByteCount size) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::initDataPointer(result.tagAsPtr(), nullptr, capTable,\n      assertMaxBits<BLOB_SIZE_BITS>(size), arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, StructReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setStructPointer(\n      nullptr, capTable, result.tagAsPtr(), copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, ListReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setListPointer(\n      nullptr, capTable, result.tagAsPtr(), copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, PointerReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::copyPointer(\n      nullptr, capTable, result.tagAsPtr(),\n      copyFrom.segment, copyFrom.capTable, copyFrom.pointer, copyFrom.nestingLimit, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, Text::Reader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setTextPointer(\n      result.tagAsPtr(), nullptr, capTable, copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, Data::Reader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setDataPointer(\n      result.tagAsPtr(), nullptr, capTable, copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\n#if !CAPNP_LITE\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, kj::Own<ClientHook> copyFrom) {\n  OrphanBuilder result;\n  WireHelpers::setCapabilityPointer(nullptr, capTable, result.tagAsPtr(), kj::mv(copyFrom));\n  result.segment = arena->getSegment(SegmentId(0));\n  result.capTable = capTable;\n  result.location = &result.tag;  // dummy to make location non-null\n  return result;\n}\n#endif  // !CAPNP_LITE\n\nOrphanBuilder OrphanBuilder::concat(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementSize elementSize, StructSize structSize,\n    kj::ArrayPtr<const ListReader> lists) {\n  KJ_REQUIRE(lists.size() > 0, \"Can't concat empty list \");\n\n  // Find the overall element count and size.\n  ListElementCount elementCount = ZERO * ELEMENTS;\n  for (auto& list: lists) {\n    elementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount + list.elementCount,\n        []() { KJ_FAIL_REQUIRE(\"concatenated list exceeds list size limit\"); });\n    if (list.elementSize != elementSize) {\n      // If element sizes don't all match, upgrade to struct list.\n      KJ_REQUIRE(list.elementSize != ElementSize::BIT && elementSize != ElementSize::BIT,\n                 \"can't upgrade bit lists to struct lists\");\n      elementSize = ElementSize::INLINE_COMPOSITE;\n    }\n    structSize.data = kj::max(structSize.data,\n        WireHelpers::roundBitsUpToWords(list.structDataSize));\n    structSize.pointers = kj::max(structSize.pointers, list.structPointerCount);\n  }\n\n  // Allocate the list.\n  OrphanBuilder result;\n  ListBuilder builder = (elementSize == ElementSize::INLINE_COMPOSITE)\n      ? WireHelpers::initStructListPointer(\n          result.tagAsPtr(), nullptr, capTable, elementCount, structSize, arena)\n      : WireHelpers::initListPointer(\n          result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n\n  // Copy elements.\n  switch (elementSize) {\n    case ElementSize::INLINE_COMPOSITE: {\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.getStructElement(pos).copyContentFrom(list.getStructElement(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    case ElementSize::POINTER: {\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.getPointerElement(pos).copyFrom(list.getPointerElement(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    case ElementSize::BIT: {\n      // It's difficult to memcpy() bits since a list could start or end mid-byte. For now we\n      // do a slow, naive loop. Probably no one will ever care.\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.setDataElement<bool>(pos, list.getDataElement<bool>(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    default: {\n      // We know all the inputs are primitives with identical size because otherwise we would have\n      // chosen INLINE_COMPOSITE. Therefore, we can safely use memcpy() here instead of copying\n      // each element manually.\n      byte* target = builder.ptr;\n      auto step = builder.step / BITS_PER_BYTE;\n      for (auto& list: lists) {\n        auto count = step * upgradeBound<uint64_t>(list.size());\n        WireHelpers::copyMemory(target, list.ptr, assumeBits<SEGMENT_WORD_COUNT_BITS>(count));\n        target += count;\n      }\n      break;\n    }\n  }\n\n  // Return orphan.\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::referenceExternalData(BuilderArena* arena, Data::Reader data) {\n  KJ_REQUIRE(reinterpret_cast<uintptr_t>(data.begin()) % sizeof(void*) == 0,\n             \"Cannot referenceExternalData() that is not aligned.\");\n\n  auto checkedSize = assertMaxBits<BLOB_SIZE_BITS>(bounded(data.size()));\n  auto wordCount = WireHelpers::roundBytesUpToWords(checkedSize * BYTES);\n  kj::ArrayPtr<const word> words(reinterpret_cast<const word*>(data.begin()),\n                                 unbound(wordCount / WORDS));\n\n  OrphanBuilder result;\n  result.tagAsPtr()->setKindForOrphan(WirePointer::LIST);\n  result.tagAsPtr()->listRef.set(ElementSize::BYTE, checkedSize * ELEMENTS);\n  result.segment = arena->addExternalSegment(words);\n\n  // External data cannot possibly contain capabilities.\n  result.capTable = nullptr;\n\n  // const_cast OK here because we will check whether the segment is writable when we try to get\n  // a builder.\n  result.location = const_cast<word*>(words.begin());\n\n  return result;\n}\n\nStructBuilder OrphanBuilder::asStruct(StructSize size) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  StructBuilder result = WireHelpers::getWritableStructPointer(\n      tagAsPtr(), location, segment, capTable, size, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = reinterpret_cast<word*>(result.data);\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asList(ElementSize elementSize) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableListPointer(\n      tagAsPtr(), location, segment, capTable, elementSize, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  // (Actually, currently this is not true for primitive lists, but let's not turn into a bug if\n  // it changes!)\n  location = result.getLocation();\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asStructList(StructSize elementSize) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableStructListPointer(\n      tagAsPtr(), location, segment, capTable, elementSize, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = result.getLocation();\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asListAnySize() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableListPointerAnySize(\n      tagAsPtr(), location, segment, capTable, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = result.getLocation();\n\n  return result;\n}\n\nText::Builder OrphanBuilder::asText() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  // Never relocates.\n  return WireHelpers::getWritableTextPointer(\n      tagAsPtr(), location, segment, capTable, nullptr, ZERO * BYTES);\n}\n\nData::Builder OrphanBuilder::asData() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  // Never relocates.\n  return WireHelpers::getWritableDataPointer(\n      tagAsPtr(), location, segment, capTable, nullptr, ZERO * BYTES);\n}\n\nStructReader OrphanBuilder::asStructReader(StructSize size) const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readStructPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, kj::maxValue);\n}\n\nListReader OrphanBuilder::asListReader(ElementSize elementSize) const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readListPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, elementSize, kj::maxValue);\n}\n\nListReader OrphanBuilder::asListReaderAnySize() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readListPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, ElementSize::VOID /* dummy */,\n      kj::maxValue);\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> OrphanBuilder::asCapability() const {\n  return WireHelpers::readCapabilityPointer(segment, capTable, tagAsPtr(), kj::maxValue);\n}\n#endif  // !CAPNP_LITE\n\nText::Reader OrphanBuilder::asTextReader() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readTextPointer(segment, tagAsPtr(), location, nullptr, ZERO * BYTES);\n}\n\nData::Reader OrphanBuilder::asDataReader() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readDataPointer(segment, tagAsPtr(), location, nullptr, ZERO * BYTES);\n}\n\nbool OrphanBuilder::truncate(ElementCount uncheckedSize, bool isText) {\n  ListElementCount size = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(uncheckedSize,\n      []() { KJ_FAIL_REQUIRE(\"requested list size is too large\"); });\n\n  WirePointer* ref = tagAsPtr();\n  SegmentBuilder* segment = this->segment;\n\n  word* target = WireHelpers::followFars(ref, location, segment);\n\n  if (ref->isNull()) {\n    // We don't know the right element size, so we can't resize this list.\n    return size == ZERO * ELEMENTS;\n  }\n\n  KJ_REQUIRE(ref->kind() == WirePointer::LIST, \"Schema mismatch: Can't truncate non-list.\") {\n    return false;\n  }\n\n  if (isText) {\n    // Add space for the NUL terminator.\n    size = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(size + ONE * ELEMENTS,\n        []() { KJ_FAIL_REQUIRE(\"requested list size is too large\"); });\n  }\n\n  auto elementSize = ref->listRef.elementSize();\n\n  if (elementSize == ElementSize::INLINE_COMPOSITE) {\n    auto oldWordCount = ref->listRef.inlineCompositeWordCount();\n\n    WirePointer* tag = reinterpret_cast<WirePointer*>(target);\n    ++target;\n    KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n               \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n      return false;\n    }\n    StructSize structSize(tag->structRef.dataSize.get(), tag->structRef.ptrCount.get());\n    auto elementStep = structSize.total() / ELEMENTS;\n\n    auto oldSize = tag->inlineCompositeListElementCount();\n\n    SegmentWordCount sizeWords = assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n        upgradeBound<uint64_t>(size) * elementStep,\n        []() { KJ_FAIL_ASSERT(\"requested list size too large to fit in message segment\"); });\n    SegmentWordCount oldSizeWords = assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n        upgradeBound<uint64_t>(oldSize) * elementStep,\n        []() { KJ_FAIL_ASSERT(\"prior to truncate, list is larger than max segment size?\"); });\n\n    word* newEndWord = target + sizeWords;\n    word* oldEndWord = target + oldWordCount;\n\n    if (size <= oldSize) {\n      // Zero the trailing elements.\n      for (auto i: kj::range(size, oldSize)) {\n        // assumeBits() safe because we checked that both sizeWords and oldSizeWords are in-range\n        // above.\n        WireHelpers::zeroObject(segment, capTable, tag, target +\n            assumeBits<SEGMENT_WORD_COUNT_BITS>(upgradeBound<uint64_t>(i) * elementStep));\n      }\n      ref->listRef.setInlineComposite(sizeWords);\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else if (newEndWord <= oldEndWord) {\n      // Apparently the old list was over-allocated? The word count is more than needed to store\n      // the elements. This is \"valid\" but shouldn't happen in practice unless someone is toying\n      // with us.\n      word* expectedEnd = target + oldSizeWords;\n      KJ_ASSERT(newEndWord >= expectedEnd);\n      WireHelpers::zeroMemory(expectedEnd,\n          intervalLength(expectedEnd, newEndWord, MAX_SEGMENT_WORDS));\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n    } else {\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the new memory is already zero'd.\n        ref->listRef.setInlineComposite(sizeWords);\n        tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initStructList(segment->getArena(), capTable, size, structSize);\n\n        ListBuilder newList = replacement.asStructList(structSize);\n        for (auto i: kj::zeroTo(oldSize)) {\n          // assumeBits() safe because we checked that both sizeWords and oldSizeWords are in-range\n          // above.\n          word* element = target +\n              assumeBits<SEGMENT_WORD_COUNT_BITS>(upgradeBound<uint64_t>(i) * elementStep);\n          newList.getStructElement(i).transferContentFrom(\n              StructBuilder(segment, capTable, element,\n                            reinterpret_cast<WirePointer*>(element + structSize.data),\n                            structSize.data * BITS_PER_WORD, structSize.pointers));\n        }\n\n        *this = kj::mv(replacement);\n      }\n    }\n  } else if (elementSize == ElementSize::POINTER) {\n    // TODO(cleanup): GCC won't let me declare this constexpr, claiming POINTERS is not constexpr,\n    //   but it is?\n    const auto POINTERS_PER_ELEMENT = ONE * POINTERS / ELEMENTS;\n\n    auto oldSize = ref->listRef.elementCount();\n    word* newEndWord = target + size * POINTERS_PER_ELEMENT * WORDS_PER_POINTER;\n    word* oldEndWord = target + oldSize * POINTERS_PER_ELEMENT * WORDS_PER_POINTER;\n\n    if (size <= oldSize) {\n      // Zero the trailing elements.\n      for (WirePointer* element = reinterpret_cast<WirePointer*>(newEndWord);\n           element < reinterpret_cast<WirePointer*>(oldEndWord); ++element) {\n        WireHelpers::zeroPointerAndFars(segment, element);\n      }\n      ref->listRef.set(ElementSize::POINTER, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else {\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the new memory is already zero'd.\n        ref->listRef.set(ElementSize::POINTER, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initList(\n            segment->getArena(), capTable, size, ElementSize::POINTER);\n        ListBuilder newList = replacement.asList(ElementSize::POINTER);\n        WirePointer* oldPointers = reinterpret_cast<WirePointer*>(target);\n        for (auto i: kj::zeroTo(oldSize)) {\n          newList.getPointerElement(i).transferFrom(\n              PointerBuilder(segment, capTable, oldPointers + i * POINTERS_PER_ELEMENT));\n        }\n        *this = kj::mv(replacement);\n      }\n    }\n  } else {\n    auto oldSize = ref->listRef.elementCount();\n    auto step = dataBitsPerElement(elementSize);\n    const auto MAX_STEP_BYTES = ONE * WORDS / ELEMENTS * BYTES_PER_WORD;\n    word* newEndWord = target + WireHelpers::roundBitsUpToWords(\n        upgradeBound<uint64_t>(size) * step);\n    word* oldEndWord = target + WireHelpers::roundBitsUpToWords(\n        upgradeBound<uint64_t>(oldSize) * step);\n\n    if (size <= oldSize) {\n      // When truncating text, we want to set the null terminator as well, so we'll do our zeroing\n      // at the byte level.\n      byte* begin = reinterpret_cast<byte*>(target);\n      byte* newEndByte = begin + WireHelpers::roundBitsUpToBytes(\n          upgradeBound<uint64_t>(size) * step) - isText;\n      byte* oldEndByte = reinterpret_cast<byte*>(oldEndWord);\n\n      WireHelpers::zeroMemory(newEndByte,\n          intervalLength(newEndByte, oldEndByte, MAX_LIST_ELEMENTS * MAX_STEP_BYTES));\n      ref->listRef.set(elementSize, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else {\n      // We're trying to extend, not truncate.\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the memory is already zero'd.\n        ref->listRef.set(elementSize, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initList(segment->getArena(), capTable, size, elementSize);\n        ListBuilder newList = replacement.asList(elementSize);\n        auto words = WireHelpers::roundBitsUpToWords(\n            dataBitsPerElement(elementSize) * upgradeBound<uint64_t>(oldSize));\n        WireHelpers::copyMemory(reinterpret_cast<word*>(newList.ptr), target, words);\n        *this = kj::mv(replacement);\n      }\n    }\n  }\n\n  return true;\n}\n\nvoid OrphanBuilder::truncate(ElementCount size, ElementSize elementSize) {\n  if (!truncate(size, false)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initList(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size), elementSize);\n  }\n}\n\nvoid OrphanBuilder::truncate(ElementCount size, StructSize elementSize) {\n  if (!truncate(size, false)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initStructList(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size), elementSize);\n  }\n}\n\nvoid OrphanBuilder::truncateText(ElementCount size) {\n  if (!truncate(size, true)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initText(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size) * (ONE * BYTES / ELEMENTS));\n  }\n}\n\nvoid OrphanBuilder::euthanize() {\n  // Carefully catch any exceptions and rethrow them as recoverable exceptions since we may be in\n  // a destructor.\n  auto exception = kj::runCatchingExceptions([&]() {\n    if (tagAsPtr()->isPositional()) {\n      WireHelpers::zeroObject(segment, capTable, tagAsPtr(), location);\n    } else {\n      WireHelpers::zeroObject(segment, capTable, tagAsPtr());\n    }\n\n    WireHelpers::zeroMemory(&tag, ONE * WORDS);\n    segment = nullptr;\n    location = nullptr;\n  });\n\n  KJ_IF_MAYBE(e, exception) {\n    kj::getExceptionCallback().onRecoverableException(kj::mv(*e));\n  }\n}\n\n}  // namespace _ (private)\n}  // namespace capnp\n", "// Copyright (c) 2013-2016 Sandstorm Development Group, Inc. and contributors\n// Licensed under the MIT License:\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n// This file is NOT intended for use by clients, except in generated code.\n//\n// This file defines low-level, non-type-safe classes for traversing the Cap'n Proto memory layout\n// (which is also its wire format).  Code generated by the Cap'n Proto compiler uses these classes,\n// as does other parts of the Cap'n proto library which provide a higher-level interface for\n// dynamic introspection.\n\n#pragma once\n\n#include <kj/common.h>\n#include <kj/memory.h>\n#include \"common.h\"\n#include \"blob.h\"\n#include \"endian.h\"\n#include <kj/windows-sanity.h>  // work-around macro conflict with `VOID`\n\nCAPNP_BEGIN_HEADER\n\n#if (defined(__mips__) || defined(__hppa__)) && !defined(CAPNP_CANONICALIZE_NAN)\n#define CAPNP_CANONICALIZE_NAN 1\n// Explicitly detect NaNs and canonicalize them to the quiet NaN value as would be returned by\n// __builtin_nan(\"\") on systems implementing the IEEE-754 recommended (but not required) NaN\n// signalling/quiet differentiation (such as x86).  Unfortunately, some architectures -- in\n// particular, MIPS -- represent quiet vs. signalling nans differently than the rest of the world.\n// Canonicalizing them makes output consistent (which is important!), but hurts performance\n// slightly.\n//\n// Note that trying to convert MIPS NaNs to standard NaNs without losing data doesn't work.\n// Signaling vs. quiet is indicated by a bit, with the meaning being the opposite on MIPS vs.\n// everyone else.  It would be great if we could just flip that bit, but we can't, because if the\n// significand is all-zero, then the value is infinity rather than NaN.  This means that on most\n// machines, where the bit indicates quietness, there is one more quiet NaN value than signalling\n// NaN value, whereas on MIPS there is one more sNaN than qNaN, and thus there is no isomorphic\n// mapping that properly preserves quietness.  Instead of doing something hacky, we just give up\n// and blow away NaN payloads, because no one uses them anyway.\n#endif\n\nnamespace capnp {\n\nclass ClientHook;\n\nnamespace _ {  // private\n\nclass PointerBuilder;\nclass PointerReader;\nclass StructBuilder;\nclass StructReader;\nclass ListBuilder;\nclass ListReader;\nclass OrphanBuilder;\nstruct WirePointer;\nstruct WireHelpers;\nclass SegmentReader;\nclass SegmentBuilder;\nclass Arena;\nclass BuilderArena;\n\n// =============================================================================\n\n#if CAPNP_DEBUG_TYPES\ntypedef kj::UnitRatio<kj::Bounded<64, uint>, BitLabel, ElementLabel> BitsPerElementTableType;\n#else\ntypedef uint BitsPerElementTableType;\n#endif\n\nstatic constexpr BitsPerElementTableType BITS_PER_ELEMENT_TABLE[8] = {\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 1>() * BITS / ELEMENTS,\n  bounded< 8>() * BITS / ELEMENTS,\n  bounded<16>() * BITS / ELEMENTS,\n  bounded<32>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS\n};\n\ninline KJ_CONSTEXPR() BitsPerElementTableType dataBitsPerElement(ElementSize size) {\n  return _::BITS_PER_ELEMENT_TABLE[static_cast<int>(size)];\n}\n\ninline constexpr PointersPerElementN<1> pointersPerElement(ElementSize size) {\n  return size == ElementSize::POINTER\n      ? PointersPerElementN<1>(ONE * POINTERS / ELEMENTS)\n      : PointersPerElementN<1>(ZERO * POINTERS / ELEMENTS);\n}\n\nstatic constexpr BitsPerElementTableType BITS_PER_ELEMENT_INCLUDING_PONITERS_TABLE[8] = {\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 1>() * BITS / ELEMENTS,\n  bounded< 8>() * BITS / ELEMENTS,\n  bounded<16>() * BITS / ELEMENTS,\n  bounded<32>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS\n};\n\ninline KJ_CONSTEXPR() BitsPerElementTableType bitsPerElementIncludingPointers(ElementSize size) {\n  return _::BITS_PER_ELEMENT_INCLUDING_PONITERS_TABLE[static_cast<int>(size)];\n}\n\ntemplate <size_t size> struct ElementSizeForByteSize;\ntemplate <> struct ElementSizeForByteSize<1> { static constexpr ElementSize value = ElementSize::BYTE; };\ntemplate <> struct ElementSizeForByteSize<2> { static constexpr ElementSize value = ElementSize::TWO_BYTES; };\ntemplate <> struct ElementSizeForByteSize<4> { static constexpr ElementSize value = ElementSize::FOUR_BYTES; };\ntemplate <> struct ElementSizeForByteSize<8> { static constexpr ElementSize value = ElementSize::EIGHT_BYTES; };\n\ntemplate <typename T> struct ElementSizeForType {\n  static constexpr ElementSize value =\n      // Primitive types that aren't special-cased below can be determined from sizeof().\n      CAPNP_KIND(T) == Kind::PRIMITIVE ? ElementSizeForByteSize<sizeof(T)>::value :\n      CAPNP_KIND(T) == Kind::ENUM ? ElementSize::TWO_BYTES :\n      CAPNP_KIND(T) == Kind::STRUCT ? ElementSize::INLINE_COMPOSITE :\n\n      // Everything else is a pointer.\n      ElementSize::POINTER;\n};\n\n// Void and bool are special.\ntemplate <> struct ElementSizeForType<Void> { static constexpr ElementSize value = ElementSize::VOID; };\ntemplate <> struct ElementSizeForType<bool> { static constexpr ElementSize value = ElementSize::BIT; };\n\n// Lists and blobs are pointers, not structs.\ntemplate <typename T, Kind K> struct ElementSizeForType<List<T, K>> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\ntemplate <> struct ElementSizeForType<Text> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\ntemplate <> struct ElementSizeForType<Data> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\n\ntemplate <typename T>\ninline constexpr ElementSize elementSizeForType() {\n  return ElementSizeForType<T>::value;\n}\n\nstruct MessageSizeCounts {\n  WordCountN<61, uint64_t> wordCount;  // 2^64 bytes\n  uint capCount;\n\n  MessageSizeCounts& operator+=(const MessageSizeCounts& other) {\n    // OK to truncate unchecked because this class is used to count actual stuff in memory, and\n    // we couldn't possibly have anywhere near 2^61 words.\n    wordCount = assumeBits<61>(wordCount + other.wordCount);\n    capCount += other.capCount;\n    return *this;\n  }\n\n  void addWords(WordCountN<61, uint64_t> other) {\n    wordCount = assumeBits<61>(wordCount + other);\n  }\n\n  MessageSize asPublic() {\n    return MessageSize { unbound(wordCount / WORDS), capCount };\n  }\n};\n\n// =============================================================================\n\ntemplate <int wordCount>\nunion AlignedData {\n  // Useful for declaring static constant data blobs as an array of bytes, but forcing those\n  // bytes to be word-aligned.\n\n  uint8_t bytes[wordCount * sizeof(word)];\n  word words[wordCount];\n};\n\nstruct StructSize {\n  StructDataWordCount data;\n  StructPointerCount pointers;\n\n  inline constexpr WordCountN<17> total() const { return data + pointers * WORDS_PER_POINTER; }\n\n  StructSize() = default;\n  inline constexpr StructSize(StructDataWordCount data, StructPointerCount pointers)\n      : data(data), pointers(pointers) {}\n};\n\ntemplate <typename T, typename CapnpPrivate = typename T::_capnpPrivate>\ninline constexpr StructSize structSize() {\n  return StructSize(bounded(CapnpPrivate::dataWordSize) * WORDS,\n                    bounded(CapnpPrivate::pointerCount) * POINTERS);\n}\n\ntemplate <typename T, typename CapnpPrivate = typename T::_capnpPrivate,\n          typename = kj::EnableIf<CAPNP_KIND(T) == Kind::STRUCT>>\ninline constexpr StructSize minStructSizeForElement() {\n  // If T is a struct, return its struct size. Otherwise return the minimum struct size big enough\n  // to hold a T.\n\n  return StructSize(bounded(CapnpPrivate::dataWordSize) * WORDS,\n                    bounded(CapnpPrivate::pointerCount) * POINTERS);\n}\n\ntemplate <typename T, typename = kj::EnableIf<CAPNP_KIND(T) != Kind::STRUCT>>\ninline constexpr StructSize minStructSizeForElement() {\n  // If T is a struct, return its struct size. Otherwise return the minimum struct size big enough\n  // to hold a T.\n\n  return StructSize(\n      dataBitsPerElement(elementSizeForType<T>()) * ELEMENTS > ZERO * BITS\n          ? StructDataWordCount(ONE * WORDS) : StructDataWordCount(ZERO * WORDS),\n      pointersPerElement(elementSizeForType<T>()) * ELEMENTS);\n}\n\n// -------------------------------------------------------------------\n// Masking of default values\n\ntemplate <typename T, Kind kind = CAPNP_KIND(T)> struct Mask_;\ntemplate <typename T> struct Mask_<T, Kind::PRIMITIVE> { typedef T Type; };\ntemplate <typename T> struct Mask_<T, Kind::ENUM> { typedef uint16_t Type; };\ntemplate <> struct Mask_<float, Kind::PRIMITIVE> { typedef uint32_t Type; };\ntemplate <> struct Mask_<double, Kind::PRIMITIVE> { typedef uint64_t Type; };\n\ntemplate <typename T> struct Mask_<T, Kind::OTHER> {\n  // Union discriminants end up here.\n  static_assert(sizeof(T) == 2, \"Don't know how to mask this type.\");\n  typedef uint16_t Type;\n};\n\ntemplate <typename T>\nusing Mask = typename Mask_<T>::Type;\n\ntemplate <typename T>\nKJ_ALWAYS_INLINE(Mask<T> mask(T value, Mask<T> mask));\ntemplate <typename T>\nKJ_ALWAYS_INLINE(T unmask(Mask<T> value, Mask<T> mask));\n\ntemplate <typename T>\ninline Mask<T> mask(T value, Mask<T> mask) {\n  return static_cast<Mask<T> >(value) ^ mask;\n}\n\ntemplate <>\ninline uint32_t mask<float>(float value, uint32_t mask) {\n#if CAPNP_CANONICALIZE_NAN\n  if (value != value) {\n    return 0x7fc00000u ^ mask;\n  }\n#endif\n\n  uint32_t i;\n  static_assert(sizeof(i) == sizeof(value), \"float is not 32 bits?\");\n  memcpy(&i, &value, sizeof(value));\n  return i ^ mask;\n}\n\ntemplate <>\ninline uint64_t mask<double>(double value, uint64_t mask) {\n#if CAPNP_CANONICALIZE_NAN\n  if (value != value) {\n    return 0x7ff8000000000000ull ^ mask;\n  }\n#endif\n\n  uint64_t i;\n  static_assert(sizeof(i) == sizeof(value), \"double is not 64 bits?\");\n  memcpy(&i, &value, sizeof(value));\n  return i ^ mask;\n}\n\ntemplate <typename T>\ninline T unmask(Mask<T> value, Mask<T> mask) {\n  return static_cast<T>(value ^ mask);\n}\n\ntemplate <>\ninline float unmask<float>(uint32_t value, uint32_t mask) {\n  value ^= mask;\n  float result;\n  static_assert(sizeof(result) == sizeof(value), \"float is not 32 bits?\");\n  memcpy(&result, &value, sizeof(value));\n  return result;\n}\n\ntemplate <>\ninline double unmask<double>(uint64_t value, uint64_t mask) {\n  value ^= mask;\n  double result;\n  static_assert(sizeof(result) == sizeof(value), \"double is not 64 bits?\");\n  memcpy(&result, &value, sizeof(value));\n  return result;\n}\n\n// -------------------------------------------------------------------\n\nclass CapTableReader {\npublic:\n  virtual kj::Maybe<kj::Own<ClientHook>> extractCap(uint index) = 0;\n  // Extract the capability at the given index.  If the index is invalid, returns null.\n};\n\nclass CapTableBuilder: public CapTableReader {\npublic:\n  virtual uint injectCap(kj::Own<ClientHook>&& cap) = 0;\n  // Add the capability to the message and return its index.  If the same ClientHook is injected\n  // twice, this may return the same index both times, but in this case dropCap() needs to be\n  // called an equal number of times to actually remove the cap.\n\n  virtual void dropCap(uint index) = 0;\n  // Remove a capability injected earlier.  Called when the pointer is overwritten or zero'd out.\n};\n\n// -------------------------------------------------------------------\n\nclass PointerBuilder: public kj::DisallowConstCopy {\n  // Represents a single pointer, usually embedded in a struct or a list.\n\npublic:\n  inline PointerBuilder(): segment(nullptr), capTable(nullptr), pointer(nullptr) {}\n\n  static inline PointerBuilder getRoot(\n      SegmentBuilder* segment, CapTableBuilder* capTable, word* location);\n  // Get a PointerBuilder representing a message root located in the given segment at the given\n  // location.\n\n  inline bool isNull() { return getPointerType() == PointerType::NULL_; }\n  PointerType getPointerType() const;\n\n  StructBuilder getStruct(StructSize size, const word* defaultValue);\n  ListBuilder getList(ElementSize elementSize, const word* defaultValue);\n  ListBuilder getStructList(StructSize elementSize, const word* defaultValue);\n  ListBuilder getListAnySize(const word* defaultValue);\n  template <typename T> typename T::Builder getBlob(\n      const void* defaultValue, ByteCount defaultSize);\n#if !CAPNP_LITE\n  kj::Own<ClientHook> getCapability();\n#endif  // !CAPNP_LITE\n  // Get methods:  Get the value.  If it is null, initialize it to a copy of the default value.\n  // The default value is encoded as an \"unchecked message\" for structs, lists, and objects, or a\n  // simple byte array for blobs.\n\n  StructBuilder initStruct(StructSize size);\n  ListBuilder initList(ElementSize elementSize, ElementCount elementCount);\n  ListBuilder initStructList(ElementCount elementCount, StructSize size);\n  template <typename T> typename T::Builder initBlob(ByteCount size);\n  // Init methods:  Initialize the pointer to a newly-allocated object, discarding the existing\n  // object.\n\n  void setStruct(const StructReader& value, bool canonical = false);\n  void setList(const ListReader& value, bool canonical = false);\n  template <typename T> void setBlob(typename T::Reader value);\n#if !CAPNP_LITE\n  void setCapability(kj::Own<ClientHook>&& cap);\n#endif  // !CAPNP_LITE\n  // Set methods:  Initialize the pointer to a newly-allocated copy of the given value, discarding\n  // the existing object.\n\n  void adopt(OrphanBuilder&& orphan);\n  // Set the pointer to point at the given orphaned value.\n\n  OrphanBuilder disown();\n  // Set the pointer to null and return its previous value as an orphan.\n\n  void clear();\n  // Clear the pointer to null, discarding its previous value.\n\n  void transferFrom(PointerBuilder other);\n  // Equivalent to `adopt(other.disown())`.\n\n  void copyFrom(PointerReader other, bool canonical = false);\n  // Equivalent to `set(other.get())`.\n  // If you set the canonical flag, it will attempt to lay the target out\n  // canonically, provided enough space is available.\n\n  PointerReader asReader() const;\n\n  BuilderArena* getArena() const;\n  // Get the arena containing this pointer.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  PointerBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;     // Memory segment in which the pointer resides.\n  CapTableBuilder* capTable;   // Table of capability indexes.\n  WirePointer* pointer;        // Pointer to the pointer.\n\n  inline PointerBuilder(SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* pointer)\n      : segment(segment), capTable(capTable), pointer(pointer) {}\n\n  friend class StructBuilder;\n  friend class ListBuilder;\n  friend class OrphanBuilder;\n};\n\nclass PointerReader {\npublic:\n  inline PointerReader()\n      : segment(nullptr), capTable(nullptr), pointer(nullptr), nestingLimit(0x7fffffff) {}\n\n  static PointerReader getRoot(SegmentReader* segment, CapTableReader* capTable,\n                               const word* location, int nestingLimit);\n  // Get a PointerReader representing a message root located in the given segment at the given\n  // location.\n\n  static inline PointerReader getRootUnchecked(const word* location);\n  // Get a PointerReader for an unchecked message.\n\n  MessageSizeCounts targetSize() const;\n  // Return the total size of the target object and everything to which it points.  Does not count\n  // far pointer overhead.  This is useful for deciding how much space is needed to copy the object\n  // into a flat array.  However, the caller is advised NOT to treat this value as secure.  Instead,\n  // use the result as a hint for allocating the first segment, do the copy, and then throw an\n  // exception if it overruns.\n\n  inline bool isNull() const { return getPointerType() == PointerType::NULL_; }\n  PointerType getPointerType() const;\n\n  StructReader getStruct(const word* defaultValue) const;\n  ListReader getList(ElementSize expectedElementSize, const word* defaultValue) const;\n  ListReader getListAnySize(const word* defaultValue) const;\n  template <typename T>\n  typename T::Reader getBlob(const void* defaultValue, ByteCount defaultSize) const;\n#if !CAPNP_LITE\n  kj::Own<ClientHook> getCapability() const;\n#endif  // !CAPNP_LITE\n  // Get methods:  Get the value.  If it is null, return the default value instead.\n  // The default value is encoded as an \"unchecked message\" for structs, lists, and objects, or a\n  // simple byte array for blobs.\n\n  const word* getUnchecked() const;\n  // If this is an unchecked message, get a word* pointing at the location of the pointer.  This\n  // word* can actually be passed to readUnchecked() to read the designated sub-object later.  If\n  // this isn't an unchecked message, throws an exception.\n\n  kj::Maybe<Arena&> getArena() const;\n  // Get the arena containing this pointer.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  PointerReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n\nprivate:\n  SegmentReader* segment;      // Memory segment in which the pointer resides.\n  CapTableReader* capTable;    // Table of capability indexes.\n  const WirePointer* pointer;  // Pointer to the pointer.  null = treat as null pointer.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n\n  inline PointerReader(SegmentReader* segment, CapTableReader* capTable,\n                       const WirePointer* pointer, int nestingLimit)\n      : segment(segment), capTable(capTable), pointer(pointer), nestingLimit(nestingLimit) {}\n\n  friend class StructReader;\n  friend class ListReader;\n  friend class PointerBuilder;\n  friend class OrphanBuilder;\n};\n\n// -------------------------------------------------------------------\n\nclass StructBuilder: public kj::DisallowConstCopy {\npublic:\n  inline StructBuilder(): segment(nullptr), capTable(nullptr), data(nullptr), pointers(nullptr) {}\n\n  inline word* getLocation() { return reinterpret_cast<word*>(data); }\n  // Get the object's location.  Only valid for independently-allocated objects (i.e. not list\n  // elements).\n\n  inline StructDataBitCount getDataSectionSize() const { return dataSize; }\n  inline StructPointerCount getPointerSectionSize() const { return pointerCount; }\n  inline kj::ArrayPtr<byte> getDataSectionAsBlob();\n  inline _::ListBuilder getPointerSectionAsList();\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(bool hasDataField(StructDataOffset offset));\n  // Return true if the field is set to something other than its default value.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset));\n  // Gets the data field value of the given type at the given offset.  The offset is measured in\n  // multiples of the field size, determined by the type.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset, Mask<T> mask));\n  // Like getDataField() but applies the given XOR mask to the data on load.  Used for reading\n  // fields with non-zero default values.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataField(StructDataOffset offset, kj::NoInfer<T> value));\n  // Sets the data field value at the given offset.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataField(StructDataOffset offset,\n                                     kj::NoInfer<T> value, Mask<T> mask));\n  // Like setDataField() but applies the given XOR mask before storing.  Used for writing fields\n  // with non-zero default values.\n\n  KJ_ALWAYS_INLINE(PointerBuilder getPointerField(StructPointerOffset ptrIndex));\n  // Get a builder for a pointer field given the index within the pointer section.\n\n  void clearAll();\n  // Clear all pointers and data.\n\n  void transferContentFrom(StructBuilder other);\n  // Adopt all pointers from `other`, and also copy all data.  If `other`'s sections are larger\n  // than this, the extra data is not transferred, meaning there is a risk of data loss when\n  // transferring from messages built with future versions of the protocol.\n\n  void copyContentFrom(StructReader other);\n  // Copy content from `other`.  If `other`'s sections are larger than this, the extra data is not\n  // copied, meaning there is a risk of data loss when copying from messages built with future\n  // versions of the protocol.\n\n  StructReader asReader() const;\n  // Gets a StructReader pointing at the same memory.\n\n  BuilderArena* getArena();\n  // Gets the arena in which this object is allocated.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  StructBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;     // Memory segment in which the struct resides.\n  CapTableBuilder* capTable;   // Table of capability indexes.\n  void* data;                  // Pointer to the encoded data.\n  WirePointer* pointers;   // Pointer to the encoded pointers.\n\n  StructDataBitCount dataSize;\n  // Size of data section.  We use a bit count rather than a word count to more easily handle the\n  // case of struct lists encoded with less than a word per element.\n\n  StructPointerCount pointerCount;  // Size of the pointer section.\n\n  inline StructBuilder(SegmentBuilder* segment, CapTableBuilder* capTable,\n                       void* data, WirePointer* pointers,\n                       StructDataBitCount dataSize, StructPointerCount pointerCount)\n      : segment(segment), capTable(capTable), data(data), pointers(pointers),\n        dataSize(dataSize), pointerCount(pointerCount) {}\n\n  friend class ListBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\nclass StructReader {\npublic:\n  inline StructReader()\n      : segment(nullptr), capTable(nullptr), data(nullptr), pointers(nullptr),\n        dataSize(ZERO * BITS), pointerCount(ZERO * POINTERS), nestingLimit(0x7fffffff) {}\n  inline StructReader(kj::ArrayPtr<const word> data)\n      : segment(nullptr), capTable(nullptr), data(data.begin()), pointers(nullptr),\n        dataSize(assumeBits<STRUCT_DATA_WORD_COUNT_BITS>(data.size()) * WORDS * BITS_PER_WORD),\n        pointerCount(ZERO * POINTERS), nestingLimit(0x7fffffff) {}\n\n  const void* getLocation() const { return data; }\n\n  inline StructDataBitCount getDataSectionSize() const { return dataSize; }\n  inline StructPointerCount getPointerSectionSize() const { return pointerCount; }\n  inline kj::ArrayPtr<const byte> getDataSectionAsBlob() const;\n  inline _::ListReader getPointerSectionAsList() const;\n\n  kj::Array<word> canonicalize();\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(bool hasDataField(StructDataOffset offset) const);\n  // Return true if the field is set to something other than its default value.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset) const);\n  // Get the data field value of the given type at the given offset.  The offset is measured in\n  // multiples of the field size, determined by the type.  Returns zero if the offset is past the\n  // end of the struct's data section.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset, Mask<T> mask) const);\n  // Like getDataField(offset), but applies the given XOR mask to the result.  Used for reading\n  // fields with non-zero default values.\n\n  KJ_ALWAYS_INLINE(PointerReader getPointerField(StructPointerOffset ptrIndex) const);\n  // Get a reader for a pointer field given the index within the pointer section.  If the index\n  // is out-of-bounds, returns a null pointer.\n\n  MessageSizeCounts totalSize() const;\n  // Return the total size of the struct and everything to which it points.  Does not count far\n  // pointer overhead.  This is useful for deciding how much space is needed to copy the struct\n  // into a flat array.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  StructReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead, const word **ptrHead,\n                   bool *dataTrunc, bool *ptrTrunc);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n  //\n  // If this function returns false, the struct is non-canonical. If it\n  // returns true, then:\n  // * If it is a composite in a list, it is canonical if at least one struct\n  //   in the list outputs dataTrunc = 1, and at least one outputs ptrTrunc = 1\n  // * If it is derived from a struct pointer, it is canonical if\n  //   dataTrunc = 1 AND ptrTrunc = 1\n\nprivate:\n  SegmentReader* segment;    // Memory segment in which the struct resides.\n  CapTableReader* capTable;  // Table of capability indexes.\n\n  const void* data;\n  const WirePointer* pointers;\n\n  StructDataBitCount dataSize;\n  // Size of data section.  We use a bit count rather than a word count to more easily handle the\n  // case of struct lists encoded with less than a word per element.\n\n  StructPointerCount pointerCount;  // Size of the pointer section.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n  // TODO(perf):  Limit to 16 bits for better packing?\n\n  inline StructReader(SegmentReader* segment, CapTableReader* capTable,\n                      const void* data, const WirePointer* pointers,\n                      StructDataBitCount dataSize, StructPointerCount pointerCount,\n                      int nestingLimit)\n      : segment(segment), capTable(capTable), data(data), pointers(pointers),\n        dataSize(dataSize), pointerCount(pointerCount),\n        nestingLimit(nestingLimit) {}\n\n  friend class ListReader;\n  friend class StructBuilder;\n  friend struct WireHelpers;\n};\n\n// -------------------------------------------------------------------\n\nclass ListBuilder: public kj::DisallowConstCopy {\npublic:\n  inline explicit ListBuilder(ElementSize elementSize)\n      : segment(nullptr), capTable(nullptr), ptr(nullptr), elementCount(ZERO * ELEMENTS),\n        step(ZERO * BITS / ELEMENTS), structDataSize(ZERO * BITS),\n        structPointerCount(ZERO * POINTERS), elementSize(elementSize) {}\n\n  inline word* getLocation() {\n    // Get the object's location.\n\n    if (elementSize == ElementSize::INLINE_COMPOSITE && ptr != nullptr) {\n      return reinterpret_cast<word*>(ptr) - POINTER_SIZE_IN_WORDS;\n    } else {\n      return reinterpret_cast<word*>(ptr);\n    }\n  }\n\n  inline ElementSize getElementSize() const { return elementSize; }\n\n  inline ListElementCount size() const;\n  // The number of elements in the list.\n\n  Text::Builder asText();\n  Data::Builder asData();\n  // Reinterpret the list as a blob.  Throws an exception if the elements are not byte-sized.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataElement(ElementCount index));\n  // Get the element of the given type at the given index.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataElement(ElementCount index, kj::NoInfer<T> value));\n  // Set the element at the given index.\n\n  KJ_ALWAYS_INLINE(PointerBuilder getPointerElement(ElementCount index));\n\n  StructBuilder getStructElement(ElementCount index);\n\n  ListReader asReader() const;\n  // Get a ListReader pointing at the same memory.\n\n  BuilderArena* getArena();\n  // Gets the arena in which this object is allocated.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  ListBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;    // Memory segment in which the list resides.\n  CapTableBuilder* capTable;  // Table of capability indexes.\n\n  byte* ptr;  // Pointer to list content.\n\n  ListElementCount elementCount;  // Number of elements in the list.\n\n  BitsPerElementN<23> step;\n  // The distance between elements. The maximum value occurs when a struct contains 2^16-1 data\n  // words and 2^16-1 pointers, i.e. 2^17 - 2 words, or 2^23 - 128 bits.\n\n  StructDataBitCount structDataSize;\n  StructPointerCount structPointerCount;\n  // The struct properties to use when interpreting the elements as structs.  All lists can be\n  // interpreted as struct lists, so these are always filled in.\n\n  ElementSize elementSize;\n  // The element size as a ElementSize. This is only really needed to disambiguate INLINE_COMPOSITE\n  // from other types when the overall size is exactly zero or one words.\n\n  inline ListBuilder(SegmentBuilder* segment, CapTableBuilder* capTable, void* ptr,\n                     BitsPerElementN<23> step, ListElementCount size,\n                     StructDataBitCount structDataSize, StructPointerCount structPointerCount,\n                     ElementSize elementSize)\n      : segment(segment), capTable(capTable), ptr(reinterpret_cast<byte*>(ptr)),\n        elementCount(size), step(step), structDataSize(structDataSize),\n        structPointerCount(structPointerCount), elementSize(elementSize) {}\n\n  friend class StructBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\nclass ListReader {\npublic:\n  inline explicit ListReader(ElementSize elementSize)\n      : segment(nullptr), capTable(nullptr), ptr(nullptr), elementCount(ZERO * ELEMENTS),\n        step(ZERO * BITS / ELEMENTS), structDataSize(ZERO * BITS),\n        structPointerCount(ZERO * POINTERS), elementSize(elementSize), nestingLimit(0x7fffffff) {}\n\n  inline ListElementCount size() const;\n  // The number of elements in the list.\n\n  inline ElementSize getElementSize() const { return elementSize; }\n\n  Text::Reader asText();\n  Data::Reader asData();\n  // Reinterpret the list as a blob.  Throws an exception if the elements are not byte-sized.\n\n  kj::ArrayPtr<const byte> asRawBytes() const;\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataElement(ElementCount index) const);\n  // Get the element of the given type at the given index.\n\n  KJ_ALWAYS_INLINE(PointerReader getPointerElement(ElementCount index) const);\n\n  StructReader getStructElement(ElementCount index) const;\n\n  MessageSizeCounts totalSize() const;\n  // Like StructReader::totalSize(). Note that for struct lists, the size includes the list tag.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  ListReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead, const WirePointer* ref);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n\nprivate:\n  SegmentReader* segment;    // Memory segment in which the list resides.\n  CapTableReader* capTable;  // Table of capability indexes.\n\n  const byte* ptr;  // Pointer to list content.\n\n  ListElementCount elementCount;  // Number of elements in the list.\n\n  BitsPerElementN<23> step;\n  // The distance between elements. The maximum value occurs when a struct contains 2^16-1 data\n  // words and 2^16-1 pointers, i.e. 2^17 - 2 words, or 2^23 - 2 bits.\n\n  StructDataBitCount structDataSize;\n  StructPointerCount structPointerCount;\n  // The struct properties to use when interpreting the elements as structs.  All lists can be\n  // interpreted as struct lists, so these are always filled in.\n\n  ElementSize elementSize;\n  // The element size as a ElementSize. This is only really needed to disambiguate INLINE_COMPOSITE\n  // from other types when the overall size is exactly zero or one words.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n\n  inline ListReader(SegmentReader* segment, CapTableReader* capTable, const void* ptr,\n                    ListElementCount elementCount, BitsPerElementN<23> step,\n                    StructDataBitCount structDataSize, StructPointerCount structPointerCount,\n                    ElementSize elementSize, int nestingLimit)\n      : segment(segment), capTable(capTable), ptr(reinterpret_cast<const byte*>(ptr)),\n        elementCount(elementCount), step(step), structDataSize(structDataSize),\n        structPointerCount(structPointerCount), elementSize(elementSize),\n        nestingLimit(nestingLimit) {}\n\n  friend class StructReader;\n  friend class ListBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\n// -------------------------------------------------------------------\n\nclass OrphanBuilder {\npublic:\n  inline OrphanBuilder(): segment(nullptr), capTable(nullptr), location(nullptr) {\n    memset(&tag, 0, sizeof(tag));\n  }\n  OrphanBuilder(const OrphanBuilder& other) = delete;\n  inline OrphanBuilder(OrphanBuilder&& other) noexcept;\n  inline ~OrphanBuilder() noexcept(false);\n\n  static OrphanBuilder initStruct(BuilderArena* arena, CapTableBuilder* capTable, StructSize size);\n  static OrphanBuilder initList(BuilderArena* arena, CapTableBuilder* capTable,\n                                ElementCount elementCount, ElementSize elementSize);\n  static OrphanBuilder initStructList(BuilderArena* arena, CapTableBuilder* capTable,\n                                      ElementCount elementCount, StructSize elementSize);\n  static OrphanBuilder initText(BuilderArena* arena, CapTableBuilder* capTable, ByteCount size);\n  static OrphanBuilder initData(BuilderArena* arena, CapTableBuilder* capTable, ByteCount size);\n\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, StructReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, ListReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, PointerReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, Text::Reader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, Data::Reader copyFrom);\n#if !CAPNP_LITE\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable,\n                            kj::Own<ClientHook> copyFrom);\n#endif  // !CAPNP_LITE\n\n  static OrphanBuilder concat(BuilderArena* arena, CapTableBuilder* capTable,\n                              ElementSize expectedElementSize, StructSize expectedStructSize,\n                              kj::ArrayPtr<const ListReader> lists);\n\n  static OrphanBuilder referenceExternalData(BuilderArena* arena, Data::Reader data);\n\n  OrphanBuilder& operator=(const OrphanBuilder& other) = delete;\n  inline OrphanBuilder& operator=(OrphanBuilder&& other);\n\n  inline bool operator==(decltype(nullptr)) const { return location == nullptr; }\n  inline bool operator!=(decltype(nullptr)) const { return location != nullptr; }\n\n  StructBuilder asStruct(StructSize size);\n  // Interpret as a struct, or throw an exception if not a struct.\n\n  ListBuilder asList(ElementSize elementSize);\n  // Interpret as a list, or throw an exception if not a list.  elementSize cannot be\n  // INLINE_COMPOSITE -- use asStructList() instead.\n\n  ListBuilder asStructList(StructSize elementSize);\n  // Interpret as a struct list, or throw an exception if not a list.\n\n  ListBuilder asListAnySize();\n  // For AnyList.\n\n  Text::Builder asText();\n  Data::Builder asData();\n  // Interpret as a blob, or throw an exception if not a blob.\n\n  StructReader asStructReader(StructSize size) const;\n  ListReader asListReader(ElementSize elementSize) const;\n  ListReader asListReaderAnySize() const;\n#if !CAPNP_LITE\n  kj::Own<ClientHook> asCapability() const;\n#endif  // !CAPNP_LITE\n  Text::Reader asTextReader() const;\n  Data::Reader asDataReader() const;\n\n  bool truncate(ElementCount size, bool isText) KJ_WARN_UNUSED_RESULT;\n  // Resize the orphan list to the given size. Returns false if the list is currently empty but\n  // the requested size is non-zero, in which case the caller will need to allocate a new list.\n\n  void truncate(ElementCount size, ElementSize elementSize);\n  void truncate(ElementCount size, StructSize elementSize);\n  void truncateText(ElementCount size);\n  // Versions of truncate() that know how to allocate a new list if needed.\n\nprivate:\n  static_assert(ONE * POINTERS * WORDS_PER_POINTER == ONE * WORDS,\n                \"This struct assumes a pointer is one word.\");\n  word tag;\n  // Contains an encoded WirePointer representing this object.  WirePointer is defined in\n  // layout.c++, but fits in a word.\n  //\n  // This may be a FAR pointer.  Even in that case, `location` points to the eventual destination\n  // of that far pointer.  The reason we keep the far pointer around rather than just making `tag`\n  // represent the final destination is because if the eventual adopter of the pointer is not in\n  // the target's segment then it may be useful to reuse the far pointer landing pad.\n  //\n  // If `tag` is not a far pointer, its offset is garbage; only `location` points to the actual\n  // target.\n\n  SegmentBuilder* segment;\n  // Segment in which the object resides.\n\n  CapTableBuilder* capTable;\n  // Table of capability indexes.\n\n  word* location;\n  // Pointer to the object, or nullptr if the pointer is null.  For capabilities, we make this\n  // 0x1 just so that it is non-null for operator==, but it is never used.\n\n  inline OrphanBuilder(const void* tagPtr, SegmentBuilder* segment,\n                       CapTableBuilder* capTable, word* location)\n      : segment(segment), capTable(capTable), location(location) {\n    memcpy(&tag, tagPtr, sizeof(tag));\n  }\n\n  inline WirePointer* tagAsPtr() { return reinterpret_cast<WirePointer*>(&tag); }\n  inline const WirePointer* tagAsPtr() const { return reinterpret_cast<const WirePointer*>(&tag); }\n\n  void euthanize();\n  // Erase the target object, zeroing it out and possibly reclaiming the memory.  Called when\n  // the OrphanBuilder is being destroyed or overwritten and it is non-null.\n\n  friend struct WireHelpers;\n};\n\n// =======================================================================================\n// Internal implementation details...\n\n// These are defined in the source file.\ntemplate <> typename Text::Builder PointerBuilder::initBlob<Text>(ByteCount size);\ntemplate <> void PointerBuilder::setBlob<Text>(typename Text::Reader value);\ntemplate <> typename Text::Builder PointerBuilder::getBlob<Text>(\n    const void* defaultValue, ByteCount defaultSize);\ntemplate <> typename Text::Reader PointerReader::getBlob<Text>(\n    const void* defaultValue, ByteCount defaultSize) const;\n\ntemplate <> typename Data::Builder PointerBuilder::initBlob<Data>(ByteCount size);\ntemplate <> void PointerBuilder::setBlob<Data>(typename Data::Reader value);\ntemplate <> typename Data::Builder PointerBuilder::getBlob<Data>(\n    const void* defaultValue, ByteCount defaultSize);\ntemplate <> typename Data::Reader PointerReader::getBlob<Data>(\n    const void* defaultValue, ByteCount defaultSize) const;\n\ninline PointerBuilder PointerBuilder::getRoot(\n    SegmentBuilder* segment, CapTableBuilder* capTable, word* location) {\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(location));\n}\n\ninline PointerReader PointerReader::getRootUnchecked(const word* location) {\n  return PointerReader(nullptr, nullptr,\n                       reinterpret_cast<const WirePointer*>(location), 0x7fffffff);\n}\n\n// -------------------------------------------------------------------\n\ninline kj::ArrayPtr<byte> StructBuilder::getDataSectionAsBlob() {\n  return kj::ArrayPtr<byte>(reinterpret_cast<byte*>(data),\n      unbound(dataSize / BITS_PER_BYTE / BYTES));\n}\n\ninline _::ListBuilder StructBuilder::getPointerSectionAsList() {\n  return _::ListBuilder(segment, capTable, pointers, ONE * POINTERS * BITS_PER_POINTER / ELEMENTS,\n                        pointerCount * (ONE * ELEMENTS / POINTERS),\n                        ZERO * BITS, ONE * POINTERS, ElementSize::POINTER);\n}\n\ntemplate <typename T>\ninline bool StructBuilder::hasDataField(StructDataOffset offset) {\n  return getDataField<Mask<T>>(offset) != 0;\n}\n\ntemplate <>\ninline bool StructBuilder::hasDataField<Void>(StructDataOffset offset) {\n  return false;\n}\n\ntemplate <typename T>\ninline T StructBuilder::getDataField(StructDataOffset offset) {\n  return reinterpret_cast<WireValue<T>*>(data)[unbound(offset / ELEMENTS)].get();\n}\n\ntemplate <>\ninline bool StructBuilder::getDataField<bool>(StructDataOffset offset) {\n  BitCount32 boffset = offset * (ONE * BITS / ELEMENTS);\n  byte* b = reinterpret_cast<byte*>(data) + boffset / BITS_PER_BYTE;\n  return (*reinterpret_cast<uint8_t*>(b) &\n      unbound(ONE << (boffset % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void StructBuilder::getDataField<Void>(StructDataOffset offset) {\n  return VOID;\n}\n\ntemplate <typename T>\ninline T StructBuilder::getDataField(StructDataOffset offset, Mask<T> mask) {\n  return unmask<T>(getDataField<Mask<T> >(offset), mask);\n}\n\ntemplate <typename T>\ninline void StructBuilder::setDataField(StructDataOffset offset, kj::NoInfer<T> value) {\n  reinterpret_cast<WireValue<T>*>(data)[unbound(offset / ELEMENTS)].set(value);\n}\n\n#if CAPNP_CANONICALIZE_NAN\n// Use mask() on floats and doubles to make sure we canonicalize NaNs.\ntemplate <>\ninline void StructBuilder::setDataField<float>(StructDataOffset offset, float value) {\n  setDataField<uint32_t>(offset, mask<float>(value, 0));\n}\ntemplate <>\ninline void StructBuilder::setDataField<double>(StructDataOffset offset, double value) {\n  setDataField<uint64_t>(offset, mask<double>(value, 0));\n}\n#endif\n\ntemplate <>\ninline void StructBuilder::setDataField<bool>(StructDataOffset offset, bool value) {\n  auto boffset = offset * (ONE * BITS / ELEMENTS);\n  byte* b = reinterpret_cast<byte*>(data) + boffset / BITS_PER_BYTE;\n  uint bitnum = unboundMaxBits<3>(boffset % BITS_PER_BYTE / BITS);\n  *reinterpret_cast<uint8_t*>(b) = (*reinterpret_cast<uint8_t*>(b) & ~(1 << bitnum))\n                                 | (static_cast<uint8_t>(value) << bitnum);\n}\n\ntemplate <>\ninline void StructBuilder::setDataField<Void>(StructDataOffset offset, Void value) {}\n\ntemplate <typename T>\ninline void StructBuilder::setDataField(StructDataOffset offset,\n                                        kj::NoInfer<T> value, Mask<T> m) {\n  setDataField<Mask<T> >(offset, mask<T>(value, m));\n}\n\ninline PointerBuilder StructBuilder::getPointerField(StructPointerOffset ptrIndex) {\n  // Hacky because WirePointer is defined in the .c++ file (so is incomplete here).\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(\n      reinterpret_cast<word*>(pointers) + ptrIndex * WORDS_PER_POINTER));\n}\n\n// -------------------------------------------------------------------\n\ninline kj::ArrayPtr<const byte> StructReader::getDataSectionAsBlob() const {\n  return kj::ArrayPtr<const byte>(reinterpret_cast<const byte*>(data),\n      unbound(dataSize / BITS_PER_BYTE / BYTES));\n}\n\ninline _::ListReader StructReader::getPointerSectionAsList() const {\n  return _::ListReader(segment, capTable, pointers, pointerCount * (ONE * ELEMENTS / POINTERS),\n                       ONE * POINTERS * BITS_PER_POINTER / ELEMENTS, ZERO * BITS, ONE * POINTERS,\n                       ElementSize::POINTER, nestingLimit);\n}\n\ntemplate <typename T>\ninline bool StructReader::hasDataField(StructDataOffset offset) const {\n  return getDataField<Mask<T>>(offset) != 0;\n}\n\ntemplate <>\ninline bool StructReader::hasDataField<Void>(StructDataOffset offset) const {\n  return false;\n}\n\ntemplate <typename T>\ninline T StructReader::getDataField(StructDataOffset offset) const {\n  if ((offset + ONE * ELEMENTS) * capnp::bitsPerElement<T>() <= dataSize) {\n    return reinterpret_cast<const WireValue<T>*>(data)[unbound(offset / ELEMENTS)].get();\n  } else {\n    return static_cast<T>(0);\n  }\n}\n\ntemplate <>\ninline bool StructReader::getDataField<bool>(StructDataOffset offset) const {\n  auto boffset = offset * (ONE * BITS / ELEMENTS);\n  if (boffset < dataSize) {\n    const byte* b = reinterpret_cast<const byte*>(data) + boffset / BITS_PER_BYTE;\n    return (*reinterpret_cast<const uint8_t*>(b) &\n        unbound(ONE << (boffset % BITS_PER_BYTE / BITS))) != 0;\n  } else {\n    return false;\n  }\n}\n\ntemplate <>\ninline Void StructReader::getDataField<Void>(StructDataOffset offset) const {\n  return VOID;\n}\n\ntemplate <typename T>\nT StructReader::getDataField(StructDataOffset offset, Mask<T> mask) const {\n  return unmask<T>(getDataField<Mask<T> >(offset), mask);\n}\n\ninline PointerReader StructReader::getPointerField(StructPointerOffset ptrIndex) const {\n  if (ptrIndex < pointerCount) {\n    // Hacky because WirePointer is defined in the .c++ file (so is incomplete here).\n    return PointerReader(segment, capTable, reinterpret_cast<const WirePointer*>(\n        reinterpret_cast<const word*>(pointers) + ptrIndex * WORDS_PER_POINTER), nestingLimit);\n  } else{\n    return PointerReader();\n  }\n}\n\n// -------------------------------------------------------------------\n\ninline ListElementCount ListBuilder::size() const { return elementCount; }\n\ntemplate <typename T>\ninline T ListBuilder::getDataElement(ElementCount index) {\n  return reinterpret_cast<WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->get();\n\n  // TODO(perf):  Benchmark this alternate implementation, which I suspect may make better use of\n  //   the x86 SIB byte.  Also use it for all the other getData/setData implementations below, and\n  //   the various non-inline methods that look up pointers.\n  //   Also if using this, consider changing ptr back to void* instead of byte*.\n//  return reinterpret_cast<WireValue<T>*>(ptr)[\n//      index / ELEMENTS * (step / capnp::bitsPerElement<T>())].get();\n}\n\ntemplate <>\ninline bool ListBuilder::getDataElement<bool>(ElementCount index) {\n  // Ignore step for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  byte* b = ptr + bindex / BITS_PER_BYTE;\n  return (*reinterpret_cast<uint8_t*>(b) &\n      unbound(ONE << (bindex % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void ListBuilder::getDataElement<Void>(ElementCount index) {\n  return VOID;\n}\n\ntemplate <typename T>\ninline void ListBuilder::setDataElement(ElementCount index, kj::NoInfer<T> value) {\n  reinterpret_cast<WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->set(value);\n}\n\n#if CAPNP_CANONICALIZE_NAN\n// Use mask() on floats and doubles to make sure we canonicalize NaNs.\ntemplate <>\ninline void ListBuilder::setDataElement<float>(ElementCount index, float value) {\n  setDataElement<uint32_t>(index, mask<float>(value, 0));\n}\ntemplate <>\ninline void ListBuilder::setDataElement<double>(ElementCount index, double value) {\n  setDataElement<uint64_t>(index, mask<double>(value, 0));\n}\n#endif\n\ntemplate <>\ninline void ListBuilder::setDataElement<bool>(ElementCount index, bool value) {\n  // Ignore stepBytes for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  byte* b = ptr + bindex / BITS_PER_BYTE;\n  auto bitnum = bindex % BITS_PER_BYTE / BITS;\n  *reinterpret_cast<uint8_t*>(b) = (*reinterpret_cast<uint8_t*>(b) & ~(1 << unbound(bitnum)))\n                                 | (static_cast<uint8_t>(value) << unbound(bitnum));\n}\n\ntemplate <>\ninline void ListBuilder::setDataElement<Void>(ElementCount index, Void value) {}\n\ninline PointerBuilder ListBuilder::getPointerElement(ElementCount index) {\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(ptr +\n      upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE));\n}\n\n// -------------------------------------------------------------------\n\ninline ListElementCount ListReader::size() const { return elementCount; }\n\ntemplate <typename T>\ninline T ListReader::getDataElement(ElementCount index) const {\n  return reinterpret_cast<const WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->get();\n}\n\ntemplate <>\ninline bool ListReader::getDataElement<bool>(ElementCount index) const {\n  // Ignore step for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  const byte* b = ptr + bindex / BITS_PER_BYTE;\n  return (*reinterpret_cast<const uint8_t*>(b) &\n      unbound(ONE << (bindex % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void ListReader::getDataElement<Void>(ElementCount index) const {\n  return VOID;\n}\n\ninline PointerReader ListReader::getPointerElement(ElementCount index) const {\n  return PointerReader(segment, capTable, reinterpret_cast<const WirePointer*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE), nestingLimit);\n}\n\n// -------------------------------------------------------------------\n\ninline OrphanBuilder::OrphanBuilder(OrphanBuilder&& other) noexcept\n    : segment(other.segment), capTable(other.capTable), location(other.location) {\n  memcpy(&tag, &other.tag, sizeof(tag));  // Needs memcpy to comply with aliasing rules.\n  other.segment = nullptr;\n  other.location = nullptr;\n}\n\ninline OrphanBuilder::~OrphanBuilder() noexcept(false) {\n  if (segment != nullptr) euthanize();\n}\n\ninline OrphanBuilder& OrphanBuilder::operator=(OrphanBuilder&& other) {\n  // With normal smart pointers, it's important to handle the case where the incoming pointer\n  // is actually transitively owned by this one.  In this case, euthanize() would destroy `other`\n  // before we copied it.  This isn't possible in the case of `OrphanBuilder` because it only\n  // owns message objects, and `other` is not itself a message object, therefore cannot possibly\n  // be transitively owned by `this`.\n\n  if (segment != nullptr) euthanize();\n  segment = other.segment;\n  capTable = other.capTable;\n  location = other.location;\n  memcpy(&tag, &other.tag, sizeof(tag));  // Needs memcpy to comply with aliasing rules.\n  other.segment = nullptr;\n  other.location = nullptr;\n  return *this;\n}\n\n}  // namespace _ (private)\n}  // namespace capnp\n\nCAPNP_END_HEADER\n"], "fixing_code": ["// Copyright (c) 2013-2016 Sandstorm Development Group, Inc. and contributors\n// Licensed under the MIT License:\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n#define CAPNP_PRIVATE\n#include \"layout.h\"\n#include <kj/debug.h>\n#include \"arena.h\"\n#include <string.h>\n#include <stdlib.h>\n\n#if !CAPNP_LITE\n#include \"capability.h\"\n#endif  // !CAPNP_LITE\n\nnamespace capnp {\nnamespace _ {  // private\n\n#if !CAPNP_LITE\nstatic BrokenCapFactory* globalBrokenCapFactory = nullptr;\n// Horrible hack:  We need to be able to construct broken caps without any capability context,\n// but we can't have a link-time dependency on libcapnp-rpc.\n\nvoid setGlobalBrokenCapFactoryForLayoutCpp(BrokenCapFactory& factory) {\n  // Called from capability.c++ when the capability API is used, to make sure that layout.c++\n  // is ready for it.  May be called multiple times but always with the same value.\n#if __GNUC__ || defined(__clang__)\n  __atomic_store_n(&globalBrokenCapFactory, &factory, __ATOMIC_RELAXED);\n#elif _MSC_VER\n  *static_cast<BrokenCapFactory* volatile*>(&globalBrokenCapFactory) = &factory;\n#else\n#error \"Platform not supported\"\n#endif\n}\n\nstatic BrokenCapFactory* readGlobalBrokenCapFactoryForLayoutCpp() {\n#if __GNUC__ || defined(__clang__)\n  // Thread-sanitizer doesn't have the right information to know this is safe without doing an\n  // atomic read. https://groups.google.com/g/capnproto/c/634juhn5ap0/m/pyRiwWl1AAAJ\n  return __atomic_load_n(&globalBrokenCapFactory, __ATOMIC_RELAXED);\n#else\n  return globalBrokenCapFactory;\n#endif\n}\n\n}  // namespace _ (private)\n\nconst uint ClientHook::NULL_CAPABILITY_BRAND = 0;\nconst uint ClientHook::BROKEN_CAPABILITY_BRAND = 0;\n// Defined here rather than capability.c++ so that we can safely call isNull() in this file.\n\nnamespace _ {  // private\n\n#endif  // !CAPNP_LITE\n\n#if CAPNP_DEBUG_TYPES\n#define G(n) bounded<n>()\n#else\n#define G(n) n\n#endif\n\n// =======================================================================================\n\n#if __GNUC__ >= 8 && !__clang__\n// GCC 8 introduced a warning which complains whenever we try to memset() or memcpy() a\n// WirePointer, because we deleted the regular copy constructor / assignment operator. Weirdly, if\n// I remove those deletions, GCC *still* complains that WirePointer is non-trivial. I don't\n// understand why -- maybe because WireValue has private members? We don't want to make WireValue's\n// member public, but memset() and memcpy() on it are certainly valid and desirable, so we'll just\n// have to disable the warning I guess.\n#pragma GCC diagnostic ignored \"-Wclass-memaccess\"\n#endif\n\nstruct WirePointer {\n  // A pointer, in exactly the format in which it appears on the wire.\n\n  // Copying and moving is not allowed because the offset would become wrong.\n  WirePointer(const WirePointer& other) = delete;\n  WirePointer(WirePointer&& other) = delete;\n  WirePointer& operator=(const WirePointer& other) = delete;\n  WirePointer& operator=(WirePointer&& other) = delete;\n\n  // -----------------------------------------------------------------\n  // Common part of all pointers:  kind + offset\n  //\n  // Actually this is not terribly common.  The \"offset\" could actually be different things\n  // depending on the context:\n  // - For a regular (e.g. struct/list) pointer, a signed word offset from the word immediately\n  //   following the pointer pointer.  (The off-by-one means the offset is more often zero, saving\n  //   bytes on the wire when packed.)\n  // - For an inline composite list tag (not really a pointer, but structured similarly), an\n  //   element count.\n  // - For a FAR pointer, an unsigned offset into the target segment.\n  // - For a FAR landing pad, zero indicates that the target value immediately follows the pad while\n  //   1 indicates that the pad is followed by another FAR pointer that actually points at the\n  //   value.\n\n  enum Kind {\n    STRUCT = 0,\n    // Reference points at / describes a struct.\n\n    LIST = 1,\n    // Reference points at / describes a list.\n\n    FAR = 2,\n    // Reference is a \"far pointer\", which points at data located in a different segment.  The\n    // eventual target is one of the other kinds.\n\n    OTHER = 3\n    // Reference has type \"other\".  If the next 30 bits are all zero (i.e. the lower 32 bits contain\n    // only the kind OTHER) then the pointer is a capability.  All other values are reserved.\n  };\n\n  WireValue<uint32_t> offsetAndKind;\n\n  KJ_ALWAYS_INLINE(Kind kind() const) {\n    return static_cast<Kind>(offsetAndKind.get() & 3);\n  }\n  KJ_ALWAYS_INLINE(bool isPositional() const) {\n    return (offsetAndKind.get() & 2) == 0;  // match STRUCT and LIST but not FAR or OTHER\n  }\n  KJ_ALWAYS_INLINE(bool isCapability() const) {\n    return offsetAndKind.get() == OTHER;\n  }\n\n  KJ_ALWAYS_INLINE(word* target()) {\n    return reinterpret_cast<word*>(this) + 1 + (static_cast<int32_t>(offsetAndKind.get()) >> 2);\n  }\n  KJ_ALWAYS_INLINE(const word* target(SegmentReader* segment) const) {\n    if (segment == nullptr) {\n      return reinterpret_cast<const word*>(this + 1) +\n          (static_cast<int32_t>(offsetAndKind.get()) >> 2);\n    } else {\n      return segment->checkOffset(reinterpret_cast<const word*>(this + 1),\n                                  static_cast<int32_t>(offsetAndKind.get()) >> 2);\n    }\n  }\n  KJ_ALWAYS_INLINE(void setKindAndTarget(Kind kind, word* target, SegmentBuilder* segment)) {\n    // Check that the target is really in the same segment, otherwise subtracting pointers is\n    // undefined behavior.  As it turns out, it's undefined behavior that actually produces\n    // unexpected results in a real-world situation that actually happened:  At one time,\n    // OrphanBuilder's \"tag\" (a WirePointer) was allowed to be initialized as if it lived in\n    // a particular segment when in fact it does not.  On 32-bit systems, where words might\n    // only be 32-bit aligned, it's possible that the difference between `this` and `target` is\n    // not a whole number of words.  But clang optimizes:\n    //     (target - (word*)this - 1) << 2\n    // to:\n    //     (((ptrdiff_t)target - (ptrdiff_t)this - 8) >> 1)\n    // So now when the pointers are not aligned the same, we can end up corrupting the bottom\n    // two bits, where `kind` is stored.  For example, this turns a struct into a far pointer.\n    // Ouch!\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(this) >=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(this) <\n                reinterpret_cast<uintptr_t>(segment->getStartPtr() + segment->getSize()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(target) >=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr()));\n    KJ_DREQUIRE(reinterpret_cast<uintptr_t>(target) <=\n                reinterpret_cast<uintptr_t>(segment->getStartPtr() + segment->getSize()));\n    offsetAndKind.set((static_cast<uint32_t>(target - reinterpret_cast<word*>(this) - 1) << 2) | kind);\n  }\n  KJ_ALWAYS_INLINE(void setKindWithZeroOffset(Kind kind)) {\n    offsetAndKind.set(kind);\n  }\n  KJ_ALWAYS_INLINE(void setKindAndTargetForEmptyStruct()) {\n    // This pointer points at an empty struct.  Assuming the WirePointer itself is in-bounds, we\n    // can set the target to point either at the WirePointer itself or immediately after it.  The\n    // latter would cause the WirePointer to be \"null\" (since for an empty struct the upper 32\n    // bits are going to be zero).  So we set an offset of -1, as if the struct were allocated\n    // immediately before this pointer, to distinguish it from null.\n    offsetAndKind.set(0xfffffffc);\n  }\n  KJ_ALWAYS_INLINE(void setKindForOrphan(Kind kind)) {\n    // OrphanBuilder contains a WirePointer, but since it isn't located in a segment, it should\n    // not have a valid offset (unless it is a FAR or OTHER pointer).  We set its offset to -1\n    // because setting it to zero would mean a pointer to an empty struct would appear to be a null\n    // pointer.\n    KJ_DREQUIRE(isPositional());\n    offsetAndKind.set(kind | 0xfffffffc);\n  }\n\n  KJ_ALWAYS_INLINE(ListElementCount inlineCompositeListElementCount() const) {\n    return ((bounded(offsetAndKind.get()) >> G(2))\n            & G(kj::maxValueForBits<LIST_ELEMENT_COUNT_BITS>())) * ELEMENTS;\n  }\n  KJ_ALWAYS_INLINE(void setKindAndInlineCompositeListElementCount(\n      Kind kind, ListElementCount elementCount)) {\n    offsetAndKind.set(unboundAs<uint32_t>((elementCount / ELEMENTS) << G(2)) | kind);\n  }\n\n  KJ_ALWAYS_INLINE(const word* farTarget(SegmentReader* segment) const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"farTarget() should only be called on FAR pointers.\");\n    return segment->checkOffset(segment->getStartPtr(), offsetAndKind.get() >> 3);\n  }\n  KJ_ALWAYS_INLINE(word* farTarget(SegmentBuilder* segment) const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"farTarget() should only be called on FAR pointers.\");\n    return segment->getPtrUnchecked((bounded(offsetAndKind.get()) >> G(3)) * WORDS);\n  }\n  KJ_ALWAYS_INLINE(bool isDoubleFar() const) {\n    KJ_DREQUIRE(kind() == FAR,\n        \"isDoubleFar() should only be called on FAR pointers.\");\n    return (offsetAndKind.get() >> 2) & 1;\n  }\n  KJ_ALWAYS_INLINE(void setFar(bool isDoubleFar, WordCountN<29> pos)) {\n    offsetAndKind.set(unboundAs<uint32_t>((pos / WORDS) << G(3)) |\n                      (static_cast<uint32_t>(isDoubleFar) << 2) |\n                      static_cast<uint32_t>(Kind::FAR));\n  }\n  KJ_ALWAYS_INLINE(void setCap(uint index)) {\n    offsetAndKind.set(static_cast<uint32_t>(Kind::OTHER));\n    capRef.index.set(index);\n  }\n\n  // -----------------------------------------------------------------\n  // Part of pointer that depends on the kind.\n\n  // Note:  Originally StructRef, ListRef, and FarRef were unnamed types, but this somehow\n  //   tickled a bug in GCC:\n  //     http://gcc.gnu.org/bugzilla/show_bug.cgi?id=58192\n  struct StructRef {\n    WireValue<WordCount16> dataSize;\n    WireValue<WirePointerCount16> ptrCount;\n\n    inline WordCountN<17> wordSize() const {\n      return upgradeBound<uint32_t>(dataSize.get()) + ptrCount.get() * WORDS_PER_POINTER;\n    }\n\n    KJ_ALWAYS_INLINE(void set(WordCount16 ds, WirePointerCount16 rc)) {\n      dataSize.set(ds);\n      ptrCount.set(rc);\n    }\n    KJ_ALWAYS_INLINE(void set(StructSize size)) {\n      dataSize.set(size.data);\n      ptrCount.set(size.pointers);\n    }\n  };\n\n  struct ListRef {\n    WireValue<uint32_t> elementSizeAndCount;\n\n    KJ_ALWAYS_INLINE(ElementSize elementSize() const) {\n      return static_cast<ElementSize>(elementSizeAndCount.get() & 7);\n    }\n    KJ_ALWAYS_INLINE(ElementCountN<29> elementCount() const) {\n      return (bounded(elementSizeAndCount.get()) >> G(3)) * ELEMENTS;\n    }\n    KJ_ALWAYS_INLINE(WordCountN<29> inlineCompositeWordCount() const) {\n      return elementCount() * (ONE * WORDS / ELEMENTS);\n    }\n\n    KJ_ALWAYS_INLINE(void set(ElementSize es, ElementCountN<29> ec)) {\n      elementSizeAndCount.set(unboundAs<uint32_t>((ec / ELEMENTS) << G(3)) |\n                              static_cast<int>(es));\n    }\n\n    KJ_ALWAYS_INLINE(void setInlineComposite(WordCountN<29> wc)) {\n      elementSizeAndCount.set(unboundAs<uint32_t>((wc / WORDS) << G(3)) |\n                              static_cast<int>(ElementSize::INLINE_COMPOSITE));\n    }\n  };\n\n  struct FarRef {\n    WireValue<SegmentId> segmentId;\n\n    KJ_ALWAYS_INLINE(void set(SegmentId si)) {\n      segmentId.set(si);\n    }\n  };\n\n  struct CapRef {\n    WireValue<uint32_t> index;\n    // Index into the message's capability table.\n  };\n\n  union {\n    uint32_t upper32Bits;\n\n    StructRef structRef;\n\n    ListRef listRef;\n\n    FarRef farRef;\n\n    CapRef capRef;\n  };\n\n  KJ_ALWAYS_INLINE(bool isNull() const) {\n    // If the upper 32 bits are zero, this is a pointer to an empty struct.  We consider that to be\n    // our \"null\" value.\n    return (offsetAndKind.get() == 0) & (upper32Bits == 0);\n  }\n\n};\nstatic_assert(sizeof(WirePointer) == sizeof(word),\n    \"capnp::WirePointer is not exactly one word.  This will probably break everything.\");\nstatic_assert(unboundAs<size_t>(POINTERS * WORDS_PER_POINTER * BYTES_PER_WORD / BYTES) ==\n              sizeof(WirePointer),\n    \"WORDS_PER_POINTER is wrong.\");\nstatic_assert(unboundAs<size_t>(POINTERS * BYTES_PER_POINTER / BYTES) == sizeof(WirePointer),\n    \"BYTES_PER_POINTER is wrong.\");\nstatic_assert(unboundAs<size_t>(POINTERS * BITS_PER_POINTER / BITS_PER_BYTE / BYTES) ==\n              sizeof(WirePointer),\n    \"BITS_PER_POINTER is wrong.\");\n\n#define OUT_OF_BOUNDS_ERROR_DETAIL \\\n    \"This usually indicates that \" \\\n    \"the input data was corrupted, used a different encoding than specified (e.g. \" \\\n    \"packed vs. non-packed), or was not a Cap'n Proto message to begin with. Note \" \\\n    \"that this error is NOT due to a schema mismatch; the input is invalid \" \\\n    \"regardless of schema.\"\n\nnamespace {\n\nstatic const union {\n  AlignedData<unbound(POINTER_SIZE_IN_WORDS / WORDS)> word;\n  WirePointer pointer;\n} zero = {{{0}}};\n\n}  // namespace\n\n// =======================================================================================\n\nnamespace {\n\ntemplate <typename T>\nstruct SegmentAnd {\n  SegmentBuilder* segment;\n  T value;\n};\n\n}  // namespace\n\nstruct WireHelpers {\n#if CAPNP_DEBUG_TYPES\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 7) / 8, T>, word> roundBytesUpToWords(\n          kj::Quantity<kj::Bounded<maxN, T>, byte> bytes)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bytes + G(7) * BYTES) / BYTES_PER_WORD;\n  }\n\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 7) / 8, T>, byte> roundBitsUpToBytes(\n          kj::Quantity<kj::Bounded<maxN, T>, BitLabel> bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n\n  template <uint64_t maxN, typename T>\n  static KJ_ALWAYS_INLINE(\n      kj::Quantity<kj::Bounded<(maxN + 63) / 64, T>, word> roundBitsUpToWords(\n          kj::Quantity<kj::Bounded<maxN, T>, BitLabel> bits)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bits + G(63) * BITS) / BITS_PER_WORD;\n  }\n#else\n  static KJ_ALWAYS_INLINE(WordCount roundBytesUpToWords(ByteCount bytes)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bytes + G(7) * BYTES) / BYTES_PER_WORD;\n  }\n\n  static KJ_ALWAYS_INLINE(ByteCount roundBitsUpToBytes(BitCount bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n\n  static KJ_ALWAYS_INLINE(WordCount64 roundBitsUpToWords(BitCount64 bits)) {\n    static_assert(sizeof(word) == 8, \"This code assumes 64-bit words.\");\n    return (bits + G(63) * BITS) / BITS_PER_WORD;\n  }\n\n  static KJ_ALWAYS_INLINE(ByteCount64 roundBitsUpToBytes(BitCount64 bits)) {\n    return (bits + G(7) * BITS) / BITS_PER_BYTE;\n  }\n#endif\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(byte* ptr, ByteCount32 count)) {\n    if (count != ZERO * BYTES) memset(ptr, 0, unbound(count / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(word* ptr, WordCountN<29> count)) {\n    if (count != ZERO * WORDS) memset(ptr, 0, unbound(count * BYTES_PER_WORD / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(WirePointer* ptr, WirePointerCountN<29> count)) {\n    if (count != ZERO * POINTERS) memset(ptr, 0, unbound(count * BYTES_PER_POINTER / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void zeroMemory(WirePointer* ptr)) {\n    memset(ptr, 0, sizeof(*ptr));\n  }\n\n  template <typename T>\n  static inline void zeroMemory(kj::ArrayPtr<T> array) {\n    if (array.size() != 0u) memset(array.begin(), 0, array.size() * sizeof(array[0]));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(byte* to, const byte* from, ByteCount32 count)) {\n    if (count != ZERO * BYTES) memcpy(to, from, unbound(count / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(word* to, const word* from, WordCountN<29> count)) {\n    if (count != ZERO * WORDS) memcpy(to, from, unbound(count * BYTES_PER_WORD / BYTES));\n  }\n\n  static KJ_ALWAYS_INLINE(void copyMemory(WirePointer* to, const WirePointer* from,\n                                          WirePointerCountN<29> count)) {\n    if (count != ZERO * POINTERS) memcpy(to, from, unbound(count * BYTES_PER_POINTER  / BYTES));\n  }\n\n  template <typename T>\n  static inline void copyMemory(T* to, const T* from) {\n    memcpy(to, from, sizeof(*from));\n  }\n\n  // TODO(cleanup): Turn these into a .copyTo() method of ArrayPtr?\n  template <typename T>\n  static inline void copyMemory(T* to, kj::ArrayPtr<T> from) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n  template <typename T>\n  static inline void copyMemory(T* to, kj::ArrayPtr<const T> from) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n  static KJ_ALWAYS_INLINE(void copyMemory(char* to, kj::StringPtr from)) {\n    if (from.size() != 0u) memcpy(to, from.begin(), from.size() * sizeof(from[0]));\n  }\n\n  static KJ_ALWAYS_INLINE(bool boundsCheck(\n      SegmentReader* segment, const word* start, WordCountN<31> size)) {\n    // If segment is null, this is an unchecked message, so we don't do bounds checks.\n    return segment == nullptr || segment->checkObject(start, size);\n  }\n\n  static KJ_ALWAYS_INLINE(bool amplifiedRead(SegmentReader* segment, WordCount virtualAmount)) {\n    // If segment is null, this is an unchecked message, so we don't do read limiter checks.\n    return segment == nullptr || segment->amplifiedRead(virtualAmount);\n  }\n\n  static KJ_ALWAYS_INLINE(word* allocate(\n      WirePointer*& ref, SegmentBuilder*& segment, CapTableBuilder* capTable,\n      SegmentWordCount amount, WirePointer::Kind kind, BuilderArena* orphanArena)) {\n    // Allocate space in the message for a new object, creating far pointers if necessary. The\n    // space is guaranteed to be zero'd (because MessageBuilder implementations are required to\n    // return zero'd memory).\n    //\n    // * `ref` starts out being a reference to the pointer which shall be assigned to point at the\n    //   new object.  On return, `ref` points to a pointer which needs to be initialized with\n    //   the object's type information.  Normally this is the same pointer, but it can change if\n    //   a far pointer was allocated -- in this case, `ref` will end up pointing to the far\n    //   pointer's tag.  Either way, `allocate()` takes care of making sure that the original\n    //   pointer ends up leading to the new object.  On return, only the upper 32 bit of `*ref`\n    //   need to be filled in by the caller.\n    // * `segment` starts out pointing to the segment containing `ref`.  On return, it points to\n    //   the segment containing the allocated object, which is usually the same segment but could\n    //   be a different one if the original segment was out of space.\n    // * `amount` is the number of words to allocate.\n    // * `kind` is the kind of object to allocate.  It is used to initialize the pointer.  It\n    //   cannot be `FAR` -- far pointers are allocated automatically as needed.\n    // * `orphanArena` is usually null.  If it is non-null, then we're allocating an orphan object.\n    //   In this case, `segment` starts out null; the allocation takes place in an arbitrary\n    //   segment belonging to the arena.  `ref` will be initialized as a non-far pointer, but its\n    //   target offset will be set to zero.\n\n    if (orphanArena == nullptr) {\n      if (!ref->isNull()) zeroObject(segment, capTable, ref);\n\n      if (amount == ZERO * WORDS && kind == WirePointer::STRUCT) {\n        // Note that the check for kind == WirePointer::STRUCT will hopefully cause this whole\n        // branch to be optimized away from all the call sites that are allocating non-structs.\n        ref->setKindAndTargetForEmptyStruct();\n        return reinterpret_cast<word*>(ref);\n      }\n\n      KJ_ASSUME(segment != nullptr);\n      word* ptr = segment->allocate(amount);\n\n      if (ptr == nullptr) {\n\n        // Need to allocate in a new segment.  We'll need to allocate an extra pointer worth of\n        // space to act as the landing pad for a far pointer.\n\n        WordCount amountPlusRef = amount + POINTER_SIZE_IN_WORDS;\n        auto allocation = segment->getArena()->allocate(\n            assertMaxBits<SEGMENT_WORD_COUNT_BITS>(amountPlusRef, []() {\n              KJ_FAIL_REQUIRE(\"requested object size exceeds maximum segment size\");\n            }));\n        segment = allocation.segment;\n        ptr = allocation.words;\n\n        // Set up the original pointer to be a far pointer to the new segment.\n        ref->setFar(false, segment->getOffsetTo(ptr));\n        ref->farRef.set(segment->getSegmentId());\n\n        // Initialize the landing pad to indicate that the data immediately follows the pad.\n        ref = reinterpret_cast<WirePointer*>(ptr);\n        ref->setKindAndTarget(kind, ptr + POINTER_SIZE_IN_WORDS, segment);\n\n        // Allocated space follows new pointer.\n        return ptr + POINTER_SIZE_IN_WORDS;\n      } else {\n        ref->setKindAndTarget(kind, ptr, segment);\n        return ptr;\n      }\n    } else {\n      // orphanArena is non-null.  Allocate an orphan.\n      KJ_DASSERT(ref->isNull());\n      auto allocation = orphanArena->allocate(amount);\n      segment = allocation.segment;\n      ref->setKindForOrphan(kind);\n      return allocation.words;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(word* followFarsNoWritableCheck(\n      WirePointer*& ref, word* refTarget, SegmentBuilder*& segment)) {\n    // If `ref` is a far pointer, follow it.  On return, `ref` will have been updated to point at\n    // a WirePointer that contains the type information about the target object, and a pointer to\n    // the object contents is returned.  The caller must NOT use `ref->target()` as this may or may\n    // not actually return a valid pointer.  `segment` is also updated to point at the segment which\n    // actually contains the object.\n    //\n    // If `ref` is not a far pointer, this simply returns `refTarget`.  Usually, `refTarget` should\n    // be the same as `ref->target()`, but may not be in cases where `ref` is only a tag.\n\n    if (ref->kind() == WirePointer::FAR) {\n      segment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n      WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(segment));\n      if (!ref->isDoubleFar()) {\n        ref = pad;\n        return pad->target();\n      }\n\n      // Landing pad is another far pointer.  It is followed by a tag describing the pointed-to\n      // object.\n      ref = pad + 1;\n\n      segment = segment->getArena()->getSegment(pad->farRef.segmentId.get());\n      return pad->farTarget(segment);\n    } else {\n      return refTarget;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(word* followFars(\n      WirePointer*& ref, word* refTarget, SegmentBuilder*& segment)) {\n    auto result = followFarsNoWritableCheck(ref, refTarget, segment);\n    segment->checkWritable();\n    return result;\n  }\n\n  static KJ_ALWAYS_INLINE(kj::Maybe<const word&> followFars(\n      const WirePointer*& ref, const word* refTarget, SegmentReader*& segment))\n      KJ_WARN_UNUSED_RESULT {\n    // Like the other followFars() but operates on readers.\n\n    // If the segment is null, this is an unchecked message, so there are no FAR pointers.\n    if (segment != nullptr && ref->kind() == WirePointer::FAR) {\n      // Look up the segment containing the landing pad.\n      segment = segment->getArena()->tryGetSegment(ref->farRef.segmentId.get());\n      KJ_REQUIRE(segment != nullptr, \"Message contains far pointer to unknown segment.\") {\n        return nullptr;\n      }\n\n      // Find the landing pad and check that it is within bounds.\n      const word* ptr = ref->farTarget(segment);\n      auto padWords = (ONE + bounded(ref->isDoubleFar())) * POINTER_SIZE_IN_WORDS;\n      KJ_REQUIRE(boundsCheck(segment, ptr, padWords),\n                 \"Message contains out-of-bounds far pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        return nullptr;\n      }\n\n      const WirePointer* pad = reinterpret_cast<const WirePointer*>(ptr);\n\n      // If this is not a double-far then the landing pad is our final pointer.\n      if (!ref->isDoubleFar()) {\n        ref = pad;\n        return pad->target(segment);\n      }\n\n      // Landing pad is another far pointer.  It is followed by a tag describing the pointed-to\n      // object.\n      ref = pad + 1;\n\n      SegmentReader* newSegment = segment->getArena()->tryGetSegment(pad->farRef.segmentId.get());\n      KJ_REQUIRE(newSegment != nullptr,\n          \"Message contains double-far pointer to unknown segment.\") {\n        return nullptr;\n      }\n      KJ_REQUIRE(pad->kind() == WirePointer::FAR,\n          \"Second word of double-far pad must be far pointer.\") {\n        return nullptr;\n      }\n\n      segment = newSegment;\n      return pad->farTarget(segment);\n    } else {\n      KJ_DASSERT(refTarget != nullptr);\n      return refTarget;\n    }\n  }\n\n  // -----------------------------------------------------------------\n\n  static void zeroObject(SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref) {\n    // Zero out the pointed-to object.  Use when the pointer is about to be overwritten making the\n    // target object no longer reachable.\n\n    // We shouldn't zero out external data linked into the message.\n    if (!segment->isWritable()) return;\n\n    switch (ref->kind()) {\n      case WirePointer::STRUCT:\n      case WirePointer::LIST:\n        zeroObject(segment, capTable, ref, ref->target());\n        break;\n      case WirePointer::FAR: {\n        segment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n        if (segment->isWritable()) {  // Don't zero external data.\n          WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(segment));\n\n          if (ref->isDoubleFar()) {\n            segment = segment->getArena()->getSegment(pad->farRef.segmentId.get());\n            if (segment->isWritable()) {\n              zeroObject(segment, capTable, pad + 1, pad->farTarget(segment));\n            }\n            zeroMemory(pad, G(2) * POINTERS);\n          } else {\n            zeroObject(segment, capTable, pad);\n            zeroMemory(pad);\n          }\n        }\n        break;\n      }\n      case WirePointer::OTHER:\n        if (ref->isCapability()) {\n#if CAPNP_LITE\n          KJ_FAIL_ASSERT(\"Capability encountered in builder in lite mode?\") { break; }\n#else  // CAPNP_LINE\n          capTable->dropCap(ref->capRef.index.get());\n#endif  // CAPNP_LITE, else\n        } else {\n          KJ_FAIL_REQUIRE(\"Unknown pointer type.\") { break; }\n        }\n        break;\n    }\n  }\n\n  static void zeroObject(SegmentBuilder* segment, CapTableBuilder* capTable,\n                         WirePointer* tag, word* ptr) {\n    // We shouldn't zero out external data linked into the message.\n    if (!segment->isWritable()) return;\n\n    switch (tag->kind()) {\n      case WirePointer::STRUCT: {\n        WirePointer* pointerSection =\n            reinterpret_cast<WirePointer*>(ptr + tag->structRef.dataSize.get());\n        for (auto i: kj::zeroTo(tag->structRef.ptrCount.get())) {\n          zeroObject(segment, capTable, pointerSection + i);\n        }\n        zeroMemory(ptr, tag->structRef.wordSize());\n        break;\n      }\n      case WirePointer::LIST: {\n        switch (tag->listRef.elementSize()) {\n          case ElementSize::VOID:\n            // Nothing.\n            break;\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            zeroMemory(ptr, roundBitsUpToWords(\n                upgradeBound<uint64_t>(tag->listRef.elementCount()) *\n                dataBitsPerElement(tag->listRef.elementSize())));\n            break;\n          }\n          case ElementSize::POINTER: {\n            WirePointer* typedPtr = reinterpret_cast<WirePointer*>(ptr);\n            auto count = tag->listRef.elementCount() * (ONE * POINTERS / ELEMENTS);\n            for (auto i: kj::zeroTo(count)) {\n              zeroObject(segment, capTable, typedPtr + i);\n            }\n            zeroMemory(typedPtr, count);\n            break;\n          }\n          case ElementSize::INLINE_COMPOSITE: {\n            WirePointer* elementTag = reinterpret_cast<WirePointer*>(ptr);\n\n            KJ_ASSERT(elementTag->kind() == WirePointer::STRUCT,\n                  \"Don't know how to handle non-STRUCT inline composite.\");\n            WordCount dataSize = elementTag->structRef.dataSize.get();\n            WirePointerCount pointerCount = elementTag->structRef.ptrCount.get();\n\n            auto count = elementTag->inlineCompositeListElementCount();\n            if (pointerCount > ZERO * POINTERS) {\n              word* pos = ptr + POINTER_SIZE_IN_WORDS;\n              for (auto i KJ_UNUSED: kj::zeroTo(count)) {\n                pos += dataSize;\n\n                for (auto j KJ_UNUSED: kj::zeroTo(pointerCount)) {\n                  zeroObject(segment, capTable, reinterpret_cast<WirePointer*>(pos));\n                  pos += POINTER_SIZE_IN_WORDS;\n                }\n              }\n            }\n\n            auto wordsPerElement = elementTag->structRef.wordSize() / ELEMENTS;\n            zeroMemory(ptr, assertMaxBits<SEGMENT_WORD_COUNT_BITS>(POINTER_SIZE_IN_WORDS +\n                upgradeBound<uint64_t>(count) * wordsPerElement, []() {\n                  KJ_FAIL_ASSERT(\"encountered list pointer in builder which is too large to \"\n                      \"possibly fit in a segment. Bug in builder code?\");\n                }));\n            break;\n          }\n        }\n        break;\n      }\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"Unexpected FAR pointer.\") {\n          break;\n        }\n        break;\n      case WirePointer::OTHER:\n        KJ_FAIL_ASSERT(\"Unexpected OTHER pointer.\") {\n          break;\n        }\n        break;\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(\n      void zeroPointerAndFars(SegmentBuilder* segment, WirePointer* ref)) {\n    // Zero out the pointer itself and, if it is a far pointer, zero the landing pad as well, but\n    // do not zero the object body.  Used when upgrading.\n\n    if (ref->kind() == WirePointer::FAR) {\n      SegmentBuilder* padSegment = segment->getArena()->getSegment(ref->farRef.segmentId.get());\n      if (padSegment->isWritable()) {  // Don't zero external data.\n        WirePointer* pad = reinterpret_cast<WirePointer*>(ref->farTarget(padSegment));\n        if (ref->isDoubleFar()) {\n          zeroMemory(pad, G(2) * POINTERS);\n        } else {\n          zeroMemory(pad);\n        }\n      }\n    }\n\n    zeroMemory(ref);\n  }\n\n\n  // -----------------------------------------------------------------\n\n  static MessageSizeCounts totalSize(\n      SegmentReader* segment, const WirePointer* ref, int nestingLimit) {\n    // Compute the total size of the object pointed to, not counting far pointer overhead.\n\n    MessageSizeCounts result = { ZERO * WORDS, 0 };\n\n    if (ref->isNull()) {\n      return result;\n    }\n\n    KJ_REQUIRE(nestingLimit > 0, \"Message is too deeply-nested.\") {\n      return result;\n    }\n    --nestingLimit;\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, ref->target(segment), segment)) {\n      ptr = p;\n    } else {\n      return result;\n    }\n\n    switch (ref->kind()) {\n      case WirePointer::STRUCT: {\n        KJ_REQUIRE(boundsCheck(segment, ptr, ref->structRef.wordSize()),\n                   \"Message contained out-of-bounds struct pointer. \"\n                   OUT_OF_BOUNDS_ERROR_DETAIL) {\n          return result;\n        }\n        result.addWords(ref->structRef.wordSize());\n\n        const WirePointer* pointerSection =\n            reinterpret_cast<const WirePointer*>(ptr + ref->structRef.dataSize.get());\n        for (auto i: kj::zeroTo(ref->structRef.ptrCount.get())) {\n          result += totalSize(segment, pointerSection + i, nestingLimit);\n        }\n        break;\n      }\n      case WirePointer::LIST: {\n        switch (ref->listRef.elementSize()) {\n          case ElementSize::VOID:\n            // Nothing.\n            break;\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            auto totalWords = roundBitsUpToWords(\n                upgradeBound<uint64_t>(ref->listRef.elementCount()) *\n                dataBitsPerElement(ref->listRef.elementSize()));\n            KJ_REQUIRE(boundsCheck(segment, ptr, totalWords),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n            result.addWords(totalWords);\n            break;\n          }\n          case ElementSize::POINTER: {\n            auto count = ref->listRef.elementCount() * (POINTERS / ELEMENTS);\n\n            KJ_REQUIRE(boundsCheck(segment, ptr, count * WORDS_PER_POINTER),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            result.addWords(count * WORDS_PER_POINTER);\n\n            for (auto i: kj::zeroTo(count)) {\n              result += totalSize(segment, reinterpret_cast<const WirePointer*>(ptr) + i,\n                                  nestingLimit);\n            }\n            break;\n          }\n          case ElementSize::INLINE_COMPOSITE: {\n            auto wordCount = ref->listRef.inlineCompositeWordCount();\n            KJ_REQUIRE(boundsCheck(segment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                       \"Message contained out-of-bounds list pointer. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            const WirePointer* elementTag = reinterpret_cast<const WirePointer*>(ptr);\n            auto count = elementTag->inlineCompositeListElementCount();\n\n            KJ_REQUIRE(elementTag->kind() == WirePointer::STRUCT,\n                       \"Don't know how to handle non-STRUCT inline composite.\") {\n              return result;\n            }\n\n            auto actualSize = elementTag->structRef.wordSize() / ELEMENTS *\n                              upgradeBound<uint64_t>(count);\n            KJ_REQUIRE(actualSize <= wordCount,\n                       \"Struct list pointer's elements overran size. \"\n                       OUT_OF_BOUNDS_ERROR_DETAIL) {\n              return result;\n            }\n\n            // We count the actual size rather than the claimed word count because that's what\n            // we'll end up with if we make a copy.\n            result.addWords(actualSize + POINTER_SIZE_IN_WORDS);\n\n            WordCount dataSize = elementTag->structRef.dataSize.get();\n            WirePointerCount pointerCount = elementTag->structRef.ptrCount.get();\n\n            if (pointerCount > ZERO * POINTERS) {\n              const word* pos = ptr + POINTER_SIZE_IN_WORDS;\n              for (auto i KJ_UNUSED: kj::zeroTo(count)) {\n                pos += dataSize;\n\n                for (auto j KJ_UNUSED: kj::zeroTo(pointerCount)) {\n                  result += totalSize(segment, reinterpret_cast<const WirePointer*>(pos),\n                                      nestingLimit);\n                  pos += POINTER_SIZE_IN_WORDS;\n                }\n              }\n            }\n            break;\n          }\n        }\n        break;\n      }\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unexpected FAR pointer.\") {\n          break;\n        }\n        break;\n      case WirePointer::OTHER:\n        if (ref->isCapability()) {\n          result.capCount++;\n        } else {\n          KJ_FAIL_REQUIRE(\"Unknown pointer type.\") { break; }\n        }\n        break;\n    }\n\n    return result;\n  }\n\n  // -----------------------------------------------------------------\n  // Copy from an unchecked message.\n\n  static KJ_ALWAYS_INLINE(\n      void copyStruct(SegmentBuilder* segment, CapTableBuilder* capTable,\n                      word* dst, const word* src,\n                      StructDataWordCount dataSize, StructPointerCount pointerCount)) {\n    copyMemory(dst, src, dataSize);\n\n    const WirePointer* srcRefs = reinterpret_cast<const WirePointer*>(src + dataSize);\n    WirePointer* dstRefs = reinterpret_cast<WirePointer*>(dst + dataSize);\n\n    for (auto i: kj::zeroTo(pointerCount)) {\n      SegmentBuilder* subSegment = segment;\n      WirePointer* dstRef = dstRefs + i;\n      copyMessage(subSegment, capTable, dstRef, srcRefs + i);\n    }\n  }\n\n  static word* copyMessage(\n      SegmentBuilder*& segment, CapTableBuilder* capTable,\n      WirePointer*& dst, const WirePointer* src) {\n    // Not always-inline because it's recursive.\n\n    switch (src->kind()) {\n      case WirePointer::STRUCT: {\n        if (src->isNull()) {\n          zeroMemory(dst);\n          return nullptr;\n        } else {\n          const word* srcPtr = src->target(nullptr);\n          word* dstPtr = allocate(\n              dst, segment, capTable, src->structRef.wordSize(), WirePointer::STRUCT, nullptr);\n\n          copyStruct(segment, capTable, dstPtr, srcPtr, src->structRef.dataSize.get(),\n                     src->structRef.ptrCount.get());\n\n          dst->structRef.set(src->structRef.dataSize.get(), src->structRef.ptrCount.get());\n          return dstPtr;\n        }\n      }\n      case WirePointer::LIST: {\n        switch (src->listRef.elementSize()) {\n          case ElementSize::VOID:\n          case ElementSize::BIT:\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES: {\n            auto wordCount = roundBitsUpToWords(\n                upgradeBound<uint64_t>(src->listRef.elementCount()) *\n                dataBitsPerElement(src->listRef.elementSize()));\n            const word* srcPtr = src->target(nullptr);\n            word* dstPtr = allocate(dst, segment, capTable, wordCount, WirePointer::LIST, nullptr);\n            copyMemory(dstPtr, srcPtr, wordCount);\n\n            dst->listRef.set(src->listRef.elementSize(), src->listRef.elementCount());\n            return dstPtr;\n          }\n\n          case ElementSize::POINTER: {\n            const WirePointer* srcRefs = reinterpret_cast<const WirePointer*>(src->target(nullptr));\n            WirePointer* dstRefs = reinterpret_cast<WirePointer*>(\n                allocate(dst, segment, capTable, src->listRef.elementCount() *\n                    (ONE * POINTERS / ELEMENTS) * WORDS_PER_POINTER,\n                    WirePointer::LIST, nullptr));\n\n            for (auto i: kj::zeroTo(src->listRef.elementCount() * (ONE * POINTERS / ELEMENTS))) {\n              SegmentBuilder* subSegment = segment;\n              WirePointer* dstRef = dstRefs + i;\n              copyMessage(subSegment, capTable, dstRef, srcRefs + i);\n            }\n\n            dst->listRef.set(ElementSize::POINTER, src->listRef.elementCount());\n            return reinterpret_cast<word*>(dstRefs);\n          }\n\n          case ElementSize::INLINE_COMPOSITE: {\n            const word* srcPtr = src->target(nullptr);\n            word* dstPtr = allocate(dst, segment, capTable,\n                assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n                    src->listRef.inlineCompositeWordCount() + POINTER_SIZE_IN_WORDS,\n                    []() { KJ_FAIL_ASSERT(\"list too big to fit in a segment\"); }),\n                WirePointer::LIST, nullptr);\n\n            dst->listRef.setInlineComposite(src->listRef.inlineCompositeWordCount());\n\n            const WirePointer* srcTag = reinterpret_cast<const WirePointer*>(srcPtr);\n            copyMemory(reinterpret_cast<WirePointer*>(dstPtr), srcTag);\n\n            const word* srcElement = srcPtr + POINTER_SIZE_IN_WORDS;\n            word* dstElement = dstPtr + POINTER_SIZE_IN_WORDS;\n\n            KJ_ASSERT(srcTag->kind() == WirePointer::STRUCT,\n                \"INLINE_COMPOSITE of lists is not yet supported.\");\n\n            for (auto i KJ_UNUSED: kj::zeroTo(srcTag->inlineCompositeListElementCount())) {\n              copyStruct(segment, capTable, dstElement, srcElement,\n                  srcTag->structRef.dataSize.get(), srcTag->structRef.ptrCount.get());\n              srcElement += srcTag->structRef.wordSize();\n              dstElement += srcTag->structRef.wordSize();\n            }\n            return dstPtr;\n          }\n        }\n        break;\n      }\n      case WirePointer::OTHER:\n        KJ_FAIL_REQUIRE(\"Unchecked messages cannot contain OTHER pointers (e.g. capabilities).\");\n        break;\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unchecked messages cannot contain far pointers.\");\n        break;\n    }\n\n    return nullptr;\n  }\n\n  static void transferPointer(SegmentBuilder* dstSegment, WirePointer* dst,\n                              SegmentBuilder* srcSegment, WirePointer* src) {\n    // Make *dst point to the same object as *src.  Both must reside in the same message, but can\n    // be in different segments.  Not always-inline because this is rarely used.\n    //\n    // Caller MUST zero out the source pointer after calling this, to make sure no later code\n    // mistakenly thinks the source location still owns the object.  transferPointer() doesn't do\n    // this zeroing itself because many callers transfer several pointers in a loop then zero out\n    // the whole section.\n\n    KJ_DASSERT(dst->isNull());\n    // We expect the caller to ensure the target is already null so won't leak.\n\n    if (src->isNull()) {\n      zeroMemory(dst);\n    } else if (src->isPositional()) {\n      transferPointer(dstSegment, dst, srcSegment, src, src->target());\n    } else {\n      // Far and other pointers are position-independent, so we can just copy.\n      copyMemory(dst, src);\n    }\n  }\n\n  static void transferPointer(SegmentBuilder* dstSegment, WirePointer* dst,\n                              SegmentBuilder* srcSegment, const WirePointer* srcTag,\n                              word* srcPtr) {\n    // Like the other overload, but splits src into a tag and a target.  Particularly useful for\n    // OrphanBuilder.\n\n    if (dstSegment == srcSegment) {\n      // Same segment, so create a direct pointer.\n\n      if (srcTag->kind() == WirePointer::STRUCT && srcTag->structRef.wordSize() == ZERO * WORDS) {\n        dst->setKindAndTargetForEmptyStruct();\n      } else {\n        dst->setKindAndTarget(srcTag->kind(), srcPtr, dstSegment);\n      }\n\n      // We can just copy the upper 32 bits.  (Use memcpy() to comply with aliasing rules.)\n      copyMemory(&dst->upper32Bits, &srcTag->upper32Bits);\n    } else {\n      // Need to create a far pointer.  Try to allocate it in the same segment as the source, so\n      // that it doesn't need to be a double-far.\n\n      WirePointer* landingPad =\n          reinterpret_cast<WirePointer*>(srcSegment->allocate(G(1) * WORDS));\n      if (landingPad == nullptr) {\n        // Darn, need a double-far.\n        auto allocation = srcSegment->getArena()->allocate(G(2) * WORDS);\n        SegmentBuilder* farSegment = allocation.segment;\n        landingPad = reinterpret_cast<WirePointer*>(allocation.words);\n\n        landingPad[0].setFar(false, srcSegment->getOffsetTo(srcPtr));\n        landingPad[0].farRef.segmentId.set(srcSegment->getSegmentId());\n\n        landingPad[1].setKindWithZeroOffset(srcTag->kind());\n        copyMemory(&landingPad[1].upper32Bits, &srcTag->upper32Bits);\n\n        dst->setFar(true, farSegment->getOffsetTo(reinterpret_cast<word*>(landingPad)));\n        dst->farRef.set(farSegment->getSegmentId());\n      } else {\n        // Simple landing pad is just a pointer.\n        landingPad->setKindAndTarget(srcTag->kind(), srcPtr, srcSegment);\n        copyMemory(&landingPad->upper32Bits, &srcTag->upper32Bits);\n\n        dst->setFar(false, srcSegment->getOffsetTo(reinterpret_cast<word*>(landingPad)));\n        dst->farRef.set(srcSegment->getSegmentId());\n      }\n    }\n  }\n\n  // -----------------------------------------------------------------\n\n  static KJ_ALWAYS_INLINE(StructBuilder initStructPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, StructSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // Allocate space for the new struct.  Newly-allocated space is automatically zeroed.\n    word* ptr = allocate(ref, segment, capTable, size.total(), WirePointer::STRUCT, orphanArena);\n\n    // Initialize the pointer.\n    ref->structRef.set(size);\n\n    // Build the StructBuilder.\n    return StructBuilder(segment, capTable, ptr, reinterpret_cast<WirePointer*>(ptr + size.data),\n                         size.data * BITS_PER_WORD, size.pointers);\n  }\n\n  static KJ_ALWAYS_INLINE(StructBuilder getWritableStructPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, StructSize size,\n      const word* defaultValue)) {\n    return getWritableStructPointer(ref, ref->target(), segment, capTable, size, defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(StructBuilder getWritableStructPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      StructSize size, const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return initStructPointer(ref, segment, capTable, size, orphanArena);\n      }\n      refTarget = copyMessage(segment, capTable, ref,\n          reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    WirePointer* oldRef = ref;\n    SegmentBuilder* oldSegment = segment;\n    word* oldPtr = followFars(oldRef, refTarget, oldSegment);\n\n    KJ_REQUIRE(oldRef->kind() == WirePointer::STRUCT,\n        \"Schema mismatch: Message contains non-struct pointer where struct pointer was expected.\") {\n      goto useDefault;\n    }\n\n    auto oldDataSize = oldRef->structRef.dataSize.get();\n    auto oldPointerCount = oldRef->structRef.ptrCount.get();\n    WirePointer* oldPointerSection =\n        reinterpret_cast<WirePointer*>(oldPtr + oldDataSize);\n\n    if (oldDataSize < size.data || oldPointerCount < size.pointers) {\n      // The space allocated for this struct is too small.  Unlike with readers, we can't just\n      // run with it and do bounds checks at access time, because how would we handle writes?\n      // Instead, we have to copy the struct to a new space now.\n\n      auto newDataSize = kj::max(oldDataSize, size.data);\n      auto newPointerCount = kj::max(oldPointerCount, size.pointers);\n      auto totalSize = newDataSize + newPointerCount * WORDS_PER_POINTER;\n\n      // Don't let allocate() zero out the object just yet.\n      zeroPointerAndFars(segment, ref);\n\n      word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::STRUCT, orphanArena);\n      ref->structRef.set(newDataSize, newPointerCount);\n\n      // Copy data section.\n      copyMemory(ptr, oldPtr, oldDataSize);\n\n      // Copy pointer section.\n      WirePointer* newPointerSection = reinterpret_cast<WirePointer*>(ptr + newDataSize);\n      for (auto i: kj::zeroTo(oldPointerCount)) {\n        transferPointer(segment, newPointerSection + i, oldSegment, oldPointerSection + i);\n      }\n\n      // Zero out old location.  This has two purposes:\n      // 1) We don't want to leak the original contents of the struct when the message is written\n      //    out as it may contain secrets that the caller intends to remove from the new copy.\n      // 2) Zeros will be deflated by packing, making this dead memory almost-free if it ever\n      //    hits the wire.\n      zeroMemory(oldPtr, oldDataSize + oldPointerCount * WORDS_PER_POINTER);\n\n      return StructBuilder(segment, capTable, ptr, newPointerSection, newDataSize * BITS_PER_WORD,\n                           newPointerCount);\n    } else {\n      return StructBuilder(oldSegment, capTable, oldPtr, oldPointerSection,\n                           oldDataSize * BITS_PER_WORD, oldPointerCount);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder initListPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      ElementCount elementCount, ElementSize elementSize, BuilderArena* orphanArena = nullptr)) {\n    KJ_DREQUIRE(elementSize != ElementSize::INLINE_COMPOSITE,\n        \"Should have called initStructListPointer() instead.\");\n\n    auto checkedElementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount,\n        []() { KJ_FAIL_REQUIRE(\"tried to allocate list with too many elements\"); });\n\n    auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n    auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n    auto step = bitsPerElementIncludingPointers(elementSize);\n    KJ_DASSERT(step * ELEMENTS == (dataSize + pointerCount * BITS_PER_POINTER));\n\n    // Calculate size of the list.\n    auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(checkedElementCount) * step);\n\n    // Allocate the list.\n    word* ptr = allocate(ref, segment, capTable, wordCount, WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(elementSize, checkedElementCount);\n\n    // Build the ListBuilder.\n    return ListBuilder(segment, capTable, ptr, step, checkedElementCount,\n                       dataSize, pointerCount, elementSize);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder initStructListPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      ElementCount elementCount, StructSize elementSize, BuilderArena* orphanArena = nullptr)) {\n    auto checkedElementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount,\n        []() { KJ_FAIL_REQUIRE(\"tried to allocate list with too many elements\"); });\n\n    WordsPerElementN<17> wordsPerElement = elementSize.total() / ELEMENTS;\n\n    // Allocate the list, prefixed by a single WirePointer.\n    auto wordCount = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n        upgradeBound<uint64_t>(checkedElementCount) * wordsPerElement,\n        []() { KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\"); });\n    word* ptr = allocate(ref, segment, capTable, POINTER_SIZE_IN_WORDS + wordCount,\n                         WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    // INLINE_COMPOSITE lists replace the element count with the word count.\n    ref->listRef.setInlineComposite(wordCount);\n\n    // Initialize the list tag.\n    reinterpret_cast<WirePointer*>(ptr)->setKindAndInlineCompositeListElementCount(\n        WirePointer::STRUCT, checkedElementCount);\n    reinterpret_cast<WirePointer*>(ptr)->structRef.set(elementSize);\n    ptr += POINTER_SIZE_IN_WORDS;\n\n    // Build the ListBuilder.\n    return ListBuilder(segment, capTable, ptr, wordsPerElement * BITS_PER_WORD, checkedElementCount,\n                       elementSize.data * BITS_PER_WORD, elementSize.pointers,\n                       ElementSize::INLINE_COMPOSITE);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointer(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      ElementSize elementSize, const word* defaultValue)) {\n    return getWritableListPointer(origRef, origRef->target(), origSegment, capTable, elementSize,\n                                  defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointer(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable, ElementSize elementSize,\n      const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    KJ_DREQUIRE(elementSize != ElementSize::INLINE_COMPOSITE,\n             \"Use getWritableStructListPointer() for struct lists.\");\n\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(elementSize);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    // We must verify that the pointer has the right size.  Unlike in\n    // getWritableStructListPointer(), we never need to \"upgrade\" the data, because this\n    // method is called only for non-struct lists, and there is no allowed upgrade path *to*\n    // a non-struct list, only *from* them.\n\n    WirePointer* ref = origRef;\n    SegmentBuilder* segment = origSegment;\n    word* ptr = followFars(ref, origRefTarget, segment);\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n        \"Schema mismatch: Called getWritableListPointer() but existing pointer is not a list.\") {\n      goto useDefault;\n    }\n\n    ElementSize oldSize = ref->listRef.elementSize();\n\n    if (oldSize == ElementSize::INLINE_COMPOSITE) {\n      // The existing element size is INLINE_COMPOSITE, though we expected a list of primitives.\n      // The existing data must have been written with a newer version of the protocol.  We\n      // therefore never need to upgrade the data in this case, but we do need to validate that it\n      // is a valid upgrade from what we expected.\n\n      // Read the tag to get the actual element count.\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n          \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\");\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      auto dataSize = tag->structRef.dataSize.get();\n      auto pointerCount = tag->structRef.ptrCount.get();\n\n      switch (elementSize) {\n        case ElementSize::VOID:\n          // Anything is a valid upgrade from Void.\n          break;\n\n        case ElementSize::BIT:\n          KJ_FAIL_REQUIRE(\n              \"Schema mismatch: Found struct list where bit list was expected; upgrading boolean \"\n              \"lists to structs is no longer supported.\") {\n            goto useDefault;\n          }\n          break;\n\n        case ElementSize::BYTE:\n        case ElementSize::TWO_BYTES:\n        case ElementSize::FOUR_BYTES:\n        case ElementSize::EIGHT_BYTES:\n          KJ_REQUIRE(dataSize >= ONE * WORDS,\n                     \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n            goto useDefault;\n          }\n          break;\n\n        case ElementSize::POINTER:\n          KJ_REQUIRE(pointerCount >= ONE * POINTERS,\n                     \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n            goto useDefault;\n          }\n          // Adjust the pointer to point at the reference segment.\n          ptr += dataSize;\n          break;\n\n        case ElementSize::INLINE_COMPOSITE:\n          KJ_UNREACHABLE;\n      }\n\n      // OK, looks valid.\n\n      return ListBuilder(segment, capTable, ptr,\n                         tag->structRef.wordSize() * BITS_PER_WORD / ELEMENTS,\n                         tag->inlineCompositeListElementCount(),\n                         dataSize * BITS_PER_WORD, pointerCount, ElementSize::INLINE_COMPOSITE);\n    } else {\n      auto dataSize = dataBitsPerElement(oldSize) * ELEMENTS;\n      auto pointerCount = pointersPerElement(oldSize) * ELEMENTS;\n\n      if (elementSize == ElementSize::BIT) {\n        KJ_REQUIRE(oldSize == ElementSize::BIT,\n            \"Schema mismatch: Found non-bit list where bit list was expected.\") {\n          goto useDefault;\n        }\n      } else {\n        KJ_REQUIRE(oldSize != ElementSize::BIT,\n            \"Schema mismatch: Found bit list where non-bit list was expected.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(dataSize >= dataBitsPerElement(elementSize) * ELEMENTS,\n                   \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(pointerCount >= pointersPerElement(elementSize) * ELEMENTS,\n                   \"Schema mismatch: Existing list value is incompatible with expected type.\") {\n          goto useDefault;\n        }\n      }\n\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n      return ListBuilder(segment, capTable, ptr, step, ref->listRef.elementCount(),\n                         dataSize, pointerCount, oldSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointerAnySize(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      const word* defaultValue)) {\n    return getWritableListPointerAnySize(origRef, origRef->target(), origSegment,\n                                         capTable, defaultValue);\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableListPointerAnySize(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(ElementSize::VOID);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    WirePointer* ref = origRef;\n    SegmentBuilder* segment = origSegment;\n    word* ptr = followFars(ref, origRefTarget, segment);\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n        \"Schema mismatch: Called getWritableListPointerAnySize() but existing pointer is not a \"\n        \"list.\") {\n      goto useDefault;\n    }\n\n    ElementSize elementSize = ref->listRef.elementSize();\n\n    if (elementSize == ElementSize::INLINE_COMPOSITE) {\n      // Read the tag to get the actual element count.\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n          \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\");\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      return ListBuilder(segment, capTable, ptr,\n                         tag->structRef.wordSize() * BITS_PER_WORD / ELEMENTS,\n                         tag->inlineCompositeListElementCount(),\n                         tag->structRef.dataSize.get() * BITS_PER_WORD,\n                         tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE);\n    } else {\n      auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n      auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n      return ListBuilder(segment, capTable, ptr, step, ref->listRef.elementCount(),\n                         dataSize, pointerCount, elementSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableStructListPointer(\n      WirePointer* origRef, SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      StructSize elementSize, const word* defaultValue)) {\n    return getWritableStructListPointer(origRef, origRef->target(), origSegment, capTable,\n                                        elementSize, defaultValue);\n  }\n  static KJ_ALWAYS_INLINE(ListBuilder getWritableStructListPointer(\n      WirePointer* origRef, word* origRefTarget,\n      SegmentBuilder* origSegment, CapTableBuilder* capTable,\n      StructSize elementSize, const word* defaultValue, BuilderArena* orphanArena = nullptr)) {\n    if (origRef->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListBuilder(ElementSize::INLINE_COMPOSITE);\n      }\n      origRefTarget = copyMessage(\n          origSegment, capTable, origRef, reinterpret_cast<const WirePointer*>(defaultValue));\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    // We must verify that the pointer has the right size and potentially upgrade it if not.\n\n    WirePointer* oldRef = origRef;\n    SegmentBuilder* oldSegment = origSegment;\n    word* oldPtr = followFars(oldRef, origRefTarget, oldSegment);\n\n    KJ_REQUIRE(oldRef->kind() == WirePointer::LIST,\n               \"Schema mismatch: Called getList{Field,Element}() but existing pointer is not a \"\n               \"list.\") {\n      goto useDefault;\n    }\n\n    ElementSize oldSize = oldRef->listRef.elementSize();\n\n    if (oldSize == ElementSize::INLINE_COMPOSITE) {\n      // Existing list is INLINE_COMPOSITE, but we need to verify that the sizes match.\n\n      WirePointer* oldTag = reinterpret_cast<WirePointer*>(oldPtr);\n      oldPtr += POINTER_SIZE_IN_WORDS;\n      KJ_REQUIRE(oldTag->kind() == WirePointer::STRUCT,\n                 \"INLINE_COMPOSITE list with non-STRUCT elements not supported.\") {\n        goto useDefault;\n      }\n\n      auto oldDataSize = oldTag->structRef.dataSize.get();\n      auto oldPointerCount = oldTag->structRef.ptrCount.get();\n      auto oldStep = (oldDataSize + oldPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n\n      auto elementCount = oldTag->inlineCompositeListElementCount();\n\n      if (oldDataSize >= elementSize.data && oldPointerCount >= elementSize.pointers) {\n        // Old size is at least as large as we need.  Ship it.\n        return ListBuilder(oldSegment, capTable, oldPtr, oldStep * BITS_PER_WORD, elementCount,\n                           oldDataSize * BITS_PER_WORD, oldPointerCount,\n                           ElementSize::INLINE_COMPOSITE);\n      }\n\n      // The structs in this list are smaller than expected, probably written using an older\n      // version of the protocol.  We need to make a copy and expand them.\n\n      auto newDataSize = kj::max(oldDataSize, elementSize.data);\n      auto newPointerCount = kj::max(oldPointerCount, elementSize.pointers);\n      auto newStep = (newDataSize + newPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n\n      auto totalSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n            newStep * upgradeBound<uint64_t>(elementCount),\n            []() { KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\"); });\n\n      // Don't let allocate() zero out the object just yet.\n      zeroPointerAndFars(origSegment, origRef);\n\n      word* newPtr = allocate(origRef, origSegment, capTable, totalSize + POINTER_SIZE_IN_WORDS,\n                              WirePointer::LIST, orphanArena);\n      origRef->listRef.setInlineComposite(totalSize);\n\n      WirePointer* newTag = reinterpret_cast<WirePointer*>(newPtr);\n      newTag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, elementCount);\n      newTag->structRef.set(newDataSize, newPointerCount);\n      newPtr += POINTER_SIZE_IN_WORDS;\n\n      word* src = oldPtr;\n      word* dst = newPtr;\n      for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n        // Copy data section.\n        copyMemory(dst, src, oldDataSize);\n\n        // Copy pointer section.\n        WirePointer* newPointerSection = reinterpret_cast<WirePointer*>(dst + newDataSize);\n        WirePointer* oldPointerSection = reinterpret_cast<WirePointer*>(src + oldDataSize);\n        for (auto j: kj::zeroTo(oldPointerCount)) {\n          transferPointer(origSegment, newPointerSection + j, oldSegment, oldPointerSection + j);\n        }\n\n        dst += newStep * (ONE * ELEMENTS);\n        src += oldStep * (ONE * ELEMENTS);\n      }\n\n      auto oldSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n            oldStep * upgradeBound<uint64_t>(elementCount),\n            []() { KJ_FAIL_ASSERT(\"old size overflows but new size doesn't?\"); });\n\n      // Zero out old location.  See explanation in getWritableStructPointer().\n      // Make sure to include the tag word.\n      zeroMemory(oldPtr - POINTER_SIZE_IN_WORDS, oldSize + POINTER_SIZE_IN_WORDS);\n\n      return ListBuilder(origSegment, capTable, newPtr, newStep * BITS_PER_WORD, elementCount,\n                         newDataSize * BITS_PER_WORD, newPointerCount,\n                         ElementSize::INLINE_COMPOSITE);\n    } else {\n      // We're upgrading from a non-struct list.\n\n      auto oldDataSize = dataBitsPerElement(oldSize) * ELEMENTS;\n      auto oldPointerCount = pointersPerElement(oldSize) * ELEMENTS;\n      auto oldStep = (oldDataSize + oldPointerCount * BITS_PER_POINTER) / ELEMENTS;\n      auto elementCount = oldRef->listRef.elementCount();\n\n      if (oldSize == ElementSize::VOID) {\n        // Nothing to copy, just allocate a new list.\n        return initStructListPointer(origRef, origSegment, capTable, elementCount, elementSize);\n      } else {\n        // Upgrading to an inline composite list.\n\n        KJ_REQUIRE(oldSize != ElementSize::BIT,\n            \"Schema mismatch: Found bit list where struct list was expected; upgrading boolean \"\n            \"lists to structs is no longer supported.\") {\n          goto useDefault;\n        }\n\n        auto newDataSize = elementSize.data;\n        auto newPointerCount = elementSize.pointers;\n\n        if (oldSize == ElementSize::POINTER) {\n          newPointerCount = kj::max(newPointerCount, ONE * POINTERS);\n        } else {\n          // Old list contains data elements, so we need at least 1 word of data.\n          newDataSize = kj::max(newDataSize, ONE * WORDS);\n        }\n\n        auto newStep = (newDataSize + newPointerCount * WORDS_PER_POINTER) / ELEMENTS;\n        auto totalWords = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n              newStep * upgradeBound<uint64_t>(elementCount),\n              []() {KJ_FAIL_REQUIRE(\"total size of struct list is larger than max segment size\");});\n\n        // Don't let allocate() zero out the object just yet.\n        zeroPointerAndFars(origSegment, origRef);\n\n        word* newPtr = allocate(origRef, origSegment, capTable, totalWords + POINTER_SIZE_IN_WORDS,\n                                WirePointer::LIST, orphanArena);\n        origRef->listRef.setInlineComposite(totalWords);\n\n        WirePointer* tag = reinterpret_cast<WirePointer*>(newPtr);\n        tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, elementCount);\n        tag->structRef.set(newDataSize, newPointerCount);\n        newPtr += POINTER_SIZE_IN_WORDS;\n\n        if (oldSize == ElementSize::POINTER) {\n          WirePointer* dst = reinterpret_cast<WirePointer*>(newPtr + newDataSize);\n          WirePointer* src = reinterpret_cast<WirePointer*>(oldPtr);\n          for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n            transferPointer(origSegment, dst, oldSegment, src);\n            dst += newStep / WORDS_PER_POINTER * (ONE * ELEMENTS);\n            ++src;\n          }\n        } else {\n          byte* dst = reinterpret_cast<byte*>(newPtr);\n          byte* src = reinterpret_cast<byte*>(oldPtr);\n          auto newByteStep = newStep * (ONE * ELEMENTS) * BYTES_PER_WORD;\n          auto oldByteStep = oldDataSize / BITS_PER_BYTE;\n          for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n            copyMemory(dst, src, oldByteStep);\n            src += oldByteStep;\n            dst += newByteStep;\n          }\n        }\n\n        auto oldSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n              roundBitsUpToWords(oldStep * upgradeBound<uint64_t>(elementCount)),\n              []() { KJ_FAIL_ASSERT(\"old size overflows but new size doesn't?\"); });\n\n        // Zero out old location.  See explanation in getWritableStructPointer().\n        zeroMemory(oldPtr, oldSize);\n\n        return ListBuilder(origSegment, capTable, newPtr, newStep * BITS_PER_WORD, elementCount,\n                           newDataSize * BITS_PER_WORD, newPointerCount,\n                           ElementSize::INLINE_COMPOSITE);\n      }\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Text::Builder> initTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, TextSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // The byte list must include a NUL terminator.\n    auto byteSize = size + ONE * BYTES;\n\n    // Allocate the space.\n    word* ptr = allocate(\n        ref, segment, capTable, roundBytesUpToWords(byteSize), WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(ElementSize::BYTE, byteSize * (ONE * ELEMENTS / BYTES));\n\n    // Build the Text::Builder. Note that since allocate()ed memory is pre-zero'd, we don't need\n    // to initialize the NUL terminator.\n    return { segment, Text::Builder(reinterpret_cast<char*>(ptr), unbound(size / BYTES)) };\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Text::Builder> setTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, Text::Reader value,\n      BuilderArena* orphanArena = nullptr)) {\n    TextSize size = assertMax<MAX_TEXT_SIZE>(bounded(value.size()),\n        []() { KJ_FAIL_REQUIRE(\"text blob too big\"); }) * BYTES;\n\n    auto allocation = initTextPointer(ref, segment, capTable, size, orphanArena);\n    copyMemory(allocation.value.begin(), value);\n    return allocation;\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Builder getWritableTextPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, TextSize defaultSize)) {\n    return getWritableTextPointer(ref, ref->target(), segment,capTable,  defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Builder getWritableTextPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, TextSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultSize == ZERO * BYTES) {\n        return nullptr;\n      } else {\n        Text::Builder builder = initTextPointer(ref, segment, capTable, defaultSize).value;\n        copyMemory(builder.asBytes().begin(), reinterpret_cast<const byte*>(defaultValue),\n                   defaultSize);\n        return builder;\n      }\n    } else {\n      word* ptr = followFars(ref, refTarget, segment);\n      byte* bptr = reinterpret_cast<byte*>(ptr);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n          \"Schema mismatch: Called getText{Field,Element}() but existing pointer is not a list.\") {\n        goto useDefault;\n      }\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n          \"Schema mismatch: Called getText{Field,Element}() but existing list pointer is not \"\n          \"byte-sized.\") {\n        goto useDefault;\n      }\n\n      auto maybeSize = trySubtract(ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS),\n                                   ONE * BYTES);\n      KJ_IF_MAYBE(size, maybeSize) {\n        KJ_REQUIRE(*(bptr + *size) == '\\0', \"Text blob missing NUL terminator.\") {\n          goto useDefault;\n        }\n\n        return Text::Builder(reinterpret_cast<char*>(bptr), unbound(*size / BYTES));\n      } else {\n        KJ_FAIL_REQUIRE(\"zero-size blob can't be text (need NUL terminator)\") {\n          goto useDefault;\n        };\n      }\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Data::Builder> initDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, BlobSize size,\n      BuilderArena* orphanArena = nullptr)) {\n    // Allocate the space.\n    word* ptr = allocate(ref, segment, capTable, roundBytesUpToWords(size),\n                         WirePointer::LIST, orphanArena);\n\n    // Initialize the pointer.\n    ref->listRef.set(ElementSize::BYTE, size * (ONE * ELEMENTS / BYTES));\n\n    // Build the Data::Builder.\n    return { segment, Data::Builder(reinterpret_cast<byte*>(ptr), unbound(size / BYTES)) };\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<Data::Builder> setDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable, Data::Reader value,\n      BuilderArena* orphanArena = nullptr)) {\n    BlobSize size = assertMaxBits<BLOB_SIZE_BITS>(bounded(value.size()),\n        []() { KJ_FAIL_REQUIRE(\"text blob too big\"); }) * BYTES;\n\n    auto allocation = initDataPointer(ref, segment, capTable, size, orphanArena);\n    copyMemory(allocation.value.begin(), value);\n    return allocation;\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Builder getWritableDataPointer(\n      WirePointer* ref, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, BlobSize defaultSize)) {\n    return getWritableDataPointer(ref, ref->target(), segment, capTable, defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Builder getWritableDataPointer(\n      WirePointer* ref, word* refTarget, SegmentBuilder* segment, CapTableBuilder* capTable,\n      const void* defaultValue, BlobSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultSize == ZERO * BYTES) {\n        return nullptr;\n      } else {\n        Data::Builder builder = initDataPointer(ref, segment, capTable, defaultSize).value;\n        copyMemory(builder.begin(), reinterpret_cast<const byte*>(defaultValue), defaultSize);\n        return builder;\n      }\n    } else {\n      word* ptr = followFars(ref, refTarget, segment);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n          \"Schema mismatch: Called getData{Field,Element}() but existing pointer is not a list.\") {\n        goto useDefault;\n      }\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n          \"Schema mismatch: Called getData{Field,Element}() but existing list pointer is not \"\n          \"byte-sized.\") {\n        goto useDefault;\n      }\n\n      return Data::Builder(reinterpret_cast<byte*>(ptr),\n          unbound(ref->listRef.elementCount() / ELEMENTS));\n    }\n  }\n\n  static SegmentAnd<word*> setStructPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref, StructReader value,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    auto dataSize = roundBitsUpToBytes(value.dataSize);\n    auto ptrCount = value.pointerCount;\n\n    if (canonical) {\n      // StructReaders should not have bitwidths other than 1, but let's be safe\n      KJ_REQUIRE((value.dataSize == ONE * BITS)\n                 || (value.dataSize % BITS_PER_BYTE == ZERO * BITS));\n\n      if (value.dataSize == ONE * BITS) {\n        // Handle the truncation case where it's a false in a 1-bit struct\n        if (!value.getDataField<bool>(ZERO * ELEMENTS)) {\n          dataSize = ZERO * BYTES;\n        }\n      } else {\n        // Truncate the data section\n        auto data = value.getDataSectionAsBlob();\n        auto end = data.end();\n        while (end > data.begin() && end[-1] == 0) --end;\n        dataSize = intervalLength(data.begin(), end, MAX_STUCT_DATA_WORDS * BYTES_PER_WORD);\n      }\n\n      // Truncate pointer section\n      const WirePointer* ptr = value.pointers + ptrCount;\n      while (ptr > value.pointers && ptr[-1].isNull()) --ptr;\n      ptrCount = intervalLength(value.pointers, ptr, MAX_STRUCT_POINTER_COUNT);\n    }\n\n    auto dataWords = roundBytesUpToWords(dataSize);\n\n    auto totalSize = dataWords + ptrCount * WORDS_PER_POINTER;\n\n    word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::STRUCT, orphanArena);\n    ref->structRef.set(dataWords, ptrCount);\n\n    if (value.dataSize == ONE * BITS) {\n      // Data size could be made 0 by truncation\n      if (dataSize != ZERO * BYTES) {\n        *reinterpret_cast<char*>(ptr) = value.getDataField<bool>(ZERO * ELEMENTS);\n      }\n    } else {\n      copyMemory(reinterpret_cast<byte*>(ptr),\n                 reinterpret_cast<const byte*>(value.data),\n                 dataSize);\n    }\n\n    WirePointer* pointerSection = reinterpret_cast<WirePointer*>(ptr + dataWords);\n    for (auto i: kj::zeroTo(ptrCount)) {\n      copyPointer(segment, capTable, pointerSection + i,\n                  value.segment, value.capTable, value.pointers + i,\n                  value.nestingLimit, nullptr, canonical);\n    }\n\n    return { segment, ptr };\n  }\n\n#if !CAPNP_LITE\n  static void setCapabilityPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref,\n      kj::Own<ClientHook>&& cap) {\n    if (!ref->isNull()) {\n      zeroObject(segment, capTable, ref);\n    }\n    if (cap->isNull()) {\n      zeroMemory(ref);\n    } else {\n      ref->setCap(capTable->injectCap(kj::mv(cap)));\n    }\n  }\n#endif  // !CAPNP_LITE\n\n  static SegmentAnd<word*> setListPointer(\n      SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* ref, ListReader value,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    auto totalSize = assertMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(\n        roundBitsUpToWords(upgradeBound<uint64_t>(value.elementCount) * value.step),\n        []() { KJ_FAIL_ASSERT(\"encountered impossibly long struct list ListReader\"); });\n\n    if (value.elementSize != ElementSize::INLINE_COMPOSITE) {\n      // List of non-structs.\n      word* ptr = allocate(ref, segment, capTable, totalSize, WirePointer::LIST, orphanArena);\n\n      if (value.elementSize == ElementSize::POINTER) {\n        // List of pointers.\n        ref->listRef.set(ElementSize::POINTER, value.elementCount);\n        for (auto i: kj::zeroTo(value.elementCount * (ONE * POINTERS / ELEMENTS))) {\n          copyPointer(segment, capTable, reinterpret_cast<WirePointer*>(ptr) + i,\n                      value.segment, value.capTable,\n                      reinterpret_cast<const WirePointer*>(value.ptr) + i,\n                      value.nestingLimit, nullptr, canonical);\n        }\n      } else {\n        // List of data.\n        ref->listRef.set(value.elementSize, value.elementCount);\n\n        auto wholeByteSize =\n          assertMax(MAX_SEGMENT_WORDS * BYTES_PER_WORD,\n            upgradeBound<uint64_t>(value.elementCount) * value.step / BITS_PER_BYTE,\n            []() { KJ_FAIL_ASSERT(\"encountered impossibly long data ListReader\"); });\n        copyMemory(reinterpret_cast<byte*>(ptr), value.ptr, wholeByteSize);\n        auto leftoverBits =\n          (upgradeBound<uint64_t>(value.elementCount) * value.step) % BITS_PER_BYTE;\n        if (leftoverBits > ZERO * BITS) {\n          // We need to copy a partial byte.\n          uint8_t mask = (1 << unbound(leftoverBits / BITS)) - 1;\n          *((reinterpret_cast<byte*>(ptr)) + wholeByteSize) = mask & *(value.ptr + wholeByteSize);\n        }\n      }\n\n      return { segment, ptr };\n    } else {\n      // List of structs.\n      StructDataWordCount declDataSize = value.structDataSize / BITS_PER_WORD;\n      StructPointerCount declPointerCount = value.structPointerCount;\n\n      StructDataWordCount dataSize = ZERO * WORDS;\n      StructPointerCount ptrCount = ZERO * POINTERS;\n\n      if (canonical) {\n        for (auto i: kj::zeroTo(value.elementCount)) {\n          auto element = value.getStructElement(i);\n\n          // Truncate the data section\n          auto data = element.getDataSectionAsBlob();\n          auto end = data.end();\n          while (end > data.begin() && end[-1] == 0) --end;\n          dataSize = kj::max(dataSize, roundBytesUpToWords(\n              intervalLength(data.begin(), end, MAX_STUCT_DATA_WORDS * BYTES_PER_WORD)));\n\n          // Truncate pointer section\n          const WirePointer* ptr = element.pointers + element.pointerCount;\n          while (ptr > element.pointers && ptr[-1].isNull()) --ptr;\n          ptrCount = kj::max(ptrCount,\n              intervalLength(element.pointers, ptr, MAX_STRUCT_POINTER_COUNT));\n        }\n        auto newTotalSize = (dataSize + upgradeBound<uint64_t>(ptrCount) * WORDS_PER_POINTER)\n            / ELEMENTS * value.elementCount;\n        KJ_ASSERT(newTotalSize <= totalSize);  // we've only removed data!\n        totalSize = assumeMax<kj::maxValueForBits<SEGMENT_WORD_COUNT_BITS>() - 1>(newTotalSize);\n      } else {\n        dataSize = declDataSize;\n        ptrCount = declPointerCount;\n      }\n\n      KJ_DASSERT(value.structDataSize % BITS_PER_WORD == ZERO * BITS);\n      word* ptr = allocate(ref, segment, capTable, totalSize + POINTER_SIZE_IN_WORDS,\n                           WirePointer::LIST, orphanArena);\n      ref->listRef.setInlineComposite(totalSize);\n\n      WirePointer* tag = reinterpret_cast<WirePointer*>(ptr);\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, value.elementCount);\n      tag->structRef.set(dataSize, ptrCount);\n      word* dst = ptr + POINTER_SIZE_IN_WORDS;\n\n      const word* src = reinterpret_cast<const word*>(value.ptr);\n      for (auto i KJ_UNUSED: kj::zeroTo(value.elementCount)) {\n        copyMemory(dst, src, dataSize);\n        dst += dataSize;\n        src += declDataSize;\n\n        for (auto j: kj::zeroTo(ptrCount)) {\n          copyPointer(segment, capTable, reinterpret_cast<WirePointer*>(dst) + j,\n              value.segment, value.capTable, reinterpret_cast<const WirePointer*>(src) + j,\n              value.nestingLimit, nullptr, canonical);\n        }\n        dst += ptrCount * WORDS_PER_POINTER;\n        src += declPointerCount * WORDS_PER_POINTER;\n      }\n\n      return { segment, ptr };\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(SegmentAnd<word*> copyPointer(\n      SegmentBuilder* dstSegment, CapTableBuilder* dstCapTable, WirePointer* dst,\n      SegmentReader* srcSegment, CapTableReader* srcCapTable, const WirePointer* src,\n      int nestingLimit, BuilderArena* orphanArena = nullptr,\n      bool canonical = false)) {\n    return copyPointer(dstSegment, dstCapTable, dst,\n                       srcSegment, srcCapTable, src, src->target(srcSegment),\n                       nestingLimit, orphanArena, canonical);\n  }\n\n  static SegmentAnd<word*> copyPointer(\n      SegmentBuilder* dstSegment, CapTableBuilder* dstCapTable, WirePointer* dst,\n      SegmentReader* srcSegment, CapTableReader* srcCapTable, const WirePointer* src,\n      const word* srcTarget, int nestingLimit,\n      BuilderArena* orphanArena = nullptr, bool canonical = false) {\n    // Deep-copy the object pointed to by src into dst.  It turns out we can't reuse\n    // readStructPointer(), etc. because they do type checking whereas here we want to accept any\n    // valid pointer.\n\n    if (src->isNull()) {\n    useDefault:\n      if (!dst->isNull()) {\n        zeroObject(dstSegment, dstCapTable, dst);\n        zeroMemory(dst);\n      }\n      return { dstSegment, nullptr };\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, WireHelpers::followFars(src, srcTarget, srcSegment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    switch (src->kind()) {\n      case WirePointer::STRUCT:\n        KJ_REQUIRE(nestingLimit > 0,\n              \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n          goto useDefault;\n        }\n\n        KJ_REQUIRE(boundsCheck(srcSegment, ptr, src->structRef.wordSize()),\n                   \"Message contained out-of-bounds struct pointer. \"\n                   OUT_OF_BOUNDS_ERROR_DETAIL) {\n          goto useDefault;\n        }\n        return setStructPointer(dstSegment, dstCapTable, dst,\n            StructReader(srcSegment, srcCapTable, ptr,\n                         reinterpret_cast<const WirePointer*>(ptr + src->structRef.dataSize.get()),\n                         src->structRef.dataSize.get() * BITS_PER_WORD,\n                         src->structRef.ptrCount.get(),\n                         nestingLimit - 1),\n            orphanArena, canonical);\n\n      case WirePointer::LIST: {\n        ElementSize elementSize = src->listRef.elementSize();\n\n        KJ_REQUIRE(nestingLimit > 0,\n              \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n          goto useDefault;\n        }\n\n        if (elementSize == ElementSize::INLINE_COMPOSITE) {\n          auto wordCount = src->listRef.inlineCompositeWordCount();\n          const WirePointer* tag = reinterpret_cast<const WirePointer*>(ptr);\n\n          KJ_REQUIRE(boundsCheck(srcSegment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                     \"Message contains out-of-bounds list pointer. \"\n                     OUT_OF_BOUNDS_ERROR_DETAIL) {\n            goto useDefault;\n          }\n\n          ptr += POINTER_SIZE_IN_WORDS;\n\n          KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n                     \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n            goto useDefault;\n          }\n\n          auto elementCount = tag->inlineCompositeListElementCount();\n          auto wordsPerElement = tag->structRef.wordSize() / ELEMENTS;\n\n          KJ_REQUIRE(wordsPerElement * upgradeBound<uint64_t>(elementCount) <= wordCount,\n                     \"INLINE_COMPOSITE list's elements overrun its word count.\") {\n            goto useDefault;\n          }\n\n          if (wordsPerElement * (ONE * ELEMENTS) == ZERO * WORDS) {\n            // Watch out for lists of zero-sized structs, which can claim to be arbitrarily large\n            // without having sent actual data.\n            KJ_REQUIRE(amplifiedRead(srcSegment, elementCount * (ONE * WORDS / ELEMENTS)),\n                       \"Message contains amplified list pointer.\") {\n              goto useDefault;\n            }\n          }\n\n          return setListPointer(dstSegment, dstCapTable, dst,\n              ListReader(srcSegment, srcCapTable, ptr,\n                         elementCount, wordsPerElement * BITS_PER_WORD,\n                         tag->structRef.dataSize.get() * BITS_PER_WORD,\n                         tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE,\n                         nestingLimit - 1),\n              orphanArena, canonical);\n        } else {\n          auto dataSize = dataBitsPerElement(elementSize) * ELEMENTS;\n          auto pointerCount = pointersPerElement(elementSize) * ELEMENTS;\n          auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n          auto elementCount = src->listRef.elementCount();\n          auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(elementCount) * step);\n\n          KJ_REQUIRE(boundsCheck(srcSegment, ptr, wordCount),\n                     \"Message contains out-of-bounds list pointer. \"\n                     OUT_OF_BOUNDS_ERROR_DETAIL) {\n            goto useDefault;\n          }\n\n          if (elementSize == ElementSize::VOID) {\n            // Watch out for lists of void, which can claim to be arbitrarily large without having\n            // sent actual data.\n            KJ_REQUIRE(amplifiedRead(srcSegment, elementCount * (ONE * WORDS / ELEMENTS)),\n                       \"Message contains amplified list pointer.\") {\n              goto useDefault;\n            }\n          }\n\n          return setListPointer(dstSegment, dstCapTable, dst,\n              ListReader(srcSegment, srcCapTable, ptr, elementCount, step, dataSize, pointerCount,\n                         elementSize, nestingLimit - 1),\n              orphanArena, canonical);\n        }\n      }\n\n      case WirePointer::FAR:\n        KJ_FAIL_REQUIRE(\"Unexpected FAR pointer.\") {\n          goto useDefault;\n        }\n\n      case WirePointer::OTHER: {\n        KJ_REQUIRE(src->isCapability(), \"Unknown pointer type.\") {\n          goto useDefault;\n        }\n\n        if (canonical) {\n          KJ_FAIL_REQUIRE(\"Cannot create a canonical message with a capability\") {\n            break;\n          }\n        }\n#if !CAPNP_LITE\n        KJ_IF_MAYBE(cap, srcCapTable->extractCap(src->capRef.index.get())) {\n          setCapabilityPointer(dstSegment, dstCapTable, dst, kj::mv(*cap));\n          // Return dummy non-null pointer so OrphanBuilder doesn't end up null.\n          return { dstSegment, reinterpret_cast<word*>(1) };\n        } else {\n#endif  // !CAPNP_LITE\n          KJ_FAIL_REQUIRE(\"Message contained invalid capability pointer.\") {\n            goto useDefault;\n          }\n#if !CAPNP_LITE\n        }\n#endif  // !CAPNP_LITE\n      }\n    }\n\n    KJ_UNREACHABLE;\n  }\n\n  static void adopt(SegmentBuilder* segment, CapTableBuilder* capTable,\n                    WirePointer* ref, OrphanBuilder&& value) {\n    KJ_REQUIRE(value.segment == nullptr || value.segment->getArena() == segment->getArena(),\n               \"Adopted object must live in the same message.\");\n\n    if (!ref->isNull()) {\n      zeroObject(segment, capTable, ref);\n    }\n\n    if (value == nullptr) {\n      // Set null.\n      zeroMemory(ref);\n    } else if (value.tagAsPtr()->isPositional()) {\n      WireHelpers::transferPointer(segment, ref, value.segment, value.tagAsPtr(), value.location);\n    } else {\n      // FAR and OTHER pointers are position-independent, so we can just copy.\n      copyMemory(ref, value.tagAsPtr());\n    }\n\n    // Take ownership away from the OrphanBuilder.\n    zeroMemory(value.tagAsPtr());\n    value.location = nullptr;\n    value.segment = nullptr;\n  }\n\n  static OrphanBuilder disown(SegmentBuilder* segment, CapTableBuilder* capTable,\n                              WirePointer* ref) {\n    word* location;\n\n    if (ref->isNull()) {\n      location = nullptr;\n    } else if (ref->kind() == WirePointer::OTHER) {\n      KJ_REQUIRE(ref->isCapability(), \"Unknown pointer type.\") { break; }\n      location = reinterpret_cast<word*>(1);  // dummy so that it is non-null\n    } else {\n      WirePointer* refCopy = ref;\n      location = followFarsNoWritableCheck(refCopy, ref->target(), segment);\n    }\n\n    OrphanBuilder result(ref, segment, capTable, location);\n\n    if (!ref->isNull() && ref->isPositional()) {\n      result.tagAsPtr()->setKindForOrphan(ref->kind());\n    }\n\n    // Zero out the pointer that was disowned.\n    zeroMemory(ref);\n\n    return result;\n  }\n\n  // -----------------------------------------------------------------\n\n  static KJ_ALWAYS_INLINE(StructReader readStructPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* defaultValue,\n      int nestingLimit)) {\n    return readStructPointer(segment, capTable, ref, ref->target(segment),\n                             defaultValue, nestingLimit);\n  }\n\n  static KJ_ALWAYS_INLINE(StructReader readStructPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* refTarget,\n      const word* defaultValue, int nestingLimit)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return StructReader();\n      }\n      segment = nullptr;\n      ref = reinterpret_cast<const WirePointer*>(defaultValue);\n      refTarget = ref->target(segment);\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    KJ_REQUIRE(nestingLimit > 0,\n               \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n      goto useDefault;\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(ref->kind() == WirePointer::STRUCT,\n               \"Schema mismatch: Message contains non-struct pointer where struct pointer\"\n               \"was expected.\") {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(boundsCheck(segment, ptr, ref->structRef.wordSize()),\n               \"Message contained out-of-bounds struct pointer. \"\n               OUT_OF_BOUNDS_ERROR_DETAIL) {\n      goto useDefault;\n    }\n\n    return StructReader(\n        segment, capTable,\n        ptr, reinterpret_cast<const WirePointer*>(ptr + ref->structRef.dataSize.get()),\n        ref->structRef.dataSize.get() * BITS_PER_WORD,\n        ref->structRef.ptrCount.get(),\n        nestingLimit - 1);\n  }\n\n#if !CAPNP_LITE\n  static KJ_ALWAYS_INLINE(kj::Own<ClientHook> readCapabilityPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, int nestingLimit)) {\n    kj::Maybe<kj::Own<ClientHook>> maybeCap;\n\n    auto brokenCapFactory = readGlobalBrokenCapFactoryForLayoutCpp();\n\n    KJ_REQUIRE(brokenCapFactory != nullptr,\n               \"Trying to read capabilities without ever having created a capability context.  \"\n               \"To read capabilities from a message, you must imbue it with CapReaderContext, or \"\n               \"use the Cap'n Proto RPC system.\");\n\n    if (ref->isNull()) {\n      return brokenCapFactory->newNullCap();\n    } else if (!ref->isCapability()) {\n      KJ_FAIL_REQUIRE(\n          \"Schema mismatch: Message contains non-capability pointer where capability pointer was \"\n          \"expected.\") {\n        break;\n      }\n      return brokenCapFactory->newBrokenCap(\n          \"Calling capability extracted from a non-capability pointer.\");\n    } else KJ_IF_MAYBE(cap, capTable->extractCap(ref->capRef.index.get())) {\n      return kj::mv(*cap);\n    } else {\n      KJ_FAIL_REQUIRE(\"Message contains invalid capability pointer.\") {\n        break;\n      }\n      return brokenCapFactory->newBrokenCap(\"Calling invalid capability pointer.\");\n    }\n  }\n#endif  // !CAPNP_LITE\n\n  static KJ_ALWAYS_INLINE(ListReader readListPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* defaultValue,\n      ElementSize expectedElementSize, int nestingLimit, bool checkElementSize = true)) {\n    return readListPointer(segment, capTable, ref, ref->target(segment), defaultValue,\n                           expectedElementSize, nestingLimit, checkElementSize);\n  }\n\n  static KJ_ALWAYS_INLINE(ListReader readListPointer(\n      SegmentReader* segment, CapTableReader* capTable,\n      const WirePointer* ref, const word* refTarget,\n      const word* defaultValue, ElementSize expectedElementSize, int nestingLimit,\n      bool checkElementSize = true)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr ||\n          reinterpret_cast<const WirePointer*>(defaultValue)->isNull()) {\n        return ListReader(expectedElementSize);\n      }\n      segment = nullptr;\n      ref = reinterpret_cast<const WirePointer*>(defaultValue);\n      refTarget = ref->target(segment);\n      defaultValue = nullptr;  // If the default value is itself invalid, don't use it again.\n    }\n\n    KJ_REQUIRE(nestingLimit > 0,\n               \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n      goto useDefault;\n    }\n\n    const word* ptr;\n    KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n      ptr = p;\n    } else {\n      goto useDefault;\n    }\n\n    KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n               \"Schema mismatch: Message contains non-list pointer where list pointer was \"\n               \"expected.\") {\n      goto useDefault;\n    }\n\n    ElementSize elementSize = ref->listRef.elementSize();\n    if (elementSize == ElementSize::INLINE_COMPOSITE) {\n      auto wordCount = ref->listRef.inlineCompositeWordCount();\n\n      // An INLINE_COMPOSITE list points to a tag, which is formatted like a pointer.\n      const WirePointer* tag = reinterpret_cast<const WirePointer*>(ptr);\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, wordCount + POINTER_SIZE_IN_WORDS),\n                 \"Message contains out-of-bounds list pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      ptr += POINTER_SIZE_IN_WORDS;\n\n      KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n                 \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n        goto useDefault;\n      }\n\n      auto size = tag->inlineCompositeListElementCount();\n      auto wordsPerElement = tag->structRef.wordSize() / ELEMENTS;\n\n      KJ_REQUIRE(upgradeBound<uint64_t>(size) * wordsPerElement <= wordCount,\n                 \"INLINE_COMPOSITE list's elements overrun its word count.\") {\n        goto useDefault;\n      }\n\n      if (wordsPerElement * (ONE * ELEMENTS) == ZERO * WORDS) {\n        // Watch out for lists of zero-sized structs, which can claim to be arbitrarily large\n        // without having sent actual data.\n        KJ_REQUIRE(amplifiedRead(segment, size * (ONE * WORDS / ELEMENTS)),\n                   \"Message contains amplified list pointer.\") {\n          goto useDefault;\n        }\n      }\n\n      if (checkElementSize) {\n        // If a struct list was not expected, then presumably a non-struct list was upgraded to a\n        // struct list. We need to manipulate the pointer to point at the first field of the\n        // struct. Together with the `step` field, this will allow the struct list to be accessed\n        // as if it were a primitive list without branching.\n\n        // Check whether the size is compatible.\n        switch (expectedElementSize) {\n          case ElementSize::VOID:\n            break;\n\n          case ElementSize::BIT:\n            KJ_FAIL_REQUIRE(\n                \"Found struct list where bit list was expected; upgrading boolean lists to structs \"\n                \"is no longer supported.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::BYTE:\n          case ElementSize::TWO_BYTES:\n          case ElementSize::FOUR_BYTES:\n          case ElementSize::EIGHT_BYTES:\n            KJ_REQUIRE(tag->structRef.dataSize.get() > ZERO * WORDS,\n                       \"Schema mismatch: Expected a primitive list, but got a list of pointer-only \"\n                       \"structs.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::POINTER:\n            KJ_REQUIRE(tag->structRef.ptrCount.get() > ZERO * POINTERS,\n                       \"Schema mismatch: Expected a pointer list, but got a list of data-only \"\n                       \"structs.\") {\n              goto useDefault;\n            }\n            break;\n\n          case ElementSize::INLINE_COMPOSITE:\n            break;\n        }\n      }\n\n      return ListReader(\n          segment, capTable, ptr, size, wordsPerElement * BITS_PER_WORD,\n          tag->structRef.dataSize.get() * BITS_PER_WORD,\n          tag->structRef.ptrCount.get(), ElementSize::INLINE_COMPOSITE,\n          nestingLimit - 1);\n\n    } else {\n      // This is a primitive or pointer list, but all such lists can also be interpreted as struct\n      // lists.  We need to compute the data size and pointer count for such structs.\n      auto dataSize = dataBitsPerElement(ref->listRef.elementSize()) * ELEMENTS;\n      auto pointerCount = pointersPerElement(ref->listRef.elementSize()) * ELEMENTS;\n      auto elementCount = ref->listRef.elementCount();\n      auto step = (dataSize + pointerCount * BITS_PER_POINTER) / ELEMENTS;\n\n      auto wordCount = roundBitsUpToWords(upgradeBound<uint64_t>(elementCount) * step);\n      KJ_REQUIRE(boundsCheck(segment, ptr, wordCount),\n            \"Message contains out-of-bounds list pointer. \"\n            OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      if (elementSize == ElementSize::VOID) {\n        // Watch out for lists of void, which can claim to be arbitrarily large without having sent\n        // actual data.\n        KJ_REQUIRE(amplifiedRead(segment, elementCount * (ONE * WORDS / ELEMENTS)),\n                   \"Message contains amplified list pointer.\") {\n          goto useDefault;\n        }\n      }\n\n      if (checkElementSize) {\n        if (elementSize == ElementSize::BIT && expectedElementSize != ElementSize::BIT) {\n          KJ_FAIL_REQUIRE(\n              \"Found bit list where struct list was expected; upgrading boolean lists to structs \"\n              \"is no longer supported.\") {\n            goto useDefault;\n          }\n        }\n\n        // Verify that the elements are at least as large as the expected type.  Note that if we\n        // expected INLINE_COMPOSITE, the expected sizes here will be zero, because bounds checking\n        // will be performed at field access time.  So this check here is for the case where we\n        // expected a list of some primitive or pointer type.\n\n        BitCount expectedDataBitsPerElement =\n            dataBitsPerElement(expectedElementSize) * ELEMENTS;\n        WirePointerCount expectedPointersPerElement =\n            pointersPerElement(expectedElementSize) * ELEMENTS;\n\n        KJ_REQUIRE(expectedDataBitsPerElement <= dataSize,\n                   \"Schema mismatch: Message contained list with incompatible element type.\") {\n          goto useDefault;\n        }\n        KJ_REQUIRE(expectedPointersPerElement <= pointerCount,\n                   \"Schema mismatch: Message contained list with incompatible element type.\") {\n          goto useDefault;\n        }\n      }\n\n      return ListReader(segment, capTable, ptr, elementCount, step,\n                        dataSize, pointerCount, elementSize, nestingLimit - 1);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Reader readTextPointer(\n      SegmentReader* segment, const WirePointer* ref,\n      const void* defaultValue, ByteCount defaultSize)) {\n    return readTextPointer(segment, ref, ref->target(segment), defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Text::Reader readTextPointer(\n      SegmentReader* segment, const WirePointer* ref, const word* refTarget,\n      const void* defaultValue, ByteCount defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      if (defaultValue == nullptr) defaultValue = \"\";\n      return Text::Reader(reinterpret_cast<const char*>(defaultValue),\n          unbound(defaultSize / BYTES));\n    } else {\n      const word* ptr;\n      KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n        ptr = p;\n      } else {\n        goto useDefault;\n      }\n\n      auto size = ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n                 \"Schema mismatch: Message contains non-list pointer where text was expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n                 \"Schema mismatch: Message contains list pointer of non-bytes where text was \"\n                 \"expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, roundBytesUpToWords(size)),\n                 \"Message contained out-of-bounds text pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(size > ZERO * BYTES, \"Message contains text that is not NUL-terminated.\") {\n        goto useDefault;\n      }\n\n      const char* cptr = reinterpret_cast<const char*>(ptr);\n      uint unboundedSize = unbound(size / BYTES) - 1;\n\n      KJ_REQUIRE(cptr[unboundedSize] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n        goto useDefault;\n      }\n\n      return Text::Reader(cptr, unboundedSize);\n    }\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Reader readDataPointer(\n      SegmentReader* segment, const WirePointer* ref,\n      const void* defaultValue, BlobSize defaultSize)) {\n    return readDataPointer(segment, ref, ref->target(segment), defaultValue, defaultSize);\n  }\n\n  static KJ_ALWAYS_INLINE(Data::Reader readDataPointer(\n      SegmentReader* segment, const WirePointer* ref, const word* refTarget,\n      const void* defaultValue, BlobSize defaultSize)) {\n    if (ref->isNull()) {\n    useDefault:\n      return Data::Reader(reinterpret_cast<const byte*>(defaultValue),\n          unbound(defaultSize / BYTES));\n    } else {\n      const word* ptr;\n      KJ_IF_MAYBE(p, followFars(ref, refTarget, segment)) {\n        ptr = p;\n      } else {\n        goto useDefault;\n      }\n\n      if (KJ_UNLIKELY(ptr == nullptr)) {\n        // Already reported error.\n        goto useDefault;\n      }\n\n      auto size = ref->listRef.elementCount() * (ONE * BYTES / ELEMENTS);\n\n      KJ_REQUIRE(ref->kind() == WirePointer::LIST,\n                 \"Schema mismatch: Message contains non-list pointer where data was expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(ref->listRef.elementSize() == ElementSize::BYTE,\n                 \"Schema mismatch: Message contains list pointer of non-bytes where data was \"\n                 \"expected.\") {\n        goto useDefault;\n      }\n\n      KJ_REQUIRE(boundsCheck(segment, ptr, roundBytesUpToWords(size)),\n                 \"Message contained out-of-bounds data pointer. \"\n                 OUT_OF_BOUNDS_ERROR_DETAIL) {\n        goto useDefault;\n      }\n\n      return Data::Reader(reinterpret_cast<const byte*>(ptr), unbound(size / BYTES));\n    }\n  }\n};\n\n// =======================================================================================\n// PointerBuilder\n\nStructBuilder PointerBuilder::initStruct(StructSize size) {\n  return WireHelpers::initStructPointer(pointer, segment, capTable, size);\n}\n\nStructBuilder PointerBuilder::getStruct(StructSize size, const word* defaultValue) {\n  return WireHelpers::getWritableStructPointer(pointer, segment, capTable, size, defaultValue);\n}\n\nListBuilder PointerBuilder::initList(ElementSize elementSize, ElementCount elementCount) {\n  return WireHelpers::initListPointer(pointer, segment, capTable, elementCount, elementSize);\n}\n\nListBuilder PointerBuilder::initStructList(ElementCount elementCount, StructSize elementSize) {\n  return WireHelpers::initStructListPointer(pointer, segment, capTable, elementCount, elementSize);\n}\n\nListBuilder PointerBuilder::getList(ElementSize elementSize, const word* defaultValue) {\n  return WireHelpers::getWritableListPointer(pointer, segment, capTable, elementSize, defaultValue);\n}\n\nListBuilder PointerBuilder::getStructList(StructSize elementSize, const word* defaultValue) {\n  return WireHelpers::getWritableStructListPointer(\n      pointer, segment, capTable, elementSize, defaultValue);\n}\n\nListBuilder PointerBuilder::getListAnySize(const word* defaultValue) {\n  return WireHelpers::getWritableListPointerAnySize(pointer, segment, capTable, defaultValue);\n}\n\ntemplate <>\nText::Builder PointerBuilder::initBlob<Text>(ByteCount size) {\n  return WireHelpers::initTextPointer(pointer, segment, capTable,\n      assertMax<MAX_TEXT_SIZE>(size, ThrowOverflow())).value;\n}\ntemplate <>\nvoid PointerBuilder::setBlob<Text>(Text::Reader value) {\n  WireHelpers::setTextPointer(pointer, segment, capTable, value);\n}\ntemplate <>\nText::Builder PointerBuilder::getBlob<Text>(const void* defaultValue, ByteCount defaultSize) {\n  return WireHelpers::getWritableTextPointer(pointer, segment, capTable, defaultValue,\n      assertMax<MAX_TEXT_SIZE>(defaultSize, ThrowOverflow()));\n}\n\ntemplate <>\nData::Builder PointerBuilder::initBlob<Data>(ByteCount size) {\n  return WireHelpers::initDataPointer(pointer, segment, capTable,\n      assertMaxBits<BLOB_SIZE_BITS>(size, ThrowOverflow())).value;\n}\ntemplate <>\nvoid PointerBuilder::setBlob<Data>(Data::Reader value) {\n  WireHelpers::setDataPointer(pointer, segment, capTable, value);\n}\ntemplate <>\nData::Builder PointerBuilder::getBlob<Data>(const void* defaultValue, ByteCount defaultSize) {\n  return WireHelpers::getWritableDataPointer(pointer, segment, capTable, defaultValue,\n      assertMaxBits<BLOB_SIZE_BITS>(defaultSize, ThrowOverflow()));\n}\n\nvoid PointerBuilder::setStruct(const StructReader& value, bool canonical) {\n  WireHelpers::setStructPointer(segment, capTable, pointer, value, nullptr, canonical);\n}\n\nvoid PointerBuilder::setList(const ListReader& value, bool canonical) {\n  WireHelpers::setListPointer(segment, capTable, pointer, value, nullptr, canonical);\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> PointerBuilder::getCapability() {\n  return WireHelpers::readCapabilityPointer(\n      segment, capTable, pointer, kj::maxValue);\n}\n\nvoid PointerBuilder::setCapability(kj::Own<ClientHook>&& cap) {\n  WireHelpers::setCapabilityPointer(segment, capTable, pointer, kj::mv(cap));\n}\n#endif  // !CAPNP_LITE\n\nvoid PointerBuilder::adopt(OrphanBuilder&& value) {\n  WireHelpers::adopt(segment, capTable, pointer, kj::mv(value));\n}\n\nOrphanBuilder PointerBuilder::disown() {\n  return WireHelpers::disown(segment, capTable, pointer);\n}\n\nvoid PointerBuilder::clear() {\n  WireHelpers::zeroObject(segment, capTable, pointer);\n  WireHelpers::zeroMemory(pointer);\n}\n\nPointerType PointerBuilder::getPointerType() const {\n  if(pointer->isNull()) {\n    return PointerType::NULL_;\n  } else {\n    WirePointer* ptr = pointer;\n    SegmentBuilder* sgmt = segment;\n    WireHelpers::followFars(ptr, ptr->target(), sgmt);\n    switch(ptr->kind()) {\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"far pointer not followed?\");\n      case WirePointer::STRUCT:\n        return PointerType::STRUCT;\n      case WirePointer::LIST:\n        return PointerType::LIST;\n      case WirePointer::OTHER:\n        KJ_REQUIRE(ptr->isCapability(), \"unknown pointer type\");\n        return PointerType::CAPABILITY;\n    }\n    KJ_UNREACHABLE;\n  }\n}\n\nvoid PointerBuilder::transferFrom(PointerBuilder other) {\n  if (!pointer->isNull()) {\n    WireHelpers::zeroObject(segment, capTable, pointer);\n    WireHelpers::zeroMemory(pointer);\n  }\n  WireHelpers::transferPointer(segment, pointer, other.segment, other.pointer);\n  WireHelpers::zeroMemory(other.pointer);\n}\n\nvoid PointerBuilder::copyFrom(PointerReader other, bool canonical) {\n  if (other.pointer == nullptr) {\n    if (!pointer->isNull()) {\n      WireHelpers::zeroObject(segment, capTable, pointer);\n      WireHelpers::zeroMemory(pointer);\n    }\n  } else {\n    WireHelpers::copyPointer(segment, capTable, pointer,\n                             other.segment, other.capTable, other.pointer, other.nestingLimit,\n                             nullptr,\n                             canonical);\n  }\n}\n\nPointerReader PointerBuilder::asReader() const {\n  return PointerReader(segment, capTable, pointer, kj::maxValue);\n}\n\nBuilderArena* PointerBuilder::getArena() const {\n  return segment->getArena();\n}\n\nCapTableBuilder* PointerBuilder::getCapTable() {\n  return capTable;\n}\n\nPointerBuilder PointerBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// PointerReader\n\nPointerReader PointerReader::getRoot(SegmentReader* segment, CapTableReader* capTable,\n                                     const word* location, int nestingLimit) {\n  KJ_REQUIRE(WireHelpers::boundsCheck(segment, location, POINTER_SIZE_IN_WORDS),\n             \"Root location out-of-bounds.\") {\n    location = nullptr;\n  }\n\n  return PointerReader(segment, capTable,\n      reinterpret_cast<const WirePointer*>(location), nestingLimit);\n}\n\nStructReader PointerReader::getStruct(const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readStructPointer(segment, capTable, ref, defaultValue, nestingLimit);\n}\n\nListReader PointerReader::getList(ElementSize expectedElementSize, const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readListPointer(\n      segment, capTable, ref, defaultValue, expectedElementSize, nestingLimit);\n}\n\nListReader PointerReader::getListAnySize(const word* defaultValue) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readListPointer(\n      segment, capTable, ref, defaultValue, ElementSize::VOID /* dummy */, nestingLimit, false);\n}\n\ntemplate <>\nText::Reader PointerReader::getBlob<Text>(const void* defaultValue, ByteCount defaultSize) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readTextPointer(segment, ref, defaultValue, defaultSize);\n}\n\ntemplate <>\nData::Reader PointerReader::getBlob<Data>(const void* defaultValue, ByteCount defaultSize) const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readDataPointer(segment, ref, defaultValue,\n      assertMaxBits<BLOB_SIZE_BITS>(defaultSize, ThrowOverflow()));\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> PointerReader::getCapability() const {\n  const WirePointer* ref = pointer == nullptr ? &zero.pointer : pointer;\n  return WireHelpers::readCapabilityPointer(segment, capTable, ref, nestingLimit);\n}\n#endif  // !CAPNP_LITE\n\nconst word* PointerReader::getUnchecked() const {\n  KJ_REQUIRE(segment == nullptr, \"getUncheckedPointer() only allowed on unchecked messages.\");\n  return reinterpret_cast<const word*>(pointer);\n}\n\nMessageSizeCounts PointerReader::targetSize() const {\n  return pointer == nullptr ? MessageSizeCounts { ZERO * WORDS, 0 }\n                            : WireHelpers::totalSize(segment, pointer, nestingLimit);\n}\n\nPointerType PointerReader::getPointerType() const {\n  if(pointer == nullptr || pointer->isNull()) {\n    return PointerType::NULL_;\n  } else {\n    const WirePointer* ptr = pointer;\n    const word* refTarget = ptr->target(segment);\n    SegmentReader* sgmt = segment;\n    if (WireHelpers::followFars(ptr, refTarget, sgmt) == nullptr) return PointerType::NULL_;\n    switch(ptr->kind()) {\n      case WirePointer::FAR:\n        KJ_FAIL_ASSERT(\"far pointer not followed?\") { return PointerType::NULL_; }\n      case WirePointer::STRUCT:\n        return PointerType::STRUCT;\n      case WirePointer::LIST:\n        return PointerType::LIST;\n      case WirePointer::OTHER:\n        KJ_REQUIRE(ptr->isCapability(), \"unknown pointer type\") { return PointerType::NULL_; }\n        return PointerType::CAPABILITY;\n    }\n    KJ_UNREACHABLE;\n  }\n}\n\nkj::Maybe<Arena&> PointerReader::getArena() const {\n  return segment == nullptr ? nullptr : segment->getArena();\n}\n\nCapTableReader* PointerReader::getCapTable() {\n  return capTable;\n}\n\nPointerReader PointerReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool PointerReader::isCanonical(const word **readHead) {\n  if (!this->pointer) {\n    // The pointer is null, so we are canonical and do not read\n    return true;\n  }\n\n  if (!this->pointer->isPositional()) {\n    // The pointer is a FAR or OTHER pointer, and is non-canonical\n    return false;\n  }\n\n  switch (this->getPointerType()) {\n    case PointerType::NULL_:\n      // The pointer is null, we are canonical and do not read\n      return true;\n    case PointerType::STRUCT: {\n      bool dataTrunc = false, ptrTrunc = false;\n      auto structReader = this->getStruct(nullptr);\n      if (structReader.getDataSectionSize() == ZERO * BITS &&\n          structReader.getPointerSectionSize() == ZERO * POINTERS) {\n        return reinterpret_cast<const word*>(this->pointer) == structReader.getLocation();\n      } else {\n        // Fun fact: Once this call to isCanonical() returns, Clang may re-order the evaluation of\n        //   the && operators. In theory this is wrong because && is short-circuiting, but Clang\n        //   apparently sees that there are no side effects to the right of &&, so decides it is\n        //   safe to skip short-circuiting. It turns out, though, this is observable under\n        //   valgrind: if we don't initialize `dataTrunc` when declaring it above, then valgrind\n        //   reports \"Conditional jump or move depends on uninitialised value(s)\". Specifically\n        //   this happens in cases where structReader.isCanonical() returns false -- it is allowed\n        //   to skip initializing `dataTrunc` in that case. The short-circuiting && should mean\n        //   that we don't read `dataTrunc` in that case, except Clang's optimizations. Ultimately\n        //   the uninitialized read is fine because eventually the whole expression evaluates false\n        //   either way. But, to make valgrind happy, we initialize the bools above...\n        return structReader.isCanonical(readHead, readHead, &dataTrunc, &ptrTrunc) && dataTrunc && ptrTrunc;\n      }\n    }\n    case PointerType::LIST:\n      return this->getListAnySize(nullptr).isCanonical(readHead, pointer);\n    case PointerType::CAPABILITY:\n      KJ_FAIL_ASSERT(\"Capabilities are not positional\");\n  }\n  KJ_UNREACHABLE;\n}\n\n// =======================================================================================\n// StructBuilder\n\nvoid StructBuilder::clearAll() {\n  if (dataSize == ONE * BITS) {\n    setDataField<bool>(ONE * ELEMENTS, false);\n  } else {\n    WireHelpers::zeroMemory(reinterpret_cast<byte*>(data), dataSize / BITS_PER_BYTE);\n  }\n\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n}\n\nvoid StructBuilder::transferContentFrom(StructBuilder other) {\n  // Determine the amount of data the builders have in common.\n  auto sharedDataSize = kj::min(dataSize, other.dataSize);\n\n  if (dataSize > sharedDataSize) {\n    // Since the target is larger than the source, make sure to zero out the extra bits that the\n    // source doesn't have.\n    if (dataSize == ONE * BITS) {\n      setDataField<bool>(ZERO * ELEMENTS, false);\n    } else {\n      byte* unshared = reinterpret_cast<byte*>(data) + sharedDataSize / BITS_PER_BYTE;\n      // Note: this subtraction can't fail due to the if() above\n      WireHelpers::zeroMemory(unshared,\n          subtractChecked(dataSize, sharedDataSize, []() {}) / BITS_PER_BYTE);\n    }\n  }\n\n  // Copy over the shared part.\n  if (sharedDataSize == ONE * BITS) {\n    setDataField<bool>(ZERO * ELEMENTS, other.getDataField<bool>(ZERO * ELEMENTS));\n  } else {\n    WireHelpers::copyMemory(reinterpret_cast<byte*>(data),\n                            reinterpret_cast<byte*>(other.data),\n                            sharedDataSize / BITS_PER_BYTE);\n  }\n\n  // Zero out all pointers in the target.\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n\n  // Transfer the pointers.\n  auto sharedPointerCount = kj::min(pointerCount, other.pointerCount);\n  for (auto i: kj::zeroTo(sharedPointerCount)) {\n    WireHelpers::transferPointer(segment, pointers + i, other.segment, other.pointers + i);\n  }\n\n  // Zero out the pointers that were transferred in the source because it no longer has ownership.\n  // If the source had any extra pointers that the destination didn't have space for, we\n  // intentionally leave them be, so that they'll be cleaned up later.\n  WireHelpers::zeroMemory(other.pointers, sharedPointerCount);\n}\n\nvoid StructBuilder::copyContentFrom(StructReader other) {\n  // Determine the amount of data the builders have in common.\n  auto sharedDataSize = kj::min(dataSize, other.dataSize);\n  auto sharedPointerCount = kj::min(pointerCount, other.pointerCount);\n\n  if ((sharedDataSize > ZERO * BITS && other.data == data) ||\n      (sharedPointerCount > ZERO * POINTERS && other.pointers == pointers)) {\n    // At least one of the section pointers is pointing to ourself. Verify that the other is two\n    // (but ignore empty sections).\n    KJ_ASSERT((sharedDataSize == ZERO * BITS || other.data == data) &&\n              (sharedPointerCount == ZERO * POINTERS || other.pointers == pointers));\n    // So `other` appears to be a reader for this same struct. No coping is needed.\n    return;\n  }\n\n  if (dataSize > sharedDataSize) {\n    // Since the target is larger than the source, make sure to zero out the extra bits that the\n    // source doesn't have.\n    if (dataSize == ONE * BITS) {\n      setDataField<bool>(ZERO * ELEMENTS, false);\n    } else {\n      byte* unshared = reinterpret_cast<byte*>(data) + sharedDataSize / BITS_PER_BYTE;\n      WireHelpers::zeroMemory(unshared,\n          subtractChecked(dataSize, sharedDataSize, []() {}) / BITS_PER_BYTE);\n    }\n  }\n\n  // Copy over the shared part.\n  if (sharedDataSize == ONE * BITS) {\n    setDataField<bool>(ZERO * ELEMENTS, other.getDataField<bool>(ZERO * ELEMENTS));\n  } else {\n    WireHelpers::copyMemory(reinterpret_cast<byte*>(data),\n                            reinterpret_cast<const byte*>(other.data),\n                            sharedDataSize / BITS_PER_BYTE);\n  }\n\n  // Zero out all pointers in the target.\n  for (auto i: kj::zeroTo(pointerCount)) {\n    WireHelpers::zeroObject(segment, capTable, pointers + i);\n  }\n  WireHelpers::zeroMemory(pointers, pointerCount);\n\n  // Copy the pointers.\n  for (auto i: kj::zeroTo(sharedPointerCount)) {\n    WireHelpers::copyPointer(segment, capTable, pointers + i,\n        other.segment, other.capTable, other.pointers + i, other.nestingLimit);\n  }\n}\n\nStructReader StructBuilder::asReader() const {\n  return StructReader(segment, capTable, data, pointers,\n      dataSize, pointerCount, kj::maxValue);\n}\n\nBuilderArena* StructBuilder::getArena() {\n  return segment->getArena();\n}\n\nCapTableBuilder* StructBuilder::getCapTable() {\n  return capTable;\n}\n\nStructBuilder StructBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// StructReader\n\nMessageSizeCounts StructReader::totalSize() const {\n  MessageSizeCounts result = {\n    WireHelpers::roundBitsUpToWords(dataSize) + pointerCount * WORDS_PER_POINTER, 0 };\n\n  for (auto i: kj::zeroTo(pointerCount)) {\n    result += WireHelpers::totalSize(segment, pointers + i, nestingLimit);\n  }\n\n  if (segment != nullptr) {\n    // This traversal should not count against the read limit, because it's highly likely that\n    // the caller is going to traverse the object again, e.g. to copy it.\n    segment->unread(result.wordCount);\n  }\n\n  return result;\n}\n\nkj::Array<word> StructReader::canonicalize() {\n  auto size = totalSize().wordCount + POINTER_SIZE_IN_WORDS;\n  kj::Array<word> backing = kj::heapArray<word>(unbound(size / WORDS));\n  WireHelpers::zeroMemory(backing.asPtr());\n  FlatMessageBuilder builder(backing);\n  _::PointerHelpers<AnyPointer>::getInternalBuilder(builder.initRoot<AnyPointer>()).setStruct(*this, true);\n  KJ_ASSERT(builder.isCanonical());\n  auto output = builder.getSegmentsForOutput()[0];\n  kj::Array<word> trunc = kj::heapArray<word>(output.size());\n  WireHelpers::copyMemory(trunc.begin(), output);\n  return trunc;\n}\n\nCapTableReader* StructReader::getCapTable() {\n  return capTable;\n}\n\nStructReader StructReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool StructReader::isCanonical(const word **readHead,\n                               const word **ptrHead,\n                               bool *dataTrunc,\n                               bool *ptrTrunc) {\n  if (this->getLocation() != *readHead) {\n    // Our target area is not at the readHead, preorder fails\n    return false;\n  }\n\n  if (this->getDataSectionSize() % BITS_PER_WORD != ZERO * BITS) {\n    // Using legacy non-word-size structs, reject\n    return false;\n  }\n  auto dataSize = this->getDataSectionSize() / BITS_PER_WORD;\n\n  // Mark whether the struct is properly truncated\n  KJ_IF_MAYBE(diff, trySubtract(dataSize, ONE * WORDS)) {\n    *dataTrunc = this->getDataField<uint64_t>(*diff / WORDS * ELEMENTS) != 0;\n  } else {\n    // Data segment empty.\n    *dataTrunc = true;\n  }\n\n  KJ_IF_MAYBE(diff, trySubtract(this->pointerCount, ONE * POINTERS)) {\n    *ptrTrunc  = !this->getPointerField(*diff).isNull();\n  } else {\n    *ptrTrunc = true;\n  }\n\n  // Advance the read head\n  *readHead += (dataSize + (this->pointerCount * WORDS_PER_POINTER));\n\n  // Check each pointer field for canonicity\n  for (auto ptrIndex: kj::zeroTo(this->pointerCount)) {\n    if (!this->getPointerField(ptrIndex).isCanonical(ptrHead)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n// =======================================================================================\n// ListBuilder\n\nText::Builder ListBuilder::asText() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Expected Text, got list of non-bytes.\") {\n    return Text::Builder();\n  }\n\n  size_t size = unbound(elementCount / ELEMENTS);\n\n  KJ_REQUIRE(size > 0, \"Message contains text that is not NUL-terminated.\") {\n    return Text::Builder();\n  }\n\n  char* cptr = reinterpret_cast<char*>(ptr);\n  --size;  // NUL terminator\n\n  KJ_REQUIRE(cptr[size] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n    return Text::Builder();\n  }\n\n  return Text::Builder(cptr, size);\n}\n\nData::Builder ListBuilder::asData() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Expected Text, got list of non-bytes.\") {\n    return Data::Builder();\n  }\n\n  return Data::Builder(reinterpret_cast<byte*>(ptr), unbound(elementCount / ELEMENTS));\n}\n\nStructBuilder ListBuilder::getStructElement(ElementCount index) {\n  auto indexBit = upgradeBound<uint64_t>(index) * step;\n  byte* structData = ptr + indexBit / BITS_PER_BYTE;\n  KJ_DASSERT(indexBit % BITS_PER_BYTE == ZERO * BITS);\n  return StructBuilder(segment, capTable, structData,\n      reinterpret_cast<WirePointer*>(structData + structDataSize / BITS_PER_BYTE),\n      structDataSize, structPointerCount);\n}\n\nListReader ListBuilder::asReader() const {\n  return ListReader(segment, capTable, ptr, elementCount, step, structDataSize, structPointerCount,\n                    elementSize, kj::maxValue);\n}\n\nBuilderArena* ListBuilder::getArena() {\n  return segment->getArena();\n}\n\nCapTableBuilder* ListBuilder::getCapTable() {\n  return capTable;\n}\n\nListBuilder ListBuilder::imbue(CapTableBuilder* capTable) {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\n// =======================================================================================\n// ListReader\n\nText::Reader ListReader::asText() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected Text, got list of non-bytes.\") {\n    return Text::Reader();\n  }\n\n  size_t size = unbound(elementCount / ELEMENTS);\n\n  KJ_REQUIRE(size > 0, \"Message contains text that is not NUL-terminated.\") {\n    return Text::Reader();\n  }\n\n  const char* cptr = reinterpret_cast<const char*>(ptr);\n  --size;  // NUL terminator\n\n  KJ_REQUIRE(cptr[size] == '\\0', \"Message contains text that is not NUL-terminated.\") {\n    return Text::Reader();\n  }\n\n  return Text::Reader(cptr, size);\n}\n\nData::Reader ListReader::asData() {\n  KJ_REQUIRE(structDataSize == G(8) * BITS && structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected Text, got list of non-bytes.\") {\n    return Data::Reader();\n  }\n\n  return Data::Reader(reinterpret_cast<const byte*>(ptr), unbound(elementCount / ELEMENTS));\n}\n\nkj::ArrayPtr<const byte> ListReader::asRawBytes() const {\n  KJ_REQUIRE(structPointerCount == ZERO * POINTERS,\n             \"Schema mismatch: Expected data only, got pointers.\") {\n    return kj::ArrayPtr<const byte>();\n  }\n\n  return arrayPtr(reinterpret_cast<const byte*>(ptr),\n      WireHelpers::roundBitsUpToBytes(\n          upgradeBound<uint64_t>(elementCount) * (structDataSize / ELEMENTS)));\n}\n\nStructReader ListReader::getStructElement(ElementCount index) const {\n  KJ_REQUIRE(nestingLimit > 0,\n             \"Message is too deeply-nested or contains cycles.  See capnp::ReaderOptions.\") {\n    return StructReader();\n  }\n\n  auto indexBit = upgradeBound<uint64_t>(index) * step;\n  const byte* structData = ptr + indexBit / BITS_PER_BYTE;\n  const WirePointer* structPointers =\n      reinterpret_cast<const WirePointer*>(structData + structDataSize / BITS_PER_BYTE);\n\n  KJ_DASSERT(indexBit % BITS_PER_BYTE == ZERO * BITS);\n  return StructReader(\n      segment, capTable, structData, structPointers,\n      structDataSize, structPointerCount,\n      nestingLimit - 1);\n}\n\nMessageSizeCounts ListReader::totalSize() const {\n  // TODO(cleanup): This is kind of a lot of logic duplicated from WireHelpers::totalSize(), but\n  //   it's unclear how to share it effectively.\n\n  MessageSizeCounts result = { ZERO * WORDS, 0 };\n\n  switch (elementSize) {\n    case ElementSize::VOID:\n      // Nothing.\n      break;\n    case ElementSize::BIT:\n    case ElementSize::BYTE:\n    case ElementSize::TWO_BYTES:\n    case ElementSize::FOUR_BYTES:\n    case ElementSize::EIGHT_BYTES:\n      result.addWords(WireHelpers::roundBitsUpToWords(\n          upgradeBound<uint64_t>(elementCount) * dataBitsPerElement(elementSize)));\n      break;\n    case ElementSize::POINTER: {\n      auto count = elementCount * (POINTERS / ELEMENTS);\n      result.addWords(count * WORDS_PER_POINTER);\n\n      for (auto i: kj::zeroTo(count)) {\n        result += WireHelpers::totalSize(segment, reinterpret_cast<const WirePointer*>(ptr) + i,\n                                         nestingLimit);\n      }\n      break;\n    }\n    case ElementSize::INLINE_COMPOSITE: {\n      // Don't forget to count the tag word.\n      auto wordSize = upgradeBound<uint64_t>(elementCount) * step / BITS_PER_WORD;\n      result.addWords(wordSize + POINTER_SIZE_IN_WORDS);\n\n      if (structPointerCount > ZERO * POINTERS) {\n        const word* pos = reinterpret_cast<const word*>(ptr);\n        for (auto i KJ_UNUSED: kj::zeroTo(elementCount)) {\n          pos += structDataSize / BITS_PER_WORD;\n\n          for (auto j KJ_UNUSED: kj::zeroTo(structPointerCount)) {\n            result += WireHelpers::totalSize(segment, reinterpret_cast<const WirePointer*>(pos),\n                                             nestingLimit);\n            pos += POINTER_SIZE_IN_WORDS;\n          }\n        }\n      }\n      break;\n    }\n  }\n\n  if (segment != nullptr) {\n    // This traversal should not count against the read limit, because it's highly likely that\n    // the caller is going to traverse the object again, e.g. to copy it.\n    segment->unread(result.wordCount);\n  }\n\n  return result;\n}\n\nCapTableReader* ListReader::getCapTable() {\n  return capTable;\n}\n\nListReader ListReader::imbue(CapTableReader* capTable) const {\n  auto result = *this;\n  result.capTable = capTable;\n  return result;\n}\n\nbool ListReader::isCanonical(const word **readHead, const WirePointer *ref) {\n  switch (this->getElementSize()) {\n    case ElementSize::INLINE_COMPOSITE: {\n      *readHead += 1;\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        // The next word to read is the tag word, but the pointer is in\n        // front of it, so our check is slightly different\n        return false;\n      }\n      if (this->structDataSize % BITS_PER_WORD != ZERO * BITS) {\n        return false;\n      }\n      auto elementSize = StructSize(this->structDataSize / BITS_PER_WORD,\n                                    this->structPointerCount).total() / ELEMENTS;\n      auto totalSize = upgradeBound<uint64_t>(this->elementCount) * elementSize;\n      if (totalSize != ref->listRef.inlineCompositeWordCount()) {\n        return false;\n      }\n      if (elementSize == ZERO * WORDS / ELEMENTS) {\n        return true;\n      }\n      auto listEnd = *readHead + totalSize;\n      auto pointerHead = listEnd;\n      bool listDataTrunc = false;\n      bool listPtrTrunc = false;\n      for (auto ec: kj::zeroTo(this->elementCount)) {\n        bool dataTrunc, ptrTrunc;\n        if (!this->getStructElement(ec).isCanonical(readHead,\n                                                    &pointerHead,\n                                                    &dataTrunc,\n                                                    &ptrTrunc)) {\n          return false;\n        }\n        listDataTrunc |= dataTrunc;\n        listPtrTrunc  |= ptrTrunc;\n      }\n      KJ_REQUIRE(*readHead == listEnd, *readHead, listEnd);\n      *readHead = pointerHead;\n      return listDataTrunc && listPtrTrunc;\n    }\n    case ElementSize::POINTER: {\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        return false;\n      }\n      *readHead += this->elementCount * (POINTERS / ELEMENTS) * WORDS_PER_POINTER;\n      for (auto ec: kj::zeroTo(this->elementCount)) {\n        if (!this->getPointerElement(ec).isCanonical(readHead)) {\n          return false;\n        }\n      }\n      return true;\n    }\n    default: {\n      if (reinterpret_cast<const word*>(this->ptr) != *readHead) {\n        return false;\n      }\n\n      auto bitSize = upgradeBound<uint64_t>(this->elementCount) *\n                     dataBitsPerElement(this->elementSize);\n      auto truncatedByteSize = bitSize / BITS_PER_BYTE;\n      auto byteReadHead = reinterpret_cast<const uint8_t*>(*readHead) + truncatedByteSize;\n      auto readHeadEnd = *readHead + WireHelpers::roundBitsUpToWords(bitSize);\n\n      auto leftoverBits = bitSize % BITS_PER_BYTE;\n      if (leftoverBits > ZERO * BITS) {\n        auto mask = ~((1 << unbound(leftoverBits / BITS)) - 1);\n\n        if (mask & *byteReadHead) {\n          return false;\n        }\n        byteReadHead += 1;\n      }\n\n      while (byteReadHead != reinterpret_cast<const uint8_t*>(readHeadEnd)) {\n        if (*byteReadHead != 0) {\n          return false;\n        }\n        byteReadHead += 1;\n      }\n\n      *readHead = readHeadEnd;\n      return true;\n    }\n  }\n  KJ_UNREACHABLE;\n}\n\n// =======================================================================================\n// OrphanBuilder\n\nOrphanBuilder OrphanBuilder::initStruct(\n    BuilderArena* arena, CapTableBuilder* capTable, StructSize size) {\n  OrphanBuilder result;\n  StructBuilder builder = WireHelpers::initStructPointer(\n      result.tagAsPtr(), nullptr, capTable, size, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initList(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementCount elementCount, ElementSize elementSize) {\n  OrphanBuilder result;\n  ListBuilder builder = WireHelpers::initListPointer(\n      result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initStructList(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementCount elementCount, StructSize elementSize) {\n  OrphanBuilder result;\n  ListBuilder builder = WireHelpers::initStructListPointer(\n      result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initText(\n    BuilderArena* arena, CapTableBuilder* capTable, ByteCount size) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::initTextPointer(result.tagAsPtr(), nullptr, capTable,\n      assertMax<MAX_TEXT_SIZE>(size, ThrowOverflow()), arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::initData(\n    BuilderArena* arena, CapTableBuilder* capTable, ByteCount size) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::initDataPointer(result.tagAsPtr(), nullptr, capTable,\n      assertMaxBits<BLOB_SIZE_BITS>(size), arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, StructReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setStructPointer(\n      nullptr, capTable, result.tagAsPtr(), copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, ListReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setListPointer(\n      nullptr, capTable, result.tagAsPtr(), copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, PointerReader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::copyPointer(\n      nullptr, capTable, result.tagAsPtr(),\n      copyFrom.segment, copyFrom.capTable, copyFrom.pointer, copyFrom.nestingLimit, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value);\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, Text::Reader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setTextPointer(\n      result.tagAsPtr(), nullptr, capTable, copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, Data::Reader copyFrom) {\n  OrphanBuilder result;\n  auto allocation = WireHelpers::setDataPointer(\n      result.tagAsPtr(), nullptr, capTable, copyFrom, arena);\n  result.segment = allocation.segment;\n  result.capTable = capTable;\n  result.location = reinterpret_cast<word*>(allocation.value.begin());\n  return result;\n}\n\n#if !CAPNP_LITE\nOrphanBuilder OrphanBuilder::copy(\n    BuilderArena* arena, CapTableBuilder* capTable, kj::Own<ClientHook> copyFrom) {\n  OrphanBuilder result;\n  WireHelpers::setCapabilityPointer(nullptr, capTable, result.tagAsPtr(), kj::mv(copyFrom));\n  result.segment = arena->getSegment(SegmentId(0));\n  result.capTable = capTable;\n  result.location = &result.tag;  // dummy to make location non-null\n  return result;\n}\n#endif  // !CAPNP_LITE\n\nOrphanBuilder OrphanBuilder::concat(\n    BuilderArena* arena, CapTableBuilder* capTable,\n    ElementSize elementSize, StructSize structSize,\n    kj::ArrayPtr<const ListReader> lists) {\n  KJ_REQUIRE(lists.size() > 0, \"Can't concat empty list \");\n\n  // Find the overall element count and size.\n  ListElementCount elementCount = ZERO * ELEMENTS;\n  for (auto& list: lists) {\n    elementCount = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(elementCount + list.elementCount,\n        []() { KJ_FAIL_REQUIRE(\"concatenated list exceeds list size limit\"); });\n    if (list.elementSize != elementSize) {\n      // If element sizes don't all match, upgrade to struct list.\n      KJ_REQUIRE(list.elementSize != ElementSize::BIT && elementSize != ElementSize::BIT,\n                 \"can't upgrade bit lists to struct lists\");\n      elementSize = ElementSize::INLINE_COMPOSITE;\n    }\n    structSize.data = kj::max(structSize.data,\n        WireHelpers::roundBitsUpToWords(list.structDataSize));\n    structSize.pointers = kj::max(structSize.pointers, list.structPointerCount);\n  }\n\n  // Allocate the list.\n  OrphanBuilder result;\n  ListBuilder builder = (elementSize == ElementSize::INLINE_COMPOSITE)\n      ? WireHelpers::initStructListPointer(\n          result.tagAsPtr(), nullptr, capTable, elementCount, structSize, arena)\n      : WireHelpers::initListPointer(\n          result.tagAsPtr(), nullptr, capTable, elementCount, elementSize, arena);\n\n  // Copy elements.\n  switch (elementSize) {\n    case ElementSize::INLINE_COMPOSITE: {\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.getStructElement(pos).copyContentFrom(list.getStructElement(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    case ElementSize::POINTER: {\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.getPointerElement(pos).copyFrom(list.getPointerElement(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    case ElementSize::BIT: {\n      // It's difficult to memcpy() bits since a list could start or end mid-byte. For now we\n      // do a slow, naive loop. Probably no one will ever care.\n      ListElementCount pos = ZERO * ELEMENTS;\n      for (auto& list: lists) {\n        for (auto i: kj::zeroTo(list.size())) {\n          builder.setDataElement<bool>(pos, list.getDataElement<bool>(i));\n          // assumeBits() safe because we checked total size earlier.\n          pos = assumeBits<LIST_ELEMENT_COUNT_BITS>(pos + ONE * ELEMENTS);\n        }\n      }\n      break;\n    }\n    default: {\n      // We know all the inputs are primitives with identical size because otherwise we would have\n      // chosen INLINE_COMPOSITE. Therefore, we can safely use memcpy() here instead of copying\n      // each element manually.\n      byte* target = builder.ptr;\n      auto step = builder.step / BITS_PER_BYTE;\n      for (auto& list: lists) {\n        auto count = step * upgradeBound<uint64_t>(list.size());\n        WireHelpers::copyMemory(target, list.ptr, assumeBits<SEGMENT_WORD_COUNT_BITS>(count));\n        target += count;\n      }\n      break;\n    }\n  }\n\n  // Return orphan.\n  result.segment = builder.segment;\n  result.capTable = capTable;\n  result.location = builder.getLocation();\n  return result;\n}\n\nOrphanBuilder OrphanBuilder::referenceExternalData(BuilderArena* arena, Data::Reader data) {\n  KJ_REQUIRE(reinterpret_cast<uintptr_t>(data.begin()) % sizeof(void*) == 0,\n             \"Cannot referenceExternalData() that is not aligned.\");\n\n  auto checkedSize = assertMaxBits<BLOB_SIZE_BITS>(bounded(data.size()));\n  auto wordCount = WireHelpers::roundBytesUpToWords(checkedSize * BYTES);\n  kj::ArrayPtr<const word> words(reinterpret_cast<const word*>(data.begin()),\n                                 unbound(wordCount / WORDS));\n\n  OrphanBuilder result;\n  result.tagAsPtr()->setKindForOrphan(WirePointer::LIST);\n  result.tagAsPtr()->listRef.set(ElementSize::BYTE, checkedSize * ELEMENTS);\n  result.segment = arena->addExternalSegment(words);\n\n  // External data cannot possibly contain capabilities.\n  result.capTable = nullptr;\n\n  // const_cast OK here because we will check whether the segment is writable when we try to get\n  // a builder.\n  result.location = const_cast<word*>(words.begin());\n\n  return result;\n}\n\nStructBuilder OrphanBuilder::asStruct(StructSize size) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  StructBuilder result = WireHelpers::getWritableStructPointer(\n      tagAsPtr(), location, segment, capTable, size, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = reinterpret_cast<word*>(result.data);\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asList(ElementSize elementSize) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableListPointer(\n      tagAsPtr(), location, segment, capTable, elementSize, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  // (Actually, currently this is not true for primitive lists, but let's not turn into a bug if\n  // it changes!)\n  location = result.getLocation();\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asStructList(StructSize elementSize) {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableStructListPointer(\n      tagAsPtr(), location, segment, capTable, elementSize, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = result.getLocation();\n\n  return result;\n}\n\nListBuilder OrphanBuilder::asListAnySize() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  ListBuilder result = WireHelpers::getWritableListPointerAnySize(\n      tagAsPtr(), location, segment, capTable, nullptr, segment->getArena());\n\n  // Watch out, the pointer could have been updated if the object had to be relocated.\n  location = result.getLocation();\n\n  return result;\n}\n\nText::Builder OrphanBuilder::asText() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  // Never relocates.\n  return WireHelpers::getWritableTextPointer(\n      tagAsPtr(), location, segment, capTable, nullptr, ZERO * BYTES);\n}\n\nData::Builder OrphanBuilder::asData() {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n\n  // Never relocates.\n  return WireHelpers::getWritableDataPointer(\n      tagAsPtr(), location, segment, capTable, nullptr, ZERO * BYTES);\n}\n\nStructReader OrphanBuilder::asStructReader(StructSize size) const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readStructPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, kj::maxValue);\n}\n\nListReader OrphanBuilder::asListReader(ElementSize elementSize) const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readListPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, elementSize, kj::maxValue);\n}\n\nListReader OrphanBuilder::asListReaderAnySize() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readListPointer(\n      segment, capTable, tagAsPtr(), location, nullptr, ElementSize::VOID /* dummy */,\n      kj::maxValue);\n}\n\n#if !CAPNP_LITE\nkj::Own<ClientHook> OrphanBuilder::asCapability() const {\n  return WireHelpers::readCapabilityPointer(segment, capTable, tagAsPtr(), kj::maxValue);\n}\n#endif  // !CAPNP_LITE\n\nText::Reader OrphanBuilder::asTextReader() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readTextPointer(segment, tagAsPtr(), location, nullptr, ZERO * BYTES);\n}\n\nData::Reader OrphanBuilder::asDataReader() const {\n  KJ_DASSERT(tagAsPtr()->isNull() == (location == nullptr));\n  return WireHelpers::readDataPointer(segment, tagAsPtr(), location, nullptr, ZERO * BYTES);\n}\n\nbool OrphanBuilder::truncate(ElementCount uncheckedSize, bool isText) {\n  ListElementCount size = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(uncheckedSize,\n      []() { KJ_FAIL_REQUIRE(\"requested list size is too large\"); });\n\n  WirePointer* ref = tagAsPtr();\n  SegmentBuilder* segment = this->segment;\n\n  word* target = WireHelpers::followFars(ref, location, segment);\n\n  if (ref->isNull()) {\n    // We don't know the right element size, so we can't resize this list.\n    return size == ZERO * ELEMENTS;\n  }\n\n  KJ_REQUIRE(ref->kind() == WirePointer::LIST, \"Schema mismatch: Can't truncate non-list.\") {\n    return false;\n  }\n\n  if (isText) {\n    // Add space for the NUL terminator.\n    size = assertMaxBits<LIST_ELEMENT_COUNT_BITS>(size + ONE * ELEMENTS,\n        []() { KJ_FAIL_REQUIRE(\"requested list size is too large\"); });\n  }\n\n  auto elementSize = ref->listRef.elementSize();\n\n  if (elementSize == ElementSize::INLINE_COMPOSITE) {\n    auto oldWordCount = ref->listRef.inlineCompositeWordCount();\n\n    WirePointer* tag = reinterpret_cast<WirePointer*>(target);\n    ++target;\n    KJ_REQUIRE(tag->kind() == WirePointer::STRUCT,\n               \"INLINE_COMPOSITE lists of non-STRUCT type are not supported.\") {\n      return false;\n    }\n    StructSize structSize(tag->structRef.dataSize.get(), tag->structRef.ptrCount.get());\n    auto elementStep = structSize.total() / ELEMENTS;\n\n    auto oldSize = tag->inlineCompositeListElementCount();\n\n    SegmentWordCount sizeWords = assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n        upgradeBound<uint64_t>(size) * elementStep,\n        []() { KJ_FAIL_ASSERT(\"requested list size too large to fit in message segment\"); });\n    SegmentWordCount oldSizeWords = assertMaxBits<SEGMENT_WORD_COUNT_BITS>(\n        upgradeBound<uint64_t>(oldSize) * elementStep,\n        []() { KJ_FAIL_ASSERT(\"prior to truncate, list is larger than max segment size?\"); });\n\n    word* newEndWord = target + sizeWords;\n    word* oldEndWord = target + oldWordCount;\n\n    if (size <= oldSize) {\n      // Zero the trailing elements.\n      for (auto i: kj::range(size, oldSize)) {\n        // assumeBits() safe because we checked that both sizeWords and oldSizeWords are in-range\n        // above.\n        WireHelpers::zeroObject(segment, capTable, tag, target +\n            assumeBits<SEGMENT_WORD_COUNT_BITS>(upgradeBound<uint64_t>(i) * elementStep));\n      }\n      ref->listRef.setInlineComposite(sizeWords);\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else if (newEndWord <= oldEndWord) {\n      // Apparently the old list was over-allocated? The word count is more than needed to store\n      // the elements. This is \"valid\" but shouldn't happen in practice unless someone is toying\n      // with us.\n      word* expectedEnd = target + oldSizeWords;\n      KJ_ASSERT(newEndWord >= expectedEnd);\n      WireHelpers::zeroMemory(expectedEnd,\n          intervalLength(expectedEnd, newEndWord, MAX_SEGMENT_WORDS));\n      tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n    } else {\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the new memory is already zero'd.\n        ref->listRef.setInlineComposite(sizeWords);\n        tag->setKindAndInlineCompositeListElementCount(WirePointer::STRUCT, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initStructList(segment->getArena(), capTable, size, structSize);\n\n        ListBuilder newList = replacement.asStructList(structSize);\n        for (auto i: kj::zeroTo(oldSize)) {\n          // assumeBits() safe because we checked that both sizeWords and oldSizeWords are in-range\n          // above.\n          word* element = target +\n              assumeBits<SEGMENT_WORD_COUNT_BITS>(upgradeBound<uint64_t>(i) * elementStep);\n          newList.getStructElement(i).transferContentFrom(\n              StructBuilder(segment, capTable, element,\n                            reinterpret_cast<WirePointer*>(element + structSize.data),\n                            structSize.data * BITS_PER_WORD, structSize.pointers));\n        }\n\n        *this = kj::mv(replacement);\n      }\n    }\n  } else if (elementSize == ElementSize::POINTER) {\n    // TODO(cleanup): GCC won't let me declare this constexpr, claiming POINTERS is not constexpr,\n    //   but it is?\n    const auto POINTERS_PER_ELEMENT = ONE * POINTERS / ELEMENTS;\n\n    auto oldSize = ref->listRef.elementCount();\n    word* newEndWord = target + size * POINTERS_PER_ELEMENT * WORDS_PER_POINTER;\n    word* oldEndWord = target + oldSize * POINTERS_PER_ELEMENT * WORDS_PER_POINTER;\n\n    if (size <= oldSize) {\n      // Zero the trailing elements.\n      for (WirePointer* element = reinterpret_cast<WirePointer*>(newEndWord);\n           element < reinterpret_cast<WirePointer*>(oldEndWord); ++element) {\n        WireHelpers::zeroPointerAndFars(segment, element);\n      }\n      ref->listRef.set(ElementSize::POINTER, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else {\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the new memory is already zero'd.\n        ref->listRef.set(ElementSize::POINTER, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initList(\n            segment->getArena(), capTable, size, ElementSize::POINTER);\n        ListBuilder newList = replacement.asList(ElementSize::POINTER);\n        WirePointer* oldPointers = reinterpret_cast<WirePointer*>(target);\n        for (auto i: kj::zeroTo(oldSize)) {\n          newList.getPointerElement(i).transferFrom(\n              PointerBuilder(segment, capTable, oldPointers + i * POINTERS_PER_ELEMENT));\n        }\n        *this = kj::mv(replacement);\n      }\n    }\n  } else {\n    auto oldSize = ref->listRef.elementCount();\n    auto step = dataBitsPerElement(elementSize);\n    const auto MAX_STEP_BYTES = ONE * WORDS / ELEMENTS * BYTES_PER_WORD;\n    word* newEndWord = target + WireHelpers::roundBitsUpToWords(\n        upgradeBound<uint64_t>(size) * step);\n    word* oldEndWord = target + WireHelpers::roundBitsUpToWords(\n        upgradeBound<uint64_t>(oldSize) * step);\n\n    if (size <= oldSize) {\n      // When truncating text, we want to set the null terminator as well, so we'll do our zeroing\n      // at the byte level.\n      byte* begin = reinterpret_cast<byte*>(target);\n      byte* newEndByte = begin + WireHelpers::roundBitsUpToBytes(\n          upgradeBound<uint64_t>(size) * step) - isText;\n      byte* oldEndByte = reinterpret_cast<byte*>(oldEndWord);\n\n      WireHelpers::zeroMemory(newEndByte,\n          intervalLength(newEndByte, oldEndByte, MAX_LIST_ELEMENTS * MAX_STEP_BYTES));\n      ref->listRef.set(elementSize, size);\n      segment->tryTruncate(oldEndWord, newEndWord);\n    } else {\n      // We're trying to extend, not truncate.\n      if (segment->tryExtend(oldEndWord, newEndWord)) {\n        // Done in-place. Nothing else to do now; the memory is already zero'd.\n        ref->listRef.set(elementSize, size);\n      } else {\n        // Need to re-allocate and transfer.\n        OrphanBuilder replacement = initList(segment->getArena(), capTable, size, elementSize);\n        ListBuilder newList = replacement.asList(elementSize);\n        auto words = WireHelpers::roundBitsUpToWords(\n            dataBitsPerElement(elementSize) * upgradeBound<uint64_t>(oldSize));\n        WireHelpers::copyMemory(reinterpret_cast<word*>(newList.ptr), target, words);\n        *this = kj::mv(replacement);\n      }\n    }\n  }\n\n  return true;\n}\n\nvoid OrphanBuilder::truncate(ElementCount size, ElementSize elementSize) {\n  if (!truncate(size, false)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initList(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size), elementSize);\n  }\n}\n\nvoid OrphanBuilder::truncate(ElementCount size, StructSize elementSize) {\n  if (!truncate(size, false)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initStructList(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size), elementSize);\n  }\n}\n\nvoid OrphanBuilder::truncateText(ElementCount size) {\n  if (!truncate(size, true)) {\n    // assumeBits() safe since it's checked inside truncate()\n    *this = initText(segment->getArena(), capTable,\n        assumeBits<LIST_ELEMENT_COUNT_BITS>(size) * (ONE * BYTES / ELEMENTS));\n  }\n}\n\nvoid OrphanBuilder::euthanize() {\n  // Carefully catch any exceptions and rethrow them as recoverable exceptions since we may be in\n  // a destructor.\n  auto exception = kj::runCatchingExceptions([&]() {\n    if (tagAsPtr()->isPositional()) {\n      WireHelpers::zeroObject(segment, capTable, tagAsPtr(), location);\n    } else {\n      WireHelpers::zeroObject(segment, capTable, tagAsPtr());\n    }\n\n    WireHelpers::zeroMemory(&tag, ONE * WORDS);\n    segment = nullptr;\n    location = nullptr;\n  });\n\n  KJ_IF_MAYBE(e, exception) {\n    kj::getExceptionCallback().onRecoverableException(kj::mv(*e));\n  }\n}\n\n}  // namespace _ (private)\n}  // namespace capnp\n", "// Copyright (c) 2013-2016 Sandstorm Development Group, Inc. and contributors\n// Licensed under the MIT License:\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n// This file is NOT intended for use by clients, except in generated code.\n//\n// This file defines low-level, non-type-safe classes for traversing the Cap'n Proto memory layout\n// (which is also its wire format).  Code generated by the Cap'n Proto compiler uses these classes,\n// as does other parts of the Cap'n proto library which provide a higher-level interface for\n// dynamic introspection.\n\n#pragma once\n\n#include <kj/common.h>\n#include <kj/memory.h>\n#include \"common.h\"\n#include \"blob.h\"\n#include \"endian.h\"\n#include <kj/windows-sanity.h>  // work-around macro conflict with `VOID`\n\nCAPNP_BEGIN_HEADER\n\n#if (defined(__mips__) || defined(__hppa__)) && !defined(CAPNP_CANONICALIZE_NAN)\n#define CAPNP_CANONICALIZE_NAN 1\n// Explicitly detect NaNs and canonicalize them to the quiet NaN value as would be returned by\n// __builtin_nan(\"\") on systems implementing the IEEE-754 recommended (but not required) NaN\n// signalling/quiet differentiation (such as x86).  Unfortunately, some architectures -- in\n// particular, MIPS -- represent quiet vs. signalling nans differently than the rest of the world.\n// Canonicalizing them makes output consistent (which is important!), but hurts performance\n// slightly.\n//\n// Note that trying to convert MIPS NaNs to standard NaNs without losing data doesn't work.\n// Signaling vs. quiet is indicated by a bit, with the meaning being the opposite on MIPS vs.\n// everyone else.  It would be great if we could just flip that bit, but we can't, because if the\n// significand is all-zero, then the value is infinity rather than NaN.  This means that on most\n// machines, where the bit indicates quietness, there is one more quiet NaN value than signalling\n// NaN value, whereas on MIPS there is one more sNaN than qNaN, and thus there is no isomorphic\n// mapping that properly preserves quietness.  Instead of doing something hacky, we just give up\n// and blow away NaN payloads, because no one uses them anyway.\n#endif\n\nnamespace capnp {\n\nclass ClientHook;\n\nnamespace _ {  // private\n\nclass PointerBuilder;\nclass PointerReader;\nclass StructBuilder;\nclass StructReader;\nclass ListBuilder;\nclass ListReader;\nclass OrphanBuilder;\nstruct WirePointer;\nstruct WireHelpers;\nclass SegmentReader;\nclass SegmentBuilder;\nclass Arena;\nclass BuilderArena;\n\n// =============================================================================\n\n#if CAPNP_DEBUG_TYPES\ntypedef kj::UnitRatio<kj::Bounded<64, uint>, BitLabel, ElementLabel> BitsPerElementTableType;\n#else\ntypedef uint BitsPerElementTableType;\n#endif\n\nstatic constexpr BitsPerElementTableType BITS_PER_ELEMENT_TABLE[8] = {\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 1>() * BITS / ELEMENTS,\n  bounded< 8>() * BITS / ELEMENTS,\n  bounded<16>() * BITS / ELEMENTS,\n  bounded<32>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS\n};\n\ninline KJ_CONSTEXPR() BitsPerElementTableType dataBitsPerElement(ElementSize size) {\n  return _::BITS_PER_ELEMENT_TABLE[static_cast<int>(size)];\n}\n\ninline constexpr PointersPerElementN<1> pointersPerElement(ElementSize size) {\n  return size == ElementSize::POINTER\n      ? PointersPerElementN<1>(ONE * POINTERS / ELEMENTS)\n      : PointersPerElementN<1>(ZERO * POINTERS / ELEMENTS);\n}\n\nstatic constexpr BitsPerElementTableType BITS_PER_ELEMENT_INCLUDING_PONITERS_TABLE[8] = {\n  bounded< 0>() * BITS / ELEMENTS,\n  bounded< 1>() * BITS / ELEMENTS,\n  bounded< 8>() * BITS / ELEMENTS,\n  bounded<16>() * BITS / ELEMENTS,\n  bounded<32>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded<64>() * BITS / ELEMENTS,\n  bounded< 0>() * BITS / ELEMENTS\n};\n\ninline KJ_CONSTEXPR() BitsPerElementTableType bitsPerElementIncludingPointers(ElementSize size) {\n  return _::BITS_PER_ELEMENT_INCLUDING_PONITERS_TABLE[static_cast<int>(size)];\n}\n\ntemplate <size_t size> struct ElementSizeForByteSize;\ntemplate <> struct ElementSizeForByteSize<1> { static constexpr ElementSize value = ElementSize::BYTE; };\ntemplate <> struct ElementSizeForByteSize<2> { static constexpr ElementSize value = ElementSize::TWO_BYTES; };\ntemplate <> struct ElementSizeForByteSize<4> { static constexpr ElementSize value = ElementSize::FOUR_BYTES; };\ntemplate <> struct ElementSizeForByteSize<8> { static constexpr ElementSize value = ElementSize::EIGHT_BYTES; };\n\ntemplate <typename T> struct ElementSizeForType {\n  static constexpr ElementSize value =\n      // Primitive types that aren't special-cased below can be determined from sizeof().\n      CAPNP_KIND(T) == Kind::PRIMITIVE ? ElementSizeForByteSize<sizeof(T)>::value :\n      CAPNP_KIND(T) == Kind::ENUM ? ElementSize::TWO_BYTES :\n      CAPNP_KIND(T) == Kind::STRUCT ? ElementSize::INLINE_COMPOSITE :\n\n      // Everything else is a pointer.\n      ElementSize::POINTER;\n};\n\n// Void and bool are special.\ntemplate <> struct ElementSizeForType<Void> { static constexpr ElementSize value = ElementSize::VOID; };\ntemplate <> struct ElementSizeForType<bool> { static constexpr ElementSize value = ElementSize::BIT; };\n\n// Lists and blobs are pointers, not structs.\ntemplate <typename T, Kind K> struct ElementSizeForType<List<T, K>> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\ntemplate <> struct ElementSizeForType<Text> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\ntemplate <> struct ElementSizeForType<Data> {\n  static constexpr ElementSize value = ElementSize::POINTER;\n};\n\ntemplate <typename T>\ninline constexpr ElementSize elementSizeForType() {\n  return ElementSizeForType<T>::value;\n}\n\nstruct MessageSizeCounts {\n  WordCountN<61, uint64_t> wordCount;  // 2^64 bytes\n  uint capCount;\n\n  MessageSizeCounts& operator+=(const MessageSizeCounts& other) {\n    // OK to truncate unchecked because this class is used to count actual stuff in memory, and\n    // we couldn't possibly have anywhere near 2^61 words.\n    wordCount = assumeBits<61>(wordCount + other.wordCount);\n    capCount += other.capCount;\n    return *this;\n  }\n\n  void addWords(WordCountN<61, uint64_t> other) {\n    wordCount = assumeBits<61>(wordCount + other);\n  }\n\n  MessageSize asPublic() {\n    return MessageSize { unbound(wordCount / WORDS), capCount };\n  }\n};\n\n// =============================================================================\n\ntemplate <int wordCount>\nunion AlignedData {\n  // Useful for declaring static constant data blobs as an array of bytes, but forcing those\n  // bytes to be word-aligned.\n\n  uint8_t bytes[wordCount * sizeof(word)];\n  word words[wordCount];\n};\n\nstruct StructSize {\n  StructDataWordCount data;\n  StructPointerCount pointers;\n\n  inline constexpr WordCountN<17> total() const { return data + pointers * WORDS_PER_POINTER; }\n\n  StructSize() = default;\n  inline constexpr StructSize(StructDataWordCount data, StructPointerCount pointers)\n      : data(data), pointers(pointers) {}\n};\n\ntemplate <typename T, typename CapnpPrivate = typename T::_capnpPrivate>\ninline constexpr StructSize structSize() {\n  return StructSize(bounded(CapnpPrivate::dataWordSize) * WORDS,\n                    bounded(CapnpPrivate::pointerCount) * POINTERS);\n}\n\ntemplate <typename T, typename CapnpPrivate = typename T::_capnpPrivate,\n          typename = kj::EnableIf<CAPNP_KIND(T) == Kind::STRUCT>>\ninline constexpr StructSize minStructSizeForElement() {\n  // If T is a struct, return its struct size. Otherwise return the minimum struct size big enough\n  // to hold a T.\n\n  return StructSize(bounded(CapnpPrivate::dataWordSize) * WORDS,\n                    bounded(CapnpPrivate::pointerCount) * POINTERS);\n}\n\ntemplate <typename T, typename = kj::EnableIf<CAPNP_KIND(T) != Kind::STRUCT>>\ninline constexpr StructSize minStructSizeForElement() {\n  // If T is a struct, return its struct size. Otherwise return the minimum struct size big enough\n  // to hold a T.\n\n  return StructSize(\n      dataBitsPerElement(elementSizeForType<T>()) * ELEMENTS > ZERO * BITS\n          ? StructDataWordCount(ONE * WORDS) : StructDataWordCount(ZERO * WORDS),\n      pointersPerElement(elementSizeForType<T>()) * ELEMENTS);\n}\n\n// -------------------------------------------------------------------\n// Masking of default values\n\ntemplate <typename T, Kind kind = CAPNP_KIND(T)> struct Mask_;\ntemplate <typename T> struct Mask_<T, Kind::PRIMITIVE> { typedef T Type; };\ntemplate <typename T> struct Mask_<T, Kind::ENUM> { typedef uint16_t Type; };\ntemplate <> struct Mask_<float, Kind::PRIMITIVE> { typedef uint32_t Type; };\ntemplate <> struct Mask_<double, Kind::PRIMITIVE> { typedef uint64_t Type; };\n\ntemplate <typename T> struct Mask_<T, Kind::OTHER> {\n  // Union discriminants end up here.\n  static_assert(sizeof(T) == 2, \"Don't know how to mask this type.\");\n  typedef uint16_t Type;\n};\n\ntemplate <typename T>\nusing Mask = typename Mask_<T>::Type;\n\ntemplate <typename T>\nKJ_ALWAYS_INLINE(Mask<T> mask(T value, Mask<T> mask));\ntemplate <typename T>\nKJ_ALWAYS_INLINE(T unmask(Mask<T> value, Mask<T> mask));\n\ntemplate <typename T>\ninline Mask<T> mask(T value, Mask<T> mask) {\n  return static_cast<Mask<T> >(value) ^ mask;\n}\n\ntemplate <>\ninline uint32_t mask<float>(float value, uint32_t mask) {\n#if CAPNP_CANONICALIZE_NAN\n  if (value != value) {\n    return 0x7fc00000u ^ mask;\n  }\n#endif\n\n  uint32_t i;\n  static_assert(sizeof(i) == sizeof(value), \"float is not 32 bits?\");\n  memcpy(&i, &value, sizeof(value));\n  return i ^ mask;\n}\n\ntemplate <>\ninline uint64_t mask<double>(double value, uint64_t mask) {\n#if CAPNP_CANONICALIZE_NAN\n  if (value != value) {\n    return 0x7ff8000000000000ull ^ mask;\n  }\n#endif\n\n  uint64_t i;\n  static_assert(sizeof(i) == sizeof(value), \"double is not 64 bits?\");\n  memcpy(&i, &value, sizeof(value));\n  return i ^ mask;\n}\n\ntemplate <typename T>\ninline T unmask(Mask<T> value, Mask<T> mask) {\n  return static_cast<T>(value ^ mask);\n}\n\ntemplate <>\ninline float unmask<float>(uint32_t value, uint32_t mask) {\n  value ^= mask;\n  float result;\n  static_assert(sizeof(result) == sizeof(value), \"float is not 32 bits?\");\n  memcpy(&result, &value, sizeof(value));\n  return result;\n}\n\ntemplate <>\ninline double unmask<double>(uint64_t value, uint64_t mask) {\n  value ^= mask;\n  double result;\n  static_assert(sizeof(result) == sizeof(value), \"double is not 64 bits?\");\n  memcpy(&result, &value, sizeof(value));\n  return result;\n}\n\n// -------------------------------------------------------------------\n\nclass CapTableReader {\npublic:\n  virtual kj::Maybe<kj::Own<ClientHook>> extractCap(uint index) = 0;\n  // Extract the capability at the given index.  If the index is invalid, returns null.\n};\n\nclass CapTableBuilder: public CapTableReader {\npublic:\n  virtual uint injectCap(kj::Own<ClientHook>&& cap) = 0;\n  // Add the capability to the message and return its index.  If the same ClientHook is injected\n  // twice, this may return the same index both times, but in this case dropCap() needs to be\n  // called an equal number of times to actually remove the cap.\n\n  virtual void dropCap(uint index) = 0;\n  // Remove a capability injected earlier.  Called when the pointer is overwritten or zero'd out.\n};\n\n// -------------------------------------------------------------------\n\nclass PointerBuilder: public kj::DisallowConstCopy {\n  // Represents a single pointer, usually embedded in a struct or a list.\n\npublic:\n  inline PointerBuilder(): segment(nullptr), capTable(nullptr), pointer(nullptr) {}\n\n  static inline PointerBuilder getRoot(\n      SegmentBuilder* segment, CapTableBuilder* capTable, word* location);\n  // Get a PointerBuilder representing a message root located in the given segment at the given\n  // location.\n\n  inline bool isNull() { return getPointerType() == PointerType::NULL_; }\n  PointerType getPointerType() const;\n\n  StructBuilder getStruct(StructSize size, const word* defaultValue);\n  ListBuilder getList(ElementSize elementSize, const word* defaultValue);\n  ListBuilder getStructList(StructSize elementSize, const word* defaultValue);\n  ListBuilder getListAnySize(const word* defaultValue);\n  template <typename T> typename T::Builder getBlob(\n      const void* defaultValue, ByteCount defaultSize);\n#if !CAPNP_LITE\n  kj::Own<ClientHook> getCapability();\n#endif  // !CAPNP_LITE\n  // Get methods:  Get the value.  If it is null, initialize it to a copy of the default value.\n  // The default value is encoded as an \"unchecked message\" for structs, lists, and objects, or a\n  // simple byte array for blobs.\n\n  StructBuilder initStruct(StructSize size);\n  ListBuilder initList(ElementSize elementSize, ElementCount elementCount);\n  ListBuilder initStructList(ElementCount elementCount, StructSize size);\n  template <typename T> typename T::Builder initBlob(ByteCount size);\n  // Init methods:  Initialize the pointer to a newly-allocated object, discarding the existing\n  // object.\n\n  void setStruct(const StructReader& value, bool canonical = false);\n  void setList(const ListReader& value, bool canonical = false);\n  template <typename T> void setBlob(typename T::Reader value);\n#if !CAPNP_LITE\n  void setCapability(kj::Own<ClientHook>&& cap);\n#endif  // !CAPNP_LITE\n  // Set methods:  Initialize the pointer to a newly-allocated copy of the given value, discarding\n  // the existing object.\n\n  void adopt(OrphanBuilder&& orphan);\n  // Set the pointer to point at the given orphaned value.\n\n  OrphanBuilder disown();\n  // Set the pointer to null and return its previous value as an orphan.\n\n  void clear();\n  // Clear the pointer to null, discarding its previous value.\n\n  void transferFrom(PointerBuilder other);\n  // Equivalent to `adopt(other.disown())`.\n\n  void copyFrom(PointerReader other, bool canonical = false);\n  // Equivalent to `set(other.get())`.\n  // If you set the canonical flag, it will attempt to lay the target out\n  // canonically, provided enough space is available.\n\n  PointerReader asReader() const;\n\n  BuilderArena* getArena() const;\n  // Get the arena containing this pointer.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  PointerBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;     // Memory segment in which the pointer resides.\n  CapTableBuilder* capTable;   // Table of capability indexes.\n  WirePointer* pointer;        // Pointer to the pointer.\n\n  inline PointerBuilder(SegmentBuilder* segment, CapTableBuilder* capTable, WirePointer* pointer)\n      : segment(segment), capTable(capTable), pointer(pointer) {}\n\n  friend class StructBuilder;\n  friend class ListBuilder;\n  friend class OrphanBuilder;\n};\n\nclass PointerReader {\npublic:\n  inline PointerReader()\n      : segment(nullptr), capTable(nullptr), pointer(nullptr), nestingLimit(0x7fffffff) {}\n\n  static PointerReader getRoot(SegmentReader* segment, CapTableReader* capTable,\n                               const word* location, int nestingLimit);\n  // Get a PointerReader representing a message root located in the given segment at the given\n  // location.\n\n  static inline PointerReader getRootUnchecked(const word* location);\n  // Get a PointerReader for an unchecked message.\n\n  MessageSizeCounts targetSize() const;\n  // Return the total size of the target object and everything to which it points.  Does not count\n  // far pointer overhead.  This is useful for deciding how much space is needed to copy the object\n  // into a flat array.  However, the caller is advised NOT to treat this value as secure.  Instead,\n  // use the result as a hint for allocating the first segment, do the copy, and then throw an\n  // exception if it overruns.\n\n  inline bool isNull() const { return getPointerType() == PointerType::NULL_; }\n  PointerType getPointerType() const;\n\n  StructReader getStruct(const word* defaultValue) const;\n  ListReader getList(ElementSize expectedElementSize, const word* defaultValue) const;\n  ListReader getListAnySize(const word* defaultValue) const;\n  template <typename T>\n  typename T::Reader getBlob(const void* defaultValue, ByteCount defaultSize) const;\n#if !CAPNP_LITE\n  kj::Own<ClientHook> getCapability() const;\n#endif  // !CAPNP_LITE\n  // Get methods:  Get the value.  If it is null, return the default value instead.\n  // The default value is encoded as an \"unchecked message\" for structs, lists, and objects, or a\n  // simple byte array for blobs.\n\n  const word* getUnchecked() const;\n  // If this is an unchecked message, get a word* pointing at the location of the pointer.  This\n  // word* can actually be passed to readUnchecked() to read the designated sub-object later.  If\n  // this isn't an unchecked message, throws an exception.\n\n  kj::Maybe<Arena&> getArena() const;\n  // Get the arena containing this pointer.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  PointerReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n\nprivate:\n  SegmentReader* segment;      // Memory segment in which the pointer resides.\n  CapTableReader* capTable;    // Table of capability indexes.\n  const WirePointer* pointer;  // Pointer to the pointer.  null = treat as null pointer.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n\n  inline PointerReader(SegmentReader* segment, CapTableReader* capTable,\n                       const WirePointer* pointer, int nestingLimit)\n      : segment(segment), capTable(capTable), pointer(pointer), nestingLimit(nestingLimit) {}\n\n  friend class StructReader;\n  friend class ListReader;\n  friend class PointerBuilder;\n  friend class OrphanBuilder;\n};\n\n// -------------------------------------------------------------------\n\nclass StructBuilder: public kj::DisallowConstCopy {\npublic:\n  inline StructBuilder(): segment(nullptr), capTable(nullptr), data(nullptr), pointers(nullptr) {}\n\n  inline word* getLocation() { return reinterpret_cast<word*>(data); }\n  // Get the object's location.  Only valid for independently-allocated objects (i.e. not list\n  // elements).\n\n  inline StructDataBitCount getDataSectionSize() const { return dataSize; }\n  inline StructPointerCount getPointerSectionSize() const { return pointerCount; }\n  inline kj::ArrayPtr<byte> getDataSectionAsBlob();\n  inline _::ListBuilder getPointerSectionAsList();\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(bool hasDataField(StructDataOffset offset));\n  // Return true if the field is set to something other than its default value.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset));\n  // Gets the data field value of the given type at the given offset.  The offset is measured in\n  // multiples of the field size, determined by the type.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset, Mask<T> mask));\n  // Like getDataField() but applies the given XOR mask to the data on load.  Used for reading\n  // fields with non-zero default values.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataField(StructDataOffset offset, kj::NoInfer<T> value));\n  // Sets the data field value at the given offset.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataField(StructDataOffset offset,\n                                     kj::NoInfer<T> value, Mask<T> mask));\n  // Like setDataField() but applies the given XOR mask before storing.  Used for writing fields\n  // with non-zero default values.\n\n  KJ_ALWAYS_INLINE(PointerBuilder getPointerField(StructPointerOffset ptrIndex));\n  // Get a builder for a pointer field given the index within the pointer section.\n\n  void clearAll();\n  // Clear all pointers and data.\n\n  void transferContentFrom(StructBuilder other);\n  // Adopt all pointers from `other`, and also copy all data.  If `other`'s sections are larger\n  // than this, the extra data is not transferred, meaning there is a risk of data loss when\n  // transferring from messages built with future versions of the protocol.\n\n  void copyContentFrom(StructReader other);\n  // Copy content from `other`.  If `other`'s sections are larger than this, the extra data is not\n  // copied, meaning there is a risk of data loss when copying from messages built with future\n  // versions of the protocol.\n\n  StructReader asReader() const;\n  // Gets a StructReader pointing at the same memory.\n\n  BuilderArena* getArena();\n  // Gets the arena in which this object is allocated.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  StructBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;     // Memory segment in which the struct resides.\n  CapTableBuilder* capTable;   // Table of capability indexes.\n  void* data;                  // Pointer to the encoded data.\n  WirePointer* pointers;   // Pointer to the encoded pointers.\n\n  StructDataBitCount dataSize;\n  // Size of data section.  We use a bit count rather than a word count to more easily handle the\n  // case of struct lists encoded with less than a word per element.\n\n  StructPointerCount pointerCount;  // Size of the pointer section.\n\n  inline StructBuilder(SegmentBuilder* segment, CapTableBuilder* capTable,\n                       void* data, WirePointer* pointers,\n                       StructDataBitCount dataSize, StructPointerCount pointerCount)\n      : segment(segment), capTable(capTable), data(data), pointers(pointers),\n        dataSize(dataSize), pointerCount(pointerCount) {}\n\n  friend class ListBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\nclass StructReader {\npublic:\n  inline StructReader()\n      : segment(nullptr), capTable(nullptr), data(nullptr), pointers(nullptr),\n        dataSize(ZERO * BITS), pointerCount(ZERO * POINTERS), nestingLimit(0x7fffffff) {}\n  inline StructReader(kj::ArrayPtr<const word> data)\n      : segment(nullptr), capTable(nullptr), data(data.begin()), pointers(nullptr),\n        dataSize(assumeBits<STRUCT_DATA_WORD_COUNT_BITS>(data.size()) * WORDS * BITS_PER_WORD),\n        pointerCount(ZERO * POINTERS), nestingLimit(0x7fffffff) {}\n\n  const void* getLocation() const { return data; }\n\n  inline StructDataBitCount getDataSectionSize() const { return dataSize; }\n  inline StructPointerCount getPointerSectionSize() const { return pointerCount; }\n  inline kj::ArrayPtr<const byte> getDataSectionAsBlob() const;\n  inline _::ListReader getPointerSectionAsList() const;\n\n  kj::Array<word> canonicalize();\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(bool hasDataField(StructDataOffset offset) const);\n  // Return true if the field is set to something other than its default value.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset) const);\n  // Get the data field value of the given type at the given offset.  The offset is measured in\n  // multiples of the field size, determined by the type.  Returns zero if the offset is past the\n  // end of the struct's data section.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataField(StructDataOffset offset, Mask<T> mask) const);\n  // Like getDataField(offset), but applies the given XOR mask to the result.  Used for reading\n  // fields with non-zero default values.\n\n  KJ_ALWAYS_INLINE(PointerReader getPointerField(StructPointerOffset ptrIndex) const);\n  // Get a reader for a pointer field given the index within the pointer section.  If the index\n  // is out-of-bounds, returns a null pointer.\n\n  MessageSizeCounts totalSize() const;\n  // Return the total size of the struct and everything to which it points.  Does not count far\n  // pointer overhead.  This is useful for deciding how much space is needed to copy the struct\n  // into a flat array.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  StructReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead, const word **ptrHead,\n                   bool *dataTrunc, bool *ptrTrunc);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n  //\n  // If this function returns false, the struct is non-canonical. If it\n  // returns true, then:\n  // * If it is a composite in a list, it is canonical if at least one struct\n  //   in the list outputs dataTrunc = 1, and at least one outputs ptrTrunc = 1\n  // * If it is derived from a struct pointer, it is canonical if\n  //   dataTrunc = 1 AND ptrTrunc = 1\n\nprivate:\n  SegmentReader* segment;    // Memory segment in which the struct resides.\n  CapTableReader* capTable;  // Table of capability indexes.\n\n  const void* data;\n  const WirePointer* pointers;\n\n  StructDataBitCount dataSize;\n  // Size of data section.  We use a bit count rather than a word count to more easily handle the\n  // case of struct lists encoded with less than a word per element.\n\n  StructPointerCount pointerCount;  // Size of the pointer section.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n  // TODO(perf):  Limit to 16 bits for better packing?\n\n  inline StructReader(SegmentReader* segment, CapTableReader* capTable,\n                      const void* data, const WirePointer* pointers,\n                      StructDataBitCount dataSize, StructPointerCount pointerCount,\n                      int nestingLimit)\n      : segment(segment), capTable(capTable), data(data), pointers(pointers),\n        dataSize(dataSize), pointerCount(pointerCount),\n        nestingLimit(nestingLimit) {}\n\n  friend class ListReader;\n  friend class StructBuilder;\n  friend struct WireHelpers;\n};\n\n// -------------------------------------------------------------------\n\nclass ListBuilder: public kj::DisallowConstCopy {\npublic:\n  inline explicit ListBuilder(ElementSize elementSize)\n      : segment(nullptr), capTable(nullptr), ptr(nullptr), elementCount(ZERO * ELEMENTS),\n        step(ZERO * BITS / ELEMENTS), structDataSize(ZERO * BITS),\n        structPointerCount(ZERO * POINTERS), elementSize(elementSize) {}\n\n  inline word* getLocation() {\n    // Get the object's location.\n\n    if (elementSize == ElementSize::INLINE_COMPOSITE && ptr != nullptr) {\n      return reinterpret_cast<word*>(ptr) - POINTER_SIZE_IN_WORDS;\n    } else {\n      return reinterpret_cast<word*>(ptr);\n    }\n  }\n\n  inline ElementSize getElementSize() const { return elementSize; }\n\n  inline ListElementCount size() const;\n  // The number of elements in the list.\n\n  Text::Builder asText();\n  Data::Builder asData();\n  // Reinterpret the list as a blob.  Throws an exception if the elements are not byte-sized.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataElement(ElementCount index));\n  // Get the element of the given type at the given index.\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(void setDataElement(ElementCount index, kj::NoInfer<T> value));\n  // Set the element at the given index.\n\n  KJ_ALWAYS_INLINE(PointerBuilder getPointerElement(ElementCount index));\n\n  StructBuilder getStructElement(ElementCount index);\n\n  ListReader asReader() const;\n  // Get a ListReader pointing at the same memory.\n\n  BuilderArena* getArena();\n  // Gets the arena in which this object is allocated.\n\n  CapTableBuilder* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  ListBuilder imbue(CapTableBuilder* capTable);\n  // Return a copy of this builder except using the given capability context.\n\nprivate:\n  SegmentBuilder* segment;    // Memory segment in which the list resides.\n  CapTableBuilder* capTable;  // Table of capability indexes.\n\n  byte* ptr;  // Pointer to list content.\n\n  ListElementCount elementCount;  // Number of elements in the list.\n\n  BitsPerElementN<23> step;\n  // The distance between elements. The maximum value occurs when a struct contains 2^16-1 data\n  // words and 2^16-1 pointers, i.e. 2^17 - 2 words, or 2^23 - 128 bits.\n\n  StructDataBitCount structDataSize;\n  StructPointerCount structPointerCount;\n  // The struct properties to use when interpreting the elements as structs.  All lists can be\n  // interpreted as struct lists, so these are always filled in.\n\n  ElementSize elementSize;\n  // The element size as a ElementSize. This is only really needed to disambiguate INLINE_COMPOSITE\n  // from other types when the overall size is exactly zero or one words.\n\n  inline ListBuilder(SegmentBuilder* segment, CapTableBuilder* capTable, void* ptr,\n                     BitsPerElementN<23> step, ListElementCount size,\n                     StructDataBitCount structDataSize, StructPointerCount structPointerCount,\n                     ElementSize elementSize)\n      : segment(segment), capTable(capTable), ptr(reinterpret_cast<byte*>(ptr)),\n        elementCount(size), step(step), structDataSize(structDataSize),\n        structPointerCount(structPointerCount), elementSize(elementSize) {}\n\n  friend class StructBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\nclass ListReader {\npublic:\n  inline explicit ListReader(ElementSize elementSize)\n      : segment(nullptr), capTable(nullptr), ptr(nullptr), elementCount(ZERO * ELEMENTS),\n        step(ZERO * BITS / ELEMENTS), structDataSize(ZERO * BITS),\n        structPointerCount(ZERO * POINTERS), elementSize(elementSize), nestingLimit(0x7fffffff) {}\n\n  inline ListElementCount size() const;\n  // The number of elements in the list.\n\n  inline ElementSize getElementSize() const { return elementSize; }\n\n  Text::Reader asText();\n  Data::Reader asData();\n  // Reinterpret the list as a blob.  Throws an exception if the elements are not byte-sized.\n\n  kj::ArrayPtr<const byte> asRawBytes() const;\n\n  template <typename T>\n  KJ_ALWAYS_INLINE(T getDataElement(ElementCount index) const);\n  // Get the element of the given type at the given index.\n\n  KJ_ALWAYS_INLINE(PointerReader getPointerElement(ElementCount index) const);\n\n  StructReader getStructElement(ElementCount index) const;\n\n  MessageSizeCounts totalSize() const;\n  // Like StructReader::totalSize(). Note that for struct lists, the size includes the list tag.\n\n  CapTableReader* getCapTable();\n  // Gets the capability context in which this object is operating.\n\n  ListReader imbue(CapTableReader* capTable) const;\n  // Return a copy of this reader except using the given capability context.\n\n  bool isCanonical(const word **readHead, const WirePointer* ref);\n  // Validate this pointer's canonicity, subject to the conditions:\n  // * All data to the left of readHead has been read thus far (for pointer\n  //   ordering)\n  // * All pointers in preorder have already been checked\n  // * This pointer is in the first and only segment of the message\n\nprivate:\n  SegmentReader* segment;    // Memory segment in which the list resides.\n  CapTableReader* capTable;  // Table of capability indexes.\n\n  const byte* ptr;  // Pointer to list content.\n\n  ListElementCount elementCount;  // Number of elements in the list.\n\n  BitsPerElementN<23> step;\n  // The distance between elements. The maximum value occurs when a struct contains 2^16-1 data\n  // words and 2^16-1 pointers, i.e. 2^17 - 2 words, or 2^23 - 2 bits.\n\n  StructDataBitCount structDataSize;\n  StructPointerCount structPointerCount;\n  // The struct properties to use when interpreting the elements as structs.  All lists can be\n  // interpreted as struct lists, so these are always filled in.\n\n  ElementSize elementSize;\n  // The element size as a ElementSize. This is only really needed to disambiguate INLINE_COMPOSITE\n  // from other types when the overall size is exactly zero or one words.\n\n  int nestingLimit;\n  // Limits the depth of message structures to guard against stack-overflow-based DoS attacks.\n  // Once this reaches zero, further pointers will be pruned.\n\n  inline ListReader(SegmentReader* segment, CapTableReader* capTable, const void* ptr,\n                    ListElementCount elementCount, BitsPerElementN<23> step,\n                    StructDataBitCount structDataSize, StructPointerCount structPointerCount,\n                    ElementSize elementSize, int nestingLimit)\n      : segment(segment), capTable(capTable), ptr(reinterpret_cast<const byte*>(ptr)),\n        elementCount(elementCount), step(step), structDataSize(structDataSize),\n        structPointerCount(structPointerCount), elementSize(elementSize),\n        nestingLimit(nestingLimit) {}\n\n  friend class StructReader;\n  friend class ListBuilder;\n  friend struct WireHelpers;\n  friend class OrphanBuilder;\n};\n\n// -------------------------------------------------------------------\n\nclass OrphanBuilder {\npublic:\n  inline OrphanBuilder(): segment(nullptr), capTable(nullptr), location(nullptr) {\n    memset(&tag, 0, sizeof(tag));\n  }\n  OrphanBuilder(const OrphanBuilder& other) = delete;\n  inline OrphanBuilder(OrphanBuilder&& other) noexcept;\n  inline ~OrphanBuilder() noexcept(false);\n\n  static OrphanBuilder initStruct(BuilderArena* arena, CapTableBuilder* capTable, StructSize size);\n  static OrphanBuilder initList(BuilderArena* arena, CapTableBuilder* capTable,\n                                ElementCount elementCount, ElementSize elementSize);\n  static OrphanBuilder initStructList(BuilderArena* arena, CapTableBuilder* capTable,\n                                      ElementCount elementCount, StructSize elementSize);\n  static OrphanBuilder initText(BuilderArena* arena, CapTableBuilder* capTable, ByteCount size);\n  static OrphanBuilder initData(BuilderArena* arena, CapTableBuilder* capTable, ByteCount size);\n\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, StructReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, ListReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, PointerReader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, Text::Reader copyFrom);\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable, Data::Reader copyFrom);\n#if !CAPNP_LITE\n  static OrphanBuilder copy(BuilderArena* arena, CapTableBuilder* capTable,\n                            kj::Own<ClientHook> copyFrom);\n#endif  // !CAPNP_LITE\n\n  static OrphanBuilder concat(BuilderArena* arena, CapTableBuilder* capTable,\n                              ElementSize expectedElementSize, StructSize expectedStructSize,\n                              kj::ArrayPtr<const ListReader> lists);\n\n  static OrphanBuilder referenceExternalData(BuilderArena* arena, Data::Reader data);\n\n  OrphanBuilder& operator=(const OrphanBuilder& other) = delete;\n  inline OrphanBuilder& operator=(OrphanBuilder&& other);\n\n  inline bool operator==(decltype(nullptr)) const { return location == nullptr; }\n  inline bool operator!=(decltype(nullptr)) const { return location != nullptr; }\n\n  StructBuilder asStruct(StructSize size);\n  // Interpret as a struct, or throw an exception if not a struct.\n\n  ListBuilder asList(ElementSize elementSize);\n  // Interpret as a list, or throw an exception if not a list.  elementSize cannot be\n  // INLINE_COMPOSITE -- use asStructList() instead.\n\n  ListBuilder asStructList(StructSize elementSize);\n  // Interpret as a struct list, or throw an exception if not a list.\n\n  ListBuilder asListAnySize();\n  // For AnyList.\n\n  Text::Builder asText();\n  Data::Builder asData();\n  // Interpret as a blob, or throw an exception if not a blob.\n\n  StructReader asStructReader(StructSize size) const;\n  ListReader asListReader(ElementSize elementSize) const;\n  ListReader asListReaderAnySize() const;\n#if !CAPNP_LITE\n  kj::Own<ClientHook> asCapability() const;\n#endif  // !CAPNP_LITE\n  Text::Reader asTextReader() const;\n  Data::Reader asDataReader() const;\n\n  bool truncate(ElementCount size, bool isText) KJ_WARN_UNUSED_RESULT;\n  // Resize the orphan list to the given size. Returns false if the list is currently empty but\n  // the requested size is non-zero, in which case the caller will need to allocate a new list.\n\n  void truncate(ElementCount size, ElementSize elementSize);\n  void truncate(ElementCount size, StructSize elementSize);\n  void truncateText(ElementCount size);\n  // Versions of truncate() that know how to allocate a new list if needed.\n\nprivate:\n  static_assert(ONE * POINTERS * WORDS_PER_POINTER == ONE * WORDS,\n                \"This struct assumes a pointer is one word.\");\n  word tag;\n  // Contains an encoded WirePointer representing this object.  WirePointer is defined in\n  // layout.c++, but fits in a word.\n  //\n  // This may be a FAR pointer.  Even in that case, `location` points to the eventual destination\n  // of that far pointer.  The reason we keep the far pointer around rather than just making `tag`\n  // represent the final destination is because if the eventual adopter of the pointer is not in\n  // the target's segment then it may be useful to reuse the far pointer landing pad.\n  //\n  // If `tag` is not a far pointer, its offset is garbage; only `location` points to the actual\n  // target.\n\n  SegmentBuilder* segment;\n  // Segment in which the object resides.\n\n  CapTableBuilder* capTable;\n  // Table of capability indexes.\n\n  word* location;\n  // Pointer to the object, or nullptr if the pointer is null.  For capabilities, we make this\n  // 0x1 just so that it is non-null for operator==, but it is never used.\n\n  inline OrphanBuilder(const void* tagPtr, SegmentBuilder* segment,\n                       CapTableBuilder* capTable, word* location)\n      : segment(segment), capTable(capTable), location(location) {\n    memcpy(&tag, tagPtr, sizeof(tag));\n  }\n\n  inline WirePointer* tagAsPtr() { return reinterpret_cast<WirePointer*>(&tag); }\n  inline const WirePointer* tagAsPtr() const { return reinterpret_cast<const WirePointer*>(&tag); }\n\n  void euthanize();\n  // Erase the target object, zeroing it out and possibly reclaiming the memory.  Called when\n  // the OrphanBuilder is being destroyed or overwritten and it is non-null.\n\n  friend struct WireHelpers;\n};\n\n// =======================================================================================\n// Internal implementation details...\n\n// These are defined in the source file.\ntemplate <> typename Text::Builder PointerBuilder::initBlob<Text>(ByteCount size);\ntemplate <> void PointerBuilder::setBlob<Text>(typename Text::Reader value);\ntemplate <> typename Text::Builder PointerBuilder::getBlob<Text>(\n    const void* defaultValue, ByteCount defaultSize);\ntemplate <> typename Text::Reader PointerReader::getBlob<Text>(\n    const void* defaultValue, ByteCount defaultSize) const;\n\ntemplate <> typename Data::Builder PointerBuilder::initBlob<Data>(ByteCount size);\ntemplate <> void PointerBuilder::setBlob<Data>(typename Data::Reader value);\ntemplate <> typename Data::Builder PointerBuilder::getBlob<Data>(\n    const void* defaultValue, ByteCount defaultSize);\ntemplate <> typename Data::Reader PointerReader::getBlob<Data>(\n    const void* defaultValue, ByteCount defaultSize) const;\n\ninline PointerBuilder PointerBuilder::getRoot(\n    SegmentBuilder* segment, CapTableBuilder* capTable, word* location) {\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(location));\n}\n\ninline PointerReader PointerReader::getRootUnchecked(const word* location) {\n  return PointerReader(nullptr, nullptr,\n                       reinterpret_cast<const WirePointer*>(location), 0x7fffffff);\n}\n\n// -------------------------------------------------------------------\n\ninline kj::ArrayPtr<byte> StructBuilder::getDataSectionAsBlob() {\n  return kj::ArrayPtr<byte>(reinterpret_cast<byte*>(data),\n      unbound(dataSize / BITS_PER_BYTE / BYTES));\n}\n\ninline _::ListBuilder StructBuilder::getPointerSectionAsList() {\n  return _::ListBuilder(segment, capTable, pointers, ONE * POINTERS * BITS_PER_POINTER / ELEMENTS,\n                        pointerCount * (ONE * ELEMENTS / POINTERS),\n                        ZERO * BITS, ONE * POINTERS, ElementSize::POINTER);\n}\n\ntemplate <typename T>\ninline bool StructBuilder::hasDataField(StructDataOffset offset) {\n  return getDataField<Mask<T>>(offset) != 0;\n}\n\ntemplate <>\ninline bool StructBuilder::hasDataField<Void>(StructDataOffset offset) {\n  return false;\n}\n\ntemplate <typename T>\ninline T StructBuilder::getDataField(StructDataOffset offset) {\n  return reinterpret_cast<WireValue<T>*>(data)[unbound(offset / ELEMENTS)].get();\n}\n\ntemplate <>\ninline bool StructBuilder::getDataField<bool>(StructDataOffset offset) {\n  BitCount32 boffset = offset * (ONE * BITS / ELEMENTS);\n  byte* b = reinterpret_cast<byte*>(data) + boffset / BITS_PER_BYTE;\n  return (*reinterpret_cast<uint8_t*>(b) &\n      unbound(ONE << (boffset % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void StructBuilder::getDataField<Void>(StructDataOffset offset) {\n  return VOID;\n}\n\ntemplate <typename T>\ninline T StructBuilder::getDataField(StructDataOffset offset, Mask<T> mask) {\n  return unmask<T>(getDataField<Mask<T> >(offset), mask);\n}\n\ntemplate <typename T>\ninline void StructBuilder::setDataField(StructDataOffset offset, kj::NoInfer<T> value) {\n  reinterpret_cast<WireValue<T>*>(data)[unbound(offset / ELEMENTS)].set(value);\n}\n\n#if CAPNP_CANONICALIZE_NAN\n// Use mask() on floats and doubles to make sure we canonicalize NaNs.\ntemplate <>\ninline void StructBuilder::setDataField<float>(StructDataOffset offset, float value) {\n  setDataField<uint32_t>(offset, mask<float>(value, 0));\n}\ntemplate <>\ninline void StructBuilder::setDataField<double>(StructDataOffset offset, double value) {\n  setDataField<uint64_t>(offset, mask<double>(value, 0));\n}\n#endif\n\ntemplate <>\ninline void StructBuilder::setDataField<bool>(StructDataOffset offset, bool value) {\n  auto boffset = offset * (ONE * BITS / ELEMENTS);\n  byte* b = reinterpret_cast<byte*>(data) + boffset / BITS_PER_BYTE;\n  uint bitnum = unboundMaxBits<3>(boffset % BITS_PER_BYTE / BITS);\n  *reinterpret_cast<uint8_t*>(b) = (*reinterpret_cast<uint8_t*>(b) & ~(1 << bitnum))\n                                 | (static_cast<uint8_t>(value) << bitnum);\n}\n\ntemplate <>\ninline void StructBuilder::setDataField<Void>(StructDataOffset offset, Void value) {}\n\ntemplate <typename T>\ninline void StructBuilder::setDataField(StructDataOffset offset,\n                                        kj::NoInfer<T> value, Mask<T> m) {\n  setDataField<Mask<T> >(offset, mask<T>(value, m));\n}\n\ninline PointerBuilder StructBuilder::getPointerField(StructPointerOffset ptrIndex) {\n  // Hacky because WirePointer is defined in the .c++ file (so is incomplete here).\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(\n      reinterpret_cast<word*>(pointers) + ptrIndex * WORDS_PER_POINTER));\n}\n\n// -------------------------------------------------------------------\n\ninline kj::ArrayPtr<const byte> StructReader::getDataSectionAsBlob() const {\n  return kj::ArrayPtr<const byte>(reinterpret_cast<const byte*>(data),\n      unbound(dataSize / BITS_PER_BYTE / BYTES));\n}\n\ninline _::ListReader StructReader::getPointerSectionAsList() const {\n  return _::ListReader(segment, capTable, pointers, pointerCount * (ONE * ELEMENTS / POINTERS),\n                       ONE * POINTERS * BITS_PER_POINTER / ELEMENTS, ZERO * BITS, ONE * POINTERS,\n                       ElementSize::POINTER, nestingLimit);\n}\n\ntemplate <typename T>\ninline bool StructReader::hasDataField(StructDataOffset offset) const {\n  return getDataField<Mask<T>>(offset) != 0;\n}\n\ntemplate <>\ninline bool StructReader::hasDataField<Void>(StructDataOffset offset) const {\n  return false;\n}\n\ntemplate <typename T>\ninline T StructReader::getDataField(StructDataOffset offset) const {\n  if ((offset + ONE * ELEMENTS) * capnp::bitsPerElement<T>() <= dataSize) {\n    return reinterpret_cast<const WireValue<T>*>(data)[unbound(offset / ELEMENTS)].get();\n  } else {\n    return static_cast<T>(0);\n  }\n}\n\ntemplate <>\ninline bool StructReader::getDataField<bool>(StructDataOffset offset) const {\n  auto boffset = offset * (ONE * BITS / ELEMENTS);\n  if (boffset < dataSize) {\n    const byte* b = reinterpret_cast<const byte*>(data) + boffset / BITS_PER_BYTE;\n    return (*reinterpret_cast<const uint8_t*>(b) &\n        unbound(ONE << (boffset % BITS_PER_BYTE / BITS))) != 0;\n  } else {\n    return false;\n  }\n}\n\ntemplate <>\ninline Void StructReader::getDataField<Void>(StructDataOffset offset) const {\n  return VOID;\n}\n\ntemplate <typename T>\nT StructReader::getDataField(StructDataOffset offset, Mask<T> mask) const {\n  return unmask<T>(getDataField<Mask<T> >(offset), mask);\n}\n\ninline PointerReader StructReader::getPointerField(StructPointerOffset ptrIndex) const {\n  if (ptrIndex < pointerCount) {\n    // Hacky because WirePointer is defined in the .c++ file (so is incomplete here).\n    return PointerReader(segment, capTable, reinterpret_cast<const WirePointer*>(\n        reinterpret_cast<const word*>(pointers) + ptrIndex * WORDS_PER_POINTER), nestingLimit);\n  } else{\n    return PointerReader();\n  }\n}\n\n// -------------------------------------------------------------------\n\ninline ListElementCount ListBuilder::size() const { return elementCount; }\n\ntemplate <typename T>\ninline T ListBuilder::getDataElement(ElementCount index) {\n  return reinterpret_cast<WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->get();\n\n  // TODO(perf):  Benchmark this alternate implementation, which I suspect may make better use of\n  //   the x86 SIB byte.  Also use it for all the other getData/setData implementations below, and\n  //   the various non-inline methods that look up pointers.\n  //   Also if using this, consider changing ptr back to void* instead of byte*.\n//  return reinterpret_cast<WireValue<T>*>(ptr)[\n//      index / ELEMENTS * (step / capnp::bitsPerElement<T>())].get();\n}\n\ntemplate <>\ninline bool ListBuilder::getDataElement<bool>(ElementCount index) {\n  // Ignore step for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  byte* b = ptr + bindex / BITS_PER_BYTE;\n  return (*reinterpret_cast<uint8_t*>(b) &\n      unbound(ONE << (bindex % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void ListBuilder::getDataElement<Void>(ElementCount index) {\n  return VOID;\n}\n\ntemplate <typename T>\ninline void ListBuilder::setDataElement(ElementCount index, kj::NoInfer<T> value) {\n  reinterpret_cast<WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->set(value);\n}\n\n#if CAPNP_CANONICALIZE_NAN\n// Use mask() on floats and doubles to make sure we canonicalize NaNs.\ntemplate <>\ninline void ListBuilder::setDataElement<float>(ElementCount index, float value) {\n  setDataElement<uint32_t>(index, mask<float>(value, 0));\n}\ntemplate <>\ninline void ListBuilder::setDataElement<double>(ElementCount index, double value) {\n  setDataElement<uint64_t>(index, mask<double>(value, 0));\n}\n#endif\n\ntemplate <>\ninline void ListBuilder::setDataElement<bool>(ElementCount index, bool value) {\n  // Ignore stepBytes for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  byte* b = ptr + bindex / BITS_PER_BYTE;\n  auto bitnum = bindex % BITS_PER_BYTE / BITS;\n  *reinterpret_cast<uint8_t*>(b) = (*reinterpret_cast<uint8_t*>(b) & ~(1 << unbound(bitnum)))\n                                 | (static_cast<uint8_t>(value) << unbound(bitnum));\n}\n\ntemplate <>\ninline void ListBuilder::setDataElement<Void>(ElementCount index, Void value) {}\n\ninline PointerBuilder ListBuilder::getPointerElement(ElementCount index) {\n  return PointerBuilder(segment, capTable, reinterpret_cast<WirePointer*>(ptr +\n      upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE));\n}\n\n// -------------------------------------------------------------------\n\ninline ListElementCount ListReader::size() const { return elementCount; }\n\ntemplate <typename T>\ninline T ListReader::getDataElement(ElementCount index) const {\n  return reinterpret_cast<const WireValue<T>*>(\n      ptr + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE)->get();\n}\n\ntemplate <>\ninline bool ListReader::getDataElement<bool>(ElementCount index) const {\n  // Ignore step for bit lists because bit lists cannot be upgraded to struct lists.\n  auto bindex = index * (ONE * BITS / ELEMENTS);\n  const byte* b = ptr + bindex / BITS_PER_BYTE;\n  return (*reinterpret_cast<const uint8_t*>(b) &\n      unbound(ONE << (bindex % BITS_PER_BYTE / BITS))) != 0;\n}\n\ntemplate <>\ninline Void ListReader::getDataElement<Void>(ElementCount index) const {\n  return VOID;\n}\n\ninline PointerReader ListReader::getPointerElement(ElementCount index) const {\n  // If the list elements have data sections we need to skip those. Note that for pointers to be\n  // present at all (which already must be true if we get here), then `structDataSize` must be a\n  // whole number of words, so we don't have to worry about unaligned reads here.\n  auto offset = structDataSize / BITS_PER_BYTE;\n  return PointerReader(segment, capTable, reinterpret_cast<const WirePointer*>(\n      ptr + offset + upgradeBound<uint64_t>(index) * step / BITS_PER_BYTE), nestingLimit);\n}\n\n// -------------------------------------------------------------------\n\ninline OrphanBuilder::OrphanBuilder(OrphanBuilder&& other) noexcept\n    : segment(other.segment), capTable(other.capTable), location(other.location) {\n  memcpy(&tag, &other.tag, sizeof(tag));  // Needs memcpy to comply with aliasing rules.\n  other.segment = nullptr;\n  other.location = nullptr;\n}\n\ninline OrphanBuilder::~OrphanBuilder() noexcept(false) {\n  if (segment != nullptr) euthanize();\n}\n\ninline OrphanBuilder& OrphanBuilder::operator=(OrphanBuilder&& other) {\n  // With normal smart pointers, it's important to handle the case where the incoming pointer\n  // is actually transitively owned by this one.  In this case, euthanize() would destroy `other`\n  // before we copied it.  This isn't possible in the case of `OrphanBuilder` because it only\n  // owns message objects, and `other` is not itself a message object, therefore cannot possibly\n  // be transitively owned by `this`.\n\n  if (segment != nullptr) euthanize();\n  segment = other.segment;\n  capTable = other.capTable;\n  location = other.location;\n  memcpy(&tag, &other.tag, sizeof(tag));  // Needs memcpy to comply with aliasing rules.\n  other.segment = nullptr;\n  other.location = nullptr;\n  return *this;\n}\n\n}  // namespace _ (private)\n}  // namespace capnp\n\nCAPNP_END_HEADER\n"], "filenames": ["c++/src/capnp/layout.c++", "c++/src/capnp/layout.h"], "buggy_code_start_loc": [2363, 1229], "buggy_code_end_loc": [2367, 1232], "fixing_code_start_loc": [2362, 1230], "fixing_code_end_loc": [2362, 1236], "type": "CWE-125", "message": "Cap'n Proto is a data interchange format and remote procedure call (RPC) system. Cap'n Proro prior to versions 0.7.1, 0.8.1, 0.9.2, and 0.10.3, as well as versions of Cap'n Proto's Rust implementation prior to 0.13.7, 0.14.11, and 0.15.2 are vulnerable to out-of-bounds read due to logic error handling list-of-list. This issue may lead someone to remotely segfault a peer by sending it a malicious message, if the victim performs certain actions on a list-of-pointer type. Exfiltration of memory is possible if the victim performs additional certain actions on a list-of-pointer type. To be vulnerable, an application must perform a specific sequence of actions, described in the GitHub Security Advisory. The bug is present in inlined code, therefore the fix will require rebuilding dependent applications. Cap'n Proto has C++ fixes available in versions 0.7.1, 0.8.1, 0.9.2, and 0.10.3. The `capnp` Rust crate has fixes available in versions 0.13.7, 0.14.11, and 0.15.2.", "other": {"cve": {"id": "CVE-2022-46149", "sourceIdentifier": "security-advisories@github.com", "published": "2022-11-30T17:15:10.097", "lastModified": "2023-02-10T18:49:36.467", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Cap'n Proto is a data interchange format and remote procedure call (RPC) system. Cap'n Proro prior to versions 0.7.1, 0.8.1, 0.9.2, and 0.10.3, as well as versions of Cap'n Proto's Rust implementation prior to 0.13.7, 0.14.11, and 0.15.2 are vulnerable to out-of-bounds read due to logic error handling list-of-list. This issue may lead someone to remotely segfault a peer by sending it a malicious message, if the victim performs certain actions on a list-of-pointer type. Exfiltration of memory is possible if the victim performs additional certain actions on a list-of-pointer type. To be vulnerable, an application must perform a specific sequence of actions, described in the GitHub Security Advisory. The bug is present in inlined code, therefore the fix will require rebuilding dependent applications. Cap'n Proto has C++ fixes available in versions 0.7.1, 0.8.1, 0.9.2, and 0.10.3. The `capnp` Rust crate has fixes available in versions 0.13.7, 0.14.11, and 0.15.2."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 2.5}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:C/C:L/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "LOW", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 5.4, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 2.7}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnproto:*:*:*:*:*:*:*:*", "versionEndExcluding": "0.7.1", "matchCriteriaId": "D329C641-F7C9-4A6F-8398-BC0A36227B7A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnproto:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.9.0", "versionEndExcluding": "0.9.2", "matchCriteriaId": "F23D31E1-3EC5-4BA9-BD74-D27A6FA809DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnproto:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.10.0", "versionEndExcluding": "0.10.3", "matchCriteriaId": "99639291-62E7-4D6E-97E4-C673B77A4FA9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnproto:0.8.0:*:*:*:*:*:*:*", "matchCriteriaId": "91549D17-7798-4A38-B7B9-E6A0417063E9"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnp:*:*:*:*:*:rust:*:*", "versionEndExcluding": "0.13.7", "matchCriteriaId": "893E526B-29B6-4FEE-A447-EDDF585FEB52"}, {"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnp:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.14.0", "versionEndExcluding": "0.14.11", "matchCriteriaId": "AA32CD2C-881D-4D86-BF91-B95E33A88B34"}, {"vulnerable": true, "criteria": "cpe:2.3:a:capnproto:capnp:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.15.0", "versionEndExcluding": "0.15.2", "matchCriteriaId": "1A3D361A-8BCB-4E0F-A4E3-53F989AAC9FC"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:36:*:*:*:*:*:*:*", "matchCriteriaId": "5C675112-476C-4D7C-BCB9-A2FB2D0BC9FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:37:*:*:*:*:*:*:*", "matchCriteriaId": "E30D0E6F-4AE8-4284-8716-991DFA48CC5D"}]}]}], "references": [{"url": "https://github.com/capnproto/capnproto/commit/25d34c67863fd960af34fc4f82a7ca3362ee74b9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/capnproto/capnproto/security/advisories/GHSA-qqff-4vw4-f6hx", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/EAHKLUMJAXJEV5BPBS5XXWBQ3ZTHGOLY/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/PTS6TWD6K2NKXLEEFBPROQXMOFUTEYWY/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/WKXM4JAFXLTXU5IQB3OUBQVCIICZWGYX/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/ZOCQQOPMVQOFUWBWAGVGN76OYAV3WXY4/", "source": "security-advisories@github.com", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/capnproto/capnproto/commit/25d34c67863fd960af34fc4f82a7ca3362ee74b9"}}