{"buggy_code": ["#ifndef _LINUX_HUGE_MM_H\n#define _LINUX_HUGE_MM_H\n\nextern int do_huge_pmd_anonymous_page(struct mm_struct *mm,\n\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address, pmd_t *pmd,\n\t\t\t\t      unsigned int flags);\nextern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t\t pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t\t\t struct vm_area_struct *vma);\nextern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t       unsigned long address, pmd_t *pmd,\n\t\t\t       pmd_t orig_pmd);\nextern pgtable_t get_pmd_huge_pte(struct mm_struct *mm);\nextern struct page *follow_trans_huge_pmd(struct mm_struct *mm,\n\t\t\t\t\t  unsigned long addr,\n\t\t\t\t\t  pmd_t *pmd,\n\t\t\t\t\t  unsigned int flags);\nextern int zap_huge_pmd(struct mmu_gather *tlb,\n\t\t\tstruct vm_area_struct *vma,\n\t\t\tpmd_t *pmd);\nextern int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned char *vec);\nextern int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\t\tunsigned long addr, pgprot_t newprot);\n\nenum transparent_hugepage_flag {\n\tTRANSPARENT_HUGEPAGE_FLAG,\n\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,\n#ifdef CONFIG_DEBUG_VM\n\tTRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG,\n#endif\n};\n\nenum page_check_address_pmd_flag {\n\tPAGE_CHECK_ADDRESS_PMD_FLAG,\n\tPAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG,\n\tPAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG,\n};\nextern pmd_t *page_check_address_pmd(struct page *page,\n\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t     unsigned long address,\n\t\t\t\t     enum page_check_address_pmd_flag flag);\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define HPAGE_PMD_SHIFT HPAGE_SHIFT\n#define HPAGE_PMD_MASK HPAGE_MASK\n#define HPAGE_PMD_SIZE HPAGE_SIZE\n\n#define transparent_hugepage_enabled(__vma)\t\t\t\t\\\n\t((transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_FLAG) ||\t\t\t\t\\\n\t  (transparent_hugepage_flags &\t\t\t\t\t\\\n\t   (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG) &&\t\t\t\\\n\t   ((__vma)->vm_flags & VM_HUGEPAGE))) &&\t\t\t\\\n\t !((__vma)->vm_flags & VM_NOHUGEPAGE) &&\t\t\t\\\n\t !is_vma_temporary_stack(__vma))\n#define transparent_hugepage_defrag(__vma)\t\t\t\t\\\n\t((transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)) ||\t\t\t\\\n\t (transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG) &&\t\t\\\n\t  (__vma)->vm_flags & VM_HUGEPAGE))\n#ifdef CONFIG_DEBUG_VM\n#define transparent_hugepage_debug_cow()\t\t\t\t\\\n\t(transparent_hugepage_flags &\t\t\t\t\t\\\n\t (1<<TRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG))\n#else /* CONFIG_DEBUG_VM */\n#define transparent_hugepage_debug_cow() 0\n#endif /* CONFIG_DEBUG_VM */\n\nextern unsigned long transparent_hugepage_flags;\nextern int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t\t  pmd_t *dst_pmd, pmd_t *src_pmd,\n\t\t\t  struct vm_area_struct *vma,\n\t\t\t  unsigned long addr, unsigned long end);\nextern int handle_pte_fault(struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pte_t *pte, pmd_t *pmd, unsigned int flags);\nextern int split_huge_page(struct page *page);\nextern void __split_huge_page_pmd(struct mm_struct *mm, pmd_t *pmd);\n#define split_huge_page_pmd(__mm, __pmd)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpmd_t *____pmd = (__pmd);\t\t\t\t\\\n\t\tif (unlikely(pmd_trans_huge(*____pmd)))\t\t\t\\\n\t\t\t__split_huge_page_pmd(__mm, ____pmd);\t\t\\\n\t}  while (0)\n#define wait_split_huge_page(__anon_vma, __pmd)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpmd_t *____pmd = (__pmd);\t\t\t\t\\\n\t\tspin_unlock_wait(&(__anon_vma)->root->lock);\t\t\\\n\t\t/*\t\t\t\t\t\t\t\\\n\t\t * spin_unlock_wait() is just a loop in C and so the\t\\\n\t\t * CPU can reorder anything around it.\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\\\n\t\tsmp_mb();\t\t\t\t\t\t\\\n\t\tBUG_ON(pmd_trans_splitting(*____pmd) ||\t\t\t\\\n\t\t       pmd_trans_huge(*____pmd));\t\t\t\\\n\t} while (0)\n#define HPAGE_PMD_ORDER (HPAGE_PMD_SHIFT-PAGE_SHIFT)\n#define HPAGE_PMD_NR (1<<HPAGE_PMD_ORDER)\n#if HPAGE_PMD_ORDER > MAX_ORDER\n#error \"hugepages can't be allocated by the buddy allocator\"\n#endif\nextern int hugepage_madvise(struct vm_area_struct *vma,\n\t\t\t    unsigned long *vm_flags, int advice);\nextern void __vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end,\n\t\t\t\t    long adjust_next);\nstatic inline void vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long end,\n\t\t\t\t\t long adjust_next)\n{\n\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\treturn;\n\t__vma_adjust_trans_huge(vma, start, end, adjust_next);\n}\nstatic inline int hpage_nr_pages(struct page *page)\n{\n\tif (unlikely(PageTransHuge(page)))\n\t\treturn HPAGE_PMD_NR;\n\treturn 1;\n}\nstatic inline struct page *compound_trans_head(struct page *page)\n{\n\tif (PageTail(page)) {\n\t\tstruct page *head;\n\t\thead = page->first_page;\n\t\tsmp_rmb();\n\t\t/*\n\t\t * head may be a dangling pointer.\n\t\t * __split_huge_page_refcount clears PageTail before\n\t\t * overwriting first_page, so if PageTail is still\n\t\t * there it means the head pointer isn't dangling.\n\t\t */\n\t\tif (PageTail(page))\n\t\t\treturn head;\n\t}\n\treturn page;\n}\n#else /* CONFIG_TRANSPARENT_HUGEPAGE */\n#define HPAGE_PMD_SHIFT ({ BUG(); 0; })\n#define HPAGE_PMD_MASK ({ BUG(); 0; })\n#define HPAGE_PMD_SIZE ({ BUG(); 0; })\n\n#define hpage_nr_pages(x) 1\n\n#define transparent_hugepage_enabled(__vma) 0\n\n#define transparent_hugepage_flags 0UL\nstatic inline int split_huge_page(struct page *page)\n{\n\treturn 0;\n}\n#define split_huge_page_pmd(__mm, __pmd)\t\\\n\tdo { } while (0)\n#define wait_split_huge_page(__anon_vma, __pmd)\t\\\n\tdo { } while (0)\n#define compound_trans_head(page) compound_head(page)\nstatic inline int hugepage_madvise(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long *vm_flags, int advice)\n{\n\tBUG();\n\treturn 0;\n}\nstatic inline void vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long end,\n\t\t\t\t\t long adjust_next)\n{\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n#endif /* _LINUX_HUGE_MM_H */\n", "#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n\n#ifdef __KERNEL__\n\n#include <linux/gfp.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/prio_tree.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/bit_spinlock.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct file_ra_state;\nstruct user_struct;\nstruct writeback_control;\n\n#ifndef CONFIG_DISCONTIGMEM          /* Don't use mapnrs, do it properly */\nextern unsigned long max_mapnr;\n#endif\n\nextern unsigned long num_physpages;\nextern unsigned long totalram_pages;\nextern void * high_memory;\nextern int page_cluster;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/processor.h>\n\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n\n/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n/*\n * Linux kernel virtual memory manager primitives.\n * The idea being to have a \"virtual\" mm in the same way\n * we have a virtual fs - giving a cleaner interface to the\n * mm details, and allowing different kinds of memory mappings\n * (from shared memory to executable loading to arbitrary\n * mmap() functions).\n */\n\nextern struct kmem_cache *vm_area_cachep;\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n/*\n * vm_flags in vm_area_struct, see mm_types.h.\n */\n#define VM_READ\t\t0x00000001\t/* currently active flags */\n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\n#define VM_MAYREAD\t0x00000010\t/* limits for mprotect() etc */\n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t/* general info on the segment */\n#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)\n#define VM_GROWSUP\t0x00000200\n#else\n#define VM_GROWSUP\t0x00000000\n#define VM_NOHUGEPAGE\t0x00000200\t/* MADV_NOHUGEPAGE marked this vma */\n#endif\n#define VM_PFNMAP\t0x00000400\t/* Page-ranges managed without \"struct page\", just pure PFN */\n#define VM_DENYWRITE\t0x00000800\t/* ETXTBSY on write attempts.. */\n\n#define VM_EXECUTABLE\t0x00001000\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t/* Memory mapped I/O or similar */\n\n\t\t\t\t\t/* Used by sys_madvise() */\n#define VM_SEQ_READ\t0x00008000\t/* App will access data sequentially */\n#define VM_RAND_READ\t0x00010000\t/* App will not benefit from clustered reads */\n\n#define VM_DONTCOPY\t0x00020000      /* Do not copy this vma on fork */\n#define VM_DONTEXPAND\t0x00040000\t/* Cannot expand with mremap() */\n#define VM_RESERVED\t0x00080000\t/* Count as reserved_vm like IO */\n#define VM_ACCOUNT\t0x00100000\t/* Is a VM accounted object */\n#define VM_NORESERVE\t0x00200000\t/* should the VM suppress accounting */\n#define VM_HUGETLB\t0x00400000\t/* Huge TLB Page VM */\n#define VM_NONLINEAR\t0x00800000\t/* Is non-linear (remap_file_pages) */\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\n#define VM_MAPPED_COPY\t0x01000000\t/* T if mapped copy of data (nommu mmap) */\n#else\n#define VM_HUGEPAGE\t0x01000000\t/* MADV_HUGEPAGE marked this vma */\n#endif\n#define VM_INSERTPAGE\t0x02000000\t/* The vma has had \"vm_insert_page()\" done on it */\n#define VM_ALWAYSDUMP\t0x04000000\t/* Always include in core dumps */\n\n#define VM_CAN_NONLINEAR 0x08000000\t/* Has ->fault & does nonlinear pages */\n#define VM_MIXEDMAP\t0x10000000\t/* Can contain \"struct page\" and pure PFN pages */\n#define VM_SAO\t\t0x20000000\t/* Strong Access Ordering (powerpc) */\n#define VM_PFN_AT_MMAP\t0x40000000\t/* PFNMAP vma that is fully mapped at mmap time */\n#define VM_MERGEABLE\t0x80000000\t/* KSM may merge identical pages */\n\n/* Bits set in the VMA until the stack is in its final location */\n#define VM_STACK_INCOMPLETE_SETUP\t(VM_RAND_READ | VM_SEQ_READ)\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK_FLAGS\t(VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n#else\n#define VM_STACK_FLAGS\t(VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n#endif\n\n#define VM_READHINTMASK\t\t\t(VM_SEQ_READ | VM_RAND_READ)\n#define VM_ClearReadHint(v)\t\t(v)->vm_flags &= ~VM_READHINTMASK\n#define VM_NormalReadHint(v)\t\t(!((v)->vm_flags & VM_READHINTMASK))\n#define VM_SequentialReadHint(v)\t((v)->vm_flags & VM_SEQ_READ)\n#define VM_RandomReadHint(v)\t\t((v)->vm_flags & VM_RAND_READ)\n\n/*\n * special vmas that are non-mergable, non-mlock()able\n */\n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_RESERVED | VM_PFNMAP)\n\n/*\n * mapping from the currently active vm_flags protection bits (the\n * low four bits) to a page protection mask..\n */\nextern pgprot_t protection_map[16];\n\n#define FAULT_FLAG_WRITE\t0x01\t/* Fault was a write access */\n#define FAULT_FLAG_NONLINEAR\t0x02\t/* Fault was via a nonlinear mapping */\n#define FAULT_FLAG_MKWRITE\t0x04\t/* Fault was mkwrite of existing pte */\n#define FAULT_FLAG_ALLOW_RETRY\t0x08\t/* Retry fault if blocking */\n#define FAULT_FLAG_RETRY_NOWAIT\t0x10\t/* Don't drop mmap_sem and wait when retrying */\n\n/*\n * This interface is used by x86 PAT code to identify a pfn mapping that is\n * linear over entire vma. This is to optimize PAT code that deals with\n * marking the physical region with a particular prot. This is not for generic\n * mm use. Note also that this check will not work if the pfn mapping is\n * linear for a vma starting at physical address 0. In which case PAT code\n * falls back to slow path of reserving physical range page by page.\n */\nstatic inline int is_linear_pfn_mapping(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & VM_PFN_AT_MMAP);\n}\n\nstatic inline int is_pfn_mapping(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & VM_PFNMAP);\n}\n\n/*\n * vm_fault is filled by the the pagefault handler and passed to the vma's\n * ->fault function. The vma's ->fault is responsible for returning a bitmask\n * of VM_FAULT_xxx flags that give details about how the fault was handled.\n *\n * pgoff should be used in favour of virtual_address, if possible. If pgoff\n * is used, one may set VM_CAN_NONLINEAR in the vma->vm_flags to get nonlinear\n * mapping support.\n */\nstruct vm_fault {\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tpgoff_t pgoff;\t\t\t/* Logical page offset based on vma */\n\tvoid __user *virtual_address;\t/* Faulting virtual address */\n\n\tstruct page *page;\t\t/* ->fault handlers should return a\n\t\t\t\t\t * page here, unless VM_FAULT_NOPAGE\n\t\t\t\t\t * is set (which is also implied by\n\t\t\t\t\t * VM_FAULT_ERROR).\n\t\t\t\t\t */\n};\n\n/*\n * These are the virtual MM functions - opening of an area, closing and\n * unmapping it (needed to keep files on disk up-to-date etc), pointer\n * to the functions called when a no-page or a wp-page exception occurs. \n */\nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\tvoid (*close)(struct vm_area_struct * area);\n\tint (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* notification that a previously read-only page is about to become\n\t * writable, if an error is returned it will cause a SIGBUS */\n\tint (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* called by access_process_vm when get_user_pages() fails, typically\n\t * for use by special VMAs that can switch between memory and hardware\n\t */\n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n#ifdef CONFIG_NUMA\n\t/*\n\t * set_policy() op must add a reference to any non-NULL @new mempolicy\n\t * to hold the policy upon return.  Caller should pass NULL @new to\n\t * remove a policy and fall back to surrounding context--i.e. do not\n\t * install a MPOL_DEFAULT policy, nor the task or system default\n\t * mempolicy.\n\t */\n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t/*\n\t * get_policy() op must add reference [mpol_get()] to any policy at\n\t * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\n\t * in mm/mempolicy.c will do this automatically.\n\t * get_policy() must NOT add a ref if the policy at (vma,addr) is not\n\t * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.\n\t * If no [shared/vma] mempolicy exists at the addr, get_policy() op\n\t * must return NULL--i.e., do not \"fallback\" to task or system default\n\t * policy.\n\t */\n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n\tint (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,\n\t\tconst nodemask_t *to, unsigned long flags);\n#endif\n};\n\nstruct mmu_gather;\nstruct inode;\n\n#define page_private(page)\t\t((page)->private)\n#define set_page_private(page, v)\t((page)->private = (v))\n\n/*\n * FIXME: take this include out, include page-flags.h in\n * files which need it (119 of them)\n */\n#include <linux/page-flags.h>\n#include <linux/huge_mm.h>\n\n/*\n * Methods to modify the page usage count.\n *\n * What counts for a page usage:\n * - cache mapping   (page->mapping)\n * - private data    (page->private)\n * - page mapped in a task's page tables, each mapping\n *   is counted separately\n *\n * Also, many kernel routines increase the page count before a critical\n * routine so they can be sure the page doesn't go away from under them.\n */\n\n/*\n * Drop a ref, return true if the refcount fell to zero (the page has no users)\n */\nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON(atomic_read(&page->_count) == 0);\n\treturn atomic_dec_and_test(&page->_count);\n}\n\n/*\n * Try to grab a ref unless the page has a refcount of zero, return false if\n * that is the case.\n */\nstatic inline int get_page_unless_zero(struct page *page)\n{\n\treturn atomic_inc_not_zero(&page->_count);\n}\n\nextern int page_is_ram(unsigned long pfn);\n\n/* Support for virtually mapped pages */\nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n/*\n * Determine if an address is within the vmalloc range\n *\n * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\n * is no special casing required.\n */\nstatic inline int is_vmalloc_addr(const void *x)\n{\n#ifdef CONFIG_MMU\n\tunsigned long addr = (unsigned long)x;\n\n\treturn addr >= VMALLOC_START && addr < VMALLOC_END;\n#else\n\treturn 0;\n#endif\n}\n#ifdef CONFIG_MMU\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline void compound_lock(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbit_spin_lock(PG_compound_lock, &page->flags);\n#endif\n}\n\nstatic inline void compound_unlock(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbit_spin_unlock(PG_compound_lock, &page->flags);\n#endif\n}\n\nstatic inline unsigned long compound_lock_irqsave(struct page *page)\n{\n\tunsigned long uninitialized_var(flags);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tlocal_irq_save(flags);\n\tcompound_lock(page);\n#endif\n\treturn flags;\n}\n\nstatic inline void compound_unlock_irqrestore(struct page *page,\n\t\t\t\t\t      unsigned long flags)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tcompound_unlock(page);\n\tlocal_irq_restore(flags);\n#endif\n}\n\nstatic inline struct page *compound_head(struct page *page)\n{\n\tif (unlikely(PageTail(page)))\n\t\treturn page->first_page;\n\treturn page;\n}\n\nstatic inline int page_count(struct page *page)\n{\n\treturn atomic_read(&compound_head(page)->_count);\n}\n\nstatic inline void get_page(struct page *page)\n{\n\t/*\n\t * Getting a normal page or the head of a compound page\n\t * requires to already have an elevated page->_count. Only if\n\t * we're getting a tail page, the elevated page->_count is\n\t * required only in the head page, so for tail pages the\n\t * bugcheck only verifies that the page->_count isn't\n\t * negative.\n\t */\n\tVM_BUG_ON(atomic_read(&page->_count) < !PageTail(page));\n\tatomic_inc(&page->_count);\n\t/*\n\t * Getting a tail page will elevate both the head and tail\n\t * page->_count(s).\n\t */\n\tif (unlikely(PageTail(page))) {\n\t\t/*\n\t\t * This is safe only because\n\t\t * __split_huge_page_refcount can't run under\n\t\t * get_page().\n\t\t */\n\t\tVM_BUG_ON(atomic_read(&page->first_page->_count) <= 0);\n\t\tatomic_inc(&page->first_page->_count);\n\t}\n}\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\treturn compound_head(page);\n}\n\n/*\n * Setup the page count before being freed into the page allocator for\n * the first time (boot or memory hotplug)\n */\nstatic inline void init_page_count(struct page *page)\n{\n\tatomic_set(&page->_count, 1);\n}\n\n/*\n * PageBuddy() indicate that the page is free and in the buddy system\n * (see mm/page_alloc.c).\n *\n * PAGE_BUDDY_MAPCOUNT_VALUE must be <= -2 but better not too close to\n * -2 so that an underflow of the page_mapcount() won't be mistaken\n * for a genuine PAGE_BUDDY_MAPCOUNT_VALUE. -128 can be created very\n * efficiently by most CPU architectures.\n */\n#define PAGE_BUDDY_MAPCOUNT_VALUE (-128)\n\nstatic inline int PageBuddy(struct page *page)\n{\n\treturn atomic_read(&page->_mapcount) == PAGE_BUDDY_MAPCOUNT_VALUE;\n}\n\nstatic inline void __SetPageBuddy(struct page *page)\n{\n\tVM_BUG_ON(atomic_read(&page->_mapcount) != -1);\n\tatomic_set(&page->_mapcount, PAGE_BUDDY_MAPCOUNT_VALUE);\n}\n\nstatic inline void __ClearPageBuddy(struct page *page)\n{\n\tVM_BUG_ON(!PageBuddy(page));\n\tatomic_set(&page->_mapcount, -1);\n}\n\nvoid put_page(struct page *page);\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\nint split_free_page(struct page *page);\n\n/*\n * Compound pages have a destructor function.  Provide a\n * prototype for that function and accessor functions.\n * These are _only_ valid on the head of a PG_compound page.\n */\ntypedef void compound_page_dtor(struct page *);\n\nstatic inline void set_compound_page_dtor(struct page *page,\n\t\t\t\t\t\tcompound_page_dtor *dtor)\n{\n\tpage[1].lru.next = (void *)dtor;\n}\n\nstatic inline compound_page_dtor *get_compound_page_dtor(struct page *page)\n{\n\treturn (compound_page_dtor *)page[1].lru.next;\n}\n\nstatic inline int compound_order(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 0;\n\treturn (unsigned long)page[1].lru.prev;\n}\n\nstatic inline int compound_trans_order(struct page *page)\n{\n\tint order;\n\tunsigned long flags;\n\n\tif (!PageHead(page))\n\t\treturn 0;\n\n\tflags = compound_lock_irqsave(page);\n\torder = compound_order(page);\n\tcompound_unlock_irqrestore(page, flags);\n\treturn order;\n}\n\nstatic inline void set_compound_order(struct page *page, unsigned long order)\n{\n\tpage[1].lru.prev = (void *)order;\n}\n\n#ifdef CONFIG_MMU\n/*\n * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\n * servicing faults for write access.  In the normal case, do always want\n * pte_mkwrite.  But get_user_pages can cause write faults for mappings\n * that do not have writing enabled, when used by access_process_vm.\n */\nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte);\n\treturn pte;\n}\n#endif\n\n/*\n * Multiple processes may \"see\" the same page. E.g. for untouched\n * mappings of /dev/null, all processes see the same page full of\n * zeroes, and text pages of executables and shared libraries have\n * only one copy in memory, at most, normally.\n *\n * For the non-reserved pages, page_count(page) denotes a reference count.\n *   page_count() == 0 means the page is free. page->lru is then used for\n *   freelist management in the buddy allocator.\n *   page_count() > 0  means the page has been allocated.\n *\n * Pages are allocated by the slab allocator in order to provide memory\n * to kmalloc and kmem_cache_alloc. In this case, the management of the\n * page, and the fields in 'struct page' are the responsibility of mm/slab.c\n * unless a particular usage is carefully commented. (the responsibility of\n * freeing the kmalloc memory is the caller's, of course).\n *\n * A page may be used by anyone else who does a __get_free_page().\n * In this case, page_count still tracks the references, and should only\n * be used through the normal accessor functions. The top bits of page->flags\n * and page->virtual store page management information, but all other fields\n * are unused and could be used privately, carefully. The management of this\n * page is the responsibility of the one who allocated it, and those who have\n * subsequently been given references to it.\n *\n * The other pages (we may call them \"pagecache pages\") are completely\n * managed by the Linux memory manager: I/O, buffers, swapping etc.\n * The following discussion applies only to them.\n *\n * A pagecache page contains an opaque `private' member, which belongs to the\n * page's address_space. Usually, this is the address of a circular list of\n * the page's disk buffers. PG_private must be set to tell the VM to call\n * into the filesystem to release these pages.\n *\n * A page may belong to an inode's memory mapping. In this case, page->mapping\n * is the pointer to the inode, and page->index is the file offset of the page,\n * in units of PAGE_CACHE_SIZE.\n *\n * If pagecache pages are not associated with an inode, they are said to be\n * anonymous pages. These may become associated with the swapcache, and in that\n * case PG_swapcache is set, and page->private is an offset into the swapcache.\n *\n * In either case (swapcache or inode backed), the pagecache itself holds one\n * reference to the page. Setting PG_private should also increment the\n * refcount. The each user mapping also has a reference to the page.\n *\n * The pagecache pages are stored in a per-mapping radix tree, which is\n * rooted at mapping->page_tree, and indexed by offset.\n * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\n * lists, we instead now tag pages as dirty/writeback in the radix tree.\n *\n * All pagecache pages may be subject to I/O:\n * - inode pages may need to be read from disk,\n * - inode pages which have been modified and are MAP_SHARED may need\n *   to be written back to the inode on disk,\n * - anonymous pages (including MAP_PRIVATE file mappings) which have been\n *   modified may need to be swapped out to swap space and (later) to be read\n *   back into memory.\n */\n\n/*\n * The zone field is never updated after free_area_init_core()\n * sets it, so none of the operations on it need to be atomic.\n */\n\n\n/*\n * page->flags layout:\n *\n * There are three possibilities for how page->flags get\n * laid out.  The first is for the normal case, without\n * sparsemem.  The second is for sparsemem when there is\n * plenty of space for node and section.  The last is when\n * we have run out of space and have to fall back to an\n * alternate (slower) way of determining the node.\n *\n * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE | ... | FLAGS |\n * classic sparse with space for node:| SECTION | NODE | ZONE | ... | FLAGS |\n * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |\n */\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTIONS_WIDTH\t\tSECTIONS_SHIFT\n#else\n#define SECTIONS_WIDTH\t\t0\n#endif\n\n#define ZONES_WIDTH\t\tZONES_SHIFT\n\n#if SECTIONS_WIDTH+ZONES_WIDTH+NODES_SHIFT <= BITS_PER_LONG - NR_PAGEFLAGS\n#define NODES_WIDTH\t\tNODES_SHIFT\n#else\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n#error \"Vmemmap: No space for nodes field in page flags\"\n#endif\n#define NODES_WIDTH\t\t0\n#endif\n\n/* Page flags: | [SECTION] | [NODE] | ZONE | ... | FLAGS | */\n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n\n/*\n * We are going to use the flags for the page to node mapping if its in\n * there.  This includes the case where there is no node, so it is implicit.\n */\n#if !(NODES_WIDTH > 0 || NODES_SHIFT == 0)\n#define NODE_NOT_IN_PAGE_FLAGS\n#endif\n\n#ifndef PFN_SECTION_SHIFT\n#define PFN_SECTION_SHIFT 0\n#endif\n\n/*\n * Define the bit shifts to access each section.  For non-existent\n * sections we define the shift as 0; that plus a 0 mask ensures\n * the compiler will optimise away reference to them.\n */\n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n\n/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#endif\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(struct page *page)\n{\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\n/*\n * The identification function is only used by the buddy allocator for\n * determining if two pages could be buddies. We are not really\n * identifying a zone since we could be using a the section number\n * id if we have not node id available in page flags.\n * We guarantee only that it will return the same value for two\n * combinable pages in a zone.\n */\nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\nstatic inline int zone_to_nid(struct zone *zone)\n{\n#ifdef CONFIG_NUMA\n\treturn zone->node;\n#else\n\treturn 0;\n#endif\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(struct page *page);\n#else\nstatic inline int page_to_nid(struct page *page)\n{\n\treturn (page->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\nstatic inline struct zone *page_zone(struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\nstatic inline unsigned long page_to_section(struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n\tset_page_section(page, pfn_to_section_nr(pfn));\n}\n\n/*\n * Some inline functions in vmstat.h depend on page_zone()\n */\n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(struct page *page)\n{\n\treturn __va(PFN_PHYS(page_to_pfn(page)));\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) ((page)->virtual)\n#define set_page_address(page, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\t(page)->virtual = (address);\t\t\\\n\t} while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\n/*\n * On an anonymous page mapped into a user virtual memory area,\n * page->mapping points to its anon_vma, not to a struct address_space;\n * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.\n *\n * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,\n * the PAGE_MAPPING_KSM bit may be set along with the PAGE_MAPPING_ANON bit;\n * and then page->mapping points, not to an anon_vma, but to a private\n * structure which KSM associates with that merged page.  See ksm.h.\n *\n * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is currently never used.\n *\n * Please note that, confusingly, \"page_mapping\" refers to the inode\n * address_space which maps the page from disk; whereas \"page_mapped\"\n * refers to user virtual address space into which the page is mapped.\n */\n#define PAGE_MAPPING_ANON\t1\n#define PAGE_MAPPING_KSM\t2\n#define PAGE_MAPPING_FLAGS\t(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM)\n\nextern struct address_space swapper_space;\nstatic inline struct address_space *page_mapping(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\n\tVM_BUG_ON(PageSlab(page));\n\tif (unlikely(PageSwapCache(page)))\n\t\tmapping = &swapper_space;\n\telse if ((unsigned long)mapping & PAGE_MAPPING_ANON)\n\t\tmapping = NULL;\n\treturn mapping;\n}\n\n/* Neutral page->mapping pointer to address_space or anon_vma or other */\nstatic inline void *page_rmapping(struct page *page)\n{\n\treturn (void *)((unsigned long)page->mapping & ~PAGE_MAPPING_FLAGS);\n}\n\nstatic inline int PageAnon(struct page *page)\n{\n\treturn ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;\n}\n\n/*\n * Return the pagecache index of the passed page.  Regular pagecache pages\n * use ->index whereas swapcache pages use ->private\n */\nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn page_private(page);\n\treturn page->index;\n}\n\n/*\n * The atomic page->_mapcount, like _count, starts from -1:\n * so that transitions both from it and to it can be tracked,\n * using atomic_inc_and_test and atomic_add_negative(-1).\n */\nstatic inline void reset_page_mapcount(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\nstatic inline int page_mapcount(struct page *page)\n{\n\treturn atomic_read(&(page)->_mapcount) + 1;\n}\n\n/*\n * Return true if this page is mapped into pagetables.\n */\nstatic inline int page_mapped(struct page *page)\n{\n\treturn atomic_read(&(page)->_mapcount) >= 0;\n}\n\n/*\n * Different kinds of faults, as returned by handle_mm_fault().\n * Used to decide whether a process gets delivered SIGBUS or\n * just gets major/minor fault counters bumped up.\n */\n\n#define VM_FAULT_MINOR\t0 /* For backwards compat. Remove me quickly. */\n\n#define VM_FAULT_OOM\t0x0001\n#define VM_FAULT_SIGBUS\t0x0002\n#define VM_FAULT_MAJOR\t0x0004\n#define VM_FAULT_WRITE\t0x0008\t/* Special case for get_user_pages */\n#define VM_FAULT_HWPOISON 0x0010\t/* Hit poisoned small page */\n#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */\n\n#define VM_FAULT_NOPAGE\t0x0100\t/* ->fault installed the pte, not return page */\n#define VM_FAULT_LOCKED\t0x0200\t/* ->fault locked the returned page */\n#define VM_FAULT_RETRY\t0x0400\t/* ->fault blocked, must retry */\n\n#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */\n\n#define VM_FAULT_ERROR\t(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON | \\\n\t\t\t VM_FAULT_HWPOISON_LARGE)\n\n/* Encode hstate index for a hwpoisoned large page */\n#define VM_FAULT_SET_HINDEX(x) ((x) << 12)\n#define VM_FAULT_GET_HINDEX(x) (((x) >> 12) & 0xf)\n\n/*\n * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\n */\nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n\n/*\n * Flags passed to show_mem() and __show_free_areas() to suppress output in\n * various contexts.\n */\n#define SHOW_MEM_FILTER_NODES\t(0x0001u)\t/* filter disallowed nodes */\n\nextern void show_free_areas(void);\nextern void __show_free_areas(unsigned int flags);\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user);\nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags);\nint shmem_zero_setup(struct vm_area_struct *);\n\n#ifndef CONFIG_MMU\nextern unsigned long shmem_get_unmapped_area(struct file *file,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len,\n\t\t\t\t\t     unsigned long pgoff,\n\t\t\t\t\t     unsigned long flags);\n#endif\n\nextern int can_do_mlock(void);\nextern int user_shm_lock(size_t, struct user_struct *);\nextern void user_shm_unlock(size_t, struct user_struct *);\n\n/*\n * Parameter block passed down to zap_pte_range in exceptional cases.\n */\nstruct zap_details {\n\tstruct vm_area_struct *nonlinear_vma;\t/* Check page->index if set */\n\tstruct address_space *check_mapping;\t/* Check page->mapping if set */\n\tpgoff_t\tfirst_index;\t\t\t/* Lowest page->index to unmap */\n\tpgoff_t last_index;\t\t\t/* Highest page->index to unmap */\n\tspinlock_t *i_mmap_lock;\t\t/* For unmap_mapping_range: */\n\tunsigned long truncate_count;\t\t/* Compare vm_truncate_count */\n};\n\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\tpte_t pte);\n\nint zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size);\nunsigned long zap_page_range(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *);\nunsigned long unmap_vmas(struct mmu_gather **tlb,\n\t\tstruct vm_area_struct *start_vma, unsigned long start_addr,\n\t\tunsigned long end_addr, unsigned long *nr_accounted,\n\t\tstruct zap_details *);\n\n/**\n * mm_walk - callbacks for walk_page_range\n * @pgd_entry: if set, called for each non-empty PGD (top-level) entry\n * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry\n * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry\n *\t       this handler is required to be able to handle\n *\t       pmd_trans_huge() pmds.  They may simply choose to\n *\t       split_huge_page() instead of handling it explicitly.\n * @pte_entry: if set, called for each non-empty PTE (4th-level) entry\n * @pte_hole: if set, called for each hole at all levels\n * @hugetlb_entry: if set, called for each hugetlb entry\n *\n * (see walk_page_range for more details)\n */\nstruct mm_walk {\n\tint (*pgd_entry)(pgd_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pud_entry)(pud_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pmd_entry)(pmd_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pte_entry)(pte_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pte_hole)(unsigned long, unsigned long, struct mm_walk *);\n\tint (*hugetlb_entry)(pte_t *, unsigned long,\n\t\t\t     unsigned long, unsigned long, struct mm_walk *);\n\tstruct mm_struct *mm;\n\tvoid *private;\n};\n\nint walk_page_range(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk);\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint copy_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\tstruct vm_area_struct *vma);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nextern void truncate_pagecache(struct inode *inode, loff_t old, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nextern int vmtruncate(struct inode *inode, loff_t offset);\nextern int vmtruncate_range(struct inode *inode, loff_t offset, loff_t end);\n\nint truncate_inode_page(struct address_space *mapping, struct page *page);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\n\nint invalidate_inode_page(struct page *page);\n\n#ifdef CONFIG_MMU\nextern int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags);\n#else\nstatic inline int handle_mm_fault(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tunsigned int flags)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\n#endif\n\nextern int make_pages_present(unsigned long addr, unsigned long end);\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, int write);\n\nint __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long start, int len, unsigned int foll_flags,\n\t\t     struct page **pages, struct vm_area_struct **vmas,\n\t\t     int *nonblocking);\nint get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\tunsigned long start, int nr_pages, int write, int force,\n\t\t\tstruct page **pages, struct vm_area_struct **vmas);\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages);\nstruct page *get_dump_page(unsigned long addr);\n\nextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\nextern void do_invalidatepage(struct page *page, unsigned long offset);\n\nint __set_page_dirty_nobuffers(struct page *page);\nint __set_page_dirty_no_writeback(struct page *page);\nint redirty_page_for_writepage(struct writeback_control *wbc,\n\t\t\t\tstruct page *page);\nvoid account_page_dirtied(struct page *page, struct address_space *mapping);\nvoid account_page_writeback(struct page *page);\nint set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\nint clear_page_dirty_for_io(struct page *page);\n\n/* Is the vma a continuation of the stack vma above it? */\nstatic inline int vma_stack_continue(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);\n}\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len);\nextern unsigned long do_mremap(unsigned long addr,\n\t\t\t       unsigned long old_len, unsigned long new_len,\n\t\t\t       unsigned long flags, unsigned long new_addr);\nextern int mprotect_fixup(struct vm_area_struct *vma,\n\t\t\t  struct vm_area_struct **pprev, unsigned long start,\n\t\t\t  unsigned long end, unsigned long newflags);\n\n/*\n * doesn't attempt to fault and will return short.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages);\n/*\n * per-process(per-mm_struct) statistics.\n */\n#if defined(SPLIT_RSS_COUNTING)\n/*\n * The mm counters are not protected by its page_table_lock,\n * so must be incremented atomically.\n */\nstatic inline void set_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_set(&mm->rss_stat.count[member], value);\n}\n\nunsigned long get_mm_counter(struct mm_struct *mm, int member);\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_add(value, &mm->rss_stat.count[member]);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_inc(&mm->rss_stat.count[member]);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_dec(&mm->rss_stat.count[member]);\n}\n\n#else  /* !USE_SPLIT_PTLOCKS */\n/*\n * The mm counters are protected by its page_table_lock,\n * so can be incremented directly.\n */\nstatic inline void set_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tmm->rss_stat.count[member] = value;\n}\n\nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\treturn mm->rss_stat.count[member];\n}\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tmm->rss_stat.count[member] += value;\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tmm->rss_stat.count[member]++;\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tmm->rss_stat.count[member]--;\n}\n\n#endif /* !USE_SPLIT_PTLOCKS */\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct task_struct *task, struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct task_struct *task, struct mm_struct *mm)\n{\n}\n#endif\n\n/*\n * A callback you can register to apply pressure to ageable caches.\n *\n * 'shrink' is passed a count 'nr_to_scan' and a 'gfpmask'.  It should\n * look through the least-recently-used 'nr_to_scan' entries and\n * attempt to free them up.  It should return the number of objects\n * which remain in the cache.  If it returns -1, it means it cannot do\n * any scanning at this time (eg. there is a risk of deadlock).\n *\n * The 'gfpmask' refers to the allocation we are currently trying to\n * fulfil.\n *\n * Note that 'shrink' will be passed nr_to_scan == 0 when the VM is\n * querying the cache size, so a fastpath for that case is appropriate.\n */\nstruct shrinker {\n\tint (*shrink)(struct shrinker *, int nr_to_scan, gfp_t gfp_mask);\n\tint seeks;\t/* seeks to recreate an obj */\n\n\t/* These are for internal use */\n\tstruct list_head list;\n\tlong nr;\t/* objs pending delete */\n};\n#define DEFAULT_SEEKS 2 /* A good number if you don't know better. */\nextern void register_shrinker(struct shrinker *);\nextern void unregister_shrinker(struct shrinker *);\n\nint vma_wants_writenotify(struct vm_area_struct *vma);\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_PUD_FOLDED\nstatic inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#ifdef __PAGETABLE_PMD_FOLDED\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n#endif\n\nint __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tpmd_t *pmd, unsigned long address);\nint __pte_alloc_kernel(pmd_t *pmd, unsigned long address);\n\n/*\n * The following ifdef needed to get the 4level-fixup.h header to work.\n * Remove it when 4level-fixup.h has been removed.\n */\n#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?\n\t\tNULL: pud_offset(pgd, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */\n\n#if USE_SPLIT_PTLOCKS\n/*\n * We tuck a spinlock to guard each pagetable page into its struct page,\n * at page->private, with BUILD_BUG_ON to make sure that this will not\n * overflow into the next struct page (as it might with DEBUG_SPINLOCK).\n * When freeing, reset page->mapping so free_pages_check won't complain.\n */\n#define __pte_lockptr(page)\t&((page)->ptl)\n#define pte_lock_init(_page)\tdo {\t\t\t\t\t\\\n\tspin_lock_init(__pte_lockptr(_page));\t\t\t\t\\\n} while (0)\n#define pte_lock_deinit(page)\t((page)->mapping = NULL)\n#define pte_lockptr(mm, pmd)\t({(void)(mm); __pte_lockptr(pmd_page(*(pmd)));})\n#else\t/* !USE_SPLIT_PTLOCKS */\n/*\n * We use mm->page_table_lock to guard all pagetable pages of the mm.\n */\n#define pte_lock_init(page)\tdo {} while (0)\n#define pte_lock_deinit(page)\tdo {} while (0)\n#define pte_lockptr(mm, pmd)\t({(void)(pmd); &(mm)->page_table_lock;})\n#endif /* USE_SPLIT_PTLOCKS */\n\nstatic inline void pgtable_page_ctor(struct page *page)\n{\n\tpte_lock_init(page);\n\tinc_zone_page_state(page, NR_PAGETABLE);\n}\n\nstatic inline void pgtable_page_dtor(struct page *page)\n{\n\tpte_lock_deinit(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n#define pte_offset_map_lock(mm, pmd, address, ptlp)\t\\\n({\t\t\t\t\t\t\t\\\n\tspinlock_t *__ptl = pte_lockptr(mm, pmd);\t\\\n\tpte_t *__pte = pte_offset_map(pmd, address);\t\\\n\t*(ptlp) = __ptl;\t\t\t\t\\\n\tspin_lock(__ptl);\t\t\t\t\\\n\t__pte;\t\t\t\t\t\t\\\n})\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc_map(mm, vma, pmd, address)\t\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, vma,\t\\\n\t\t\t\t\t\t\tpmd, address))?\t\\\n\t NULL: pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, NULL,\t\\\n\t\t\t\t\t\t\tpmd, address))?\t\\\n\t\tNULL: pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd, address))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\nextern void free_area_init(unsigned long * zones_size);\nextern void free_area_init_node(int nid, unsigned long * zones_size,\n\t\tunsigned long zone_start_pfn, unsigned long *zholes_size);\n#ifdef CONFIG_ARCH_POPULATES_NODE_MAP\n/*\n * With CONFIG_ARCH_POPULATES_NODE_MAP set, an architecture may initialise its\n * zones, allocate the backing mem_map and account for memory holes in a more\n * architecture independent manner. This is a substitute for creating the\n * zone_sizes[] and zholes_size[] arrays and passing them to\n * free_area_init_node()\n *\n * An architecture is expected to register range of page frames backed by\n * physical memory with add_active_range() before calling\n * free_area_init_nodes() passing in the PFN each zone ends at. At a basic\n * usage, an architecture is expected to do something like\n *\n * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\n * \t\t\t\t\t\t\t max_highmem_pfn};\n * for_each_valid_physical_page_range()\n * \tadd_active_range(node_id, start_pfn, end_pfn)\n * free_area_init_nodes(max_zone_pfns);\n *\n * If the architecture guarantees that there are no holes in the ranges\n * registered with add_active_range(), free_bootmem_active_regions()\n * will call free_bootmem_node() for each registered physical page range.\n * Similarly sparse_memory_present_with_active_regions() calls\n * memory_present() for each range when SPARSEMEM is enabled.\n *\n * See mm/page_alloc.c for more information on each function exposed by\n * CONFIG_ARCH_POPULATES_NODE_MAP\n */\nextern void free_area_init_nodes(unsigned long *max_zone_pfn);\nextern void add_active_range(unsigned int nid, unsigned long start_pfn,\n\t\t\t\t\tunsigned long end_pfn);\nextern void remove_active_range(unsigned int nid, unsigned long start_pfn,\n\t\t\t\t\tunsigned long end_pfn);\nextern void remove_all_active_ranges(void);\nvoid sort_node_map(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\nextern unsigned long find_min_pfn_with_active_regions(void);\nextern void free_bootmem_with_active_regions(int nid,\n\t\t\t\t\t\tunsigned long max_low_pfn);\nint add_from_early_node_map(struct range *range, int az,\n\t\t\t\t   int nr_range, int nid);\nu64 __init find_memory_core_early(int nid, u64 size, u64 align,\n\t\t\t\t\tu64 goal, u64 limit);\ntypedef int (*work_fn_t)(unsigned long, unsigned long, void *);\nextern void work_with_active_regions(int nid, work_fn_t work_fn, void *data);\nextern void sparse_memory_present_with_active_regions(int nid);\n#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */\n\n#if !defined(CONFIG_ARCH_POPULATES_NODE_MAP) && \\\n    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)\nstatic inline int __early_pfn_to_nid(unsigned long pfn)\n{\n\treturn 0;\n}\n#else\n/* please see mm/page_alloc.c */\nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n#ifdef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n/* there is a per-arch backend function. */\nextern int __meminit __early_pfn_to_nid(unsigned long pfn);\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void memmap_init_zone(unsigned long, int, unsigned long,\n\t\t\t\tunsigned long, enum memmap_context);\nextern void setup_per_zone_wmarks(void);\nextern void calculate_zone_inactive_ratio(struct zone *zone);\nextern void mem_init(void);\nextern void __init mmap_init(void);\nextern void show_mem(unsigned int flags);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\nextern int after_bootmem;\n\nextern void setup_per_cpu_pageset(void);\n\nextern void zone_pcp_update(struct zone *zone);\n\n/* nommu.c */\nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n/* prio_tree.c */\nvoid vma_prio_tree_add(struct vm_area_struct *, struct vm_area_struct *old);\nvoid vma_prio_tree_insert(struct vm_area_struct *, struct prio_tree_root *);\nvoid vma_prio_tree_remove(struct vm_area_struct *, struct prio_tree_root *);\nstruct vm_area_struct *vma_prio_tree_next(struct vm_area_struct *vma,\n\tstruct prio_tree_iter *iter);\n\n#define vma_prio_tree_foreach(vma, iter, root, begin, end)\t\\\n\tfor (prio_tree_iter_init(iter, root, begin, end), vma = NULL;\t\\\n\t\t(vma = vma_prio_tree_next(vma, iter)); )\n\nstatic inline void vma_nonlinear_insert(struct vm_area_struct *vma,\n\t\t\t\t\tstruct list_head *list)\n{\n\tvma->shared.vm_set.parent = NULL;\n\tlist_add_tail(&vma->shared.vm_set.list, list);\n}\n\n/* mmap.c */\nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert);\nextern struct vm_area_struct *vma_merge(struct mm_struct *,\n\tstruct vm_area_struct *prev, unsigned long addr, unsigned long end,\n\tunsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\n\tstruct mempolicy *);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int split_vma(struct mm_struct *,\n\tstruct vm_area_struct *, unsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\n\tstruct rb_node **, struct rb_node *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff);\nextern void exit_mmap(struct mm_struct *);\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\n#ifdef CONFIG_PROC_FS\n/* From fs/proc/base.c. callers must _not_ hold the mm's exe_file_lock */\nextern void added_exe_file_vma(struct mm_struct *mm);\nextern void removed_exe_file_vma(struct mm_struct *mm);\n#else\nstatic inline void added_exe_file_vma(struct mm_struct *mm)\n{}\n\nstatic inline void removed_exe_file_vma(struct mm_struct *mm)\n{}\n#endif /* CONFIG_PROC_FS */\n\nextern int may_expand_vm(struct mm_struct *mm, unsigned long npages);\nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long pgoff);\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long flags,\n\tunsigned int vm_flags, unsigned long pgoff);\n\nstatic inline unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long offset)\n{\n\tunsigned long ret = -EINVAL;\n\tif ((offset + PAGE_ALIGN(len)) < offset)\n\t\tgoto out;\n\tif (!(offset & ~PAGE_MASK))\n\t\tret = do_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);\nout:\n\treturn ret;\n}\n\nextern int do_munmap(struct mm_struct *, unsigned long, size_t);\n\nextern unsigned long do_brk(unsigned long, unsigned long);\n\n/* filemap.c */\nextern unsigned long page_unuse(struct page *);\nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\n\n/* generic vm_area_ops exported for stackable file systems */\nextern int filemap_fault(struct vm_area_struct *, struct vm_fault *);\n\n/* mm/page-writeback.c */\nint write_one_page(struct page *page, int wait);\nvoid task_dirty_inc(struct task_struct *tsk);\n\n/* readahead.c */\n#define VM_MAX_READAHEAD\t128\t/* kbytes */\n#define VM_MIN_READAHEAD\t16\t/* kbytes (includes current page) */\n\nint force_page_cache_readahead(struct address_space *mapping, struct file *filp,\n\t\t\tpgoff_t offset, unsigned long nr_to_read);\n\nvoid page_cache_sync_readahead(struct address_space *mapping,\n\t\t\t       struct file_ra_state *ra,\n\t\t\t       struct file *filp,\n\t\t\t       pgoff_t offset,\n\t\t\t       unsigned long size);\n\nvoid page_cache_async_readahead(struct address_space *mapping,\n\t\t\t\tstruct file_ra_state *ra,\n\t\t\t\tstruct file *filp,\n\t\t\t\tstruct page *pg,\n\t\t\t\tpgoff_t offset,\n\t\t\t\tunsigned long size);\n\nunsigned long max_sane_readahead(unsigned long nr);\nunsigned long ra_submit(struct file_ra_state *ra,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct file *filp);\n\n/* Do stack extension */\nextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\n#if VM_GROWSUP\nextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\n#else\n  #define expand_upwards(vma, address) do { } while (0)\n#endif\nextern int expand_stack_downwards(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address);\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\n   NULL if none.  Assume start_addr < end_addr. */\nstatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\n{\n\tstruct vm_area_struct * vma = find_vma(mm,start_addr);\n\n\tif (vma && end_addr <= vma->vm_start)\n\t\tvma = NULL;\n\treturn vma;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\n#endif\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nint vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\n\nstruct page *follow_page(struct vm_area_struct *, unsigned long address,\n\t\t\tunsigned int foll_flags);\n#define FOLL_WRITE\t0x01\t/* check pte is writable */\n#define FOLL_TOUCH\t0x02\t/* mark page accessed */\n#define FOLL_GET\t0x04\t/* do get_page on page */\n#define FOLL_DUMP\t0x08\t/* give error on hole if it would be zero */\n#define FOLL_FORCE\t0x10\t/* get_user_pages read/write w/o permission */\n#define FOLL_NOWAIT\t0x20\t/* if a disk transfer is needed, start the IO\n\t\t\t\t * and return without waiting upon it */\n#define FOLL_MLOCK\t0x40\t/* mark page as mlocked */\n#define FOLL_SPLIT\t0x80\t/* don't return transhuge pages, split them */\n#define FOLL_HWPOISON\t0x100\t/* check page is hwpoisoned */\n\ntypedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\tvoid *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\n\n#ifdef CONFIG_PROC_FS\nvoid vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);\n#else\nstatic inline void vm_stat_account(struct mm_struct *mm,\n\t\t\tunsigned long flags, struct file *file, long pages)\n{\n}\n#endif /* CONFIG_PROC_FS */\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern int debug_pagealloc_enabled;\n\nextern void kernel_map_pages(struct page *page, int numpages, int enable);\n\nstatic inline void enable_debug_pagealloc(void)\n{\n\tdebug_pagealloc_enabled = 1;\n}\n#ifdef CONFIG_HIBERNATION\nextern bool kernel_page_present(struct page *page);\n#endif /* CONFIG_HIBERNATION */\n#else\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable) {}\nstatic inline void enable_debug_pagealloc(void)\n{\n}\n#ifdef CONFIG_HIBERNATION\nstatic inline bool kernel_page_present(struct page *page) { return true; }\n#endif /* CONFIG_HIBERNATION */\n#endif\n\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\n#ifdef\t__HAVE_ARCH_GATE_AREA\nint in_gate_area_no_mm(unsigned long addr);\nint in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nint in_gate_area_no_mm(unsigned long addr);\n#define in_gate_area(mm, addr) ({(void)mm; in_gate_area_no_mm(addr);})\n#endif\t/* __HAVE_ARCH_GATE_AREA */\n\nint drop_caches_sysctl_handler(struct ctl_table *, int,\n\t\t\t\t\tvoid __user *, size_t *, loff_t *);\nunsigned long shrink_slab(unsigned long scanned, gfp_t gfp_mask,\n\t\t\tunsigned long lru_pages);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\nvoid print_vma_addr(char *prefix, unsigned long rip);\n\nvoid sparse_mem_maps_populate_node(struct page **map_map,\n\t\t\t\t   unsigned long pnum_begin,\n\t\t\t\t   unsigned long pnum_end,\n\t\t\t\t   unsigned long map_count,\n\t\t\t\t   int nodeid);\n\nstruct page *sparse_mem_map_populate(unsigned long pnum, int nid);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\npud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nvoid *vmemmap_alloc_block_buf(unsigned long size, int node);\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nint vmemmap_populate_basepages(struct page *start_page,\n\t\t\t\t\t\tunsigned long pages, int node);\nint vmemmap_populate(struct page *start_page, unsigned long pages, int node);\nvoid vmemmap_populate_print_last(void);\n\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n};\nextern void memory_failure(unsigned long pfn, int trapno);\nextern int __memory_failure(unsigned long pfn, int trapno, int flags);\nextern int unpoison_memory(unsigned long pfn);\nextern int sysctl_memory_failure_early_kill;\nextern int sysctl_memory_failure_recovery;\nextern void shake_page(struct page *p, int access);\nextern atomic_long_t mce_bad_pages;\nextern int soft_offline_page(struct page *page, int flags);\n\nextern void dump_page(struct page *page);\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr,\n\t\t\t    unsigned int pages_per_huge_page);\nextern void copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t\tunsigned long addr, struct vm_area_struct *vma,\n\t\t\t\tunsigned int pages_per_huge_page);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_MM_H */\n", "/*\n *  Copyright (C) 2009  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n */\n\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/highmem.h>\n#include <linux/hugetlb.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/mm_inline.h>\n#include <linux/kthread.h>\n#include <linux/khugepaged.h>\n#include <linux/freezer.h>\n#include <linux/mman.h>\n#include <asm/tlb.h>\n#include <asm/pgalloc.h>\n#include \"internal.h\"\n\n/*\n * By default transparent hugepage support is enabled for all mappings\n * and khugepaged scans all mappings. Defrag is only invoked by\n * khugepaged hugepage allocations and by page faults inside\n * MADV_HUGEPAGE regions to avoid the risk of slowing down short lived\n * allocations.\n */\nunsigned long transparent_hugepage_flags __read_mostly =\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS\n\t(1<<TRANSPARENT_HUGEPAGE_FLAG)|\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE\n\t(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|\n#endif\n\t(1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)|\n\t(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n\n/* default scan 8*512 pte (or vmas) every 30 second */\nstatic unsigned int khugepaged_pages_to_scan __read_mostly = HPAGE_PMD_NR*8;\nstatic unsigned int khugepaged_pages_collapsed;\nstatic unsigned int khugepaged_full_scans;\nstatic unsigned int khugepaged_scan_sleep_millisecs __read_mostly = 10000;\n/* during fragmentation poll the hugepage allocator once every minute */\nstatic unsigned int khugepaged_alloc_sleep_millisecs __read_mostly = 60000;\nstatic struct task_struct *khugepaged_thread __read_mostly;\nstatic DEFINE_MUTEX(khugepaged_mutex);\nstatic DEFINE_SPINLOCK(khugepaged_mm_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);\n/*\n * default collapse hugepages if there is at least one pte mapped like\n * it would have happened if the vma was large enough during page\n * fault.\n */\nstatic unsigned int khugepaged_max_ptes_none __read_mostly = HPAGE_PMD_NR-1;\n\nstatic int khugepaged(void *none);\nstatic int mm_slots_hash_init(void);\nstatic int khugepaged_slab_init(void);\nstatic void khugepaged_slab_free(void);\n\n#define MM_SLOTS_HASH_HEADS 1024\nstatic struct hlist_head *mm_slots_hash __read_mostly;\nstatic struct kmem_cache *mm_slot_cache __read_mostly;\n\n/**\n * struct mm_slot - hash lookup from mm to mm_slot\n * @hash: hash collision list\n * @mm_node: khugepaged scan list headed in khugepaged_scan.mm_head\n * @mm: the mm that this information is valid for\n */\nstruct mm_slot {\n\tstruct hlist_node hash;\n\tstruct list_head mm_node;\n\tstruct mm_struct *mm;\n};\n\n/**\n * struct khugepaged_scan - cursor for scanning\n * @mm_head: the head of the mm list to scan\n * @mm_slot: the current mm_slot we are scanning\n * @address: the next address inside that to be scanned\n *\n * There is only the one khugepaged_scan instance of this cursor structure.\n */\nstruct khugepaged_scan {\n\tstruct list_head mm_head;\n\tstruct mm_slot *mm_slot;\n\tunsigned long address;\n} khugepaged_scan = {\n\t.mm_head = LIST_HEAD_INIT(khugepaged_scan.mm_head),\n};\n\n\nstatic int set_recommended_min_free_kbytes(void)\n{\n\tstruct zone *zone;\n\tint nr_zones = 0;\n\tunsigned long recommended_min;\n\textern int min_free_kbytes;\n\n\tif (!test_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t      &transparent_hugepage_flags) &&\n\t    !test_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t      &transparent_hugepage_flags))\n\t\treturn 0;\n\n\tfor_each_populated_zone(zone)\n\t\tnr_zones++;\n\n\t/* Make sure at least 2 hugepages are free for MIGRATE_RESERVE */\n\trecommended_min = pageblock_nr_pages * nr_zones * 2;\n\n\t/*\n\t * Make sure that on average at least two pageblocks are almost free\n\t * of another type, one for a migratetype to fall back to and a\n\t * second to avoid subsequent fallbacks of other types There are 3\n\t * MIGRATE_TYPES we care about.\n\t */\n\trecommended_min += pageblock_nr_pages * nr_zones *\n\t\t\t   MIGRATE_PCPTYPES * MIGRATE_PCPTYPES;\n\n\t/* don't ever allow to reserve more than 5% of the lowmem */\n\trecommended_min = min(recommended_min,\n\t\t\t      (unsigned long) nr_free_buffer_pages() / 20);\n\trecommended_min <<= (PAGE_SHIFT-10);\n\n\tif (recommended_min > min_free_kbytes)\n\t\tmin_free_kbytes = recommended_min;\n\tsetup_per_zone_wmarks();\n\treturn 0;\n}\nlate_initcall(set_recommended_min_free_kbytes);\n\nstatic int start_khugepaged(void)\n{\n\tint err = 0;\n\tif (khugepaged_enabled()) {\n\t\tint wakeup;\n\t\tif (unlikely(!mm_slot_cache || !mm_slots_hash)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_thread)\n\t\t\tkhugepaged_thread = kthread_run(khugepaged, NULL,\n\t\t\t\t\t\t\t\"khugepaged\");\n\t\tif (unlikely(IS_ERR(khugepaged_thread))) {\n\t\t\tprintk(KERN_ERR\n\t\t\t       \"khugepaged: kthread_run(khugepaged) failed\\n\");\n\t\t\terr = PTR_ERR(khugepaged_thread);\n\t\t\tkhugepaged_thread = NULL;\n\t\t}\n\t\twakeup = !list_empty(&khugepaged_scan.mm_head);\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tif (wakeup)\n\t\t\twake_up_interruptible(&khugepaged_wait);\n\n\t\tset_recommended_min_free_kbytes();\n\t} else\n\t\t/* wakeup to exit */\n\t\twake_up_interruptible(&khugepaged_wait);\nout:\n\treturn err;\n}\n\n#ifdef CONFIG_SYSFS\n\nstatic ssize_t double_flag_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf,\n\t\t\t\tenum transparent_hugepage_flag enabled,\n\t\t\t\tenum transparent_hugepage_flag req_madv)\n{\n\tif (test_bit(enabled, &transparent_hugepage_flags)) {\n\t\tVM_BUG_ON(test_bit(req_madv, &transparent_hugepage_flags));\n\t\treturn sprintf(buf, \"[always] madvise never\\n\");\n\t} else if (test_bit(req_madv, &transparent_hugepage_flags))\n\t\treturn sprintf(buf, \"always [madvise] never\\n\");\n\telse\n\t\treturn sprintf(buf, \"always madvise [never]\\n\");\n}\nstatic ssize_t double_flag_store(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t const char *buf, size_t count,\n\t\t\t\t enum transparent_hugepage_flag enabled,\n\t\t\t\t enum transparent_hugepage_flag req_madv)\n{\n\tif (!memcmp(\"always\", buf,\n\t\t    min(sizeof(\"always\")-1, count))) {\n\t\tset_bit(enabled, &transparent_hugepage_flags);\n\t\tclear_bit(req_madv, &transparent_hugepage_flags);\n\t} else if (!memcmp(\"madvise\", buf,\n\t\t\t   min(sizeof(\"madvise\")-1, count))) {\n\t\tclear_bit(enabled, &transparent_hugepage_flags);\n\t\tset_bit(req_madv, &transparent_hugepage_flags);\n\t} else if (!memcmp(\"never\", buf,\n\t\t\t   min(sizeof(\"never\")-1, count))) {\n\t\tclear_bit(enabled, &transparent_hugepage_flags);\n\t\tclear_bit(req_madv, &transparent_hugepage_flags);\n\t} else\n\t\treturn -EINVAL;\n\n\treturn count;\n}\n\nstatic ssize_t enabled_show(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\treturn double_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG);\n}\nstatic ssize_t enabled_store(struct kobject *kobj,\n\t\t\t     struct kobj_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tret = double_flag_store(kobj, attr, buf, count,\n\t\t\t\tTRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG);\n\n\tif (ret > 0) {\n\t\tint err = start_khugepaged();\n\t\tif (err)\n\t\t\tret = err;\n\t}\n\n\tif (ret > 0 &&\n\t    (test_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t      &transparent_hugepage_flags) ||\n\t     test_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t      &transparent_hugepage_flags)))\n\t\tset_recommended_min_free_kbytes();\n\n\treturn ret;\n}\nstatic struct kobj_attribute enabled_attr =\n\t__ATTR(enabled, 0644, enabled_show, enabled_store);\n\nstatic ssize_t single_flag_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf,\n\t\t\t\tenum transparent_hugepage_flag flag)\n{\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       !!test_bit(flag, &transparent_hugepage_flags));\n}\n\nstatic ssize_t single_flag_store(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t const char *buf, size_t count,\n\t\t\t\t enum transparent_hugepage_flag flag)\n{\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(buf, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (value > 1)\n\t\treturn -EINVAL;\n\n\tif (value)\n\t\tset_bit(flag, &transparent_hugepage_flags);\n\telse\n\t\tclear_bit(flag, &transparent_hugepage_flags);\n\n\treturn count;\n}\n\n/*\n * Currently defrag only disables __GFP_NOWAIT for allocation. A blind\n * __GFP_REPEAT is too aggressive, it's never worth swapping tons of\n * memory just to allocate one more hugepage.\n */\nstatic ssize_t defrag_show(struct kobject *kobj,\n\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn double_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG);\n}\nstatic ssize_t defrag_store(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\treturn double_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG);\n}\nstatic struct kobj_attribute defrag_attr =\n\t__ATTR(defrag, 0644, defrag_show, defrag_store);\n\n#ifdef CONFIG_DEBUG_VM\nstatic ssize_t debug_cow_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\treturn single_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG);\n}\nstatic ssize_t debug_cow_store(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr,\n\t\t\t       const char *buf, size_t count)\n{\n\treturn single_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG);\n}\nstatic struct kobj_attribute debug_cow_attr =\n\t__ATTR(debug_cow, 0644, debug_cow_show, debug_cow_store);\n#endif /* CONFIG_DEBUG_VM */\n\nstatic struct attribute *hugepage_attr[] = {\n\t&enabled_attr.attr,\n\t&defrag_attr.attr,\n#ifdef CONFIG_DEBUG_VM\n\t&debug_cow_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hugepage_attr_group = {\n\t.attrs = hugepage_attr,\n};\n\nstatic ssize_t scan_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_scan_sleep_millisecs);\n}\n\nstatic ssize_t scan_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_scan_sleep_millisecs = msecs;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute scan_sleep_millisecs_attr =\n\t__ATTR(scan_sleep_millisecs, 0644, scan_sleep_millisecs_show,\n\t       scan_sleep_millisecs_store);\n\nstatic ssize_t alloc_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_alloc_sleep_millisecs);\n}\n\nstatic ssize_t alloc_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_alloc_sleep_millisecs = msecs;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute alloc_sleep_millisecs_attr =\n\t__ATTR(alloc_sleep_millisecs, 0644, alloc_sleep_millisecs_show,\n\t       alloc_sleep_millisecs_store);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_pages_to_scan);\n}\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long pages;\n\n\terr = strict_strtoul(buf, 10, &pages);\n\tif (err || !pages || pages > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_pages_to_scan = pages;\n\n\treturn count;\n}\nstatic struct kobj_attribute pages_to_scan_attr =\n\t__ATTR(pages_to_scan, 0644, pages_to_scan_show,\n\t       pages_to_scan_store);\n\nstatic ssize_t pages_collapsed_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_pages_collapsed);\n}\nstatic struct kobj_attribute pages_collapsed_attr =\n\t__ATTR_RO(pages_collapsed);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr,\n\t\t\t       char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_full_scans);\n}\nstatic struct kobj_attribute full_scans_attr =\n\t__ATTR_RO(full_scans);\n\nstatic ssize_t khugepaged_defrag_show(struct kobject *kobj,\n\t\t\t\t      struct kobj_attribute *attr, char *buf)\n{\n\treturn single_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic ssize_t khugepaged_defrag_store(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\treturn single_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic struct kobj_attribute khugepaged_defrag_attr =\n\t__ATTR(defrag, 0644, khugepaged_defrag_show,\n\t       khugepaged_defrag_store);\n\n/*\n * max_ptes_none controls if khugepaged should collapse hugepages over\n * any unmapped ptes in turn potentially increasing the memory\n * footprint of the vmas. When max_ptes_none is 0 khugepaged will not\n * reduce the available free memory in the system as it\n * runs. Increasing max_ptes_none will instead potentially reduce the\n * free memory in the system during the khugepaged scan.\n */\nstatic ssize_t khugepaged_max_ptes_none_show(struct kobject *kobj,\n\t\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t\t     char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_max_ptes_none);\n}\nstatic ssize_t khugepaged_max_ptes_none_store(struct kobject *kobj,\n\t\t\t\t\t      struct kobj_attribute *attr,\n\t\t\t\t\t      const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long max_ptes_none;\n\n\terr = strict_strtoul(buf, 10, &max_ptes_none);\n\tif (err || max_ptes_none > HPAGE_PMD_NR-1)\n\t\treturn -EINVAL;\n\n\tkhugepaged_max_ptes_none = max_ptes_none;\n\n\treturn count;\n}\nstatic struct kobj_attribute khugepaged_max_ptes_none_attr =\n\t__ATTR(max_ptes_none, 0644, khugepaged_max_ptes_none_show,\n\t       khugepaged_max_ptes_none_store);\n\nstatic struct attribute *khugepaged_attr[] = {\n\t&khugepaged_defrag_attr.attr,\n\t&khugepaged_max_ptes_none_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&pages_collapsed_attr.attr,\n\t&full_scans_attr.attr,\n\t&scan_sleep_millisecs_attr.attr,\n\t&alloc_sleep_millisecs_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group khugepaged_attr_group = {\n\t.attrs = khugepaged_attr,\n\t.name = \"khugepaged\",\n};\n#endif /* CONFIG_SYSFS */\n\nstatic int __init hugepage_init(void)\n{\n\tint err;\n#ifdef CONFIG_SYSFS\n\tstatic struct kobject *hugepage_kobj;\n#endif\n\n\terr = -EINVAL;\n\tif (!has_transparent_hugepage()) {\n\t\ttransparent_hugepage_flags = 0;\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_SYSFS\n\terr = -ENOMEM;\n\thugepage_kobj = kobject_create_and_add(\"transparent_hugepage\", mm_kobj);\n\tif (unlikely(!hugepage_kobj)) {\n\t\tprintk(KERN_ERR \"hugepage: failed kobject create\\n\");\n\t\tgoto out;\n\t}\n\n\terr = sysfs_create_group(hugepage_kobj, &hugepage_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"hugepage: failed register hugeage group\\n\");\n\t\tgoto out;\n\t}\n\n\terr = sysfs_create_group(hugepage_kobj, &khugepaged_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"hugepage: failed register hugeage group\\n\");\n\t\tgoto out;\n\t}\n#endif\n\n\terr = khugepaged_slab_init();\n\tif (err)\n\t\tgoto out;\n\n\terr = mm_slots_hash_init();\n\tif (err) {\n\t\tkhugepaged_slab_free();\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By default disable transparent hugepages on smaller systems,\n\t * where the extra memory used could hurt more than TLB overhead\n\t * is likely to save.  The admin can still enable it through /sys.\n\t */\n\tif (totalram_pages < (512 << (20 - PAGE_SHIFT)))\n\t\ttransparent_hugepage_flags = 0;\n\n\tstart_khugepaged();\n\n\tset_recommended_min_free_kbytes();\n\nout:\n\treturn err;\n}\nmodule_init(hugepage_init)\n\nstatic int __init setup_transparent_hugepage(char *str)\n{\n\tint ret = 0;\n\tif (!str)\n\t\tgoto out;\n\tif (!strcmp(str, \"always\")) {\n\t\tset_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t&transparent_hugepage_flags);\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tret = 1;\n\t} else if (!strcmp(str, \"madvise\")) {\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tset_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t&transparent_hugepage_flags);\n\t\tret = 1;\n\t} else if (!strcmp(str, \"never\")) {\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tret = 1;\n\t}\nout:\n\tif (!ret)\n\t\tprintk(KERN_WARNING\n\t\t       \"transparent_hugepage= cannot parse, ignored\\n\");\n\treturn ret;\n}\n__setup(\"transparent_hugepage=\", setup_transparent_hugepage);\n\nstatic void prepare_pmd_huge_pte(pgtable_t pgtable,\n\t\t\t\t struct mm_struct *mm)\n{\n\tassert_spin_locked(&mm->page_table_lock);\n\n\t/* FIFO */\n\tif (!mm->pmd_huge_pte)\n\t\tINIT_LIST_HEAD(&pgtable->lru);\n\telse\n\t\tlist_add(&pgtable->lru, &mm->pmd_huge_pte->lru);\n\tmm->pmd_huge_pte = pgtable;\n}\n\nstatic inline pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpmd = pmd_mkwrite(pmd);\n\treturn pmd;\n}\n\nstatic int __do_huge_pmd_anonymous_page(struct mm_struct *mm,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned long haddr, pmd_t *pmd,\n\t\t\t\t\tstruct page *page)\n{\n\tint ret = 0;\n\tpgtable_t pgtable;\n\n\tVM_BUG_ON(!PageCompound(page));\n\tpgtable = pte_alloc_one(mm, haddr);\n\tif (unlikely(!pgtable)) {\n\t\tmem_cgroup_uncharge_page(page);\n\t\tput_page(page);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\tclear_huge_page(page, haddr, HPAGE_PMD_NR);\n\t__SetPageUptodate(page);\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_none(*pmd))) {\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tmem_cgroup_uncharge_page(page);\n\t\tput_page(page);\n\t\tpte_free(mm, pgtable);\n\t} else {\n\t\tpmd_t entry;\n\t\tentry = mk_pmd(page, vma->vm_page_prot);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tentry = pmd_mkhuge(entry);\n\t\t/*\n\t\t * The spinlocking to take the lru_lock inside\n\t\t * page_add_new_anon_rmap() acts as a full memory\n\t\t * barrier to be sure clear_huge_page writes become\n\t\t * visible after the set_pmd_at() write.\n\t\t */\n\t\tpage_add_new_anon_rmap(page, vma, haddr);\n\t\tset_pmd_at(mm, haddr, pmd, entry);\n\t\tprepare_pmd_huge_pte(pgtable, mm);\n\t\tadd_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);\n\t\tspin_unlock(&mm->page_table_lock);\n\t}\n\n\treturn ret;\n}\n\nstatic inline gfp_t alloc_hugepage_gfpmask(int defrag, gfp_t extra_gfp)\n{\n\treturn (GFP_TRANSHUGE & ~(defrag ? 0 : __GFP_WAIT)) | extra_gfp;\n}\n\nstatic inline struct page *alloc_hugepage_vma(int defrag,\n\t\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t\t      unsigned long haddr, int nd,\n\t\t\t\t\t      gfp_t extra_gfp)\n{\n\treturn alloc_pages_vma(alloc_hugepage_gfpmask(defrag, extra_gfp),\n\t\t\t       HPAGE_PMD_ORDER, vma, haddr, nd);\n}\n\n#ifndef CONFIG_NUMA\nstatic inline struct page *alloc_hugepage(int defrag)\n{\n\treturn alloc_pages(alloc_hugepage_gfpmask(defrag, 0),\n\t\t\t   HPAGE_PMD_ORDER);\n}\n#endif\n\nint do_huge_pmd_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t       unsigned long address, pmd_t *pmd,\n\t\t\t       unsigned int flags)\n{\n\tstruct page *page;\n\tunsigned long haddr = address & HPAGE_PMD_MASK;\n\tpte_t *pte;\n\n\tif (haddr >= vma->vm_start && haddr + HPAGE_PMD_SIZE <= vma->vm_end) {\n\t\tif (unlikely(anon_vma_prepare(vma)))\n\t\t\treturn VM_FAULT_OOM;\n\t\tif (unlikely(khugepaged_enter(vma)))\n\t\t\treturn VM_FAULT_OOM;\n\t\tpage = alloc_hugepage_vma(transparent_hugepage_defrag(vma),\n\t\t\t\t\t  vma, haddr, numa_node_id(), 0);\n\t\tif (unlikely(!page)) {\n\t\t\tcount_vm_event(THP_FAULT_FALLBACK);\n\t\t\tgoto out;\n\t\t}\n\t\tcount_vm_event(THP_FAULT_ALLOC);\n\t\tif (unlikely(mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))) {\n\t\t\tput_page(page);\n\t\t\tgoto out;\n\t\t}\n\n\t\treturn __do_huge_pmd_anonymous_page(mm, vma, haddr, pmd, page);\n\t}\nout:\n\t/*\n\t * Use __pte_alloc instead of pte_alloc_map, because we can't\n\t * run pte_offset_map on the pmd, if an huge pmd could\n\t * materialize from under us from a different thread.\n\t */\n\tif (unlikely(__pte_alloc(mm, vma, pmd, address)))\n\t\treturn VM_FAULT_OOM;\n\t/* if an huge pmd materialized from under us just retry later */\n\tif (unlikely(pmd_trans_huge(*pmd)))\n\t\treturn 0;\n\t/*\n\t * A regular pmd is established and it can't morph into a huge pmd\n\t * from under us anymore at this point because we hold the mmap_sem\n\t * read mode and khugepaged takes it in write mode. So now it's\n\t * safe to run pte_offset_map().\n\t */\n\tpte = pte_offset_map(pmd, address);\n\treturn handle_pte_fault(mm, vma, address, pte, pmd, flags);\n}\n\nint copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t\t  struct vm_area_struct *vma)\n{\n\tstruct page *src_page;\n\tpmd_t pmd;\n\tpgtable_t pgtable;\n\tint ret;\n\n\tret = -ENOMEM;\n\tpgtable = pte_alloc_one(dst_mm, addr);\n\tif (unlikely(!pgtable))\n\t\tgoto out;\n\n\tspin_lock(&dst_mm->page_table_lock);\n\tspin_lock_nested(&src_mm->page_table_lock, SINGLE_DEPTH_NESTING);\n\n\tret = -EAGAIN;\n\tpmd = *src_pmd;\n\tif (unlikely(!pmd_trans_huge(pmd))) {\n\t\tpte_free(dst_mm, pgtable);\n\t\tgoto out_unlock;\n\t}\n\tif (unlikely(pmd_trans_splitting(pmd))) {\n\t\t/* split huge page running from under us */\n\t\tspin_unlock(&src_mm->page_table_lock);\n\t\tspin_unlock(&dst_mm->page_table_lock);\n\t\tpte_free(dst_mm, pgtable);\n\n\t\twait_split_huge_page(vma->anon_vma, src_pmd); /* src_vma */\n\t\tgoto out;\n\t}\n\tsrc_page = pmd_page(pmd);\n\tVM_BUG_ON(!PageHead(src_page));\n\tget_page(src_page);\n\tpage_dup_rmap(src_page);\n\tadd_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);\n\n\tpmdp_set_wrprotect(src_mm, addr, src_pmd);\n\tpmd = pmd_mkold(pmd_wrprotect(pmd));\n\tset_pmd_at(dst_mm, addr, dst_pmd, pmd);\n\tprepare_pmd_huge_pte(pgtable, dst_mm);\n\n\tret = 0;\nout_unlock:\n\tspin_unlock(&src_mm->page_table_lock);\n\tspin_unlock(&dst_mm->page_table_lock);\nout:\n\treturn ret;\n}\n\n/* no \"address\" argument so destroys page coloring of some arch */\npgtable_t get_pmd_huge_pte(struct mm_struct *mm)\n{\n\tpgtable_t pgtable;\n\n\tassert_spin_locked(&mm->page_table_lock);\n\n\t/* FIFO */\n\tpgtable = mm->pmd_huge_pte;\n\tif (list_empty(&pgtable->lru))\n\t\tmm->pmd_huge_pte = NULL;\n\telse {\n\t\tmm->pmd_huge_pte = list_entry(pgtable->lru.next,\n\t\t\t\t\t      struct page, lru);\n\t\tlist_del(&pgtable->lru);\n\t}\n\treturn pgtable;\n}\n\nstatic int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpmd_t *pmd, pmd_t orig_pmd,\n\t\t\t\t\tstruct page *page,\n\t\t\t\t\tunsigned long haddr)\n{\n\tpgtable_t pgtable;\n\tpmd_t _pmd;\n\tint ret = 0, i;\n\tstruct page **pages;\n\n\tpages = kmalloc(sizeof(struct page *) * HPAGE_PMD_NR,\n\t\t\tGFP_KERNEL);\n\tif (unlikely(!pages)) {\n\t\tret |= VM_FAULT_OOM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tpages[i] = alloc_page_vma_node(GFP_HIGHUSER_MOVABLE |\n\t\t\t\t\t       __GFP_OTHER_NODE,\n\t\t\t\t\t       vma, address, page_to_nid(page));\n\t\tif (unlikely(!pages[i] ||\n\t\t\t     mem_cgroup_newpage_charge(pages[i], mm,\n\t\t\t\t\t\t       GFP_KERNEL))) {\n\t\t\tif (pages[i])\n\t\t\t\tput_page(pages[i]);\n\t\t\tmem_cgroup_uncharge_start();\n\t\t\twhile (--i >= 0) {\n\t\t\t\tmem_cgroup_uncharge_page(pages[i]);\n\t\t\t\tput_page(pages[i]);\n\t\t\t}\n\t\t\tmem_cgroup_uncharge_end();\n\t\t\tkfree(pages);\n\t\t\tret |= VM_FAULT_OOM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tcopy_user_highpage(pages[i], page + i,\n\t\t\t\t   haddr + PAGE_SHIFT*i, vma);\n\t\t__SetPageUptodate(pages[i]);\n\t\tcond_resched();\n\t}\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd)))\n\t\tgoto out_free_pages;\n\tVM_BUG_ON(!PageHead(page));\n\n\tpmdp_clear_flush_notify(vma, haddr, pmd);\n\t/* leave pmd empty until pte is filled */\n\n\tpgtable = get_pmd_huge_pte(mm);\n\tpmd_populate(mm, &_pmd, pgtable);\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {\n\t\tpte_t *pte, entry;\n\t\tentry = mk_pte(pages[i], vma->vm_page_prot);\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\tpage_add_new_anon_rmap(pages[i], vma, haddr);\n\t\tpte = pte_offset_map(&_pmd, haddr);\n\t\tVM_BUG_ON(!pte_none(*pte));\n\t\tset_pte_at(mm, haddr, pte, entry);\n\t\tpte_unmap(pte);\n\t}\n\tkfree(pages);\n\n\tmm->nr_ptes++;\n\tsmp_wmb(); /* make pte visible before pmd */\n\tpmd_populate(mm, pmd, pgtable);\n\tpage_remove_rmap(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tret |= VM_FAULT_WRITE;\n\tput_page(page);\n\nout:\n\treturn ret;\n\nout_free_pages:\n\tspin_unlock(&mm->page_table_lock);\n\tmem_cgroup_uncharge_start();\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tmem_cgroup_uncharge_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tmem_cgroup_uncharge_end();\n\tkfree(pages);\n\tgoto out;\n}\n\nint do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pmd_t *pmd, pmd_t orig_pmd)\n{\n\tint ret = 0;\n\tstruct page *page, *new_page;\n\tunsigned long haddr;\n\n\tVM_BUG_ON(!vma->anon_vma);\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd)))\n\t\tgoto out_unlock;\n\n\tpage = pmd_page(orig_pmd);\n\tVM_BUG_ON(!PageCompound(page) || !PageHead(page));\n\thaddr = address & HPAGE_PMD_MASK;\n\tif (page_mapcount(page) == 1) {\n\t\tpmd_t entry;\n\t\tentry = pmd_mkyoung(orig_pmd);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tif (pmdp_set_access_flags(vma, haddr, pmd, entry,  1))\n\t\t\tupdate_mmu_cache(vma, address, entry);\n\t\tret |= VM_FAULT_WRITE;\n\t\tgoto out_unlock;\n\t}\n\tget_page(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tif (transparent_hugepage_enabled(vma) &&\n\t    !transparent_hugepage_debug_cow())\n\t\tnew_page = alloc_hugepage_vma(transparent_hugepage_defrag(vma),\n\t\t\t\t\t      vma, haddr, numa_node_id(), 0);\n\telse\n\t\tnew_page = NULL;\n\n\tif (unlikely(!new_page)) {\n\t\tcount_vm_event(THP_FAULT_FALLBACK);\n\t\tret = do_huge_pmd_wp_page_fallback(mm, vma, address,\n\t\t\t\t\t\t   pmd, orig_pmd, page, haddr);\n\t\tput_page(page);\n\t\tgoto out;\n\t}\n\tcount_vm_event(THP_FAULT_ALLOC);\n\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tput_page(new_page);\n\t\tput_page(page);\n\t\tret |= VM_FAULT_OOM;\n\t\tgoto out;\n\t}\n\n\tcopy_user_huge_page(new_page, page, haddr, vma, HPAGE_PMD_NR);\n\t__SetPageUptodate(new_page);\n\n\tspin_lock(&mm->page_table_lock);\n\tput_page(page);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd))) {\n\t\tmem_cgroup_uncharge_page(new_page);\n\t\tput_page(new_page);\n\t} else {\n\t\tpmd_t entry;\n\t\tVM_BUG_ON(!PageHead(page));\n\t\tentry = mk_pmd(new_page, vma->vm_page_prot);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tentry = pmd_mkhuge(entry);\n\t\tpmdp_clear_flush_notify(vma, haddr, pmd);\n\t\tpage_add_new_anon_rmap(new_page, vma, haddr);\n\t\tset_pmd_at(mm, haddr, pmd, entry);\n\t\tupdate_mmu_cache(vma, address, entry);\n\t\tpage_remove_rmap(page);\n\t\tput_page(page);\n\t\tret |= VM_FAULT_WRITE;\n\t}\nout_unlock:\n\tspin_unlock(&mm->page_table_lock);\nout:\n\treturn ret;\n}\n\nstruct page *follow_trans_huge_pmd(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct page *page = NULL;\n\n\tassert_spin_locked(&mm->page_table_lock);\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\tgoto out;\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON(!PageHead(page));\n\tif (flags & FOLL_TOUCH) {\n\t\tpmd_t _pmd;\n\t\t/*\n\t\t * We should set the dirty bit only for FOLL_WRITE but\n\t\t * for now the dirty bit in the pmd is meaningless.\n\t\t * And if the dirty bit will become meaningful and\n\t\t * we'll only set it with FOLL_WRITE, an atomic\n\t\t * set_bit will be required on the pmd to set the\n\t\t * young bit, instead of the current set_pmd_at.\n\t\t */\n\t\t_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));\n\t\tset_pmd_at(mm, addr & HPAGE_PMD_MASK, pmd, _pmd);\n\t}\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON(!PageCompound(page));\n\tif (flags & FOLL_GET)\n\t\tget_page(page);\n\nout:\n\treturn page;\n}\n\nint zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t pmd_t *pmd)\n{\n\tint ret = 0;\n\n\tspin_lock(&tlb->mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tif (unlikely(pmd_trans_splitting(*pmd))) {\n\t\t\tspin_unlock(&tlb->mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma,\n\t\t\t\t\t     pmd);\n\t\t} else {\n\t\t\tstruct page *page;\n\t\t\tpgtable_t pgtable;\n\t\t\tpgtable = get_pmd_huge_pte(tlb->mm);\n\t\t\tpage = pmd_page(*pmd);\n\t\t\tpmd_clear(pmd);\n\t\t\tpage_remove_rmap(page);\n\t\t\tVM_BUG_ON(page_mapcount(page) < 0);\n\t\t\tadd_mm_counter(tlb->mm, MM_ANONPAGES, -HPAGE_PMD_NR);\n\t\t\tVM_BUG_ON(!PageHead(page));\n\t\t\tspin_unlock(&tlb->mm->page_table_lock);\n\t\t\ttlb_remove_page(tlb, page);\n\t\t\tpte_free(tlb->mm, pgtable);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\tspin_unlock(&tlb->mm->page_table_lock);\n\n\treturn ret;\n}\n\nint mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, unsigned long end,\n\t\tunsigned char *vec)\n{\n\tint ret = 0;\n\n\tspin_lock(&vma->vm_mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tret = !pmd_trans_splitting(*pmd);\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\t\tif (unlikely(!ret))\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\telse {\n\t\t\t/*\n\t\t\t * All logical pages in the range are present\n\t\t\t * if backed by a huge page.\n\t\t\t */\n\t\t\tmemset(vec, 1, (end - addr) >> PAGE_SHIFT);\n\t\t}\n\t} else\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\n\treturn ret;\n}\n\nint change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint ret = 0;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tif (unlikely(pmd_trans_splitting(*pmd))) {\n\t\t\tspin_unlock(&mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\t} else {\n\t\t\tpmd_t entry;\n\n\t\t\tentry = pmdp_get_and_clear(mm, addr, pmd);\n\t\t\tentry = pmd_modify(entry, newprot);\n\t\t\tset_pmd_at(mm, addr, pmd, entry);\n\t\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\t\t\tflush_tlb_range(vma, addr, addr + HPAGE_PMD_SIZE);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\n\treturn ret;\n}\n\npmd_t *page_check_address_pmd(struct page *page,\n\t\t\t      struct mm_struct *mm,\n\t\t\t      unsigned long address,\n\t\t\t      enum page_check_address_pmd_flag flag)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, *ret = NULL;\n\n\tif (address & ~HPAGE_PMD_MASK)\n\t\tgoto out;\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\tgoto out;\n\tif (pmd_page(*pmd) != page)\n\t\tgoto out;\n\t/*\n\t * split_vma() may create temporary aliased mappings. There is\n\t * no risk as long as all huge pmd are found and have their\n\t * splitting bit set before __split_huge_page_refcount\n\t * runs. Finding the same huge pmd more than once during the\n\t * same rmap walk is not a problem.\n\t */\n\tif (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG &&\n\t    pmd_trans_splitting(*pmd))\n\t\tgoto out;\n\tif (pmd_trans_huge(*pmd)) {\n\t\tVM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG &&\n\t\t\t  !pmd_trans_splitting(*pmd));\n\t\tret = pmd;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int __split_huge_page_splitting(struct page *page,\n\t\t\t\t       struct vm_area_struct *vma,\n\t\t\t\t       unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tint ret = 0;\n\n\tspin_lock(&mm->page_table_lock);\n\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG);\n\tif (pmd) {\n\t\t/*\n\t\t * We can't temporarily set the pmd to null in order\n\t\t * to split it, the pmd must remain marked huge at all\n\t\t * times or the VM won't take the pmd_trans_huge paths\n\t\t * and it won't wait on the anon_vma->root->lock to\n\t\t * serialize against split_huge_page*.\n\t\t */\n\t\tpmdp_splitting_flush_notify(vma, address, pmd);\n\t\tret = 1;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\n\treturn ret;\n}\n\nstatic void __split_huge_page_refcount(struct page *page)\n{\n\tint i;\n\tunsigned long head_index = page->index;\n\tstruct zone *zone = page_zone(page);\n\tint zonestat;\n\n\t/* prevent PageLRU to go away from under us, and freeze lru stats */\n\tspin_lock_irq(&zone->lru_lock);\n\tcompound_lock(page);\n\n\tfor (i = 1; i < HPAGE_PMD_NR; i++) {\n\t\tstruct page *page_tail = page + i;\n\n\t\t/* tail_page->_count cannot change */\n\t\tatomic_sub(atomic_read(&page_tail->_count), &page->_count);\n\t\tBUG_ON(page_count(page) <= 0);\n\t\tatomic_add(page_mapcount(page) + 1, &page_tail->_count);\n\t\tBUG_ON(atomic_read(&page_tail->_count) <= 0);\n\n\t\t/* after clearing PageTail the gup refcount can be released */\n\t\tsmp_mb();\n\n\t\t/*\n\t\t * retain hwpoison flag of the poisoned tail page:\n\t\t *   fix for the unsuitable process killed on Guest Machine(KVM)\n\t\t *   by the memory-failure.\n\t\t */\n\t\tpage_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP | __PG_HWPOISON;\n\t\tpage_tail->flags |= (page->flags &\n\t\t\t\t     ((1L << PG_referenced) |\n\t\t\t\t      (1L << PG_swapbacked) |\n\t\t\t\t      (1L << PG_mlocked) |\n\t\t\t\t      (1L << PG_uptodate)));\n\t\tpage_tail->flags |= (1L << PG_dirty);\n\n\t\t/*\n\t\t * 1) clear PageTail before overwriting first_page\n\t\t * 2) clear PageTail before clearing PageHead for VM_BUG_ON\n\t\t */\n\t\tsmp_wmb();\n\n\t\t/*\n\t\t * __split_huge_page_splitting() already set the\n\t\t * splitting bit in all pmd that could map this\n\t\t * hugepage, that will ensure no CPU can alter the\n\t\t * mapcount on the head page. The mapcount is only\n\t\t * accounted in the head page and it has to be\n\t\t * transferred to all tail pages in the below code. So\n\t\t * for this code to be safe, the split the mapcount\n\t\t * can't change. But that doesn't mean userland can't\n\t\t * keep changing and reading the page contents while\n\t\t * we transfer the mapcount, so the pmd splitting\n\t\t * status is achieved setting a reserved bit in the\n\t\t * pmd, not by clearing the present bit.\n\t\t*/\n\t\tBUG_ON(page_mapcount(page_tail));\n\t\tpage_tail->_mapcount = page->_mapcount;\n\n\t\tBUG_ON(page_tail->mapping);\n\t\tpage_tail->mapping = page->mapping;\n\n\t\tpage_tail->index = ++head_index;\n\n\t\tBUG_ON(!PageAnon(page_tail));\n\t\tBUG_ON(!PageUptodate(page_tail));\n\t\tBUG_ON(!PageDirty(page_tail));\n\t\tBUG_ON(!PageSwapBacked(page_tail));\n\n\t\tmem_cgroup_split_huge_fixup(page, page_tail);\n\n\t\tlru_add_page_tail(zone, page, page_tail);\n\t}\n\n\t__dec_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);\n\t__mod_zone_page_state(zone, NR_ANON_PAGES, HPAGE_PMD_NR);\n\n\t/*\n\t * A hugepage counts for HPAGE_PMD_NR pages on the LRU statistics,\n\t * so adjust those appropriately if this page is on the LRU.\n\t */\n\tif (PageLRU(page)) {\n\t\tzonestat = NR_LRU_BASE + page_lru(page);\n\t\t__mod_zone_page_state(zone, zonestat, -(HPAGE_PMD_NR-1));\n\t}\n\n\tClearPageCompound(page);\n\tcompound_unlock(page);\n\tspin_unlock_irq(&zone->lru_lock);\n\n\tfor (i = 1; i < HPAGE_PMD_NR; i++) {\n\t\tstruct page *page_tail = page + i;\n\t\tBUG_ON(page_count(page_tail) <= 0);\n\t\t/*\n\t\t * Tail pages may be freed if there wasn't any mapping\n\t\t * like if add_to_swap() is running on a lru page that\n\t\t * had its mapping zapped. And freeing these pages\n\t\t * requires taking the lru_lock so we do the put_page\n\t\t * of the tail pages after the split is complete.\n\t\t */\n\t\tput_page(page_tail);\n\t}\n\n\t/*\n\t * Only the head page (now become a regular page) is required\n\t * to be pinned by the caller.\n\t */\n\tBUG_ON(page_count(page) <= 0);\n}\n\nstatic int __split_huge_page_map(struct page *page,\n\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd, _pmd;\n\tint ret = 0, i;\n\tpgtable_t pgtable;\n\tunsigned long haddr;\n\n\tspin_lock(&mm->page_table_lock);\n\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG);\n\tif (pmd) {\n\t\tpgtable = get_pmd_huge_pte(mm);\n\t\tpmd_populate(mm, &_pmd, pgtable);\n\n\t\tfor (i = 0, haddr = address; i < HPAGE_PMD_NR;\n\t\t     i++, haddr += PAGE_SIZE) {\n\t\t\tpte_t *pte, entry;\n\t\t\tBUG_ON(PageCompound(page+i));\n\t\t\tentry = mk_pte(page + i, vma->vm_page_prot);\n\t\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t\tif (!pmd_write(*pmd))\n\t\t\t\tentry = pte_wrprotect(entry);\n\t\t\telse\n\t\t\t\tBUG_ON(page_mapcount(page) != 1);\n\t\t\tif (!pmd_young(*pmd))\n\t\t\t\tentry = pte_mkold(entry);\n\t\t\tpte = pte_offset_map(&_pmd, haddr);\n\t\t\tBUG_ON(!pte_none(*pte));\n\t\t\tset_pte_at(mm, haddr, pte, entry);\n\t\t\tpte_unmap(pte);\n\t\t}\n\n\t\tmm->nr_ptes++;\n\t\tsmp_wmb(); /* make pte visible before pmd */\n\t\t/*\n\t\t * Up to this point the pmd is present and huge and\n\t\t * userland has the whole access to the hugepage\n\t\t * during the split (which happens in place). If we\n\t\t * overwrite the pmd with the not-huge version\n\t\t * pointing to the pte here (which of course we could\n\t\t * if all CPUs were bug free), userland could trigger\n\t\t * a small page size TLB miss on the small sized TLB\n\t\t * while the hugepage TLB entry is still established\n\t\t * in the huge TLB. Some CPU doesn't like that. See\n\t\t * http://support.amd.com/us/Processor_TechDocs/41322.pdf,\n\t\t * Erratum 383 on page 93. Intel should be safe but is\n\t\t * also warns that it's only safe if the permission\n\t\t * and cache attributes of the two entries loaded in\n\t\t * the two TLB is identical (which should be the case\n\t\t * here). But it is generally safer to never allow\n\t\t * small and huge TLB entries for the same virtual\n\t\t * address to be loaded simultaneously. So instead of\n\t\t * doing \"pmd_populate(); flush_tlb_range();\" we first\n\t\t * mark the current pmd notpresent (atomically because\n\t\t * here the pmd_trans_huge and pmd_trans_splitting\n\t\t * must remain set at all times on the pmd until the\n\t\t * split is complete for this pmd), then we flush the\n\t\t * SMP TLB and finally we write the non-huge version\n\t\t * of the pmd entry with pmd_populate.\n\t\t */\n\t\tset_pmd_at(mm, address, pmd, pmd_mknotpresent(*pmd));\n\t\tflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\t\tpmd_populate(mm, pmd, pgtable);\n\t\tret = 1;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\n\treturn ret;\n}\n\n/* must be called with anon_vma->root->lock hold */\nstatic void __split_huge_page(struct page *page,\n\t\t\t      struct anon_vma *anon_vma)\n{\n\tint mapcount, mapcount2;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(!PageHead(page));\n\tBUG_ON(PageTail(page));\n\n\tmapcount = 0;\n\tlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long addr = vma_address(page, vma);\n\t\tBUG_ON(is_vma_temporary_stack(vma));\n\t\tif (addr == -EFAULT)\n\t\t\tcontinue;\n\t\tmapcount += __split_huge_page_splitting(page, vma, addr);\n\t}\n\t/*\n\t * It is critical that new vmas are added to the tail of the\n\t * anon_vma list. This guarantes that if copy_huge_pmd() runs\n\t * and establishes a child pmd before\n\t * __split_huge_page_splitting() freezes the parent pmd (so if\n\t * we fail to prevent copy_huge_pmd() from running until the\n\t * whole __split_huge_page() is complete), we will still see\n\t * the newly established pmd of the child later during the\n\t * walk, to be able to set it as pmd_trans_splitting too.\n\t */\n\tif (mapcount != page_mapcount(page))\n\t\tprintk(KERN_ERR \"mapcount %d page_mapcount %d\\n\",\n\t\t       mapcount, page_mapcount(page));\n\tBUG_ON(mapcount != page_mapcount(page));\n\n\t__split_huge_page_refcount(page);\n\n\tmapcount2 = 0;\n\tlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long addr = vma_address(page, vma);\n\t\tBUG_ON(is_vma_temporary_stack(vma));\n\t\tif (addr == -EFAULT)\n\t\t\tcontinue;\n\t\tmapcount2 += __split_huge_page_map(page, vma, addr);\n\t}\n\tif (mapcount != mapcount2)\n\t\tprintk(KERN_ERR \"mapcount %d mapcount2 %d page_mapcount %d\\n\",\n\t\t       mapcount, mapcount2, page_mapcount(page));\n\tBUG_ON(mapcount != mapcount2);\n}\n\nint split_huge_page(struct page *page)\n{\n\tstruct anon_vma *anon_vma;\n\tint ret = 1;\n\n\tBUG_ON(!PageAnon(page));\n\tanon_vma = page_lock_anon_vma(page);\n\tif (!anon_vma)\n\t\tgoto out;\n\tret = 0;\n\tif (!PageCompound(page))\n\t\tgoto out_unlock;\n\n\tBUG_ON(!PageSwapBacked(page));\n\t__split_huge_page(page, anon_vma);\n\tcount_vm_event(THP_SPLIT);\n\n\tBUG_ON(PageCompound(page));\nout_unlock:\n\tpage_unlock_anon_vma(anon_vma);\nout:\n\treturn ret;\n}\n\nint hugepage_madvise(struct vm_area_struct *vma,\n\t\t     unsigned long *vm_flags, int advice)\n{\n\tswitch (advice) {\n\tcase MADV_HUGEPAGE:\n\t\t/*\n\t\t * Be somewhat over-protective like KSM for now!\n\t\t */\n\t\tif (*vm_flags & (VM_HUGEPAGE |\n\t\t\t\t VM_SHARED   | VM_MAYSHARE   |\n\t\t\t\t VM_PFNMAP   | VM_IO      | VM_DONTEXPAND |\n\t\t\t\t VM_RESERVED | VM_HUGETLB | VM_INSERTPAGE |\n\t\t\t\t VM_MIXEDMAP | VM_SAO))\n\t\t\treturn -EINVAL;\n\t\t*vm_flags &= ~VM_NOHUGEPAGE;\n\t\t*vm_flags |= VM_HUGEPAGE;\n\t\t/*\n\t\t * If the vma become good for khugepaged to scan,\n\t\t * register it here without waiting a page fault that\n\t\t * may not happen any time soon.\n\t\t */\n\t\tif (unlikely(khugepaged_enter_vma_merge(vma)))\n\t\t\treturn -ENOMEM;\n\t\tbreak;\n\tcase MADV_NOHUGEPAGE:\n\t\t/*\n\t\t * Be somewhat over-protective like KSM for now!\n\t\t */\n\t\tif (*vm_flags & (VM_NOHUGEPAGE |\n\t\t\t\t VM_SHARED   | VM_MAYSHARE   |\n\t\t\t\t VM_PFNMAP   | VM_IO      | VM_DONTEXPAND |\n\t\t\t\t VM_RESERVED | VM_HUGETLB | VM_INSERTPAGE |\n\t\t\t\t VM_MIXEDMAP | VM_SAO))\n\t\t\treturn -EINVAL;\n\t\t*vm_flags &= ~VM_HUGEPAGE;\n\t\t*vm_flags |= VM_NOHUGEPAGE;\n\t\t/*\n\t\t * Setting VM_NOHUGEPAGE will prevent khugepaged from scanning\n\t\t * this vma even if we leave the mm registered in khugepaged if\n\t\t * it got registered before VM_NOHUGEPAGE was set.\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int __init khugepaged_slab_init(void)\n{\n\tmm_slot_cache = kmem_cache_create(\"khugepaged_mm_slot\",\n\t\t\t\t\t  sizeof(struct mm_slot),\n\t\t\t\t\t  __alignof__(struct mm_slot), 0, NULL);\n\tif (!mm_slot_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __init khugepaged_slab_free(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n\tmm_slot_cache = NULL;\n}\n\nstatic inline struct mm_slot *alloc_mm_slot(void)\n{\n\tif (!mm_slot_cache)\t/* initialization failed */\n\t\treturn NULL;\n\treturn kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);\n}\n\nstatic inline void free_mm_slot(struct mm_slot *mm_slot)\n{\n\tkmem_cache_free(mm_slot_cache, mm_slot);\n}\n\nstatic int __init mm_slots_hash_init(void)\n{\n\tmm_slots_hash = kzalloc(MM_SLOTS_HASH_HEADS * sizeof(struct hlist_head),\n\t\t\t\tGFP_KERNEL);\n\tif (!mm_slots_hash)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n#if 0\nstatic void __init mm_slots_hash_free(void)\n{\n\tkfree(mm_slots_hash);\n\tmm_slots_hash = NULL;\n}\n#endif\n\nstatic struct mm_slot *get_mm_slot(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct hlist_head *bucket;\n\tstruct hlist_node *node;\n\n\tbucket = &mm_slots_hash[((unsigned long)mm / sizeof(struct mm_struct))\n\t\t\t\t% MM_SLOTS_HASH_HEADS];\n\thlist_for_each_entry(mm_slot, node, bucket, hash) {\n\t\tif (mm == mm_slot->mm)\n\t\t\treturn mm_slot;\n\t}\n\treturn NULL;\n}\n\nstatic void insert_to_mm_slots_hash(struct mm_struct *mm,\n\t\t\t\t    struct mm_slot *mm_slot)\n{\n\tstruct hlist_head *bucket;\n\n\tbucket = &mm_slots_hash[((unsigned long)mm / sizeof(struct mm_struct))\n\t\t\t\t% MM_SLOTS_HASH_HEADS];\n\tmm_slot->mm = mm;\n\thlist_add_head(&mm_slot->hash, bucket);\n}\n\nstatic inline int khugepaged_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\nint __khugepaged_enter(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint wakeup;\n\n\tmm_slot = alloc_mm_slot();\n\tif (!mm_slot)\n\t\treturn -ENOMEM;\n\n\t/* __khugepaged_exit() must not run from under us */\n\tVM_BUG_ON(khugepaged_test_exit(mm));\n\tif (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags))) {\n\t\tfree_mm_slot(mm_slot);\n\t\treturn 0;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tinsert_to_mm_slots_hash(mm, mm_slot);\n\t/*\n\t * Insert just behind the scanning cursor, to let the area settle\n\t * down a little.\n\t */\n\twakeup = list_empty(&khugepaged_scan.mm_head);\n\tlist_add_tail(&mm_slot->mm_node, &khugepaged_scan.mm_head);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tatomic_inc(&mm->mm_count);\n\tif (wakeup)\n\t\twake_up_interruptible(&khugepaged_wait);\n\n\treturn 0;\n}\n\nint khugepaged_enter_vma_merge(struct vm_area_struct *vma)\n{\n\tunsigned long hstart, hend;\n\tif (!vma->anon_vma)\n\t\t/*\n\t\t * Not yet faulted in so we will register later in the\n\t\t * page fault if needed.\n\t\t */\n\t\treturn 0;\n\tif (vma->vm_file || vma->vm_ops)\n\t\t/* khugepaged not yet working on file or special mappings */\n\t\treturn 0;\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (hstart < hend)\n\t\treturn khugepaged_enter(vma);\n\treturn 0;\n}\n\nvoid __khugepaged_exit(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint free = 0;\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = get_mm_slot(mm);\n\tif (mm_slot && khugepaged_scan.mm_slot != mm_slot) {\n\t\thlist_del(&mm_slot->hash);\n\t\tlist_del(&mm_slot->mm_node);\n\t\tfree = 1;\n\t}\n\n\tif (free) {\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t\tclear_bit(MMF_VM_HUGEPAGE, &mm->flags);\n\t\tfree_mm_slot(mm_slot);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t\t/*\n\t\t * This is required to serialize against\n\t\t * khugepaged_test_exit() (which is guaranteed to run\n\t\t * under mmap sem read mode). Stop here (after we\n\t\t * return all pagetables will be destroyed) until\n\t\t * khugepaged has finished working on the pagetables\n\t\t * under the mmap_sem.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t} else\n\t\tspin_unlock(&khugepaged_mm_lock);\n}\n\nstatic void release_pte_page(struct page *page)\n{\n\t/* 0 stands for page_is_file_cache(page) == false */\n\tdec_zone_page_state(page, NR_ISOLATED_ANON + 0);\n\tunlock_page(page);\n\tputback_lru_page(page);\n}\n\nstatic void release_pte_pages(pte_t *pte, pte_t *_pte)\n{\n\twhile (--_pte >= pte) {\n\t\tpte_t pteval = *_pte;\n\t\tif (!pte_none(pteval))\n\t\t\trelease_pte_page(pte_page(pteval));\n\t}\n}\n\nstatic void release_all_pte_pages(pte_t *pte)\n{\n\trelease_pte_pages(pte, pte + HPAGE_PMD_NR);\n}\n\nstatic int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t *pte)\n{\n\tstruct page *page;\n\tpte_t *_pte;\n\tint referenced = 0, isolated = 0, none = 0;\n\tfor (_pte = pte; _pte < pte+HPAGE_PMD_NR;\n\t     _pte++, address += PAGE_SIZE) {\n\t\tpte_t pteval = *_pte;\n\t\tif (pte_none(pteval)) {\n\t\t\tif (++none <= khugepaged_max_ptes_none)\n\t\t\t\tcontinue;\n\t\t\telse {\n\t\t\t\trelease_pte_pages(pte, _pte);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (!pte_present(pteval) || !pte_write(pteval)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\tpage = vm_normal_page(vma, address, pteval);\n\t\tif (unlikely(!page)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\tVM_BUG_ON(PageCompound(page));\n\t\tBUG_ON(!PageAnon(page));\n\t\tVM_BUG_ON(!PageSwapBacked(page));\n\n\t\t/* cannot use mapcount: can't collapse if there's a gup pin */\n\t\tif (page_count(page) != 1) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * We can do it before isolate_lru_page because the\n\t\t * page can't be freed from under us. NOTE: PG_lock\n\t\t * is needed to serialize against split_huge_page\n\t\t * when invoked from the VM.\n\t\t */\n\t\tif (!trylock_page(page)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Isolate the page to avoid collapsing an hugepage\n\t\t * currently in use by the VM.\n\t\t */\n\t\tif (isolate_lru_page(page)) {\n\t\t\tunlock_page(page);\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/* 0 stands for page_is_file_cache(page) == false */\n\t\tinc_zone_page_state(page, NR_ISOLATED_ANON + 0);\n\t\tVM_BUG_ON(!PageLocked(page));\n\t\tVM_BUG_ON(PageLRU(page));\n\n\t\t/* If there is no mapped pte young don't collapse the page */\n\t\tif (pte_young(pteval) || PageReferenced(page) ||\n\t\t    mmu_notifier_test_young(vma->vm_mm, address))\n\t\t\treferenced = 1;\n\t}\n\tif (unlikely(!referenced))\n\t\trelease_all_pte_pages(pte);\n\telse\n\t\tisolated = 1;\nout:\n\treturn isolated;\n}\n\nstatic void __collapse_huge_page_copy(pte_t *pte, struct page *page,\n\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address,\n\t\t\t\t      spinlock_t *ptl)\n{\n\tpte_t *_pte;\n\tfor (_pte = pte; _pte < pte+HPAGE_PMD_NR; _pte++) {\n\t\tpte_t pteval = *_pte;\n\t\tstruct page *src_page;\n\n\t\tif (pte_none(pteval)) {\n\t\t\tclear_user_highpage(page, address);\n\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);\n\t\t} else {\n\t\t\tsrc_page = pte_page(pteval);\n\t\t\tcopy_user_highpage(page, src_page, address, vma);\n\t\t\tVM_BUG_ON(page_mapcount(src_page) != 1);\n\t\t\tVM_BUG_ON(page_count(src_page) != 2);\n\t\t\trelease_pte_page(src_page);\n\t\t\t/*\n\t\t\t * ptl mostly unnecessary, but preempt has to\n\t\t\t * be disabled to update the per-cpu stats\n\t\t\t * inside page_remove_rmap().\n\t\t\t */\n\t\t\tspin_lock(ptl);\n\t\t\t/*\n\t\t\t * paravirt calls inside pte_clear here are\n\t\t\t * superfluous.\n\t\t\t */\n\t\t\tpte_clear(vma->vm_mm, address, _pte);\n\t\t\tpage_remove_rmap(src_page);\n\t\t\tspin_unlock(ptl);\n\t\t\tfree_page_and_swap_cache(src_page);\n\t\t}\n\n\t\taddress += PAGE_SIZE;\n\t\tpage++;\n\t}\n}\n\nstatic void collapse_huge_page(struct mm_struct *mm,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage,\n\t\t\t       struct vm_area_struct *vma,\n\t\t\t       int node)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpgtable_t pgtable;\n\tstruct page *new_page;\n\tspinlock_t *ptl;\n\tint isolated;\n\tunsigned long hstart, hend;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n#ifndef CONFIG_NUMA\n\tVM_BUG_ON(!*hpage);\n\tnew_page = *hpage;\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\treturn;\n\t}\n#else\n\tVM_BUG_ON(*hpage);\n\t/*\n\t * Allocate the page while the vma is still valid and under\n\t * the mmap_sem read mode so there is no memory allocation\n\t * later when we take the mmap_sem in write mode. This is more\n\t * friendly behavior (OTOH it may actually hide bugs) to\n\t * filesystems in userland with daemons allocating memory in\n\t * the userland I/O paths.  Allocating memory with the\n\t * mmap_sem in read mode is good idea also to allow greater\n\t * scalability.\n\t */\n\tnew_page = alloc_hugepage_vma(khugepaged_defrag(), vma, address,\n\t\t\t\t      node, __GFP_OTHER_NODE);\n\tif (unlikely(!new_page)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t*hpage = ERR_PTR(-ENOMEM);\n\t\treturn;\n\t}\n\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\tput_page(new_page);\n\t\treturn;\n\t}\n#endif\n\n\t/* after allocating the hugepage upgrade to mmap_sem write mode */\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Prevent all access to pagetables with the exception of\n\t * gup_fast later hanlded by the ptep_clear_flush and the VM\n\t * handled by the anon_vma lock + PG_lock.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tgoto out;\n\n\tvma = find_vma(mm, address);\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (address < hstart || address + HPAGE_PMD_SIZE > hend)\n\t\tgoto out;\n\n\tif ((!(vma->vm_flags & VM_HUGEPAGE) && !khugepaged_always()) ||\n\t    (vma->vm_flags & VM_NOHUGEPAGE))\n\t\tgoto out;\n\n\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\tgoto out;\n\tif (is_vma_temporary_stack(vma))\n\t\tgoto out;\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\t/* pmd can't go away or become huge under us */\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tanon_vma_lock(vma->anon_vma);\n\n\tpte = pte_offset_map(pmd, address);\n\tptl = pte_lockptr(mm, pmd);\n\n\tspin_lock(&mm->page_table_lock); /* probably unnecessary */\n\t/*\n\t * After this gup_fast can't run anymore. This also removes\n\t * any huge TLB entry from the CPU so we won't allow\n\t * huge and small TLB entries for the same virtual address\n\t * to avoid the risk of CPU bugs in that area.\n\t */\n\t_pmd = pmdp_clear_flush_notify(vma, address, pmd);\n\tspin_unlock(&mm->page_table_lock);\n\n\tspin_lock(ptl);\n\tisolated = __collapse_huge_page_isolate(vma, address, pte);\n\tspin_unlock(ptl);\n\n\tif (unlikely(!isolated)) {\n\t\tpte_unmap(pte);\n\t\tspin_lock(&mm->page_table_lock);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\tset_pmd_at(mm, address, pmd, _pmd);\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock(vma->anon_vma);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * All pages are isolated and locked so anon_vma rmap\n\t * can't run anymore.\n\t */\n\tanon_vma_unlock(vma->anon_vma);\n\n\t__collapse_huge_page_copy(pte, new_page, vma, address, ptl);\n\tpte_unmap(pte);\n\t__SetPageUptodate(new_page);\n\tpgtable = pmd_pgtable(_pmd);\n\tVM_BUG_ON(page_count(pgtable) != 1);\n\tVM_BUG_ON(page_mapcount(pgtable) != 0);\n\n\t_pmd = mk_pmd(new_page, vma->vm_page_prot);\n\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);\n\t_pmd = pmd_mkhuge(_pmd);\n\n\t/*\n\t * spin_lock() below is not the equivalent of smp_wmb(), so\n\t * this is needed to avoid the copy_huge_page writes to become\n\t * visible after the set_pmd_at() write.\n\t */\n\tsmp_wmb();\n\n\tspin_lock(&mm->page_table_lock);\n\tBUG_ON(!pmd_none(*pmd));\n\tpage_add_new_anon_rmap(new_page, vma, address);\n\tset_pmd_at(mm, address, pmd, _pmd);\n\tupdate_mmu_cache(vma, address, entry);\n\tprepare_pmd_huge_pte(pgtable, mm);\n\tmm->nr_ptes--;\n\tspin_unlock(&mm->page_table_lock);\n\n#ifndef CONFIG_NUMA\n\t*hpage = NULL;\n#endif\n\tkhugepaged_pages_collapsed++;\nout_up_write:\n\tup_write(&mm->mmap_sem);\n\treturn;\n\nout:\n\tmem_cgroup_uncharge_page(new_page);\n#ifdef CONFIG_NUMA\n\tput_page(new_page);\n#endif\n\tgoto out_up_write;\n}\n\nstatic int khugepaged_scan_pmd(struct mm_struct *mm,\n\t\t\t       struct vm_area_struct *vma,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte, *_pte;\n\tint ret = 0, referenced = 0, none = 0;\n\tstruct page *page;\n\tunsigned long _address;\n\tspinlock_t *ptl;\n\tint node = -1;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tfor (_address = address, _pte = pte; _pte < pte+HPAGE_PMD_NR;\n\t     _pte++, _address += PAGE_SIZE) {\n\t\tpte_t pteval = *_pte;\n\t\tif (pte_none(pteval)) {\n\t\t\tif (++none <= khugepaged_max_ptes_none)\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\tgoto out_unmap;\n\t\t}\n\t\tif (!pte_present(pteval) || !pte_write(pteval))\n\t\t\tgoto out_unmap;\n\t\tpage = vm_normal_page(vma, _address, pteval);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_unmap;\n\t\t/*\n\t\t * Chose the node of the first page. This could\n\t\t * be more sophisticated and look at more pages,\n\t\t * but isn't for now.\n\t\t */\n\t\tif (node == -1)\n\t\t\tnode = page_to_nid(page);\n\t\tVM_BUG_ON(PageCompound(page));\n\t\tif (!PageLRU(page) || PageLocked(page) || !PageAnon(page))\n\t\t\tgoto out_unmap;\n\t\t/* cannot use mapcount: can't collapse if there's a gup pin */\n\t\tif (page_count(page) != 1)\n\t\t\tgoto out_unmap;\n\t\tif (pte_young(pteval) || PageReferenced(page) ||\n\t\t    mmu_notifier_test_young(vma->vm_mm, address))\n\t\t\treferenced = 1;\n\t}\n\tif (referenced)\n\t\tret = 1;\nout_unmap:\n\tpte_unmap_unlock(pte, ptl);\n\tif (ret)\n\t\t/* collapse_huge_page will return with the mmap_sem released */\n\t\tcollapse_huge_page(mm, address, hpage, vma, node);\nout:\n\treturn ret;\n}\n\nstatic void collect_mm_slot(struct mm_slot *mm_slot)\n{\n\tstruct mm_struct *mm = mm_slot->mm;\n\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_test_exit(mm)) {\n\t\t/* free mm_slot */\n\t\thlist_del(&mm_slot->hash);\n\t\tlist_del(&mm_slot->mm_node);\n\n\t\t/*\n\t\t * Not strictly needed because the mm exited already.\n\t\t *\n\t\t * clear_bit(MMF_VM_HUGEPAGE, &mm->flags);\n\t\t */\n\n\t\t/* khugepaged_mm_lock actually not necessary for the below */\n\t\tfree_mm_slot(mm_slot);\n\t\tmmdrop(mm);\n\t}\n}\n\nstatic unsigned int khugepaged_scan_mm_slot(unsigned int pages,\n\t\t\t\t\t    struct page **hpage)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint progress = 0;\n\n\tVM_BUG_ON(!pages);\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_scan.mm_slot)\n\t\tmm_slot = khugepaged_scan.mm_slot;\n\telse {\n\t\tmm_slot = list_entry(khugepaged_scan.mm_head.next,\n\t\t\t\t     struct mm_slot, mm_node);\n\t\tkhugepaged_scan.address = 0;\n\t\tkhugepaged_scan.mm_slot = mm_slot;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmm = mm_slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, khugepaged_scan.address);\n\n\tprogress++;\n\tfor (; vma; vma = vma->vm_next) {\n\t\tunsigned long hstart, hend;\n\n\t\tcond_resched();\n\t\tif (unlikely(khugepaged_test_exit(mm))) {\n\t\t\tprogress++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((!(vma->vm_flags & VM_HUGEPAGE) &&\n\t\t     !khugepaged_always()) ||\n\t\t    (vma->vm_flags & VM_NOHUGEPAGE)) {\n\t\tskip:\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\t/* VM_PFNMAP vmas may have vm_ops null but vm_file set */\n\t\tif (!vma->anon_vma || vma->vm_ops || vma->vm_file)\n\t\t\tgoto skip;\n\t\tif (is_vma_temporary_stack(vma))\n\t\t\tgoto skip;\n\n\t\tVM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));\n\n\t\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\t\thend = vma->vm_end & HPAGE_PMD_MASK;\n\t\tif (hstart >= hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address > hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address < hstart)\n\t\t\tkhugepaged_scan.address = hstart;\n\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n\n\t\twhile (khugepaged_scan.address < hend) {\n\t\t\tint ret;\n\t\t\tcond_resched();\n\t\t\tif (unlikely(khugepaged_test_exit(mm)))\n\t\t\t\tgoto breakouterloop;\n\n\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||\n\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >\n\t\t\t\t  hend);\n\t\t\tret = khugepaged_scan_pmd(mm, vma,\n\t\t\t\t\t\t  khugepaged_scan.address,\n\t\t\t\t\t\t  hpage);\n\t\t\t/* move to next address */\n\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;\n\t\t\tprogress += HPAGE_PMD_NR;\n\t\t\tif (ret)\n\t\t\t\t/* we released mmap_sem so break loop */\n\t\t\t\tgoto breakouterloop_mmap_sem;\n\t\t\tif (progress >= pages)\n\t\t\t\tgoto breakouterloop;\n\t\t}\n\t}\nbreakouterloop:\n\tup_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */\nbreakouterloop_mmap_sem:\n\n\tspin_lock(&khugepaged_mm_lock);\n\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);\n\t/*\n\t * Release the current mm_slot if this mm is about to die, or\n\t * if we scanned all vmas of this mm.\n\t */\n\tif (khugepaged_test_exit(mm) || !vma) {\n\t\t/*\n\t\t * Make sure that if mm_users is reaching zero while\n\t\t * khugepaged runs here, khugepaged_exit will find\n\t\t * mm_slot not pointing to the exiting mm.\n\t\t */\n\t\tif (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {\n\t\t\tkhugepaged_scan.mm_slot = list_entry(\n\t\t\t\tmm_slot->mm_node.next,\n\t\t\t\tstruct mm_slot, mm_node);\n\t\t\tkhugepaged_scan.address = 0;\n\t\t} else {\n\t\t\tkhugepaged_scan.mm_slot = NULL;\n\t\t\tkhugepaged_full_scans++;\n\t\t}\n\n\t\tcollect_mm_slot(mm_slot);\n\t}\n\n\treturn progress;\n}\n\nstatic int khugepaged_has_work(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) &&\n\t\tkhugepaged_enabled();\n}\n\nstatic int khugepaged_wait_event(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) ||\n\t\t!khugepaged_enabled();\n}\n\nstatic void khugepaged_do_scan(struct page **hpage)\n{\n\tunsigned int progress = 0, pass_through_head = 0;\n\tunsigned int pages = khugepaged_pages_to_scan;\n\n\tbarrier(); /* write khugepaged_pages_to_scan to local stack */\n\n\twhile (progress < pages) {\n\t\tcond_resched();\n\n#ifndef CONFIG_NUMA\n\t\tif (!*hpage) {\n\t\t\t*hpage = alloc_hugepage(khugepaged_defrag());\n\t\t\tif (unlikely(!*hpage)) {\n\t\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\t\t}\n#else\n\t\tif (IS_ERR(*hpage))\n\t\t\tbreak;\n#endif\n\n\t\tif (unlikely(kthread_should_stop() || freezing(current)))\n\t\t\tbreak;\n\n\t\tspin_lock(&khugepaged_mm_lock);\n\t\tif (!khugepaged_scan.mm_slot)\n\t\t\tpass_through_head++;\n\t\tif (khugepaged_has_work() &&\n\t\t    pass_through_head < 2)\n\t\t\tprogress += khugepaged_scan_mm_slot(pages - progress,\n\t\t\t\t\t\t\t    hpage);\n\t\telse\n\t\t\tprogress = pages;\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t}\n}\n\nstatic void khugepaged_alloc_sleep(void)\n{\n\tDEFINE_WAIT(wait);\n\tadd_wait_queue(&khugepaged_wait, &wait);\n\tschedule_timeout_interruptible(\n\t\tmsecs_to_jiffies(\n\t\t\tkhugepaged_alloc_sleep_millisecs));\n\tremove_wait_queue(&khugepaged_wait, &wait);\n}\n\n#ifndef CONFIG_NUMA\nstatic struct page *khugepaged_alloc_hugepage(void)\n{\n\tstruct page *hpage;\n\n\tdo {\n\t\thpage = alloc_hugepage(khugepaged_defrag());\n\t\tif (!hpage) {\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\tkhugepaged_alloc_sleep();\n\t\t} else\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\t} while (unlikely(!hpage) &&\n\t\t likely(khugepaged_enabled()));\n\treturn hpage;\n}\n#endif\n\nstatic void khugepaged_loop(void)\n{\n\tstruct page *hpage;\n\n#ifdef CONFIG_NUMA\n\thpage = NULL;\n#endif\n\twhile (likely(khugepaged_enabled())) {\n#ifndef CONFIG_NUMA\n\t\thpage = khugepaged_alloc_hugepage();\n\t\tif (unlikely(!hpage)) {\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\tbreak;\n\t\t}\n\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n#else\n\t\tif (IS_ERR(hpage)) {\n\t\t\tkhugepaged_alloc_sleep();\n\t\t\thpage = NULL;\n\t\t}\n#endif\n\n\t\tkhugepaged_do_scan(&hpage);\n#ifndef CONFIG_NUMA\n\t\tif (hpage)\n\t\t\tput_page(hpage);\n#endif\n\t\ttry_to_freeze();\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t\tif (khugepaged_has_work()) {\n\t\t\tDEFINE_WAIT(wait);\n\t\t\tif (!khugepaged_scan_sleep_millisecs)\n\t\t\t\tcontinue;\n\t\t\tadd_wait_queue(&khugepaged_wait, &wait);\n\t\t\tschedule_timeout_interruptible(\n\t\t\t\tmsecs_to_jiffies(\n\t\t\t\t\tkhugepaged_scan_sleep_millisecs));\n\t\t\tremove_wait_queue(&khugepaged_wait, &wait);\n\t\t} else if (khugepaged_enabled())\n\t\t\twait_event_freezable(khugepaged_wait,\n\t\t\t\t\t     khugepaged_wait_event());\n\t}\n}\n\nstatic int khugepaged(void *none)\n{\n\tstruct mm_slot *mm_slot;\n\n\tset_freezable();\n\tset_user_nice(current, 19);\n\n\t/* serialize with start_khugepaged() */\n\tmutex_lock(&khugepaged_mutex);\n\n\tfor (;;) {\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\t\tkhugepaged_loop();\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_enabled())\n\t\t\tbreak;\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = khugepaged_scan.mm_slot;\n\tkhugepaged_scan.mm_slot = NULL;\n\tif (mm_slot)\n\t\tcollect_mm_slot(mm_slot);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tkhugepaged_thread = NULL;\n\tmutex_unlock(&khugepaged_mutex);\n\n\treturn 0;\n}\n\nvoid __split_huge_page_pmd(struct mm_struct *mm, pmd_t *pmd)\n{\n\tstruct page *page;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(&mm->page_table_lock);\n\t\treturn;\n\t}\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON(!page_count(page));\n\tget_page(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tsplit_huge_page(page);\n\n\tput_page(page);\n\tBUG_ON(pmd_trans_huge(*pmd));\n}\n\nstatic void split_huge_page_address(struct mm_struct *mm,\n\t\t\t\t    unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tVM_BUG_ON(!(address & ~HPAGE_PMD_MASK));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\treturn;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\treturn;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn;\n\t/*\n\t * Caller holds the mmap_sem write mode, so a huge pmd cannot\n\t * materialize from under us.\n\t */\n\tsplit_huge_page_pmd(mm, pmd);\n}\n\nvoid __vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t     unsigned long start,\n\t\t\t     unsigned long end,\n\t\t\t     long adjust_next)\n{\n\t/*\n\t * If the new start address isn't hpage aligned and it could\n\t * previously contain an hugepage: check if we need to split\n\t * an huge pmd.\n\t */\n\tif (start & ~HPAGE_PMD_MASK &&\n\t    (start & HPAGE_PMD_MASK) >= vma->vm_start &&\n\t    (start & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)\n\t\tsplit_huge_page_address(vma->vm_mm, start);\n\n\t/*\n\t * If the new end address isn't hpage aligned and it could\n\t * previously contain an hugepage: check if we need to split\n\t * an huge pmd.\n\t */\n\tif (end & ~HPAGE_PMD_MASK &&\n\t    (end & HPAGE_PMD_MASK) >= vma->vm_start &&\n\t    (end & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)\n\t\tsplit_huge_page_address(vma->vm_mm, end);\n\n\t/*\n\t * If we're also updating the vma->vm_next->vm_start, if the new\n\t * vm_next->vm_start isn't page aligned and it could previously\n\t * contain an hugepage: check if we need to split an huge pmd.\n\t */\n\tif (adjust_next > 0) {\n\t\tstruct vm_area_struct *next = vma->vm_next;\n\t\tunsigned long nstart = next->vm_start;\n\t\tnstart += adjust_next << PAGE_SHIFT;\n\t\tif (nstart & ~HPAGE_PMD_MASK &&\n\t\t    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&\n\t\t    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)\n\t\t\tsplit_huge_page_address(next->vm_mm, nstart);\n\t}\n}\n"], "fixing_code": ["#ifndef _LINUX_HUGE_MM_H\n#define _LINUX_HUGE_MM_H\n\nextern int do_huge_pmd_anonymous_page(struct mm_struct *mm,\n\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address, pmd_t *pmd,\n\t\t\t\t      unsigned int flags);\nextern int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t\t pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t\t\t struct vm_area_struct *vma);\nextern int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t       unsigned long address, pmd_t *pmd,\n\t\t\t       pmd_t orig_pmd);\nextern pgtable_t get_pmd_huge_pte(struct mm_struct *mm);\nextern struct page *follow_trans_huge_pmd(struct mm_struct *mm,\n\t\t\t\t\t  unsigned long addr,\n\t\t\t\t\t  pmd_t *pmd,\n\t\t\t\t\t  unsigned int flags);\nextern int zap_huge_pmd(struct mmu_gather *tlb,\n\t\t\tstruct vm_area_struct *vma,\n\t\t\tpmd_t *pmd);\nextern int mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\t\tunsigned long addr, unsigned long end,\n\t\t\tunsigned char *vec);\nextern int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\t\tunsigned long addr, pgprot_t newprot);\n\nenum transparent_hugepage_flag {\n\tTRANSPARENT_HUGEPAGE_FLAG,\n\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,\n#ifdef CONFIG_DEBUG_VM\n\tTRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG,\n#endif\n};\n\nenum page_check_address_pmd_flag {\n\tPAGE_CHECK_ADDRESS_PMD_FLAG,\n\tPAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG,\n\tPAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG,\n};\nextern pmd_t *page_check_address_pmd(struct page *page,\n\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t     unsigned long address,\n\t\t\t\t     enum page_check_address_pmd_flag flag);\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define HPAGE_PMD_SHIFT HPAGE_SHIFT\n#define HPAGE_PMD_MASK HPAGE_MASK\n#define HPAGE_PMD_SIZE HPAGE_SIZE\n\n#define transparent_hugepage_enabled(__vma)\t\t\t\t\\\n\t((transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_FLAG) ||\t\t\t\t\\\n\t  (transparent_hugepage_flags &\t\t\t\t\t\\\n\t   (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG) &&\t\t\t\\\n\t   ((__vma)->vm_flags & VM_HUGEPAGE))) &&\t\t\t\\\n\t !((__vma)->vm_flags & VM_NOHUGEPAGE) &&\t\t\t\\\n\t !is_vma_temporary_stack(__vma))\n#define transparent_hugepage_defrag(__vma)\t\t\t\t\\\n\t((transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)) ||\t\t\t\\\n\t (transparent_hugepage_flags &\t\t\t\t\t\\\n\t  (1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG) &&\t\t\\\n\t  (__vma)->vm_flags & VM_HUGEPAGE))\n#ifdef CONFIG_DEBUG_VM\n#define transparent_hugepage_debug_cow()\t\t\t\t\\\n\t(transparent_hugepage_flags &\t\t\t\t\t\\\n\t (1<<TRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG))\n#else /* CONFIG_DEBUG_VM */\n#define transparent_hugepage_debug_cow() 0\n#endif /* CONFIG_DEBUG_VM */\n\nextern unsigned long transparent_hugepage_flags;\nextern int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t\t  pmd_t *dst_pmd, pmd_t *src_pmd,\n\t\t\t  struct vm_area_struct *vma,\n\t\t\t  unsigned long addr, unsigned long end);\nextern int handle_pte_fault(struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pte_t *pte, pmd_t *pmd, unsigned int flags);\nextern int split_huge_page(struct page *page);\nextern void __split_huge_page_pmd(struct mm_struct *mm, pmd_t *pmd);\n#define split_huge_page_pmd(__mm, __pmd)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpmd_t *____pmd = (__pmd);\t\t\t\t\\\n\t\tif (unlikely(pmd_trans_huge(*____pmd)))\t\t\t\\\n\t\t\t__split_huge_page_pmd(__mm, ____pmd);\t\t\\\n\t}  while (0)\n#define wait_split_huge_page(__anon_vma, __pmd)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpmd_t *____pmd = (__pmd);\t\t\t\t\\\n\t\tspin_unlock_wait(&(__anon_vma)->root->lock);\t\t\\\n\t\t/*\t\t\t\t\t\t\t\\\n\t\t * spin_unlock_wait() is just a loop in C and so the\t\\\n\t\t * CPU can reorder anything around it.\t\t\t\\\n\t\t */\t\t\t\t\t\t\t\\\n\t\tsmp_mb();\t\t\t\t\t\t\\\n\t\tBUG_ON(pmd_trans_splitting(*____pmd) ||\t\t\t\\\n\t\t       pmd_trans_huge(*____pmd));\t\t\t\\\n\t} while (0)\n#define HPAGE_PMD_ORDER (HPAGE_PMD_SHIFT-PAGE_SHIFT)\n#define HPAGE_PMD_NR (1<<HPAGE_PMD_ORDER)\n#if HPAGE_PMD_ORDER > MAX_ORDER\n#error \"hugepages can't be allocated by the buddy allocator\"\n#endif\nextern int hugepage_madvise(struct vm_area_struct *vma,\n\t\t\t    unsigned long *vm_flags, int advice);\nextern void __vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end,\n\t\t\t\t    long adjust_next);\nstatic inline void vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long end,\n\t\t\t\t\t long adjust_next)\n{\n\tif (!vma->anon_vma || vma->vm_ops)\n\t\treturn;\n\t__vma_adjust_trans_huge(vma, start, end, adjust_next);\n}\nstatic inline int hpage_nr_pages(struct page *page)\n{\n\tif (unlikely(PageTransHuge(page)))\n\t\treturn HPAGE_PMD_NR;\n\treturn 1;\n}\nstatic inline struct page *compound_trans_head(struct page *page)\n{\n\tif (PageTail(page)) {\n\t\tstruct page *head;\n\t\thead = page->first_page;\n\t\tsmp_rmb();\n\t\t/*\n\t\t * head may be a dangling pointer.\n\t\t * __split_huge_page_refcount clears PageTail before\n\t\t * overwriting first_page, so if PageTail is still\n\t\t * there it means the head pointer isn't dangling.\n\t\t */\n\t\tif (PageTail(page))\n\t\t\treturn head;\n\t}\n\treturn page;\n}\n#else /* CONFIG_TRANSPARENT_HUGEPAGE */\n#define HPAGE_PMD_SHIFT ({ BUG(); 0; })\n#define HPAGE_PMD_MASK ({ BUG(); 0; })\n#define HPAGE_PMD_SIZE ({ BUG(); 0; })\n\n#define hpage_nr_pages(x) 1\n\n#define transparent_hugepage_enabled(__vma) 0\n\n#define transparent_hugepage_flags 0UL\nstatic inline int split_huge_page(struct page *page)\n{\n\treturn 0;\n}\n#define split_huge_page_pmd(__mm, __pmd)\t\\\n\tdo { } while (0)\n#define wait_split_huge_page(__anon_vma, __pmd)\t\\\n\tdo { } while (0)\n#define compound_trans_head(page) compound_head(page)\nstatic inline int hugepage_madvise(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long *vm_flags, int advice)\n{\n\tBUG();\n\treturn 0;\n}\nstatic inline void vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long end,\n\t\t\t\t\t long adjust_next)\n{\n}\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE */\n\n#endif /* _LINUX_HUGE_MM_H */\n", "#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n\n#ifdef __KERNEL__\n\n#include <linux/gfp.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/prio_tree.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/bit_spinlock.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct file_ra_state;\nstruct user_struct;\nstruct writeback_control;\n\n#ifndef CONFIG_DISCONTIGMEM          /* Don't use mapnrs, do it properly */\nextern unsigned long max_mapnr;\n#endif\n\nextern unsigned long num_physpages;\nextern unsigned long totalram_pages;\nextern void * high_memory;\nextern int page_cluster;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <asm/processor.h>\n\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n\n/* to align the pointer to the (next) page boundary */\n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n/*\n * Linux kernel virtual memory manager primitives.\n * The idea being to have a \"virtual\" mm in the same way\n * we have a virtual fs - giving a cleaner interface to the\n * mm details, and allowing different kinds of memory mappings\n * (from shared memory to executable loading to arbitrary\n * mmap() functions).\n */\n\nextern struct kmem_cache *vm_area_cachep;\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n/*\n * vm_flags in vm_area_struct, see mm_types.h.\n */\n#define VM_READ\t\t0x00000001\t/* currently active flags */\n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n/* mprotect() hardcodes VM_MAYREAD >> 4 == VM_READ, and so for r/w/x bits. */\n#define VM_MAYREAD\t0x00000010\t/* limits for mprotect() etc */\n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t/* general info on the segment */\n#if defined(CONFIG_STACK_GROWSUP) || defined(CONFIG_IA64)\n#define VM_GROWSUP\t0x00000200\n#else\n#define VM_GROWSUP\t0x00000000\n#define VM_NOHUGEPAGE\t0x00000200\t/* MADV_NOHUGEPAGE marked this vma */\n#endif\n#define VM_PFNMAP\t0x00000400\t/* Page-ranges managed without \"struct page\", just pure PFN */\n#define VM_DENYWRITE\t0x00000800\t/* ETXTBSY on write attempts.. */\n\n#define VM_EXECUTABLE\t0x00001000\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t/* Memory mapped I/O or similar */\n\n\t\t\t\t\t/* Used by sys_madvise() */\n#define VM_SEQ_READ\t0x00008000\t/* App will access data sequentially */\n#define VM_RAND_READ\t0x00010000\t/* App will not benefit from clustered reads */\n\n#define VM_DONTCOPY\t0x00020000      /* Do not copy this vma on fork */\n#define VM_DONTEXPAND\t0x00040000\t/* Cannot expand with mremap() */\n#define VM_RESERVED\t0x00080000\t/* Count as reserved_vm like IO */\n#define VM_ACCOUNT\t0x00100000\t/* Is a VM accounted object */\n#define VM_NORESERVE\t0x00200000\t/* should the VM suppress accounting */\n#define VM_HUGETLB\t0x00400000\t/* Huge TLB Page VM */\n#define VM_NONLINEAR\t0x00800000\t/* Is non-linear (remap_file_pages) */\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\n#define VM_MAPPED_COPY\t0x01000000\t/* T if mapped copy of data (nommu mmap) */\n#else\n#define VM_HUGEPAGE\t0x01000000\t/* MADV_HUGEPAGE marked this vma */\n#endif\n#define VM_INSERTPAGE\t0x02000000\t/* The vma has had \"vm_insert_page()\" done on it */\n#define VM_ALWAYSDUMP\t0x04000000\t/* Always include in core dumps */\n\n#define VM_CAN_NONLINEAR 0x08000000\t/* Has ->fault & does nonlinear pages */\n#define VM_MIXEDMAP\t0x10000000\t/* Can contain \"struct page\" and pure PFN pages */\n#define VM_SAO\t\t0x20000000\t/* Strong Access Ordering (powerpc) */\n#define VM_PFN_AT_MMAP\t0x40000000\t/* PFNMAP vma that is fully mapped at mmap time */\n#define VM_MERGEABLE\t0x80000000\t/* KSM may merge identical pages */\n\n/* Bits set in the VMA until the stack is in its final location */\n#define VM_STACK_INCOMPLETE_SETUP\t(VM_RAND_READ | VM_SEQ_READ)\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t/* arch can override this */\n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK_FLAGS\t(VM_GROWSUP | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n#else\n#define VM_STACK_FLAGS\t(VM_GROWSDOWN | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n#endif\n\n#define VM_READHINTMASK\t\t\t(VM_SEQ_READ | VM_RAND_READ)\n#define VM_ClearReadHint(v)\t\t(v)->vm_flags &= ~VM_READHINTMASK\n#define VM_NormalReadHint(v)\t\t(!((v)->vm_flags & VM_READHINTMASK))\n#define VM_SequentialReadHint(v)\t((v)->vm_flags & VM_SEQ_READ)\n#define VM_RandomReadHint(v)\t\t((v)->vm_flags & VM_RAND_READ)\n\n/*\n * Special vmas that are non-mergable, non-mlock()able.\n * Note: mm/huge_memory.c VM_NO_THP depends on this definition.\n */\n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_RESERVED | VM_PFNMAP)\n\n/*\n * mapping from the currently active vm_flags protection bits (the\n * low four bits) to a page protection mask..\n */\nextern pgprot_t protection_map[16];\n\n#define FAULT_FLAG_WRITE\t0x01\t/* Fault was a write access */\n#define FAULT_FLAG_NONLINEAR\t0x02\t/* Fault was via a nonlinear mapping */\n#define FAULT_FLAG_MKWRITE\t0x04\t/* Fault was mkwrite of existing pte */\n#define FAULT_FLAG_ALLOW_RETRY\t0x08\t/* Retry fault if blocking */\n#define FAULT_FLAG_RETRY_NOWAIT\t0x10\t/* Don't drop mmap_sem and wait when retrying */\n\n/*\n * This interface is used by x86 PAT code to identify a pfn mapping that is\n * linear over entire vma. This is to optimize PAT code that deals with\n * marking the physical region with a particular prot. This is not for generic\n * mm use. Note also that this check will not work if the pfn mapping is\n * linear for a vma starting at physical address 0. In which case PAT code\n * falls back to slow path of reserving physical range page by page.\n */\nstatic inline int is_linear_pfn_mapping(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & VM_PFN_AT_MMAP);\n}\n\nstatic inline int is_pfn_mapping(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & VM_PFNMAP);\n}\n\n/*\n * vm_fault is filled by the the pagefault handler and passed to the vma's\n * ->fault function. The vma's ->fault is responsible for returning a bitmask\n * of VM_FAULT_xxx flags that give details about how the fault was handled.\n *\n * pgoff should be used in favour of virtual_address, if possible. If pgoff\n * is used, one may set VM_CAN_NONLINEAR in the vma->vm_flags to get nonlinear\n * mapping support.\n */\nstruct vm_fault {\n\tunsigned int flags;\t\t/* FAULT_FLAG_xxx flags */\n\tpgoff_t pgoff;\t\t\t/* Logical page offset based on vma */\n\tvoid __user *virtual_address;\t/* Faulting virtual address */\n\n\tstruct page *page;\t\t/* ->fault handlers should return a\n\t\t\t\t\t * page here, unless VM_FAULT_NOPAGE\n\t\t\t\t\t * is set (which is also implied by\n\t\t\t\t\t * VM_FAULT_ERROR).\n\t\t\t\t\t */\n};\n\n/*\n * These are the virtual MM functions - opening of an area, closing and\n * unmapping it (needed to keep files on disk up-to-date etc), pointer\n * to the functions called when a no-page or a wp-page exception occurs. \n */\nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\tvoid (*close)(struct vm_area_struct * area);\n\tint (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* notification that a previously read-only page is about to become\n\t * writable, if an error is returned it will cause a SIGBUS */\n\tint (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n\n\t/* called by access_process_vm when get_user_pages() fails, typically\n\t * for use by special VMAs that can switch between memory and hardware\n\t */\n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n#ifdef CONFIG_NUMA\n\t/*\n\t * set_policy() op must add a reference to any non-NULL @new mempolicy\n\t * to hold the policy upon return.  Caller should pass NULL @new to\n\t * remove a policy and fall back to surrounding context--i.e. do not\n\t * install a MPOL_DEFAULT policy, nor the task or system default\n\t * mempolicy.\n\t */\n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t/*\n\t * get_policy() op must add reference [mpol_get()] to any policy at\n\t * (vma,addr) marked as MPOL_SHARED.  The shared policy infrastructure\n\t * in mm/mempolicy.c will do this automatically.\n\t * get_policy() must NOT add a ref if the policy at (vma,addr) is not\n\t * marked as MPOL_SHARED. vma policies are protected by the mmap_sem.\n\t * If no [shared/vma] mempolicy exists at the addr, get_policy() op\n\t * must return NULL--i.e., do not \"fallback\" to task or system default\n\t * policy.\n\t */\n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n\tint (*migrate)(struct vm_area_struct *vma, const nodemask_t *from,\n\t\tconst nodemask_t *to, unsigned long flags);\n#endif\n};\n\nstruct mmu_gather;\nstruct inode;\n\n#define page_private(page)\t\t((page)->private)\n#define set_page_private(page, v)\t((page)->private = (v))\n\n/*\n * FIXME: take this include out, include page-flags.h in\n * files which need it (119 of them)\n */\n#include <linux/page-flags.h>\n#include <linux/huge_mm.h>\n\n/*\n * Methods to modify the page usage count.\n *\n * What counts for a page usage:\n * - cache mapping   (page->mapping)\n * - private data    (page->private)\n * - page mapped in a task's page tables, each mapping\n *   is counted separately\n *\n * Also, many kernel routines increase the page count before a critical\n * routine so they can be sure the page doesn't go away from under them.\n */\n\n/*\n * Drop a ref, return true if the refcount fell to zero (the page has no users)\n */\nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON(atomic_read(&page->_count) == 0);\n\treturn atomic_dec_and_test(&page->_count);\n}\n\n/*\n * Try to grab a ref unless the page has a refcount of zero, return false if\n * that is the case.\n */\nstatic inline int get_page_unless_zero(struct page *page)\n{\n\treturn atomic_inc_not_zero(&page->_count);\n}\n\nextern int page_is_ram(unsigned long pfn);\n\n/* Support for virtually mapped pages */\nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n/*\n * Determine if an address is within the vmalloc range\n *\n * On nommu, vmalloc/vfree wrap through kmalloc/kfree directly, so there\n * is no special casing required.\n */\nstatic inline int is_vmalloc_addr(const void *x)\n{\n#ifdef CONFIG_MMU\n\tunsigned long addr = (unsigned long)x;\n\n\treturn addr >= VMALLOC_START && addr < VMALLOC_END;\n#else\n\treturn 0;\n#endif\n}\n#ifdef CONFIG_MMU\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline void compound_lock(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbit_spin_lock(PG_compound_lock, &page->flags);\n#endif\n}\n\nstatic inline void compound_unlock(struct page *page)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbit_spin_unlock(PG_compound_lock, &page->flags);\n#endif\n}\n\nstatic inline unsigned long compound_lock_irqsave(struct page *page)\n{\n\tunsigned long uninitialized_var(flags);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tlocal_irq_save(flags);\n\tcompound_lock(page);\n#endif\n\treturn flags;\n}\n\nstatic inline void compound_unlock_irqrestore(struct page *page,\n\t\t\t\t\t      unsigned long flags)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tcompound_unlock(page);\n\tlocal_irq_restore(flags);\n#endif\n}\n\nstatic inline struct page *compound_head(struct page *page)\n{\n\tif (unlikely(PageTail(page)))\n\t\treturn page->first_page;\n\treturn page;\n}\n\nstatic inline int page_count(struct page *page)\n{\n\treturn atomic_read(&compound_head(page)->_count);\n}\n\nstatic inline void get_page(struct page *page)\n{\n\t/*\n\t * Getting a normal page or the head of a compound page\n\t * requires to already have an elevated page->_count. Only if\n\t * we're getting a tail page, the elevated page->_count is\n\t * required only in the head page, so for tail pages the\n\t * bugcheck only verifies that the page->_count isn't\n\t * negative.\n\t */\n\tVM_BUG_ON(atomic_read(&page->_count) < !PageTail(page));\n\tatomic_inc(&page->_count);\n\t/*\n\t * Getting a tail page will elevate both the head and tail\n\t * page->_count(s).\n\t */\n\tif (unlikely(PageTail(page))) {\n\t\t/*\n\t\t * This is safe only because\n\t\t * __split_huge_page_refcount can't run under\n\t\t * get_page().\n\t\t */\n\t\tVM_BUG_ON(atomic_read(&page->first_page->_count) <= 0);\n\t\tatomic_inc(&page->first_page->_count);\n\t}\n}\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\treturn compound_head(page);\n}\n\n/*\n * Setup the page count before being freed into the page allocator for\n * the first time (boot or memory hotplug)\n */\nstatic inline void init_page_count(struct page *page)\n{\n\tatomic_set(&page->_count, 1);\n}\n\n/*\n * PageBuddy() indicate that the page is free and in the buddy system\n * (see mm/page_alloc.c).\n *\n * PAGE_BUDDY_MAPCOUNT_VALUE must be <= -2 but better not too close to\n * -2 so that an underflow of the page_mapcount() won't be mistaken\n * for a genuine PAGE_BUDDY_MAPCOUNT_VALUE. -128 can be created very\n * efficiently by most CPU architectures.\n */\n#define PAGE_BUDDY_MAPCOUNT_VALUE (-128)\n\nstatic inline int PageBuddy(struct page *page)\n{\n\treturn atomic_read(&page->_mapcount) == PAGE_BUDDY_MAPCOUNT_VALUE;\n}\n\nstatic inline void __SetPageBuddy(struct page *page)\n{\n\tVM_BUG_ON(atomic_read(&page->_mapcount) != -1);\n\tatomic_set(&page->_mapcount, PAGE_BUDDY_MAPCOUNT_VALUE);\n}\n\nstatic inline void __ClearPageBuddy(struct page *page)\n{\n\tVM_BUG_ON(!PageBuddy(page));\n\tatomic_set(&page->_mapcount, -1);\n}\n\nvoid put_page(struct page *page);\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\nint split_free_page(struct page *page);\n\n/*\n * Compound pages have a destructor function.  Provide a\n * prototype for that function and accessor functions.\n * These are _only_ valid on the head of a PG_compound page.\n */\ntypedef void compound_page_dtor(struct page *);\n\nstatic inline void set_compound_page_dtor(struct page *page,\n\t\t\t\t\t\tcompound_page_dtor *dtor)\n{\n\tpage[1].lru.next = (void *)dtor;\n}\n\nstatic inline compound_page_dtor *get_compound_page_dtor(struct page *page)\n{\n\treturn (compound_page_dtor *)page[1].lru.next;\n}\n\nstatic inline int compound_order(struct page *page)\n{\n\tif (!PageHead(page))\n\t\treturn 0;\n\treturn (unsigned long)page[1].lru.prev;\n}\n\nstatic inline int compound_trans_order(struct page *page)\n{\n\tint order;\n\tunsigned long flags;\n\n\tif (!PageHead(page))\n\t\treturn 0;\n\n\tflags = compound_lock_irqsave(page);\n\torder = compound_order(page);\n\tcompound_unlock_irqrestore(page, flags);\n\treturn order;\n}\n\nstatic inline void set_compound_order(struct page *page, unsigned long order)\n{\n\tpage[1].lru.prev = (void *)order;\n}\n\n#ifdef CONFIG_MMU\n/*\n * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when\n * servicing faults for write access.  In the normal case, do always want\n * pte_mkwrite.  But get_user_pages can cause write faults for mappings\n * that do not have writing enabled, when used by access_process_vm.\n */\nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte);\n\treturn pte;\n}\n#endif\n\n/*\n * Multiple processes may \"see\" the same page. E.g. for untouched\n * mappings of /dev/null, all processes see the same page full of\n * zeroes, and text pages of executables and shared libraries have\n * only one copy in memory, at most, normally.\n *\n * For the non-reserved pages, page_count(page) denotes a reference count.\n *   page_count() == 0 means the page is free. page->lru is then used for\n *   freelist management in the buddy allocator.\n *   page_count() > 0  means the page has been allocated.\n *\n * Pages are allocated by the slab allocator in order to provide memory\n * to kmalloc and kmem_cache_alloc. In this case, the management of the\n * page, and the fields in 'struct page' are the responsibility of mm/slab.c\n * unless a particular usage is carefully commented. (the responsibility of\n * freeing the kmalloc memory is the caller's, of course).\n *\n * A page may be used by anyone else who does a __get_free_page().\n * In this case, page_count still tracks the references, and should only\n * be used through the normal accessor functions. The top bits of page->flags\n * and page->virtual store page management information, but all other fields\n * are unused and could be used privately, carefully. The management of this\n * page is the responsibility of the one who allocated it, and those who have\n * subsequently been given references to it.\n *\n * The other pages (we may call them \"pagecache pages\") are completely\n * managed by the Linux memory manager: I/O, buffers, swapping etc.\n * The following discussion applies only to them.\n *\n * A pagecache page contains an opaque `private' member, which belongs to the\n * page's address_space. Usually, this is the address of a circular list of\n * the page's disk buffers. PG_private must be set to tell the VM to call\n * into the filesystem to release these pages.\n *\n * A page may belong to an inode's memory mapping. In this case, page->mapping\n * is the pointer to the inode, and page->index is the file offset of the page,\n * in units of PAGE_CACHE_SIZE.\n *\n * If pagecache pages are not associated with an inode, they are said to be\n * anonymous pages. These may become associated with the swapcache, and in that\n * case PG_swapcache is set, and page->private is an offset into the swapcache.\n *\n * In either case (swapcache or inode backed), the pagecache itself holds one\n * reference to the page. Setting PG_private should also increment the\n * refcount. The each user mapping also has a reference to the page.\n *\n * The pagecache pages are stored in a per-mapping radix tree, which is\n * rooted at mapping->page_tree, and indexed by offset.\n * Where 2.4 and early 2.6 kernels kept dirty/clean pages in per-address_space\n * lists, we instead now tag pages as dirty/writeback in the radix tree.\n *\n * All pagecache pages may be subject to I/O:\n * - inode pages may need to be read from disk,\n * - inode pages which have been modified and are MAP_SHARED may need\n *   to be written back to the inode on disk,\n * - anonymous pages (including MAP_PRIVATE file mappings) which have been\n *   modified may need to be swapped out to swap space and (later) to be read\n *   back into memory.\n */\n\n/*\n * The zone field is never updated after free_area_init_core()\n * sets it, so none of the operations on it need to be atomic.\n */\n\n\n/*\n * page->flags layout:\n *\n * There are three possibilities for how page->flags get\n * laid out.  The first is for the normal case, without\n * sparsemem.  The second is for sparsemem when there is\n * plenty of space for node and section.  The last is when\n * we have run out of space and have to fall back to an\n * alternate (slower) way of determining the node.\n *\n * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE | ... | FLAGS |\n * classic sparse with space for node:| SECTION | NODE | ZONE | ... | FLAGS |\n * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |\n */\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTIONS_WIDTH\t\tSECTIONS_SHIFT\n#else\n#define SECTIONS_WIDTH\t\t0\n#endif\n\n#define ZONES_WIDTH\t\tZONES_SHIFT\n\n#if SECTIONS_WIDTH+ZONES_WIDTH+NODES_SHIFT <= BITS_PER_LONG - NR_PAGEFLAGS\n#define NODES_WIDTH\t\tNODES_SHIFT\n#else\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n#error \"Vmemmap: No space for nodes field in page flags\"\n#endif\n#define NODES_WIDTH\t\t0\n#endif\n\n/* Page flags: | [SECTION] | [NODE] | ZONE | ... | FLAGS | */\n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n\n/*\n * We are going to use the flags for the page to node mapping if its in\n * there.  This includes the case where there is no node, so it is implicit.\n */\n#if !(NODES_WIDTH > 0 || NODES_SHIFT == 0)\n#define NODE_NOT_IN_PAGE_FLAGS\n#endif\n\n#ifndef PFN_SECTION_SHIFT\n#define PFN_SECTION_SHIFT 0\n#endif\n\n/*\n * Define the bit shifts to access each section.  For non-existent\n * sections we define the shift as 0; that plus a 0 mask ensures\n * the compiler will optimise away reference to them.\n */\n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n\n/* NODE:ZONE or SECTION:ZONE is used to ID a zone for the buddy allocator */\n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF)? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#if SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#error SECTIONS_WIDTH+NODES_WIDTH+ZONES_WIDTH > BITS_PER_LONG - NR_PAGEFLAGS\n#endif\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(struct page *page)\n{\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\n/*\n * The identification function is only used by the buddy allocator for\n * determining if two pages could be buddies. We are not really\n * identifying a zone since we could be using a the section number\n * id if we have not node id available in page flags.\n * We guarantee only that it will return the same value for two\n * combinable pages in a zone.\n */\nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\nstatic inline int zone_to_nid(struct zone *zone)\n{\n#ifdef CONFIG_NUMA\n\treturn zone->node;\n#else\n\treturn 0;\n#endif\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(struct page *page);\n#else\nstatic inline int page_to_nid(struct page *page)\n{\n\treturn (page->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\nstatic inline struct zone *page_zone(struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\nstatic inline unsigned long page_to_section(struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n\tset_page_section(page, pfn_to_section_nr(pfn));\n}\n\n/*\n * Some inline functions in vmstat.h depend on page_zone()\n */\n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(struct page *page)\n{\n\treturn __va(PFN_PHYS(page_to_pfn(page)));\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) ((page)->virtual)\n#define set_page_address(page, address)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\t(page)->virtual = (address);\t\t\\\n\t} while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\n/*\n * On an anonymous page mapped into a user virtual memory area,\n * page->mapping points to its anon_vma, not to a struct address_space;\n * with the PAGE_MAPPING_ANON bit set to distinguish it.  See rmap.h.\n *\n * On an anonymous page in a VM_MERGEABLE area, if CONFIG_KSM is enabled,\n * the PAGE_MAPPING_KSM bit may be set along with the PAGE_MAPPING_ANON bit;\n * and then page->mapping points, not to an anon_vma, but to a private\n * structure which KSM associates with that merged page.  See ksm.h.\n *\n * PAGE_MAPPING_KSM without PAGE_MAPPING_ANON is currently never used.\n *\n * Please note that, confusingly, \"page_mapping\" refers to the inode\n * address_space which maps the page from disk; whereas \"page_mapped\"\n * refers to user virtual address space into which the page is mapped.\n */\n#define PAGE_MAPPING_ANON\t1\n#define PAGE_MAPPING_KSM\t2\n#define PAGE_MAPPING_FLAGS\t(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM)\n\nextern struct address_space swapper_space;\nstatic inline struct address_space *page_mapping(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\n\tVM_BUG_ON(PageSlab(page));\n\tif (unlikely(PageSwapCache(page)))\n\t\tmapping = &swapper_space;\n\telse if ((unsigned long)mapping & PAGE_MAPPING_ANON)\n\t\tmapping = NULL;\n\treturn mapping;\n}\n\n/* Neutral page->mapping pointer to address_space or anon_vma or other */\nstatic inline void *page_rmapping(struct page *page)\n{\n\treturn (void *)((unsigned long)page->mapping & ~PAGE_MAPPING_FLAGS);\n}\n\nstatic inline int PageAnon(struct page *page)\n{\n\treturn ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;\n}\n\n/*\n * Return the pagecache index of the passed page.  Regular pagecache pages\n * use ->index whereas swapcache pages use ->private\n */\nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn page_private(page);\n\treturn page->index;\n}\n\n/*\n * The atomic page->_mapcount, like _count, starts from -1:\n * so that transitions both from it and to it can be tracked,\n * using atomic_inc_and_test and atomic_add_negative(-1).\n */\nstatic inline void reset_page_mapcount(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\nstatic inline int page_mapcount(struct page *page)\n{\n\treturn atomic_read(&(page)->_mapcount) + 1;\n}\n\n/*\n * Return true if this page is mapped into pagetables.\n */\nstatic inline int page_mapped(struct page *page)\n{\n\treturn atomic_read(&(page)->_mapcount) >= 0;\n}\n\n/*\n * Different kinds of faults, as returned by handle_mm_fault().\n * Used to decide whether a process gets delivered SIGBUS or\n * just gets major/minor fault counters bumped up.\n */\n\n#define VM_FAULT_MINOR\t0 /* For backwards compat. Remove me quickly. */\n\n#define VM_FAULT_OOM\t0x0001\n#define VM_FAULT_SIGBUS\t0x0002\n#define VM_FAULT_MAJOR\t0x0004\n#define VM_FAULT_WRITE\t0x0008\t/* Special case for get_user_pages */\n#define VM_FAULT_HWPOISON 0x0010\t/* Hit poisoned small page */\n#define VM_FAULT_HWPOISON_LARGE 0x0020  /* Hit poisoned large page. Index encoded in upper bits */\n\n#define VM_FAULT_NOPAGE\t0x0100\t/* ->fault installed the pte, not return page */\n#define VM_FAULT_LOCKED\t0x0200\t/* ->fault locked the returned page */\n#define VM_FAULT_RETRY\t0x0400\t/* ->fault blocked, must retry */\n\n#define VM_FAULT_HWPOISON_LARGE_MASK 0xf000 /* encodes hpage index for large hwpoison */\n\n#define VM_FAULT_ERROR\t(VM_FAULT_OOM | VM_FAULT_SIGBUS | VM_FAULT_HWPOISON | \\\n\t\t\t VM_FAULT_HWPOISON_LARGE)\n\n/* Encode hstate index for a hwpoisoned large page */\n#define VM_FAULT_SET_HINDEX(x) ((x) << 12)\n#define VM_FAULT_GET_HINDEX(x) (((x) >> 12) & 0xf)\n\n/*\n * Can be called by the pagefault handler when it gets a VM_FAULT_OOM.\n */\nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n\n/*\n * Flags passed to show_mem() and __show_free_areas() to suppress output in\n * various contexts.\n */\n#define SHOW_MEM_FILTER_NODES\t(0x0001u)\t/* filter disallowed nodes */\n\nextern void show_free_areas(void);\nextern void __show_free_areas(unsigned int flags);\n\nint shmem_lock(struct file *file, int lock, struct user_struct *user);\nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags);\nint shmem_zero_setup(struct vm_area_struct *);\n\n#ifndef CONFIG_MMU\nextern unsigned long shmem_get_unmapped_area(struct file *file,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     unsigned long len,\n\t\t\t\t\t     unsigned long pgoff,\n\t\t\t\t\t     unsigned long flags);\n#endif\n\nextern int can_do_mlock(void);\nextern int user_shm_lock(size_t, struct user_struct *);\nextern void user_shm_unlock(size_t, struct user_struct *);\n\n/*\n * Parameter block passed down to zap_pte_range in exceptional cases.\n */\nstruct zap_details {\n\tstruct vm_area_struct *nonlinear_vma;\t/* Check page->index if set */\n\tstruct address_space *check_mapping;\t/* Check page->mapping if set */\n\tpgoff_t\tfirst_index;\t\t\t/* Lowest page->index to unmap */\n\tpgoff_t last_index;\t\t\t/* Highest page->index to unmap */\n\tspinlock_t *i_mmap_lock;\t\t/* For unmap_mapping_range: */\n\tunsigned long truncate_count;\t\t/* Compare vm_truncate_count */\n};\n\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\tpte_t pte);\n\nint zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size);\nunsigned long zap_page_range(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned long size, struct zap_details *);\nunsigned long unmap_vmas(struct mmu_gather **tlb,\n\t\tstruct vm_area_struct *start_vma, unsigned long start_addr,\n\t\tunsigned long end_addr, unsigned long *nr_accounted,\n\t\tstruct zap_details *);\n\n/**\n * mm_walk - callbacks for walk_page_range\n * @pgd_entry: if set, called for each non-empty PGD (top-level) entry\n * @pud_entry: if set, called for each non-empty PUD (2nd-level) entry\n * @pmd_entry: if set, called for each non-empty PMD (3rd-level) entry\n *\t       this handler is required to be able to handle\n *\t       pmd_trans_huge() pmds.  They may simply choose to\n *\t       split_huge_page() instead of handling it explicitly.\n * @pte_entry: if set, called for each non-empty PTE (4th-level) entry\n * @pte_hole: if set, called for each hole at all levels\n * @hugetlb_entry: if set, called for each hugetlb entry\n *\n * (see walk_page_range for more details)\n */\nstruct mm_walk {\n\tint (*pgd_entry)(pgd_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pud_entry)(pud_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pmd_entry)(pmd_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pte_entry)(pte_t *, unsigned long, unsigned long, struct mm_walk *);\n\tint (*pte_hole)(unsigned long, unsigned long, struct mm_walk *);\n\tint (*hugetlb_entry)(pte_t *, unsigned long,\n\t\t\t     unsigned long, unsigned long, struct mm_walk *);\n\tstruct mm_struct *mm;\n\tvoid *private;\n};\n\nint walk_page_range(unsigned long addr, unsigned long end,\n\t\tstruct mm_walk *walk);\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint copy_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\tstruct vm_area_struct *vma);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nextern void truncate_pagecache(struct inode *inode, loff_t old, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nextern int vmtruncate(struct inode *inode, loff_t offset);\nextern int vmtruncate_range(struct inode *inode, loff_t offset, loff_t end);\n\nint truncate_inode_page(struct address_space *mapping, struct page *page);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\n\nint invalidate_inode_page(struct page *page);\n\n#ifdef CONFIG_MMU\nextern int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags);\n#else\nstatic inline int handle_mm_fault(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tunsigned int flags)\n{\n\t/* should never happen if there's no MMU */\n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\n#endif\n\nextern int make_pages_present(unsigned long addr, unsigned long end);\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf, int len, int write);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, int write);\n\nint __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long start, int len, unsigned int foll_flags,\n\t\t     struct page **pages, struct vm_area_struct **vmas,\n\t\t     int *nonblocking);\nint get_user_pages(struct task_struct *tsk, struct mm_struct *mm,\n\t\t\tunsigned long start, int nr_pages, int write, int force,\n\t\t\tstruct page **pages, struct vm_area_struct **vmas);\nint get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\tstruct page **pages);\nstruct page *get_dump_page(unsigned long addr);\n\nextern int try_to_release_page(struct page * page, gfp_t gfp_mask);\nextern void do_invalidatepage(struct page *page, unsigned long offset);\n\nint __set_page_dirty_nobuffers(struct page *page);\nint __set_page_dirty_no_writeback(struct page *page);\nint redirty_page_for_writepage(struct writeback_control *wbc,\n\t\t\t\tstruct page *page);\nvoid account_page_dirtied(struct page *page, struct address_space *mapping);\nvoid account_page_writeback(struct page *page);\nint set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\nint clear_page_dirty_for_io(struct page *page);\n\n/* Is the vma a continuation of the stack vma above it? */\nstatic inline int vma_stack_continue(struct vm_area_struct *vma, unsigned long addr)\n{\n\treturn vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);\n}\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len);\nextern unsigned long do_mremap(unsigned long addr,\n\t\t\t       unsigned long old_len, unsigned long new_len,\n\t\t\t       unsigned long flags, unsigned long new_addr);\nextern int mprotect_fixup(struct vm_area_struct *vma,\n\t\t\t  struct vm_area_struct **pprev, unsigned long start,\n\t\t\t  unsigned long end, unsigned long newflags);\n\n/*\n * doesn't attempt to fault and will return short.\n */\nint __get_user_pages_fast(unsigned long start, int nr_pages, int write,\n\t\t\t  struct page **pages);\n/*\n * per-process(per-mm_struct) statistics.\n */\n#if defined(SPLIT_RSS_COUNTING)\n/*\n * The mm counters are not protected by its page_table_lock,\n * so must be incremented atomically.\n */\nstatic inline void set_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_set(&mm->rss_stat.count[member], value);\n}\n\nunsigned long get_mm_counter(struct mm_struct *mm, int member);\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tatomic_long_add(value, &mm->rss_stat.count[member]);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_inc(&mm->rss_stat.count[member]);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tatomic_long_dec(&mm->rss_stat.count[member]);\n}\n\n#else  /* !USE_SPLIT_PTLOCKS */\n/*\n * The mm counters are protected by its page_table_lock,\n * so can be incremented directly.\n */\nstatic inline void set_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tmm->rss_stat.count[member] = value;\n}\n\nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\treturn mm->rss_stat.count[member];\n}\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tmm->rss_stat.count[member] += value;\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tmm->rss_stat.count[member]++;\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tmm->rss_stat.count[member]--;\n}\n\n#endif /* !USE_SPLIT_PTLOCKS */\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct task_struct *task, struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct task_struct *task, struct mm_struct *mm)\n{\n}\n#endif\n\n/*\n * A callback you can register to apply pressure to ageable caches.\n *\n * 'shrink' is passed a count 'nr_to_scan' and a 'gfpmask'.  It should\n * look through the least-recently-used 'nr_to_scan' entries and\n * attempt to free them up.  It should return the number of objects\n * which remain in the cache.  If it returns -1, it means it cannot do\n * any scanning at this time (eg. there is a risk of deadlock).\n *\n * The 'gfpmask' refers to the allocation we are currently trying to\n * fulfil.\n *\n * Note that 'shrink' will be passed nr_to_scan == 0 when the VM is\n * querying the cache size, so a fastpath for that case is appropriate.\n */\nstruct shrinker {\n\tint (*shrink)(struct shrinker *, int nr_to_scan, gfp_t gfp_mask);\n\tint seeks;\t/* seeks to recreate an obj */\n\n\t/* These are for internal use */\n\tstruct list_head list;\n\tlong nr;\t/* objs pending delete */\n};\n#define DEFAULT_SEEKS 2 /* A good number if you don't know better. */\nextern void register_shrinker(struct shrinker *);\nextern void unregister_shrinker(struct shrinker *);\n\nint vma_wants_writenotify(struct vm_area_struct *vma);\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_PUD_FOLDED\nstatic inline int __pud_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#ifdef __PAGETABLE_PMD_FOLDED\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n#endif\n\nint __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\tpmd_t *pmd, unsigned long address);\nint __pte_alloc_kernel(pmd_t *pmd, unsigned long address);\n\n/*\n * The following ifdef needed to get the 4level-fixup.h header to work.\n * Remove it when 4level-fixup.h has been removed.\n */\n#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?\n\t\tNULL: pud_offset(pgd, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif /* CONFIG_MMU && !__ARCH_HAS_4LEVEL_HACK */\n\n#if USE_SPLIT_PTLOCKS\n/*\n * We tuck a spinlock to guard each pagetable page into its struct page,\n * at page->private, with BUILD_BUG_ON to make sure that this will not\n * overflow into the next struct page (as it might with DEBUG_SPINLOCK).\n * When freeing, reset page->mapping so free_pages_check won't complain.\n */\n#define __pte_lockptr(page)\t&((page)->ptl)\n#define pte_lock_init(_page)\tdo {\t\t\t\t\t\\\n\tspin_lock_init(__pte_lockptr(_page));\t\t\t\t\\\n} while (0)\n#define pte_lock_deinit(page)\t((page)->mapping = NULL)\n#define pte_lockptr(mm, pmd)\t({(void)(mm); __pte_lockptr(pmd_page(*(pmd)));})\n#else\t/* !USE_SPLIT_PTLOCKS */\n/*\n * We use mm->page_table_lock to guard all pagetable pages of the mm.\n */\n#define pte_lock_init(page)\tdo {} while (0)\n#define pte_lock_deinit(page)\tdo {} while (0)\n#define pte_lockptr(mm, pmd)\t({(void)(pmd); &(mm)->page_table_lock;})\n#endif /* USE_SPLIT_PTLOCKS */\n\nstatic inline void pgtable_page_ctor(struct page *page)\n{\n\tpte_lock_init(page);\n\tinc_zone_page_state(page, NR_PAGETABLE);\n}\n\nstatic inline void pgtable_page_dtor(struct page *page)\n{\n\tpte_lock_deinit(page);\n\tdec_zone_page_state(page, NR_PAGETABLE);\n}\n\n#define pte_offset_map_lock(mm, pmd, address, ptlp)\t\\\n({\t\t\t\t\t\t\t\\\n\tspinlock_t *__ptl = pte_lockptr(mm, pmd);\t\\\n\tpte_t *__pte = pte_offset_map(pmd, address);\t\\\n\t*(ptlp) = __ptl;\t\t\t\t\\\n\tspin_lock(__ptl);\t\t\t\t\\\n\t__pte;\t\t\t\t\t\t\\\n})\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc_map(mm, vma, pmd, address)\t\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, vma,\t\\\n\t\t\t\t\t\t\tpmd, address))?\t\\\n\t NULL: pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, NULL,\t\\\n\t\t\t\t\t\t\tpmd, address))?\t\\\n\t\tNULL: pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd, address))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\nextern void free_area_init(unsigned long * zones_size);\nextern void free_area_init_node(int nid, unsigned long * zones_size,\n\t\tunsigned long zone_start_pfn, unsigned long *zholes_size);\n#ifdef CONFIG_ARCH_POPULATES_NODE_MAP\n/*\n * With CONFIG_ARCH_POPULATES_NODE_MAP set, an architecture may initialise its\n * zones, allocate the backing mem_map and account for memory holes in a more\n * architecture independent manner. This is a substitute for creating the\n * zone_sizes[] and zholes_size[] arrays and passing them to\n * free_area_init_node()\n *\n * An architecture is expected to register range of page frames backed by\n * physical memory with add_active_range() before calling\n * free_area_init_nodes() passing in the PFN each zone ends at. At a basic\n * usage, an architecture is expected to do something like\n *\n * unsigned long max_zone_pfns[MAX_NR_ZONES] = {max_dma, max_normal_pfn,\n * \t\t\t\t\t\t\t max_highmem_pfn};\n * for_each_valid_physical_page_range()\n * \tadd_active_range(node_id, start_pfn, end_pfn)\n * free_area_init_nodes(max_zone_pfns);\n *\n * If the architecture guarantees that there are no holes in the ranges\n * registered with add_active_range(), free_bootmem_active_regions()\n * will call free_bootmem_node() for each registered physical page range.\n * Similarly sparse_memory_present_with_active_regions() calls\n * memory_present() for each range when SPARSEMEM is enabled.\n *\n * See mm/page_alloc.c for more information on each function exposed by\n * CONFIG_ARCH_POPULATES_NODE_MAP\n */\nextern void free_area_init_nodes(unsigned long *max_zone_pfn);\nextern void add_active_range(unsigned int nid, unsigned long start_pfn,\n\t\t\t\t\tunsigned long end_pfn);\nextern void remove_active_range(unsigned int nid, unsigned long start_pfn,\n\t\t\t\t\tunsigned long end_pfn);\nextern void remove_all_active_ranges(void);\nvoid sort_node_map(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\nextern unsigned long find_min_pfn_with_active_regions(void);\nextern void free_bootmem_with_active_regions(int nid,\n\t\t\t\t\t\tunsigned long max_low_pfn);\nint add_from_early_node_map(struct range *range, int az,\n\t\t\t\t   int nr_range, int nid);\nu64 __init find_memory_core_early(int nid, u64 size, u64 align,\n\t\t\t\t\tu64 goal, u64 limit);\ntypedef int (*work_fn_t)(unsigned long, unsigned long, void *);\nextern void work_with_active_regions(int nid, work_fn_t work_fn, void *data);\nextern void sparse_memory_present_with_active_regions(int nid);\n#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */\n\n#if !defined(CONFIG_ARCH_POPULATES_NODE_MAP) && \\\n    !defined(CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID)\nstatic inline int __early_pfn_to_nid(unsigned long pfn)\n{\n\treturn 0;\n}\n#else\n/* please see mm/page_alloc.c */\nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n#ifdef CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID\n/* there is a per-arch backend function. */\nextern int __meminit __early_pfn_to_nid(unsigned long pfn);\n#endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void memmap_init_zone(unsigned long, int, unsigned long,\n\t\t\t\tunsigned long, enum memmap_context);\nextern void setup_per_zone_wmarks(void);\nextern void calculate_zone_inactive_ratio(struct zone *zone);\nextern void mem_init(void);\nextern void __init mmap_init(void);\nextern void show_mem(unsigned int flags);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\nextern int after_bootmem;\n\nextern void setup_per_cpu_pageset(void);\n\nextern void zone_pcp_update(struct zone *zone);\n\n/* nommu.c */\nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n/* prio_tree.c */\nvoid vma_prio_tree_add(struct vm_area_struct *, struct vm_area_struct *old);\nvoid vma_prio_tree_insert(struct vm_area_struct *, struct prio_tree_root *);\nvoid vma_prio_tree_remove(struct vm_area_struct *, struct prio_tree_root *);\nstruct vm_area_struct *vma_prio_tree_next(struct vm_area_struct *vma,\n\tstruct prio_tree_iter *iter);\n\n#define vma_prio_tree_foreach(vma, iter, root, begin, end)\t\\\n\tfor (prio_tree_iter_init(iter, root, begin, end), vma = NULL;\t\\\n\t\t(vma = vma_prio_tree_next(vma, iter)); )\n\nstatic inline void vma_nonlinear_insert(struct vm_area_struct *vma,\n\t\t\t\t\tstruct list_head *list)\n{\n\tvma->shared.vm_set.parent = NULL;\n\tlist_add_tail(&vma->shared.vm_set.list, list);\n}\n\n/* mmap.c */\nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int vma_adjust(struct vm_area_struct *vma, unsigned long start,\n\tunsigned long end, pgoff_t pgoff, struct vm_area_struct *insert);\nextern struct vm_area_struct *vma_merge(struct mm_struct *,\n\tstruct vm_area_struct *prev, unsigned long addr, unsigned long end,\n\tunsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,\n\tstruct mempolicy *);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int split_vma(struct mm_struct *,\n\tstruct vm_area_struct *, unsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,\n\tstruct rb_node **, struct rb_node *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff);\nextern void exit_mmap(struct mm_struct *);\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\n#ifdef CONFIG_PROC_FS\n/* From fs/proc/base.c. callers must _not_ hold the mm's exe_file_lock */\nextern void added_exe_file_vma(struct mm_struct *mm);\nextern void removed_exe_file_vma(struct mm_struct *mm);\n#else\nstatic inline void added_exe_file_vma(struct mm_struct *mm)\n{}\n\nstatic inline void removed_exe_file_vma(struct mm_struct *mm)\n{}\n#endif /* CONFIG_PROC_FS */\n\nextern int may_expand_vm(struct mm_struct *mm, unsigned long npages);\nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long do_mmap_pgoff(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long pgoff);\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long flags,\n\tunsigned int vm_flags, unsigned long pgoff);\n\nstatic inline unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot,\n\tunsigned long flag, unsigned long offset)\n{\n\tunsigned long ret = -EINVAL;\n\tif ((offset + PAGE_ALIGN(len)) < offset)\n\t\tgoto out;\n\tif (!(offset & ~PAGE_MASK))\n\t\tret = do_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);\nout:\n\treturn ret;\n}\n\nextern int do_munmap(struct mm_struct *, unsigned long, size_t);\n\nextern unsigned long do_brk(unsigned long, unsigned long);\n\n/* filemap.c */\nextern unsigned long page_unuse(struct page *);\nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\n\n/* generic vm_area_ops exported for stackable file systems */\nextern int filemap_fault(struct vm_area_struct *, struct vm_fault *);\n\n/* mm/page-writeback.c */\nint write_one_page(struct page *page, int wait);\nvoid task_dirty_inc(struct task_struct *tsk);\n\n/* readahead.c */\n#define VM_MAX_READAHEAD\t128\t/* kbytes */\n#define VM_MIN_READAHEAD\t16\t/* kbytes (includes current page) */\n\nint force_page_cache_readahead(struct address_space *mapping, struct file *filp,\n\t\t\tpgoff_t offset, unsigned long nr_to_read);\n\nvoid page_cache_sync_readahead(struct address_space *mapping,\n\t\t\t       struct file_ra_state *ra,\n\t\t\t       struct file *filp,\n\t\t\t       pgoff_t offset,\n\t\t\t       unsigned long size);\n\nvoid page_cache_async_readahead(struct address_space *mapping,\n\t\t\t\tstruct file_ra_state *ra,\n\t\t\t\tstruct file *filp,\n\t\t\t\tstruct page *pg,\n\t\t\t\tpgoff_t offset,\n\t\t\t\tunsigned long size);\n\nunsigned long max_sane_readahead(unsigned long nr);\nunsigned long ra_submit(struct file_ra_state *ra,\n\t\t\tstruct address_space *mapping,\n\t\t\tstruct file *filp);\n\n/* Do stack extension */\nextern int expand_stack(struct vm_area_struct *vma, unsigned long address);\n#if VM_GROWSUP\nextern int expand_upwards(struct vm_area_struct *vma, unsigned long address);\n#else\n  #define expand_upwards(vma, address) do { } while (0)\n#endif\nextern int expand_stack_downwards(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address);\n\n/* Look up the first VMA which satisfies  addr < vm_end,  NULL if none. */\nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n/* Look up the first VMA which intersects the interval start_addr..end_addr-1,\n   NULL if none.  Assume start_addr < end_addr. */\nstatic inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)\n{\n\tstruct vm_area_struct * vma = find_vma(mm,start_addr);\n\n\tif (vma && end_addr <= vma->vm_start)\n\t\tvma = NULL;\n\treturn vma;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\n#endif\n\nstruct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nint vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\n\nstruct page *follow_page(struct vm_area_struct *, unsigned long address,\n\t\t\tunsigned int foll_flags);\n#define FOLL_WRITE\t0x01\t/* check pte is writable */\n#define FOLL_TOUCH\t0x02\t/* mark page accessed */\n#define FOLL_GET\t0x04\t/* do get_page on page */\n#define FOLL_DUMP\t0x08\t/* give error on hole if it would be zero */\n#define FOLL_FORCE\t0x10\t/* get_user_pages read/write w/o permission */\n#define FOLL_NOWAIT\t0x20\t/* if a disk transfer is needed, start the IO\n\t\t\t\t * and return without waiting upon it */\n#define FOLL_MLOCK\t0x40\t/* mark page as mlocked */\n#define FOLL_SPLIT\t0x80\t/* don't return transhuge pages, split them */\n#define FOLL_HWPOISON\t0x100\t/* check page is hwpoisoned */\n\ntypedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,\n\t\t\tvoid *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\n\n#ifdef CONFIG_PROC_FS\nvoid vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);\n#else\nstatic inline void vm_stat_account(struct mm_struct *mm,\n\t\t\tunsigned long flags, struct file *file, long pages)\n{\n}\n#endif /* CONFIG_PROC_FS */\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nextern int debug_pagealloc_enabled;\n\nextern void kernel_map_pages(struct page *page, int numpages, int enable);\n\nstatic inline void enable_debug_pagealloc(void)\n{\n\tdebug_pagealloc_enabled = 1;\n}\n#ifdef CONFIG_HIBERNATION\nextern bool kernel_page_present(struct page *page);\n#endif /* CONFIG_HIBERNATION */\n#else\nstatic inline void\nkernel_map_pages(struct page *page, int numpages, int enable) {}\nstatic inline void enable_debug_pagealloc(void)\n{\n}\n#ifdef CONFIG_HIBERNATION\nstatic inline bool kernel_page_present(struct page *page) { return true; }\n#endif /* CONFIG_HIBERNATION */\n#endif\n\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\n#ifdef\t__HAVE_ARCH_GATE_AREA\nint in_gate_area_no_mm(unsigned long addr);\nint in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nint in_gate_area_no_mm(unsigned long addr);\n#define in_gate_area(mm, addr) ({(void)mm; in_gate_area_no_mm(addr);})\n#endif\t/* __HAVE_ARCH_GATE_AREA */\n\nint drop_caches_sysctl_handler(struct ctl_table *, int,\n\t\t\t\t\tvoid __user *, size_t *, loff_t *);\nunsigned long shrink_slab(unsigned long scanned, gfp_t gfp_mask,\n\t\t\tunsigned long lru_pages);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\nvoid print_vma_addr(char *prefix, unsigned long rip);\n\nvoid sparse_mem_maps_populate_node(struct page **map_map,\n\t\t\t\t   unsigned long pnum_begin,\n\t\t\t\t   unsigned long pnum_end,\n\t\t\t\t   unsigned long map_count,\n\t\t\t\t   int nodeid);\n\nstruct page *sparse_mem_map_populate(unsigned long pnum, int nid);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\npud_t *vmemmap_pud_populate(pgd_t *pgd, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nvoid *vmemmap_alloc_block_buf(unsigned long size, int node);\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nint vmemmap_populate_basepages(struct page *start_page,\n\t\t\t\t\t\tunsigned long pages, int node);\nint vmemmap_populate(struct page *start_page, unsigned long pages, int node);\nvoid vmemmap_populate_print_last(void);\n\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n};\nextern void memory_failure(unsigned long pfn, int trapno);\nextern int __memory_failure(unsigned long pfn, int trapno, int flags);\nextern int unpoison_memory(unsigned long pfn);\nextern int sysctl_memory_failure_early_kill;\nextern int sysctl_memory_failure_recovery;\nextern void shake_page(struct page *p, int access);\nextern atomic_long_t mce_bad_pages;\nextern int soft_offline_page(struct page *page, int flags);\n\nextern void dump_page(struct page *page);\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr,\n\t\t\t    unsigned int pages_per_huge_page);\nextern void copy_user_huge_page(struct page *dst, struct page *src,\n\t\t\t\tunsigned long addr, struct vm_area_struct *vma,\n\t\t\t\tunsigned int pages_per_huge_page);\n#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */\n\n#endif /* __KERNEL__ */\n#endif /* _LINUX_MM_H */\n", "/*\n *  Copyright (C) 2009  Red Hat, Inc.\n *\n *  This work is licensed under the terms of the GNU GPL, version 2. See\n *  the COPYING file in the top-level directory.\n */\n\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/highmem.h>\n#include <linux/hugetlb.h>\n#include <linux/mmu_notifier.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/mm_inline.h>\n#include <linux/kthread.h>\n#include <linux/khugepaged.h>\n#include <linux/freezer.h>\n#include <linux/mman.h>\n#include <asm/tlb.h>\n#include <asm/pgalloc.h>\n#include \"internal.h\"\n\n/*\n * By default transparent hugepage support is enabled for all mappings\n * and khugepaged scans all mappings. Defrag is only invoked by\n * khugepaged hugepage allocations and by page faults inside\n * MADV_HUGEPAGE regions to avoid the risk of slowing down short lived\n * allocations.\n */\nunsigned long transparent_hugepage_flags __read_mostly =\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS\n\t(1<<TRANSPARENT_HUGEPAGE_FLAG)|\n#endif\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE\n\t(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|\n#endif\n\t(1<<TRANSPARENT_HUGEPAGE_DEFRAG_FLAG)|\n\t(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n\n/* default scan 8*512 pte (or vmas) every 30 second */\nstatic unsigned int khugepaged_pages_to_scan __read_mostly = HPAGE_PMD_NR*8;\nstatic unsigned int khugepaged_pages_collapsed;\nstatic unsigned int khugepaged_full_scans;\nstatic unsigned int khugepaged_scan_sleep_millisecs __read_mostly = 10000;\n/* during fragmentation poll the hugepage allocator once every minute */\nstatic unsigned int khugepaged_alloc_sleep_millisecs __read_mostly = 60000;\nstatic struct task_struct *khugepaged_thread __read_mostly;\nstatic DEFINE_MUTEX(khugepaged_mutex);\nstatic DEFINE_SPINLOCK(khugepaged_mm_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(khugepaged_wait);\n/*\n * default collapse hugepages if there is at least one pte mapped like\n * it would have happened if the vma was large enough during page\n * fault.\n */\nstatic unsigned int khugepaged_max_ptes_none __read_mostly = HPAGE_PMD_NR-1;\n\nstatic int khugepaged(void *none);\nstatic int mm_slots_hash_init(void);\nstatic int khugepaged_slab_init(void);\nstatic void khugepaged_slab_free(void);\n\n#define MM_SLOTS_HASH_HEADS 1024\nstatic struct hlist_head *mm_slots_hash __read_mostly;\nstatic struct kmem_cache *mm_slot_cache __read_mostly;\n\n/**\n * struct mm_slot - hash lookup from mm to mm_slot\n * @hash: hash collision list\n * @mm_node: khugepaged scan list headed in khugepaged_scan.mm_head\n * @mm: the mm that this information is valid for\n */\nstruct mm_slot {\n\tstruct hlist_node hash;\n\tstruct list_head mm_node;\n\tstruct mm_struct *mm;\n};\n\n/**\n * struct khugepaged_scan - cursor for scanning\n * @mm_head: the head of the mm list to scan\n * @mm_slot: the current mm_slot we are scanning\n * @address: the next address inside that to be scanned\n *\n * There is only the one khugepaged_scan instance of this cursor structure.\n */\nstruct khugepaged_scan {\n\tstruct list_head mm_head;\n\tstruct mm_slot *mm_slot;\n\tunsigned long address;\n} khugepaged_scan = {\n\t.mm_head = LIST_HEAD_INIT(khugepaged_scan.mm_head),\n};\n\n\nstatic int set_recommended_min_free_kbytes(void)\n{\n\tstruct zone *zone;\n\tint nr_zones = 0;\n\tunsigned long recommended_min;\n\textern int min_free_kbytes;\n\n\tif (!test_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t      &transparent_hugepage_flags) &&\n\t    !test_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t      &transparent_hugepage_flags))\n\t\treturn 0;\n\n\tfor_each_populated_zone(zone)\n\t\tnr_zones++;\n\n\t/* Make sure at least 2 hugepages are free for MIGRATE_RESERVE */\n\trecommended_min = pageblock_nr_pages * nr_zones * 2;\n\n\t/*\n\t * Make sure that on average at least two pageblocks are almost free\n\t * of another type, one for a migratetype to fall back to and a\n\t * second to avoid subsequent fallbacks of other types There are 3\n\t * MIGRATE_TYPES we care about.\n\t */\n\trecommended_min += pageblock_nr_pages * nr_zones *\n\t\t\t   MIGRATE_PCPTYPES * MIGRATE_PCPTYPES;\n\n\t/* don't ever allow to reserve more than 5% of the lowmem */\n\trecommended_min = min(recommended_min,\n\t\t\t      (unsigned long) nr_free_buffer_pages() / 20);\n\trecommended_min <<= (PAGE_SHIFT-10);\n\n\tif (recommended_min > min_free_kbytes)\n\t\tmin_free_kbytes = recommended_min;\n\tsetup_per_zone_wmarks();\n\treturn 0;\n}\nlate_initcall(set_recommended_min_free_kbytes);\n\nstatic int start_khugepaged(void)\n{\n\tint err = 0;\n\tif (khugepaged_enabled()) {\n\t\tint wakeup;\n\t\tif (unlikely(!mm_slot_cache || !mm_slots_hash)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_thread)\n\t\t\tkhugepaged_thread = kthread_run(khugepaged, NULL,\n\t\t\t\t\t\t\t\"khugepaged\");\n\t\tif (unlikely(IS_ERR(khugepaged_thread))) {\n\t\t\tprintk(KERN_ERR\n\t\t\t       \"khugepaged: kthread_run(khugepaged) failed\\n\");\n\t\t\terr = PTR_ERR(khugepaged_thread);\n\t\t\tkhugepaged_thread = NULL;\n\t\t}\n\t\twakeup = !list_empty(&khugepaged_scan.mm_head);\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tif (wakeup)\n\t\t\twake_up_interruptible(&khugepaged_wait);\n\n\t\tset_recommended_min_free_kbytes();\n\t} else\n\t\t/* wakeup to exit */\n\t\twake_up_interruptible(&khugepaged_wait);\nout:\n\treturn err;\n}\n\n#ifdef CONFIG_SYSFS\n\nstatic ssize_t double_flag_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf,\n\t\t\t\tenum transparent_hugepage_flag enabled,\n\t\t\t\tenum transparent_hugepage_flag req_madv)\n{\n\tif (test_bit(enabled, &transparent_hugepage_flags)) {\n\t\tVM_BUG_ON(test_bit(req_madv, &transparent_hugepage_flags));\n\t\treturn sprintf(buf, \"[always] madvise never\\n\");\n\t} else if (test_bit(req_madv, &transparent_hugepage_flags))\n\t\treturn sprintf(buf, \"always [madvise] never\\n\");\n\telse\n\t\treturn sprintf(buf, \"always madvise [never]\\n\");\n}\nstatic ssize_t double_flag_store(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t const char *buf, size_t count,\n\t\t\t\t enum transparent_hugepage_flag enabled,\n\t\t\t\t enum transparent_hugepage_flag req_madv)\n{\n\tif (!memcmp(\"always\", buf,\n\t\t    min(sizeof(\"always\")-1, count))) {\n\t\tset_bit(enabled, &transparent_hugepage_flags);\n\t\tclear_bit(req_madv, &transparent_hugepage_flags);\n\t} else if (!memcmp(\"madvise\", buf,\n\t\t\t   min(sizeof(\"madvise\")-1, count))) {\n\t\tclear_bit(enabled, &transparent_hugepage_flags);\n\t\tset_bit(req_madv, &transparent_hugepage_flags);\n\t} else if (!memcmp(\"never\", buf,\n\t\t\t   min(sizeof(\"never\")-1, count))) {\n\t\tclear_bit(enabled, &transparent_hugepage_flags);\n\t\tclear_bit(req_madv, &transparent_hugepage_flags);\n\t} else\n\t\treturn -EINVAL;\n\n\treturn count;\n}\n\nstatic ssize_t enabled_show(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr, char *buf)\n{\n\treturn double_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG);\n}\nstatic ssize_t enabled_store(struct kobject *kobj,\n\t\t\t     struct kobj_attribute *attr,\n\t\t\t     const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tret = double_flag_store(kobj, attr, buf, count,\n\t\t\t\tTRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG);\n\n\tif (ret > 0) {\n\t\tint err = start_khugepaged();\n\t\tif (err)\n\t\t\tret = err;\n\t}\n\n\tif (ret > 0 &&\n\t    (test_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t      &transparent_hugepage_flags) ||\n\t     test_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t      &transparent_hugepage_flags)))\n\t\tset_recommended_min_free_kbytes();\n\n\treturn ret;\n}\nstatic struct kobj_attribute enabled_attr =\n\t__ATTR(enabled, 0644, enabled_show, enabled_store);\n\nstatic ssize_t single_flag_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf,\n\t\t\t\tenum transparent_hugepage_flag flag)\n{\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       !!test_bit(flag, &transparent_hugepage_flags));\n}\n\nstatic ssize_t single_flag_store(struct kobject *kobj,\n\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t const char *buf, size_t count,\n\t\t\t\t enum transparent_hugepage_flag flag)\n{\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(buf, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (value > 1)\n\t\treturn -EINVAL;\n\n\tif (value)\n\t\tset_bit(flag, &transparent_hugepage_flags);\n\telse\n\t\tclear_bit(flag, &transparent_hugepage_flags);\n\n\treturn count;\n}\n\n/*\n * Currently defrag only disables __GFP_NOWAIT for allocation. A blind\n * __GFP_REPEAT is too aggressive, it's never worth swapping tons of\n * memory just to allocate one more hugepage.\n */\nstatic ssize_t defrag_show(struct kobject *kobj,\n\t\t\t   struct kobj_attribute *attr, char *buf)\n{\n\treturn double_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG);\n}\nstatic ssize_t defrag_store(struct kobject *kobj,\n\t\t\t    struct kobj_attribute *attr,\n\t\t\t    const char *buf, size_t count)\n{\n\treturn double_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_FLAG,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG);\n}\nstatic struct kobj_attribute defrag_attr =\n\t__ATTR(defrag, 0644, defrag_show, defrag_store);\n\n#ifdef CONFIG_DEBUG_VM\nstatic ssize_t debug_cow_show(struct kobject *kobj,\n\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\treturn single_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG);\n}\nstatic ssize_t debug_cow_store(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr,\n\t\t\t       const char *buf, size_t count)\n{\n\treturn single_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEBUG_COW_FLAG);\n}\nstatic struct kobj_attribute debug_cow_attr =\n\t__ATTR(debug_cow, 0644, debug_cow_show, debug_cow_store);\n#endif /* CONFIG_DEBUG_VM */\n\nstatic struct attribute *hugepage_attr[] = {\n\t&enabled_attr.attr,\n\t&defrag_attr.attr,\n#ifdef CONFIG_DEBUG_VM\n\t&debug_cow_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hugepage_attr_group = {\n\t.attrs = hugepage_attr,\n};\n\nstatic ssize_t scan_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t struct kobj_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_scan_sleep_millisecs);\n}\n\nstatic ssize_t scan_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_scan_sleep_millisecs = msecs;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute scan_sleep_millisecs_attr =\n\t__ATTR(scan_sleep_millisecs, 0644, scan_sleep_millisecs_show,\n\t       scan_sleep_millisecs_store);\n\nstatic ssize_t alloc_sleep_millisecs_show(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_alloc_sleep_millisecs);\n}\n\nstatic ssize_t alloc_sleep_millisecs_store(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned long msecs;\n\tint err;\n\n\terr = strict_strtoul(buf, 10, &msecs);\n\tif (err || msecs > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_alloc_sleep_millisecs = msecs;\n\twake_up_interruptible(&khugepaged_wait);\n\n\treturn count;\n}\nstatic struct kobj_attribute alloc_sleep_millisecs_attr =\n\t__ATTR(alloc_sleep_millisecs, 0644, alloc_sleep_millisecs_show,\n\t       alloc_sleep_millisecs_store);\n\nstatic ssize_t pages_to_scan_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_pages_to_scan);\n}\nstatic ssize_t pages_to_scan_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long pages;\n\n\terr = strict_strtoul(buf, 10, &pages);\n\tif (err || !pages || pages > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tkhugepaged_pages_to_scan = pages;\n\n\treturn count;\n}\nstatic struct kobj_attribute pages_to_scan_attr =\n\t__ATTR(pages_to_scan, 0644, pages_to_scan_show,\n\t       pages_to_scan_store);\n\nstatic ssize_t pages_collapsed_show(struct kobject *kobj,\n\t\t\t\t    struct kobj_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_pages_collapsed);\n}\nstatic struct kobj_attribute pages_collapsed_attr =\n\t__ATTR_RO(pages_collapsed);\n\nstatic ssize_t full_scans_show(struct kobject *kobj,\n\t\t\t       struct kobj_attribute *attr,\n\t\t\t       char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_full_scans);\n}\nstatic struct kobj_attribute full_scans_attr =\n\t__ATTR_RO(full_scans);\n\nstatic ssize_t khugepaged_defrag_show(struct kobject *kobj,\n\t\t\t\t      struct kobj_attribute *attr, char *buf)\n{\n\treturn single_flag_show(kobj, attr, buf,\n\t\t\t\tTRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic ssize_t khugepaged_defrag_store(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr,\n\t\t\t\t       const char *buf, size_t count)\n{\n\treturn single_flag_store(kobj, attr, buf, count,\n\t\t\t\t TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG);\n}\nstatic struct kobj_attribute khugepaged_defrag_attr =\n\t__ATTR(defrag, 0644, khugepaged_defrag_show,\n\t       khugepaged_defrag_store);\n\n/*\n * max_ptes_none controls if khugepaged should collapse hugepages over\n * any unmapped ptes in turn potentially increasing the memory\n * footprint of the vmas. When max_ptes_none is 0 khugepaged will not\n * reduce the available free memory in the system as it\n * runs. Increasing max_ptes_none will instead potentially reduce the\n * free memory in the system during the khugepaged scan.\n */\nstatic ssize_t khugepaged_max_ptes_none_show(struct kobject *kobj,\n\t\t\t\t\t     struct kobj_attribute *attr,\n\t\t\t\t\t     char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", khugepaged_max_ptes_none);\n}\nstatic ssize_t khugepaged_max_ptes_none_store(struct kobject *kobj,\n\t\t\t\t\t      struct kobj_attribute *attr,\n\t\t\t\t\t      const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long max_ptes_none;\n\n\terr = strict_strtoul(buf, 10, &max_ptes_none);\n\tif (err || max_ptes_none > HPAGE_PMD_NR-1)\n\t\treturn -EINVAL;\n\n\tkhugepaged_max_ptes_none = max_ptes_none;\n\n\treturn count;\n}\nstatic struct kobj_attribute khugepaged_max_ptes_none_attr =\n\t__ATTR(max_ptes_none, 0644, khugepaged_max_ptes_none_show,\n\t       khugepaged_max_ptes_none_store);\n\nstatic struct attribute *khugepaged_attr[] = {\n\t&khugepaged_defrag_attr.attr,\n\t&khugepaged_max_ptes_none_attr.attr,\n\t&pages_to_scan_attr.attr,\n\t&pages_collapsed_attr.attr,\n\t&full_scans_attr.attr,\n\t&scan_sleep_millisecs_attr.attr,\n\t&alloc_sleep_millisecs_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group khugepaged_attr_group = {\n\t.attrs = khugepaged_attr,\n\t.name = \"khugepaged\",\n};\n#endif /* CONFIG_SYSFS */\n\nstatic int __init hugepage_init(void)\n{\n\tint err;\n#ifdef CONFIG_SYSFS\n\tstatic struct kobject *hugepage_kobj;\n#endif\n\n\terr = -EINVAL;\n\tif (!has_transparent_hugepage()) {\n\t\ttransparent_hugepage_flags = 0;\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_SYSFS\n\terr = -ENOMEM;\n\thugepage_kobj = kobject_create_and_add(\"transparent_hugepage\", mm_kobj);\n\tif (unlikely(!hugepage_kobj)) {\n\t\tprintk(KERN_ERR \"hugepage: failed kobject create\\n\");\n\t\tgoto out;\n\t}\n\n\terr = sysfs_create_group(hugepage_kobj, &hugepage_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"hugepage: failed register hugeage group\\n\");\n\t\tgoto out;\n\t}\n\n\terr = sysfs_create_group(hugepage_kobj, &khugepaged_attr_group);\n\tif (err) {\n\t\tprintk(KERN_ERR \"hugepage: failed register hugeage group\\n\");\n\t\tgoto out;\n\t}\n#endif\n\n\terr = khugepaged_slab_init();\n\tif (err)\n\t\tgoto out;\n\n\terr = mm_slots_hash_init();\n\tif (err) {\n\t\tkhugepaged_slab_free();\n\t\tgoto out;\n\t}\n\n\t/*\n\t * By default disable transparent hugepages on smaller systems,\n\t * where the extra memory used could hurt more than TLB overhead\n\t * is likely to save.  The admin can still enable it through /sys.\n\t */\n\tif (totalram_pages < (512 << (20 - PAGE_SHIFT)))\n\t\ttransparent_hugepage_flags = 0;\n\n\tstart_khugepaged();\n\n\tset_recommended_min_free_kbytes();\n\nout:\n\treturn err;\n}\nmodule_init(hugepage_init)\n\nstatic int __init setup_transparent_hugepage(char *str)\n{\n\tint ret = 0;\n\tif (!str)\n\t\tgoto out;\n\tif (!strcmp(str, \"always\")) {\n\t\tset_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t&transparent_hugepage_flags);\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tret = 1;\n\t} else if (!strcmp(str, \"madvise\")) {\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tset_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t&transparent_hugepage_flags);\n\t\tret = 1;\n\t} else if (!strcmp(str, \"never\")) {\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tclear_bit(TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\t\t\t  &transparent_hugepage_flags);\n\t\tret = 1;\n\t}\nout:\n\tif (!ret)\n\t\tprintk(KERN_WARNING\n\t\t       \"transparent_hugepage= cannot parse, ignored\\n\");\n\treturn ret;\n}\n__setup(\"transparent_hugepage=\", setup_transparent_hugepage);\n\nstatic void prepare_pmd_huge_pte(pgtable_t pgtable,\n\t\t\t\t struct mm_struct *mm)\n{\n\tassert_spin_locked(&mm->page_table_lock);\n\n\t/* FIFO */\n\tif (!mm->pmd_huge_pte)\n\t\tINIT_LIST_HEAD(&pgtable->lru);\n\telse\n\t\tlist_add(&pgtable->lru, &mm->pmd_huge_pte->lru);\n\tmm->pmd_huge_pte = pgtable;\n}\n\nstatic inline pmd_t maybe_pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpmd = pmd_mkwrite(pmd);\n\treturn pmd;\n}\n\nstatic int __do_huge_pmd_anonymous_page(struct mm_struct *mm,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned long haddr, pmd_t *pmd,\n\t\t\t\t\tstruct page *page)\n{\n\tint ret = 0;\n\tpgtable_t pgtable;\n\n\tVM_BUG_ON(!PageCompound(page));\n\tpgtable = pte_alloc_one(mm, haddr);\n\tif (unlikely(!pgtable)) {\n\t\tmem_cgroup_uncharge_page(page);\n\t\tput_page(page);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\tclear_huge_page(page, haddr, HPAGE_PMD_NR);\n\t__SetPageUptodate(page);\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_none(*pmd))) {\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tmem_cgroup_uncharge_page(page);\n\t\tput_page(page);\n\t\tpte_free(mm, pgtable);\n\t} else {\n\t\tpmd_t entry;\n\t\tentry = mk_pmd(page, vma->vm_page_prot);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tentry = pmd_mkhuge(entry);\n\t\t/*\n\t\t * The spinlocking to take the lru_lock inside\n\t\t * page_add_new_anon_rmap() acts as a full memory\n\t\t * barrier to be sure clear_huge_page writes become\n\t\t * visible after the set_pmd_at() write.\n\t\t */\n\t\tpage_add_new_anon_rmap(page, vma, haddr);\n\t\tset_pmd_at(mm, haddr, pmd, entry);\n\t\tprepare_pmd_huge_pte(pgtable, mm);\n\t\tadd_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);\n\t\tspin_unlock(&mm->page_table_lock);\n\t}\n\n\treturn ret;\n}\n\nstatic inline gfp_t alloc_hugepage_gfpmask(int defrag, gfp_t extra_gfp)\n{\n\treturn (GFP_TRANSHUGE & ~(defrag ? 0 : __GFP_WAIT)) | extra_gfp;\n}\n\nstatic inline struct page *alloc_hugepage_vma(int defrag,\n\t\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t\t      unsigned long haddr, int nd,\n\t\t\t\t\t      gfp_t extra_gfp)\n{\n\treturn alloc_pages_vma(alloc_hugepage_gfpmask(defrag, extra_gfp),\n\t\t\t       HPAGE_PMD_ORDER, vma, haddr, nd);\n}\n\n#ifndef CONFIG_NUMA\nstatic inline struct page *alloc_hugepage(int defrag)\n{\n\treturn alloc_pages(alloc_hugepage_gfpmask(defrag, 0),\n\t\t\t   HPAGE_PMD_ORDER);\n}\n#endif\n\nint do_huge_pmd_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t       unsigned long address, pmd_t *pmd,\n\t\t\t       unsigned int flags)\n{\n\tstruct page *page;\n\tunsigned long haddr = address & HPAGE_PMD_MASK;\n\tpte_t *pte;\n\n\tif (haddr >= vma->vm_start && haddr + HPAGE_PMD_SIZE <= vma->vm_end) {\n\t\tif (unlikely(anon_vma_prepare(vma)))\n\t\t\treturn VM_FAULT_OOM;\n\t\tif (unlikely(khugepaged_enter(vma)))\n\t\t\treturn VM_FAULT_OOM;\n\t\tpage = alloc_hugepage_vma(transparent_hugepage_defrag(vma),\n\t\t\t\t\t  vma, haddr, numa_node_id(), 0);\n\t\tif (unlikely(!page)) {\n\t\t\tcount_vm_event(THP_FAULT_FALLBACK);\n\t\t\tgoto out;\n\t\t}\n\t\tcount_vm_event(THP_FAULT_ALLOC);\n\t\tif (unlikely(mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))) {\n\t\t\tput_page(page);\n\t\t\tgoto out;\n\t\t}\n\n\t\treturn __do_huge_pmd_anonymous_page(mm, vma, haddr, pmd, page);\n\t}\nout:\n\t/*\n\t * Use __pte_alloc instead of pte_alloc_map, because we can't\n\t * run pte_offset_map on the pmd, if an huge pmd could\n\t * materialize from under us from a different thread.\n\t */\n\tif (unlikely(__pte_alloc(mm, vma, pmd, address)))\n\t\treturn VM_FAULT_OOM;\n\t/* if an huge pmd materialized from under us just retry later */\n\tif (unlikely(pmd_trans_huge(*pmd)))\n\t\treturn 0;\n\t/*\n\t * A regular pmd is established and it can't morph into a huge pmd\n\t * from under us anymore at this point because we hold the mmap_sem\n\t * read mode and khugepaged takes it in write mode. So now it's\n\t * safe to run pte_offset_map().\n\t */\n\tpte = pte_offset_map(pmd, address);\n\treturn handle_pte_fault(mm, vma, address, pte, pmd, flags);\n}\n\nint copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t\t  struct vm_area_struct *vma)\n{\n\tstruct page *src_page;\n\tpmd_t pmd;\n\tpgtable_t pgtable;\n\tint ret;\n\n\tret = -ENOMEM;\n\tpgtable = pte_alloc_one(dst_mm, addr);\n\tif (unlikely(!pgtable))\n\t\tgoto out;\n\n\tspin_lock(&dst_mm->page_table_lock);\n\tspin_lock_nested(&src_mm->page_table_lock, SINGLE_DEPTH_NESTING);\n\n\tret = -EAGAIN;\n\tpmd = *src_pmd;\n\tif (unlikely(!pmd_trans_huge(pmd))) {\n\t\tpte_free(dst_mm, pgtable);\n\t\tgoto out_unlock;\n\t}\n\tif (unlikely(pmd_trans_splitting(pmd))) {\n\t\t/* split huge page running from under us */\n\t\tspin_unlock(&src_mm->page_table_lock);\n\t\tspin_unlock(&dst_mm->page_table_lock);\n\t\tpte_free(dst_mm, pgtable);\n\n\t\twait_split_huge_page(vma->anon_vma, src_pmd); /* src_vma */\n\t\tgoto out;\n\t}\n\tsrc_page = pmd_page(pmd);\n\tVM_BUG_ON(!PageHead(src_page));\n\tget_page(src_page);\n\tpage_dup_rmap(src_page);\n\tadd_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);\n\n\tpmdp_set_wrprotect(src_mm, addr, src_pmd);\n\tpmd = pmd_mkold(pmd_wrprotect(pmd));\n\tset_pmd_at(dst_mm, addr, dst_pmd, pmd);\n\tprepare_pmd_huge_pte(pgtable, dst_mm);\n\n\tret = 0;\nout_unlock:\n\tspin_unlock(&src_mm->page_table_lock);\n\tspin_unlock(&dst_mm->page_table_lock);\nout:\n\treturn ret;\n}\n\n/* no \"address\" argument so destroys page coloring of some arch */\npgtable_t get_pmd_huge_pte(struct mm_struct *mm)\n{\n\tpgtable_t pgtable;\n\n\tassert_spin_locked(&mm->page_table_lock);\n\n\t/* FIFO */\n\tpgtable = mm->pmd_huge_pte;\n\tif (list_empty(&pgtable->lru))\n\t\tmm->pmd_huge_pte = NULL;\n\telse {\n\t\tmm->pmd_huge_pte = list_entry(pgtable->lru.next,\n\t\t\t\t\t      struct page, lru);\n\t\tlist_del(&pgtable->lru);\n\t}\n\treturn pgtable;\n}\n\nstatic int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpmd_t *pmd, pmd_t orig_pmd,\n\t\t\t\t\tstruct page *page,\n\t\t\t\t\tunsigned long haddr)\n{\n\tpgtable_t pgtable;\n\tpmd_t _pmd;\n\tint ret = 0, i;\n\tstruct page **pages;\n\n\tpages = kmalloc(sizeof(struct page *) * HPAGE_PMD_NR,\n\t\t\tGFP_KERNEL);\n\tif (unlikely(!pages)) {\n\t\tret |= VM_FAULT_OOM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tpages[i] = alloc_page_vma_node(GFP_HIGHUSER_MOVABLE |\n\t\t\t\t\t       __GFP_OTHER_NODE,\n\t\t\t\t\t       vma, address, page_to_nid(page));\n\t\tif (unlikely(!pages[i] ||\n\t\t\t     mem_cgroup_newpage_charge(pages[i], mm,\n\t\t\t\t\t\t       GFP_KERNEL))) {\n\t\t\tif (pages[i])\n\t\t\t\tput_page(pages[i]);\n\t\t\tmem_cgroup_uncharge_start();\n\t\t\twhile (--i >= 0) {\n\t\t\t\tmem_cgroup_uncharge_page(pages[i]);\n\t\t\t\tput_page(pages[i]);\n\t\t\t}\n\t\t\tmem_cgroup_uncharge_end();\n\t\t\tkfree(pages);\n\t\t\tret |= VM_FAULT_OOM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tcopy_user_highpage(pages[i], page + i,\n\t\t\t\t   haddr + PAGE_SHIFT*i, vma);\n\t\t__SetPageUptodate(pages[i]);\n\t\tcond_resched();\n\t}\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd)))\n\t\tgoto out_free_pages;\n\tVM_BUG_ON(!PageHead(page));\n\n\tpmdp_clear_flush_notify(vma, haddr, pmd);\n\t/* leave pmd empty until pte is filled */\n\n\tpgtable = get_pmd_huge_pte(mm);\n\tpmd_populate(mm, &_pmd, pgtable);\n\n\tfor (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {\n\t\tpte_t *pte, entry;\n\t\tentry = mk_pte(pages[i], vma->vm_page_prot);\n\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\tpage_add_new_anon_rmap(pages[i], vma, haddr);\n\t\tpte = pte_offset_map(&_pmd, haddr);\n\t\tVM_BUG_ON(!pte_none(*pte));\n\t\tset_pte_at(mm, haddr, pte, entry);\n\t\tpte_unmap(pte);\n\t}\n\tkfree(pages);\n\n\tmm->nr_ptes++;\n\tsmp_wmb(); /* make pte visible before pmd */\n\tpmd_populate(mm, pmd, pgtable);\n\tpage_remove_rmap(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tret |= VM_FAULT_WRITE;\n\tput_page(page);\n\nout:\n\treturn ret;\n\nout_free_pages:\n\tspin_unlock(&mm->page_table_lock);\n\tmem_cgroup_uncharge_start();\n\tfor (i = 0; i < HPAGE_PMD_NR; i++) {\n\t\tmem_cgroup_uncharge_page(pages[i]);\n\t\tput_page(pages[i]);\n\t}\n\tmem_cgroup_uncharge_end();\n\tkfree(pages);\n\tgoto out;\n}\n\nint do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pmd_t *pmd, pmd_t orig_pmd)\n{\n\tint ret = 0;\n\tstruct page *page, *new_page;\n\tunsigned long haddr;\n\n\tVM_BUG_ON(!vma->anon_vma);\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd)))\n\t\tgoto out_unlock;\n\n\tpage = pmd_page(orig_pmd);\n\tVM_BUG_ON(!PageCompound(page) || !PageHead(page));\n\thaddr = address & HPAGE_PMD_MASK;\n\tif (page_mapcount(page) == 1) {\n\t\tpmd_t entry;\n\t\tentry = pmd_mkyoung(orig_pmd);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tif (pmdp_set_access_flags(vma, haddr, pmd, entry,  1))\n\t\t\tupdate_mmu_cache(vma, address, entry);\n\t\tret |= VM_FAULT_WRITE;\n\t\tgoto out_unlock;\n\t}\n\tget_page(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tif (transparent_hugepage_enabled(vma) &&\n\t    !transparent_hugepage_debug_cow())\n\t\tnew_page = alloc_hugepage_vma(transparent_hugepage_defrag(vma),\n\t\t\t\t\t      vma, haddr, numa_node_id(), 0);\n\telse\n\t\tnew_page = NULL;\n\n\tif (unlikely(!new_page)) {\n\t\tcount_vm_event(THP_FAULT_FALLBACK);\n\t\tret = do_huge_pmd_wp_page_fallback(mm, vma, address,\n\t\t\t\t\t\t   pmd, orig_pmd, page, haddr);\n\t\tput_page(page);\n\t\tgoto out;\n\t}\n\tcount_vm_event(THP_FAULT_ALLOC);\n\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tput_page(new_page);\n\t\tput_page(page);\n\t\tret |= VM_FAULT_OOM;\n\t\tgoto out;\n\t}\n\n\tcopy_user_huge_page(new_page, page, haddr, vma, HPAGE_PMD_NR);\n\t__SetPageUptodate(new_page);\n\n\tspin_lock(&mm->page_table_lock);\n\tput_page(page);\n\tif (unlikely(!pmd_same(*pmd, orig_pmd))) {\n\t\tmem_cgroup_uncharge_page(new_page);\n\t\tput_page(new_page);\n\t} else {\n\t\tpmd_t entry;\n\t\tVM_BUG_ON(!PageHead(page));\n\t\tentry = mk_pmd(new_page, vma->vm_page_prot);\n\t\tentry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);\n\t\tentry = pmd_mkhuge(entry);\n\t\tpmdp_clear_flush_notify(vma, haddr, pmd);\n\t\tpage_add_new_anon_rmap(new_page, vma, haddr);\n\t\tset_pmd_at(mm, haddr, pmd, entry);\n\t\tupdate_mmu_cache(vma, address, entry);\n\t\tpage_remove_rmap(page);\n\t\tput_page(page);\n\t\tret |= VM_FAULT_WRITE;\n\t}\nout_unlock:\n\tspin_unlock(&mm->page_table_lock);\nout:\n\treturn ret;\n}\n\nstruct page *follow_trans_huge_pmd(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr,\n\t\t\t\t   pmd_t *pmd,\n\t\t\t\t   unsigned int flags)\n{\n\tstruct page *page = NULL;\n\n\tassert_spin_locked(&mm->page_table_lock);\n\n\tif (flags & FOLL_WRITE && !pmd_write(*pmd))\n\t\tgoto out;\n\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON(!PageHead(page));\n\tif (flags & FOLL_TOUCH) {\n\t\tpmd_t _pmd;\n\t\t/*\n\t\t * We should set the dirty bit only for FOLL_WRITE but\n\t\t * for now the dirty bit in the pmd is meaningless.\n\t\t * And if the dirty bit will become meaningful and\n\t\t * we'll only set it with FOLL_WRITE, an atomic\n\t\t * set_bit will be required on the pmd to set the\n\t\t * young bit, instead of the current set_pmd_at.\n\t\t */\n\t\t_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));\n\t\tset_pmd_at(mm, addr & HPAGE_PMD_MASK, pmd, _pmd);\n\t}\n\tpage += (addr & ~HPAGE_PMD_MASK) >> PAGE_SHIFT;\n\tVM_BUG_ON(!PageCompound(page));\n\tif (flags & FOLL_GET)\n\t\tget_page(page);\n\nout:\n\treturn page;\n}\n\nint zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t pmd_t *pmd)\n{\n\tint ret = 0;\n\n\tspin_lock(&tlb->mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tif (unlikely(pmd_trans_splitting(*pmd))) {\n\t\t\tspin_unlock(&tlb->mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma,\n\t\t\t\t\t     pmd);\n\t\t} else {\n\t\t\tstruct page *page;\n\t\t\tpgtable_t pgtable;\n\t\t\tpgtable = get_pmd_huge_pte(tlb->mm);\n\t\t\tpage = pmd_page(*pmd);\n\t\t\tpmd_clear(pmd);\n\t\t\tpage_remove_rmap(page);\n\t\t\tVM_BUG_ON(page_mapcount(page) < 0);\n\t\t\tadd_mm_counter(tlb->mm, MM_ANONPAGES, -HPAGE_PMD_NR);\n\t\t\tVM_BUG_ON(!PageHead(page));\n\t\t\tspin_unlock(&tlb->mm->page_table_lock);\n\t\t\ttlb_remove_page(tlb, page);\n\t\t\tpte_free(tlb->mm, pgtable);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\tspin_unlock(&tlb->mm->page_table_lock);\n\n\treturn ret;\n}\n\nint mincore_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, unsigned long end,\n\t\tunsigned char *vec)\n{\n\tint ret = 0;\n\n\tspin_lock(&vma->vm_mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tret = !pmd_trans_splitting(*pmd);\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\t\tif (unlikely(!ret))\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\telse {\n\t\t\t/*\n\t\t\t * All logical pages in the range are present\n\t\t\t * if backed by a huge page.\n\t\t\t */\n\t\t\tmemset(vec, 1, (end - addr) >> PAGE_SHIFT);\n\t\t}\n\t} else\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\n\treturn ret;\n}\n\nint change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long addr, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tint ret = 0;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (likely(pmd_trans_huge(*pmd))) {\n\t\tif (unlikely(pmd_trans_splitting(*pmd))) {\n\t\t\tspin_unlock(&mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\t} else {\n\t\t\tpmd_t entry;\n\n\t\t\tentry = pmdp_get_and_clear(mm, addr, pmd);\n\t\t\tentry = pmd_modify(entry, newprot);\n\t\t\tset_pmd_at(mm, addr, pmd, entry);\n\t\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\t\t\tflush_tlb_range(vma, addr, addr + HPAGE_PMD_SIZE);\n\t\t\tret = 1;\n\t\t}\n\t} else\n\t\tspin_unlock(&vma->vm_mm->page_table_lock);\n\n\treturn ret;\n}\n\npmd_t *page_check_address_pmd(struct page *page,\n\t\t\t      struct mm_struct *mm,\n\t\t\t      unsigned long address,\n\t\t\t      enum page_check_address_pmd_flag flag)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, *ret = NULL;\n\n\tif (address & ~HPAGE_PMD_MASK)\n\t\tgoto out;\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\tgoto out;\n\tif (pmd_page(*pmd) != page)\n\t\tgoto out;\n\t/*\n\t * split_vma() may create temporary aliased mappings. There is\n\t * no risk as long as all huge pmd are found and have their\n\t * splitting bit set before __split_huge_page_refcount\n\t * runs. Finding the same huge pmd more than once during the\n\t * same rmap walk is not a problem.\n\t */\n\tif (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG &&\n\t    pmd_trans_splitting(*pmd))\n\t\tgoto out;\n\tif (pmd_trans_huge(*pmd)) {\n\t\tVM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG &&\n\t\t\t  !pmd_trans_splitting(*pmd));\n\t\tret = pmd;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int __split_huge_page_splitting(struct page *page,\n\t\t\t\t       struct vm_area_struct *vma,\n\t\t\t\t       unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tint ret = 0;\n\n\tspin_lock(&mm->page_table_lock);\n\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG);\n\tif (pmd) {\n\t\t/*\n\t\t * We can't temporarily set the pmd to null in order\n\t\t * to split it, the pmd must remain marked huge at all\n\t\t * times or the VM won't take the pmd_trans_huge paths\n\t\t * and it won't wait on the anon_vma->root->lock to\n\t\t * serialize against split_huge_page*.\n\t\t */\n\t\tpmdp_splitting_flush_notify(vma, address, pmd);\n\t\tret = 1;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\n\treturn ret;\n}\n\nstatic void __split_huge_page_refcount(struct page *page)\n{\n\tint i;\n\tunsigned long head_index = page->index;\n\tstruct zone *zone = page_zone(page);\n\tint zonestat;\n\n\t/* prevent PageLRU to go away from under us, and freeze lru stats */\n\tspin_lock_irq(&zone->lru_lock);\n\tcompound_lock(page);\n\n\tfor (i = 1; i < HPAGE_PMD_NR; i++) {\n\t\tstruct page *page_tail = page + i;\n\n\t\t/* tail_page->_count cannot change */\n\t\tatomic_sub(atomic_read(&page_tail->_count), &page->_count);\n\t\tBUG_ON(page_count(page) <= 0);\n\t\tatomic_add(page_mapcount(page) + 1, &page_tail->_count);\n\t\tBUG_ON(atomic_read(&page_tail->_count) <= 0);\n\n\t\t/* after clearing PageTail the gup refcount can be released */\n\t\tsmp_mb();\n\n\t\t/*\n\t\t * retain hwpoison flag of the poisoned tail page:\n\t\t *   fix for the unsuitable process killed on Guest Machine(KVM)\n\t\t *   by the memory-failure.\n\t\t */\n\t\tpage_tail->flags &= ~PAGE_FLAGS_CHECK_AT_PREP | __PG_HWPOISON;\n\t\tpage_tail->flags |= (page->flags &\n\t\t\t\t     ((1L << PG_referenced) |\n\t\t\t\t      (1L << PG_swapbacked) |\n\t\t\t\t      (1L << PG_mlocked) |\n\t\t\t\t      (1L << PG_uptodate)));\n\t\tpage_tail->flags |= (1L << PG_dirty);\n\n\t\t/*\n\t\t * 1) clear PageTail before overwriting first_page\n\t\t * 2) clear PageTail before clearing PageHead for VM_BUG_ON\n\t\t */\n\t\tsmp_wmb();\n\n\t\t/*\n\t\t * __split_huge_page_splitting() already set the\n\t\t * splitting bit in all pmd that could map this\n\t\t * hugepage, that will ensure no CPU can alter the\n\t\t * mapcount on the head page. The mapcount is only\n\t\t * accounted in the head page and it has to be\n\t\t * transferred to all tail pages in the below code. So\n\t\t * for this code to be safe, the split the mapcount\n\t\t * can't change. But that doesn't mean userland can't\n\t\t * keep changing and reading the page contents while\n\t\t * we transfer the mapcount, so the pmd splitting\n\t\t * status is achieved setting a reserved bit in the\n\t\t * pmd, not by clearing the present bit.\n\t\t*/\n\t\tBUG_ON(page_mapcount(page_tail));\n\t\tpage_tail->_mapcount = page->_mapcount;\n\n\t\tBUG_ON(page_tail->mapping);\n\t\tpage_tail->mapping = page->mapping;\n\n\t\tpage_tail->index = ++head_index;\n\n\t\tBUG_ON(!PageAnon(page_tail));\n\t\tBUG_ON(!PageUptodate(page_tail));\n\t\tBUG_ON(!PageDirty(page_tail));\n\t\tBUG_ON(!PageSwapBacked(page_tail));\n\n\t\tmem_cgroup_split_huge_fixup(page, page_tail);\n\n\t\tlru_add_page_tail(zone, page, page_tail);\n\t}\n\n\t__dec_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);\n\t__mod_zone_page_state(zone, NR_ANON_PAGES, HPAGE_PMD_NR);\n\n\t/*\n\t * A hugepage counts for HPAGE_PMD_NR pages on the LRU statistics,\n\t * so adjust those appropriately if this page is on the LRU.\n\t */\n\tif (PageLRU(page)) {\n\t\tzonestat = NR_LRU_BASE + page_lru(page);\n\t\t__mod_zone_page_state(zone, zonestat, -(HPAGE_PMD_NR-1));\n\t}\n\n\tClearPageCompound(page);\n\tcompound_unlock(page);\n\tspin_unlock_irq(&zone->lru_lock);\n\n\tfor (i = 1; i < HPAGE_PMD_NR; i++) {\n\t\tstruct page *page_tail = page + i;\n\t\tBUG_ON(page_count(page_tail) <= 0);\n\t\t/*\n\t\t * Tail pages may be freed if there wasn't any mapping\n\t\t * like if add_to_swap() is running on a lru page that\n\t\t * had its mapping zapped. And freeing these pages\n\t\t * requires taking the lru_lock so we do the put_page\n\t\t * of the tail pages after the split is complete.\n\t\t */\n\t\tput_page(page_tail);\n\t}\n\n\t/*\n\t * Only the head page (now become a regular page) is required\n\t * to be pinned by the caller.\n\t */\n\tBUG_ON(page_count(page) <= 0);\n}\n\nstatic int __split_huge_page_map(struct page *page,\n\t\t\t\t struct vm_area_struct *vma,\n\t\t\t\t unsigned long address)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd, _pmd;\n\tint ret = 0, i;\n\tpgtable_t pgtable;\n\tunsigned long haddr;\n\n\tspin_lock(&mm->page_table_lock);\n\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG);\n\tif (pmd) {\n\t\tpgtable = get_pmd_huge_pte(mm);\n\t\tpmd_populate(mm, &_pmd, pgtable);\n\n\t\tfor (i = 0, haddr = address; i < HPAGE_PMD_NR;\n\t\t     i++, haddr += PAGE_SIZE) {\n\t\t\tpte_t *pte, entry;\n\t\t\tBUG_ON(PageCompound(page+i));\n\t\t\tentry = mk_pte(page + i, vma->vm_page_prot);\n\t\t\tentry = maybe_mkwrite(pte_mkdirty(entry), vma);\n\t\t\tif (!pmd_write(*pmd))\n\t\t\t\tentry = pte_wrprotect(entry);\n\t\t\telse\n\t\t\t\tBUG_ON(page_mapcount(page) != 1);\n\t\t\tif (!pmd_young(*pmd))\n\t\t\t\tentry = pte_mkold(entry);\n\t\t\tpte = pte_offset_map(&_pmd, haddr);\n\t\t\tBUG_ON(!pte_none(*pte));\n\t\t\tset_pte_at(mm, haddr, pte, entry);\n\t\t\tpte_unmap(pte);\n\t\t}\n\n\t\tmm->nr_ptes++;\n\t\tsmp_wmb(); /* make pte visible before pmd */\n\t\t/*\n\t\t * Up to this point the pmd is present and huge and\n\t\t * userland has the whole access to the hugepage\n\t\t * during the split (which happens in place). If we\n\t\t * overwrite the pmd with the not-huge version\n\t\t * pointing to the pte here (which of course we could\n\t\t * if all CPUs were bug free), userland could trigger\n\t\t * a small page size TLB miss on the small sized TLB\n\t\t * while the hugepage TLB entry is still established\n\t\t * in the huge TLB. Some CPU doesn't like that. See\n\t\t * http://support.amd.com/us/Processor_TechDocs/41322.pdf,\n\t\t * Erratum 383 on page 93. Intel should be safe but is\n\t\t * also warns that it's only safe if the permission\n\t\t * and cache attributes of the two entries loaded in\n\t\t * the two TLB is identical (which should be the case\n\t\t * here). But it is generally safer to never allow\n\t\t * small and huge TLB entries for the same virtual\n\t\t * address to be loaded simultaneously. So instead of\n\t\t * doing \"pmd_populate(); flush_tlb_range();\" we first\n\t\t * mark the current pmd notpresent (atomically because\n\t\t * here the pmd_trans_huge and pmd_trans_splitting\n\t\t * must remain set at all times on the pmd until the\n\t\t * split is complete for this pmd), then we flush the\n\t\t * SMP TLB and finally we write the non-huge version\n\t\t * of the pmd entry with pmd_populate.\n\t\t */\n\t\tset_pmd_at(mm, address, pmd, pmd_mknotpresent(*pmd));\n\t\tflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\t\tpmd_populate(mm, pmd, pgtable);\n\t\tret = 1;\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\n\treturn ret;\n}\n\n/* must be called with anon_vma->root->lock hold */\nstatic void __split_huge_page(struct page *page,\n\t\t\t      struct anon_vma *anon_vma)\n{\n\tint mapcount, mapcount2;\n\tstruct anon_vma_chain *avc;\n\n\tBUG_ON(!PageHead(page));\n\tBUG_ON(PageTail(page));\n\n\tmapcount = 0;\n\tlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long addr = vma_address(page, vma);\n\t\tBUG_ON(is_vma_temporary_stack(vma));\n\t\tif (addr == -EFAULT)\n\t\t\tcontinue;\n\t\tmapcount += __split_huge_page_splitting(page, vma, addr);\n\t}\n\t/*\n\t * It is critical that new vmas are added to the tail of the\n\t * anon_vma list. This guarantes that if copy_huge_pmd() runs\n\t * and establishes a child pmd before\n\t * __split_huge_page_splitting() freezes the parent pmd (so if\n\t * we fail to prevent copy_huge_pmd() from running until the\n\t * whole __split_huge_page() is complete), we will still see\n\t * the newly established pmd of the child later during the\n\t * walk, to be able to set it as pmd_trans_splitting too.\n\t */\n\tif (mapcount != page_mapcount(page))\n\t\tprintk(KERN_ERR \"mapcount %d page_mapcount %d\\n\",\n\t\t       mapcount, page_mapcount(page));\n\tBUG_ON(mapcount != page_mapcount(page));\n\n\t__split_huge_page_refcount(page);\n\n\tmapcount2 = 0;\n\tlist_for_each_entry(avc, &anon_vma->head, same_anon_vma) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long addr = vma_address(page, vma);\n\t\tBUG_ON(is_vma_temporary_stack(vma));\n\t\tif (addr == -EFAULT)\n\t\t\tcontinue;\n\t\tmapcount2 += __split_huge_page_map(page, vma, addr);\n\t}\n\tif (mapcount != mapcount2)\n\t\tprintk(KERN_ERR \"mapcount %d mapcount2 %d page_mapcount %d\\n\",\n\t\t       mapcount, mapcount2, page_mapcount(page));\n\tBUG_ON(mapcount != mapcount2);\n}\n\nint split_huge_page(struct page *page)\n{\n\tstruct anon_vma *anon_vma;\n\tint ret = 1;\n\n\tBUG_ON(!PageAnon(page));\n\tanon_vma = page_lock_anon_vma(page);\n\tif (!anon_vma)\n\t\tgoto out;\n\tret = 0;\n\tif (!PageCompound(page))\n\t\tgoto out_unlock;\n\n\tBUG_ON(!PageSwapBacked(page));\n\t__split_huge_page(page, anon_vma);\n\tcount_vm_event(THP_SPLIT);\n\n\tBUG_ON(PageCompound(page));\nout_unlock:\n\tpage_unlock_anon_vma(anon_vma);\nout:\n\treturn ret;\n}\n\n#define VM_NO_THP (VM_SPECIAL|VM_INSERTPAGE|VM_MIXEDMAP|VM_SAO| \\\n\t\t   VM_HUGETLB|VM_SHARED|VM_MAYSHARE)\n\nint hugepage_madvise(struct vm_area_struct *vma,\n\t\t     unsigned long *vm_flags, int advice)\n{\n\tswitch (advice) {\n\tcase MADV_HUGEPAGE:\n\t\t/*\n\t\t * Be somewhat over-protective like KSM for now!\n\t\t */\n\t\tif (*vm_flags & (VM_HUGEPAGE | VM_NO_THP))\n\t\t\treturn -EINVAL;\n\t\t*vm_flags &= ~VM_NOHUGEPAGE;\n\t\t*vm_flags |= VM_HUGEPAGE;\n\t\t/*\n\t\t * If the vma become good for khugepaged to scan,\n\t\t * register it here without waiting a page fault that\n\t\t * may not happen any time soon.\n\t\t */\n\t\tif (unlikely(khugepaged_enter_vma_merge(vma)))\n\t\t\treturn -ENOMEM;\n\t\tbreak;\n\tcase MADV_NOHUGEPAGE:\n\t\t/*\n\t\t * Be somewhat over-protective like KSM for now!\n\t\t */\n\t\tif (*vm_flags & (VM_NOHUGEPAGE | VM_NO_THP))\n\t\t\treturn -EINVAL;\n\t\t*vm_flags &= ~VM_HUGEPAGE;\n\t\t*vm_flags |= VM_NOHUGEPAGE;\n\t\t/*\n\t\t * Setting VM_NOHUGEPAGE will prevent khugepaged from scanning\n\t\t * this vma even if we leave the mm registered in khugepaged if\n\t\t * it got registered before VM_NOHUGEPAGE was set.\n\t\t */\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int __init khugepaged_slab_init(void)\n{\n\tmm_slot_cache = kmem_cache_create(\"khugepaged_mm_slot\",\n\t\t\t\t\t  sizeof(struct mm_slot),\n\t\t\t\t\t  __alignof__(struct mm_slot), 0, NULL);\n\tif (!mm_slot_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __init khugepaged_slab_free(void)\n{\n\tkmem_cache_destroy(mm_slot_cache);\n\tmm_slot_cache = NULL;\n}\n\nstatic inline struct mm_slot *alloc_mm_slot(void)\n{\n\tif (!mm_slot_cache)\t/* initialization failed */\n\t\treturn NULL;\n\treturn kmem_cache_zalloc(mm_slot_cache, GFP_KERNEL);\n}\n\nstatic inline void free_mm_slot(struct mm_slot *mm_slot)\n{\n\tkmem_cache_free(mm_slot_cache, mm_slot);\n}\n\nstatic int __init mm_slots_hash_init(void)\n{\n\tmm_slots_hash = kzalloc(MM_SLOTS_HASH_HEADS * sizeof(struct hlist_head),\n\t\t\t\tGFP_KERNEL);\n\tif (!mm_slots_hash)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n#if 0\nstatic void __init mm_slots_hash_free(void)\n{\n\tkfree(mm_slots_hash);\n\tmm_slots_hash = NULL;\n}\n#endif\n\nstatic struct mm_slot *get_mm_slot(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct hlist_head *bucket;\n\tstruct hlist_node *node;\n\n\tbucket = &mm_slots_hash[((unsigned long)mm / sizeof(struct mm_struct))\n\t\t\t\t% MM_SLOTS_HASH_HEADS];\n\thlist_for_each_entry(mm_slot, node, bucket, hash) {\n\t\tif (mm == mm_slot->mm)\n\t\t\treturn mm_slot;\n\t}\n\treturn NULL;\n}\n\nstatic void insert_to_mm_slots_hash(struct mm_struct *mm,\n\t\t\t\t    struct mm_slot *mm_slot)\n{\n\tstruct hlist_head *bucket;\n\n\tbucket = &mm_slots_hash[((unsigned long)mm / sizeof(struct mm_struct))\n\t\t\t\t% MM_SLOTS_HASH_HEADS];\n\tmm_slot->mm = mm;\n\thlist_add_head(&mm_slot->hash, bucket);\n}\n\nstatic inline int khugepaged_test_exit(struct mm_struct *mm)\n{\n\treturn atomic_read(&mm->mm_users) == 0;\n}\n\nint __khugepaged_enter(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint wakeup;\n\n\tmm_slot = alloc_mm_slot();\n\tif (!mm_slot)\n\t\treturn -ENOMEM;\n\n\t/* __khugepaged_exit() must not run from under us */\n\tVM_BUG_ON(khugepaged_test_exit(mm));\n\tif (unlikely(test_and_set_bit(MMF_VM_HUGEPAGE, &mm->flags))) {\n\t\tfree_mm_slot(mm_slot);\n\t\treturn 0;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tinsert_to_mm_slots_hash(mm, mm_slot);\n\t/*\n\t * Insert just behind the scanning cursor, to let the area settle\n\t * down a little.\n\t */\n\twakeup = list_empty(&khugepaged_scan.mm_head);\n\tlist_add_tail(&mm_slot->mm_node, &khugepaged_scan.mm_head);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tatomic_inc(&mm->mm_count);\n\tif (wakeup)\n\t\twake_up_interruptible(&khugepaged_wait);\n\n\treturn 0;\n}\n\nint khugepaged_enter_vma_merge(struct vm_area_struct *vma)\n{\n\tunsigned long hstart, hend;\n\tif (!vma->anon_vma)\n\t\t/*\n\t\t * Not yet faulted in so we will register later in the\n\t\t * page fault if needed.\n\t\t */\n\t\treturn 0;\n\tif (vma->vm_ops)\n\t\t/* khugepaged not yet working on file or special mappings */\n\t\treturn 0;\n\t/*\n\t * If is_pfn_mapping() is true is_learn_pfn_mapping() must be\n\t * true too, verify it here.\n\t */\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || vma->vm_flags & VM_NO_THP);\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (hstart < hend)\n\t\treturn khugepaged_enter(vma);\n\treturn 0;\n}\n\nvoid __khugepaged_exit(struct mm_struct *mm)\n{\n\tstruct mm_slot *mm_slot;\n\tint free = 0;\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = get_mm_slot(mm);\n\tif (mm_slot && khugepaged_scan.mm_slot != mm_slot) {\n\t\thlist_del(&mm_slot->hash);\n\t\tlist_del(&mm_slot->mm_node);\n\t\tfree = 1;\n\t}\n\n\tif (free) {\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t\tclear_bit(MMF_VM_HUGEPAGE, &mm->flags);\n\t\tfree_mm_slot(mm_slot);\n\t\tmmdrop(mm);\n\t} else if (mm_slot) {\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t\t/*\n\t\t * This is required to serialize against\n\t\t * khugepaged_test_exit() (which is guaranteed to run\n\t\t * under mmap sem read mode). Stop here (after we\n\t\t * return all pagetables will be destroyed) until\n\t\t * khugepaged has finished working on the pagetables\n\t\t * under the mmap_sem.\n\t\t */\n\t\tdown_write(&mm->mmap_sem);\n\t\tup_write(&mm->mmap_sem);\n\t} else\n\t\tspin_unlock(&khugepaged_mm_lock);\n}\n\nstatic void release_pte_page(struct page *page)\n{\n\t/* 0 stands for page_is_file_cache(page) == false */\n\tdec_zone_page_state(page, NR_ISOLATED_ANON + 0);\n\tunlock_page(page);\n\tputback_lru_page(page);\n}\n\nstatic void release_pte_pages(pte_t *pte, pte_t *_pte)\n{\n\twhile (--_pte >= pte) {\n\t\tpte_t pteval = *_pte;\n\t\tif (!pte_none(pteval))\n\t\t\trelease_pte_page(pte_page(pteval));\n\t}\n}\n\nstatic void release_all_pte_pages(pte_t *pte)\n{\n\trelease_pte_pages(pte, pte + HPAGE_PMD_NR);\n}\n\nstatic int __collapse_huge_page_isolate(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpte_t *pte)\n{\n\tstruct page *page;\n\tpte_t *_pte;\n\tint referenced = 0, isolated = 0, none = 0;\n\tfor (_pte = pte; _pte < pte+HPAGE_PMD_NR;\n\t     _pte++, address += PAGE_SIZE) {\n\t\tpte_t pteval = *_pte;\n\t\tif (pte_none(pteval)) {\n\t\t\tif (++none <= khugepaged_max_ptes_none)\n\t\t\t\tcontinue;\n\t\t\telse {\n\t\t\t\trelease_pte_pages(pte, _pte);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tif (!pte_present(pteval) || !pte_write(pteval)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\tpage = vm_normal_page(vma, address, pteval);\n\t\tif (unlikely(!page)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\tVM_BUG_ON(PageCompound(page));\n\t\tBUG_ON(!PageAnon(page));\n\t\tVM_BUG_ON(!PageSwapBacked(page));\n\n\t\t/* cannot use mapcount: can't collapse if there's a gup pin */\n\t\tif (page_count(page) != 1) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * We can do it before isolate_lru_page because the\n\t\t * page can't be freed from under us. NOTE: PG_lock\n\t\t * is needed to serialize against split_huge_page\n\t\t * when invoked from the VM.\n\t\t */\n\t\tif (!trylock_page(page)) {\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/*\n\t\t * Isolate the page to avoid collapsing an hugepage\n\t\t * currently in use by the VM.\n\t\t */\n\t\tif (isolate_lru_page(page)) {\n\t\t\tunlock_page(page);\n\t\t\trelease_pte_pages(pte, _pte);\n\t\t\tgoto out;\n\t\t}\n\t\t/* 0 stands for page_is_file_cache(page) == false */\n\t\tinc_zone_page_state(page, NR_ISOLATED_ANON + 0);\n\t\tVM_BUG_ON(!PageLocked(page));\n\t\tVM_BUG_ON(PageLRU(page));\n\n\t\t/* If there is no mapped pte young don't collapse the page */\n\t\tif (pte_young(pteval) || PageReferenced(page) ||\n\t\t    mmu_notifier_test_young(vma->vm_mm, address))\n\t\t\treferenced = 1;\n\t}\n\tif (unlikely(!referenced))\n\t\trelease_all_pte_pages(pte);\n\telse\n\t\tisolated = 1;\nout:\n\treturn isolated;\n}\n\nstatic void __collapse_huge_page_copy(pte_t *pte, struct page *page,\n\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address,\n\t\t\t\t      spinlock_t *ptl)\n{\n\tpte_t *_pte;\n\tfor (_pte = pte; _pte < pte+HPAGE_PMD_NR; _pte++) {\n\t\tpte_t pteval = *_pte;\n\t\tstruct page *src_page;\n\n\t\tif (pte_none(pteval)) {\n\t\t\tclear_user_highpage(page, address);\n\t\t\tadd_mm_counter(vma->vm_mm, MM_ANONPAGES, 1);\n\t\t} else {\n\t\t\tsrc_page = pte_page(pteval);\n\t\t\tcopy_user_highpage(page, src_page, address, vma);\n\t\t\tVM_BUG_ON(page_mapcount(src_page) != 1);\n\t\t\tVM_BUG_ON(page_count(src_page) != 2);\n\t\t\trelease_pte_page(src_page);\n\t\t\t/*\n\t\t\t * ptl mostly unnecessary, but preempt has to\n\t\t\t * be disabled to update the per-cpu stats\n\t\t\t * inside page_remove_rmap().\n\t\t\t */\n\t\t\tspin_lock(ptl);\n\t\t\t/*\n\t\t\t * paravirt calls inside pte_clear here are\n\t\t\t * superfluous.\n\t\t\t */\n\t\t\tpte_clear(vma->vm_mm, address, _pte);\n\t\t\tpage_remove_rmap(src_page);\n\t\t\tspin_unlock(ptl);\n\t\t\tfree_page_and_swap_cache(src_page);\n\t\t}\n\n\t\taddress += PAGE_SIZE;\n\t\tpage++;\n\t}\n}\n\nstatic void collapse_huge_page(struct mm_struct *mm,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage,\n\t\t\t       struct vm_area_struct *vma,\n\t\t\t       int node)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpgtable_t pgtable;\n\tstruct page *new_page;\n\tspinlock_t *ptl;\n\tint isolated;\n\tunsigned long hstart, hend;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n#ifndef CONFIG_NUMA\n\tVM_BUG_ON(!*hpage);\n\tnew_page = *hpage;\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\treturn;\n\t}\n#else\n\tVM_BUG_ON(*hpage);\n\t/*\n\t * Allocate the page while the vma is still valid and under\n\t * the mmap_sem read mode so there is no memory allocation\n\t * later when we take the mmap_sem in write mode. This is more\n\t * friendly behavior (OTOH it may actually hide bugs) to\n\t * filesystems in userland with daemons allocating memory in\n\t * the userland I/O paths.  Allocating memory with the\n\t * mmap_sem in read mode is good idea also to allow greater\n\t * scalability.\n\t */\n\tnew_page = alloc_hugepage_vma(khugepaged_defrag(), vma, address,\n\t\t\t\t      node, __GFP_OTHER_NODE);\n\tif (unlikely(!new_page)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t*hpage = ERR_PTR(-ENOMEM);\n\t\treturn;\n\t}\n\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\tif (unlikely(mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))) {\n\t\tup_read(&mm->mmap_sem);\n\t\tput_page(new_page);\n\t\treturn;\n\t}\n#endif\n\n\t/* after allocating the hugepage upgrade to mmap_sem write mode */\n\tup_read(&mm->mmap_sem);\n\n\t/*\n\t * Prevent all access to pagetables with the exception of\n\t * gup_fast later hanlded by the ptep_clear_flush and the VM\n\t * handled by the anon_vma lock + PG_lock.\n\t */\n\tdown_write(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tgoto out;\n\n\tvma = find_vma(mm, address);\n\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\thend = vma->vm_end & HPAGE_PMD_MASK;\n\tif (address < hstart || address + HPAGE_PMD_SIZE > hend)\n\t\tgoto out;\n\n\tif ((!(vma->vm_flags & VM_HUGEPAGE) && !khugepaged_always()) ||\n\t    (vma->vm_flags & VM_NOHUGEPAGE))\n\t\tgoto out;\n\n\tif (!vma->anon_vma || vma->vm_ops)\n\t\tgoto out;\n\tif (is_vma_temporary_stack(vma))\n\t\tgoto out;\n\t/*\n\t * If is_pfn_mapping() is true is_learn_pfn_mapping() must be\n\t * true too, verify it here.\n\t */\n\tVM_BUG_ON(is_linear_pfn_mapping(vma) || vma->vm_flags & VM_NO_THP);\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\t/* pmd can't go away or become huge under us */\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tanon_vma_lock(vma->anon_vma);\n\n\tpte = pte_offset_map(pmd, address);\n\tptl = pte_lockptr(mm, pmd);\n\n\tspin_lock(&mm->page_table_lock); /* probably unnecessary */\n\t/*\n\t * After this gup_fast can't run anymore. This also removes\n\t * any huge TLB entry from the CPU so we won't allow\n\t * huge and small TLB entries for the same virtual address\n\t * to avoid the risk of CPU bugs in that area.\n\t */\n\t_pmd = pmdp_clear_flush_notify(vma, address, pmd);\n\tspin_unlock(&mm->page_table_lock);\n\n\tspin_lock(ptl);\n\tisolated = __collapse_huge_page_isolate(vma, address, pte);\n\tspin_unlock(ptl);\n\n\tif (unlikely(!isolated)) {\n\t\tpte_unmap(pte);\n\t\tspin_lock(&mm->page_table_lock);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\tset_pmd_at(mm, address, pmd, _pmd);\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock(vma->anon_vma);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * All pages are isolated and locked so anon_vma rmap\n\t * can't run anymore.\n\t */\n\tanon_vma_unlock(vma->anon_vma);\n\n\t__collapse_huge_page_copy(pte, new_page, vma, address, ptl);\n\tpte_unmap(pte);\n\t__SetPageUptodate(new_page);\n\tpgtable = pmd_pgtable(_pmd);\n\tVM_BUG_ON(page_count(pgtable) != 1);\n\tVM_BUG_ON(page_mapcount(pgtable) != 0);\n\n\t_pmd = mk_pmd(new_page, vma->vm_page_prot);\n\t_pmd = maybe_pmd_mkwrite(pmd_mkdirty(_pmd), vma);\n\t_pmd = pmd_mkhuge(_pmd);\n\n\t/*\n\t * spin_lock() below is not the equivalent of smp_wmb(), so\n\t * this is needed to avoid the copy_huge_page writes to become\n\t * visible after the set_pmd_at() write.\n\t */\n\tsmp_wmb();\n\n\tspin_lock(&mm->page_table_lock);\n\tBUG_ON(!pmd_none(*pmd));\n\tpage_add_new_anon_rmap(new_page, vma, address);\n\tset_pmd_at(mm, address, pmd, _pmd);\n\tupdate_mmu_cache(vma, address, entry);\n\tprepare_pmd_huge_pte(pgtable, mm);\n\tmm->nr_ptes--;\n\tspin_unlock(&mm->page_table_lock);\n\n#ifndef CONFIG_NUMA\n\t*hpage = NULL;\n#endif\n\tkhugepaged_pages_collapsed++;\nout_up_write:\n\tup_write(&mm->mmap_sem);\n\treturn;\n\nout:\n\tmem_cgroup_uncharge_page(new_page);\n#ifdef CONFIG_NUMA\n\tput_page(new_page);\n#endif\n\tgoto out_up_write;\n}\n\nstatic int khugepaged_scan_pmd(struct mm_struct *mm,\n\t\t\t       struct vm_area_struct *vma,\n\t\t\t       unsigned long address,\n\t\t\t       struct page **hpage)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte, *_pte;\n\tint ret = 0, referenced = 0, none = 0;\n\tstruct page *page;\n\tunsigned long _address;\n\tspinlock_t *ptl;\n\tint node = -1;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd) || pmd_trans_huge(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\tfor (_address = address, _pte = pte; _pte < pte+HPAGE_PMD_NR;\n\t     _pte++, _address += PAGE_SIZE) {\n\t\tpte_t pteval = *_pte;\n\t\tif (pte_none(pteval)) {\n\t\t\tif (++none <= khugepaged_max_ptes_none)\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\tgoto out_unmap;\n\t\t}\n\t\tif (!pte_present(pteval) || !pte_write(pteval))\n\t\t\tgoto out_unmap;\n\t\tpage = vm_normal_page(vma, _address, pteval);\n\t\tif (unlikely(!page))\n\t\t\tgoto out_unmap;\n\t\t/*\n\t\t * Chose the node of the first page. This could\n\t\t * be more sophisticated and look at more pages,\n\t\t * but isn't for now.\n\t\t */\n\t\tif (node == -1)\n\t\t\tnode = page_to_nid(page);\n\t\tVM_BUG_ON(PageCompound(page));\n\t\tif (!PageLRU(page) || PageLocked(page) || !PageAnon(page))\n\t\t\tgoto out_unmap;\n\t\t/* cannot use mapcount: can't collapse if there's a gup pin */\n\t\tif (page_count(page) != 1)\n\t\t\tgoto out_unmap;\n\t\tif (pte_young(pteval) || PageReferenced(page) ||\n\t\t    mmu_notifier_test_young(vma->vm_mm, address))\n\t\t\treferenced = 1;\n\t}\n\tif (referenced)\n\t\tret = 1;\nout_unmap:\n\tpte_unmap_unlock(pte, ptl);\n\tif (ret)\n\t\t/* collapse_huge_page will return with the mmap_sem released */\n\t\tcollapse_huge_page(mm, address, hpage, vma, node);\nout:\n\treturn ret;\n}\n\nstatic void collect_mm_slot(struct mm_slot *mm_slot)\n{\n\tstruct mm_struct *mm = mm_slot->mm;\n\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_test_exit(mm)) {\n\t\t/* free mm_slot */\n\t\thlist_del(&mm_slot->hash);\n\t\tlist_del(&mm_slot->mm_node);\n\n\t\t/*\n\t\t * Not strictly needed because the mm exited already.\n\t\t *\n\t\t * clear_bit(MMF_VM_HUGEPAGE, &mm->flags);\n\t\t */\n\n\t\t/* khugepaged_mm_lock actually not necessary for the below */\n\t\tfree_mm_slot(mm_slot);\n\t\tmmdrop(mm);\n\t}\n}\n\nstatic unsigned int khugepaged_scan_mm_slot(unsigned int pages,\n\t\t\t\t\t    struct page **hpage)\n{\n\tstruct mm_slot *mm_slot;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tint progress = 0;\n\n\tVM_BUG_ON(!pages);\n\tVM_BUG_ON(!spin_is_locked(&khugepaged_mm_lock));\n\n\tif (khugepaged_scan.mm_slot)\n\t\tmm_slot = khugepaged_scan.mm_slot;\n\telse {\n\t\tmm_slot = list_entry(khugepaged_scan.mm_head.next,\n\t\t\t\t     struct mm_slot, mm_node);\n\t\tkhugepaged_scan.address = 0;\n\t\tkhugepaged_scan.mm_slot = mm_slot;\n\t}\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tmm = mm_slot->mm;\n\tdown_read(&mm->mmap_sem);\n\tif (unlikely(khugepaged_test_exit(mm)))\n\t\tvma = NULL;\n\telse\n\t\tvma = find_vma(mm, khugepaged_scan.address);\n\n\tprogress++;\n\tfor (; vma; vma = vma->vm_next) {\n\t\tunsigned long hstart, hend;\n\n\t\tcond_resched();\n\t\tif (unlikely(khugepaged_test_exit(mm))) {\n\t\t\tprogress++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((!(vma->vm_flags & VM_HUGEPAGE) &&\n\t\t     !khugepaged_always()) ||\n\t\t    (vma->vm_flags & VM_NOHUGEPAGE)) {\n\t\tskip:\n\t\t\tprogress++;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!vma->anon_vma || vma->vm_ops)\n\t\t\tgoto skip;\n\t\tif (is_vma_temporary_stack(vma))\n\t\t\tgoto skip;\n\t\t/*\n\t\t * If is_pfn_mapping() is true is_learn_pfn_mapping()\n\t\t * must be true too, verify it here.\n\t\t */\n\t\tVM_BUG_ON(is_linear_pfn_mapping(vma) ||\n\t\t\t  vma->vm_flags & VM_NO_THP);\n\n\t\thstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;\n\t\thend = vma->vm_end & HPAGE_PMD_MASK;\n\t\tif (hstart >= hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address > hend)\n\t\t\tgoto skip;\n\t\tif (khugepaged_scan.address < hstart)\n\t\t\tkhugepaged_scan.address = hstart;\n\t\tVM_BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);\n\n\t\twhile (khugepaged_scan.address < hend) {\n\t\t\tint ret;\n\t\t\tcond_resched();\n\t\t\tif (unlikely(khugepaged_test_exit(mm)))\n\t\t\t\tgoto breakouterloop;\n\n\t\t\tVM_BUG_ON(khugepaged_scan.address < hstart ||\n\t\t\t\t  khugepaged_scan.address + HPAGE_PMD_SIZE >\n\t\t\t\t  hend);\n\t\t\tret = khugepaged_scan_pmd(mm, vma,\n\t\t\t\t\t\t  khugepaged_scan.address,\n\t\t\t\t\t\t  hpage);\n\t\t\t/* move to next address */\n\t\t\tkhugepaged_scan.address += HPAGE_PMD_SIZE;\n\t\t\tprogress += HPAGE_PMD_NR;\n\t\t\tif (ret)\n\t\t\t\t/* we released mmap_sem so break loop */\n\t\t\t\tgoto breakouterloop_mmap_sem;\n\t\t\tif (progress >= pages)\n\t\t\t\tgoto breakouterloop;\n\t\t}\n\t}\nbreakouterloop:\n\tup_read(&mm->mmap_sem); /* exit_mmap will destroy ptes after this */\nbreakouterloop_mmap_sem:\n\n\tspin_lock(&khugepaged_mm_lock);\n\tVM_BUG_ON(khugepaged_scan.mm_slot != mm_slot);\n\t/*\n\t * Release the current mm_slot if this mm is about to die, or\n\t * if we scanned all vmas of this mm.\n\t */\n\tif (khugepaged_test_exit(mm) || !vma) {\n\t\t/*\n\t\t * Make sure that if mm_users is reaching zero while\n\t\t * khugepaged runs here, khugepaged_exit will find\n\t\t * mm_slot not pointing to the exiting mm.\n\t\t */\n\t\tif (mm_slot->mm_node.next != &khugepaged_scan.mm_head) {\n\t\t\tkhugepaged_scan.mm_slot = list_entry(\n\t\t\t\tmm_slot->mm_node.next,\n\t\t\t\tstruct mm_slot, mm_node);\n\t\t\tkhugepaged_scan.address = 0;\n\t\t} else {\n\t\t\tkhugepaged_scan.mm_slot = NULL;\n\t\t\tkhugepaged_full_scans++;\n\t\t}\n\n\t\tcollect_mm_slot(mm_slot);\n\t}\n\n\treturn progress;\n}\n\nstatic int khugepaged_has_work(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) &&\n\t\tkhugepaged_enabled();\n}\n\nstatic int khugepaged_wait_event(void)\n{\n\treturn !list_empty(&khugepaged_scan.mm_head) ||\n\t\t!khugepaged_enabled();\n}\n\nstatic void khugepaged_do_scan(struct page **hpage)\n{\n\tunsigned int progress = 0, pass_through_head = 0;\n\tunsigned int pages = khugepaged_pages_to_scan;\n\n\tbarrier(); /* write khugepaged_pages_to_scan to local stack */\n\n\twhile (progress < pages) {\n\t\tcond_resched();\n\n#ifndef CONFIG_NUMA\n\t\tif (!*hpage) {\n\t\t\t*hpage = alloc_hugepage(khugepaged_defrag());\n\t\t\tif (unlikely(!*hpage)) {\n\t\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\t\t}\n#else\n\t\tif (IS_ERR(*hpage))\n\t\t\tbreak;\n#endif\n\n\t\tif (unlikely(kthread_should_stop() || freezing(current)))\n\t\t\tbreak;\n\n\t\tspin_lock(&khugepaged_mm_lock);\n\t\tif (!khugepaged_scan.mm_slot)\n\t\t\tpass_through_head++;\n\t\tif (khugepaged_has_work() &&\n\t\t    pass_through_head < 2)\n\t\t\tprogress += khugepaged_scan_mm_slot(pages - progress,\n\t\t\t\t\t\t\t    hpage);\n\t\telse\n\t\t\tprogress = pages;\n\t\tspin_unlock(&khugepaged_mm_lock);\n\t}\n}\n\nstatic void khugepaged_alloc_sleep(void)\n{\n\tDEFINE_WAIT(wait);\n\tadd_wait_queue(&khugepaged_wait, &wait);\n\tschedule_timeout_interruptible(\n\t\tmsecs_to_jiffies(\n\t\t\tkhugepaged_alloc_sleep_millisecs));\n\tremove_wait_queue(&khugepaged_wait, &wait);\n}\n\n#ifndef CONFIG_NUMA\nstatic struct page *khugepaged_alloc_hugepage(void)\n{\n\tstruct page *hpage;\n\n\tdo {\n\t\thpage = alloc_hugepage(khugepaged_defrag());\n\t\tif (!hpage) {\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\tkhugepaged_alloc_sleep();\n\t\t} else\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n\t} while (unlikely(!hpage) &&\n\t\t likely(khugepaged_enabled()));\n\treturn hpage;\n}\n#endif\n\nstatic void khugepaged_loop(void)\n{\n\tstruct page *hpage;\n\n#ifdef CONFIG_NUMA\n\thpage = NULL;\n#endif\n\twhile (likely(khugepaged_enabled())) {\n#ifndef CONFIG_NUMA\n\t\thpage = khugepaged_alloc_hugepage();\n\t\tif (unlikely(!hpage)) {\n\t\t\tcount_vm_event(THP_COLLAPSE_ALLOC_FAILED);\n\t\t\tbreak;\n\t\t}\n\t\tcount_vm_event(THP_COLLAPSE_ALLOC);\n#else\n\t\tif (IS_ERR(hpage)) {\n\t\t\tkhugepaged_alloc_sleep();\n\t\t\thpage = NULL;\n\t\t}\n#endif\n\n\t\tkhugepaged_do_scan(&hpage);\n#ifndef CONFIG_NUMA\n\t\tif (hpage)\n\t\t\tput_page(hpage);\n#endif\n\t\ttry_to_freeze();\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t\tif (khugepaged_has_work()) {\n\t\t\tDEFINE_WAIT(wait);\n\t\t\tif (!khugepaged_scan_sleep_millisecs)\n\t\t\t\tcontinue;\n\t\t\tadd_wait_queue(&khugepaged_wait, &wait);\n\t\t\tschedule_timeout_interruptible(\n\t\t\t\tmsecs_to_jiffies(\n\t\t\t\t\tkhugepaged_scan_sleep_millisecs));\n\t\t\tremove_wait_queue(&khugepaged_wait, &wait);\n\t\t} else if (khugepaged_enabled())\n\t\t\twait_event_freezable(khugepaged_wait,\n\t\t\t\t\t     khugepaged_wait_event());\n\t}\n}\n\nstatic int khugepaged(void *none)\n{\n\tstruct mm_slot *mm_slot;\n\n\tset_freezable();\n\tset_user_nice(current, 19);\n\n\t/* serialize with start_khugepaged() */\n\tmutex_lock(&khugepaged_mutex);\n\n\tfor (;;) {\n\t\tmutex_unlock(&khugepaged_mutex);\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\t\tkhugepaged_loop();\n\t\tVM_BUG_ON(khugepaged_thread != current);\n\n\t\tmutex_lock(&khugepaged_mutex);\n\t\tif (!khugepaged_enabled())\n\t\t\tbreak;\n\t\tif (unlikely(kthread_should_stop()))\n\t\t\tbreak;\n\t}\n\n\tspin_lock(&khugepaged_mm_lock);\n\tmm_slot = khugepaged_scan.mm_slot;\n\tkhugepaged_scan.mm_slot = NULL;\n\tif (mm_slot)\n\t\tcollect_mm_slot(mm_slot);\n\tspin_unlock(&khugepaged_mm_lock);\n\n\tkhugepaged_thread = NULL;\n\tmutex_unlock(&khugepaged_mutex);\n\n\treturn 0;\n}\n\nvoid __split_huge_page_pmd(struct mm_struct *mm, pmd_t *pmd)\n{\n\tstruct page *page;\n\n\tspin_lock(&mm->page_table_lock);\n\tif (unlikely(!pmd_trans_huge(*pmd))) {\n\t\tspin_unlock(&mm->page_table_lock);\n\t\treturn;\n\t}\n\tpage = pmd_page(*pmd);\n\tVM_BUG_ON(!page_count(page));\n\tget_page(page);\n\tspin_unlock(&mm->page_table_lock);\n\n\tsplit_huge_page(page);\n\n\tput_page(page);\n\tBUG_ON(pmd_trans_huge(*pmd));\n}\n\nstatic void split_huge_page_address(struct mm_struct *mm,\n\t\t\t\t    unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tVM_BUG_ON(!(address & ~HPAGE_PMD_MASK));\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\treturn;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\treturn;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn;\n\t/*\n\t * Caller holds the mmap_sem write mode, so a huge pmd cannot\n\t * materialize from under us.\n\t */\n\tsplit_huge_page_pmd(mm, pmd);\n}\n\nvoid __vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t     unsigned long start,\n\t\t\t     unsigned long end,\n\t\t\t     long adjust_next)\n{\n\t/*\n\t * If the new start address isn't hpage aligned and it could\n\t * previously contain an hugepage: check if we need to split\n\t * an huge pmd.\n\t */\n\tif (start & ~HPAGE_PMD_MASK &&\n\t    (start & HPAGE_PMD_MASK) >= vma->vm_start &&\n\t    (start & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)\n\t\tsplit_huge_page_address(vma->vm_mm, start);\n\n\t/*\n\t * If the new end address isn't hpage aligned and it could\n\t * previously contain an hugepage: check if we need to split\n\t * an huge pmd.\n\t */\n\tif (end & ~HPAGE_PMD_MASK &&\n\t    (end & HPAGE_PMD_MASK) >= vma->vm_start &&\n\t    (end & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= vma->vm_end)\n\t\tsplit_huge_page_address(vma->vm_mm, end);\n\n\t/*\n\t * If we're also updating the vma->vm_next->vm_start, if the new\n\t * vm_next->vm_start isn't page aligned and it could previously\n\t * contain an hugepage: check if we need to split an huge pmd.\n\t */\n\tif (adjust_next > 0) {\n\t\tstruct vm_area_struct *next = vma->vm_next;\n\t\tunsigned long nstart = next->vm_start;\n\t\tnstart += adjust_next << PAGE_SHIFT;\n\t\tif (nstart & ~HPAGE_PMD_MASK &&\n\t\t    (nstart & HPAGE_PMD_MASK) >= next->vm_start &&\n\t\t    (nstart & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE <= next->vm_end)\n\t\t\tsplit_huge_page_address(next->vm_mm, nstart);\n\t}\n}\n"], "filenames": ["include/linux/huge_mm.h", "include/linux/mm.h", "mm/huge_memory.c"], "buggy_code_start_loc": [120, 140, 1410], "buggy_code_end_loc": [121, 141, 2076], "fixing_code_start_loc": [120, 140, 1411], "fixing_code_end_loc": [121, 142, 2081], "type": "CWE-399", "message": "The Linux kernel before 2.6.39 does not properly create transparent huge pages in response to a MAP_PRIVATE mmap system call on /dev/zero, which allows local users to cause a denial of service (system crash) via a crafted application.", "other": {"cve": {"id": "CVE-2011-2479", "sourceIdentifier": "secalert@redhat.com", "published": "2013-03-01T12:37:53.693", "lastModified": "2023-02-13T04:31:05.227", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 2.6.39 does not properly create transparent huge pages in response to a MAP_PRIVATE mmap system call on /dev/zero, which allows local users to cause a denial of service (system crash) via a crafted application."}, {"lang": "es", "value": "El kernel de Linux anterior a v2.6.39  no crea correctamente p\u00e1ginas grandes en respuesta a una llamada al sistema mmap MAP_PRIVATE en /dev/zero, permitiendo a usuarios locales provocar una denegaci\u00f3n de servicio (ca\u00edda del sistema) a trav\u00e9s de una aplicaci\u00f3n especialmente dise\u00f1ada."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.39", "matchCriteriaId": "176353CE-F17E-4776-AD9F-19014DA75B76"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.39", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=78f11a255749d09025f54d4e2df4fbcb031530e2", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/06/20/14", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=714761", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/78f11a255749d09025f54d4e2df4fbcb031530e2", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/78f11a255749d09025f54d4e2df4fbcb031530e2"}}