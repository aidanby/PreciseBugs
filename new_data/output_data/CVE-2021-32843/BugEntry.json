{"buggy_code": ["/*-\n * Copyright (c) 2013  Chris Torek <torek @ torek net>\n * Copyright (c) 2015 xhyve developers\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n */\n\n#include <stdio.h>\n#include <stdint.h>\n#include <pthread.h>\n#include <sys/param.h>\n#include <sys/uio.h>\n#include <xhyve/support/misc.h>\n#include <xhyve/xhyve.h>\n#include <xhyve/pci_emul.h>\n#include <xhyve/virtio.h>\n\n/*\n * Functions for dealing with generalized \"virtual devices\" as\n * defined by <https://www.google.com/#output=search&q=virtio+spec>\n */\n\n/*\n * In case we decide to relax the \"virtio softc comes at the\n * front of virtio-based device softc\" constraint, let's use\n * this to convert.\n */\n#define DEV_SOFTC(vs) ((void *)(vs))\n\n/*\n * Link a virtio_softc to its constants, the device softc, and\n * the PCI emulation.\n */\nvoid\nvi_softc_linkup(struct virtio_softc *vs, struct virtio_consts *vc,\n\t\tvoid *dev_softc, struct pci_devinst *pi,\n\t\tstruct vqueue_info *queues)\n{\n\tint i;\n\n\t/* vs and dev_softc addresses must match */\n\tassert((void *)vs == dev_softc);\n\tvs->vs_vc = vc;\n\tvs->vs_pi = pi;\n\tpi->pi_arg = vs;\n\n\tvs->vs_queues = queues;\n\tfor (i = 0; i < vc->vc_nvq; i++) {\n\t\tqueues[i].vq_vs = vs;\n\t\tqueues[i].vq_num = (uint16_t) i;\n\t}\n}\n\n/*\n * Reset device (device-wide).  This erases all queues, i.e.,\n * all the queues become invalid (though we don't wipe out the\n * internal pointers, we just clear the VQ_ALLOC flag).\n *\n * It resets negotiated features to \"none\".\n *\n * If MSI-X is enabled, this also resets all the vectors to NO_VECTOR.\n */\nvoid\nvi_reset_dev(struct virtio_softc *vs)\n{\n\tstruct vqueue_info *vq;\n\tint i, nvq;\n\n\tnvq = vs->vs_vc->vc_nvq;\n\tfor (vq = vs->vs_queues, i = 0; i < nvq; vq++, i++) {\n\t\tvq->vq_flags = 0;\n\t\tvq->vq_last_avail = 0;\n\t\tvq->vq_save_used = 0;\n\t\tvq->vq_pfn = 0;\n\t\tvq->vq_msix_idx = VIRTIO_MSI_NO_VECTOR;\n\t}\n\tvs->vs_negotiated_caps = 0;\n\tvs->vs_curq = 0;\n\t/* vs->vs_status = 0; -- redundant */\n\tif (vs->vs_isr)\n\t\tpci_lintr_deassert(vs->vs_pi);\n\tvs->vs_isr = 0;\n\tvs->vs_msix_cfg_idx = VIRTIO_MSI_NO_VECTOR;\n}\n\n/*\n * Set I/O BAR (usually 0) to map PCI config registers.\n */\nvoid\nvi_set_io_bar(struct virtio_softc *vs, int barnum)\n{\n\tsize_t size;\n\n\t/*\n\t * ??? should we use CFG0 if MSI-X is disabled?\n\t * Existing code did not...\n\t */\n\tsize = VTCFG_R_CFG1 + vs->vs_vc->vc_cfgsize;\n\tpci_emul_alloc_bar(vs->vs_pi, barnum, PCIBAR_IO, size);\n}\n\n/*\n * Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * We assume we want one MSI-X vector per queue, here, plus one\n * for the config vec.\n */\nint\nvi_intr_init(struct virtio_softc *vs, int barnum, int use_msix)\n{\n\tint nvec;\n\n\tif (use_msix) {\n\t\tvs->vs_flags |= VIRTIO_USE_MSIX;\n\t\tVS_LOCK(vs);\n\t\tvi_reset_dev(vs); /* set all vectors to NO_VECTOR */\n\t\tVS_UNLOCK(vs);\n\t\tnvec = vs->vs_vc->vc_nvq + 1;\n\t\tif (pci_emul_add_msixcap(vs->vs_pi, nvec, barnum))\n\t\t\treturn (1);\n\t} else\n\t\tvs->vs_flags &= ~VIRTIO_USE_MSIX;\n\n\t/* Only 1 MSI vector for bhyve */\n\tpci_emul_add_msicap(vs->vs_pi, 1);\n\n\t/* Legacy interrupts are mandatory for virtio devices */\n\tpci_lintr_request(vs->vs_pi);\n\n\treturn (0);\n}\n\n/*\n * Initialize the currently-selected virtio queue (vs->vs_curq).\n * The guest just gave us a page frame number, from which we can\n * calculate the addresses of the queue.\n */\nstatic void\nvi_vq_init(struct virtio_softc *vs, uint32_t pfn)\n{\n\tstruct vqueue_info *vq;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *base;\n\n\tvq = &vs->vs_queues[vs->vs_curq];\n\tvq->vq_pfn = pfn;\n\tphys = (uint64_t)pfn << VRING_PFN;\n\tsize = vring_size(vq->vq_qsize);\n\tbase = paddr_guest2host(phys, size);\n\n\t/* First page(s) are descriptors... */\n\tvq->vq_desc = (struct virtio_desc *)base;\n\tbase += vq->vq_qsize * sizeof(struct virtio_desc);\n\n\t/* ... immediately followed by \"avail\" ring (entirely uint16_t's) */\n\tvq->vq_avail = (struct vring_avail *)base;\n\tbase += (2 + vq->vq_qsize + 1) * sizeof(uint16_t);\n\n\t/* Then it's rounded up to the next page... */\n\tbase = (char *) roundup2(((uintptr_t) base), ((uintptr_t) VRING_ALIGN));\n\n\t/* ... and the last page(s) are the used ring. */\n\tvq->vq_used = (struct vring_used *)base;\n\n\t/* Mark queue as allocated, and start at 0 when we use it. */\n\tvq->vq_flags = VQ_ALLOC;\n\tvq->vq_last_avail = 0;\n\tvq->vq_save_used = 0;\n}\n\n/*\n * Helper inline for vq_getchain(): record the i'th \"real\"\n * descriptor.\n */\nstatic inline void\n_vq_record(int i, volatile struct virtio_desc *vd, struct iovec *iov, int n_iov,\n\tuint16_t *flags)\n{\n\tif (i >= n_iov)\n\t\treturn;\n\tiov[i].iov_base = paddr_guest2host(vd->vd_addr, vd->vd_len);\n\tiov[i].iov_len = vd->vd_len;\n\tif (flags != NULL)\n\t\tflags[i] = vd->vd_flags;\n}\n#define\tVQ_MAX_DESCRIPTORS\t512\t/* see below */\n\n/*\n * Examine the chain of descriptors starting at the \"next one\" to\n * make sure that they describe a sensible request.  If so, return\n * the number of \"real\" descriptors that would be needed/used in\n * acting on this request.  This may be smaller than the number of\n * available descriptors, e.g., if there are two available but\n * they are two separate requests, this just returns 1.  Or, it\n * may be larger: if there are indirect descriptors involved,\n * there may only be one descriptor available but it may be an\n * indirect pointing to eight more.  We return 8 in this case,\n * i.e., we do not count the indirect descriptors, only the \"real\"\n * ones.\n *\n * Basically, this vets the vd_flags and vd_next field of each\n * descriptor and tells you how many are involved.  Since some may\n * be indirect, this also needs the vmctx (in the pci_devinst\n * at vs->vs_pi) so that it can find indirect descriptors.\n *\n * As we process each descriptor, we copy and adjust it (guest to\n * host address wise, also using the vmtctx) into the given iov[]\n * array (of the given size).  If the array overflows, we stop\n * placing values into the array but keep processing descriptors,\n * up to VQ_MAX_DESCRIPTORS, before giving up and returning -1.\n * So you, the caller, must not assume that iov[] is as big as the\n * return value (you can process the same thing twice to allocate\n * a larger iov array if needed, or supply a zero length to find\n * out how much space is needed).\n *\n * If you want to verify the WRITE flag on each descriptor, pass a\n * non-NULL \"flags\" pointer to an array of \"uint16_t\" of the same size\n * as n_iov and we'll copy each vd_flags field after unwinding any\n * indirects.\n *\n * If some descriptor(s) are invalid, this prints a diagnostic message\n * and returns -1.  If no descriptors are ready now it simply returns 0.\n *\n * You are assumed to have done a vq_ring_ready() if needed (note\n * that vq_has_descs() does one).\n */\nint\nvq_getchain(struct vqueue_info *vq, uint16_t *pidx, struct iovec *iov,\n\tint n_iov, uint16_t *flags)\n{\n\tint i;\n\tu_int ndesc, n_indir;\n\tu_int idx, next;\n\tvolatile struct virtio_desc *vdir, *vindir, *vp;\n\tstruct virtio_softc *vs;\n\tconst char *name;\n\n\tvs = vq->vq_vs;\n\tname = vs->vs_vc->vc_name;\n\n\t/*\n\t * Note: it's the responsibility of the guest not to\n\t * update vq->vq_avail->va_idx until all of the descriptors\n         * the guest has written are valid (including all their\n         * vd_next fields and vd_flags).\n\t *\n\t * Compute (last_avail - va_idx) in integers mod 2**16.  This is\n\t * the number of descriptors the device has made available\n\t * since the last time we updated vq->vq_last_avail.\n\t *\n\t * We just need to do the subtraction as an unsigned int,\n\t * then trim off excess bits.\n\t */\n\tidx = vq->vq_last_avail;\n\tndesc = (uint16_t)((u_int)vq->vq_avail->va_idx - idx);\n\tif (ndesc == 0)\n\t\treturn (0);\n\tif (ndesc > vq->vq_qsize) {\n\t\t/* XXX need better way to diagnose issues */\n\t\tfprintf(stderr,\n\t\t    \"%s: ndesc (%u) out of range, driver confused?\\r\\n\",\n\t\t    name, (u_int)ndesc);\n\t\treturn (-1);\n\t}\n\n\t/*\n\t * Now count/parse \"involved\" descriptors starting from\n\t * the head of the chain.\n\t *\n\t * To prevent loops, we could be more complicated and\n\t * check whether we're re-visiting a previously visited\n\t * index, but we just abort if the count gets excessive.\n\t */\n\t*pidx = next = vq->vq_avail->va_ring[idx & (vq->vq_qsize - 1)];\n\tvq->vq_last_avail++;\n\tfor (i = 0; i < VQ_MAX_DESCRIPTORS; next = vdir->vd_next) {\n\t\tif (next >= vq->vq_qsize) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: descriptor index %u out of range, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name, next);\n\t\t\treturn (-1);\n\t\t}\n\t\tvdir = &vq->vq_desc[next];\n\t\tif ((vdir->vd_flags & VRING_DESC_F_INDIRECT) == 0) {\n\t\t\t_vq_record(i, vdir, iov, n_iov, flags);\n\t\t\ti++;\n\t\t} else if ((vs->vs_vc->vc_hv_caps &\n\t\t    VIRTIO_RING_F_INDIRECT_DESC) == 0) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: descriptor has forbidden INDIRECT flag, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name);\n\t\t\treturn (-1);\n\t\t} else {\n\t\t\tn_indir = vdir->vd_len / 16;\n\t\t\tif ((vdir->vd_len & 0xf) || n_indir == 0) {\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: invalid indir len 0x%x, \"\n\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t    name, (u_int)vdir->vd_len);\n\t\t\t\treturn (-1);\n\t\t\t}\n\t\t\tvindir = paddr_guest2host(vdir->vd_addr, vdir->vd_len);\n\t\t\t/*\n\t\t\t * Indirects start at the 0th, then follow\n\t\t\t * their own embedded \"next\"s until those run\n\t\t\t * out.  Each one's indirect flag must be off\n\t\t\t * (we don't really have to check, could just\n\t\t\t * ignore errors...).\n\t\t\t */\n\t\t\tnext = 0;\n\t\t\tfor (;;) {\n\t\t\t\tvp = &vindir[next];\n\t\t\t\tif (vp->vd_flags & VRING_DESC_F_INDIRECT) {\n\t\t\t\t\tfprintf(stderr,\n\t\t\t\t\t    \"%s: indirect desc has INDIR flag,\"\n\t\t\t\t\t    \" driver confused?\\r\\n\",\n\t\t\t\t\t    name);\n\t\t\t\t\treturn (-1);\n\t\t\t\t}\n\t\t\t\t_vq_record(i, vp, iov, n_iov, flags);\n\t\t\t\tif (++i > VQ_MAX_DESCRIPTORS)\n\t\t\t\t\tgoto loopy;\n\t\t\t\tif ((vp->vd_flags & VRING_DESC_F_NEXT) == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tnext = vp->vd_next;\n\t\t\t\tif (next >= n_indir) {\n\t\t\t\t\tfprintf(stderr,\n\t\t\t\t\t    \"%s: invalid next %u > %u, \"\n\t\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t\t    name, (u_int)next, n_indir);\n\t\t\t\t\treturn (-1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((vdir->vd_flags & VRING_DESC_F_NEXT) == 0)\n\t\t\treturn (i);\n\t}\nloopy:\n\tfprintf(stderr,\n\t    \"%s: descriptor loop? count > %d - driver confused?\\r\\n\",\n\t    name, i);\n\treturn (-1);\n}\n\n/*\n * Return the currently-first request chain back to the available queue.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_retchain(struct vqueue_info *vq)\n{\n\n\tvq->vq_last_avail--;\n}\n\n/*\n * Return specified request chain to the guest, setting its I/O length\n * to the provided value.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_relchain(struct vqueue_info *vq, uint16_t idx, uint32_t iolen)\n{\n\tuint16_t uidx, mask;\n\tvolatile struct vring_used *vuh;\n\tvolatile struct virtio_used *vue;\n\n\t/*\n\t * Notes:\n\t *  - mask is N-1 where N is a power of 2 so computes x % N\n\t *  - vuh points to the \"used\" data shared with guest\n\t *  - vue points to the \"used\" ring entry we want to update\n\t *  - head is the same value we compute in vq_iovecs().\n\t *\n\t * (I apologize for the two fields named vu_idx; the\n\t * virtio spec calls the one that vue points to, \"id\"...)\n\t */\n\tmask = vq->vq_qsize - 1;\n\tvuh = vq->vq_used;\n\n\tuidx = vuh->vu_idx;\n\tvue = &vuh->vu_ring[uidx++ & mask];\n\tvue->vu_idx = idx;\n\tvue->vu_tlen = iolen;\n\tvuh->vu_idx = uidx;\n}\n\n/*\n * Driver has finished processing \"available\" chains and calling\n * vq_relchain on each one.  If driver used all the available\n * chains, used_all should be set.\n *\n * If the \"used\" index moved we may need to inform the guest, i.e.,\n * deliver an interrupt.  Even if the used index did NOT move we\n * may need to deliver an interrupt, if the avail ring is empty and\n * we are supposed to interrupt on empty.\n *\n * Note that used_all_avail is provided by the caller because it's\n * a snapshot of the ring state when he decided to finish interrupt\n * processing -- it's possible that descriptors became available after\n * that point.  (It's also typically a constant 1/True as well.)\n */\nvoid\nvq_endchains(struct vqueue_info *vq, int used_all_avail)\n{\n\tstruct virtio_softc *vs;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,\n\t * but suppressed by VRING_AVAIL_F_NO_INTERRUPT.\n\t *\n\t * In any case, though, if NOTIFY_ON_EMPTY is set and the\n\t * entire avail was processed, we need to interrupt always.\n\t */\n\tvs = vq->vq_vs;\n\told_idx = vq->vq_save_used;\n\tvq->vq_save_used = new_idx = vq->vq_used->vu_idx;\n\tif (used_all_avail &&\n\t    (vs->vs_negotiated_caps & VIRTIO_F_NOTIFY_ON_EMPTY))\n\t\tintr = 1;\n\telse if (vs->vs_negotiated_caps & VIRTIO_RING_F_EVENT_IDX) {\n\t\tevent_idx = VQ_USED_EVENT_IDX(vq);\n\t\t/*\n\t\t * This calculation is per docs and the kernel\n\t\t * (see src/sys/dev/virtio/virtio_ring.h).\n\t\t */\n\t\tintr = (uint16_t)(new_idx - event_idx - 1) <\n\t\t\t(uint16_t)(new_idx - old_idx);\n\t} else {\n\t\tintr = new_idx != old_idx &&\n\t\t    !(vq->vq_avail->va_flags & VRING_AVAIL_F_NO_INTERRUPT);\n\t}\n\tif (intr)\n\t\tvq_interrupt(vs, vq);\n}\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg {\n\tuint16_t\tcr_offset;\t/* register offset */\n\tuint8_t\t\tcr_size;\t/* size (bytes) */\n\tuint8_t\t\tcr_ro;\t\t/* true => reg is read only */\n\tconst char\t*cr_name;\t/* name of reg */\n} config_regs[] = {\n\t{ VTCFG_R_HOSTCAP,\t4, 1, \"HOSTCAP\" },\n\t{ VTCFG_R_GUESTCAP,\t4, 0, \"GUESTCAP\" },\n\t{ VTCFG_R_PFN,\t\t4, 0, \"PFN\" },\n\t{ VTCFG_R_QNUM,\t\t2, 1, \"QNUM\" },\n\t{ VTCFG_R_QSEL,\t\t2, 0, \"QSEL\" },\n\t{ VTCFG_R_QNOTIFY,\t2, 0, \"QNOTIFY\" },\n\t{ VTCFG_R_STATUS,\t1, 0, \"STATUS\" },\n\t{ VTCFG_R_ISR,\t\t1, 0, \"ISR\" },\n\t{ VTCFG_R_CFGVEC,\t2, 0, \"CFGVEC\" },\n\t{ VTCFG_R_QVEC,\t\t2, 0, \"QVEC\" },\n};\n\nstatic inline struct config_reg *\nvi_find_cr(int offset) {\n\tu_int hi, lo, mid;\n\tstruct config_reg *cr;\n\n\tlo = 0;\n\thi = sizeof(config_regs) / sizeof(*config_regs) - 1;\n\twhile (hi >= lo) {\n\t\tmid = (hi + lo) >> 1;\n\t\tcr = &config_regs[mid];\n\t\tif (cr->cr_offset == offset)\n\t\t\treturn (cr);\n\t\tif (cr->cr_offset < offset)\n\t\t\tlo = mid + 1;\n\t\telse\n\t\t\thi = mid - 1;\n\t}\n\treturn (NULL);\n}\n\n/*\n * Handle pci config space reads.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nuint64_t\nvi_pci_read(UNUSED int vcpu, struct pci_devinst *pi, int baridx,\n\tuint64_t offset, int size)\n{\n\tstruct virtio_softc *vs = pi->pi_arg;\n\tstruct virtio_consts *vc;\n\tstruct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tuint32_t value;\n\tint error;\n\n\tif (vs->vs_flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(pi) ||\n\t\t    baridx == pci_msix_pba_bar(pi)) {\n\t\t\treturn (pci_emul_msix_tread(pi, offset, size));\n\t\t}\n\t}\n\n\t/* XXX probably should do something better than just assert() */\n\tassert(baridx == 0);\n\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_lock(vs->vs_mtx);\n\n\tvc = vs->vs_vc;\n\tname = vc->vc_name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(pi))\n\t\tvirtio_config_size = VTCFG_R_CFG1;\n\telse\n\t\tvirtio_config_size = VTCFG_R_CFG0;\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t * If that fails, fall into general code.\n\t\t */\n\t\tnewoff = (uint32_t) (offset - virtio_config_size);\n\t\tmax = vc->vc_cfgsize ? vc->vc_cfgsize : 0x100000000;\n\t\tif ((newoff + ((unsigned) size)) > max)\n\t\t\tgoto bad;\n\t\terror = (*vc->vc_cfgread)(DEV_SOFTC(vs), ((int) newoff), size, &value);\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = vi_find_cr((int) offset);\n\tif (cr == NULL || cr->cr_size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t    name, cr->cr_name, size);\n\t\t} else {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VTCFG_R_HOSTCAP:\n\t\tvalue = (uint32_t) vc->vc_hv_caps;\n\t\tbreak;\n\tcase VTCFG_R_GUESTCAP:\n\t\tvalue = vs->vs_negotiated_caps;\n\t\tbreak;\n\tcase VTCFG_R_PFN:\n\t\tif (vs->vs_curq < vc->vc_nvq)\n\t\t\tvalue = vs->vs_queues[vs->vs_curq].vq_pfn;\n\t\tbreak;\n\tcase VTCFG_R_QNUM:\n\t\tvalue = vs->vs_curq < vc->vc_nvq ?\n\t\t    vs->vs_queues[vs->vs_curq].vq_qsize : 0;\n\t\tbreak;\n\tcase VTCFG_R_QSEL:\n\t\tvalue = (uint32_t) (vs->vs_curq);\n\t\tbreak;\n\tcase VTCFG_R_QNOTIFY:\n\t\tvalue = 0;\t/* XXX */\n\t\tbreak;\n\tcase VTCFG_R_STATUS:\n\t\tvalue = vs->vs_status;\n\t\tbreak;\n\tcase VTCFG_R_ISR:\n\t\tvalue = vs->vs_isr;\n\t\tvs->vs_isr = 0;\t\t/* a read clears this flag */\n\t\tif (value)\n\t\t\tpci_lintr_deassert(pi);\n\t\tbreak;\n\tcase VTCFG_R_CFGVEC:\n\t\tvalue = vs->vs_msix_cfg_idx;\n\t\tbreak;\n\tcase VTCFG_R_QVEC:\n\t\tvalue = vs->vs_curq < vc->vc_nvq ?\n\t\t    vs->vs_queues[vs->vs_curq].vq_msix_idx :\n\t\t    VIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\t}\ndone:\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_unlock(vs->vs_mtx);\n\treturn (value);\n}\n\n/*\n * Handle pci config space writes.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nvoid\nvi_pci_write(UNUSED int vcpu, struct pci_devinst *pi, int baridx,\n\tuint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_softc *vs = pi->pi_arg;\n\tstruct vqueue_info *vq;\n\tstruct virtio_consts *vc;\n\tstruct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tint error;\n\n\tif (vs->vs_flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(pi) ||\n\t\t    baridx == pci_msix_pba_bar(pi)) {\n\t\t\tpci_emul_msix_twrite(pi, offset, size, value);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* XXX probably should do something better than just assert() */\n\tassert(baridx == 0);\n\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_lock(vs->vs_mtx);\n\n\tvc = vs->vs_vc;\n\tname = vc->vc_name;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(pi))\n\t\tvirtio_config_size = VTCFG_R_CFG1;\n\telse\n\t\tvirtio_config_size = VTCFG_R_CFG0;\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t */\n\t\tnewoff = (uint32_t) (offset - virtio_config_size);\n\t\tmax = vc->vc_cfgsize ? vc->vc_cfgsize : 0x100000000;\n\t\tif ((newoff + ((unsigned) size)) > max)\n\t\t\tgoto bad;\n\t\terror = (*vc->vc_cfgwrite)(DEV_SOFTC(vs), ((int) newoff), size,\n\t\t\t((uint32_t) value));\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = vi_find_cr((int) offset);\n\tif (cr == NULL || cr->cr_size != size || cr->cr_ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->cr_size != size)\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t    name, cr->cr_name, size);\n\t\t\tif (cr->cr_ro)\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t    name, cr->cr_name);\n\t\t} else {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VTCFG_R_GUESTCAP:\n\t\tvs->vs_negotiated_caps = (uint32_t) (value & vc->vc_hv_caps);\n\t\tif (vc->vc_apply_features)\n\t\t\t(*vc->vc_apply_features)(DEV_SOFTC(vs),\n\t\t\t    vs->vs_negotiated_caps);\n\t\tbreak;\n\tcase VTCFG_R_PFN:\n\t\tif (vs->vs_curq >= vc->vc_nvq)\n\t\t\tgoto bad_qindex;\n\t\tvi_vq_init(vs, ((uint32_t) value));\n\t\tbreak;\n\tcase VTCFG_R_QSEL:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tvs->vs_curq = (int) value;\n\t\tbreak;\n\tcase VTCFG_R_QNOTIFY:\n\t\tif (value >= ((uint64_t) vc->vc_nvq)) {\n\t\t\tfprintf(stderr, \"%s: queue %d notify out of range\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\t\tgoto done;\n\t\t}\n\t\tvq = &vs->vs_queues[value];\n\t\tif (vq->vq_notify)\n\t\t\t(*vq->vq_notify)(DEV_SOFTC(vs), vq);\n\t\telse if (vc->vc_qnotify)\n\t\t\t(*vc->vc_qnotify)(DEV_SOFTC(vs), vq);\n\t\telse\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: qnotify queue %d: missing vq/vc notify\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\tbreak;\n\tcase VTCFG_R_STATUS:\n\t\tvs->vs_status = (uint8_t) value;\n\t\tif (value == 0)\n\t\t\t(*vc->vc_reset)(DEV_SOFTC(vs));\n\t\tbreak;\n\tcase VTCFG_R_CFGVEC:\n\t\tvs->vs_msix_cfg_idx = (uint16_t) value;\n\t\tbreak;\n\tcase VTCFG_R_QVEC:\n\t\tif (vs->vs_curq >= vc->vc_nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &vs->vs_queues[vs->vs_curq];\n\t\tvq->vq_msix_idx = (uint16_t) value;\n\t\tbreak;\n\t}\n\tgoto done;\n\nbad_qindex:\n\tfprintf(stderr,\n\t    \"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t    name, cr->cr_name, vs->vs_curq, vc->vc_nvq);\ndone:\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_unlock(vs->vs_mtx);\n}\n"], "fixing_code": ["/*-\n * Copyright (c) 2013  Chris Torek <torek @ torek net>\n * Copyright (c) 2015 xhyve developers\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n */\n\n#include <stdio.h>\n#include <stdint.h>\n#include <pthread.h>\n#include <sys/param.h>\n#include <sys/uio.h>\n#include <xhyve/support/misc.h>\n#include <xhyve/xhyve.h>\n#include <xhyve/pci_emul.h>\n#include <xhyve/virtio.h>\n\n/*\n * Functions for dealing with generalized \"virtual devices\" as\n * defined by <https://www.google.com/#output=search&q=virtio+spec>\n */\n\n/*\n * In case we decide to relax the \"virtio softc comes at the\n * front of virtio-based device softc\" constraint, let's use\n * this to convert.\n */\n#define DEV_SOFTC(vs) ((void *)(vs))\n\n/*\n * Link a virtio_softc to its constants, the device softc, and\n * the PCI emulation.\n */\nvoid\nvi_softc_linkup(struct virtio_softc *vs, struct virtio_consts *vc,\n\t\tvoid *dev_softc, struct pci_devinst *pi,\n\t\tstruct vqueue_info *queues)\n{\n\tint i;\n\n\t/* vs and dev_softc addresses must match */\n\tassert((void *)vs == dev_softc);\n\tvs->vs_vc = vc;\n\tvs->vs_pi = pi;\n\tpi->pi_arg = vs;\n\n\tvs->vs_queues = queues;\n\tfor (i = 0; i < vc->vc_nvq; i++) {\n\t\tqueues[i].vq_vs = vs;\n\t\tqueues[i].vq_num = (uint16_t) i;\n\t}\n}\n\n/*\n * Reset device (device-wide).  This erases all queues, i.e.,\n * all the queues become invalid (though we don't wipe out the\n * internal pointers, we just clear the VQ_ALLOC flag).\n *\n * It resets negotiated features to \"none\".\n *\n * If MSI-X is enabled, this also resets all the vectors to NO_VECTOR.\n */\nvoid\nvi_reset_dev(struct virtio_softc *vs)\n{\n\tstruct vqueue_info *vq;\n\tint i, nvq;\n\n\tnvq = vs->vs_vc->vc_nvq;\n\tfor (vq = vs->vs_queues, i = 0; i < nvq; vq++, i++) {\n\t\tvq->vq_flags = 0;\n\t\tvq->vq_last_avail = 0;\n\t\tvq->vq_save_used = 0;\n\t\tvq->vq_pfn = 0;\n\t\tvq->vq_msix_idx = VIRTIO_MSI_NO_VECTOR;\n\t}\n\tvs->vs_negotiated_caps = 0;\n\tvs->vs_curq = 0;\n\t/* vs->vs_status = 0; -- redundant */\n\tif (vs->vs_isr)\n\t\tpci_lintr_deassert(vs->vs_pi);\n\tvs->vs_isr = 0;\n\tvs->vs_msix_cfg_idx = VIRTIO_MSI_NO_VECTOR;\n}\n\n/*\n * Set I/O BAR (usually 0) to map PCI config registers.\n */\nvoid\nvi_set_io_bar(struct virtio_softc *vs, int barnum)\n{\n\tsize_t size;\n\n\t/*\n\t * ??? should we use CFG0 if MSI-X is disabled?\n\t * Existing code did not...\n\t */\n\tsize = VTCFG_R_CFG1 + vs->vs_vc->vc_cfgsize;\n\tpci_emul_alloc_bar(vs->vs_pi, barnum, PCIBAR_IO, size);\n}\n\n/*\n * Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * We assume we want one MSI-X vector per queue, here, plus one\n * for the config vec.\n */\nint\nvi_intr_init(struct virtio_softc *vs, int barnum, int use_msix)\n{\n\tint nvec;\n\n\tif (use_msix) {\n\t\tvs->vs_flags |= VIRTIO_USE_MSIX;\n\t\tVS_LOCK(vs);\n\t\tvi_reset_dev(vs); /* set all vectors to NO_VECTOR */\n\t\tVS_UNLOCK(vs);\n\t\tnvec = vs->vs_vc->vc_nvq + 1;\n\t\tif (pci_emul_add_msixcap(vs->vs_pi, nvec, barnum))\n\t\t\treturn (1);\n\t} else\n\t\tvs->vs_flags &= ~VIRTIO_USE_MSIX;\n\n\t/* Only 1 MSI vector for bhyve */\n\tpci_emul_add_msicap(vs->vs_pi, 1);\n\n\t/* Legacy interrupts are mandatory for virtio devices */\n\tpci_lintr_request(vs->vs_pi);\n\n\treturn (0);\n}\n\n/*\n * Initialize the currently-selected virtio queue (vs->vs_curq).\n * The guest just gave us a page frame number, from which we can\n * calculate the addresses of the queue.\n */\nstatic void\nvi_vq_init(struct virtio_softc *vs, uint32_t pfn)\n{\n\tstruct vqueue_info *vq;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *base;\n\n\tvq = &vs->vs_queues[vs->vs_curq];\n\tvq->vq_pfn = pfn;\n\tphys = (uint64_t)pfn << VRING_PFN;\n\tsize = vring_size(vq->vq_qsize);\n\tbase = paddr_guest2host(phys, size);\n\n\t/* First page(s) are descriptors... */\n\tvq->vq_desc = (struct virtio_desc *)base;\n\tbase += vq->vq_qsize * sizeof(struct virtio_desc);\n\n\t/* ... immediately followed by \"avail\" ring (entirely uint16_t's) */\n\tvq->vq_avail = (struct vring_avail *)base;\n\tbase += (2 + vq->vq_qsize + 1) * sizeof(uint16_t);\n\n\t/* Then it's rounded up to the next page... */\n\tbase = (char *) roundup2(((uintptr_t) base), ((uintptr_t) VRING_ALIGN));\n\n\t/* ... and the last page(s) are the used ring. */\n\tvq->vq_used = (struct vring_used *)base;\n\n\t/* Mark queue as allocated, and start at 0 when we use it. */\n\tvq->vq_flags = VQ_ALLOC;\n\tvq->vq_last_avail = 0;\n\tvq->vq_save_used = 0;\n}\n\n/*\n * Helper inline for vq_getchain(): record the i'th \"real\"\n * descriptor.\n */\nstatic inline void\n_vq_record(int i, volatile struct virtio_desc *vd, struct iovec *iov, int n_iov,\n\tuint16_t *flags)\n{\n\tif (i >= n_iov)\n\t\treturn;\n\tiov[i].iov_base = paddr_guest2host(vd->vd_addr, vd->vd_len);\n\tiov[i].iov_len = vd->vd_len;\n\tif (flags != NULL)\n\t\tflags[i] = vd->vd_flags;\n}\n#define\tVQ_MAX_DESCRIPTORS\t512\t/* see below */\n\n/*\n * Examine the chain of descriptors starting at the \"next one\" to\n * make sure that they describe a sensible request.  If so, return\n * the number of \"real\" descriptors that would be needed/used in\n * acting on this request.  This may be smaller than the number of\n * available descriptors, e.g., if there are two available but\n * they are two separate requests, this just returns 1.  Or, it\n * may be larger: if there are indirect descriptors involved,\n * there may only be one descriptor available but it may be an\n * indirect pointing to eight more.  We return 8 in this case,\n * i.e., we do not count the indirect descriptors, only the \"real\"\n * ones.\n *\n * Basically, this vets the vd_flags and vd_next field of each\n * descriptor and tells you how many are involved.  Since some may\n * be indirect, this also needs the vmctx (in the pci_devinst\n * at vs->vs_pi) so that it can find indirect descriptors.\n *\n * As we process each descriptor, we copy and adjust it (guest to\n * host address wise, also using the vmtctx) into the given iov[]\n * array (of the given size).  If the array overflows, we stop\n * placing values into the array but keep processing descriptors,\n * up to VQ_MAX_DESCRIPTORS, before giving up and returning -1.\n * So you, the caller, must not assume that iov[] is as big as the\n * return value (you can process the same thing twice to allocate\n * a larger iov array if needed, or supply a zero length to find\n * out how much space is needed).\n *\n * If you want to verify the WRITE flag on each descriptor, pass a\n * non-NULL \"flags\" pointer to an array of \"uint16_t\" of the same size\n * as n_iov and we'll copy each vd_flags field after unwinding any\n * indirects.\n *\n * If some descriptor(s) are invalid, this prints a diagnostic message\n * and returns -1.  If no descriptors are ready now it simply returns 0.\n *\n * You are assumed to have done a vq_ring_ready() if needed (note\n * that vq_has_descs() does one).\n */\nint\nvq_getchain(struct vqueue_info *vq, uint16_t *pidx, struct iovec *iov,\n\tint n_iov, uint16_t *flags)\n{\n\tint i;\n\tu_int ndesc, n_indir;\n\tu_int idx, next;\n\tvolatile struct virtio_desc *vdir, *vindir, *vp;\n\tstruct virtio_softc *vs;\n\tconst char *name;\n\n\tvs = vq->vq_vs;\n\tname = vs->vs_vc->vc_name;\n\n\t/*\n\t * Note: it's the responsibility of the guest not to\n\t * update vq->vq_avail->va_idx until all of the descriptors\n         * the guest has written are valid (including all their\n         * vd_next fields and vd_flags).\n\t *\n\t * Compute (last_avail - va_idx) in integers mod 2**16.  This is\n\t * the number of descriptors the device has made available\n\t * since the last time we updated vq->vq_last_avail.\n\t *\n\t * We just need to do the subtraction as an unsigned int,\n\t * then trim off excess bits.\n\t */\n\tidx = vq->vq_last_avail;\n\tndesc = (uint16_t)((u_int)vq->vq_avail->va_idx - idx);\n\tif (ndesc == 0)\n\t\treturn (0);\n\tif (ndesc > vq->vq_qsize) {\n\t\t/* XXX need better way to diagnose issues */\n\t\tfprintf(stderr,\n\t\t    \"%s: ndesc (%u) out of range, driver confused?\\r\\n\",\n\t\t    name, (u_int)ndesc);\n\t\treturn (-1);\n\t}\n\n\t/*\n\t * Now count/parse \"involved\" descriptors starting from\n\t * the head of the chain.\n\t *\n\t * To prevent loops, we could be more complicated and\n\t * check whether we're re-visiting a previously visited\n\t * index, but we just abort if the count gets excessive.\n\t */\n\t*pidx = next = vq->vq_avail->va_ring[idx & (vq->vq_qsize - 1)];\n\tvq->vq_last_avail++;\n\tfor (i = 0; i < VQ_MAX_DESCRIPTORS; next = vdir->vd_next) {\n\t\tif (next >= vq->vq_qsize) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: descriptor index %u out of range, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name, next);\n\t\t\treturn (-1);\n\t\t}\n\t\tvdir = &vq->vq_desc[next];\n\t\tif ((vdir->vd_flags & VRING_DESC_F_INDIRECT) == 0) {\n\t\t\t_vq_record(i, vdir, iov, n_iov, flags);\n\t\t\ti++;\n\t\t} else if ((vs->vs_vc->vc_hv_caps &\n\t\t    VIRTIO_RING_F_INDIRECT_DESC) == 0) {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: descriptor has forbidden INDIRECT flag, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name);\n\t\t\treturn (-1);\n\t\t} else {\n\t\t\tn_indir = vdir->vd_len / 16;\n\t\t\tif ((vdir->vd_len & 0xf) || n_indir == 0) {\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: invalid indir len 0x%x, \"\n\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t    name, (u_int)vdir->vd_len);\n\t\t\t\treturn (-1);\n\t\t\t}\n\t\t\tvindir = paddr_guest2host(vdir->vd_addr, vdir->vd_len);\n\t\t\t/*\n\t\t\t * Indirects start at the 0th, then follow\n\t\t\t * their own embedded \"next\"s until those run\n\t\t\t * out.  Each one's indirect flag must be off\n\t\t\t * (we don't really have to check, could just\n\t\t\t * ignore errors...).\n\t\t\t */\n\t\t\tnext = 0;\n\t\t\tfor (;;) {\n\t\t\t\tvp = &vindir[next];\n\t\t\t\tif (vp->vd_flags & VRING_DESC_F_INDIRECT) {\n\t\t\t\t\tfprintf(stderr,\n\t\t\t\t\t    \"%s: indirect desc has INDIR flag,\"\n\t\t\t\t\t    \" driver confused?\\r\\n\",\n\t\t\t\t\t    name);\n\t\t\t\t\treturn (-1);\n\t\t\t\t}\n\t\t\t\t_vq_record(i, vp, iov, n_iov, flags);\n\t\t\t\tif (++i > VQ_MAX_DESCRIPTORS)\n\t\t\t\t\tgoto loopy;\n\t\t\t\tif ((vp->vd_flags & VRING_DESC_F_NEXT) == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tnext = vp->vd_next;\n\t\t\t\tif (next >= n_indir) {\n\t\t\t\t\tfprintf(stderr,\n\t\t\t\t\t    \"%s: invalid next %u > %u, \"\n\t\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t\t    name, (u_int)next, n_indir);\n\t\t\t\t\treturn (-1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((vdir->vd_flags & VRING_DESC_F_NEXT) == 0)\n\t\t\treturn (i);\n\t}\nloopy:\n\tfprintf(stderr,\n\t    \"%s: descriptor loop? count > %d - driver confused?\\r\\n\",\n\t    name, i);\n\treturn (-1);\n}\n\n/*\n * Return the currently-first request chain back to the available queue.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_retchain(struct vqueue_info *vq)\n{\n\n\tvq->vq_last_avail--;\n}\n\n/*\n * Return specified request chain to the guest, setting its I/O length\n * to the provided value.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_relchain(struct vqueue_info *vq, uint16_t idx, uint32_t iolen)\n{\n\tuint16_t uidx, mask;\n\tvolatile struct vring_used *vuh;\n\tvolatile struct virtio_used *vue;\n\n\t/*\n\t * Notes:\n\t *  - mask is N-1 where N is a power of 2 so computes x % N\n\t *  - vuh points to the \"used\" data shared with guest\n\t *  - vue points to the \"used\" ring entry we want to update\n\t *  - head is the same value we compute in vq_iovecs().\n\t *\n\t * (I apologize for the two fields named vu_idx; the\n\t * virtio spec calls the one that vue points to, \"id\"...)\n\t */\n\tmask = vq->vq_qsize - 1;\n\tvuh = vq->vq_used;\n\n\tuidx = vuh->vu_idx;\n\tvue = &vuh->vu_ring[uidx++ & mask];\n\tvue->vu_idx = idx;\n\tvue->vu_tlen = iolen;\n\tvuh->vu_idx = uidx;\n}\n\n/*\n * Driver has finished processing \"available\" chains and calling\n * vq_relchain on each one.  If driver used all the available\n * chains, used_all should be set.\n *\n * If the \"used\" index moved we may need to inform the guest, i.e.,\n * deliver an interrupt.  Even if the used index did NOT move we\n * may need to deliver an interrupt, if the avail ring is empty and\n * we are supposed to interrupt on empty.\n *\n * Note that used_all_avail is provided by the caller because it's\n * a snapshot of the ring state when he decided to finish interrupt\n * processing -- it's possible that descriptors became available after\n * that point.  (It's also typically a constant 1/True as well.)\n */\nvoid\nvq_endchains(struct vqueue_info *vq, int used_all_avail)\n{\n\tstruct virtio_softc *vs;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,\n\t * but suppressed by VRING_AVAIL_F_NO_INTERRUPT.\n\t *\n\t * In any case, though, if NOTIFY_ON_EMPTY is set and the\n\t * entire avail was processed, we need to interrupt always.\n\t */\n\tvs = vq->vq_vs;\n\told_idx = vq->vq_save_used;\n\tvq->vq_save_used = new_idx = vq->vq_used->vu_idx;\n\tif (used_all_avail &&\n\t    (vs->vs_negotiated_caps & VIRTIO_F_NOTIFY_ON_EMPTY))\n\t\tintr = 1;\n\telse if (vs->vs_negotiated_caps & VIRTIO_RING_F_EVENT_IDX) {\n\t\tevent_idx = VQ_USED_EVENT_IDX(vq);\n\t\t/*\n\t\t * This calculation is per docs and the kernel\n\t\t * (see src/sys/dev/virtio/virtio_ring.h).\n\t\t */\n\t\tintr = (uint16_t)(new_idx - event_idx - 1) <\n\t\t\t(uint16_t)(new_idx - old_idx);\n\t} else {\n\t\tintr = new_idx != old_idx &&\n\t\t    !(vq->vq_avail->va_flags & VRING_AVAIL_F_NO_INTERRUPT);\n\t}\n\tif (intr)\n\t\tvq_interrupt(vs, vq);\n}\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg {\n\tuint16_t\tcr_offset;\t/* register offset */\n\tuint8_t\t\tcr_size;\t/* size (bytes) */\n\tuint8_t\t\tcr_ro;\t\t/* true => reg is read only */\n\tconst char\t*cr_name;\t/* name of reg */\n} config_regs[] = {\n\t{ VTCFG_R_HOSTCAP,\t4, 1, \"HOSTCAP\" },\n\t{ VTCFG_R_GUESTCAP,\t4, 0, \"GUESTCAP\" },\n\t{ VTCFG_R_PFN,\t\t4, 0, \"PFN\" },\n\t{ VTCFG_R_QNUM,\t\t2, 1, \"QNUM\" },\n\t{ VTCFG_R_QSEL,\t\t2, 0, \"QSEL\" },\n\t{ VTCFG_R_QNOTIFY,\t2, 0, \"QNOTIFY\" },\n\t{ VTCFG_R_STATUS,\t1, 0, \"STATUS\" },\n\t{ VTCFG_R_ISR,\t\t1, 0, \"ISR\" },\n\t{ VTCFG_R_CFGVEC,\t2, 0, \"CFGVEC\" },\n\t{ VTCFG_R_QVEC,\t\t2, 0, \"QVEC\" },\n};\n\nstatic inline struct config_reg *\nvi_find_cr(int offset) {\n\tu_int hi, lo, mid;\n\tstruct config_reg *cr;\n\n\tlo = 0;\n\thi = sizeof(config_regs) / sizeof(*config_regs) - 1;\n\twhile (hi >= lo) {\n\t\tmid = (hi + lo) >> 1;\n\t\tcr = &config_regs[mid];\n\t\tif (cr->cr_offset == offset)\n\t\t\treturn (cr);\n\t\tif (cr->cr_offset < offset)\n\t\t\tlo = mid + 1;\n\t\telse\n\t\t\thi = mid - 1;\n\t}\n\treturn (NULL);\n}\n\n/*\n * Handle pci config space reads.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nuint64_t\nvi_pci_read(UNUSED int vcpu, struct pci_devinst *pi, int baridx,\n\tuint64_t offset, int size)\n{\n\tstruct virtio_softc *vs = pi->pi_arg;\n\tstruct virtio_consts *vc;\n\tstruct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tuint32_t value;\n\tint error;\n\n\tif (vs->vs_flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(pi) ||\n\t\t    baridx == pci_msix_pba_bar(pi)) {\n\t\t\treturn (pci_emul_msix_tread(pi, offset, size));\n\t\t}\n\t}\n\n\t/* XXX probably should do something better than just assert() */\n\tassert(baridx == 0);\n\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_lock(vs->vs_mtx);\n\n\tvc = vs->vs_vc;\n\tname = vc->vc_name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(pi))\n\t\tvirtio_config_size = VTCFG_R_CFG1;\n\telse\n\t\tvirtio_config_size = VTCFG_R_CFG0;\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t * If that fails, fall into general code.\n\t\t */\n\t\tnewoff = (uint32_t) (offset - virtio_config_size);\n\t\tmax = vc->vc_cfgsize ? vc->vc_cfgsize : 0x100000000;\n\t\tif ((newoff + ((unsigned) size)) > max)\n\t\t\tgoto bad;\n\t\tif (vc->vc_cfgread != NULL)\n\t\t\terror = (*vc->vc_cfgread)(DEV_SOFTC(vs), ((int) newoff), size, &value);\n\t\telse\n\t\t\terror = 0;\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = vi_find_cr((int) offset);\n\tif (cr == NULL || cr->cr_size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t    name, cr->cr_name, size);\n\t\t} else {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VTCFG_R_HOSTCAP:\n\t\tvalue = (uint32_t) vc->vc_hv_caps;\n\t\tbreak;\n\tcase VTCFG_R_GUESTCAP:\n\t\tvalue = vs->vs_negotiated_caps;\n\t\tbreak;\n\tcase VTCFG_R_PFN:\n\t\tif (vs->vs_curq < vc->vc_nvq)\n\t\t\tvalue = vs->vs_queues[vs->vs_curq].vq_pfn;\n\t\tbreak;\n\tcase VTCFG_R_QNUM:\n\t\tvalue = vs->vs_curq < vc->vc_nvq ?\n\t\t    vs->vs_queues[vs->vs_curq].vq_qsize : 0;\n\t\tbreak;\n\tcase VTCFG_R_QSEL:\n\t\tvalue = (uint32_t) (vs->vs_curq);\n\t\tbreak;\n\tcase VTCFG_R_QNOTIFY:\n\t\tvalue = 0;\t/* XXX */\n\t\tbreak;\n\tcase VTCFG_R_STATUS:\n\t\tvalue = vs->vs_status;\n\t\tbreak;\n\tcase VTCFG_R_ISR:\n\t\tvalue = vs->vs_isr;\n\t\tvs->vs_isr = 0;\t\t/* a read clears this flag */\n\t\tif (value)\n\t\t\tpci_lintr_deassert(pi);\n\t\tbreak;\n\tcase VTCFG_R_CFGVEC:\n\t\tvalue = vs->vs_msix_cfg_idx;\n\t\tbreak;\n\tcase VTCFG_R_QVEC:\n\t\tvalue = vs->vs_curq < vc->vc_nvq ?\n\t\t    vs->vs_queues[vs->vs_curq].vq_msix_idx :\n\t\t    VIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\t}\ndone:\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_unlock(vs->vs_mtx);\n\treturn (value);\n}\n\n/*\n * Handle pci config space writes.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nvoid\nvi_pci_write(UNUSED int vcpu, struct pci_devinst *pi, int baridx,\n\tuint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_softc *vs = pi->pi_arg;\n\tstruct vqueue_info *vq;\n\tstruct virtio_consts *vc;\n\tstruct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tint error;\n\n\tif (vs->vs_flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(pi) ||\n\t\t    baridx == pci_msix_pba_bar(pi)) {\n\t\t\tpci_emul_msix_twrite(pi, offset, size, value);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t/* XXX probably should do something better than just assert() */\n\tassert(baridx == 0);\n\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_lock(vs->vs_mtx);\n\n\tvc = vs->vs_vc;\n\tname = vc->vc_name;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(pi))\n\t\tvirtio_config_size = VTCFG_R_CFG1;\n\telse\n\t\tvirtio_config_size = VTCFG_R_CFG0;\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t */\n\t\tnewoff = (uint32_t) (offset - virtio_config_size);\n\t\tmax = vc->vc_cfgsize ? vc->vc_cfgsize : 0x100000000;\n\t\tif ((newoff + ((unsigned) size)) > max)\n\t\t\tgoto bad;\n\t\terror = (*vc->vc_cfgwrite)(DEV_SOFTC(vs), ((int) newoff), size,\n\t\t\t((uint32_t) value));\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = vi_find_cr((int) offset);\n\tif (cr == NULL || cr->cr_size != size || cr->cr_ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->cr_size != size)\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t    name, cr->cr_name, size);\n\t\t\tif (cr->cr_ro)\n\t\t\t\tfprintf(stderr,\n\t\t\t\t    \"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t    name, cr->cr_name);\n\t\t} else {\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VTCFG_R_GUESTCAP:\n\t\tvs->vs_negotiated_caps = (uint32_t) (value & vc->vc_hv_caps);\n\t\tif (vc->vc_apply_features)\n\t\t\t(*vc->vc_apply_features)(DEV_SOFTC(vs),\n\t\t\t    vs->vs_negotiated_caps);\n\t\tbreak;\n\tcase VTCFG_R_PFN:\n\t\tif (vs->vs_curq >= vc->vc_nvq)\n\t\t\tgoto bad_qindex;\n\t\tvi_vq_init(vs, ((uint32_t) value));\n\t\tbreak;\n\tcase VTCFG_R_QSEL:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tvs->vs_curq = (int) value;\n\t\tbreak;\n\tcase VTCFG_R_QNOTIFY:\n\t\tif (value >= ((uint64_t) vc->vc_nvq)) {\n\t\t\tfprintf(stderr, \"%s: queue %d notify out of range\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\t\tgoto done;\n\t\t}\n\t\tvq = &vs->vs_queues[value];\n\t\tif (vq->vq_notify)\n\t\t\t(*vq->vq_notify)(DEV_SOFTC(vs), vq);\n\t\telse if (vc->vc_qnotify)\n\t\t\t(*vc->vc_qnotify)(DEV_SOFTC(vs), vq);\n\t\telse\n\t\t\tfprintf(stderr,\n\t\t\t    \"%s: qnotify queue %d: missing vq/vc notify\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\tbreak;\n\tcase VTCFG_R_STATUS:\n\t\tvs->vs_status = (uint8_t) value;\n\t\tif (value == 0)\n\t\t\t(*vc->vc_reset)(DEV_SOFTC(vs));\n\t\tbreak;\n\tcase VTCFG_R_CFGVEC:\n\t\tvs->vs_msix_cfg_idx = (uint16_t) value;\n\t\tbreak;\n\tcase VTCFG_R_QVEC:\n\t\tif (vs->vs_curq >= vc->vc_nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &vs->vs_queues[vs->vs_curq];\n\t\tvq->vq_msix_idx = (uint16_t) value;\n\t\tbreak;\n\t}\n\tgoto done;\n\nbad_qindex:\n\tfprintf(stderr,\n\t    \"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t    name, cr->cr_name, vs->vs_curq, vc->vc_nvq);\ndone:\n\tif (vs->vs_mtx)\n\t\tpthread_mutex_unlock(vs->vs_mtx);\n}\n"], "filenames": ["src/lib/virtio.c"], "buggy_code_start_loc": [562], "buggy_code_end_loc": [563], "fixing_code_start_loc": [562], "fixing_code_end_loc": [566], "type": "CWE-476", "message": "HyperKit is a toolkit for embedding hypervisor capabilities in an application. In versions 0.20210107 and prior of HyperKit, `virtio.c` has is a call to `vc_cfgread` that does not check for null which when called makes the host crash. This issue may lead to a guest crashing the host causing a denial of service. This issue is fixed in commit df0e46c7dbfd81a957d85e449ba41b52f6f7beb4.", "other": {"cve": {"id": "CVE-2021-32843", "sourceIdentifier": "security-advisories@github.com", "published": "2023-02-17T23:15:11.803", "lastModified": "2023-02-28T20:30:17.870", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "HyperKit is a toolkit for embedding hypervisor capabilities in an application. In versions 0.20210107 and prior of HyperKit, `virtio.c` has is a call to `vc_cfgread` that does not check for null which when called makes the host crash. This issue may lead to a guest crashing the host causing a denial of service. This issue is fixed in commit df0e46c7dbfd81a957d85e449ba41b52f6f7beb4."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.2, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.5, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:mobyproject:hyperkit:*:*:*:*:*:*:*:*", "versionEndIncluding": "0.20210107", "matchCriteriaId": "0AE32831-24E2-44FD-939C-E6F799A5D632"}]}]}], "references": [{"url": "https://github.com/moby/hyperkit/commit/df0e46c7dbfd81a957d85e449ba41b52f6f7beb4", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/moby/hyperkit/pull/313", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://securitylab.github.com/advisories/GHSL-2021-054_057-moby-hyperkit/", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/moby/hyperkit/commit/df0e46c7dbfd81a957d85e449ba41b52f6f7beb4"}}