{"buggy_code": ["/*\n  FUSE: Filesystem in Userspace\n  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>\n\n  This program can be distributed under the terms of the GNU GPL.\n  See the file COPYING.\n*/\n\n#include \"fuse_i.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/poll.h>\n#include <linux/sched/signal.h>\n#include <linux/uio.h>\n#include <linux/miscdevice.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/swap.h>\n#include <linux/splice.h>\n#include <linux/sched.h>\n\nMODULE_ALIAS_MISCDEV(FUSE_MINOR);\nMODULE_ALIAS(\"devname:fuse\");\n\n/* Ordinary requests have even IDs, while interrupts IDs are odd */\n#define FUSE_INT_REQ_BIT (1ULL << 0)\n#define FUSE_REQ_ID_STEP (1ULL << 1)\n\nstatic struct kmem_cache *fuse_req_cachep;\n\nstatic struct fuse_dev *fuse_get_dev(struct file *file)\n{\n\t/*\n\t * Lockless access is OK, because file->private data is set\n\t * once during mount and is valid until the file is released.\n\t */\n\treturn READ_ONCE(file->private_data);\n}\n\nstatic void fuse_request_init(struct fuse_req *req, struct page **pages,\n\t\t\t      struct fuse_page_desc *page_descs,\n\t\t\t      unsigned npages)\n{\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\trefcount_set(&req->count, 1);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\t__set_bit(FR_PENDING, &req->flags);\n}\n\nstatic struct page **fuse_req_pages_alloc(unsigned int npages, gfp_t flags,\n\t\t\t\t\t  struct fuse_page_desc **desc)\n{\n\tstruct page **pages;\n\n\tpages = kzalloc(npages * (sizeof(struct page *) +\n\t\t\t\t  sizeof(struct fuse_page_desc)), flags);\n\t*desc = (void *) pages + npages * sizeof(struct page *);\n\n\treturn pages;\n}\n\nstatic struct fuse_req *__fuse_request_alloc(unsigned npages, gfp_t flags)\n{\n\tstruct fuse_req *req = kmem_cache_zalloc(fuse_req_cachep, flags);\n\tif (req) {\n\t\tstruct page **pages = NULL;\n\t\tstruct fuse_page_desc *page_descs = NULL;\n\n\t\tWARN_ON(npages > FUSE_MAX_MAX_PAGES);\n\t\tif (npages > FUSE_REQ_INLINE_PAGES) {\n\t\t\tpages = fuse_req_pages_alloc(npages, flags,\n\t\t\t\t\t\t     &page_descs);\n\t\t\tif (!pages) {\n\t\t\t\tkmem_cache_free(fuse_req_cachep, req);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t} else if (npages) {\n\t\t\tpages = req->inline_pages;\n\t\t\tpage_descs = req->inline_page_descs;\n\t\t}\n\n\t\tfuse_request_init(req, pages, page_descs, npages);\n\t}\n\treturn req;\n}\n\nstruct fuse_req *fuse_request_alloc(unsigned npages)\n{\n\treturn __fuse_request_alloc(npages, GFP_KERNEL);\n}\nEXPORT_SYMBOL_GPL(fuse_request_alloc);\n\nstruct fuse_req *fuse_request_alloc_nofs(unsigned npages)\n{\n\treturn __fuse_request_alloc(npages, GFP_NOFS);\n}\n\nstatic void fuse_req_pages_free(struct fuse_req *req)\n{\n\tif (req->pages != req->inline_pages)\n\t\tkfree(req->pages);\n}\n\nbool fuse_req_realloc_pages(struct fuse_conn *fc, struct fuse_req *req,\n\t\t\t    gfp_t flags)\n{\n\tstruct page **pages;\n\tstruct fuse_page_desc *page_descs;\n\tunsigned int npages = min_t(unsigned int,\n\t\t\t\t    max_t(unsigned int, req->max_pages * 2,\n\t\t\t\t\t  FUSE_DEFAULT_MAX_PAGES_PER_REQ),\n\t\t\t\t    fc->max_pages);\n\tWARN_ON(npages <= req->max_pages);\n\n\tpages = fuse_req_pages_alloc(npages, flags, &page_descs);\n\tif (!pages)\n\t\treturn false;\n\n\tmemcpy(pages, req->pages, sizeof(struct page *) * req->max_pages);\n\tmemcpy(page_descs, req->page_descs,\n\t       sizeof(struct fuse_page_desc) * req->max_pages);\n\tfuse_req_pages_free(req);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\n\treturn true;\n}\n\nvoid fuse_request_free(struct fuse_req *req)\n{\n\tfuse_req_pages_free(req);\n\tkmem_cache_free(fuse_req_cachep, req);\n}\n\nvoid __fuse_get_request(struct fuse_req *req)\n{\n\trefcount_inc(&req->count);\n}\n\n/* Must be called with > 1 refcount */\nstatic void __fuse_put_request(struct fuse_req *req)\n{\n\trefcount_dec(&req->count);\n}\n\nvoid fuse_set_initialized(struct fuse_conn *fc)\n{\n\t/* Make sure stores before this are seen on another CPU */\n\tsmp_wmb();\n\tfc->initialized = 1;\n}\n\nstatic bool fuse_block_alloc(struct fuse_conn *fc, bool for_background)\n{\n\treturn !fc->initialized || (for_background && fc->blocked);\n}\n\nstatic void fuse_drop_waiting(struct fuse_conn *fc)\n{\n\t/*\n\t * lockess check of fc->connected is okay, because atomic_dec_and_test()\n\t * provides a memory barrier mached with the one in fuse_wait_aborted()\n\t * to ensure no wake-up is missed.\n\t */\n\tif (atomic_dec_and_test(&fc->num_waiting) &&\n\t    !READ_ONCE(fc->connected)) {\n\t\t/* wake up aborters */\n\t\twake_up_all(&fc->blocked_waitq);\n\t}\n}\n\nstatic struct fuse_req *__fuse_get_req(struct fuse_conn *fc, unsigned npages,\n\t\t\t\t       bool for_background)\n{\n\tstruct fuse_req *req;\n\tint err;\n\tatomic_inc(&fc->num_waiting);\n\n\tif (fuse_block_alloc(fc, for_background)) {\n\t\terr = -EINTR;\n\t\tif (wait_event_killable_exclusive(fc->blocked_waitq,\n\t\t\t\t!fuse_block_alloc(fc, for_background)))\n\t\t\tgoto out;\n\t}\n\t/* Matches smp_wmb() in fuse_set_initialized() */\n\tsmp_rmb();\n\n\terr = -ENOTCONN;\n\tif (!fc->connected)\n\t\tgoto out;\n\n\terr = -ECONNREFUSED;\n\tif (fc->conn_error)\n\t\tgoto out;\n\n\treq = fuse_request_alloc(npages);\n\terr = -ENOMEM;\n\tif (!req) {\n\t\tif (for_background)\n\t\t\twake_up(&fc->blocked_waitq);\n\t\tgoto out;\n\t}\n\n\treq->in.h.uid = from_kuid(fc->user_ns, current_fsuid());\n\treq->in.h.gid = from_kgid(fc->user_ns, current_fsgid());\n\treq->in.h.pid = pid_nr_ns(task_pid(current), fc->pid_ns);\n\n\t__set_bit(FR_WAITING, &req->flags);\n\tif (for_background)\n\t\t__set_bit(FR_BACKGROUND, &req->flags);\n\n\tif (unlikely(req->in.h.uid == ((uid_t)-1) ||\n\t\t     req->in.h.gid == ((gid_t)-1))) {\n\t\tfuse_put_request(fc, req);\n\t\treturn ERR_PTR(-EOVERFLOW);\n\t}\n\treturn req;\n\n out:\n\tfuse_drop_waiting(fc);\n\treturn ERR_PTR(err);\n}\n\nstruct fuse_req *fuse_get_req(struct fuse_conn *fc, unsigned npages)\n{\n\treturn __fuse_get_req(fc, npages, false);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req);\n\nstruct fuse_req *fuse_get_req_for_background(struct fuse_conn *fc,\n\t\t\t\t\t     unsigned npages)\n{\n\treturn __fuse_get_req(fc, npages, true);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req_for_background);\n\n/*\n * Return request in fuse_file->reserved_req.  However that may\n * currently be in use.  If that is the case, wait for it to become\n * available.\n */\nstatic struct fuse_req *get_reserved_req(struct fuse_conn *fc,\n\t\t\t\t\t struct file *file)\n{\n\tstruct fuse_req *req = NULL;\n\tstruct fuse_file *ff = file->private_data;\n\n\tdo {\n\t\twait_event(fc->reserved_req_waitq, ff->reserved_req);\n\t\tspin_lock(&fc->lock);\n\t\tif (ff->reserved_req) {\n\t\t\treq = ff->reserved_req;\n\t\t\tff->reserved_req = NULL;\n\t\t\treq->stolen_file = get_file(file);\n\t\t}\n\t\tspin_unlock(&fc->lock);\n\t} while (!req);\n\n\treturn req;\n}\n\n/*\n * Put stolen request back into fuse_file->reserved_req\n */\nstatic void put_reserved_req(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct file *file = req->stolen_file;\n\tstruct fuse_file *ff = file->private_data;\n\n\tWARN_ON(req->max_pages);\n\tspin_lock(&fc->lock);\n\tmemset(req, 0, sizeof(*req));\n\tfuse_request_init(req, NULL, NULL, 0);\n\tBUG_ON(ff->reserved_req);\n\tff->reserved_req = req;\n\twake_up_all(&fc->reserved_req_waitq);\n\tspin_unlock(&fc->lock);\n\tfput(file);\n}\n\n/*\n * Gets a requests for a file operation, always succeeds\n *\n * This is used for sending the FLUSH request, which must get to\n * userspace, due to POSIX locks which may need to be unlocked.\n *\n * If allocation fails due to OOM, use the reserved request in\n * fuse_file.\n *\n * This is very unlikely to deadlock accidentally, since the\n * filesystem should not have it's own file open.  If deadlock is\n * intentional, it can still be broken by \"aborting\" the filesystem.\n */\nstruct fuse_req *fuse_get_req_nofail_nopages(struct fuse_conn *fc,\n\t\t\t\t\t     struct file *file)\n{\n\tstruct fuse_req *req;\n\n\tatomic_inc(&fc->num_waiting);\n\twait_event(fc->blocked_waitq, fc->initialized);\n\t/* Matches smp_wmb() in fuse_set_initialized() */\n\tsmp_rmb();\n\treq = fuse_request_alloc(0);\n\tif (!req)\n\t\treq = get_reserved_req(fc, file);\n\n\treq->in.h.uid = from_kuid_munged(fc->user_ns, current_fsuid());\n\treq->in.h.gid = from_kgid_munged(fc->user_ns, current_fsgid());\n\treq->in.h.pid = pid_nr_ns(task_pid(current), fc->pid_ns);\n\n\t__set_bit(FR_WAITING, &req->flags);\n\t__clear_bit(FR_BACKGROUND, &req->flags);\n\treturn req;\n}\n\nvoid fuse_put_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (refcount_dec_and_test(&req->count)) {\n\t\tif (test_bit(FR_BACKGROUND, &req->flags)) {\n\t\t\t/*\n\t\t\t * We get here in the unlikely case that a background\n\t\t\t * request was allocated but not sent\n\t\t\t */\n\t\t\tspin_lock(&fc->bg_lock);\n\t\t\tif (!fc->blocked)\n\t\t\t\twake_up(&fc->blocked_waitq);\n\t\t\tspin_unlock(&fc->bg_lock);\n\t\t}\n\n\t\tif (test_bit(FR_WAITING, &req->flags)) {\n\t\t\t__clear_bit(FR_WAITING, &req->flags);\n\t\t\tfuse_drop_waiting(fc);\n\t\t}\n\n\t\tif (req->stolen_file)\n\t\t\tput_reserved_req(fc, req);\n\t\telse\n\t\t\tfuse_request_free(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_put_request);\n\nstatic unsigned len_args(unsigned numargs, struct fuse_arg *args)\n{\n\tunsigned nbytes = 0;\n\tunsigned i;\n\n\tfor (i = 0; i < numargs; i++)\n\t\tnbytes += args[i].size;\n\n\treturn nbytes;\n}\n\nstatic u64 fuse_get_unique(struct fuse_iqueue *fiq)\n{\n\tfiq->reqctr += FUSE_REQ_ID_STEP;\n\treturn fiq->reqctr;\n}\n\nstatic unsigned int fuse_req_hash(u64 unique)\n{\n\treturn hash_long(unique & ~FUSE_INT_REQ_BIT, FUSE_PQ_HASH_BITS);\n}\n\nstatic void queue_request(struct fuse_iqueue *fiq, struct fuse_req *req)\n{\n\treq->in.h.len = sizeof(struct fuse_in_header) +\n\t\tlen_args(req->in.numargs, (struct fuse_arg *) req->in.args);\n\tlist_add_tail(&req->list, &fiq->pending);\n\twake_up_locked(&fiq->waitq);\n\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n}\n\nvoid fuse_queue_forget(struct fuse_conn *fc, struct fuse_forget_link *forget,\n\t\t       u64 nodeid, u64 nlookup)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tforget->forget_one.nodeid = nodeid;\n\tforget->forget_one.nlookup = nlookup;\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (fiq->connected) {\n\t\tfiq->forget_list_tail->next = forget;\n\t\tfiq->forget_list_tail = forget;\n\t\twake_up_locked(&fiq->waitq);\n\t\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n\t} else {\n\t\tkfree(forget);\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n}\n\nstatic void flush_bg_queue(struct fuse_conn *fc)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\twhile (fc->active_background < fc->max_background &&\n\t       !list_empty(&fc->bg_queue)) {\n\t\tstruct fuse_req *req;\n\n\t\treq = list_first_entry(&fc->bg_queue, struct fuse_req, list);\n\t\tlist_del(&req->list);\n\t\tfc->active_background++;\n\t\tspin_lock(&fiq->waitq.lock);\n\t\treq->in.h.unique = fuse_get_unique(fiq);\n\t\tqueue_request(fiq, req);\n\t\tspin_unlock(&fiq->waitq.lock);\n\t}\n}\n\n/*\n * This function is called when a request is finished.  Either a reply\n * has arrived or it was aborted (and not yet sent) or some error\n * occurred during communication with userspace, or the device file\n * was closed.  The requester thread is woken up (if still waiting),\n * the 'end' callback is called if given, else the reference to the\n * request is released\n */\nstatic void request_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tif (test_and_set_bit(FR_FINISHED, &req->flags))\n\t\tgoto put_request;\n\n\tspin_lock(&fiq->waitq.lock);\n\tlist_del_init(&req->intr_entry);\n\tspin_unlock(&fiq->waitq.lock);\n\tWARN_ON(test_bit(FR_PENDING, &req->flags));\n\tWARN_ON(test_bit(FR_SENT, &req->flags));\n\tif (test_bit(FR_BACKGROUND, &req->flags)) {\n\t\tspin_lock(&fc->bg_lock);\n\t\tclear_bit(FR_BACKGROUND, &req->flags);\n\t\tif (fc->num_background == fc->max_background) {\n\t\t\tfc->blocked = 0;\n\t\t\twake_up(&fc->blocked_waitq);\n\t\t} else if (!fc->blocked) {\n\t\t\t/*\n\t\t\t * Wake up next waiter, if any.  It's okay to use\n\t\t\t * waitqueue_active(), as we've already synced up\n\t\t\t * fc->blocked with waiters with the wake_up() call\n\t\t\t * above.\n\t\t\t */\n\t\t\tif (waitqueue_active(&fc->blocked_waitq))\n\t\t\t\twake_up(&fc->blocked_waitq);\n\t\t}\n\n\t\tif (fc->num_background == fc->congestion_threshold && fc->sb) {\n\t\t\tclear_bdi_congested(fc->sb->s_bdi, BLK_RW_SYNC);\n\t\t\tclear_bdi_congested(fc->sb->s_bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tfc->num_background--;\n\t\tfc->active_background--;\n\t\tflush_bg_queue(fc);\n\t\tspin_unlock(&fc->bg_lock);\n\t}\n\twake_up(&req->waitq);\n\tif (req->end)\n\t\treq->end(fc, req);\nput_request:\n\tfuse_put_request(fc, req);\n}\n\nstatic void queue_interrupt(struct fuse_iqueue *fiq, struct fuse_req *req)\n{\n\tspin_lock(&fiq->waitq.lock);\n\tif (test_bit(FR_FINISHED, &req->flags)) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treturn;\n\t}\n\tif (list_empty(&req->intr_entry)) {\n\t\tlist_add_tail(&req->intr_entry, &fiq->interrupts);\n\t\twake_up_locked(&fiq->waitq);\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n}\n\nstatic void request_wait_answer(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\tint err;\n\n\tif (!fc->no_interrupt) {\n\t\t/* Any signal may interrupt this */\n\t\terr = wait_event_interruptible(req->waitq,\n\t\t\t\t\ttest_bit(FR_FINISHED, &req->flags));\n\t\tif (!err)\n\t\t\treturn;\n\n\t\tset_bit(FR_INTERRUPTED, &req->flags);\n\t\t/* matches barrier in fuse_dev_do_read() */\n\t\tsmp_mb__after_atomic();\n\t\tif (test_bit(FR_SENT, &req->flags))\n\t\t\tqueue_interrupt(fiq, req);\n\t}\n\n\tif (!test_bit(FR_FORCE, &req->flags)) {\n\t\t/* Only fatal signals may interrupt this */\n\t\terr = wait_event_killable(req->waitq,\n\t\t\t\t\ttest_bit(FR_FINISHED, &req->flags));\n\t\tif (!err)\n\t\t\treturn;\n\n\t\tspin_lock(&fiq->waitq.lock);\n\t\t/* Request is not yet in userspace, bail out */\n\t\tif (test_bit(FR_PENDING, &req->flags)) {\n\t\t\tlist_del(&req->list);\n\t\t\tspin_unlock(&fiq->waitq.lock);\n\t\t\t__fuse_put_request(req);\n\t\t\treq->out.h.error = -EINTR;\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock(&fiq->waitq.lock);\n\t}\n\n\t/*\n\t * Either request is already in userspace, or it was forced.\n\t * Wait it out.\n\t */\n\twait_event(req->waitq, test_bit(FR_FINISHED, &req->flags));\n}\n\nstatic void __fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tBUG_ON(test_bit(FR_BACKGROUND, &req->flags));\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treq->out.h.error = -ENOTCONN;\n\t} else {\n\t\treq->in.h.unique = fuse_get_unique(fiq);\n\t\tqueue_request(fiq, req);\n\t\t/* acquire extra reference, since request is still needed\n\t\t   after request_end() */\n\t\t__fuse_get_request(req);\n\t\tspin_unlock(&fiq->waitq.lock);\n\n\t\trequest_wait_answer(fc, req);\n\t\t/* Pairs with smp_wmb() in request_end() */\n\t\tsmp_rmb();\n\t}\n}\n\nvoid fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\t__set_bit(FR_ISREPLY, &req->flags);\n\tif (!test_bit(FR_WAITING, &req->flags)) {\n\t\t__set_bit(FR_WAITING, &req->flags);\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\t__fuse_request_send(fc, req);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send);\n\nstatic void fuse_adjust_compat(struct fuse_conn *fc, struct fuse_args *args)\n{\n\tif (fc->minor < 4 && args->in.h.opcode == FUSE_STATFS)\n\t\targs->out.args[0].size = FUSE_COMPAT_STATFS_SIZE;\n\n\tif (fc->minor < 9) {\n\t\tswitch (args->in.h.opcode) {\n\t\tcase FUSE_LOOKUP:\n\t\tcase FUSE_CREATE:\n\t\tcase FUSE_MKNOD:\n\t\tcase FUSE_MKDIR:\n\t\tcase FUSE_SYMLINK:\n\t\tcase FUSE_LINK:\n\t\t\targs->out.args[0].size = FUSE_COMPAT_ENTRY_OUT_SIZE;\n\t\t\tbreak;\n\t\tcase FUSE_GETATTR:\n\t\tcase FUSE_SETATTR:\n\t\t\targs->out.args[0].size = FUSE_COMPAT_ATTR_OUT_SIZE;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (fc->minor < 12) {\n\t\tswitch (args->in.h.opcode) {\n\t\tcase FUSE_CREATE:\n\t\t\targs->in.args[0].size = sizeof(struct fuse_open_in);\n\t\t\tbreak;\n\t\tcase FUSE_MKNOD:\n\t\t\targs->in.args[0].size = FUSE_COMPAT_MKNOD_IN_SIZE;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nssize_t fuse_simple_request(struct fuse_conn *fc, struct fuse_args *args)\n{\n\tstruct fuse_req *req;\n\tssize_t ret;\n\n\treq = fuse_get_req(fc, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\t/* Needs to be done after fuse_get_req() so that fc->minor is valid */\n\tfuse_adjust_compat(fc, args);\n\n\treq->in.h.opcode = args->in.h.opcode;\n\treq->in.h.nodeid = args->in.h.nodeid;\n\treq->in.numargs = args->in.numargs;\n\tmemcpy(req->in.args, args->in.args,\n\t       args->in.numargs * sizeof(struct fuse_in_arg));\n\treq->out.argvar = args->out.argvar;\n\treq->out.numargs = args->out.numargs;\n\tmemcpy(req->out.args, args->out.args,\n\t       args->out.numargs * sizeof(struct fuse_arg));\n\tfuse_request_send(fc, req);\n\tret = req->out.h.error;\n\tif (!ret && args->out.argvar) {\n\t\tBUG_ON(args->out.numargs != 1);\n\t\tret = req->out.args[0].size;\n\t}\n\tfuse_put_request(fc, req);\n\n\treturn ret;\n}\n\nbool fuse_request_queue_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tbool queued = false;\n\n\tWARN_ON(!test_bit(FR_BACKGROUND, &req->flags));\n\tif (!test_bit(FR_WAITING, &req->flags)) {\n\t\t__set_bit(FR_WAITING, &req->flags);\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\t__set_bit(FR_ISREPLY, &req->flags);\n\tspin_lock(&fc->bg_lock);\n\tif (likely(fc->connected)) {\n\t\tfc->num_background++;\n\t\tif (fc->num_background == fc->max_background)\n\t\t\tfc->blocked = 1;\n\t\tif (fc->num_background == fc->congestion_threshold && fc->sb) {\n\t\t\tset_bdi_congested(fc->sb->s_bdi, BLK_RW_SYNC);\n\t\t\tset_bdi_congested(fc->sb->s_bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tlist_add_tail(&req->list, &fc->bg_queue);\n\t\tflush_bg_queue(fc);\n\t\tqueued = true;\n\t}\n\tspin_unlock(&fc->bg_lock);\n\n\treturn queued;\n}\n\nvoid fuse_request_send_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tWARN_ON(!req->end);\n\tif (!fuse_request_queue_background(fc, req)) {\n\t\treq->out.h.error = -ENOTCONN;\n\t\treq->end(fc, req);\n\t\tfuse_put_request(fc, req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_request_send_background);\n\nstatic int fuse_request_send_notify_reply(struct fuse_conn *fc,\n\t\t\t\t\t  struct fuse_req *req, u64 unique)\n{\n\tint err = -ENODEV;\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\t__clear_bit(FR_ISREPLY, &req->flags);\n\treq->in.h.unique = unique;\n\tspin_lock(&fiq->waitq.lock);\n\tif (fiq->connected) {\n\t\tqueue_request(fiq, req);\n\t\terr = 0;\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn err;\n}\n\nvoid fuse_force_forget(struct file *file, u64 nodeid)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_req *req;\n\tstruct fuse_forget_in inarg;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.nlookup = 1;\n\treq = fuse_get_req_nofail_nopages(fc, file);\n\treq->in.h.opcode = FUSE_FORGET;\n\treq->in.h.nodeid = nodeid;\n\treq->in.numargs = 1;\n\treq->in.args[0].size = sizeof(inarg);\n\treq->in.args[0].value = &inarg;\n\t__clear_bit(FR_ISREPLY, &req->flags);\n\t__fuse_request_send(fc, req);\n\t/* ignore errors */\n\tfuse_put_request(fc, req);\n}\n\n/*\n * Lock the request.  Up to the next unlock_request() there mustn't be\n * anything that could cause a page-fault.  If the request was already\n * aborted bail out.\n */\nstatic int lock_request(struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tset_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}\n\n/*\n * Unlock request.  If it was aborted while locked, caller is responsible\n * for unlocking and ending the request.\n */\nstatic int unlock_request(struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tclear_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}\n\nstruct fuse_copy_state {\n\tint write;\n\tstruct fuse_req *req;\n\tstruct iov_iter *iter;\n\tstruct pipe_buffer *pipebufs;\n\tstruct pipe_buffer *currbuf;\n\tstruct pipe_inode_info *pipe;\n\tunsigned long nr_segs;\n\tstruct page *pg;\n\tunsigned len;\n\tunsigned offset;\n\tunsigned move_pages:1;\n};\n\nstatic void fuse_copy_init(struct fuse_copy_state *cs, int write,\n\t\t\t   struct iov_iter *iter)\n{\n\tmemset(cs, 0, sizeof(*cs));\n\tcs->write = write;\n\tcs->iter = iter;\n}\n\n/* Unmap and put previous page of userspace buffer */\nstatic void fuse_copy_finish(struct fuse_copy_state *cs)\n{\n\tif (cs->currbuf) {\n\t\tstruct pipe_buffer *buf = cs->currbuf;\n\n\t\tif (cs->write)\n\t\t\tbuf->len = PAGE_SIZE - cs->len;\n\t\tcs->currbuf = NULL;\n\t} else if (cs->pg) {\n\t\tif (cs->write) {\n\t\t\tflush_dcache_page(cs->pg);\n\t\t\tset_page_dirty_lock(cs->pg);\n\t\t}\n\t\tput_page(cs->pg);\n\t}\n\tcs->pg = NULL;\n}\n\n/*\n * Get another pagefull of userspace buffer, and map it to kernel\n * address space, and lock request\n */\nstatic int fuse_copy_fill(struct fuse_copy_state *cs)\n{\n\tstruct page *page;\n\tint err;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\tif (cs->pipebufs) {\n\t\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\t\tif (!cs->write) {\n\t\t\terr = pipe_buf_confirm(cs->pipe, buf);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->pg = buf->page;\n\t\t\tcs->offset = buf->offset;\n\t\t\tcs->len = buf->len;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs--;\n\t\t} else {\n\t\t\tif (cs->nr_segs == cs->pipe->buffers)\n\t\t\t\treturn -EIO;\n\n\t\t\tpage = alloc_page(GFP_HIGHUSER);\n\t\t\tif (!page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tbuf->page = page;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->pg = page;\n\t\t\tcs->offset = 0;\n\t\t\tcs->len = PAGE_SIZE;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs++;\n\t\t}\n\t} else {\n\t\tsize_t off;\n\t\terr = iov_iter_get_pages(cs->iter, &page, PAGE_SIZE, 1, &off);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tBUG_ON(!err);\n\t\tcs->len = err;\n\t\tcs->offset = off;\n\t\tcs->pg = page;\n\t\tiov_iter_advance(cs->iter, err);\n\t}\n\n\treturn lock_request(cs->req);\n}\n\n/* Do as much copy to/from userspace buffer as we can */\nstatic int fuse_copy_do(struct fuse_copy_state *cs, void **val, unsigned *size)\n{\n\tunsigned ncpy = min(*size, cs->len);\n\tif (val) {\n\t\tvoid *pgaddr = kmap_atomic(cs->pg);\n\t\tvoid *buf = pgaddr + cs->offset;\n\n\t\tif (cs->write)\n\t\t\tmemcpy(buf, *val, ncpy);\n\t\telse\n\t\t\tmemcpy(*val, buf, ncpy);\n\n\t\tkunmap_atomic(pgaddr);\n\t\t*val += ncpy;\n\t}\n\t*size -= ncpy;\n\tcs->len -= ncpy;\n\tcs->offset += ncpy;\n\treturn ncpy;\n}\n\nstatic int fuse_check_page(struct page *page)\n{\n\tif (page_mapcount(page) ||\n\t    page->mapping != NULL ||\n\t    page_count(page) != 1 ||\n\t    (page->flags & PAGE_FLAGS_CHECK_AT_PREP &\n\t     ~(1 << PG_locked |\n\t       1 << PG_referenced |\n\t       1 << PG_uptodate |\n\t       1 << PG_lru |\n\t       1 << PG_active |\n\t       1 << PG_reclaim))) {\n\t\tprintk(KERN_WARNING \"fuse: trying to steal weird page\\n\");\n\t\tprintk(KERN_WARNING \"  page=%p index=%li flags=%08lx, count=%i, mapcount=%i, mapping=%p\\n\", page, page->index, page->flags, page_count(page), page_mapcount(page), page->mapping);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int fuse_try_move_page(struct fuse_copy_state *cs, struct page **pagep)\n{\n\tint err;\n\tstruct page *oldpage = *pagep;\n\tstruct page *newpage;\n\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\n\terr = pipe_buf_confirm(cs->pipe, buf);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(!cs->nr_segs);\n\tcs->currbuf = buf;\n\tcs->len = buf->len;\n\tcs->pipebufs++;\n\tcs->nr_segs--;\n\n\tif (cs->len != PAGE_SIZE)\n\t\tgoto out_fallback;\n\n\tif (pipe_buf_steal(cs->pipe, buf) != 0)\n\t\tgoto out_fallback;\n\n\tnewpage = buf->page;\n\n\tif (!PageUptodate(newpage))\n\t\tSetPageUptodate(newpage);\n\n\tClearPageMappedToDisk(newpage);\n\n\tif (fuse_check_page(newpage) != 0)\n\t\tgoto out_fallback_unlock;\n\n\t/*\n\t * This is a new and locked page, it shouldn't be mapped or\n\t * have any special flags on it\n\t */\n\tif (WARN_ON(page_mapped(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(page_has_private(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageDirty(oldpage) || PageWriteback(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageMlocked(oldpage)))\n\t\tgoto out_fallback_unlock;\n\n\terr = replace_page_cache_page(oldpage, newpage, GFP_KERNEL);\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\treturn err;\n\t}\n\n\tget_page(newpage);\n\n\tif (!(buf->flags & PIPE_BUF_FLAG_LRU))\n\t\tlru_cache_add_file(newpage);\n\n\terr = 0;\n\tspin_lock(&cs->req->waitq.lock);\n\tif (test_bit(FR_ABORTED, &cs->req->flags))\n\t\terr = -ENOENT;\n\telse\n\t\t*pagep = newpage;\n\tspin_unlock(&cs->req->waitq.lock);\n\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\tput_page(newpage);\n\t\treturn err;\n\t}\n\n\tunlock_page(oldpage);\n\tput_page(oldpage);\n\tcs->len = 0;\n\n\treturn 0;\n\nout_fallback_unlock:\n\tunlock_page(newpage);\nout_fallback:\n\tcs->pg = buf->page;\n\tcs->offset = buf->offset;\n\n\terr = lock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\n\nstatic int fuse_ref_page(struct fuse_copy_state *cs, struct page *page,\n\t\t\t unsigned offset, unsigned count)\n{\n\tstruct pipe_buffer *buf;\n\tint err;\n\n\tif (cs->nr_segs == cs->pipe->buffers)\n\t\treturn -EIO;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\n\tbuf = cs->pipebufs;\n\tget_page(page);\n\tbuf->page = page;\n\tbuf->offset = offset;\n\tbuf->len = count;\n\n\tcs->pipebufs++;\n\tcs->nr_segs++;\n\tcs->len = 0;\n\n\treturn 0;\n}\n\n/*\n * Copy a page in the request to/from the userspace buffer.  Must be\n * done atomically\n */\nstatic int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_atomic(page);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_atomic(mapaddr);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}\n\n/* Copy pages in the request to/from userspace buffer */\nstatic int fuse_copy_pages(struct fuse_copy_state *cs, unsigned nbytes,\n\t\t\t   int zeroing)\n{\n\tunsigned i;\n\tstruct fuse_req *req = cs->req;\n\n\tfor (i = 0; i < req->num_pages && (nbytes || zeroing); i++) {\n\t\tint err;\n\t\tunsigned offset = req->page_descs[i].offset;\n\t\tunsigned count = min(nbytes, req->page_descs[i].length);\n\n\t\terr = fuse_copy_page(cs, &req->pages[i], offset, count,\n\t\t\t\t     zeroing);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tnbytes -= count;\n\t}\n\treturn 0;\n}\n\n/* Copy a single argument in the request to/from userspace buffer */\nstatic int fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size)\n{\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}\n\n/* Copy request arguments to/from userspace buffer */\nstatic int fuse_copy_args(struct fuse_copy_state *cs, unsigned numargs,\n\t\t\t  unsigned argpages, struct fuse_arg *args,\n\t\t\t  int zeroing)\n{\n\tint err = 0;\n\tunsigned i;\n\n\tfor (i = 0; !err && i < numargs; i++)  {\n\t\tstruct fuse_arg *arg = &args[i];\n\t\tif (i == numargs - 1 && argpages)\n\t\t\terr = fuse_copy_pages(cs, arg->size, zeroing);\n\t\telse\n\t\t\terr = fuse_copy_one(cs, arg->value, arg->size);\n\t}\n\treturn err;\n}\n\nstatic int forget_pending(struct fuse_iqueue *fiq)\n{\n\treturn fiq->forget_list_head.next != NULL;\n}\n\nstatic int request_pending(struct fuse_iqueue *fiq)\n{\n\treturn !list_empty(&fiq->pending) || !list_empty(&fiq->interrupts) ||\n\t\tforget_pending(fiq);\n}\n\n/*\n * Transfer an interrupt request to userspace\n *\n * Unlike other requests this is assembled on demand, without a need\n * to allocate a separate fuse_req structure.\n *\n * Called with fiq->waitq.lock held, releases it\n */\nstatic int fuse_read_interrupt(struct fuse_iqueue *fiq,\n\t\t\t       struct fuse_copy_state *cs,\n\t\t\t       size_t nbytes, struct fuse_req *req)\n__releases(fiq->waitq.lock)\n{\n\tstruct fuse_in_header ih;\n\tstruct fuse_interrupt_in arg;\n\tunsigned reqsize = sizeof(ih) + sizeof(arg);\n\tint err;\n\n\tlist_del_init(&req->intr_entry);\n\tmemset(&ih, 0, sizeof(ih));\n\tmemset(&arg, 0, sizeof(arg));\n\tih.len = reqsize;\n\tih.opcode = FUSE_INTERRUPT;\n\tih.unique = (req->in.h.unique | FUSE_INT_REQ_BIT);\n\targ.unique = req->in.h.unique;\n\n\tspin_unlock(&fiq->waitq.lock);\n\tif (nbytes < reqsize)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\treturn err ? err : reqsize;\n}\n\nstatic struct fuse_forget_link *dequeue_forget(struct fuse_iqueue *fiq,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp)\n{\n\tstruct fuse_forget_link *head = fiq->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfiq->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fiq->forget_list_head.next == NULL)\n\t\tfiq->forget_list_tail = &fiq->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}\n\nstatic int fuse_read_single_forget(struct fuse_iqueue *fiq,\n\t\t\t\t   struct fuse_copy_state *cs,\n\t\t\t\t   size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tint err;\n\tstruct fuse_forget_link *forget = dequeue_forget(fiq, 1, NULL);\n\tstruct fuse_forget_in arg = {\n\t\t.nlookup = forget->forget_one.nlookup,\n\t};\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_FORGET,\n\t\t.nodeid = forget->forget_one.nodeid,\n\t\t.unique = fuse_get_unique(fiq),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tspin_unlock(&fiq->waitq.lock);\n\tkfree(forget);\n\tif (nbytes < ih.len)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_batch_forget(struct fuse_iqueue *fiq,\n\t\t\t\t   struct fuse_copy_state *cs, size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tint err;\n\tunsigned max_forgets;\n\tunsigned count;\n\tstruct fuse_forget_link *head;\n\tstruct fuse_batch_forget_in arg = { .count = 0 };\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_BATCH_FORGET,\n\t\t.unique = fuse_get_unique(fiq),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tif (nbytes < ih.len) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treturn -EINVAL;\n\t}\n\n\tmax_forgets = (nbytes - ih.len) / sizeof(struct fuse_forget_one);\n\thead = dequeue_forget(fiq, max_forgets, &count);\n\tspin_unlock(&fiq->waitq.lock);\n\n\targ.count = count;\n\tih.len += count * sizeof(struct fuse_forget_one);\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\n\twhile (head) {\n\t\tstruct fuse_forget_link *forget = head;\n\n\t\tif (!err) {\n\t\t\terr = fuse_copy_one(cs, &forget->forget_one,\n\t\t\t\t\t    sizeof(forget->forget_one));\n\t\t}\n\t\thead = forget->next;\n\t\tkfree(forget);\n\t}\n\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_forget(struct fuse_conn *fc, struct fuse_iqueue *fiq,\n\t\t\t    struct fuse_copy_state *cs,\n\t\t\t    size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tif (fc->minor < 16 || fiq->forget_list_head.next->next == NULL)\n\t\treturn fuse_read_single_forget(fiq, cs, nbytes);\n\telse\n\t\treturn fuse_read_batch_forget(fiq, cs, nbytes);\n}\n\n/*\n * Read a single request into the userspace filesystem's buffer.  This\n * function waits until a request is available, then removes it from\n * the pending list and copies request data to userspace buffer.  If\n * no reply is needed (FORGET) or request has been aborted or there\n * was an error during the copying then it's finished by calling\n * request_end().  Otherwise add it to the processing list, and set\n * the 'sent' flag.\n */\nstatic ssize_t fuse_dev_do_read(struct fuse_dev *fud, struct file *file,\n\t\t\t\tstruct fuse_copy_state *cs, size_t nbytes)\n{\n\tssize_t err;\n\tstruct fuse_conn *fc = fud->fc;\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\tstruct fuse_pqueue *fpq = &fud->pq;\n\tstruct fuse_req *req;\n\tstruct fuse_in *in;\n\tunsigned reqsize;\n\tunsigned int hash;\n\n restart:\n\tspin_lock(&fiq->waitq.lock);\n\terr = -EAGAIN;\n\tif ((file->f_flags & O_NONBLOCK) && fiq->connected &&\n\t    !request_pending(fiq))\n\t\tgoto err_unlock;\n\n\terr = wait_event_interruptible_exclusive_locked(fiq->waitq,\n\t\t\t\t!fiq->connected || request_pending(fiq));\n\tif (err)\n\t\tgoto err_unlock;\n\n\tif (!fiq->connected) {\n\t\terr = (fc->aborted && fc->abort_err) ? -ECONNABORTED : -ENODEV;\n\t\tgoto err_unlock;\n\t}\n\n\tif (!list_empty(&fiq->interrupts)) {\n\t\treq = list_entry(fiq->interrupts.next, struct fuse_req,\n\t\t\t\t intr_entry);\n\t\treturn fuse_read_interrupt(fiq, cs, nbytes, req);\n\t}\n\n\tif (forget_pending(fiq)) {\n\t\tif (list_empty(&fiq->pending) || fiq->forget_batch-- > 0)\n\t\t\treturn fuse_read_forget(fc, fiq, cs, nbytes);\n\n\t\tif (fiq->forget_batch <= -8)\n\t\t\tfiq->forget_batch = 16;\n\t}\n\n\treq = list_entry(fiq->pending.next, struct fuse_req, list);\n\tclear_bit(FR_PENDING, &req->flags);\n\tlist_del_init(&req->list);\n\tspin_unlock(&fiq->waitq.lock);\n\n\tin = &req->in;\n\treqsize = in->h.len;\n\n\t/* If request is too large, reply with an error and restart the read */\n\tif (nbytes < reqsize) {\n\t\treq->out.h.error = -EIO;\n\t\t/* SETXATTR is special, since it may contain too large data */\n\t\tif (in->h.opcode == FUSE_SETXATTR)\n\t\t\treq->out.h.error = -E2BIG;\n\t\trequest_end(fc, req);\n\t\tgoto restart;\n\t}\n\tspin_lock(&fpq->lock);\n\tlist_add(&req->list, &fpq->io);\n\tspin_unlock(&fpq->lock);\n\tcs->req = req;\n\terr = fuse_copy_one(cs, &in->h, sizeof(in->h));\n\tif (!err)\n\t\terr = fuse_copy_args(cs, in->numargs, in->argpages,\n\t\t\t\t     (struct fuse_arg *) in->args, 0);\n\tfuse_copy_finish(cs);\n\tspin_lock(&fpq->lock);\n\tclear_bit(FR_LOCKED, &req->flags);\n\tif (!fpq->connected) {\n\t\terr = (fc->aborted && fc->abort_err) ? -ECONNABORTED : -ENODEV;\n\t\tgoto out_end;\n\t}\n\tif (err) {\n\t\treq->out.h.error = -EIO;\n\t\tgoto out_end;\n\t}\n\tif (!test_bit(FR_ISREPLY, &req->flags)) {\n\t\terr = reqsize;\n\t\tgoto out_end;\n\t}\n\thash = fuse_req_hash(req->in.h.unique);\n\tlist_move_tail(&req->list, &fpq->processing[hash]);\n\t__fuse_get_request(req);\n\tset_bit(FR_SENT, &req->flags);\n\tspin_unlock(&fpq->lock);\n\t/* matches barrier in request_wait_answer() */\n\tsmp_mb__after_atomic();\n\tif (test_bit(FR_INTERRUPTED, &req->flags))\n\t\tqueue_interrupt(fiq, req);\n\tfuse_put_request(fc, req);\n\n\treturn reqsize;\n\nout_end:\n\tif (!test_bit(FR_PRIVATE, &req->flags))\n\t\tlist_del_init(&req->list);\n\tspin_unlock(&fpq->lock);\n\trequest_end(fc, req);\n\treturn err;\n\n err_unlock:\n\tspin_unlock(&fiq->waitq.lock);\n\treturn err;\n}\n\nstatic int fuse_dev_open(struct inode *inode, struct file *file)\n{\n\t/*\n\t * The fuse device's file's private_data is used to hold\n\t * the fuse_conn(ection) when it is mounted, and is used to\n\t * keep track of whether the file has been mounted already.\n\t */\n\tfile->private_data = NULL;\n\treturn 0;\n}\n\nstatic ssize_t fuse_dev_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct fuse_copy_state cs;\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tif (!iter_is_iovec(to))\n\t\treturn -EINVAL;\n\n\tfuse_copy_init(&cs, 1, to);\n\n\treturn fuse_dev_do_read(fud, file, &cs, iov_iter_count(to));\n}\n\nstatic ssize_t fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags)\n{\n\tint total, ret;\n\tint page_nr = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(in);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tbufs = kvmalloc_array(pipe->buffers, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, 1, NULL);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fud, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tfor (ret = total = 0; page_nr < cs.nr_segs; total += ret) {\n\t\t/*\n\t\t * Need to be careful about this.  Having buf->ops in module\n\t\t * code can Oops if the buffer persists after module unload.\n\t\t */\n\t\tbufs[page_nr].ops = &nosteal_pipe_buf_ops;\n\t\tbufs[page_nr].flags = 0;\n\t\tret = add_to_pipe(pipe, &bufs[page_nr++]);\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\t}\n\tif (total)\n\t\tret = total;\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tput_page(bufs[page_nr].page);\n\n\tkvfree(bufs);\n\treturn ret;\n}\n\nstatic int fuse_notify_poll(struct fuse_conn *fc, unsigned int size,\n\t\t\t    struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_poll_wakeup_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\tfuse_copy_finish(cs);\n\treturn fuse_notify_poll_wakeup(fc, &outarg);\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_inode(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_inode_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\terr = fuse_reverse_inval_inode(fc->sb, outarg.ino,\n\t\t\t\t\t       outarg.off, outarg.len);\n\t}\n\tup_read(&fc->killsb);\n\treturn err;\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_entry(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_entry_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg) + outarg.namelen + 1)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent, 0, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_delete(struct fuse_conn *fc, unsigned int size,\n\t\t\t      struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_delete_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg) + outarg.namelen + 1)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent,\n\t\t\t\t\t       outarg.child, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n\t\t\t     struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_store_out outarg;\n\tstruct inode *inode;\n\tstruct address_space *mapping;\n\tu64 nodeid;\n\tint err;\n\tpgoff_t index;\n\tunsigned int offset;\n\tunsigned int num;\n\tloff_t file_size;\n\tloff_t end;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto out_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto out_finish;\n\n\terr = -EINVAL;\n\tif (size - sizeof(outarg) != outarg.size)\n\t\tgoto out_finish;\n\n\tnodeid = outarg.nodeid;\n\n\tdown_read(&fc->killsb);\n\n\terr = -ENOENT;\n\tif (!fc->sb)\n\t\tgoto out_up_killsb;\n\n\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\tif (!inode)\n\t\tgoto out_up_killsb;\n\n\tmapping = inode->i_mapping;\n\tindex = outarg.offset >> PAGE_SHIFT;\n\toffset = outarg.offset & ~PAGE_MASK;\n\tfile_size = i_size_read(inode);\n\tend = outarg.offset + outarg.size;\n\tif (end > file_size) {\n\t\tfile_size = end;\n\t\tfuse_write_update_size(inode, file_size);\n\t}\n\n\tnum = outarg.size;\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\terr = -ENOMEM;\n\t\tpage = find_or_create_page(mapping, index,\n\t\t\t\t\t   mapping_gfp_mask(mapping));\n\t\tif (!page)\n\t\t\tgoto out_iput;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_SIZE - offset);\n\t\terr = fuse_copy_page(cs, &page, offset, this_num, 0);\n\t\tif (!err && offset == 0 &&\n\t\t    (this_num == PAGE_SIZE || file_size == end))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (err)\n\t\t\tgoto out_iput;\n\n\t\tnum -= this_num;\n\t\toffset = 0;\n\t\tindex++;\n\t}\n\n\terr = 0;\n\nout_iput:\n\tiput(inode);\nout_up_killsb:\n\tup_read(&fc->killsb);\nout_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic void fuse_retrieve_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\trelease_pages(req->pages, req->num_pages);\n}\n\nstatic int fuse_retrieve(struct fuse_conn *fc, struct inode *inode,\n\t\t\t struct fuse_notify_retrieve_out *outarg)\n{\n\tint err;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct fuse_req *req;\n\tpgoff_t index;\n\tloff_t file_size;\n\tunsigned int num;\n\tunsigned int offset;\n\tsize_t total_len = 0;\n\tunsigned int num_pages;\n\n\toffset = outarg->offset & ~PAGE_MASK;\n\tfile_size = i_size_read(inode);\n\n\tnum = outarg->size;\n\tif (outarg->offset > file_size)\n\t\tnum = 0;\n\telse if (outarg->offset + num > file_size)\n\t\tnum = file_size - outarg->offset;\n\n\tnum_pages = (num + offset + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tnum_pages = min(num_pages, fc->max_pages);\n\n\treq = fuse_get_req(fc, num_pages);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq->in.h.opcode = FUSE_NOTIFY_REPLY;\n\treq->in.h.nodeid = outarg->nodeid;\n\treq->in.numargs = 2;\n\treq->in.argpages = 1;\n\treq->end = fuse_retrieve_end;\n\n\tindex = outarg->offset >> PAGE_SHIFT;\n\n\twhile (num && req->num_pages < num_pages) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_SIZE - offset);\n\t\treq->pages[req->num_pages] = page;\n\t\treq->page_descs[req->num_pages].offset = offset;\n\t\treq->page_descs[req->num_pages].length = this_num;\n\t\treq->num_pages++;\n\n\t\toffset = 0;\n\t\tnum -= this_num;\n\t\ttotal_len += this_num;\n\t\tindex++;\n\t}\n\treq->misc.retrieve_in.offset = outarg->offset;\n\treq->misc.retrieve_in.size = total_len;\n\treq->in.args[0].size = sizeof(req->misc.retrieve_in);\n\treq->in.args[0].value = &req->misc.retrieve_in;\n\treq->in.args[1].size = total_len;\n\n\terr = fuse_request_send_notify_reply(fc, req, outarg->notify_unique);\n\tif (err) {\n\t\tfuse_retrieve_end(fc, req);\n\t\tfuse_put_request(fc, req);\n\t}\n\n\treturn err;\n}\n\nstatic int fuse_notify_retrieve(struct fuse_conn *fc, unsigned int size,\n\t\t\t\tstruct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_retrieve_out outarg;\n\tstruct inode *inode;\n\tint err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg))\n\t\tgoto copy_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto copy_finish;\n\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\tu64 nodeid = outarg.nodeid;\n\n\t\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\t\tif (inode) {\n\t\t\terr = fuse_retrieve(fc, inode, &outarg);\n\t\t\tiput(inode);\n\t\t}\n\t}\n\tup_read(&fc->killsb);\n\n\treturn err;\n\ncopy_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs)\n{\n\t/* Don't try to move pages (yet) */\n\tcs->move_pages = 0;\n\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tcase FUSE_NOTIFY_DELETE:\n\t\treturn fuse_notify_delete(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Look up request on processing list by unique ID */\nstatic struct fuse_req *request_find(struct fuse_pqueue *fpq, u64 unique)\n{\n\tunsigned int hash = fuse_req_hash(unique);\n\tstruct fuse_req *req;\n\n\tlist_for_each_entry(req, &fpq->processing[hash], list) {\n\t\tif (req->in.h.unique == unique)\n\t\t\treturn req;\n\t}\n\treturn NULL;\n}\n\nstatic int copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes)\n{\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}\n\n/*\n * Write a single reply to a request.  First the header is copied from\n * the write buffer.  The request is then searched on the processing\n * list by the unique ID found in the header.  If found, then remove\n * it from the list and copy the rest of the buffer to the request.\n * The request is finished by calling request_end()\n */\nstatic ssize_t fuse_dev_do_write(struct fuse_dev *fud,\n\t\t\t\t struct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_conn *fc = fud->fc;\n\tstruct fuse_pqueue *fpq = &fud->pq;\n\tstruct fuse_req *req;\n\tstruct fuse_out_header oh;\n\n\tif (nbytes < sizeof(struct fuse_out_header))\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &oh, sizeof(oh));\n\tif (err)\n\t\tgoto err_finish;\n\n\terr = -EINVAL;\n\tif (oh.len != nbytes)\n\t\tgoto err_finish;\n\n\t/*\n\t * Zero oh.unique indicates unsolicited notification message\n\t * and error contains notification code.\n\t */\n\tif (!oh.unique) {\n\t\terr = fuse_notify(fc, oh.error, nbytes - sizeof(oh), cs);\n\t\treturn err ? err : nbytes;\n\t}\n\n\terr = -EINVAL;\n\tif (oh.error <= -1000 || oh.error > 0)\n\t\tgoto err_finish;\n\n\tspin_lock(&fpq->lock);\n\terr = -ENOENT;\n\tif (!fpq->connected)\n\t\tgoto err_unlock_pq;\n\n\treq = request_find(fpq, oh.unique & ~FUSE_INT_REQ_BIT);\n\tif (!req)\n\t\tgoto err_unlock_pq;\n\n\t/* Is it an interrupt reply ID? */\n\tif (oh.unique & FUSE_INT_REQ_BIT) {\n\t\t__fuse_get_request(req);\n\t\tspin_unlock(&fpq->lock);\n\n\t\terr = -EINVAL;\n\t\tif (nbytes != sizeof(struct fuse_out_header)) {\n\t\t\tfuse_put_request(fc, req);\n\t\t\tgoto err_finish;\n\t\t}\n\n\t\tif (oh.error == -ENOSYS)\n\t\t\tfc->no_interrupt = 1;\n\t\telse if (oh.error == -EAGAIN)\n\t\t\tqueue_interrupt(&fc->iq, req);\n\t\tfuse_put_request(fc, req);\n\n\t\tfuse_copy_finish(cs);\n\t\treturn nbytes;\n\t}\n\n\tclear_bit(FR_SENT, &req->flags);\n\tlist_move(&req->list, &fpq->io);\n\treq->out.h = oh;\n\tset_bit(FR_LOCKED, &req->flags);\n\tspin_unlock(&fpq->lock);\n\tcs->req = req;\n\tif (!req->out.page_replace)\n\t\tcs->move_pages = 0;\n\n\terr = copy_out_args(cs, &req->out, nbytes);\n\tfuse_copy_finish(cs);\n\n\tspin_lock(&fpq->lock);\n\tclear_bit(FR_LOCKED, &req->flags);\n\tif (!fpq->connected)\n\t\terr = -ENOENT;\n\telse if (err)\n\t\treq->out.h.error = -EIO;\n\tif (!test_bit(FR_PRIVATE, &req->flags))\n\t\tlist_del_init(&req->list);\n\tspin_unlock(&fpq->lock);\n\n\trequest_end(fc, req);\n\n\treturn err ? err : nbytes;\n\n err_unlock_pq:\n\tspin_unlock(&fpq->lock);\n err_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(iocb->ki_filp);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tif (!iter_is_iovec(from))\n\t\treturn -EINVAL;\n\n\tfuse_copy_init(&cs, 0, from);\n\n\treturn fuse_dev_do_write(fud, &cs, iov_iter_count(from));\n}\n\nstatic ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len) {\n\t\tpipe_unlock(pipe);\n\t\tgoto out;\n\t}\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tpipe_buf_get(pipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\nout:\n\tkvfree(bufs);\n\treturn ret;\n}\n\nstatic __poll_t fuse_dev_poll(struct file *file, poll_table *wait)\n{\n\t__poll_t mask = EPOLLOUT | EPOLLWRNORM;\n\tstruct fuse_iqueue *fiq;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn EPOLLERR;\n\n\tfiq = &fud->fc->iq;\n\tpoll_wait(file, &fiq->waitq, wait);\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected)\n\t\tmask = EPOLLERR;\n\telse if (request_pending(fiq))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn mask;\n}\n\n/*\n * Abort all requests on the given list (pending or processing)\n *\n * This function releases and reacquires fc->lock\n */\nstatic void end_requests(struct fuse_conn *fc, struct list_head *head)\n{\n\twhile (!list_empty(head)) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(head->next, struct fuse_req, list);\n\t\treq->out.h.error = -ECONNABORTED;\n\t\tclear_bit(FR_SENT, &req->flags);\n\t\tlist_del_init(&req->list);\n\t\trequest_end(fc, req);\n\t}\n}\n\nstatic void end_polls(struct fuse_conn *fc)\n{\n\tstruct rb_node *p;\n\n\tp = rb_first(&fc->polled_files);\n\n\twhile (p) {\n\t\tstruct fuse_file *ff;\n\t\tff = rb_entry(p, struct fuse_file, polled_node);\n\t\twake_up_interruptible_all(&ff->poll_wait);\n\n\t\tp = rb_next(p);\n\t}\n}\n\n/*\n * Abort all requests.\n *\n * Emergency exit in case of a malicious or accidental deadlock, or just a hung\n * filesystem.\n *\n * The same effect is usually achievable through killing the filesystem daemon\n * and all users of the filesystem.  The exception is the combination of an\n * asynchronous request and the tricky deadlock (see\n * Documentation/filesystems/fuse.txt).\n *\n * Aborting requests under I/O goes as follows: 1: Separate out unlocked\n * requests, they should be finished off immediately.  Locked requests will be\n * finished after unlock; see unlock_request(). 2: Finish off the unlocked\n * requests.  It is possible that some request will finish before we can.  This\n * is OK, the request will in that case be removed from the list before we touch\n * it.\n */\nvoid fuse_abort_conn(struct fuse_conn *fc, bool is_abort)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tstruct fuse_dev *fud;\n\t\tstruct fuse_req *req, *next;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\t/* Background queuing checks fc->connected under bg_lock */\n\t\tspin_lock(&fc->bg_lock);\n\t\tfc->connected = 0;\n\t\tspin_unlock(&fc->bg_lock);\n\n\t\tfc->aborted = is_abort;\n\t\tfuse_set_initialized(fc);\n\t\tlist_for_each_entry(fud, &fc->devices, entry) {\n\t\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\n\t\t\tspin_lock(&fpq->lock);\n\t\t\tfpq->connected = 0;\n\t\t\tlist_for_each_entry_safe(req, next, &fpq->io, list) {\n\t\t\t\treq->out.h.error = -ECONNABORTED;\n\t\t\t\tspin_lock(&req->waitq.lock);\n\t\t\t\tset_bit(FR_ABORTED, &req->flags);\n\t\t\t\tif (!test_bit(FR_LOCKED, &req->flags)) {\n\t\t\t\t\tset_bit(FR_PRIVATE, &req->flags);\n\t\t\t\t\t__fuse_get_request(req);\n\t\t\t\t\tlist_move(&req->list, &to_end);\n\t\t\t\t}\n\t\t\t\tspin_unlock(&req->waitq.lock);\n\t\t\t}\n\t\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\t\tlist_splice_tail_init(&fpq->processing[i],\n\t\t\t\t\t\t      &to_end);\n\t\t\tspin_unlock(&fpq->lock);\n\t\t}\n\t\tspin_lock(&fc->bg_lock);\n\t\tfc->blocked = 0;\n\t\tfc->max_background = UINT_MAX;\n\t\tflush_bg_queue(fc);\n\t\tspin_unlock(&fc->bg_lock);\n\n\t\tspin_lock(&fiq->waitq.lock);\n\t\tfiq->connected = 0;\n\t\tlist_for_each_entry(req, &fiq->pending, list)\n\t\t\tclear_bit(FR_PENDING, &req->flags);\n\t\tlist_splice_tail_init(&fiq->pending, &to_end);\n\t\twhile (forget_pending(fiq))\n\t\t\tkfree(dequeue_forget(fiq, 1, NULL));\n\t\twake_up_all_locked(&fiq->waitq);\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tspin_unlock(&fc->lock);\n\n\t\tend_requests(fc, &to_end);\n\t} else {\n\t\tspin_unlock(&fc->lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_abort_conn);\n\nvoid fuse_wait_aborted(struct fuse_conn *fc)\n{\n\t/* matches implicit memory barrier in fuse_drop_waiting() */\n\tsmp_mb();\n\twait_event(fc->blocked_waitq, atomic_read(&fc->num_waiting) == 0);\n}\n\nint fuse_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (fud) {\n\t\tstruct fuse_conn *fc = fud->fc;\n\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\tspin_lock(&fpq->lock);\n\t\tWARN_ON(!list_empty(&fpq->io));\n\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\tlist_splice_init(&fpq->processing[i], &to_end);\n\t\tspin_unlock(&fpq->lock);\n\n\t\tend_requests(fc, &to_end);\n\n\t\t/* Are we the last open device? */\n\t\tif (atomic_dec_and_test(&fc->dev_count)) {\n\t\t\tWARN_ON(fc->iq.fasync != NULL);\n\t\t\tfuse_abort_conn(fc, false);\n\t\t}\n\t\tfuse_dev_free(fud);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fuse_dev_release);\n\nstatic int fuse_dev_fasync(int fd, struct file *file, int on)\n{\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fud->fc->iq.fasync);\n}\n\nstatic int fuse_device_clone(struct fuse_conn *fc, struct file *new)\n{\n\tstruct fuse_dev *fud;\n\n\tif (new->private_data)\n\t\treturn -EINVAL;\n\n\tfud = fuse_dev_alloc(fc);\n\tif (!fud)\n\t\treturn -ENOMEM;\n\n\tnew->private_data = fud;\n\tatomic_inc(&fc->dev_count);\n\n\treturn 0;\n}\n\nstatic long fuse_dev_ioctl(struct file *file, unsigned int cmd,\n\t\t\t   unsigned long arg)\n{\n\tint err = -ENOTTY;\n\n\tif (cmd == FUSE_DEV_IOC_CLONE) {\n\t\tint oldfd;\n\n\t\terr = -EFAULT;\n\t\tif (!get_user(oldfd, (__u32 __user *) arg)) {\n\t\t\tstruct file *old = fget(oldfd);\n\n\t\t\terr = -EINVAL;\n\t\t\tif (old) {\n\t\t\t\tstruct fuse_dev *fud = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * Check against file->f_op because CUSE\n\t\t\t\t * uses the same ioctl handler.\n\t\t\t\t */\n\t\t\t\tif (old->f_op == file->f_op &&\n\t\t\t\t    old->f_cred->user_ns == file->f_cred->user_ns)\n\t\t\t\t\tfud = fuse_get_dev(old);\n\n\t\t\t\tif (fud) {\n\t\t\t\t\tmutex_lock(&fuse_mutex);\n\t\t\t\t\terr = fuse_device_clone(fud->fc, file);\n\t\t\t\t\tmutex_unlock(&fuse_mutex);\n\t\t\t\t}\n\t\t\t\tfput(old);\n\t\t\t}\n\t\t}\n\t}\n\treturn err;\n}\n\nconst struct file_operations fuse_dev_operations = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= fuse_dev_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= fuse_dev_read,\n\t.splice_read\t= fuse_dev_splice_read,\n\t.write_iter\t= fuse_dev_write,\n\t.splice_write\t= fuse_dev_splice_write,\n\t.poll\t\t= fuse_dev_poll,\n\t.release\t= fuse_dev_release,\n\t.fasync\t\t= fuse_dev_fasync,\n\t.unlocked_ioctl = fuse_dev_ioctl,\n\t.compat_ioctl   = fuse_dev_ioctl,\n};\nEXPORT_SYMBOL_GPL(fuse_dev_operations);\n\nstatic struct miscdevice fuse_miscdevice = {\n\t.minor = FUSE_MINOR,\n\t.name  = \"fuse\",\n\t.fops = &fuse_dev_operations,\n};\n\nint __init fuse_dev_init(void)\n{\n\tint err = -ENOMEM;\n\tfuse_req_cachep = kmem_cache_create(\"fuse_request\",\n\t\t\t\t\t    sizeof(struct fuse_req),\n\t\t\t\t\t    0, 0, NULL);\n\tif (!fuse_req_cachep)\n\t\tgoto out;\n\n\terr = misc_register(&fuse_miscdevice);\n\tif (err)\n\t\tgoto out_cache_clean;\n\n\treturn 0;\n\n out_cache_clean:\n\tkmem_cache_destroy(fuse_req_cachep);\n out:\n\treturn err;\n}\n\nvoid fuse_dev_cleanup(void)\n{\n\tmisc_deregister(&fuse_miscdevice);\n\tkmem_cache_destroy(fuse_req_cachep);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/pipe.c\n *\n *  Copyright (C) 1991, 1992, 1999  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/log2.h>\n#include <linux/mount.h>\n#include <linux/magic.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/uio.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/memcontrol.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include \"internal.h\"\n\n/*\n * The max size that a non-root user is allowed to grow the pipe. Can\n * be set by root in /proc/sys/fs/pipe-max-size\n */\nunsigned int pipe_max_size = 1048576;\n\n/* Maximum allocatable pages per user. Hard limit is unset by default, soft\n * matches default values.\n */\nunsigned long pipe_user_pages_hard;\nunsigned long pipe_user_pages_soft = PIPE_DEF_BUFFERS * INR_OPEN_CUR;\n\n/*\n * We use a start+len construction, which provides full use of the \n * allocated memory.\n * -- Florian Coosmann (FGC)\n * \n * Reads with count = 0 should always return 0.\n * -- Julian Bradfield 1999-06-07.\n *\n * FIFOs and Pipes now generate SIGIO for both readers and writers.\n * -- Jeremy Elson <jelson@circlemud.org> 2001-08-16\n *\n * pipe_read & write cleanup\n * -- Manfred Spraul <manfred@colorfullife.com> 2002-05-09\n */\n\nstatic void pipe_lock_nested(struct pipe_inode_info *pipe, int subclass)\n{\n\tif (pipe->files)\n\t\tmutex_lock_nested(&pipe->mutex, subclass);\n}\n\nvoid pipe_lock(struct pipe_inode_info *pipe)\n{\n\t/*\n\t * pipe_lock() nests non-pipe inode locks (for writing to a file)\n\t */\n\tpipe_lock_nested(pipe, I_MUTEX_PARENT);\n}\nEXPORT_SYMBOL(pipe_lock);\n\nvoid pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tif (pipe->files)\n\t\tmutex_unlock(&pipe->mutex);\n}\nEXPORT_SYMBOL(pipe_unlock);\n\nstatic inline void __pipe_lock(struct pipe_inode_info *pipe)\n{\n\tmutex_lock_nested(&pipe->mutex, I_MUTEX_PARENT);\n}\n\nstatic inline void __pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tmutex_unlock(&pipe->mutex);\n}\n\nvoid pipe_double_lock(struct pipe_inode_info *pipe1,\n\t\t      struct pipe_inode_info *pipe2)\n{\n\tBUG_ON(pipe1 == pipe2);\n\n\tif (pipe1 < pipe2) {\n\t\tpipe_lock_nested(pipe1, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe2, I_MUTEX_CHILD);\n\t} else {\n\t\tpipe_lock_nested(pipe2, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe1, I_MUTEX_CHILD);\n\t}\n}\n\n/* Drop the inode semaphore and wait for a pipe event, atomically */\nvoid pipe_wait(struct pipe_inode_info *pipe)\n{\n\tDEFINE_WAIT(wait);\n\n\t/*\n\t * Pipes are system-local resources, so sleeping on them\n\t * is considered a noninteractive wait:\n\t */\n\tprepare_to_wait(&pipe->wait, &wait, TASK_INTERRUPTIBLE);\n\tpipe_unlock(pipe);\n\tschedule();\n\tfinish_wait(&pipe->wait, &wait);\n\tpipe_lock(pipe);\n}\n\nstatic void anon_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t  struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * If nobody else uses this page, and we don't already have a\n\t * temporary page, let's keep track of it as a one-deep\n\t * allocation cache. (Otherwise just release our reference to it)\n\t */\n\tif (page_count(page) == 1 && !pipe->tmp_page)\n\t\tpipe->tmp_page = page;\n\telse\n\t\tput_page(page);\n}\n\nstatic int anon_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t       struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\tif (page_count(page) == 1) {\n\t\tif (memcg_kmem_enabled())\n\t\t\tmemcg_kmem_uncharge(page, 0);\n\t\t__SetPageLocked(page);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n/**\n * generic_pipe_buf_steal - attempt to take ownership of a &pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n *\n * Description:\n *\tThis function attempts to steal the &struct page attached to\n *\t@buf. If successful, this function returns 0 and returns with\n *\tthe page locked. The caller may then reuse the page for whatever\n *\the wishes; the typical use is insertion into a different file\n *\tpage cache.\n */\nint generic_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t   struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * A reference of one is golden, that means that the owner of this\n\t * page is the only one holding a reference to it. lock the page\n\t * and return OK.\n\t */\n\tif (page_count(page) == 1) {\n\t\tlock_page(page);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\nEXPORT_SYMBOL(generic_pipe_buf_steal);\n\n/**\n * generic_pipe_buf_get - get a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n *\n * Description:\n *\tThis function grabs an extra reference to @buf. It's used in\n *\tin the tee() system call, when we duplicate the buffers in one\n *\tpipe into another.\n */\nvoid generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\tget_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_get);\n\n/**\n * generic_pipe_buf_confirm - verify contents of the pipe buffer\n * @info:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to confirm\n *\n * Description:\n *\tThis function does nothing, because the generic pipe code uses\n *\tpages that are always good when inserted into the pipe.\n */\nint generic_pipe_buf_confirm(struct pipe_inode_info *info,\n\t\t\t     struct pipe_buffer *buf)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_pipe_buf_confirm);\n\n/**\n * generic_pipe_buf_release - put a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n *\n * Description:\n *\tThis function releases a reference to @buf.\n */\nvoid generic_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t      struct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_release);\n\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = {\n\t.can_merge = 1,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = anon_pipe_buf_release,\n\t.steal = anon_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic const struct pipe_buf_operations packet_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = anon_pipe_buf_release,\n\t.steal = anon_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic ssize_t\npipe_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tsize_t total_len = iov_iter_count(to);\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint do_wakeup;\n\tssize_t ret;\n\n\t/* Null read succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\tdo_wakeup = 0;\n\tret = 0;\n\t__pipe_lock(pipe);\n\tfor (;;) {\n\t\tint bufs = pipe->nrbufs;\n\t\tif (bufs) {\n\t\t\tint curbuf = pipe->curbuf;\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + curbuf;\n\t\t\tsize_t chars = buf->len;\n\t\t\tsize_t written;\n\t\t\tint error;\n\n\t\t\tif (chars > total_len)\n\t\t\t\tchars = total_len;\n\n\t\t\terror = pipe_buf_confirm(pipe, buf);\n\t\t\tif (error) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = error;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twritten = copy_page_to_iter(buf->page, buf->offset, chars, to);\n\t\t\tif (unlikely(written < chars)) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += chars;\n\t\t\tbuf->offset += chars;\n\t\t\tbuf->len -= chars;\n\n\t\t\t/* Was it a packet buffer? Clean up and exit */\n\t\t\tif (buf->flags & PIPE_BUF_FLAG_PACKET) {\n\t\t\t\ttotal_len = chars;\n\t\t\t\tbuf->len = 0;\n\t\t\t}\n\n\t\t\tif (!buf->len) {\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tcurbuf = (curbuf + 1) & (pipe->buffers - 1);\n\t\t\t\tpipe->curbuf = curbuf;\n\t\t\t\tpipe->nrbufs = --bufs;\n\t\t\t\tdo_wakeup = 1;\n\t\t\t}\n\t\t\ttotal_len -= chars;\n\t\t\tif (!total_len)\n\t\t\t\tbreak;\t/* common path: read succeeded */\n\t\t}\n\t\tif (bufs)\t/* More to do? */\n\t\t\tcontinue;\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (!pipe->waiting_writers) {\n\t\t\t/* syscall merging: Usually we must not sleep\n\t\t\t * if O_NONBLOCK is set, or if we got some data.\n\t\t\t * But if a writer sleeps in kernel space, then\n\t\t\t * we can wait for that data without violating POSIX.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (do_wakeup) {\n\t\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLOUT | EPOLLWRNORM);\n \t\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t\t}\n\t\tpipe_wait(pipe);\n\t}\n\t__pipe_unlock(pipe);\n\n\t/* Signal writers asynchronously that there is more room. */\n\tif (do_wakeup) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLOUT | EPOLLWRNORM);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\tif (ret > 0)\n\t\tfile_accessed(filp);\n\treturn ret;\n}\n\nstatic inline int is_packetized(struct file *file)\n{\n\treturn (file->f_flags & O_DIRECT) != 0;\n}\n\nstatic ssize_t\npipe_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tssize_t ret = 0;\n\tint do_wakeup = 0;\n\tsize_t total_len = iov_iter_count(from);\n\tssize_t chars;\n\n\t/* Null write succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\t__pipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n\t/* We try to merge small writes */\n\tchars = total_len & (PAGE_SIZE-1); /* size of the last buffer */\n\tif (pipe->nrbufs && chars != 0) {\n\t\tint lastbuf = (pipe->curbuf + pipe->nrbufs - 1) &\n\t\t\t\t\t\t\t(pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + lastbuf;\n\t\tint offset = buf->offset + buf->len;\n\n\t\tif (buf->ops->can_merge && offset + chars <= PAGE_SIZE) {\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = copy_page_from_iter(buf->page, offset, chars, from);\n\t\t\tif (unlikely(ret < chars)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdo_wakeup = 1;\n\t\t\tbuf->len += ret;\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (;;) {\n\t\tint bufs;\n\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\t\tbufs = pipe->nrbufs;\n\t\tif (bufs < pipe->buffers) {\n\t\t\tint newbuf = (pipe->curbuf + bufs) & (pipe->buffers-1);\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\t\t\tstruct page *page = pipe->tmp_page;\n\t\t\tint copied;\n\n\t\t\tif (!page) {\n\t\t\t\tpage = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT);\n\t\t\t\tif (unlikely(!page)) {\n\t\t\t\t\tret = ret ? : -ENOMEM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tpipe->tmp_page = page;\n\t\t\t}\n\t\t\t/* Always wake up, even if the copy fails. Otherwise\n\t\t\t * we lock up (O_NONBLOCK-)readers that sleep due to\n\t\t\t * syscall merging.\n\t\t\t * FIXME! Is this really true?\n\t\t\t */\n\t\t\tdo_wakeup = 1;\n\t\t\tcopied = copy_page_from_iter(page, 0, PAGE_SIZE, from);\n\t\t\tif (unlikely(copied < PAGE_SIZE && iov_iter_count(from))) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += copied;\n\n\t\t\t/* Insert it into the buffer array */\n\t\t\tbuf->page = page;\n\t\t\tbuf->ops = &anon_pipe_buf_ops;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = copied;\n\t\t\tbuf->flags = 0;\n\t\t\tif (is_packetized(filp)) {\n\t\t\t\tbuf->ops = &packet_pipe_buf_ops;\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_PACKET;\n\t\t\t}\n\t\t\tpipe->nrbufs = ++bufs;\n\t\t\tpipe->tmp_page = NULL;\n\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (bufs < pipe->buffers)\n\t\t\tcontinue;\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tif (!ret)\n\t\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (do_wakeup) {\n\t\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLRDNORM);\n\t\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\t\tdo_wakeup = 0;\n\t\t}\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\nout:\n\t__pipe_unlock(pipe);\n\tif (do_wakeup) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLRDNORM);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t}\n\tif (ret > 0 && sb_start_write_trylock(file_inode(filp)->i_sb)) {\n\t\tint err = file_update_time(filp);\n\t\tif (err)\n\t\t\tret = err;\n\t\tsb_end_write(file_inode(filp)->i_sb);\n\t}\n\treturn ret;\n}\n\nstatic long pipe_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint count, buf, nrbufs;\n\n\tswitch (cmd) {\n\t\tcase FIONREAD:\n\t\t\t__pipe_lock(pipe);\n\t\t\tcount = 0;\n\t\t\tbuf = pipe->curbuf;\n\t\t\tnrbufs = pipe->nrbufs;\n\t\t\twhile (--nrbufs >= 0) {\n\t\t\t\tcount += pipe->bufs[buf].len;\n\t\t\t\tbuf = (buf+1) & (pipe->buffers - 1);\n\t\t\t}\n\t\t\t__pipe_unlock(pipe);\n\n\t\t\treturn put_user(count, (int __user *)arg);\n\t\tdefault:\n\t\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* No kernel lock held - fine */\nstatic __poll_t\npipe_poll(struct file *filp, poll_table *wait)\n{\n\t__poll_t mask;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint nrbufs;\n\n\tpoll_wait(filp, &pipe->wait, wait);\n\n\t/* Reading only -- no need for acquiring the semaphore.  */\n\tnrbufs = pipe->nrbufs;\n\tmask = 0;\n\tif (filp->f_mode & FMODE_READ) {\n\t\tmask = (nrbufs > 0) ? EPOLLIN | EPOLLRDNORM : 0;\n\t\tif (!pipe->writers && filp->f_version != pipe->w_counter)\n\t\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tmask |= (nrbufs < pipe->buffers) ? EPOLLOUT | EPOLLWRNORM : 0;\n\t\t/*\n\t\t * Most Unices do not set EPOLLERR for FIFOs but on Linux they\n\t\t * behave exactly like pipes for poll().\n\t\t */\n\t\tif (!pipe->readers)\n\t\t\tmask |= EPOLLERR;\n\t}\n\n\treturn mask;\n}\n\nstatic void put_pipe_info(struct inode *inode, struct pipe_inode_info *pipe)\n{\n\tint kill = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (!--pipe->files) {\n\t\tinode->i_pipe = NULL;\n\t\tkill = 1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (kill)\n\t\tfree_pipe_info(pipe);\n}\n\nstatic int\npipe_release(struct inode *inode, struct file *file)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\tif (pipe->readers || pipe->writers) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLOUT | EPOLLRDNORM | EPOLLWRNORM | EPOLLERR | EPOLLHUP);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}\n\nstatic int\npipe_fasync(int fd, struct file *filp, int on)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint retval = 0;\n\n\t__pipe_lock(pipe);\n\tif (filp->f_mode & FMODE_READ)\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_readers);\n\tif ((filp->f_mode & FMODE_WRITE) && retval >= 0) {\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_writers);\n\t\tif (retval < 0 && (filp->f_mode & FMODE_READ))\n\t\t\t/* this can happen only if on == T */\n\t\t\tfasync_helper(-1, filp, 0, &pipe->fasync_readers);\n\t}\n\t__pipe_unlock(pipe);\n\treturn retval;\n}\n\nstatic unsigned long account_pipe_buffers(struct user_struct *user,\n                                 unsigned long old, unsigned long new)\n{\n\treturn atomic_long_add_return(new - old, &user->pipe_bufs);\n}\n\nstatic bool too_many_pipe_buffers_soft(unsigned long user_bufs)\n{\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}\n\nstatic bool too_many_pipe_buffers_hard(unsigned long user_bufs)\n{\n\tunsigned long hard_limit = READ_ONCE(pipe_user_pages_hard);\n\n\treturn hard_limit && user_bufs > hard_limit;\n}\n\nstatic bool is_unprivileged_user(void)\n{\n\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n}\n\nstruct pipe_inode_info *alloc_pipe_info(void)\n{\n\tstruct pipe_inode_info *pipe;\n\tunsigned long pipe_bufs = PIPE_DEF_BUFFERS;\n\tstruct user_struct *user = get_current_user();\n\tunsigned long user_bufs;\n\tunsigned int max_size = READ_ONCE(pipe_max_size);\n\n\tpipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT);\n\tif (pipe == NULL)\n\t\tgoto out_free_uid;\n\n\tif (pipe_bufs * PAGE_SIZE > max_size && !capable(CAP_SYS_RESOURCE))\n\t\tpipe_bufs = max_size >> PAGE_SHIFT;\n\n\tuser_bufs = account_pipe_buffers(user, 0, pipe_bufs);\n\n\tif (too_many_pipe_buffers_soft(user_bufs) && is_unprivileged_user()) {\n\t\tuser_bufs = account_pipe_buffers(user, pipe_bufs, 1);\n\t\tpipe_bufs = 1;\n\t}\n\n\tif (too_many_pipe_buffers_hard(user_bufs) && is_unprivileged_user())\n\t\tgoto out_revert_acct;\n\n\tpipe->bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer),\n\t\t\t     GFP_KERNEL_ACCOUNT);\n\n\tif (pipe->bufs) {\n\t\tinit_waitqueue_head(&pipe->wait);\n\t\tpipe->r_counter = pipe->w_counter = 1;\n\t\tpipe->buffers = pipe_bufs;\n\t\tpipe->user = user;\n\t\tmutex_init(&pipe->mutex);\n\t\treturn pipe;\n\t}\n\nout_revert_acct:\n\t(void) account_pipe_buffers(user, pipe_bufs, 0);\n\tkfree(pipe);\nout_free_uid:\n\tfree_uid(user);\n\treturn NULL;\n}\n\nvoid free_pipe_info(struct pipe_inode_info *pipe)\n{\n\tint i;\n\n\t(void) account_pipe_buffers(pipe->user, pipe->buffers, 0);\n\tfree_uid(pipe->user);\n\tfor (i = 0; i < pipe->buffers; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n\tif (pipe->tmp_page)\n\t\t__free_page(pipe->tmp_page);\n\tkfree(pipe->bufs);\n\tkfree(pipe);\n}\n\nstatic struct vfsmount *pipe_mnt __read_mostly;\n\n/*\n * pipefs_dname() is called from d_path().\n */\nstatic char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"pipe:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations pipefs_dentry_operations = {\n\t.d_dname\t= pipefs_dname,\n};\n\nstatic struct inode * get_pipe_inode(void)\n{\n\tstruct inode *inode = new_inode_pseudo(pipe_mnt->mnt_sb);\n\tstruct pipe_inode_info *pipe;\n\n\tif (!inode)\n\t\tgoto fail_inode;\n\n\tinode->i_ino = get_next_ino();\n\n\tpipe = alloc_pipe_info();\n\tif (!pipe)\n\t\tgoto fail_iput;\n\n\tinode->i_pipe = pipe;\n\tpipe->files = 2;\n\tpipe->readers = pipe->writers = 1;\n\tinode->i_fop = &pipefifo_fops;\n\n\t/*\n\t * Mark the inode dirty from the very beginning,\n\t * that way it will never be moved to the dirty\n\t * list because \"mark_inode_dirty()\" will think\n\t * that it already _is_ on the dirty list.\n\t */\n\tinode->i_state = I_DIRTY;\n\tinode->i_mode = S_IFIFO | S_IRUSR | S_IWUSR;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);\n\n\treturn inode;\n\nfail_iput:\n\tiput(inode);\n\nfail_inode:\n\treturn NULL;\n}\n\nint create_pipe_files(struct file **res, int flags)\n{\n\tstruct inode *inode = get_pipe_inode();\n\tstruct file *f;\n\n\tif (!inode)\n\t\treturn -ENFILE;\n\n\tf = alloc_file_pseudo(inode, pipe_mnt, \"\",\n\t\t\t\tO_WRONLY | (flags & (O_NONBLOCK | O_DIRECT)),\n\t\t\t\t&pipefifo_fops);\n\tif (IS_ERR(f)) {\n\t\tfree_pipe_info(inode->i_pipe);\n\t\tiput(inode);\n\t\treturn PTR_ERR(f);\n\t}\n\n\tf->private_data = inode->i_pipe;\n\n\tres[0] = alloc_file_clone(f, O_RDONLY | (flags & O_NONBLOCK),\n\t\t\t\t  &pipefifo_fops);\n\tif (IS_ERR(res[0])) {\n\t\tput_pipe_info(inode, inode->i_pipe);\n\t\tfput(f);\n\t\treturn PTR_ERR(res[0]);\n\t}\n\tres[0]->private_data = inode->i_pipe;\n\tres[1] = f;\n\treturn 0;\n}\n\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags)\n{\n\tint error;\n\tint fdw, fdr;\n\n\tif (flags & ~(O_CLOEXEC | O_NONBLOCK | O_DIRECT))\n\t\treturn -EINVAL;\n\n\terror = create_pipe_files(files, flags);\n\tif (error)\n\t\treturn error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_read_pipe;\n\tfdr = error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_fdr;\n\tfdw = error;\n\n\taudit_fd_pair(fdr, fdw);\n\tfd[0] = fdr;\n\tfd[1] = fdw;\n\treturn 0;\n\n err_fdr:\n\tput_unused_fd(fdr);\n err_read_pipe:\n\tfput(files[0]);\n\tfput(files[1]);\n\treturn error;\n}\n\nint do_pipe_flags(int *fd, int flags)\n{\n\tstruct file *files[2];\n\tint error = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tfd_install(fd[0], files[0]);\n\t\tfd_install(fd[1], files[1]);\n\t}\n\treturn error;\n}\n\n/*\n * sys_pipe() is the normal C calling standard for creating\n * a pipe. It's not the way Unix traditionally does this, though.\n */\nstatic int do_pipe2(int __user *fildes, int flags)\n{\n\tstruct file *files[2];\n\tint fd[2];\n\tint error;\n\n\terror = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tif (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) {\n\t\t\tfput(files[0]);\n\t\t\tfput(files[1]);\n\t\t\tput_unused_fd(fd[0]);\n\t\t\tput_unused_fd(fd[1]);\n\t\t\terror = -EFAULT;\n\t\t} else {\n\t\t\tfd_install(fd[0], files[0]);\n\t\t\tfd_install(fd[1], files[1]);\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags)\n{\n\treturn do_pipe2(fildes, flags);\n}\n\nSYSCALL_DEFINE1(pipe, int __user *, fildes)\n{\n\treturn do_pipe2(fildes, 0);\n}\n\nstatic int wait_for_partner(struct pipe_inode_info *pipe, unsigned int *cnt)\n{\n\tint cur = *cnt;\t\n\n\twhile (cur == *cnt) {\n\t\tpipe_wait(pipe);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t}\n\treturn cur == *cnt ? -ERESTARTSYS : 0;\n}\n\nstatic void wake_up_partner(struct pipe_inode_info *pipe)\n{\n\twake_up_interruptible(&pipe->wait);\n}\n\nstatic int fifo_open(struct inode *inode, struct file *filp)\n{\n\tstruct pipe_inode_info *pipe;\n\tbool is_pipe = inode->i_sb->s_magic == PIPEFS_MAGIC;\n\tint ret;\n\n\tfilp->f_version = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (inode->i_pipe) {\n\t\tpipe = inode->i_pipe;\n\t\tpipe->files++;\n\t\tspin_unlock(&inode->i_lock);\n\t} else {\n\t\tspin_unlock(&inode->i_lock);\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\t\tpipe->files = 1;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (unlikely(inode->i_pipe)) {\n\t\t\tinode->i_pipe->files++;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tfree_pipe_info(pipe);\n\t\t\tpipe = inode->i_pipe;\n\t\t} else {\n\t\t\tinode->i_pipe = pipe;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tfilp->private_data = pipe;\n\t/* OK, we have a pipe and it's pinned down */\n\n\t__pipe_lock(pipe);\n\n\t/* We can only do regular read/write on fifos */\n\tfilp->f_mode &= (FMODE_READ | FMODE_WRITE);\n\n\tswitch (filp->f_mode) {\n\tcase FMODE_READ:\n\t/*\n\t *  O_RDONLY\n\t *  POSIX.1 says that O_NONBLOCK means return with the FIFO\n\t *  opened, even when there is no process writing the FIFO.\n\t */\n\t\tpipe->r_counter++;\n\t\tif (pipe->readers++ == 0)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->writers) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\t\t/* suppress EPOLLHUP until we have\n\t\t\t\t * seen a writer */\n\t\t\t\tfilp->f_version = pipe->w_counter;\n\t\t\t} else {\n\t\t\t\tif (wait_for_partner(pipe, &pipe->w_counter))\n\t\t\t\t\tgoto err_rd;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t\n\tcase FMODE_WRITE:\n\t/*\n\t *  O_WRONLY\n\t *  POSIX.1 says that O_NONBLOCK means return -1 with\n\t *  errno=ENXIO when there is no process reading the FIFO.\n\t */\n\t\tret = -ENXIO;\n\t\tif (!is_pipe && (filp->f_flags & O_NONBLOCK) && !pipe->readers)\n\t\t\tgoto err;\n\n\t\tpipe->w_counter++;\n\t\tif (!pipe->writers++)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->readers) {\n\t\t\tif (wait_for_partner(pipe, &pipe->r_counter))\n\t\t\t\tgoto err_wr;\n\t\t}\n\t\tbreak;\n\t\n\tcase FMODE_READ | FMODE_WRITE:\n\t/*\n\t *  O_RDWR\n\t *  POSIX.1 leaves this case \"undefined\" when O_NONBLOCK is set.\n\t *  This implementation will NEVER block on a O_RDWR open, since\n\t *  the process can at least talk to itself.\n\t */\n\n\t\tpipe->readers++;\n\t\tpipe->writers++;\n\t\tpipe->r_counter++;\n\t\tpipe->w_counter++;\n\t\tif (pipe->readers == 1 || pipe->writers == 1)\n\t\t\twake_up_partner(pipe);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* Ok! */\n\t__pipe_unlock(pipe);\n\treturn 0;\n\nerr_rd:\n\tif (!--pipe->readers)\n\t\twake_up_interruptible(&pipe->wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr_wr:\n\tif (!--pipe->writers)\n\t\twake_up_interruptible(&pipe->wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr:\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn ret;\n}\n\nconst struct file_operations pipefifo_fops = {\n\t.open\t\t= fifo_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= pipe_read,\n\t.write_iter\t= pipe_write,\n\t.poll\t\t= pipe_poll,\n\t.unlocked_ioctl\t= pipe_ioctl,\n\t.release\t= pipe_release,\n\t.fasync\t\t= pipe_fasync,\n};\n\n/*\n * Currently we rely on the pipe array holding a power-of-2 number\n * of pages. Returns 0 on error.\n */\nunsigned int round_pipe_size(unsigned long size)\n{\n\tif (size > (1U << 31))\n\t\treturn 0;\n\n\t/* Minimum pipe size, as required by POSIX */\n\tif (size < PAGE_SIZE)\n\t\treturn PAGE_SIZE;\n\n\treturn roundup_pow_of_two(size);\n}\n\n/*\n * Allocate a new array of pipe buffers and copy the info over. Returns the\n * pipe size if successful, or return -ERROR on error.\n */\nstatic long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int size, nr_pages;\n\tunsigned long user_bufs;\n\tlong ret = 0;\n\n\tsize = round_pipe_size(arg);\n\tnr_pages = size >> PAGE_SHIFT;\n\n\tif (!nr_pages)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If trying to increase the pipe capacity, check that an\n\t * unprivileged user is not trying to exceed various limits\n\t * (soft limit check here, hard limit check just below).\n\t * Decreasing the pipe capacity is always permitted, even\n\t * if the user is currently over a limit.\n\t */\n\tif (nr_pages > pipe->buffers &&\n\t\t\tsize > pipe_max_size && !capable(CAP_SYS_RESOURCE))\n\t\treturn -EPERM;\n\n\tuser_bufs = account_pipe_buffers(pipe->user, pipe->buffers, nr_pages);\n\n\tif (nr_pages > pipe->buffers &&\n\t\t\t(too_many_pipe_buffers_hard(user_bufs) ||\n\t\t\t too_many_pipe_buffers_soft(user_bufs)) &&\n\t\t\tis_unprivileged_user()) {\n\t\tret = -EPERM;\n\t\tgoto out_revert_acct;\n\t}\n\n\t/*\n\t * We can shrink the pipe, if arg >= pipe->nrbufs. Since we don't\n\t * expect a lot of shrink+grow operations, just free and allocate\n\t * again like we would do for growing. If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tif (nr_pages < pipe->nrbufs) {\n\t\tret = -EBUSY;\n\t\tgoto out_revert_acct;\n\t}\n\n\tbufs = kcalloc(nr_pages, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_revert_acct;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indexes.\n\t */\n\tif (pipe->nrbufs) {\n\t\tunsigned int tail;\n\t\tunsigned int head;\n\n\t\ttail = pipe->curbuf + pipe->nrbufs;\n\t\tif (tail < pipe->buffers)\n\t\t\ttail = 0;\n\t\telse\n\t\t\ttail &= (pipe->buffers - 1);\n\n\t\thead = pipe->nrbufs - tail;\n\t\tif (head)\n\t\t\tmemcpy(bufs, pipe->bufs + pipe->curbuf, head * sizeof(struct pipe_buffer));\n\t\tif (tail)\n\t\t\tmemcpy(bufs + head, pipe->bufs, tail * sizeof(struct pipe_buffer));\n\t}\n\n\tpipe->curbuf = 0;\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->buffers = nr_pages;\n\treturn nr_pages * PAGE_SIZE;\n\nout_revert_acct:\n\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->buffers);\n\treturn ret;\n}\n\n/*\n * After the inode slimming patch, i_pipe/i_bdev/i_cdev share the same\n * location, so checking ->i_pipe is not enough to verify that this is a\n * pipe.\n */\nstruct pipe_inode_info *get_pipe_info(struct file *file)\n{\n\treturn file->f_op == &pipefifo_fops ? file->private_data : NULL;\n}\n\nlong pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->buffers * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}\n\nstatic const struct super_operations pipefs_ops = {\n\t.destroy_inode = free_inode_nonrcu,\n\t.statfs = simple_statfs,\n};\n\n/*\n * pipefs should _never_ be mounted by userland - too much of security hassle,\n * no real gain from having the whole whorehouse mounted. So we don't need\n * any operations on the root directory. However, we need a non-trivial\n * d_name - pipe: will go nicely and kill the special-casing in procfs.\n */\nstatic struct dentry *pipefs_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name, void *data)\n{\n\treturn mount_pseudo(fs_type, \"pipe:\", &pipefs_ops,\n\t\t\t&pipefs_dentry_operations, PIPEFS_MAGIC);\n}\n\nstatic struct file_system_type pipe_fs_type = {\n\t.name\t\t= \"pipefs\",\n\t.mount\t\t= pipefs_mount,\n\t.kill_sb\t= kill_anon_super,\n};\n\nstatic int __init init_pipe_fs(void)\n{\n\tint err = register_filesystem(&pipe_fs_type);\n\n\tif (!err) {\n\t\tpipe_mnt = kern_mount(&pipe_fs_type);\n\t\tif (IS_ERR(pipe_mnt)) {\n\t\t\terr = PTR_ERR(pipe_mnt);\n\t\t\tunregister_filesystem(&pipe_fs_type);\n\t\t}\n\t}\n\treturn err;\n}\n\nfs_initcall(init_pipe_fs);\n", "/*\n * \"splice\": joining two ropes together by interweaving their strands.\n *\n * This is the \"extended pipe\" functionality, where a pipe is used as\n * an arbitrary in-memory buffer. Think of a pipe as a small kernel\n * buffer that you can use to transfer data from one end to the other.\n *\n * The traditional unix read/write is extended with a \"splice()\" operation\n * that transfers data buffers to or from a pipe buffer.\n *\n * Named by Larry McVoy, original implementation from Linus, extended by\n * Jens to support splicing to files, network, direct splicing, etc and\n * fixing lots of bugs.\n *\n * Copyright (C) 2005-2006 Jens Axboe <axboe@kernel.dk>\n * Copyright (C) 2005-2006 Linus Torvalds <torvalds@osdl.org>\n * Copyright (C) 2006 Ingo Molnar <mingo@elte.hu>\n *\n */\n#include <linux/bvec.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/splice.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/export.h>\n#include <linux/syscalls.h>\n#include <linux/uio.h>\n#include <linux/security.h>\n#include <linux/gfp.h>\n#include <linux/socket.h>\n#include <linux/compat.h>\n#include <linux/sched/signal.h>\n\n#include \"internal.h\"\n\n/*\n * Attempt to steal a page from a pipe buffer. This should perhaps go into\n * a vm helper function, it's already simplified quite a bit by the\n * addition of remove_mapping(). If success is returned, the caller may\n * attempt to reuse this page for another destination.\n */\nstatic int page_cache_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t     struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\tstruct address_space *mapping;\n\n\tlock_page(page);\n\n\tmapping = page_mapping(page);\n\tif (mapping) {\n\t\tWARN_ON(!PageUptodate(page));\n\n\t\t/*\n\t\t * At least for ext2 with nobh option, we need to wait on\n\t\t * writeback completing on this page, since we'll remove it\n\t\t * from the pagecache.  Otherwise truncate wont wait on the\n\t\t * page, allowing the disk blocks to be reused by someone else\n\t\t * before we actually wrote our data to them. fs corruption\n\t\t * ensues.\n\t\t */\n\t\twait_on_page_writeback(page);\n\n\t\tif (page_has_private(page) &&\n\t\t    !try_to_release_page(page, GFP_KERNEL))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If we succeeded in removing the mapping, set LRU flag\n\t\t * and return good.\n\t\t */\n\t\tif (remove_mapping(mapping, page)) {\n\t\t\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Raced with truncate or failed to remove page from current\n\t * address space, unlock and return failure.\n\t */\nout_unlock:\n\tunlock_page(page);\n\treturn 1;\n}\n\nstatic void page_cache_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t\tstruct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n\tbuf->flags &= ~PIPE_BUF_FLAG_LRU;\n}\n\n/*\n * Check whether the contents of buf is OK to access. Since the content\n * is a page cache page, IO may be in flight.\n */\nstatic int page_cache_pipe_buf_confirm(struct pipe_inode_info *pipe,\n\t\t\t\t       struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\tint err;\n\n\tif (!PageUptodate(page)) {\n\t\tlock_page(page);\n\n\t\t/*\n\t\t * Page got truncated/unhashed. This will cause a 0-byte\n\t\t * splice, if this is the first page.\n\t\t */\n\t\tif (!page->mapping) {\n\t\t\terr = -ENODATA;\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * Uh oh, read-error from disk.\n\t\t */\n\t\tif (!PageUptodate(page)) {\n\t\t\terr = -EIO;\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * Page is ok afterall, we are done.\n\t\t */\n\t\tunlock_page(page);\n\t}\n\n\treturn 0;\nerror:\n\tunlock_page(page);\n\treturn err;\n}\n\nconst struct pipe_buf_operations page_cache_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = page_cache_pipe_buf_confirm,\n\t.release = page_cache_pipe_buf_release,\n\t.steal = page_cache_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic int user_page_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tif (!(buf->flags & PIPE_BUF_FLAG_GIFT))\n\t\treturn 1;\n\n\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\treturn generic_pipe_buf_steal(pipe, buf);\n}\n\nstatic const struct pipe_buf_operations user_page_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = page_cache_pipe_buf_release,\n\t.steal = user_page_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic void wakeup_pipe_readers(struct pipe_inode_info *pipe)\n{\n\tsmp_mb();\n\tif (waitqueue_active(&pipe->wait))\n\t\twake_up_interruptible(&pipe->wait);\n\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n}\n\n/**\n * splice_to_pipe - fill passed data into a pipe\n * @pipe:\tpipe to fill\n * @spd:\tdata to fill\n *\n * Description:\n *    @spd contains a map of pages and len/offset tuples, along with\n *    the struct pipe_buf_operations associated with these pages. This\n *    function will link that data to the pipe.\n *\n */\nssize_t splice_to_pipe(struct pipe_inode_info *pipe,\n\t\t       struct splice_pipe_desc *spd)\n{\n\tunsigned int spd_pages = spd->nr_pages;\n\tint ret = 0, page_nr = 0;\n\n\tif (!spd_pages)\n\t\treturn 0;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n\twhile (pipe->nrbufs < pipe->buffers) {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\n\t\tbuf->page = spd->pages[page_nr];\n\t\tbuf->offset = spd->partial[page_nr].offset;\n\t\tbuf->len = spd->partial[page_nr].len;\n\t\tbuf->private = spd->partial[page_nr].private;\n\t\tbuf->ops = spd->ops;\n\t\tbuf->flags = 0;\n\n\t\tpipe->nrbufs++;\n\t\tpage_nr++;\n\t\tret += buf->len;\n\n\t\tif (!--spd->nr_pages)\n\t\t\tbreak;\n\t}\n\n\tif (!ret)\n\t\tret = -EAGAIN;\n\nout:\n\twhile (page_nr < spd_pages)\n\t\tspd->spd_release(spd, page_nr++);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(splice_to_pipe);\n\nssize_t add_to_pipe(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\tint ret;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t} else if (pipe->nrbufs == pipe->buffers) {\n\t\tret = -EAGAIN;\n\t} else {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tpipe->bufs[newbuf] = *buf;\n\t\tpipe->nrbufs++;\n\t\treturn buf->len;\n\t}\n\tpipe_buf_release(pipe, buf);\n\treturn ret;\n}\nEXPORT_SYMBOL(add_to_pipe);\n\n/*\n * Check if we need to grow the arrays holding pages and partial page\n * descriptions.\n */\nint splice_grow_spd(const struct pipe_inode_info *pipe, struct splice_pipe_desc *spd)\n{\n\tunsigned int buffers = READ_ONCE(pipe->buffers);\n\n\tspd->nr_pages_max = buffers;\n\tif (buffers <= PIPE_DEF_BUFFERS)\n\t\treturn 0;\n\n\tspd->pages = kmalloc_array(buffers, sizeof(struct page *), GFP_KERNEL);\n\tspd->partial = kmalloc_array(buffers, sizeof(struct partial_page),\n\t\t\t\t     GFP_KERNEL);\n\n\tif (spd->pages && spd->partial)\n\t\treturn 0;\n\n\tkfree(spd->pages);\n\tkfree(spd->partial);\n\treturn -ENOMEM;\n}\n\nvoid splice_shrink_spd(struct splice_pipe_desc *spd)\n{\n\tif (spd->nr_pages_max <= PIPE_DEF_BUFFERS)\n\t\treturn;\n\n\tkfree(spd->pages);\n\tkfree(spd->partial);\n}\n\n/**\n * generic_file_splice_read - splice data from file to a pipe\n * @in:\t\tfile to splice from\n * @ppos:\tposition in @in\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given file and fill them into a pipe. Can be\n *    used as long as it has more or less sane ->read_iter().\n *\n */\nssize_t generic_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t\t unsigned int flags)\n{\n\tstruct iov_iter to;\n\tstruct kiocb kiocb;\n\tint idx, ret;\n\n\tiov_iter_pipe(&to, READ, pipe, len);\n\tidx = to.idx;\n\tinit_sync_kiocb(&kiocb, in);\n\tkiocb.ki_pos = *ppos;\n\tret = call_read_iter(in, &kiocb, &to);\n\tif (ret > 0) {\n\t\t*ppos = kiocb.ki_pos;\n\t\tfile_accessed(in);\n\t} else if (ret < 0) {\n\t\tto.idx = idx;\n\t\tto.iov_offset = 0;\n\t\tiov_iter_advance(&to, 0); /* to free what was emitted */\n\t\t/*\n\t\t * callers of ->splice_read() expect -EAGAIN on\n\t\t * \"can't put anything in there\", rather than -EFAULT.\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tret = -EAGAIN;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_file_splice_read);\n\nconst struct pipe_buf_operations default_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = generic_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic int generic_pipe_buf_nosteal(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\treturn 1;\n}\n\n/* Pipe buffer operations for a socket and similar. */\nconst struct pipe_buf_operations nosteal_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = generic_pipe_buf_nosteal,\n\t.get = generic_pipe_buf_get,\n};\nEXPORT_SYMBOL(nosteal_pipe_buf_ops);\n\nstatic ssize_t kernel_readv(struct file *file, const struct kvec *vec,\n\t\t\t    unsigned long vlen, loff_t offset)\n{\n\tmm_segment_t old_fs;\n\tloff_t pos = offset;\n\tssize_t res;\n\n\told_fs = get_fs();\n\tset_fs(get_ds());\n\t/* The cast to a user pointer is valid due to the set_fs() */\n\tres = vfs_readv(file, (const struct iovec __user *)vec, vlen, &pos, 0);\n\tset_fs(old_fs);\n\n\treturn res;\n}\n\nstatic ssize_t default_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t\t unsigned int flags)\n{\n\tstruct kvec *vec, __vec[PIPE_DEF_BUFFERS];\n\tstruct iov_iter to;\n\tstruct page **pages;\n\tunsigned int nr_pages;\n\tsize_t offset, base, copied = 0;\n\tssize_t res;\n\tint i;\n\n\tif (pipe->nrbufs == pipe->buffers)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * Try to keep page boundaries matching to source pagecache ones -\n\t * it probably won't be much help, but...\n\t */\n\toffset = *ppos & ~PAGE_MASK;\n\n\tiov_iter_pipe(&to, READ, pipe, len + offset);\n\n\tres = iov_iter_get_pages_alloc(&to, &pages, len + offset, &base);\n\tif (res <= 0)\n\t\treturn -ENOMEM;\n\n\tnr_pages = DIV_ROUND_UP(res + base, PAGE_SIZE);\n\n\tvec = __vec;\n\tif (nr_pages > PIPE_DEF_BUFFERS) {\n\t\tvec = kmalloc_array(nr_pages, sizeof(struct kvec), GFP_KERNEL);\n\t\tif (unlikely(!vec)) {\n\t\t\tres = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpipe->bufs[to.idx].offset = offset;\n\tpipe->bufs[to.idx].len -= offset;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t this_len = min_t(size_t, len, PAGE_SIZE - offset);\n\t\tvec[i].iov_base = page_address(pages[i]) + offset;\n\t\tvec[i].iov_len = this_len;\n\t\tlen -= this_len;\n\t\toffset = 0;\n\t}\n\n\tres = kernel_readv(in, vec, nr_pages, *ppos);\n\tif (res > 0) {\n\t\tcopied = res;\n\t\t*ppos += res;\n\t}\n\n\tif (vec != __vec)\n\t\tkfree(vec);\nout:\n\tfor (i = 0; i < nr_pages; i++)\n\t\tput_page(pages[i]);\n\tkvfree(pages);\n\tiov_iter_advance(&to, copied);\t/* truncates and discards */\n\treturn res;\n}\n\n/*\n * Send 'sd->len' bytes to socket from 'sd->file' at position 'sd->pos'\n * using sendpage(). Return the number of bytes sent.\n */\nstatic int pipe_to_sendpage(struct pipe_inode_info *pipe,\n\t\t\t    struct pipe_buffer *buf, struct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tloff_t pos = sd->pos;\n\tint more;\n\n\tif (!likely(file->f_op->sendpage))\n\t\treturn -EINVAL;\n\n\tmore = (sd->flags & SPLICE_F_MORE) ? MSG_MORE : 0;\n\n\tif (sd->len < sd->total_len && pipe->nrbufs > 1)\n\t\tmore |= MSG_SENDPAGE_NOTLAST;\n\n\treturn file->f_op->sendpage(file, buf->page, buf->offset,\n\t\t\t\t    sd->len, &pos, more);\n}\n\nstatic void wakeup_pipe_writers(struct pipe_inode_info *pipe)\n{\n\tsmp_mb();\n\tif (waitqueue_active(&pipe->wait))\n\t\twake_up_interruptible(&pipe->wait);\n\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n}\n\n/**\n * splice_from_pipe_feed - feed available data from a pipe to a file\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation to @actor\n * @actor:\thandler that splices the data\n *\n * Description:\n *    This function loops over the pipe and calls @actor to do the\n *    actual moving of a single struct pipe_buffer to the desired\n *    destination.  It returns when there's no more buffers left in\n *    the pipe or if the requested number of bytes (@sd->total_len)\n *    have been copied.  It returns a positive number (one) if the\n *    pipe needs to be filled with more data, zero if the required\n *    number of bytes have been copied and -errno on error.\n *\n *    This, together with splice_from_pipe_{begin,end,next}, may be\n *    used to implement the functionality of __splice_from_pipe() when\n *    locking is required around copying the pipe buffers to the\n *    destination.\n */\nstatic int splice_from_pipe_feed(struct pipe_inode_info *pipe, struct splice_desc *sd,\n\t\t\t  splice_actor *actor)\n{\n\tint ret;\n\n\twhile (pipe->nrbufs) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + pipe->curbuf;\n\n\t\tsd->len = buf->len;\n\t\tif (sd->len > sd->total_len)\n\t\t\tsd->len = sd->total_len;\n\n\t\tret = pipe_buf_confirm(pipe, buf);\n\t\tif (unlikely(ret)) {\n\t\t\tif (ret == -ENODATA)\n\t\t\t\tret = 0;\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = actor(pipe, buf, sd);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tbuf->offset += ret;\n\t\tbuf->len -= ret;\n\n\t\tsd->num_spliced += ret;\n\t\tsd->len -= ret;\n\t\tsd->pos += ret;\n\t\tsd->total_len -= ret;\n\n\t\tif (!buf->len) {\n\t\t\tpipe_buf_release(pipe, buf);\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t\tif (pipe->files)\n\t\t\t\tsd->need_wakeup = true;\n\t\t}\n\n\t\tif (!sd->total_len)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/**\n * splice_from_pipe_next - wait for some data to splice from\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function will wait for some data and return a positive\n *    value (one) if pipe buffers are available.  It will return zero\n *    or -errno if no more data needs to be spliced.\n */\nstatic int splice_from_pipe_next(struct pipe_inode_info *pipe, struct splice_desc *sd)\n{\n\t/*\n\t * Check for signal early to make process killable when there are\n\t * always buffers available\n\t */\n\tif (signal_pending(current))\n\t\treturn -ERESTARTSYS;\n\n\twhile (!pipe->nrbufs) {\n\t\tif (!pipe->writers)\n\t\t\treturn 0;\n\n\t\tif (!pipe->waiting_writers && sd->num_spliced)\n\t\t\treturn 0;\n\n\t\tif (sd->flags & SPLICE_F_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\n\t\tif (sd->need_wakeup) {\n\t\t\twakeup_pipe_writers(pipe);\n\t\t\tsd->need_wakeup = false;\n\t\t}\n\n\t\tpipe_wait(pipe);\n\t}\n\n\treturn 1;\n}\n\n/**\n * splice_from_pipe_begin - start splicing from pipe\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function should be called before a loop containing\n *    splice_from_pipe_next() and splice_from_pipe_feed() to\n *    initialize the necessary fields of @sd.\n */\nstatic void splice_from_pipe_begin(struct splice_desc *sd)\n{\n\tsd->num_spliced = 0;\n\tsd->need_wakeup = false;\n}\n\n/**\n * splice_from_pipe_end - finish splicing from pipe\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function will wake up pipe writers if necessary.  It should\n *    be called after a loop containing splice_from_pipe_next() and\n *    splice_from_pipe_feed().\n */\nstatic void splice_from_pipe_end(struct pipe_inode_info *pipe, struct splice_desc *sd)\n{\n\tif (sd->need_wakeup)\n\t\twakeup_pipe_writers(pipe);\n}\n\n/**\n * __splice_from_pipe - splice data from a pipe to given actor\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation to @actor\n * @actor:\thandler that splices the data\n *\n * Description:\n *    This function does little more than loop over the pipe and call\n *    @actor to do the actual moving of a single struct pipe_buffer to\n *    the desired destination. See pipe_to_file, pipe_to_sendpage, or\n *    pipe_to_user.\n *\n */\nssize_t __splice_from_pipe(struct pipe_inode_info *pipe, struct splice_desc *sd,\n\t\t\t   splice_actor *actor)\n{\n\tint ret;\n\n\tsplice_from_pipe_begin(sd);\n\tdo {\n\t\tcond_resched();\n\t\tret = splice_from_pipe_next(pipe, sd);\n\t\tif (ret > 0)\n\t\t\tret = splice_from_pipe_feed(pipe, sd, actor);\n\t} while (ret > 0);\n\tsplice_from_pipe_end(pipe, sd);\n\n\treturn sd->num_spliced ? sd->num_spliced : ret;\n}\nEXPORT_SYMBOL(__splice_from_pipe);\n\n/**\n * splice_from_pipe - splice data from a pipe to a file\n * @pipe:\tpipe to splice from\n * @out:\tfile to splice to\n * @ppos:\tposition in @out\n * @len:\thow many bytes to splice\n * @flags:\tsplice modifier flags\n * @actor:\thandler that splices the data\n *\n * Description:\n *    See __splice_from_pipe. This function locks the pipe inode,\n *    otherwise it's identical to __splice_from_pipe().\n *\n */\nssize_t splice_from_pipe(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t loff_t *ppos, size_t len, unsigned int flags,\n\t\t\t splice_actor *actor)\n{\n\tssize_t ret;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\n\tpipe_lock(pipe);\n\tret = __splice_from_pipe(pipe, &sd, actor);\n\tpipe_unlock(pipe);\n\n\treturn ret;\n}\n\n/**\n * iter_file_splice_write - splice data from a pipe to a file\n * @pipe:\tpipe info\n * @out:\tfile to write to\n * @ppos:\tposition in @out\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will either move or copy pages (determined by @flags options) from\n *    the given pipe inode to the given file.\n *    This one is ->write_iter-based.\n *\n */\nssize_t\niter_file_splice_write(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t  loff_t *ppos, size_t len, unsigned int flags)\n{\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\tint nbufs = pipe->buffers;\n\tstruct bio_vec *array = kcalloc(nbufs, sizeof(struct bio_vec),\n\t\t\t\t\tGFP_KERNEL);\n\tssize_t ret;\n\n\tif (unlikely(!array))\n\t\treturn -ENOMEM;\n\n\tpipe_lock(pipe);\n\n\tsplice_from_pipe_begin(&sd);\n\twhile (sd.total_len) {\n\t\tstruct iov_iter from;\n\t\tsize_t left;\n\t\tint n, idx;\n\n\t\tret = splice_from_pipe_next(pipe, &sd);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\n\t\tif (unlikely(nbufs < pipe->buffers)) {\n\t\t\tkfree(array);\n\t\t\tnbufs = pipe->buffers;\n\t\t\tarray = kcalloc(nbufs, sizeof(struct bio_vec),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!array) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* build the vector */\n\t\tleft = sd.total_len;\n\t\tfor (n = 0, idx = pipe->curbuf; left && n < pipe->nrbufs; n++, idx++) {\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + idx;\n\t\t\tsize_t this_len = buf->len;\n\n\t\t\tif (this_len > left)\n\t\t\t\tthis_len = left;\n\n\t\t\tif (idx == pipe->buffers - 1)\n\t\t\t\tidx = -1;\n\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tif (ret == -ENODATA)\n\t\t\t\t\tret = 0;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tarray[n].bv_page = buf->page;\n\t\t\tarray[n].bv_len = this_len;\n\t\t\tarray[n].bv_offset = buf->offset;\n\t\t\tleft -= this_len;\n\t\t}\n\n\t\tiov_iter_bvec(&from, WRITE, array, n, sd.total_len - left);\n\t\tret = vfs_iter_write(out, &from, &sd.pos, 0);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\n\t\tsd.num_spliced += ret;\n\t\tsd.total_len -= ret;\n\t\t*ppos = sd.pos;\n\n\t\t/* dismiss the fully eaten buffers, adjust the partial one */\n\t\twhile (ret) {\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + pipe->curbuf;\n\t\t\tif (ret >= buf->len) {\n\t\t\t\tret -= buf->len;\n\t\t\t\tbuf->len = 0;\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\t\tpipe->nrbufs--;\n\t\t\t\tif (pipe->files)\n\t\t\t\t\tsd.need_wakeup = true;\n\t\t\t} else {\n\t\t\t\tbuf->offset += ret;\n\t\t\t\tbuf->len -= ret;\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\t}\ndone:\n\tkfree(array);\n\tsplice_from_pipe_end(pipe, &sd);\n\n\tpipe_unlock(pipe);\n\n\tif (sd.num_spliced)\n\t\tret = sd.num_spliced;\n\n\treturn ret;\n}\n\nEXPORT_SYMBOL(iter_file_splice_write);\n\nstatic int write_pipe_buf(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\t  struct splice_desc *sd)\n{\n\tint ret;\n\tvoid *data;\n\tloff_t tmp = sd->pos;\n\n\tdata = kmap(buf->page);\n\tret = __kernel_write(sd->u.file, data + buf->offset, sd->len, &tmp);\n\tkunmap(buf->page);\n\n\treturn ret;\n}\n\nstatic ssize_t default_file_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t\t struct file *out, loff_t *ppos,\n\t\t\t\t\t size_t len, unsigned int flags)\n{\n\tssize_t ret;\n\n\tret = splice_from_pipe(pipe, out, ppos, len, flags, write_pipe_buf);\n\tif (ret > 0)\n\t\t*ppos += ret;\n\n\treturn ret;\n}\n\n/**\n * generic_splice_sendpage - splice data from a pipe to a socket\n * @pipe:\tpipe to splice from\n * @out:\tsocket to write to\n * @ppos:\tposition in @out\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will send @len bytes from the pipe to a network socket. No data copying\n *    is involved.\n *\n */\nssize_t generic_splice_sendpage(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t\tloff_t *ppos, size_t len, unsigned int flags)\n{\n\treturn splice_from_pipe(pipe, out, ppos, len, flags, pipe_to_sendpage);\n}\n\nEXPORT_SYMBOL(generic_splice_sendpage);\n\n/*\n * Attempt to initiate a splice from pipe to file.\n */\nstatic long do_splice_from(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t   loff_t *ppos, size_t len, unsigned int flags)\n{\n\tssize_t (*splice_write)(struct pipe_inode_info *, struct file *,\n\t\t\t\tloff_t *, size_t, unsigned int);\n\n\tif (out->f_op->splice_write)\n\t\tsplice_write = out->f_op->splice_write;\n\telse\n\t\tsplice_write = default_file_splice_write;\n\n\treturn splice_write(pipe, out, ppos, len, flags);\n}\n\n/*\n * Attempt to initiate a splice from a file to a pipe.\n */\nstatic long do_splice_to(struct file *in, loff_t *ppos,\n\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t unsigned int flags)\n{\n\tssize_t (*splice_read)(struct file *, loff_t *,\n\t\t\t       struct pipe_inode_info *, size_t, unsigned int);\n\tint ret;\n\n\tif (unlikely(!(in->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\tret = rw_verify_area(READ, in, ppos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (unlikely(len > MAX_RW_COUNT))\n\t\tlen = MAX_RW_COUNT;\n\n\tif (in->f_op->splice_read)\n\t\tsplice_read = in->f_op->splice_read;\n\telse\n\t\tsplice_read = default_file_splice_read;\n\n\treturn splice_read(in, ppos, pipe, len, flags);\n}\n\n/**\n * splice_direct_to_actor - splices data directly between two non-pipes\n * @in:\t\tfile to splice from\n * @sd:\t\tactor information on where to splice to\n * @actor:\thandles the data splicing\n *\n * Description:\n *    This is a special case helper to splice directly between two\n *    points, without requiring an explicit pipe. Internally an allocated\n *    pipe is cached in the process, and reused during the lifetime of\n *    that process.\n *\n */\nssize_t splice_direct_to_actor(struct file *in, struct splice_desc *sd,\n\t\t\t       splice_direct_actor *actor)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret, bytes;\n\tumode_t i_mode;\n\tsize_t len;\n\tint i, flags, more;\n\n\t/*\n\t * We require the input being a regular file, as we don't want to\n\t * randomly drop data for eg socket -> socket splicing. Use the\n\t * piped splicing for that!\n\t */\n\ti_mode = file_inode(in)->i_mode;\n\tif (unlikely(!S_ISREG(i_mode) && !S_ISBLK(i_mode)))\n\t\treturn -EINVAL;\n\n\t/*\n\t * neither in nor out is a pipe, setup an internal pipe attached to\n\t * 'out' and transfer the wanted data from 'in' to 'out' through that\n\t */\n\tpipe = current->splice_pipe;\n\tif (unlikely(!pipe)) {\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\n\t\t/*\n\t\t * We don't have an immediate reader, but we'll read the stuff\n\t\t * out of the pipe right after the splice_to_pipe(). So set\n\t\t * PIPE_READERS appropriately.\n\t\t */\n\t\tpipe->readers = 1;\n\n\t\tcurrent->splice_pipe = pipe;\n\t}\n\n\t/*\n\t * Do the splice.\n\t */\n\tret = 0;\n\tbytes = 0;\n\tlen = sd->total_len;\n\tflags = sd->flags;\n\n\t/*\n\t * Don't block on output, we have to drain the direct pipe.\n\t */\n\tsd->flags &= ~SPLICE_F_NONBLOCK;\n\tmore = sd->flags & SPLICE_F_MORE;\n\n\tWARN_ON_ONCE(pipe->nrbufs != 0);\n\n\twhile (len) {\n\t\tsize_t read_len;\n\t\tloff_t pos = sd->pos, prev_pos = pos;\n\n\t\t/* Don't try to read more the pipe has space for. */\n\t\tread_len = min_t(size_t, len,\n\t\t\t\t (pipe->buffers - pipe->nrbufs) << PAGE_SHIFT);\n\t\tret = do_splice_to(in, &pos, pipe, read_len, flags);\n\t\tif (unlikely(ret <= 0))\n\t\t\tgoto out_release;\n\n\t\tread_len = ret;\n\t\tsd->total_len = read_len;\n\n\t\t/*\n\t\t * If more data is pending, set SPLICE_F_MORE\n\t\t * If this is the last data and SPLICE_F_MORE was not set\n\t\t * initially, clears it.\n\t\t */\n\t\tif (read_len < len)\n\t\t\tsd->flags |= SPLICE_F_MORE;\n\t\telse if (!more)\n\t\t\tsd->flags &= ~SPLICE_F_MORE;\n\t\t/*\n\t\t * NOTE: nonblocking mode only applies to the input. We\n\t\t * must not do the output in nonblocking mode as then we\n\t\t * could get stuck data in the internal pipe:\n\t\t */\n\t\tret = actor(pipe, sd);\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tsd->pos = prev_pos;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\tbytes += ret;\n\t\tlen -= ret;\n\t\tsd->pos = pos;\n\n\t\tif (ret < read_len) {\n\t\t\tsd->pos = prev_pos + ret;\n\t\t\tgoto out_release;\n\t\t}\n\t}\n\ndone:\n\tpipe->nrbufs = pipe->curbuf = 0;\n\tfile_accessed(in);\n\treturn bytes;\n\nout_release:\n\t/*\n\t * If we did an incomplete transfer we must release\n\t * the pipe buffers in question:\n\t */\n\tfor (i = 0; i < pipe->buffers; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n\n\tif (!bytes)\n\t\tbytes = ret;\n\n\tgoto done;\n}\nEXPORT_SYMBOL(splice_direct_to_actor);\n\nstatic int direct_splice_actor(struct pipe_inode_info *pipe,\n\t\t\t       struct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\n\treturn do_splice_from(pipe, file, sd->opos, sd->total_len,\n\t\t\t      sd->flags);\n}\n\n/**\n * do_splice_direct - splices data directly between two files\n * @in:\t\tfile to splice from\n * @ppos:\tinput file offset\n * @out:\tfile to splice to\n * @opos:\toutput file offset\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    For use by do_sendfile(). splice can easily emulate sendfile, but\n *    doing it in the application would incur an extra system call\n *    (splice in + splice out, as compared to just sendfile()). So this helper\n *    can splice directly through a process-private pipe.\n *\n */\nlong do_splice_direct(struct file *in, loff_t *ppos, struct file *out,\n\t\t      loff_t *opos, size_t len, unsigned int flags)\n{\n\tstruct splice_desc sd = {\n\t\t.len\t\t= len,\n\t\t.total_len\t= len,\n\t\t.flags\t\t= flags,\n\t\t.pos\t\t= *ppos,\n\t\t.u.file\t\t= out,\n\t\t.opos\t\t= opos,\n\t};\n\tlong ret;\n\n\tif (unlikely(!(out->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\n\tif (unlikely(out->f_flags & O_APPEND))\n\t\treturn -EINVAL;\n\n\tret = rw_verify_area(WRITE, out, opos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = splice_direct_to_actor(in, &sd, direct_splice_actor);\n\tif (ret > 0)\n\t\t*ppos = sd.pos;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(do_splice_direct);\n\nstatic int wait_for_space(struct pipe_inode_info *pipe, unsigned flags)\n{\n\tfor (;;) {\n\t\tif (unlikely(!pipe->readers)) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\treturn -EPIPE;\n\t\t}\n\t\tif (pipe->nrbufs != pipe->buffers)\n\t\t\treturn 0;\n\t\tif (flags & SPLICE_F_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\t\tif (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\n}\n\nstatic int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags);\n\n/*\n * Determine where to splice to/from.\n */\nstatic long do_splice(struct file *in, loff_t __user *off_in,\n\t\t      struct file *out, loff_t __user *off_out,\n\t\t      size_t len, unsigned int flags)\n{\n\tstruct pipe_inode_info *ipipe;\n\tstruct pipe_inode_info *opipe;\n\tloff_t offset;\n\tlong ret;\n\n\tipipe = get_pipe_info(in);\n\topipe = get_pipe_info(out);\n\n\tif (ipipe && opipe) {\n\t\tif (off_in || off_out)\n\t\t\treturn -ESPIPE;\n\n\t\tif (!(in->f_mode & FMODE_READ))\n\t\t\treturn -EBADF;\n\n\t\tif (!(out->f_mode & FMODE_WRITE))\n\t\t\treturn -EBADF;\n\n\t\t/* Splicing to self would be fun, but... */\n\t\tif (ipipe == opipe)\n\t\t\treturn -EINVAL;\n\n\t\treturn splice_pipe_to_pipe(ipipe, opipe, len, flags);\n\t}\n\n\tif (ipipe) {\n\t\tif (off_in)\n\t\t\treturn -ESPIPE;\n\t\tif (off_out) {\n\t\t\tif (!(out->f_mode & FMODE_PWRITE))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (copy_from_user(&offset, off_out, sizeof(loff_t)))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\toffset = out->f_pos;\n\t\t}\n\n\t\tif (unlikely(!(out->f_mode & FMODE_WRITE)))\n\t\t\treturn -EBADF;\n\n\t\tif (unlikely(out->f_flags & O_APPEND))\n\t\t\treturn -EINVAL;\n\n\t\tret = rw_verify_area(WRITE, out, &offset, len);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\n\t\tfile_start_write(out);\n\t\tret = do_splice_from(ipipe, out, &offset, len, flags);\n\t\tfile_end_write(out);\n\n\t\tif (!off_out)\n\t\t\tout->f_pos = offset;\n\t\telse if (copy_to_user(off_out, &offset, sizeof(loff_t)))\n\t\t\tret = -EFAULT;\n\n\t\treturn ret;\n\t}\n\n\tif (opipe) {\n\t\tif (off_out)\n\t\t\treturn -ESPIPE;\n\t\tif (off_in) {\n\t\t\tif (!(in->f_mode & FMODE_PREAD))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (copy_from_user(&offset, off_in, sizeof(loff_t)))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\toffset = in->f_pos;\n\t\t}\n\n\t\tpipe_lock(opipe);\n\t\tret = wait_for_space(opipe, flags);\n\t\tif (!ret)\n\t\t\tret = do_splice_to(in, &offset, opipe, len, flags);\n\t\tpipe_unlock(opipe);\n\t\tif (ret > 0)\n\t\t\twakeup_pipe_readers(opipe);\n\t\tif (!off_in)\n\t\t\tin->f_pos = offset;\n\t\telse if (copy_to_user(off_in, &offset, sizeof(loff_t)))\n\t\t\tret = -EFAULT;\n\n\t\treturn ret;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int iter_to_pipe(struct iov_iter *from,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tunsigned flags)\n{\n\tstruct pipe_buffer buf = {\n\t\t.ops = &user_page_pipe_buf_ops,\n\t\t.flags = flags\n\t};\n\tsize_t total = 0;\n\tint ret = 0;\n\tbool failed = false;\n\n\twhile (iov_iter_count(from) && !failed) {\n\t\tstruct page *pages[16];\n\t\tssize_t copied;\n\t\tsize_t start;\n\t\tint n;\n\n\t\tcopied = iov_iter_get_pages(from, pages, ~0UL, 16, &start);\n\t\tif (copied <= 0) {\n\t\t\tret = copied;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (n = 0; copied; n++, start = 0) {\n\t\t\tint size = min_t(int, copied, PAGE_SIZE - start);\n\t\t\tif (!failed) {\n\t\t\t\tbuf.page = pages[n];\n\t\t\t\tbuf.offset = start;\n\t\t\t\tbuf.len = size;\n\t\t\t\tret = add_to_pipe(pipe, &buf);\n\t\t\t\tif (unlikely(ret < 0)) {\n\t\t\t\t\tfailed = true;\n\t\t\t\t} else {\n\t\t\t\t\tiov_iter_advance(from, ret);\n\t\t\t\t\ttotal += ret;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tput_page(pages[n]);\n\t\t\t}\n\t\t\tcopied -= size;\n\t\t}\n\t}\n\treturn total ? total : ret;\n}\n\nstatic int pipe_to_user(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tint n = copy_page_to_iter(buf->page, buf->offset, sd->len, sd->u.data);\n\treturn n == sd->len ? n : -EFAULT;\n}\n\n/*\n * For lack of a better implementation, implement vmsplice() to userspace\n * as a simple copy of the pipes pages to the user iov.\n */\nstatic long vmsplice_to_user(struct file *file, struct iov_iter *iter,\n\t\t\t     unsigned int flags)\n{\n\tstruct pipe_inode_info *pipe = get_pipe_info(file);\n\tstruct splice_desc sd = {\n\t\t.total_len = iov_iter_count(iter),\n\t\t.flags = flags,\n\t\t.u.data = iter\n\t};\n\tlong ret = 0;\n\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\tif (sd.total_len) {\n\t\tpipe_lock(pipe);\n\t\tret = __splice_from_pipe(pipe, &sd, pipe_to_user);\n\t\tpipe_unlock(pipe);\n\t}\n\n\treturn ret;\n}\n\n/*\n * vmsplice splices a user address range into a pipe. It can be thought of\n * as splice-from-memory, where the regular splice is splice-from-file (or\n * to file). In both cases the output is a pipe, naturally.\n */\nstatic long vmsplice_to_pipe(struct file *file, struct iov_iter *iter,\n\t\t\t     unsigned int flags)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret = 0;\n\tunsigned buf_flag = 0;\n\n\tif (flags & SPLICE_F_GIFT)\n\t\tbuf_flag = PIPE_BUF_FLAG_GIFT;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\tpipe_lock(pipe);\n\tret = wait_for_space(pipe, flags);\n\tif (!ret)\n\t\tret = iter_to_pipe(iter, pipe, buf_flag);\n\tpipe_unlock(pipe);\n\tif (ret > 0)\n\t\twakeup_pipe_readers(pipe);\n\treturn ret;\n}\n\nstatic int vmsplice_type(struct fd f, int *type)\n{\n\tif (!f.file)\n\t\treturn -EBADF;\n\tif (f.file->f_mode & FMODE_WRITE) {\n\t\t*type = WRITE;\n\t} else if (f.file->f_mode & FMODE_READ) {\n\t\t*type = READ;\n\t} else {\n\t\tfdput(f);\n\t\treturn -EBADF;\n\t}\n\treturn 0;\n}\n\n/*\n * Note that vmsplice only really supports true splicing _from_ user memory\n * to a pipe, not the other way around. Splicing from user memory is a simple\n * operation that can be supported without any funky alignment restrictions\n * or nasty vm tricks. We simply map in the user memory and fill them into\n * a pipe. The reverse isn't quite as easy, though. There are two possible\n * solutions for that:\n *\n *\t- memcpy() the data internally, at which point we might as well just\n *\t  do a regular read() on the buffer anyway.\n *\t- Lots of nasty vm tricks, that are neither fast nor flexible (it\n *\t  has restriction limitations on both ends of the pipe).\n *\n * Currently we punt and implement it as a normal copy, see pipe_to_user().\n *\n */\nstatic long do_vmsplice(struct file *f, struct iov_iter *iter, unsigned int flags)\n{\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (!iov_iter_count(iter))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == WRITE)\n\t\treturn vmsplice_to_pipe(f, iter, flags);\n\telse\n\t\treturn vmsplice_to_user(f, iter, flags);\n}\n\nSYSCALL_DEFINE4(vmsplice, int, fd, const struct iovec __user *, uiov,\n\t\tunsigned long, nr_segs, unsigned int, flags)\n{\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tstruct iov_iter iter;\n\tlong error;\n\tstruct fd f;\n\tint type;\n\n\tf = fdget(fd);\n\terror = vmsplice_type(f, &type);\n\tif (error)\n\t\treturn error;\n\n\terror = import_iovec(type, uiov, nr_segs,\n\t\t\t     ARRAY_SIZE(iovstack), &iov, &iter);\n\tif (!error) {\n\t\terror = do_vmsplice(f.file, &iter, flags);\n\t\tkfree(iov);\n\t}\n\tfdput(f);\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(vmsplice, int, fd, const struct compat_iovec __user *, iov32,\n\t\t    unsigned int, nr_segs, unsigned int, flags)\n{\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tstruct iov_iter iter;\n\tlong error;\n\tstruct fd f;\n\tint type;\n\n\tf = fdget(fd);\n\terror = vmsplice_type(f, &type);\n\tif (error)\n\t\treturn error;\n\n\terror = compat_import_iovec(type, iov32, nr_segs,\n\t\t\t     ARRAY_SIZE(iovstack), &iov, &iter);\n\tif (!error) {\n\t\terror = do_vmsplice(f.file, &iter, flags);\n\t\tkfree(iov);\n\t}\n\tfdput(f);\n\treturn error;\n}\n#endif\n\nSYSCALL_DEFINE6(splice, int, fd_in, loff_t __user *, off_in,\n\t\tint, fd_out, loff_t __user *, off_out,\n\t\tsize_t, len, unsigned int, flags)\n{\n\tstruct fd in, out;\n\tlong error;\n\n\tif (unlikely(!len))\n\t\treturn 0;\n\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\terror = -EBADF;\n\tin = fdget(fd_in);\n\tif (in.file) {\n\t\tif (in.file->f_mode & FMODE_READ) {\n\t\t\tout = fdget(fd_out);\n\t\t\tif (out.file) {\n\t\t\t\tif (out.file->f_mode & FMODE_WRITE)\n\t\t\t\t\terror = do_splice(in.file, off_in,\n\t\t\t\t\t\t\t  out.file, off_out,\n\t\t\t\t\t\t\t  len, flags);\n\t\t\t\tfdput(out);\n\t\t\t}\n\t\t}\n\t\tfdput(in);\n\t}\n\treturn error;\n}\n\n/*\n * Make sure there's data to read. Wait for input if we can, otherwise\n * return an appropriate error.\n */\nstatic int ipipe_prep(struct pipe_inode_info *pipe, unsigned int flags)\n{\n\tint ret;\n\n\t/*\n\t * Check ->nrbufs without the inode lock first. This function\n\t * is speculative anyways, so missing one is ok.\n\t */\n\tif (pipe->nrbufs)\n\t\treturn 0;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\twhile (!pipe->nrbufs) {\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (!pipe->waiting_writers) {\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpipe_wait(pipe);\n\t}\n\n\tpipe_unlock(pipe);\n\treturn ret;\n}\n\n/*\n * Make sure there's writeable room. Wait for room if we can, otherwise\n * return an appropriate error.\n */\nstatic int opipe_prep(struct pipe_inode_info *pipe, unsigned int flags)\n{\n\tint ret;\n\n\t/*\n\t * Check ->nrbufs without the inode lock first. This function\n\t * is speculative anyways, so missing one is ok.\n\t */\n\tif (pipe->nrbufs < pipe->buffers)\n\t\treturn 0;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\twhile (pipe->nrbufs >= pipe->buffers) {\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\n\n\tpipe_unlock(pipe);\n\treturn ret;\n}\n\n/*\n * Splice contents of ipipe to opipe.\n */\nstatic int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tpipe_buf_get(ipipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}\n\n/*\n * Link contents of ipipe to opipe.\n */\nstatic int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tpipe_buf_get(ipipe, ibuf);\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}\n\n/*\n * This is a tee(1) implementation that works on pipes. It doesn't copy\n * any data, it simply references the 'in' pages on the 'out' pipe.\n * The 'flags' used are the SPLICE_F_* variants, currently the only\n * applicable one is SPLICE_F_NONBLOCK.\n */\nstatic long do_tee(struct file *in, struct file *out, size_t len,\n\t\t   unsigned int flags)\n{\n\tstruct pipe_inode_info *ipipe = get_pipe_info(in);\n\tstruct pipe_inode_info *opipe = get_pipe_info(out);\n\tint ret = -EINVAL;\n\n\t/*\n\t * Duplicate the contents of ipipe to opipe without actually\n\t * copying the data.\n\t */\n\tif (ipipe && opipe && ipipe != opipe) {\n\t\t/*\n\t\t * Keep going, unless we encounter an error. The ipipe/opipe\n\t\t * ordering doesn't really matter.\n\t\t */\n\t\tret = ipipe_prep(ipipe, flags);\n\t\tif (!ret) {\n\t\t\tret = opipe_prep(opipe, flags);\n\t\t\tif (!ret)\n\t\t\t\tret = link_pipe(ipipe, opipe, len, flags);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(tee, int, fdin, int, fdout, size_t, len, unsigned int, flags)\n{\n\tstruct fd in;\n\tint error;\n\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!len))\n\t\treturn 0;\n\n\terror = -EBADF;\n\tin = fdget(fdin);\n\tif (in.file) {\n\t\tif (in.file->f_mode & FMODE_READ) {\n\t\t\tstruct fd out = fdget(fdout);\n\t\t\tif (out.file) {\n\t\t\t\tif (out.file->f_mode & FMODE_WRITE)\n\t\t\t\t\terror = do_tee(in.file, out.file,\n\t\t\t\t\t\t\tlen, flags);\n\t\t\t\tfdput(out);\n\t\t\t}\n\t\t}\n \t\tfdput(in);\n \t}\n\n\treturn error;\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_PIPE_FS_I_H\n#define _LINUX_PIPE_FS_I_H\n\n#define PIPE_DEF_BUFFERS\t16\n\n#define PIPE_BUF_FLAG_LRU\t0x01\t/* page is on the LRU */\n#define PIPE_BUF_FLAG_ATOMIC\t0x02\t/* was atomically mapped */\n#define PIPE_BUF_FLAG_GIFT\t0x04\t/* page is a gift */\n#define PIPE_BUF_FLAG_PACKET\t0x08\t/* read() as a packet */\n\n/**\n *\tstruct pipe_buffer - a linux kernel pipe buffer\n *\t@page: the page containing the data for the pipe buffer\n *\t@offset: offset of data inside the @page\n *\t@len: length of data inside the @page\n *\t@ops: operations associated with this buffer. See @pipe_buf_operations.\n *\t@flags: pipe buffer flags. See above.\n *\t@private: private data owned by the ops.\n **/\nstruct pipe_buffer {\n\tstruct page *page;\n\tunsigned int offset, len;\n\tconst struct pipe_buf_operations *ops;\n\tunsigned int flags;\n\tunsigned long private;\n};\n\n/**\n *\tstruct pipe_inode_info - a linux kernel pipe\n *\t@mutex: mutex protecting the whole thing\n *\t@wait: reader/writer wait point in case of empty/full pipe\n *\t@nrbufs: the number of non-empty pipe buffers in this pipe\n *\t@buffers: total number of buffers (should be a power of 2)\n *\t@curbuf: the current pipe buffer entry\n *\t@tmp_page: cached released page\n *\t@readers: number of current readers of this pipe\n *\t@writers: number of current writers of this pipe\n *\t@files: number of struct file referring this pipe (protected by ->i_lock)\n *\t@waiting_writers: number of writers blocked waiting for room\n *\t@r_counter: reader counter\n *\t@w_counter: writer counter\n *\t@fasync_readers: reader side fasync\n *\t@fasync_writers: writer side fasync\n *\t@bufs: the circular array of pipe buffers\n *\t@user: the user who created this pipe\n **/\nstruct pipe_inode_info {\n\tstruct mutex mutex;\n\twait_queue_head_t wait;\n\tunsigned int nrbufs, curbuf, buffers;\n\tunsigned int readers;\n\tunsigned int writers;\n\tunsigned int files;\n\tunsigned int waiting_writers;\n\tunsigned int r_counter;\n\tunsigned int w_counter;\n\tstruct page *tmp_page;\n\tstruct fasync_struct *fasync_readers;\n\tstruct fasync_struct *fasync_writers;\n\tstruct pipe_buffer *bufs;\n\tstruct user_struct *user;\n};\n\n/*\n * Note on the nesting of these functions:\n *\n * ->confirm()\n *\t->steal()\n *\n * That is, ->steal() must be called on a confirmed buffer.\n * See below for the meaning of each operation. Also see kerneldoc\n * in fs/pipe.c for the pipe and generic variants of these hooks.\n */\nstruct pipe_buf_operations {\n\t/*\n\t * This is set to 1, if the generic pipe read/write may coalesce\n\t * data into an existing buffer. If this is set to 0, a new pipe\n\t * page segment is always used for new data.\n\t */\n\tint can_merge;\n\n\t/*\n\t * ->confirm() verifies that the data in the pipe buffer is there\n\t * and that the contents are good. If the pages in the pipe belong\n\t * to a file system, we may need to wait for IO completion in this\n\t * hook. Returns 0 for good, or a negative error value in case of\n\t * error.\n\t */\n\tint (*confirm)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * When the contents of this pipe buffer has been completely\n\t * consumed by a reader, ->release() is called.\n\t */\n\tvoid (*release)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * Attempt to take ownership of the pipe buffer and its contents.\n\t * ->steal() returns 0 for success, in which case the contents\n\t * of the pipe (the buf->page) is locked and now completely owned\n\t * by the caller. The page may then be transferred to a different\n\t * mapping, the most often used case is insertion into different\n\t * file address space cache.\n\t */\n\tint (*steal)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * Get a reference to the pipe buffer.\n\t */\n\tvoid (*get)(struct pipe_inode_info *, struct pipe_buffer *);\n};\n\n/**\n * pipe_buf_get - get a reference to a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n */\nstatic inline void pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tbuf->ops->get(pipe, buf);\n}\n\n/**\n * pipe_buf_release - put a reference to a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n */\nstatic inline void pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tconst struct pipe_buf_operations *ops = buf->ops;\n\n\tbuf->ops = NULL;\n\tops->release(pipe, buf);\n}\n\n/**\n * pipe_buf_confirm - verify contents of the pipe buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to confirm\n */\nstatic inline int pipe_buf_confirm(struct pipe_inode_info *pipe,\n\t\t\t\t   struct pipe_buffer *buf)\n{\n\treturn buf->ops->confirm(pipe, buf);\n}\n\n/**\n * pipe_buf_steal - attempt to take ownership of a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n */\nstatic inline int pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t struct pipe_buffer *buf)\n{\n\treturn buf->ops->steal(pipe, buf);\n}\n\n/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual\n   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */\n#define PIPE_SIZE\t\tPAGE_SIZE\n\n/* Pipe lock and unlock operations */\nvoid pipe_lock(struct pipe_inode_info *);\nvoid pipe_unlock(struct pipe_inode_info *);\nvoid pipe_double_lock(struct pipe_inode_info *, struct pipe_inode_info *);\n\nextern unsigned int pipe_max_size;\nextern unsigned long pipe_user_pages_hard;\nextern unsigned long pipe_user_pages_soft;\n\n/* Drop the inode semaphore and wait for a pipe event, atomically */\nvoid pipe_wait(struct pipe_inode_info *pipe);\n\nstruct pipe_inode_info *alloc_pipe_info(void);\nvoid free_pipe_info(struct pipe_inode_info *);\n\n/* Generic pipe buffer ops functions */\nvoid generic_pipe_buf_get(struct pipe_inode_info *, struct pipe_buffer *);\nint generic_pipe_buf_confirm(struct pipe_inode_info *, struct pipe_buffer *);\nint generic_pipe_buf_steal(struct pipe_inode_info *, struct pipe_buffer *);\nvoid generic_pipe_buf_release(struct pipe_inode_info *, struct pipe_buffer *);\n\nextern const struct pipe_buf_operations nosteal_pipe_buf_ops;\n\n/* for F_SETPIPE_SZ and F_GETPIPE_SZ */\nlong pipe_fcntl(struct file *, unsigned int, unsigned long arg);\nstruct pipe_inode_info *get_pipe_info(struct file *file);\n\nint create_pipe_files(struct file **, int);\nunsigned int round_pipe_size(unsigned long size);\n\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * ring buffer based function tracer\n *\n * Copyright (C) 2007-2012 Steven Rostedt <srostedt@redhat.com>\n * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>\n *\n * Originally taken from the RT patch by:\n *    Arnaldo Carvalho de Melo <acme@redhat.com>\n *\n * Based on code from the latency_tracer, that is:\n *  Copyright (C) 2004-2006 Ingo Molnar\n *  Copyright (C) 2004 Nadia Yvette Chambers\n */\n#include <linux/ring_buffer.h>\n#include <generated/utsrelease.h>\n#include <linux/stacktrace.h>\n#include <linux/writeback.h>\n#include <linux/kallsyms.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/irqflags.h>\n#include <linux/debugfs.h>\n#include <linux/tracefs.h>\n#include <linux/pagemap.h>\n#include <linux/hardirq.h>\n#include <linux/linkage.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/splice.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/mount.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/fs.h>\n#include <linux/trace.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/rt.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n/*\n * On boot up, the ring buffer is set to the minimum size, so that\n * we do not waste memory on systems that are not using tracing.\n */\nbool ring_buffer_expanded;\n\n/*\n * We need to change this state when a selftest is running.\n * A selftest will lurk into the ring-buffer to count the\n * entries inserted during the selftest although some concurrent\n * insertions into the ring-buffer such as trace_printk could occurred\n * at the same time, giving false positive or negative results.\n */\nstatic bool __read_mostly tracing_selftest_running;\n\n/*\n * If a tracer is running, we do not want to run SELFTEST.\n */\nbool __read_mostly tracing_selftest_disabled;\n\n/* Pipe tracepoints to printk */\nstruct trace_iterator *tracepoint_print_iter;\nint tracepoint_printk;\nstatic DEFINE_STATIC_KEY_FALSE(tracepoint_printk_key);\n\n/* For tracers that don't implement custom flags */\nstatic struct tracer_opt dummy_tracer_opt[] = {\n\t{ }\n};\n\nstatic int\ndummy_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\treturn 0;\n}\n\n/*\n * To prevent the comm cache from being overwritten when no\n * tracing is active, only save the comm when a trace event\n * occurred.\n */\nstatic DEFINE_PER_CPU(bool, trace_taskinfo_save);\n\n/*\n * Kill all tracing for good (never come back).\n * It is initialized to 1 but will turn to zero if the initialization\n * of the tracer is successful. But that is the only place that sets\n * this back to zero.\n */\nstatic int tracing_disabled = 1;\n\ncpumask_var_t __read_mostly\ttracing_buffer_mask;\n\n/*\n * ftrace_dump_on_oops - variable to dump ftrace buffer on oops\n *\n * If there is an oops (or kernel panic) and the ftrace_dump_on_oops\n * is set, then ftrace_dump is called. This will output the contents\n * of the ftrace buffers to the console.  This is very useful for\n * capturing traces that lead to crashes and outputing it to a\n * serial console.\n *\n * It is default off, but you can enable it with either specifying\n * \"ftrace_dump_on_oops\" in the kernel command line, or setting\n * /proc/sys/kernel/ftrace_dump_on_oops\n * Set 1 if you want to dump buffers of all CPUs\n * Set 2 if you want to dump the buffer of the CPU that triggered oops\n */\n\nenum ftrace_dump_mode ftrace_dump_on_oops;\n\n/* When set, tracing will stop when a WARN*() is hit */\nint __disable_trace_on_warning;\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\n/* Map of enums to their values, for \"eval_map\" file */\nstruct trace_eval_map_head {\n\tstruct module\t\t\t*mod;\n\tunsigned long\t\t\tlength;\n};\n\nunion trace_eval_map_item;\n\nstruct trace_eval_map_tail {\n\t/*\n\t * \"end\" is first and points to NULL as it must be different\n\t * than \"mod\" or \"eval_string\"\n\t */\n\tunion trace_eval_map_item\t*next;\n\tconst char\t\t\t*end;\t/* points to NULL */\n};\n\nstatic DEFINE_MUTEX(trace_eval_mutex);\n\n/*\n * The trace_eval_maps are saved in an array with two extra elements,\n * one at the beginning, and one at the end. The beginning item contains\n * the count of the saved maps (head.length), and the module they\n * belong to if not built in (head.mod). The ending item contains a\n * pointer to the next array of saved eval_map items.\n */\nunion trace_eval_map_item {\n\tstruct trace_eval_map\t\tmap;\n\tstruct trace_eval_map_head\thead;\n\tstruct trace_eval_map_tail\ttail;\n};\n\nstatic union trace_eval_map_item *trace_eval_maps;\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf);\n\n#define MAX_TRACER_SIZE\t\t100\nstatic char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *default_bootup_tracer;\n\nstatic bool allocate_snapshot;\n\nstatic int __init set_cmdline_ftrace(char *str)\n{\n\tstrlcpy(bootup_tracer_buf, str, MAX_TRACER_SIZE);\n\tdefault_bootup_tracer = bootup_tracer_buf;\n\t/* We are using ftrace early, expand it */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"ftrace=\", set_cmdline_ftrace);\n\nstatic int __init set_ftrace_dump_on_oops(char *str)\n{\n\tif (*str++ != '=' || !*str) {\n\t\tftrace_dump_on_oops = DUMP_ALL;\n\t\treturn 1;\n\t}\n\n\tif (!strcmp(\"orig_cpu\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ORIG;\n                return 1;\n        }\n\n        return 0;\n}\n__setup(\"ftrace_dump_on_oops\", set_ftrace_dump_on_oops);\n\nstatic int __init stop_trace_on_warning(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\t__disable_trace_on_warning = 1;\n\treturn 1;\n}\n__setup(\"traceoff_on_warning\", stop_trace_on_warning);\n\nstatic int __init boot_alloc_snapshot(char *str)\n{\n\tallocate_snapshot = true;\n\t/* We also need the main ring buffer expanded */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"alloc_snapshot\", boot_alloc_snapshot);\n\n\nstatic char trace_boot_options_buf[MAX_TRACER_SIZE] __initdata;\n\nstatic int __init set_trace_boot_options(char *str)\n{\n\tstrlcpy(trace_boot_options_buf, str, MAX_TRACER_SIZE);\n\treturn 0;\n}\n__setup(\"trace_options=\", set_trace_boot_options);\n\nstatic char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *trace_boot_clock __initdata;\n\nstatic int __init set_trace_boot_clock(char *str)\n{\n\tstrlcpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);\n\ttrace_boot_clock = trace_boot_clock_buf;\n\treturn 0;\n}\n__setup(\"trace_clock=\", set_trace_boot_clock);\n\nstatic int __init set_tracepoint_printk(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\ttracepoint_printk = 1;\n\treturn 1;\n}\n__setup(\"tp_printk\", set_tracepoint_printk);\n\nunsigned long long ns2usecs(u64 nsec)\n{\n\tnsec += 500;\n\tdo_div(nsec, 1000);\n\treturn nsec;\n}\n\n/* trace_flags holds trace_options default values */\n#define TRACE_DEFAULT_FLAGS\t\t\t\t\t\t\\\n\t(FUNCTION_DEFAULT_FLAGS |\t\t\t\t\t\\\n\t TRACE_ITER_PRINT_PARENT | TRACE_ITER_PRINTK |\t\t\t\\\n\t TRACE_ITER_ANNOTATE | TRACE_ITER_CONTEXT_INFO |\t\t\\\n\t TRACE_ITER_RECORD_CMD | TRACE_ITER_OVERWRITE |\t\t\t\\\n\t TRACE_ITER_IRQ_INFO | TRACE_ITER_MARKERS)\n\n/* trace_options that are only supported by global_trace */\n#define TOP_LEVEL_TRACE_FLAGS (TRACE_ITER_PRINTK |\t\t\t\\\n\t       TRACE_ITER_PRINTK_MSGONLY | TRACE_ITER_RECORD_CMD)\n\n/* trace_flags that are default zero for instances */\n#define ZEROED_TRACE_FLAGS \\\n\t(TRACE_ITER_EVENT_FORK | TRACE_ITER_FUNC_FORK)\n\n/*\n * The global_trace is the descriptor that holds the top-level tracing\n * buffers for the live tracing.\n */\nstatic struct trace_array global_trace = {\n\t.trace_flags = TRACE_DEFAULT_FLAGS,\n};\n\nLIST_HEAD(ftrace_trace_arrays);\n\nint trace_array_get(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\ttr->ref++;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic void __trace_array_put(struct trace_array *this_tr)\n{\n\tWARN_ON(!this_tr->ref);\n\tthis_tr->ref--;\n}\n\nvoid trace_array_put(struct trace_array *this_tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__trace_array_put(this_tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nint call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t      struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tif (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&\n\t    !filter_match_preds(call->filter, rec)) {\n\t\t__trace_event_discard_commit(buffer, event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nvoid trace_free_pid_list(struct trace_pid_list *pid_list)\n{\n\tvfree(pid_list->pids);\n\tkfree(pid_list);\n}\n\n/**\n * trace_find_filtered_pid - check if a pid exists in a filtered_pid list\n * @filtered_pids: The list of pids to check\n * @search_pid: The PID to find in @filtered_pids\n *\n * Returns true if @search_pid is fonud in @filtered_pids, and false otherwis.\n */\nbool\ntrace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)\n{\n\t/*\n\t * If pid_max changed after filtered_pids was created, we\n\t * by default ignore all pids greater than the previous pid_max.\n\t */\n\tif (search_pid >= filtered_pids->pid_max)\n\t\treturn false;\n\n\treturn test_bit(search_pid, filtered_pids->pids);\n}\n\n/**\n * trace_ignore_this_task - should a task be ignored for tracing\n * @filtered_pids: The list of pids to check\n * @task: The task that should be ignored if not filtered\n *\n * Checks if @task should be traced or not from @filtered_pids.\n * Returns true if @task should *NOT* be traced.\n * Returns false if @task should be traced.\n */\nbool\ntrace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)\n{\n\t/*\n\t * Return false, because if filtered_pids does not exist,\n\t * all pids are good to trace.\n\t */\n\tif (!filtered_pids)\n\t\treturn false;\n\n\treturn !trace_find_filtered_pid(filtered_pids, task->pid);\n}\n\n/**\n * trace_pid_filter_add_remove_task - Add or remove a task from a pid_list\n * @pid_list: The list to modify\n * @self: The current task for fork or NULL for exit\n * @task: The task to add or remove\n *\n * If adding a task, if @self is defined, the task is only added if @self\n * is also included in @pid_list. This happens on fork and tasks should\n * only be added when the parent is listed. If @self is NULL, then the\n * @task pid will be removed from the list, which would happen on exit\n * of a task.\n */\nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task)\n{\n\tif (!pid_list)\n\t\treturn;\n\n\t/* For forks, we only add if the forking task is listed */\n\tif (self) {\n\t\tif (!trace_find_filtered_pid(pid_list, self->pid))\n\t\t\treturn;\n\t}\n\n\t/* Sorry, but we don't support pid_max changing after setting */\n\tif (task->pid >= pid_list->pid_max)\n\t\treturn;\n\n\t/* \"self\" is set for forks, and NULL for exits */\n\tif (self)\n\t\tset_bit(task->pid, pid_list->pids);\n\telse\n\t\tclear_bit(task->pid, pid_list->pids);\n}\n\n/**\n * trace_pid_next - Used for seq_file to get to the next pid of a pid_list\n * @pid_list: The pid list to show\n * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)\n * @pos: The position of the file\n *\n * This is used by the seq_file \"next\" operation to iterate the pids\n * listed in a trace_pid_list structure.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)\n{\n\tunsigned long pid = (unsigned long)v;\n\n\t(*pos)++;\n\n\t/* pid already is +1 of the actual prevous bit */\n\tpid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);\n\n\t/* Return pid + 1 to allow zero to be represented */\n\tif (pid < pid_list->pid_max)\n\t\treturn (void *)(pid + 1);\n\n\treturn NULL;\n}\n\n/**\n * trace_pid_start - Used for seq_file to start reading pid lists\n * @pid_list: The pid list to show\n * @pos: The position of the file\n *\n * This is used by seq_file \"start\" operation to start the iteration\n * of listing pids.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)\n{\n\tunsigned long pid;\n\tloff_t l = 0;\n\n\tpid = find_first_bit(pid_list->pids, pid_list->pid_max);\n\tif (pid >= pid_list->pid_max)\n\t\treturn NULL;\n\n\t/* Return pid + 1 so that zero can be the exit value */\n\tfor (pid++; pid && l < *pos;\n\t     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))\n\t\t;\n\treturn (void *)pid;\n}\n\n/**\n * trace_pid_show - show the current pid in seq_file processing\n * @m: The seq_file structure to write into\n * @v: A void pointer of the pid (+1) value to display\n *\n * Can be directly used by seq_file operations to display the current\n * pid value.\n */\nint trace_pid_show(struct seq_file *m, void *v)\n{\n\tunsigned long pid = (unsigned long)v - 1;\n\n\tseq_printf(m, \"%lu\\n\", pid);\n\treturn 0;\n}\n\n/* 128 should be much more than enough */\n#define PID_BUF_SIZE\t\t127\n\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_parser parser;\n\tunsigned long val;\n\tint nr_pids = 0;\n\tssize_t read = 0;\n\tssize_t ret = 0;\n\tloff_t pos;\n\tpid_t pid;\n\n\tif (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Always recreate a new array. The write is an all or nothing\n\t * operation. Always create a new array when adding new pids by\n\t * the user. If the operation fails, then the current list is\n\t * not modified.\n\t */\n\tpid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);\n\tif (!pid_list)\n\t\treturn -ENOMEM;\n\n\tpid_list->pid_max = READ_ONCE(pid_max);\n\n\t/* Only truncating will shrink pid_max */\n\tif (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)\n\t\tpid_list->pid_max = filtered_pids->pid_max;\n\n\tpid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);\n\tif (!pid_list->pids) {\n\t\tkfree(pid_list);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (filtered_pids) {\n\t\t/* copy the current bits to the new max */\n\t\tfor_each_set_bit(pid, filtered_pids->pids,\n\t\t\t\t filtered_pids->pid_max) {\n\t\t\tset_bit(pid, pid_list->pids);\n\t\t\tnr_pids++;\n\t\t}\n\t}\n\n\twhile (cnt > 0) {\n\n\t\tpos = 0;\n\n\t\tret = trace_get_user(&parser, ubuf, cnt, &pos);\n\t\tif (ret < 0 || !trace_parser_loaded(&parser))\n\t\t\tbreak;\n\n\t\tread += ret;\n\t\tubuf += ret;\n\t\tcnt -= ret;\n\n\t\tret = -EINVAL;\n\t\tif (kstrtoul(parser.buffer, 0, &val))\n\t\t\tbreak;\n\t\tif (val >= pid_list->pid_max)\n\t\t\tbreak;\n\n\t\tpid = (pid_t)val;\n\n\t\tset_bit(pid, pid_list->pids);\n\t\tnr_pids++;\n\n\t\ttrace_parser_clear(&parser);\n\t\tret = 0;\n\t}\n\ttrace_parser_put(&parser);\n\n\tif (ret < 0) {\n\t\ttrace_free_pid_list(pid_list);\n\t\treturn ret;\n\t}\n\n\tif (!nr_pids) {\n\t\t/* Cleared the list of pids */\n\t\ttrace_free_pid_list(pid_list);\n\t\tread = ret;\n\t\tpid_list = NULL;\n\t}\n\n\t*new_pid_list = pid_list;\n\n\treturn read;\n}\n\nstatic u64 buffer_ftrace_now(struct trace_buffer *buf, int cpu)\n{\n\tu64 ts;\n\n\t/* Early boot up does not have a buffer yet */\n\tif (!buf->buffer)\n\t\treturn trace_clock_local();\n\n\tts = ring_buffer_time_stamp(buf->buffer, cpu);\n\tring_buffer_normalize_time_stamp(buf->buffer, cpu, &ts);\n\n\treturn ts;\n}\n\nu64 ftrace_now(int cpu)\n{\n\treturn buffer_ftrace_now(&global_trace.trace_buffer, cpu);\n}\n\n/**\n * tracing_is_enabled - Show if global_trace has been disabled\n *\n * Shows if the global trace has been enabled or not. It uses the\n * mirror flag \"buffer_disabled\" to be used in fast paths such as for\n * the irqsoff tracer. But it may be inaccurate due to races. If you\n * need to know the accurate state, use tracing_is_on() which is a little\n * slower, but accurate.\n */\nint tracing_is_enabled(void)\n{\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}\n\n/*\n * trace_buf_size is the size in bytes that is allocated\n * for a buffer. Note, the number of bytes is always rounded\n * to page size.\n *\n * This number is purposely set to a low number of 16384.\n * If the dump on oops happens, it will be much appreciated\n * to not have to wait for all that output. Anyway this can be\n * boot time and run time configurable.\n */\n#define TRACE_BUF_SIZE_DEFAULT\t1441792UL /* 16384 * 88 (sizeof(entry)) */\n\nstatic unsigned long\t\ttrace_buf_size = TRACE_BUF_SIZE_DEFAULT;\n\n/* trace_types holds a link list of available tracers. */\nstatic struct tracer\t\t*trace_types __read_mostly;\n\n/*\n * trace_types_lock is used to protect the trace_types list.\n */\nDEFINE_MUTEX(trace_types_lock);\n\n/*\n * serialize the access of the ring buffer\n *\n * ring buffer serializes readers, but it is low level protection.\n * The validity of the events (which returns by ring_buffer_peek() ..etc)\n * are not protected by ring buffer.\n *\n * The content of events may become garbage if we allow other process consumes\n * these events concurrently:\n *   A) the page of the consumed events may become a normal page\n *      (not reader page) in ring buffer, and this page will be rewrited\n *      by events producer.\n *   B) The page of the consumed events may become a page for splice_read,\n *      and this page will be returned to system.\n *\n * These primitives allow multi process access to different cpu ring buffer\n * concurrently.\n *\n * These primitives don't distinguish read-only and read-consume access.\n * Multi read-only access are also serialized.\n */\n\n#ifdef CONFIG_SMP\nstatic DECLARE_RWSEM(all_cpu_access_lock);\nstatic DEFINE_PER_CPU(struct mutex, cpu_access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\t/* gain it for accessing the whole ring buffer. */\n\t\tdown_write(&all_cpu_access_lock);\n\t} else {\n\t\t/* gain it for accessing a cpu ring buffer. */\n\n\t\t/* Firstly block other trace_access_lock(RING_BUFFER_ALL_CPUS). */\n\t\tdown_read(&all_cpu_access_lock);\n\n\t\t/* Secondly block other access to this @cpu ring buffer. */\n\t\tmutex_lock(&per_cpu(cpu_access_lock, cpu));\n\t}\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tup_write(&all_cpu_access_lock);\n\t} else {\n\t\tmutex_unlock(&per_cpu(cpu_access_lock, cpu));\n\t\tup_read(&all_cpu_access_lock);\n\t}\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}\n\n#else\n\nstatic DEFINE_MUTEX(access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\t(void)cpu;\n\tmutex_lock(&access_lock);\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\t(void)cpu;\n\tmutex_unlock(&access_lock);\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n}\n\n#endif\n\n#ifdef CONFIG_STACKTRACE\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs);\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs);\n\n#else\nstatic inline void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs)\n{\n}\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n}\n\n#endif\n\nstatic __always_inline void\ntrace_event_setup(struct ring_buffer_event *event,\n\t\t  int type, unsigned long flags, int pc)\n{\n\tstruct trace_entry *ent = ring_buffer_event_data(event);\n\n\ttracing_generic_entry_update(ent, flags, pc);\n\tent->type = type;\n}\n\nstatic __always_inline struct ring_buffer_event *\n__trace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *event;\n\n\tevent = ring_buffer_lock_reserve(buffer, len);\n\tif (event != NULL)\n\t\ttrace_event_setup(event, type, flags, pc);\n\n\treturn event;\n}\n\nvoid tracer_tracing_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_on(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 0;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_on - enable tracing buffers\n *\n * This function enables tracing buffers that may have been\n * disabled with tracing_off.\n */\nvoid tracing_on(void)\n{\n\ttracer_tracing_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_on);\n\n\nstatic __always_inline void\n__buffer_unlock_commit(struct ring_buffer *buffer, struct ring_buffer_event *event)\n{\n\t__this_cpu_write(trace_taskinfo_save, true);\n\n\t/* If this is the temp buffer, we need to commit fully */\n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t/* Length is in event->array[0] */\n\t\tring_buffer_write(buffer, event->array[0], &event->array[1]);\n\t\t/* Release the temp buffer */\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t} else\n\t\tring_buffer_unlock_commit(buffer, event);\n}\n\n/**\n * __trace_puts - write a constant string into the trace buffer.\n * @ip:\t   The address of the caller\n * @str:   The constant string to write\n * @size:  The size of the string.\n */\nint __trace_puts(unsigned long ip, const char *str, int size)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(__trace_puts);\n\n/**\n * __trace_bputs - write the pointer to a constant string into trace buffer\n * @ip:\t   The address of the caller\n * @str:   The constant string to write to the buffer to\n */\nint __trace_bputs(unsigned long ip, const char *str)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct bputs_entry *entry;\n\tunsigned long irq_flags;\n\tint size = sizeof(struct bputs_entry);\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,\n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->str\t\t\t= str;\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(__trace_bputs);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nvoid tracing_snapshot_instance(struct trace_array *tr)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\tunsigned long flags;\n\n\tif (in_nmi()) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT CALLED FROM NMI CONTEXT ***\\n\");\n\t\tinternal_trace_puts(\"*** snapshot is being ignored        ***\\n\");\n\t\treturn;\n\t}\n\n\tif (!tr->allocated_snapshot) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT NOT ALLOCATED ***\\n\");\n\t\tinternal_trace_puts(\"*** stopping trace here!   ***\\n\");\n\t\ttracing_off();\n\t\treturn;\n\t}\n\n\t/* Note, snapshot can not be used when the tracer uses it */\n\tif (tracer->use_max_tr) {\n\t\tinternal_trace_puts(\"*** LATENCY TRACER ACTIVE ***\\n\");\n\t\tinternal_trace_puts(\"*** Can not use snapshot (sorry) ***\\n\");\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tupdate_max_tr(tr, current, smp_processor_id());\n\tlocal_irq_restore(flags);\n}\n\n/**\n * tracing_snapshot - take a snapshot of the current buffer.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n *\n * Note, make sure to allocate the snapshot with either\n * a tracing_snapshot_alloc(), or by doing it manually\n * with: echo 1 > /sys/kernel/debug/tracing/snapshot\n *\n * If the snapshot buffer is not allocated, it will stop tracing.\n * Basically making a permanent snapshot.\n */\nvoid tracing_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\ttracing_snapshot_instance(tr);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\n\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id);\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val);\n\nint tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\tint ret;\n\n\tif (!tr->allocated_snapshot) {\n\n\t\t/* allocate spare buffer */\n\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t   &tr->trace_buffer, RING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\ttr->allocated_snapshot = true;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_snapshot(struct trace_array *tr)\n{\n\t/*\n\t * We don't free the ring buffer. instead, resize it because\n\t * The max_tr ring buffer has some state (e.g. ring->clock) and\n\t * we want preserve it.\n\t */\n\tring_buffer_resize(tr->max_buffer.buffer, 1, RING_BUFFER_ALL_CPUS);\n\tset_buffer_entries(&tr->max_buffer, 1);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n\ttr->allocated_snapshot = false;\n}\n\n/**\n * tracing_alloc_snapshot - allocate snapshot buffer.\n *\n * This only allocates the snapshot buffer if it isn't already\n * allocated - it doesn't also take a snapshot.\n *\n * This is meant to be used in cases where the snapshot buffer needs\n * to be set up for events that can't sleep but need to be able to\n * trigger a snapshot.\n */\nint tracing_alloc_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\tint ret;\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\n\n/**\n * tracing_snapshot_alloc - allocate and take a snapshot of the current buffer.\n *\n * This is similar to tracing_snapshot(), but it will allocate the\n * snapshot buffer if it isn't already allocated. Use this only\n * where it is safe to sleep, as the allocation may sleep.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n */\nvoid tracing_snapshot_alloc(void)\n{\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#else\nvoid tracing_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\nint tracing_alloc_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but snapshot allocation used\");\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\nvoid tracing_snapshot_alloc(void)\n{\n\t/* Give warning */\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nvoid tracer_tracing_off(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_off(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 1;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_off - turn off tracing buffers\n *\n * This function stops the tracing buffers from recording data.\n * It does not disable any overhead the tracers themselves may\n * be causing. This function simply causes all recording to\n * the ring buffers to fail.\n */\nvoid tracing_off(void)\n{\n\ttracer_tracing_off(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_off);\n\nvoid disable_trace_on_warning(void)\n{\n\tif (__disable_trace_on_warning)\n\t\ttracing_off();\n}\n\n/**\n * tracer_tracing_is_on - show real state of ring buffer enabled\n * @tr : the trace array to know if ring buffer is enabled\n *\n * Shows real state of the ring buffer if it is enabled or not.\n */\nbool tracer_tracing_is_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\treturn ring_buffer_record_is_on(tr->trace_buffer.buffer);\n\treturn !tr->buffer_disabled;\n}\n\n/**\n * tracing_is_on - show state of ring buffers enabled\n */\nint tracing_is_on(void)\n{\n\treturn tracer_tracing_is_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_is_on);\n\nstatic int __init set_buf_size(char *str)\n{\n\tunsigned long buf_size;\n\n\tif (!str)\n\t\treturn 0;\n\tbuf_size = memparse(str, &str);\n\t/* nr_entries can not be zero */\n\tif (buf_size == 0)\n\t\treturn 0;\n\ttrace_buf_size = buf_size;\n\treturn 1;\n}\n__setup(\"trace_buf_size=\", set_buf_size);\n\nstatic int __init set_tracing_thresh(char *str)\n{\n\tunsigned long threshold;\n\tint ret;\n\n\tif (!str)\n\t\treturn 0;\n\tret = kstrtoul(str, 0, &threshold);\n\tif (ret < 0)\n\t\treturn 0;\n\ttracing_thresh = threshold * 1000;\n\treturn 1;\n}\n__setup(\"tracing_thresh=\", set_tracing_thresh);\n\nunsigned long nsecs_to_usecs(unsigned long nsecs)\n{\n\treturn nsecs / 1000;\n}\n\n/*\n * TRACE_FLAGS is defined as a tuple matching bit masks with strings.\n * It uses C(a, b) where 'a' is the eval (enum) name and 'b' is the string that\n * matches it. By defining \"C(a, b) b\", TRACE_FLAGS becomes a list\n * of strings in the order that the evals (enum) were defined.\n */\n#undef C\n#define C(a, b) b\n\n/* These must match the bit postions in trace_iterator_flags */\nstatic const char *trace_options[] = {\n\tTRACE_FLAGS\n\tNULL\n};\n\nstatic struct {\n\tu64 (*func)(void);\n\tconst char *name;\n\tint in_ns;\t\t/* is this clock in nanoseconds? */\n} trace_clocks[] = {\n\t{ trace_clock_local,\t\t\"local\",\t1 },\n\t{ trace_clock_global,\t\t\"global\",\t1 },\n\t{ trace_clock_counter,\t\t\"counter\",\t0 },\n\t{ trace_clock_jiffies,\t\t\"uptime\",\t0 },\n\t{ trace_clock,\t\t\t\"perf\",\t\t1 },\n\t{ ktime_get_mono_fast_ns,\t\"mono\",\t\t1 },\n\t{ ktime_get_raw_fast_ns,\t\"mono_raw\",\t1 },\n\t{ ktime_get_boot_fast_ns,\t\"boot\",\t\t1 },\n\tARCH_TRACE_CLOCKS\n};\n\nbool trace_clock_in_ns(struct trace_array *tr)\n{\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * trace_parser_get_init - gets the buffer for trace parser\n */\nint trace_parser_get_init(struct trace_parser *parser, int size)\n{\n\tmemset(parser, 0, sizeof(*parser));\n\n\tparser->buffer = kmalloc(size, GFP_KERNEL);\n\tif (!parser->buffer)\n\t\treturn 1;\n\n\tparser->size = size;\n\treturn 0;\n}\n\n/*\n * trace_parser_put - frees the buffer for trace parser\n */\nvoid trace_parser_put(struct trace_parser *parser)\n{\n\tkfree(parser->buffer);\n\tparser->buffer = NULL;\n}\n\n/*\n * trace_get_user - reads the user input string separated by  space\n * (matched by isspace(ch))\n *\n * For each string found the 'struct trace_parser' is updated,\n * and the function returns.\n *\n * Returns number of bytes read.\n *\n * See kernel/trace/trace.h for 'struct trace_parser' details.\n */\nint trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos)\n{\n\tchar ch;\n\tsize_t read = 0;\n\tssize_t ret;\n\n\tif (!*ppos)\n\t\ttrace_parser_clear(parser);\n\n\tret = get_user(ch, ubuf++);\n\tif (ret)\n\t\tgoto out;\n\n\tread++;\n\tcnt--;\n\n\t/*\n\t * The parser is not finished with the last write,\n\t * continue reading the user input without skipping spaces.\n\t */\n\tif (!parser->cont) {\n\t\t/* skip white space */\n\t\twhile (cnt && isspace(ch)) {\n\t\t\tret = get_user(ch, ubuf++);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tread++;\n\t\t\tcnt--;\n\t\t}\n\n\t\tparser->idx = 0;\n\n\t\t/* only spaces were written */\n\t\tif (isspace(ch) || !ch) {\n\t\t\t*ppos += read;\n\t\t\tret = read;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* read the non-space input */\n\twhile (cnt && !isspace(ch) && ch) {\n\t\tif (parser->idx < parser->size - 1)\n\t\t\tparser->buffer[parser->idx++] = ch;\n\t\telse {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = get_user(ch, ubuf++);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tread++;\n\t\tcnt--;\n\t}\n\n\t/* We either got finished input or we have to wait for another call. */\n\tif (isspace(ch) || !ch) {\n\t\tparser->buffer[parser->idx] = 0;\n\t\tparser->cont = false;\n\t} else if (parser->idx < parser->size - 1) {\n\t\tparser->cont = true;\n\t\tparser->buffer[parser->idx++] = ch;\n\t\t/* Make sure the parsed string always terminates with '\\0'. */\n\t\tparser->buffer[parser->idx] = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ppos += read;\n\tret = read;\n\nout:\n\treturn ret;\n}\n\n/* TODO add a seq_buf_to_buffer() */\nstatic ssize_t trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt)\n{\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}\n\nunsigned long __read_mostly\ttracing_thresh;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/*\n * Copy the new maximum trace into the separate maximum-trace\n * structure. (this way the maximum trace is permanently saved,\n * for later retrieval via /sys/kernel/tracing/tracing_max_latency)\n */\nstatic void\n__update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tstruct trace_buffer *max_buf = &tr->max_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(trace_buf->data, cpu);\n\tstruct trace_array_cpu *max_data = per_cpu_ptr(max_buf->data, cpu);\n\n\tmax_buf->cpu = cpu;\n\tmax_buf->time_start = data->preempt_timestamp;\n\n\tmax_data->saved_latency = tr->max_latency;\n\tmax_data->critical_start = data->critical_start;\n\tmax_data->critical_end = data->critical_end;\n\n\tmemcpy(max_data->comm, tsk->comm, TASK_COMM_LEN);\n\tmax_data->pid = tsk->pid;\n\t/*\n\t * If tsk == current, then use current_uid(), as that does not use\n\t * RCU. The irq tracer can be called out of RCU scope.\n\t */\n\tif (tsk == current)\n\t\tmax_data->uid = current_uid();\n\telse\n\t\tmax_data->uid = task_uid(tsk);\n\n\tmax_data->nice = tsk->static_prio - 20 - MAX_RT_PRIO;\n\tmax_data->policy = tsk->policy;\n\tmax_data->rt_priority = tsk->rt_priority;\n\n\t/* record this tasks comm */\n\ttracing_record_cmdline(tsk);\n}\n\n/**\n * update_max_tr - snapshot all trace buffers from global_trace to max_tr\n * @tr: tracer\n * @tsk: the task with the latency\n * @cpu: The cpu that initiated the trace.\n *\n * Flip the buffers between the @tr and the max_tr and record information\n * about which task was the cause of this latency.\n */\nvoid\nupdate_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\t/* Inherit the recordable setting from trace_buffer */\n\tif (ring_buffer_record_is_set_on(tr->trace_buffer.buffer))\n\t\tring_buffer_record_on(tr->max_buffer.buffer);\n\telse\n\t\tring_buffer_record_off(tr->max_buffer.buffer);\n\n\tswap(tr->trace_buffer.buffer, tr->max_buffer.buffer);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n\n/**\n * update_max_tr_single - only copy one trace over, and reset the rest\n * @tr - tracer\n * @tsk - task with the latency\n * @cpu - the cpu of the buffer to copy.\n *\n * Flip the trace of a single CPU buffer between the @tr and the max_tr.\n */\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int wait_on_pipe(struct trace_iterator *iter, int full)\n{\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn 0;\n\n\treturn ring_buffer_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\tfull);\n}\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nstatic bool selftests_can_run;\n\nstruct trace_selftests {\n\tstruct list_head\t\tlist;\n\tstruct tracer\t\t\t*type;\n};\n\nstatic LIST_HEAD(postponed_selftests);\n\nstatic int save_selftest(struct tracer *type)\n{\n\tstruct trace_selftests *selftest;\n\n\tselftest = kmalloc(sizeof(*selftest), GFP_KERNEL);\n\tif (!selftest)\n\t\treturn -ENOMEM;\n\n\tselftest->type = type;\n\tlist_add(&selftest->list, &postponed_selftests);\n\treturn 0;\n}\n\nstatic int run_tracer_selftest(struct tracer *type)\n{\n\tstruct trace_array *tr = &global_trace;\n\tstruct tracer *saved_tracer = tr->current_trace;\n\tint ret;\n\n\tif (!type->selftest || tracing_selftest_disabled)\n\t\treturn 0;\n\n\t/*\n\t * If a tracer registers early in boot up (before scheduling is\n\t * initialized and such), then do not run its selftests yet.\n\t * Instead, run it a little later in the boot process.\n\t */\n\tif (!selftests_can_run)\n\t\treturn save_selftest(type);\n\n\t/*\n\t * Run a selftest on this tracer.\n\t * Here we reset the trace buffer, and set the current\n\t * tracer to be this tracer. The tracer can then run some\n\t * internal tracing to verify that everything is in order.\n\t * If we fail, we do not register this tracer.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n\ttr->current_trace = type;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\t/* If we expanded the buffers, make sure the max is expanded too */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, trace_buf_size,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t\ttr->allocated_snapshot = true;\n\t}\n#endif\n\n\t/* the test is responsible for initializing and enabling */\n\tpr_info(\"Testing tracer %s: \", type->name);\n\tret = type->selftest(type, tr);\n\t/* the test is responsible for resetting too */\n\ttr->current_trace = saved_tracer;\n\tif (ret) {\n\t\tprintk(KERN_CONT \"FAILED!\\n\");\n\t\t/* Add the warning after printing 'FAILED' */\n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n\t/* Only reset on passing, to avoid touching corrupted buffers */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\ttr->allocated_snapshot = false;\n\n\t\t/* Shrink the max buffer again */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, 1,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t}\n#endif\n\n\tprintk(KERN_CONT \"PASSED\\n\");\n\treturn 0;\n}\n\nstatic __init int init_trace_selftests(void)\n{\n\tstruct trace_selftests *p, *n;\n\tstruct tracer *t, **last;\n\tint ret;\n\n\tselftests_can_run = true;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (list_empty(&postponed_selftests))\n\t\tgoto out;\n\n\tpr_info(\"Running postponed tracer tests:\\n\");\n\n\tlist_for_each_entry_safe(p, n, &postponed_selftests, list) {\n\t\tret = run_tracer_selftest(p->type);\n\t\t/* If the test fails, then warn and remove from available_tracers */\n\t\tif (ret < 0) {\n\t\t\tWARN(1, \"tracer: %s failed selftest, disabling\\n\",\n\t\t\t     p->type->name);\n\t\t\tlast = &trace_types;\n\t\t\tfor (t = trace_types; t; t = t->next) {\n\t\t\t\tif (t == p->type) {\n\t\t\t\t\t*last = t->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlast = &t->next;\n\t\t\t}\n\t\t}\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\ncore_initcall(init_trace_selftests);\n#else\nstatic inline int run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t);\n\nstatic void __init apply_trace_boot_options(void);\n\n/**\n * register_tracer - register a tracer with the ftrace system.\n * @type - the plugin for the tracer\n *\n * Register a new plugin tracer.\n */\nint __init register_tracer(struct tracer *type)\n{\n\tstruct tracer *t;\n\tint ret = 0;\n\n\tif (!type->name) {\n\t\tpr_info(\"Tracer must have a name\\n\");\n\t\treturn -1;\n\t}\n\n\tif (strlen(type->name) >= MAX_TRACER_SIZE) {\n\t\tpr_info(\"Tracer has a name longer than %d\\n\", MAX_TRACER_SIZE);\n\t\treturn -1;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\ttracing_selftest_running = true;\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(type->name, t->name) == 0) {\n\t\t\t/* already found */\n\t\t\tpr_info(\"Tracer %s already registered\\n\",\n\t\t\t\ttype->name);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!type->set_flag)\n\t\ttype->set_flag = &dummy_set_flag;\n\tif (!type->flags) {\n\t\t/*allocate a dummy tracer_flags*/\n\t\ttype->flags = kmalloc(sizeof(*type->flags), GFP_KERNEL);\n\t\tif (!type->flags) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttype->flags->val = 0;\n\t\ttype->flags->opts = dummy_tracer_opt;\n\t} else\n\t\tif (!type->flags->opts)\n\t\t\ttype->flags->opts = dummy_tracer_opt;\n\n\t/* store the tracer for __set_tracer_option */\n\ttype->flags->trace = type;\n\n\tret = run_tracer_selftest(type);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttype->next = trace_types;\n\ttrace_types = type;\n\tadd_tracer_options(&global_trace, type);\n\n out:\n\ttracing_selftest_running = false;\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret || !default_bootup_tracer)\n\t\tgoto out_unlock;\n\n\tif (strncmp(default_bootup_tracer, type->name, MAX_TRACER_SIZE))\n\t\tgoto out_unlock;\n\n\tprintk(KERN_INFO \"Starting tracer '%s'\\n\", type->name);\n\t/* Do we want this tracer to start on bootup? */\n\ttracing_set_tracer(&global_trace, type->name);\n\tdefault_bootup_tracer = NULL;\n\n\tapply_trace_boot_options();\n\n\t/* disable other selftests, since this will break it. */\n\ttracing_selftest_disabled = true;\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\tprintk(KERN_INFO \"Disabling FTRACE selftests due to running tracer '%s'\\n\",\n\t       type->name);\n#endif\n\n out_unlock:\n\treturn ret;\n}\n\nvoid tracing_reset(struct trace_buffer *buf, int cpu)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\nvoid tracing_reset_online_cpus(struct trace_buffer *buf)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\tint cpu;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tbuf->time_start = buffer_ftrace_now(buf, buf->cpu);\n\n\tfor_each_online_cpu(cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\n/* Must have trace_types_lock held */\nvoid tracing_reset_all_online_cpus(void)\n{\n\tstruct trace_array *tr;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->clear_trace)\n\t\t\tcontinue;\n\t\ttr->clear_trace = false;\n\t\ttracing_reset_online_cpus(&tr->trace_buffer);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\t}\n}\n\nstatic int *tgid_map;\n\n#define SAVED_CMDLINES_DEFAULT 128\n#define NO_CMDLINE_MAP UINT_MAX\nstatic arch_spinlock_t trace_cmdline_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstruct saved_cmdlines_buffer {\n\tunsigned map_pid_to_cmdline[PID_MAX_DEFAULT+1];\n\tunsigned *map_cmdline_to_pid;\n\tunsigned cmdline_num;\n\tint cmdline_idx;\n\tchar *saved_cmdlines;\n};\nstatic struct saved_cmdlines_buffer *savedcmd;\n\n/* temporary disable recording */\nstatic atomic_t trace_record_taskinfo_disabled __read_mostly;\n\nstatic inline char *get_saved_cmdlines(int idx)\n{\n\treturn &savedcmd->saved_cmdlines[idx * TASK_COMM_LEN];\n}\n\nstatic inline void set_cmdline(int idx, const char *cmdline)\n{\n\tmemcpy(get_saved_cmdlines(idx), cmdline, TASK_COMM_LEN);\n}\n\nstatic int allocate_cmdlines_buffer(unsigned int val,\n\t\t\t\t    struct saved_cmdlines_buffer *s)\n{\n\ts->map_cmdline_to_pid = kmalloc_array(val,\n\t\t\t\t\t      sizeof(*s->map_cmdline_to_pid),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!s->map_cmdline_to_pid)\n\t\treturn -ENOMEM;\n\n\ts->saved_cmdlines = kmalloc_array(TASK_COMM_LEN, val, GFP_KERNEL);\n\tif (!s->saved_cmdlines) {\n\t\tkfree(s->map_cmdline_to_pid);\n\t\treturn -ENOMEM;\n\t}\n\n\ts->cmdline_idx = 0;\n\ts->cmdline_num = val;\n\tmemset(&s->map_pid_to_cmdline, NO_CMDLINE_MAP,\n\t       sizeof(s->map_pid_to_cmdline));\n\tmemset(s->map_cmdline_to_pid, NO_CMDLINE_MAP,\n\t       val * sizeof(*s->map_cmdline_to_pid));\n\n\treturn 0;\n}\n\nstatic int trace_create_savedcmd(void)\n{\n\tint ret;\n\n\tsavedcmd = kmalloc(sizeof(*savedcmd), GFP_KERNEL);\n\tif (!savedcmd)\n\t\treturn -ENOMEM;\n\n\tret = allocate_cmdlines_buffer(SAVED_CMDLINES_DEFAULT, savedcmd);\n\tif (ret < 0) {\n\t\tkfree(savedcmd);\n\t\tsavedcmd = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint is_tracing_stopped(void)\n{\n\treturn global_trace.stop_count;\n}\n\n/**\n * tracing_start - quick start of the tracer\n *\n * If tracing is enabled but was stopped by tracing_stop,\n * this will start the tracer back up.\n */\nvoid tracing_start(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (--global_trace.stop_count) {\n\t\tif (global_trace.stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tglobal_trace.stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_start_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\t/* If global, we need to also start the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_start();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\n\tif (--tr->stop_count) {\n\t\tif (tr->stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\ttr->stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n/**\n * tracing_stop - quick stop of the tracer\n *\n * Light weight way to stop tracing. Use in conjunction with\n * tracing_start.\n */\nvoid tracing_stop(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (global_trace.stop_count++)\n\t\tgoto out;\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_stop_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\t/* If global, we need to also stop the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_stop();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (tr->stop_count++)\n\t\tgoto out;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\nstatic int trace_save_cmdline(struct task_struct *tsk)\n{\n\tunsigned pid, idx;\n\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\t/*\n\t * It's not the end of the world if we don't get\n\t * the lock, but we also don't want to spin\n\t * nor do we want to disable interrupts,\n\t * so if we miss here, then better luck next time.\n\t */\n\tif (!arch_spin_trylock(&trace_cmdline_lock))\n\t\treturn 0;\n\n\tidx = savedcmd->map_pid_to_cmdline[tsk->pid];\n\tif (idx == NO_CMDLINE_MAP) {\n\t\tidx = (savedcmd->cmdline_idx + 1) % savedcmd->cmdline_num;\n\n\t\t/*\n\t\t * Check whether the cmdline buffer at idx has a pid\n\t\t * mapped. We are going to overwrite that entry so we\n\t\t * need to clear the map_pid_to_cmdline. Otherwise we\n\t\t * would read the new comm for the old pid.\n\t\t */\n\t\tpid = savedcmd->map_cmdline_to_pid[idx];\n\t\tif (pid != NO_CMDLINE_MAP)\n\t\t\tsavedcmd->map_pid_to_cmdline[pid] = NO_CMDLINE_MAP;\n\n\t\tsavedcmd->map_cmdline_to_pid[idx] = tsk->pid;\n\t\tsavedcmd->map_pid_to_cmdline[tsk->pid] = idx;\n\n\t\tsavedcmd->cmdline_idx = idx;\n\t}\n\n\tset_cmdline(idx, tsk->comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn 1;\n}\n\nstatic void __trace_find_cmdline(int pid, char comm[])\n{\n\tunsigned map;\n\n\tif (!pid) {\n\t\tstrcpy(comm, \"<idle>\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(pid < 0)) {\n\t\tstrcpy(comm, \"<XXX>\");\n\t\treturn;\n\t}\n\n\tif (pid > PID_MAX_DEFAULT) {\n\t\tstrcpy(comm, \"<...>\");\n\t\treturn;\n\t}\n\n\tmap = savedcmd->map_pid_to_cmdline[pid];\n\tif (map != NO_CMDLINE_MAP)\n\t\tstrlcpy(comm, get_saved_cmdlines(map), TASK_COMM_LEN);\n\telse\n\t\tstrcpy(comm, \"<...>\");\n}\n\nvoid trace_find_cmdline(int pid, char comm[])\n{\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\t__trace_find_cmdline(pid, comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nint trace_find_tgid(int pid)\n{\n\tif (unlikely(!tgid_map || !pid || pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\treturn tgid_map[pid];\n}\n\nstatic int trace_save_tgid(struct task_struct *tsk)\n{\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(!tgid_map || tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\ttgid_map[tsk->pid] = tsk->tgid;\n\treturn 1;\n}\n\nstatic bool tracing_record_taskinfo_skip(int flags)\n{\n\tif (unlikely(!(flags & (TRACE_RECORD_CMDLINE | TRACE_RECORD_TGID))))\n\t\treturn true;\n\tif (atomic_read(&trace_record_taskinfo_disabled) || !tracing_is_on())\n\t\treturn true;\n\tif (!__this_cpu_read(trace_taskinfo_save))\n\t\treturn true;\n\treturn false;\n}\n\n/**\n * tracing_record_taskinfo - record the task info of a task\n *\n * @task  - task to record\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *        - TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo(struct task_struct *task, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(task);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(task);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/**\n * tracing_record_taskinfo_sched_switch - record task info for sched_switch\n *\n * @prev - previous task during sched_switch\n * @next - next task during sched_switch\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *          TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone  = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(prev);\n\tdone &= !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(next);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(prev);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(next);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/* Helpers to record a specific task information */\nvoid tracing_record_cmdline(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_CMDLINE);\n}\n\nvoid tracing_record_tgid(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_TGID);\n}\n\n/*\n * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq\n * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function\n * simplifies those functions and keeps them in sync.\n */\nenum print_line_t trace_handle_return(struct trace_seq *s)\n{\n\treturn trace_seq_has_overflowed(s) ?\n\t\tTRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;\n}\nEXPORT_SYMBOL_GPL(trace_handle_return);\n\nvoid\ntracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,\n\t\t\t     int pc)\n{\n\tstruct task_struct *tsk = current;\n\n\tentry->preempt_count\t\t= pc & 0xff;\n\tentry->pid\t\t\t= (tsk) ? tsk->pid : 0;\n\tentry->flags =\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n\t\t(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |\n#else\n\t\tTRACE_FLAG_IRQS_NOSUPPORT |\n#endif\n\t\t((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |\n\t\t((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |\n\t\t((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |\n\t\t(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |\n\t\t(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);\n}\nEXPORT_SYMBOL_GPL(tracing_generic_entry_update);\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\treturn __trace_buffer_lock_reserve(buffer, type, len, flags, pc);\n}\n\nDEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDEFINE_PER_CPU(int, trace_buffered_event_cnt);\nstatic int trace_buffered_event_ref;\n\n/**\n * trace_buffered_event_enable - enable buffering events\n *\n * When events are being filtered, it is quicker to use a temporary\n * buffer to write the event data into if there's a likely chance\n * that it will not be committed. The discard of the ring buffer\n * is not as fast as committing, and is much slower than copying\n * a commit.\n *\n * When an event is to be filtered, allocate per cpu buffers to\n * write the event data into, and if the event is filtered and discarded\n * it is simply dropped, otherwise, the entire data is to be committed\n * in one shot.\n */\nvoid trace_buffered_event_enable(void)\n{\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}\n\nstatic void enable_trace_buffered_event(void *data)\n{\n\t/* Probably not needed, but do it anyway */\n\tsmp_rmb();\n\tthis_cpu_dec(trace_buffered_event_cnt);\n}\n\nstatic void disable_trace_buffered_event(void *data)\n{\n\tthis_cpu_inc(trace_buffered_event_cnt);\n}\n\n/**\n * trace_buffered_event_disable - disable buffering events\n *\n * When a filter is removed, it is faster to not use the buffered\n * events, and to commit directly into the ring buffer. Free up\n * the temp buffers when there are no more users. This requires\n * special synchronization with current events.\n */\nvoid trace_buffered_event_disable(void)\n{\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (WARN_ON_ONCE(!trace_buffered_event_ref))\n\t\treturn;\n\n\tif (--trace_buffered_event_ref)\n\t\treturn;\n\n\tpreempt_disable();\n\t/* For each CPU, set the buffer as used. */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       disable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n\n\t/* Wait for all current users to finish */\n\tsynchronize_rcu();\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tfree_page((unsigned long)per_cpu(trace_buffered_event, cpu));\n\t\tper_cpu(trace_buffered_event, cpu) = NULL;\n\t}\n\t/*\n\t * Make sure trace_buffered_event is NULL before clearing\n\t * trace_buffered_event_cnt.\n\t */\n\tsmp_wmb();\n\n\tpreempt_disable();\n\t/* Do the work on each cpu */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       enable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n}\n\nstatic struct ring_buffer *temp_buffer;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct ring_buffer **current_rb,\n\t\t\t  struct trace_event_file *trace_file,\n\t\t\t  int type, unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *entry;\n\tint val;\n\n\t*current_rb = trace_file->tr->trace_buffer.buffer;\n\n\tif (!ring_buffer_time_stamp_abs(*current_rb) && (trace_file->flags &\n\t     (EVENT_FILE_FL_SOFT_DISABLED | EVENT_FILE_FL_FILTERED)) &&\n\t    (entry = this_cpu_read(trace_buffered_event))) {\n\t\t/* Try to use the per cpu buffer first */\n\t\tval = this_cpu_inc_return(trace_buffered_event_cnt);\n\t\tif (val == 1) {\n\t\t\ttrace_event_setup(entry, type, flags, pc);\n\t\t\tentry->array[0] = len;\n\t\t\treturn entry;\n\t\t}\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t}\n\n\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t    type, len, flags, pc);\n\t/*\n\t * If tracing is off, but we have triggers enabled\n\t * we still need to look at the event data. Use the temp_buffer\n\t * to store the trace event for the tigger to use. It's recusive\n\t * safe and will not be recorded anywhere.\n\t */\n\tif (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {\n\t\t*current_rb = temp_buffer;\n\t\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t\t    type, len, flags, pc);\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_lock_reserve);\n\nstatic DEFINE_SPINLOCK(tracepoint_iter_lock);\nstatic DEFINE_MUTEX(tracepoint_printk_mutex);\n\nstatic void output_printk(struct trace_event_buffer *fbuffer)\n{\n\tstruct trace_event_call *event_call;\n\tstruct trace_event *event;\n\tunsigned long flags;\n\tstruct trace_iterator *iter = tracepoint_print_iter;\n\n\t/* We should never get here if iter is NULL */\n\tif (WARN_ON_ONCE(!iter))\n\t\treturn;\n\n\tevent_call = fbuffer->trace_file->event_call;\n\tif (!event_call || !event_call->event.funcs ||\n\t    !event_call->event.funcs->trace)\n\t\treturn;\n\n\tevent = &fbuffer->trace_file->event_call->event;\n\n\tspin_lock_irqsave(&tracepoint_iter_lock, flags);\n\ttrace_seq_init(&iter->seq);\n\titer->ent = fbuffer->entry;\n\tevent_call->event.funcs->trace(iter, 0, event);\n\ttrace_seq_putc(&iter->seq, 0);\n\tprintk(\"%s\", iter->seq.buffer);\n\n\tspin_unlock_irqrestore(&tracepoint_iter_lock, flags);\n}\n\nint tracepoint_printk_sysctl(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tint save_tracepoint_printk;\n\tint ret;\n\n\tmutex_lock(&tracepoint_printk_mutex);\n\tsave_tracepoint_printk = tracepoint_printk;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\t/*\n\t * This will force exiting early, as tracepoint_printk\n\t * is always zero when tracepoint_printk_iter is not allocated\n\t */\n\tif (!tracepoint_print_iter)\n\t\ttracepoint_printk = 0;\n\n\tif (save_tracepoint_printk == tracepoint_printk)\n\t\tgoto out;\n\n\tif (tracepoint_printk)\n\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\telse\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\n out:\n\tmutex_unlock(&tracepoint_printk_mutex);\n\n\treturn ret;\n}\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer)\n{\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_commit);\n\n/*\n * Skip 3:\n *\n *   trace_buffer_unlock_commit_regs()\n *   trace_event_buffer_commit()\n *   trace_event_raw_event_xxx()\n */\n# define STACK_SKIP 3\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct ring_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned long flags, int pc,\n\t\t\t\t     struct pt_regs *regs)\n{\n\t__buffer_unlock_commit(buffer, event);\n\n\t/*\n\t * If regs is not set, then skip the necessary functions.\n\t * Note, we can still get here via blktrace, wakeup tracer\n\t * and mmiotrace, but that's ok if they lose a function or\n\t * two. They are not that meaningful.\n\t */\n\tftrace_trace_stack(tr, buffer, flags, regs ? 0 : STACK_SKIP, pc, regs);\n\tftrace_trace_userstack(buffer, flags, pc);\n}\n\n/*\n * Similar to trace_buffer_unlock_commit_regs() but do not dump stack.\n */\nvoid\ntrace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event)\n{\n\t__buffer_unlock_commit(buffer, event);\n}\n\nstatic void\ntrace_process_export(struct trace_export *export,\n\t       struct ring_buffer_event *event)\n{\n\tstruct trace_entry *entry;\n\tunsigned int size = 0;\n\n\tentry = ring_buffer_event_data(event);\n\tsize = ring_buffer_event_length(event);\n\texport->write(export, entry, size);\n}\n\nstatic DEFINE_MUTEX(ftrace_export_lock);\n\nstatic struct trace_export __rcu *ftrace_exports_list __read_mostly;\n\nstatic DEFINE_STATIC_KEY_FALSE(ftrace_exports_enabled);\n\nstatic inline void ftrace_exports_enable(void)\n{\n\tstatic_branch_enable(&ftrace_exports_enabled);\n}\n\nstatic inline void ftrace_exports_disable(void)\n{\n\tstatic_branch_disable(&ftrace_exports_enabled);\n}\n\nstatic void ftrace_exports(struct ring_buffer_event *event)\n{\n\tstruct trace_export *export;\n\n\tpreempt_disable_notrace();\n\n\texport = rcu_dereference_raw_notrace(ftrace_exports_list);\n\twhile (export) {\n\t\ttrace_process_export(export, event);\n\t\texport = rcu_dereference_raw_notrace(export->next);\n\t}\n\n\tpreempt_enable_notrace();\n}\n\nstatic inline void\nadd_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}\n\nstatic inline int\nrm_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\tstruct trace_export **p;\n\n\tfor (p = list; *p != NULL; p = &(*p)->next)\n\t\tif (*p == export)\n\t\t\tbreak;\n\n\tif (*p != export)\n\t\treturn -1;\n\n\trcu_assign_pointer(*p, (*p)->next);\n\n\treturn 0;\n}\n\nstatic inline void\nadd_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tif (*list == NULL)\n\t\tftrace_exports_enable();\n\n\tadd_trace_export(list, export);\n}\n\nstatic inline int\nrm_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tint ret;\n\n\tret = rm_trace_export(list, export);\n\tif (*list == NULL)\n\t\tftrace_exports_disable();\n\n\treturn ret;\n}\n\nint register_ftrace_export(struct trace_export *export)\n{\n\tif (WARN_ON_ONCE(!export->write))\n\t\treturn -1;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tadd_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_export);\n\nint unregister_ftrace_export(struct trace_export *export)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tret = rm_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_export);\n\nvoid\ntrace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc)\n{\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}\n\n#ifdef CONFIG_STACKTRACE\n\n#define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))\nstruct ftrace_stack {\n\tunsigned long\t\tcalls[FTRACE_STACK_MAX_ENTRIES];\n};\n\nstatic DEFINE_PER_CPU(struct ftrace_stack, ftrace_stack);\nstatic DEFINE_PER_CPU(int, ftrace_stack_reserve);\n\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs)\n{\n\tstruct trace_event_call *call = &event_kernel_stack;\n\tstruct ring_buffer_event *event;\n\tstruct stack_entry *entry;\n\tstruct stack_trace trace;\n\tint use_stack;\n\tint size = FTRACE_STACK_ENTRIES;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.skip\t\t= skip;\n\n\t/*\n\t * Add one, for this function and the call to save_stack_trace()\n\t * If regs is set, then these functions will not be in the way.\n\t */\n#ifndef CONFIG_UNWINDER_ORC\n\tif (!regs)\n\t\ttrace.skip++;\n#endif\n\n\t/*\n\t * Since events can happen in NMIs there's no safe way to\n\t * use the per cpu ftrace_stacks. We reserve it and if an interrupt\n\t * or NMI comes in, it will just have to use the default\n\t * FTRACE_STACK_SIZE.\n\t */\n\tpreempt_disable_notrace();\n\n\tuse_stack = __this_cpu_inc_return(ftrace_stack_reserve);\n\t/*\n\t * We don't need any atomic variables, just a barrier.\n\t * If an interrupt comes in, we don't care, because it would\n\t * have exited and put the counter back to what we want.\n\t * We just need a barrier to keep gcc from moving things\n\t * around.\n\t */\n\tbarrier();\n\tif (use_stack == 1) {\n\t\ttrace.entries\t\t= this_cpu_ptr(ftrace_stack.calls);\n\t\ttrace.max_entries\t= FTRACE_STACK_MAX_ENTRIES;\n\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\n\t\tif (trace.nr_entries > size)\n\t\t\tsize = trace.nr_entries;\n\t} else\n\t\t/* From now on, use_stack is a boolean */\n\t\tuse_stack = 0;\n\n\tsize *= sizeof(unsigned long);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_STACK,\n\t\t\t\t\t    sizeof(*entry) + size, flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\n\tmemset(&entry->caller, 0, size);\n\n\tif (use_stack)\n\t\tmemcpy(&entry->caller, trace.entries,\n\t\t       trace.nr_entries * sizeof(unsigned long));\n\telse {\n\t\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\t\ttrace.entries\t\t= entry->caller;\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\t}\n\n\tentry->size = trace.nr_entries;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out:\n\t/* Again, don't let gcc optimize things here */\n\tbarrier();\n\t__this_cpu_dec(ftrace_stack_reserve);\n\tpreempt_enable_notrace();\n\n}\n\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n\tif (!(tr->trace_flags & TRACE_ITER_STACKTRACE))\n\t\treturn;\n\n\t__ftrace_trace_stack(buffer, flags, skip, pc, regs);\n}\n\nvoid __trace_stack(struct trace_array *tr, unsigned long flags, int skip,\n\t\t   int pc)\n{\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\n\tif (rcu_is_watching()) {\n\t\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\t\treturn;\n\t}\n\n\t/*\n\t * When an NMI triggers, RCU is enabled via rcu_nmi_enter(),\n\t * but if the above rcu_is_watching() failed, then the NMI\n\t * triggered someplace critical, and rcu_irq_enter() should\n\t * not be called from NMI.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\trcu_irq_enter_irqson();\n\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\trcu_irq_exit_irqson();\n}\n\n/**\n * trace_dump_stack - record a stack back trace in the trace buffer\n * @skip: Number of functions to skip (helper handlers)\n */\nvoid trace_dump_stack(int skip)\n{\n\tunsigned long flags;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn;\n\n\tlocal_save_flags(flags);\n\n#ifndef CONFIG_UNWINDER_ORC\n\t/* Skip 1 to skip this function. */\n\tskip++;\n#endif\n\t__ftrace_trace_stack(global_trace.trace_buffer.buffer,\n\t\t\t     flags, skip, preempt_count(), NULL);\n}\nEXPORT_SYMBOL_GPL(trace_dump_stack);\n\nstatic DEFINE_PER_CPU(int, user_stack_count);\n\nvoid\nftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags, int pc)\n{\n\tstruct trace_event_call *call = &event_user_stack;\n\tstruct ring_buffer_event *event;\n\tstruct userstack_entry *entry;\n\tstruct stack_trace trace;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_USERSTACKTRACE))\n\t\treturn;\n\n\t/*\n\t * NMIs can not handle page faults, even with fix ups.\n\t * The save user stack can (and often does) fault.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\t/*\n\t * prevent recursion, since the user stack tracing may\n\t * trigger other kernel events.\n\t */\n\tpreempt_disable();\n\tif (__this_cpu_read(user_stack_count))\n\t\tgoto out;\n\n\t__this_cpu_inc(user_stack_count);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,\n\t\t\t\t\t    sizeof(*entry), flags, pc);\n\tif (!event)\n\t\tgoto out_drop_count;\n\tentry\t= ring_buffer_event_data(event);\n\n\tentry->tgid\t\t= current->tgid;\n\tmemset(&entry->caller, 0, sizeof(entry->caller));\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\ttrace.skip\t\t= 0;\n\ttrace.entries\t\t= entry->caller;\n\n\tsave_stack_trace_user(&trace);\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out_drop_count:\n\t__this_cpu_dec(user_stack_count);\n out:\n\tpreempt_enable();\n}\n\n#ifdef UNUSED\nstatic void __trace_userstack(struct trace_array *tr, unsigned long flags)\n{\n\tftrace_trace_userstack(tr, flags, preempt_count());\n}\n#endif /* UNUSED */\n\n#endif /* CONFIG_STACKTRACE */\n\n/* created for use with alloc_percpu */\nstruct trace_buffer_struct {\n\tint nesting;\n\tchar buffer[4][TRACE_BUF_SIZE];\n};\n\nstatic struct trace_buffer_struct *trace_percpu_buffer;\n\n/*\n * Thise allows for lockless recording.  If we're nested too deeply, then\n * this returns NULL.\n */\nstatic char *get_trace_buf(void)\n{\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}\n\nstatic void put_trace_buf(void)\n{\n\t/* Don't let the decrement of nesting leak before this */\n\tbarrier();\n\tthis_cpu_dec(trace_percpu_buffer->nesting);\n}\n\nstatic int alloc_percpu_trace_buffer(void)\n{\n\tstruct trace_buffer_struct *buffers;\n\n\tbuffers = alloc_percpu(struct trace_buffer_struct);\n\tif (WARN(!buffers, \"Could not allocate percpu trace_printk buffer\"))\n\t\treturn -ENOMEM;\n\n\ttrace_percpu_buffer = buffers;\n\treturn 0;\n}\n\nstatic int buffers_allocated;\n\nvoid trace_printk_init_buffers(void)\n{\n\tif (buffers_allocated)\n\t\treturn;\n\n\tif (alloc_percpu_trace_buffer())\n\t\treturn;\n\n\t/* trace_printk() is for debug use only. Don't use it in production. */\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** trace_printk() being used. Allocating extra memory.  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** This means that this is a DEBUG kernel and it is     **\\n\");\n\tpr_warn(\"** unsafe for production use.                           **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging    **\\n\");\n\tpr_warn(\"** the kernel, report this immediately to your vendor!  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\n\t/* Expand the buffers to set size */\n\ttracing_update_buffers();\n\n\tbuffers_allocated = 1;\n\n\t/*\n\t * trace_printk_init_buffers() can be called by modules.\n\t * If that happens, then we need to start cmdline recording\n\t * directly here. If the global_trace.buffer is already\n\t * allocated here, then this was called by module code.\n\t */\n\tif (global_trace.trace_buffer.buffer)\n\t\ttracing_start_cmdline_record();\n}\n\nvoid trace_printk_start_comm(void)\n{\n\t/* Start tracing comms if trace printk is set */\n\tif (!buffers_allocated)\n\t\treturn;\n\ttracing_start_cmdline_record();\n}\n\nstatic void trace_printk_start_stop_comm(int enabled)\n{\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}\n\n/**\n * trace_vbprintk - write binary msg to tracing buffer\n *\n */\nint trace_vbprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_bprint;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct trace_array *tr = &global_trace;\n\tstruct bprint_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\tint len = 0, size, pc;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vbin_printf((u32 *)tbuffer, TRACE_BUF_SIZE/sizeof(int), fmt, args);\n\n\tif (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)\n\t\tgoto out;\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + sizeof(u32) * len;\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->fmt\t\t\t= fmt;\n\n\tmemcpy(entry->buf, tbuffer, sizeof(u32) * len);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(tr, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(trace_vbprintk);\n\n__printf(3, 0)\nstatic int\n__trace_array_vprintk(struct ring_buffer *buffer,\n\t\t      unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_print;\n\tstruct ring_buffer_event *event;\n\tint len = 0, size, pc;\n\tstruct print_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + len + 1;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, tbuffer, len + 1);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(&global_trace, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\n\n__printf(3, 0)\nint trace_array_vprintk(struct trace_array *tr,\n\t\t\tunsigned long ip, const char *fmt, va_list args)\n{\n\treturn __trace_array_vprintk(tr->trace_buffer.buffer, ip, fmt, args);\n}\n\n__printf(3, 0)\nint trace_array_printk(struct trace_array *tr,\n\t\t       unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = trace_array_vprintk(tr, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\n__printf(3, 4)\nint trace_array_printk_buf(struct ring_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = __trace_array_vprintk(buffer, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\n__printf(2, 0)\nint trace_vprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\treturn trace_array_vprintk(&global_trace, ip, fmt, args);\n}\nEXPORT_SYMBOL_GPL(trace_vprintk);\n\nstatic void trace_iterator_increment(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, iter->cpu);\n\n\titer->idx++;\n\tif (buf_iter)\n\t\tring_buffer_read(buf_iter, NULL);\n}\n\nstatic struct trace_entry *\npeek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,\n\t\tunsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, cpu);\n\n\tif (buf_iter)\n\t\tevent = ring_buffer_iter_peek(buf_iter, ts);\n\telse\n\t\tevent = ring_buffer_peek(iter->trace_buffer->buffer, cpu, ts,\n\t\t\t\t\t lost_events);\n\n\tif (event) {\n\t\titer->ent_size = ring_buffer_event_length(event);\n\t\treturn ring_buffer_event_data(event);\n\t}\n\titer->ent_size = 0;\n\treturn NULL;\n}\n\nstatic struct trace_entry *\n__find_next_entry(struct trace_iterator *iter, int *ent_cpu,\n\t\t  unsigned long *missing_events, u64 *ent_ts)\n{\n\tstruct ring_buffer *buffer = iter->trace_buffer->buffer;\n\tstruct trace_entry *ent, *next = NULL;\n\tunsigned long lost_events = 0, next_lost = 0;\n\tint cpu_file = iter->cpu_file;\n\tu64 next_ts = 0, ts;\n\tint next_cpu = -1;\n\tint next_size = 0;\n\tint cpu;\n\n\t/*\n\t * If we are in a per_cpu trace file, don't bother by iterating over\n\t * all cpu and peek directly.\n\t */\n\tif (cpu_file > RING_BUFFER_ALL_CPUS) {\n\t\tif (ring_buffer_empty_cpu(buffer, cpu_file))\n\t\t\treturn NULL;\n\t\tent = peek_next_entry(iter, cpu_file, ent_ts, missing_events);\n\t\tif (ent_cpu)\n\t\t\t*ent_cpu = cpu_file;\n\n\t\treturn ent;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\n\t\tif (ring_buffer_empty_cpu(buffer, cpu))\n\t\t\tcontinue;\n\n\t\tent = peek_next_entry(iter, cpu, &ts, &lost_events);\n\n\t\t/*\n\t\t * Pick the entry with the smallest timestamp:\n\t\t */\n\t\tif (ent && (!next || ts < next_ts)) {\n\t\t\tnext = ent;\n\t\t\tnext_cpu = cpu;\n\t\t\tnext_ts = ts;\n\t\t\tnext_lost = lost_events;\n\t\t\tnext_size = iter->ent_size;\n\t\t}\n\t}\n\n\titer->ent_size = next_size;\n\n\tif (ent_cpu)\n\t\t*ent_cpu = next_cpu;\n\n\tif (ent_ts)\n\t\t*ent_ts = next_ts;\n\n\tif (missing_events)\n\t\t*missing_events = next_lost;\n\n\treturn next;\n}\n\n/* Find the next real entry, without updating the iterator itself */\nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts)\n{\n\treturn __find_next_entry(iter, ent_cpu, NULL, ent_ts);\n}\n\n/* Find the next real entry, and increment the iterator to the next entry */\nvoid *trace_find_next_entry_inc(struct trace_iterator *iter)\n{\n\titer->ent = __find_next_entry(iter, &iter->cpu,\n\t\t\t\t      &iter->lost_events, &iter->ts);\n\n\tif (iter->ent)\n\t\ttrace_iterator_increment(iter);\n\n\treturn iter->ent ? iter : NULL;\n}\n\nstatic void trace_consume(struct trace_iterator *iter)\n{\n\tring_buffer_consume(iter->trace_buffer->buffer, iter->cpu, &iter->ts,\n\t\t\t    &iter->lost_events);\n}\n\nstatic void *s_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tint i = (int)*pos;\n\tvoid *ent;\n\n\tWARN_ON_ONCE(iter->leftover);\n\n\t(*pos)++;\n\n\t/* can't go backwards */\n\tif (iter->idx > i)\n\t\treturn NULL;\n\n\tif (iter->idx < 0)\n\t\tent = trace_find_next_entry_inc(iter);\n\telse\n\t\tent = iter;\n\n\twhile (ent && iter->idx < i)\n\t\tent = trace_find_next_entry_inc(iter);\n\n\titer->pos = *pos;\n\n\treturn ent;\n}\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter;\n\tunsigned long entries = 0;\n\tu64 ts;\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = 0;\n\n\tbuf_iter = trace_buffer_iter(iter, cpu);\n\tif (!buf_iter)\n\t\treturn;\n\n\tring_buffer_iter_reset(buf_iter);\n\n\t/*\n\t * We could have the case with the max latency tracers\n\t * that a reset never took place on a cpu. This is evident\n\t * by the timestamp being before the start of the buffer.\n\t */\n\twhile ((event = ring_buffer_iter_peek(buf_iter, &ts))) {\n\t\tif (ts >= iter->trace_buffer->time_start)\n\t\t\tbreak;\n\t\tentries++;\n\t\tring_buffer_read(buf_iter, NULL);\n\t}\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = entries;\n}\n\n/*\n * The current tracer is copied to avoid a global locking\n * all around.\n */\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu_file = iter->cpu_file;\n\tvoid *p = NULL;\n\tloff_t l = 0;\n\tint cpu;\n\n\t/*\n\t * copy the tracer to avoid using a global lock all around.\n\t * iter->trace is a copy of current_trace, the pointer to the\n\t * name may be used instead of a strcmp(), as iter->trace->name\n\t * will point to the same string as current_trace->name.\n\t */\n\tmutex_lock(&trace_types_lock);\n\tif (unlikely(tr->current_trace && iter->trace->name != tr->current_trace->name))\n\t\t*iter->trace = *tr->current_trace;\n\tmutex_unlock(&trace_types_lock);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn ERR_PTR(-EBUSY);\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_inc(&trace_record_taskinfo_disabled);\n\n\tif (*pos != iter->pos) {\n\t\titer->ent = NULL;\n\t\titer->cpu = 0;\n\t\titer->idx = -1;\n\n\t\tif (cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tfor_each_tracing_cpu(cpu)\n\t\t\t\ttracing_iter_reset(iter, cpu);\n\t\t} else\n\t\t\ttracing_iter_reset(iter, cpu_file);\n\n\t\titer->leftover = 0;\n\t\tfor (p = iter; p && l < *pos; p = s_next(m, p, &l))\n\t\t\t;\n\n\t} else {\n\t\t/*\n\t\t * If we overflowed the seq_file before, then we want\n\t\t * to just reuse the trace_seq buffer again.\n\t\t */\n\t\tif (iter->leftover)\n\t\t\tp = iter;\n\t\telse {\n\t\t\tl = *pos - 1;\n\t\t\tp = s_next(m, p, &l);\n\t\t}\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(cpu_file);\n\treturn p;\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n{\n\tstruct trace_iterator *iter = m->private;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn;\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_dec(&trace_record_taskinfo_disabled);\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n}\n\nstatic void\nget_total_entries(struct trace_buffer *buf,\n\t\t  unsigned long *total, unsigned long *entries)\n{\n\tunsigned long count;\n\tint cpu;\n\n\t*total = 0;\n\t*entries = 0;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tcount = ring_buffer_entries_cpu(buf->buffer, cpu);\n\t\t/*\n\t\t * If this buffer has skipped entries, then we hold all\n\t\t * entries for the trace and we need to ignore the\n\t\t * ones before the time stamp.\n\t\t */\n\t\tif (per_cpu_ptr(buf->data, cpu)->skipped_entries) {\n\t\t\tcount -= per_cpu_ptr(buf->data, cpu)->skipped_entries;\n\t\t\t/* total is the same as the entries */\n\t\t\t*total += count;\n\t\t} else\n\t\t\t*total += count +\n\t\t\t\tring_buffer_overrun_cpu(buf->buffer, cpu);\n\t\t*entries += count;\n\t}\n}\n\nstatic void print_lat_help_header(struct seq_file *m)\n{\n\tseq_puts(m, \"#                  _------=> CPU#            \\n\"\n\t\t    \"#                 / _-----=> irqs-off        \\n\"\n\t\t    \"#                | / _----=> need-resched    \\n\"\n\t\t    \"#                || / _---=> hardirq/softirq \\n\"\n\t\t    \"#                ||| / _--=> preempt-depth   \\n\"\n\t\t    \"#                |||| /     delay            \\n\"\n\t\t    \"#  cmd     pid   ||||| time  |   caller      \\n\"\n\t\t    \"#     \\\\   /      |||||  \\\\    |   /         \\n\");\n}\n\nstatic void print_event_info(struct trace_buffer *buf, struct seq_file *m)\n{\n\tunsigned long total;\n\tunsigned long entries;\n\n\tget_total_entries(buf, &total, &entries);\n\tseq_printf(m, \"# entries-in-buffer/entries-written: %lu/%lu   #P:%d\\n\",\n\t\t   entries, total, num_online_cpus());\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void print_func_help_header(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t   unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#           TASK-PID   %s  CPU#   TIMESTAMP  FUNCTION\\n\", tgid ? \"TGID     \" : \"\");\n\tseq_printf(m, \"#              | |     %s    |       |         |\\n\",\t tgid ? \"  |      \" : \"\");\n}\n\nstatic void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t       unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\tconst char tgid_space[] = \"          \";\n\tconst char space[] = \"  \";\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#                          %s  _-----=> irqs-off\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s / _----=> need-resched\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s| / _---=> hardirq/softirq\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s|| / _--=> preempt-depth\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s||| /     delay\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#           TASK-PID %sCPU#  ||||    TIMESTAMP  FUNCTION\\n\",\n\t\t   tgid ? \"   TGID   \" : space);\n\tseq_printf(m, \"#              | |   %s  |   ||||       |         |\\n\",\n\t\t   tgid ? \"     |    \" : space);\n}\n\nvoid\nprint_trace_header(struct seq_file *m, struct trace_iterator *iter)\n{\n\tunsigned long sym_flags = (global_trace.trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_buffer *buf = iter->trace_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(buf->data, buf->cpu);\n\tstruct tracer *type = iter->trace;\n\tunsigned long entries;\n\tunsigned long total;\n\tconst char *name = \"preemption\";\n\n\tname = type->name;\n\n\tget_total_entries(buf, &total, &entries);\n\n\tseq_printf(m, \"# %s latency trace v1.1.5 on %s\\n\",\n\t\t   name, UTS_RELEASE);\n\tseq_puts(m, \"# -----------------------------------\"\n\t\t \"---------------------------------\\n\");\n\tseq_printf(m, \"# latency: %lu us, #%lu/%lu, CPU#%d |\"\n\t\t   \" (M:%s VP:%d, KP:%d, SP:%d HP:%d\",\n\t\t   nsecs_to_usecs(data->saved_latency),\n\t\t   entries,\n\t\t   total,\n\t\t   buf->cpu,\n#if defined(CONFIG_PREEMPT_NONE)\n\t\t   \"server\",\n#elif defined(CONFIG_PREEMPT_VOLUNTARY)\n\t\t   \"desktop\",\n#elif defined(CONFIG_PREEMPT)\n\t\t   \"preempt\",\n#else\n\t\t   \"unknown\",\n#endif\n\t\t   /* These are reserved for later use */\n\t\t   0, 0, 0, 0);\n#ifdef CONFIG_SMP\n\tseq_printf(m, \" #P:%d)\\n\", num_online_cpus());\n#else\n\tseq_puts(m, \")\\n\");\n#endif\n\tseq_puts(m, \"#    -----------------\\n\");\n\tseq_printf(m, \"#    | task: %.16s-%d \"\n\t\t   \"(uid:%d nice:%ld policy:%ld rt_prio:%ld)\\n\",\n\t\t   data->comm, data->pid,\n\t\t   from_kuid_munged(seq_user_ns(m), data->uid), data->nice,\n\t\t   data->policy, data->rt_priority);\n\tseq_puts(m, \"#    -----------------\\n\");\n\n\tif (data->critical_start) {\n\t\tseq_puts(m, \"#  => started at: \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_start, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#  => ended at:   \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_end, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#\\n\");\n\t}\n\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void test_cpu_buff_start(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}\n\nstatic enum print_line_t print_trace_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\ttest_cpu_buff_start(iter);\n\n\tevent = ftrace_find_event(entry->type);\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\t\ttrace_print_lat_context(iter);\n\t\telse\n\t\t\ttrace_print_context(iter);\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tif (event)\n\t\treturn event->funcs->trace(iter, sym_flags, event);\n\n\ttrace_seq_printf(s, \"Unknown type %d\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_raw_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO)\n\t\ttrace_seq_printf(s, \"%d %d %llu \",\n\t\t\t\t entry->pid, iter->cpu, iter->ts);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event)\n\t\treturn event->funcs->raw(iter, 0, event);\n\n\ttrace_seq_printf(s, \"%d ?\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_hex_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned char newline = '\\n';\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_HEX_FIELD(s, entry->pid);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event) {\n\t\tenum print_line_t ret = event->funcs->hex(iter, 0, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tSEQ_PUT_FIELD(s, newline);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_bin_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_FIELD(s, entry->pid);\n\t\tSEQ_PUT_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\treturn event ? event->funcs->binary(iter, 0, event) :\n\t\tTRACE_TYPE_HANDLED;\n}\n\nint trace_empty(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n/*  Called with trace_event_read_lock() held. */\nenum print_line_t print_trace_line(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\tenum print_line_t ret;\n\n\tif (iter->lost_events) {\n\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST %lu EVENTS]\\n\",\n\t\t\t\t iter->cpu, iter->lost_events);\n\t\tif (trace_seq_has_overflowed(&iter->seq))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tif (iter->trace && iter->trace->print_line) {\n\t\tret = iter->trace->print_line(iter);\n\t\tif (ret != TRACE_TYPE_UNHANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (iter->ent->type == TRACE_BPUTS &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bputs_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_BPRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bprintk_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_PRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_printk_msg_only(iter);\n\n\tif (trace_flags & TRACE_ITER_BIN)\n\t\treturn print_bin_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_HEX)\n\t\treturn print_hex_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_RAW)\n\t\treturn print_raw_fmt(iter);\n\n\treturn print_trace_fmt(iter);\n}\n\nvoid trace_latency_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t/* print nothing if the buffers are empty */\n\tif (trace_empty(iter))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\tprint_trace_header(m, iter);\n\n\tif (!(tr->trace_flags & TRACE_ITER_VERBOSE))\n\t\tprint_lat_help_header(m);\n}\n\nvoid trace_default_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\n\tif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT) {\n\t\t/* print nothing if the buffers are empty */\n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\t\tprint_trace_header(m, iter);\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE))\n\t\t\tprint_lat_help_header(m);\n\t} else {\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE)) {\n\t\t\tif (trace_flags & TRACE_ITER_IRQ_INFO)\n\t\t\t\tprint_func_help_header_irq(iter->trace_buffer,\n\t\t\t\t\t\t\t   m, trace_flags);\n\t\t\telse\n\t\t\t\tprint_func_help_header(iter->trace_buffer, m,\n\t\t\t\t\t\t       trace_flags);\n\t\t}\n\t}\n}\n\nstatic void test_ftrace_alive(struct seq_file *m)\n{\n\tif (!ftrace_is_dead())\n\t\treturn;\n\tseq_puts(m, \"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\"\n\t\t    \"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic void show_snapshot_main_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Clears and frees snapshot buffer\\n\"\n\t\t    \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer.\\n\"\n\t\t    \"# echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void show_snapshot_percpu_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Invalid for per_cpu snapshot file.\\n\");\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\tseq_puts(m, \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer for this cpu.\\n\");\n#else\n\tseq_puts(m, \"# echo 1 > snapshot : Not supported with this kernel.\\n\"\n\t\t    \"#                     Must use main snapshot file to allocate.\\n\");\n#endif\n\tseq_puts(m, \"# echo 2 > snapshot : Clears this cpu's snapshot buffer (but does not allocate)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter)\n{\n\tif (iter->tr->allocated_snapshot)\n\t\tseq_puts(m, \"#\\n# * Snapshot is allocated *\\n#\\n\");\n\telse\n\t\tseq_puts(m, \"#\\n# * Snapshot is freed *\\n#\\n\");\n\n\tseq_puts(m, \"# Snapshot commands:\\n\");\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\tshow_snapshot_main_help(m);\n\telse\n\t\tshow_snapshot_percpu_help(m);\n}\n#else\n/* Should never be called */\nstatic inline void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter) { }\n#endif\n\nstatic int s_show(struct seq_file *m, void *v)\n{\n\tstruct trace_iterator *iter = v;\n\tint ret;\n\n\tif (iter->ent == NULL) {\n\t\tif (iter->tr) {\n\t\t\tseq_printf(m, \"# tracer: %s\\n\", iter->trace->name);\n\t\t\tseq_puts(m, \"#\\n\");\n\t\t\ttest_ftrace_alive(m);\n\t\t}\n\t\tif (iter->snapshot && trace_empty(iter))\n\t\t\tprint_snapshot_help(m, iter);\n\t\telse if (iter->trace && iter->trace->print_header)\n\t\t\titer->trace->print_header(m);\n\t\telse\n\t\t\ttrace_default_header(m);\n\n\t} else if (iter->leftover) {\n\t\t/*\n\t\t * If we filled the seq_file buffer earlier, we\n\t\t * want to just show it now.\n\t\t */\n\t\tret = trace_print_seq(m, &iter->seq);\n\n\t\t/* ret should this time be zero, but you never know */\n\t\titer->leftover = ret;\n\n\t} else {\n\t\tprint_trace_line(iter);\n\t\tret = trace_print_seq(m, &iter->seq);\n\t\t/*\n\t\t * If we overflow the seq_file buffer, then it will\n\t\t * ask us for this data again at start up.\n\t\t * Use that instead.\n\t\t *  ret is 0 if seq_file write succeeded.\n\t\t *        -1 otherwise.\n\t\t */\n\t\titer->leftover = ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Should be used after trace_array_get(), trace_types_lock\n * ensures that i_cdev was already initialized.\n */\nstatic inline int tracing_get_cpu(struct inode *inode)\n{\n\tif (inode->i_cdev) /* See trace_create_cpu_file() */\n\t\treturn (long)inode->i_cdev - 1;\n\treturn RING_BUFFER_ALL_CPUS;\n}\n\nstatic const struct seq_operations tracer_seq_ops = {\n\t.start\t\t= s_start,\n\t.next\t\t= s_next,\n\t.stop\t\t= s_stop,\n\t.show\t\t= s_show,\n};\n\nstatic struct trace_iterator *\n__tracing_open(struct inode *inode, struct file *file, bool snapshot)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (tracing_disabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\titer = __seq_open_private(file, &tracer_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\titer->buffer_iter = kcalloc(nr_cpu_ids, sizeof(*iter->buffer_iter),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iter->buffer_iter)\n\t\tgoto release;\n\n\t/*\n\t * We make a copy of the current tracer to avoid concurrent\n\t * changes on it while we are reading.\n\t */\n\tmutex_lock(&trace_types_lock);\n\titer->trace = kzalloc(sizeof(*iter->trace), GFP_KERNEL);\n\tif (!iter->trace)\n\t\tgoto fail;\n\n\t*iter->trace = *tr->current_trace;\n\n\tif (!zalloc_cpumask_var(&iter->started, GFP_KERNEL))\n\t\tgoto fail;\n\n\titer->tr = tr;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t/* Currently only the top directory has a snapshot */\n\tif (tr->current_trace->print_max || snapshot)\n\t\titer->trace_buffer = &tr->max_buffer;\n\telse\n#endif\n\t\titer->trace_buffer = &tr->trace_buffer;\n\titer->snapshot = snapshot;\n\titer->pos = -1;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\n\t/* Notify the tracer early; before we stop tracing. */\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t/* stop the trace while dumping if we are not opening \"snapshot\" */\n\tif (!iter->snapshot)\n\t\ttracing_stop_tr(tr);\n\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\titer->buffer_iter[cpu] =\n\t\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\t}\n\t\tring_buffer_read_prepare_sync();\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\t\ttracing_iter_reset(iter, cpu);\n\t\t}\n\t} else {\n\t\tcpu = iter->cpu_file;\n\t\titer->buffer_iter[cpu] =\n\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\tring_buffer_read_prepare_sync();\n\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\ttracing_iter_reset(iter, cpu);\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn iter;\n\n fail:\n\tmutex_unlock(&trace_types_lock);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\nrelease:\n\tseq_release_private(inode, file);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint tracing_open_generic(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nbool tracing_is_disabled(void)\n{\n\treturn (tracing_disabled) ? true: false;\n}\n\n/*\n * Open and update trace_array ref count.\n * Must have the current trace_array passed to it.\n */\nstatic int tracing_open_generic_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\nstatic int tracing_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}\n\nstatic int tracing_release_generic_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn 0;\n}\n\nstatic int tracing_single_release_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn single_release(inode, file);\n}\n\nstatic int tracing_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\t/* If this file was open for write, then erase contents */\n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) {\n\t\tint cpu = tracing_get_cpu(inode);\n\t\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tif (tr->current_trace->print_max)\n\t\t\ttrace_buf = &tr->max_buffer;\n#endif\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\t\ttracing_reset_online_cpus(trace_buf);\n\t\telse\n\t\t\ttracing_reset(trace_buf, cpu);\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, false);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t\telse if (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\t}\n\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\n/*\n * Some tracers are not suitable for instance buffers.\n * A tracer is always available for the global array (toplevel)\n * or if it explicitly states that it is.\n */\nstatic bool\ntrace_ok_for_array(struct tracer *t, struct trace_array *tr)\n{\n\treturn (tr->flags & TRACE_ARRAY_FL_GLOBAL) || t->allow_instances;\n}\n\n/* Find the next tracer that this trace array may use */\nstatic struct tracer *\nget_tracer_for_array(struct trace_array *tr, struct tracer *t)\n{\n\twhile (t && !trace_ok_for_array(t, tr))\n\t\tt = t->next;\n\n\treturn t;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t = v;\n\n\t(*pos)++;\n\n\tif (t)\n\t\tt = get_tracer_for_array(tr, t->next);\n\n\treturn t;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tt = get_tracer_for_array(tr, trace_types);\n\tfor (; t && l < *pos; t = t_next(m, t, &l))\n\t\t\t;\n\n\treturn t;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct tracer *t = v;\n\n\tif (!t)\n\t\treturn 0;\n\n\tseq_puts(m, t->name);\n\tif (t->next)\n\t\tseq_putc(m, ' ');\n\telse\n\t\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_traces_seq_ops = {\n\t.start\t\t= t_start,\n\t.next\t\t= t_next,\n\t.stop\t\t= t_stop,\n\t.show\t\t= t_show,\n};\n\nstatic int show_traces_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_write_stub(struct file *filp, const char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence)\n{\n\tint ret;\n\n\tif (file->f_mode & FMODE_READ)\n\t\tret = seq_lseek(file, offset, whence);\n\telse\n\t\tfile->f_pos = ret = 0;\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_fops = {\n\t.open\t\t= tracing_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_write_stub,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_release,\n};\n\nstatic const struct file_operations show_traces_fops = {\n\t.open\t\t= show_traces_open,\n\t.read\t\t= seq_read,\n\t.release\t= seq_release,\n\t.llseek\t\t= seq_lseek,\n};\n\nstatic ssize_t\ntracing_cpumask_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tchar *mask_str;\n\tint len;\n\n\tlen = snprintf(NULL, 0, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask)) + 1;\n\tmask_str = kmalloc(len, GFP_KERNEL);\n\tif (!mask_str)\n\t\treturn -ENOMEM;\n\n\tlen = snprintf(mask_str, len, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask));\n\tif (len >= count) {\n\t\tcount = -EINVAL;\n\t\tgoto out_err;\n\t}\n\tcount = simple_read_from_buffer(ubuf, count, ppos, mask_str, len);\n\nout_err:\n\tkfree(mask_str);\n\n\treturn count;\n}\n\nstatic ssize_t\ntracing_cpumask_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tcpumask_var_t tracing_cpumask_new;\n\tint err, cpu;\n\n\tif (!alloc_cpumask_var(&tracing_cpumask_new, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\terr = cpumask_parse_user(ubuf, count, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\t/*\n\t\t * Increase/decrease the disabled counter if we are\n\t\t * about to flip a bit in the cpumask:\n\t\t */\n\t\tif (cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\t!cpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_inc(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_disable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t\tif (!cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\tcpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_dec(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_enable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t}\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tcpumask_copy(tr->tracing_cpumask, tracing_cpumask_new);\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn count;\n\nerr_unlock:\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn err;\n}\n\nstatic const struct file_operations tracing_cpumask_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_cpumask_read,\n\t.write\t\t= tracing_cpumask_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic int tracing_trace_options_show(struct seq_file *m, void *v)\n{\n\tstruct tracer_opt *trace_opts;\n\tstruct trace_array *tr = m->private;\n\tu32 tracer_flags;\n\tint i;\n\n\tmutex_lock(&trace_types_lock);\n\ttracer_flags = tr->current_trace->flags->val;\n\ttrace_opts = tr->current_trace->flags->opts;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (tr->trace_flags & (1 << i))\n\t\t\tseq_printf(m, \"%s\\n\", trace_options[i]);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_options[i]);\n\t}\n\n\tfor (i = 0; trace_opts[i].name; i++) {\n\t\tif (tracer_flags & trace_opts[i].bit)\n\t\t\tseq_printf(m, \"%s\\n\", trace_opts[i].name);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_opts[i].name);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int __set_tracer_option(struct trace_array *tr,\n\t\t\t       struct tracer_flags *tracer_flags,\n\t\t\t       struct tracer_opt *opts, int neg)\n{\n\tstruct tracer *trace = tracer_flags->trace;\n\tint ret;\n\n\tret = trace->set_flag(tr, tracer_flags->val, opts->bit, !neg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (neg)\n\t\ttracer_flags->val &= ~opts->bit;\n\telse\n\t\ttracer_flags->val |= opts->bit;\n\treturn 0;\n}\n\n/* Try to assign a tracer specific option */\nstatic int set_tracer_option(struct trace_array *tr, char *cmp, int neg)\n{\n\tstruct tracer *trace = tr->current_trace;\n\tstruct tracer_flags *tracer_flags = trace->flags;\n\tstruct tracer_opt *opts = NULL;\n\tint i;\n\n\tfor (i = 0; tracer_flags->opts[i].name; i++) {\n\t\topts = &tracer_flags->opts[i];\n\n\t\tif (strcmp(cmp, opts->name) == 0)\n\t\t\treturn __set_tracer_option(tr, trace->flags, opts, neg);\n\t}\n\n\treturn -EINVAL;\n}\n\n/* Some tracers require overwrite to stay enabled */\nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set)\n{\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled)\n{\n\t/* do nothing if flag is already set */\n\tif (!!(tr->trace_flags & mask) == !!enabled)\n\t\treturn 0;\n\n\t/* Give the tracer a chance to approve the change */\n\tif (tr->current_trace->flag_changed)\n\t\tif (tr->current_trace->flag_changed(tr, mask, !!enabled))\n\t\t\treturn -EINVAL;\n\n\tif (enabled)\n\t\ttr->trace_flags |= mask;\n\telse\n\t\ttr->trace_flags &= ~mask;\n\n\tif (mask == TRACE_ITER_RECORD_CMD)\n\t\ttrace_event_enable_cmd_record(enabled);\n\n\tif (mask == TRACE_ITER_RECORD_TGID) {\n\t\tif (!tgid_map)\n\t\t\ttgid_map = kcalloc(PID_MAX_DEFAULT + 1,\n\t\t\t\t\t   sizeof(*tgid_map),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tgid_map) {\n\t\t\ttr->trace_flags &= ~TRACE_ITER_RECORD_TGID;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttrace_event_enable_tgid_record(enabled);\n\t}\n\n\tif (mask == TRACE_ITER_EVENT_FORK)\n\t\ttrace_event_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_FUNC_FORK)\n\t\tftrace_pid_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_OVERWRITE) {\n\t\tring_buffer_change_overwrite(tr->trace_buffer.buffer, enabled);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tring_buffer_change_overwrite(tr->max_buffer.buffer, enabled);\n#endif\n\t}\n\n\tif (mask == TRACE_ITER_PRINTK) {\n\t\ttrace_printk_start_stop_comm(enabled);\n\t\ttrace_printk_control(enabled);\n\t}\n\n\treturn 0;\n}\n\nstatic int trace_set_options(struct trace_array *tr, char *option)\n{\n\tchar *cmp;\n\tint neg = 0;\n\tint ret;\n\tsize_t orig_len = strlen(option);\n\tint len;\n\n\tcmp = strstrip(option);\n\n\tlen = str_has_prefix(cmp, \"no\");\n\tif (len)\n\t\tneg = 1;\n\n\tcmp += len;\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = match_string(trace_options, -1, cmp);\n\t/* If no option could be set, test the specific tracer options */\n\tif (ret < 0)\n\t\tret = set_tracer_option(tr, cmp, neg);\n\telse\n\t\tret = set_tracer_flag(tr, 1 << ret, !neg);\n\n\tmutex_unlock(&trace_types_lock);\n\n\t/*\n\t * If the first trailing whitespace is replaced with '\\0' by strstrip,\n\t * turn it back into a space.\n\t */\n\tif (orig_len > strlen(option))\n\t\toption[strlen(option)] = ' ';\n\n\treturn ret;\n}\n\nstatic void __init apply_trace_boot_options(void)\n{\n\tchar *buf = trace_boot_options_buf;\n\tchar *option;\n\n\twhile (true) {\n\t\toption = strsep(&buf, \",\");\n\n\t\tif (!option)\n\t\t\tbreak;\n\n\t\tif (*option)\n\t\t\ttrace_set_options(&global_trace, option);\n\n\t\t/* Put back the comma to allow this to be called again */\n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic ssize_t\ntracing_trace_options_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tret = trace_set_options(tr, buf);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_trace_options_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_trace_options_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_iter_fops = {\n\t.open\t\t= tracing_trace_options_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_trace_options_write,\n};\n\nstatic const char readme_msg[] =\n\t\"tracing mini-HOWTO:\\n\\n\"\n\t\"# echo 0 > tracing_on : quick way to disable tracing\\n\"\n\t\"# echo 1 > tracing_on : quick way to re-enable tracing\\n\\n\"\n\t\" Important files:\\n\"\n\t\"  trace\\t\\t\\t- The static contents of the buffer\\n\"\n\t\"\\t\\t\\t  To clear the buffer write into this file: echo > trace\\n\"\n\t\"  trace_pipe\\t\\t- A consuming read to see the contents of the buffer\\n\"\n\t\"  current_tracer\\t- function and latency tracers\\n\"\n\t\"  available_tracers\\t- list of configured tracers for current_tracer\\n\"\n\t\"  buffer_size_kb\\t- view and modify size of per cpu buffer\\n\"\n\t\"  buffer_total_size_kb  - view total size of all cpu buffers\\n\\n\"\n\t\"  trace_clock\\t\\t-change the clock used to order events\\n\"\n\t\"       local:   Per cpu clock but may not be synced across CPUs\\n\"\n\t\"      global:   Synced across CPUs but slows tracing down.\\n\"\n\t\"     counter:   Not a clock, but just an increment\\n\"\n\t\"      uptime:   Jiffy counter from time of boot\\n\"\n\t\"        perf:   Same clock that perf events use\\n\"\n#ifdef CONFIG_X86_64\n\t\"     x86-tsc:   TSC cycle counter\\n\"\n#endif\n\t\"\\n  timestamp_mode\\t-view the mode used to timestamp events\\n\"\n\t\"       delta:   Delta difference against a buffer-wide timestamp\\n\"\n\t\"    absolute:   Absolute (standalone) timestamp\\n\"\n\t\"\\n  trace_marker\\t\\t- Writes into this file writes into the kernel buffer\\n\"\n\t\"\\n  trace_marker_raw\\t\\t- Writes into this file writes binary data into the kernel buffer\\n\"\n\t\"  tracing_cpumask\\t- Limit which CPUs to trace\\n\"\n\t\"  instances\\t\\t- Make sub-buffers with: mkdir instances/foo\\n\"\n\t\"\\t\\t\\t  Remove sub-buffer with rmdir\\n\"\n\t\"  trace_options\\t\\t- Set format or modify how tracing happens\\n\"\n\t\"\\t\\t\\t  Disable an option by adding a suffix 'no' to the\\n\"\n\t\"\\t\\t\\t  option name\\n\"\n\t\"  saved_cmdlines_size\\t- echo command number in here to store comm-pid list\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"\\n  available_filter_functions - list of functions that can be filtered on\\n\"\n\t\"  set_ftrace_filter\\t- echo function name in here to only trace these\\n\"\n\t\"\\t\\t\\t  functions\\n\"\n\t\"\\t     accepts: func_full_name or glob-matching-pattern\\n\"\n\t\"\\t     modules: Can select a group via module\\n\"\n\t\"\\t      Format: :mod:<module-name>\\n\"\n\t\"\\t     example: echo :mod:ext3 > set_ftrace_filter\\n\"\n\t\"\\t    triggers: a command to perform when function is hit\\n\"\n\t\"\\t      Format: <function>:<trigger>[:count]\\n\"\n\t\"\\t     trigger: traceon, traceoff\\n\"\n\t\"\\t\\t      enable_event:<system>:<event>\\n\"\n\t\"\\t\\t      disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t      stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t      snapshot\\n\"\n#endif\n\t\"\\t\\t      dump\\n\"\n\t\"\\t\\t      cpudump\\n\"\n\t\"\\t     example: echo do_fault:traceoff > set_ftrace_filter\\n\"\n\t\"\\t              echo do_trap:traceoff:3 > set_ftrace_filter\\n\"\n\t\"\\t     The first one will disable tracing every time do_fault is hit\\n\"\n\t\"\\t     The second will disable tracing at most 3 times when do_trap is hit\\n\"\n\t\"\\t       The first time do trap is hit and it disables tracing, the\\n\"\n\t\"\\t       counter will decrement to 2. If tracing is already disabled,\\n\"\n\t\"\\t       the counter will not decrement. It only decrements when the\\n\"\n\t\"\\t       trigger did work\\n\"\n\t\"\\t     To remove trigger without count:\\n\"\n\t\"\\t       echo '!<function>:<trigger> > set_ftrace_filter\\n\"\n\t\"\\t     To remove trigger with a count:\\n\"\n\t\"\\t       echo '!<function>:<trigger>:0 > set_ftrace_filter\\n\"\n\t\"  set_ftrace_notrace\\t- echo function name in here to never trace.\\n\"\n\t\"\\t    accepts: func_full_name, *func_end, func_begin*, *func_middle*\\n\"\n\t\"\\t    modules: Can select a group via module command :mod:\\n\"\n\t\"\\t    Does not accept triggers\\n\"\n#endif /* CONFIG_DYNAMIC_FTRACE */\n#ifdef CONFIG_FUNCTION_TRACER\n\t\"  set_ftrace_pid\\t- Write pid(s) to only function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n#endif\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\"  set_graph_function\\t- Trace the nested calls of a function (function_graph)\\n\"\n\t\"  set_graph_notrace\\t- Do not trace the nested calls of a function (function_graph)\\n\"\n\t\"  max_graph_depth\\t- Trace a limited depth of nested calls (0 is unlimited)\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\n  snapshot\\t\\t- Like 'trace' but shows the content of the static\\n\"\n\t\"\\t\\t\\t  snapshot buffer. Read the contents for more\\n\"\n\t\"\\t\\t\\t  information\\n\"\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t\"  stack_trace\\t\\t- Shows the max stack trace when active\\n\"\n\t\"  stack_max_size\\t- Shows current max stack size that was traced\\n\"\n\t\"\\t\\t\\t  Write into this file to reset the max size (trigger a\\n\"\n\t\"\\t\\t\\t  new trace)\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"  stack_trace_filter\\t- Like set_ftrace_filter but limits what stack_trace\\n\"\n\t\"\\t\\t\\t  traces\\n\"\n#endif\n#endif /* CONFIG_STACK_TRACER */\n#ifdef CONFIG_DYNAMIC_EVENTS\n\t\"  dynamic_events\\t\\t- Add/remove/show the generic dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"  kprobe_events\\t\\t- Add/remove/show the kernel dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"  uprobe_events\\t\\t- Add/remove/show the userspace dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n\t\"\\t  accepts: event-definitions (one definition per line)\\n\"\n\t\"\\t   Format: p[:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           r[maxactive][:[<group>/]<event>] <place> [<args>]\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t           s:[synthetic/]<event> <field> [<field>]\\n\"\n#endif\n\t\"\\t           -:[<group>/]<event>\\n\"\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"\\t    place: [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n  \"place (kretprobe): [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n  \"   place (uprobe): <path>:<offset>[(ref_ctr_offset)]\\n\"\n#endif\n\t\"\\t     args: <name>=fetcharg[:type]\\n\"\n\t\"\\t fetcharg: %<register>, @<address>, @<symbol>[+|-<offset>],\\n\"\n#ifdef CONFIG_HAVE_FUNCTION_ARG_ACCESS_API\n\t\"\\t           $stack<index>, $stack, $retval, $comm, $arg<N>\\n\"\n#else\n\t\"\\t           $stack<index>, $stack, $retval, $comm\\n\"\n#endif\n\t\"\\t     type: s8/16/32/64, u8/16/32/64, x8/16/32/64, string, symbol,\\n\"\n\t\"\\t           b<bit-width>@<bit-offset>/<container-size>,\\n\"\n\t\"\\t           <type>\\\\[<array-size>\\\\]\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t    field: <stype> <name>;\\n\"\n\t\"\\t    stype: u8/u16/u32/u64, s8/s16/s32/s64, pid_t,\\n\"\n\t\"\\t           [unsigned] char/int/long\\n\"\n#endif\n#endif\n\t\"  events/\\t\\t- Directory containing all trace event subsystems:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all events\\n\"\n\t\"  events/<system>/\\t- Directory containing all trace events for <system>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all <system>\\n\"\n\t\"\\t\\t\\t  events\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"  events/<system>/<event>/\\t- Directory containing control files for\\n\"\n\t\"\\t\\t\\t  <event>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of <event>\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"      trigger\\t\\t- If set, a command to perform when event is hit\\n\"\n\t\"\\t    Format: <trigger>[:count][if <filter>]\\n\"\n\t\"\\t   trigger: traceon, traceoff\\n\"\n\t\"\\t            enable_event:<system>:<event>\\n\"\n\t\"\\t            disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t            enable_hist:<system>:<event>\\n\"\n\t\"\\t            disable_hist:<system>:<event>\\n\"\n#endif\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t    stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t    snapshot\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t\\t    hist (see below)\\n\"\n#endif\n\t\"\\t   example: echo traceoff > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo traceoff:3 > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo 'enable_event:kmem:kmalloc:3 if nr_rq > 1' > \\\\\\n\"\n\t\"\\t                  events/block/block_unplug/trigger\\n\"\n\t\"\\t   The first disables tracing every time block_unplug is hit.\\n\"\n\t\"\\t   The second disables tracing the first 3 times block_unplug is hit.\\n\"\n\t\"\\t   The third enables the kmalloc event the first 3 times block_unplug\\n\"\n\t\"\\t     is hit and has value of greater than 1 for the 'nr_rq' event field.\\n\"\n\t\"\\t   Like function triggers, the counter is only decremented if it\\n\"\n\t\"\\t    enabled or disabled tracing.\\n\"\n\t\"\\t   To remove a trigger without a count:\\n\"\n\t\"\\t     echo '!<trigger> > <system>/<event>/trigger\\n\"\n\t\"\\t   To remove a trigger with a count:\\n\"\n\t\"\\t     echo '!<trigger>:0 > <system>/<event>/trigger\\n\"\n\t\"\\t   Filters can be ignored when removing a trigger.\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"      hist trigger\\t- If set, event hits are aggregated into a hash table\\n\"\n\t\"\\t    Format: hist:keys=<field1[,field2,...]>\\n\"\n\t\"\\t            [:values=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:sort=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:size=#entries]\\n\"\n\t\"\\t            [:pause][:continue][:clear]\\n\"\n\t\"\\t            [:name=histname1]\\n\"\n\t\"\\t            [if <filter>]\\n\\n\"\n\t\"\\t    When a matching event is hit, an entry is added to a hash\\n\"\n\t\"\\t    table using the key(s) and value(s) named, and the value of a\\n\"\n\t\"\\t    sum called 'hitcount' is incremented.  Keys and values\\n\"\n\t\"\\t    correspond to fields in the event's format description.  Keys\\n\"\n\t\"\\t    can be any field, or the special string 'stacktrace'.\\n\"\n\t\"\\t    Compound keys consisting of up to two fields can be specified\\n\"\n\t\"\\t    by the 'keys' keyword.  Values must correspond to numeric\\n\"\n\t\"\\t    fields.  Sort keys consisting of up to two fields can be\\n\"\n\t\"\\t    specified using the 'sort' keyword.  The sort direction can\\n\"\n\t\"\\t    be modified by appending '.descending' or '.ascending' to a\\n\"\n\t\"\\t    sort field.  The 'size' parameter can be used to specify more\\n\"\n\t\"\\t    or fewer than the default 2048 entries for the hashtable size.\\n\"\n\t\"\\t    If a hist trigger is given a name using the 'name' parameter,\\n\"\n\t\"\\t    its histogram data will be shared with other triggers of the\\n\"\n\t\"\\t    same name, and trigger hits will update this common data.\\n\\n\"\n\t\"\\t    Reading the 'hist' file for the event will dump the hash\\n\"\n\t\"\\t    table in its entirety to stdout.  If there are multiple hist\\n\"\n\t\"\\t    triggers attached to an event, there will be a table for each\\n\"\n\t\"\\t    trigger in the output.  The table displayed for a named\\n\"\n\t\"\\t    trigger will be the same as any other instance having the\\n\"\n\t\"\\t    same name.  The default format used to display a given field\\n\"\n\t\"\\t    can be modified by appending any of the following modifiers\\n\"\n\t\"\\t    to the field name, as applicable:\\n\\n\"\n\t\"\\t            .hex        display a number as a hex value\\n\"\n\t\"\\t            .sym        display an address as a symbol\\n\"\n\t\"\\t            .sym-offset display an address as a symbol and offset\\n\"\n\t\"\\t            .execname   display a common_pid as a program name\\n\"\n\t\"\\t            .syscall    display a syscall id as a syscall name\\n\"\n\t\"\\t            .log2       display log2 value rather than raw number\\n\"\n\t\"\\t            .usecs      display a common_timestamp in microseconds\\n\\n\"\n\t\"\\t    The 'pause' parameter can be used to pause an existing hist\\n\"\n\t\"\\t    trigger or to start a hist trigger but not log any events\\n\"\n\t\"\\t    until told to do so.  'continue' can be used to start or\\n\"\n\t\"\\t    restart a paused hist trigger.\\n\\n\"\n\t\"\\t    The 'clear' parameter will clear the contents of a running\\n\"\n\t\"\\t    hist trigger and leave its current paused/active state\\n\"\n\t\"\\t    unchanged.\\n\\n\"\n\t\"\\t    The enable_hist and disable_hist triggers can be used to\\n\"\n\t\"\\t    have one event conditionally start and stop another event's\\n\"\n\t\"\\t    already-attached hist trigger.  The syntax is analagous to\\n\"\n\t\"\\t    the enable_event and disable_event triggers.\\n\"\n#endif\n;\n\nstatic ssize_t\ntracing_readme_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\treadme_msg, strlen(readme_msg));\n}\n\nstatic const struct file_operations tracing_readme_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_readme_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic void *saved_tgids_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr <= &tgid_map[PID_MAX_DEFAULT]; ptr++) {\n\t\tif (trace_find_tgid(*ptr))\n\t\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_tgids_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tif (!tgid_map)\n\t\treturn NULL;\n\n\tv = &tgid_map[0];\n\twhile (l <= *pos) {\n\t\tv = saved_tgids_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_tgids_stop(struct seq_file *m, void *v)\n{\n}\n\nstatic int saved_tgids_show(struct seq_file *m, void *v)\n{\n\tint pid = (int *)v - tgid_map;\n\n\tseq_printf(m, \"%d %d\\n\", pid, trace_find_tgid(pid));\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_tgids_seq_ops = {\n\t.start\t\t= saved_tgids_start,\n\t.stop\t\t= saved_tgids_stop,\n\t.next\t\t= saved_tgids_next,\n\t.show\t\t= saved_tgids_show,\n};\n\nstatic int tracing_saved_tgids_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_tgids_seq_ops);\n}\n\n\nstatic const struct file_operations tracing_saved_tgids_fops = {\n\t.open\t\t= tracing_saved_tgids_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void *saved_cmdlines_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunsigned int *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr < &savedcmd->map_cmdline_to_pid[savedcmd->cmdline_num];\n\t     ptr++) {\n\t\tif (*ptr == -1 || *ptr == NO_CMDLINE_MAP)\n\t\t\tcontinue;\n\n\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_cmdlines_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_cmdlines_stop(struct seq_file *m, void *v)\n{\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int saved_cmdlines_show(struct seq_file *m, void *v)\n{\n\tchar buf[TASK_COMM_LEN];\n\tunsigned int *pid = v;\n\n\t__trace_find_cmdline(*pid, buf);\n\tseq_printf(m, \"%d %s\\n\", *pid, buf);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_cmdlines_seq_ops = {\n\t.start\t\t= saved_cmdlines_start,\n\t.next\t\t= saved_cmdlines_next,\n\t.stop\t\t= saved_cmdlines_stop,\n\t.show\t\t= saved_cmdlines_show,\n};\n\nstatic int tracing_saved_cmdlines_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_cmdlines_seq_ops);\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_fops = {\n\t.open\t\t= tracing_saved_cmdlines_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic ssize_t\ntracing_saved_cmdlines_size_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tr = scnprintf(buf, sizeof(buf), \"%u\\n\", savedcmd->cmdline_num);\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic void free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s)\n{\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}\n\nstatic int tracing_resize_saved_cmdlines(unsigned int val)\n{\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_saved_cmdlines_size_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry or less than PID_MAX_DEFAULT */\n\tif (!val || val > PID_MAX_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = tracing_resize_saved_cmdlines((unsigned int)val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_size_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_saved_cmdlines_size_read,\n\t.write\t\t= tracing_saved_cmdlines_size_write,\n};\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic union trace_eval_map_item *\nupdate_eval_map(union trace_eval_map_item *ptr)\n{\n\tif (!ptr->map.eval_string) {\n\t\tif (ptr->tail.next) {\n\t\t\tptr = ptr->tail.next;\n\t\t\t/* Set ptr to the next real item (skip head) */\n\t\t\tptr++;\n\t\t} else\n\t\t\treturn NULL;\n\t}\n\treturn ptr;\n}\n\nstatic void *eval_map_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\t/*\n\t * Paranoid! If ptr points to end, we don't want to increment past it.\n\t * This really should never happen.\n\t */\n\tptr = update_eval_map(ptr);\n\tif (WARN_ON_ONCE(!ptr))\n\t\treturn NULL;\n\n\tptr++;\n\n\t(*pos)++;\n\n\tptr = update_eval_map(ptr);\n\n\treturn ptr;\n}\n\nstatic void *eval_map_start(struct seq_file *m, loff_t *pos)\n{\n\tunion trace_eval_map_item *v;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tv = trace_eval_maps;\n\tif (v)\n\t\tv++;\n\n\twhile (v && l < *pos) {\n\t\tv = eval_map_next(m, v, &l);\n\t}\n\n\treturn v;\n}\n\nstatic void eval_map_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic int eval_map_show(struct seq_file *m, void *v)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\tseq_printf(m, \"%s %ld (%s)\\n\",\n\t\t   ptr->map.eval_string, ptr->map.eval_value,\n\t\t   ptr->map.system);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_eval_map_seq_ops = {\n\t.start\t\t= eval_map_start,\n\t.next\t\t= eval_map_next,\n\t.stop\t\t= eval_map_stop,\n\t.show\t\t= eval_map_show,\n};\n\nstatic int tracing_eval_map_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_eval_map_seq_ops);\n}\n\nstatic const struct file_operations tracing_eval_map_fops = {\n\t.open\t\t= tracing_eval_map_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic inline union trace_eval_map_item *\ntrace_eval_jmp_to_tail(union trace_eval_map_item *ptr)\n{\n\t/* Return tail of array given the head */\n\treturn ptr + ptr->head.length + 1;\n}\n\nstatic void\ntrace_insert_eval_map_file(struct module *mod, struct trace_eval_map **start,\n\t\t\t   int len)\n{\n\tstruct trace_eval_map **stop;\n\tstruct trace_eval_map **map;\n\tunion trace_eval_map_item *map_array;\n\tunion trace_eval_map_item *ptr;\n\n\tstop = start + len;\n\n\t/*\n\t * The trace_eval_maps contains the map plus a head and tail item,\n\t * where the head holds the module and length of array, and the\n\t * tail holds a pointer to the next list.\n\t */\n\tmap_array = kmalloc_array(len + 2, sizeof(*map_array), GFP_KERNEL);\n\tif (!map_array) {\n\t\tpr_warn(\"Unable to allocate trace eval mapping\\n\");\n\t\treturn;\n\t}\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tif (!trace_eval_maps)\n\t\ttrace_eval_maps = map_array;\n\telse {\n\t\tptr = trace_eval_maps;\n\t\tfor (;;) {\n\t\t\tptr = trace_eval_jmp_to_tail(ptr);\n\t\t\tif (!ptr->tail.next)\n\t\t\t\tbreak;\n\t\t\tptr = ptr->tail.next;\n\n\t\t}\n\t\tptr->tail.next = map_array;\n\t}\n\tmap_array->head.mod = mod;\n\tmap_array->head.length = len;\n\tmap_array++;\n\n\tfor (map = start; (unsigned long)map < (unsigned long)stop; map++) {\n\t\tmap_array->map = **map;\n\t\tmap_array++;\n\t}\n\tmemset(map_array, 0, sizeof(*map_array));\n\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic void trace_create_eval_file(struct dentry *d_tracer)\n{\n\ttrace_create_file(\"eval_map\", 0444, d_tracer,\n\t\t\t  NULL, &tracing_eval_map_fops);\n}\n\n#else /* CONFIG_TRACE_EVAL_MAP_FILE */\nstatic inline void trace_create_eval_file(struct dentry *d_tracer) { }\nstatic inline void trace_insert_eval_map_file(struct module *mod,\n\t\t\t      struct trace_eval_map **start, int len) { }\n#endif /* !CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic void trace_insert_eval_map(struct module *mod,\n\t\t\t\t  struct trace_eval_map **start, int len)\n{\n\tstruct trace_eval_map **map;\n\n\tif (len <= 0)\n\t\treturn;\n\n\tmap = start;\n\n\ttrace_event_eval_update(map, len);\n\n\ttrace_insert_eval_map_file(mod, start, len);\n}\n\nstatic ssize_t\ntracing_set_trace_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+2];\n\tint r;\n\n\tmutex_lock(&trace_types_lock);\n\tr = sprintf(buf, \"%s\\n\", tr->current_trace->name);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr)\n{\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\treturn t->init(tr);\n}\n\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val)\n{\n\tint cpu;\n\n\tfor_each_tracing_cpu(cpu)\n\t\tper_cpu_ptr(buf->data, cpu)->entries = val;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/* resize @tr's buffer to the size of @size_tr's entries */\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id)\n{\n\tint cpu, ret = 0;\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu)->entries, cpu);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tper_cpu_ptr(trace_buf->data, cpu)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu)->entries;\n\t\t}\n\t} else {\n\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu_id)->entries, cpu_id);\n\t\tif (ret == 0)\n\t\t\tper_cpu_ptr(trace_buf->data, cpu_id)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu_id)->entries;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int __tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\tunsigned long size, int cpu)\n{\n\tint ret;\n\n\t/*\n\t * If kernel or user changes the size of the ring buffer\n\t * we use the size that was given, and we can forget about\n\t * expanding it later.\n\t */\n\tring_buffer_expanded = true;\n\n\t/* May be called before buffers are initialized */\n\tif (!tr->trace_buffer.buffer)\n\t\treturn 0;\n\n\tret = ring_buffer_resize(tr->trace_buffer.buffer, size, cpu);\n\tif (ret < 0)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (!(tr->flags & TRACE_ARRAY_FL_GLOBAL) ||\n\t    !tr->current_trace->use_max_tr)\n\t\tgoto out;\n\n\tret = ring_buffer_resize(tr->max_buffer.buffer, size, cpu);\n\tif (ret < 0) {\n\t\tint r = resize_buffer_duplicate_size(&tr->trace_buffer,\n\t\t\t\t\t\t     &tr->trace_buffer, cpu);\n\t\tif (r < 0) {\n\t\t\t/*\n\t\t\t * AARGH! We are left with different\n\t\t\t * size max buffer!!!!\n\t\t\t * The max buffer is our \"snapshot\" buffer.\n\t\t\t * When a tracer needs a snapshot (one of the\n\t\t\t * latency tracers), it swaps the max buffer\n\t\t\t * with the saved snap shot. We succeeded to\n\t\t\t * update the size of the main buffer, but failed to\n\t\t\t * update the size of the max buffer. But when we tried\n\t\t\t * to reset the main buffer to the original size, we\n\t\t\t * failed there too. This is very unlikely to\n\t\t\t * happen, but if it does, warn and kill all\n\t\t\t * tracing.\n\t\t\t */\n\t\t\tWARN_ON(1);\n\t\t\ttracing_disabled = 1;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->max_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->max_buffer.data, cpu)->entries = size;\n\n out:\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->trace_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->trace_buffer.data, cpu)->entries = size;\n\n\treturn ret;\n}\n\nstatic ssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\t  unsigned long size, int cpu_id)\n{\n\tint ret = size;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu_id != RING_BUFFER_ALL_CPUS) {\n\t\t/* make sure, this cpu is enabled in the mask */\n\t\tif (!cpumask_test_cpu(cpu_id, tracing_buffer_mask)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = __tracing_resize_ring_buffer(tr, size, cpu_id);\n\tif (ret < 0)\n\t\tret = -ENOMEM;\n\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n\n/**\n * tracing_update_buffers - used by tracing facility to expand ring buffers\n *\n * To save on memory when the tracing is never used on a system with it\n * configured in. The ring buffers are set to a minimum size. But once\n * a user starts to use the tracing facility, then they need to grow\n * to their default size.\n *\n * This function is to be called when a tracer is about to be used.\n */\nint tracing_update_buffers(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tif (!ring_buffer_expanded)\n\t\tret = __tracing_resize_ring_buffer(&global_trace, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct trace_option_dentry;\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer);\n\n/*\n * Used to clear out the tracer before deletion of an instance.\n * Must have trace_types_lock held.\n */\nstatic void tracing_set_nop(struct trace_array *tr)\n{\n\tif (tr->current_trace == &nop_trace)\n\t\treturn;\n\t\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\ttr->current_trace = &nop_trace;\n}\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t)\n{\n\t/* Only enable if the directory has been created already. */\n\tif (!tr->dir)\n\t\treturn;\n\n\tcreate_trace_option_files(tr, t);\n}\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf)\n{\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_rcu */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_rcu();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_set_trace_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+1];\n\tint i;\n\tsize_t ret;\n\tint err;\n\n\tret = cnt;\n\n\tif (cnt > MAX_TRACER_SIZE)\n\t\tcnt = MAX_TRACER_SIZE;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\t/* strip ending whitespace. */\n\tfor (i = cnt - 1; i > 0 && isspace(buf[i]); i--)\n\t\tbuf[i] = 0;\n\n\terr = tracing_set_tracer(tr, buf);\n\tif (err)\n\t\treturn err;\n\n\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_nsecs_read(unsigned long *ptr, char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tr = snprintf(buf, sizeof(buf), \"%ld\\n\",\n\t\t     *ptr == (unsigned long)-1 ? -1 : nsecs_to_usecs(*ptr));\n\tif (r > sizeof(buf))\n\t\tr = sizeof(buf);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_nsecs_write(unsigned long *ptr, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t*ptr = val * 1000;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_thresh_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(&tracing_thresh, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_thresh_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\tret = tracing_nsecs_write(&tracing_thresh, ubuf, cnt, ppos);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (tr->current_trace->update_thresh) {\n\t\tret = tr->current_trace->update_thresh(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = cnt;\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\nstatic ssize_t\ntracing_max_lat_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(filp->private_data, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_max_lat_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_write(filp->private_data, ubuf, cnt, ppos);\n}\n\n#endif\n\nstatic int tracing_open_pipe(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\n\t/* create a buffer to store the information to pass to userspace */\n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter) {\n\t\tret = -ENOMEM;\n\t\t__trace_array_put(tr);\n\t\tgoto out;\n\t}\n\n\ttrace_seq_init(&iter->seq);\n\titer->trace = tr->current_trace;\n\n\tif (!alloc_cpumask_var(&iter->started, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t/* trace pipe does not show start of buffer */\n\tcpumask_setall(iter->started);\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\titer->tr = tr;\n\titer->trace_buffer = &tr->trace_buffer;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\tfilp->private_data = iter;\n\n\tif (iter->trace->pipe_open)\n\t\titer->trace->pipe_open(iter);\n\n\tnonseekable_open(inode, filp);\n\n\ttr->current_trace->ref++;\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n\nfail:\n\tkfree(iter->trace);\n\tkfree(iter);\n\t__trace_array_put(tr);\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_release_pipe(struct inode *inode, struct file *file)\n{\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic __poll_t\ntrace_poll(struct trace_iterator *iter, struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_array *tr = iter->tr;\n\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\tif (tr->trace_flags & TRACE_ITER_BLOCK)\n\t\t/*\n\t\t * Always select as readable when in blocking mode\n\t\t */\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\telse\n\t\treturn ring_buffer_poll_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\t\t     filp, poll_table);\n}\n\nstatic __poll_t\ntracing_poll_pipe(struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\n/* Must be called with iter->mutex held. */\nstatic int tracing_wait_pipe(struct file *filp)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tint ret;\n\n\twhile (trace_empty(iter)) {\n\n\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * We block until we read something and tracing is disabled.\n\t\t * We still block if tracing is disabled, but we have never\n\t\t * read anything. This allows a user to cat this file, and\n\t\t * then enable tracing. But after we have read something,\n\t\t * we give an EOF when tracing is again disabled.\n\t\t *\n\t\t * iter->pos will be 0 if we haven't read anything.\n\t\t */\n\t\tif (!tracer_tracing_is_on(iter->tr) && iter->pos)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&iter->mutex);\n\n\t\tret = wait_on_pipe(iter, 0);\n\n\t\tmutex_lock(&iter->mutex);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\n/*\n * Consumer reader.\n */\nstatic ssize_t\ntracing_read_pipe(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tssize_t sret;\n\n\t/*\n\t * Avoid more than one consumer on a single file descriptor\n\t * This is just a matter of traces coherency, the ring buffer itself\n\t * is protected.\n\t */\n\tmutex_lock(&iter->mutex);\n\n\t/* return any leftover data */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (sret != -EBUSY)\n\t\tgoto out;\n\n\ttrace_seq_init(&iter->seq);\n\n\tif (iter->trace->read) {\n\t\tsret = iter->trace->read(iter, filp, ubuf, cnt, ppos);\n\t\tif (sret)\n\t\t\tgoto out;\n\t}\n\nwaitagain:\n\tsret = tracing_wait_pipe(filp);\n\tif (sret <= 0)\n\t\tgoto out;\n\n\t/* stop when tracing is finished */\n\tif (trace_empty(iter)) {\n\t\tsret = 0;\n\t\tgoto out;\n\t}\n\n\tif (cnt >= PAGE_SIZE)\n\t\tcnt = PAGE_SIZE - 1;\n\n\t/* reset all but tr, trace, and overruns */\n\tmemset(&iter->seq, 0,\n\t       sizeof(struct trace_iterator) -\n\t       offsetof(struct trace_iterator, seq));\n\tcpumask_clear(iter->started);\n\titer->pos = -1;\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\twhile (trace_find_next_entry_inc(iter) != NULL) {\n\t\tenum print_line_t ret;\n\t\tint save_len = iter->seq.seq.len;\n\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\t/* don't print partial lines */\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\n\t\tif (trace_seq_used(&iter->seq) >= cnt)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Setting the full flag means we reached the trace_seq buffer\n\t\t * size and we should leave by partial output condition above.\n\t\t * One of the trace_seq_* functions is not used properly.\n\t\t */\n\t\tWARN_ONCE(iter->seq.full, \"full flag set for trace type %d\",\n\t\t\t  iter->ent->type);\n\t}\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\n\t/* Now copy what we have to the user */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (iter->seq.seq.readpos >= trace_seq_used(&iter->seq))\n\t\ttrace_seq_init(&iter->seq);\n\n\t/*\n\t * If there was nothing to send to user, in spite of consuming trace\n\t * entries, go back to wait for more entries.\n\t */\n\tif (sret == -EBUSY)\n\t\tgoto waitagain;\n\nout:\n\tmutex_unlock(&iter->mutex);\n\n\treturn sret;\n}\n\nstatic void tracing_spd_release_pipe(struct splice_pipe_desc *spd,\n\t\t\t\t     unsigned int idx)\n{\n\t__free_page(spd->pages[idx]);\n}\n\nstatic const struct pipe_buf_operations tracing_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= generic_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= generic_pipe_buf_get,\n};\n\nstatic size_t\ntracing_fill_pipe_page(size_t rem, struct trace_iterator *iter)\n{\n\tsize_t count;\n\tint save_len;\n\tint ret;\n\n\t/* Seq buffer is page-sized, exactly what we need. */\n\tfor (;;) {\n\t\tsave_len = iter->seq.seq.len;\n\t\tret = print_trace_line(iter);\n\n\t\tif (trace_seq_has_overflowed(&iter->seq)) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This should not be hit, because it should only\n\t\t * be set if the iter->seq overflowed. But check it\n\t\t * anyway to be safe.\n\t\t */\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tcount = trace_seq_used(&iter->seq) - save_len;\n\t\tif (rem < count) {\n\t\t\trem = 0;\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\t\trem -= count;\n\t\tif (!trace_find_next_entry_inc(iter))\t{\n\t\t\trem = 0;\n\t\t\titer->ent = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rem;\n}\n\nstatic ssize_t tracing_splice_read_pipe(struct file *filp,\n\t\t\t\t\tloff_t *ppos,\n\t\t\t\t\tstruct pipe_inode_info *pipe,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct trace_iterator *iter = filp->private_data;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages\t= 0, /* This gets updated below. */\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &tracing_pipe_buf_ops,\n\t\t.spd_release\t= tracing_spd_release_pipe,\n\t};\n\tssize_t ret;\n\tsize_t rem;\n\tunsigned int i;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&iter->mutex);\n\n\tif (iter->trace->splice_read) {\n\t\tret = iter->trace->splice_read(iter, filp,\n\t\t\t\t\t       ppos, pipe, len, flags);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tret = tracing_wait_pipe(filp);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\n\tif (!iter->ent && !trace_find_next_entry_inc(iter)) {\n\t\tret = -EFAULT;\n\t\tgoto out_err;\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\n\t/* Fill as many pages as possible. */\n\tfor (i = 0, rem = len; i < spd.nr_pages_max && rem; i++) {\n\t\tspd.pages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!spd.pages[i])\n\t\t\tbreak;\n\n\t\trem = tracing_fill_pipe_page(rem, iter);\n\n\t\t/* Copy the data into the page, so we can start over. */\n\t\tret = trace_seq_to_buffer(&iter->seq,\n\t\t\t\t\t  page_address(spd.pages[i]),\n\t\t\t\t\t  trace_seq_used(&iter->seq));\n\t\tif (ret < 0) {\n\t\t\t__free_page(spd.pages[i]);\n\t\t\tbreak;\n\t\t}\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].len = trace_seq_used(&iter->seq);\n\n\t\ttrace_seq_init(&iter->seq);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\tmutex_unlock(&iter->mutex);\n\n\tspd.nr_pages = i;\n\n\tif (i)\n\t\tret = splice_to_pipe(pipe, &spd);\n\telse\n\t\tret = 0;\nout:\n\tsplice_shrink_spd(&spd);\n\treturn ret;\n\nout_err:\n\tmutex_unlock(&iter->mutex);\n\tgoto out;\n}\n\nstatic ssize_t\ntracing_entries_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tint cpu = tracing_get_cpu(inode);\n\tchar buf[64];\n\tint r = 0;\n\tssize_t ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tint cpu, buf_size_same;\n\t\tunsigned long size;\n\n\t\tsize = 0;\n\t\tbuf_size_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (size == 0)\n\t\t\t\tsize = per_cpu_ptr(tr->trace_buffer.data, cpu)->entries;\n\t\t\tif (size != per_cpu_ptr(tr->trace_buffer.data, cpu)->entries) {\n\t\t\t\tbuf_size_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (buf_size_same) {\n\t\t\tif (!ring_buffer_expanded)\n\t\t\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\",\n\t\t\t\t\t    size >> 10,\n\t\t\t\t\t    trace_buf_size >> 10);\n\t\t\telse\n\t\t\t\tr = sprintf(buf, \"%lu\\n\", size >> 10);\n\t\t} else\n\t\t\tr = sprintf(buf, \"X\\n\");\n\t} else\n\t\tr = sprintf(buf, \"%lu\\n\", per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_entries_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry */\n\tif (!val)\n\t\treturn -EINVAL;\n\n\t/* value is in KB */\n\tval <<= 10;\n\tret = tracing_resize_ring_buffer(tr, val, tracing_get_cpu(inode));\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_total_entries_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r, cpu;\n\tunsigned long size = 0, expanded_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\tsize += per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10;\n\t\tif (!ring_buffer_expanded)\n\t\t\texpanded_size += trace_buf_size >> 10;\n\t}\n\tif (ring_buffer_expanded)\n\t\tr = sprintf(buf, \"%lu\\n\", size);\n\telse\n\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\", size, expanded_size);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_free_buffer_write(struct file *filp, const char __user *ubuf,\n\t\t\t  size_t cnt, loff_t *ppos)\n{\n\t/*\n\t * There is no need to read what the user has written, this function\n\t * is just to make sure that there is no error when \"echo\" is used\n\t */\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int\ntracing_free_buffer_release(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\t/* disable tracing ? */\n\tif (tr->trace_flags & TRACE_ITER_STOP_ON_FREE)\n\t\ttracer_tracing_off(tr);\n\t/* resize the ring buffer to 0 */\n\ttracing_resize_ring_buffer(tr, 0, RING_BUFFER_ALL_CPUS);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_mark_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tconst char faulted[] = \"<faulted>\";\n\tssize_t written;\n\tint size;\n\tint len;\n\n/* Used in tracing_mark_raw_write() as well */\n#define FAULTED_SIZE (sizeof(faulted) - 1) /* '\\0' is already accounted for */\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt + 2; /* add '\\0' and possible '\\n' */\n\n\t/* If less than \"<faulted>\", then make sure we can still add that */\n\tif (cnt < FAULTED_SIZE)\n\t\tsize += FAULTED_SIZE - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (unlikely(!event))\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = _THIS_IP_;\n\n\tlen = __copy_from_user_inatomic(&entry->buf, ubuf, cnt);\n\tif (len) {\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\tcnt = FAULTED_SIZE;\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\tlen = cnt;\n\n\tif (tr->trace_marker_file && !list_empty(&tr->trace_marker_file->triggers)) {\n\t\t/* do not add \\n before testing triggers, but add \\0 */\n\t\tentry->buf[cnt] = '\\0';\n\t\ttt = event_triggers_call(tr->trace_marker_file, entry, event);\n\t}\n\n\tif (entry->buf[cnt - 1] != '\\n') {\n\t\tentry->buf[cnt] = '\\n';\n\t\tentry->buf[cnt + 1] = '\\0';\n\t} else\n\t\tentry->buf[cnt] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (tt)\n\t\tevent_triggers_post_call(tr->trace_marker_file, tt);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\n/* Limit it for now to 3K (including tag) */\n#define RAW_DATA_MAX_SIZE (1024*3)\n\nstatic ssize_t\ntracing_mark_raw_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct raw_data_entry *entry;\n\tconst char faulted[] = \"<faulted>\";\n\tunsigned long irq_flags;\n\tssize_t written;\n\tint size;\n\tint len;\n\n#define FAULT_SIZE_ID (FAULTED_SIZE + sizeof(int))\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\t/* The marker must at least have a tag id */\n\tif (cnt < sizeof(unsigned int) || cnt > RAW_DATA_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt;\n\tif (cnt < FAULT_SIZE_ID)\n\t\tsize += FAULT_SIZE_ID - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (!event)\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\n\tlen = __copy_from_user_inatomic(&entry->id, ubuf, cnt);\n\tif (len) {\n\t\tentry->id = -1;\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\nstatic int tracing_clock_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++)\n\t\tseq_printf(m,\n\t\t\t\"%s%s%s%s\", i ? \" \" : \"\",\n\t\t\ti == tr->clock_id ? \"[\" : \"\", trace_clocks[i].name,\n\t\t\ti == tr->clock_id ? \"]\" : \"\");\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nint tracing_set_clock(struct trace_array *tr, const char *clockstr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++) {\n\t\tif (strcmp(trace_clocks[i].name, clockstr) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(trace_clocks))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->clock_id = i;\n\n\tring_buffer_set_clock(tr->trace_buffer.buffer, trace_clocks[i].func);\n\n\t/*\n\t * New clock may not be consistent with the previous clock.\n\t * Reset the buffer so that it doesn't have incomparable timestamps.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_clock(tr->max_buffer.buffer, trace_clocks[i].func);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t tracing_clock_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *fpos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tconst char *clockstr;\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tclockstr = strstrip(buf);\n\n\tret = tracing_set_clock(tr, clockstr);\n\tif (ret)\n\t\treturn ret;\n\n\t*fpos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_clock_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_clock_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic int tracing_time_stamp_mode_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (ring_buffer_time_stamp_abs(tr->trace_buffer.buffer))\n\t\tseq_puts(m, \"delta [absolute]\\n\");\n\telse\n\t\tseq_puts(m, \"[delta] absolute\\n\");\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int tracing_time_stamp_mode_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_time_stamp_mode_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nint tracing_set_time_stamp_abs(struct trace_array *tr, bool abs)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (abs && tr->time_stamp_abs_ref++)\n\t\tgoto out;\n\n\tif (!abs) {\n\t\tif (WARN_ON_ONCE(!tr->time_stamp_abs_ref)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (--tr->time_stamp_abs_ref)\n\t\t\tgoto out;\n\t}\n\n\tring_buffer_set_time_stamp_abs(tr->trace_buffer.buffer, abs);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_time_stamp_abs(tr->max_buffer.buffer, abs);\n#endif\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct ftrace_buffer_info {\n\tstruct trace_iterator\titer;\n\tvoid\t\t\t*spare;\n\tunsigned int\t\tspare_cpu;\n\tunsigned int\t\tread;\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic int tracing_snapshot_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tstruct seq_file *m;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, true);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t} else {\n\t\t/* Writes still need the seq_file to hold the private data */\n\t\tret = -ENOMEM;\n\t\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\t\tif (!m)\n\t\t\tgoto out;\n\t\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\t\tif (!iter) {\n\t\t\tkfree(m);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\n\t\titer->tr = tr;\n\t\titer->trace_buffer = &tr->max_buffer;\n\t\titer->cpu_file = tracing_get_cpu(inode);\n\t\tm->private = iter;\n\t\tfile->private_data = m;\n\t}\nout:\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long val;\n\tint ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tswitch (val) {\n\tcase 0:\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (tr->allocated_snapshot)\n\t\t\tfree_snapshot(tr);\n\t\tbreak;\n\tcase 1:\n/* Only allow per-cpu swap if the ring buffer supports it */\n#ifndef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tif (!tr->allocated_snapshot) {\n\t\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tlocal_irq_disable();\n\t\t/* Now, we're going to swap */\n\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\tupdate_max_tr(tr, current, smp_processor_id());\n\t\telse\n\t\t\tupdate_max_tr_single(tr, current, iter->cpu_file);\n\t\tlocal_irq_enable();\n\t\tbreak;\n\tdefault:\n\t\tif (tr->allocated_snapshot) {\n\t\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\t\ttracing_reset_online_cpus(&tr->max_buffer);\n\t\t\telse\n\t\t\t\ttracing_reset(&tr->max_buffer, iter->cpu_file);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (ret >= 0) {\n\t\t*ppos += cnt;\n\t\tret = cnt;\n\t}\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_snapshot_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = file->private_data;\n\tint ret;\n\n\tret = tracing_release(inode, file);\n\n\tif (file->f_mode & FMODE_READ)\n\t\treturn ret;\n\n\t/* If write only, the seq_file is just a stub */\n\tif (m)\n\t\tkfree(m->private);\n\tkfree(m);\n\n\treturn 0;\n}\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp);\nstatic ssize_t tracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos);\nstatic int tracing_buffers_release(struct inode *inode, struct file *file);\nstatic ssize_t tracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t   struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\nstatic int snapshot_raw_open(struct inode *inode, struct file *filp)\n{\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tret = tracing_buffers_open(inode, filp);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tinfo = filp->private_data;\n\n\tif (info->iter.trace->use_max_tr) {\n\t\ttracing_buffers_release(inode, filp);\n\t\treturn -EBUSY;\n\t}\n\n\tinfo->iter.snapshot = true;\n\tinfo->iter.trace_buffer = &info->iter.tr->max_buffer;\n\n\treturn ret;\n}\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\n\nstatic const struct file_operations tracing_thresh_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_thresh_read,\n\t.write\t\t= tracing_thresh_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\nstatic const struct file_operations tracing_max_lat_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_max_lat_read,\n\t.write\t\t= tracing_max_lat_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif\n\nstatic const struct file_operations set_tracer_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_set_trace_read,\n\t.write\t\t= tracing_set_trace_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic const struct file_operations tracing_pipe_fops = {\n\t.open\t\t= tracing_open_pipe,\n\t.poll\t\t= tracing_poll_pipe,\n\t.read\t\t= tracing_read_pipe,\n\t.splice_read\t= tracing_splice_read_pipe,\n\t.release\t= tracing_release_pipe,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic const struct file_operations tracing_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_entries_read,\n\t.write\t\t= tracing_entries_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_total_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_total_entries_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_free_buffer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_free_buffer_write,\n\t.release\t= tracing_free_buffer_release,\n};\n\nstatic const struct file_operations tracing_mark_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_mark_raw_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_raw_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations trace_clock_fops = {\n\t.open\t\t= tracing_clock_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_clock_write,\n};\n\nstatic const struct file_operations trace_time_stamp_mode_fops = {\n\t.open\t\t= tracing_time_stamp_mode_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic const struct file_operations snapshot_fops = {\n\t.open\t\t= tracing_snapshot_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_snapshot_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_snapshot_release,\n};\n\nstatic const struct file_operations snapshot_raw_fops = {\n\t.open\t\t= snapshot_raw_open,\n\t.read\t\t= tracing_buffers_read,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\ttrace_array_put(tr);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tinfo->iter.tr\t\t= tr;\n\tinfo->iter.cpu_file\t= tracing_get_cpu(inode);\n\tinfo->iter.trace\t= tr->current_trace;\n\tinfo->iter.trace_buffer = &tr->trace_buffer;\n\tinfo->spare\t\t= NULL;\n\t/* Force reading ring buffer for first read */\n\tinfo->read\t\t= (unsigned int)-1;\n\n\tfilp->private_data = info;\n\n\ttr->current_trace->ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = nonseekable_open(inode, filp);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic __poll_t\ntracing_buffers_poll(struct file *filp, poll_table *poll_table)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\nstatic ssize_t\ntracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tssize_t ret = 0;\n\tssize_t size;\n\n\tif (!count)\n\t\treturn 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (!info->spare) {\n\t\tinfo->spare = ring_buffer_alloc_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t\t\t  iter->cpu_file);\n\t\tif (IS_ERR(info->spare)) {\n\t\t\tret = PTR_ERR(info->spare);\n\t\t\tinfo->spare = NULL;\n\t\t} else {\n\t\t\tinfo->spare_cpu = iter->cpu_file;\n\t\t}\n\t}\n\tif (!info->spare)\n\t\treturn ret;\n\n\t/* Do we have previous read data to read? */\n\tif (info->read < PAGE_SIZE)\n\t\tgoto read;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tret = ring_buffer_read_page(iter->trace_buffer->buffer,\n\t\t\t\t    &info->spare,\n\t\t\t\t    count,\n\t\t\t\t    iter->cpu_file, 0);\n\ttrace_access_unlock(iter->cpu_file);\n\n\tif (ret < 0) {\n\t\tif (trace_empty(iter)) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tret = wait_on_pipe(iter, 0);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tgoto again;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinfo->read = 0;\n read:\n\tsize = PAGE_SIZE - info->read;\n\tif (size > count)\n\t\tsize = count;\n\n\tret = copy_to_user(ubuf, info->spare + info->read, size);\n\tif (ret == size)\n\t\treturn -EFAULT;\n\n\tsize -= ret;\n\n\t*ppos += size;\n\tinfo->read += size;\n\n\treturn size;\n}\n\nstatic int tracing_buffers_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->tr->current_trace->ref--;\n\n\t__trace_array_put(iter->tr);\n\n\tif (info->spare)\n\t\tring_buffer_free_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t   info->spare_cpu, info->spare);\n\tkfree(info);\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstruct buffer_ref {\n\tstruct ring_buffer\t*buffer;\n\tvoid\t\t\t*page;\n\tint\t\t\tcpu;\n\tint\t\t\tref;\n};\n\nstatic void buffer_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tbuf->private = 0;\n}\n\nstatic void buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tref->ref++;\n}\n\n/* Pipe buffer operations for a buffer. */\nstatic const struct pipe_buf_operations buffer_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= buffer_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= buffer_pipe_buf_get,\n};\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void buffer_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tstruct buffer_ref *ref =\n\t\t(struct buffer_ref *)spd->partial[i].private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tspd->partial[i].private = 0;\n}\n\nstatic ssize_t\ntracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe, size_t len,\n\t\t\t    unsigned int flags)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &buffer_pipe_buf_ops,\n\t\t.spd_release\t= buffer_spd_release,\n\t};\n\tstruct buffer_ref *ref;\n\tint entries, i;\n\tssize_t ret = 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (*ppos & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (len & (PAGE_SIZE - 1)) {\n\t\tif (len < PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tlen &= PAGE_MASK;\n\t}\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\n\tfor (i = 0; i < spd.nr_pages_max && len && entries; i++, len -= PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint r;\n\n\t\tref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!ref) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tref->ref = 1;\n\t\tref->buffer = iter->trace_buffer->buffer;\n\t\tref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);\n\t\tif (IS_ERR(ref->page)) {\n\t\t\tret = PTR_ERR(ref->page);\n\t\t\tref->page = NULL;\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\t\tref->cpu = iter->cpu_file;\n\n\t\tr = ring_buffer_read_page(ref->buffer, &ref->page,\n\t\t\t\t\t  len, iter->cpu_file, 1);\n\t\tif (r < 0) {\n\t\t\tring_buffer_free_read_page(ref->buffer, ref->cpu,\n\t\t\t\t\t\t   ref->page);\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage = virt_to_page(ref->page);\n\n\t\tspd.pages[i] = page;\n\t\tspd.partial[i].len = PAGE_SIZE;\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].private = (unsigned long)ref;\n\t\tspd.nr_pages++;\n\t\t*ppos += PAGE_SIZE;\n\n\t\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\tspd.nr_pages = i;\n\n\t/* did we read anything? */\n\tif (!spd.nr_pages) {\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = -EAGAIN;\n\t\tif ((file->f_flags & O_NONBLOCK) || (flags & SPLICE_F_NONBLOCK))\n\t\t\tgoto out;\n\n\t\tret = wait_on_pipe(iter, iter->tr->buffer_percent);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\n\tret = splice_to_pipe(pipe, &spd);\nout:\n\tsplice_shrink_spd(&spd);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_buffers_fops = {\n\t.open\t\t= tracing_buffers_open,\n\t.read\t\t= tracing_buffers_read,\n\t.poll\t\t= tracing_buffers_poll,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic ssize_t\ntracing_stats_read(struct file *filp, char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tint cpu = tracing_get_cpu(inode);\n\tstruct trace_seq *s;\n\tunsigned long cnt;\n\tunsigned long long t;\n\tunsigned long usec_rem;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tcnt = ring_buffer_entries_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"entries: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_commit_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"commit overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_bytes_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"bytes: %ld\\n\", cnt);\n\n\tif (trace_clocks[tr->clock_id].in_ns) {\n\t\t/* local or global for trace_clock */\n\t\tt = ns2usecs(ring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"oldest event ts: %5llu.%06lu\\n\",\n\t\t\t\t\t\t\t\tt, usec_rem);\n\n\t\tt = ns2usecs(ring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"now ts: %5llu.%06lu\\n\", t, usec_rem);\n\t} else {\n\t\t/* counter or tsc mode for trace_clock */\n\t\ttrace_seq_printf(s, \"oldest event ts: %llu\\n\",\n\t\t\t\tring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\n\t\ttrace_seq_printf(s, \"now ts: %llu\\n\",\n\t\t\t\tring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t}\n\n\tcnt = ring_buffer_dropped_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"dropped events: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_read_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"read events: %ld\\n\", cnt);\n\n\tcount = simple_read_from_buffer(ubuf, count, ppos,\n\t\t\t\t\ts->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn count;\n}\n\nstatic const struct file_operations tracing_stats_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_stats_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic ssize_t\ntracing_read_dyn_info(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long *p = filp->private_data;\n\tchar buf[64]; /* Not too big for a shallow stack */\n\tint r;\n\n\tr = scnprintf(buf, 63, \"%ld\", *p);\n\tbuf[r++] = '\\n';\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic const struct file_operations tracing_dyn_info_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_read_dyn_info,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif /* CONFIG_DYNAMIC_FTRACE */\n\n#if defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE)\nstatic void\nftrace_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\tstruct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\tvoid *data)\n{\n\ttracing_snapshot_instance(tr);\n}\n\nstatic void\nftrace_count_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\t      struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t      void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count) {\n\n\t\tif (*count <= 0)\n\t\t\treturn;\n\n\t\t(*count)--;\n\t}\n\n\ttracing_snapshot_instance(tr);\n}\n\nstatic int\nftrace_snapshot_print(struct seq_file *m, unsigned long ip,\n\t\t      struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_puts(m, \"snapshot\");\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count)\n\t\tseq_printf(m, \":count=%ld\\n\", *count);\n\telse\n\t\tseq_puts(m, \":unlimited\\n\");\n\n\treturn 0;\n}\n\nstatic int\nftrace_snapshot_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENOMEM;\n\t\t*data = mapper;\n\t}\n\n\treturn ftrace_func_mapper_add_ip(mapper, ip, init_data);\n}\n\nstatic void\nftrace_snapshot_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, NULL);\n\t\treturn;\n\t}\n\n\tftrace_func_mapper_remove_ip(mapper, ip);\n}\n\nstatic struct ftrace_probe_ops snapshot_probe_ops = {\n\t.func\t\t\t= ftrace_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n};\n\nstatic struct ftrace_probe_ops snapshot_count_probe_ops = {\n\t.func\t\t\t= ftrace_count_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n\t.init\t\t\t= ftrace_snapshot_init,\n\t.free\t\t\t= ftrace_snapshot_free,\n};\n\nstatic int\nftrace_trace_snapshot_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t\t       char *glob, char *cmd, char *param, int enable)\n{\n\tstruct ftrace_probe_ops *ops;\n\tvoid *count = (void *)-1;\n\tchar *number;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t/* hash funcs only work with set_ftrace_filter */\n\tif (!enable)\n\t\treturn -EINVAL;\n\n\tops = param ? &snapshot_count_probe_ops :  &snapshot_probe_ops;\n\n\tif (glob[0] == '!')\n\t\treturn unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tif (!strlen(number))\n\t\tgoto out_reg;\n\n\t/*\n\t * We use the callback data field (which is a pointer)\n\t * as our counter.\n\t */\n\tret = kstrtoul(number, 0, (unsigned long *)&count);\n\tif (ret)\n\t\treturn ret;\n\n out_reg:\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, count);\n\n out:\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct ftrace_func_command ftrace_snapshot_cmd = {\n\t.name\t\t\t= \"snapshot\",\n\t.func\t\t\t= ftrace_trace_snapshot_callback,\n};\n\nstatic __init int register_snapshot_cmd(void)\n{\n\treturn register_ftrace_command(&ftrace_snapshot_cmd);\n}\n#else\nstatic inline __init int register_snapshot_cmd(void) { return 0; }\n#endif /* defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE) */\n\nstatic struct dentry *tracing_get_dentry(struct trace_array *tr)\n{\n\tif (WARN_ON(!tr->dir))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/* Top directory uses NULL as the parent */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn NULL;\n\n\t/* All sub buffers have a descriptor */\n\treturn tr->dir;\n}\n\nstatic struct dentry *tracing_dentry_percpu(struct trace_array *tr, int cpu)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->percpu_dir)\n\t\treturn tr->percpu_dir;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->percpu_dir = tracefs_create_dir(\"per_cpu\", d_tracer);\n\n\tWARN_ONCE(!tr->percpu_dir,\n\t\t  \"Could not create tracefs directory 'per_cpu/%d'\\n\", cpu);\n\n\treturn tr->percpu_dir;\n}\n\nstatic struct dentry *\ntrace_create_cpu_file(const char *name, umode_t mode, struct dentry *parent,\n\t\t      void *data, long cpu, const struct file_operations *fops)\n{\n\tstruct dentry *ret = trace_create_file(name, mode, parent, data, fops);\n\n\tif (ret) /* See tracing_get_cpu() */\n\t\td_inode(ret)->i_cdev = (void *)(cpu + 1);\n\treturn ret;\n}\n\nstatic void\ntracing_init_tracefs_percpu(struct trace_array *tr, long cpu)\n{\n\tstruct dentry *d_percpu = tracing_dentry_percpu(tr, cpu);\n\tstruct dentry *d_cpu;\n\tchar cpu_dir[30]; /* 30 characters should be more than enough */\n\n\tif (!d_percpu)\n\t\treturn;\n\n\tsnprintf(cpu_dir, 30, \"cpu%ld\", cpu);\n\td_cpu = tracefs_create_dir(cpu_dir, d_percpu);\n\tif (!d_cpu) {\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", cpu_dir);\n\t\treturn;\n\t}\n\n\t/* per cpu trace_pipe */\n\ttrace_create_cpu_file(\"trace_pipe\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_pipe_fops);\n\n\t/* per cpu trace */\n\ttrace_create_cpu_file(\"trace\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &tracing_fops);\n\n\ttrace_create_cpu_file(\"trace_pipe_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_buffers_fops);\n\n\ttrace_create_cpu_file(\"stats\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_stats_fops);\n\n\ttrace_create_cpu_file(\"buffer_size_kb\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_entries_fops);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_cpu_file(\"snapshot\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_fops);\n\n\ttrace_create_cpu_file(\"snapshot_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_raw_fops);\n#endif\n}\n\n#ifdef CONFIG_FTRACE_SELFTEST\n/* Let selftest have access to static functions in this file */\n#include \"trace_selftest.c\"\n#endif\n\nstatic ssize_t\ntrace_options_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tchar *buf;\n\n\tif (topt->flags->val & topt->opt->bit)\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tif (!!(topt->flags->val & topt->opt->bit) != val) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tret = __set_tracer_option(topt->tr, topt->flags,\n\t\t\t\t\t  topt->opt, !val);\n\t\tmutex_unlock(&trace_types_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\n\nstatic const struct file_operations trace_options_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_read,\n\t.write = trace_options_write,\n\t.llseek\t= generic_file_llseek,\n};\n\n/*\n * In order to pass in both the trace_array descriptor as well as the index\n * to the flag that the trace option file represents, the trace_array\n * has a character array of trace_flags_index[], which holds the index\n * of the bit for the flag it represents. index[0] == 0, index[1] == 1, etc.\n * The address of this character array is passed to the flag option file\n * read/write callbacks.\n *\n * In order to extract both the index and the trace_array descriptor,\n * get_tr_index() uses the following algorithm.\n *\n *   idx = *ptr;\n *\n * As the pointer itself contains the address of the index (remember\n * index[1] == 1).\n *\n * Then to get the trace_array descriptor, by subtracting that index\n * from the ptr, we get to the start of the index itself.\n *\n *   ptr - idx == &index[0]\n *\n * Then a simple container_of() from that pointer gets us to the\n * trace_array descriptor.\n */\nstatic void get_tr_index(void *data, struct trace_array **ptr,\n\t\t\t unsigned int *pindex)\n{\n\t*pindex = *(unsigned char *)data;\n\n\t*ptr = container_of(data - *pindex, struct trace_array,\n\t\t\t    trace_flags_index);\n}\n\nstatic ssize_t\ntrace_options_core_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tchar *buf;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tif (tr->trace_flags & (1 << index))\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_core_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tunsigned long val;\n\tint ret;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\tret = set_tracer_flag(tr, 1 << index, val);\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations trace_options_core_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_core_read,\n\t.write = trace_options_core_write,\n\t.llseek = generic_file_llseek,\n};\n\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops)\n{\n\tstruct dentry *ret;\n\n\tret = tracefs_create_file(name, mode, parent, data, fops);\n\tif (!ret)\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", name);\n\n\treturn ret;\n}\n\n\nstatic struct dentry *trace_options_init_dentry(struct trace_array *tr)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->options)\n\t\treturn tr->options;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->options = tracefs_create_dir(\"options\", d_tracer);\n\tif (!tr->options) {\n\t\tpr_warn(\"Could not create tracefs directory 'options'\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn tr->options;\n}\n\nstatic void\ncreate_trace_option_file(struct trace_array *tr,\n\t\t\t struct trace_option_dentry *topt,\n\t\t\t struct tracer_flags *flags,\n\t\t\t struct tracer_opt *opt)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\ttopt->flags = flags;\n\ttopt->opt = opt;\n\ttopt->tr = tr;\n\n\ttopt->entry = trace_create_file(opt->name, 0644, t_options, topt,\n\t\t\t\t    &trace_options_fops);\n\n}\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer)\n{\n\tstruct trace_option_dentry *topts;\n\tstruct trace_options *tr_topts;\n\tstruct tracer_flags *flags;\n\tstruct tracer_opt *opts;\n\tint cnt;\n\tint i;\n\n\tif (!tracer)\n\t\treturn;\n\n\tflags = tracer->flags;\n\n\tif (!flags || !flags->opts)\n\t\treturn;\n\n\t/*\n\t * If this is an instance, only create flags for tracers\n\t * the instance may have.\n\t */\n\tif (!trace_ok_for_array(tracer, tr))\n\t\treturn;\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\t/* Make sure there's no duplicate flags. */\n\t\tif (WARN_ON_ONCE(tr->topts[i].tracer->flags == tracer->flags))\n\t\t\treturn;\n\t}\n\n\topts = flags->opts;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++)\n\t\t;\n\n\ttopts = kcalloc(cnt + 1, sizeof(*topts), GFP_KERNEL);\n\tif (!topts)\n\t\treturn;\n\n\ttr_topts = krealloc(tr->topts, sizeof(*tr->topts) * (tr->nr_topts + 1),\n\t\t\t    GFP_KERNEL);\n\tif (!tr_topts) {\n\t\tkfree(topts);\n\t\treturn;\n\t}\n\n\ttr->topts = tr_topts;\n\ttr->topts[tr->nr_topts].tracer = tracer;\n\ttr->topts[tr->nr_topts].topts = topts;\n\ttr->nr_topts++;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++) {\n\t\tcreate_trace_option_file(tr, &topts[cnt], flags,\n\t\t\t\t\t &opts[cnt]);\n\t\tWARN_ONCE(topts[cnt].entry == NULL,\n\t\t\t  \"Failed to create trace option: %s\",\n\t\t\t  opts[cnt].name);\n\t}\n}\n\nstatic struct dentry *\ncreate_trace_option_core_file(struct trace_array *tr,\n\t\t\t      const char *option, long index)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn NULL;\n\n\treturn trace_create_file(option, 0644, t_options,\n\t\t\t\t (void *)&tr->trace_flags_index[index],\n\t\t\t\t &trace_options_core_fops);\n}\n\nstatic void create_trace_options_dir(struct trace_array *tr)\n{\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}\n\nstatic ssize_t\nrb_simple_read(struct file *filp, char __user *ubuf,\n\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tracer_tracing_is_on(tr);\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nrb_simple_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (buffer) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tif (!!val == tracer_tracing_is_on(tr)) {\n\t\t\tval = 0; /* do nothing */\n\t\t} else if (val) {\n\t\t\ttracer_tracing_on(tr);\n\t\t\tif (tr->current_trace->start)\n\t\t\t\ttr->current_trace->start(tr);\n\t\t} else {\n\t\t\ttracer_tracing_off(tr);\n\t\t\tif (tr->current_trace->stop)\n\t\t\t\ttr->current_trace->stop(tr);\n\t\t}\n\t\tmutex_unlock(&trace_types_lock);\n\t}\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations rb_simple_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= rb_simple_read,\n\t.write\t\t= rb_simple_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstatic ssize_t\nbuffer_percent_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tr->buffer_percent;\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nbuffer_percent_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val > 100)\n\t\treturn -EINVAL;\n\n\tif (!val)\n\t\tval = 1;\n\n\ttr->buffer_percent = val;\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations buffer_percent_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= buffer_percent_read,\n\t.write\t\t= buffer_percent_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstruct dentry *trace_instance_dir;\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer);\n\nstatic int\nallocate_trace_buffer(struct trace_array *tr, struct trace_buffer *buf, int size)\n{\n\tenum ring_buffer_flags rb_flags;\n\n\trb_flags = tr->trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0;\n\n\tbuf->tr = tr;\n\n\tbuf->buffer = ring_buffer_alloc(size, rb_flags);\n\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,\n\t\t\t   ring_buffer_size(tr->trace_buffer.buffer, 0));\n\n\treturn 0;\n}\n\nstatic int allocate_trace_buffers(struct trace_array *tr, int size)\n{\n\tint ret;\n\n\tret = allocate_trace_buffer(tr, &tr->trace_buffer, size);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tret = allocate_trace_buffer(tr, &tr->max_buffer,\n\t\t\t\t    allocate_snapshot ? size : 1);\n\tif (WARN_ON(ret)) {\n\t\tring_buffer_free(tr->trace_buffer.buffer);\n\t\ttr->trace_buffer.buffer = NULL;\n\t\tfree_percpu(tr->trace_buffer.data);\n\t\ttr->trace_buffer.data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\ttr->allocated_snapshot = allocate_snapshot;\n\n\t/*\n\t * Only the top level trace array gets its snapshot allocated\n\t * from the kernel command line.\n\t */\n\tallocate_snapshot = false;\n#endif\n\treturn 0;\n}\n\nstatic void free_trace_buffer(struct trace_buffer *buf)\n{\n\tif (buf->buffer) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\tfree_percpu(buf->data);\n\t\tbuf->data = NULL;\n\t}\n}\n\nstatic void free_trace_buffers(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn;\n\n\tfree_trace_buffer(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tfree_trace_buffer(&tr->max_buffer);\n#endif\n}\n\nstatic void init_trace_flags_index(struct trace_array *tr)\n{\n\tint i;\n\n\t/* Used by the trace options files */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++)\n\t\ttr->trace_flags_index[i] = i;\n}\n\nstatic void __update_tracer_options(struct trace_array *tr)\n{\n\tstruct tracer *t;\n\n\tfor (t = trace_types; t; t = t->next)\n\t\tadd_tracer_options(tr, t);\n}\n\nstatic void update_tracer_options(struct trace_array *tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__update_tracer_options(tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int instance_mkdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}\n\nstatic int instance_rmdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic __init void create_trace_instances(struct dentry *d_tracer)\n{\n\ttrace_instance_dir = tracefs_create_instance_dir(\"instances\", d_tracer,\n\t\t\t\t\t\t\t instance_mkdir,\n\t\t\t\t\t\t\t instance_rmdir);\n\tif (WARN_ON(!trace_instance_dir))\n\t\treturn;\n}\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\ttrace_create_file(\"available_tracers\", 0444, d_tracer,\n\t\t\ttr, &show_traces_fops);\n\n\ttrace_create_file(\"current_tracer\", 0644, d_tracer,\n\t\t\ttr, &set_tracer_fops);\n\n\ttrace_create_file(\"tracing_cpumask\", 0644, d_tracer,\n\t\t\t  tr, &tracing_cpumask_fops);\n\n\ttrace_create_file(\"trace_options\", 0644, d_tracer,\n\t\t\t  tr, &tracing_iter_fops);\n\n\ttrace_create_file(\"trace\", 0644, d_tracer,\n\t\t\t  tr, &tracing_fops);\n\n\ttrace_create_file(\"trace_pipe\", 0444, d_tracer,\n\t\t\t  tr, &tracing_pipe_fops);\n\n\ttrace_create_file(\"buffer_size_kb\", 0644, d_tracer,\n\t\t\t  tr, &tracing_entries_fops);\n\n\ttrace_create_file(\"buffer_total_size_kb\", 0444, d_tracer,\n\t\t\t  tr, &tracing_total_entries_fops);\n\n\ttrace_create_file(\"free_buffer\", 0200, d_tracer,\n\t\t\t  tr, &tracing_free_buffer_fops);\n\n\ttrace_create_file(\"trace_marker\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_fops);\n\n\tfile = __find_event_file(tr, \"ftrace\", \"print\");\n\tif (file && file->dir)\n\t\ttrace_create_file(\"trigger\", 0644, file->dir, file,\n\t\t\t\t  &event_trigger_fops);\n\ttr->trace_marker_file = file;\n\n\ttrace_create_file(\"trace_marker_raw\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_raw_fops);\n\n\ttrace_create_file(\"trace_clock\", 0644, d_tracer, tr,\n\t\t\t  &trace_clock_fops);\n\n\ttrace_create_file(\"tracing_on\", 0644, d_tracer,\n\t\t\t  tr, &rb_simple_fops);\n\n\ttrace_create_file(\"timestamp_mode\", 0444, d_tracer, tr,\n\t\t\t  &trace_time_stamp_mode_fops);\n\n\ttr->buffer_percent = 50;\n\n\ttrace_create_file(\"buffer_percent\", 0444, d_tracer,\n\t\t\ttr, &buffer_percent_fops);\n\n\tcreate_trace_options_dir(tr);\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\ttrace_create_file(\"tracing_max_latency\", 0644, d_tracer,\n\t\t\t&tr->max_latency, &tracing_max_lat_fops);\n#endif\n\n\tif (ftrace_create_function_files(tr, d_tracer))\n\t\tWARN(1, \"Could not allocate function filter files\");\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_file(\"snapshot\", 0644, d_tracer,\n\t\t\t  tr, &snapshot_fops);\n#endif\n\n\tfor_each_tracing_cpu(cpu)\n\t\ttracing_init_tracefs_percpu(tr, cpu);\n\n\tftrace_init_tracefs(tr, d_tracer);\n}\n\nstatic struct vfsmount *trace_automount(struct dentry *mntpt, void *ingore)\n{\n\tstruct vfsmount *mnt;\n\tstruct file_system_type *type;\n\n\t/*\n\t * To maintain backward compatibility for tools that mount\n\t * debugfs to get to the tracing facility, tracefs is automatically\n\t * mounted to the debugfs/tracing directory.\n\t */\n\ttype = get_fs_type(\"tracefs\");\n\tif (!type)\n\t\treturn NULL;\n\tmnt = vfs_submount(mntpt, type, \"tracefs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn NULL;\n\tmntget(mnt);\n\n\treturn mnt;\n}\n\n/**\n * tracing_init_dentry - initialize top level trace array\n *\n * This is called when creating files or directories in the tracing\n * directory. It is called via fs_initcall() by any of the boot up code\n * and expects to return the dentry of the top level tracing directory.\n */\nstruct dentry *tracing_init_dentry(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\t/* The top level trace array uses  NULL as parent */\n\tif (tr->dir)\n\t\treturn NULL;\n\n\tif (WARN_ON(!tracefs_initialized()) ||\n\t\t(IS_ENABLED(CONFIG_DEBUG_FS) &&\n\t\t WARN_ON(!debugfs_initialized())))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/*\n\t * As there may still be users that expect the tracing\n\t * files to exist in debugfs/tracing, we must automount\n\t * the tracefs file system there, so older tools still\n\t * work with the newer kerenl.\n\t */\n\ttr->dir = debugfs_create_automount(\"tracing\", NULL,\n\t\t\t\t\t   trace_automount, NULL);\n\tif (!tr->dir) {\n\t\tpr_warn_once(\"Could not create debugfs directory 'tracing'\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn NULL;\n}\n\nextern struct trace_eval_map *__start_ftrace_eval_maps[];\nextern struct trace_eval_map *__stop_ftrace_eval_maps[];\n\nstatic void __init trace_eval_init(void)\n{\n\tint len;\n\n\tlen = __stop_ftrace_eval_maps - __start_ftrace_eval_maps;\n\ttrace_insert_eval_map(NULL, __start_ftrace_eval_maps, len);\n}\n\n#ifdef CONFIG_MODULES\nstatic void trace_module_add_evals(struct module *mod)\n{\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\t/*\n\t * Modules with bad taint do not have events created, do\n\t * not bother with enums either.\n\t */\n\tif (trace_module_has_bad_taint(mod))\n\t\treturn;\n\n\ttrace_insert_eval_map(mod, mod->trace_evals, mod->num_trace_evals);\n}\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic void trace_module_remove_evals(struct module *mod)\n{\n\tunion trace_eval_map_item *map;\n\tunion trace_eval_map_item **last = &trace_eval_maps;\n\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tmap = trace_eval_maps;\n\n\twhile (map) {\n\t\tif (map->head.mod == mod)\n\t\t\tbreak;\n\t\tmap = trace_eval_jmp_to_tail(map);\n\t\tlast = &map->tail.next;\n\t\tmap = map->tail.next;\n\t}\n\tif (!map)\n\t\tgoto out;\n\n\t*last = trace_eval_jmp_to_tail(map)->tail.next;\n\tkfree(map);\n out:\n\tmutex_unlock(&trace_eval_mutex);\n}\n#else\nstatic inline void trace_module_remove_evals(struct module *mod) { }\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_evals(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_evals(mod);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 0,\n};\n#endif /* CONFIG_MODULES */\n\nstatic __init int tracer_init_tracefs(void)\n{\n\tstruct dentry *d_tracer;\n\n\ttrace_access_lock_init();\n\n\td_tracer = tracing_init_dentry();\n\tif (IS_ERR(d_tracer))\n\t\treturn 0;\n\n\tevent_trace_init();\n\n\tinit_tracer_tracefs(&global_trace, d_tracer);\n\tftrace_init_tracefs_toplevel(&global_trace, d_tracer);\n\n\ttrace_create_file(\"tracing_thresh\", 0644, d_tracer,\n\t\t\t&global_trace, &tracing_thresh_fops);\n\n\ttrace_create_file(\"README\", 0444, d_tracer,\n\t\t\tNULL, &tracing_readme_fops);\n\n\ttrace_create_file(\"saved_cmdlines\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_cmdlines_fops);\n\n\ttrace_create_file(\"saved_cmdlines_size\", 0644, d_tracer,\n\t\t\t  NULL, &tracing_saved_cmdlines_size_fops);\n\n\ttrace_create_file(\"saved_tgids\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_tgids_fops);\n\n\ttrace_eval_init();\n\n\ttrace_create_eval_file(d_tracer);\n\n#ifdef CONFIG_MODULES\n\tregister_module_notifier(&trace_module_nb);\n#endif\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\ttrace_create_file(\"dyn_ftrace_total_info\", 0444, d_tracer,\n\t\t\t&ftrace_update_tot_cnt, &tracing_dyn_info_fops);\n#endif\n\n\tcreate_trace_instances(d_tracer);\n\n\tupdate_tracer_options(&global_trace);\n\n\treturn 0;\n}\n\nstatic int trace_panic_handler(struct notifier_block *this,\n\t\t\t       unsigned long event, void *unused)\n{\n\tif (ftrace_dump_on_oops)\n\t\tftrace_dump(ftrace_dump_on_oops);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_panic_notifier = {\n\t.notifier_call  = trace_panic_handler,\n\t.next           = NULL,\n\t.priority       = 150   /* priority: INT_MAX >= x >= 0 */\n};\n\nstatic int trace_die_handler(struct notifier_block *self,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tswitch (val) {\n\tcase DIE_OOPS:\n\t\tif (ftrace_dump_on_oops)\n\t\t\tftrace_dump(ftrace_dump_on_oops);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_die_notifier = {\n\t.notifier_call = trace_die_handler,\n\t.priority = 200\n};\n\n/*\n * printk is set to max of 1024, we really don't need it that big.\n * Nothing should be printing 1000 characters anyway.\n */\n#define TRACE_MAX_PRINT\t\t1000\n\n/*\n * Define here KERN_TRACE so that we have one place to modify\n * it if we decide to change what log level the ftrace dump\n * should be at.\n */\n#define KERN_TRACE\t\tKERN_EMERG\n\nvoid\ntrace_printk_seq(struct trace_seq *s)\n{\n\t/* Probably should print a warning here. */\n\tif (s->seq.len >= TRACE_MAX_PRINT)\n\t\ts->seq.len = TRACE_MAX_PRINT;\n\n\t/*\n\t * More paranoid code. Although the buffer size is set to\n\t * PAGE_SIZE, and TRACE_MAX_PRINT is 1000, this is just\n\t * an extra layer of protection.\n\t */\n\tif (WARN_ON_ONCE(s->seq.len >= s->seq.size))\n\t\ts->seq.len = s->seq.size - 1;\n\n\t/* should be zero ended, but we are paranoid. */\n\ts->buffer[s->seq.len] = 0;\n\n\tprintk(KERN_TRACE \"%s\", s->buffer);\n\n\ttrace_seq_init(s);\n}\n\nvoid trace_init_global_iter(struct trace_iterator *iter)\n{\n\titer->tr = &global_trace;\n\titer->trace = iter->tr->current_trace;\n\titer->cpu_file = RING_BUFFER_ALL_CPUS;\n\titer->trace_buffer = &global_trace.trace_buffer;\n\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[iter->tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n}\n\nvoid ftrace_dump(enum ftrace_dump_mode oops_dump_mode)\n{\n\t/* use static because iter can be a bit big for the stack */\n\tstatic struct trace_iterator iter;\n\tstatic atomic_t dump_running;\n\tstruct trace_array *tr = &global_trace;\n\tunsigned int old_userobj;\n\tunsigned long flags;\n\tint cnt = 0, cpu;\n\n\t/* Only allow one dump user at a time. */\n\tif (atomic_inc_return(&dump_running) != 1) {\n\t\tatomic_dec(&dump_running);\n\t\treturn;\n\t}\n\n\t/*\n\t * Always turn off tracing when we dump.\n\t * We don't need to show trace output of what happens\n\t * between multiple crashes.\n\t *\n\t * If the user does a sysrq-z, then they can re-enable\n\t * tracing with echo 1 > tracing_on.\n\t */\n\ttracing_off();\n\n\tlocal_irq_save(flags);\n\tprintk_nmi_direct_enter();\n\n\t/* Simulate the iterator */\n\ttrace_init_global_iter(&iter);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_inc(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\n\told_userobj = tr->trace_flags & TRACE_ITER_SYM_USEROBJ;\n\n\t/* don't look at user memory in panic mode */\n\ttr->trace_flags &= ~TRACE_ITER_SYM_USEROBJ;\n\n\tswitch (oops_dump_mode) {\n\tcase DUMP_ALL:\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t\tbreak;\n\tcase DUMP_ORIG:\n\t\titer.cpu_file = raw_smp_processor_id();\n\t\tbreak;\n\tcase DUMP_NONE:\n\t\tgoto out_enable;\n\tdefault:\n\t\tprintk(KERN_TRACE \"Bad dumping mode, switching to all CPUs dump\\n\");\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t}\n\n\tprintk(KERN_TRACE \"Dumping ftrace buffer:\\n\");\n\n\t/* Did function tracer already get disabled? */\n\tif (ftrace_is_dead()) {\n\t\tprintk(\"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\");\n\t\tprintk(\"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n\t}\n\n\t/*\n\t * We need to stop all tracing on all CPUS to read the\n\t * the next buffer. This is a bit expensive, but is\n\t * not done often. We fill all what we can read,\n\t * and then release the locks again.\n\t */\n\n\twhile (!trace_empty(&iter)) {\n\n\t\tif (!cnt)\n\t\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n\t\tcnt++;\n\n\t\t/* reset all but tr, trace, and overruns */\n\t\tmemset(&iter.seq, 0,\n\t\t       sizeof(struct trace_iterator) -\n\t\t       offsetof(struct trace_iterator, seq));\n\t\titer.iter_flags |= TRACE_FILE_LAT_FMT;\n\t\titer.pos = -1;\n\n\t\tif (trace_find_next_entry_inc(&iter) != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = print_trace_line(&iter);\n\t\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\t\ttrace_consume(&iter);\n\t\t}\n\t\ttouch_nmi_watchdog();\n\n\t\ttrace_printk_seq(&iter.seq);\n\t}\n\n\tif (!cnt)\n\t\tprintk(KERN_TRACE \"   (ftrace buffer empty)\\n\");\n\telse\n\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n out_enable:\n\ttr->trace_flags |= old_userobj;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_dec(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\tatomic_dec(&dump_running);\n\tprintk_nmi_direct_exit();\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(ftrace_dump);\n\nint trace_run_command(const char *buf, int (*createfn)(int, char **))\n{\n\tchar **argv;\n\tint argc, ret;\n\n\targc = 0;\n\tret = 0;\n\targv = argv_split(GFP_KERNEL, buf, &argc);\n\tif (!argv)\n\t\treturn -ENOMEM;\n\n\tif (argc)\n\t\tret = createfn(argc, argv);\n\n\targv_free(argv);\n\n\treturn ret;\n}\n\n#define WRITE_BUFSIZE  4096\n\nssize_t trace_parse_run_command(struct file *file, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\tint (*createfn)(int, char **))\n{\n\tchar *kbuf, *buf, *tmp;\n\tint ret = 0;\n\tsize_t done = 0;\n\tsize_t size;\n\n\tkbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\twhile (done < count) {\n\t\tsize = count - done;\n\n\t\tif (size >= WRITE_BUFSIZE)\n\t\t\tsize = WRITE_BUFSIZE - 1;\n\n\t\tif (copy_from_user(kbuf, buffer + done, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkbuf[size] = '\\0';\n\t\tbuf = kbuf;\n\t\tdo {\n\t\t\ttmp = strchr(buf, '\\n');\n\t\t\tif (tmp) {\n\t\t\t\t*tmp = '\\0';\n\t\t\t\tsize = tmp - buf + 1;\n\t\t\t} else {\n\t\t\t\tsize = strlen(buf);\n\t\t\t\tif (done + size < count) {\n\t\t\t\t\tif (buf != kbuf)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t/* This can accept WRITE_BUFSIZE - 2 ('\\n' + '\\0') */\n\t\t\t\t\tpr_warn(\"Line length is too long: Should be less than %d\\n\",\n\t\t\t\t\t\tWRITE_BUFSIZE - 2);\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdone += size;\n\n\t\t\t/* Remove comments */\n\t\t\ttmp = strchr(buf, '#');\n\n\t\t\tif (tmp)\n\t\t\t\t*tmp = '\\0';\n\n\t\t\tret = trace_run_command(buf, createfn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbuf += size;\n\n\t\t} while (done < count);\n\t}\n\tret = done;\n\nout:\n\tkfree(kbuf);\n\n\treturn ret;\n}\n\n__init static int tracer_alloc_buffers(void)\n{\n\tint ring_buf_size;\n\tint ret = -ENOMEM;\n\n\t/*\n\t * Make sure we don't accidently add more trace options\n\t * than we have bits for.\n\t */\n\tBUILD_BUG_ON(TRACE_ITER_LAST_BIT > TRACE_FLAGS_MAX_SIZE);\n\n\tif (!alloc_cpumask_var(&tracing_buffer_mask, GFP_KERNEL))\n\t\tgoto out;\n\n\tif (!alloc_cpumask_var(&global_trace.tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_buffer_mask;\n\n\t/* Only allocate trace_printk buffers if a trace_printk exists */\n\tif (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt)\n\t\t/* Must be called before global_trace.buffer is allocated */\n\t\ttrace_printk_init_buffers();\n\n\t/* To save memory, keep the ring buffer size to its minimum */\n\tif (ring_buffer_expanded)\n\t\tring_buf_size = trace_buf_size;\n\telse\n\t\tring_buf_size = 1;\n\n\tcpumask_copy(tracing_buffer_mask, cpu_possible_mask);\n\tcpumask_copy(global_trace.tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&global_trace.start_lock);\n\n\t/*\n\t * The prepare callbacks allocates some memory for the ring buffer. We\n\t * don't free the buffer if the if the CPU goes down. If we were to free\n\t * the buffer, then the user would lose any trace that was in the\n\t * buffer. The memory will be removed once the \"instance\" is removed.\n\t */\n\tret = cpuhp_setup_state_multi(CPUHP_TRACE_RB_PREPARE,\n\t\t\t\t      \"trace/RB:preapre\", trace_rb_cpu_prepare,\n\t\t\t\t      NULL);\n\tif (ret < 0)\n\t\tgoto out_free_cpumask;\n\t/* Used for event triggers */\n\tret = -ENOMEM;\n\ttemp_buffer = ring_buffer_alloc(PAGE_SIZE, RB_FL_OVERWRITE);\n\tif (!temp_buffer)\n\t\tgoto out_rm_hp_state;\n\n\tif (trace_create_savedcmd() < 0)\n\t\tgoto out_free_temp_buffer;\n\n\t/* TODO: make the number of buffers hot pluggable with CPUS */\n\tif (allocate_trace_buffers(&global_trace, ring_buf_size) < 0) {\n\t\tprintk(KERN_ERR \"tracer: failed to allocate ring buffer!\\n\");\n\t\tWARN_ON(1);\n\t\tgoto out_free_savedcmd;\n\t}\n\n\tif (global_trace.buffer_disabled)\n\t\ttracing_off();\n\n\tif (trace_boot_clock) {\n\t\tret = tracing_set_clock(&global_trace, trace_boot_clock);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Trace clock %s not defined, going back to default\\n\",\n\t\t\t\ttrace_boot_clock);\n\t}\n\n\t/*\n\t * register_tracer() might reference current_trace, so it\n\t * needs to be set before we register anything. This is\n\t * just a bootstrap of current_trace anyway.\n\t */\n\tglobal_trace.current_trace = &nop_trace;\n\n\tglobal_trace.max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\tftrace_init_global_array_ops(&global_trace);\n\n\tinit_trace_flags_index(&global_trace);\n\n\tregister_tracer(&nop_trace);\n\n\t/* Function tracing may start here (via kernel command line) */\n\tinit_function_trace();\n\n\t/* All seems OK, enable tracing */\n\ttracing_disabled = 0;\n\n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t\t       &trace_panic_notifier);\n\n\tregister_die_notifier(&trace_die_notifier);\n\n\tglobal_trace.flags = TRACE_ARRAY_FL_GLOBAL;\n\n\tINIT_LIST_HEAD(&global_trace.systems);\n\tINIT_LIST_HEAD(&global_trace.events);\n\tINIT_LIST_HEAD(&global_trace.hist_vars);\n\tlist_add(&global_trace.list, &ftrace_trace_arrays);\n\n\tapply_trace_boot_options();\n\n\tregister_snapshot_cmd();\n\n\treturn 0;\n\nout_free_savedcmd:\n\tfree_saved_cmdlines_buffer(savedcmd);\nout_free_temp_buffer:\n\tring_buffer_free(temp_buffer);\nout_rm_hp_state:\n\tcpuhp_remove_multi_state(CPUHP_TRACE_RB_PREPARE);\nout_free_cpumask:\n\tfree_cpumask_var(global_trace.tracing_cpumask);\nout_free_buffer_mask:\n\tfree_cpumask_var(tracing_buffer_mask);\nout:\n\treturn ret;\n}\n\nvoid __init early_trace_init(void)\n{\n\tif (tracepoint_printk) {\n\t\ttracepoint_print_iter =\n\t\t\tkmalloc(sizeof(*tracepoint_print_iter), GFP_KERNEL);\n\t\tif (WARN_ON(!tracepoint_print_iter))\n\t\t\ttracepoint_printk = 0;\n\t\telse\n\t\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\t}\n\ttracer_alloc_buffers();\n}\n\nvoid __init trace_init(void)\n{\n\ttrace_event_init();\n}\n\n__init static int clear_boot_tracer(void)\n{\n\t/*\n\t * The default tracer at boot buffer is an init section.\n\t * This function is called in lateinit. If we did not\n\t * find the boot tracer, then clear it out, to prevent\n\t * later registration from accessing the buffer that is\n\t * about to be freed.\n\t */\n\tif (!default_bootup_tracer)\n\t\treturn 0;\n\n\tprintk(KERN_INFO \"ftrace bootup tracer '%s' not registered.\\n\",\n\t       default_bootup_tracer);\n\tdefault_bootup_tracer = NULL;\n\n\treturn 0;\n}\n\nfs_initcall(tracer_init_tracefs);\nlate_initcall_sync(clear_boot_tracer);\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n__init static int tracing_set_default_clock(void)\n{\n\t/* sched_clock_stable() is determined in late_initcall */\n\tif (!trace_boot_clock && !sched_clock_stable()) {\n\t\tprintk(KERN_WARNING\n\t\t       \"Unstable clock detected, switching default tracing clock to \\\"global\\\"\\n\"\n\t\t       \"If you want to keep using the local clock, then add:\\n\"\n\t\t       \"  \\\"trace_clock=local\\\"\\n\"\n\t\t       \"on the kernel command line\\n\");\n\t\ttracing_set_clock(&global_trace, \"global\");\n\t}\n\n\treturn 0;\n}\nlate_initcall_sync(tracing_set_default_clock);\n#endif\n"], "fixing_code": ["/*\n  FUSE: Filesystem in Userspace\n  Copyright (C) 2001-2008  Miklos Szeredi <miklos@szeredi.hu>\n\n  This program can be distributed under the terms of the GNU GPL.\n  See the file COPYING.\n*/\n\n#include \"fuse_i.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/poll.h>\n#include <linux/sched/signal.h>\n#include <linux/uio.h>\n#include <linux/miscdevice.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/swap.h>\n#include <linux/splice.h>\n#include <linux/sched.h>\n\nMODULE_ALIAS_MISCDEV(FUSE_MINOR);\nMODULE_ALIAS(\"devname:fuse\");\n\n/* Ordinary requests have even IDs, while interrupts IDs are odd */\n#define FUSE_INT_REQ_BIT (1ULL << 0)\n#define FUSE_REQ_ID_STEP (1ULL << 1)\n\nstatic struct kmem_cache *fuse_req_cachep;\n\nstatic struct fuse_dev *fuse_get_dev(struct file *file)\n{\n\t/*\n\t * Lockless access is OK, because file->private data is set\n\t * once during mount and is valid until the file is released.\n\t */\n\treturn READ_ONCE(file->private_data);\n}\n\nstatic void fuse_request_init(struct fuse_req *req, struct page **pages,\n\t\t\t      struct fuse_page_desc *page_descs,\n\t\t\t      unsigned npages)\n{\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\trefcount_set(&req->count, 1);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\t__set_bit(FR_PENDING, &req->flags);\n}\n\nstatic struct page **fuse_req_pages_alloc(unsigned int npages, gfp_t flags,\n\t\t\t\t\t  struct fuse_page_desc **desc)\n{\n\tstruct page **pages;\n\n\tpages = kzalloc(npages * (sizeof(struct page *) +\n\t\t\t\t  sizeof(struct fuse_page_desc)), flags);\n\t*desc = (void *) pages + npages * sizeof(struct page *);\n\n\treturn pages;\n}\n\nstatic struct fuse_req *__fuse_request_alloc(unsigned npages, gfp_t flags)\n{\n\tstruct fuse_req *req = kmem_cache_zalloc(fuse_req_cachep, flags);\n\tif (req) {\n\t\tstruct page **pages = NULL;\n\t\tstruct fuse_page_desc *page_descs = NULL;\n\n\t\tWARN_ON(npages > FUSE_MAX_MAX_PAGES);\n\t\tif (npages > FUSE_REQ_INLINE_PAGES) {\n\t\t\tpages = fuse_req_pages_alloc(npages, flags,\n\t\t\t\t\t\t     &page_descs);\n\t\t\tif (!pages) {\n\t\t\t\tkmem_cache_free(fuse_req_cachep, req);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t} else if (npages) {\n\t\t\tpages = req->inline_pages;\n\t\t\tpage_descs = req->inline_page_descs;\n\t\t}\n\n\t\tfuse_request_init(req, pages, page_descs, npages);\n\t}\n\treturn req;\n}\n\nstruct fuse_req *fuse_request_alloc(unsigned npages)\n{\n\treturn __fuse_request_alloc(npages, GFP_KERNEL);\n}\nEXPORT_SYMBOL_GPL(fuse_request_alloc);\n\nstruct fuse_req *fuse_request_alloc_nofs(unsigned npages)\n{\n\treturn __fuse_request_alloc(npages, GFP_NOFS);\n}\n\nstatic void fuse_req_pages_free(struct fuse_req *req)\n{\n\tif (req->pages != req->inline_pages)\n\t\tkfree(req->pages);\n}\n\nbool fuse_req_realloc_pages(struct fuse_conn *fc, struct fuse_req *req,\n\t\t\t    gfp_t flags)\n{\n\tstruct page **pages;\n\tstruct fuse_page_desc *page_descs;\n\tunsigned int npages = min_t(unsigned int,\n\t\t\t\t    max_t(unsigned int, req->max_pages * 2,\n\t\t\t\t\t  FUSE_DEFAULT_MAX_PAGES_PER_REQ),\n\t\t\t\t    fc->max_pages);\n\tWARN_ON(npages <= req->max_pages);\n\n\tpages = fuse_req_pages_alloc(npages, flags, &page_descs);\n\tif (!pages)\n\t\treturn false;\n\n\tmemcpy(pages, req->pages, sizeof(struct page *) * req->max_pages);\n\tmemcpy(page_descs, req->page_descs,\n\t       sizeof(struct fuse_page_desc) * req->max_pages);\n\tfuse_req_pages_free(req);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\n\treturn true;\n}\n\nvoid fuse_request_free(struct fuse_req *req)\n{\n\tfuse_req_pages_free(req);\n\tkmem_cache_free(fuse_req_cachep, req);\n}\n\nvoid __fuse_get_request(struct fuse_req *req)\n{\n\trefcount_inc(&req->count);\n}\n\n/* Must be called with > 1 refcount */\nstatic void __fuse_put_request(struct fuse_req *req)\n{\n\trefcount_dec(&req->count);\n}\n\nvoid fuse_set_initialized(struct fuse_conn *fc)\n{\n\t/* Make sure stores before this are seen on another CPU */\n\tsmp_wmb();\n\tfc->initialized = 1;\n}\n\nstatic bool fuse_block_alloc(struct fuse_conn *fc, bool for_background)\n{\n\treturn !fc->initialized || (for_background && fc->blocked);\n}\n\nstatic void fuse_drop_waiting(struct fuse_conn *fc)\n{\n\t/*\n\t * lockess check of fc->connected is okay, because atomic_dec_and_test()\n\t * provides a memory barrier mached with the one in fuse_wait_aborted()\n\t * to ensure no wake-up is missed.\n\t */\n\tif (atomic_dec_and_test(&fc->num_waiting) &&\n\t    !READ_ONCE(fc->connected)) {\n\t\t/* wake up aborters */\n\t\twake_up_all(&fc->blocked_waitq);\n\t}\n}\n\nstatic struct fuse_req *__fuse_get_req(struct fuse_conn *fc, unsigned npages,\n\t\t\t\t       bool for_background)\n{\n\tstruct fuse_req *req;\n\tint err;\n\tatomic_inc(&fc->num_waiting);\n\n\tif (fuse_block_alloc(fc, for_background)) {\n\t\terr = -EINTR;\n\t\tif (wait_event_killable_exclusive(fc->blocked_waitq,\n\t\t\t\t!fuse_block_alloc(fc, for_background)))\n\t\t\tgoto out;\n\t}\n\t/* Matches smp_wmb() in fuse_set_initialized() */\n\tsmp_rmb();\n\n\terr = -ENOTCONN;\n\tif (!fc->connected)\n\t\tgoto out;\n\n\terr = -ECONNREFUSED;\n\tif (fc->conn_error)\n\t\tgoto out;\n\n\treq = fuse_request_alloc(npages);\n\terr = -ENOMEM;\n\tif (!req) {\n\t\tif (for_background)\n\t\t\twake_up(&fc->blocked_waitq);\n\t\tgoto out;\n\t}\n\n\treq->in.h.uid = from_kuid(fc->user_ns, current_fsuid());\n\treq->in.h.gid = from_kgid(fc->user_ns, current_fsgid());\n\treq->in.h.pid = pid_nr_ns(task_pid(current), fc->pid_ns);\n\n\t__set_bit(FR_WAITING, &req->flags);\n\tif (for_background)\n\t\t__set_bit(FR_BACKGROUND, &req->flags);\n\n\tif (unlikely(req->in.h.uid == ((uid_t)-1) ||\n\t\t     req->in.h.gid == ((gid_t)-1))) {\n\t\tfuse_put_request(fc, req);\n\t\treturn ERR_PTR(-EOVERFLOW);\n\t}\n\treturn req;\n\n out:\n\tfuse_drop_waiting(fc);\n\treturn ERR_PTR(err);\n}\n\nstruct fuse_req *fuse_get_req(struct fuse_conn *fc, unsigned npages)\n{\n\treturn __fuse_get_req(fc, npages, false);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req);\n\nstruct fuse_req *fuse_get_req_for_background(struct fuse_conn *fc,\n\t\t\t\t\t     unsigned npages)\n{\n\treturn __fuse_get_req(fc, npages, true);\n}\nEXPORT_SYMBOL_GPL(fuse_get_req_for_background);\n\n/*\n * Return request in fuse_file->reserved_req.  However that may\n * currently be in use.  If that is the case, wait for it to become\n * available.\n */\nstatic struct fuse_req *get_reserved_req(struct fuse_conn *fc,\n\t\t\t\t\t struct file *file)\n{\n\tstruct fuse_req *req = NULL;\n\tstruct fuse_file *ff = file->private_data;\n\n\tdo {\n\t\twait_event(fc->reserved_req_waitq, ff->reserved_req);\n\t\tspin_lock(&fc->lock);\n\t\tif (ff->reserved_req) {\n\t\t\treq = ff->reserved_req;\n\t\t\tff->reserved_req = NULL;\n\t\t\treq->stolen_file = get_file(file);\n\t\t}\n\t\tspin_unlock(&fc->lock);\n\t} while (!req);\n\n\treturn req;\n}\n\n/*\n * Put stolen request back into fuse_file->reserved_req\n */\nstatic void put_reserved_req(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct file *file = req->stolen_file;\n\tstruct fuse_file *ff = file->private_data;\n\n\tWARN_ON(req->max_pages);\n\tspin_lock(&fc->lock);\n\tmemset(req, 0, sizeof(*req));\n\tfuse_request_init(req, NULL, NULL, 0);\n\tBUG_ON(ff->reserved_req);\n\tff->reserved_req = req;\n\twake_up_all(&fc->reserved_req_waitq);\n\tspin_unlock(&fc->lock);\n\tfput(file);\n}\n\n/*\n * Gets a requests for a file operation, always succeeds\n *\n * This is used for sending the FLUSH request, which must get to\n * userspace, due to POSIX locks which may need to be unlocked.\n *\n * If allocation fails due to OOM, use the reserved request in\n * fuse_file.\n *\n * This is very unlikely to deadlock accidentally, since the\n * filesystem should not have it's own file open.  If deadlock is\n * intentional, it can still be broken by \"aborting\" the filesystem.\n */\nstruct fuse_req *fuse_get_req_nofail_nopages(struct fuse_conn *fc,\n\t\t\t\t\t     struct file *file)\n{\n\tstruct fuse_req *req;\n\n\tatomic_inc(&fc->num_waiting);\n\twait_event(fc->blocked_waitq, fc->initialized);\n\t/* Matches smp_wmb() in fuse_set_initialized() */\n\tsmp_rmb();\n\treq = fuse_request_alloc(0);\n\tif (!req)\n\t\treq = get_reserved_req(fc, file);\n\n\treq->in.h.uid = from_kuid_munged(fc->user_ns, current_fsuid());\n\treq->in.h.gid = from_kgid_munged(fc->user_ns, current_fsgid());\n\treq->in.h.pid = pid_nr_ns(task_pid(current), fc->pid_ns);\n\n\t__set_bit(FR_WAITING, &req->flags);\n\t__clear_bit(FR_BACKGROUND, &req->flags);\n\treturn req;\n}\n\nvoid fuse_put_request(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tif (refcount_dec_and_test(&req->count)) {\n\t\tif (test_bit(FR_BACKGROUND, &req->flags)) {\n\t\t\t/*\n\t\t\t * We get here in the unlikely case that a background\n\t\t\t * request was allocated but not sent\n\t\t\t */\n\t\t\tspin_lock(&fc->bg_lock);\n\t\t\tif (!fc->blocked)\n\t\t\t\twake_up(&fc->blocked_waitq);\n\t\t\tspin_unlock(&fc->bg_lock);\n\t\t}\n\n\t\tif (test_bit(FR_WAITING, &req->flags)) {\n\t\t\t__clear_bit(FR_WAITING, &req->flags);\n\t\t\tfuse_drop_waiting(fc);\n\t\t}\n\n\t\tif (req->stolen_file)\n\t\t\tput_reserved_req(fc, req);\n\t\telse\n\t\t\tfuse_request_free(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_put_request);\n\nstatic unsigned len_args(unsigned numargs, struct fuse_arg *args)\n{\n\tunsigned nbytes = 0;\n\tunsigned i;\n\n\tfor (i = 0; i < numargs; i++)\n\t\tnbytes += args[i].size;\n\n\treturn nbytes;\n}\n\nstatic u64 fuse_get_unique(struct fuse_iqueue *fiq)\n{\n\tfiq->reqctr += FUSE_REQ_ID_STEP;\n\treturn fiq->reqctr;\n}\n\nstatic unsigned int fuse_req_hash(u64 unique)\n{\n\treturn hash_long(unique & ~FUSE_INT_REQ_BIT, FUSE_PQ_HASH_BITS);\n}\n\nstatic void queue_request(struct fuse_iqueue *fiq, struct fuse_req *req)\n{\n\treq->in.h.len = sizeof(struct fuse_in_header) +\n\t\tlen_args(req->in.numargs, (struct fuse_arg *) req->in.args);\n\tlist_add_tail(&req->list, &fiq->pending);\n\twake_up_locked(&fiq->waitq);\n\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n}\n\nvoid fuse_queue_forget(struct fuse_conn *fc, struct fuse_forget_link *forget,\n\t\t       u64 nodeid, u64 nlookup)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tforget->forget_one.nodeid = nodeid;\n\tforget->forget_one.nlookup = nlookup;\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (fiq->connected) {\n\t\tfiq->forget_list_tail->next = forget;\n\t\tfiq->forget_list_tail = forget;\n\t\twake_up_locked(&fiq->waitq);\n\t\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n\t} else {\n\t\tkfree(forget);\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n}\n\nstatic void flush_bg_queue(struct fuse_conn *fc)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\twhile (fc->active_background < fc->max_background &&\n\t       !list_empty(&fc->bg_queue)) {\n\t\tstruct fuse_req *req;\n\n\t\treq = list_first_entry(&fc->bg_queue, struct fuse_req, list);\n\t\tlist_del(&req->list);\n\t\tfc->active_background++;\n\t\tspin_lock(&fiq->waitq.lock);\n\t\treq->in.h.unique = fuse_get_unique(fiq);\n\t\tqueue_request(fiq, req);\n\t\tspin_unlock(&fiq->waitq.lock);\n\t}\n}\n\n/*\n * This function is called when a request is finished.  Either a reply\n * has arrived or it was aborted (and not yet sent) or some error\n * occurred during communication with userspace, or the device file\n * was closed.  The requester thread is woken up (if still waiting),\n * the 'end' callback is called if given, else the reference to the\n * request is released\n */\nstatic void request_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tif (test_and_set_bit(FR_FINISHED, &req->flags))\n\t\tgoto put_request;\n\n\tspin_lock(&fiq->waitq.lock);\n\tlist_del_init(&req->intr_entry);\n\tspin_unlock(&fiq->waitq.lock);\n\tWARN_ON(test_bit(FR_PENDING, &req->flags));\n\tWARN_ON(test_bit(FR_SENT, &req->flags));\n\tif (test_bit(FR_BACKGROUND, &req->flags)) {\n\t\tspin_lock(&fc->bg_lock);\n\t\tclear_bit(FR_BACKGROUND, &req->flags);\n\t\tif (fc->num_background == fc->max_background) {\n\t\t\tfc->blocked = 0;\n\t\t\twake_up(&fc->blocked_waitq);\n\t\t} else if (!fc->blocked) {\n\t\t\t/*\n\t\t\t * Wake up next waiter, if any.  It's okay to use\n\t\t\t * waitqueue_active(), as we've already synced up\n\t\t\t * fc->blocked with waiters with the wake_up() call\n\t\t\t * above.\n\t\t\t */\n\t\t\tif (waitqueue_active(&fc->blocked_waitq))\n\t\t\t\twake_up(&fc->blocked_waitq);\n\t\t}\n\n\t\tif (fc->num_background == fc->congestion_threshold && fc->sb) {\n\t\t\tclear_bdi_congested(fc->sb->s_bdi, BLK_RW_SYNC);\n\t\t\tclear_bdi_congested(fc->sb->s_bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tfc->num_background--;\n\t\tfc->active_background--;\n\t\tflush_bg_queue(fc);\n\t\tspin_unlock(&fc->bg_lock);\n\t}\n\twake_up(&req->waitq);\n\tif (req->end)\n\t\treq->end(fc, req);\nput_request:\n\tfuse_put_request(fc, req);\n}\n\nstatic void queue_interrupt(struct fuse_iqueue *fiq, struct fuse_req *req)\n{\n\tspin_lock(&fiq->waitq.lock);\n\tif (test_bit(FR_FINISHED, &req->flags)) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treturn;\n\t}\n\tif (list_empty(&req->intr_entry)) {\n\t\tlist_add_tail(&req->intr_entry, &fiq->interrupts);\n\t\twake_up_locked(&fiq->waitq);\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n}\n\nstatic void request_wait_answer(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\tint err;\n\n\tif (!fc->no_interrupt) {\n\t\t/* Any signal may interrupt this */\n\t\terr = wait_event_interruptible(req->waitq,\n\t\t\t\t\ttest_bit(FR_FINISHED, &req->flags));\n\t\tif (!err)\n\t\t\treturn;\n\n\t\tset_bit(FR_INTERRUPTED, &req->flags);\n\t\t/* matches barrier in fuse_dev_do_read() */\n\t\tsmp_mb__after_atomic();\n\t\tif (test_bit(FR_SENT, &req->flags))\n\t\t\tqueue_interrupt(fiq, req);\n\t}\n\n\tif (!test_bit(FR_FORCE, &req->flags)) {\n\t\t/* Only fatal signals may interrupt this */\n\t\terr = wait_event_killable(req->waitq,\n\t\t\t\t\ttest_bit(FR_FINISHED, &req->flags));\n\t\tif (!err)\n\t\t\treturn;\n\n\t\tspin_lock(&fiq->waitq.lock);\n\t\t/* Request is not yet in userspace, bail out */\n\t\tif (test_bit(FR_PENDING, &req->flags)) {\n\t\t\tlist_del(&req->list);\n\t\t\tspin_unlock(&fiq->waitq.lock);\n\t\t\t__fuse_put_request(req);\n\t\t\treq->out.h.error = -EINTR;\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock(&fiq->waitq.lock);\n\t}\n\n\t/*\n\t * Either request is already in userspace, or it was forced.\n\t * Wait it out.\n\t */\n\twait_event(req->waitq, test_bit(FR_FINISHED, &req->flags));\n}\n\nstatic void __fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tBUG_ON(test_bit(FR_BACKGROUND, &req->flags));\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treq->out.h.error = -ENOTCONN;\n\t} else {\n\t\treq->in.h.unique = fuse_get_unique(fiq);\n\t\tqueue_request(fiq, req);\n\t\t/* acquire extra reference, since request is still needed\n\t\t   after request_end() */\n\t\t__fuse_get_request(req);\n\t\tspin_unlock(&fiq->waitq.lock);\n\n\t\trequest_wait_answer(fc, req);\n\t\t/* Pairs with smp_wmb() in request_end() */\n\t\tsmp_rmb();\n\t}\n}\n\nvoid fuse_request_send(struct fuse_conn *fc, struct fuse_req *req)\n{\n\t__set_bit(FR_ISREPLY, &req->flags);\n\tif (!test_bit(FR_WAITING, &req->flags)) {\n\t\t__set_bit(FR_WAITING, &req->flags);\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\t__fuse_request_send(fc, req);\n}\nEXPORT_SYMBOL_GPL(fuse_request_send);\n\nstatic void fuse_adjust_compat(struct fuse_conn *fc, struct fuse_args *args)\n{\n\tif (fc->minor < 4 && args->in.h.opcode == FUSE_STATFS)\n\t\targs->out.args[0].size = FUSE_COMPAT_STATFS_SIZE;\n\n\tif (fc->minor < 9) {\n\t\tswitch (args->in.h.opcode) {\n\t\tcase FUSE_LOOKUP:\n\t\tcase FUSE_CREATE:\n\t\tcase FUSE_MKNOD:\n\t\tcase FUSE_MKDIR:\n\t\tcase FUSE_SYMLINK:\n\t\tcase FUSE_LINK:\n\t\t\targs->out.args[0].size = FUSE_COMPAT_ENTRY_OUT_SIZE;\n\t\t\tbreak;\n\t\tcase FUSE_GETATTR:\n\t\tcase FUSE_SETATTR:\n\t\t\targs->out.args[0].size = FUSE_COMPAT_ATTR_OUT_SIZE;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (fc->minor < 12) {\n\t\tswitch (args->in.h.opcode) {\n\t\tcase FUSE_CREATE:\n\t\t\targs->in.args[0].size = sizeof(struct fuse_open_in);\n\t\t\tbreak;\n\t\tcase FUSE_MKNOD:\n\t\t\targs->in.args[0].size = FUSE_COMPAT_MKNOD_IN_SIZE;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nssize_t fuse_simple_request(struct fuse_conn *fc, struct fuse_args *args)\n{\n\tstruct fuse_req *req;\n\tssize_t ret;\n\n\treq = fuse_get_req(fc, 0);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\t/* Needs to be done after fuse_get_req() so that fc->minor is valid */\n\tfuse_adjust_compat(fc, args);\n\n\treq->in.h.opcode = args->in.h.opcode;\n\treq->in.h.nodeid = args->in.h.nodeid;\n\treq->in.numargs = args->in.numargs;\n\tmemcpy(req->in.args, args->in.args,\n\t       args->in.numargs * sizeof(struct fuse_in_arg));\n\treq->out.argvar = args->out.argvar;\n\treq->out.numargs = args->out.numargs;\n\tmemcpy(req->out.args, args->out.args,\n\t       args->out.numargs * sizeof(struct fuse_arg));\n\tfuse_request_send(fc, req);\n\tret = req->out.h.error;\n\tif (!ret && args->out.argvar) {\n\t\tBUG_ON(args->out.numargs != 1);\n\t\tret = req->out.args[0].size;\n\t}\n\tfuse_put_request(fc, req);\n\n\treturn ret;\n}\n\nbool fuse_request_queue_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tbool queued = false;\n\n\tWARN_ON(!test_bit(FR_BACKGROUND, &req->flags));\n\tif (!test_bit(FR_WAITING, &req->flags)) {\n\t\t__set_bit(FR_WAITING, &req->flags);\n\t\tatomic_inc(&fc->num_waiting);\n\t}\n\t__set_bit(FR_ISREPLY, &req->flags);\n\tspin_lock(&fc->bg_lock);\n\tif (likely(fc->connected)) {\n\t\tfc->num_background++;\n\t\tif (fc->num_background == fc->max_background)\n\t\t\tfc->blocked = 1;\n\t\tif (fc->num_background == fc->congestion_threshold && fc->sb) {\n\t\t\tset_bdi_congested(fc->sb->s_bdi, BLK_RW_SYNC);\n\t\t\tset_bdi_congested(fc->sb->s_bdi, BLK_RW_ASYNC);\n\t\t}\n\t\tlist_add_tail(&req->list, &fc->bg_queue);\n\t\tflush_bg_queue(fc);\n\t\tqueued = true;\n\t}\n\tspin_unlock(&fc->bg_lock);\n\n\treturn queued;\n}\n\nvoid fuse_request_send_background(struct fuse_conn *fc, struct fuse_req *req)\n{\n\tWARN_ON(!req->end);\n\tif (!fuse_request_queue_background(fc, req)) {\n\t\treq->out.h.error = -ENOTCONN;\n\t\treq->end(fc, req);\n\t\tfuse_put_request(fc, req);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_request_send_background);\n\nstatic int fuse_request_send_notify_reply(struct fuse_conn *fc,\n\t\t\t\t\t  struct fuse_req *req, u64 unique)\n{\n\tint err = -ENODEV;\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\t__clear_bit(FR_ISREPLY, &req->flags);\n\treq->in.h.unique = unique;\n\tspin_lock(&fiq->waitq.lock);\n\tif (fiq->connected) {\n\t\tqueue_request(fiq, req);\n\t\terr = 0;\n\t}\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn err;\n}\n\nvoid fuse_force_forget(struct file *file, u64 nodeid)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_req *req;\n\tstruct fuse_forget_in inarg;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.nlookup = 1;\n\treq = fuse_get_req_nofail_nopages(fc, file);\n\treq->in.h.opcode = FUSE_FORGET;\n\treq->in.h.nodeid = nodeid;\n\treq->in.numargs = 1;\n\treq->in.args[0].size = sizeof(inarg);\n\treq->in.args[0].value = &inarg;\n\t__clear_bit(FR_ISREPLY, &req->flags);\n\t__fuse_request_send(fc, req);\n\t/* ignore errors */\n\tfuse_put_request(fc, req);\n}\n\n/*\n * Lock the request.  Up to the next unlock_request() there mustn't be\n * anything that could cause a page-fault.  If the request was already\n * aborted bail out.\n */\nstatic int lock_request(struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tset_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}\n\n/*\n * Unlock request.  If it was aborted while locked, caller is responsible\n * for unlocking and ending the request.\n */\nstatic int unlock_request(struct fuse_req *req)\n{\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tclear_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}\n\nstruct fuse_copy_state {\n\tint write;\n\tstruct fuse_req *req;\n\tstruct iov_iter *iter;\n\tstruct pipe_buffer *pipebufs;\n\tstruct pipe_buffer *currbuf;\n\tstruct pipe_inode_info *pipe;\n\tunsigned long nr_segs;\n\tstruct page *pg;\n\tunsigned len;\n\tunsigned offset;\n\tunsigned move_pages:1;\n};\n\nstatic void fuse_copy_init(struct fuse_copy_state *cs, int write,\n\t\t\t   struct iov_iter *iter)\n{\n\tmemset(cs, 0, sizeof(*cs));\n\tcs->write = write;\n\tcs->iter = iter;\n}\n\n/* Unmap and put previous page of userspace buffer */\nstatic void fuse_copy_finish(struct fuse_copy_state *cs)\n{\n\tif (cs->currbuf) {\n\t\tstruct pipe_buffer *buf = cs->currbuf;\n\n\t\tif (cs->write)\n\t\t\tbuf->len = PAGE_SIZE - cs->len;\n\t\tcs->currbuf = NULL;\n\t} else if (cs->pg) {\n\t\tif (cs->write) {\n\t\t\tflush_dcache_page(cs->pg);\n\t\t\tset_page_dirty_lock(cs->pg);\n\t\t}\n\t\tput_page(cs->pg);\n\t}\n\tcs->pg = NULL;\n}\n\n/*\n * Get another pagefull of userspace buffer, and map it to kernel\n * address space, and lock request\n */\nstatic int fuse_copy_fill(struct fuse_copy_state *cs)\n{\n\tstruct page *page;\n\tint err;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\tif (cs->pipebufs) {\n\t\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\t\tif (!cs->write) {\n\t\t\terr = pipe_buf_confirm(cs->pipe, buf);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tBUG_ON(!cs->nr_segs);\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->pg = buf->page;\n\t\t\tcs->offset = buf->offset;\n\t\t\tcs->len = buf->len;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs--;\n\t\t} else {\n\t\t\tif (cs->nr_segs == cs->pipe->buffers)\n\t\t\t\treturn -EIO;\n\n\t\t\tpage = alloc_page(GFP_HIGHUSER);\n\t\t\tif (!page)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tbuf->page = page;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = 0;\n\n\t\t\tcs->currbuf = buf;\n\t\t\tcs->pg = page;\n\t\t\tcs->offset = 0;\n\t\t\tcs->len = PAGE_SIZE;\n\t\t\tcs->pipebufs++;\n\t\t\tcs->nr_segs++;\n\t\t}\n\t} else {\n\t\tsize_t off;\n\t\terr = iov_iter_get_pages(cs->iter, &page, PAGE_SIZE, 1, &off);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tBUG_ON(!err);\n\t\tcs->len = err;\n\t\tcs->offset = off;\n\t\tcs->pg = page;\n\t\tiov_iter_advance(cs->iter, err);\n\t}\n\n\treturn lock_request(cs->req);\n}\n\n/* Do as much copy to/from userspace buffer as we can */\nstatic int fuse_copy_do(struct fuse_copy_state *cs, void **val, unsigned *size)\n{\n\tunsigned ncpy = min(*size, cs->len);\n\tif (val) {\n\t\tvoid *pgaddr = kmap_atomic(cs->pg);\n\t\tvoid *buf = pgaddr + cs->offset;\n\n\t\tif (cs->write)\n\t\t\tmemcpy(buf, *val, ncpy);\n\t\telse\n\t\t\tmemcpy(*val, buf, ncpy);\n\n\t\tkunmap_atomic(pgaddr);\n\t\t*val += ncpy;\n\t}\n\t*size -= ncpy;\n\tcs->len -= ncpy;\n\tcs->offset += ncpy;\n\treturn ncpy;\n}\n\nstatic int fuse_check_page(struct page *page)\n{\n\tif (page_mapcount(page) ||\n\t    page->mapping != NULL ||\n\t    page_count(page) != 1 ||\n\t    (page->flags & PAGE_FLAGS_CHECK_AT_PREP &\n\t     ~(1 << PG_locked |\n\t       1 << PG_referenced |\n\t       1 << PG_uptodate |\n\t       1 << PG_lru |\n\t       1 << PG_active |\n\t       1 << PG_reclaim))) {\n\t\tprintk(KERN_WARNING \"fuse: trying to steal weird page\\n\");\n\t\tprintk(KERN_WARNING \"  page=%p index=%li flags=%08lx, count=%i, mapcount=%i, mapping=%p\\n\", page, page->index, page->flags, page_count(page), page_mapcount(page), page->mapping);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int fuse_try_move_page(struct fuse_copy_state *cs, struct page **pagep)\n{\n\tint err;\n\tstruct page *oldpage = *pagep;\n\tstruct page *newpage;\n\tstruct pipe_buffer *buf = cs->pipebufs;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\n\terr = pipe_buf_confirm(cs->pipe, buf);\n\tif (err)\n\t\treturn err;\n\n\tBUG_ON(!cs->nr_segs);\n\tcs->currbuf = buf;\n\tcs->len = buf->len;\n\tcs->pipebufs++;\n\tcs->nr_segs--;\n\n\tif (cs->len != PAGE_SIZE)\n\t\tgoto out_fallback;\n\n\tif (pipe_buf_steal(cs->pipe, buf) != 0)\n\t\tgoto out_fallback;\n\n\tnewpage = buf->page;\n\n\tif (!PageUptodate(newpage))\n\t\tSetPageUptodate(newpage);\n\n\tClearPageMappedToDisk(newpage);\n\n\tif (fuse_check_page(newpage) != 0)\n\t\tgoto out_fallback_unlock;\n\n\t/*\n\t * This is a new and locked page, it shouldn't be mapped or\n\t * have any special flags on it\n\t */\n\tif (WARN_ON(page_mapped(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(page_has_private(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageDirty(oldpage) || PageWriteback(oldpage)))\n\t\tgoto out_fallback_unlock;\n\tif (WARN_ON(PageMlocked(oldpage)))\n\t\tgoto out_fallback_unlock;\n\n\terr = replace_page_cache_page(oldpage, newpage, GFP_KERNEL);\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\treturn err;\n\t}\n\n\tget_page(newpage);\n\n\tif (!(buf->flags & PIPE_BUF_FLAG_LRU))\n\t\tlru_cache_add_file(newpage);\n\n\terr = 0;\n\tspin_lock(&cs->req->waitq.lock);\n\tif (test_bit(FR_ABORTED, &cs->req->flags))\n\t\terr = -ENOENT;\n\telse\n\t\t*pagep = newpage;\n\tspin_unlock(&cs->req->waitq.lock);\n\n\tif (err) {\n\t\tunlock_page(newpage);\n\t\tput_page(newpage);\n\t\treturn err;\n\t}\n\n\tunlock_page(oldpage);\n\tput_page(oldpage);\n\tcs->len = 0;\n\n\treturn 0;\n\nout_fallback_unlock:\n\tunlock_page(newpage);\nout_fallback:\n\tcs->pg = buf->page;\n\tcs->offset = buf->offset;\n\n\terr = lock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\n\nstatic int fuse_ref_page(struct fuse_copy_state *cs, struct page *page,\n\t\t\t unsigned offset, unsigned count)\n{\n\tstruct pipe_buffer *buf;\n\tint err;\n\n\tif (cs->nr_segs == cs->pipe->buffers)\n\t\treturn -EIO;\n\n\terr = unlock_request(cs->req);\n\tif (err)\n\t\treturn err;\n\n\tfuse_copy_finish(cs);\n\n\tbuf = cs->pipebufs;\n\tget_page(page);\n\tbuf->page = page;\n\tbuf->offset = offset;\n\tbuf->len = count;\n\n\tcs->pipebufs++;\n\tcs->nr_segs++;\n\tcs->len = 0;\n\n\treturn 0;\n}\n\n/*\n * Copy a page in the request to/from the userspace buffer.  Must be\n * done atomically\n */\nstatic int fuse_copy_page(struct fuse_copy_state *cs, struct page **pagep,\n\t\t\t  unsigned offset, unsigned count, int zeroing)\n{\n\tint err;\n\tstruct page *page = *pagep;\n\n\tif (page && zeroing && count < PAGE_SIZE)\n\t\tclear_highpage(page);\n\n\twhile (count) {\n\t\tif (cs->write && cs->pipebufs && page) {\n\t\t\treturn fuse_ref_page(cs, page, offset, count);\n\t\t} else if (!cs->len) {\n\t\t\tif (cs->move_pages && page &&\n\t\t\t    offset == 0 && count == PAGE_SIZE) {\n\t\t\t\terr = fuse_try_move_page(cs, pagep);\n\t\t\t\tif (err <= 0)\n\t\t\t\t\treturn err;\n\t\t\t} else {\n\t\t\t\terr = fuse_copy_fill(cs);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t\tif (page) {\n\t\t\tvoid *mapaddr = kmap_atomic(page);\n\t\t\tvoid *buf = mapaddr + offset;\n\t\t\toffset += fuse_copy_do(cs, &buf, &count);\n\t\t\tkunmap_atomic(mapaddr);\n\t\t} else\n\t\t\toffset += fuse_copy_do(cs, NULL, &count);\n\t}\n\tif (page && !cs->write)\n\t\tflush_dcache_page(page);\n\treturn 0;\n}\n\n/* Copy pages in the request to/from userspace buffer */\nstatic int fuse_copy_pages(struct fuse_copy_state *cs, unsigned nbytes,\n\t\t\t   int zeroing)\n{\n\tunsigned i;\n\tstruct fuse_req *req = cs->req;\n\n\tfor (i = 0; i < req->num_pages && (nbytes || zeroing); i++) {\n\t\tint err;\n\t\tunsigned offset = req->page_descs[i].offset;\n\t\tunsigned count = min(nbytes, req->page_descs[i].length);\n\n\t\terr = fuse_copy_page(cs, &req->pages[i], offset, count,\n\t\t\t\t     zeroing);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tnbytes -= count;\n\t}\n\treturn 0;\n}\n\n/* Copy a single argument in the request to/from userspace buffer */\nstatic int fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size)\n{\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}\n\n/* Copy request arguments to/from userspace buffer */\nstatic int fuse_copy_args(struct fuse_copy_state *cs, unsigned numargs,\n\t\t\t  unsigned argpages, struct fuse_arg *args,\n\t\t\t  int zeroing)\n{\n\tint err = 0;\n\tunsigned i;\n\n\tfor (i = 0; !err && i < numargs; i++)  {\n\t\tstruct fuse_arg *arg = &args[i];\n\t\tif (i == numargs - 1 && argpages)\n\t\t\terr = fuse_copy_pages(cs, arg->size, zeroing);\n\t\telse\n\t\t\terr = fuse_copy_one(cs, arg->value, arg->size);\n\t}\n\treturn err;\n}\n\nstatic int forget_pending(struct fuse_iqueue *fiq)\n{\n\treturn fiq->forget_list_head.next != NULL;\n}\n\nstatic int request_pending(struct fuse_iqueue *fiq)\n{\n\treturn !list_empty(&fiq->pending) || !list_empty(&fiq->interrupts) ||\n\t\tforget_pending(fiq);\n}\n\n/*\n * Transfer an interrupt request to userspace\n *\n * Unlike other requests this is assembled on demand, without a need\n * to allocate a separate fuse_req structure.\n *\n * Called with fiq->waitq.lock held, releases it\n */\nstatic int fuse_read_interrupt(struct fuse_iqueue *fiq,\n\t\t\t       struct fuse_copy_state *cs,\n\t\t\t       size_t nbytes, struct fuse_req *req)\n__releases(fiq->waitq.lock)\n{\n\tstruct fuse_in_header ih;\n\tstruct fuse_interrupt_in arg;\n\tunsigned reqsize = sizeof(ih) + sizeof(arg);\n\tint err;\n\n\tlist_del_init(&req->intr_entry);\n\tmemset(&ih, 0, sizeof(ih));\n\tmemset(&arg, 0, sizeof(arg));\n\tih.len = reqsize;\n\tih.opcode = FUSE_INTERRUPT;\n\tih.unique = (req->in.h.unique | FUSE_INT_REQ_BIT);\n\targ.unique = req->in.h.unique;\n\n\tspin_unlock(&fiq->waitq.lock);\n\tif (nbytes < reqsize)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\treturn err ? err : reqsize;\n}\n\nstatic struct fuse_forget_link *dequeue_forget(struct fuse_iqueue *fiq,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp)\n{\n\tstruct fuse_forget_link *head = fiq->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfiq->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fiq->forget_list_head.next == NULL)\n\t\tfiq->forget_list_tail = &fiq->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}\n\nstatic int fuse_read_single_forget(struct fuse_iqueue *fiq,\n\t\t\t\t   struct fuse_copy_state *cs,\n\t\t\t\t   size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tint err;\n\tstruct fuse_forget_link *forget = dequeue_forget(fiq, 1, NULL);\n\tstruct fuse_forget_in arg = {\n\t\t.nlookup = forget->forget_one.nlookup,\n\t};\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_FORGET,\n\t\t.nodeid = forget->forget_one.nodeid,\n\t\t.unique = fuse_get_unique(fiq),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tspin_unlock(&fiq->waitq.lock);\n\tkfree(forget);\n\tif (nbytes < ih.len)\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_batch_forget(struct fuse_iqueue *fiq,\n\t\t\t\t   struct fuse_copy_state *cs, size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tint err;\n\tunsigned max_forgets;\n\tunsigned count;\n\tstruct fuse_forget_link *head;\n\tstruct fuse_batch_forget_in arg = { .count = 0 };\n\tstruct fuse_in_header ih = {\n\t\t.opcode = FUSE_BATCH_FORGET,\n\t\t.unique = fuse_get_unique(fiq),\n\t\t.len = sizeof(ih) + sizeof(arg),\n\t};\n\n\tif (nbytes < ih.len) {\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\treturn -EINVAL;\n\t}\n\n\tmax_forgets = (nbytes - ih.len) / sizeof(struct fuse_forget_one);\n\thead = dequeue_forget(fiq, max_forgets, &count);\n\tspin_unlock(&fiq->waitq.lock);\n\n\targ.count = count;\n\tih.len += count * sizeof(struct fuse_forget_one);\n\terr = fuse_copy_one(cs, &ih, sizeof(ih));\n\tif (!err)\n\t\terr = fuse_copy_one(cs, &arg, sizeof(arg));\n\n\twhile (head) {\n\t\tstruct fuse_forget_link *forget = head;\n\n\t\tif (!err) {\n\t\t\terr = fuse_copy_one(cs, &forget->forget_one,\n\t\t\t\t\t    sizeof(forget->forget_one));\n\t\t}\n\t\thead = forget->next;\n\t\tkfree(forget);\n\t}\n\n\tfuse_copy_finish(cs);\n\n\tif (err)\n\t\treturn err;\n\n\treturn ih.len;\n}\n\nstatic int fuse_read_forget(struct fuse_conn *fc, struct fuse_iqueue *fiq,\n\t\t\t    struct fuse_copy_state *cs,\n\t\t\t    size_t nbytes)\n__releases(fiq->waitq.lock)\n{\n\tif (fc->minor < 16 || fiq->forget_list_head.next->next == NULL)\n\t\treturn fuse_read_single_forget(fiq, cs, nbytes);\n\telse\n\t\treturn fuse_read_batch_forget(fiq, cs, nbytes);\n}\n\n/*\n * Read a single request into the userspace filesystem's buffer.  This\n * function waits until a request is available, then removes it from\n * the pending list and copies request data to userspace buffer.  If\n * no reply is needed (FORGET) or request has been aborted or there\n * was an error during the copying then it's finished by calling\n * request_end().  Otherwise add it to the processing list, and set\n * the 'sent' flag.\n */\nstatic ssize_t fuse_dev_do_read(struct fuse_dev *fud, struct file *file,\n\t\t\t\tstruct fuse_copy_state *cs, size_t nbytes)\n{\n\tssize_t err;\n\tstruct fuse_conn *fc = fud->fc;\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\tstruct fuse_pqueue *fpq = &fud->pq;\n\tstruct fuse_req *req;\n\tstruct fuse_in *in;\n\tunsigned reqsize;\n\tunsigned int hash;\n\n restart:\n\tspin_lock(&fiq->waitq.lock);\n\terr = -EAGAIN;\n\tif ((file->f_flags & O_NONBLOCK) && fiq->connected &&\n\t    !request_pending(fiq))\n\t\tgoto err_unlock;\n\n\terr = wait_event_interruptible_exclusive_locked(fiq->waitq,\n\t\t\t\t!fiq->connected || request_pending(fiq));\n\tif (err)\n\t\tgoto err_unlock;\n\n\tif (!fiq->connected) {\n\t\terr = (fc->aborted && fc->abort_err) ? -ECONNABORTED : -ENODEV;\n\t\tgoto err_unlock;\n\t}\n\n\tif (!list_empty(&fiq->interrupts)) {\n\t\treq = list_entry(fiq->interrupts.next, struct fuse_req,\n\t\t\t\t intr_entry);\n\t\treturn fuse_read_interrupt(fiq, cs, nbytes, req);\n\t}\n\n\tif (forget_pending(fiq)) {\n\t\tif (list_empty(&fiq->pending) || fiq->forget_batch-- > 0)\n\t\t\treturn fuse_read_forget(fc, fiq, cs, nbytes);\n\n\t\tif (fiq->forget_batch <= -8)\n\t\t\tfiq->forget_batch = 16;\n\t}\n\n\treq = list_entry(fiq->pending.next, struct fuse_req, list);\n\tclear_bit(FR_PENDING, &req->flags);\n\tlist_del_init(&req->list);\n\tspin_unlock(&fiq->waitq.lock);\n\n\tin = &req->in;\n\treqsize = in->h.len;\n\n\t/* If request is too large, reply with an error and restart the read */\n\tif (nbytes < reqsize) {\n\t\treq->out.h.error = -EIO;\n\t\t/* SETXATTR is special, since it may contain too large data */\n\t\tif (in->h.opcode == FUSE_SETXATTR)\n\t\t\treq->out.h.error = -E2BIG;\n\t\trequest_end(fc, req);\n\t\tgoto restart;\n\t}\n\tspin_lock(&fpq->lock);\n\tlist_add(&req->list, &fpq->io);\n\tspin_unlock(&fpq->lock);\n\tcs->req = req;\n\terr = fuse_copy_one(cs, &in->h, sizeof(in->h));\n\tif (!err)\n\t\terr = fuse_copy_args(cs, in->numargs, in->argpages,\n\t\t\t\t     (struct fuse_arg *) in->args, 0);\n\tfuse_copy_finish(cs);\n\tspin_lock(&fpq->lock);\n\tclear_bit(FR_LOCKED, &req->flags);\n\tif (!fpq->connected) {\n\t\terr = (fc->aborted && fc->abort_err) ? -ECONNABORTED : -ENODEV;\n\t\tgoto out_end;\n\t}\n\tif (err) {\n\t\treq->out.h.error = -EIO;\n\t\tgoto out_end;\n\t}\n\tif (!test_bit(FR_ISREPLY, &req->flags)) {\n\t\terr = reqsize;\n\t\tgoto out_end;\n\t}\n\thash = fuse_req_hash(req->in.h.unique);\n\tlist_move_tail(&req->list, &fpq->processing[hash]);\n\t__fuse_get_request(req);\n\tset_bit(FR_SENT, &req->flags);\n\tspin_unlock(&fpq->lock);\n\t/* matches barrier in request_wait_answer() */\n\tsmp_mb__after_atomic();\n\tif (test_bit(FR_INTERRUPTED, &req->flags))\n\t\tqueue_interrupt(fiq, req);\n\tfuse_put_request(fc, req);\n\n\treturn reqsize;\n\nout_end:\n\tif (!test_bit(FR_PRIVATE, &req->flags))\n\t\tlist_del_init(&req->list);\n\tspin_unlock(&fpq->lock);\n\trequest_end(fc, req);\n\treturn err;\n\n err_unlock:\n\tspin_unlock(&fiq->waitq.lock);\n\treturn err;\n}\n\nstatic int fuse_dev_open(struct inode *inode, struct file *file)\n{\n\t/*\n\t * The fuse device's file's private_data is used to hold\n\t * the fuse_conn(ection) when it is mounted, and is used to\n\t * keep track of whether the file has been mounted already.\n\t */\n\tfile->private_data = NULL;\n\treturn 0;\n}\n\nstatic ssize_t fuse_dev_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct fuse_copy_state cs;\n\tstruct file *file = iocb->ki_filp;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tif (!iter_is_iovec(to))\n\t\treturn -EINVAL;\n\n\tfuse_copy_init(&cs, 1, to);\n\n\treturn fuse_dev_do_read(fud, file, &cs, iov_iter_count(to));\n}\n\nstatic ssize_t fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags)\n{\n\tint total, ret;\n\tint page_nr = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(in);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tbufs = kvmalloc_array(pipe->buffers, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, 1, NULL);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fud, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tfor (ret = total = 0; page_nr < cs.nr_segs; total += ret) {\n\t\t/*\n\t\t * Need to be careful about this.  Having buf->ops in module\n\t\t * code can Oops if the buffer persists after module unload.\n\t\t */\n\t\tbufs[page_nr].ops = &nosteal_pipe_buf_ops;\n\t\tbufs[page_nr].flags = 0;\n\t\tret = add_to_pipe(pipe, &bufs[page_nr++]);\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\t}\n\tif (total)\n\t\tret = total;\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tput_page(bufs[page_nr].page);\n\n\tkvfree(bufs);\n\treturn ret;\n}\n\nstatic int fuse_notify_poll(struct fuse_conn *fc, unsigned int size,\n\t\t\t    struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_poll_wakeup_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\tfuse_copy_finish(cs);\n\treturn fuse_notify_poll_wakeup(fc, &outarg);\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_inode(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_inode_out outarg;\n\tint err = -EINVAL;\n\n\tif (size != sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\terr = fuse_reverse_inval_inode(fc->sb, outarg.ino,\n\t\t\t\t\t       outarg.off, outarg.len);\n\t}\n\tup_read(&fc->killsb);\n\treturn err;\n\nerr:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_inval_entry(struct fuse_conn *fc, unsigned int size,\n\t\t\t\t   struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_inval_entry_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg) + outarg.namelen + 1)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent, 0, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_delete(struct fuse_conn *fc, unsigned int size,\n\t\t\t      struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_delete_out outarg;\n\tint err = -ENOMEM;\n\tchar *buf;\n\tstruct qstr name;\n\n\tbuf = kzalloc(FUSE_NAME_MAX + 1, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto err;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto err;\n\n\terr = -ENAMETOOLONG;\n\tif (outarg.namelen > FUSE_NAME_MAX)\n\t\tgoto err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg) + outarg.namelen + 1)\n\t\tgoto err;\n\n\tname.name = buf;\n\tname.len = outarg.namelen;\n\terr = fuse_copy_one(cs, buf, outarg.namelen + 1);\n\tif (err)\n\t\tgoto err;\n\tfuse_copy_finish(cs);\n\tbuf[outarg.namelen] = 0;\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb)\n\t\terr = fuse_reverse_inval_entry(fc->sb, outarg.parent,\n\t\t\t\t\t       outarg.child, &name);\n\tup_read(&fc->killsb);\n\tkfree(buf);\n\treturn err;\n\nerr:\n\tkfree(buf);\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify_store(struct fuse_conn *fc, unsigned int size,\n\t\t\t     struct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_store_out outarg;\n\tstruct inode *inode;\n\tstruct address_space *mapping;\n\tu64 nodeid;\n\tint err;\n\tpgoff_t index;\n\tunsigned int offset;\n\tunsigned int num;\n\tloff_t file_size;\n\tloff_t end;\n\n\terr = -EINVAL;\n\tif (size < sizeof(outarg))\n\t\tgoto out_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto out_finish;\n\n\terr = -EINVAL;\n\tif (size - sizeof(outarg) != outarg.size)\n\t\tgoto out_finish;\n\n\tnodeid = outarg.nodeid;\n\n\tdown_read(&fc->killsb);\n\n\terr = -ENOENT;\n\tif (!fc->sb)\n\t\tgoto out_up_killsb;\n\n\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\tif (!inode)\n\t\tgoto out_up_killsb;\n\n\tmapping = inode->i_mapping;\n\tindex = outarg.offset >> PAGE_SHIFT;\n\toffset = outarg.offset & ~PAGE_MASK;\n\tfile_size = i_size_read(inode);\n\tend = outarg.offset + outarg.size;\n\tif (end > file_size) {\n\t\tfile_size = end;\n\t\tfuse_write_update_size(inode, file_size);\n\t}\n\n\tnum = outarg.size;\n\twhile (num) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\terr = -ENOMEM;\n\t\tpage = find_or_create_page(mapping, index,\n\t\t\t\t\t   mapping_gfp_mask(mapping));\n\t\tif (!page)\n\t\t\tgoto out_iput;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_SIZE - offset);\n\t\terr = fuse_copy_page(cs, &page, offset, this_num, 0);\n\t\tif (!err && offset == 0 &&\n\t\t    (this_num == PAGE_SIZE || file_size == end))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\n\t\tif (err)\n\t\t\tgoto out_iput;\n\n\t\tnum -= this_num;\n\t\toffset = 0;\n\t\tindex++;\n\t}\n\n\terr = 0;\n\nout_iput:\n\tiput(inode);\nout_up_killsb:\n\tup_read(&fc->killsb);\nout_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic void fuse_retrieve_end(struct fuse_conn *fc, struct fuse_req *req)\n{\n\trelease_pages(req->pages, req->num_pages);\n}\n\nstatic int fuse_retrieve(struct fuse_conn *fc, struct inode *inode,\n\t\t\t struct fuse_notify_retrieve_out *outarg)\n{\n\tint err;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct fuse_req *req;\n\tpgoff_t index;\n\tloff_t file_size;\n\tunsigned int num;\n\tunsigned int offset;\n\tsize_t total_len = 0;\n\tunsigned int num_pages;\n\n\toffset = outarg->offset & ~PAGE_MASK;\n\tfile_size = i_size_read(inode);\n\n\tnum = outarg->size;\n\tif (outarg->offset > file_size)\n\t\tnum = 0;\n\telse if (outarg->offset + num > file_size)\n\t\tnum = file_size - outarg->offset;\n\n\tnum_pages = (num + offset + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tnum_pages = min(num_pages, fc->max_pages);\n\n\treq = fuse_get_req(fc, num_pages);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\treq->in.h.opcode = FUSE_NOTIFY_REPLY;\n\treq->in.h.nodeid = outarg->nodeid;\n\treq->in.numargs = 2;\n\treq->in.argpages = 1;\n\treq->end = fuse_retrieve_end;\n\n\tindex = outarg->offset >> PAGE_SHIFT;\n\n\twhile (num && req->num_pages < num_pages) {\n\t\tstruct page *page;\n\t\tunsigned int this_num;\n\n\t\tpage = find_get_page(mapping, index);\n\t\tif (!page)\n\t\t\tbreak;\n\n\t\tthis_num = min_t(unsigned, num, PAGE_SIZE - offset);\n\t\treq->pages[req->num_pages] = page;\n\t\treq->page_descs[req->num_pages].offset = offset;\n\t\treq->page_descs[req->num_pages].length = this_num;\n\t\treq->num_pages++;\n\n\t\toffset = 0;\n\t\tnum -= this_num;\n\t\ttotal_len += this_num;\n\t\tindex++;\n\t}\n\treq->misc.retrieve_in.offset = outarg->offset;\n\treq->misc.retrieve_in.size = total_len;\n\treq->in.args[0].size = sizeof(req->misc.retrieve_in);\n\treq->in.args[0].value = &req->misc.retrieve_in;\n\treq->in.args[1].size = total_len;\n\n\terr = fuse_request_send_notify_reply(fc, req, outarg->notify_unique);\n\tif (err) {\n\t\tfuse_retrieve_end(fc, req);\n\t\tfuse_put_request(fc, req);\n\t}\n\n\treturn err;\n}\n\nstatic int fuse_notify_retrieve(struct fuse_conn *fc, unsigned int size,\n\t\t\t\tstruct fuse_copy_state *cs)\n{\n\tstruct fuse_notify_retrieve_out outarg;\n\tstruct inode *inode;\n\tint err;\n\n\terr = -EINVAL;\n\tif (size != sizeof(outarg))\n\t\tgoto copy_finish;\n\n\terr = fuse_copy_one(cs, &outarg, sizeof(outarg));\n\tif (err)\n\t\tgoto copy_finish;\n\n\tfuse_copy_finish(cs);\n\n\tdown_read(&fc->killsb);\n\terr = -ENOENT;\n\tif (fc->sb) {\n\t\tu64 nodeid = outarg.nodeid;\n\n\t\tinode = ilookup5(fc->sb, nodeid, fuse_inode_eq, &nodeid);\n\t\tif (inode) {\n\t\t\terr = fuse_retrieve(fc, inode, &outarg);\n\t\t\tiput(inode);\n\t\t}\n\t}\n\tup_read(&fc->killsb);\n\n\treturn err;\n\ncopy_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic int fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs)\n{\n\t/* Don't try to move pages (yet) */\n\tcs->move_pages = 0;\n\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tcase FUSE_NOTIFY_DELETE:\n\t\treturn fuse_notify_delete(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}\n\n/* Look up request on processing list by unique ID */\nstatic struct fuse_req *request_find(struct fuse_pqueue *fpq, u64 unique)\n{\n\tunsigned int hash = fuse_req_hash(unique);\n\tstruct fuse_req *req;\n\n\tlist_for_each_entry(req, &fpq->processing[hash], list) {\n\t\tif (req->in.h.unique == unique)\n\t\t\treturn req;\n\t}\n\treturn NULL;\n}\n\nstatic int copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes)\n{\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}\n\n/*\n * Write a single reply to a request.  First the header is copied from\n * the write buffer.  The request is then searched on the processing\n * list by the unique ID found in the header.  If found, then remove\n * it from the list and copy the rest of the buffer to the request.\n * The request is finished by calling request_end()\n */\nstatic ssize_t fuse_dev_do_write(struct fuse_dev *fud,\n\t\t\t\t struct fuse_copy_state *cs, size_t nbytes)\n{\n\tint err;\n\tstruct fuse_conn *fc = fud->fc;\n\tstruct fuse_pqueue *fpq = &fud->pq;\n\tstruct fuse_req *req;\n\tstruct fuse_out_header oh;\n\n\tif (nbytes < sizeof(struct fuse_out_header))\n\t\treturn -EINVAL;\n\n\terr = fuse_copy_one(cs, &oh, sizeof(oh));\n\tif (err)\n\t\tgoto err_finish;\n\n\terr = -EINVAL;\n\tif (oh.len != nbytes)\n\t\tgoto err_finish;\n\n\t/*\n\t * Zero oh.unique indicates unsolicited notification message\n\t * and error contains notification code.\n\t */\n\tif (!oh.unique) {\n\t\terr = fuse_notify(fc, oh.error, nbytes - sizeof(oh), cs);\n\t\treturn err ? err : nbytes;\n\t}\n\n\terr = -EINVAL;\n\tif (oh.error <= -1000 || oh.error > 0)\n\t\tgoto err_finish;\n\n\tspin_lock(&fpq->lock);\n\terr = -ENOENT;\n\tif (!fpq->connected)\n\t\tgoto err_unlock_pq;\n\n\treq = request_find(fpq, oh.unique & ~FUSE_INT_REQ_BIT);\n\tif (!req)\n\t\tgoto err_unlock_pq;\n\n\t/* Is it an interrupt reply ID? */\n\tif (oh.unique & FUSE_INT_REQ_BIT) {\n\t\t__fuse_get_request(req);\n\t\tspin_unlock(&fpq->lock);\n\n\t\terr = -EINVAL;\n\t\tif (nbytes != sizeof(struct fuse_out_header)) {\n\t\t\tfuse_put_request(fc, req);\n\t\t\tgoto err_finish;\n\t\t}\n\n\t\tif (oh.error == -ENOSYS)\n\t\t\tfc->no_interrupt = 1;\n\t\telse if (oh.error == -EAGAIN)\n\t\t\tqueue_interrupt(&fc->iq, req);\n\t\tfuse_put_request(fc, req);\n\n\t\tfuse_copy_finish(cs);\n\t\treturn nbytes;\n\t}\n\n\tclear_bit(FR_SENT, &req->flags);\n\tlist_move(&req->list, &fpq->io);\n\treq->out.h = oh;\n\tset_bit(FR_LOCKED, &req->flags);\n\tspin_unlock(&fpq->lock);\n\tcs->req = req;\n\tif (!req->out.page_replace)\n\t\tcs->move_pages = 0;\n\n\terr = copy_out_args(cs, &req->out, nbytes);\n\tfuse_copy_finish(cs);\n\n\tspin_lock(&fpq->lock);\n\tclear_bit(FR_LOCKED, &req->flags);\n\tif (!fpq->connected)\n\t\terr = -ENOENT;\n\telse if (err)\n\t\treq->out.h.error = -EIO;\n\tif (!test_bit(FR_PRIVATE, &req->flags))\n\t\tlist_del_init(&req->list);\n\tspin_unlock(&fpq->lock);\n\n\trequest_end(fc, req);\n\n\treturn err ? err : nbytes;\n\n err_unlock_pq:\n\tspin_unlock(&fpq->lock);\n err_finish:\n\tfuse_copy_finish(cs);\n\treturn err;\n}\n\nstatic ssize_t fuse_dev_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(iocb->ki_filp);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tif (!iter_is_iovec(from))\n\t\treturn -EINVAL;\n\n\tfuse_copy_init(&cs, 0, from);\n\n\treturn fuse_dev_do_write(fud, &cs, iov_iter_count(from));\n}\n\nstatic ssize_t fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags)\n{\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len)\n\t\tgoto out_free;\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tif (!pipe_buf_get(pipe, ibuf))\n\t\t\t\tgoto out_free;\n\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\nout_free:\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\n\tkvfree(bufs);\n\treturn ret;\n}\n\nstatic __poll_t fuse_dev_poll(struct file *file, poll_table *wait)\n{\n\t__poll_t mask = EPOLLOUT | EPOLLWRNORM;\n\tstruct fuse_iqueue *fiq;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn EPOLLERR;\n\n\tfiq = &fud->fc->iq;\n\tpoll_wait(file, &fiq->waitq, wait);\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected)\n\t\tmask = EPOLLERR;\n\telse if (request_pending(fiq))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn mask;\n}\n\n/*\n * Abort all requests on the given list (pending or processing)\n *\n * This function releases and reacquires fc->lock\n */\nstatic void end_requests(struct fuse_conn *fc, struct list_head *head)\n{\n\twhile (!list_empty(head)) {\n\t\tstruct fuse_req *req;\n\t\treq = list_entry(head->next, struct fuse_req, list);\n\t\treq->out.h.error = -ECONNABORTED;\n\t\tclear_bit(FR_SENT, &req->flags);\n\t\tlist_del_init(&req->list);\n\t\trequest_end(fc, req);\n\t}\n}\n\nstatic void end_polls(struct fuse_conn *fc)\n{\n\tstruct rb_node *p;\n\n\tp = rb_first(&fc->polled_files);\n\n\twhile (p) {\n\t\tstruct fuse_file *ff;\n\t\tff = rb_entry(p, struct fuse_file, polled_node);\n\t\twake_up_interruptible_all(&ff->poll_wait);\n\n\t\tp = rb_next(p);\n\t}\n}\n\n/*\n * Abort all requests.\n *\n * Emergency exit in case of a malicious or accidental deadlock, or just a hung\n * filesystem.\n *\n * The same effect is usually achievable through killing the filesystem daemon\n * and all users of the filesystem.  The exception is the combination of an\n * asynchronous request and the tricky deadlock (see\n * Documentation/filesystems/fuse.txt).\n *\n * Aborting requests under I/O goes as follows: 1: Separate out unlocked\n * requests, they should be finished off immediately.  Locked requests will be\n * finished after unlock; see unlock_request(). 2: Finish off the unlocked\n * requests.  It is possible that some request will finish before we can.  This\n * is OK, the request will in that case be removed from the list before we touch\n * it.\n */\nvoid fuse_abort_conn(struct fuse_conn *fc, bool is_abort)\n{\n\tstruct fuse_iqueue *fiq = &fc->iq;\n\n\tspin_lock(&fc->lock);\n\tif (fc->connected) {\n\t\tstruct fuse_dev *fud;\n\t\tstruct fuse_req *req, *next;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\t/* Background queuing checks fc->connected under bg_lock */\n\t\tspin_lock(&fc->bg_lock);\n\t\tfc->connected = 0;\n\t\tspin_unlock(&fc->bg_lock);\n\n\t\tfc->aborted = is_abort;\n\t\tfuse_set_initialized(fc);\n\t\tlist_for_each_entry(fud, &fc->devices, entry) {\n\t\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\n\t\t\tspin_lock(&fpq->lock);\n\t\t\tfpq->connected = 0;\n\t\t\tlist_for_each_entry_safe(req, next, &fpq->io, list) {\n\t\t\t\treq->out.h.error = -ECONNABORTED;\n\t\t\t\tspin_lock(&req->waitq.lock);\n\t\t\t\tset_bit(FR_ABORTED, &req->flags);\n\t\t\t\tif (!test_bit(FR_LOCKED, &req->flags)) {\n\t\t\t\t\tset_bit(FR_PRIVATE, &req->flags);\n\t\t\t\t\t__fuse_get_request(req);\n\t\t\t\t\tlist_move(&req->list, &to_end);\n\t\t\t\t}\n\t\t\t\tspin_unlock(&req->waitq.lock);\n\t\t\t}\n\t\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\t\tlist_splice_tail_init(&fpq->processing[i],\n\t\t\t\t\t\t      &to_end);\n\t\t\tspin_unlock(&fpq->lock);\n\t\t}\n\t\tspin_lock(&fc->bg_lock);\n\t\tfc->blocked = 0;\n\t\tfc->max_background = UINT_MAX;\n\t\tflush_bg_queue(fc);\n\t\tspin_unlock(&fc->bg_lock);\n\n\t\tspin_lock(&fiq->waitq.lock);\n\t\tfiq->connected = 0;\n\t\tlist_for_each_entry(req, &fiq->pending, list)\n\t\t\tclear_bit(FR_PENDING, &req->flags);\n\t\tlist_splice_tail_init(&fiq->pending, &to_end);\n\t\twhile (forget_pending(fiq))\n\t\t\tkfree(dequeue_forget(fiq, 1, NULL));\n\t\twake_up_all_locked(&fiq->waitq);\n\t\tspin_unlock(&fiq->waitq.lock);\n\t\tkill_fasync(&fiq->fasync, SIGIO, POLL_IN);\n\t\tend_polls(fc);\n\t\twake_up_all(&fc->blocked_waitq);\n\t\tspin_unlock(&fc->lock);\n\n\t\tend_requests(fc, &to_end);\n\t} else {\n\t\tspin_unlock(&fc->lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(fuse_abort_conn);\n\nvoid fuse_wait_aborted(struct fuse_conn *fc)\n{\n\t/* matches implicit memory barrier in fuse_drop_waiting() */\n\tsmp_mb();\n\twait_event(fc->blocked_waitq, atomic_read(&fc->num_waiting) == 0);\n}\n\nint fuse_dev_release(struct inode *inode, struct file *file)\n{\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (fud) {\n\t\tstruct fuse_conn *fc = fud->fc;\n\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\tspin_lock(&fpq->lock);\n\t\tWARN_ON(!list_empty(&fpq->io));\n\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\tlist_splice_init(&fpq->processing[i], &to_end);\n\t\tspin_unlock(&fpq->lock);\n\n\t\tend_requests(fc, &to_end);\n\n\t\t/* Are we the last open device? */\n\t\tif (atomic_dec_and_test(&fc->dev_count)) {\n\t\t\tWARN_ON(fc->iq.fasync != NULL);\n\t\t\tfuse_abort_conn(fc, false);\n\t\t}\n\t\tfuse_dev_free(fud);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fuse_dev_release);\n\nstatic int fuse_dev_fasync(int fd, struct file *file, int on)\n{\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fud->fc->iq.fasync);\n}\n\nstatic int fuse_device_clone(struct fuse_conn *fc, struct file *new)\n{\n\tstruct fuse_dev *fud;\n\n\tif (new->private_data)\n\t\treturn -EINVAL;\n\n\tfud = fuse_dev_alloc(fc);\n\tif (!fud)\n\t\treturn -ENOMEM;\n\n\tnew->private_data = fud;\n\tatomic_inc(&fc->dev_count);\n\n\treturn 0;\n}\n\nstatic long fuse_dev_ioctl(struct file *file, unsigned int cmd,\n\t\t\t   unsigned long arg)\n{\n\tint err = -ENOTTY;\n\n\tif (cmd == FUSE_DEV_IOC_CLONE) {\n\t\tint oldfd;\n\n\t\terr = -EFAULT;\n\t\tif (!get_user(oldfd, (__u32 __user *) arg)) {\n\t\t\tstruct file *old = fget(oldfd);\n\n\t\t\terr = -EINVAL;\n\t\t\tif (old) {\n\t\t\t\tstruct fuse_dev *fud = NULL;\n\n\t\t\t\t/*\n\t\t\t\t * Check against file->f_op because CUSE\n\t\t\t\t * uses the same ioctl handler.\n\t\t\t\t */\n\t\t\t\tif (old->f_op == file->f_op &&\n\t\t\t\t    old->f_cred->user_ns == file->f_cred->user_ns)\n\t\t\t\t\tfud = fuse_get_dev(old);\n\n\t\t\t\tif (fud) {\n\t\t\t\t\tmutex_lock(&fuse_mutex);\n\t\t\t\t\terr = fuse_device_clone(fud->fc, file);\n\t\t\t\t\tmutex_unlock(&fuse_mutex);\n\t\t\t\t}\n\t\t\t\tfput(old);\n\t\t\t}\n\t\t}\n\t}\n\treturn err;\n}\n\nconst struct file_operations fuse_dev_operations = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= fuse_dev_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= fuse_dev_read,\n\t.splice_read\t= fuse_dev_splice_read,\n\t.write_iter\t= fuse_dev_write,\n\t.splice_write\t= fuse_dev_splice_write,\n\t.poll\t\t= fuse_dev_poll,\n\t.release\t= fuse_dev_release,\n\t.fasync\t\t= fuse_dev_fasync,\n\t.unlocked_ioctl = fuse_dev_ioctl,\n\t.compat_ioctl   = fuse_dev_ioctl,\n};\nEXPORT_SYMBOL_GPL(fuse_dev_operations);\n\nstatic struct miscdevice fuse_miscdevice = {\n\t.minor = FUSE_MINOR,\n\t.name  = \"fuse\",\n\t.fops = &fuse_dev_operations,\n};\n\nint __init fuse_dev_init(void)\n{\n\tint err = -ENOMEM;\n\tfuse_req_cachep = kmem_cache_create(\"fuse_request\",\n\t\t\t\t\t    sizeof(struct fuse_req),\n\t\t\t\t\t    0, 0, NULL);\n\tif (!fuse_req_cachep)\n\t\tgoto out;\n\n\terr = misc_register(&fuse_miscdevice);\n\tif (err)\n\t\tgoto out_cache_clean;\n\n\treturn 0;\n\n out_cache_clean:\n\tkmem_cache_destroy(fuse_req_cachep);\n out:\n\treturn err;\n}\n\nvoid fuse_dev_cleanup(void)\n{\n\tmisc_deregister(&fuse_miscdevice);\n\tkmem_cache_destroy(fuse_req_cachep);\n}\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n *  linux/fs/pipe.c\n *\n *  Copyright (C) 1991, 1992, 1999  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/log2.h>\n#include <linux/mount.h>\n#include <linux/magic.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/uio.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/audit.h>\n#include <linux/syscalls.h>\n#include <linux/fcntl.h>\n#include <linux/memcontrol.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n\n#include \"internal.h\"\n\n/*\n * The max size that a non-root user is allowed to grow the pipe. Can\n * be set by root in /proc/sys/fs/pipe-max-size\n */\nunsigned int pipe_max_size = 1048576;\n\n/* Maximum allocatable pages per user. Hard limit is unset by default, soft\n * matches default values.\n */\nunsigned long pipe_user_pages_hard;\nunsigned long pipe_user_pages_soft = PIPE_DEF_BUFFERS * INR_OPEN_CUR;\n\n/*\n * We use a start+len construction, which provides full use of the \n * allocated memory.\n * -- Florian Coosmann (FGC)\n * \n * Reads with count = 0 should always return 0.\n * -- Julian Bradfield 1999-06-07.\n *\n * FIFOs and Pipes now generate SIGIO for both readers and writers.\n * -- Jeremy Elson <jelson@circlemud.org> 2001-08-16\n *\n * pipe_read & write cleanup\n * -- Manfred Spraul <manfred@colorfullife.com> 2002-05-09\n */\n\nstatic void pipe_lock_nested(struct pipe_inode_info *pipe, int subclass)\n{\n\tif (pipe->files)\n\t\tmutex_lock_nested(&pipe->mutex, subclass);\n}\n\nvoid pipe_lock(struct pipe_inode_info *pipe)\n{\n\t/*\n\t * pipe_lock() nests non-pipe inode locks (for writing to a file)\n\t */\n\tpipe_lock_nested(pipe, I_MUTEX_PARENT);\n}\nEXPORT_SYMBOL(pipe_lock);\n\nvoid pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tif (pipe->files)\n\t\tmutex_unlock(&pipe->mutex);\n}\nEXPORT_SYMBOL(pipe_unlock);\n\nstatic inline void __pipe_lock(struct pipe_inode_info *pipe)\n{\n\tmutex_lock_nested(&pipe->mutex, I_MUTEX_PARENT);\n}\n\nstatic inline void __pipe_unlock(struct pipe_inode_info *pipe)\n{\n\tmutex_unlock(&pipe->mutex);\n}\n\nvoid pipe_double_lock(struct pipe_inode_info *pipe1,\n\t\t      struct pipe_inode_info *pipe2)\n{\n\tBUG_ON(pipe1 == pipe2);\n\n\tif (pipe1 < pipe2) {\n\t\tpipe_lock_nested(pipe1, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe2, I_MUTEX_CHILD);\n\t} else {\n\t\tpipe_lock_nested(pipe2, I_MUTEX_PARENT);\n\t\tpipe_lock_nested(pipe1, I_MUTEX_CHILD);\n\t}\n}\n\n/* Drop the inode semaphore and wait for a pipe event, atomically */\nvoid pipe_wait(struct pipe_inode_info *pipe)\n{\n\tDEFINE_WAIT(wait);\n\n\t/*\n\t * Pipes are system-local resources, so sleeping on them\n\t * is considered a noninteractive wait:\n\t */\n\tprepare_to_wait(&pipe->wait, &wait, TASK_INTERRUPTIBLE);\n\tpipe_unlock(pipe);\n\tschedule();\n\tfinish_wait(&pipe->wait, &wait);\n\tpipe_lock(pipe);\n}\n\nstatic void anon_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t  struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * If nobody else uses this page, and we don't already have a\n\t * temporary page, let's keep track of it as a one-deep\n\t * allocation cache. (Otherwise just release our reference to it)\n\t */\n\tif (page_count(page) == 1 && !pipe->tmp_page)\n\t\tpipe->tmp_page = page;\n\telse\n\t\tput_page(page);\n}\n\nstatic int anon_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t       struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\tif (page_count(page) == 1) {\n\t\tif (memcg_kmem_enabled())\n\t\t\tmemcg_kmem_uncharge(page, 0);\n\t\t__SetPageLocked(page);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n/**\n * generic_pipe_buf_steal - attempt to take ownership of a &pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n *\n * Description:\n *\tThis function attempts to steal the &struct page attached to\n *\t@buf. If successful, this function returns 0 and returns with\n *\tthe page locked. The caller may then reuse the page for whatever\n *\the wishes; the typical use is insertion into a different file\n *\tpage cache.\n */\nint generic_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t   struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\n\t/*\n\t * A reference of one is golden, that means that the owner of this\n\t * page is the only one holding a reference to it. lock the page\n\t * and return OK.\n\t */\n\tif (page_count(page) == 1) {\n\t\tlock_page(page);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\nEXPORT_SYMBOL(generic_pipe_buf_steal);\n\n/**\n * generic_pipe_buf_get - get a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n *\n * Description:\n *\tThis function grabs an extra reference to @buf. It's used in\n *\tin the tee() system call, when we duplicate the buffers in one\n *\tpipe into another.\n */\nbool generic_pipe_buf_get(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\treturn try_get_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_get);\n\n/**\n * generic_pipe_buf_confirm - verify contents of the pipe buffer\n * @info:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to confirm\n *\n * Description:\n *\tThis function does nothing, because the generic pipe code uses\n *\tpages that are always good when inserted into the pipe.\n */\nint generic_pipe_buf_confirm(struct pipe_inode_info *info,\n\t\t\t     struct pipe_buffer *buf)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(generic_pipe_buf_confirm);\n\n/**\n * generic_pipe_buf_release - put a reference to a &struct pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n *\n * Description:\n *\tThis function releases a reference to @buf.\n */\nvoid generic_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t      struct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n}\nEXPORT_SYMBOL(generic_pipe_buf_release);\n\nstatic const struct pipe_buf_operations anon_pipe_buf_ops = {\n\t.can_merge = 1,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = anon_pipe_buf_release,\n\t.steal = anon_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic const struct pipe_buf_operations packet_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = anon_pipe_buf_release,\n\t.steal = anon_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic ssize_t\npipe_read(struct kiocb *iocb, struct iov_iter *to)\n{\n\tsize_t total_len = iov_iter_count(to);\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint do_wakeup;\n\tssize_t ret;\n\n\t/* Null read succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\tdo_wakeup = 0;\n\tret = 0;\n\t__pipe_lock(pipe);\n\tfor (;;) {\n\t\tint bufs = pipe->nrbufs;\n\t\tif (bufs) {\n\t\t\tint curbuf = pipe->curbuf;\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + curbuf;\n\t\t\tsize_t chars = buf->len;\n\t\t\tsize_t written;\n\t\t\tint error;\n\n\t\t\tif (chars > total_len)\n\t\t\t\tchars = total_len;\n\n\t\t\terror = pipe_buf_confirm(pipe, buf);\n\t\t\tif (error) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = error;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\twritten = copy_page_to_iter(buf->page, buf->offset, chars, to);\n\t\t\tif (unlikely(written < chars)) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += chars;\n\t\t\tbuf->offset += chars;\n\t\t\tbuf->len -= chars;\n\n\t\t\t/* Was it a packet buffer? Clean up and exit */\n\t\t\tif (buf->flags & PIPE_BUF_FLAG_PACKET) {\n\t\t\t\ttotal_len = chars;\n\t\t\t\tbuf->len = 0;\n\t\t\t}\n\n\t\t\tif (!buf->len) {\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tcurbuf = (curbuf + 1) & (pipe->buffers - 1);\n\t\t\t\tpipe->curbuf = curbuf;\n\t\t\t\tpipe->nrbufs = --bufs;\n\t\t\t\tdo_wakeup = 1;\n\t\t\t}\n\t\t\ttotal_len -= chars;\n\t\t\tif (!total_len)\n\t\t\t\tbreak;\t/* common path: read succeeded */\n\t\t}\n\t\tif (bufs)\t/* More to do? */\n\t\t\tcontinue;\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (!pipe->waiting_writers) {\n\t\t\t/* syscall merging: Usually we must not sleep\n\t\t\t * if O_NONBLOCK is set, or if we got some data.\n\t\t\t * But if a writer sleeps in kernel space, then\n\t\t\t * we can wait for that data without violating POSIX.\n\t\t\t */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (do_wakeup) {\n\t\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLOUT | EPOLLWRNORM);\n \t\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t\t}\n\t\tpipe_wait(pipe);\n\t}\n\t__pipe_unlock(pipe);\n\n\t/* Signal writers asynchronously that there is more room. */\n\tif (do_wakeup) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLOUT | EPOLLWRNORM);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\tif (ret > 0)\n\t\tfile_accessed(filp);\n\treturn ret;\n}\n\nstatic inline int is_packetized(struct file *file)\n{\n\treturn (file->f_flags & O_DIRECT) != 0;\n}\n\nstatic ssize_t\npipe_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct file *filp = iocb->ki_filp;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tssize_t ret = 0;\n\tint do_wakeup = 0;\n\tsize_t total_len = iov_iter_count(from);\n\tssize_t chars;\n\n\t/* Null write succeeds. */\n\tif (unlikely(total_len == 0))\n\t\treturn 0;\n\n\t__pipe_lock(pipe);\n\n\tif (!pipe->readers) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n\t/* We try to merge small writes */\n\tchars = total_len & (PAGE_SIZE-1); /* size of the last buffer */\n\tif (pipe->nrbufs && chars != 0) {\n\t\tint lastbuf = (pipe->curbuf + pipe->nrbufs - 1) &\n\t\t\t\t\t\t\t(pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + lastbuf;\n\t\tint offset = buf->offset + buf->len;\n\n\t\tif (buf->ops->can_merge && offset + chars <= PAGE_SIZE) {\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tret = copy_page_from_iter(buf->page, offset, chars, from);\n\t\t\tif (unlikely(ret < chars)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdo_wakeup = 1;\n\t\t\tbuf->len += ret;\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tfor (;;) {\n\t\tint bufs;\n\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\t\tbufs = pipe->nrbufs;\n\t\tif (bufs < pipe->buffers) {\n\t\t\tint newbuf = (pipe->curbuf + bufs) & (pipe->buffers-1);\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\t\t\tstruct page *page = pipe->tmp_page;\n\t\t\tint copied;\n\n\t\t\tif (!page) {\n\t\t\t\tpage = alloc_page(GFP_HIGHUSER | __GFP_ACCOUNT);\n\t\t\t\tif (unlikely(!page)) {\n\t\t\t\t\tret = ret ? : -ENOMEM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tpipe->tmp_page = page;\n\t\t\t}\n\t\t\t/* Always wake up, even if the copy fails. Otherwise\n\t\t\t * we lock up (O_NONBLOCK-)readers that sleep due to\n\t\t\t * syscall merging.\n\t\t\t * FIXME! Is this really true?\n\t\t\t */\n\t\t\tdo_wakeup = 1;\n\t\t\tcopied = copy_page_from_iter(page, 0, PAGE_SIZE, from);\n\t\t\tif (unlikely(copied < PAGE_SIZE && iov_iter_count(from))) {\n\t\t\t\tif (!ret)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret += copied;\n\n\t\t\t/* Insert it into the buffer array */\n\t\t\tbuf->page = page;\n\t\t\tbuf->ops = &anon_pipe_buf_ops;\n\t\t\tbuf->offset = 0;\n\t\t\tbuf->len = copied;\n\t\t\tbuf->flags = 0;\n\t\t\tif (is_packetized(filp)) {\n\t\t\t\tbuf->ops = &packet_pipe_buf_ops;\n\t\t\t\tbuf->flags = PIPE_BUF_FLAG_PACKET;\n\t\t\t}\n\t\t\tpipe->nrbufs = ++bufs;\n\t\t\tpipe->tmp_page = NULL;\n\n\t\t\tif (!iov_iter_count(from))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (bufs < pipe->buffers)\n\t\t\tcontinue;\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tif (!ret)\n\t\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tif (!ret)\n\t\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (do_wakeup) {\n\t\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLRDNORM);\n\t\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\t\tdo_wakeup = 0;\n\t\t}\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\nout:\n\t__pipe_unlock(pipe);\n\tif (do_wakeup) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLRDNORM);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t}\n\tif (ret > 0 && sb_start_write_trylock(file_inode(filp)->i_sb)) {\n\t\tint err = file_update_time(filp);\n\t\tif (err)\n\t\t\tret = err;\n\t\tsb_end_write(file_inode(filp)->i_sb);\n\t}\n\treturn ret;\n}\n\nstatic long pipe_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint count, buf, nrbufs;\n\n\tswitch (cmd) {\n\t\tcase FIONREAD:\n\t\t\t__pipe_lock(pipe);\n\t\t\tcount = 0;\n\t\t\tbuf = pipe->curbuf;\n\t\t\tnrbufs = pipe->nrbufs;\n\t\t\twhile (--nrbufs >= 0) {\n\t\t\t\tcount += pipe->bufs[buf].len;\n\t\t\t\tbuf = (buf+1) & (pipe->buffers - 1);\n\t\t\t}\n\t\t\t__pipe_unlock(pipe);\n\n\t\t\treturn put_user(count, (int __user *)arg);\n\t\tdefault:\n\t\t\treturn -ENOIOCTLCMD;\n\t}\n}\n\n/* No kernel lock held - fine */\nstatic __poll_t\npipe_poll(struct file *filp, poll_table *wait)\n{\n\t__poll_t mask;\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint nrbufs;\n\n\tpoll_wait(filp, &pipe->wait, wait);\n\n\t/* Reading only -- no need for acquiring the semaphore.  */\n\tnrbufs = pipe->nrbufs;\n\tmask = 0;\n\tif (filp->f_mode & FMODE_READ) {\n\t\tmask = (nrbufs > 0) ? EPOLLIN | EPOLLRDNORM : 0;\n\t\tif (!pipe->writers && filp->f_version != pipe->w_counter)\n\t\t\tmask |= EPOLLHUP;\n\t}\n\n\tif (filp->f_mode & FMODE_WRITE) {\n\t\tmask |= (nrbufs < pipe->buffers) ? EPOLLOUT | EPOLLWRNORM : 0;\n\t\t/*\n\t\t * Most Unices do not set EPOLLERR for FIFOs but on Linux they\n\t\t * behave exactly like pipes for poll().\n\t\t */\n\t\tif (!pipe->readers)\n\t\t\tmask |= EPOLLERR;\n\t}\n\n\treturn mask;\n}\n\nstatic void put_pipe_info(struct inode *inode, struct pipe_inode_info *pipe)\n{\n\tint kill = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (!--pipe->files) {\n\t\tinode->i_pipe = NULL;\n\t\tkill = 1;\n\t}\n\tspin_unlock(&inode->i_lock);\n\n\tif (kill)\n\t\tfree_pipe_info(pipe);\n}\n\nstatic int\npipe_release(struct inode *inode, struct file *file)\n{\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\tif (pipe->readers || pipe->writers) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLOUT | EPOLLRDNORM | EPOLLWRNORM | EPOLLERR | EPOLLHUP);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}\n\nstatic int\npipe_fasync(int fd, struct file *filp, int on)\n{\n\tstruct pipe_inode_info *pipe = filp->private_data;\n\tint retval = 0;\n\n\t__pipe_lock(pipe);\n\tif (filp->f_mode & FMODE_READ)\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_readers);\n\tif ((filp->f_mode & FMODE_WRITE) && retval >= 0) {\n\t\tretval = fasync_helper(fd, filp, on, &pipe->fasync_writers);\n\t\tif (retval < 0 && (filp->f_mode & FMODE_READ))\n\t\t\t/* this can happen only if on == T */\n\t\t\tfasync_helper(-1, filp, 0, &pipe->fasync_readers);\n\t}\n\t__pipe_unlock(pipe);\n\treturn retval;\n}\n\nstatic unsigned long account_pipe_buffers(struct user_struct *user,\n                                 unsigned long old, unsigned long new)\n{\n\treturn atomic_long_add_return(new - old, &user->pipe_bufs);\n}\n\nstatic bool too_many_pipe_buffers_soft(unsigned long user_bufs)\n{\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}\n\nstatic bool too_many_pipe_buffers_hard(unsigned long user_bufs)\n{\n\tunsigned long hard_limit = READ_ONCE(pipe_user_pages_hard);\n\n\treturn hard_limit && user_bufs > hard_limit;\n}\n\nstatic bool is_unprivileged_user(void)\n{\n\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n}\n\nstruct pipe_inode_info *alloc_pipe_info(void)\n{\n\tstruct pipe_inode_info *pipe;\n\tunsigned long pipe_bufs = PIPE_DEF_BUFFERS;\n\tstruct user_struct *user = get_current_user();\n\tunsigned long user_bufs;\n\tunsigned int max_size = READ_ONCE(pipe_max_size);\n\n\tpipe = kzalloc(sizeof(struct pipe_inode_info), GFP_KERNEL_ACCOUNT);\n\tif (pipe == NULL)\n\t\tgoto out_free_uid;\n\n\tif (pipe_bufs * PAGE_SIZE > max_size && !capable(CAP_SYS_RESOURCE))\n\t\tpipe_bufs = max_size >> PAGE_SHIFT;\n\n\tuser_bufs = account_pipe_buffers(user, 0, pipe_bufs);\n\n\tif (too_many_pipe_buffers_soft(user_bufs) && is_unprivileged_user()) {\n\t\tuser_bufs = account_pipe_buffers(user, pipe_bufs, 1);\n\t\tpipe_bufs = 1;\n\t}\n\n\tif (too_many_pipe_buffers_hard(user_bufs) && is_unprivileged_user())\n\t\tgoto out_revert_acct;\n\n\tpipe->bufs = kcalloc(pipe_bufs, sizeof(struct pipe_buffer),\n\t\t\t     GFP_KERNEL_ACCOUNT);\n\n\tif (pipe->bufs) {\n\t\tinit_waitqueue_head(&pipe->wait);\n\t\tpipe->r_counter = pipe->w_counter = 1;\n\t\tpipe->buffers = pipe_bufs;\n\t\tpipe->user = user;\n\t\tmutex_init(&pipe->mutex);\n\t\treturn pipe;\n\t}\n\nout_revert_acct:\n\t(void) account_pipe_buffers(user, pipe_bufs, 0);\n\tkfree(pipe);\nout_free_uid:\n\tfree_uid(user);\n\treturn NULL;\n}\n\nvoid free_pipe_info(struct pipe_inode_info *pipe)\n{\n\tint i;\n\n\t(void) account_pipe_buffers(pipe->user, pipe->buffers, 0);\n\tfree_uid(pipe->user);\n\tfor (i = 0; i < pipe->buffers; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n\tif (pipe->tmp_page)\n\t\t__free_page(pipe->tmp_page);\n\tkfree(pipe->bufs);\n\tkfree(pipe);\n}\n\nstatic struct vfsmount *pipe_mnt __read_mostly;\n\n/*\n * pipefs_dname() is called from d_path().\n */\nstatic char *pipefs_dname(struct dentry *dentry, char *buffer, int buflen)\n{\n\treturn dynamic_dname(dentry, buffer, buflen, \"pipe:[%lu]\",\n\t\t\t\td_inode(dentry)->i_ino);\n}\n\nstatic const struct dentry_operations pipefs_dentry_operations = {\n\t.d_dname\t= pipefs_dname,\n};\n\nstatic struct inode * get_pipe_inode(void)\n{\n\tstruct inode *inode = new_inode_pseudo(pipe_mnt->mnt_sb);\n\tstruct pipe_inode_info *pipe;\n\n\tif (!inode)\n\t\tgoto fail_inode;\n\n\tinode->i_ino = get_next_ino();\n\n\tpipe = alloc_pipe_info();\n\tif (!pipe)\n\t\tgoto fail_iput;\n\n\tinode->i_pipe = pipe;\n\tpipe->files = 2;\n\tpipe->readers = pipe->writers = 1;\n\tinode->i_fop = &pipefifo_fops;\n\n\t/*\n\t * Mark the inode dirty from the very beginning,\n\t * that way it will never be moved to the dirty\n\t * list because \"mark_inode_dirty()\" will think\n\t * that it already _is_ on the dirty list.\n\t */\n\tinode->i_state = I_DIRTY;\n\tinode->i_mode = S_IFIFO | S_IRUSR | S_IWUSR;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);\n\n\treturn inode;\n\nfail_iput:\n\tiput(inode);\n\nfail_inode:\n\treturn NULL;\n}\n\nint create_pipe_files(struct file **res, int flags)\n{\n\tstruct inode *inode = get_pipe_inode();\n\tstruct file *f;\n\n\tif (!inode)\n\t\treturn -ENFILE;\n\n\tf = alloc_file_pseudo(inode, pipe_mnt, \"\",\n\t\t\t\tO_WRONLY | (flags & (O_NONBLOCK | O_DIRECT)),\n\t\t\t\t&pipefifo_fops);\n\tif (IS_ERR(f)) {\n\t\tfree_pipe_info(inode->i_pipe);\n\t\tiput(inode);\n\t\treturn PTR_ERR(f);\n\t}\n\n\tf->private_data = inode->i_pipe;\n\n\tres[0] = alloc_file_clone(f, O_RDONLY | (flags & O_NONBLOCK),\n\t\t\t\t  &pipefifo_fops);\n\tif (IS_ERR(res[0])) {\n\t\tput_pipe_info(inode, inode->i_pipe);\n\t\tfput(f);\n\t\treturn PTR_ERR(res[0]);\n\t}\n\tres[0]->private_data = inode->i_pipe;\n\tres[1] = f;\n\treturn 0;\n}\n\nstatic int __do_pipe_flags(int *fd, struct file **files, int flags)\n{\n\tint error;\n\tint fdw, fdr;\n\n\tif (flags & ~(O_CLOEXEC | O_NONBLOCK | O_DIRECT))\n\t\treturn -EINVAL;\n\n\terror = create_pipe_files(files, flags);\n\tif (error)\n\t\treturn error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_read_pipe;\n\tfdr = error;\n\n\terror = get_unused_fd_flags(flags);\n\tif (error < 0)\n\t\tgoto err_fdr;\n\tfdw = error;\n\n\taudit_fd_pair(fdr, fdw);\n\tfd[0] = fdr;\n\tfd[1] = fdw;\n\treturn 0;\n\n err_fdr:\n\tput_unused_fd(fdr);\n err_read_pipe:\n\tfput(files[0]);\n\tfput(files[1]);\n\treturn error;\n}\n\nint do_pipe_flags(int *fd, int flags)\n{\n\tstruct file *files[2];\n\tint error = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tfd_install(fd[0], files[0]);\n\t\tfd_install(fd[1], files[1]);\n\t}\n\treturn error;\n}\n\n/*\n * sys_pipe() is the normal C calling standard for creating\n * a pipe. It's not the way Unix traditionally does this, though.\n */\nstatic int do_pipe2(int __user *fildes, int flags)\n{\n\tstruct file *files[2];\n\tint fd[2];\n\tint error;\n\n\terror = __do_pipe_flags(fd, files, flags);\n\tif (!error) {\n\t\tif (unlikely(copy_to_user(fildes, fd, sizeof(fd)))) {\n\t\t\tfput(files[0]);\n\t\t\tfput(files[1]);\n\t\t\tput_unused_fd(fd[0]);\n\t\t\tput_unused_fd(fd[1]);\n\t\t\terror = -EFAULT;\n\t\t} else {\n\t\t\tfd_install(fd[0], files[0]);\n\t\t\tfd_install(fd[1], files[1]);\n\t\t}\n\t}\n\treturn error;\n}\n\nSYSCALL_DEFINE2(pipe2, int __user *, fildes, int, flags)\n{\n\treturn do_pipe2(fildes, flags);\n}\n\nSYSCALL_DEFINE1(pipe, int __user *, fildes)\n{\n\treturn do_pipe2(fildes, 0);\n}\n\nstatic int wait_for_partner(struct pipe_inode_info *pipe, unsigned int *cnt)\n{\n\tint cur = *cnt;\t\n\n\twhile (cur == *cnt) {\n\t\tpipe_wait(pipe);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t}\n\treturn cur == *cnt ? -ERESTARTSYS : 0;\n}\n\nstatic void wake_up_partner(struct pipe_inode_info *pipe)\n{\n\twake_up_interruptible(&pipe->wait);\n}\n\nstatic int fifo_open(struct inode *inode, struct file *filp)\n{\n\tstruct pipe_inode_info *pipe;\n\tbool is_pipe = inode->i_sb->s_magic == PIPEFS_MAGIC;\n\tint ret;\n\n\tfilp->f_version = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (inode->i_pipe) {\n\t\tpipe = inode->i_pipe;\n\t\tpipe->files++;\n\t\tspin_unlock(&inode->i_lock);\n\t} else {\n\t\tspin_unlock(&inode->i_lock);\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\t\tpipe->files = 1;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (unlikely(inode->i_pipe)) {\n\t\t\tinode->i_pipe->files++;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tfree_pipe_info(pipe);\n\t\t\tpipe = inode->i_pipe;\n\t\t} else {\n\t\t\tinode->i_pipe = pipe;\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tfilp->private_data = pipe;\n\t/* OK, we have a pipe and it's pinned down */\n\n\t__pipe_lock(pipe);\n\n\t/* We can only do regular read/write on fifos */\n\tfilp->f_mode &= (FMODE_READ | FMODE_WRITE);\n\n\tswitch (filp->f_mode) {\n\tcase FMODE_READ:\n\t/*\n\t *  O_RDONLY\n\t *  POSIX.1 says that O_NONBLOCK means return with the FIFO\n\t *  opened, even when there is no process writing the FIFO.\n\t */\n\t\tpipe->r_counter++;\n\t\tif (pipe->readers++ == 0)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->writers) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\t\t/* suppress EPOLLHUP until we have\n\t\t\t\t * seen a writer */\n\t\t\t\tfilp->f_version = pipe->w_counter;\n\t\t\t} else {\n\t\t\t\tif (wait_for_partner(pipe, &pipe->w_counter))\n\t\t\t\t\tgoto err_rd;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t\n\tcase FMODE_WRITE:\n\t/*\n\t *  O_WRONLY\n\t *  POSIX.1 says that O_NONBLOCK means return -1 with\n\t *  errno=ENXIO when there is no process reading the FIFO.\n\t */\n\t\tret = -ENXIO;\n\t\tif (!is_pipe && (filp->f_flags & O_NONBLOCK) && !pipe->readers)\n\t\t\tgoto err;\n\n\t\tpipe->w_counter++;\n\t\tif (!pipe->writers++)\n\t\t\twake_up_partner(pipe);\n\n\t\tif (!is_pipe && !pipe->readers) {\n\t\t\tif (wait_for_partner(pipe, &pipe->r_counter))\n\t\t\t\tgoto err_wr;\n\t\t}\n\t\tbreak;\n\t\n\tcase FMODE_READ | FMODE_WRITE:\n\t/*\n\t *  O_RDWR\n\t *  POSIX.1 leaves this case \"undefined\" when O_NONBLOCK is set.\n\t *  This implementation will NEVER block on a O_RDWR open, since\n\t *  the process can at least talk to itself.\n\t */\n\n\t\tpipe->readers++;\n\t\tpipe->writers++;\n\t\tpipe->r_counter++;\n\t\tpipe->w_counter++;\n\t\tif (pipe->readers == 1 || pipe->writers == 1)\n\t\t\twake_up_partner(pipe);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t/* Ok! */\n\t__pipe_unlock(pipe);\n\treturn 0;\n\nerr_rd:\n\tif (!--pipe->readers)\n\t\twake_up_interruptible(&pipe->wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr_wr:\n\tif (!--pipe->writers)\n\t\twake_up_interruptible(&pipe->wait);\n\tret = -ERESTARTSYS;\n\tgoto err;\n\nerr:\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn ret;\n}\n\nconst struct file_operations pipefifo_fops = {\n\t.open\t\t= fifo_open,\n\t.llseek\t\t= no_llseek,\n\t.read_iter\t= pipe_read,\n\t.write_iter\t= pipe_write,\n\t.poll\t\t= pipe_poll,\n\t.unlocked_ioctl\t= pipe_ioctl,\n\t.release\t= pipe_release,\n\t.fasync\t\t= pipe_fasync,\n};\n\n/*\n * Currently we rely on the pipe array holding a power-of-2 number\n * of pages. Returns 0 on error.\n */\nunsigned int round_pipe_size(unsigned long size)\n{\n\tif (size > (1U << 31))\n\t\treturn 0;\n\n\t/* Minimum pipe size, as required by POSIX */\n\tif (size < PAGE_SIZE)\n\t\treturn PAGE_SIZE;\n\n\treturn roundup_pow_of_two(size);\n}\n\n/*\n * Allocate a new array of pipe buffers and copy the info over. Returns the\n * pipe size if successful, or return -ERROR on error.\n */\nstatic long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)\n{\n\tstruct pipe_buffer *bufs;\n\tunsigned int size, nr_pages;\n\tunsigned long user_bufs;\n\tlong ret = 0;\n\n\tsize = round_pipe_size(arg);\n\tnr_pages = size >> PAGE_SHIFT;\n\n\tif (!nr_pages)\n\t\treturn -EINVAL;\n\n\t/*\n\t * If trying to increase the pipe capacity, check that an\n\t * unprivileged user is not trying to exceed various limits\n\t * (soft limit check here, hard limit check just below).\n\t * Decreasing the pipe capacity is always permitted, even\n\t * if the user is currently over a limit.\n\t */\n\tif (nr_pages > pipe->buffers &&\n\t\t\tsize > pipe_max_size && !capable(CAP_SYS_RESOURCE))\n\t\treturn -EPERM;\n\n\tuser_bufs = account_pipe_buffers(pipe->user, pipe->buffers, nr_pages);\n\n\tif (nr_pages > pipe->buffers &&\n\t\t\t(too_many_pipe_buffers_hard(user_bufs) ||\n\t\t\t too_many_pipe_buffers_soft(user_bufs)) &&\n\t\t\tis_unprivileged_user()) {\n\t\tret = -EPERM;\n\t\tgoto out_revert_acct;\n\t}\n\n\t/*\n\t * We can shrink the pipe, if arg >= pipe->nrbufs. Since we don't\n\t * expect a lot of shrink+grow operations, just free and allocate\n\t * again like we would do for growing. If the pipe currently\n\t * contains more buffers than arg, then return busy.\n\t */\n\tif (nr_pages < pipe->nrbufs) {\n\t\tret = -EBUSY;\n\t\tgoto out_revert_acct;\n\t}\n\n\tbufs = kcalloc(nr_pages, sizeof(*bufs),\n\t\t       GFP_KERNEL_ACCOUNT | __GFP_NOWARN);\n\tif (unlikely(!bufs)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_revert_acct;\n\t}\n\n\t/*\n\t * The pipe array wraps around, so just start the new one at zero\n\t * and adjust the indexes.\n\t */\n\tif (pipe->nrbufs) {\n\t\tunsigned int tail;\n\t\tunsigned int head;\n\n\t\ttail = pipe->curbuf + pipe->nrbufs;\n\t\tif (tail < pipe->buffers)\n\t\t\ttail = 0;\n\t\telse\n\t\t\ttail &= (pipe->buffers - 1);\n\n\t\thead = pipe->nrbufs - tail;\n\t\tif (head)\n\t\t\tmemcpy(bufs, pipe->bufs + pipe->curbuf, head * sizeof(struct pipe_buffer));\n\t\tif (tail)\n\t\t\tmemcpy(bufs + head, pipe->bufs, tail * sizeof(struct pipe_buffer));\n\t}\n\n\tpipe->curbuf = 0;\n\tkfree(pipe->bufs);\n\tpipe->bufs = bufs;\n\tpipe->buffers = nr_pages;\n\treturn nr_pages * PAGE_SIZE;\n\nout_revert_acct:\n\t(void) account_pipe_buffers(pipe->user, nr_pages, pipe->buffers);\n\treturn ret;\n}\n\n/*\n * After the inode slimming patch, i_pipe/i_bdev/i_cdev share the same\n * location, so checking ->i_pipe is not enough to verify that this is a\n * pipe.\n */\nstruct pipe_inode_info *get_pipe_info(struct file *file)\n{\n\treturn file->f_op == &pipefifo_fops ? file->private_data : NULL;\n}\n\nlong pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->buffers * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}\n\nstatic const struct super_operations pipefs_ops = {\n\t.destroy_inode = free_inode_nonrcu,\n\t.statfs = simple_statfs,\n};\n\n/*\n * pipefs should _never_ be mounted by userland - too much of security hassle,\n * no real gain from having the whole whorehouse mounted. So we don't need\n * any operations on the root directory. However, we need a non-trivial\n * d_name - pipe: will go nicely and kill the special-casing in procfs.\n */\nstatic struct dentry *pipefs_mount(struct file_system_type *fs_type,\n\t\t\t int flags, const char *dev_name, void *data)\n{\n\treturn mount_pseudo(fs_type, \"pipe:\", &pipefs_ops,\n\t\t\t&pipefs_dentry_operations, PIPEFS_MAGIC);\n}\n\nstatic struct file_system_type pipe_fs_type = {\n\t.name\t\t= \"pipefs\",\n\t.mount\t\t= pipefs_mount,\n\t.kill_sb\t= kill_anon_super,\n};\n\nstatic int __init init_pipe_fs(void)\n{\n\tint err = register_filesystem(&pipe_fs_type);\n\n\tif (!err) {\n\t\tpipe_mnt = kern_mount(&pipe_fs_type);\n\t\tif (IS_ERR(pipe_mnt)) {\n\t\t\terr = PTR_ERR(pipe_mnt);\n\t\t\tunregister_filesystem(&pipe_fs_type);\n\t\t}\n\t}\n\treturn err;\n}\n\nfs_initcall(init_pipe_fs);\n", "/*\n * \"splice\": joining two ropes together by interweaving their strands.\n *\n * This is the \"extended pipe\" functionality, where a pipe is used as\n * an arbitrary in-memory buffer. Think of a pipe as a small kernel\n * buffer that you can use to transfer data from one end to the other.\n *\n * The traditional unix read/write is extended with a \"splice()\" operation\n * that transfers data buffers to or from a pipe buffer.\n *\n * Named by Larry McVoy, original implementation from Linus, extended by\n * Jens to support splicing to files, network, direct splicing, etc and\n * fixing lots of bugs.\n *\n * Copyright (C) 2005-2006 Jens Axboe <axboe@kernel.dk>\n * Copyright (C) 2005-2006 Linus Torvalds <torvalds@osdl.org>\n * Copyright (C) 2006 Ingo Molnar <mingo@elte.hu>\n *\n */\n#include <linux/bvec.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/splice.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/export.h>\n#include <linux/syscalls.h>\n#include <linux/uio.h>\n#include <linux/security.h>\n#include <linux/gfp.h>\n#include <linux/socket.h>\n#include <linux/compat.h>\n#include <linux/sched/signal.h>\n\n#include \"internal.h\"\n\n/*\n * Attempt to steal a page from a pipe buffer. This should perhaps go into\n * a vm helper function, it's already simplified quite a bit by the\n * addition of remove_mapping(). If success is returned, the caller may\n * attempt to reuse this page for another destination.\n */\nstatic int page_cache_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t     struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\tstruct address_space *mapping;\n\n\tlock_page(page);\n\n\tmapping = page_mapping(page);\n\tif (mapping) {\n\t\tWARN_ON(!PageUptodate(page));\n\n\t\t/*\n\t\t * At least for ext2 with nobh option, we need to wait on\n\t\t * writeback completing on this page, since we'll remove it\n\t\t * from the pagecache.  Otherwise truncate wont wait on the\n\t\t * page, allowing the disk blocks to be reused by someone else\n\t\t * before we actually wrote our data to them. fs corruption\n\t\t * ensues.\n\t\t */\n\t\twait_on_page_writeback(page);\n\n\t\tif (page_has_private(page) &&\n\t\t    !try_to_release_page(page, GFP_KERNEL))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If we succeeded in removing the mapping, set LRU flag\n\t\t * and return good.\n\t\t */\n\t\tif (remove_mapping(mapping, page)) {\n\t\t\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Raced with truncate or failed to remove page from current\n\t * address space, unlock and return failure.\n\t */\nout_unlock:\n\tunlock_page(page);\n\treturn 1;\n}\n\nstatic void page_cache_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t\tstruct pipe_buffer *buf)\n{\n\tput_page(buf->page);\n\tbuf->flags &= ~PIPE_BUF_FLAG_LRU;\n}\n\n/*\n * Check whether the contents of buf is OK to access. Since the content\n * is a page cache page, IO may be in flight.\n */\nstatic int page_cache_pipe_buf_confirm(struct pipe_inode_info *pipe,\n\t\t\t\t       struct pipe_buffer *buf)\n{\n\tstruct page *page = buf->page;\n\tint err;\n\n\tif (!PageUptodate(page)) {\n\t\tlock_page(page);\n\n\t\t/*\n\t\t * Page got truncated/unhashed. This will cause a 0-byte\n\t\t * splice, if this is the first page.\n\t\t */\n\t\tif (!page->mapping) {\n\t\t\terr = -ENODATA;\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * Uh oh, read-error from disk.\n\t\t */\n\t\tif (!PageUptodate(page)) {\n\t\t\terr = -EIO;\n\t\t\tgoto error;\n\t\t}\n\n\t\t/*\n\t\t * Page is ok afterall, we are done.\n\t\t */\n\t\tunlock_page(page);\n\t}\n\n\treturn 0;\nerror:\n\tunlock_page(page);\n\treturn err;\n}\n\nconst struct pipe_buf_operations page_cache_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = page_cache_pipe_buf_confirm,\n\t.release = page_cache_pipe_buf_release,\n\t.steal = page_cache_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic int user_page_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tif (!(buf->flags & PIPE_BUF_FLAG_GIFT))\n\t\treturn 1;\n\n\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\treturn generic_pipe_buf_steal(pipe, buf);\n}\n\nstatic const struct pipe_buf_operations user_page_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = page_cache_pipe_buf_release,\n\t.steal = user_page_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic void wakeup_pipe_readers(struct pipe_inode_info *pipe)\n{\n\tsmp_mb();\n\tif (waitqueue_active(&pipe->wait))\n\t\twake_up_interruptible(&pipe->wait);\n\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n}\n\n/**\n * splice_to_pipe - fill passed data into a pipe\n * @pipe:\tpipe to fill\n * @spd:\tdata to fill\n *\n * Description:\n *    @spd contains a map of pages and len/offset tuples, along with\n *    the struct pipe_buf_operations associated with these pages. This\n *    function will link that data to the pipe.\n *\n */\nssize_t splice_to_pipe(struct pipe_inode_info *pipe,\n\t\t       struct splice_pipe_desc *spd)\n{\n\tunsigned int spd_pages = spd->nr_pages;\n\tint ret = 0, page_nr = 0;\n\n\tif (!spd_pages)\n\t\treturn 0;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t\tgoto out;\n\t}\n\n\twhile (pipe->nrbufs < pipe->buffers) {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tstruct pipe_buffer *buf = pipe->bufs + newbuf;\n\n\t\tbuf->page = spd->pages[page_nr];\n\t\tbuf->offset = spd->partial[page_nr].offset;\n\t\tbuf->len = spd->partial[page_nr].len;\n\t\tbuf->private = spd->partial[page_nr].private;\n\t\tbuf->ops = spd->ops;\n\t\tbuf->flags = 0;\n\n\t\tpipe->nrbufs++;\n\t\tpage_nr++;\n\t\tret += buf->len;\n\n\t\tif (!--spd->nr_pages)\n\t\t\tbreak;\n\t}\n\n\tif (!ret)\n\t\tret = -EAGAIN;\n\nout:\n\twhile (page_nr < spd_pages)\n\t\tspd->spd_release(spd, page_nr++);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(splice_to_pipe);\n\nssize_t add_to_pipe(struct pipe_inode_info *pipe, struct pipe_buffer *buf)\n{\n\tint ret;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t} else if (pipe->nrbufs == pipe->buffers) {\n\t\tret = -EAGAIN;\n\t} else {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tpipe->bufs[newbuf] = *buf;\n\t\tpipe->nrbufs++;\n\t\treturn buf->len;\n\t}\n\tpipe_buf_release(pipe, buf);\n\treturn ret;\n}\nEXPORT_SYMBOL(add_to_pipe);\n\n/*\n * Check if we need to grow the arrays holding pages and partial page\n * descriptions.\n */\nint splice_grow_spd(const struct pipe_inode_info *pipe, struct splice_pipe_desc *spd)\n{\n\tunsigned int buffers = READ_ONCE(pipe->buffers);\n\n\tspd->nr_pages_max = buffers;\n\tif (buffers <= PIPE_DEF_BUFFERS)\n\t\treturn 0;\n\n\tspd->pages = kmalloc_array(buffers, sizeof(struct page *), GFP_KERNEL);\n\tspd->partial = kmalloc_array(buffers, sizeof(struct partial_page),\n\t\t\t\t     GFP_KERNEL);\n\n\tif (spd->pages && spd->partial)\n\t\treturn 0;\n\n\tkfree(spd->pages);\n\tkfree(spd->partial);\n\treturn -ENOMEM;\n}\n\nvoid splice_shrink_spd(struct splice_pipe_desc *spd)\n{\n\tif (spd->nr_pages_max <= PIPE_DEF_BUFFERS)\n\t\treturn;\n\n\tkfree(spd->pages);\n\tkfree(spd->partial);\n}\n\n/**\n * generic_file_splice_read - splice data from file to a pipe\n * @in:\t\tfile to splice from\n * @ppos:\tposition in @in\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given file and fill them into a pipe. Can be\n *    used as long as it has more or less sane ->read_iter().\n *\n */\nssize_t generic_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t\t unsigned int flags)\n{\n\tstruct iov_iter to;\n\tstruct kiocb kiocb;\n\tint idx, ret;\n\n\tiov_iter_pipe(&to, READ, pipe, len);\n\tidx = to.idx;\n\tinit_sync_kiocb(&kiocb, in);\n\tkiocb.ki_pos = *ppos;\n\tret = call_read_iter(in, &kiocb, &to);\n\tif (ret > 0) {\n\t\t*ppos = kiocb.ki_pos;\n\t\tfile_accessed(in);\n\t} else if (ret < 0) {\n\t\tto.idx = idx;\n\t\tto.iov_offset = 0;\n\t\tiov_iter_advance(&to, 0); /* to free what was emitted */\n\t\t/*\n\t\t * callers of ->splice_read() expect -EAGAIN on\n\t\t * \"can't put anything in there\", rather than -EFAULT.\n\t\t */\n\t\tif (ret == -EFAULT)\n\t\t\tret = -EAGAIN;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_file_splice_read);\n\nconst struct pipe_buf_operations default_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = generic_pipe_buf_steal,\n\t.get = generic_pipe_buf_get,\n};\n\nstatic int generic_pipe_buf_nosteal(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\treturn 1;\n}\n\n/* Pipe buffer operations for a socket and similar. */\nconst struct pipe_buf_operations nosteal_pipe_buf_ops = {\n\t.can_merge = 0,\n\t.confirm = generic_pipe_buf_confirm,\n\t.release = generic_pipe_buf_release,\n\t.steal = generic_pipe_buf_nosteal,\n\t.get = generic_pipe_buf_get,\n};\nEXPORT_SYMBOL(nosteal_pipe_buf_ops);\n\nstatic ssize_t kernel_readv(struct file *file, const struct kvec *vec,\n\t\t\t    unsigned long vlen, loff_t offset)\n{\n\tmm_segment_t old_fs;\n\tloff_t pos = offset;\n\tssize_t res;\n\n\told_fs = get_fs();\n\tset_fs(get_ds());\n\t/* The cast to a user pointer is valid due to the set_fs() */\n\tres = vfs_readv(file, (const struct iovec __user *)vec, vlen, &pos, 0);\n\tset_fs(old_fs);\n\n\treturn res;\n}\n\nstatic ssize_t default_file_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t\t unsigned int flags)\n{\n\tstruct kvec *vec, __vec[PIPE_DEF_BUFFERS];\n\tstruct iov_iter to;\n\tstruct page **pages;\n\tunsigned int nr_pages;\n\tsize_t offset, base, copied = 0;\n\tssize_t res;\n\tint i;\n\n\tif (pipe->nrbufs == pipe->buffers)\n\t\treturn -EAGAIN;\n\n\t/*\n\t * Try to keep page boundaries matching to source pagecache ones -\n\t * it probably won't be much help, but...\n\t */\n\toffset = *ppos & ~PAGE_MASK;\n\n\tiov_iter_pipe(&to, READ, pipe, len + offset);\n\n\tres = iov_iter_get_pages_alloc(&to, &pages, len + offset, &base);\n\tif (res <= 0)\n\t\treturn -ENOMEM;\n\n\tnr_pages = DIV_ROUND_UP(res + base, PAGE_SIZE);\n\n\tvec = __vec;\n\tif (nr_pages > PIPE_DEF_BUFFERS) {\n\t\tvec = kmalloc_array(nr_pages, sizeof(struct kvec), GFP_KERNEL);\n\t\tif (unlikely(!vec)) {\n\t\t\tres = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tpipe->bufs[to.idx].offset = offset;\n\tpipe->bufs[to.idx].len -= offset;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t this_len = min_t(size_t, len, PAGE_SIZE - offset);\n\t\tvec[i].iov_base = page_address(pages[i]) + offset;\n\t\tvec[i].iov_len = this_len;\n\t\tlen -= this_len;\n\t\toffset = 0;\n\t}\n\n\tres = kernel_readv(in, vec, nr_pages, *ppos);\n\tif (res > 0) {\n\t\tcopied = res;\n\t\t*ppos += res;\n\t}\n\n\tif (vec != __vec)\n\t\tkfree(vec);\nout:\n\tfor (i = 0; i < nr_pages; i++)\n\t\tput_page(pages[i]);\n\tkvfree(pages);\n\tiov_iter_advance(&to, copied);\t/* truncates and discards */\n\treturn res;\n}\n\n/*\n * Send 'sd->len' bytes to socket from 'sd->file' at position 'sd->pos'\n * using sendpage(). Return the number of bytes sent.\n */\nstatic int pipe_to_sendpage(struct pipe_inode_info *pipe,\n\t\t\t    struct pipe_buffer *buf, struct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\tloff_t pos = sd->pos;\n\tint more;\n\n\tif (!likely(file->f_op->sendpage))\n\t\treturn -EINVAL;\n\n\tmore = (sd->flags & SPLICE_F_MORE) ? MSG_MORE : 0;\n\n\tif (sd->len < sd->total_len && pipe->nrbufs > 1)\n\t\tmore |= MSG_SENDPAGE_NOTLAST;\n\n\treturn file->f_op->sendpage(file, buf->page, buf->offset,\n\t\t\t\t    sd->len, &pos, more);\n}\n\nstatic void wakeup_pipe_writers(struct pipe_inode_info *pipe)\n{\n\tsmp_mb();\n\tif (waitqueue_active(&pipe->wait))\n\t\twake_up_interruptible(&pipe->wait);\n\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n}\n\n/**\n * splice_from_pipe_feed - feed available data from a pipe to a file\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation to @actor\n * @actor:\thandler that splices the data\n *\n * Description:\n *    This function loops over the pipe and calls @actor to do the\n *    actual moving of a single struct pipe_buffer to the desired\n *    destination.  It returns when there's no more buffers left in\n *    the pipe or if the requested number of bytes (@sd->total_len)\n *    have been copied.  It returns a positive number (one) if the\n *    pipe needs to be filled with more data, zero if the required\n *    number of bytes have been copied and -errno on error.\n *\n *    This, together with splice_from_pipe_{begin,end,next}, may be\n *    used to implement the functionality of __splice_from_pipe() when\n *    locking is required around copying the pipe buffers to the\n *    destination.\n */\nstatic int splice_from_pipe_feed(struct pipe_inode_info *pipe, struct splice_desc *sd,\n\t\t\t  splice_actor *actor)\n{\n\tint ret;\n\n\twhile (pipe->nrbufs) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + pipe->curbuf;\n\n\t\tsd->len = buf->len;\n\t\tif (sd->len > sd->total_len)\n\t\t\tsd->len = sd->total_len;\n\n\t\tret = pipe_buf_confirm(pipe, buf);\n\t\tif (unlikely(ret)) {\n\t\t\tif (ret == -ENODATA)\n\t\t\t\tret = 0;\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = actor(pipe, buf, sd);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tbuf->offset += ret;\n\t\tbuf->len -= ret;\n\n\t\tsd->num_spliced += ret;\n\t\tsd->len -= ret;\n\t\tsd->pos += ret;\n\t\tsd->total_len -= ret;\n\n\t\tif (!buf->len) {\n\t\t\tpipe_buf_release(pipe, buf);\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t\tif (pipe->files)\n\t\t\t\tsd->need_wakeup = true;\n\t\t}\n\n\t\tif (!sd->total_len)\n\t\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n/**\n * splice_from_pipe_next - wait for some data to splice from\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function will wait for some data and return a positive\n *    value (one) if pipe buffers are available.  It will return zero\n *    or -errno if no more data needs to be spliced.\n */\nstatic int splice_from_pipe_next(struct pipe_inode_info *pipe, struct splice_desc *sd)\n{\n\t/*\n\t * Check for signal early to make process killable when there are\n\t * always buffers available\n\t */\n\tif (signal_pending(current))\n\t\treturn -ERESTARTSYS;\n\n\twhile (!pipe->nrbufs) {\n\t\tif (!pipe->writers)\n\t\t\treturn 0;\n\n\t\tif (!pipe->waiting_writers && sd->num_spliced)\n\t\t\treturn 0;\n\n\t\tif (sd->flags & SPLICE_F_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\n\t\tif (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\n\t\tif (sd->need_wakeup) {\n\t\t\twakeup_pipe_writers(pipe);\n\t\t\tsd->need_wakeup = false;\n\t\t}\n\n\t\tpipe_wait(pipe);\n\t}\n\n\treturn 1;\n}\n\n/**\n * splice_from_pipe_begin - start splicing from pipe\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function should be called before a loop containing\n *    splice_from_pipe_next() and splice_from_pipe_feed() to\n *    initialize the necessary fields of @sd.\n */\nstatic void splice_from_pipe_begin(struct splice_desc *sd)\n{\n\tsd->num_spliced = 0;\n\tsd->need_wakeup = false;\n}\n\n/**\n * splice_from_pipe_end - finish splicing from pipe\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation about the splice operation\n *\n * Description:\n *    This function will wake up pipe writers if necessary.  It should\n *    be called after a loop containing splice_from_pipe_next() and\n *    splice_from_pipe_feed().\n */\nstatic void splice_from_pipe_end(struct pipe_inode_info *pipe, struct splice_desc *sd)\n{\n\tif (sd->need_wakeup)\n\t\twakeup_pipe_writers(pipe);\n}\n\n/**\n * __splice_from_pipe - splice data from a pipe to given actor\n * @pipe:\tpipe to splice from\n * @sd:\t\tinformation to @actor\n * @actor:\thandler that splices the data\n *\n * Description:\n *    This function does little more than loop over the pipe and call\n *    @actor to do the actual moving of a single struct pipe_buffer to\n *    the desired destination. See pipe_to_file, pipe_to_sendpage, or\n *    pipe_to_user.\n *\n */\nssize_t __splice_from_pipe(struct pipe_inode_info *pipe, struct splice_desc *sd,\n\t\t\t   splice_actor *actor)\n{\n\tint ret;\n\n\tsplice_from_pipe_begin(sd);\n\tdo {\n\t\tcond_resched();\n\t\tret = splice_from_pipe_next(pipe, sd);\n\t\tif (ret > 0)\n\t\t\tret = splice_from_pipe_feed(pipe, sd, actor);\n\t} while (ret > 0);\n\tsplice_from_pipe_end(pipe, sd);\n\n\treturn sd->num_spliced ? sd->num_spliced : ret;\n}\nEXPORT_SYMBOL(__splice_from_pipe);\n\n/**\n * splice_from_pipe - splice data from a pipe to a file\n * @pipe:\tpipe to splice from\n * @out:\tfile to splice to\n * @ppos:\tposition in @out\n * @len:\thow many bytes to splice\n * @flags:\tsplice modifier flags\n * @actor:\thandler that splices the data\n *\n * Description:\n *    See __splice_from_pipe. This function locks the pipe inode,\n *    otherwise it's identical to __splice_from_pipe().\n *\n */\nssize_t splice_from_pipe(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t loff_t *ppos, size_t len, unsigned int flags,\n\t\t\t splice_actor *actor)\n{\n\tssize_t ret;\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\n\tpipe_lock(pipe);\n\tret = __splice_from_pipe(pipe, &sd, actor);\n\tpipe_unlock(pipe);\n\n\treturn ret;\n}\n\n/**\n * iter_file_splice_write - splice data from a pipe to a file\n * @pipe:\tpipe info\n * @out:\tfile to write to\n * @ppos:\tposition in @out\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will either move or copy pages (determined by @flags options) from\n *    the given pipe inode to the given file.\n *    This one is ->write_iter-based.\n *\n */\nssize_t\niter_file_splice_write(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t  loff_t *ppos, size_t len, unsigned int flags)\n{\n\tstruct splice_desc sd = {\n\t\t.total_len = len,\n\t\t.flags = flags,\n\t\t.pos = *ppos,\n\t\t.u.file = out,\n\t};\n\tint nbufs = pipe->buffers;\n\tstruct bio_vec *array = kcalloc(nbufs, sizeof(struct bio_vec),\n\t\t\t\t\tGFP_KERNEL);\n\tssize_t ret;\n\n\tif (unlikely(!array))\n\t\treturn -ENOMEM;\n\n\tpipe_lock(pipe);\n\n\tsplice_from_pipe_begin(&sd);\n\twhile (sd.total_len) {\n\t\tstruct iov_iter from;\n\t\tsize_t left;\n\t\tint n, idx;\n\n\t\tret = splice_from_pipe_next(pipe, &sd);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\n\t\tif (unlikely(nbufs < pipe->buffers)) {\n\t\t\tkfree(array);\n\t\t\tnbufs = pipe->buffers;\n\t\t\tarray = kcalloc(nbufs, sizeof(struct bio_vec),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!array) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* build the vector */\n\t\tleft = sd.total_len;\n\t\tfor (n = 0, idx = pipe->curbuf; left && n < pipe->nrbufs; n++, idx++) {\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + idx;\n\t\t\tsize_t this_len = buf->len;\n\n\t\t\tif (this_len > left)\n\t\t\t\tthis_len = left;\n\n\t\t\tif (idx == pipe->buffers - 1)\n\t\t\t\tidx = -1;\n\n\t\t\tret = pipe_buf_confirm(pipe, buf);\n\t\t\tif (unlikely(ret)) {\n\t\t\t\tif (ret == -ENODATA)\n\t\t\t\t\tret = 0;\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tarray[n].bv_page = buf->page;\n\t\t\tarray[n].bv_len = this_len;\n\t\t\tarray[n].bv_offset = buf->offset;\n\t\t\tleft -= this_len;\n\t\t}\n\n\t\tiov_iter_bvec(&from, WRITE, array, n, sd.total_len - left);\n\t\tret = vfs_iter_write(out, &from, &sd.pos, 0);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\n\t\tsd.num_spliced += ret;\n\t\tsd.total_len -= ret;\n\t\t*ppos = sd.pos;\n\n\t\t/* dismiss the fully eaten buffers, adjust the partial one */\n\t\twhile (ret) {\n\t\t\tstruct pipe_buffer *buf = pipe->bufs + pipe->curbuf;\n\t\t\tif (ret >= buf->len) {\n\t\t\t\tret -= buf->len;\n\t\t\t\tbuf->len = 0;\n\t\t\t\tpipe_buf_release(pipe, buf);\n\t\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\t\tpipe->nrbufs--;\n\t\t\t\tif (pipe->files)\n\t\t\t\t\tsd.need_wakeup = true;\n\t\t\t} else {\n\t\t\t\tbuf->offset += ret;\n\t\t\t\tbuf->len -= ret;\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\t}\ndone:\n\tkfree(array);\n\tsplice_from_pipe_end(pipe, &sd);\n\n\tpipe_unlock(pipe);\n\n\tif (sd.num_spliced)\n\t\tret = sd.num_spliced;\n\n\treturn ret;\n}\n\nEXPORT_SYMBOL(iter_file_splice_write);\n\nstatic int write_pipe_buf(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\t  struct splice_desc *sd)\n{\n\tint ret;\n\tvoid *data;\n\tloff_t tmp = sd->pos;\n\n\tdata = kmap(buf->page);\n\tret = __kernel_write(sd->u.file, data + buf->offset, sd->len, &tmp);\n\tkunmap(buf->page);\n\n\treturn ret;\n}\n\nstatic ssize_t default_file_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t\t struct file *out, loff_t *ppos,\n\t\t\t\t\t size_t len, unsigned int flags)\n{\n\tssize_t ret;\n\n\tret = splice_from_pipe(pipe, out, ppos, len, flags, write_pipe_buf);\n\tif (ret > 0)\n\t\t*ppos += ret;\n\n\treturn ret;\n}\n\n/**\n * generic_splice_sendpage - splice data from a pipe to a socket\n * @pipe:\tpipe to splice from\n * @out:\tsocket to write to\n * @ppos:\tposition in @out\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will send @len bytes from the pipe to a network socket. No data copying\n *    is involved.\n *\n */\nssize_t generic_splice_sendpage(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t\tloff_t *ppos, size_t len, unsigned int flags)\n{\n\treturn splice_from_pipe(pipe, out, ppos, len, flags, pipe_to_sendpage);\n}\n\nEXPORT_SYMBOL(generic_splice_sendpage);\n\n/*\n * Attempt to initiate a splice from pipe to file.\n */\nstatic long do_splice_from(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t   loff_t *ppos, size_t len, unsigned int flags)\n{\n\tssize_t (*splice_write)(struct pipe_inode_info *, struct file *,\n\t\t\t\tloff_t *, size_t, unsigned int);\n\n\tif (out->f_op->splice_write)\n\t\tsplice_write = out->f_op->splice_write;\n\telse\n\t\tsplice_write = default_file_splice_write;\n\n\treturn splice_write(pipe, out, ppos, len, flags);\n}\n\n/*\n * Attempt to initiate a splice from a file to a pipe.\n */\nstatic long do_splice_to(struct file *in, loff_t *ppos,\n\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t unsigned int flags)\n{\n\tssize_t (*splice_read)(struct file *, loff_t *,\n\t\t\t       struct pipe_inode_info *, size_t, unsigned int);\n\tint ret;\n\n\tif (unlikely(!(in->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\tret = rw_verify_area(READ, in, ppos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (unlikely(len > MAX_RW_COUNT))\n\t\tlen = MAX_RW_COUNT;\n\n\tif (in->f_op->splice_read)\n\t\tsplice_read = in->f_op->splice_read;\n\telse\n\t\tsplice_read = default_file_splice_read;\n\n\treturn splice_read(in, ppos, pipe, len, flags);\n}\n\n/**\n * splice_direct_to_actor - splices data directly between two non-pipes\n * @in:\t\tfile to splice from\n * @sd:\t\tactor information on where to splice to\n * @actor:\thandles the data splicing\n *\n * Description:\n *    This is a special case helper to splice directly between two\n *    points, without requiring an explicit pipe. Internally an allocated\n *    pipe is cached in the process, and reused during the lifetime of\n *    that process.\n *\n */\nssize_t splice_direct_to_actor(struct file *in, struct splice_desc *sd,\n\t\t\t       splice_direct_actor *actor)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret, bytes;\n\tumode_t i_mode;\n\tsize_t len;\n\tint i, flags, more;\n\n\t/*\n\t * We require the input being a regular file, as we don't want to\n\t * randomly drop data for eg socket -> socket splicing. Use the\n\t * piped splicing for that!\n\t */\n\ti_mode = file_inode(in)->i_mode;\n\tif (unlikely(!S_ISREG(i_mode) && !S_ISBLK(i_mode)))\n\t\treturn -EINVAL;\n\n\t/*\n\t * neither in nor out is a pipe, setup an internal pipe attached to\n\t * 'out' and transfer the wanted data from 'in' to 'out' through that\n\t */\n\tpipe = current->splice_pipe;\n\tif (unlikely(!pipe)) {\n\t\tpipe = alloc_pipe_info();\n\t\tif (!pipe)\n\t\t\treturn -ENOMEM;\n\n\t\t/*\n\t\t * We don't have an immediate reader, but we'll read the stuff\n\t\t * out of the pipe right after the splice_to_pipe(). So set\n\t\t * PIPE_READERS appropriately.\n\t\t */\n\t\tpipe->readers = 1;\n\n\t\tcurrent->splice_pipe = pipe;\n\t}\n\n\t/*\n\t * Do the splice.\n\t */\n\tret = 0;\n\tbytes = 0;\n\tlen = sd->total_len;\n\tflags = sd->flags;\n\n\t/*\n\t * Don't block on output, we have to drain the direct pipe.\n\t */\n\tsd->flags &= ~SPLICE_F_NONBLOCK;\n\tmore = sd->flags & SPLICE_F_MORE;\n\n\tWARN_ON_ONCE(pipe->nrbufs != 0);\n\n\twhile (len) {\n\t\tsize_t read_len;\n\t\tloff_t pos = sd->pos, prev_pos = pos;\n\n\t\t/* Don't try to read more the pipe has space for. */\n\t\tread_len = min_t(size_t, len,\n\t\t\t\t (pipe->buffers - pipe->nrbufs) << PAGE_SHIFT);\n\t\tret = do_splice_to(in, &pos, pipe, read_len, flags);\n\t\tif (unlikely(ret <= 0))\n\t\t\tgoto out_release;\n\n\t\tread_len = ret;\n\t\tsd->total_len = read_len;\n\n\t\t/*\n\t\t * If more data is pending, set SPLICE_F_MORE\n\t\t * If this is the last data and SPLICE_F_MORE was not set\n\t\t * initially, clears it.\n\t\t */\n\t\tif (read_len < len)\n\t\t\tsd->flags |= SPLICE_F_MORE;\n\t\telse if (!more)\n\t\t\tsd->flags &= ~SPLICE_F_MORE;\n\t\t/*\n\t\t * NOTE: nonblocking mode only applies to the input. We\n\t\t * must not do the output in nonblocking mode as then we\n\t\t * could get stuck data in the internal pipe:\n\t\t */\n\t\tret = actor(pipe, sd);\n\t\tif (unlikely(ret <= 0)) {\n\t\t\tsd->pos = prev_pos;\n\t\t\tgoto out_release;\n\t\t}\n\n\t\tbytes += ret;\n\t\tlen -= ret;\n\t\tsd->pos = pos;\n\n\t\tif (ret < read_len) {\n\t\t\tsd->pos = prev_pos + ret;\n\t\t\tgoto out_release;\n\t\t}\n\t}\n\ndone:\n\tpipe->nrbufs = pipe->curbuf = 0;\n\tfile_accessed(in);\n\treturn bytes;\n\nout_release:\n\t/*\n\t * If we did an incomplete transfer we must release\n\t * the pipe buffers in question:\n\t */\n\tfor (i = 0; i < pipe->buffers; i++) {\n\t\tstruct pipe_buffer *buf = pipe->bufs + i;\n\n\t\tif (buf->ops)\n\t\t\tpipe_buf_release(pipe, buf);\n\t}\n\n\tif (!bytes)\n\t\tbytes = ret;\n\n\tgoto done;\n}\nEXPORT_SYMBOL(splice_direct_to_actor);\n\nstatic int direct_splice_actor(struct pipe_inode_info *pipe,\n\t\t\t       struct splice_desc *sd)\n{\n\tstruct file *file = sd->u.file;\n\n\treturn do_splice_from(pipe, file, sd->opos, sd->total_len,\n\t\t\t      sd->flags);\n}\n\n/**\n * do_splice_direct - splices data directly between two files\n * @in:\t\tfile to splice from\n * @ppos:\tinput file offset\n * @out:\tfile to splice to\n * @opos:\toutput file offset\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    For use by do_sendfile(). splice can easily emulate sendfile, but\n *    doing it in the application would incur an extra system call\n *    (splice in + splice out, as compared to just sendfile()). So this helper\n *    can splice directly through a process-private pipe.\n *\n */\nlong do_splice_direct(struct file *in, loff_t *ppos, struct file *out,\n\t\t      loff_t *opos, size_t len, unsigned int flags)\n{\n\tstruct splice_desc sd = {\n\t\t.len\t\t= len,\n\t\t.total_len\t= len,\n\t\t.flags\t\t= flags,\n\t\t.pos\t\t= *ppos,\n\t\t.u.file\t\t= out,\n\t\t.opos\t\t= opos,\n\t};\n\tlong ret;\n\n\tif (unlikely(!(out->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\n\tif (unlikely(out->f_flags & O_APPEND))\n\t\treturn -EINVAL;\n\n\tret = rw_verify_area(WRITE, out, opos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tret = splice_direct_to_actor(in, &sd, direct_splice_actor);\n\tif (ret > 0)\n\t\t*ppos = sd.pos;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(do_splice_direct);\n\nstatic int wait_for_space(struct pipe_inode_info *pipe, unsigned flags)\n{\n\tfor (;;) {\n\t\tif (unlikely(!pipe->readers)) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\treturn -EPIPE;\n\t\t}\n\t\tif (pipe->nrbufs != pipe->buffers)\n\t\t\treturn 0;\n\t\tif (flags & SPLICE_F_NONBLOCK)\n\t\t\treturn -EAGAIN;\n\t\tif (signal_pending(current))\n\t\t\treturn -ERESTARTSYS;\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\n}\n\nstatic int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags);\n\n/*\n * Determine where to splice to/from.\n */\nstatic long do_splice(struct file *in, loff_t __user *off_in,\n\t\t      struct file *out, loff_t __user *off_out,\n\t\t      size_t len, unsigned int flags)\n{\n\tstruct pipe_inode_info *ipipe;\n\tstruct pipe_inode_info *opipe;\n\tloff_t offset;\n\tlong ret;\n\n\tipipe = get_pipe_info(in);\n\topipe = get_pipe_info(out);\n\n\tif (ipipe && opipe) {\n\t\tif (off_in || off_out)\n\t\t\treturn -ESPIPE;\n\n\t\tif (!(in->f_mode & FMODE_READ))\n\t\t\treturn -EBADF;\n\n\t\tif (!(out->f_mode & FMODE_WRITE))\n\t\t\treturn -EBADF;\n\n\t\t/* Splicing to self would be fun, but... */\n\t\tif (ipipe == opipe)\n\t\t\treturn -EINVAL;\n\n\t\treturn splice_pipe_to_pipe(ipipe, opipe, len, flags);\n\t}\n\n\tif (ipipe) {\n\t\tif (off_in)\n\t\t\treturn -ESPIPE;\n\t\tif (off_out) {\n\t\t\tif (!(out->f_mode & FMODE_PWRITE))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (copy_from_user(&offset, off_out, sizeof(loff_t)))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\toffset = out->f_pos;\n\t\t}\n\n\t\tif (unlikely(!(out->f_mode & FMODE_WRITE)))\n\t\t\treturn -EBADF;\n\n\t\tif (unlikely(out->f_flags & O_APPEND))\n\t\t\treturn -EINVAL;\n\n\t\tret = rw_verify_area(WRITE, out, &offset, len);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\n\t\tfile_start_write(out);\n\t\tret = do_splice_from(ipipe, out, &offset, len, flags);\n\t\tfile_end_write(out);\n\n\t\tif (!off_out)\n\t\t\tout->f_pos = offset;\n\t\telse if (copy_to_user(off_out, &offset, sizeof(loff_t)))\n\t\t\tret = -EFAULT;\n\n\t\treturn ret;\n\t}\n\n\tif (opipe) {\n\t\tif (off_out)\n\t\t\treturn -ESPIPE;\n\t\tif (off_in) {\n\t\t\tif (!(in->f_mode & FMODE_PREAD))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (copy_from_user(&offset, off_in, sizeof(loff_t)))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\toffset = in->f_pos;\n\t\t}\n\n\t\tpipe_lock(opipe);\n\t\tret = wait_for_space(opipe, flags);\n\t\tif (!ret)\n\t\t\tret = do_splice_to(in, &offset, opipe, len, flags);\n\t\tpipe_unlock(opipe);\n\t\tif (ret > 0)\n\t\t\twakeup_pipe_readers(opipe);\n\t\tif (!off_in)\n\t\t\tin->f_pos = offset;\n\t\telse if (copy_to_user(off_in, &offset, sizeof(loff_t)))\n\t\t\tret = -EFAULT;\n\n\t\treturn ret;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int iter_to_pipe(struct iov_iter *from,\n\t\t\tstruct pipe_inode_info *pipe,\n\t\t\tunsigned flags)\n{\n\tstruct pipe_buffer buf = {\n\t\t.ops = &user_page_pipe_buf_ops,\n\t\t.flags = flags\n\t};\n\tsize_t total = 0;\n\tint ret = 0;\n\tbool failed = false;\n\n\twhile (iov_iter_count(from) && !failed) {\n\t\tstruct page *pages[16];\n\t\tssize_t copied;\n\t\tsize_t start;\n\t\tint n;\n\n\t\tcopied = iov_iter_get_pages(from, pages, ~0UL, 16, &start);\n\t\tif (copied <= 0) {\n\t\t\tret = copied;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (n = 0; copied; n++, start = 0) {\n\t\t\tint size = min_t(int, copied, PAGE_SIZE - start);\n\t\t\tif (!failed) {\n\t\t\t\tbuf.page = pages[n];\n\t\t\t\tbuf.offset = start;\n\t\t\t\tbuf.len = size;\n\t\t\t\tret = add_to_pipe(pipe, &buf);\n\t\t\t\tif (unlikely(ret < 0)) {\n\t\t\t\t\tfailed = true;\n\t\t\t\t} else {\n\t\t\t\t\tiov_iter_advance(from, ret);\n\t\t\t\t\ttotal += ret;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tput_page(pages[n]);\n\t\t\t}\n\t\t\tcopied -= size;\n\t\t}\n\t}\n\treturn total ? total : ret;\n}\n\nstatic int pipe_to_user(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\tint n = copy_page_to_iter(buf->page, buf->offset, sd->len, sd->u.data);\n\treturn n == sd->len ? n : -EFAULT;\n}\n\n/*\n * For lack of a better implementation, implement vmsplice() to userspace\n * as a simple copy of the pipes pages to the user iov.\n */\nstatic long vmsplice_to_user(struct file *file, struct iov_iter *iter,\n\t\t\t     unsigned int flags)\n{\n\tstruct pipe_inode_info *pipe = get_pipe_info(file);\n\tstruct splice_desc sd = {\n\t\t.total_len = iov_iter_count(iter),\n\t\t.flags = flags,\n\t\t.u.data = iter\n\t};\n\tlong ret = 0;\n\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\tif (sd.total_len) {\n\t\tpipe_lock(pipe);\n\t\tret = __splice_from_pipe(pipe, &sd, pipe_to_user);\n\t\tpipe_unlock(pipe);\n\t}\n\n\treturn ret;\n}\n\n/*\n * vmsplice splices a user address range into a pipe. It can be thought of\n * as splice-from-memory, where the regular splice is splice-from-file (or\n * to file). In both cases the output is a pipe, naturally.\n */\nstatic long vmsplice_to_pipe(struct file *file, struct iov_iter *iter,\n\t\t\t     unsigned int flags)\n{\n\tstruct pipe_inode_info *pipe;\n\tlong ret = 0;\n\tunsigned buf_flag = 0;\n\n\tif (flags & SPLICE_F_GIFT)\n\t\tbuf_flag = PIPE_BUF_FLAG_GIFT;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\tpipe_lock(pipe);\n\tret = wait_for_space(pipe, flags);\n\tif (!ret)\n\t\tret = iter_to_pipe(iter, pipe, buf_flag);\n\tpipe_unlock(pipe);\n\tif (ret > 0)\n\t\twakeup_pipe_readers(pipe);\n\treturn ret;\n}\n\nstatic int vmsplice_type(struct fd f, int *type)\n{\n\tif (!f.file)\n\t\treturn -EBADF;\n\tif (f.file->f_mode & FMODE_WRITE) {\n\t\t*type = WRITE;\n\t} else if (f.file->f_mode & FMODE_READ) {\n\t\t*type = READ;\n\t} else {\n\t\tfdput(f);\n\t\treturn -EBADF;\n\t}\n\treturn 0;\n}\n\n/*\n * Note that vmsplice only really supports true splicing _from_ user memory\n * to a pipe, not the other way around. Splicing from user memory is a simple\n * operation that can be supported without any funky alignment restrictions\n * or nasty vm tricks. We simply map in the user memory and fill them into\n * a pipe. The reverse isn't quite as easy, though. There are two possible\n * solutions for that:\n *\n *\t- memcpy() the data internally, at which point we might as well just\n *\t  do a regular read() on the buffer anyway.\n *\t- Lots of nasty vm tricks, that are neither fast nor flexible (it\n *\t  has restriction limitations on both ends of the pipe).\n *\n * Currently we punt and implement it as a normal copy, see pipe_to_user().\n *\n */\nstatic long do_vmsplice(struct file *f, struct iov_iter *iter, unsigned int flags)\n{\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (!iov_iter_count(iter))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == WRITE)\n\t\treturn vmsplice_to_pipe(f, iter, flags);\n\telse\n\t\treturn vmsplice_to_user(f, iter, flags);\n}\n\nSYSCALL_DEFINE4(vmsplice, int, fd, const struct iovec __user *, uiov,\n\t\tunsigned long, nr_segs, unsigned int, flags)\n{\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tstruct iov_iter iter;\n\tlong error;\n\tstruct fd f;\n\tint type;\n\n\tf = fdget(fd);\n\terror = vmsplice_type(f, &type);\n\tif (error)\n\t\treturn error;\n\n\terror = import_iovec(type, uiov, nr_segs,\n\t\t\t     ARRAY_SIZE(iovstack), &iov, &iter);\n\tif (!error) {\n\t\terror = do_vmsplice(f.file, &iter, flags);\n\t\tkfree(iov);\n\t}\n\tfdput(f);\n\treturn error;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(vmsplice, int, fd, const struct compat_iovec __user *, iov32,\n\t\t    unsigned int, nr_segs, unsigned int, flags)\n{\n\tstruct iovec iovstack[UIO_FASTIOV];\n\tstruct iovec *iov = iovstack;\n\tstruct iov_iter iter;\n\tlong error;\n\tstruct fd f;\n\tint type;\n\n\tf = fdget(fd);\n\terror = vmsplice_type(f, &type);\n\tif (error)\n\t\treturn error;\n\n\terror = compat_import_iovec(type, iov32, nr_segs,\n\t\t\t     ARRAY_SIZE(iovstack), &iov, &iter);\n\tif (!error) {\n\t\terror = do_vmsplice(f.file, &iter, flags);\n\t\tkfree(iov);\n\t}\n\tfdput(f);\n\treturn error;\n}\n#endif\n\nSYSCALL_DEFINE6(splice, int, fd_in, loff_t __user *, off_in,\n\t\tint, fd_out, loff_t __user *, off_out,\n\t\tsize_t, len, unsigned int, flags)\n{\n\tstruct fd in, out;\n\tlong error;\n\n\tif (unlikely(!len))\n\t\treturn 0;\n\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\terror = -EBADF;\n\tin = fdget(fd_in);\n\tif (in.file) {\n\t\tif (in.file->f_mode & FMODE_READ) {\n\t\t\tout = fdget(fd_out);\n\t\t\tif (out.file) {\n\t\t\t\tif (out.file->f_mode & FMODE_WRITE)\n\t\t\t\t\terror = do_splice(in.file, off_in,\n\t\t\t\t\t\t\t  out.file, off_out,\n\t\t\t\t\t\t\t  len, flags);\n\t\t\t\tfdput(out);\n\t\t\t}\n\t\t}\n\t\tfdput(in);\n\t}\n\treturn error;\n}\n\n/*\n * Make sure there's data to read. Wait for input if we can, otherwise\n * return an appropriate error.\n */\nstatic int ipipe_prep(struct pipe_inode_info *pipe, unsigned int flags)\n{\n\tint ret;\n\n\t/*\n\t * Check ->nrbufs without the inode lock first. This function\n\t * is speculative anyways, so missing one is ok.\n\t */\n\tif (pipe->nrbufs)\n\t\treturn 0;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\twhile (!pipe->nrbufs) {\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (!pipe->writers)\n\t\t\tbreak;\n\t\tif (!pipe->waiting_writers) {\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tpipe_wait(pipe);\n\t}\n\n\tpipe_unlock(pipe);\n\treturn ret;\n}\n\n/*\n * Make sure there's writeable room. Wait for room if we can, otherwise\n * return an appropriate error.\n */\nstatic int opipe_prep(struct pipe_inode_info *pipe, unsigned int flags)\n{\n\tint ret;\n\n\t/*\n\t * Check ->nrbufs without the inode lock first. This function\n\t * is speculative anyways, so missing one is ok.\n\t */\n\tif (pipe->nrbufs < pipe->buffers)\n\t\treturn 0;\n\n\tret = 0;\n\tpipe_lock(pipe);\n\n\twhile (pipe->nrbufs >= pipe->buffers) {\n\t\tif (!pipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tpipe->waiting_writers++;\n\t\tpipe_wait(pipe);\n\t\tpipe->waiting_writers--;\n\t}\n\n\tpipe_unlock(pipe);\n\treturn ret;\n}\n\n/*\n * Splice contents of ipipe to opipe.\n */\nstatic int splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}\n\n/*\n * Link contents of ipipe to opipe.\n */\nstatic int link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags)\n{\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\tif (ret == 0)\n\t\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}\n\n/*\n * This is a tee(1) implementation that works on pipes. It doesn't copy\n * any data, it simply references the 'in' pages on the 'out' pipe.\n * The 'flags' used are the SPLICE_F_* variants, currently the only\n * applicable one is SPLICE_F_NONBLOCK.\n */\nstatic long do_tee(struct file *in, struct file *out, size_t len,\n\t\t   unsigned int flags)\n{\n\tstruct pipe_inode_info *ipipe = get_pipe_info(in);\n\tstruct pipe_inode_info *opipe = get_pipe_info(out);\n\tint ret = -EINVAL;\n\n\t/*\n\t * Duplicate the contents of ipipe to opipe without actually\n\t * copying the data.\n\t */\n\tif (ipipe && opipe && ipipe != opipe) {\n\t\t/*\n\t\t * Keep going, unless we encounter an error. The ipipe/opipe\n\t\t * ordering doesn't really matter.\n\t\t */\n\t\tret = ipipe_prep(ipipe, flags);\n\t\tif (!ret) {\n\t\t\tret = opipe_prep(opipe, flags);\n\t\t\tif (!ret)\n\t\t\t\tret = link_pipe(ipipe, opipe, len, flags);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(tee, int, fdin, int, fdout, size_t, len, unsigned int, flags)\n{\n\tstruct fd in;\n\tint error;\n\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!len))\n\t\treturn 0;\n\n\terror = -EBADF;\n\tin = fdget(fdin);\n\tif (in.file) {\n\t\tif (in.file->f_mode & FMODE_READ) {\n\t\t\tstruct fd out = fdget(fdout);\n\t\t\tif (out.file) {\n\t\t\t\tif (out.file->f_mode & FMODE_WRITE)\n\t\t\t\t\terror = do_tee(in.file, out.file,\n\t\t\t\t\t\t\tlen, flags);\n\t\t\t\tfdput(out);\n\t\t\t}\n\t\t}\n \t\tfdput(in);\n \t}\n\n\treturn error;\n}\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef _LINUX_PIPE_FS_I_H\n#define _LINUX_PIPE_FS_I_H\n\n#define PIPE_DEF_BUFFERS\t16\n\n#define PIPE_BUF_FLAG_LRU\t0x01\t/* page is on the LRU */\n#define PIPE_BUF_FLAG_ATOMIC\t0x02\t/* was atomically mapped */\n#define PIPE_BUF_FLAG_GIFT\t0x04\t/* page is a gift */\n#define PIPE_BUF_FLAG_PACKET\t0x08\t/* read() as a packet */\n\n/**\n *\tstruct pipe_buffer - a linux kernel pipe buffer\n *\t@page: the page containing the data for the pipe buffer\n *\t@offset: offset of data inside the @page\n *\t@len: length of data inside the @page\n *\t@ops: operations associated with this buffer. See @pipe_buf_operations.\n *\t@flags: pipe buffer flags. See above.\n *\t@private: private data owned by the ops.\n **/\nstruct pipe_buffer {\n\tstruct page *page;\n\tunsigned int offset, len;\n\tconst struct pipe_buf_operations *ops;\n\tunsigned int flags;\n\tunsigned long private;\n};\n\n/**\n *\tstruct pipe_inode_info - a linux kernel pipe\n *\t@mutex: mutex protecting the whole thing\n *\t@wait: reader/writer wait point in case of empty/full pipe\n *\t@nrbufs: the number of non-empty pipe buffers in this pipe\n *\t@buffers: total number of buffers (should be a power of 2)\n *\t@curbuf: the current pipe buffer entry\n *\t@tmp_page: cached released page\n *\t@readers: number of current readers of this pipe\n *\t@writers: number of current writers of this pipe\n *\t@files: number of struct file referring this pipe (protected by ->i_lock)\n *\t@waiting_writers: number of writers blocked waiting for room\n *\t@r_counter: reader counter\n *\t@w_counter: writer counter\n *\t@fasync_readers: reader side fasync\n *\t@fasync_writers: writer side fasync\n *\t@bufs: the circular array of pipe buffers\n *\t@user: the user who created this pipe\n **/\nstruct pipe_inode_info {\n\tstruct mutex mutex;\n\twait_queue_head_t wait;\n\tunsigned int nrbufs, curbuf, buffers;\n\tunsigned int readers;\n\tunsigned int writers;\n\tunsigned int files;\n\tunsigned int waiting_writers;\n\tunsigned int r_counter;\n\tunsigned int w_counter;\n\tstruct page *tmp_page;\n\tstruct fasync_struct *fasync_readers;\n\tstruct fasync_struct *fasync_writers;\n\tstruct pipe_buffer *bufs;\n\tstruct user_struct *user;\n};\n\n/*\n * Note on the nesting of these functions:\n *\n * ->confirm()\n *\t->steal()\n *\n * That is, ->steal() must be called on a confirmed buffer.\n * See below for the meaning of each operation. Also see kerneldoc\n * in fs/pipe.c for the pipe and generic variants of these hooks.\n */\nstruct pipe_buf_operations {\n\t/*\n\t * This is set to 1, if the generic pipe read/write may coalesce\n\t * data into an existing buffer. If this is set to 0, a new pipe\n\t * page segment is always used for new data.\n\t */\n\tint can_merge;\n\n\t/*\n\t * ->confirm() verifies that the data in the pipe buffer is there\n\t * and that the contents are good. If the pages in the pipe belong\n\t * to a file system, we may need to wait for IO completion in this\n\t * hook. Returns 0 for good, or a negative error value in case of\n\t * error.\n\t */\n\tint (*confirm)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * When the contents of this pipe buffer has been completely\n\t * consumed by a reader, ->release() is called.\n\t */\n\tvoid (*release)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * Attempt to take ownership of the pipe buffer and its contents.\n\t * ->steal() returns 0 for success, in which case the contents\n\t * of the pipe (the buf->page) is locked and now completely owned\n\t * by the caller. The page may then be transferred to a different\n\t * mapping, the most often used case is insertion into different\n\t * file address space cache.\n\t */\n\tint (*steal)(struct pipe_inode_info *, struct pipe_buffer *);\n\n\t/*\n\t * Get a reference to the pipe buffer.\n\t */\n\tbool (*get)(struct pipe_inode_info *, struct pipe_buffer *);\n};\n\n/**\n * pipe_buf_get - get a reference to a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to get a reference to\n *\n * Return: %true if the reference was successfully obtained.\n */\nstatic inline __must_check bool pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\treturn buf->ops->get(pipe, buf);\n}\n\n/**\n * pipe_buf_release - put a reference to a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to put a reference to\n */\nstatic inline void pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tconst struct pipe_buf_operations *ops = buf->ops;\n\n\tbuf->ops = NULL;\n\tops->release(pipe, buf);\n}\n\n/**\n * pipe_buf_confirm - verify contents of the pipe buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to confirm\n */\nstatic inline int pipe_buf_confirm(struct pipe_inode_info *pipe,\n\t\t\t\t   struct pipe_buffer *buf)\n{\n\treturn buf->ops->confirm(pipe, buf);\n}\n\n/**\n * pipe_buf_steal - attempt to take ownership of a pipe_buffer\n * @pipe:\tthe pipe that the buffer belongs to\n * @buf:\tthe buffer to attempt to steal\n */\nstatic inline int pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t struct pipe_buffer *buf)\n{\n\treturn buf->ops->steal(pipe, buf);\n}\n\n/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual\n   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */\n#define PIPE_SIZE\t\tPAGE_SIZE\n\n/* Pipe lock and unlock operations */\nvoid pipe_lock(struct pipe_inode_info *);\nvoid pipe_unlock(struct pipe_inode_info *);\nvoid pipe_double_lock(struct pipe_inode_info *, struct pipe_inode_info *);\n\nextern unsigned int pipe_max_size;\nextern unsigned long pipe_user_pages_hard;\nextern unsigned long pipe_user_pages_soft;\n\n/* Drop the inode semaphore and wait for a pipe event, atomically */\nvoid pipe_wait(struct pipe_inode_info *pipe);\n\nstruct pipe_inode_info *alloc_pipe_info(void);\nvoid free_pipe_info(struct pipe_inode_info *);\n\n/* Generic pipe buffer ops functions */\nbool generic_pipe_buf_get(struct pipe_inode_info *, struct pipe_buffer *);\nint generic_pipe_buf_confirm(struct pipe_inode_info *, struct pipe_buffer *);\nint generic_pipe_buf_steal(struct pipe_inode_info *, struct pipe_buffer *);\nvoid generic_pipe_buf_release(struct pipe_inode_info *, struct pipe_buffer *);\n\nextern const struct pipe_buf_operations nosteal_pipe_buf_ops;\n\n/* for F_SETPIPE_SZ and F_GETPIPE_SZ */\nlong pipe_fcntl(struct file *, unsigned int, unsigned long arg);\nstruct pipe_inode_info *get_pipe_info(struct file *file);\n\nint create_pipe_files(struct file **, int);\nunsigned int round_pipe_size(unsigned long size);\n\n#endif\n", "// SPDX-License-Identifier: GPL-2.0\n/*\n * ring buffer based function tracer\n *\n * Copyright (C) 2007-2012 Steven Rostedt <srostedt@redhat.com>\n * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>\n *\n * Originally taken from the RT patch by:\n *    Arnaldo Carvalho de Melo <acme@redhat.com>\n *\n * Based on code from the latency_tracer, that is:\n *  Copyright (C) 2004-2006 Ingo Molnar\n *  Copyright (C) 2004 Nadia Yvette Chambers\n */\n#include <linux/ring_buffer.h>\n#include <generated/utsrelease.h>\n#include <linux/stacktrace.h>\n#include <linux/writeback.h>\n#include <linux/kallsyms.h>\n#include <linux/seq_file.h>\n#include <linux/notifier.h>\n#include <linux/irqflags.h>\n#include <linux/debugfs.h>\n#include <linux/tracefs.h>\n#include <linux/pagemap.h>\n#include <linux/hardirq.h>\n#include <linux/linkage.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/splice.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/mount.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/fs.h>\n#include <linux/trace.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/rt.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n/*\n * On boot up, the ring buffer is set to the minimum size, so that\n * we do not waste memory on systems that are not using tracing.\n */\nbool ring_buffer_expanded;\n\n/*\n * We need to change this state when a selftest is running.\n * A selftest will lurk into the ring-buffer to count the\n * entries inserted during the selftest although some concurrent\n * insertions into the ring-buffer such as trace_printk could occurred\n * at the same time, giving false positive or negative results.\n */\nstatic bool __read_mostly tracing_selftest_running;\n\n/*\n * If a tracer is running, we do not want to run SELFTEST.\n */\nbool __read_mostly tracing_selftest_disabled;\n\n/* Pipe tracepoints to printk */\nstruct trace_iterator *tracepoint_print_iter;\nint tracepoint_printk;\nstatic DEFINE_STATIC_KEY_FALSE(tracepoint_printk_key);\n\n/* For tracers that don't implement custom flags */\nstatic struct tracer_opt dummy_tracer_opt[] = {\n\t{ }\n};\n\nstatic int\ndummy_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\treturn 0;\n}\n\n/*\n * To prevent the comm cache from being overwritten when no\n * tracing is active, only save the comm when a trace event\n * occurred.\n */\nstatic DEFINE_PER_CPU(bool, trace_taskinfo_save);\n\n/*\n * Kill all tracing for good (never come back).\n * It is initialized to 1 but will turn to zero if the initialization\n * of the tracer is successful. But that is the only place that sets\n * this back to zero.\n */\nstatic int tracing_disabled = 1;\n\ncpumask_var_t __read_mostly\ttracing_buffer_mask;\n\n/*\n * ftrace_dump_on_oops - variable to dump ftrace buffer on oops\n *\n * If there is an oops (or kernel panic) and the ftrace_dump_on_oops\n * is set, then ftrace_dump is called. This will output the contents\n * of the ftrace buffers to the console.  This is very useful for\n * capturing traces that lead to crashes and outputing it to a\n * serial console.\n *\n * It is default off, but you can enable it with either specifying\n * \"ftrace_dump_on_oops\" in the kernel command line, or setting\n * /proc/sys/kernel/ftrace_dump_on_oops\n * Set 1 if you want to dump buffers of all CPUs\n * Set 2 if you want to dump the buffer of the CPU that triggered oops\n */\n\nenum ftrace_dump_mode ftrace_dump_on_oops;\n\n/* When set, tracing will stop when a WARN*() is hit */\nint __disable_trace_on_warning;\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\n/* Map of enums to their values, for \"eval_map\" file */\nstruct trace_eval_map_head {\n\tstruct module\t\t\t*mod;\n\tunsigned long\t\t\tlength;\n};\n\nunion trace_eval_map_item;\n\nstruct trace_eval_map_tail {\n\t/*\n\t * \"end\" is first and points to NULL as it must be different\n\t * than \"mod\" or \"eval_string\"\n\t */\n\tunion trace_eval_map_item\t*next;\n\tconst char\t\t\t*end;\t/* points to NULL */\n};\n\nstatic DEFINE_MUTEX(trace_eval_mutex);\n\n/*\n * The trace_eval_maps are saved in an array with two extra elements,\n * one at the beginning, and one at the end. The beginning item contains\n * the count of the saved maps (head.length), and the module they\n * belong to if not built in (head.mod). The ending item contains a\n * pointer to the next array of saved eval_map items.\n */\nunion trace_eval_map_item {\n\tstruct trace_eval_map\t\tmap;\n\tstruct trace_eval_map_head\thead;\n\tstruct trace_eval_map_tail\ttail;\n};\n\nstatic union trace_eval_map_item *trace_eval_maps;\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf);\n\n#define MAX_TRACER_SIZE\t\t100\nstatic char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *default_bootup_tracer;\n\nstatic bool allocate_snapshot;\n\nstatic int __init set_cmdline_ftrace(char *str)\n{\n\tstrlcpy(bootup_tracer_buf, str, MAX_TRACER_SIZE);\n\tdefault_bootup_tracer = bootup_tracer_buf;\n\t/* We are using ftrace early, expand it */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"ftrace=\", set_cmdline_ftrace);\n\nstatic int __init set_ftrace_dump_on_oops(char *str)\n{\n\tif (*str++ != '=' || !*str) {\n\t\tftrace_dump_on_oops = DUMP_ALL;\n\t\treturn 1;\n\t}\n\n\tif (!strcmp(\"orig_cpu\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ORIG;\n                return 1;\n        }\n\n        return 0;\n}\n__setup(\"ftrace_dump_on_oops\", set_ftrace_dump_on_oops);\n\nstatic int __init stop_trace_on_warning(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\t__disable_trace_on_warning = 1;\n\treturn 1;\n}\n__setup(\"traceoff_on_warning\", stop_trace_on_warning);\n\nstatic int __init boot_alloc_snapshot(char *str)\n{\n\tallocate_snapshot = true;\n\t/* We also need the main ring buffer expanded */\n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"alloc_snapshot\", boot_alloc_snapshot);\n\n\nstatic char trace_boot_options_buf[MAX_TRACER_SIZE] __initdata;\n\nstatic int __init set_trace_boot_options(char *str)\n{\n\tstrlcpy(trace_boot_options_buf, str, MAX_TRACER_SIZE);\n\treturn 0;\n}\n__setup(\"trace_options=\", set_trace_boot_options);\n\nstatic char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *trace_boot_clock __initdata;\n\nstatic int __init set_trace_boot_clock(char *str)\n{\n\tstrlcpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);\n\ttrace_boot_clock = trace_boot_clock_buf;\n\treturn 0;\n}\n__setup(\"trace_clock=\", set_trace_boot_clock);\n\nstatic int __init set_tracepoint_printk(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\ttracepoint_printk = 1;\n\treturn 1;\n}\n__setup(\"tp_printk\", set_tracepoint_printk);\n\nunsigned long long ns2usecs(u64 nsec)\n{\n\tnsec += 500;\n\tdo_div(nsec, 1000);\n\treturn nsec;\n}\n\n/* trace_flags holds trace_options default values */\n#define TRACE_DEFAULT_FLAGS\t\t\t\t\t\t\\\n\t(FUNCTION_DEFAULT_FLAGS |\t\t\t\t\t\\\n\t TRACE_ITER_PRINT_PARENT | TRACE_ITER_PRINTK |\t\t\t\\\n\t TRACE_ITER_ANNOTATE | TRACE_ITER_CONTEXT_INFO |\t\t\\\n\t TRACE_ITER_RECORD_CMD | TRACE_ITER_OVERWRITE |\t\t\t\\\n\t TRACE_ITER_IRQ_INFO | TRACE_ITER_MARKERS)\n\n/* trace_options that are only supported by global_trace */\n#define TOP_LEVEL_TRACE_FLAGS (TRACE_ITER_PRINTK |\t\t\t\\\n\t       TRACE_ITER_PRINTK_MSGONLY | TRACE_ITER_RECORD_CMD)\n\n/* trace_flags that are default zero for instances */\n#define ZEROED_TRACE_FLAGS \\\n\t(TRACE_ITER_EVENT_FORK | TRACE_ITER_FUNC_FORK)\n\n/*\n * The global_trace is the descriptor that holds the top-level tracing\n * buffers for the live tracing.\n */\nstatic struct trace_array global_trace = {\n\t.trace_flags = TRACE_DEFAULT_FLAGS,\n};\n\nLIST_HEAD(ftrace_trace_arrays);\n\nint trace_array_get(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\ttr->ref++;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic void __trace_array_put(struct trace_array *this_tr)\n{\n\tWARN_ON(!this_tr->ref);\n\tthis_tr->ref--;\n}\n\nvoid trace_array_put(struct trace_array *this_tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__trace_array_put(this_tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nint call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t      struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tif (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&\n\t    !filter_match_preds(call->filter, rec)) {\n\t\t__trace_event_discard_commit(buffer, event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nvoid trace_free_pid_list(struct trace_pid_list *pid_list)\n{\n\tvfree(pid_list->pids);\n\tkfree(pid_list);\n}\n\n/**\n * trace_find_filtered_pid - check if a pid exists in a filtered_pid list\n * @filtered_pids: The list of pids to check\n * @search_pid: The PID to find in @filtered_pids\n *\n * Returns true if @search_pid is fonud in @filtered_pids, and false otherwis.\n */\nbool\ntrace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)\n{\n\t/*\n\t * If pid_max changed after filtered_pids was created, we\n\t * by default ignore all pids greater than the previous pid_max.\n\t */\n\tif (search_pid >= filtered_pids->pid_max)\n\t\treturn false;\n\n\treturn test_bit(search_pid, filtered_pids->pids);\n}\n\n/**\n * trace_ignore_this_task - should a task be ignored for tracing\n * @filtered_pids: The list of pids to check\n * @task: The task that should be ignored if not filtered\n *\n * Checks if @task should be traced or not from @filtered_pids.\n * Returns true if @task should *NOT* be traced.\n * Returns false if @task should be traced.\n */\nbool\ntrace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)\n{\n\t/*\n\t * Return false, because if filtered_pids does not exist,\n\t * all pids are good to trace.\n\t */\n\tif (!filtered_pids)\n\t\treturn false;\n\n\treturn !trace_find_filtered_pid(filtered_pids, task->pid);\n}\n\n/**\n * trace_pid_filter_add_remove_task - Add or remove a task from a pid_list\n * @pid_list: The list to modify\n * @self: The current task for fork or NULL for exit\n * @task: The task to add or remove\n *\n * If adding a task, if @self is defined, the task is only added if @self\n * is also included in @pid_list. This happens on fork and tasks should\n * only be added when the parent is listed. If @self is NULL, then the\n * @task pid will be removed from the list, which would happen on exit\n * of a task.\n */\nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task)\n{\n\tif (!pid_list)\n\t\treturn;\n\n\t/* For forks, we only add if the forking task is listed */\n\tif (self) {\n\t\tif (!trace_find_filtered_pid(pid_list, self->pid))\n\t\t\treturn;\n\t}\n\n\t/* Sorry, but we don't support pid_max changing after setting */\n\tif (task->pid >= pid_list->pid_max)\n\t\treturn;\n\n\t/* \"self\" is set for forks, and NULL for exits */\n\tif (self)\n\t\tset_bit(task->pid, pid_list->pids);\n\telse\n\t\tclear_bit(task->pid, pid_list->pids);\n}\n\n/**\n * trace_pid_next - Used for seq_file to get to the next pid of a pid_list\n * @pid_list: The pid list to show\n * @v: The last pid that was shown (+1 the actual pid to let zero be displayed)\n * @pos: The position of the file\n *\n * This is used by the seq_file \"next\" operation to iterate the pids\n * listed in a trace_pid_list structure.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)\n{\n\tunsigned long pid = (unsigned long)v;\n\n\t(*pos)++;\n\n\t/* pid already is +1 of the actual prevous bit */\n\tpid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);\n\n\t/* Return pid + 1 to allow zero to be represented */\n\tif (pid < pid_list->pid_max)\n\t\treturn (void *)(pid + 1);\n\n\treturn NULL;\n}\n\n/**\n * trace_pid_start - Used for seq_file to start reading pid lists\n * @pid_list: The pid list to show\n * @pos: The position of the file\n *\n * This is used by seq_file \"start\" operation to start the iteration\n * of listing pids.\n *\n * Returns the pid+1 as we want to display pid of zero, but NULL would\n * stop the iteration.\n */\nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)\n{\n\tunsigned long pid;\n\tloff_t l = 0;\n\n\tpid = find_first_bit(pid_list->pids, pid_list->pid_max);\n\tif (pid >= pid_list->pid_max)\n\t\treturn NULL;\n\n\t/* Return pid + 1 so that zero can be the exit value */\n\tfor (pid++; pid && l < *pos;\n\t     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))\n\t\t;\n\treturn (void *)pid;\n}\n\n/**\n * trace_pid_show - show the current pid in seq_file processing\n * @m: The seq_file structure to write into\n * @v: A void pointer of the pid (+1) value to display\n *\n * Can be directly used by seq_file operations to display the current\n * pid value.\n */\nint trace_pid_show(struct seq_file *m, void *v)\n{\n\tunsigned long pid = (unsigned long)v - 1;\n\n\tseq_printf(m, \"%lu\\n\", pid);\n\treturn 0;\n}\n\n/* 128 should be much more than enough */\n#define PID_BUF_SIZE\t\t127\n\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_parser parser;\n\tunsigned long val;\n\tint nr_pids = 0;\n\tssize_t read = 0;\n\tssize_t ret = 0;\n\tloff_t pos;\n\tpid_t pid;\n\n\tif (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Always recreate a new array. The write is an all or nothing\n\t * operation. Always create a new array when adding new pids by\n\t * the user. If the operation fails, then the current list is\n\t * not modified.\n\t */\n\tpid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);\n\tif (!pid_list)\n\t\treturn -ENOMEM;\n\n\tpid_list->pid_max = READ_ONCE(pid_max);\n\n\t/* Only truncating will shrink pid_max */\n\tif (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)\n\t\tpid_list->pid_max = filtered_pids->pid_max;\n\n\tpid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);\n\tif (!pid_list->pids) {\n\t\tkfree(pid_list);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (filtered_pids) {\n\t\t/* copy the current bits to the new max */\n\t\tfor_each_set_bit(pid, filtered_pids->pids,\n\t\t\t\t filtered_pids->pid_max) {\n\t\t\tset_bit(pid, pid_list->pids);\n\t\t\tnr_pids++;\n\t\t}\n\t}\n\n\twhile (cnt > 0) {\n\n\t\tpos = 0;\n\n\t\tret = trace_get_user(&parser, ubuf, cnt, &pos);\n\t\tif (ret < 0 || !trace_parser_loaded(&parser))\n\t\t\tbreak;\n\n\t\tread += ret;\n\t\tubuf += ret;\n\t\tcnt -= ret;\n\n\t\tret = -EINVAL;\n\t\tif (kstrtoul(parser.buffer, 0, &val))\n\t\t\tbreak;\n\t\tif (val >= pid_list->pid_max)\n\t\t\tbreak;\n\n\t\tpid = (pid_t)val;\n\n\t\tset_bit(pid, pid_list->pids);\n\t\tnr_pids++;\n\n\t\ttrace_parser_clear(&parser);\n\t\tret = 0;\n\t}\n\ttrace_parser_put(&parser);\n\n\tif (ret < 0) {\n\t\ttrace_free_pid_list(pid_list);\n\t\treturn ret;\n\t}\n\n\tif (!nr_pids) {\n\t\t/* Cleared the list of pids */\n\t\ttrace_free_pid_list(pid_list);\n\t\tread = ret;\n\t\tpid_list = NULL;\n\t}\n\n\t*new_pid_list = pid_list;\n\n\treturn read;\n}\n\nstatic u64 buffer_ftrace_now(struct trace_buffer *buf, int cpu)\n{\n\tu64 ts;\n\n\t/* Early boot up does not have a buffer yet */\n\tif (!buf->buffer)\n\t\treturn trace_clock_local();\n\n\tts = ring_buffer_time_stamp(buf->buffer, cpu);\n\tring_buffer_normalize_time_stamp(buf->buffer, cpu, &ts);\n\n\treturn ts;\n}\n\nu64 ftrace_now(int cpu)\n{\n\treturn buffer_ftrace_now(&global_trace.trace_buffer, cpu);\n}\n\n/**\n * tracing_is_enabled - Show if global_trace has been disabled\n *\n * Shows if the global trace has been enabled or not. It uses the\n * mirror flag \"buffer_disabled\" to be used in fast paths such as for\n * the irqsoff tracer. But it may be inaccurate due to races. If you\n * need to know the accurate state, use tracing_is_on() which is a little\n * slower, but accurate.\n */\nint tracing_is_enabled(void)\n{\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}\n\n/*\n * trace_buf_size is the size in bytes that is allocated\n * for a buffer. Note, the number of bytes is always rounded\n * to page size.\n *\n * This number is purposely set to a low number of 16384.\n * If the dump on oops happens, it will be much appreciated\n * to not have to wait for all that output. Anyway this can be\n * boot time and run time configurable.\n */\n#define TRACE_BUF_SIZE_DEFAULT\t1441792UL /* 16384 * 88 (sizeof(entry)) */\n\nstatic unsigned long\t\ttrace_buf_size = TRACE_BUF_SIZE_DEFAULT;\n\n/* trace_types holds a link list of available tracers. */\nstatic struct tracer\t\t*trace_types __read_mostly;\n\n/*\n * trace_types_lock is used to protect the trace_types list.\n */\nDEFINE_MUTEX(trace_types_lock);\n\n/*\n * serialize the access of the ring buffer\n *\n * ring buffer serializes readers, but it is low level protection.\n * The validity of the events (which returns by ring_buffer_peek() ..etc)\n * are not protected by ring buffer.\n *\n * The content of events may become garbage if we allow other process consumes\n * these events concurrently:\n *   A) the page of the consumed events may become a normal page\n *      (not reader page) in ring buffer, and this page will be rewrited\n *      by events producer.\n *   B) The page of the consumed events may become a page for splice_read,\n *      and this page will be returned to system.\n *\n * These primitives allow multi process access to different cpu ring buffer\n * concurrently.\n *\n * These primitives don't distinguish read-only and read-consume access.\n * Multi read-only access are also serialized.\n */\n\n#ifdef CONFIG_SMP\nstatic DECLARE_RWSEM(all_cpu_access_lock);\nstatic DEFINE_PER_CPU(struct mutex, cpu_access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\t/* gain it for accessing the whole ring buffer. */\n\t\tdown_write(&all_cpu_access_lock);\n\t} else {\n\t\t/* gain it for accessing a cpu ring buffer. */\n\n\t\t/* Firstly block other trace_access_lock(RING_BUFFER_ALL_CPUS). */\n\t\tdown_read(&all_cpu_access_lock);\n\n\t\t/* Secondly block other access to this @cpu ring buffer. */\n\t\tmutex_lock(&per_cpu(cpu_access_lock, cpu));\n\t}\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tup_write(&all_cpu_access_lock);\n\t} else {\n\t\tmutex_unlock(&per_cpu(cpu_access_lock, cpu));\n\t\tup_read(&all_cpu_access_lock);\n\t}\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}\n\n#else\n\nstatic DEFINE_MUTEX(access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\t(void)cpu;\n\tmutex_lock(&access_lock);\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\t(void)cpu;\n\tmutex_unlock(&access_lock);\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n}\n\n#endif\n\n#ifdef CONFIG_STACKTRACE\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs);\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs);\n\n#else\nstatic inline void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs)\n{\n}\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n}\n\n#endif\n\nstatic __always_inline void\ntrace_event_setup(struct ring_buffer_event *event,\n\t\t  int type, unsigned long flags, int pc)\n{\n\tstruct trace_entry *ent = ring_buffer_event_data(event);\n\n\ttracing_generic_entry_update(ent, flags, pc);\n\tent->type = type;\n}\n\nstatic __always_inline struct ring_buffer_event *\n__trace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *event;\n\n\tevent = ring_buffer_lock_reserve(buffer, len);\n\tif (event != NULL)\n\t\ttrace_event_setup(event, type, flags, pc);\n\n\treturn event;\n}\n\nvoid tracer_tracing_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_on(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 0;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_on - enable tracing buffers\n *\n * This function enables tracing buffers that may have been\n * disabled with tracing_off.\n */\nvoid tracing_on(void)\n{\n\ttracer_tracing_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_on);\n\n\nstatic __always_inline void\n__buffer_unlock_commit(struct ring_buffer *buffer, struct ring_buffer_event *event)\n{\n\t__this_cpu_write(trace_taskinfo_save, true);\n\n\t/* If this is the temp buffer, we need to commit fully */\n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t/* Length is in event->array[0] */\n\t\tring_buffer_write(buffer, event->array[0], &event->array[1]);\n\t\t/* Release the temp buffer */\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t} else\n\t\tring_buffer_unlock_commit(buffer, event);\n}\n\n/**\n * __trace_puts - write a constant string into the trace buffer.\n * @ip:\t   The address of the caller\n * @str:   The constant string to write\n * @size:  The size of the string.\n */\nint __trace_puts(unsigned long ip, const char *str, int size)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(__trace_puts);\n\n/**\n * __trace_bputs - write the pointer to a constant string into trace buffer\n * @ip:\t   The address of the caller\n * @str:   The constant string to write to the buffer to\n */\nint __trace_bputs(unsigned long ip, const char *str)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct bputs_entry *entry;\n\tunsigned long irq_flags;\n\tint size = sizeof(struct bputs_entry);\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,\n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->str\t\t\t= str;\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(__trace_bputs);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nvoid tracing_snapshot_instance(struct trace_array *tr)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\tunsigned long flags;\n\n\tif (in_nmi()) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT CALLED FROM NMI CONTEXT ***\\n\");\n\t\tinternal_trace_puts(\"*** snapshot is being ignored        ***\\n\");\n\t\treturn;\n\t}\n\n\tif (!tr->allocated_snapshot) {\n\t\tinternal_trace_puts(\"*** SNAPSHOT NOT ALLOCATED ***\\n\");\n\t\tinternal_trace_puts(\"*** stopping trace here!   ***\\n\");\n\t\ttracing_off();\n\t\treturn;\n\t}\n\n\t/* Note, snapshot can not be used when the tracer uses it */\n\tif (tracer->use_max_tr) {\n\t\tinternal_trace_puts(\"*** LATENCY TRACER ACTIVE ***\\n\");\n\t\tinternal_trace_puts(\"*** Can not use snapshot (sorry) ***\\n\");\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tupdate_max_tr(tr, current, smp_processor_id());\n\tlocal_irq_restore(flags);\n}\n\n/**\n * tracing_snapshot - take a snapshot of the current buffer.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n *\n * Note, make sure to allocate the snapshot with either\n * a tracing_snapshot_alloc(), or by doing it manually\n * with: echo 1 > /sys/kernel/debug/tracing/snapshot\n *\n * If the snapshot buffer is not allocated, it will stop tracing.\n * Basically making a permanent snapshot.\n */\nvoid tracing_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\ttracing_snapshot_instance(tr);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\n\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id);\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val);\n\nint tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\tint ret;\n\n\tif (!tr->allocated_snapshot) {\n\n\t\t/* allocate spare buffer */\n\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t   &tr->trace_buffer, RING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\ttr->allocated_snapshot = true;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_snapshot(struct trace_array *tr)\n{\n\t/*\n\t * We don't free the ring buffer. instead, resize it because\n\t * The max_tr ring buffer has some state (e.g. ring->clock) and\n\t * we want preserve it.\n\t */\n\tring_buffer_resize(tr->max_buffer.buffer, 1, RING_BUFFER_ALL_CPUS);\n\tset_buffer_entries(&tr->max_buffer, 1);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n\ttr->allocated_snapshot = false;\n}\n\n/**\n * tracing_alloc_snapshot - allocate snapshot buffer.\n *\n * This only allocates the snapshot buffer if it isn't already\n * allocated - it doesn't also take a snapshot.\n *\n * This is meant to be used in cases where the snapshot buffer needs\n * to be set up for events that can't sleep but need to be able to\n * trigger a snapshot.\n */\nint tracing_alloc_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\tint ret;\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\n\n/**\n * tracing_snapshot_alloc - allocate and take a snapshot of the current buffer.\n *\n * This is similar to tracing_snapshot(), but it will allocate the\n * snapshot buffer if it isn't already allocated. Use this only\n * where it is safe to sleep, as the allocation may sleep.\n *\n * This causes a swap between the snapshot buffer and the current live\n * tracing buffer. You can use this to take snapshots of the live\n * trace when some condition is triggered, but continue to trace.\n */\nvoid tracing_snapshot_alloc(void)\n{\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#else\nvoid tracing_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\nint tracing_alloc_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but snapshot allocation used\");\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\nvoid tracing_snapshot_alloc(void)\n{\n\t/* Give warning */\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nvoid tracer_tracing_off(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\tring_buffer_record_off(tr->trace_buffer.buffer);\n\t/*\n\t * This flag is looked at when buffers haven't been allocated\n\t * yet, or by some tracers (like irqsoff), that just want to\n\t * know if the ring buffer has been disabled, but it can handle\n\t * races of where it gets disabled but we still do a record.\n\t * As the check is in the fast path of the tracers, it is more\n\t * important to be fast than accurate.\n\t */\n\ttr->buffer_disabled = 1;\n\t/* Make the flag seen by readers */\n\tsmp_wmb();\n}\n\n/**\n * tracing_off - turn off tracing buffers\n *\n * This function stops the tracing buffers from recording data.\n * It does not disable any overhead the tracers themselves may\n * be causing. This function simply causes all recording to\n * the ring buffers to fail.\n */\nvoid tracing_off(void)\n{\n\ttracer_tracing_off(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_off);\n\nvoid disable_trace_on_warning(void)\n{\n\tif (__disable_trace_on_warning)\n\t\ttracing_off();\n}\n\n/**\n * tracer_tracing_is_on - show real state of ring buffer enabled\n * @tr : the trace array to know if ring buffer is enabled\n *\n * Shows real state of the ring buffer if it is enabled or not.\n */\nbool tracer_tracing_is_on(struct trace_array *tr)\n{\n\tif (tr->trace_buffer.buffer)\n\t\treturn ring_buffer_record_is_on(tr->trace_buffer.buffer);\n\treturn !tr->buffer_disabled;\n}\n\n/**\n * tracing_is_on - show state of ring buffers enabled\n */\nint tracing_is_on(void)\n{\n\treturn tracer_tracing_is_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_is_on);\n\nstatic int __init set_buf_size(char *str)\n{\n\tunsigned long buf_size;\n\n\tif (!str)\n\t\treturn 0;\n\tbuf_size = memparse(str, &str);\n\t/* nr_entries can not be zero */\n\tif (buf_size == 0)\n\t\treturn 0;\n\ttrace_buf_size = buf_size;\n\treturn 1;\n}\n__setup(\"trace_buf_size=\", set_buf_size);\n\nstatic int __init set_tracing_thresh(char *str)\n{\n\tunsigned long threshold;\n\tint ret;\n\n\tif (!str)\n\t\treturn 0;\n\tret = kstrtoul(str, 0, &threshold);\n\tif (ret < 0)\n\t\treturn 0;\n\ttracing_thresh = threshold * 1000;\n\treturn 1;\n}\n__setup(\"tracing_thresh=\", set_tracing_thresh);\n\nunsigned long nsecs_to_usecs(unsigned long nsecs)\n{\n\treturn nsecs / 1000;\n}\n\n/*\n * TRACE_FLAGS is defined as a tuple matching bit masks with strings.\n * It uses C(a, b) where 'a' is the eval (enum) name and 'b' is the string that\n * matches it. By defining \"C(a, b) b\", TRACE_FLAGS becomes a list\n * of strings in the order that the evals (enum) were defined.\n */\n#undef C\n#define C(a, b) b\n\n/* These must match the bit postions in trace_iterator_flags */\nstatic const char *trace_options[] = {\n\tTRACE_FLAGS\n\tNULL\n};\n\nstatic struct {\n\tu64 (*func)(void);\n\tconst char *name;\n\tint in_ns;\t\t/* is this clock in nanoseconds? */\n} trace_clocks[] = {\n\t{ trace_clock_local,\t\t\"local\",\t1 },\n\t{ trace_clock_global,\t\t\"global\",\t1 },\n\t{ trace_clock_counter,\t\t\"counter\",\t0 },\n\t{ trace_clock_jiffies,\t\t\"uptime\",\t0 },\n\t{ trace_clock,\t\t\t\"perf\",\t\t1 },\n\t{ ktime_get_mono_fast_ns,\t\"mono\",\t\t1 },\n\t{ ktime_get_raw_fast_ns,\t\"mono_raw\",\t1 },\n\t{ ktime_get_boot_fast_ns,\t\"boot\",\t\t1 },\n\tARCH_TRACE_CLOCKS\n};\n\nbool trace_clock_in_ns(struct trace_array *tr)\n{\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * trace_parser_get_init - gets the buffer for trace parser\n */\nint trace_parser_get_init(struct trace_parser *parser, int size)\n{\n\tmemset(parser, 0, sizeof(*parser));\n\n\tparser->buffer = kmalloc(size, GFP_KERNEL);\n\tif (!parser->buffer)\n\t\treturn 1;\n\n\tparser->size = size;\n\treturn 0;\n}\n\n/*\n * trace_parser_put - frees the buffer for trace parser\n */\nvoid trace_parser_put(struct trace_parser *parser)\n{\n\tkfree(parser->buffer);\n\tparser->buffer = NULL;\n}\n\n/*\n * trace_get_user - reads the user input string separated by  space\n * (matched by isspace(ch))\n *\n * For each string found the 'struct trace_parser' is updated,\n * and the function returns.\n *\n * Returns number of bytes read.\n *\n * See kernel/trace/trace.h for 'struct trace_parser' details.\n */\nint trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos)\n{\n\tchar ch;\n\tsize_t read = 0;\n\tssize_t ret;\n\n\tif (!*ppos)\n\t\ttrace_parser_clear(parser);\n\n\tret = get_user(ch, ubuf++);\n\tif (ret)\n\t\tgoto out;\n\n\tread++;\n\tcnt--;\n\n\t/*\n\t * The parser is not finished with the last write,\n\t * continue reading the user input without skipping spaces.\n\t */\n\tif (!parser->cont) {\n\t\t/* skip white space */\n\t\twhile (cnt && isspace(ch)) {\n\t\t\tret = get_user(ch, ubuf++);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tread++;\n\t\t\tcnt--;\n\t\t}\n\n\t\tparser->idx = 0;\n\n\t\t/* only spaces were written */\n\t\tif (isspace(ch) || !ch) {\n\t\t\t*ppos += read;\n\t\t\tret = read;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* read the non-space input */\n\twhile (cnt && !isspace(ch) && ch) {\n\t\tif (parser->idx < parser->size - 1)\n\t\t\tparser->buffer[parser->idx++] = ch;\n\t\telse {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = get_user(ch, ubuf++);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tread++;\n\t\tcnt--;\n\t}\n\n\t/* We either got finished input or we have to wait for another call. */\n\tif (isspace(ch) || !ch) {\n\t\tparser->buffer[parser->idx] = 0;\n\t\tparser->cont = false;\n\t} else if (parser->idx < parser->size - 1) {\n\t\tparser->cont = true;\n\t\tparser->buffer[parser->idx++] = ch;\n\t\t/* Make sure the parsed string always terminates with '\\0'. */\n\t\tparser->buffer[parser->idx] = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ppos += read;\n\tret = read;\n\nout:\n\treturn ret;\n}\n\n/* TODO add a seq_buf_to_buffer() */\nstatic ssize_t trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt)\n{\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}\n\nunsigned long __read_mostly\ttracing_thresh;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/*\n * Copy the new maximum trace into the separate maximum-trace\n * structure. (this way the maximum trace is permanently saved,\n * for later retrieval via /sys/kernel/tracing/tracing_max_latency)\n */\nstatic void\n__update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tstruct trace_buffer *max_buf = &tr->max_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(trace_buf->data, cpu);\n\tstruct trace_array_cpu *max_data = per_cpu_ptr(max_buf->data, cpu);\n\n\tmax_buf->cpu = cpu;\n\tmax_buf->time_start = data->preempt_timestamp;\n\n\tmax_data->saved_latency = tr->max_latency;\n\tmax_data->critical_start = data->critical_start;\n\tmax_data->critical_end = data->critical_end;\n\n\tmemcpy(max_data->comm, tsk->comm, TASK_COMM_LEN);\n\tmax_data->pid = tsk->pid;\n\t/*\n\t * If tsk == current, then use current_uid(), as that does not use\n\t * RCU. The irq tracer can be called out of RCU scope.\n\t */\n\tif (tsk == current)\n\t\tmax_data->uid = current_uid();\n\telse\n\t\tmax_data->uid = task_uid(tsk);\n\n\tmax_data->nice = tsk->static_prio - 20 - MAX_RT_PRIO;\n\tmax_data->policy = tsk->policy;\n\tmax_data->rt_priority = tsk->rt_priority;\n\n\t/* record this tasks comm */\n\ttracing_record_cmdline(tsk);\n}\n\n/**\n * update_max_tr - snapshot all trace buffers from global_trace to max_tr\n * @tr: tracer\n * @tsk: the task with the latency\n * @cpu: The cpu that initiated the trace.\n *\n * Flip the buffers between the @tr and the max_tr and record information\n * about which task was the cause of this latency.\n */\nvoid\nupdate_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\t/* Inherit the recordable setting from trace_buffer */\n\tif (ring_buffer_record_is_set_on(tr->trace_buffer.buffer))\n\t\tring_buffer_record_on(tr->max_buffer.buffer);\n\telse\n\t\tring_buffer_record_off(tr->max_buffer.buffer);\n\n\tswap(tr->trace_buffer.buffer, tr->max_buffer.buffer);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n\n/**\n * update_max_tr_single - only copy one trace over, and reset the rest\n * @tr - tracer\n * @tsk - task with the latency\n * @cpu - the cpu of the buffer to copy.\n *\n * Flip the trace of a single CPU buffer between the @tr and the max_tr.\n */\nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t/* Only the nop tracer should hit this when disabling */\n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->trace_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t/*\n\t\t * We failed to swap the buffer due to a commit taking\n\t\t * place on this CPU. We fail to record, but we reset\n\t\t * the max trace buffer (no one writes directly to it)\n\t\t * and flag that it failed.\n\t\t */\n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int wait_on_pipe(struct trace_iterator *iter, int full)\n{\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn 0;\n\n\treturn ring_buffer_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\tfull);\n}\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nstatic bool selftests_can_run;\n\nstruct trace_selftests {\n\tstruct list_head\t\tlist;\n\tstruct tracer\t\t\t*type;\n};\n\nstatic LIST_HEAD(postponed_selftests);\n\nstatic int save_selftest(struct tracer *type)\n{\n\tstruct trace_selftests *selftest;\n\n\tselftest = kmalloc(sizeof(*selftest), GFP_KERNEL);\n\tif (!selftest)\n\t\treturn -ENOMEM;\n\n\tselftest->type = type;\n\tlist_add(&selftest->list, &postponed_selftests);\n\treturn 0;\n}\n\nstatic int run_tracer_selftest(struct tracer *type)\n{\n\tstruct trace_array *tr = &global_trace;\n\tstruct tracer *saved_tracer = tr->current_trace;\n\tint ret;\n\n\tif (!type->selftest || tracing_selftest_disabled)\n\t\treturn 0;\n\n\t/*\n\t * If a tracer registers early in boot up (before scheduling is\n\t * initialized and such), then do not run its selftests yet.\n\t * Instead, run it a little later in the boot process.\n\t */\n\tif (!selftests_can_run)\n\t\treturn save_selftest(type);\n\n\t/*\n\t * Run a selftest on this tracer.\n\t * Here we reset the trace buffer, and set the current\n\t * tracer to be this tracer. The tracer can then run some\n\t * internal tracing to verify that everything is in order.\n\t * If we fail, we do not register this tracer.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n\ttr->current_trace = type;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\t/* If we expanded the buffers, make sure the max is expanded too */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, trace_buf_size,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t\ttr->allocated_snapshot = true;\n\t}\n#endif\n\n\t/* the test is responsible for initializing and enabling */\n\tpr_info(\"Testing tracer %s: \", type->name);\n\tret = type->selftest(type, tr);\n\t/* the test is responsible for resetting too */\n\ttr->current_trace = saved_tracer;\n\tif (ret) {\n\t\tprintk(KERN_CONT \"FAILED!\\n\");\n\t\t/* Add the warning after printing 'FAILED' */\n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n\t/* Only reset on passing, to avoid touching corrupted buffers */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\ttr->allocated_snapshot = false;\n\n\t\t/* Shrink the max buffer again */\n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, 1,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t}\n#endif\n\n\tprintk(KERN_CONT \"PASSED\\n\");\n\treturn 0;\n}\n\nstatic __init int init_trace_selftests(void)\n{\n\tstruct trace_selftests *p, *n;\n\tstruct tracer *t, **last;\n\tint ret;\n\n\tselftests_can_run = true;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (list_empty(&postponed_selftests))\n\t\tgoto out;\n\n\tpr_info(\"Running postponed tracer tests:\\n\");\n\n\tlist_for_each_entry_safe(p, n, &postponed_selftests, list) {\n\t\tret = run_tracer_selftest(p->type);\n\t\t/* If the test fails, then warn and remove from available_tracers */\n\t\tif (ret < 0) {\n\t\t\tWARN(1, \"tracer: %s failed selftest, disabling\\n\",\n\t\t\t     p->type->name);\n\t\t\tlast = &trace_types;\n\t\t\tfor (t = trace_types; t; t = t->next) {\n\t\t\t\tif (t == p->type) {\n\t\t\t\t\t*last = t->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlast = &t->next;\n\t\t\t}\n\t\t}\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\ncore_initcall(init_trace_selftests);\n#else\nstatic inline int run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\n#endif /* CONFIG_FTRACE_STARTUP_TEST */\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t);\n\nstatic void __init apply_trace_boot_options(void);\n\n/**\n * register_tracer - register a tracer with the ftrace system.\n * @type - the plugin for the tracer\n *\n * Register a new plugin tracer.\n */\nint __init register_tracer(struct tracer *type)\n{\n\tstruct tracer *t;\n\tint ret = 0;\n\n\tif (!type->name) {\n\t\tpr_info(\"Tracer must have a name\\n\");\n\t\treturn -1;\n\t}\n\n\tif (strlen(type->name) >= MAX_TRACER_SIZE) {\n\t\tpr_info(\"Tracer has a name longer than %d\\n\", MAX_TRACER_SIZE);\n\t\treturn -1;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\ttracing_selftest_running = true;\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(type->name, t->name) == 0) {\n\t\t\t/* already found */\n\t\t\tpr_info(\"Tracer %s already registered\\n\",\n\t\t\t\ttype->name);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!type->set_flag)\n\t\ttype->set_flag = &dummy_set_flag;\n\tif (!type->flags) {\n\t\t/*allocate a dummy tracer_flags*/\n\t\ttype->flags = kmalloc(sizeof(*type->flags), GFP_KERNEL);\n\t\tif (!type->flags) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttype->flags->val = 0;\n\t\ttype->flags->opts = dummy_tracer_opt;\n\t} else\n\t\tif (!type->flags->opts)\n\t\t\ttype->flags->opts = dummy_tracer_opt;\n\n\t/* store the tracer for __set_tracer_option */\n\ttype->flags->trace = type;\n\n\tret = run_tracer_selftest(type);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttype->next = trace_types;\n\ttrace_types = type;\n\tadd_tracer_options(&global_trace, type);\n\n out:\n\ttracing_selftest_running = false;\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret || !default_bootup_tracer)\n\t\tgoto out_unlock;\n\n\tif (strncmp(default_bootup_tracer, type->name, MAX_TRACER_SIZE))\n\t\tgoto out_unlock;\n\n\tprintk(KERN_INFO \"Starting tracer '%s'\\n\", type->name);\n\t/* Do we want this tracer to start on bootup? */\n\ttracing_set_tracer(&global_trace, type->name);\n\tdefault_bootup_tracer = NULL;\n\n\tapply_trace_boot_options();\n\n\t/* disable other selftests, since this will break it. */\n\ttracing_selftest_disabled = true;\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\tprintk(KERN_INFO \"Disabling FTRACE selftests due to running tracer '%s'\\n\",\n\t       type->name);\n#endif\n\n out_unlock:\n\treturn ret;\n}\n\nvoid tracing_reset(struct trace_buffer *buf, int cpu)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\nvoid tracing_reset_online_cpus(struct trace_buffer *buf)\n{\n\tstruct ring_buffer *buffer = buf->buffer;\n\tint cpu;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_rcu();\n\n\tbuf->time_start = buffer_ftrace_now(buf, buf->cpu);\n\n\tfor_each_online_cpu(cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\n/* Must have trace_types_lock held */\nvoid tracing_reset_all_online_cpus(void)\n{\n\tstruct trace_array *tr;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->clear_trace)\n\t\t\tcontinue;\n\t\ttr->clear_trace = false;\n\t\ttracing_reset_online_cpus(&tr->trace_buffer);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\t}\n}\n\nstatic int *tgid_map;\n\n#define SAVED_CMDLINES_DEFAULT 128\n#define NO_CMDLINE_MAP UINT_MAX\nstatic arch_spinlock_t trace_cmdline_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstruct saved_cmdlines_buffer {\n\tunsigned map_pid_to_cmdline[PID_MAX_DEFAULT+1];\n\tunsigned *map_cmdline_to_pid;\n\tunsigned cmdline_num;\n\tint cmdline_idx;\n\tchar *saved_cmdlines;\n};\nstatic struct saved_cmdlines_buffer *savedcmd;\n\n/* temporary disable recording */\nstatic atomic_t trace_record_taskinfo_disabled __read_mostly;\n\nstatic inline char *get_saved_cmdlines(int idx)\n{\n\treturn &savedcmd->saved_cmdlines[idx * TASK_COMM_LEN];\n}\n\nstatic inline void set_cmdline(int idx, const char *cmdline)\n{\n\tmemcpy(get_saved_cmdlines(idx), cmdline, TASK_COMM_LEN);\n}\n\nstatic int allocate_cmdlines_buffer(unsigned int val,\n\t\t\t\t    struct saved_cmdlines_buffer *s)\n{\n\ts->map_cmdline_to_pid = kmalloc_array(val,\n\t\t\t\t\t      sizeof(*s->map_cmdline_to_pid),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!s->map_cmdline_to_pid)\n\t\treturn -ENOMEM;\n\n\ts->saved_cmdlines = kmalloc_array(TASK_COMM_LEN, val, GFP_KERNEL);\n\tif (!s->saved_cmdlines) {\n\t\tkfree(s->map_cmdline_to_pid);\n\t\treturn -ENOMEM;\n\t}\n\n\ts->cmdline_idx = 0;\n\ts->cmdline_num = val;\n\tmemset(&s->map_pid_to_cmdline, NO_CMDLINE_MAP,\n\t       sizeof(s->map_pid_to_cmdline));\n\tmemset(s->map_cmdline_to_pid, NO_CMDLINE_MAP,\n\t       val * sizeof(*s->map_cmdline_to_pid));\n\n\treturn 0;\n}\n\nstatic int trace_create_savedcmd(void)\n{\n\tint ret;\n\n\tsavedcmd = kmalloc(sizeof(*savedcmd), GFP_KERNEL);\n\tif (!savedcmd)\n\t\treturn -ENOMEM;\n\n\tret = allocate_cmdlines_buffer(SAVED_CMDLINES_DEFAULT, savedcmd);\n\tif (ret < 0) {\n\t\tkfree(savedcmd);\n\t\tsavedcmd = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint is_tracing_stopped(void)\n{\n\treturn global_trace.stop_count;\n}\n\n/**\n * tracing_start - quick start of the tracer\n *\n * If tracing is enabled but was stopped by tracing_stop,\n * this will start the tracer back up.\n */\nvoid tracing_start(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (--global_trace.stop_count) {\n\t\tif (global_trace.stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tglobal_trace.stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_start_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\t/* If global, we need to also start the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_start();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\n\tif (--tr->stop_count) {\n\t\tif (tr->stop_count < 0) {\n\t\t\t/* Someone screwed up their debugging */\n\t\t\tWARN_ON_ONCE(1);\n\t\t\ttr->stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n/**\n * tracing_stop - quick stop of the tracer\n *\n * Light weight way to stop tracing. Use in conjunction with\n * tracing_start.\n */\nvoid tracing_stop(void)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&global_trace.start_lock, flags);\n\tif (global_trace.stop_count++)\n\t\tgoto out;\n\n\t/* Prevent the buffers from switching */\n\tarch_spin_lock(&global_trace.max_lock);\n\n\tbuffer = global_trace.trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = global_trace.max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n#endif\n\n\tarch_spin_unlock(&global_trace.max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&global_trace.start_lock, flags);\n}\n\nstatic void tracing_stop_tr(struct trace_array *tr)\n{\n\tstruct ring_buffer *buffer;\n\tunsigned long flags;\n\n\t/* If global, we need to also stop the max tracer */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn tracing_stop();\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (tr->stop_count++)\n\t\tgoto out;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\nstatic int trace_save_cmdline(struct task_struct *tsk)\n{\n\tunsigned pid, idx;\n\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\t/*\n\t * It's not the end of the world if we don't get\n\t * the lock, but we also don't want to spin\n\t * nor do we want to disable interrupts,\n\t * so if we miss here, then better luck next time.\n\t */\n\tif (!arch_spin_trylock(&trace_cmdline_lock))\n\t\treturn 0;\n\n\tidx = savedcmd->map_pid_to_cmdline[tsk->pid];\n\tif (idx == NO_CMDLINE_MAP) {\n\t\tidx = (savedcmd->cmdline_idx + 1) % savedcmd->cmdline_num;\n\n\t\t/*\n\t\t * Check whether the cmdline buffer at idx has a pid\n\t\t * mapped. We are going to overwrite that entry so we\n\t\t * need to clear the map_pid_to_cmdline. Otherwise we\n\t\t * would read the new comm for the old pid.\n\t\t */\n\t\tpid = savedcmd->map_cmdline_to_pid[idx];\n\t\tif (pid != NO_CMDLINE_MAP)\n\t\t\tsavedcmd->map_pid_to_cmdline[pid] = NO_CMDLINE_MAP;\n\n\t\tsavedcmd->map_cmdline_to_pid[idx] = tsk->pid;\n\t\tsavedcmd->map_pid_to_cmdline[tsk->pid] = idx;\n\n\t\tsavedcmd->cmdline_idx = idx;\n\t}\n\n\tset_cmdline(idx, tsk->comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn 1;\n}\n\nstatic void __trace_find_cmdline(int pid, char comm[])\n{\n\tunsigned map;\n\n\tif (!pid) {\n\t\tstrcpy(comm, \"<idle>\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(pid < 0)) {\n\t\tstrcpy(comm, \"<XXX>\");\n\t\treturn;\n\t}\n\n\tif (pid > PID_MAX_DEFAULT) {\n\t\tstrcpy(comm, \"<...>\");\n\t\treturn;\n\t}\n\n\tmap = savedcmd->map_pid_to_cmdline[pid];\n\tif (map != NO_CMDLINE_MAP)\n\t\tstrlcpy(comm, get_saved_cmdlines(map), TASK_COMM_LEN);\n\telse\n\t\tstrcpy(comm, \"<...>\");\n}\n\nvoid trace_find_cmdline(int pid, char comm[])\n{\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\t__trace_find_cmdline(pid, comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nint trace_find_tgid(int pid)\n{\n\tif (unlikely(!tgid_map || !pid || pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\treturn tgid_map[pid];\n}\n\nstatic int trace_save_tgid(struct task_struct *tsk)\n{\n\t/* treat recording of idle task as a success */\n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tif (unlikely(!tgid_map || tsk->pid > PID_MAX_DEFAULT))\n\t\treturn 0;\n\n\ttgid_map[tsk->pid] = tsk->tgid;\n\treturn 1;\n}\n\nstatic bool tracing_record_taskinfo_skip(int flags)\n{\n\tif (unlikely(!(flags & (TRACE_RECORD_CMDLINE | TRACE_RECORD_TGID))))\n\t\treturn true;\n\tif (atomic_read(&trace_record_taskinfo_disabled) || !tracing_is_on())\n\t\treturn true;\n\tif (!__this_cpu_read(trace_taskinfo_save))\n\t\treturn true;\n\treturn false;\n}\n\n/**\n * tracing_record_taskinfo - record the task info of a task\n *\n * @task  - task to record\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *        - TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo(struct task_struct *task, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(task);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(task);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/**\n * tracing_record_taskinfo_sched_switch - record task info for sched_switch\n *\n * @prev - previous task during sched_switch\n * @next - next task during sched_switch\n * @flags - TRACE_RECORD_CMDLINE for recording comm\n *          TRACE_RECORD_TGID for recording tgid\n */\nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t/*\n\t * Record as much task information as possible. If some fail, continue\n\t * to try to record the others.\n\t */\n\tdone  = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(prev);\n\tdone &= !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(next);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(prev);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(next);\n\n\t/* If recording any information failed, retry again soon. */\n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n/* Helpers to record a specific task information */\nvoid tracing_record_cmdline(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_CMDLINE);\n}\n\nvoid tracing_record_tgid(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_TGID);\n}\n\n/*\n * Several functions return TRACE_TYPE_PARTIAL_LINE if the trace_seq\n * overflowed, and TRACE_TYPE_HANDLED otherwise. This helper function\n * simplifies those functions and keeps them in sync.\n */\nenum print_line_t trace_handle_return(struct trace_seq *s)\n{\n\treturn trace_seq_has_overflowed(s) ?\n\t\tTRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;\n}\nEXPORT_SYMBOL_GPL(trace_handle_return);\n\nvoid\ntracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,\n\t\t\t     int pc)\n{\n\tstruct task_struct *tsk = current;\n\n\tentry->preempt_count\t\t= pc & 0xff;\n\tentry->pid\t\t\t= (tsk) ? tsk->pid : 0;\n\tentry->flags =\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\n\t\t(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |\n#else\n\t\tTRACE_FLAG_IRQS_NOSUPPORT |\n#endif\n\t\t((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |\n\t\t((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |\n\t\t((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |\n\t\t(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |\n\t\t(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);\n}\nEXPORT_SYMBOL_GPL(tracing_generic_entry_update);\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct ring_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\treturn __trace_buffer_lock_reserve(buffer, type, len, flags, pc);\n}\n\nDEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDEFINE_PER_CPU(int, trace_buffered_event_cnt);\nstatic int trace_buffered_event_ref;\n\n/**\n * trace_buffered_event_enable - enable buffering events\n *\n * When events are being filtered, it is quicker to use a temporary\n * buffer to write the event data into if there's a likely chance\n * that it will not be committed. The discard of the ring buffer\n * is not as fast as committing, and is much slower than copying\n * a commit.\n *\n * When an event is to be filtered, allocate per cpu buffers to\n * write the event data into, and if the event is filtered and discarded\n * it is simply dropped, otherwise, the entire data is to be committed\n * in one shot.\n */\nvoid trace_buffered_event_enable(void)\n{\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}\n\nstatic void enable_trace_buffered_event(void *data)\n{\n\t/* Probably not needed, but do it anyway */\n\tsmp_rmb();\n\tthis_cpu_dec(trace_buffered_event_cnt);\n}\n\nstatic void disable_trace_buffered_event(void *data)\n{\n\tthis_cpu_inc(trace_buffered_event_cnt);\n}\n\n/**\n * trace_buffered_event_disable - disable buffering events\n *\n * When a filter is removed, it is faster to not use the buffered\n * events, and to commit directly into the ring buffer. Free up\n * the temp buffers when there are no more users. This requires\n * special synchronization with current events.\n */\nvoid trace_buffered_event_disable(void)\n{\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (WARN_ON_ONCE(!trace_buffered_event_ref))\n\t\treturn;\n\n\tif (--trace_buffered_event_ref)\n\t\treturn;\n\n\tpreempt_disable();\n\t/* For each CPU, set the buffer as used. */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       disable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n\n\t/* Wait for all current users to finish */\n\tsynchronize_rcu();\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tfree_page((unsigned long)per_cpu(trace_buffered_event, cpu));\n\t\tper_cpu(trace_buffered_event, cpu) = NULL;\n\t}\n\t/*\n\t * Make sure trace_buffered_event is NULL before clearing\n\t * trace_buffered_event_cnt.\n\t */\n\tsmp_wmb();\n\n\tpreempt_disable();\n\t/* Do the work on each cpu */\n\tsmp_call_function_many(tracing_buffer_mask,\n\t\t\t       enable_trace_buffered_event, NULL, 1);\n\tpreempt_enable();\n}\n\nstatic struct ring_buffer *temp_buffer;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct ring_buffer **current_rb,\n\t\t\t  struct trace_event_file *trace_file,\n\t\t\t  int type, unsigned long len,\n\t\t\t  unsigned long flags, int pc)\n{\n\tstruct ring_buffer_event *entry;\n\tint val;\n\n\t*current_rb = trace_file->tr->trace_buffer.buffer;\n\n\tif (!ring_buffer_time_stamp_abs(*current_rb) && (trace_file->flags &\n\t     (EVENT_FILE_FL_SOFT_DISABLED | EVENT_FILE_FL_FILTERED)) &&\n\t    (entry = this_cpu_read(trace_buffered_event))) {\n\t\t/* Try to use the per cpu buffer first */\n\t\tval = this_cpu_inc_return(trace_buffered_event_cnt);\n\t\tif (val == 1) {\n\t\t\ttrace_event_setup(entry, type, flags, pc);\n\t\t\tentry->array[0] = len;\n\t\t\treturn entry;\n\t\t}\n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t}\n\n\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t    type, len, flags, pc);\n\t/*\n\t * If tracing is off, but we have triggers enabled\n\t * we still need to look at the event data. Use the temp_buffer\n\t * to store the trace event for the tigger to use. It's recusive\n\t * safe and will not be recorded anywhere.\n\t */\n\tif (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {\n\t\t*current_rb = temp_buffer;\n\t\tentry = __trace_buffer_lock_reserve(*current_rb,\n\t\t\t\t\t\t    type, len, flags, pc);\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_lock_reserve);\n\nstatic DEFINE_SPINLOCK(tracepoint_iter_lock);\nstatic DEFINE_MUTEX(tracepoint_printk_mutex);\n\nstatic void output_printk(struct trace_event_buffer *fbuffer)\n{\n\tstruct trace_event_call *event_call;\n\tstruct trace_event *event;\n\tunsigned long flags;\n\tstruct trace_iterator *iter = tracepoint_print_iter;\n\n\t/* We should never get here if iter is NULL */\n\tif (WARN_ON_ONCE(!iter))\n\t\treturn;\n\n\tevent_call = fbuffer->trace_file->event_call;\n\tif (!event_call || !event_call->event.funcs ||\n\t    !event_call->event.funcs->trace)\n\t\treturn;\n\n\tevent = &fbuffer->trace_file->event_call->event;\n\n\tspin_lock_irqsave(&tracepoint_iter_lock, flags);\n\ttrace_seq_init(&iter->seq);\n\titer->ent = fbuffer->entry;\n\tevent_call->event.funcs->trace(iter, 0, event);\n\ttrace_seq_putc(&iter->seq, 0);\n\tprintk(\"%s\", iter->seq.buffer);\n\n\tspin_unlock_irqrestore(&tracepoint_iter_lock, flags);\n}\n\nint tracepoint_printk_sysctl(struct ctl_table *table, int write,\n\t\t\t     void __user *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tint save_tracepoint_printk;\n\tint ret;\n\n\tmutex_lock(&tracepoint_printk_mutex);\n\tsave_tracepoint_printk = tracepoint_printk;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\t/*\n\t * This will force exiting early, as tracepoint_printk\n\t * is always zero when tracepoint_printk_iter is not allocated\n\t */\n\tif (!tracepoint_print_iter)\n\t\ttracepoint_printk = 0;\n\n\tif (save_tracepoint_printk == tracepoint_printk)\n\t\tgoto out;\n\n\tif (tracepoint_printk)\n\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\telse\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\n out:\n\tmutex_unlock(&tracepoint_printk_mutex);\n\n\treturn ret;\n}\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer)\n{\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_commit);\n\n/*\n * Skip 3:\n *\n *   trace_buffer_unlock_commit_regs()\n *   trace_event_buffer_commit()\n *   trace_event_raw_event_xxx()\n */\n# define STACK_SKIP 3\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct ring_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned long flags, int pc,\n\t\t\t\t     struct pt_regs *regs)\n{\n\t__buffer_unlock_commit(buffer, event);\n\n\t/*\n\t * If regs is not set, then skip the necessary functions.\n\t * Note, we can still get here via blktrace, wakeup tracer\n\t * and mmiotrace, but that's ok if they lose a function or\n\t * two. They are not that meaningful.\n\t */\n\tftrace_trace_stack(tr, buffer, flags, regs ? 0 : STACK_SKIP, pc, regs);\n\tftrace_trace_userstack(buffer, flags, pc);\n}\n\n/*\n * Similar to trace_buffer_unlock_commit_regs() but do not dump stack.\n */\nvoid\ntrace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event)\n{\n\t__buffer_unlock_commit(buffer, event);\n}\n\nstatic void\ntrace_process_export(struct trace_export *export,\n\t       struct ring_buffer_event *event)\n{\n\tstruct trace_entry *entry;\n\tunsigned int size = 0;\n\n\tentry = ring_buffer_event_data(event);\n\tsize = ring_buffer_event_length(event);\n\texport->write(export, entry, size);\n}\n\nstatic DEFINE_MUTEX(ftrace_export_lock);\n\nstatic struct trace_export __rcu *ftrace_exports_list __read_mostly;\n\nstatic DEFINE_STATIC_KEY_FALSE(ftrace_exports_enabled);\n\nstatic inline void ftrace_exports_enable(void)\n{\n\tstatic_branch_enable(&ftrace_exports_enabled);\n}\n\nstatic inline void ftrace_exports_disable(void)\n{\n\tstatic_branch_disable(&ftrace_exports_enabled);\n}\n\nstatic void ftrace_exports(struct ring_buffer_event *event)\n{\n\tstruct trace_export *export;\n\n\tpreempt_disable_notrace();\n\n\texport = rcu_dereference_raw_notrace(ftrace_exports_list);\n\twhile (export) {\n\t\ttrace_process_export(export, event);\n\t\texport = rcu_dereference_raw_notrace(export->next);\n\t}\n\n\tpreempt_enable_notrace();\n}\n\nstatic inline void\nadd_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}\n\nstatic inline int\nrm_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\tstruct trace_export **p;\n\n\tfor (p = list; *p != NULL; p = &(*p)->next)\n\t\tif (*p == export)\n\t\t\tbreak;\n\n\tif (*p != export)\n\t\treturn -1;\n\n\trcu_assign_pointer(*p, (*p)->next);\n\n\treturn 0;\n}\n\nstatic inline void\nadd_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tif (*list == NULL)\n\t\tftrace_exports_enable();\n\n\tadd_trace_export(list, export);\n}\n\nstatic inline int\nrm_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tint ret;\n\n\tret = rm_trace_export(list, export);\n\tif (*list == NULL)\n\t\tftrace_exports_disable();\n\n\treturn ret;\n}\n\nint register_ftrace_export(struct trace_export *export)\n{\n\tif (WARN_ON_ONCE(!export->write))\n\t\treturn -1;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tadd_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_export);\n\nint unregister_ftrace_export(struct trace_export *export)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tret = rm_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_export);\n\nvoid\ntrace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc)\n{\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}\n\n#ifdef CONFIG_STACKTRACE\n\n#define FTRACE_STACK_MAX_ENTRIES (PAGE_SIZE / sizeof(unsigned long))\nstruct ftrace_stack {\n\tunsigned long\t\tcalls[FTRACE_STACK_MAX_ENTRIES];\n};\n\nstatic DEFINE_PER_CPU(struct ftrace_stack, ftrace_stack);\nstatic DEFINE_PER_CPU(int, ftrace_stack_reserve);\n\nstatic void __ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t unsigned long flags,\n\t\t\t\t int skip, int pc, struct pt_regs *regs)\n{\n\tstruct trace_event_call *call = &event_kernel_stack;\n\tstruct ring_buffer_event *event;\n\tstruct stack_entry *entry;\n\tstruct stack_trace trace;\n\tint use_stack;\n\tint size = FTRACE_STACK_ENTRIES;\n\n\ttrace.nr_entries\t= 0;\n\ttrace.skip\t\t= skip;\n\n\t/*\n\t * Add one, for this function and the call to save_stack_trace()\n\t * If regs is set, then these functions will not be in the way.\n\t */\n#ifndef CONFIG_UNWINDER_ORC\n\tif (!regs)\n\t\ttrace.skip++;\n#endif\n\n\t/*\n\t * Since events can happen in NMIs there's no safe way to\n\t * use the per cpu ftrace_stacks. We reserve it and if an interrupt\n\t * or NMI comes in, it will just have to use the default\n\t * FTRACE_STACK_SIZE.\n\t */\n\tpreempt_disable_notrace();\n\n\tuse_stack = __this_cpu_inc_return(ftrace_stack_reserve);\n\t/*\n\t * We don't need any atomic variables, just a barrier.\n\t * If an interrupt comes in, we don't care, because it would\n\t * have exited and put the counter back to what we want.\n\t * We just need a barrier to keep gcc from moving things\n\t * around.\n\t */\n\tbarrier();\n\tif (use_stack == 1) {\n\t\ttrace.entries\t\t= this_cpu_ptr(ftrace_stack.calls);\n\t\ttrace.max_entries\t= FTRACE_STACK_MAX_ENTRIES;\n\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\n\t\tif (trace.nr_entries > size)\n\t\t\tsize = trace.nr_entries;\n\t} else\n\t\t/* From now on, use_stack is a boolean */\n\t\tuse_stack = 0;\n\n\tsize *= sizeof(unsigned long);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_STACK,\n\t\t\t\t\t    sizeof(*entry) + size, flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\n\tmemset(&entry->caller, 0, size);\n\n\tif (use_stack)\n\t\tmemcpy(&entry->caller, trace.entries,\n\t\t       trace.nr_entries * sizeof(unsigned long));\n\telse {\n\t\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\t\ttrace.entries\t\t= entry->caller;\n\t\tif (regs)\n\t\t\tsave_stack_trace_regs(regs, &trace);\n\t\telse\n\t\t\tsave_stack_trace(&trace);\n\t}\n\n\tentry->size = trace.nr_entries;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out:\n\t/* Again, don't let gcc optimize things here */\n\tbarrier();\n\t__this_cpu_dec(ftrace_stack_reserve);\n\tpreempt_enable_notrace();\n\n}\n\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs)\n{\n\tif (!(tr->trace_flags & TRACE_ITER_STACKTRACE))\n\t\treturn;\n\n\t__ftrace_trace_stack(buffer, flags, skip, pc, regs);\n}\n\nvoid __trace_stack(struct trace_array *tr, unsigned long flags, int skip,\n\t\t   int pc)\n{\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\n\tif (rcu_is_watching()) {\n\t\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\t\treturn;\n\t}\n\n\t/*\n\t * When an NMI triggers, RCU is enabled via rcu_nmi_enter(),\n\t * but if the above rcu_is_watching() failed, then the NMI\n\t * triggered someplace critical, and rcu_irq_enter() should\n\t * not be called from NMI.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\trcu_irq_enter_irqson();\n\t__ftrace_trace_stack(buffer, flags, skip, pc, NULL);\n\trcu_irq_exit_irqson();\n}\n\n/**\n * trace_dump_stack - record a stack back trace in the trace buffer\n * @skip: Number of functions to skip (helper handlers)\n */\nvoid trace_dump_stack(int skip)\n{\n\tunsigned long flags;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn;\n\n\tlocal_save_flags(flags);\n\n#ifndef CONFIG_UNWINDER_ORC\n\t/* Skip 1 to skip this function. */\n\tskip++;\n#endif\n\t__ftrace_trace_stack(global_trace.trace_buffer.buffer,\n\t\t\t     flags, skip, preempt_count(), NULL);\n}\nEXPORT_SYMBOL_GPL(trace_dump_stack);\n\nstatic DEFINE_PER_CPU(int, user_stack_count);\n\nvoid\nftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags, int pc)\n{\n\tstruct trace_event_call *call = &event_user_stack;\n\tstruct ring_buffer_event *event;\n\tstruct userstack_entry *entry;\n\tstruct stack_trace trace;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_USERSTACKTRACE))\n\t\treturn;\n\n\t/*\n\t * NMIs can not handle page faults, even with fix ups.\n\t * The save user stack can (and often does) fault.\n\t */\n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\t/*\n\t * prevent recursion, since the user stack tracing may\n\t * trigger other kernel events.\n\t */\n\tpreempt_disable();\n\tif (__this_cpu_read(user_stack_count))\n\t\tgoto out;\n\n\t__this_cpu_inc(user_stack_count);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,\n\t\t\t\t\t    sizeof(*entry), flags, pc);\n\tif (!event)\n\t\tgoto out_drop_count;\n\tentry\t= ring_buffer_event_data(event);\n\n\tentry->tgid\t\t= current->tgid;\n\tmemset(&entry->caller, 0, sizeof(entry->caller));\n\n\ttrace.nr_entries\t= 0;\n\ttrace.max_entries\t= FTRACE_STACK_ENTRIES;\n\ttrace.skip\t\t= 0;\n\ttrace.entries\t\t= entry->caller;\n\n\tsave_stack_trace_user(&trace);\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out_drop_count:\n\t__this_cpu_dec(user_stack_count);\n out:\n\tpreempt_enable();\n}\n\n#ifdef UNUSED\nstatic void __trace_userstack(struct trace_array *tr, unsigned long flags)\n{\n\tftrace_trace_userstack(tr, flags, preempt_count());\n}\n#endif /* UNUSED */\n\n#endif /* CONFIG_STACKTRACE */\n\n/* created for use with alloc_percpu */\nstruct trace_buffer_struct {\n\tint nesting;\n\tchar buffer[4][TRACE_BUF_SIZE];\n};\n\nstatic struct trace_buffer_struct *trace_percpu_buffer;\n\n/*\n * Thise allows for lockless recording.  If we're nested too deeply, then\n * this returns NULL.\n */\nstatic char *get_trace_buf(void)\n{\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}\n\nstatic void put_trace_buf(void)\n{\n\t/* Don't let the decrement of nesting leak before this */\n\tbarrier();\n\tthis_cpu_dec(trace_percpu_buffer->nesting);\n}\n\nstatic int alloc_percpu_trace_buffer(void)\n{\n\tstruct trace_buffer_struct *buffers;\n\n\tbuffers = alloc_percpu(struct trace_buffer_struct);\n\tif (WARN(!buffers, \"Could not allocate percpu trace_printk buffer\"))\n\t\treturn -ENOMEM;\n\n\ttrace_percpu_buffer = buffers;\n\treturn 0;\n}\n\nstatic int buffers_allocated;\n\nvoid trace_printk_init_buffers(void)\n{\n\tif (buffers_allocated)\n\t\treturn;\n\n\tif (alloc_percpu_trace_buffer())\n\t\treturn;\n\n\t/* trace_printk() is for debug use only. Don't use it in production. */\n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** trace_printk() being used. Allocating extra memory.  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** This means that this is a DEBUG kernel and it is     **\\n\");\n\tpr_warn(\"** unsafe for production use.                           **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging    **\\n\");\n\tpr_warn(\"** the kernel, report this immediately to your vendor!  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\n\t/* Expand the buffers to set size */\n\ttracing_update_buffers();\n\n\tbuffers_allocated = 1;\n\n\t/*\n\t * trace_printk_init_buffers() can be called by modules.\n\t * If that happens, then we need to start cmdline recording\n\t * directly here. If the global_trace.buffer is already\n\t * allocated here, then this was called by module code.\n\t */\n\tif (global_trace.trace_buffer.buffer)\n\t\ttracing_start_cmdline_record();\n}\n\nvoid trace_printk_start_comm(void)\n{\n\t/* Start tracing comms if trace printk is set */\n\tif (!buffers_allocated)\n\t\treturn;\n\ttracing_start_cmdline_record();\n}\n\nstatic void trace_printk_start_stop_comm(int enabled)\n{\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}\n\n/**\n * trace_vbprintk - write binary msg to tracing buffer\n *\n */\nint trace_vbprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_bprint;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct trace_array *tr = &global_trace;\n\tstruct bprint_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\tint len = 0, size, pc;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vbin_printf((u32 *)tbuffer, TRACE_BUF_SIZE/sizeof(int), fmt, args);\n\n\tif (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)\n\t\tgoto out;\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + sizeof(u32) * len;\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->fmt\t\t\t= fmt;\n\n\tmemcpy(entry->buf, tbuffer, sizeof(u32) * len);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(tr, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(trace_vbprintk);\n\n__printf(3, 0)\nstatic int\n__trace_array_vprintk(struct ring_buffer *buffer,\n\t\t      unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_print;\n\tstruct ring_buffer_event *event;\n\tint len = 0, size, pc;\n\tstruct print_entry *entry;\n\tunsigned long flags;\n\tchar *tbuffer;\n\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn 0;\n\n\t/* Don't pollute graph traces with trace_vprintk internals */\n\tpause_graph_tracing();\n\n\tpc = preempt_count();\n\tpreempt_disable_notrace();\n\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);\n\n\tlocal_save_flags(flags);\n\tsize = sizeof(*entry) + len + 1;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, tbuffer, len + 1);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(&global_trace, buffer, flags, 6, pc, NULL);\n\t}\n\nout:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\n\n__printf(3, 0)\nint trace_array_vprintk(struct trace_array *tr,\n\t\t\tunsigned long ip, const char *fmt, va_list args)\n{\n\treturn __trace_array_vprintk(tr->trace_buffer.buffer, ip, fmt, args);\n}\n\n__printf(3, 0)\nint trace_array_printk(struct trace_array *tr,\n\t\t       unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = trace_array_vprintk(tr, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\n__printf(3, 4)\nint trace_array_printk_buf(struct ring_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = __trace_array_vprintk(buffer, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\n__printf(2, 0)\nint trace_vprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\treturn trace_array_vprintk(&global_trace, ip, fmt, args);\n}\nEXPORT_SYMBOL_GPL(trace_vprintk);\n\nstatic void trace_iterator_increment(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, iter->cpu);\n\n\titer->idx++;\n\tif (buf_iter)\n\t\tring_buffer_read(buf_iter, NULL);\n}\n\nstatic struct trace_entry *\npeek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,\n\t\tunsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, cpu);\n\n\tif (buf_iter)\n\t\tevent = ring_buffer_iter_peek(buf_iter, ts);\n\telse\n\t\tevent = ring_buffer_peek(iter->trace_buffer->buffer, cpu, ts,\n\t\t\t\t\t lost_events);\n\n\tif (event) {\n\t\titer->ent_size = ring_buffer_event_length(event);\n\t\treturn ring_buffer_event_data(event);\n\t}\n\titer->ent_size = 0;\n\treturn NULL;\n}\n\nstatic struct trace_entry *\n__find_next_entry(struct trace_iterator *iter, int *ent_cpu,\n\t\t  unsigned long *missing_events, u64 *ent_ts)\n{\n\tstruct ring_buffer *buffer = iter->trace_buffer->buffer;\n\tstruct trace_entry *ent, *next = NULL;\n\tunsigned long lost_events = 0, next_lost = 0;\n\tint cpu_file = iter->cpu_file;\n\tu64 next_ts = 0, ts;\n\tint next_cpu = -1;\n\tint next_size = 0;\n\tint cpu;\n\n\t/*\n\t * If we are in a per_cpu trace file, don't bother by iterating over\n\t * all cpu and peek directly.\n\t */\n\tif (cpu_file > RING_BUFFER_ALL_CPUS) {\n\t\tif (ring_buffer_empty_cpu(buffer, cpu_file))\n\t\t\treturn NULL;\n\t\tent = peek_next_entry(iter, cpu_file, ent_ts, missing_events);\n\t\tif (ent_cpu)\n\t\t\t*ent_cpu = cpu_file;\n\n\t\treturn ent;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\n\t\tif (ring_buffer_empty_cpu(buffer, cpu))\n\t\t\tcontinue;\n\n\t\tent = peek_next_entry(iter, cpu, &ts, &lost_events);\n\n\t\t/*\n\t\t * Pick the entry with the smallest timestamp:\n\t\t */\n\t\tif (ent && (!next || ts < next_ts)) {\n\t\t\tnext = ent;\n\t\t\tnext_cpu = cpu;\n\t\t\tnext_ts = ts;\n\t\t\tnext_lost = lost_events;\n\t\t\tnext_size = iter->ent_size;\n\t\t}\n\t}\n\n\titer->ent_size = next_size;\n\n\tif (ent_cpu)\n\t\t*ent_cpu = next_cpu;\n\n\tif (ent_ts)\n\t\t*ent_ts = next_ts;\n\n\tif (missing_events)\n\t\t*missing_events = next_lost;\n\n\treturn next;\n}\n\n/* Find the next real entry, without updating the iterator itself */\nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts)\n{\n\treturn __find_next_entry(iter, ent_cpu, NULL, ent_ts);\n}\n\n/* Find the next real entry, and increment the iterator to the next entry */\nvoid *trace_find_next_entry_inc(struct trace_iterator *iter)\n{\n\titer->ent = __find_next_entry(iter, &iter->cpu,\n\t\t\t\t      &iter->lost_events, &iter->ts);\n\n\tif (iter->ent)\n\t\ttrace_iterator_increment(iter);\n\n\treturn iter->ent ? iter : NULL;\n}\n\nstatic void trace_consume(struct trace_iterator *iter)\n{\n\tring_buffer_consume(iter->trace_buffer->buffer, iter->cpu, &iter->ts,\n\t\t\t    &iter->lost_events);\n}\n\nstatic void *s_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tint i = (int)*pos;\n\tvoid *ent;\n\n\tWARN_ON_ONCE(iter->leftover);\n\n\t(*pos)++;\n\n\t/* can't go backwards */\n\tif (iter->idx > i)\n\t\treturn NULL;\n\n\tif (iter->idx < 0)\n\t\tent = trace_find_next_entry_inc(iter);\n\telse\n\t\tent = iter;\n\n\twhile (ent && iter->idx < i)\n\t\tent = trace_find_next_entry_inc(iter);\n\n\titer->pos = *pos;\n\n\treturn ent;\n}\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter;\n\tunsigned long entries = 0;\n\tu64 ts;\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = 0;\n\n\tbuf_iter = trace_buffer_iter(iter, cpu);\n\tif (!buf_iter)\n\t\treturn;\n\n\tring_buffer_iter_reset(buf_iter);\n\n\t/*\n\t * We could have the case with the max latency tracers\n\t * that a reset never took place on a cpu. This is evident\n\t * by the timestamp being before the start of the buffer.\n\t */\n\twhile ((event = ring_buffer_iter_peek(buf_iter, &ts))) {\n\t\tif (ts >= iter->trace_buffer->time_start)\n\t\t\tbreak;\n\t\tentries++;\n\t\tring_buffer_read(buf_iter, NULL);\n\t}\n\n\tper_cpu_ptr(iter->trace_buffer->data, cpu)->skipped_entries = entries;\n}\n\n/*\n * The current tracer is copied to avoid a global locking\n * all around.\n */\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu_file = iter->cpu_file;\n\tvoid *p = NULL;\n\tloff_t l = 0;\n\tint cpu;\n\n\t/*\n\t * copy the tracer to avoid using a global lock all around.\n\t * iter->trace is a copy of current_trace, the pointer to the\n\t * name may be used instead of a strcmp(), as iter->trace->name\n\t * will point to the same string as current_trace->name.\n\t */\n\tmutex_lock(&trace_types_lock);\n\tif (unlikely(tr->current_trace && iter->trace->name != tr->current_trace->name))\n\t\t*iter->trace = *tr->current_trace;\n\tmutex_unlock(&trace_types_lock);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn ERR_PTR(-EBUSY);\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_inc(&trace_record_taskinfo_disabled);\n\n\tif (*pos != iter->pos) {\n\t\titer->ent = NULL;\n\t\titer->cpu = 0;\n\t\titer->idx = -1;\n\n\t\tif (cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tfor_each_tracing_cpu(cpu)\n\t\t\t\ttracing_iter_reset(iter, cpu);\n\t\t} else\n\t\t\ttracing_iter_reset(iter, cpu_file);\n\n\t\titer->leftover = 0;\n\t\tfor (p = iter; p && l < *pos; p = s_next(m, p, &l))\n\t\t\t;\n\n\t} else {\n\t\t/*\n\t\t * If we overflowed the seq_file before, then we want\n\t\t * to just reuse the trace_seq buffer again.\n\t\t */\n\t\tif (iter->leftover)\n\t\t\tp = iter;\n\t\telse {\n\t\t\tl = *pos - 1;\n\t\t\tp = s_next(m, p, &l);\n\t\t}\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(cpu_file);\n\treturn p;\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n{\n\tstruct trace_iterator *iter = m->private;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn;\n#endif\n\n\tif (!iter->snapshot)\n\t\tatomic_dec(&trace_record_taskinfo_disabled);\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n}\n\nstatic void\nget_total_entries(struct trace_buffer *buf,\n\t\t  unsigned long *total, unsigned long *entries)\n{\n\tunsigned long count;\n\tint cpu;\n\n\t*total = 0;\n\t*entries = 0;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tcount = ring_buffer_entries_cpu(buf->buffer, cpu);\n\t\t/*\n\t\t * If this buffer has skipped entries, then we hold all\n\t\t * entries for the trace and we need to ignore the\n\t\t * ones before the time stamp.\n\t\t */\n\t\tif (per_cpu_ptr(buf->data, cpu)->skipped_entries) {\n\t\t\tcount -= per_cpu_ptr(buf->data, cpu)->skipped_entries;\n\t\t\t/* total is the same as the entries */\n\t\t\t*total += count;\n\t\t} else\n\t\t\t*total += count +\n\t\t\t\tring_buffer_overrun_cpu(buf->buffer, cpu);\n\t\t*entries += count;\n\t}\n}\n\nstatic void print_lat_help_header(struct seq_file *m)\n{\n\tseq_puts(m, \"#                  _------=> CPU#            \\n\"\n\t\t    \"#                 / _-----=> irqs-off        \\n\"\n\t\t    \"#                | / _----=> need-resched    \\n\"\n\t\t    \"#                || / _---=> hardirq/softirq \\n\"\n\t\t    \"#                ||| / _--=> preempt-depth   \\n\"\n\t\t    \"#                |||| /     delay            \\n\"\n\t\t    \"#  cmd     pid   ||||| time  |   caller      \\n\"\n\t\t    \"#     \\\\   /      |||||  \\\\    |   /         \\n\");\n}\n\nstatic void print_event_info(struct trace_buffer *buf, struct seq_file *m)\n{\n\tunsigned long total;\n\tunsigned long entries;\n\n\tget_total_entries(buf, &total, &entries);\n\tseq_printf(m, \"# entries-in-buffer/entries-written: %lu/%lu   #P:%d\\n\",\n\t\t   entries, total, num_online_cpus());\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void print_func_help_header(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t   unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#           TASK-PID   %s  CPU#   TIMESTAMP  FUNCTION\\n\", tgid ? \"TGID     \" : \"\");\n\tseq_printf(m, \"#              | |     %s    |       |         |\\n\",\t tgid ? \"  |      \" : \"\");\n}\n\nstatic void print_func_help_header_irq(struct trace_buffer *buf, struct seq_file *m,\n\t\t\t\t       unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\tconst char tgid_space[] = \"          \";\n\tconst char space[] = \"  \";\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#                          %s  _-----=> irqs-off\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s / _----=> need-resched\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s| / _---=> hardirq/softirq\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s|| / _--=> preempt-depth\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#                          %s||| /     delay\\n\",\n\t\t   tgid ? tgid_space : space);\n\tseq_printf(m, \"#           TASK-PID %sCPU#  ||||    TIMESTAMP  FUNCTION\\n\",\n\t\t   tgid ? \"   TGID   \" : space);\n\tseq_printf(m, \"#              | |   %s  |   ||||       |         |\\n\",\n\t\t   tgid ? \"     |    \" : space);\n}\n\nvoid\nprint_trace_header(struct seq_file *m, struct trace_iterator *iter)\n{\n\tunsigned long sym_flags = (global_trace.trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_buffer *buf = iter->trace_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(buf->data, buf->cpu);\n\tstruct tracer *type = iter->trace;\n\tunsigned long entries;\n\tunsigned long total;\n\tconst char *name = \"preemption\";\n\n\tname = type->name;\n\n\tget_total_entries(buf, &total, &entries);\n\n\tseq_printf(m, \"# %s latency trace v1.1.5 on %s\\n\",\n\t\t   name, UTS_RELEASE);\n\tseq_puts(m, \"# -----------------------------------\"\n\t\t \"---------------------------------\\n\");\n\tseq_printf(m, \"# latency: %lu us, #%lu/%lu, CPU#%d |\"\n\t\t   \" (M:%s VP:%d, KP:%d, SP:%d HP:%d\",\n\t\t   nsecs_to_usecs(data->saved_latency),\n\t\t   entries,\n\t\t   total,\n\t\t   buf->cpu,\n#if defined(CONFIG_PREEMPT_NONE)\n\t\t   \"server\",\n#elif defined(CONFIG_PREEMPT_VOLUNTARY)\n\t\t   \"desktop\",\n#elif defined(CONFIG_PREEMPT)\n\t\t   \"preempt\",\n#else\n\t\t   \"unknown\",\n#endif\n\t\t   /* These are reserved for later use */\n\t\t   0, 0, 0, 0);\n#ifdef CONFIG_SMP\n\tseq_printf(m, \" #P:%d)\\n\", num_online_cpus());\n#else\n\tseq_puts(m, \")\\n\");\n#endif\n\tseq_puts(m, \"#    -----------------\\n\");\n\tseq_printf(m, \"#    | task: %.16s-%d \"\n\t\t   \"(uid:%d nice:%ld policy:%ld rt_prio:%ld)\\n\",\n\t\t   data->comm, data->pid,\n\t\t   from_kuid_munged(seq_user_ns(m), data->uid), data->nice,\n\t\t   data->policy, data->rt_priority);\n\tseq_puts(m, \"#    -----------------\\n\");\n\n\tif (data->critical_start) {\n\t\tseq_puts(m, \"#  => started at: \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_start, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#  => ended at:   \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_end, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#\\n\");\n\t}\n\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void test_cpu_buff_start(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}\n\nstatic enum print_line_t print_trace_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\ttest_cpu_buff_start(iter);\n\n\tevent = ftrace_find_event(entry->type);\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\t\ttrace_print_lat_context(iter);\n\t\telse\n\t\t\ttrace_print_context(iter);\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tif (event)\n\t\treturn event->funcs->trace(iter, sym_flags, event);\n\n\ttrace_seq_printf(s, \"Unknown type %d\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_raw_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO)\n\t\ttrace_seq_printf(s, \"%d %d %llu \",\n\t\t\t\t entry->pid, iter->cpu, iter->ts);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event)\n\t\treturn event->funcs->raw(iter, 0, event);\n\n\ttrace_seq_printf(s, \"%d ?\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_hex_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned char newline = '\\n';\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_HEX_FIELD(s, entry->pid);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event) {\n\t\tenum print_line_t ret = event->funcs->hex(iter, 0, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tSEQ_PUT_FIELD(s, newline);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_bin_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_FIELD(s, entry->pid);\n\t\tSEQ_PUT_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\treturn event ? event->funcs->binary(iter, 0, event) :\n\t\tTRACE_TYPE_HANDLED;\n}\n\nint trace_empty(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n/*  Called with trace_event_read_lock() held. */\nenum print_line_t print_trace_line(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\tenum print_line_t ret;\n\n\tif (iter->lost_events) {\n\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST %lu EVENTS]\\n\",\n\t\t\t\t iter->cpu, iter->lost_events);\n\t\tif (trace_seq_has_overflowed(&iter->seq))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tif (iter->trace && iter->trace->print_line) {\n\t\tret = iter->trace->print_line(iter);\n\t\tif (ret != TRACE_TYPE_UNHANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (iter->ent->type == TRACE_BPUTS &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bputs_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_BPRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bprintk_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_PRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_printk_msg_only(iter);\n\n\tif (trace_flags & TRACE_ITER_BIN)\n\t\treturn print_bin_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_HEX)\n\t\treturn print_hex_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_RAW)\n\t\treturn print_raw_fmt(iter);\n\n\treturn print_trace_fmt(iter);\n}\n\nvoid trace_latency_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t/* print nothing if the buffers are empty */\n\tif (trace_empty(iter))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\tprint_trace_header(m, iter);\n\n\tif (!(tr->trace_flags & TRACE_ITER_VERBOSE))\n\t\tprint_lat_help_header(m);\n}\n\nvoid trace_default_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\n\tif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT) {\n\t\t/* print nothing if the buffers are empty */\n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\t\tprint_trace_header(m, iter);\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE))\n\t\t\tprint_lat_help_header(m);\n\t} else {\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE)) {\n\t\t\tif (trace_flags & TRACE_ITER_IRQ_INFO)\n\t\t\t\tprint_func_help_header_irq(iter->trace_buffer,\n\t\t\t\t\t\t\t   m, trace_flags);\n\t\t\telse\n\t\t\t\tprint_func_help_header(iter->trace_buffer, m,\n\t\t\t\t\t\t       trace_flags);\n\t\t}\n\t}\n}\n\nstatic void test_ftrace_alive(struct seq_file *m)\n{\n\tif (!ftrace_is_dead())\n\t\treturn;\n\tseq_puts(m, \"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\"\n\t\t    \"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic void show_snapshot_main_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Clears and frees snapshot buffer\\n\"\n\t\t    \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer.\\n\"\n\t\t    \"# echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void show_snapshot_percpu_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Invalid for per_cpu snapshot file.\\n\");\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\tseq_puts(m, \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer for this cpu.\\n\");\n#else\n\tseq_puts(m, \"# echo 1 > snapshot : Not supported with this kernel.\\n\"\n\t\t    \"#                     Must use main snapshot file to allocate.\\n\");\n#endif\n\tseq_puts(m, \"# echo 2 > snapshot : Clears this cpu's snapshot buffer (but does not allocate)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter)\n{\n\tif (iter->tr->allocated_snapshot)\n\t\tseq_puts(m, \"#\\n# * Snapshot is allocated *\\n#\\n\");\n\telse\n\t\tseq_puts(m, \"#\\n# * Snapshot is freed *\\n#\\n\");\n\n\tseq_puts(m, \"# Snapshot commands:\\n\");\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\tshow_snapshot_main_help(m);\n\telse\n\t\tshow_snapshot_percpu_help(m);\n}\n#else\n/* Should never be called */\nstatic inline void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter) { }\n#endif\n\nstatic int s_show(struct seq_file *m, void *v)\n{\n\tstruct trace_iterator *iter = v;\n\tint ret;\n\n\tif (iter->ent == NULL) {\n\t\tif (iter->tr) {\n\t\t\tseq_printf(m, \"# tracer: %s\\n\", iter->trace->name);\n\t\t\tseq_puts(m, \"#\\n\");\n\t\t\ttest_ftrace_alive(m);\n\t\t}\n\t\tif (iter->snapshot && trace_empty(iter))\n\t\t\tprint_snapshot_help(m, iter);\n\t\telse if (iter->trace && iter->trace->print_header)\n\t\t\titer->trace->print_header(m);\n\t\telse\n\t\t\ttrace_default_header(m);\n\n\t} else if (iter->leftover) {\n\t\t/*\n\t\t * If we filled the seq_file buffer earlier, we\n\t\t * want to just show it now.\n\t\t */\n\t\tret = trace_print_seq(m, &iter->seq);\n\n\t\t/* ret should this time be zero, but you never know */\n\t\titer->leftover = ret;\n\n\t} else {\n\t\tprint_trace_line(iter);\n\t\tret = trace_print_seq(m, &iter->seq);\n\t\t/*\n\t\t * If we overflow the seq_file buffer, then it will\n\t\t * ask us for this data again at start up.\n\t\t * Use that instead.\n\t\t *  ret is 0 if seq_file write succeeded.\n\t\t *        -1 otherwise.\n\t\t */\n\t\titer->leftover = ret;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Should be used after trace_array_get(), trace_types_lock\n * ensures that i_cdev was already initialized.\n */\nstatic inline int tracing_get_cpu(struct inode *inode)\n{\n\tif (inode->i_cdev) /* See trace_create_cpu_file() */\n\t\treturn (long)inode->i_cdev - 1;\n\treturn RING_BUFFER_ALL_CPUS;\n}\n\nstatic const struct seq_operations tracer_seq_ops = {\n\t.start\t\t= s_start,\n\t.next\t\t= s_next,\n\t.stop\t\t= s_stop,\n\t.show\t\t= s_show,\n};\n\nstatic struct trace_iterator *\n__tracing_open(struct inode *inode, struct file *file, bool snapshot)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (tracing_disabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\titer = __seq_open_private(file, &tracer_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\titer->buffer_iter = kcalloc(nr_cpu_ids, sizeof(*iter->buffer_iter),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iter->buffer_iter)\n\t\tgoto release;\n\n\t/*\n\t * We make a copy of the current tracer to avoid concurrent\n\t * changes on it while we are reading.\n\t */\n\tmutex_lock(&trace_types_lock);\n\titer->trace = kzalloc(sizeof(*iter->trace), GFP_KERNEL);\n\tif (!iter->trace)\n\t\tgoto fail;\n\n\t*iter->trace = *tr->current_trace;\n\n\tif (!zalloc_cpumask_var(&iter->started, GFP_KERNEL))\n\t\tgoto fail;\n\n\titer->tr = tr;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t/* Currently only the top directory has a snapshot */\n\tif (tr->current_trace->print_max || snapshot)\n\t\titer->trace_buffer = &tr->max_buffer;\n\telse\n#endif\n\t\titer->trace_buffer = &tr->trace_buffer;\n\titer->snapshot = snapshot;\n\titer->pos = -1;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\n\t/* Notify the tracer early; before we stop tracing. */\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t/* stop the trace while dumping if we are not opening \"snapshot\" */\n\tif (!iter->snapshot)\n\t\ttracing_stop_tr(tr);\n\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\titer->buffer_iter[cpu] =\n\t\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\t}\n\t\tring_buffer_read_prepare_sync();\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\t\ttracing_iter_reset(iter, cpu);\n\t\t}\n\t} else {\n\t\tcpu = iter->cpu_file;\n\t\titer->buffer_iter[cpu] =\n\t\t\tring_buffer_read_prepare(iter->trace_buffer->buffer, cpu);\n\t\tring_buffer_read_prepare_sync();\n\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\ttracing_iter_reset(iter, cpu);\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn iter;\n\n fail:\n\tmutex_unlock(&trace_types_lock);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\nrelease:\n\tseq_release_private(inode, file);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint tracing_open_generic(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nbool tracing_is_disabled(void)\n{\n\treturn (tracing_disabled) ? true: false;\n}\n\n/*\n * Open and update trace_array ref count.\n * Must have the current trace_array passed to it.\n */\nstatic int tracing_open_generic_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\nstatic int tracing_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}\n\nstatic int tracing_release_generic_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn 0;\n}\n\nstatic int tracing_single_release_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn single_release(inode, file);\n}\n\nstatic int tracing_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\t/* If this file was open for write, then erase contents */\n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) {\n\t\tint cpu = tracing_get_cpu(inode);\n\t\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tif (tr->current_trace->print_max)\n\t\t\ttrace_buf = &tr->max_buffer;\n#endif\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\t\ttracing_reset_online_cpus(trace_buf);\n\t\telse\n\t\t\ttracing_reset(trace_buf, cpu);\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, false);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t\telse if (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\t}\n\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\n/*\n * Some tracers are not suitable for instance buffers.\n * A tracer is always available for the global array (toplevel)\n * or if it explicitly states that it is.\n */\nstatic bool\ntrace_ok_for_array(struct tracer *t, struct trace_array *tr)\n{\n\treturn (tr->flags & TRACE_ARRAY_FL_GLOBAL) || t->allow_instances;\n}\n\n/* Find the next tracer that this trace array may use */\nstatic struct tracer *\nget_tracer_for_array(struct trace_array *tr, struct tracer *t)\n{\n\twhile (t && !trace_ok_for_array(t, tr))\n\t\tt = t->next;\n\n\treturn t;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t = v;\n\n\t(*pos)++;\n\n\tif (t)\n\t\tt = get_tracer_for_array(tr, t->next);\n\n\treturn t;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tt = get_tracer_for_array(tr, trace_types);\n\tfor (; t && l < *pos; t = t_next(m, t, &l))\n\t\t\t;\n\n\treturn t;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct tracer *t = v;\n\n\tif (!t)\n\t\treturn 0;\n\n\tseq_puts(m, t->name);\n\tif (t->next)\n\t\tseq_putc(m, ' ');\n\telse\n\t\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_traces_seq_ops = {\n\t.start\t\t= t_start,\n\t.next\t\t= t_next,\n\t.stop\t\t= t_stop,\n\t.show\t\t= t_show,\n};\n\nstatic int show_traces_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_write_stub(struct file *filp, const char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence)\n{\n\tint ret;\n\n\tif (file->f_mode & FMODE_READ)\n\t\tret = seq_lseek(file, offset, whence);\n\telse\n\t\tfile->f_pos = ret = 0;\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_fops = {\n\t.open\t\t= tracing_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_write_stub,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_release,\n};\n\nstatic const struct file_operations show_traces_fops = {\n\t.open\t\t= show_traces_open,\n\t.read\t\t= seq_read,\n\t.release\t= seq_release,\n\t.llseek\t\t= seq_lseek,\n};\n\nstatic ssize_t\ntracing_cpumask_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tchar *mask_str;\n\tint len;\n\n\tlen = snprintf(NULL, 0, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask)) + 1;\n\tmask_str = kmalloc(len, GFP_KERNEL);\n\tif (!mask_str)\n\t\treturn -ENOMEM;\n\n\tlen = snprintf(mask_str, len, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask));\n\tif (len >= count) {\n\t\tcount = -EINVAL;\n\t\tgoto out_err;\n\t}\n\tcount = simple_read_from_buffer(ubuf, count, ppos, mask_str, len);\n\nout_err:\n\tkfree(mask_str);\n\n\treturn count;\n}\n\nstatic ssize_t\ntracing_cpumask_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tcpumask_var_t tracing_cpumask_new;\n\tint err, cpu;\n\n\tif (!alloc_cpumask_var(&tracing_cpumask_new, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\terr = cpumask_parse_user(ubuf, count, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\t/*\n\t\t * Increase/decrease the disabled counter if we are\n\t\t * about to flip a bit in the cpumask:\n\t\t */\n\t\tif (cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\t!cpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_inc(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_disable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t\tif (!cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\tcpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_dec(&per_cpu_ptr(tr->trace_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_enable_cpu(tr->trace_buffer.buffer, cpu);\n\t\t}\n\t}\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tcpumask_copy(tr->tracing_cpumask, tracing_cpumask_new);\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn count;\n\nerr_unlock:\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn err;\n}\n\nstatic const struct file_operations tracing_cpumask_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_cpumask_read,\n\t.write\t\t= tracing_cpumask_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic int tracing_trace_options_show(struct seq_file *m, void *v)\n{\n\tstruct tracer_opt *trace_opts;\n\tstruct trace_array *tr = m->private;\n\tu32 tracer_flags;\n\tint i;\n\n\tmutex_lock(&trace_types_lock);\n\ttracer_flags = tr->current_trace->flags->val;\n\ttrace_opts = tr->current_trace->flags->opts;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (tr->trace_flags & (1 << i))\n\t\t\tseq_printf(m, \"%s\\n\", trace_options[i]);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_options[i]);\n\t}\n\n\tfor (i = 0; trace_opts[i].name; i++) {\n\t\tif (tracer_flags & trace_opts[i].bit)\n\t\t\tseq_printf(m, \"%s\\n\", trace_opts[i].name);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_opts[i].name);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int __set_tracer_option(struct trace_array *tr,\n\t\t\t       struct tracer_flags *tracer_flags,\n\t\t\t       struct tracer_opt *opts, int neg)\n{\n\tstruct tracer *trace = tracer_flags->trace;\n\tint ret;\n\n\tret = trace->set_flag(tr, tracer_flags->val, opts->bit, !neg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (neg)\n\t\ttracer_flags->val &= ~opts->bit;\n\telse\n\t\ttracer_flags->val |= opts->bit;\n\treturn 0;\n}\n\n/* Try to assign a tracer specific option */\nstatic int set_tracer_option(struct trace_array *tr, char *cmp, int neg)\n{\n\tstruct tracer *trace = tr->current_trace;\n\tstruct tracer_flags *tracer_flags = trace->flags;\n\tstruct tracer_opt *opts = NULL;\n\tint i;\n\n\tfor (i = 0; tracer_flags->opts[i].name; i++) {\n\t\topts = &tracer_flags->opts[i];\n\n\t\tif (strcmp(cmp, opts->name) == 0)\n\t\t\treturn __set_tracer_option(tr, trace->flags, opts, neg);\n\t}\n\n\treturn -EINVAL;\n}\n\n/* Some tracers require overwrite to stay enabled */\nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set)\n{\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled)\n{\n\t/* do nothing if flag is already set */\n\tif (!!(tr->trace_flags & mask) == !!enabled)\n\t\treturn 0;\n\n\t/* Give the tracer a chance to approve the change */\n\tif (tr->current_trace->flag_changed)\n\t\tif (tr->current_trace->flag_changed(tr, mask, !!enabled))\n\t\t\treturn -EINVAL;\n\n\tif (enabled)\n\t\ttr->trace_flags |= mask;\n\telse\n\t\ttr->trace_flags &= ~mask;\n\n\tif (mask == TRACE_ITER_RECORD_CMD)\n\t\ttrace_event_enable_cmd_record(enabled);\n\n\tif (mask == TRACE_ITER_RECORD_TGID) {\n\t\tif (!tgid_map)\n\t\t\ttgid_map = kcalloc(PID_MAX_DEFAULT + 1,\n\t\t\t\t\t   sizeof(*tgid_map),\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!tgid_map) {\n\t\t\ttr->trace_flags &= ~TRACE_ITER_RECORD_TGID;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttrace_event_enable_tgid_record(enabled);\n\t}\n\n\tif (mask == TRACE_ITER_EVENT_FORK)\n\t\ttrace_event_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_FUNC_FORK)\n\t\tftrace_pid_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_OVERWRITE) {\n\t\tring_buffer_change_overwrite(tr->trace_buffer.buffer, enabled);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tring_buffer_change_overwrite(tr->max_buffer.buffer, enabled);\n#endif\n\t}\n\n\tif (mask == TRACE_ITER_PRINTK) {\n\t\ttrace_printk_start_stop_comm(enabled);\n\t\ttrace_printk_control(enabled);\n\t}\n\n\treturn 0;\n}\n\nstatic int trace_set_options(struct trace_array *tr, char *option)\n{\n\tchar *cmp;\n\tint neg = 0;\n\tint ret;\n\tsize_t orig_len = strlen(option);\n\tint len;\n\n\tcmp = strstrip(option);\n\n\tlen = str_has_prefix(cmp, \"no\");\n\tif (len)\n\t\tneg = 1;\n\n\tcmp += len;\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = match_string(trace_options, -1, cmp);\n\t/* If no option could be set, test the specific tracer options */\n\tif (ret < 0)\n\t\tret = set_tracer_option(tr, cmp, neg);\n\telse\n\t\tret = set_tracer_flag(tr, 1 << ret, !neg);\n\n\tmutex_unlock(&trace_types_lock);\n\n\t/*\n\t * If the first trailing whitespace is replaced with '\\0' by strstrip,\n\t * turn it back into a space.\n\t */\n\tif (orig_len > strlen(option))\n\t\toption[strlen(option)] = ' ';\n\n\treturn ret;\n}\n\nstatic void __init apply_trace_boot_options(void)\n{\n\tchar *buf = trace_boot_options_buf;\n\tchar *option;\n\n\twhile (true) {\n\t\toption = strsep(&buf, \",\");\n\n\t\tif (!option)\n\t\t\tbreak;\n\n\t\tif (*option)\n\t\t\ttrace_set_options(&global_trace, option);\n\n\t\t/* Put back the comma to allow this to be called again */\n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic ssize_t\ntracing_trace_options_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tret = trace_set_options(tr, buf);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_trace_options_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_trace_options_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_iter_fops = {\n\t.open\t\t= tracing_trace_options_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_trace_options_write,\n};\n\nstatic const char readme_msg[] =\n\t\"tracing mini-HOWTO:\\n\\n\"\n\t\"# echo 0 > tracing_on : quick way to disable tracing\\n\"\n\t\"# echo 1 > tracing_on : quick way to re-enable tracing\\n\\n\"\n\t\" Important files:\\n\"\n\t\"  trace\\t\\t\\t- The static contents of the buffer\\n\"\n\t\"\\t\\t\\t  To clear the buffer write into this file: echo > trace\\n\"\n\t\"  trace_pipe\\t\\t- A consuming read to see the contents of the buffer\\n\"\n\t\"  current_tracer\\t- function and latency tracers\\n\"\n\t\"  available_tracers\\t- list of configured tracers for current_tracer\\n\"\n\t\"  buffer_size_kb\\t- view and modify size of per cpu buffer\\n\"\n\t\"  buffer_total_size_kb  - view total size of all cpu buffers\\n\\n\"\n\t\"  trace_clock\\t\\t-change the clock used to order events\\n\"\n\t\"       local:   Per cpu clock but may not be synced across CPUs\\n\"\n\t\"      global:   Synced across CPUs but slows tracing down.\\n\"\n\t\"     counter:   Not a clock, but just an increment\\n\"\n\t\"      uptime:   Jiffy counter from time of boot\\n\"\n\t\"        perf:   Same clock that perf events use\\n\"\n#ifdef CONFIG_X86_64\n\t\"     x86-tsc:   TSC cycle counter\\n\"\n#endif\n\t\"\\n  timestamp_mode\\t-view the mode used to timestamp events\\n\"\n\t\"       delta:   Delta difference against a buffer-wide timestamp\\n\"\n\t\"    absolute:   Absolute (standalone) timestamp\\n\"\n\t\"\\n  trace_marker\\t\\t- Writes into this file writes into the kernel buffer\\n\"\n\t\"\\n  trace_marker_raw\\t\\t- Writes into this file writes binary data into the kernel buffer\\n\"\n\t\"  tracing_cpumask\\t- Limit which CPUs to trace\\n\"\n\t\"  instances\\t\\t- Make sub-buffers with: mkdir instances/foo\\n\"\n\t\"\\t\\t\\t  Remove sub-buffer with rmdir\\n\"\n\t\"  trace_options\\t\\t- Set format or modify how tracing happens\\n\"\n\t\"\\t\\t\\t  Disable an option by adding a suffix 'no' to the\\n\"\n\t\"\\t\\t\\t  option name\\n\"\n\t\"  saved_cmdlines_size\\t- echo command number in here to store comm-pid list\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"\\n  available_filter_functions - list of functions that can be filtered on\\n\"\n\t\"  set_ftrace_filter\\t- echo function name in here to only trace these\\n\"\n\t\"\\t\\t\\t  functions\\n\"\n\t\"\\t     accepts: func_full_name or glob-matching-pattern\\n\"\n\t\"\\t     modules: Can select a group via module\\n\"\n\t\"\\t      Format: :mod:<module-name>\\n\"\n\t\"\\t     example: echo :mod:ext3 > set_ftrace_filter\\n\"\n\t\"\\t    triggers: a command to perform when function is hit\\n\"\n\t\"\\t      Format: <function>:<trigger>[:count]\\n\"\n\t\"\\t     trigger: traceon, traceoff\\n\"\n\t\"\\t\\t      enable_event:<system>:<event>\\n\"\n\t\"\\t\\t      disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t      stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t      snapshot\\n\"\n#endif\n\t\"\\t\\t      dump\\n\"\n\t\"\\t\\t      cpudump\\n\"\n\t\"\\t     example: echo do_fault:traceoff > set_ftrace_filter\\n\"\n\t\"\\t              echo do_trap:traceoff:3 > set_ftrace_filter\\n\"\n\t\"\\t     The first one will disable tracing every time do_fault is hit\\n\"\n\t\"\\t     The second will disable tracing at most 3 times when do_trap is hit\\n\"\n\t\"\\t       The first time do trap is hit and it disables tracing, the\\n\"\n\t\"\\t       counter will decrement to 2. If tracing is already disabled,\\n\"\n\t\"\\t       the counter will not decrement. It only decrements when the\\n\"\n\t\"\\t       trigger did work\\n\"\n\t\"\\t     To remove trigger without count:\\n\"\n\t\"\\t       echo '!<function>:<trigger> > set_ftrace_filter\\n\"\n\t\"\\t     To remove trigger with a count:\\n\"\n\t\"\\t       echo '!<function>:<trigger>:0 > set_ftrace_filter\\n\"\n\t\"  set_ftrace_notrace\\t- echo function name in here to never trace.\\n\"\n\t\"\\t    accepts: func_full_name, *func_end, func_begin*, *func_middle*\\n\"\n\t\"\\t    modules: Can select a group via module command :mod:\\n\"\n\t\"\\t    Does not accept triggers\\n\"\n#endif /* CONFIG_DYNAMIC_FTRACE */\n#ifdef CONFIG_FUNCTION_TRACER\n\t\"  set_ftrace_pid\\t- Write pid(s) to only function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n#endif\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\"  set_graph_function\\t- Trace the nested calls of a function (function_graph)\\n\"\n\t\"  set_graph_notrace\\t- Do not trace the nested calls of a function (function_graph)\\n\"\n\t\"  max_graph_depth\\t- Trace a limited depth of nested calls (0 is unlimited)\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\n  snapshot\\t\\t- Like 'trace' but shows the content of the static\\n\"\n\t\"\\t\\t\\t  snapshot buffer. Read the contents for more\\n\"\n\t\"\\t\\t\\t  information\\n\"\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t\"  stack_trace\\t\\t- Shows the max stack trace when active\\n\"\n\t\"  stack_max_size\\t- Shows current max stack size that was traced\\n\"\n\t\"\\t\\t\\t  Write into this file to reset the max size (trigger a\\n\"\n\t\"\\t\\t\\t  new trace)\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"  stack_trace_filter\\t- Like set_ftrace_filter but limits what stack_trace\\n\"\n\t\"\\t\\t\\t  traces\\n\"\n#endif\n#endif /* CONFIG_STACK_TRACER */\n#ifdef CONFIG_DYNAMIC_EVENTS\n\t\"  dynamic_events\\t\\t- Add/remove/show the generic dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"  kprobe_events\\t\\t- Add/remove/show the kernel dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"  uprobe_events\\t\\t- Add/remove/show the userspace dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n\t\"\\t  accepts: event-definitions (one definition per line)\\n\"\n\t\"\\t   Format: p[:[<group>/]<event>] <place> [<args>]\\n\"\n\t\"\\t           r[maxactive][:[<group>/]<event>] <place> [<args>]\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t           s:[synthetic/]<event> <field> [<field>]\\n\"\n#endif\n\t\"\\t           -:[<group>/]<event>\\n\"\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"\\t    place: [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n  \"place (kretprobe): [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n  \"   place (uprobe): <path>:<offset>[(ref_ctr_offset)]\\n\"\n#endif\n\t\"\\t     args: <name>=fetcharg[:type]\\n\"\n\t\"\\t fetcharg: %<register>, @<address>, @<symbol>[+|-<offset>],\\n\"\n#ifdef CONFIG_HAVE_FUNCTION_ARG_ACCESS_API\n\t\"\\t           $stack<index>, $stack, $retval, $comm, $arg<N>\\n\"\n#else\n\t\"\\t           $stack<index>, $stack, $retval, $comm\\n\"\n#endif\n\t\"\\t     type: s8/16/32/64, u8/16/32/64, x8/16/32/64, string, symbol,\\n\"\n\t\"\\t           b<bit-width>@<bit-offset>/<container-size>,\\n\"\n\t\"\\t           <type>\\\\[<array-size>\\\\]\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t    field: <stype> <name>;\\n\"\n\t\"\\t    stype: u8/u16/u32/u64, s8/s16/s32/s64, pid_t,\\n\"\n\t\"\\t           [unsigned] char/int/long\\n\"\n#endif\n#endif\n\t\"  events/\\t\\t- Directory containing all trace event subsystems:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all events\\n\"\n\t\"  events/<system>/\\t- Directory containing all trace events for <system>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all <system>\\n\"\n\t\"\\t\\t\\t  events\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"  events/<system>/<event>/\\t- Directory containing control files for\\n\"\n\t\"\\t\\t\\t  <event>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of <event>\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"      trigger\\t\\t- If set, a command to perform when event is hit\\n\"\n\t\"\\t    Format: <trigger>[:count][if <filter>]\\n\"\n\t\"\\t   trigger: traceon, traceoff\\n\"\n\t\"\\t            enable_event:<system>:<event>\\n\"\n\t\"\\t            disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t            enable_hist:<system>:<event>\\n\"\n\t\"\\t            disable_hist:<system>:<event>\\n\"\n#endif\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t    stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t    snapshot\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t\\t    hist (see below)\\n\"\n#endif\n\t\"\\t   example: echo traceoff > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo traceoff:3 > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo 'enable_event:kmem:kmalloc:3 if nr_rq > 1' > \\\\\\n\"\n\t\"\\t                  events/block/block_unplug/trigger\\n\"\n\t\"\\t   The first disables tracing every time block_unplug is hit.\\n\"\n\t\"\\t   The second disables tracing the first 3 times block_unplug is hit.\\n\"\n\t\"\\t   The third enables the kmalloc event the first 3 times block_unplug\\n\"\n\t\"\\t     is hit and has value of greater than 1 for the 'nr_rq' event field.\\n\"\n\t\"\\t   Like function triggers, the counter is only decremented if it\\n\"\n\t\"\\t    enabled or disabled tracing.\\n\"\n\t\"\\t   To remove a trigger without a count:\\n\"\n\t\"\\t     echo '!<trigger> > <system>/<event>/trigger\\n\"\n\t\"\\t   To remove a trigger with a count:\\n\"\n\t\"\\t     echo '!<trigger>:0 > <system>/<event>/trigger\\n\"\n\t\"\\t   Filters can be ignored when removing a trigger.\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"      hist trigger\\t- If set, event hits are aggregated into a hash table\\n\"\n\t\"\\t    Format: hist:keys=<field1[,field2,...]>\\n\"\n\t\"\\t            [:values=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:sort=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:size=#entries]\\n\"\n\t\"\\t            [:pause][:continue][:clear]\\n\"\n\t\"\\t            [:name=histname1]\\n\"\n\t\"\\t            [if <filter>]\\n\\n\"\n\t\"\\t    When a matching event is hit, an entry is added to a hash\\n\"\n\t\"\\t    table using the key(s) and value(s) named, and the value of a\\n\"\n\t\"\\t    sum called 'hitcount' is incremented.  Keys and values\\n\"\n\t\"\\t    correspond to fields in the event's format description.  Keys\\n\"\n\t\"\\t    can be any field, or the special string 'stacktrace'.\\n\"\n\t\"\\t    Compound keys consisting of up to two fields can be specified\\n\"\n\t\"\\t    by the 'keys' keyword.  Values must correspond to numeric\\n\"\n\t\"\\t    fields.  Sort keys consisting of up to two fields can be\\n\"\n\t\"\\t    specified using the 'sort' keyword.  The sort direction can\\n\"\n\t\"\\t    be modified by appending '.descending' or '.ascending' to a\\n\"\n\t\"\\t    sort field.  The 'size' parameter can be used to specify more\\n\"\n\t\"\\t    or fewer than the default 2048 entries for the hashtable size.\\n\"\n\t\"\\t    If a hist trigger is given a name using the 'name' parameter,\\n\"\n\t\"\\t    its histogram data will be shared with other triggers of the\\n\"\n\t\"\\t    same name, and trigger hits will update this common data.\\n\\n\"\n\t\"\\t    Reading the 'hist' file for the event will dump the hash\\n\"\n\t\"\\t    table in its entirety to stdout.  If there are multiple hist\\n\"\n\t\"\\t    triggers attached to an event, there will be a table for each\\n\"\n\t\"\\t    trigger in the output.  The table displayed for a named\\n\"\n\t\"\\t    trigger will be the same as any other instance having the\\n\"\n\t\"\\t    same name.  The default format used to display a given field\\n\"\n\t\"\\t    can be modified by appending any of the following modifiers\\n\"\n\t\"\\t    to the field name, as applicable:\\n\\n\"\n\t\"\\t            .hex        display a number as a hex value\\n\"\n\t\"\\t            .sym        display an address as a symbol\\n\"\n\t\"\\t            .sym-offset display an address as a symbol and offset\\n\"\n\t\"\\t            .execname   display a common_pid as a program name\\n\"\n\t\"\\t            .syscall    display a syscall id as a syscall name\\n\"\n\t\"\\t            .log2       display log2 value rather than raw number\\n\"\n\t\"\\t            .usecs      display a common_timestamp in microseconds\\n\\n\"\n\t\"\\t    The 'pause' parameter can be used to pause an existing hist\\n\"\n\t\"\\t    trigger or to start a hist trigger but not log any events\\n\"\n\t\"\\t    until told to do so.  'continue' can be used to start or\\n\"\n\t\"\\t    restart a paused hist trigger.\\n\\n\"\n\t\"\\t    The 'clear' parameter will clear the contents of a running\\n\"\n\t\"\\t    hist trigger and leave its current paused/active state\\n\"\n\t\"\\t    unchanged.\\n\\n\"\n\t\"\\t    The enable_hist and disable_hist triggers can be used to\\n\"\n\t\"\\t    have one event conditionally start and stop another event's\\n\"\n\t\"\\t    already-attached hist trigger.  The syntax is analagous to\\n\"\n\t\"\\t    the enable_event and disable_event triggers.\\n\"\n#endif\n;\n\nstatic ssize_t\ntracing_readme_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\treadme_msg, strlen(readme_msg));\n}\n\nstatic const struct file_operations tracing_readme_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_readme_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic void *saved_tgids_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr <= &tgid_map[PID_MAX_DEFAULT]; ptr++) {\n\t\tif (trace_find_tgid(*ptr))\n\t\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_tgids_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tif (!tgid_map)\n\t\treturn NULL;\n\n\tv = &tgid_map[0];\n\twhile (l <= *pos) {\n\t\tv = saved_tgids_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_tgids_stop(struct seq_file *m, void *v)\n{\n}\n\nstatic int saved_tgids_show(struct seq_file *m, void *v)\n{\n\tint pid = (int *)v - tgid_map;\n\n\tseq_printf(m, \"%d %d\\n\", pid, trace_find_tgid(pid));\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_tgids_seq_ops = {\n\t.start\t\t= saved_tgids_start,\n\t.stop\t\t= saved_tgids_stop,\n\t.next\t\t= saved_tgids_next,\n\t.show\t\t= saved_tgids_show,\n};\n\nstatic int tracing_saved_tgids_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_tgids_seq_ops);\n}\n\n\nstatic const struct file_operations tracing_saved_tgids_fops = {\n\t.open\t\t= tracing_saved_tgids_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void *saved_cmdlines_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunsigned int *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr < &savedcmd->map_cmdline_to_pid[savedcmd->cmdline_num];\n\t     ptr++) {\n\t\tif (*ptr == -1 || *ptr == NO_CMDLINE_MAP)\n\t\t\tcontinue;\n\n\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_cmdlines_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_cmdlines_stop(struct seq_file *m, void *v)\n{\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int saved_cmdlines_show(struct seq_file *m, void *v)\n{\n\tchar buf[TASK_COMM_LEN];\n\tunsigned int *pid = v;\n\n\t__trace_find_cmdline(*pid, buf);\n\tseq_printf(m, \"%d %s\\n\", *pid, buf);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_cmdlines_seq_ops = {\n\t.start\t\t= saved_cmdlines_start,\n\t.next\t\t= saved_cmdlines_next,\n\t.stop\t\t= saved_cmdlines_stop,\n\t.show\t\t= saved_cmdlines_show,\n};\n\nstatic int tracing_saved_cmdlines_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_saved_cmdlines_seq_ops);\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_fops = {\n\t.open\t\t= tracing_saved_cmdlines_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic ssize_t\ntracing_saved_cmdlines_size_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tr = scnprintf(buf, sizeof(buf), \"%u\\n\", savedcmd->cmdline_num);\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic void free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s)\n{\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}\n\nstatic int tracing_resize_saved_cmdlines(unsigned int val)\n{\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_saved_cmdlines_size_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry or less than PID_MAX_DEFAULT */\n\tif (!val || val > PID_MAX_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = tracing_resize_saved_cmdlines((unsigned int)val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_size_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_saved_cmdlines_size_read,\n\t.write\t\t= tracing_saved_cmdlines_size_write,\n};\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic union trace_eval_map_item *\nupdate_eval_map(union trace_eval_map_item *ptr)\n{\n\tif (!ptr->map.eval_string) {\n\t\tif (ptr->tail.next) {\n\t\t\tptr = ptr->tail.next;\n\t\t\t/* Set ptr to the next real item (skip head) */\n\t\t\tptr++;\n\t\t} else\n\t\t\treturn NULL;\n\t}\n\treturn ptr;\n}\n\nstatic void *eval_map_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\t/*\n\t * Paranoid! If ptr points to end, we don't want to increment past it.\n\t * This really should never happen.\n\t */\n\tptr = update_eval_map(ptr);\n\tif (WARN_ON_ONCE(!ptr))\n\t\treturn NULL;\n\n\tptr++;\n\n\t(*pos)++;\n\n\tptr = update_eval_map(ptr);\n\n\treturn ptr;\n}\n\nstatic void *eval_map_start(struct seq_file *m, loff_t *pos)\n{\n\tunion trace_eval_map_item *v;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tv = trace_eval_maps;\n\tif (v)\n\t\tv++;\n\n\twhile (v && l < *pos) {\n\t\tv = eval_map_next(m, v, &l);\n\t}\n\n\treturn v;\n}\n\nstatic void eval_map_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic int eval_map_show(struct seq_file *m, void *v)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\tseq_printf(m, \"%s %ld (%s)\\n\",\n\t\t   ptr->map.eval_string, ptr->map.eval_value,\n\t\t   ptr->map.system);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_eval_map_seq_ops = {\n\t.start\t\t= eval_map_start,\n\t.next\t\t= eval_map_next,\n\t.stop\t\t= eval_map_stop,\n\t.show\t\t= eval_map_show,\n};\n\nstatic int tracing_eval_map_open(struct inode *inode, struct file *filp)\n{\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\treturn seq_open(filp, &tracing_eval_map_seq_ops);\n}\n\nstatic const struct file_operations tracing_eval_map_fops = {\n\t.open\t\t= tracing_eval_map_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic inline union trace_eval_map_item *\ntrace_eval_jmp_to_tail(union trace_eval_map_item *ptr)\n{\n\t/* Return tail of array given the head */\n\treturn ptr + ptr->head.length + 1;\n}\n\nstatic void\ntrace_insert_eval_map_file(struct module *mod, struct trace_eval_map **start,\n\t\t\t   int len)\n{\n\tstruct trace_eval_map **stop;\n\tstruct trace_eval_map **map;\n\tunion trace_eval_map_item *map_array;\n\tunion trace_eval_map_item *ptr;\n\n\tstop = start + len;\n\n\t/*\n\t * The trace_eval_maps contains the map plus a head and tail item,\n\t * where the head holds the module and length of array, and the\n\t * tail holds a pointer to the next list.\n\t */\n\tmap_array = kmalloc_array(len + 2, sizeof(*map_array), GFP_KERNEL);\n\tif (!map_array) {\n\t\tpr_warn(\"Unable to allocate trace eval mapping\\n\");\n\t\treturn;\n\t}\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tif (!trace_eval_maps)\n\t\ttrace_eval_maps = map_array;\n\telse {\n\t\tptr = trace_eval_maps;\n\t\tfor (;;) {\n\t\t\tptr = trace_eval_jmp_to_tail(ptr);\n\t\t\tif (!ptr->tail.next)\n\t\t\t\tbreak;\n\t\t\tptr = ptr->tail.next;\n\n\t\t}\n\t\tptr->tail.next = map_array;\n\t}\n\tmap_array->head.mod = mod;\n\tmap_array->head.length = len;\n\tmap_array++;\n\n\tfor (map = start; (unsigned long)map < (unsigned long)stop; map++) {\n\t\tmap_array->map = **map;\n\t\tmap_array++;\n\t}\n\tmemset(map_array, 0, sizeof(*map_array));\n\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic void trace_create_eval_file(struct dentry *d_tracer)\n{\n\ttrace_create_file(\"eval_map\", 0444, d_tracer,\n\t\t\t  NULL, &tracing_eval_map_fops);\n}\n\n#else /* CONFIG_TRACE_EVAL_MAP_FILE */\nstatic inline void trace_create_eval_file(struct dentry *d_tracer) { }\nstatic inline void trace_insert_eval_map_file(struct module *mod,\n\t\t\t      struct trace_eval_map **start, int len) { }\n#endif /* !CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic void trace_insert_eval_map(struct module *mod,\n\t\t\t\t  struct trace_eval_map **start, int len)\n{\n\tstruct trace_eval_map **map;\n\n\tif (len <= 0)\n\t\treturn;\n\n\tmap = start;\n\n\ttrace_event_eval_update(map, len);\n\n\ttrace_insert_eval_map_file(mod, start, len);\n}\n\nstatic ssize_t\ntracing_set_trace_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+2];\n\tint r;\n\n\tmutex_lock(&trace_types_lock);\n\tr = sprintf(buf, \"%s\\n\", tr->current_trace->name);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr)\n{\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\treturn t->init(tr);\n}\n\nstatic void set_buffer_entries(struct trace_buffer *buf, unsigned long val)\n{\n\tint cpu;\n\n\tfor_each_tracing_cpu(cpu)\n\t\tper_cpu_ptr(buf->data, cpu)->entries = val;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n/* resize @tr's buffer to the size of @size_tr's entries */\nstatic int resize_buffer_duplicate_size(struct trace_buffer *trace_buf,\n\t\t\t\t\tstruct trace_buffer *size_buf, int cpu_id)\n{\n\tint cpu, ret = 0;\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu)->entries, cpu);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tper_cpu_ptr(trace_buf->data, cpu)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu)->entries;\n\t\t}\n\t} else {\n\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu_id)->entries, cpu_id);\n\t\tif (ret == 0)\n\t\t\tper_cpu_ptr(trace_buf->data, cpu_id)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu_id)->entries;\n\t}\n\n\treturn ret;\n}\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\nstatic int __tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\tunsigned long size, int cpu)\n{\n\tint ret;\n\n\t/*\n\t * If kernel or user changes the size of the ring buffer\n\t * we use the size that was given, and we can forget about\n\t * expanding it later.\n\t */\n\tring_buffer_expanded = true;\n\n\t/* May be called before buffers are initialized */\n\tif (!tr->trace_buffer.buffer)\n\t\treturn 0;\n\n\tret = ring_buffer_resize(tr->trace_buffer.buffer, size, cpu);\n\tif (ret < 0)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (!(tr->flags & TRACE_ARRAY_FL_GLOBAL) ||\n\t    !tr->current_trace->use_max_tr)\n\t\tgoto out;\n\n\tret = ring_buffer_resize(tr->max_buffer.buffer, size, cpu);\n\tif (ret < 0) {\n\t\tint r = resize_buffer_duplicate_size(&tr->trace_buffer,\n\t\t\t\t\t\t     &tr->trace_buffer, cpu);\n\t\tif (r < 0) {\n\t\t\t/*\n\t\t\t * AARGH! We are left with different\n\t\t\t * size max buffer!!!!\n\t\t\t * The max buffer is our \"snapshot\" buffer.\n\t\t\t * When a tracer needs a snapshot (one of the\n\t\t\t * latency tracers), it swaps the max buffer\n\t\t\t * with the saved snap shot. We succeeded to\n\t\t\t * update the size of the main buffer, but failed to\n\t\t\t * update the size of the max buffer. But when we tried\n\t\t\t * to reset the main buffer to the original size, we\n\t\t\t * failed there too. This is very unlikely to\n\t\t\t * happen, but if it does, warn and kill all\n\t\t\t * tracing.\n\t\t\t */\n\t\t\tWARN_ON(1);\n\t\t\ttracing_disabled = 1;\n\t\t}\n\t\treturn ret;\n\t}\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->max_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->max_buffer.data, cpu)->entries = size;\n\n out:\n#endif /* CONFIG_TRACER_MAX_TRACE */\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\tset_buffer_entries(&tr->trace_buffer, size);\n\telse\n\t\tper_cpu_ptr(tr->trace_buffer.data, cpu)->entries = size;\n\n\treturn ret;\n}\n\nstatic ssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\t  unsigned long size, int cpu_id)\n{\n\tint ret = size;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu_id != RING_BUFFER_ALL_CPUS) {\n\t\t/* make sure, this cpu is enabled in the mask */\n\t\tif (!cpumask_test_cpu(cpu_id, tracing_buffer_mask)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = __tracing_resize_ring_buffer(tr, size, cpu_id);\n\tif (ret < 0)\n\t\tret = -ENOMEM;\n\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n\n/**\n * tracing_update_buffers - used by tracing facility to expand ring buffers\n *\n * To save on memory when the tracing is never used on a system with it\n * configured in. The ring buffers are set to a minimum size. But once\n * a user starts to use the tracing facility, then they need to grow\n * to their default size.\n *\n * This function is to be called when a tracer is about to be used.\n */\nint tracing_update_buffers(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tif (!ring_buffer_expanded)\n\t\tret = __tracing_resize_ring_buffer(&global_trace, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct trace_option_dentry;\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer);\n\n/*\n * Used to clear out the tracer before deletion of an instance.\n * Must have trace_types_lock held.\n */\nstatic void tracing_set_nop(struct trace_array *tr)\n{\n\tif (tr->current_trace == &nop_trace)\n\t\treturn;\n\t\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\ttr->current_trace = &nop_trace;\n}\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t)\n{\n\t/* Only enable if the directory has been created already. */\n\tif (!tr->dir)\n\t\treturn;\n\n\tcreate_trace_option_files(tr, t);\n}\n\nstatic int tracing_set_tracer(struct trace_array *tr, const char *buf)\n{\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_rcu */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_rcu();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_set_trace_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+1];\n\tint i;\n\tsize_t ret;\n\tint err;\n\n\tret = cnt;\n\n\tif (cnt > MAX_TRACER_SIZE)\n\t\tcnt = MAX_TRACER_SIZE;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\t/* strip ending whitespace. */\n\tfor (i = cnt - 1; i > 0 && isspace(buf[i]); i--)\n\t\tbuf[i] = 0;\n\n\terr = tracing_set_tracer(tr, buf);\n\tif (err)\n\t\treturn err;\n\n\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_nsecs_read(unsigned long *ptr, char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tr = snprintf(buf, sizeof(buf), \"%ld\\n\",\n\t\t     *ptr == (unsigned long)-1 ? -1 : nsecs_to_usecs(*ptr));\n\tif (r > sizeof(buf))\n\t\tr = sizeof(buf);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_nsecs_write(unsigned long *ptr, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t*ptr = val * 1000;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_thresh_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(&tracing_thresh, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_thresh_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\tret = tracing_nsecs_write(&tracing_thresh, ubuf, cnt, ppos);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (tr->current_trace->update_thresh) {\n\t\tret = tr->current_trace->update_thresh(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = cnt;\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\nstatic ssize_t\ntracing_max_lat_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(filp->private_data, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_max_lat_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_write(filp->private_data, ubuf, cnt, ppos);\n}\n\n#endif\n\nstatic int tracing_open_pipe(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret = 0;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\n\t/* create a buffer to store the information to pass to userspace */\n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter) {\n\t\tret = -ENOMEM;\n\t\t__trace_array_put(tr);\n\t\tgoto out;\n\t}\n\n\ttrace_seq_init(&iter->seq);\n\titer->trace = tr->current_trace;\n\n\tif (!alloc_cpumask_var(&iter->started, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t/* trace pipe does not show start of buffer */\n\tcpumask_setall(iter->started);\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\titer->tr = tr;\n\titer->trace_buffer = &tr->trace_buffer;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\tfilp->private_data = iter;\n\n\tif (iter->trace->pipe_open)\n\t\titer->trace->pipe_open(iter);\n\n\tnonseekable_open(inode, filp);\n\n\ttr->current_trace->ref++;\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n\nfail:\n\tkfree(iter->trace);\n\tkfree(iter);\n\t__trace_array_put(tr);\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_release_pipe(struct inode *inode, struct file *file)\n{\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic __poll_t\ntrace_poll(struct trace_iterator *iter, struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_array *tr = iter->tr;\n\n\t/* Iterators are static, they should be filled or empty */\n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\tif (tr->trace_flags & TRACE_ITER_BLOCK)\n\t\t/*\n\t\t * Always select as readable when in blocking mode\n\t\t */\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\telse\n\t\treturn ring_buffer_poll_wait(iter->trace_buffer->buffer, iter->cpu_file,\n\t\t\t\t\t     filp, poll_table);\n}\n\nstatic __poll_t\ntracing_poll_pipe(struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\n/* Must be called with iter->mutex held. */\nstatic int tracing_wait_pipe(struct file *filp)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tint ret;\n\n\twhile (trace_empty(iter)) {\n\n\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t/*\n\t\t * We block until we read something and tracing is disabled.\n\t\t * We still block if tracing is disabled, but we have never\n\t\t * read anything. This allows a user to cat this file, and\n\t\t * then enable tracing. But after we have read something,\n\t\t * we give an EOF when tracing is again disabled.\n\t\t *\n\t\t * iter->pos will be 0 if we haven't read anything.\n\t\t */\n\t\tif (!tracer_tracing_is_on(iter->tr) && iter->pos)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&iter->mutex);\n\n\t\tret = wait_on_pipe(iter, 0);\n\n\t\tmutex_lock(&iter->mutex);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\n/*\n * Consumer reader.\n */\nstatic ssize_t\ntracing_read_pipe(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tssize_t sret;\n\n\t/*\n\t * Avoid more than one consumer on a single file descriptor\n\t * This is just a matter of traces coherency, the ring buffer itself\n\t * is protected.\n\t */\n\tmutex_lock(&iter->mutex);\n\n\t/* return any leftover data */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (sret != -EBUSY)\n\t\tgoto out;\n\n\ttrace_seq_init(&iter->seq);\n\n\tif (iter->trace->read) {\n\t\tsret = iter->trace->read(iter, filp, ubuf, cnt, ppos);\n\t\tif (sret)\n\t\t\tgoto out;\n\t}\n\nwaitagain:\n\tsret = tracing_wait_pipe(filp);\n\tif (sret <= 0)\n\t\tgoto out;\n\n\t/* stop when tracing is finished */\n\tif (trace_empty(iter)) {\n\t\tsret = 0;\n\t\tgoto out;\n\t}\n\n\tif (cnt >= PAGE_SIZE)\n\t\tcnt = PAGE_SIZE - 1;\n\n\t/* reset all but tr, trace, and overruns */\n\tmemset(&iter->seq, 0,\n\t       sizeof(struct trace_iterator) -\n\t       offsetof(struct trace_iterator, seq));\n\tcpumask_clear(iter->started);\n\titer->pos = -1;\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\twhile (trace_find_next_entry_inc(iter) != NULL) {\n\t\tenum print_line_t ret;\n\t\tint save_len = iter->seq.seq.len;\n\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\t/* don't print partial lines */\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\n\t\tif (trace_seq_used(&iter->seq) >= cnt)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Setting the full flag means we reached the trace_seq buffer\n\t\t * size and we should leave by partial output condition above.\n\t\t * One of the trace_seq_* functions is not used properly.\n\t\t */\n\t\tWARN_ONCE(iter->seq.full, \"full flag set for trace type %d\",\n\t\t\t  iter->ent->type);\n\t}\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\n\t/* Now copy what we have to the user */\n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (iter->seq.seq.readpos >= trace_seq_used(&iter->seq))\n\t\ttrace_seq_init(&iter->seq);\n\n\t/*\n\t * If there was nothing to send to user, in spite of consuming trace\n\t * entries, go back to wait for more entries.\n\t */\n\tif (sret == -EBUSY)\n\t\tgoto waitagain;\n\nout:\n\tmutex_unlock(&iter->mutex);\n\n\treturn sret;\n}\n\nstatic void tracing_spd_release_pipe(struct splice_pipe_desc *spd,\n\t\t\t\t     unsigned int idx)\n{\n\t__free_page(spd->pages[idx]);\n}\n\nstatic const struct pipe_buf_operations tracing_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= generic_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= generic_pipe_buf_get,\n};\n\nstatic size_t\ntracing_fill_pipe_page(size_t rem, struct trace_iterator *iter)\n{\n\tsize_t count;\n\tint save_len;\n\tint ret;\n\n\t/* Seq buffer is page-sized, exactly what we need. */\n\tfor (;;) {\n\t\tsave_len = iter->seq.seq.len;\n\t\tret = print_trace_line(iter);\n\n\t\tif (trace_seq_has_overflowed(&iter->seq)) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * This should not be hit, because it should only\n\t\t * be set if the iter->seq overflowed. But check it\n\t\t * anyway to be safe.\n\t\t */\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tcount = trace_seq_used(&iter->seq) - save_len;\n\t\tif (rem < count) {\n\t\t\trem = 0;\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\t\trem -= count;\n\t\tif (!trace_find_next_entry_inc(iter))\t{\n\t\t\trem = 0;\n\t\t\titer->ent = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rem;\n}\n\nstatic ssize_t tracing_splice_read_pipe(struct file *filp,\n\t\t\t\t\tloff_t *ppos,\n\t\t\t\t\tstruct pipe_inode_info *pipe,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct trace_iterator *iter = filp->private_data;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages\t= 0, /* This gets updated below. */\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &tracing_pipe_buf_ops,\n\t\t.spd_release\t= tracing_spd_release_pipe,\n\t};\n\tssize_t ret;\n\tsize_t rem;\n\tunsigned int i;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&iter->mutex);\n\n\tif (iter->trace->splice_read) {\n\t\tret = iter->trace->splice_read(iter, filp,\n\t\t\t\t\t       ppos, pipe, len, flags);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tret = tracing_wait_pipe(filp);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\n\tif (!iter->ent && !trace_find_next_entry_inc(iter)) {\n\t\tret = -EFAULT;\n\t\tgoto out_err;\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\n\t/* Fill as many pages as possible. */\n\tfor (i = 0, rem = len; i < spd.nr_pages_max && rem; i++) {\n\t\tspd.pages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!spd.pages[i])\n\t\t\tbreak;\n\n\t\trem = tracing_fill_pipe_page(rem, iter);\n\n\t\t/* Copy the data into the page, so we can start over. */\n\t\tret = trace_seq_to_buffer(&iter->seq,\n\t\t\t\t\t  page_address(spd.pages[i]),\n\t\t\t\t\t  trace_seq_used(&iter->seq));\n\t\tif (ret < 0) {\n\t\t\t__free_page(spd.pages[i]);\n\t\t\tbreak;\n\t\t}\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].len = trace_seq_used(&iter->seq);\n\n\t\ttrace_seq_init(&iter->seq);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\tmutex_unlock(&iter->mutex);\n\n\tspd.nr_pages = i;\n\n\tif (i)\n\t\tret = splice_to_pipe(pipe, &spd);\n\telse\n\t\tret = 0;\nout:\n\tsplice_shrink_spd(&spd);\n\treturn ret;\n\nout_err:\n\tmutex_unlock(&iter->mutex);\n\tgoto out;\n}\n\nstatic ssize_t\ntracing_entries_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tint cpu = tracing_get_cpu(inode);\n\tchar buf[64];\n\tint r = 0;\n\tssize_t ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tint cpu, buf_size_same;\n\t\tunsigned long size;\n\n\t\tsize = 0;\n\t\tbuf_size_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (size == 0)\n\t\t\t\tsize = per_cpu_ptr(tr->trace_buffer.data, cpu)->entries;\n\t\t\tif (size != per_cpu_ptr(tr->trace_buffer.data, cpu)->entries) {\n\t\t\t\tbuf_size_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (buf_size_same) {\n\t\t\tif (!ring_buffer_expanded)\n\t\t\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\",\n\t\t\t\t\t    size >> 10,\n\t\t\t\t\t    trace_buf_size >> 10);\n\t\t\telse\n\t\t\t\tr = sprintf(buf, \"%lu\\n\", size >> 10);\n\t\t} else\n\t\t\tr = sprintf(buf, \"X\\n\");\n\t} else\n\t\tr = sprintf(buf, \"%lu\\n\", per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_entries_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t/* must have at least 1 entry */\n\tif (!val)\n\t\treturn -EINVAL;\n\n\t/* value is in KB */\n\tval <<= 10;\n\tret = tracing_resize_ring_buffer(tr, val, tracing_get_cpu(inode));\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_total_entries_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r, cpu;\n\tunsigned long size = 0, expanded_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\tsize += per_cpu_ptr(tr->trace_buffer.data, cpu)->entries >> 10;\n\t\tif (!ring_buffer_expanded)\n\t\t\texpanded_size += trace_buf_size >> 10;\n\t}\n\tif (ring_buffer_expanded)\n\t\tr = sprintf(buf, \"%lu\\n\", size);\n\telse\n\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\", size, expanded_size);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_free_buffer_write(struct file *filp, const char __user *ubuf,\n\t\t\t  size_t cnt, loff_t *ppos)\n{\n\t/*\n\t * There is no need to read what the user has written, this function\n\t * is just to make sure that there is no error when \"echo\" is used\n\t */\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int\ntracing_free_buffer_release(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\t/* disable tracing ? */\n\tif (tr->trace_flags & TRACE_ITER_STOP_ON_FREE)\n\t\ttracer_tracing_off(tr);\n\t/* resize the ring buffer to 0 */\n\ttracing_resize_ring_buffer(tr, 0, RING_BUFFER_ALL_CPUS);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_mark_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tconst char faulted[] = \"<faulted>\";\n\tssize_t written;\n\tint size;\n\tint len;\n\n/* Used in tracing_mark_raw_write() as well */\n#define FAULTED_SIZE (sizeof(faulted) - 1) /* '\\0' is already accounted for */\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt + 2; /* add '\\0' and possible '\\n' */\n\n\t/* If less than \"<faulted>\", then make sure we can still add that */\n\tif (cnt < FAULTED_SIZE)\n\t\tsize += FAULTED_SIZE - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (unlikely(!event))\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = _THIS_IP_;\n\n\tlen = __copy_from_user_inatomic(&entry->buf, ubuf, cnt);\n\tif (len) {\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\tcnt = FAULTED_SIZE;\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\tlen = cnt;\n\n\tif (tr->trace_marker_file && !list_empty(&tr->trace_marker_file->triggers)) {\n\t\t/* do not add \\n before testing triggers, but add \\0 */\n\t\tentry->buf[cnt] = '\\0';\n\t\ttt = event_triggers_call(tr->trace_marker_file, entry, event);\n\t}\n\n\tif (entry->buf[cnt - 1] != '\\n') {\n\t\tentry->buf[cnt] = '\\n';\n\t\tentry->buf[cnt + 1] = '\\0';\n\t} else\n\t\tentry->buf[cnt] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (tt)\n\t\tevent_triggers_post_call(tr->trace_marker_file, tt);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\n/* Limit it for now to 3K (including tag) */\n#define RAW_DATA_MAX_SIZE (1024*3)\n\nstatic ssize_t\ntracing_mark_raw_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct raw_data_entry *entry;\n\tconst char faulted[] = \"<faulted>\";\n\tunsigned long irq_flags;\n\tssize_t written;\n\tint size;\n\tint len;\n\n#define FAULT_SIZE_ID (FAULTED_SIZE + sizeof(int))\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\t/* The marker must at least have a tag id */\n\tif (cnt < sizeof(unsigned int) || cnt > RAW_DATA_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tlocal_save_flags(irq_flags);\n\tsize = sizeof(*entry) + cnt;\n\tif (cnt < FAULT_SIZE_ID)\n\t\tsize += FAULT_SIZE_ID - cnt;\n\n\tbuffer = tr->trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,\n\t\t\t\t\t    irq_flags, preempt_count());\n\tif (!event)\n\t\t/* Ring buffer disabled, return as if not open for write */\n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\n\tlen = __copy_from_user_inatomic(&entry->id, ubuf, cnt);\n\tif (len) {\n\t\tentry->id = -1;\n\t\tmemcpy(&entry->buf, faulted, FAULTED_SIZE);\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (written > 0)\n\t\t*fpos += written;\n\n\treturn written;\n}\n\nstatic int tracing_clock_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++)\n\t\tseq_printf(m,\n\t\t\t\"%s%s%s%s\", i ? \" \" : \"\",\n\t\t\ti == tr->clock_id ? \"[\" : \"\", trace_clocks[i].name,\n\t\t\ti == tr->clock_id ? \"]\" : \"\");\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nint tracing_set_clock(struct trace_array *tr, const char *clockstr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++) {\n\t\tif (strcmp(trace_clocks[i].name, clockstr) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(trace_clocks))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->clock_id = i;\n\n\tring_buffer_set_clock(tr->trace_buffer.buffer, trace_clocks[i].func);\n\n\t/*\n\t * New clock may not be consistent with the previous clock.\n\t * Reset the buffer so that it doesn't have incomparable timestamps.\n\t */\n\ttracing_reset_online_cpus(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_clock(tr->max_buffer.buffer, trace_clocks[i].func);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t tracing_clock_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *fpos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tconst char *clockstr;\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tclockstr = strstrip(buf);\n\n\tret = tracing_set_clock(tr, clockstr);\n\tif (ret)\n\t\treturn ret;\n\n\t*fpos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_clock_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_clock_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic int tracing_time_stamp_mode_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (ring_buffer_time_stamp_abs(tr->trace_buffer.buffer))\n\t\tseq_puts(m, \"delta [absolute]\\n\");\n\telse\n\t\tseq_puts(m, \"[delta] absolute\\n\");\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int tracing_time_stamp_mode_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr))\n\t\treturn -ENODEV;\n\n\tret = single_open(file, tracing_time_stamp_mode_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nint tracing_set_time_stamp_abs(struct trace_array *tr, bool abs)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (abs && tr->time_stamp_abs_ref++)\n\t\tgoto out;\n\n\tif (!abs) {\n\t\tif (WARN_ON_ONCE(!tr->time_stamp_abs_ref)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (--tr->time_stamp_abs_ref)\n\t\t\tgoto out;\n\t}\n\n\tring_buffer_set_time_stamp_abs(tr->trace_buffer.buffer, abs);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_time_stamp_abs(tr->max_buffer.buffer, abs);\n#endif\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct ftrace_buffer_info {\n\tstruct trace_iterator\titer;\n\tvoid\t\t\t*spare;\n\tunsigned int\t\tspare_cpu;\n\tunsigned int\t\tread;\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic int tracing_snapshot_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tstruct seq_file *m;\n\tint ret = 0;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, true);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t} else {\n\t\t/* Writes still need the seq_file to hold the private data */\n\t\tret = -ENOMEM;\n\t\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\t\tif (!m)\n\t\t\tgoto out;\n\t\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\t\tif (!iter) {\n\t\t\tkfree(m);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\n\t\titer->tr = tr;\n\t\titer->trace_buffer = &tr->max_buffer;\n\t\titer->cpu_file = tracing_get_cpu(inode);\n\t\tm->private = iter;\n\t\tfile->private_data = m;\n\t}\nout:\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long val;\n\tint ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tswitch (val) {\n\tcase 0:\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (tr->allocated_snapshot)\n\t\t\tfree_snapshot(tr);\n\t\tbreak;\n\tcase 1:\n/* Only allow per-cpu swap if the ring buffer supports it */\n#ifndef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tif (!tr->allocated_snapshot) {\n\t\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tlocal_irq_disable();\n\t\t/* Now, we're going to swap */\n\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\tupdate_max_tr(tr, current, smp_processor_id());\n\t\telse\n\t\t\tupdate_max_tr_single(tr, current, iter->cpu_file);\n\t\tlocal_irq_enable();\n\t\tbreak;\n\tdefault:\n\t\tif (tr->allocated_snapshot) {\n\t\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\t\ttracing_reset_online_cpus(&tr->max_buffer);\n\t\t\telse\n\t\t\t\ttracing_reset(&tr->max_buffer, iter->cpu_file);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (ret >= 0) {\n\t\t*ppos += cnt;\n\t\tret = cnt;\n\t}\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_snapshot_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = file->private_data;\n\tint ret;\n\n\tret = tracing_release(inode, file);\n\n\tif (file->f_mode & FMODE_READ)\n\t\treturn ret;\n\n\t/* If write only, the seq_file is just a stub */\n\tif (m)\n\t\tkfree(m->private);\n\tkfree(m);\n\n\treturn 0;\n}\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp);\nstatic ssize_t tracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos);\nstatic int tracing_buffers_release(struct inode *inode, struct file *file);\nstatic ssize_t tracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t   struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\nstatic int snapshot_raw_open(struct inode *inode, struct file *filp)\n{\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tret = tracing_buffers_open(inode, filp);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tinfo = filp->private_data;\n\n\tif (info->iter.trace->use_max_tr) {\n\t\ttracing_buffers_release(inode, filp);\n\t\treturn -EBUSY;\n\t}\n\n\tinfo->iter.snapshot = true;\n\tinfo->iter.trace_buffer = &info->iter.tr->max_buffer;\n\n\treturn ret;\n}\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\n\nstatic const struct file_operations tracing_thresh_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_thresh_read,\n\t.write\t\t= tracing_thresh_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\nstatic const struct file_operations tracing_max_lat_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_max_lat_read,\n\t.write\t\t= tracing_max_lat_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif\n\nstatic const struct file_operations set_tracer_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_set_trace_read,\n\t.write\t\t= tracing_set_trace_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic const struct file_operations tracing_pipe_fops = {\n\t.open\t\t= tracing_open_pipe,\n\t.poll\t\t= tracing_poll_pipe,\n\t.read\t\t= tracing_read_pipe,\n\t.splice_read\t= tracing_splice_read_pipe,\n\t.release\t= tracing_release_pipe,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic const struct file_operations tracing_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_entries_read,\n\t.write\t\t= tracing_entries_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_total_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_total_entries_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_free_buffer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_free_buffer_write,\n\t.release\t= tracing_free_buffer_release,\n};\n\nstatic const struct file_operations tracing_mark_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_mark_raw_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_mark_raw_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations trace_clock_fops = {\n\t.open\t\t= tracing_clock_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_clock_write,\n};\n\nstatic const struct file_operations trace_time_stamp_mode_fops = {\n\t.open\t\t= tracing_time_stamp_mode_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic const struct file_operations snapshot_fops = {\n\t.open\t\t= tracing_snapshot_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_snapshot_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_snapshot_release,\n};\n\nstatic const struct file_operations snapshot_raw_fops = {\n\t.open\t\t= snapshot_raw_open,\n\t.read\t\t= tracing_buffers_read,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\n#endif /* CONFIG_TRACER_SNAPSHOT */\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\ttrace_array_put(tr);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tinfo->iter.tr\t\t= tr;\n\tinfo->iter.cpu_file\t= tracing_get_cpu(inode);\n\tinfo->iter.trace\t= tr->current_trace;\n\tinfo->iter.trace_buffer = &tr->trace_buffer;\n\tinfo->spare\t\t= NULL;\n\t/* Force reading ring buffer for first read */\n\tinfo->read\t\t= (unsigned int)-1;\n\n\tfilp->private_data = info;\n\n\ttr->current_trace->ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = nonseekable_open(inode, filp);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic __poll_t\ntracing_buffers_poll(struct file *filp, poll_table *poll_table)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\nstatic ssize_t\ntracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tssize_t ret = 0;\n\tssize_t size;\n\n\tif (!count)\n\t\treturn 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (!info->spare) {\n\t\tinfo->spare = ring_buffer_alloc_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t\t\t  iter->cpu_file);\n\t\tif (IS_ERR(info->spare)) {\n\t\t\tret = PTR_ERR(info->spare);\n\t\t\tinfo->spare = NULL;\n\t\t} else {\n\t\t\tinfo->spare_cpu = iter->cpu_file;\n\t\t}\n\t}\n\tif (!info->spare)\n\t\treturn ret;\n\n\t/* Do we have previous read data to read? */\n\tif (info->read < PAGE_SIZE)\n\t\tgoto read;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tret = ring_buffer_read_page(iter->trace_buffer->buffer,\n\t\t\t\t    &info->spare,\n\t\t\t\t    count,\n\t\t\t\t    iter->cpu_file, 0);\n\ttrace_access_unlock(iter->cpu_file);\n\n\tif (ret < 0) {\n\t\tif (trace_empty(iter)) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tret = wait_on_pipe(iter, 0);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tgoto again;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinfo->read = 0;\n read:\n\tsize = PAGE_SIZE - info->read;\n\tif (size > count)\n\t\tsize = count;\n\n\tret = copy_to_user(ubuf, info->spare + info->read, size);\n\tif (ret == size)\n\t\treturn -EFAULT;\n\n\tsize -= ret;\n\n\t*ppos += size;\n\tinfo->read += size;\n\n\treturn size;\n}\n\nstatic int tracing_buffers_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->tr->current_trace->ref--;\n\n\t__trace_array_put(iter->tr);\n\n\tif (info->spare)\n\t\tring_buffer_free_read_page(iter->trace_buffer->buffer,\n\t\t\t\t\t   info->spare_cpu, info->spare);\n\tkfree(info);\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstruct buffer_ref {\n\tstruct ring_buffer\t*buffer;\n\tvoid\t\t\t*page;\n\tint\t\t\tcpu;\n\tint\t\t\tref;\n};\n\nstatic void buffer_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tbuf->private = 0;\n}\n\nstatic bool buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (ref->ref > INT_MAX/2)\n\t\treturn false;\n\n\tref->ref++;\n\treturn true;\n}\n\n/* Pipe buffer operations for a buffer. */\nstatic const struct pipe_buf_operations buffer_pipe_buf_ops = {\n\t.can_merge\t\t= 0,\n\t.confirm\t\t= generic_pipe_buf_confirm,\n\t.release\t\t= buffer_pipe_buf_release,\n\t.steal\t\t\t= generic_pipe_buf_steal,\n\t.get\t\t\t= buffer_pipe_buf_get,\n};\n\n/*\n * Callback from splice_to_pipe(), if we need to release some pages\n * at the end of the spd in case we error'ed out in filling the pipe.\n */\nstatic void buffer_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tstruct buffer_ref *ref =\n\t\t(struct buffer_ref *)spd->partial[i].private;\n\n\tif (--ref->ref)\n\t\treturn;\n\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n\tspd->partial[i].private = 0;\n}\n\nstatic ssize_t\ntracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe, size_t len,\n\t\t\t    unsigned int flags)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &buffer_pipe_buf_ops,\n\t\t.spd_release\t= buffer_spd_release,\n\t};\n\tstruct buffer_ref *ref;\n\tint entries, i;\n\tssize_t ret = 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (*ppos & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (len & (PAGE_SIZE - 1)) {\n\t\tif (len < PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tlen &= PAGE_MASK;\n\t}\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\n\tfor (i = 0; i < spd.nr_pages_max && len && entries; i++, len -= PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint r;\n\n\t\tref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!ref) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tref->ref = 1;\n\t\tref->buffer = iter->trace_buffer->buffer;\n\t\tref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);\n\t\tif (IS_ERR(ref->page)) {\n\t\t\tret = PTR_ERR(ref->page);\n\t\t\tref->page = NULL;\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\t\tref->cpu = iter->cpu_file;\n\n\t\tr = ring_buffer_read_page(ref->buffer, &ref->page,\n\t\t\t\t\t  len, iter->cpu_file, 1);\n\t\tif (r < 0) {\n\t\t\tring_buffer_free_read_page(ref->buffer, ref->cpu,\n\t\t\t\t\t\t   ref->page);\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage = virt_to_page(ref->page);\n\n\t\tspd.pages[i] = page;\n\t\tspd.partial[i].len = PAGE_SIZE;\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].private = (unsigned long)ref;\n\t\tspd.nr_pages++;\n\t\t*ppos += PAGE_SIZE;\n\n\t\tentries = ring_buffer_entries_cpu(iter->trace_buffer->buffer, iter->cpu_file);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\tspd.nr_pages = i;\n\n\t/* did we read anything? */\n\tif (!spd.nr_pages) {\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = -EAGAIN;\n\t\tif ((file->f_flags & O_NONBLOCK) || (flags & SPLICE_F_NONBLOCK))\n\t\t\tgoto out;\n\n\t\tret = wait_on_pipe(iter, iter->tr->buffer_percent);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\n\tret = splice_to_pipe(pipe, &spd);\nout:\n\tsplice_shrink_spd(&spd);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_buffers_fops = {\n\t.open\t\t= tracing_buffers_open,\n\t.read\t\t= tracing_buffers_read,\n\t.poll\t\t= tracing_buffers_poll,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic ssize_t\ntracing_stats_read(struct file *filp, char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_buffer *trace_buf = &tr->trace_buffer;\n\tint cpu = tracing_get_cpu(inode);\n\tstruct trace_seq *s;\n\tunsigned long cnt;\n\tunsigned long long t;\n\tunsigned long usec_rem;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tcnt = ring_buffer_entries_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"entries: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_commit_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"commit overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_bytes_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"bytes: %ld\\n\", cnt);\n\n\tif (trace_clocks[tr->clock_id].in_ns) {\n\t\t/* local or global for trace_clock */\n\t\tt = ns2usecs(ring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"oldest event ts: %5llu.%06lu\\n\",\n\t\t\t\t\t\t\t\tt, usec_rem);\n\n\t\tt = ns2usecs(ring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"now ts: %5llu.%06lu\\n\", t, usec_rem);\n\t} else {\n\t\t/* counter or tsc mode for trace_clock */\n\t\ttrace_seq_printf(s, \"oldest event ts: %llu\\n\",\n\t\t\t\tring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\n\t\ttrace_seq_printf(s, \"now ts: %llu\\n\",\n\t\t\t\tring_buffer_time_stamp(trace_buf->buffer, cpu));\n\t}\n\n\tcnt = ring_buffer_dropped_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"dropped events: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_read_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"read events: %ld\\n\", cnt);\n\n\tcount = simple_read_from_buffer(ubuf, count, ppos,\n\t\t\t\t\ts->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn count;\n}\n\nstatic const struct file_operations tracing_stats_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_stats_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic ssize_t\ntracing_read_dyn_info(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long *p = filp->private_data;\n\tchar buf[64]; /* Not too big for a shallow stack */\n\tint r;\n\n\tr = scnprintf(buf, 63, \"%ld\", *p);\n\tbuf[r++] = '\\n';\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic const struct file_operations tracing_dyn_info_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_read_dyn_info,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif /* CONFIG_DYNAMIC_FTRACE */\n\n#if defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE)\nstatic void\nftrace_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\tstruct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\tvoid *data)\n{\n\ttracing_snapshot_instance(tr);\n}\n\nstatic void\nftrace_count_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\t      struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t      void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count) {\n\n\t\tif (*count <= 0)\n\t\t\treturn;\n\n\t\t(*count)--;\n\t}\n\n\ttracing_snapshot_instance(tr);\n}\n\nstatic int\nftrace_snapshot_print(struct seq_file *m, unsigned long ip,\n\t\t      struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_puts(m, \"snapshot\");\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count)\n\t\tseq_printf(m, \":count=%ld\\n\", *count);\n\telse\n\t\tseq_puts(m, \":unlimited\\n\");\n\n\treturn 0;\n}\n\nstatic int\nftrace_snapshot_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENOMEM;\n\t\t*data = mapper;\n\t}\n\n\treturn ftrace_func_mapper_add_ip(mapper, ip, init_data);\n}\n\nstatic void\nftrace_snapshot_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, NULL);\n\t\treturn;\n\t}\n\n\tftrace_func_mapper_remove_ip(mapper, ip);\n}\n\nstatic struct ftrace_probe_ops snapshot_probe_ops = {\n\t.func\t\t\t= ftrace_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n};\n\nstatic struct ftrace_probe_ops snapshot_count_probe_ops = {\n\t.func\t\t\t= ftrace_count_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n\t.init\t\t\t= ftrace_snapshot_init,\n\t.free\t\t\t= ftrace_snapshot_free,\n};\n\nstatic int\nftrace_trace_snapshot_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t\t       char *glob, char *cmd, char *param, int enable)\n{\n\tstruct ftrace_probe_ops *ops;\n\tvoid *count = (void *)-1;\n\tchar *number;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t/* hash funcs only work with set_ftrace_filter */\n\tif (!enable)\n\t\treturn -EINVAL;\n\n\tops = param ? &snapshot_count_probe_ops :  &snapshot_probe_ops;\n\n\tif (glob[0] == '!')\n\t\treturn unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tif (!strlen(number))\n\t\tgoto out_reg;\n\n\t/*\n\t * We use the callback data field (which is a pointer)\n\t * as our counter.\n\t */\n\tret = kstrtoul(number, 0, (unsigned long *)&count);\n\tif (ret)\n\t\treturn ret;\n\n out_reg:\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, count);\n\n out:\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct ftrace_func_command ftrace_snapshot_cmd = {\n\t.name\t\t\t= \"snapshot\",\n\t.func\t\t\t= ftrace_trace_snapshot_callback,\n};\n\nstatic __init int register_snapshot_cmd(void)\n{\n\treturn register_ftrace_command(&ftrace_snapshot_cmd);\n}\n#else\nstatic inline __init int register_snapshot_cmd(void) { return 0; }\n#endif /* defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE) */\n\nstatic struct dentry *tracing_get_dentry(struct trace_array *tr)\n{\n\tif (WARN_ON(!tr->dir))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/* Top directory uses NULL as the parent */\n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn NULL;\n\n\t/* All sub buffers have a descriptor */\n\treturn tr->dir;\n}\n\nstatic struct dentry *tracing_dentry_percpu(struct trace_array *tr, int cpu)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->percpu_dir)\n\t\treturn tr->percpu_dir;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->percpu_dir = tracefs_create_dir(\"per_cpu\", d_tracer);\n\n\tWARN_ONCE(!tr->percpu_dir,\n\t\t  \"Could not create tracefs directory 'per_cpu/%d'\\n\", cpu);\n\n\treturn tr->percpu_dir;\n}\n\nstatic struct dentry *\ntrace_create_cpu_file(const char *name, umode_t mode, struct dentry *parent,\n\t\t      void *data, long cpu, const struct file_operations *fops)\n{\n\tstruct dentry *ret = trace_create_file(name, mode, parent, data, fops);\n\n\tif (ret) /* See tracing_get_cpu() */\n\t\td_inode(ret)->i_cdev = (void *)(cpu + 1);\n\treturn ret;\n}\n\nstatic void\ntracing_init_tracefs_percpu(struct trace_array *tr, long cpu)\n{\n\tstruct dentry *d_percpu = tracing_dentry_percpu(tr, cpu);\n\tstruct dentry *d_cpu;\n\tchar cpu_dir[30]; /* 30 characters should be more than enough */\n\n\tif (!d_percpu)\n\t\treturn;\n\n\tsnprintf(cpu_dir, 30, \"cpu%ld\", cpu);\n\td_cpu = tracefs_create_dir(cpu_dir, d_percpu);\n\tif (!d_cpu) {\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", cpu_dir);\n\t\treturn;\n\t}\n\n\t/* per cpu trace_pipe */\n\ttrace_create_cpu_file(\"trace_pipe\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_pipe_fops);\n\n\t/* per cpu trace */\n\ttrace_create_cpu_file(\"trace\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &tracing_fops);\n\n\ttrace_create_cpu_file(\"trace_pipe_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_buffers_fops);\n\n\ttrace_create_cpu_file(\"stats\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_stats_fops);\n\n\ttrace_create_cpu_file(\"buffer_size_kb\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &tracing_entries_fops);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_cpu_file(\"snapshot\", 0644, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_fops);\n\n\ttrace_create_cpu_file(\"snapshot_raw\", 0444, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_raw_fops);\n#endif\n}\n\n#ifdef CONFIG_FTRACE_SELFTEST\n/* Let selftest have access to static functions in this file */\n#include \"trace_selftest.c\"\n#endif\n\nstatic ssize_t\ntrace_options_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tchar *buf;\n\n\tif (topt->flags->val & topt->opt->bit)\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tif (!!(topt->flags->val & topt->opt->bit) != val) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tret = __set_tracer_option(topt->tr, topt->flags,\n\t\t\t\t\t  topt->opt, !val);\n\t\tmutex_unlock(&trace_types_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\n\nstatic const struct file_operations trace_options_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_read,\n\t.write = trace_options_write,\n\t.llseek\t= generic_file_llseek,\n};\n\n/*\n * In order to pass in both the trace_array descriptor as well as the index\n * to the flag that the trace option file represents, the trace_array\n * has a character array of trace_flags_index[], which holds the index\n * of the bit for the flag it represents. index[0] == 0, index[1] == 1, etc.\n * The address of this character array is passed to the flag option file\n * read/write callbacks.\n *\n * In order to extract both the index and the trace_array descriptor,\n * get_tr_index() uses the following algorithm.\n *\n *   idx = *ptr;\n *\n * As the pointer itself contains the address of the index (remember\n * index[1] == 1).\n *\n * Then to get the trace_array descriptor, by subtracting that index\n * from the ptr, we get to the start of the index itself.\n *\n *   ptr - idx == &index[0]\n *\n * Then a simple container_of() from that pointer gets us to the\n * trace_array descriptor.\n */\nstatic void get_tr_index(void *data, struct trace_array **ptr,\n\t\t\t unsigned int *pindex)\n{\n\t*pindex = *(unsigned char *)data;\n\n\t*ptr = container_of(data - *pindex, struct trace_array,\n\t\t\t    trace_flags_index);\n}\n\nstatic ssize_t\ntrace_options_core_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tchar *buf;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tif (tr->trace_flags & (1 << index))\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_core_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tunsigned long val;\n\tint ret;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\tret = set_tracer_flag(tr, 1 << index, val);\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations trace_options_core_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_core_read,\n\t.write = trace_options_core_write,\n\t.llseek = generic_file_llseek,\n};\n\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops)\n{\n\tstruct dentry *ret;\n\n\tret = tracefs_create_file(name, mode, parent, data, fops);\n\tif (!ret)\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", name);\n\n\treturn ret;\n}\n\n\nstatic struct dentry *trace_options_init_dentry(struct trace_array *tr)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->options)\n\t\treturn tr->options;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->options = tracefs_create_dir(\"options\", d_tracer);\n\tif (!tr->options) {\n\t\tpr_warn(\"Could not create tracefs directory 'options'\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn tr->options;\n}\n\nstatic void\ncreate_trace_option_file(struct trace_array *tr,\n\t\t\t struct trace_option_dentry *topt,\n\t\t\t struct tracer_flags *flags,\n\t\t\t struct tracer_opt *opt)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\ttopt->flags = flags;\n\ttopt->opt = opt;\n\ttopt->tr = tr;\n\n\ttopt->entry = trace_create_file(opt->name, 0644, t_options, topt,\n\t\t\t\t    &trace_options_fops);\n\n}\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer)\n{\n\tstruct trace_option_dentry *topts;\n\tstruct trace_options *tr_topts;\n\tstruct tracer_flags *flags;\n\tstruct tracer_opt *opts;\n\tint cnt;\n\tint i;\n\n\tif (!tracer)\n\t\treturn;\n\n\tflags = tracer->flags;\n\n\tif (!flags || !flags->opts)\n\t\treturn;\n\n\t/*\n\t * If this is an instance, only create flags for tracers\n\t * the instance may have.\n\t */\n\tif (!trace_ok_for_array(tracer, tr))\n\t\treturn;\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\t/* Make sure there's no duplicate flags. */\n\t\tif (WARN_ON_ONCE(tr->topts[i].tracer->flags == tracer->flags))\n\t\t\treturn;\n\t}\n\n\topts = flags->opts;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++)\n\t\t;\n\n\ttopts = kcalloc(cnt + 1, sizeof(*topts), GFP_KERNEL);\n\tif (!topts)\n\t\treturn;\n\n\ttr_topts = krealloc(tr->topts, sizeof(*tr->topts) * (tr->nr_topts + 1),\n\t\t\t    GFP_KERNEL);\n\tif (!tr_topts) {\n\t\tkfree(topts);\n\t\treturn;\n\t}\n\n\ttr->topts = tr_topts;\n\ttr->topts[tr->nr_topts].tracer = tracer;\n\ttr->topts[tr->nr_topts].topts = topts;\n\ttr->nr_topts++;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++) {\n\t\tcreate_trace_option_file(tr, &topts[cnt], flags,\n\t\t\t\t\t &opts[cnt]);\n\t\tWARN_ONCE(topts[cnt].entry == NULL,\n\t\t\t  \"Failed to create trace option: %s\",\n\t\t\t  opts[cnt].name);\n\t}\n}\n\nstatic struct dentry *\ncreate_trace_option_core_file(struct trace_array *tr,\n\t\t\t      const char *option, long index)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn NULL;\n\n\treturn trace_create_file(option, 0644, t_options,\n\t\t\t\t (void *)&tr->trace_flags_index[index],\n\t\t\t\t &trace_options_core_fops);\n}\n\nstatic void create_trace_options_dir(struct trace_array *tr)\n{\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}\n\nstatic ssize_t\nrb_simple_read(struct file *filp, char __user *ubuf,\n\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tracer_tracing_is_on(tr);\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nrb_simple_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (buffer) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tif (!!val == tracer_tracing_is_on(tr)) {\n\t\t\tval = 0; /* do nothing */\n\t\t} else if (val) {\n\t\t\ttracer_tracing_on(tr);\n\t\t\tif (tr->current_trace->start)\n\t\t\t\ttr->current_trace->start(tr);\n\t\t} else {\n\t\t\ttracer_tracing_off(tr);\n\t\t\tif (tr->current_trace->stop)\n\t\t\t\ttr->current_trace->stop(tr);\n\t\t}\n\t\tmutex_unlock(&trace_types_lock);\n\t}\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations rb_simple_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= rb_simple_read,\n\t.write\t\t= rb_simple_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstatic ssize_t\nbuffer_percent_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tr->buffer_percent;\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nbuffer_percent_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val > 100)\n\t\treturn -EINVAL;\n\n\tif (!val)\n\t\tval = 1;\n\n\ttr->buffer_percent = val;\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations buffer_percent_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= buffer_percent_read,\n\t.write\t\t= buffer_percent_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstruct dentry *trace_instance_dir;\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer);\n\nstatic int\nallocate_trace_buffer(struct trace_array *tr, struct trace_buffer *buf, int size)\n{\n\tenum ring_buffer_flags rb_flags;\n\n\trb_flags = tr->trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0;\n\n\tbuf->tr = tr;\n\n\tbuf->buffer = ring_buffer_alloc(size, rb_flags);\n\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,\n\t\t\t   ring_buffer_size(tr->trace_buffer.buffer, 0));\n\n\treturn 0;\n}\n\nstatic int allocate_trace_buffers(struct trace_array *tr, int size)\n{\n\tint ret;\n\n\tret = allocate_trace_buffer(tr, &tr->trace_buffer, size);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tret = allocate_trace_buffer(tr, &tr->max_buffer,\n\t\t\t\t    allocate_snapshot ? size : 1);\n\tif (WARN_ON(ret)) {\n\t\tring_buffer_free(tr->trace_buffer.buffer);\n\t\ttr->trace_buffer.buffer = NULL;\n\t\tfree_percpu(tr->trace_buffer.data);\n\t\ttr->trace_buffer.data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\ttr->allocated_snapshot = allocate_snapshot;\n\n\t/*\n\t * Only the top level trace array gets its snapshot allocated\n\t * from the kernel command line.\n\t */\n\tallocate_snapshot = false;\n#endif\n\treturn 0;\n}\n\nstatic void free_trace_buffer(struct trace_buffer *buf)\n{\n\tif (buf->buffer) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\tfree_percpu(buf->data);\n\t\tbuf->data = NULL;\n\t}\n}\n\nstatic void free_trace_buffers(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn;\n\n\tfree_trace_buffer(&tr->trace_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tfree_trace_buffer(&tr->max_buffer);\n#endif\n}\n\nstatic void init_trace_flags_index(struct trace_array *tr)\n{\n\tint i;\n\n\t/* Used by the trace options files */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++)\n\t\ttr->trace_flags_index[i] = i;\n}\n\nstatic void __update_tracer_options(struct trace_array *tr)\n{\n\tstruct tracer *t;\n\n\tfor (t = trace_types; t; t = t->next)\n\t\tadd_tracer_options(tr, t);\n}\n\nstatic void update_tracer_options(struct trace_array *tr)\n{\n\tmutex_lock(&trace_types_lock);\n\t__update_tracer_options(tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int instance_mkdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}\n\nstatic int instance_rmdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic __init void create_trace_instances(struct dentry *d_tracer)\n{\n\ttrace_instance_dir = tracefs_create_instance_dir(\"instances\", d_tracer,\n\t\t\t\t\t\t\t instance_mkdir,\n\t\t\t\t\t\t\t instance_rmdir);\n\tif (WARN_ON(!trace_instance_dir))\n\t\treturn;\n}\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\ttrace_create_file(\"available_tracers\", 0444, d_tracer,\n\t\t\ttr, &show_traces_fops);\n\n\ttrace_create_file(\"current_tracer\", 0644, d_tracer,\n\t\t\ttr, &set_tracer_fops);\n\n\ttrace_create_file(\"tracing_cpumask\", 0644, d_tracer,\n\t\t\t  tr, &tracing_cpumask_fops);\n\n\ttrace_create_file(\"trace_options\", 0644, d_tracer,\n\t\t\t  tr, &tracing_iter_fops);\n\n\ttrace_create_file(\"trace\", 0644, d_tracer,\n\t\t\t  tr, &tracing_fops);\n\n\ttrace_create_file(\"trace_pipe\", 0444, d_tracer,\n\t\t\t  tr, &tracing_pipe_fops);\n\n\ttrace_create_file(\"buffer_size_kb\", 0644, d_tracer,\n\t\t\t  tr, &tracing_entries_fops);\n\n\ttrace_create_file(\"buffer_total_size_kb\", 0444, d_tracer,\n\t\t\t  tr, &tracing_total_entries_fops);\n\n\ttrace_create_file(\"free_buffer\", 0200, d_tracer,\n\t\t\t  tr, &tracing_free_buffer_fops);\n\n\ttrace_create_file(\"trace_marker\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_fops);\n\n\tfile = __find_event_file(tr, \"ftrace\", \"print\");\n\tif (file && file->dir)\n\t\ttrace_create_file(\"trigger\", 0644, file->dir, file,\n\t\t\t\t  &event_trigger_fops);\n\ttr->trace_marker_file = file;\n\n\ttrace_create_file(\"trace_marker_raw\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_raw_fops);\n\n\ttrace_create_file(\"trace_clock\", 0644, d_tracer, tr,\n\t\t\t  &trace_clock_fops);\n\n\ttrace_create_file(\"tracing_on\", 0644, d_tracer,\n\t\t\t  tr, &rb_simple_fops);\n\n\ttrace_create_file(\"timestamp_mode\", 0444, d_tracer, tr,\n\t\t\t  &trace_time_stamp_mode_fops);\n\n\ttr->buffer_percent = 50;\n\n\ttrace_create_file(\"buffer_percent\", 0444, d_tracer,\n\t\t\ttr, &buffer_percent_fops);\n\n\tcreate_trace_options_dir(tr);\n\n#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)\n\ttrace_create_file(\"tracing_max_latency\", 0644, d_tracer,\n\t\t\t&tr->max_latency, &tracing_max_lat_fops);\n#endif\n\n\tif (ftrace_create_function_files(tr, d_tracer))\n\t\tWARN(1, \"Could not allocate function filter files\");\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_file(\"snapshot\", 0644, d_tracer,\n\t\t\t  tr, &snapshot_fops);\n#endif\n\n\tfor_each_tracing_cpu(cpu)\n\t\ttracing_init_tracefs_percpu(tr, cpu);\n\n\tftrace_init_tracefs(tr, d_tracer);\n}\n\nstatic struct vfsmount *trace_automount(struct dentry *mntpt, void *ingore)\n{\n\tstruct vfsmount *mnt;\n\tstruct file_system_type *type;\n\n\t/*\n\t * To maintain backward compatibility for tools that mount\n\t * debugfs to get to the tracing facility, tracefs is automatically\n\t * mounted to the debugfs/tracing directory.\n\t */\n\ttype = get_fs_type(\"tracefs\");\n\tif (!type)\n\t\treturn NULL;\n\tmnt = vfs_submount(mntpt, type, \"tracefs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn NULL;\n\tmntget(mnt);\n\n\treturn mnt;\n}\n\n/**\n * tracing_init_dentry - initialize top level trace array\n *\n * This is called when creating files or directories in the tracing\n * directory. It is called via fs_initcall() by any of the boot up code\n * and expects to return the dentry of the top level tracing directory.\n */\nstruct dentry *tracing_init_dentry(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\t/* The top level trace array uses  NULL as parent */\n\tif (tr->dir)\n\t\treturn NULL;\n\n\tif (WARN_ON(!tracefs_initialized()) ||\n\t\t(IS_ENABLED(CONFIG_DEBUG_FS) &&\n\t\t WARN_ON(!debugfs_initialized())))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t/*\n\t * As there may still be users that expect the tracing\n\t * files to exist in debugfs/tracing, we must automount\n\t * the tracefs file system there, so older tools still\n\t * work with the newer kerenl.\n\t */\n\ttr->dir = debugfs_create_automount(\"tracing\", NULL,\n\t\t\t\t\t   trace_automount, NULL);\n\tif (!tr->dir) {\n\t\tpr_warn_once(\"Could not create debugfs directory 'tracing'\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn NULL;\n}\n\nextern struct trace_eval_map *__start_ftrace_eval_maps[];\nextern struct trace_eval_map *__stop_ftrace_eval_maps[];\n\nstatic void __init trace_eval_init(void)\n{\n\tint len;\n\n\tlen = __stop_ftrace_eval_maps - __start_ftrace_eval_maps;\n\ttrace_insert_eval_map(NULL, __start_ftrace_eval_maps, len);\n}\n\n#ifdef CONFIG_MODULES\nstatic void trace_module_add_evals(struct module *mod)\n{\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\t/*\n\t * Modules with bad taint do not have events created, do\n\t * not bother with enums either.\n\t */\n\tif (trace_module_has_bad_taint(mod))\n\t\treturn;\n\n\ttrace_insert_eval_map(mod, mod->trace_evals, mod->num_trace_evals);\n}\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic void trace_module_remove_evals(struct module *mod)\n{\n\tunion trace_eval_map_item *map;\n\tunion trace_eval_map_item **last = &trace_eval_maps;\n\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tmap = trace_eval_maps;\n\n\twhile (map) {\n\t\tif (map->head.mod == mod)\n\t\t\tbreak;\n\t\tmap = trace_eval_jmp_to_tail(map);\n\t\tlast = &map->tail.next;\n\t\tmap = map->tail.next;\n\t}\n\tif (!map)\n\t\tgoto out;\n\n\t*last = trace_eval_jmp_to_tail(map)->tail.next;\n\tkfree(map);\n out:\n\tmutex_unlock(&trace_eval_mutex);\n}\n#else\nstatic inline void trace_module_remove_evals(struct module *mod) { }\n#endif /* CONFIG_TRACE_EVAL_MAP_FILE */\n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_evals(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_evals(mod);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 0,\n};\n#endif /* CONFIG_MODULES */\n\nstatic __init int tracer_init_tracefs(void)\n{\n\tstruct dentry *d_tracer;\n\n\ttrace_access_lock_init();\n\n\td_tracer = tracing_init_dentry();\n\tif (IS_ERR(d_tracer))\n\t\treturn 0;\n\n\tevent_trace_init();\n\n\tinit_tracer_tracefs(&global_trace, d_tracer);\n\tftrace_init_tracefs_toplevel(&global_trace, d_tracer);\n\n\ttrace_create_file(\"tracing_thresh\", 0644, d_tracer,\n\t\t\t&global_trace, &tracing_thresh_fops);\n\n\ttrace_create_file(\"README\", 0444, d_tracer,\n\t\t\tNULL, &tracing_readme_fops);\n\n\ttrace_create_file(\"saved_cmdlines\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_cmdlines_fops);\n\n\ttrace_create_file(\"saved_cmdlines_size\", 0644, d_tracer,\n\t\t\t  NULL, &tracing_saved_cmdlines_size_fops);\n\n\ttrace_create_file(\"saved_tgids\", 0444, d_tracer,\n\t\t\tNULL, &tracing_saved_tgids_fops);\n\n\ttrace_eval_init();\n\n\ttrace_create_eval_file(d_tracer);\n\n#ifdef CONFIG_MODULES\n\tregister_module_notifier(&trace_module_nb);\n#endif\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\ttrace_create_file(\"dyn_ftrace_total_info\", 0444, d_tracer,\n\t\t\t&ftrace_update_tot_cnt, &tracing_dyn_info_fops);\n#endif\n\n\tcreate_trace_instances(d_tracer);\n\n\tupdate_tracer_options(&global_trace);\n\n\treturn 0;\n}\n\nstatic int trace_panic_handler(struct notifier_block *this,\n\t\t\t       unsigned long event, void *unused)\n{\n\tif (ftrace_dump_on_oops)\n\t\tftrace_dump(ftrace_dump_on_oops);\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_panic_notifier = {\n\t.notifier_call  = trace_panic_handler,\n\t.next           = NULL,\n\t.priority       = 150   /* priority: INT_MAX >= x >= 0 */\n};\n\nstatic int trace_die_handler(struct notifier_block *self,\n\t\t\t     unsigned long val,\n\t\t\t     void *data)\n{\n\tswitch (val) {\n\tcase DIE_OOPS:\n\t\tif (ftrace_dump_on_oops)\n\t\t\tftrace_dump(ftrace_dump_on_oops);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_die_notifier = {\n\t.notifier_call = trace_die_handler,\n\t.priority = 200\n};\n\n/*\n * printk is set to max of 1024, we really don't need it that big.\n * Nothing should be printing 1000 characters anyway.\n */\n#define TRACE_MAX_PRINT\t\t1000\n\n/*\n * Define here KERN_TRACE so that we have one place to modify\n * it if we decide to change what log level the ftrace dump\n * should be at.\n */\n#define KERN_TRACE\t\tKERN_EMERG\n\nvoid\ntrace_printk_seq(struct trace_seq *s)\n{\n\t/* Probably should print a warning here. */\n\tif (s->seq.len >= TRACE_MAX_PRINT)\n\t\ts->seq.len = TRACE_MAX_PRINT;\n\n\t/*\n\t * More paranoid code. Although the buffer size is set to\n\t * PAGE_SIZE, and TRACE_MAX_PRINT is 1000, this is just\n\t * an extra layer of protection.\n\t */\n\tif (WARN_ON_ONCE(s->seq.len >= s->seq.size))\n\t\ts->seq.len = s->seq.size - 1;\n\n\t/* should be zero ended, but we are paranoid. */\n\ts->buffer[s->seq.len] = 0;\n\n\tprintk(KERN_TRACE \"%s\", s->buffer);\n\n\ttrace_seq_init(s);\n}\n\nvoid trace_init_global_iter(struct trace_iterator *iter)\n{\n\titer->tr = &global_trace;\n\titer->trace = iter->tr->current_trace;\n\titer->cpu_file = RING_BUFFER_ALL_CPUS;\n\titer->trace_buffer = &global_trace.trace_buffer;\n\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t/* Annotate start of buffers if we had overruns */\n\tif (ring_buffer_overruns(iter->trace_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t/* Output in nanoseconds only if we are using a clock in nanoseconds. */\n\tif (trace_clocks[iter->tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n}\n\nvoid ftrace_dump(enum ftrace_dump_mode oops_dump_mode)\n{\n\t/* use static because iter can be a bit big for the stack */\n\tstatic struct trace_iterator iter;\n\tstatic atomic_t dump_running;\n\tstruct trace_array *tr = &global_trace;\n\tunsigned int old_userobj;\n\tunsigned long flags;\n\tint cnt = 0, cpu;\n\n\t/* Only allow one dump user at a time. */\n\tif (atomic_inc_return(&dump_running) != 1) {\n\t\tatomic_dec(&dump_running);\n\t\treturn;\n\t}\n\n\t/*\n\t * Always turn off tracing when we dump.\n\t * We don't need to show trace output of what happens\n\t * between multiple crashes.\n\t *\n\t * If the user does a sysrq-z, then they can re-enable\n\t * tracing with echo 1 > tracing_on.\n\t */\n\ttracing_off();\n\n\tlocal_irq_save(flags);\n\tprintk_nmi_direct_enter();\n\n\t/* Simulate the iterator */\n\ttrace_init_global_iter(&iter);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_inc(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\n\told_userobj = tr->trace_flags & TRACE_ITER_SYM_USEROBJ;\n\n\t/* don't look at user memory in panic mode */\n\ttr->trace_flags &= ~TRACE_ITER_SYM_USEROBJ;\n\n\tswitch (oops_dump_mode) {\n\tcase DUMP_ALL:\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t\tbreak;\n\tcase DUMP_ORIG:\n\t\titer.cpu_file = raw_smp_processor_id();\n\t\tbreak;\n\tcase DUMP_NONE:\n\t\tgoto out_enable;\n\tdefault:\n\t\tprintk(KERN_TRACE \"Bad dumping mode, switching to all CPUs dump\\n\");\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t}\n\n\tprintk(KERN_TRACE \"Dumping ftrace buffer:\\n\");\n\n\t/* Did function tracer already get disabled? */\n\tif (ftrace_is_dead()) {\n\t\tprintk(\"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\");\n\t\tprintk(\"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n\t}\n\n\t/*\n\t * We need to stop all tracing on all CPUS to read the\n\t * the next buffer. This is a bit expensive, but is\n\t * not done often. We fill all what we can read,\n\t * and then release the locks again.\n\t */\n\n\twhile (!trace_empty(&iter)) {\n\n\t\tif (!cnt)\n\t\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n\t\tcnt++;\n\n\t\t/* reset all but tr, trace, and overruns */\n\t\tmemset(&iter.seq, 0,\n\t\t       sizeof(struct trace_iterator) -\n\t\t       offsetof(struct trace_iterator, seq));\n\t\titer.iter_flags |= TRACE_FILE_LAT_FMT;\n\t\titer.pos = -1;\n\n\t\tif (trace_find_next_entry_inc(&iter) != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = print_trace_line(&iter);\n\t\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\t\ttrace_consume(&iter);\n\t\t}\n\t\ttouch_nmi_watchdog();\n\n\t\ttrace_printk_seq(&iter.seq);\n\t}\n\n\tif (!cnt)\n\t\tprintk(KERN_TRACE \"   (ftrace buffer empty)\\n\");\n\telse\n\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n out_enable:\n\ttr->trace_flags |= old_userobj;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_dec(&per_cpu_ptr(iter.trace_buffer->data, cpu)->disabled);\n\t}\n\tatomic_dec(&dump_running);\n\tprintk_nmi_direct_exit();\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(ftrace_dump);\n\nint trace_run_command(const char *buf, int (*createfn)(int, char **))\n{\n\tchar **argv;\n\tint argc, ret;\n\n\targc = 0;\n\tret = 0;\n\targv = argv_split(GFP_KERNEL, buf, &argc);\n\tif (!argv)\n\t\treturn -ENOMEM;\n\n\tif (argc)\n\t\tret = createfn(argc, argv);\n\n\targv_free(argv);\n\n\treturn ret;\n}\n\n#define WRITE_BUFSIZE  4096\n\nssize_t trace_parse_run_command(struct file *file, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\tint (*createfn)(int, char **))\n{\n\tchar *kbuf, *buf, *tmp;\n\tint ret = 0;\n\tsize_t done = 0;\n\tsize_t size;\n\n\tkbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\twhile (done < count) {\n\t\tsize = count - done;\n\n\t\tif (size >= WRITE_BUFSIZE)\n\t\t\tsize = WRITE_BUFSIZE - 1;\n\n\t\tif (copy_from_user(kbuf, buffer + done, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkbuf[size] = '\\0';\n\t\tbuf = kbuf;\n\t\tdo {\n\t\t\ttmp = strchr(buf, '\\n');\n\t\t\tif (tmp) {\n\t\t\t\t*tmp = '\\0';\n\t\t\t\tsize = tmp - buf + 1;\n\t\t\t} else {\n\t\t\t\tsize = strlen(buf);\n\t\t\t\tif (done + size < count) {\n\t\t\t\t\tif (buf != kbuf)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t/* This can accept WRITE_BUFSIZE - 2 ('\\n' + '\\0') */\n\t\t\t\t\tpr_warn(\"Line length is too long: Should be less than %d\\n\",\n\t\t\t\t\t\tWRITE_BUFSIZE - 2);\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdone += size;\n\n\t\t\t/* Remove comments */\n\t\t\ttmp = strchr(buf, '#');\n\n\t\t\tif (tmp)\n\t\t\t\t*tmp = '\\0';\n\n\t\t\tret = trace_run_command(buf, createfn);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbuf += size;\n\n\t\t} while (done < count);\n\t}\n\tret = done;\n\nout:\n\tkfree(kbuf);\n\n\treturn ret;\n}\n\n__init static int tracer_alloc_buffers(void)\n{\n\tint ring_buf_size;\n\tint ret = -ENOMEM;\n\n\t/*\n\t * Make sure we don't accidently add more trace options\n\t * than we have bits for.\n\t */\n\tBUILD_BUG_ON(TRACE_ITER_LAST_BIT > TRACE_FLAGS_MAX_SIZE);\n\n\tif (!alloc_cpumask_var(&tracing_buffer_mask, GFP_KERNEL))\n\t\tgoto out;\n\n\tif (!alloc_cpumask_var(&global_trace.tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_buffer_mask;\n\n\t/* Only allocate trace_printk buffers if a trace_printk exists */\n\tif (__stop___trace_bprintk_fmt != __start___trace_bprintk_fmt)\n\t\t/* Must be called before global_trace.buffer is allocated */\n\t\ttrace_printk_init_buffers();\n\n\t/* To save memory, keep the ring buffer size to its minimum */\n\tif (ring_buffer_expanded)\n\t\tring_buf_size = trace_buf_size;\n\telse\n\t\tring_buf_size = 1;\n\n\tcpumask_copy(tracing_buffer_mask, cpu_possible_mask);\n\tcpumask_copy(global_trace.tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&global_trace.start_lock);\n\n\t/*\n\t * The prepare callbacks allocates some memory for the ring buffer. We\n\t * don't free the buffer if the if the CPU goes down. If we were to free\n\t * the buffer, then the user would lose any trace that was in the\n\t * buffer. The memory will be removed once the \"instance\" is removed.\n\t */\n\tret = cpuhp_setup_state_multi(CPUHP_TRACE_RB_PREPARE,\n\t\t\t\t      \"trace/RB:preapre\", trace_rb_cpu_prepare,\n\t\t\t\t      NULL);\n\tif (ret < 0)\n\t\tgoto out_free_cpumask;\n\t/* Used for event triggers */\n\tret = -ENOMEM;\n\ttemp_buffer = ring_buffer_alloc(PAGE_SIZE, RB_FL_OVERWRITE);\n\tif (!temp_buffer)\n\t\tgoto out_rm_hp_state;\n\n\tif (trace_create_savedcmd() < 0)\n\t\tgoto out_free_temp_buffer;\n\n\t/* TODO: make the number of buffers hot pluggable with CPUS */\n\tif (allocate_trace_buffers(&global_trace, ring_buf_size) < 0) {\n\t\tprintk(KERN_ERR \"tracer: failed to allocate ring buffer!\\n\");\n\t\tWARN_ON(1);\n\t\tgoto out_free_savedcmd;\n\t}\n\n\tif (global_trace.buffer_disabled)\n\t\ttracing_off();\n\n\tif (trace_boot_clock) {\n\t\tret = tracing_set_clock(&global_trace, trace_boot_clock);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Trace clock %s not defined, going back to default\\n\",\n\t\t\t\ttrace_boot_clock);\n\t}\n\n\t/*\n\t * register_tracer() might reference current_trace, so it\n\t * needs to be set before we register anything. This is\n\t * just a bootstrap of current_trace anyway.\n\t */\n\tglobal_trace.current_trace = &nop_trace;\n\n\tglobal_trace.max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\tftrace_init_global_array_ops(&global_trace);\n\n\tinit_trace_flags_index(&global_trace);\n\n\tregister_tracer(&nop_trace);\n\n\t/* Function tracing may start here (via kernel command line) */\n\tinit_function_trace();\n\n\t/* All seems OK, enable tracing */\n\ttracing_disabled = 0;\n\n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t\t       &trace_panic_notifier);\n\n\tregister_die_notifier(&trace_die_notifier);\n\n\tglobal_trace.flags = TRACE_ARRAY_FL_GLOBAL;\n\n\tINIT_LIST_HEAD(&global_trace.systems);\n\tINIT_LIST_HEAD(&global_trace.events);\n\tINIT_LIST_HEAD(&global_trace.hist_vars);\n\tlist_add(&global_trace.list, &ftrace_trace_arrays);\n\n\tapply_trace_boot_options();\n\n\tregister_snapshot_cmd();\n\n\treturn 0;\n\nout_free_savedcmd:\n\tfree_saved_cmdlines_buffer(savedcmd);\nout_free_temp_buffer:\n\tring_buffer_free(temp_buffer);\nout_rm_hp_state:\n\tcpuhp_remove_multi_state(CPUHP_TRACE_RB_PREPARE);\nout_free_cpumask:\n\tfree_cpumask_var(global_trace.tracing_cpumask);\nout_free_buffer_mask:\n\tfree_cpumask_var(tracing_buffer_mask);\nout:\n\treturn ret;\n}\n\nvoid __init early_trace_init(void)\n{\n\tif (tracepoint_printk) {\n\t\ttracepoint_print_iter =\n\t\t\tkmalloc(sizeof(*tracepoint_print_iter), GFP_KERNEL);\n\t\tif (WARN_ON(!tracepoint_print_iter))\n\t\t\ttracepoint_printk = 0;\n\t\telse\n\t\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\t}\n\ttracer_alloc_buffers();\n}\n\nvoid __init trace_init(void)\n{\n\ttrace_event_init();\n}\n\n__init static int clear_boot_tracer(void)\n{\n\t/*\n\t * The default tracer at boot buffer is an init section.\n\t * This function is called in lateinit. If we did not\n\t * find the boot tracer, then clear it out, to prevent\n\t * later registration from accessing the buffer that is\n\t * about to be freed.\n\t */\n\tif (!default_bootup_tracer)\n\t\treturn 0;\n\n\tprintk(KERN_INFO \"ftrace bootup tracer '%s' not registered.\\n\",\n\t       default_bootup_tracer);\n\tdefault_bootup_tracer = NULL;\n\n\treturn 0;\n}\n\nfs_initcall(tracer_init_tracefs);\nlate_initcall_sync(clear_boot_tracer);\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n__init static int tracing_set_default_clock(void)\n{\n\t/* sched_clock_stable() is determined in late_initcall */\n\tif (!trace_boot_clock && !sched_clock_stable()) {\n\t\tprintk(KERN_WARNING\n\t\t       \"Unstable clock detected, switching default tracing clock to \\\"global\\\"\\n\"\n\t\t       \"If you want to keep using the local clock, then add:\\n\"\n\t\t       \"  \\\"trace_clock=local\\\"\\n\"\n\t\t       \"on the kernel command line\\n\");\n\t\ttracing_set_clock(&global_trace, \"global\");\n\t}\n\n\treturn 0;\n}\nlate_initcall_sync(tracing_set_default_clock);\n#endif\n"], "filenames": ["fs/fuse/dev.c", "fs/pipe.c", "fs/splice.c", "include/linux/pipe_fs_i.h", "kernel/trace/trace.c"], "buggy_code_start_loc": [2037, 192, 1591, 111, 6838], "buggy_code_end_loc": [2086, 195, 1664, 182, 6843], "fixing_code_start_loc": [2037, 192, 1591, 111, 6838], "fixing_code_end_loc": [2085, 195, 1672, 184, 6848], "type": "CWE-416", "message": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests.", "other": {"cve": {"id": "CVE-2019-11487", "sourceIdentifier": "cve@mitre.org", "published": "2019-04-23T22:29:05.367", "lastModified": "2023-02-24T18:43:05.920", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The Linux kernel before 5.1-rc5 allows page->_refcount reference count overflow, with resultant use-after-free issues, if about 140 GiB of RAM exists. This is related to fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, and mm/hugetlb.c. It can occur with FUSE requests."}, {"lang": "es", "value": "El kernel de Linux, en versiones anteriores a 5.1-rc5, permite el desbordamiento de la cuenta de referencia de p\u00e1gina->_refcount, con los consiguientes problemas de uso de memoria despu\u00e9s de su liberaci\u00f3n, si existen alrededor de 140 GiB de RAM. Esto est\u00e1 relacionado con fs/fuse/dev.c, fs/pipe.c, fs/splice.c, include/linux/mm.h, include/linux/pipe_fs_i.h, kernel/trace/trace.c, mm/gup.c, y mm/hugetlb.c. Puede ocurrir con las peticiones de FUSE."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.4.216", "matchCriteriaId": "84F5A0DE-DC7D-4E4D-B782-02E5E65DA3FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.181", "matchCriteriaId": "26A52A04-54E8-4081-ABFD-F3F78CFE91D0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.14.116", "matchCriteriaId": "CDAD30D2-82DF-4958-A36A-C081D0D11B2F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.15", "versionEndExcluding": "4.19.39", "matchCriteriaId": "0FE90DA3-FCF8-440A-9DBB-5B6A7F58DBF1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.20", "versionEndExcluding": "5.0.12", "matchCriteriaId": "B74AFC84-0586-4442-9101-51E7B295293C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "2258D313-BAF7-482D-98E0-79F2A448287B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "1578A37C-C7CC-4B36-8668-6A1AED63B0A8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "49BD6839-AB64-48DA-9D1D-18B4508AF652"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "A1E5129A-F85C-432A-988D-6C3ED03EC04D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:esm:*:*:*", "matchCriteriaId": "7A5301BF-1402-4BE0-A0F8-69FBE79BC6D6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:19.04:*:*:*:*:*:*:*", "matchCriteriaId": "CD783B0C-9246-47D9-A937-6144FE8BFF0F"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00039.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00040.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-06/msg00048.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2019/04/29/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/108054", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:2703", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2019:2741", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2020:0174", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1752", "source": "cve@mitre.org", "tags": ["Mitigation", "Exploit", "Third Party Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=15fab63e1e57be9fdb5eec1bbc5916e9825e9acb", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=6b3a707736301c2128ca85ce85fb13f60b5e350a", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=88b1a17dfc3ed7728316478fae0f5ad508f50397", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=8fde12ca79aff9b5ba951fce1a2641901b8d8e64", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=f958d7b528b1b40c44cfda5eabe2d82760d868c3", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/15fab63e1e57be9fdb5eec1bbc5916e9825e9acb", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/6b3a707736301c2128ca85ce85fb13f60b5e350a", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/88b1a17dfc3ed7728316478fae0f5ad508f50397", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/8fde12ca79aff9b5ba951fce1a2641901b8d8e64", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/f958d7b528b1b40c44cfda5eabe2d82760d868c3", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/09/msg00014.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/09/msg00015.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lwn.net/Articles/786044/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20190517-0005/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://support.f5.com/csp/article/K14255532", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4069-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4069-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4115-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/4145-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.oracle.com/security-alerts/cpuApr2021.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/15fab63e1e57be9fdb5eec1bbc5916e9825e9acb"}}