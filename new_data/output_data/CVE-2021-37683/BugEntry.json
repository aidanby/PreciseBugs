{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stddef.h>\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace div {\n\n// This file has three implementation of Div.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconstexpr int kInputTensor1 = 0;\nconstexpr int kInputTensor2 = 1;\nconstexpr int kOutputTensor = 0;\n\nstruct OpData {\n  bool requires_broadcast;\n\n  // Parameters used in the quantized paths where the output is 8bit\n  int32 output_activation_min;\n  int32 output_activation_max;\n\n  // Parameters used in all quantized paths\n  int32_t output_multiplier;\n  int output_shift;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* data = new OpData;\n  data->requires_broadcast = false;\n  return data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_TYPES_EQ(context, input1->type, input2->type);\n  output->type = input2->type;\n\n  data->requires_broadcast = !HaveSameShapes(input1, input2);\n\n  TfLiteIntArray* output_size = nullptr;\n  if (data->requires_broadcast) {\n    TF_LITE_ENSURE_OK(context, CalculateShapeForBroadcast(\n                                   context, input1, input2, &output_size));\n  } else {\n    output_size = TfLiteIntArrayCopy(input1->dims);\n  }\n\n  if (output->type == kTfLiteUInt8) {\n    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(\n        context, params->activation, output, &data->output_activation_min,\n        &data->output_activation_max));\n    const double real_multiplier =\n        input1->params.scale / (input2->params.scale * output->params.scale);\n    QuantizeMultiplier(real_multiplier, &data->output_multiplier,\n                       &data->output_shift);\n  }\n\n  return context->ResizeTensor(context, output, output_size);\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalDiv(TfLiteContext* context, TfLiteNode* node, TfLiteDivParams* params,\n             const OpData* data, const TfLiteTensor* input1,\n             const TfLiteTensor* input2, TfLiteTensor* output) {\n#define TF_LITE_DIV(type, opname, data_type)                             \\\n  tflite::ArithmeticParams op_params;                                    \\\n  data_type output_activation_min, output_activation_max;                \\\n  CalculateActivationRange(params->activation, &output_activation_min,   \\\n                           &output_activation_max);                      \\\n  SetActivationParams(output_activation_min, output_activation_max,      \\\n                      &op_params);                                       \\\n  type::opname(op_params, GetTensorShape(input1),                        \\\n               GetTensorData<data_type>(input1), GetTensorShape(input2), \\\n               GetTensorData<data_type>(input2), GetTensorShape(output), \\\n               GetTensorData<data_type>(output))\n  if (output->type == kTfLiteInt32) {\n    if (kernel_type == kReference) {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, int32_t);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, int32_t);\n      }\n    } else {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, int32_t);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, int32_t);\n      }\n    }\n  } else if (output->type == kTfLiteFloat32) {\n    if (kernel_type == kReference) {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, float);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, float);\n      }\n    } else {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, float);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, float);\n      }\n    }\n  }\n#undef TF_LITE_DIV\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                           TfLiteDivParams* params, const OpData* data,\n                           const TfLiteTensor* input1,\n                           const TfLiteTensor* input2, TfLiteTensor* output) {\n  if (input1->type == kTfLiteUInt8 && input2->type == kTfLiteUInt8 &&\n      output->type == kTfLiteUInt8) {\n    tflite::ArithmeticParams op_params;\n    SetActivationParams(data->output_activation_min,\n                        data->output_activation_max, &op_params);\n    op_params.input1_offset = -input1->params.zero_point;\n    op_params.input2_offset = -input2->params.zero_point;\n    op_params.output_offset = output->params.zero_point;\n    op_params.output_multiplier = data->output_multiplier;\n    op_params.output_shift = data->output_shift;\n    bool need_broadcast = optimized_ops::ProcessBroadcastShapes(\n        GetTensorShape(input1), GetTensorShape(input2), &op_params);\n#define TF_LITE_DIV(type, opname, dtype)                             \\\n  type::opname(op_params, GetTensorShape(input1),                    \\\n               GetTensorData<dtype>(input1), GetTensorShape(input2), \\\n               GetTensorData<dtype>(input2), GetTensorShape(output), \\\n               GetTensorData<dtype>(output))\n    if (kernel_type == kReference) {\n      if (need_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, uint8_t);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, uint8_t);\n      }\n    } else {\n      if (need_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, uint8_t);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, uint8_t);\n      }\n    }\n#undef TF_LITE_DIV\n  } else {\n    context->ReportError(\n        context, \"Unsupported combination of input and output types in Div.\");\n    return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  if (output->type == kTfLiteFloat32 || output->type == kTfLiteInt32) {\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n\n  return kTfLiteOk;\n}\n\n}  // namespace div\n\nTfLiteRegistration* Register_DIV_REF() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV_GENERIC_OPT() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV_NEON_OPT() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kNeonOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV() {\n#ifdef USE_NEON\n  return Register_DIV_NEON_OPT();\n#else\n  return Register_DIV_GENERIC_OPT();\n#endif\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <stddef.h>\n#include <stdint.h>\n\n#include \"tensorflow/lite/c/builtin_op_data.h\"\n#include \"tensorflow/lite/c/common.h\"\n#include \"tensorflow/lite/kernels/internal/compatibility.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/cpu_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/neon_check.h\"\n#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"\n#include \"tensorflow/lite/kernels/internal/quantization_util.h\"\n#include \"tensorflow/lite/kernels/internal/reference/process_broadcast_shapes.h\"\n#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"\n#include \"tensorflow/lite/kernels/internal/tensor.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_ctypes.h\"\n#include \"tensorflow/lite/kernels/internal/types.h\"\n#include \"tensorflow/lite/kernels/kernel_util.h\"\n\nnamespace tflite {\nnamespace ops {\nnamespace builtin {\nnamespace div {\n\n// This file has three implementation of Div.\nenum KernelType {\n  kReference,\n  kGenericOptimized,  // Neon-free\n  kNeonOptimized,\n};\n\nconstexpr int kInputTensor1 = 0;\nconstexpr int kInputTensor2 = 1;\nconstexpr int kOutputTensor = 0;\n\nstruct OpData {\n  bool requires_broadcast;\n\n  // Parameters used in the quantized paths where the output is 8bit\n  int32 output_activation_min;\n  int32 output_activation_max;\n\n  // Parameters used in all quantized paths\n  int32_t output_multiplier;\n  int output_shift;\n};\n\nvoid* Init(TfLiteContext* context, const char* buffer, size_t length) {\n  auto* data = new OpData;\n  data->requires_broadcast = false;\n  return data;\n}\n\nvoid Free(TfLiteContext* context, void* buffer) {\n  delete reinterpret_cast<OpData*>(buffer);\n}\n\nTfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  TF_LITE_ENSURE_EQ(context, NumInputs(node), 2);\n  TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_TYPES_EQ(context, input1->type, input2->type);\n  output->type = input2->type;\n\n  data->requires_broadcast = !HaveSameShapes(input1, input2);\n\n  TfLiteIntArray* output_size = nullptr;\n  if (data->requires_broadcast) {\n    TF_LITE_ENSURE_OK(context, CalculateShapeForBroadcast(\n                                   context, input1, input2, &output_size));\n  } else {\n    output_size = TfLiteIntArrayCopy(input1->dims);\n  }\n\n  if (output->type == kTfLiteUInt8) {\n    TF_LITE_ENSURE_STATUS(CalculateActivationRangeQuantized(\n        context, params->activation, output, &data->output_activation_min,\n        &data->output_activation_max));\n    const double real_multiplier =\n        input1->params.scale / (input2->params.scale * output->params.scale);\n    QuantizeMultiplier(real_multiplier, &data->output_multiplier,\n                       &data->output_shift);\n  }\n\n  return context->ResizeTensor(context, output, output_size);\n}\n\ntemplate <KernelType kernel_type>\nvoid EvalDiv(TfLiteContext* context, TfLiteNode* node, TfLiteDivParams* params,\n             const OpData* data, const TfLiteTensor* input1,\n             const TfLiteTensor* input2, TfLiteTensor* output) {\n#define TF_LITE_DIV(type, opname, data_type)                             \\\n  tflite::ArithmeticParams op_params;                                    \\\n  data_type output_activation_min, output_activation_max;                \\\n  CalculateActivationRange(params->activation, &output_activation_min,   \\\n                           &output_activation_max);                      \\\n  SetActivationParams(output_activation_min, output_activation_max,      \\\n                      &op_params);                                       \\\n  type::opname(op_params, GetTensorShape(input1),                        \\\n               GetTensorData<data_type>(input1), GetTensorShape(input2), \\\n               GetTensorData<data_type>(input2), GetTensorShape(output), \\\n               GetTensorData<data_type>(output))\n  if (output->type == kTfLiteInt32) {\n    if (kernel_type == kReference) {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, int32_t);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, int32_t);\n      }\n    } else {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, int32_t);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, int32_t);\n      }\n    }\n  } else if (output->type == kTfLiteFloat32) {\n    if (kernel_type == kReference) {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, float);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, float);\n      }\n    } else {\n      if (data->requires_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, float);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, float);\n      }\n    }\n  }\n#undef TF_LITE_DIV\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\n                           TfLiteDivParams* params, const OpData* data,\n                           const TfLiteTensor* input1,\n                           const TfLiteTensor* input2, TfLiteTensor* output) {\n  if (input1->type == kTfLiteUInt8 && input2->type == kTfLiteUInt8 &&\n      output->type == kTfLiteUInt8) {\n    tflite::ArithmeticParams op_params;\n    SetActivationParams(data->output_activation_min,\n                        data->output_activation_max, &op_params);\n    op_params.input1_offset = -input1->params.zero_point;\n    op_params.input2_offset = -input2->params.zero_point;\n    op_params.output_offset = output->params.zero_point;\n    op_params.output_multiplier = data->output_multiplier;\n    op_params.output_shift = data->output_shift;\n    bool need_broadcast = optimized_ops::ProcessBroadcastShapes(\n        GetTensorShape(input1), GetTensorShape(input2), &op_params);\n#define TF_LITE_DIV(type, opname, dtype)                             \\\n  type::opname(op_params, GetTensorShape(input1),                    \\\n               GetTensorData<dtype>(input1), GetTensorShape(input2), \\\n               GetTensorData<dtype>(input2), GetTensorShape(output), \\\n               GetTensorData<dtype>(output))\n    if (kernel_type == kReference) {\n      if (need_broadcast) {\n        TF_LITE_DIV(reference_ops, BroadcastDivSlow, uint8_t);\n      } else {\n        TF_LITE_DIV(reference_ops, Div, uint8_t);\n      }\n    } else {\n      if (need_broadcast) {\n        TF_LITE_DIV(optimized_ops, BroadcastDivSlow, uint8_t);\n      } else {\n        TF_LITE_DIV(optimized_ops, Div, uint8_t);\n      }\n    }\n#undef TF_LITE_DIV\n  } else {\n    context->ReportError(\n        context, \"Unsupported combination of input and output types in Div.\");\n    return kTfLiteError;\n  }\n  return kTfLiteOk;\n}\n\ntemplate <KernelType kernel_type>\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n  auto* params = reinterpret_cast<TfLiteDivParams*>(node->builtin_data);\n  OpData* data = reinterpret_cast<OpData*>(node->user_data);\n\n  const TfLiteTensor* input1;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor1, &input1));\n  const TfLiteTensor* input2;\n  TF_LITE_ENSURE_OK(context,\n                    GetInputSafe(context, node, kInputTensor2, &input2));\n  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  // TODO(b/193904910): This can written with C++ templates\n#define TF_LITE_CHECK_DIV_NON_ZERO(data_type)                       \\\n  const auto* input2_data = GetTensorData<data_type>(input2);       \\\n  const size_t input2_elements = input2->bytes / sizeof(data_type); \\\n  for (size_t i = 0; i < input2_elements; i++) {                    \\\n    TF_LITE_ENSURE(context, input2_data[i] != 0);                   \\\n  }\n\n  if (output->type == kTfLiteFloat32) {\n    // Div by zero seems ok in this case, just like in TF case infinities are\n    // returned. So we don't do a check at this point.\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteInt32) {\n    TF_LITE_CHECK_DIV_NON_ZERO(int32_t);\n    EvalDiv<kernel_type>(context, node, params, data, input1, input2, output);\n  } else if (output->type == kTfLiteUInt8) {\n    TF_LITE_CHECK_DIV_NON_ZERO(uint8_t);\n    TF_LITE_ENSURE_OK(\n        context, EvalQuantized<kernel_type>(context, node, params, data, input1,\n                                            input2, output));\n  } else {\n    context->ReportError(\n        context,\n        \"Div only supports FLOAT32, INT32 and quantized UINT8 now, got %d.\",\n        output->type);\n    return kTfLiteError;\n  }\n#undef TF_LITE_CHECK_DIV_NON_ZERO\n\n  return kTfLiteOk;\n}\n\n}  // namespace div\n\nTfLiteRegistration* Register_DIV_REF() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kReference>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV_GENERIC_OPT() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kGenericOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV_NEON_OPT() {\n  static TfLiteRegistration r = {div::Init, div::Free, div::Prepare,\n                                 div::Eval<div::kNeonOptimized>};\n  return &r;\n}\n\nTfLiteRegistration* Register_DIV() {\n#ifdef USE_NEON\n  return Register_DIV_NEON_OPT();\n#else\n  return Register_DIV_GENERIC_OPT();\n#endif\n}\n\n}  // namespace builtin\n}  // namespace ops\n}  // namespace tflite\n"], "filenames": ["tensorflow/lite/kernels/div.cc"], "buggy_code_start_loc": [219], "buggy_code_end_loc": [231], "fixing_code_start_loc": [219], "fixing_code_end_loc": [247], "type": "CWE-369", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of division in TFLite is [vulnerable to a division by 0 error](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/div.cc). There is no check that the divisor tensor does not contain zero elements. We have patched the issue in GitHub commit 1e206baedf8bef0334cca3eb92bab134ef525a28. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37683", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:08.487", "lastModified": "2021-08-18T21:57:45.837", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions the implementation of division in TFLite is [vulnerable to a division by 0 error](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/div.cc). There is no check that the divisor tensor does not contain zero elements. We have patched the issue in GitHub commit 1e206baedf8bef0334cca3eb92bab134ef525a28. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas, la implementaci\u00f3n de la divisi\u00f3n en TFLite es [vulnerable a una divisi\u00f3n por 0] (https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/lite/kernels/div.cc).&#xa0;no es comprobada que el tensor del divisor no contenga elementos cero.&#xa0;Hemos solucionado el problema en el commit de GitHub 1e206baedf8bef0334cca3eb92bab134ef525a28.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3 y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan se encuentran en el rango admitido."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/1e206baedf8bef0334cca3eb92bab134ef525a28", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-rhrq-64mq-hf9h", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/1e206baedf8bef0334cca3eb92bab134ef525a28"}}