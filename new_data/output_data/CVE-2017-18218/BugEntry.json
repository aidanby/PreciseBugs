{"buggy_code": ["/*\n * Copyright (c) 2014-2015 Hisilicon Limited.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n */\n\n#include <linux/clk.h>\n#include <linux/cpumask.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/module.h>\n#include <linux/phy.h>\n#include <linux/platform_device.h>\n#include <linux/skbuff.h>\n\n#include \"hnae.h\"\n#include \"hns_enet.h\"\n#include \"hns_dsaf_mac.h\"\n\n#define NIC_MAX_Q_PER_VF 16\n#define HNS_NIC_TX_TIMEOUT (5 * HZ)\n\n#define SERVICE_TIMER_HZ (1 * HZ)\n\n#define NIC_TX_CLEAN_MAX_NUM 256\n#define NIC_RX_CLEAN_MAX_NUM 64\n\n#define RCB_IRQ_NOT_INITED 0\n#define RCB_IRQ_INITED 1\n#define HNS_BUFFER_SIZE_2048 2048\n\n#define BD_MAX_SEND_SIZE 8191\n#define SKB_TMP_LEN(SKB) \\\n\t(((SKB)->transport_header - (SKB)->mac_header) + tcp_hdrlen(SKB))\n\nstatic void fill_v2_desc(struct hnae_ring *ring, void *priv,\n\t\t\t int size, dma_addr_t dma, int frag_end,\n\t\t\t int buf_num, enum hns_desc_type type, int mtu)\n{\n\tstruct hnae_desc *desc = &ring->desc[ring->next_to_use];\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_use];\n\tstruct iphdr *iphdr;\n\tstruct ipv6hdr *ipv6hdr;\n\tstruct sk_buff *skb;\n\t__be16 protocol;\n\tu8 bn_pid = 0;\n\tu8 rrcfv = 0;\n\tu8 ip_offset = 0;\n\tu8 tvsvsn = 0;\n\tu16 mss = 0;\n\tu8 l4_len = 0;\n\tu16 paylen = 0;\n\n\tdesc_cb->priv = priv;\n\tdesc_cb->length = size;\n\tdesc_cb->dma = dma;\n\tdesc_cb->type = type;\n\n\tdesc->addr = cpu_to_le64(dma);\n\tdesc->tx.send_size = cpu_to_le16((u16)size);\n\n\t/* config bd buffer end */\n\thnae_set_bit(rrcfv, HNSV2_TXD_VLD_B, 1);\n\thnae_set_field(bn_pid, HNSV2_TXD_BUFNUM_M, 0, buf_num - 1);\n\n\t/* fill port_id in the tx bd for sending management pkts */\n\thnae_set_field(bn_pid, HNSV2_TXD_PORTID_M,\n\t\t       HNSV2_TXD_PORTID_S, ring->q->handle->dport_id);\n\n\tif (type == DESC_TYPE_SKB) {\n\t\tskb = (struct sk_buff *)priv;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_mac_len(skb);\n\t\t\tprotocol = skb->protocol;\n\t\t\tip_offset = ETH_HLEN;\n\n\t\t\tif (protocol == htons(ETH_P_8021Q)) {\n\t\t\t\tip_offset += VLAN_HLEN;\n\t\t\t\tprotocol = vlan_get_protocol(skb);\n\t\t\t\tskb->protocol = protocol;\n\t\t\t}\n\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tiphdr = ip_hdr(skb);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L3CS_B, 1);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L4CS_B, 1);\n\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tif (iphdr->protocol == IPPROTO_TCP &&\n\t\t\t\t    skb_is_gso(skb)) {\n\t\t\t\t\thnae_set_bit(tvsvsn,\n\t\t\t\t\t\t     HNSV2_TXD_TSE_B, 1);\n\t\t\t\t\tl4_len = tcp_hdrlen(skb);\n\t\t\t\t\tmss = skb_shinfo(skb)->gso_size;\n\t\t\t\t\tpaylen = skb->len - SKB_TMP_LEN(skb);\n\t\t\t\t}\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\thnae_set_bit(tvsvsn, HNSV2_TXD_IPV6_B, 1);\n\t\t\t\tipv6hdr = ipv6_hdr(skb);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L4CS_B, 1);\n\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tif (ipv6hdr->nexthdr == IPPROTO_TCP &&\n\t\t\t\t    skb_is_gso(skb) && skb_is_gso_v6(skb)) {\n\t\t\t\t\thnae_set_bit(tvsvsn,\n\t\t\t\t\t\t     HNSV2_TXD_TSE_B, 1);\n\t\t\t\t\tl4_len = tcp_hdrlen(skb);\n\t\t\t\t\tmss = skb_shinfo(skb)->gso_size;\n\t\t\t\t\tpaylen = skb->len - SKB_TMP_LEN(skb);\n\t\t\t\t}\n\t\t\t}\n\t\t\tdesc->tx.ip_offset = ip_offset;\n\t\t\tdesc->tx.tse_vlan_snap_v6_sctp_nth = tvsvsn;\n\t\t\tdesc->tx.mss = cpu_to_le16(mss);\n\t\t\tdesc->tx.l4_len = l4_len;\n\t\t\tdesc->tx.paylen = cpu_to_le16(paylen);\n\t\t}\n\t}\n\n\thnae_set_bit(rrcfv, HNSV2_TXD_FE_B, frag_end);\n\n\tdesc->tx.bn_pid = bn_pid;\n\tdesc->tx.ra_ri_cs_fe_vld = rrcfv;\n\n\tring_ptr_move_fw(ring, next_to_use);\n}\n\nstatic const struct acpi_device_id hns_enet_acpi_match[] = {\n\t{ \"HISI00C1\", 0 },\n\t{ \"HISI00C2\", 0 },\n\t{ },\n};\nMODULE_DEVICE_TABLE(acpi, hns_enet_acpi_match);\n\nstatic void fill_desc(struct hnae_ring *ring, void *priv,\n\t\t      int size, dma_addr_t dma, int frag_end,\n\t\t      int buf_num, enum hns_desc_type type, int mtu)\n{\n\tstruct hnae_desc *desc = &ring->desc[ring->next_to_use];\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_use];\n\tstruct sk_buff *skb;\n\t__be16 protocol;\n\tu32 ip_offset;\n\tu32 asid_bufnum_pid = 0;\n\tu32 flag_ipoffset = 0;\n\n\tdesc_cb->priv = priv;\n\tdesc_cb->length = size;\n\tdesc_cb->dma = dma;\n\tdesc_cb->type = type;\n\n\tdesc->addr = cpu_to_le64(dma);\n\tdesc->tx.send_size = cpu_to_le16((u16)size);\n\n\t/*config bd buffer end */\n\tflag_ipoffset |= 1 << HNS_TXD_VLD_B;\n\n\tasid_bufnum_pid |= buf_num << HNS_TXD_BUFNUM_S;\n\n\tif (type == DESC_TYPE_SKB) {\n\t\tskb = (struct sk_buff *)priv;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tprotocol = skb->protocol;\n\t\t\tip_offset = ETH_HLEN;\n\n\t\t\t/*if it is a SW VLAN check the next protocol*/\n\t\t\tif (protocol == htons(ETH_P_8021Q)) {\n\t\t\t\tip_offset += VLAN_HLEN;\n\t\t\t\tprotocol = vlan_get_protocol(skb);\n\t\t\t\tskb->protocol = protocol;\n\t\t\t}\n\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L3CS_B;\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L4CS_B;\n\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\t/* ipv6 has not l3 cs, check for L4 header */\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L4CS_B;\n\t\t\t}\n\n\t\t\tflag_ipoffset |= ip_offset << HNS_TXD_IPOFFSET_S;\n\t\t}\n\t}\n\n\tflag_ipoffset |= frag_end << HNS_TXD_FE_B;\n\n\tdesc->tx.asid_bufnum_pid = cpu_to_le16(asid_bufnum_pid);\n\tdesc->tx.flag_ipoffset = cpu_to_le32(flag_ipoffset);\n\n\tring_ptr_move_fw(ring, next_to_use);\n}\n\nstatic void unfill_desc(struct hnae_ring *ring)\n{\n\tring_ptr_move_bw(ring, next_to_use);\n}\n\nstatic int hns_nic_maybe_stop_tx(\n\tstruct sk_buff **out_skb, int *bnum, struct hnae_ring *ring)\n{\n\tstruct sk_buff *skb = *out_skb;\n\tstruct sk_buff *new_skb = NULL;\n\tint buf_num;\n\n\t/* no. of segments (plus a header) */\n\tbuf_num = skb_shinfo(skb)->nr_frags + 1;\n\n\tif (unlikely(buf_num > ring->max_desc_num_per_pkt)) {\n\t\tif (ring_space(ring) < 1)\n\t\t\treturn -EBUSY;\n\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!new_skb)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_kfree_skb_any(skb);\n\t\t*out_skb = new_skb;\n\t\tbuf_num = 1;\n\t} else if (buf_num > ring_space(ring)) {\n\t\treturn -EBUSY;\n\t}\n\n\t*bnum = buf_num;\n\treturn 0;\n}\n\nstatic int hns_nic_maybe_stop_tso(\n\tstruct sk_buff **out_skb, int *bnum, struct hnae_ring *ring)\n{\n\tint i;\n\tint size;\n\tint buf_num;\n\tint frag_num;\n\tstruct sk_buff *skb = *out_skb;\n\tstruct sk_buff *new_skb = NULL;\n\tstruct skb_frag_struct *frag;\n\n\tsize = skb_headlen(skb);\n\tbuf_num = (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\n\tfrag_num = skb_shinfo(skb)->nr_frags;\n\tfor (i = 0; i < frag_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tsize = skb_frag_size(frag);\n\t\tbuf_num += (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\t}\n\n\tif (unlikely(buf_num > ring->max_desc_num_per_pkt)) {\n\t\tbuf_num = (skb->len + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\t\tif (ring_space(ring) < buf_num)\n\t\t\treturn -EBUSY;\n\t\t/* manual split the send packet */\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!new_skb)\n\t\t\treturn -ENOMEM;\n\t\tdev_kfree_skb_any(skb);\n\t\t*out_skb = new_skb;\n\n\t} else if (ring_space(ring) < buf_num) {\n\t\treturn -EBUSY;\n\t}\n\n\t*bnum = buf_num;\n\treturn 0;\n}\n\nstatic void fill_tso_desc(struct hnae_ring *ring, void *priv,\n\t\t\t  int size, dma_addr_t dma, int frag_end,\n\t\t\t  int buf_num, enum hns_desc_type type, int mtu)\n{\n\tint frag_buf_num;\n\tint sizeoflast;\n\tint k;\n\n\tfrag_buf_num = (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\tsizeoflast = size % BD_MAX_SEND_SIZE;\n\tsizeoflast = sizeoflast ? sizeoflast : BD_MAX_SEND_SIZE;\n\n\t/* when the frag size is bigger than hardware, split this frag */\n\tfor (k = 0; k < frag_buf_num; k++)\n\t\tfill_v2_desc(ring, priv,\n\t\t\t     (k == frag_buf_num - 1) ?\n\t\t\t\t\tsizeoflast : BD_MAX_SEND_SIZE,\n\t\t\t     dma + BD_MAX_SEND_SIZE * k,\n\t\t\t     frag_end && (k == frag_buf_num - 1) ? 1 : 0,\n\t\t\t     buf_num,\n\t\t\t     (type == DESC_TYPE_SKB && !k) ?\n\t\t\t\t\tDESC_TYPE_SKB : DESC_TYPE_PAGE,\n\t\t\t     mtu);\n}\n\nint hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\tstruct sk_buff *skb,\n\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\n\treturn NETDEV_TX_OK;\n\nout_map_frag_fail:\n\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\nout_err_tx_ok:\n\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n\nout_net_tx_busy:\n\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}\n\n/**\n * hns_nic_get_headlen - determine size of header for RSC/LRO/GRO/FCOE\n * @data: pointer to the start of the headers\n * @max: total length of section to find headers in\n *\n * This function is meant to determine the length of headers that will\n * be recognized by hardware for LRO, GRO, and RSC offloads.  The main\n * motivation of doing this is to only perform one pull for IPv4 TCP\n * packets so that we can do basic things like calculating the gso_size\n * based on the average data per packet.\n **/\nstatic unsigned int hns_nic_get_headlen(unsigned char *data, u32 flag,\n\t\t\t\t\tunsigned int max_size)\n{\n\tunsigned char *network;\n\tu8 hlen;\n\n\t/* this should never happen, but better safe than sorry */\n\tif (max_size < ETH_HLEN)\n\t\treturn max_size;\n\n\t/* initialize network frame pointer */\n\tnetwork = data;\n\n\t/* set first protocol and move network header forward */\n\tnetwork += ETH_HLEN;\n\n\t/* handle any vlan tag if present */\n\tif (hnae_get_field(flag, HNS_RXD_VLAN_M, HNS_RXD_VLAN_S)\n\t\t== HNS_RX_FLAG_VLAN_PRESENT) {\n\t\tif ((typeof(max_size))(network - data) > (max_size - VLAN_HLEN))\n\t\t\treturn max_size;\n\n\t\tnetwork += VLAN_HLEN;\n\t}\n\n\t/* handle L3 protocols */\n\tif (hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S)\n\t\t== HNS_RX_FLAG_L3ID_IPV4) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct iphdr)))\n\t\t\treturn max_size;\n\n\t\t/* access ihl as a u8 to avoid unaligned access on ia64 */\n\t\thlen = (network[0] & 0x0F) << 2;\n\n\t\t/* verify hlen meets minimum size requirements */\n\t\tif (hlen < sizeof(struct iphdr))\n\t\t\treturn network - data;\n\n\t\t/* record next protocol if header is present */\n\t} else if (hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S)\n\t\t== HNS_RX_FLAG_L3ID_IPV6) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct ipv6hdr)))\n\t\t\treturn max_size;\n\n\t\t/* record next protocol */\n\t\thlen = sizeof(struct ipv6hdr);\n\t} else {\n\t\treturn network - data;\n\t}\n\n\t/* relocate pointer to start of L4 header */\n\tnetwork += hlen;\n\n\t/* finally sort out TCP/UDP */\n\tif (hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S)\n\t\t== HNS_RX_FLAG_L4ID_TCP) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct tcphdr)))\n\t\t\treturn max_size;\n\n\t\t/* access doff as a u8 to avoid unaligned access on ia64 */\n\t\thlen = (network[12] & 0xF0) >> 2;\n\n\t\t/* verify hlen meets minimum size requirements */\n\t\tif (hlen < sizeof(struct tcphdr))\n\t\t\treturn network - data;\n\n\t\tnetwork += hlen;\n\t} else if (hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S)\n\t\t== HNS_RX_FLAG_L4ID_UDP) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct udphdr)))\n\t\t\treturn max_size;\n\n\t\tnetwork += sizeof(struct udphdr);\n\t}\n\n\t/* If everything has gone correctly network should be the\n\t * data section of the packet and will be the end of the header.\n\t * If not then it probably represents the end of the last recognized\n\t * header.\n\t */\n\tif ((typeof(max_size))(network - data) < max_size)\n\t\treturn network - data;\n\telse\n\t\treturn max_size;\n}\n\nstatic void hns_nic_reuse_page(struct sk_buff *skb, int i,\n\t\t\t       struct hnae_ring *ring, int pull_len,\n\t\t\t       struct hnae_desc_cb *desc_cb)\n{\n\tstruct hnae_desc *desc;\n\tint truesize, size;\n\tint last_offset;\n\tbool twobufs;\n\n\ttwobufs = ((PAGE_SIZE < 8192) &&\n\t\thnae_buf_size(ring) == HNS_BUFFER_SIZE_2048);\n\n\tdesc = &ring->desc[ring->next_to_clean];\n\tsize = le16_to_cpu(desc->rx.size);\n\n\tif (twobufs) {\n\t\ttruesize = hnae_buf_size(ring);\n\t} else {\n\t\ttruesize = ALIGN(size, L1_CACHE_BYTES);\n\t\tlast_offset = hnae_page_size(ring) - hnae_buf_size(ring);\n\t}\n\n\tskb_add_rx_frag(skb, i, desc_cb->priv, desc_cb->page_offset + pull_len,\n\t\t\tsize - pull_len, truesize - pull_len);\n\n\t /* avoid re-using remote pages,flag default unreuse */\n\tif (unlikely(page_to_nid(desc_cb->priv) != numa_node_id()))\n\t\treturn;\n\n\tif (twobufs) {\n\t\t/* if we are only owner of page we can reuse it */\n\t\tif (likely(page_count(desc_cb->priv) == 1)) {\n\t\t\t/* flip page offset to other buffer */\n\t\t\tdesc_cb->page_offset ^= truesize;\n\n\t\t\tdesc_cb->reuse_flag = 1;\n\t\t\t/* bump ref count on page before it is given*/\n\t\t\tget_page(desc_cb->priv);\n\t\t}\n\t\treturn;\n\t}\n\n\t/* move offset up to the next cache line */\n\tdesc_cb->page_offset += truesize;\n\n\tif (desc_cb->page_offset <= last_offset) {\n\t\tdesc_cb->reuse_flag = 1;\n\t\t/* bump ref count on page before it is given*/\n\t\tget_page(desc_cb->priv);\n\t}\n}\n\nstatic void get_v2rx_desc_bnum(u32 bnum_flag, int *out_bnum)\n{\n\t*out_bnum = hnae_get_field(bnum_flag,\n\t\t\t\t   HNS_RXD_BUFNUM_M, HNS_RXD_BUFNUM_S) + 1;\n}\n\nstatic void get_rx_desc_bnum(u32 bnum_flag, int *out_bnum)\n{\n\t*out_bnum = hnae_get_field(bnum_flag,\n\t\t\t\t   HNS_RXD_BUFNUM_M, HNS_RXD_BUFNUM_S);\n}\n\nstatic void hns_nic_rx_checksum(struct hns_nic_ring_data *ring_data,\n\t\t\t\tstruct sk_buff *skb, u32 flag)\n{\n\tstruct net_device *netdev = ring_data->napi.dev;\n\tu32 l3id;\n\tu32 l4id;\n\n\t/* check if RX checksum offload is enabled */\n\tif (unlikely(!(netdev->features & NETIF_F_RXCSUM)))\n\t\treturn;\n\n\t/* In hardware, we only support checksum for the following protocols:\n\t * 1) IPv4,\n\t * 2) TCP(over IPv4 or IPv6),\n\t * 3) UDP(over IPv4 or IPv6),\n\t * 4) SCTP(over IPv4 or IPv6)\n\t * but we support many L3(IPv4, IPv6, MPLS, PPPoE etc) and L4(TCP,\n\t * UDP, GRE, SCTP, IGMP, ICMP etc.) protocols.\n\t *\n\t * Hardware limitation:\n\t * Our present hardware RX Descriptor lacks L3/L4 checksum \"Status &\n\t * Error\" bit (which usually can be used to indicate whether checksum\n\t * was calculated by the hardware and if there was any error encountered\n\t * during checksum calculation).\n\t *\n\t * Software workaround:\n\t * We do get info within the RX descriptor about the kind of L3/L4\n\t * protocol coming in the packet and the error status. These errors\n\t * might not just be checksum errors but could be related to version,\n\t * length of IPv4, UDP, TCP etc.\n\t * Because there is no-way of knowing if it is a L3/L4 error due to bad\n\t * checksum or any other L3/L4 error, we will not (cannot) convey\n\t * checksum status for such cases to upper stack and will not maintain\n\t * the RX L3/L4 checksum counters as well.\n\t */\n\n\tl3id = hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S);\n\tl4id = hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S);\n\n\t/*  check L3 protocol for which checksum is supported */\n\tif ((l3id != HNS_RX_FLAG_L3ID_IPV4) && (l3id != HNS_RX_FLAG_L3ID_IPV6))\n\t\treturn;\n\n\t/* check for any(not just checksum)flagged L3 protocol errors */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_L3E_B)))\n\t\treturn;\n\n\t/* we do not support checksum of fragmented packets */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_FRAG_B)))\n\t\treturn;\n\n\t/*  check L4 protocol for which checksum is supported */\n\tif ((l4id != HNS_RX_FLAG_L4ID_TCP) &&\n\t    (l4id != HNS_RX_FLAG_L4ID_UDP) &&\n\t    (l4id != HNS_RX_FLAG_L4ID_SCTP))\n\t\treturn;\n\n\t/* check for any(not just checksum)flagged L4 protocol errors */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_L4E_B)))\n\t\treturn;\n\n\t/* now, this has to be a packet with valid RX checksum */\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\nstatic int hns_nic_poll_rx_skb(struct hns_nic_ring_data *ring_data,\n\t\t\t       struct sk_buff **out_skb, int *out_bnum)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct sk_buff *skb;\n\tstruct hnae_desc *desc;\n\tstruct hnae_desc_cb *desc_cb;\n\tunsigned char *va;\n\tint bnum, length, i;\n\tint pull_len;\n\tu32 bnum_flag;\n\n\tdesc = &ring->desc[ring->next_to_clean];\n\tdesc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\tprefetch(desc);\n\n\tva = (unsigned char *)desc_cb->buf + desc_cb->page_offset;\n\n\t/* prefetch first cache line of first page */\n\tprefetch(va);\n#if L1_CACHE_BYTES < 128\n\tprefetch(va + L1_CACHE_BYTES);\n#endif\n\n\tskb = *out_skb = napi_alloc_skb(&ring_data->napi,\n\t\t\t\t\tHNS_RX_HEAD_SIZE);\n\tif (unlikely(!skb)) {\n\t\tnetdev_err(ndev, \"alloc rx skb fail\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\treturn -ENOMEM;\n\t}\n\n\tprefetchw(skb->data);\n\tlength = le16_to_cpu(desc->rx.pkt_len);\n\tbnum_flag = le32_to_cpu(desc->rx.ipoff_bnum_pid_flag);\n\tpriv->ops.get_rxd_bnum(bnum_flag, &bnum);\n\t*out_bnum = bnum;\n\n\tif (length <= HNS_RX_HEAD_SIZE) {\n\t\tmemcpy(__skb_put(skb, length), va, ALIGN(length, sizeof(long)));\n\n\t\t/* we can reuse buffer as-is, just make sure it is local */\n\t\tif (likely(page_to_nid(desc_cb->priv) == numa_node_id()))\n\t\t\tdesc_cb->reuse_flag = 1;\n\t\telse /* this page cannot be reused so discard it */\n\t\t\tput_page(desc_cb->priv);\n\n\t\tring_ptr_move_fw(ring, next_to_clean);\n\n\t\tif (unlikely(bnum != 1)) { /* check err*/\n\t\t\t*out_bnum = 1;\n\t\t\tgoto out_bnum_err;\n\t\t}\n\t} else {\n\t\tring->stats.seg_pkt_cnt++;\n\n\t\tpull_len = hns_nic_get_headlen(va, bnum_flag, HNS_RX_HEAD_SIZE);\n\t\tmemcpy(__skb_put(skb, pull_len), va,\n\t\t       ALIGN(pull_len, sizeof(long)));\n\n\t\thns_nic_reuse_page(skb, 0, ring, pull_len, desc_cb);\n\t\tring_ptr_move_fw(ring, next_to_clean);\n\n\t\tif (unlikely(bnum >= (int)MAX_SKB_FRAGS)) { /* check err*/\n\t\t\t*out_bnum = 1;\n\t\t\tgoto out_bnum_err;\n\t\t}\n\t\tfor (i = 1; i < bnum; i++) {\n\t\t\tdesc = &ring->desc[ring->next_to_clean];\n\t\t\tdesc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\t\t\thns_nic_reuse_page(skb, i, ring, 0, desc_cb);\n\t\t\tring_ptr_move_fw(ring, next_to_clean);\n\t\t}\n\t}\n\n\t/* check except process, free skb and jump the desc */\n\tif (unlikely((!bnum) || (bnum > ring->max_desc_num_per_pkt))) {\nout_bnum_err:\n\t\t*out_bnum = *out_bnum ? *out_bnum : 1; /* ntc moved,cannot 0*/\n\t\tnetdev_err(ndev, \"invalid bnum(%d,%d,%d,%d),%016llx,%016llx\\n\",\n\t\t\t   bnum, ring->max_desc_num_per_pkt,\n\t\t\t   length, (int)MAX_SKB_FRAGS,\n\t\t\t   ((u64 *)desc)[0], ((u64 *)desc)[1]);\n\t\tring->stats.err_bd_num++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EDOM;\n\t}\n\n\tbnum_flag = le32_to_cpu(desc->rx.ipoff_bnum_pid_flag);\n\n\tif (unlikely(!hnae_get_bit(bnum_flag, HNS_RXD_VLD_B))) {\n\t\tnetdev_err(ndev, \"no valid bd,%016llx,%016llx\\n\",\n\t\t\t   ((u64 *)desc)[0], ((u64 *)desc)[1]);\n\t\tring->stats.non_vld_descs++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely((!desc->rx.pkt_len) ||\n\t\t     hnae_get_bit(bnum_flag, HNS_RXD_DROP_B))) {\n\t\tring->stats.err_pkt_len++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EFAULT;\n\t}\n\n\tif (unlikely(hnae_get_bit(bnum_flag, HNS_RXD_L2E_B))) {\n\t\tring->stats.l2_err++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EFAULT;\n\t}\n\n\tring->stats.rx_pkts++;\n\tring->stats.rx_bytes += skb->len;\n\n\t/* indicate to upper stack if our hardware has already calculated\n\t * the RX checksum\n\t */\n\thns_nic_rx_checksum(ring_data, skb, bnum_flag);\n\n\treturn 0;\n}\n\nstatic void\nhns_nic_alloc_rx_buffers(struct hns_nic_ring_data *ring_data, int cleand_count)\n{\n\tint i, ret;\n\tstruct hnae_desc_cb res_cbs;\n\tstruct hnae_desc_cb *desc_cb;\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\n\tfor (i = 0; i < cleand_count; i++) {\n\t\tdesc_cb = &ring->desc_cb[ring->next_to_use];\n\t\tif (desc_cb->reuse_flag) {\n\t\t\tring->stats.reuse_pg_cnt++;\n\t\t\thnae_reuse_buffer(ring, ring->next_to_use);\n\t\t} else {\n\t\t\tret = hnae_reserve_buffer_map(ring, &res_cbs);\n\t\t\tif (ret) {\n\t\t\t\tring->stats.sw_err_cnt++;\n\t\t\t\tnetdev_err(ndev, \"hnae reserve buffer map failed.\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\thnae_replace_buffer(ring, ring->next_to_use, &res_cbs);\n\t\t}\n\n\t\tring_ptr_move_fw(ring, next_to_use);\n\t}\n\n\twmb(); /* make all data has been write before submit */\n\twritel_relaxed(i, ring->io_base + RCB_REG_HEAD);\n}\n\n/* return error number for error or number of desc left to take\n */\nstatic void hns_nic_rx_up_pro(struct hns_nic_ring_data *ring_data,\n\t\t\t      struct sk_buff *skb)\n{\n\tstruct net_device *ndev = ring_data->napi.dev;\n\n\tskb->protocol = eth_type_trans(skb, ndev);\n\t(void)napi_gro_receive(&ring_data->napi, skb);\n}\n\nstatic int hns_desc_unused(struct hnae_ring *ring)\n{\n\tint ntc = ring->next_to_clean;\n\tint ntu = ring->next_to_use;\n\n\treturn ((ntc >= ntu) ? 0 : ring->desc_num) + ntc - ntu;\n}\n\nstatic int hns_nic_rx_poll_one(struct hns_nic_ring_data *ring_data,\n\t\t\t       int budget, void *v)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct sk_buff *skb;\n\tint num, bnum;\n#define RCB_NOF_ALLOC_RX_BUFF_ONCE 16\n\tint recv_pkts, recv_bds, clean_count, err;\n\tint unused_count = hns_desc_unused(ring);\n\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\trmb(); /* make sure num taken effect before the other data is touched */\n\n\trecv_pkts = 0, recv_bds = 0, clean_count = 0;\n\tnum -= unused_count;\n\n\twhile (recv_pkts < budget && recv_bds < num) {\n\t\t/* reuse or realloc buffers */\n\t\tif (clean_count + unused_count >= RCB_NOF_ALLOC_RX_BUFF_ONCE) {\n\t\t\thns_nic_alloc_rx_buffers(ring_data,\n\t\t\t\t\t\t clean_count + unused_count);\n\t\t\tclean_count = 0;\n\t\t\tunused_count = hns_desc_unused(ring);\n\t\t}\n\n\t\t/* poll one pkt */\n\t\terr = hns_nic_poll_rx_skb(ring_data, &skb, &bnum);\n\t\tif (unlikely(!skb)) /* this fault cannot be repaired */\n\t\t\tgoto out;\n\n\t\trecv_bds += bnum;\n\t\tclean_count += bnum;\n\t\tif (unlikely(err)) {  /* do jump the err */\n\t\t\trecv_pkts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* do update ip stack process*/\n\t\t((void (*)(struct hns_nic_ring_data *, struct sk_buff *))v)(\n\t\t\t\t\t\t\tring_data, skb);\n\t\trecv_pkts++;\n\t}\n\nout:\n\t/* make all data has been write before submit */\n\tif (clean_count + unused_count > 0)\n\t\thns_nic_alloc_rx_buffers(ring_data,\n\t\t\t\t\t clean_count + unused_count);\n\n\treturn recv_pkts;\n}\n\nstatic bool hns_nic_rx_fini_pro(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint num = 0;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\n\t/* for hardware bug fixed */\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\n\tif (num > 0) {\n\t\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\t\tring_data->ring, 1);\n\n\t\treturn false;\n\t} else {\n\t\treturn true;\n\t}\n}\n\nstatic bool hns_nic_rx_fini_pro_v2(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint num;\n\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\n\tif (!num)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic inline void hns_nic_reclaim_one_desc(struct hnae_ring *ring,\n\t\t\t\t\t    int *bytes, int *pkts)\n{\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\t(*pkts) += (desc_cb->type == DESC_TYPE_SKB);\n\t(*bytes) += desc_cb->length;\n\t/* desc_cb will be cleaned, after hnae_free_buffer_detach*/\n\thnae_free_buffer_detach(ring, ring->next_to_clean);\n\n\tring_ptr_move_fw(ring, next_to_clean);\n}\n\nstatic int is_valid_clean_head(struct hnae_ring *ring, int h)\n{\n\tint u = ring->next_to_use;\n\tint c = ring->next_to_clean;\n\n\tif (unlikely(h > ring->desc_num))\n\t\treturn 0;\n\n\tassert(u > 0 && u < ring->desc_num);\n\tassert(c > 0 && c < ring->desc_num);\n\tassert(u != c && h != c); /* must be checked before call this func */\n\n\treturn u > c ? (h > c && h <= u) : (h > c || h <= u);\n}\n\n/* netif_tx_lock will turn down the performance, set only when necessary */\n#ifdef CONFIG_NET_POLL_CONTROLLER\n#define NETIF_TX_LOCK(ring) spin_lock(&(ring)->lock)\n#define NETIF_TX_UNLOCK(ring) spin_unlock(&(ring)->lock)\n#else\n#define NETIF_TX_LOCK(ring)\n#define NETIF_TX_UNLOCK(ring)\n#endif\n\n/* reclaim all desc in one budget\n * return error or number of desc left\n */\nstatic int hns_nic_tx_poll_one(struct hns_nic_ring_data *ring_data,\n\t\t\t       int budget, void *v)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct netdev_queue *dev_queue;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tint head;\n\tint bytes, pkts;\n\n\tNETIF_TX_LOCK(ring);\n\n\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\trmb(); /* make sure head is ready before touch any data */\n\n\tif (is_ring_empty(ring) || head == ring->next_to_clean) {\n\t\tNETIF_TX_UNLOCK(ring);\n\t\treturn 0; /* no data to poll */\n\t}\n\n\tif (!is_valid_clean_head(ring, head)) {\n\t\tnetdev_err(ndev, \"wrong head (%d, %d-%d)\\n\", head,\n\t\t\t   ring->next_to_use, ring->next_to_clean);\n\t\tring->stats.io_err_cnt++;\n\t\tNETIF_TX_UNLOCK(ring);\n\t\treturn -EIO;\n\t}\n\n\tbytes = 0;\n\tpkts = 0;\n\twhile (head != ring->next_to_clean) {\n\t\thns_nic_reclaim_one_desc(ring, &bytes, &pkts);\n\t\t/* issue prefetch for next Tx descriptor */\n\t\tprefetch(&ring->desc_cb[ring->next_to_clean]);\n\t}\n\n\tNETIF_TX_UNLOCK(ring);\n\n\tdev_queue = netdev_get_tx_queue(ndev, ring_data->queue_index);\n\tnetdev_tx_completed_queue(dev_queue, pkts, bytes);\n\n\tif (unlikely(priv->link && !netif_carrier_ok(ndev)))\n\t\tnetif_carrier_on(ndev);\n\n\tif (unlikely(pkts && netif_carrier_ok(ndev) &&\n\t\t     (ring_space(ring) >= ring->max_desc_num_per_pkt * 2))) {\n\t\t/* Make sure that anybody stopping the queue after this\n\t\t * sees the new next_to_clean.\n\t\t */\n\t\tsmp_mb();\n\t\tif (netif_tx_queue_stopped(dev_queue) &&\n\t\t    !test_bit(NIC_STATE_DOWN, &priv->state)) {\n\t\t\tnetif_tx_wake_queue(dev_queue);\n\t\t\tring->stats.restart_queue++;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool hns_nic_tx_fini_pro(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint head;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\n\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\n\tif (head != ring->next_to_clean) {\n\t\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\t\tring_data->ring, 1);\n\n\t\treturn false;\n\t} else {\n\t\treturn true;\n\t}\n}\n\nstatic bool hns_nic_tx_fini_pro_v2(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint head = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\n\tif (head == ring->next_to_clean)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void hns_nic_tx_clr_all_bufs(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct netdev_queue *dev_queue;\n\tint head;\n\tint bytes, pkts;\n\n\tNETIF_TX_LOCK(ring);\n\n\thead = ring->next_to_use; /* ntu :soft setted ring position*/\n\tbytes = 0;\n\tpkts = 0;\n\twhile (head != ring->next_to_clean)\n\t\thns_nic_reclaim_one_desc(ring, &bytes, &pkts);\n\n\tNETIF_TX_UNLOCK(ring);\n\n\tdev_queue = netdev_get_tx_queue(ndev, ring_data->queue_index);\n\tnetdev_tx_reset_queue(dev_queue);\n}\n\nstatic int hns_nic_common_poll(struct napi_struct *napi, int budget)\n{\n\tint clean_complete = 0;\n\tstruct hns_nic_ring_data *ring_data =\n\t\tcontainer_of(napi, struct hns_nic_ring_data, napi);\n\tstruct hnae_ring *ring = ring_data->ring;\n\ntry_again:\n\tclean_complete += ring_data->poll_one(\n\t\t\t\tring_data, budget - clean_complete,\n\t\t\t\tring_data->ex_process);\n\n\tif (clean_complete < budget) {\n\t\tif (ring_data->fini_process(ring_data)) {\n\t\t\tnapi_complete(napi);\n\t\t\tring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\t\t} else {\n\t\t\tgoto try_again;\n\t\t}\n\t}\n\n\treturn clean_complete;\n}\n\nstatic irqreturn_t hns_irq_handle(int irq, void *dev)\n{\n\tstruct hns_nic_ring_data *ring_data = (struct hns_nic_ring_data *)dev;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\tring_data->ring, 1);\n\tnapi_schedule(&ring_data->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/**\n *hns_nic_adjust_link - adjust net work mode by the phy stat or new param\n *@ndev: net device\n */\nstatic void hns_nic_adjust_link(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint state = 1;\n\n\tif (ndev->phydev) {\n\t\th->dev->ops->adjust_link(h, ndev->phydev->speed,\n\t\t\t\t\t ndev->phydev->duplex);\n\t\tstate = ndev->phydev->link;\n\t}\n\tstate = state && h->dev->ops->get_status(h);\n\n\tif (state != priv->link) {\n\t\tif (state) {\n\t\t\tnetif_carrier_on(ndev);\n\t\t\tnetif_tx_wake_all_queues(ndev);\n\t\t\tnetdev_info(ndev, \"link up\\n\");\n\t\t} else {\n\t\t\tnetif_carrier_off(ndev);\n\t\t\tnetdev_info(ndev, \"link down\\n\");\n\t\t}\n\t\tpriv->link = state;\n\t}\n}\n\n/**\n *hns_nic_init_phy - init phy\n *@ndev: net device\n *@h: ae handle\n * Return 0 on success, negative on failure\n */\nint hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h)\n{\n\tstruct phy_device *phy_dev = h->phy_dev;\n\tint ret;\n\n\tif (!h->phy_dev)\n\t\treturn 0;\n\n\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII) {\n\t\tphy_dev->dev_flags = 0;\n\n\t\tret = phy_connect_direct(ndev, phy_dev, hns_nic_adjust_link,\n\t\t\t\t\t h->phy_if);\n\t} else {\n\t\tret = phy_attach_direct(ndev, phy_dev, 0, h->phy_if);\n\t}\n\tif (unlikely(ret))\n\t\treturn -ENODEV;\n\n\tphy_dev->supported &= h->if_support;\n\tphy_dev->advertising = phy_dev->supported;\n\n\tif (h->phy_if == PHY_INTERFACE_MODE_XGMII)\n\t\tphy_dev->autoneg = false;\n\n\treturn 0;\n}\n\nstatic int hns_nic_ring_open(struct net_device *netdev, int idx)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tnapi_enable(&priv->ring_data[idx].napi);\n\n\tenable_irq(priv->ring_data[idx].ring->irq);\n\th->dev->ops->toggle_ring_irq(priv->ring_data[idx].ring, 0);\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_set_mac_address(struct net_device *ndev, void *p)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct sockaddr *mac_addr = p;\n\tint ret;\n\n\tif (!mac_addr || !is_valid_ether_addr((const u8 *)mac_addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tret = h->dev->ops->set_mac_addr(h, mac_addr->sa_data);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"set_mac_address fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tmemcpy(ndev->dev_addr, mac_addr->sa_data, ndev->addr_len);\n\n\treturn 0;\n}\n\nvoid hns_nic_update_stats(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\th->dev->ops->update_stats(h, &netdev->stats);\n}\n\n/* set mac addr if it is configed. or leave it to the AE driver */\nstatic void hns_init_mac_addr(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (!device_get_mac_address(priv->dev, ndev->dev_addr, ETH_ALEN)) {\n\t\teth_hw_addr_random(ndev);\n\t\tdev_warn(priv->dev, \"No valid mac, use random mac %pM\",\n\t\t\t ndev->dev_addr);\n\t}\n}\n\nstatic void hns_nic_ring_close(struct net_device *netdev, int idx)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\th->dev->ops->toggle_ring_irq(priv->ring_data[idx].ring, 1);\n\tdisable_irq(priv->ring_data[idx].ring->irq);\n\n\tnapi_disable(&priv->ring_data[idx].napi);\n}\n\nstatic int hns_nic_init_affinity_mask(int q_num, int ring_idx,\n\t\t\t\t      struct hnae_ring *ring, cpumask_t *mask)\n{\n\tint cpu;\n\n\t/* Diffrent irq banlance between 16core and 32core.\n\t * The cpu mask set by ring index according to the ring flag\n\t * which indicate the ring is tx or rx.\n\t */\n\tif (q_num == num_possible_cpus()) {\n\t\tif (is_tx_ring(ring))\n\t\t\tcpu = ring_idx;\n\t\telse\n\t\t\tcpu = ring_idx - q_num;\n\t} else {\n\t\tif (is_tx_ring(ring))\n\t\t\tcpu = ring_idx * 2;\n\t\telse\n\t\t\tcpu = (ring_idx - q_num) * 2 + 1;\n\t}\n\n\tcpumask_clear(mask);\n\tcpumask_set_cpu(cpu, mask);\n\n\treturn cpu;\n}\n\nstatic int hns_nic_init_irq(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hns_nic_ring_data *rd;\n\tint i;\n\tint ret;\n\tint cpu;\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\trd = &priv->ring_data[i];\n\n\t\tif (rd->ring->irq_init_flag == RCB_IRQ_INITED)\n\t\t\tbreak;\n\n\t\tsnprintf(rd->ring->ring_name, RCB_RING_NAME_LEN,\n\t\t\t \"%s-%s%d\", priv->netdev->name,\n\t\t\t (is_tx_ring(rd->ring) ? \"tx\" : \"rx\"), rd->queue_index);\n\n\t\trd->ring->ring_name[RCB_RING_NAME_LEN - 1] = '\\0';\n\n\t\tret = request_irq(rd->ring->irq,\n\t\t\t\t  hns_irq_handle, 0, rd->ring->ring_name, rd);\n\t\tif (ret) {\n\t\t\tnetdev_err(priv->netdev, \"request irq(%d) fail\\n\",\n\t\t\t\t   rd->ring->irq);\n\t\t\treturn ret;\n\t\t}\n\t\tdisable_irq(rd->ring->irq);\n\n\t\tcpu = hns_nic_init_affinity_mask(h->q_num, i,\n\t\t\t\t\t\t rd->ring, &rd->mask);\n\n\t\tif (cpu_online(cpu))\n\t\t\tirq_set_affinity_hint(rd->ring->irq,\n\t\t\t\t\t      &rd->mask);\n\n\t\trd->ring->irq_init_flag = RCB_IRQ_INITED;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_up(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint i, j;\n\tint ret;\n\n\tret = hns_nic_init_irq(priv);\n\tif (ret != 0) {\n\t\tnetdev_err(ndev, \"hns init irq failed! ret=%d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\tret = hns_nic_ring_open(ndev, i);\n\t\tif (ret)\n\t\t\tgoto out_has_some_queues;\n\t}\n\n\tret = h->dev->ops->set_mac_addr(h, ndev->dev_addr);\n\tif (ret)\n\t\tgoto out_set_mac_addr_err;\n\n\tret = h->dev->ops->start ? h->dev->ops->start(h) : 0;\n\tif (ret)\n\t\tgoto out_start_err;\n\n\tif (ndev->phydev)\n\t\tphy_start(ndev->phydev);\n\n\tclear_bit(NIC_STATE_DOWN, &priv->state);\n\t(void)mod_timer(&priv->service_timer, jiffies + SERVICE_TIMER_HZ);\n\n\treturn 0;\n\nout_start_err:\n\tnetif_stop_queue(ndev);\nout_set_mac_addr_err:\nout_has_some_queues:\n\tfor (j = i - 1; j >= 0; j--)\n\t\thns_nic_ring_close(ndev, j);\n\n\tset_bit(NIC_STATE_DOWN, &priv->state);\n\n\treturn ret;\n}\n\nstatic void hns_nic_net_down(struct net_device *ndev)\n{\n\tint i;\n\tstruct hnae_ae_ops *ops;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (test_and_set_bit(NIC_STATE_DOWN, &priv->state))\n\t\treturn;\n\n\t(void)del_timer_sync(&priv->service_timer);\n\tnetif_tx_stop_all_queues(ndev);\n\tnetif_carrier_off(ndev);\n\tnetif_tx_disable(ndev);\n\tpriv->link = 0;\n\n\tif (ndev->phydev)\n\t\tphy_stop(ndev->phydev);\n\n\tops = priv->ae_handle->dev->ops;\n\n\tif (ops->stop)\n\t\tops->stop(priv->ae_handle);\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\tfor (i = priv->ae_handle->q_num - 1; i >= 0; i--) {\n\t\thns_nic_ring_close(ndev, i);\n\t\thns_nic_ring_close(ndev, i + priv->ae_handle->q_num);\n\n\t\t/* clean tx buffers*/\n\t\thns_nic_tx_clr_all_bufs(priv->ring_data + i);\n\t}\n}\n\nvoid hns_nic_net_reset(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *handle = priv->ae_handle;\n\n\twhile (test_and_set_bit(NIC_STATE_RESETTING, &priv->state))\n\t\tusleep_range(1000, 2000);\n\n\t(void)hnae_reinit_handle(handle);\n\n\tclear_bit(NIC_STATE_RESETTING, &priv->state);\n}\n\nvoid hns_nic_net_reinit(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tnetif_trans_update(priv->netdev);\n\twhile (test_and_set_bit(NIC_STATE_REINITING, &priv->state))\n\t\tusleep_range(1000, 2000);\n\n\thns_nic_net_down(netdev);\n\thns_nic_net_reset(netdev);\n\t(void)hns_nic_net_up(netdev);\n\tclear_bit(NIC_STATE_REINITING, &priv->state);\n}\n\nstatic int hns_nic_net_open(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint ret;\n\n\tif (test_bit(NIC_STATE_TESTING, &priv->state))\n\t\treturn -EBUSY;\n\n\tpriv->link = 0;\n\tnetif_carrier_off(ndev);\n\n\tret = netif_set_real_num_tx_queues(ndev, h->q_num);\n\tif (ret < 0) {\n\t\tnetdev_err(ndev, \"netif_set_real_num_tx_queues fail, ret=%d!\\n\",\n\t\t\t   ret);\n\t\treturn ret;\n\t}\n\n\tret = netif_set_real_num_rx_queues(ndev, h->q_num);\n\tif (ret < 0) {\n\t\tnetdev_err(ndev,\n\t\t\t   \"netif_set_real_num_rx_queues fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_nic_net_up(ndev);\n\tif (ret) {\n\t\tnetdev_err(ndev,\n\t\t\t   \"hns net up fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_stop(struct net_device *ndev)\n{\n\thns_nic_net_down(ndev);\n\n\treturn 0;\n}\n\nstatic void hns_tx_timeout_reset(struct hns_nic_priv *priv);\nstatic void hns_nic_net_timeout(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\thns_tx_timeout_reset(priv);\n}\n\nstatic int hns_nic_do_ioctl(struct net_device *netdev, struct ifreq *ifr,\n\t\t\t    int cmd)\n{\n\tstruct phy_device *phy_dev = netdev->phydev;\n\n\tif (!netif_running(netdev))\n\t\treturn -EINVAL;\n\n\tif (!phy_dev)\n\t\treturn -ENOTSUPP;\n\n\treturn phy_mii_ioctl(phy_dev, ifr, cmd);\n}\n\n/* use only for netconsole to poll with the device without interrupt */\n#ifdef CONFIG_NET_POLL_CONTROLLER\nvoid hns_nic_poll_controller(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tunsigned long flags;\n\tint i;\n\n\tlocal_irq_save(flags);\n\tfor (i = 0; i < priv->ae_handle->q_num * 2; i++)\n\t\tnapi_schedule(&priv->ring_data[i].napi);\n\tlocal_irq_restore(flags);\n}\n#endif\n\nstatic netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tint ret;\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\tret = hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t  &tx_ring_data(priv, skb->queue_mapping));\n\tif (ret == NETDEV_TX_OK) {\n\t\tnetif_trans_update(ndev);\n\t\tndev->stats.tx_bytes += skb->len;\n\t\tndev->stats.tx_packets++;\n\t}\n\treturn (netdev_tx_t)ret;\n}\n\nstatic void hns_nic_drop_rx_fetch(struct hns_nic_ring_data *ring_data,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tdev_kfree_skb_any(skb);\n}\n\n#define HNS_LB_TX_RING\t0\nstatic struct sk_buff *hns_assemble_skb(struct net_device *ndev)\n{\n\tstruct sk_buff *skb;\n\tstruct ethhdr *ethhdr;\n\tint frame_len;\n\n\t/* allocate test skb */\n\tskb = alloc_skb(64, GFP_KERNEL);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_put(skb, 64);\n\tskb->dev = ndev;\n\tmemset(skb->data, 0xFF, skb->len);\n\n\t/* must be tcp/ip package */\n\tethhdr = (struct ethhdr *)skb->data;\n\tethhdr->h_proto = htons(ETH_P_IP);\n\n\tframe_len = skb->len & (~1ul);\n\tmemset(&skb->data[frame_len / 2], 0xAA,\n\t       frame_len / 2 - 1);\n\n\tskb->queue_mapping = HNS_LB_TX_RING;\n\n\treturn skb;\n}\n\nstatic int hns_enable_serdes_lb(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tint speed, duplex;\n\tint ret;\n\n\tret = ops->set_loopback(h, MAC_INTERNALLOOP_SERDES, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ops->start ? ops->start(h) : 0;\n\tif (ret)\n\t\treturn ret;\n\n\t/* link adjust duplex*/\n\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII)\n\t\tspeed = 1000;\n\telse\n\t\tspeed = 10000;\n\tduplex = 1;\n\n\tops->adjust_link(h, speed, duplex);\n\n\t/* wait h/w ready */\n\tmdelay(300);\n\n\treturn 0;\n}\n\nstatic void hns_disable_serdes_lb(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\n\tops->stop(h);\n\tops->set_loopback(h, MAC_INTERNALLOOP_SERDES, 0);\n}\n\n/**\n *hns_nic_clear_all_rx_fetch - clear the chip fetched descriptions. The\n *function as follows:\n *    1. if one rx ring has found the page_offset is not equal 0 between head\n *       and tail, it means that the chip fetched the wrong descs for the ring\n *       which buffer size is 4096.\n *    2. we set the chip serdes loopback and set rss indirection to the ring.\n *    3. construct 64-bytes ip broadcast packages, wait the associated rx ring\n *       recieving all packages and it will fetch new descriptions.\n *    4. recover to the original state.\n *\n *@ndev: net device\n */\nstatic int hns_nic_clear_all_rx_fetch(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tstruct hns_nic_ring_data *rd;\n\tstruct hnae_ring *ring;\n\tstruct sk_buff *skb;\n\tu32 *org_indir;\n\tu32 *cur_indir;\n\tint indir_size;\n\tint head, tail;\n\tint fetch_num;\n\tint i, j;\n\tbool found;\n\tint retry_times;\n\tint ret = 0;\n\n\t/* alloc indir memory */\n\tindir_size = ops->get_rss_indir_size(h) * sizeof(*org_indir);\n\torg_indir = kzalloc(indir_size, GFP_KERNEL);\n\tif (!org_indir)\n\t\treturn -ENOMEM;\n\n\t/* store the orginal indirection */\n\tops->get_rss(h, org_indir, NULL, NULL);\n\n\tcur_indir = kzalloc(indir_size, GFP_KERNEL);\n\tif (!cur_indir) {\n\t\tret = -ENOMEM;\n\t\tgoto cur_indir_alloc_err;\n\t}\n\n\t/* set loopback */\n\tif (hns_enable_serdes_lb(ndev)) {\n\t\tret = -EINVAL;\n\t\tgoto enable_serdes_lb_err;\n\t}\n\n\t/* foreach every rx ring to clear fetch desc */\n\tfor (i = 0; i < h->q_num; i++) {\n\t\tring = &h->qs[i]->rx_ring;\n\t\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\t\ttail = readl_relaxed(ring->io_base + RCB_REG_TAIL);\n\t\tfound = false;\n\t\tfetch_num = ring_dist(ring, head, tail);\n\n\t\twhile (head != tail) {\n\t\t\tif (ring->desc_cb[head].page_offset != 0) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\thead++;\n\t\t\tif (head == ring->desc_num)\n\t\t\t\thead = 0;\n\t\t}\n\n\t\tif (found) {\n\t\t\tfor (j = 0; j < indir_size / sizeof(*org_indir); j++)\n\t\t\t\tcur_indir[j] = i;\n\t\t\tops->set_rss(h, cur_indir, NULL, 0);\n\n\t\t\tfor (j = 0; j < fetch_num; j++) {\n\t\t\t\t/* alloc one skb and init */\n\t\t\t\tskb = hns_assemble_skb(ndev);\n\t\t\t\tif (!skb)\n\t\t\t\t\tgoto out;\n\t\t\t\trd = &tx_ring_data(priv, skb->queue_mapping);\n\t\t\t\thns_nic_net_xmit_hw(ndev, skb, rd);\n\n\t\t\t\tretry_times = 0;\n\t\t\t\twhile (retry_times++ < 10) {\n\t\t\t\t\tmdelay(10);\n\t\t\t\t\t/* clean rx */\n\t\t\t\t\trd = &rx_ring_data(priv, i);\n\t\t\t\t\tif (rd->poll_one(rd, fetch_num,\n\t\t\t\t\t\t\t hns_nic_drop_rx_fetch))\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tretry_times = 0;\n\t\t\t\twhile (retry_times++ < 10) {\n\t\t\t\t\tmdelay(10);\n\t\t\t\t\t/* clean tx ring 0 send package */\n\t\t\t\t\trd = &tx_ring_data(priv,\n\t\t\t\t\t\t\t   HNS_LB_TX_RING);\n\t\t\t\t\tif (rd->poll_one(rd, fetch_num, NULL))\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\t/* restore everything */\n\tops->set_rss(h, org_indir, NULL, 0);\n\thns_disable_serdes_lb(ndev);\nenable_serdes_lb_err:\n\tkfree(cur_indir);\ncur_indir_alloc_err:\n\tkfree(org_indir);\n\n\treturn ret;\n}\n\nstatic int hns_nic_change_mtu(struct net_device *ndev, int new_mtu)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tbool if_running = netif_running(ndev);\n\tint ret;\n\n\t/* MTU < 68 is an error and causes problems on some kernels */\n\tif (new_mtu < 68)\n\t\treturn -EINVAL;\n\n\t/* MTU no change */\n\tif (new_mtu == ndev->mtu)\n\t\treturn 0;\n\n\tif (!h->dev->ops->set_mtu)\n\t\treturn -ENOTSUPP;\n\n\tif (if_running) {\n\t\t(void)hns_nic_net_stop(ndev);\n\t\tmsleep(100);\n\t}\n\n\tif (priv->enet_ver != AE_VERSION_1 &&\n\t    ndev->mtu <= BD_SIZE_2048_MAX_MTU &&\n\t    new_mtu > BD_SIZE_2048_MAX_MTU) {\n\t\t/* update desc */\n\t\thnae_reinit_all_ring_desc(h);\n\n\t\t/* clear the package which the chip has fetched */\n\t\tret = hns_nic_clear_all_rx_fetch(ndev);\n\n\t\t/* the page offset must be consist with desc */\n\t\thnae_reinit_all_ring_page_off(h);\n\n\t\tif (ret) {\n\t\t\tnetdev_err(ndev, \"clear the fetched desc fail\\n\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = h->dev->ops->set_mtu(h, new_mtu);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"set mtu fail, return value %d\\n\",\n\t\t\t   ret);\n\t\tgoto out;\n\t}\n\n\t/* finally, set new mtu to netdevice */\n\tndev->mtu = new_mtu;\n\nout:\n\tif (if_running) {\n\t\tif (hns_nic_net_open(ndev)) {\n\t\t\tnetdev_err(ndev, \"hns net open fail\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_nic_set_features(struct net_device *netdev,\n\t\t\t\tnetdev_features_t features)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_1:\n\t\tif (features & (NETIF_F_TSO | NETIF_F_TSO6))\n\t\t\tnetdev_info(netdev, \"enet v1 do not support tso!\\n\");\n\t\tbreak;\n\tdefault:\n\t\tif (features & (NETIF_F_TSO | NETIF_F_TSO6)) {\n\t\t\tpriv->ops.fill_desc = fill_tso_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tso;\n\t\t\t/* The chip only support 7*4096 */\n\t\t\tnetif_set_gso_max_size(netdev, 7 * 4096);\n\t\t} else {\n\t\t\tpriv->ops.fill_desc = fill_v2_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t\t}\n\t\tbreak;\n\t}\n\tnetdev->features = features;\n\treturn 0;\n}\n\nstatic netdev_features_t hns_nic_fix_features(\n\t\tstruct net_device *netdev, netdev_features_t features)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_1:\n\t\tfeatures &= ~(NETIF_F_TSO | NETIF_F_TSO6 |\n\t\t\t\tNETIF_F_HW_VLAN_CTAG_FILTER);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn features;\n}\n\nstatic int hns_nic_uc_sync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->add_uc_addr)\n\t\treturn h->dev->ops->add_uc_addr(h, addr);\n\n\treturn 0;\n}\n\nstatic int hns_nic_uc_unsync(struct net_device *netdev,\n\t\t\t     const unsigned char *addr)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->rm_uc_addr)\n\t\treturn h->dev->ops->rm_uc_addr(h, addr);\n\n\treturn 0;\n}\n\n/**\n * nic_set_multicast_list - set mutl mac address\n * @netdev: net device\n * @p: mac address\n *\n * return void\n */\nvoid hns_set_multicast_list(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct netdev_hw_addr *ha = NULL;\n\n\tif (!h)\t{\n\t\tnetdev_err(ndev, \"hnae handle is null\\n\");\n\t\treturn;\n\t}\n\n\tif (h->dev->ops->clr_mc_addr)\n\t\tif (h->dev->ops->clr_mc_addr(h))\n\t\t\tnetdev_err(ndev, \"clear multicast address fail\\n\");\n\n\tif (h->dev->ops->set_mc_addr) {\n\t\tnetdev_for_each_mc_addr(ha, ndev)\n\t\t\tif (h->dev->ops->set_mc_addr(h, ha->addr))\n\t\t\t\tnetdev_err(ndev, \"set multicast fail\\n\");\n\t}\n}\n\nvoid hns_nic_set_rx_mode(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->set_promisc_mode) {\n\t\tif (ndev->flags & IFF_PROMISC)\n\t\t\th->dev->ops->set_promisc_mode(h, 1);\n\t\telse\n\t\t\th->dev->ops->set_promisc_mode(h, 0);\n\t}\n\n\thns_set_multicast_list(ndev);\n\n\tif (__dev_uc_sync(ndev, hns_nic_uc_sync, hns_nic_uc_unsync))\n\t\tnetdev_err(ndev, \"sync uc address fail\\n\");\n}\n\nstatic void hns_nic_get_stats64(struct net_device *ndev,\n\t\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tint idx = 0;\n\tu64 tx_bytes = 0;\n\tu64 rx_bytes = 0;\n\tu64 tx_pkts = 0;\n\tu64 rx_pkts = 0;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tfor (idx = 0; idx < h->q_num; idx++) {\n\t\ttx_bytes += h->qs[idx]->tx_ring.stats.tx_bytes;\n\t\ttx_pkts += h->qs[idx]->tx_ring.stats.tx_pkts;\n\t\trx_bytes += h->qs[idx]->rx_ring.stats.rx_bytes;\n\t\trx_pkts += h->qs[idx]->rx_ring.stats.rx_pkts;\n\t}\n\n\tstats->tx_bytes = tx_bytes;\n\tstats->tx_packets = tx_pkts;\n\tstats->rx_bytes = rx_bytes;\n\tstats->rx_packets = rx_pkts;\n\n\tstats->rx_errors = ndev->stats.rx_errors;\n\tstats->multicast = ndev->stats.multicast;\n\tstats->rx_length_errors = ndev->stats.rx_length_errors;\n\tstats->rx_crc_errors = ndev->stats.rx_crc_errors;\n\tstats->rx_missed_errors = ndev->stats.rx_missed_errors;\n\n\tstats->tx_errors = ndev->stats.tx_errors;\n\tstats->rx_dropped = ndev->stats.rx_dropped;\n\tstats->tx_dropped = ndev->stats.tx_dropped;\n\tstats->collisions = ndev->stats.collisions;\n\tstats->rx_over_errors = ndev->stats.rx_over_errors;\n\tstats->rx_frame_errors = ndev->stats.rx_frame_errors;\n\tstats->rx_fifo_errors = ndev->stats.rx_fifo_errors;\n\tstats->tx_aborted_errors = ndev->stats.tx_aborted_errors;\n\tstats->tx_carrier_errors = ndev->stats.tx_carrier_errors;\n\tstats->tx_fifo_errors = ndev->stats.tx_fifo_errors;\n\tstats->tx_heartbeat_errors = ndev->stats.tx_heartbeat_errors;\n\tstats->tx_window_errors = ndev->stats.tx_window_errors;\n\tstats->rx_compressed = ndev->stats.rx_compressed;\n\tstats->tx_compressed = ndev->stats.tx_compressed;\n}\n\nstatic u16\nhns_nic_select_queue(struct net_device *ndev, struct sk_buff *skb,\n\t\t     void *accel_priv, select_queue_fallback_t fallback)\n{\n\tstruct ethhdr *eth_hdr = (struct ethhdr *)skb->data;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\t/* fix hardware broadcast/multicast packets queue loopback */\n\tif (!AE_IS_VER1(priv->enet_ver) &&\n\t    is_multicast_ether_addr(eth_hdr->h_dest))\n\t\treturn 0;\n\telse\n\t\treturn fallback(ndev, skb);\n}\n\nstatic const struct net_device_ops hns_nic_netdev_ops = {\n\t.ndo_open = hns_nic_net_open,\n\t.ndo_stop = hns_nic_net_stop,\n\t.ndo_start_xmit = hns_nic_net_xmit,\n\t.ndo_tx_timeout = hns_nic_net_timeout,\n\t.ndo_set_mac_address = hns_nic_net_set_mac_address,\n\t.ndo_change_mtu = hns_nic_change_mtu,\n\t.ndo_do_ioctl = hns_nic_do_ioctl,\n\t.ndo_set_features = hns_nic_set_features,\n\t.ndo_fix_features = hns_nic_fix_features,\n\t.ndo_get_stats64 = hns_nic_get_stats64,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller = hns_nic_poll_controller,\n#endif\n\t.ndo_set_rx_mode = hns_nic_set_rx_mode,\n\t.ndo_select_queue = hns_nic_select_queue,\n};\n\nstatic void hns_nic_update_link_status(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->phy_dev) {\n\t\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII)\n\t\t\treturn;\n\n\t\t(void)genphy_read_status(h->phy_dev);\n\t}\n\thns_nic_adjust_link(netdev);\n}\n\n/* for dumping key regs*/\nstatic void hns_nic_dump(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tu32 *data, reg_num, i;\n\n\tif (ops->get_regs_len && ops->get_regs) {\n\t\treg_num = ops->get_regs_len(priv->ae_handle);\n\t\treg_num = (reg_num + 3ul) & ~3ul;\n\t\tdata = kcalloc(reg_num, sizeof(u32), GFP_KERNEL);\n\t\tif (data) {\n\t\t\tops->get_regs(priv->ae_handle, data);\n\t\t\tfor (i = 0; i < reg_num; i += 4)\n\t\t\t\tpr_info(\"0x%08x: 0x%08x 0x%08x 0x%08x 0x%08x\\n\",\n\t\t\t\t\ti, data[i], data[i + 1],\n\t\t\t\t\tdata[i + 2], data[i + 3]);\n\t\t\tkfree(data);\n\t\t}\n\t}\n\n\tfor (i = 0; i < h->q_num; i++) {\n\t\tpr_info(\"tx_queue%d_next_to_clean:%d\\n\",\n\t\t\ti, h->qs[i]->tx_ring.next_to_clean);\n\t\tpr_info(\"tx_queue%d_next_to_use:%d\\n\",\n\t\t\ti, h->qs[i]->tx_ring.next_to_use);\n\t\tpr_info(\"rx_queue%d_next_to_clean:%d\\n\",\n\t\t\ti, h->qs[i]->rx_ring.next_to_clean);\n\t\tpr_info(\"rx_queue%d_next_to_use:%d\\n\",\n\t\t\ti, h->qs[i]->rx_ring.next_to_use);\n\t}\n}\n\n/* for resetting subtask */\nstatic void hns_nic_reset_subtask(struct hns_nic_priv *priv)\n{\n\tenum hnae_port_type type = priv->ae_handle->port_type;\n\n\tif (!test_bit(NIC_STATE2_RESET_REQUESTED, &priv->state))\n\t\treturn;\n\tclear_bit(NIC_STATE2_RESET_REQUESTED, &priv->state);\n\n\t/* If we're already down, removing or resetting, just bail */\n\tif (test_bit(NIC_STATE_DOWN, &priv->state) ||\n\t    test_bit(NIC_STATE_REMOVING, &priv->state) ||\n\t    test_bit(NIC_STATE_RESETTING, &priv->state))\n\t\treturn;\n\n\thns_nic_dump(priv);\n\tnetdev_info(priv->netdev, \"try to reset %s port!\\n\",\n\t\t    (type == HNAE_PORT_DEBUG ? \"debug\" : \"service\"));\n\n\trtnl_lock();\n\t/* put off any impending NetWatchDogTimeout */\n\tnetif_trans_update(priv->netdev);\n\n\tif (type == HNAE_PORT_DEBUG) {\n\t\thns_nic_net_reinit(priv->netdev);\n\t} else {\n\t\tnetif_carrier_off(priv->netdev);\n\t\tnetif_tx_disable(priv->netdev);\n\t}\n\trtnl_unlock();\n}\n\n/* for doing service complete*/\nstatic void hns_nic_service_event_complete(struct hns_nic_priv *priv)\n{\n\tWARN_ON(!test_bit(NIC_STATE_SERVICE_SCHED, &priv->state));\n\t/* make sure to commit the things */\n\tsmp_mb__before_atomic();\n\tclear_bit(NIC_STATE_SERVICE_SCHED, &priv->state);\n}\n\nstatic void hns_nic_service_task(struct work_struct *work)\n{\n\tstruct hns_nic_priv *priv\n\t\t= container_of(work, struct hns_nic_priv, service_task);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\thns_nic_update_link_status(priv->netdev);\n\th->dev->ops->update_led_status(h);\n\thns_nic_update_stats(priv->netdev);\n\n\thns_nic_reset_subtask(priv);\n\thns_nic_service_event_complete(priv);\n}\n\nstatic void hns_nic_task_schedule(struct hns_nic_priv *priv)\n{\n\tif (!test_bit(NIC_STATE_DOWN, &priv->state) &&\n\t    !test_bit(NIC_STATE_REMOVING, &priv->state) &&\n\t    !test_and_set_bit(NIC_STATE_SERVICE_SCHED, &priv->state))\n\t\t(void)schedule_work(&priv->service_task);\n}\n\nstatic void hns_nic_service_timer(unsigned long data)\n{\n\tstruct hns_nic_priv *priv = (struct hns_nic_priv *)data;\n\n\t(void)mod_timer(&priv->service_timer, jiffies + SERVICE_TIMER_HZ);\n\n\thns_nic_task_schedule(priv);\n}\n\n/**\n * hns_tx_timeout_reset - initiate reset due to Tx timeout\n * @priv: driver private struct\n **/\nstatic void hns_tx_timeout_reset(struct hns_nic_priv *priv)\n{\n\t/* Do the reset outside of interrupt context */\n\tif (!test_bit(NIC_STATE_DOWN, &priv->state)) {\n\t\tset_bit(NIC_STATE2_RESET_REQUESTED, &priv->state);\n\t\tnetdev_warn(priv->netdev,\n\t\t\t    \"initiating reset due to tx timeout(%llu,0x%lx)\\n\",\n\t\t\t    priv->tx_timeout_count, priv->state);\n\t\tpriv->tx_timeout_count++;\n\t\thns_nic_task_schedule(priv);\n\t}\n}\n\nstatic int hns_nic_init_ring_data(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hns_nic_ring_data *rd;\n\tbool is_ver1 = AE_IS_VER1(priv->enet_ver);\n\tint i;\n\n\tif (h->q_num > NIC_MAX_Q_PER_VF) {\n\t\tnetdev_err(priv->netdev, \"too much queue (%d)\\n\", h->q_num);\n\t\treturn -EINVAL;\n\t}\n\n\tpriv->ring_data = kzalloc(h->q_num * sizeof(*priv->ring_data) * 2,\n\t\t\t\t  GFP_KERNEL);\n\tif (!priv->ring_data)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < h->q_num; i++) {\n\t\trd = &priv->ring_data[i];\n\t\trd->queue_index = i;\n\t\trd->ring = &h->qs[i]->tx_ring;\n\t\trd->poll_one = hns_nic_tx_poll_one;\n\t\trd->fini_process = is_ver1 ? hns_nic_tx_fini_pro :\n\t\t\thns_nic_tx_fini_pro_v2;\n\n\t\tnetif_napi_add(priv->netdev, &rd->napi,\n\t\t\t       hns_nic_common_poll, NIC_TX_CLEAN_MAX_NUM);\n\t\trd->ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\tfor (i = h->q_num; i < h->q_num * 2; i++) {\n\t\trd = &priv->ring_data[i];\n\t\trd->queue_index = i - h->q_num;\n\t\trd->ring = &h->qs[i - h->q_num]->rx_ring;\n\t\trd->poll_one = hns_nic_rx_poll_one;\n\t\trd->ex_process = hns_nic_rx_up_pro;\n\t\trd->fini_process = is_ver1 ? hns_nic_rx_fini_pro :\n\t\t\thns_nic_rx_fini_pro_v2;\n\n\t\tnetif_napi_add(priv->netdev, &rd->napi,\n\t\t\t       hns_nic_common_poll, NIC_RX_CLEAN_MAX_NUM);\n\t\trd->ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\n\treturn 0;\n}\n\nstatic void hns_nic_uninit_ring_data(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint i;\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\tnetif_napi_del(&priv->ring_data[i].napi);\n\t\tif (priv->ring_data[i].ring->irq_init_flag == RCB_IRQ_INITED) {\n\t\t\t(void)irq_set_affinity_hint(\n\t\t\t\tpriv->ring_data[i].ring->irq,\n\t\t\t\tNULL);\n\t\t\tfree_irq(priv->ring_data[i].ring->irq,\n\t\t\t\t &priv->ring_data[i]);\n\t\t}\n\n\t\tpriv->ring_data[i].ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\tkfree(priv->ring_data);\n}\n\nstatic void hns_nic_set_priv_ops(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (AE_IS_VER1(priv->enet_ver)) {\n\t\tpriv->ops.fill_desc = fill_desc;\n\t\tpriv->ops.get_rxd_bnum = get_rx_desc_bnum;\n\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t} else {\n\t\tpriv->ops.get_rxd_bnum = get_v2rx_desc_bnum;\n\t\tif ((netdev->features & NETIF_F_TSO) ||\n\t\t    (netdev->features & NETIF_F_TSO6)) {\n\t\t\tpriv->ops.fill_desc = fill_tso_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tso;\n\t\t\t/* This chip only support 7*4096 */\n\t\t\tnetif_set_gso_max_size(netdev, 7 * 4096);\n\t\t} else {\n\t\t\tpriv->ops.fill_desc = fill_v2_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t\t}\n\t\t/* enable tso when init\n\t\t * control tso on/off through TSE bit in bd\n\t\t */\n\t\th->dev->ops->set_tso_stats(h, 1);\n\t}\n}\n\nstatic int hns_nic_try_get_ae(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h;\n\tint ret;\n\n\th = hnae_get_handle(&priv->netdev->dev,\n\t\t\t    priv->fwnode, priv->port_id, NULL);\n\tif (IS_ERR_OR_NULL(h)) {\n\t\tret = -ENODEV;\n\t\tdev_dbg(priv->dev, \"has not handle, register notifier!\\n\");\n\t\tgoto out;\n\t}\n\tpriv->ae_handle = h;\n\n\tret = hns_nic_init_phy(ndev, h);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"probe phy device fail!\\n\");\n\t\tgoto out_init_phy;\n\t}\n\n\tret = hns_nic_init_ring_data(priv);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_init_ring_data;\n\t}\n\n\thns_nic_set_priv_ops(ndev);\n\n\tret = register_netdev(ndev);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"probe register netdev fail!\\n\");\n\t\tgoto out_reg_ndev_fail;\n\t}\n\treturn 0;\n\nout_reg_ndev_fail:\n\thns_nic_uninit_ring_data(priv);\n\tpriv->ring_data = NULL;\nout_init_phy:\nout_init_ring_data:\n\thnae_put_handle(priv->ae_handle);\n\tpriv->ae_handle = NULL;\nout:\n\treturn ret;\n}\n\nstatic int hns_nic_notifier_action(struct notifier_block *nb,\n\t\t\t\t   unsigned long action, void *data)\n{\n\tstruct hns_nic_priv *priv =\n\t\tcontainer_of(nb, struct hns_nic_priv, notifier_block);\n\n\tassert(action == HNAE_AE_REGISTER);\n\n\tif (!hns_nic_try_get_ae(priv->netdev)) {\n\t\thnae_unregister_notifier(&priv->notifier_block);\n\t\tpriv->notifier_block.notifier_call = NULL;\n\t}\n\treturn 0;\n}\n\nstatic int hns_nic_dev_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct net_device *ndev;\n\tstruct hns_nic_priv *priv;\n\tu32 port_id;\n\tint ret;\n\n\tndev = alloc_etherdev_mq(sizeof(struct hns_nic_priv), NIC_MAX_Q_PER_VF);\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, ndev);\n\n\tpriv = netdev_priv(ndev);\n\tpriv->dev = dev;\n\tpriv->netdev = ndev;\n\n\tif (dev_of_node(dev)) {\n\t\tstruct device_node *ae_node;\n\n\t\tif (of_device_is_compatible(dev->of_node,\n\t\t\t\t\t    \"hisilicon,hns-nic-v1\"))\n\t\t\tpriv->enet_ver = AE_VERSION_1;\n\t\telse\n\t\t\tpriv->enet_ver = AE_VERSION_2;\n\n\t\tae_node = of_parse_phandle(dev->of_node, \"ae-handle\", 0);\n\t\tif (IS_ERR_OR_NULL(ae_node)) {\n\t\t\tret = PTR_ERR(ae_node);\n\t\t\tdev_err(dev, \"not find ae-handle\\n\");\n\t\t\tgoto out_read_prop_fail;\n\t\t}\n\t\tpriv->fwnode = &ae_node->fwnode;\n\t} else if (is_acpi_node(dev->fwnode)) {\n\t\tstruct acpi_reference_args args;\n\n\t\tif (acpi_dev_found(hns_enet_acpi_match[0].id))\n\t\t\tpriv->enet_ver = AE_VERSION_1;\n\t\telse if (acpi_dev_found(hns_enet_acpi_match[1].id))\n\t\t\tpriv->enet_ver = AE_VERSION_2;\n\t\telse\n\t\t\treturn -ENXIO;\n\n\t\t/* try to find port-idx-in-ae first */\n\t\tret = acpi_node_get_property_reference(dev->fwnode,\n\t\t\t\t\t\t       \"ae-handle\", 0, &args);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"not find ae-handle\\n\");\n\t\t\tgoto out_read_prop_fail;\n\t\t}\n\t\tpriv->fwnode = acpi_fwnode_handle(args.adev);\n\t} else {\n\t\tdev_err(dev, \"cannot read cfg data from OF or acpi\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tret = device_property_read_u32(dev, \"port-idx-in-ae\", &port_id);\n\tif (ret) {\n\t\t/* only for old code compatible */\n\t\tret = device_property_read_u32(dev, \"port-id\", &port_id);\n\t\tif (ret)\n\t\t\tgoto out_read_prop_fail;\n\t\t/* for old dts, we need to caculate the port offset */\n\t\tport_id = port_id < HNS_SRV_OFFSET ? port_id + HNS_DEBUG_OFFSET\n\t\t\t: port_id - HNS_SRV_OFFSET;\n\t}\n\tpriv->port_id = port_id;\n\n\thns_init_mac_addr(ndev);\n\n\tndev->watchdog_timeo = HNS_NIC_TX_TIMEOUT;\n\tndev->priv_flags |= IFF_UNICAST_FLT;\n\tndev->netdev_ops = &hns_nic_netdev_ops;\n\thns_ethtool_set_ops(ndev);\n\n\tndev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\tNETIF_F_RXCSUM | NETIF_F_SG | NETIF_F_GSO |\n\t\tNETIF_F_GRO;\n\tndev->vlan_features |=\n\t\tNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM | NETIF_F_RXCSUM;\n\tndev->vlan_features |= NETIF_F_SG | NETIF_F_GSO | NETIF_F_GRO;\n\n\t/* MTU range: 68 - 9578 (v1) or 9706 (v2) */\n\tndev->min_mtu = MAC_MIN_MTU;\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_2:\n\t\tndev->features |= NETIF_F_TSO | NETIF_F_TSO6;\n\t\tndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\tNETIF_F_RXCSUM | NETIF_F_SG | NETIF_F_GSO |\n\t\t\tNETIF_F_GRO | NETIF_F_TSO | NETIF_F_TSO6;\n\t\tndev->max_mtu = MAC_MAX_MTU_V2 -\n\t\t\t\t(ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\t\tbreak;\n\tdefault:\n\t\tndev->max_mtu = MAC_MAX_MTU -\n\t\t\t\t(ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\t\tbreak;\n\t}\n\n\tSET_NETDEV_DEV(ndev, dev);\n\n\tif (!dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)))\n\t\tdev_dbg(dev, \"set mask to 64bit\\n\");\n\telse\n\t\tdev_err(dev, \"set mask to 64bit fail!\\n\");\n\n\t/* carrier off reporting is important to ethtool even BEFORE open */\n\tnetif_carrier_off(ndev);\n\n\tsetup_timer(&priv->service_timer, hns_nic_service_timer,\n\t\t    (unsigned long)priv);\n\tINIT_WORK(&priv->service_task, hns_nic_service_task);\n\n\tset_bit(NIC_STATE_SERVICE_INITED, &priv->state);\n\tclear_bit(NIC_STATE_SERVICE_SCHED, &priv->state);\n\tset_bit(NIC_STATE_DOWN, &priv->state);\n\n\tif (hns_nic_try_get_ae(priv->netdev)) {\n\t\tpriv->notifier_block.notifier_call = hns_nic_notifier_action;\n\t\tret = hnae_register_notifier(&priv->notifier_block);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"register notifier fail!\\n\");\n\t\t\tgoto out_notify_fail;\n\t\t}\n\t\tdev_dbg(dev, \"has not handle, register notifier!\\n\");\n\t}\n\n\treturn 0;\n\nout_notify_fail:\n\t(void)cancel_work_sync(&priv->service_task);\nout_read_prop_fail:\n\tfree_netdev(ndev);\n\treturn ret;\n}\n\nstatic int hns_nic_dev_remove(struct platform_device *pdev)\n{\n\tstruct net_device *ndev = platform_get_drvdata(pdev);\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (ndev->reg_state != NETREG_UNINITIALIZED)\n\t\tunregister_netdev(ndev);\n\n\tif (priv->ring_data)\n\t\thns_nic_uninit_ring_data(priv);\n\tpriv->ring_data = NULL;\n\n\tif (ndev->phydev)\n\t\tphy_disconnect(ndev->phydev);\n\n\tif (!IS_ERR_OR_NULL(priv->ae_handle))\n\t\thnae_put_handle(priv->ae_handle);\n\tpriv->ae_handle = NULL;\n\tif (priv->notifier_block.notifier_call)\n\t\thnae_unregister_notifier(&priv->notifier_block);\n\tpriv->notifier_block.notifier_call = NULL;\n\n\tset_bit(NIC_STATE_REMOVING, &priv->state);\n\t(void)cancel_work_sync(&priv->service_task);\n\n\tfree_netdev(ndev);\n\treturn 0;\n}\n\nstatic const struct of_device_id hns_enet_of_match[] = {\n\t{.compatible = \"hisilicon,hns-nic-v1\",},\n\t{.compatible = \"hisilicon,hns-nic-v2\",},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, hns_enet_of_match);\n\nstatic struct platform_driver hns_nic_dev_driver = {\n\t.driver = {\n\t\t.name = \"hns-nic\",\n\t\t.of_match_table = hns_enet_of_match,\n\t\t.acpi_match_table = ACPI_PTR(hns_enet_acpi_match),\n\t},\n\t.probe = hns_nic_dev_probe,\n\t.remove = hns_nic_dev_remove,\n};\n\nmodule_platform_driver(hns_nic_dev_driver);\n\nMODULE_DESCRIPTION(\"HISILICON HNS Ethernet driver\");\nMODULE_AUTHOR(\"Hisilicon, Inc.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"platform:hns-nic\");\n", "/*\n * Copyright (c) 2014-2015 Hisilicon Limited.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n */\n\n#ifndef __HNS_ENET_H\n#define __HNS_ENET_H\n\n#include <linux/netdevice.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n\n#include \"hnae.h\"\n\n#define HNS_DEBUG_OFFSET\t6\n#define HNS_SRV_OFFSET\t\t2\n\nenum hns_nic_state {\n\tNIC_STATE_TESTING = 0,\n\tNIC_STATE_RESETTING,\n\tNIC_STATE_REINITING,\n\tNIC_STATE_DOWN,\n\tNIC_STATE_DISABLED,\n\tNIC_STATE_REMOVING,\n\tNIC_STATE_SERVICE_INITED,\n\tNIC_STATE_SERVICE_SCHED,\n\tNIC_STATE2_RESET_REQUESTED,\n\tNIC_STATE_MAX\n};\n\nstruct hns_nic_ring_data {\n\tstruct hnae_ring *ring;\n\tstruct napi_struct napi;\n\tcpumask_t mask; /* affinity mask */\n\tint queue_index;\n\tint (*poll_one)(struct hns_nic_ring_data *, int, void *);\n\tvoid (*ex_process)(struct hns_nic_ring_data *, struct sk_buff *);\n\tbool (*fini_process)(struct hns_nic_ring_data *);\n};\n\n/* compatible the difference between two versions */\nstruct hns_nic_ops {\n\tvoid (*fill_desc)(struct hnae_ring *ring, void *priv,\n\t\t\t  int size, dma_addr_t dma, int frag_end,\n\t\t\t  int buf_num, enum hns_desc_type type, int mtu);\n\tint (*maybe_stop_tx)(struct sk_buff **out_skb,\n\t\t\t     int *bnum, struct hnae_ring *ring);\n\tvoid (*get_rxd_bnum)(u32 bnum_flag, int *out_bnum);\n};\n\nstruct hns_nic_priv {\n\tconst struct fwnode_handle      *fwnode;\n\tu32 enet_ver;\n\tu32 port_id;\n\tint phy_mode;\n\tint phy_led_val;\n\tstruct net_device *netdev;\n\tstruct device *dev;\n\tstruct hnae_handle *ae_handle;\n\n\tstruct hns_nic_ops ops;\n\n\t/* the cb for nic to manage the ring buffer, the first half of the\n\t * array is for tx_ring and vice versa for the second half\n\t */\n\tstruct hns_nic_ring_data *ring_data;\n\n\t/* The most recently read link state */\n\tint link;\n\tu64 tx_timeout_count;\n\n\tunsigned long state;\n\n\tstruct timer_list service_timer;\n\n\tstruct work_struct service_task;\n\n\tstruct notifier_block notifier_block;\n};\n\n#define tx_ring_data(priv, idx) ((priv)->ring_data[idx])\n#define rx_ring_data(priv, idx) \\\n\t((priv)->ring_data[(priv)->ae_handle->q_num + (idx)])\n\nvoid hns_ethtool_set_ops(struct net_device *ndev);\nvoid hns_nic_net_reset(struct net_device *ndev);\nvoid hns_nic_net_reinit(struct net_device *netdev);\nint hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h);\nint hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\tstruct sk_buff *skb,\n\t\t\tstruct hns_nic_ring_data *ring_data);\n\n#endif\t/**__HNS_ENET_H */\n"], "fixing_code": ["/*\n * Copyright (c) 2014-2015 Hisilicon Limited.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n */\n\n#include <linux/clk.h>\n#include <linux/cpumask.h>\n#include <linux/etherdevice.h>\n#include <linux/if_vlan.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/module.h>\n#include <linux/phy.h>\n#include <linux/platform_device.h>\n#include <linux/skbuff.h>\n\n#include \"hnae.h\"\n#include \"hns_enet.h\"\n#include \"hns_dsaf_mac.h\"\n\n#define NIC_MAX_Q_PER_VF 16\n#define HNS_NIC_TX_TIMEOUT (5 * HZ)\n\n#define SERVICE_TIMER_HZ (1 * HZ)\n\n#define NIC_TX_CLEAN_MAX_NUM 256\n#define NIC_RX_CLEAN_MAX_NUM 64\n\n#define RCB_IRQ_NOT_INITED 0\n#define RCB_IRQ_INITED 1\n#define HNS_BUFFER_SIZE_2048 2048\n\n#define BD_MAX_SEND_SIZE 8191\n#define SKB_TMP_LEN(SKB) \\\n\t(((SKB)->transport_header - (SKB)->mac_header) + tcp_hdrlen(SKB))\n\nstatic void fill_v2_desc(struct hnae_ring *ring, void *priv,\n\t\t\t int size, dma_addr_t dma, int frag_end,\n\t\t\t int buf_num, enum hns_desc_type type, int mtu)\n{\n\tstruct hnae_desc *desc = &ring->desc[ring->next_to_use];\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_use];\n\tstruct iphdr *iphdr;\n\tstruct ipv6hdr *ipv6hdr;\n\tstruct sk_buff *skb;\n\t__be16 protocol;\n\tu8 bn_pid = 0;\n\tu8 rrcfv = 0;\n\tu8 ip_offset = 0;\n\tu8 tvsvsn = 0;\n\tu16 mss = 0;\n\tu8 l4_len = 0;\n\tu16 paylen = 0;\n\n\tdesc_cb->priv = priv;\n\tdesc_cb->length = size;\n\tdesc_cb->dma = dma;\n\tdesc_cb->type = type;\n\n\tdesc->addr = cpu_to_le64(dma);\n\tdesc->tx.send_size = cpu_to_le16((u16)size);\n\n\t/* config bd buffer end */\n\thnae_set_bit(rrcfv, HNSV2_TXD_VLD_B, 1);\n\thnae_set_field(bn_pid, HNSV2_TXD_BUFNUM_M, 0, buf_num - 1);\n\n\t/* fill port_id in the tx bd for sending management pkts */\n\thnae_set_field(bn_pid, HNSV2_TXD_PORTID_M,\n\t\t       HNSV2_TXD_PORTID_S, ring->q->handle->dport_id);\n\n\tif (type == DESC_TYPE_SKB) {\n\t\tskb = (struct sk_buff *)priv;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tskb_reset_mac_len(skb);\n\t\t\tprotocol = skb->protocol;\n\t\t\tip_offset = ETH_HLEN;\n\n\t\t\tif (protocol == htons(ETH_P_8021Q)) {\n\t\t\t\tip_offset += VLAN_HLEN;\n\t\t\t\tprotocol = vlan_get_protocol(skb);\n\t\t\t\tskb->protocol = protocol;\n\t\t\t}\n\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tiphdr = ip_hdr(skb);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L3CS_B, 1);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L4CS_B, 1);\n\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tif (iphdr->protocol == IPPROTO_TCP &&\n\t\t\t\t    skb_is_gso(skb)) {\n\t\t\t\t\thnae_set_bit(tvsvsn,\n\t\t\t\t\t\t     HNSV2_TXD_TSE_B, 1);\n\t\t\t\t\tl4_len = tcp_hdrlen(skb);\n\t\t\t\t\tmss = skb_shinfo(skb)->gso_size;\n\t\t\t\t\tpaylen = skb->len - SKB_TMP_LEN(skb);\n\t\t\t\t}\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\thnae_set_bit(tvsvsn, HNSV2_TXD_IPV6_B, 1);\n\t\t\t\tipv6hdr = ipv6_hdr(skb);\n\t\t\t\thnae_set_bit(rrcfv, HNSV2_TXD_L4CS_B, 1);\n\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tif (ipv6hdr->nexthdr == IPPROTO_TCP &&\n\t\t\t\t    skb_is_gso(skb) && skb_is_gso_v6(skb)) {\n\t\t\t\t\thnae_set_bit(tvsvsn,\n\t\t\t\t\t\t     HNSV2_TXD_TSE_B, 1);\n\t\t\t\t\tl4_len = tcp_hdrlen(skb);\n\t\t\t\t\tmss = skb_shinfo(skb)->gso_size;\n\t\t\t\t\tpaylen = skb->len - SKB_TMP_LEN(skb);\n\t\t\t\t}\n\t\t\t}\n\t\t\tdesc->tx.ip_offset = ip_offset;\n\t\t\tdesc->tx.tse_vlan_snap_v6_sctp_nth = tvsvsn;\n\t\t\tdesc->tx.mss = cpu_to_le16(mss);\n\t\t\tdesc->tx.l4_len = l4_len;\n\t\t\tdesc->tx.paylen = cpu_to_le16(paylen);\n\t\t}\n\t}\n\n\thnae_set_bit(rrcfv, HNSV2_TXD_FE_B, frag_end);\n\n\tdesc->tx.bn_pid = bn_pid;\n\tdesc->tx.ra_ri_cs_fe_vld = rrcfv;\n\n\tring_ptr_move_fw(ring, next_to_use);\n}\n\nstatic const struct acpi_device_id hns_enet_acpi_match[] = {\n\t{ \"HISI00C1\", 0 },\n\t{ \"HISI00C2\", 0 },\n\t{ },\n};\nMODULE_DEVICE_TABLE(acpi, hns_enet_acpi_match);\n\nstatic void fill_desc(struct hnae_ring *ring, void *priv,\n\t\t      int size, dma_addr_t dma, int frag_end,\n\t\t      int buf_num, enum hns_desc_type type, int mtu)\n{\n\tstruct hnae_desc *desc = &ring->desc[ring->next_to_use];\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_use];\n\tstruct sk_buff *skb;\n\t__be16 protocol;\n\tu32 ip_offset;\n\tu32 asid_bufnum_pid = 0;\n\tu32 flag_ipoffset = 0;\n\n\tdesc_cb->priv = priv;\n\tdesc_cb->length = size;\n\tdesc_cb->dma = dma;\n\tdesc_cb->type = type;\n\n\tdesc->addr = cpu_to_le64(dma);\n\tdesc->tx.send_size = cpu_to_le16((u16)size);\n\n\t/*config bd buffer end */\n\tflag_ipoffset |= 1 << HNS_TXD_VLD_B;\n\n\tasid_bufnum_pid |= buf_num << HNS_TXD_BUFNUM_S;\n\n\tif (type == DESC_TYPE_SKB) {\n\t\tskb = (struct sk_buff *)priv;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tprotocol = skb->protocol;\n\t\t\tip_offset = ETH_HLEN;\n\n\t\t\t/*if it is a SW VLAN check the next protocol*/\n\t\t\tif (protocol == htons(ETH_P_8021Q)) {\n\t\t\t\tip_offset += VLAN_HLEN;\n\t\t\t\tprotocol = vlan_get_protocol(skb);\n\t\t\t\tskb->protocol = protocol;\n\t\t\t}\n\n\t\t\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L3CS_B;\n\t\t\t\t/* check for tcp/udp header */\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L4CS_B;\n\n\t\t\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\t\t\t/* ipv6 has not l3 cs, check for L4 header */\n\t\t\t\tflag_ipoffset |= 1 << HNS_TXD_L4CS_B;\n\t\t\t}\n\n\t\t\tflag_ipoffset |= ip_offset << HNS_TXD_IPOFFSET_S;\n\t\t}\n\t}\n\n\tflag_ipoffset |= frag_end << HNS_TXD_FE_B;\n\n\tdesc->tx.asid_bufnum_pid = cpu_to_le16(asid_bufnum_pid);\n\tdesc->tx.flag_ipoffset = cpu_to_le32(flag_ipoffset);\n\n\tring_ptr_move_fw(ring, next_to_use);\n}\n\nstatic void unfill_desc(struct hnae_ring *ring)\n{\n\tring_ptr_move_bw(ring, next_to_use);\n}\n\nstatic int hns_nic_maybe_stop_tx(\n\tstruct sk_buff **out_skb, int *bnum, struct hnae_ring *ring)\n{\n\tstruct sk_buff *skb = *out_skb;\n\tstruct sk_buff *new_skb = NULL;\n\tint buf_num;\n\n\t/* no. of segments (plus a header) */\n\tbuf_num = skb_shinfo(skb)->nr_frags + 1;\n\n\tif (unlikely(buf_num > ring->max_desc_num_per_pkt)) {\n\t\tif (ring_space(ring) < 1)\n\t\t\treturn -EBUSY;\n\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!new_skb)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_kfree_skb_any(skb);\n\t\t*out_skb = new_skb;\n\t\tbuf_num = 1;\n\t} else if (buf_num > ring_space(ring)) {\n\t\treturn -EBUSY;\n\t}\n\n\t*bnum = buf_num;\n\treturn 0;\n}\n\nstatic int hns_nic_maybe_stop_tso(\n\tstruct sk_buff **out_skb, int *bnum, struct hnae_ring *ring)\n{\n\tint i;\n\tint size;\n\tint buf_num;\n\tint frag_num;\n\tstruct sk_buff *skb = *out_skb;\n\tstruct sk_buff *new_skb = NULL;\n\tstruct skb_frag_struct *frag;\n\n\tsize = skb_headlen(skb);\n\tbuf_num = (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\n\tfrag_num = skb_shinfo(skb)->nr_frags;\n\tfor (i = 0; i < frag_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tsize = skb_frag_size(frag);\n\t\tbuf_num += (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\t}\n\n\tif (unlikely(buf_num > ring->max_desc_num_per_pkt)) {\n\t\tbuf_num = (skb->len + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\t\tif (ring_space(ring) < buf_num)\n\t\t\treturn -EBUSY;\n\t\t/* manual split the send packet */\n\t\tnew_skb = skb_copy(skb, GFP_ATOMIC);\n\t\tif (!new_skb)\n\t\t\treturn -ENOMEM;\n\t\tdev_kfree_skb_any(skb);\n\t\t*out_skb = new_skb;\n\n\t} else if (ring_space(ring) < buf_num) {\n\t\treturn -EBUSY;\n\t}\n\n\t*bnum = buf_num;\n\treturn 0;\n}\n\nstatic void fill_tso_desc(struct hnae_ring *ring, void *priv,\n\t\t\t  int size, dma_addr_t dma, int frag_end,\n\t\t\t  int buf_num, enum hns_desc_type type, int mtu)\n{\n\tint frag_buf_num;\n\tint sizeoflast;\n\tint k;\n\n\tfrag_buf_num = (size + BD_MAX_SEND_SIZE - 1) / BD_MAX_SEND_SIZE;\n\tsizeoflast = size % BD_MAX_SEND_SIZE;\n\tsizeoflast = sizeoflast ? sizeoflast : BD_MAX_SEND_SIZE;\n\n\t/* when the frag size is bigger than hardware, split this frag */\n\tfor (k = 0; k < frag_buf_num; k++)\n\t\tfill_v2_desc(ring, priv,\n\t\t\t     (k == frag_buf_num - 1) ?\n\t\t\t\t\tsizeoflast : BD_MAX_SEND_SIZE,\n\t\t\t     dma + BD_MAX_SEND_SIZE * k,\n\t\t\t     frag_end && (k == frag_buf_num - 1) ? 1 : 0,\n\t\t\t     buf_num,\n\t\t\t     (type == DESC_TYPE_SKB && !k) ?\n\t\t\t\t\tDESC_TYPE_SKB : DESC_TYPE_PAGE,\n\t\t\t     mtu);\n}\n\nnetdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct hns_nic_ring_data *ring_data)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct device *dev = ring_to_dev(ring);\n\tstruct netdev_queue *dev_queue;\n\tstruct skb_frag_struct *frag;\n\tint buf_num;\n\tint seg_num;\n\tdma_addr_t dma;\n\tint size, next_to_use;\n\tint i;\n\n\tswitch (priv->ops.maybe_stop_tx(&skb, &buf_num, ring)) {\n\tcase -EBUSY:\n\t\tring->stats.tx_busy++;\n\t\tgoto out_net_tx_busy;\n\tcase -ENOMEM:\n\t\tring->stats.sw_err_cnt++;\n\t\tnetdev_err(ndev, \"no memory to xmit!\\n\");\n\t\tgoto out_err_tx_ok;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* no. of segments (plus a header) */\n\tseg_num = skb_shinfo(skb)->nr_frags + 1;\n\tnext_to_use = ring->next_to_use;\n\n\t/* fill the first part */\n\tsize = skb_headlen(skb);\n\tdma = dma_map_single(dev, skb->data, size, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma)) {\n\t\tnetdev_err(ndev, \"TX head DMA map failed\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\tgoto out_err_tx_ok;\n\t}\n\tpriv->ops.fill_desc(ring, skb, size, dma, seg_num == 1 ? 1 : 0,\n\t\t\t    buf_num, DESC_TYPE_SKB, ndev->mtu);\n\n\t/* fill the fragments */\n\tfor (i = 1; i < seg_num; i++) {\n\t\tfrag = &skb_shinfo(skb)->frags[i - 1];\n\t\tsize = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(dev, frag, 0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, dma)) {\n\t\t\tnetdev_err(ndev, \"TX frag(%d) DMA map failed\\n\", i);\n\t\t\tring->stats.sw_err_cnt++;\n\t\t\tgoto out_map_frag_fail;\n\t\t}\n\t\tpriv->ops.fill_desc(ring, skb_frag_page(frag), size, dma,\n\t\t\t\t    seg_num - 1 == i ? 1 : 0, buf_num,\n\t\t\t\t    DESC_TYPE_PAGE, ndev->mtu);\n\t}\n\n\t/*complete translate all packets*/\n\tdev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);\n\tnetdev_tx_sent_queue(dev_queue, skb->len);\n\n\tnetif_trans_update(ndev);\n\tndev->stats.tx_bytes += skb->len;\n\tndev->stats.tx_packets++;\n\n\twmb(); /* commit all data before submit */\n\tassert(skb->queue_mapping < priv->ae_handle->q_num);\n\thnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);\n\tring->stats.tx_pkts++;\n\tring->stats.tx_bytes += skb->len;\n\n\treturn NETDEV_TX_OK;\n\nout_map_frag_fail:\n\n\twhile (ring->next_to_use != next_to_use) {\n\t\tunfill_desc(ring);\n\t\tif (ring->next_to_use != next_to_use)\n\t\t\tdma_unmap_page(dev,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].dma,\n\t\t\t\t       ring->desc_cb[ring->next_to_use].length,\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(dev,\n\t\t\t\t\t ring->desc_cb[next_to_use].dma,\n\t\t\t\t\t ring->desc_cb[next_to_use].length,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\nout_err_tx_ok:\n\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n\nout_net_tx_busy:\n\n\tnetif_stop_subqueue(ndev, skb->queue_mapping);\n\n\t/* Herbert's original patch had:\n\t *  smp_mb__after_netif_stop_queue();\n\t * but since that doesn't exist yet, just open code it.\n\t */\n\tsmp_mb();\n\treturn NETDEV_TX_BUSY;\n}\n\n/**\n * hns_nic_get_headlen - determine size of header for RSC/LRO/GRO/FCOE\n * @data: pointer to the start of the headers\n * @max: total length of section to find headers in\n *\n * This function is meant to determine the length of headers that will\n * be recognized by hardware for LRO, GRO, and RSC offloads.  The main\n * motivation of doing this is to only perform one pull for IPv4 TCP\n * packets so that we can do basic things like calculating the gso_size\n * based on the average data per packet.\n **/\nstatic unsigned int hns_nic_get_headlen(unsigned char *data, u32 flag,\n\t\t\t\t\tunsigned int max_size)\n{\n\tunsigned char *network;\n\tu8 hlen;\n\n\t/* this should never happen, but better safe than sorry */\n\tif (max_size < ETH_HLEN)\n\t\treturn max_size;\n\n\t/* initialize network frame pointer */\n\tnetwork = data;\n\n\t/* set first protocol and move network header forward */\n\tnetwork += ETH_HLEN;\n\n\t/* handle any vlan tag if present */\n\tif (hnae_get_field(flag, HNS_RXD_VLAN_M, HNS_RXD_VLAN_S)\n\t\t== HNS_RX_FLAG_VLAN_PRESENT) {\n\t\tif ((typeof(max_size))(network - data) > (max_size - VLAN_HLEN))\n\t\t\treturn max_size;\n\n\t\tnetwork += VLAN_HLEN;\n\t}\n\n\t/* handle L3 protocols */\n\tif (hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S)\n\t\t== HNS_RX_FLAG_L3ID_IPV4) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct iphdr)))\n\t\t\treturn max_size;\n\n\t\t/* access ihl as a u8 to avoid unaligned access on ia64 */\n\t\thlen = (network[0] & 0x0F) << 2;\n\n\t\t/* verify hlen meets minimum size requirements */\n\t\tif (hlen < sizeof(struct iphdr))\n\t\t\treturn network - data;\n\n\t\t/* record next protocol if header is present */\n\t} else if (hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S)\n\t\t== HNS_RX_FLAG_L3ID_IPV6) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct ipv6hdr)))\n\t\t\treturn max_size;\n\n\t\t/* record next protocol */\n\t\thlen = sizeof(struct ipv6hdr);\n\t} else {\n\t\treturn network - data;\n\t}\n\n\t/* relocate pointer to start of L4 header */\n\tnetwork += hlen;\n\n\t/* finally sort out TCP/UDP */\n\tif (hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S)\n\t\t== HNS_RX_FLAG_L4ID_TCP) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct tcphdr)))\n\t\t\treturn max_size;\n\n\t\t/* access doff as a u8 to avoid unaligned access on ia64 */\n\t\thlen = (network[12] & 0xF0) >> 2;\n\n\t\t/* verify hlen meets minimum size requirements */\n\t\tif (hlen < sizeof(struct tcphdr))\n\t\t\treturn network - data;\n\n\t\tnetwork += hlen;\n\t} else if (hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S)\n\t\t== HNS_RX_FLAG_L4ID_UDP) {\n\t\tif ((typeof(max_size))(network - data) >\n\t\t    (max_size - sizeof(struct udphdr)))\n\t\t\treturn max_size;\n\n\t\tnetwork += sizeof(struct udphdr);\n\t}\n\n\t/* If everything has gone correctly network should be the\n\t * data section of the packet and will be the end of the header.\n\t * If not then it probably represents the end of the last recognized\n\t * header.\n\t */\n\tif ((typeof(max_size))(network - data) < max_size)\n\t\treturn network - data;\n\telse\n\t\treturn max_size;\n}\n\nstatic void hns_nic_reuse_page(struct sk_buff *skb, int i,\n\t\t\t       struct hnae_ring *ring, int pull_len,\n\t\t\t       struct hnae_desc_cb *desc_cb)\n{\n\tstruct hnae_desc *desc;\n\tint truesize, size;\n\tint last_offset;\n\tbool twobufs;\n\n\ttwobufs = ((PAGE_SIZE < 8192) &&\n\t\thnae_buf_size(ring) == HNS_BUFFER_SIZE_2048);\n\n\tdesc = &ring->desc[ring->next_to_clean];\n\tsize = le16_to_cpu(desc->rx.size);\n\n\tif (twobufs) {\n\t\ttruesize = hnae_buf_size(ring);\n\t} else {\n\t\ttruesize = ALIGN(size, L1_CACHE_BYTES);\n\t\tlast_offset = hnae_page_size(ring) - hnae_buf_size(ring);\n\t}\n\n\tskb_add_rx_frag(skb, i, desc_cb->priv, desc_cb->page_offset + pull_len,\n\t\t\tsize - pull_len, truesize - pull_len);\n\n\t /* avoid re-using remote pages,flag default unreuse */\n\tif (unlikely(page_to_nid(desc_cb->priv) != numa_node_id()))\n\t\treturn;\n\n\tif (twobufs) {\n\t\t/* if we are only owner of page we can reuse it */\n\t\tif (likely(page_count(desc_cb->priv) == 1)) {\n\t\t\t/* flip page offset to other buffer */\n\t\t\tdesc_cb->page_offset ^= truesize;\n\n\t\t\tdesc_cb->reuse_flag = 1;\n\t\t\t/* bump ref count on page before it is given*/\n\t\t\tget_page(desc_cb->priv);\n\t\t}\n\t\treturn;\n\t}\n\n\t/* move offset up to the next cache line */\n\tdesc_cb->page_offset += truesize;\n\n\tif (desc_cb->page_offset <= last_offset) {\n\t\tdesc_cb->reuse_flag = 1;\n\t\t/* bump ref count on page before it is given*/\n\t\tget_page(desc_cb->priv);\n\t}\n}\n\nstatic void get_v2rx_desc_bnum(u32 bnum_flag, int *out_bnum)\n{\n\t*out_bnum = hnae_get_field(bnum_flag,\n\t\t\t\t   HNS_RXD_BUFNUM_M, HNS_RXD_BUFNUM_S) + 1;\n}\n\nstatic void get_rx_desc_bnum(u32 bnum_flag, int *out_bnum)\n{\n\t*out_bnum = hnae_get_field(bnum_flag,\n\t\t\t\t   HNS_RXD_BUFNUM_M, HNS_RXD_BUFNUM_S);\n}\n\nstatic void hns_nic_rx_checksum(struct hns_nic_ring_data *ring_data,\n\t\t\t\tstruct sk_buff *skb, u32 flag)\n{\n\tstruct net_device *netdev = ring_data->napi.dev;\n\tu32 l3id;\n\tu32 l4id;\n\n\t/* check if RX checksum offload is enabled */\n\tif (unlikely(!(netdev->features & NETIF_F_RXCSUM)))\n\t\treturn;\n\n\t/* In hardware, we only support checksum for the following protocols:\n\t * 1) IPv4,\n\t * 2) TCP(over IPv4 or IPv6),\n\t * 3) UDP(over IPv4 or IPv6),\n\t * 4) SCTP(over IPv4 or IPv6)\n\t * but we support many L3(IPv4, IPv6, MPLS, PPPoE etc) and L4(TCP,\n\t * UDP, GRE, SCTP, IGMP, ICMP etc.) protocols.\n\t *\n\t * Hardware limitation:\n\t * Our present hardware RX Descriptor lacks L3/L4 checksum \"Status &\n\t * Error\" bit (which usually can be used to indicate whether checksum\n\t * was calculated by the hardware and if there was any error encountered\n\t * during checksum calculation).\n\t *\n\t * Software workaround:\n\t * We do get info within the RX descriptor about the kind of L3/L4\n\t * protocol coming in the packet and the error status. These errors\n\t * might not just be checksum errors but could be related to version,\n\t * length of IPv4, UDP, TCP etc.\n\t * Because there is no-way of knowing if it is a L3/L4 error due to bad\n\t * checksum or any other L3/L4 error, we will not (cannot) convey\n\t * checksum status for such cases to upper stack and will not maintain\n\t * the RX L3/L4 checksum counters as well.\n\t */\n\n\tl3id = hnae_get_field(flag, HNS_RXD_L3ID_M, HNS_RXD_L3ID_S);\n\tl4id = hnae_get_field(flag, HNS_RXD_L4ID_M, HNS_RXD_L4ID_S);\n\n\t/*  check L3 protocol for which checksum is supported */\n\tif ((l3id != HNS_RX_FLAG_L3ID_IPV4) && (l3id != HNS_RX_FLAG_L3ID_IPV6))\n\t\treturn;\n\n\t/* check for any(not just checksum)flagged L3 protocol errors */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_L3E_B)))\n\t\treturn;\n\n\t/* we do not support checksum of fragmented packets */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_FRAG_B)))\n\t\treturn;\n\n\t/*  check L4 protocol for which checksum is supported */\n\tif ((l4id != HNS_RX_FLAG_L4ID_TCP) &&\n\t    (l4id != HNS_RX_FLAG_L4ID_UDP) &&\n\t    (l4id != HNS_RX_FLAG_L4ID_SCTP))\n\t\treturn;\n\n\t/* check for any(not just checksum)flagged L4 protocol errors */\n\tif (unlikely(hnae_get_bit(flag, HNS_RXD_L4E_B)))\n\t\treturn;\n\n\t/* now, this has to be a packet with valid RX checksum */\n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n}\n\nstatic int hns_nic_poll_rx_skb(struct hns_nic_ring_data *ring_data,\n\t\t\t       struct sk_buff **out_skb, int *out_bnum)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct sk_buff *skb;\n\tstruct hnae_desc *desc;\n\tstruct hnae_desc_cb *desc_cb;\n\tunsigned char *va;\n\tint bnum, length, i;\n\tint pull_len;\n\tu32 bnum_flag;\n\n\tdesc = &ring->desc[ring->next_to_clean];\n\tdesc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\tprefetch(desc);\n\n\tva = (unsigned char *)desc_cb->buf + desc_cb->page_offset;\n\n\t/* prefetch first cache line of first page */\n\tprefetch(va);\n#if L1_CACHE_BYTES < 128\n\tprefetch(va + L1_CACHE_BYTES);\n#endif\n\n\tskb = *out_skb = napi_alloc_skb(&ring_data->napi,\n\t\t\t\t\tHNS_RX_HEAD_SIZE);\n\tif (unlikely(!skb)) {\n\t\tnetdev_err(ndev, \"alloc rx skb fail\\n\");\n\t\tring->stats.sw_err_cnt++;\n\t\treturn -ENOMEM;\n\t}\n\n\tprefetchw(skb->data);\n\tlength = le16_to_cpu(desc->rx.pkt_len);\n\tbnum_flag = le32_to_cpu(desc->rx.ipoff_bnum_pid_flag);\n\tpriv->ops.get_rxd_bnum(bnum_flag, &bnum);\n\t*out_bnum = bnum;\n\n\tif (length <= HNS_RX_HEAD_SIZE) {\n\t\tmemcpy(__skb_put(skb, length), va, ALIGN(length, sizeof(long)));\n\n\t\t/* we can reuse buffer as-is, just make sure it is local */\n\t\tif (likely(page_to_nid(desc_cb->priv) == numa_node_id()))\n\t\t\tdesc_cb->reuse_flag = 1;\n\t\telse /* this page cannot be reused so discard it */\n\t\t\tput_page(desc_cb->priv);\n\n\t\tring_ptr_move_fw(ring, next_to_clean);\n\n\t\tif (unlikely(bnum != 1)) { /* check err*/\n\t\t\t*out_bnum = 1;\n\t\t\tgoto out_bnum_err;\n\t\t}\n\t} else {\n\t\tring->stats.seg_pkt_cnt++;\n\n\t\tpull_len = hns_nic_get_headlen(va, bnum_flag, HNS_RX_HEAD_SIZE);\n\t\tmemcpy(__skb_put(skb, pull_len), va,\n\t\t       ALIGN(pull_len, sizeof(long)));\n\n\t\thns_nic_reuse_page(skb, 0, ring, pull_len, desc_cb);\n\t\tring_ptr_move_fw(ring, next_to_clean);\n\n\t\tif (unlikely(bnum >= (int)MAX_SKB_FRAGS)) { /* check err*/\n\t\t\t*out_bnum = 1;\n\t\t\tgoto out_bnum_err;\n\t\t}\n\t\tfor (i = 1; i < bnum; i++) {\n\t\t\tdesc = &ring->desc[ring->next_to_clean];\n\t\t\tdesc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\t\t\thns_nic_reuse_page(skb, i, ring, 0, desc_cb);\n\t\t\tring_ptr_move_fw(ring, next_to_clean);\n\t\t}\n\t}\n\n\t/* check except process, free skb and jump the desc */\n\tif (unlikely((!bnum) || (bnum > ring->max_desc_num_per_pkt))) {\nout_bnum_err:\n\t\t*out_bnum = *out_bnum ? *out_bnum : 1; /* ntc moved,cannot 0*/\n\t\tnetdev_err(ndev, \"invalid bnum(%d,%d,%d,%d),%016llx,%016llx\\n\",\n\t\t\t   bnum, ring->max_desc_num_per_pkt,\n\t\t\t   length, (int)MAX_SKB_FRAGS,\n\t\t\t   ((u64 *)desc)[0], ((u64 *)desc)[1]);\n\t\tring->stats.err_bd_num++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EDOM;\n\t}\n\n\tbnum_flag = le32_to_cpu(desc->rx.ipoff_bnum_pid_flag);\n\n\tif (unlikely(!hnae_get_bit(bnum_flag, HNS_RXD_VLD_B))) {\n\t\tnetdev_err(ndev, \"no valid bd,%016llx,%016llx\\n\",\n\t\t\t   ((u64 *)desc)[0], ((u64 *)desc)[1]);\n\t\tring->stats.non_vld_descs++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EINVAL;\n\t}\n\n\tif (unlikely((!desc->rx.pkt_len) ||\n\t\t     hnae_get_bit(bnum_flag, HNS_RXD_DROP_B))) {\n\t\tring->stats.err_pkt_len++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EFAULT;\n\t}\n\n\tif (unlikely(hnae_get_bit(bnum_flag, HNS_RXD_L2E_B))) {\n\t\tring->stats.l2_err++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EFAULT;\n\t}\n\n\tring->stats.rx_pkts++;\n\tring->stats.rx_bytes += skb->len;\n\n\t/* indicate to upper stack if our hardware has already calculated\n\t * the RX checksum\n\t */\n\thns_nic_rx_checksum(ring_data, skb, bnum_flag);\n\n\treturn 0;\n}\n\nstatic void\nhns_nic_alloc_rx_buffers(struct hns_nic_ring_data *ring_data, int cleand_count)\n{\n\tint i, ret;\n\tstruct hnae_desc_cb res_cbs;\n\tstruct hnae_desc_cb *desc_cb;\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\n\tfor (i = 0; i < cleand_count; i++) {\n\t\tdesc_cb = &ring->desc_cb[ring->next_to_use];\n\t\tif (desc_cb->reuse_flag) {\n\t\t\tring->stats.reuse_pg_cnt++;\n\t\t\thnae_reuse_buffer(ring, ring->next_to_use);\n\t\t} else {\n\t\t\tret = hnae_reserve_buffer_map(ring, &res_cbs);\n\t\t\tif (ret) {\n\t\t\t\tring->stats.sw_err_cnt++;\n\t\t\t\tnetdev_err(ndev, \"hnae reserve buffer map failed.\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\thnae_replace_buffer(ring, ring->next_to_use, &res_cbs);\n\t\t}\n\n\t\tring_ptr_move_fw(ring, next_to_use);\n\t}\n\n\twmb(); /* make all data has been write before submit */\n\twritel_relaxed(i, ring->io_base + RCB_REG_HEAD);\n}\n\n/* return error number for error or number of desc left to take\n */\nstatic void hns_nic_rx_up_pro(struct hns_nic_ring_data *ring_data,\n\t\t\t      struct sk_buff *skb)\n{\n\tstruct net_device *ndev = ring_data->napi.dev;\n\n\tskb->protocol = eth_type_trans(skb, ndev);\n\t(void)napi_gro_receive(&ring_data->napi, skb);\n}\n\nstatic int hns_desc_unused(struct hnae_ring *ring)\n{\n\tint ntc = ring->next_to_clean;\n\tint ntu = ring->next_to_use;\n\n\treturn ((ntc >= ntu) ? 0 : ring->desc_num) + ntc - ntu;\n}\n\nstatic int hns_nic_rx_poll_one(struct hns_nic_ring_data *ring_data,\n\t\t\t       int budget, void *v)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct sk_buff *skb;\n\tint num, bnum;\n#define RCB_NOF_ALLOC_RX_BUFF_ONCE 16\n\tint recv_pkts, recv_bds, clean_count, err;\n\tint unused_count = hns_desc_unused(ring);\n\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\trmb(); /* make sure num taken effect before the other data is touched */\n\n\trecv_pkts = 0, recv_bds = 0, clean_count = 0;\n\tnum -= unused_count;\n\n\twhile (recv_pkts < budget && recv_bds < num) {\n\t\t/* reuse or realloc buffers */\n\t\tif (clean_count + unused_count >= RCB_NOF_ALLOC_RX_BUFF_ONCE) {\n\t\t\thns_nic_alloc_rx_buffers(ring_data,\n\t\t\t\t\t\t clean_count + unused_count);\n\t\t\tclean_count = 0;\n\t\t\tunused_count = hns_desc_unused(ring);\n\t\t}\n\n\t\t/* poll one pkt */\n\t\terr = hns_nic_poll_rx_skb(ring_data, &skb, &bnum);\n\t\tif (unlikely(!skb)) /* this fault cannot be repaired */\n\t\t\tgoto out;\n\n\t\trecv_bds += bnum;\n\t\tclean_count += bnum;\n\t\tif (unlikely(err)) {  /* do jump the err */\n\t\t\trecv_pkts++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* do update ip stack process*/\n\t\t((void (*)(struct hns_nic_ring_data *, struct sk_buff *))v)(\n\t\t\t\t\t\t\tring_data, skb);\n\t\trecv_pkts++;\n\t}\n\nout:\n\t/* make all data has been write before submit */\n\tif (clean_count + unused_count > 0)\n\t\thns_nic_alloc_rx_buffers(ring_data,\n\t\t\t\t\t clean_count + unused_count);\n\n\treturn recv_pkts;\n}\n\nstatic bool hns_nic_rx_fini_pro(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint num = 0;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\n\t/* for hardware bug fixed */\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\n\tif (num > 0) {\n\t\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\t\tring_data->ring, 1);\n\n\t\treturn false;\n\t} else {\n\t\treturn true;\n\t}\n}\n\nstatic bool hns_nic_rx_fini_pro_v2(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint num;\n\n\tnum = readl_relaxed(ring->io_base + RCB_REG_FBDNUM);\n\n\tif (!num)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic inline void hns_nic_reclaim_one_desc(struct hnae_ring *ring,\n\t\t\t\t\t    int *bytes, int *pkts)\n{\n\tstruct hnae_desc_cb *desc_cb = &ring->desc_cb[ring->next_to_clean];\n\n\t(*pkts) += (desc_cb->type == DESC_TYPE_SKB);\n\t(*bytes) += desc_cb->length;\n\t/* desc_cb will be cleaned, after hnae_free_buffer_detach*/\n\thnae_free_buffer_detach(ring, ring->next_to_clean);\n\n\tring_ptr_move_fw(ring, next_to_clean);\n}\n\nstatic int is_valid_clean_head(struct hnae_ring *ring, int h)\n{\n\tint u = ring->next_to_use;\n\tint c = ring->next_to_clean;\n\n\tif (unlikely(h > ring->desc_num))\n\t\treturn 0;\n\n\tassert(u > 0 && u < ring->desc_num);\n\tassert(c > 0 && c < ring->desc_num);\n\tassert(u != c && h != c); /* must be checked before call this func */\n\n\treturn u > c ? (h > c && h <= u) : (h > c || h <= u);\n}\n\n/* netif_tx_lock will turn down the performance, set only when necessary */\n#ifdef CONFIG_NET_POLL_CONTROLLER\n#define NETIF_TX_LOCK(ring) spin_lock(&(ring)->lock)\n#define NETIF_TX_UNLOCK(ring) spin_unlock(&(ring)->lock)\n#else\n#define NETIF_TX_LOCK(ring)\n#define NETIF_TX_UNLOCK(ring)\n#endif\n\n/* reclaim all desc in one budget\n * return error or number of desc left\n */\nstatic int hns_nic_tx_poll_one(struct hns_nic_ring_data *ring_data,\n\t\t\t       int budget, void *v)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct netdev_queue *dev_queue;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tint head;\n\tint bytes, pkts;\n\n\tNETIF_TX_LOCK(ring);\n\n\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\trmb(); /* make sure head is ready before touch any data */\n\n\tif (is_ring_empty(ring) || head == ring->next_to_clean) {\n\t\tNETIF_TX_UNLOCK(ring);\n\t\treturn 0; /* no data to poll */\n\t}\n\n\tif (!is_valid_clean_head(ring, head)) {\n\t\tnetdev_err(ndev, \"wrong head (%d, %d-%d)\\n\", head,\n\t\t\t   ring->next_to_use, ring->next_to_clean);\n\t\tring->stats.io_err_cnt++;\n\t\tNETIF_TX_UNLOCK(ring);\n\t\treturn -EIO;\n\t}\n\n\tbytes = 0;\n\tpkts = 0;\n\twhile (head != ring->next_to_clean) {\n\t\thns_nic_reclaim_one_desc(ring, &bytes, &pkts);\n\t\t/* issue prefetch for next Tx descriptor */\n\t\tprefetch(&ring->desc_cb[ring->next_to_clean]);\n\t}\n\n\tNETIF_TX_UNLOCK(ring);\n\n\tdev_queue = netdev_get_tx_queue(ndev, ring_data->queue_index);\n\tnetdev_tx_completed_queue(dev_queue, pkts, bytes);\n\n\tif (unlikely(priv->link && !netif_carrier_ok(ndev)))\n\t\tnetif_carrier_on(ndev);\n\n\tif (unlikely(pkts && netif_carrier_ok(ndev) &&\n\t\t     (ring_space(ring) >= ring->max_desc_num_per_pkt * 2))) {\n\t\t/* Make sure that anybody stopping the queue after this\n\t\t * sees the new next_to_clean.\n\t\t */\n\t\tsmp_mb();\n\t\tif (netif_tx_queue_stopped(dev_queue) &&\n\t\t    !test_bit(NIC_STATE_DOWN, &priv->state)) {\n\t\t\tnetif_tx_wake_queue(dev_queue);\n\t\t\tring->stats.restart_queue++;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool hns_nic_tx_fini_pro(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint head;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\n\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\n\tif (head != ring->next_to_clean) {\n\t\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\t\tring_data->ring, 1);\n\n\t\treturn false;\n\t} else {\n\t\treturn true;\n\t}\n}\n\nstatic bool hns_nic_tx_fini_pro_v2(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tint head = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\n\tif (head == ring->next_to_clean)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void hns_nic_tx_clr_all_bufs(struct hns_nic_ring_data *ring_data)\n{\n\tstruct hnae_ring *ring = ring_data->ring;\n\tstruct net_device *ndev = ring_data->napi.dev;\n\tstruct netdev_queue *dev_queue;\n\tint head;\n\tint bytes, pkts;\n\n\tNETIF_TX_LOCK(ring);\n\n\thead = ring->next_to_use; /* ntu :soft setted ring position*/\n\tbytes = 0;\n\tpkts = 0;\n\twhile (head != ring->next_to_clean)\n\t\thns_nic_reclaim_one_desc(ring, &bytes, &pkts);\n\n\tNETIF_TX_UNLOCK(ring);\n\n\tdev_queue = netdev_get_tx_queue(ndev, ring_data->queue_index);\n\tnetdev_tx_reset_queue(dev_queue);\n}\n\nstatic int hns_nic_common_poll(struct napi_struct *napi, int budget)\n{\n\tint clean_complete = 0;\n\tstruct hns_nic_ring_data *ring_data =\n\t\tcontainer_of(napi, struct hns_nic_ring_data, napi);\n\tstruct hnae_ring *ring = ring_data->ring;\n\ntry_again:\n\tclean_complete += ring_data->poll_one(\n\t\t\t\tring_data, budget - clean_complete,\n\t\t\t\tring_data->ex_process);\n\n\tif (clean_complete < budget) {\n\t\tif (ring_data->fini_process(ring_data)) {\n\t\t\tnapi_complete(napi);\n\t\t\tring->q->handle->dev->ops->toggle_ring_irq(ring, 0);\n\t\t} else {\n\t\t\tgoto try_again;\n\t\t}\n\t}\n\n\treturn clean_complete;\n}\n\nstatic irqreturn_t hns_irq_handle(int irq, void *dev)\n{\n\tstruct hns_nic_ring_data *ring_data = (struct hns_nic_ring_data *)dev;\n\n\tring_data->ring->q->handle->dev->ops->toggle_ring_irq(\n\t\tring_data->ring, 1);\n\tnapi_schedule(&ring_data->napi);\n\n\treturn IRQ_HANDLED;\n}\n\n/**\n *hns_nic_adjust_link - adjust net work mode by the phy stat or new param\n *@ndev: net device\n */\nstatic void hns_nic_adjust_link(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint state = 1;\n\n\tif (ndev->phydev) {\n\t\th->dev->ops->adjust_link(h, ndev->phydev->speed,\n\t\t\t\t\t ndev->phydev->duplex);\n\t\tstate = ndev->phydev->link;\n\t}\n\tstate = state && h->dev->ops->get_status(h);\n\n\tif (state != priv->link) {\n\t\tif (state) {\n\t\t\tnetif_carrier_on(ndev);\n\t\t\tnetif_tx_wake_all_queues(ndev);\n\t\t\tnetdev_info(ndev, \"link up\\n\");\n\t\t} else {\n\t\t\tnetif_carrier_off(ndev);\n\t\t\tnetdev_info(ndev, \"link down\\n\");\n\t\t}\n\t\tpriv->link = state;\n\t}\n}\n\n/**\n *hns_nic_init_phy - init phy\n *@ndev: net device\n *@h: ae handle\n * Return 0 on success, negative on failure\n */\nint hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h)\n{\n\tstruct phy_device *phy_dev = h->phy_dev;\n\tint ret;\n\n\tif (!h->phy_dev)\n\t\treturn 0;\n\n\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII) {\n\t\tphy_dev->dev_flags = 0;\n\n\t\tret = phy_connect_direct(ndev, phy_dev, hns_nic_adjust_link,\n\t\t\t\t\t h->phy_if);\n\t} else {\n\t\tret = phy_attach_direct(ndev, phy_dev, 0, h->phy_if);\n\t}\n\tif (unlikely(ret))\n\t\treturn -ENODEV;\n\n\tphy_dev->supported &= h->if_support;\n\tphy_dev->advertising = phy_dev->supported;\n\n\tif (h->phy_if == PHY_INTERFACE_MODE_XGMII)\n\t\tphy_dev->autoneg = false;\n\n\treturn 0;\n}\n\nstatic int hns_nic_ring_open(struct net_device *netdev, int idx)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tnapi_enable(&priv->ring_data[idx].napi);\n\n\tenable_irq(priv->ring_data[idx].ring->irq);\n\th->dev->ops->toggle_ring_irq(priv->ring_data[idx].ring, 0);\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_set_mac_address(struct net_device *ndev, void *p)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct sockaddr *mac_addr = p;\n\tint ret;\n\n\tif (!mac_addr || !is_valid_ether_addr((const u8 *)mac_addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tret = h->dev->ops->set_mac_addr(h, mac_addr->sa_data);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"set_mac_address fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tmemcpy(ndev->dev_addr, mac_addr->sa_data, ndev->addr_len);\n\n\treturn 0;\n}\n\nvoid hns_nic_update_stats(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\th->dev->ops->update_stats(h, &netdev->stats);\n}\n\n/* set mac addr if it is configed. or leave it to the AE driver */\nstatic void hns_init_mac_addr(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (!device_get_mac_address(priv->dev, ndev->dev_addr, ETH_ALEN)) {\n\t\teth_hw_addr_random(ndev);\n\t\tdev_warn(priv->dev, \"No valid mac, use random mac %pM\",\n\t\t\t ndev->dev_addr);\n\t}\n}\n\nstatic void hns_nic_ring_close(struct net_device *netdev, int idx)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\th->dev->ops->toggle_ring_irq(priv->ring_data[idx].ring, 1);\n\tdisable_irq(priv->ring_data[idx].ring->irq);\n\n\tnapi_disable(&priv->ring_data[idx].napi);\n}\n\nstatic int hns_nic_init_affinity_mask(int q_num, int ring_idx,\n\t\t\t\t      struct hnae_ring *ring, cpumask_t *mask)\n{\n\tint cpu;\n\n\t/* Diffrent irq banlance between 16core and 32core.\n\t * The cpu mask set by ring index according to the ring flag\n\t * which indicate the ring is tx or rx.\n\t */\n\tif (q_num == num_possible_cpus()) {\n\t\tif (is_tx_ring(ring))\n\t\t\tcpu = ring_idx;\n\t\telse\n\t\t\tcpu = ring_idx - q_num;\n\t} else {\n\t\tif (is_tx_ring(ring))\n\t\t\tcpu = ring_idx * 2;\n\t\telse\n\t\t\tcpu = (ring_idx - q_num) * 2 + 1;\n\t}\n\n\tcpumask_clear(mask);\n\tcpumask_set_cpu(cpu, mask);\n\n\treturn cpu;\n}\n\nstatic int hns_nic_init_irq(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hns_nic_ring_data *rd;\n\tint i;\n\tint ret;\n\tint cpu;\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\trd = &priv->ring_data[i];\n\n\t\tif (rd->ring->irq_init_flag == RCB_IRQ_INITED)\n\t\t\tbreak;\n\n\t\tsnprintf(rd->ring->ring_name, RCB_RING_NAME_LEN,\n\t\t\t \"%s-%s%d\", priv->netdev->name,\n\t\t\t (is_tx_ring(rd->ring) ? \"tx\" : \"rx\"), rd->queue_index);\n\n\t\trd->ring->ring_name[RCB_RING_NAME_LEN - 1] = '\\0';\n\n\t\tret = request_irq(rd->ring->irq,\n\t\t\t\t  hns_irq_handle, 0, rd->ring->ring_name, rd);\n\t\tif (ret) {\n\t\t\tnetdev_err(priv->netdev, \"request irq(%d) fail\\n\",\n\t\t\t\t   rd->ring->irq);\n\t\t\treturn ret;\n\t\t}\n\t\tdisable_irq(rd->ring->irq);\n\n\t\tcpu = hns_nic_init_affinity_mask(h->q_num, i,\n\t\t\t\t\t\t rd->ring, &rd->mask);\n\n\t\tif (cpu_online(cpu))\n\t\t\tirq_set_affinity_hint(rd->ring->irq,\n\t\t\t\t\t      &rd->mask);\n\n\t\trd->ring->irq_init_flag = RCB_IRQ_INITED;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_up(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint i, j;\n\tint ret;\n\n\tret = hns_nic_init_irq(priv);\n\tif (ret != 0) {\n\t\tnetdev_err(ndev, \"hns init irq failed! ret=%d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\tret = hns_nic_ring_open(ndev, i);\n\t\tif (ret)\n\t\t\tgoto out_has_some_queues;\n\t}\n\n\tret = h->dev->ops->set_mac_addr(h, ndev->dev_addr);\n\tif (ret)\n\t\tgoto out_set_mac_addr_err;\n\n\tret = h->dev->ops->start ? h->dev->ops->start(h) : 0;\n\tif (ret)\n\t\tgoto out_start_err;\n\n\tif (ndev->phydev)\n\t\tphy_start(ndev->phydev);\n\n\tclear_bit(NIC_STATE_DOWN, &priv->state);\n\t(void)mod_timer(&priv->service_timer, jiffies + SERVICE_TIMER_HZ);\n\n\treturn 0;\n\nout_start_err:\n\tnetif_stop_queue(ndev);\nout_set_mac_addr_err:\nout_has_some_queues:\n\tfor (j = i - 1; j >= 0; j--)\n\t\thns_nic_ring_close(ndev, j);\n\n\tset_bit(NIC_STATE_DOWN, &priv->state);\n\n\treturn ret;\n}\n\nstatic void hns_nic_net_down(struct net_device *ndev)\n{\n\tint i;\n\tstruct hnae_ae_ops *ops;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (test_and_set_bit(NIC_STATE_DOWN, &priv->state))\n\t\treturn;\n\n\t(void)del_timer_sync(&priv->service_timer);\n\tnetif_tx_stop_all_queues(ndev);\n\tnetif_carrier_off(ndev);\n\tnetif_tx_disable(ndev);\n\tpriv->link = 0;\n\n\tif (ndev->phydev)\n\t\tphy_stop(ndev->phydev);\n\n\tops = priv->ae_handle->dev->ops;\n\n\tif (ops->stop)\n\t\tops->stop(priv->ae_handle);\n\n\tnetif_tx_stop_all_queues(ndev);\n\n\tfor (i = priv->ae_handle->q_num - 1; i >= 0; i--) {\n\t\thns_nic_ring_close(ndev, i);\n\t\thns_nic_ring_close(ndev, i + priv->ae_handle->q_num);\n\n\t\t/* clean tx buffers*/\n\t\thns_nic_tx_clr_all_bufs(priv->ring_data + i);\n\t}\n}\n\nvoid hns_nic_net_reset(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *handle = priv->ae_handle;\n\n\twhile (test_and_set_bit(NIC_STATE_RESETTING, &priv->state))\n\t\tusleep_range(1000, 2000);\n\n\t(void)hnae_reinit_handle(handle);\n\n\tclear_bit(NIC_STATE_RESETTING, &priv->state);\n}\n\nvoid hns_nic_net_reinit(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tnetif_trans_update(priv->netdev);\n\twhile (test_and_set_bit(NIC_STATE_REINITING, &priv->state))\n\t\tusleep_range(1000, 2000);\n\n\thns_nic_net_down(netdev);\n\thns_nic_net_reset(netdev);\n\t(void)hns_nic_net_up(netdev);\n\tclear_bit(NIC_STATE_REINITING, &priv->state);\n}\n\nstatic int hns_nic_net_open(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint ret;\n\n\tif (test_bit(NIC_STATE_TESTING, &priv->state))\n\t\treturn -EBUSY;\n\n\tpriv->link = 0;\n\tnetif_carrier_off(ndev);\n\n\tret = netif_set_real_num_tx_queues(ndev, h->q_num);\n\tif (ret < 0) {\n\t\tnetdev_err(ndev, \"netif_set_real_num_tx_queues fail, ret=%d!\\n\",\n\t\t\t   ret);\n\t\treturn ret;\n\t}\n\n\tret = netif_set_real_num_rx_queues(ndev, h->q_num);\n\tif (ret < 0) {\n\t\tnetdev_err(ndev,\n\t\t\t   \"netif_set_real_num_rx_queues fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = hns_nic_net_up(ndev);\n\tif (ret) {\n\t\tnetdev_err(ndev,\n\t\t\t   \"hns net up fail, ret=%d!\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int hns_nic_net_stop(struct net_device *ndev)\n{\n\thns_nic_net_down(ndev);\n\n\treturn 0;\n}\n\nstatic void hns_tx_timeout_reset(struct hns_nic_priv *priv);\nstatic void hns_nic_net_timeout(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\thns_tx_timeout_reset(priv);\n}\n\nstatic int hns_nic_do_ioctl(struct net_device *netdev, struct ifreq *ifr,\n\t\t\t    int cmd)\n{\n\tstruct phy_device *phy_dev = netdev->phydev;\n\n\tif (!netif_running(netdev))\n\t\treturn -EINVAL;\n\n\tif (!phy_dev)\n\t\treturn -ENOTSUPP;\n\n\treturn phy_mii_ioctl(phy_dev, ifr, cmd);\n}\n\n/* use only for netconsole to poll with the device without interrupt */\n#ifdef CONFIG_NET_POLL_CONTROLLER\nvoid hns_nic_poll_controller(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tunsigned long flags;\n\tint i;\n\n\tlocal_irq_save(flags);\n\tfor (i = 0; i < priv->ae_handle->q_num * 2; i++)\n\t\tnapi_schedule(&priv->ring_data[i].napi);\n\tlocal_irq_restore(flags);\n}\n#endif\n\nstatic netdev_tx_t hns_nic_net_xmit(struct sk_buff *skb,\n\t\t\t\t    struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tassert(skb->queue_mapping < ndev->ae_handle->q_num);\n\n\treturn hns_nic_net_xmit_hw(ndev, skb,\n\t\t\t\t   &tx_ring_data(priv, skb->queue_mapping));\n}\n\nstatic void hns_nic_drop_rx_fetch(struct hns_nic_ring_data *ring_data,\n\t\t\t\t  struct sk_buff *skb)\n{\n\tdev_kfree_skb_any(skb);\n}\n\n#define HNS_LB_TX_RING\t0\nstatic struct sk_buff *hns_assemble_skb(struct net_device *ndev)\n{\n\tstruct sk_buff *skb;\n\tstruct ethhdr *ethhdr;\n\tint frame_len;\n\n\t/* allocate test skb */\n\tskb = alloc_skb(64, GFP_KERNEL);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_put(skb, 64);\n\tskb->dev = ndev;\n\tmemset(skb->data, 0xFF, skb->len);\n\n\t/* must be tcp/ip package */\n\tethhdr = (struct ethhdr *)skb->data;\n\tethhdr->h_proto = htons(ETH_P_IP);\n\n\tframe_len = skb->len & (~1ul);\n\tmemset(&skb->data[frame_len / 2], 0xAA,\n\t       frame_len / 2 - 1);\n\n\tskb->queue_mapping = HNS_LB_TX_RING;\n\n\treturn skb;\n}\n\nstatic int hns_enable_serdes_lb(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tint speed, duplex;\n\tint ret;\n\n\tret = ops->set_loopback(h, MAC_INTERNALLOOP_SERDES, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ops->start ? ops->start(h) : 0;\n\tif (ret)\n\t\treturn ret;\n\n\t/* link adjust duplex*/\n\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII)\n\t\tspeed = 1000;\n\telse\n\t\tspeed = 10000;\n\tduplex = 1;\n\n\tops->adjust_link(h, speed, duplex);\n\n\t/* wait h/w ready */\n\tmdelay(300);\n\n\treturn 0;\n}\n\nstatic void hns_disable_serdes_lb(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\n\tops->stop(h);\n\tops->set_loopback(h, MAC_INTERNALLOOP_SERDES, 0);\n}\n\n/**\n *hns_nic_clear_all_rx_fetch - clear the chip fetched descriptions. The\n *function as follows:\n *    1. if one rx ring has found the page_offset is not equal 0 between head\n *       and tail, it means that the chip fetched the wrong descs for the ring\n *       which buffer size is 4096.\n *    2. we set the chip serdes loopback and set rss indirection to the ring.\n *    3. construct 64-bytes ip broadcast packages, wait the associated rx ring\n *       recieving all packages and it will fetch new descriptions.\n *    4. recover to the original state.\n *\n *@ndev: net device\n */\nstatic int hns_nic_clear_all_rx_fetch(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tstruct hns_nic_ring_data *rd;\n\tstruct hnae_ring *ring;\n\tstruct sk_buff *skb;\n\tu32 *org_indir;\n\tu32 *cur_indir;\n\tint indir_size;\n\tint head, tail;\n\tint fetch_num;\n\tint i, j;\n\tbool found;\n\tint retry_times;\n\tint ret = 0;\n\n\t/* alloc indir memory */\n\tindir_size = ops->get_rss_indir_size(h) * sizeof(*org_indir);\n\torg_indir = kzalloc(indir_size, GFP_KERNEL);\n\tif (!org_indir)\n\t\treturn -ENOMEM;\n\n\t/* store the orginal indirection */\n\tops->get_rss(h, org_indir, NULL, NULL);\n\n\tcur_indir = kzalloc(indir_size, GFP_KERNEL);\n\tif (!cur_indir) {\n\t\tret = -ENOMEM;\n\t\tgoto cur_indir_alloc_err;\n\t}\n\n\t/* set loopback */\n\tif (hns_enable_serdes_lb(ndev)) {\n\t\tret = -EINVAL;\n\t\tgoto enable_serdes_lb_err;\n\t}\n\n\t/* foreach every rx ring to clear fetch desc */\n\tfor (i = 0; i < h->q_num; i++) {\n\t\tring = &h->qs[i]->rx_ring;\n\t\thead = readl_relaxed(ring->io_base + RCB_REG_HEAD);\n\t\ttail = readl_relaxed(ring->io_base + RCB_REG_TAIL);\n\t\tfound = false;\n\t\tfetch_num = ring_dist(ring, head, tail);\n\n\t\twhile (head != tail) {\n\t\t\tif (ring->desc_cb[head].page_offset != 0) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\thead++;\n\t\t\tif (head == ring->desc_num)\n\t\t\t\thead = 0;\n\t\t}\n\n\t\tif (found) {\n\t\t\tfor (j = 0; j < indir_size / sizeof(*org_indir); j++)\n\t\t\t\tcur_indir[j] = i;\n\t\t\tops->set_rss(h, cur_indir, NULL, 0);\n\n\t\t\tfor (j = 0; j < fetch_num; j++) {\n\t\t\t\t/* alloc one skb and init */\n\t\t\t\tskb = hns_assemble_skb(ndev);\n\t\t\t\tif (!skb)\n\t\t\t\t\tgoto out;\n\t\t\t\trd = &tx_ring_data(priv, skb->queue_mapping);\n\t\t\t\thns_nic_net_xmit_hw(ndev, skb, rd);\n\n\t\t\t\tretry_times = 0;\n\t\t\t\twhile (retry_times++ < 10) {\n\t\t\t\t\tmdelay(10);\n\t\t\t\t\t/* clean rx */\n\t\t\t\t\trd = &rx_ring_data(priv, i);\n\t\t\t\t\tif (rd->poll_one(rd, fetch_num,\n\t\t\t\t\t\t\t hns_nic_drop_rx_fetch))\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tretry_times = 0;\n\t\t\t\twhile (retry_times++ < 10) {\n\t\t\t\t\tmdelay(10);\n\t\t\t\t\t/* clean tx ring 0 send package */\n\t\t\t\t\trd = &tx_ring_data(priv,\n\t\t\t\t\t\t\t   HNS_LB_TX_RING);\n\t\t\t\t\tif (rd->poll_one(rd, fetch_num, NULL))\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\t/* restore everything */\n\tops->set_rss(h, org_indir, NULL, 0);\n\thns_disable_serdes_lb(ndev);\nenable_serdes_lb_err:\n\tkfree(cur_indir);\ncur_indir_alloc_err:\n\tkfree(org_indir);\n\n\treturn ret;\n}\n\nstatic int hns_nic_change_mtu(struct net_device *ndev, int new_mtu)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tbool if_running = netif_running(ndev);\n\tint ret;\n\n\t/* MTU < 68 is an error and causes problems on some kernels */\n\tif (new_mtu < 68)\n\t\treturn -EINVAL;\n\n\t/* MTU no change */\n\tif (new_mtu == ndev->mtu)\n\t\treturn 0;\n\n\tif (!h->dev->ops->set_mtu)\n\t\treturn -ENOTSUPP;\n\n\tif (if_running) {\n\t\t(void)hns_nic_net_stop(ndev);\n\t\tmsleep(100);\n\t}\n\n\tif (priv->enet_ver != AE_VERSION_1 &&\n\t    ndev->mtu <= BD_SIZE_2048_MAX_MTU &&\n\t    new_mtu > BD_SIZE_2048_MAX_MTU) {\n\t\t/* update desc */\n\t\thnae_reinit_all_ring_desc(h);\n\n\t\t/* clear the package which the chip has fetched */\n\t\tret = hns_nic_clear_all_rx_fetch(ndev);\n\n\t\t/* the page offset must be consist with desc */\n\t\thnae_reinit_all_ring_page_off(h);\n\n\t\tif (ret) {\n\t\t\tnetdev_err(ndev, \"clear the fetched desc fail\\n\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = h->dev->ops->set_mtu(h, new_mtu);\n\tif (ret) {\n\t\tnetdev_err(ndev, \"set mtu fail, return value %d\\n\",\n\t\t\t   ret);\n\t\tgoto out;\n\t}\n\n\t/* finally, set new mtu to netdevice */\n\tndev->mtu = new_mtu;\n\nout:\n\tif (if_running) {\n\t\tif (hns_nic_net_open(ndev)) {\n\t\t\tnetdev_err(ndev, \"hns net open fail\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int hns_nic_set_features(struct net_device *netdev,\n\t\t\t\tnetdev_features_t features)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_1:\n\t\tif (features & (NETIF_F_TSO | NETIF_F_TSO6))\n\t\t\tnetdev_info(netdev, \"enet v1 do not support tso!\\n\");\n\t\tbreak;\n\tdefault:\n\t\tif (features & (NETIF_F_TSO | NETIF_F_TSO6)) {\n\t\t\tpriv->ops.fill_desc = fill_tso_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tso;\n\t\t\t/* The chip only support 7*4096 */\n\t\t\tnetif_set_gso_max_size(netdev, 7 * 4096);\n\t\t} else {\n\t\t\tpriv->ops.fill_desc = fill_v2_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t\t}\n\t\tbreak;\n\t}\n\tnetdev->features = features;\n\treturn 0;\n}\n\nstatic netdev_features_t hns_nic_fix_features(\n\t\tstruct net_device *netdev, netdev_features_t features)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_1:\n\t\tfeatures &= ~(NETIF_F_TSO | NETIF_F_TSO6 |\n\t\t\t\tNETIF_F_HW_VLAN_CTAG_FILTER);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn features;\n}\n\nstatic int hns_nic_uc_sync(struct net_device *netdev, const unsigned char *addr)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->add_uc_addr)\n\t\treturn h->dev->ops->add_uc_addr(h, addr);\n\n\treturn 0;\n}\n\nstatic int hns_nic_uc_unsync(struct net_device *netdev,\n\t\t\t     const unsigned char *addr)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->rm_uc_addr)\n\t\treturn h->dev->ops->rm_uc_addr(h, addr);\n\n\treturn 0;\n}\n\n/**\n * nic_set_multicast_list - set mutl mac address\n * @netdev: net device\n * @p: mac address\n *\n * return void\n */\nvoid hns_set_multicast_list(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct netdev_hw_addr *ha = NULL;\n\n\tif (!h)\t{\n\t\tnetdev_err(ndev, \"hnae handle is null\\n\");\n\t\treturn;\n\t}\n\n\tif (h->dev->ops->clr_mc_addr)\n\t\tif (h->dev->ops->clr_mc_addr(h))\n\t\t\tnetdev_err(ndev, \"clear multicast address fail\\n\");\n\n\tif (h->dev->ops->set_mc_addr) {\n\t\tnetdev_for_each_mc_addr(ha, ndev)\n\t\t\tif (h->dev->ops->set_mc_addr(h, ha->addr))\n\t\t\t\tnetdev_err(ndev, \"set multicast fail\\n\");\n\t}\n}\n\nvoid hns_nic_set_rx_mode(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->dev->ops->set_promisc_mode) {\n\t\tif (ndev->flags & IFF_PROMISC)\n\t\t\th->dev->ops->set_promisc_mode(h, 1);\n\t\telse\n\t\t\th->dev->ops->set_promisc_mode(h, 0);\n\t}\n\n\thns_set_multicast_list(ndev);\n\n\tif (__dev_uc_sync(ndev, hns_nic_uc_sync, hns_nic_uc_unsync))\n\t\tnetdev_err(ndev, \"sync uc address fail\\n\");\n}\n\nstatic void hns_nic_get_stats64(struct net_device *ndev,\n\t\t\t\tstruct rtnl_link_stats64 *stats)\n{\n\tint idx = 0;\n\tu64 tx_bytes = 0;\n\tu64 rx_bytes = 0;\n\tu64 tx_pkts = 0;\n\tu64 rx_pkts = 0;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tfor (idx = 0; idx < h->q_num; idx++) {\n\t\ttx_bytes += h->qs[idx]->tx_ring.stats.tx_bytes;\n\t\ttx_pkts += h->qs[idx]->tx_ring.stats.tx_pkts;\n\t\trx_bytes += h->qs[idx]->rx_ring.stats.rx_bytes;\n\t\trx_pkts += h->qs[idx]->rx_ring.stats.rx_pkts;\n\t}\n\n\tstats->tx_bytes = tx_bytes;\n\tstats->tx_packets = tx_pkts;\n\tstats->rx_bytes = rx_bytes;\n\tstats->rx_packets = rx_pkts;\n\n\tstats->rx_errors = ndev->stats.rx_errors;\n\tstats->multicast = ndev->stats.multicast;\n\tstats->rx_length_errors = ndev->stats.rx_length_errors;\n\tstats->rx_crc_errors = ndev->stats.rx_crc_errors;\n\tstats->rx_missed_errors = ndev->stats.rx_missed_errors;\n\n\tstats->tx_errors = ndev->stats.tx_errors;\n\tstats->rx_dropped = ndev->stats.rx_dropped;\n\tstats->tx_dropped = ndev->stats.tx_dropped;\n\tstats->collisions = ndev->stats.collisions;\n\tstats->rx_over_errors = ndev->stats.rx_over_errors;\n\tstats->rx_frame_errors = ndev->stats.rx_frame_errors;\n\tstats->rx_fifo_errors = ndev->stats.rx_fifo_errors;\n\tstats->tx_aborted_errors = ndev->stats.tx_aborted_errors;\n\tstats->tx_carrier_errors = ndev->stats.tx_carrier_errors;\n\tstats->tx_fifo_errors = ndev->stats.tx_fifo_errors;\n\tstats->tx_heartbeat_errors = ndev->stats.tx_heartbeat_errors;\n\tstats->tx_window_errors = ndev->stats.tx_window_errors;\n\tstats->rx_compressed = ndev->stats.rx_compressed;\n\tstats->tx_compressed = ndev->stats.tx_compressed;\n}\n\nstatic u16\nhns_nic_select_queue(struct net_device *ndev, struct sk_buff *skb,\n\t\t     void *accel_priv, select_queue_fallback_t fallback)\n{\n\tstruct ethhdr *eth_hdr = (struct ethhdr *)skb->data;\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\t/* fix hardware broadcast/multicast packets queue loopback */\n\tif (!AE_IS_VER1(priv->enet_ver) &&\n\t    is_multicast_ether_addr(eth_hdr->h_dest))\n\t\treturn 0;\n\telse\n\t\treturn fallback(ndev, skb);\n}\n\nstatic const struct net_device_ops hns_nic_netdev_ops = {\n\t.ndo_open = hns_nic_net_open,\n\t.ndo_stop = hns_nic_net_stop,\n\t.ndo_start_xmit = hns_nic_net_xmit,\n\t.ndo_tx_timeout = hns_nic_net_timeout,\n\t.ndo_set_mac_address = hns_nic_net_set_mac_address,\n\t.ndo_change_mtu = hns_nic_change_mtu,\n\t.ndo_do_ioctl = hns_nic_do_ioctl,\n\t.ndo_set_features = hns_nic_set_features,\n\t.ndo_fix_features = hns_nic_fix_features,\n\t.ndo_get_stats64 = hns_nic_get_stats64,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller = hns_nic_poll_controller,\n#endif\n\t.ndo_set_rx_mode = hns_nic_set_rx_mode,\n\t.ndo_select_queue = hns_nic_select_queue,\n};\n\nstatic void hns_nic_update_link_status(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (h->phy_dev) {\n\t\tif (h->phy_if != PHY_INTERFACE_MODE_XGMII)\n\t\t\treturn;\n\n\t\t(void)genphy_read_status(h->phy_dev);\n\t}\n\thns_nic_adjust_link(netdev);\n}\n\n/* for dumping key regs*/\nstatic void hns_nic_dump(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hnae_ae_ops *ops = h->dev->ops;\n\tu32 *data, reg_num, i;\n\n\tif (ops->get_regs_len && ops->get_regs) {\n\t\treg_num = ops->get_regs_len(priv->ae_handle);\n\t\treg_num = (reg_num + 3ul) & ~3ul;\n\t\tdata = kcalloc(reg_num, sizeof(u32), GFP_KERNEL);\n\t\tif (data) {\n\t\t\tops->get_regs(priv->ae_handle, data);\n\t\t\tfor (i = 0; i < reg_num; i += 4)\n\t\t\t\tpr_info(\"0x%08x: 0x%08x 0x%08x 0x%08x 0x%08x\\n\",\n\t\t\t\t\ti, data[i], data[i + 1],\n\t\t\t\t\tdata[i + 2], data[i + 3]);\n\t\t\tkfree(data);\n\t\t}\n\t}\n\n\tfor (i = 0; i < h->q_num; i++) {\n\t\tpr_info(\"tx_queue%d_next_to_clean:%d\\n\",\n\t\t\ti, h->qs[i]->tx_ring.next_to_clean);\n\t\tpr_info(\"tx_queue%d_next_to_use:%d\\n\",\n\t\t\ti, h->qs[i]->tx_ring.next_to_use);\n\t\tpr_info(\"rx_queue%d_next_to_clean:%d\\n\",\n\t\t\ti, h->qs[i]->rx_ring.next_to_clean);\n\t\tpr_info(\"rx_queue%d_next_to_use:%d\\n\",\n\t\t\ti, h->qs[i]->rx_ring.next_to_use);\n\t}\n}\n\n/* for resetting subtask */\nstatic void hns_nic_reset_subtask(struct hns_nic_priv *priv)\n{\n\tenum hnae_port_type type = priv->ae_handle->port_type;\n\n\tif (!test_bit(NIC_STATE2_RESET_REQUESTED, &priv->state))\n\t\treturn;\n\tclear_bit(NIC_STATE2_RESET_REQUESTED, &priv->state);\n\n\t/* If we're already down, removing or resetting, just bail */\n\tif (test_bit(NIC_STATE_DOWN, &priv->state) ||\n\t    test_bit(NIC_STATE_REMOVING, &priv->state) ||\n\t    test_bit(NIC_STATE_RESETTING, &priv->state))\n\t\treturn;\n\n\thns_nic_dump(priv);\n\tnetdev_info(priv->netdev, \"try to reset %s port!\\n\",\n\t\t    (type == HNAE_PORT_DEBUG ? \"debug\" : \"service\"));\n\n\trtnl_lock();\n\t/* put off any impending NetWatchDogTimeout */\n\tnetif_trans_update(priv->netdev);\n\n\tif (type == HNAE_PORT_DEBUG) {\n\t\thns_nic_net_reinit(priv->netdev);\n\t} else {\n\t\tnetif_carrier_off(priv->netdev);\n\t\tnetif_tx_disable(priv->netdev);\n\t}\n\trtnl_unlock();\n}\n\n/* for doing service complete*/\nstatic void hns_nic_service_event_complete(struct hns_nic_priv *priv)\n{\n\tWARN_ON(!test_bit(NIC_STATE_SERVICE_SCHED, &priv->state));\n\t/* make sure to commit the things */\n\tsmp_mb__before_atomic();\n\tclear_bit(NIC_STATE_SERVICE_SCHED, &priv->state);\n}\n\nstatic void hns_nic_service_task(struct work_struct *work)\n{\n\tstruct hns_nic_priv *priv\n\t\t= container_of(work, struct hns_nic_priv, service_task);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\thns_nic_update_link_status(priv->netdev);\n\th->dev->ops->update_led_status(h);\n\thns_nic_update_stats(priv->netdev);\n\n\thns_nic_reset_subtask(priv);\n\thns_nic_service_event_complete(priv);\n}\n\nstatic void hns_nic_task_schedule(struct hns_nic_priv *priv)\n{\n\tif (!test_bit(NIC_STATE_DOWN, &priv->state) &&\n\t    !test_bit(NIC_STATE_REMOVING, &priv->state) &&\n\t    !test_and_set_bit(NIC_STATE_SERVICE_SCHED, &priv->state))\n\t\t(void)schedule_work(&priv->service_task);\n}\n\nstatic void hns_nic_service_timer(unsigned long data)\n{\n\tstruct hns_nic_priv *priv = (struct hns_nic_priv *)data;\n\n\t(void)mod_timer(&priv->service_timer, jiffies + SERVICE_TIMER_HZ);\n\n\thns_nic_task_schedule(priv);\n}\n\n/**\n * hns_tx_timeout_reset - initiate reset due to Tx timeout\n * @priv: driver private struct\n **/\nstatic void hns_tx_timeout_reset(struct hns_nic_priv *priv)\n{\n\t/* Do the reset outside of interrupt context */\n\tif (!test_bit(NIC_STATE_DOWN, &priv->state)) {\n\t\tset_bit(NIC_STATE2_RESET_REQUESTED, &priv->state);\n\t\tnetdev_warn(priv->netdev,\n\t\t\t    \"initiating reset due to tx timeout(%llu,0x%lx)\\n\",\n\t\t\t    priv->tx_timeout_count, priv->state);\n\t\tpriv->tx_timeout_count++;\n\t\thns_nic_task_schedule(priv);\n\t}\n}\n\nstatic int hns_nic_init_ring_data(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tstruct hns_nic_ring_data *rd;\n\tbool is_ver1 = AE_IS_VER1(priv->enet_ver);\n\tint i;\n\n\tif (h->q_num > NIC_MAX_Q_PER_VF) {\n\t\tnetdev_err(priv->netdev, \"too much queue (%d)\\n\", h->q_num);\n\t\treturn -EINVAL;\n\t}\n\n\tpriv->ring_data = kzalloc(h->q_num * sizeof(*priv->ring_data) * 2,\n\t\t\t\t  GFP_KERNEL);\n\tif (!priv->ring_data)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < h->q_num; i++) {\n\t\trd = &priv->ring_data[i];\n\t\trd->queue_index = i;\n\t\trd->ring = &h->qs[i]->tx_ring;\n\t\trd->poll_one = hns_nic_tx_poll_one;\n\t\trd->fini_process = is_ver1 ? hns_nic_tx_fini_pro :\n\t\t\thns_nic_tx_fini_pro_v2;\n\n\t\tnetif_napi_add(priv->netdev, &rd->napi,\n\t\t\t       hns_nic_common_poll, NIC_TX_CLEAN_MAX_NUM);\n\t\trd->ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\tfor (i = h->q_num; i < h->q_num * 2; i++) {\n\t\trd = &priv->ring_data[i];\n\t\trd->queue_index = i - h->q_num;\n\t\trd->ring = &h->qs[i - h->q_num]->rx_ring;\n\t\trd->poll_one = hns_nic_rx_poll_one;\n\t\trd->ex_process = hns_nic_rx_up_pro;\n\t\trd->fini_process = is_ver1 ? hns_nic_rx_fini_pro :\n\t\t\thns_nic_rx_fini_pro_v2;\n\n\t\tnetif_napi_add(priv->netdev, &rd->napi,\n\t\t\t       hns_nic_common_poll, NIC_RX_CLEAN_MAX_NUM);\n\t\trd->ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\n\treturn 0;\n}\n\nstatic void hns_nic_uninit_ring_data(struct hns_nic_priv *priv)\n{\n\tstruct hnae_handle *h = priv->ae_handle;\n\tint i;\n\n\tfor (i = 0; i < h->q_num * 2; i++) {\n\t\tnetif_napi_del(&priv->ring_data[i].napi);\n\t\tif (priv->ring_data[i].ring->irq_init_flag == RCB_IRQ_INITED) {\n\t\t\t(void)irq_set_affinity_hint(\n\t\t\t\tpriv->ring_data[i].ring->irq,\n\t\t\t\tNULL);\n\t\t\tfree_irq(priv->ring_data[i].ring->irq,\n\t\t\t\t &priv->ring_data[i]);\n\t\t}\n\n\t\tpriv->ring_data[i].ring->irq_init_flag = RCB_IRQ_NOT_INITED;\n\t}\n\tkfree(priv->ring_data);\n}\n\nstatic void hns_nic_set_priv_ops(struct net_device *netdev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(netdev);\n\tstruct hnae_handle *h = priv->ae_handle;\n\n\tif (AE_IS_VER1(priv->enet_ver)) {\n\t\tpriv->ops.fill_desc = fill_desc;\n\t\tpriv->ops.get_rxd_bnum = get_rx_desc_bnum;\n\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t} else {\n\t\tpriv->ops.get_rxd_bnum = get_v2rx_desc_bnum;\n\t\tif ((netdev->features & NETIF_F_TSO) ||\n\t\t    (netdev->features & NETIF_F_TSO6)) {\n\t\t\tpriv->ops.fill_desc = fill_tso_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tso;\n\t\t\t/* This chip only support 7*4096 */\n\t\t\tnetif_set_gso_max_size(netdev, 7 * 4096);\n\t\t} else {\n\t\t\tpriv->ops.fill_desc = fill_v2_desc;\n\t\t\tpriv->ops.maybe_stop_tx = hns_nic_maybe_stop_tx;\n\t\t}\n\t\t/* enable tso when init\n\t\t * control tso on/off through TSE bit in bd\n\t\t */\n\t\th->dev->ops->set_tso_stats(h, 1);\n\t}\n}\n\nstatic int hns_nic_try_get_ae(struct net_device *ndev)\n{\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\tstruct hnae_handle *h;\n\tint ret;\n\n\th = hnae_get_handle(&priv->netdev->dev,\n\t\t\t    priv->fwnode, priv->port_id, NULL);\n\tif (IS_ERR_OR_NULL(h)) {\n\t\tret = -ENODEV;\n\t\tdev_dbg(priv->dev, \"has not handle, register notifier!\\n\");\n\t\tgoto out;\n\t}\n\tpriv->ae_handle = h;\n\n\tret = hns_nic_init_phy(ndev, h);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"probe phy device fail!\\n\");\n\t\tgoto out_init_phy;\n\t}\n\n\tret = hns_nic_init_ring_data(priv);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_init_ring_data;\n\t}\n\n\thns_nic_set_priv_ops(ndev);\n\n\tret = register_netdev(ndev);\n\tif (ret) {\n\t\tdev_err(priv->dev, \"probe register netdev fail!\\n\");\n\t\tgoto out_reg_ndev_fail;\n\t}\n\treturn 0;\n\nout_reg_ndev_fail:\n\thns_nic_uninit_ring_data(priv);\n\tpriv->ring_data = NULL;\nout_init_phy:\nout_init_ring_data:\n\thnae_put_handle(priv->ae_handle);\n\tpriv->ae_handle = NULL;\nout:\n\treturn ret;\n}\n\nstatic int hns_nic_notifier_action(struct notifier_block *nb,\n\t\t\t\t   unsigned long action, void *data)\n{\n\tstruct hns_nic_priv *priv =\n\t\tcontainer_of(nb, struct hns_nic_priv, notifier_block);\n\n\tassert(action == HNAE_AE_REGISTER);\n\n\tif (!hns_nic_try_get_ae(priv->netdev)) {\n\t\thnae_unregister_notifier(&priv->notifier_block);\n\t\tpriv->notifier_block.notifier_call = NULL;\n\t}\n\treturn 0;\n}\n\nstatic int hns_nic_dev_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct net_device *ndev;\n\tstruct hns_nic_priv *priv;\n\tu32 port_id;\n\tint ret;\n\n\tndev = alloc_etherdev_mq(sizeof(struct hns_nic_priv), NIC_MAX_Q_PER_VF);\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\tplatform_set_drvdata(pdev, ndev);\n\n\tpriv = netdev_priv(ndev);\n\tpriv->dev = dev;\n\tpriv->netdev = ndev;\n\n\tif (dev_of_node(dev)) {\n\t\tstruct device_node *ae_node;\n\n\t\tif (of_device_is_compatible(dev->of_node,\n\t\t\t\t\t    \"hisilicon,hns-nic-v1\"))\n\t\t\tpriv->enet_ver = AE_VERSION_1;\n\t\telse\n\t\t\tpriv->enet_ver = AE_VERSION_2;\n\n\t\tae_node = of_parse_phandle(dev->of_node, \"ae-handle\", 0);\n\t\tif (IS_ERR_OR_NULL(ae_node)) {\n\t\t\tret = PTR_ERR(ae_node);\n\t\t\tdev_err(dev, \"not find ae-handle\\n\");\n\t\t\tgoto out_read_prop_fail;\n\t\t}\n\t\tpriv->fwnode = &ae_node->fwnode;\n\t} else if (is_acpi_node(dev->fwnode)) {\n\t\tstruct acpi_reference_args args;\n\n\t\tif (acpi_dev_found(hns_enet_acpi_match[0].id))\n\t\t\tpriv->enet_ver = AE_VERSION_1;\n\t\telse if (acpi_dev_found(hns_enet_acpi_match[1].id))\n\t\t\tpriv->enet_ver = AE_VERSION_2;\n\t\telse\n\t\t\treturn -ENXIO;\n\n\t\t/* try to find port-idx-in-ae first */\n\t\tret = acpi_node_get_property_reference(dev->fwnode,\n\t\t\t\t\t\t       \"ae-handle\", 0, &args);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"not find ae-handle\\n\");\n\t\t\tgoto out_read_prop_fail;\n\t\t}\n\t\tpriv->fwnode = acpi_fwnode_handle(args.adev);\n\t} else {\n\t\tdev_err(dev, \"cannot read cfg data from OF or acpi\\n\");\n\t\treturn -ENXIO;\n\t}\n\n\tret = device_property_read_u32(dev, \"port-idx-in-ae\", &port_id);\n\tif (ret) {\n\t\t/* only for old code compatible */\n\t\tret = device_property_read_u32(dev, \"port-id\", &port_id);\n\t\tif (ret)\n\t\t\tgoto out_read_prop_fail;\n\t\t/* for old dts, we need to caculate the port offset */\n\t\tport_id = port_id < HNS_SRV_OFFSET ? port_id + HNS_DEBUG_OFFSET\n\t\t\t: port_id - HNS_SRV_OFFSET;\n\t}\n\tpriv->port_id = port_id;\n\n\thns_init_mac_addr(ndev);\n\n\tndev->watchdog_timeo = HNS_NIC_TX_TIMEOUT;\n\tndev->priv_flags |= IFF_UNICAST_FLT;\n\tndev->netdev_ops = &hns_nic_netdev_ops;\n\thns_ethtool_set_ops(ndev);\n\n\tndev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\tNETIF_F_RXCSUM | NETIF_F_SG | NETIF_F_GSO |\n\t\tNETIF_F_GRO;\n\tndev->vlan_features |=\n\t\tNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM | NETIF_F_RXCSUM;\n\tndev->vlan_features |= NETIF_F_SG | NETIF_F_GSO | NETIF_F_GRO;\n\n\t/* MTU range: 68 - 9578 (v1) or 9706 (v2) */\n\tndev->min_mtu = MAC_MIN_MTU;\n\tswitch (priv->enet_ver) {\n\tcase AE_VERSION_2:\n\t\tndev->features |= NETIF_F_TSO | NETIF_F_TSO6;\n\t\tndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\t\tNETIF_F_RXCSUM | NETIF_F_SG | NETIF_F_GSO |\n\t\t\tNETIF_F_GRO | NETIF_F_TSO | NETIF_F_TSO6;\n\t\tndev->max_mtu = MAC_MAX_MTU_V2 -\n\t\t\t\t(ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\t\tbreak;\n\tdefault:\n\t\tndev->max_mtu = MAC_MAX_MTU -\n\t\t\t\t(ETH_HLEN + ETH_FCS_LEN + VLAN_HLEN);\n\t\tbreak;\n\t}\n\n\tSET_NETDEV_DEV(ndev, dev);\n\n\tif (!dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)))\n\t\tdev_dbg(dev, \"set mask to 64bit\\n\");\n\telse\n\t\tdev_err(dev, \"set mask to 64bit fail!\\n\");\n\n\t/* carrier off reporting is important to ethtool even BEFORE open */\n\tnetif_carrier_off(ndev);\n\n\tsetup_timer(&priv->service_timer, hns_nic_service_timer,\n\t\t    (unsigned long)priv);\n\tINIT_WORK(&priv->service_task, hns_nic_service_task);\n\n\tset_bit(NIC_STATE_SERVICE_INITED, &priv->state);\n\tclear_bit(NIC_STATE_SERVICE_SCHED, &priv->state);\n\tset_bit(NIC_STATE_DOWN, &priv->state);\n\n\tif (hns_nic_try_get_ae(priv->netdev)) {\n\t\tpriv->notifier_block.notifier_call = hns_nic_notifier_action;\n\t\tret = hnae_register_notifier(&priv->notifier_block);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"register notifier fail!\\n\");\n\t\t\tgoto out_notify_fail;\n\t\t}\n\t\tdev_dbg(dev, \"has not handle, register notifier!\\n\");\n\t}\n\n\treturn 0;\n\nout_notify_fail:\n\t(void)cancel_work_sync(&priv->service_task);\nout_read_prop_fail:\n\tfree_netdev(ndev);\n\treturn ret;\n}\n\nstatic int hns_nic_dev_remove(struct platform_device *pdev)\n{\n\tstruct net_device *ndev = platform_get_drvdata(pdev);\n\tstruct hns_nic_priv *priv = netdev_priv(ndev);\n\n\tif (ndev->reg_state != NETREG_UNINITIALIZED)\n\t\tunregister_netdev(ndev);\n\n\tif (priv->ring_data)\n\t\thns_nic_uninit_ring_data(priv);\n\tpriv->ring_data = NULL;\n\n\tif (ndev->phydev)\n\t\tphy_disconnect(ndev->phydev);\n\n\tif (!IS_ERR_OR_NULL(priv->ae_handle))\n\t\thnae_put_handle(priv->ae_handle);\n\tpriv->ae_handle = NULL;\n\tif (priv->notifier_block.notifier_call)\n\t\thnae_unregister_notifier(&priv->notifier_block);\n\tpriv->notifier_block.notifier_call = NULL;\n\n\tset_bit(NIC_STATE_REMOVING, &priv->state);\n\t(void)cancel_work_sync(&priv->service_task);\n\n\tfree_netdev(ndev);\n\treturn 0;\n}\n\nstatic const struct of_device_id hns_enet_of_match[] = {\n\t{.compatible = \"hisilicon,hns-nic-v1\",},\n\t{.compatible = \"hisilicon,hns-nic-v2\",},\n\t{},\n};\n\nMODULE_DEVICE_TABLE(of, hns_enet_of_match);\n\nstatic struct platform_driver hns_nic_dev_driver = {\n\t.driver = {\n\t\t.name = \"hns-nic\",\n\t\t.of_match_table = hns_enet_of_match,\n\t\t.acpi_match_table = ACPI_PTR(hns_enet_acpi_match),\n\t},\n\t.probe = hns_nic_dev_probe,\n\t.remove = hns_nic_dev_remove,\n};\n\nmodule_platform_driver(hns_nic_dev_driver);\n\nMODULE_DESCRIPTION(\"HISILICON HNS Ethernet driver\");\nMODULE_AUTHOR(\"Hisilicon, Inc.\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"platform:hns-nic\");\n", "/*\n * Copyright (c) 2014-2015 Hisilicon Limited.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2 of the License, or\n * (at your option) any later version.\n */\n\n#ifndef __HNS_ENET_H\n#define __HNS_ENET_H\n\n#include <linux/netdevice.h>\n#include <linux/of_net.h>\n#include <linux/of_mdio.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n\n#include \"hnae.h\"\n\n#define HNS_DEBUG_OFFSET\t6\n#define HNS_SRV_OFFSET\t\t2\n\nenum hns_nic_state {\n\tNIC_STATE_TESTING = 0,\n\tNIC_STATE_RESETTING,\n\tNIC_STATE_REINITING,\n\tNIC_STATE_DOWN,\n\tNIC_STATE_DISABLED,\n\tNIC_STATE_REMOVING,\n\tNIC_STATE_SERVICE_INITED,\n\tNIC_STATE_SERVICE_SCHED,\n\tNIC_STATE2_RESET_REQUESTED,\n\tNIC_STATE_MAX\n};\n\nstruct hns_nic_ring_data {\n\tstruct hnae_ring *ring;\n\tstruct napi_struct napi;\n\tcpumask_t mask; /* affinity mask */\n\tint queue_index;\n\tint (*poll_one)(struct hns_nic_ring_data *, int, void *);\n\tvoid (*ex_process)(struct hns_nic_ring_data *, struct sk_buff *);\n\tbool (*fini_process)(struct hns_nic_ring_data *);\n};\n\n/* compatible the difference between two versions */\nstruct hns_nic_ops {\n\tvoid (*fill_desc)(struct hnae_ring *ring, void *priv,\n\t\t\t  int size, dma_addr_t dma, int frag_end,\n\t\t\t  int buf_num, enum hns_desc_type type, int mtu);\n\tint (*maybe_stop_tx)(struct sk_buff **out_skb,\n\t\t\t     int *bnum, struct hnae_ring *ring);\n\tvoid (*get_rxd_bnum)(u32 bnum_flag, int *out_bnum);\n};\n\nstruct hns_nic_priv {\n\tconst struct fwnode_handle      *fwnode;\n\tu32 enet_ver;\n\tu32 port_id;\n\tint phy_mode;\n\tint phy_led_val;\n\tstruct net_device *netdev;\n\tstruct device *dev;\n\tstruct hnae_handle *ae_handle;\n\n\tstruct hns_nic_ops ops;\n\n\t/* the cb for nic to manage the ring buffer, the first half of the\n\t * array is for tx_ring and vice versa for the second half\n\t */\n\tstruct hns_nic_ring_data *ring_data;\n\n\t/* The most recently read link state */\n\tint link;\n\tu64 tx_timeout_count;\n\n\tunsigned long state;\n\n\tstruct timer_list service_timer;\n\n\tstruct work_struct service_task;\n\n\tstruct notifier_block notifier_block;\n};\n\n#define tx_ring_data(priv, idx) ((priv)->ring_data[idx])\n#define rx_ring_data(priv, idx) \\\n\t((priv)->ring_data[(priv)->ae_handle->q_num + (idx)])\n\nvoid hns_ethtool_set_ops(struct net_device *ndev);\nvoid hns_nic_net_reset(struct net_device *ndev);\nvoid hns_nic_net_reinit(struct net_device *netdev);\nint hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h);\nnetdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct hns_nic_ring_data *ring_data);\n\n#endif\t/**__HNS_ENET_H */\n"], "filenames": ["drivers/net/ethernet/hisilicon/hns/hns_enet.c", "drivers/net/ethernet/hisilicon/hns/hns_enet.h"], "buggy_code_start_loc": [303, 95], "buggy_code_end_loc": [1483, 98], "fixing_code_start_loc": [303, 95], "fixing_code_end_loc": [1481, 98], "type": "CWE-416", "message": "In drivers/net/ethernet/hisilicon/hns/hns_enet.c in the Linux kernel before 4.13, local users can cause a denial of service (use-after-free and BUG) or possibly have unspecified other impact by leveraging differences in skb handling between hns_nic_net_xmit_hw and hns_nic_net_xmit.", "other": {"cve": {"id": "CVE-2017-18218", "sourceIdentifier": "cve@mitre.org", "published": "2018-03-05T20:29:00.270", "lastModified": "2023-02-07T22:01:10.090", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In drivers/net/ethernet/hisilicon/hns/hns_enet.c in the Linux kernel before 4.13, local users can cause a denial of service (use-after-free and BUG) or possibly have unspecified other impact by leveraging differences in skb handling between hns_nic_net_xmit_hw and hns_nic_net_xmit."}, {"lang": "es", "value": "En drivers/net/ethernet/hisilicon/hns/hns_enet.c en el kernel de Linux, en versiones anteriores a la 4.13, los usuarios locales pueden provocar una denegaci\u00f3n de servicio (uso de memoria previamente liberada y error) u otro tipo de impacto sin especificar aprovechando las diferencias en la gesti\u00f3n skb en hns_nic_net_xmit_hw y hns_nic_net_xmit."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.92", "matchCriteriaId": "8347294E-AA12-4F18-939B-D38A58907D06"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.13", "matchCriteriaId": "6FCB36DB-56CB-4283-9F37-46669CDBB0B9"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=27463ad99f738ed93c7c8b3e2e5bc8c4853a2ff2", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/103277", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/27463ad99f738ed93c7c8b3e2e5bc8c4853a2ff2", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4188", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/27463ad99f738ed93c7c8b3e2e5bc8c4853a2ff2"}}