{"buggy_code": ["/*\n *\tlinux/mm/mlock.c\n *\n *  (C) Copyright 1995 Linus Torvalds\n *  (C) Copyright 2002 Christoph Hellwig\n */\n\n#include <linux/capability.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/mempolicy.h>\n#include <linux/syscalls.h>\n#include <linux/sched.h>\n#include <linux/export.h>\n#include <linux/rmap.h>\n#include <linux/mmzone.h>\n#include <linux/hugetlb.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n\n#include \"internal.h\"\n\nint can_do_mlock(void)\n{\n\tif (capable(CAP_IPC_LOCK))\n\t\treturn 1;\n\tif (rlimit(RLIMIT_MEMLOCK) != 0)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL(can_do_mlock);\n\n/*\n * Mlocked pages are marked with PageMlocked() flag for efficient testing\n * in vmscan and, possibly, the fault path; and to support semi-accurate\n * statistics.\n *\n * An mlocked page [PageMlocked(page)] is unevictable.  As such, it will\n * be placed on the LRU \"unevictable\" list, rather than the [in]active lists.\n * The unevictable list is an LRU sibling list to the [in]active lists.\n * PageUnevictable is set to indicate the unevictable state.\n *\n * When lazy mlocking via vmscan, it is important to ensure that the\n * vma's VM_LOCKED status is not concurrently being modified, otherwise we\n * may have mlocked a page that is being munlocked. So lazy mlock must take\n * the mmap_sem for read, and verify that the vma really is locked\n * (see mm/rmap.c).\n */\n\n/*\n *  LRU accounting for clear_page_mlock()\n */\nvoid clear_page_mlock(struct page *page)\n{\n\tif (!TestClearPageMlocked(page))\n\t\treturn;\n\n\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t    -hpage_nr_pages(page));\n\tcount_vm_event(UNEVICTABLE_PGCLEARED);\n\tif (!isolate_lru_page(page)) {\n\t\tputback_lru_page(page);\n\t} else {\n\t\t/*\n\t\t * We lost the race. the page already moved to evictable list.\n\t\t */\n\t\tif (PageUnevictable(page))\n\t\t\tcount_vm_event(UNEVICTABLE_PGSTRANDED);\n\t}\n}\n\n/*\n * Mark page as mlocked if not already.\n * If page on LRU, isolate and putback to move to unevictable list.\n */\nvoid mlock_vma_page(struct page *page)\n{\n\tBUG_ON(!PageLocked(page));\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Isolate a page from LRU with optional get_page() pin.\n * Assumes lru_lock already held and page already pinned.\n */\nstatic bool __munlock_isolate_lru_page(struct page *page, bool getpage)\n{\n\tif (PageLRU(page)) {\n\t\tstruct lruvec *lruvec;\n\n\t\tlruvec = mem_cgroup_page_lruvec(page, page_zone(page));\n\t\tif (getpage)\n\t\t\tget_page(page);\n\t\tClearPageLRU(page);\n\t\tdel_page_from_lru_list(page, lruvec, page_lru(page));\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Finish munlock after successful page isolation\n *\n * Page must be locked. This is a wrapper for try_to_munlock()\n * and putback_lru_page() with munlock accounting.\n */\nstatic void __munlock_isolated_page(struct page *page)\n{\n\tint ret = SWAP_AGAIN;\n\n\t/*\n\t * Optimization: if the page was mapped just once, that's our mapping\n\t * and we don't need to check all the other vmas.\n\t */\n\tif (page_mapcount(page) > 1)\n\t\tret = try_to_munlock(page);\n\n\t/* Did try_to_unlock() succeed or punt? */\n\tif (ret != SWAP_MLOCK)\n\t\tcount_vm_event(UNEVICTABLE_PGMUNLOCKED);\n\n\tputback_lru_page(page);\n}\n\n/*\n * Accounting for page isolation fail during munlock\n *\n * Performs accounting when page isolation fails in munlock. There is nothing\n * else to do because it means some other task has already removed the page\n * from the LRU. putback_lru_page() will take care of removing the page from\n * the unevictable list, if necessary. vmscan [page_referenced()] will move\n * the page back to the unevictable list if some other vma has it mlocked.\n */\nstatic void __munlock_isolation_failed(struct page *page)\n{\n\tif (PageUnevictable(page))\n\t\t__count_vm_event(UNEVICTABLE_PGSTRANDED);\n\telse\n\t\t__count_vm_event(UNEVICTABLE_PGMUNLOCKED);\n}\n\n/**\n * munlock_vma_page - munlock a vma page\n * @page - page to be unlocked, either a normal page or THP page head\n *\n * returns the size of the page as a page mask (0 for normal page,\n *         HPAGE_PMD_NR - 1 for THP head page)\n *\n * called from munlock()/munmap() path with page supposedly on the LRU.\n * When we munlock a page, because the vma where we found the page is being\n * munlock()ed or munmap()ed, we want to check whether other vmas hold the\n * page locked so that we can leave it on the unevictable lru list and not\n * bother vmscan with it.  However, to walk the page's rmap list in\n * try_to_munlock() we must isolate the page from the LRU.  If some other\n * task has removed the page from the LRU, we won't be able to do that.\n * So we clear the PageMlocked as we might not get another chance.  If we\n * can't isolate the page, we leave it for putback_lru_page() and vmscan\n * [page_referenced()/try_to_unmap()] to deal with.\n */\nunsigned int munlock_vma_page(struct page *page)\n{\n\tunsigned int nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\tBUG_ON(!PageLocked(page));\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(&zone->lru_lock);\n\n\tnr_pages = hpage_nr_pages(page);\n\tif (!TestClearPageMlocked(page))\n\t\tgoto unlock_out;\n\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(&zone->lru_lock);\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(&zone->lru_lock);\n\nout:\n\treturn nr_pages - 1;\n}\n\n/**\n * __mlock_vma_pages_range() -  mlock a range of pages in the vma.\n * @vma:   target vma\n * @start: start address\n * @end:   end address\n *\n * This takes care of making the pages present too.\n *\n * return 0 on success, negative error code on error.\n *\n * vma->vm_mm->mmap_sem must be held for at least read.\n */\nlong __mlock_vma_pages_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *nonblocking)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint gup_flags;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(end   & ~PAGE_MASK);\n\tVM_BUG_ON(start < vma->vm_start);\n\tVM_BUG_ON(end   > vma->vm_end);\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tgup_flags = FOLL_TOUCH | FOLL_MLOCK;\n\t/*\n\t * We want to touch writable mappings with a write fault in order\n\t * to break COW, except for shared mappings because these don't COW\n\t * and we would not want to dirty them for nothing.\n\t */\n\tif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We want mlock to succeed for regions that have any permissions\n\t * other than PROT_NONE.\n\t */\n\tif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\n\t\tgup_flags |= FOLL_FORCE;\n\n\t/*\n\t * We made sure addr is within a VMA, so the following will\n\t * not result in a stack expansion that recurses back here.\n\t */\n\treturn __get_user_pages(current, mm, start, nr_pages, gup_flags,\n\t\t\t\tNULL, NULL, nonblocking);\n}\n\n/*\n * convert get_user_pages() return value to posix mlock() error\n */\nstatic int __mlock_posix_error_return(long retval)\n{\n\tif (retval == -EFAULT)\n\t\tretval = -ENOMEM;\n\telse if (retval == -ENOMEM)\n\t\tretval = -EAGAIN;\n\treturn retval;\n}\n\n/*\n * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()\n *\n * The fast path is available only for evictable pages with single mapping.\n * Then we can bypass the per-cpu pvec and get better performance.\n * when mapcount > 1 we need try_to_munlock() which can fail.\n * when !page_evictable(), we need the full redo logic of putback_lru_page to\n * avoid leaving evictable page in unevictable list.\n *\n * In case of success, @page is added to @pvec and @pgrescued is incremented\n * in case that the page was previously unevictable. @page is also unlocked.\n */\nstatic bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,\n\t\tint *pgrescued)\n{\n\tVM_BUG_ON_PAGE(PageLRU(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\n\tif (page_mapcount(page) <= 1 && page_evictable(page)) {\n\t\tpagevec_add(pvec, page);\n\t\tif (TestClearPageUnevictable(page))\n\t\t\t(*pgrescued)++;\n\t\tunlock_page(page);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Putback multiple evictable pages to the LRU\n *\n * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of\n * the pages might have meanwhile become unevictable but that is OK.\n */\nstatic void __putback_lru_fast(struct pagevec *pvec, int pgrescued)\n{\n\tcount_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));\n\t/*\n\t *__pagevec_lru_add() calls release_pages() so we don't call\n\t * put_page() explicitly\n\t */\n\t__pagevec_lru_add(pvec);\n\tcount_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\n}\n\n/*\n * Munlock a batch of pages from the same zone\n *\n * The work is split to two main phases. First phase clears the Mlocked flag\n * and attempts to isolate the pages, all under a single zone lru lock.\n * The second phase finishes the munlock only for pages where isolation\n * succeeded.\n *\n * Note that the pagevec may be modified during the process.\n */\nstatic void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(&zone->lru_lock);\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(&zone->lru_lock);\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}\n\n/*\n * Fill up pagevec for __munlock_pagevec using pte walk\n *\n * The function expects that the struct page corresponding to @start address is\n * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.\n *\n * The rest of @pvec is filled by subsequent pages within the same pmd and same\n * zone, as long as the pte's are present and vm_normal_page() succeeds. These\n * pages also get pinned.\n *\n * Returns the address of the next page that should be scanned. This equals\n * @start + PAGE_SIZE when no page could be added by the pte walk.\n */\nstatic unsigned long __munlock_pagevec_fill(struct pagevec *pvec,\n\t\tstruct vm_area_struct *vma, int zoneid,\tunsigned long start,\n\t\tunsigned long end)\n{\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Initialize pte walk starting at the already pinned page where we\n\t * are sure that there is a pte, as it was pinned under the same\n\t * mmap_sem write op.\n\t */\n\tpte = get_locked_pte(vma->vm_mm, start,\t&ptl);\n\t/* Make sure we do not cross the page table boundary */\n\tend = pgd_addr_end(start, end);\n\tend = pud_addr_end(start, end);\n\tend = pmd_addr_end(start, end);\n\n\t/* The page next to the pinned page is the first we will try to get */\n\tstart += PAGE_SIZE;\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tpte++;\n\t\tif (pte_present(*pte))\n\t\t\tpage = vm_normal_page(vma, start, *pte);\n\t\t/*\n\t\t * Break if page could not be obtained or the page's node+zone does not\n\t\t * match\n\t\t */\n\t\tif (!page || page_zone_id(page) != zoneid)\n\t\t\tbreak;\n\n\t\tget_page(page);\n\t\t/*\n\t\t * Increase the address that will be returned *before* the\n\t\t * eventual break due to pvec becoming full by adding the page\n\t\t */\n\t\tstart += PAGE_SIZE;\n\t\tif (pagevec_add(pvec, page) == 0)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn start;\n}\n\n/*\n * munlock_vma_pages_range() - munlock all pages in the vma range.'\n * @vma - vma containing range to be munlock()ed.\n * @start - start address in @vma of the range\n * @end - end of range in @vma.\n *\n *  For mremap(), munmap() and exit().\n *\n * Called with @vma VM_LOCKED.\n *\n * Returns with VM_LOCKED cleared.  Callers must be prepared to\n * deal with this.\n *\n * We don't save and restore VM_LOCKED here because pages are\n * still on lru.  In unmap path, pages might be scanned by reclaim\n * and re-mlocked by try_to_{munlock|unmap} before we unmap and\n * free them.  This will result in freeing mlocked pages.\n */\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tvma->vm_flags &= ~VM_LOCKED;\n\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tunsigned int page_mask;\n\t\tunsigned long page_increm;\n\t\tstruct pagevec pvec;\n\t\tstruct zone *zone;\n\t\tint zoneid;\n\n\t\tpagevec_init(&pvec, 0);\n\t\t/*\n\t\t * Although FOLL_DUMP is intended for get_dump_page(),\n\t\t * it just so happens that its special treatment of the\n\t\t * ZERO_PAGE (returning an error instead of doing get_page)\n\t\t * suits munlock very well (and if somehow an abnormal page\n\t\t * has sneaked into the range, we won't oops here: great).\n\t\t */\n\t\tpage = follow_page_mask(vma, start, FOLL_GET | FOLL_DUMP,\n\t\t\t\t&page_mask);\n\n\t\tif (page && !IS_ERR(page)) {\n\t\t\tif (PageTransHuge(page)) {\n\t\t\t\tlock_page(page);\n\t\t\t\t/*\n\t\t\t\t * Any THP page found by follow_page_mask() may\n\t\t\t\t * have gotten split before reaching\n\t\t\t\t * munlock_vma_page(), so we need to recompute\n\t\t\t\t * the page_mask here.\n\t\t\t\t */\n\t\t\t\tpage_mask = munlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Non-huge pages are handled in batches via\n\t\t\t\t * pagevec. The pin from follow_page_mask()\n\t\t\t\t * prevents them from collapsing by THP.\n\t\t\t\t */\n\t\t\t\tpagevec_add(&pvec, page);\n\t\t\t\tzone = page_zone(page);\n\t\t\t\tzoneid = page_zone_id(page);\n\n\t\t\t\t/*\n\t\t\t\t * Try to fill the rest of pagevec using fast\n\t\t\t\t * pte walk. This will also update start to\n\t\t\t\t * the next page to process. Then munlock the\n\t\t\t\t * pagevec.\n\t\t\t\t */\n\t\t\t\tstart = __munlock_pagevec_fill(&pvec, vma,\n\t\t\t\t\t\tzoneid, start, end);\n\t\t\t\t__munlock_pagevec(&pvec, zone);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\t\t/* It's a bug to munlock in the middle of a THP page */\n\t\tVM_BUG_ON((start >> PAGE_SHIFT) & page_mask);\n\t\tpage_increm = 1 + page_mask;\n\t\tstart += page_increm * PAGE_SIZE;\nnext:\n\t\tcond_resched();\n\t}\n}\n\n/*\n * mlock_fixup  - handle mlock[all]/munlock[all] requests.\n *\n * Filters out \"special\" vmas -- VM_LOCKED never gets set for these, and\n * munlock is a no-op.  However, for some special vmas, we go ahead and\n * populate the ptes.\n *\n * For vmas that pass the filters, merge/split as appropriate.\n */\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\n\tunsigned long start, unsigned long end, vm_flags_t newflags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgoff_t pgoff;\n\tint nr_pages;\n\tint ret = 0;\n\tint lock = !!(newflags & VM_LOCKED);\n\n\tif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\n\t    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\n\t\tgoto out;\t/* don't set VM_LOCKED,  don't count */\n\n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\n\t\t\t  vma->vm_file, pgoff, vma_policy(vma));\n\tif (*prev) {\n\t\tvma = *prev;\n\t\tgoto success;\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tret = split_vma(mm, vma, start, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\tret = split_vma(mm, vma, end, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nsuccess:\n\t/*\n\t * Keep track of amount of locked VM.\n\t */\n\tnr_pages = (end - start) >> PAGE_SHIFT;\n\tif (!lock)\n\t\tnr_pages = -nr_pages;\n\tmm->locked_vm += nr_pages;\n\n\t/*\n\t * vm_flags is protected by the mmap_sem held in write mode.\n\t * It's okay if try_to_unmap_one unmaps a page just after we\n\t * set VM_LOCKED, __mlock_vma_pages_range will bring it back.\n\t */\n\n\tif (lock)\n\t\tvma->vm_flags = newflags;\n\telse\n\t\tmunlock_vma_pages_range(vma, start, end);\n\nout:\n\t*prev = vma;\n\treturn ret;\n}\n\nstatic int do_mlock(unsigned long start, size_t len, int on)\n{\n\tunsigned long nstart, end, tmp;\n\tstruct vm_area_struct * vma, * prev;\n\tint error;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tvma = find_vma(current->mm, start);\n\tif (!vma || vma->vm_start > start)\n\t\treturn -ENOMEM;\n\n\tprev = vma->vm_prev;\n\tif (start > vma->vm_start)\n\t\tprev = vma;\n\n\tfor (nstart = start ; ; ) {\n\t\tvm_flags_t newflags;\n\n\t\t/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */\n\n\t\tnewflags = vma->vm_flags & ~VM_LOCKED;\n\t\tif (on)\n\t\t\tnewflags |= VM_LOCKED;\n\n\t\ttmp = vma->vm_end;\n\t\tif (tmp > end)\n\t\t\ttmp = end;\n\t\terror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\n\t\tif (error)\n\t\t\tbreak;\n\t\tnstart = tmp;\n\t\tif (nstart < prev->vm_end)\n\t\t\tnstart = prev->vm_end;\n\t\tif (nstart >= end)\n\t\t\tbreak;\n\n\t\tvma = prev->vm_next;\n\t\tif (!vma || vma->vm_start != nstart) {\n\t\t\terror = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * __mm_populate - populate and/or mlock pages within a range of address space.\n *\n * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap\n * flags. VMAs must be already marked with the desired vm_flags, and\n * mmap_sem must not be held.\n */\nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long end, nstart, nend;\n\tstruct vm_area_struct *vma = NULL;\n\tint locked = 0;\n\tlong ret = 0;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\n\tfor (nstart = start; nstart < end; nstart = nend) {\n\t\t/*\n\t\t * We want to fault in pages for [nstart; end) address range.\n\t\t * Find first corresponding VMA.\n\t\t */\n\t\tif (!locked) {\n\t\t\tlocked = 1;\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tvma = find_vma(mm, nstart);\n\t\t} else if (nstart >= vma->vm_end)\n\t\t\tvma = vma->vm_next;\n\t\tif (!vma || vma->vm_start >= end)\n\t\t\tbreak;\n\t\t/*\n\t\t * Set [nstart; nend) to intersection of desired address\n\t\t * range with the first VMA. Also, skip undesirable VMA types.\n\t\t */\n\t\tnend = min(end, vma->vm_end);\n\t\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\tcontinue;\n\t\tif (nstart < vma->vm_start)\n\t\t\tnstart = vma->vm_start;\n\t\t/*\n\t\t * Now fault in a range of pages. __mlock_vma_pages_range()\n\t\t * double checks the vma flags, so that it won't mlock pages\n\t\t * if the vma was already munlocked.\n\t\t */\n\t\tret = __mlock_vma_pages_range(vma, nstart, nend, &locked);\n\t\tif (ret < 0) {\n\t\t\tif (ignore_errors) {\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\t/* continue at next VMA */\n\t\t\t}\n\t\t\tret = __mlock_posix_error_return(ret);\n\t\t\tbreak;\n\t\t}\n\t\tnend = nstart + ret * PAGE_SIZE;\n\t\tret = 0;\n\t}\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\t/* 0 or negative error code */\n}\n\nSYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)\n{\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tint error = -ENOMEM;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlen = PAGE_ALIGN(len + (start & ~PAGE_MASK));\n\tstart &= PAGE_MASK;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = len >> PAGE_SHIFT;\n\n\tdown_write(&current->mm->mmap_sem);\n\n\tlocked += current->mm->locked_vm;\n\n\t/* check against resource limits */\n\tif ((locked <= lock_limit) || capable(CAP_IPC_LOCK))\n\t\terror = do_mlock(start, len, 1);\n\n\tup_write(&current->mm->mmap_sem);\n\tif (!error)\n\t\terror = __mm_populate(start, len, 0);\n\treturn error;\n}\n\nSYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)\n{\n\tint ret;\n\n\tlen = PAGE_ALIGN(len + (start & ~PAGE_MASK));\n\tstart &= PAGE_MASK;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mlock(start, len, 0);\n\tup_write(&current->mm->mmap_sem);\n\n\treturn ret;\n}\n\nstatic int do_mlockall(int flags)\n{\n\tstruct vm_area_struct * vma, * prev = NULL;\n\n\tif (flags & MCL_FUTURE)\n\t\tcurrent->mm->def_flags |= VM_LOCKED;\n\telse\n\t\tcurrent->mm->def_flags &= ~VM_LOCKED;\n\tif (flags == MCL_FUTURE)\n\t\tgoto out;\n\n\tfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\n\t\tvm_flags_t newflags;\n\n\t\tnewflags = vma->vm_flags & ~VM_LOCKED;\n\t\tif (flags & MCL_CURRENT)\n\t\t\tnewflags |= VM_LOCKED;\n\n\t\t/* Ignore errors */\n\t\tmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\n\t\tcond_resched();\n\t}\nout:\n\treturn 0;\n}\n\nSYSCALL_DEFINE1(mlockall, int, flags)\n{\n\tunsigned long lock_limit;\n\tint ret = -EINVAL;\n\n\tif (!flags || (flags & ~(MCL_CURRENT | MCL_FUTURE)))\n\t\tgoto out;\n\n\tret = -EPERM;\n\tif (!can_do_mlock())\n\t\tgoto out;\n\n\tif (flags & MCL_CURRENT)\n\t\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\n\tret = -ENOMEM;\n\tdown_write(&current->mm->mmap_sem);\n\n\tif (!(flags & MCL_CURRENT) || (current->mm->total_vm <= lock_limit) ||\n\t    capable(CAP_IPC_LOCK))\n\t\tret = do_mlockall(flags);\n\tup_write(&current->mm->mmap_sem);\n\tif (!ret && (flags & MCL_CURRENT))\n\t\tmm_populate(0, TASK_SIZE);\nout:\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(munlockall)\n{\n\tint ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mlockall(0);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n\n/*\n * Objects with different lifetime than processes (SHM_LOCK and SHM_HUGETLB\n * shm segments) get accounted against the user_struct instead.\n */\nstatic DEFINE_SPINLOCK(shmlock_user_lock);\n\nint user_shm_lock(size_t size, struct user_struct *user)\n{\n\tunsigned long lock_limit, locked;\n\tint allowed = 0;\n\n\tlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tif (lock_limit == RLIM_INFINITY)\n\t\tallowed = 1;\n\tlock_limit >>= PAGE_SHIFT;\n\tspin_lock(&shmlock_user_lock);\n\tif (!allowed &&\n\t    locked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\n\t\tgoto out;\n\tget_uid(user);\n\tuser->locked_shm += locked;\n\tallowed = 1;\nout:\n\tspin_unlock(&shmlock_user_lock);\n\treturn allowed;\n}\n\nvoid user_shm_unlock(size_t size, struct user_struct *user)\n{\n\tspin_lock(&shmlock_user_lock);\n\tuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tspin_unlock(&shmlock_user_lock);\n\tfree_uid(user);\n}\n", "/*\n * mm/rmap.c - physical to virtual reverse mappings\n *\n * Copyright 2001, Rik van Riel <riel@conectiva.com.br>\n * Released under the General Public License (GPL).\n *\n * Simple, low overhead reverse mapping scheme.\n * Please try to keep this thing as modular as possible.\n *\n * Provides methods for unmapping each kind of mapped page:\n * the anon methods track anonymous pages, and\n * the file methods track pages belonging to an inode.\n *\n * Original design by Rik van Riel <riel@conectiva.com.br> 2001\n * File methods by Dave McCracken <dmccr@us.ibm.com> 2003, 2004\n * Anonymous methods by Andrea Arcangeli <andrea@suse.de> 2004\n * Contributions by Hugh Dickins 2003, 2004\n */\n\n/*\n * Lock ordering in mm:\n *\n * inode->i_mutex\t(while writing or truncating, not reading or faulting)\n *   mm->mmap_sem\n *     page->flags PG_locked (lock_page)\n *       mapping->i_mmap_mutex\n *         anon_vma->rwsem\n *           mm->page_table_lock or pte_lock\n *             zone->lru_lock (in mark_page_accessed, isolate_lru_page)\n *             swap_lock (in swap_duplicate, swap_info_get)\n *               mmlist_lock (in mmput, drain_mmlist and others)\n *               mapping->private_lock (in __set_page_dirty_buffers)\n *               inode->i_lock (in set_page_dirty's __mark_inode_dirty)\n *               bdi.wb->list_lock (in set_page_dirty's __mark_inode_dirty)\n *                 sb_lock (within inode_lock in fs/fs-writeback.c)\n *                 mapping->tree_lock (widely used, in set_page_dirty,\n *                           in arch-dependent flush_dcache_mmap_lock,\n *                           within bdi.wb->list_lock in __sync_single_inode)\n *\n * anon_vma->rwsem,mapping->i_mutex      (memory_failure, collect_procs_anon)\n *   ->tasklist_lock\n *     pte map lock\n */\n\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <linux/memcontrol.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/hugetlb.h>\n#include <linux/backing-dev.h>\n\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic struct kmem_cache *anon_vma_cachep;\nstatic struct kmem_cache *anon_vma_chain_cachep;\n\nstatic inline struct anon_vma *anon_vma_alloc(void)\n{\n\tstruct anon_vma *anon_vma;\n\n\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n\tif (anon_vma) {\n\t\tatomic_set(&anon_vma->refcount, 1);\n\t\t/*\n\t\t * Initialise the anon_vma root to point to itself. If called\n\t\t * from fork, the root will be reset to the parents anon_vma.\n\t\t */\n\t\tanon_vma->root = anon_vma;\n\t}\n\n\treturn anon_vma;\n}\n\nstatic inline void anon_vma_free(struct anon_vma *anon_vma)\n{\n\tVM_BUG_ON(atomic_read(&anon_vma->refcount));\n\n\t/*\n\t * Synchronize against page_lock_anon_vma_read() such that\n\t * we can safely hold the lock without the anon_vma getting\n\t * freed.\n\t *\n\t * Relies on the full mb implied by the atomic_dec_and_test() from\n\t * put_anon_vma() against the acquire barrier implied by\n\t * down_read_trylock() from page_lock_anon_vma_read(). This orders:\n\t *\n\t * page_lock_anon_vma_read()\tVS\tput_anon_vma()\n\t *   down_read_trylock()\t\t  atomic_dec_and_test()\n\t *   LOCK\t\t\t\t  MB\n\t *   atomic_read()\t\t\t  rwsem_is_locked()\n\t *\n\t * LOCK should suffice since the actual taking of the lock must\n\t * happen _before_ what follows.\n\t */\n\tif (rwsem_is_locked(&anon_vma->root->rwsem)) {\n\t\tanon_vma_lock_write(anon_vma);\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n\n\tkmem_cache_free(anon_vma_cachep, anon_vma);\n}\n\nstatic inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)\n{\n\treturn kmem_cache_alloc(anon_vma_chain_cachep, gfp);\n}\n\nstatic void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)\n{\n\tkmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);\n}\n\nstatic void anon_vma_chain_link(struct vm_area_struct *vma,\n\t\t\t\tstruct anon_vma_chain *avc,\n\t\t\t\tstruct anon_vma *anon_vma)\n{\n\tavc->vma = vma;\n\tavc->anon_vma = anon_vma;\n\tlist_add(&avc->same_vma, &vma->anon_vma_chain);\n\tanon_vma_interval_tree_insert(avc, &anon_vma->rb_root);\n}\n\n/**\n * anon_vma_prepare - attach an anon_vma to a memory region\n * @vma: the memory region in question\n *\n * This makes sure the memory mapping described by 'vma' has\n * an 'anon_vma' attached to it, so that we can associate the\n * anonymous pages mapped into it with that anon_vma.\n *\n * The common case will be that we already have one, but if\n * not we either need to find an adjacent mapping that we\n * can re-use the anon_vma from (very common when the only\n * reason for splitting a vma has been mprotect()), or we\n * allocate a new one.\n *\n * Anon-vma allocations are very subtle, because we may have\n * optimistically looked up an anon_vma in page_lock_anon_vma_read()\n * and that may actually touch the spinlock even in the newly\n * allocated vma (it depends on RCU to make sure that the\n * anon_vma isn't actually destroyed).\n *\n * As a result, we need to do proper anon_vma locking even\n * for the new allocation. At the same time, we do not want\n * to do any locking for the common case of already having\n * an anon_vma.\n *\n * This must be called with the mmap_sem held for reading.\n */\nint anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\tif (unlikely(!anon_vma)) {\n\t\tstruct mm_struct *mm = vma->vm_mm;\n\t\tstruct anon_vma *allocated;\n\n\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\tif (!avc)\n\t\t\tgoto out_enomem;\n\n\t\tanon_vma = find_mergeable_anon_vma(vma);\n\t\tallocated = NULL;\n\t\tif (!anon_vma) {\n\t\t\tanon_vma = anon_vma_alloc();\n\t\t\tif (unlikely(!anon_vma))\n\t\t\t\tgoto out_enomem_free_avc;\n\t\t\tallocated = anon_vma;\n\t\t}\n\n\t\tanon_vma_lock_write(anon_vma);\n\t\t/* page_table_lock to protect against threads */\n\t\tspin_lock(&mm->page_table_lock);\n\t\tif (likely(!vma->anon_vma)) {\n\t\t\tvma->anon_vma = anon_vma;\n\t\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\t\tallocated = NULL;\n\t\t\tavc = NULL;\n\t\t}\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock_write(anon_vma);\n\n\t\tif (unlikely(allocated))\n\t\t\tput_anon_vma(allocated);\n\t\tif (unlikely(avc))\n\t\t\tanon_vma_chain_free(avc);\n\t}\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}\n\n/*\n * This is a useful helper function for locking the anon_vma root as\n * we traverse the vma->anon_vma_chain, looping over anon_vma's that\n * have the same vma.\n *\n * Such anon_vma's should have the same root, so you'd expect to see\n * just a single mutex_lock for the whole traversal.\n */\nstatic inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *new_root = anon_vma->root;\n\tif (new_root != root) {\n\t\tif (WARN_ON_ONCE(root))\n\t\t\tup_write(&root->rwsem);\n\t\troot = new_root;\n\t\tdown_write(&root->rwsem);\n\t}\n\treturn root;\n}\n\nstatic inline void unlock_anon_vma_root(struct anon_vma *root)\n{\n\tif (root)\n\t\tup_write(&root->rwsem);\n}\n\n/*\n * Attach the anon_vmas from src to dst.\n * Returns 0 on success, -ENOMEM on failure.\n */\nint anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\n{\n\tstruct anon_vma_chain *avc, *pavc;\n\tstruct anon_vma *root = NULL;\n\n\tlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma;\n\n\t\tavc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!avc)) {\n\t\t\tunlock_anon_vma_root(root);\n\t\t\troot = NULL;\n\t\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\t\tif (!avc)\n\t\t\t\tgoto enomem_failure;\n\t\t}\n\t\tanon_vma = pavc->anon_vma;\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_chain_link(dst, avc, anon_vma);\n\t}\n\tunlock_anon_vma_root(root);\n\treturn 0;\n\n enomem_failure:\n\tunlink_anon_vmas(dst);\n\treturn -ENOMEM;\n}\n\n/*\n * Attach vma to its own anon_vma, as well as to the anon_vmas that\n * the corresponding VMA in the parent process is attached to.\n * Returns 0 on success, non-zero on failure.\n */\nint anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\tif (anon_vma_clone(vma, pvma))\n\t\treturn -ENOMEM;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's spinlock is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}\n\nvoid unlink_anon_vmas(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc, *next;\n\tstruct anon_vma *root = NULL;\n\n\t/*\n\t * Unlink each anon_vma chained to the VMA.  This list is ordered\n\t * from newest to oldest, ensuring the root anon_vma gets freed last.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\n\n\t\t/*\n\t\t * Leave empty anon_vmas on the list - we'll need\n\t\t * to free them outside the lock.\n\t\t */\n\t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root))\n\t\t\tcontinue;\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n\tunlock_anon_vma_root(root);\n\n\t/*\n\t * Iterate the list once more, it now only contains empty and unlinked\n\t * anon_vmas, destroy them. Could not do before due to __put_anon_vma()\n\t * needing to write-acquire the anon_vma->root->rwsem.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\tput_anon_vma(anon_vma);\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n}\n\nstatic void anon_vma_ctor(void *data)\n{\n\tstruct anon_vma *anon_vma = data;\n\n\tinit_rwsem(&anon_vma->rwsem);\n\tatomic_set(&anon_vma->refcount, 0);\n\tanon_vma->rb_root = RB_ROOT;\n}\n\nvoid __init anon_vma_init(void)\n{\n\tanon_vma_cachep = kmem_cache_create(\"anon_vma\", sizeof(struct anon_vma),\n\t\t\t0, SLAB_DESTROY_BY_RCU|SLAB_PANIC, anon_vma_ctor);\n\tanon_vma_chain_cachep = KMEM_CACHE(anon_vma_chain, SLAB_PANIC);\n}\n\n/*\n * Getting a lock on a stable anon_vma from a page off the LRU is tricky!\n *\n * Since there is no serialization what so ever against page_remove_rmap()\n * the best this function can do is return a locked anon_vma that might\n * have been relevant to this page.\n *\n * The page might have been remapped to a different anon_vma or the anon_vma\n * returned may already be freed (and even reused).\n *\n * In case it was remapped to a different anon_vma, the new anon_vma will be a\n * child of the old anon_vma, and the anon_vma lifetime rules will therefore\n * ensure that any anon_vma obtained from the page will still be valid for as\n * long as we observe page_mapped() [ hence all those page_mapped() tests ].\n *\n * All users of this function must be very careful when walking the anon_vma\n * chain and verify that the page in question is indeed mapped in it\n * [ something equivalent to page_mapped_in_vma() ].\n *\n * Since anon_vma's slab is DESTROY_BY_RCU and we know from page_remove_rmap()\n * that the anon_vma pointer from page->mapping is valid if there is a\n * mapcount, we can dereference the anon_vma after observing those.\n */\nstruct anon_vma *page_get_anon_vma(struct page *page)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!page_mapped(page))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this page is still mapped, then its anon_vma cannot have been\n\t * freed.  But if it has been unmapped, we have no security against the\n\t * anon_vma structure being freed and reused (for another anon_vma:\n\t * SLAB_DESTROY_BY_RCU guarantees that - so the atomic_inc_not_zero()\n\t * above cannot corrupt).\n\t */\n\tif (!page_mapped(page)) {\n\t\tput_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t}\nout:\n\trcu_read_unlock();\n\n\treturn anon_vma;\n}\n\n/*\n * Similar to page_get_anon_vma() except it locks the anon_vma.\n *\n * Its a little more complex as it tries to keep the fast path to a single\n * atomic op -- the trylock. If we fail the trylock, we fall back to getting a\n * reference like with page_get_anon_vma() and then block on the mutex.\n */\nstruct anon_vma *page_lock_anon_vma_read(struct page *page)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct anon_vma *root_anon_vma;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!page_mapped(page))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\troot_anon_vma = ACCESS_ONCE(anon_vma->root);\n\tif (down_read_trylock(&root_anon_vma->rwsem)) {\n\t\t/*\n\t\t * If the page is still mapped, then this anon_vma is still\n\t\t * its anon_vma, and holding the mutex ensures that it will\n\t\t * not go away, see anon_vma_free().\n\t\t */\n\t\tif (!page_mapped(page)) {\n\t\t\tup_read(&root_anon_vma->rwsem);\n\t\t\tanon_vma = NULL;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* trylock failed, we got to sleep */\n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page_mapped(page)) {\n\t\tput_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\t/* we pinned the anon_vma, its safe to sleep */\n\trcu_read_unlock();\n\tanon_vma_lock_read(anon_vma);\n\n\tif (atomic_dec_and_test(&anon_vma->refcount)) {\n\t\t/*\n\t\t * Oops, we held the last refcount, release the lock\n\t\t * and bail -- can't simply use put_anon_vma() because\n\t\t * we'll deadlock on the anon_vma_lock_write() recursion.\n\t\t */\n\t\tanon_vma_unlock_read(anon_vma);\n\t\t__put_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t}\n\n\treturn anon_vma;\n\nout:\n\trcu_read_unlock();\n\treturn anon_vma;\n}\n\nvoid page_unlock_anon_vma_read(struct anon_vma *anon_vma)\n{\n\tanon_vma_unlock_read(anon_vma);\n}\n\n/*\n * At what user virtual address is page expected in @vma?\n */\nstatic inline unsigned long\n__vma_address(struct page *page, struct vm_area_struct *vma)\n{\n\tpgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\n\n\tif (unlikely(is_vm_hugetlb_page(vma)))\n\t\tpgoff = page->index << huge_page_order(page_hstate(page));\n\n\treturn vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n}\n\ninline unsigned long\nvma_address(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address = __vma_address(page, vma);\n\n\t/* page should be within @vma mapping range */\n\tVM_BUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\n\treturn address;\n}\n\n/*\n * At what user virtual address is page expected in vma?\n * Caller should check the page is actually part of the vma.\n */\nunsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address;\n\tif (PageAnon(page)) {\n\t\tstruct anon_vma *page__anon_vma = page_anon_vma(page);\n\t\t/*\n\t\t * Note: swapoff's unuse_vma() is more efficient with this\n\t\t * check, and needs it to match anon_vma when KSM is active.\n\t\t */\n\t\tif (!vma->anon_vma || !page__anon_vma ||\n\t\t    vma->anon_vma->root != page__anon_vma->root)\n\t\t\treturn -EFAULT;\n\t} else if (page->mapping && !(vma->vm_flags & VM_NONLINEAR)) {\n\t\tif (!vma->vm_file ||\n\t\t    vma->vm_file->f_mapping != page->mapping)\n\t\t\treturn -EFAULT;\n\t} else\n\t\treturn -EFAULT;\n\taddress = __vma_address(page, vma);\n\tif (unlikely(address < vma->vm_start || address >= vma->vm_end))\n\t\treturn -EFAULT;\n\treturn address;\n}\n\npmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd = NULL;\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\tpmd = NULL;\nout:\n\treturn pmd;\n}\n\n/*\n * Check that @page is mapped at @address into @mm.\n *\n * If @sync is false, page_check_address may perform a racy check to avoid\n * the page table lock when the pte is not present (helpful when reclaiming\n * highly shared pages).\n *\n * On success returns with pte mapped and locked.\n */\npte_t *__page_check_address(struct page *page, struct mm_struct *mm,\n\t\t\t  unsigned long address, spinlock_t **ptlp, int sync)\n{\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tif (unlikely(PageHuge(page))) {\n\t\t/* when pud is not present, pte will be NULL */\n\t\tpte = huge_pte_offset(mm, address);\n\t\tif (!pte)\n\t\t\treturn NULL;\n\n\t\tptl = huge_pte_lockptr(page_hstate(page), mm, pte);\n\t\tgoto check;\n\t}\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tif (pmd_trans_huge(*pmd))\n\t\treturn NULL;\n\n\tpte = pte_offset_map(pmd, address);\n\t/* Make a quick check before getting the lock */\n\tif (!sync && !pte_present(*pte)) {\n\t\tpte_unmap(pte);\n\t\treturn NULL;\n\t}\n\n\tptl = pte_lockptr(mm, pmd);\ncheck:\n\tspin_lock(ptl);\n\tif (pte_present(*pte) && page_to_pfn(page) == pte_pfn(*pte)) {\n\t\t*ptlp = ptl;\n\t\treturn pte;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn NULL;\n}\n\n/**\n * page_mapped_in_vma - check whether a page is really mapped in a VMA\n * @page: the page to test\n * @vma: the VMA to test\n *\n * Returns 1 if the page is mapped into the page tables of the VMA, 0\n * if the page is not mapped into the page tables of this VMA.  Only\n * valid for normal file or anonymous VMAs.\n */\nint page_mapped_in_vma(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\taddress = __vma_address(page, vma);\n\tif (unlikely(address < vma->vm_start || address >= vma->vm_end))\n\t\treturn 0;\n\tpte = page_check_address(page, vma->vm_mm, address, &ptl, 1);\n\tif (!pte)\t\t\t/* the page is not in this mm */\n\t\treturn 0;\n\tpte_unmap_unlock(pte, ptl);\n\n\treturn 1;\n}\n\nstruct page_referenced_arg {\n\tint mapcount;\n\tint referenced;\n\tunsigned long vm_flags;\n\tstruct mem_cgroup *memcg;\n};\n/*\n * arg: page_referenced_arg will be passed\n */\nint page_referenced_one(struct page *page, struct vm_area_struct *vma,\n\t\t\tunsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tint referenced = 0;\n\tstruct page_referenced_arg *pra = arg;\n\n\tif (unlikely(PageTransHuge(page))) {\n\t\tpmd_t *pmd;\n\n\t\t/*\n\t\t * rmap might return false positives; we must filter\n\t\t * these out using page_check_address_pmd().\n\t\t */\n\t\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_FLAG, &ptl);\n\t\tif (!pmd)\n\t\t\treturn SWAP_AGAIN;\n\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tspin_unlock(ptl);\n\t\t\tpra->vm_flags |= VM_LOCKED;\n\t\t\treturn SWAP_FAIL; /* To break the loop */\n\t\t}\n\n\t\t/* go ahead even if the pmd is pmd_trans_splitting() */\n\t\tif (pmdp_clear_flush_young_notify(vma, address, pmd))\n\t\t\treferenced++;\n\t\tspin_unlock(ptl);\n\t} else {\n\t\tpte_t *pte;\n\n\t\t/*\n\t\t * rmap might return false positives; we must filter\n\t\t * these out using page_check_address().\n\t\t */\n\t\tpte = page_check_address(page, mm, address, &ptl, 0);\n\t\tif (!pte)\n\t\t\treturn SWAP_AGAIN;\n\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tpra->vm_flags |= VM_LOCKED;\n\t\t\treturn SWAP_FAIL; /* To break the loop */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte)) {\n\t\t\t/*\n\t\t\t * Don't treat a reference through a sequentially read\n\t\t\t * mapping as such.  If the page has been used in\n\t\t\t * another mapping, we will catch it; if this other\n\t\t\t * mapping is already gone, the unmap path will have\n\t\t\t * set PG_referenced or activated the page.\n\t\t\t */\n\t\t\tif (likely(!(vma->vm_flags & VM_SEQ_READ)))\n\t\t\t\treferenced++;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t}\n\n\tif (referenced) {\n\t\tpra->referenced++;\n\t\tpra->vm_flags |= vma->vm_flags;\n\t}\n\n\tpra->mapcount--;\n\tif (!pra->mapcount)\n\t\treturn SWAP_SUCCESS; /* To break the loop */\n\n\treturn SWAP_AGAIN;\n}\n\nstatic bool invalid_page_referenced_vma(struct vm_area_struct *vma, void *arg)\n{\n\tstruct page_referenced_arg *pra = arg;\n\tstruct mem_cgroup *memcg = pra->memcg;\n\n\tif (!mm_match_cgroup(vma->vm_mm, memcg))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * page_referenced - test if the page was referenced\n * @page: the page to test\n * @is_locked: caller holds lock on the page\n * @memcg: target memory cgroup\n * @vm_flags: collect encountered vma->vm_flags who actually referenced the page\n *\n * Quick test_and_clear_referenced for all mappings to a page,\n * returns the number of ptes which referenced the page.\n */\nint page_referenced(struct page *page,\n\t\t    int is_locked,\n\t\t    struct mem_cgroup *memcg,\n\t\t    unsigned long *vm_flags)\n{\n\tint ret;\n\tint we_locked = 0;\n\tstruct page_referenced_arg pra = {\n\t\t.mapcount = page_mapcount(page),\n\t\t.memcg = memcg,\n\t};\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = page_referenced_one,\n\t\t.arg = (void *)&pra,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\t};\n\n\t*vm_flags = 0;\n\tif (!page_mapped(page))\n\t\treturn 0;\n\n\tif (!page_rmapping(page))\n\t\treturn 0;\n\n\tif (!is_locked && (!PageAnon(page) || PageKsm(page))) {\n\t\twe_locked = trylock_page(page);\n\t\tif (!we_locked)\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If we are reclaiming on behalf of a cgroup, skip\n\t * counting on behalf of references from different\n\t * cgroups\n\t */\n\tif (memcg) {\n\t\trwc.invalid_vma = invalid_page_referenced_vma;\n\t}\n\n\tret = rmap_walk(page, &rwc);\n\t*vm_flags = pra.vm_flags;\n\n\tif (we_locked)\n\t\tunlock_page(page);\n\n\treturn pra.referenced;\n}\n\nstatic int page_mkclean_one(struct page *page, struct vm_area_struct *vma,\n\t\t\t    unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tint ret = 0;\n\tint *cleaned = arg;\n\n\tpte = page_check_address(page, mm, address, &ptl, 1);\n\tif (!pte)\n\t\tgoto out;\n\n\tif (pte_dirty(*pte) || pte_write(*pte)) {\n\t\tpte_t entry;\n\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tentry = ptep_clear_flush(vma, address, pte);\n\t\tentry = pte_wrprotect(entry);\n\t\tentry = pte_mkclean(entry);\n\t\tset_pte_at(mm, address, pte, entry);\n\t\tret = 1;\n\t}\n\n\tpte_unmap_unlock(pte, ptl);\n\n\tif (ret) {\n\t\tmmu_notifier_invalidate_page(mm, address);\n\t\t(*cleaned)++;\n\t}\nout:\n\treturn SWAP_AGAIN;\n}\n\nstatic bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)\n{\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn false;\n\n\treturn true;\n}\n\nint page_mkclean(struct page *page)\n{\n\tint cleaned = 0;\n\tstruct address_space *mapping;\n\tstruct rmap_walk_control rwc = {\n\t\t.arg = (void *)&cleaned,\n\t\t.rmap_one = page_mkclean_one,\n\t\t.invalid_vma = invalid_mkclean_vma,\n\t};\n\n\tBUG_ON(!PageLocked(page));\n\n\tif (!page_mapped(page))\n\t\treturn 0;\n\n\tmapping = page_mapping(page);\n\tif (!mapping)\n\t\treturn 0;\n\n\trmap_walk(page, &rwc);\n\n\treturn cleaned;\n}\nEXPORT_SYMBOL_GPL(page_mkclean);\n\n/**\n * page_move_anon_rmap - move a page to our anon_vma\n * @page:\tthe page to move to our anon_vma\n * @vma:\tthe vma the page belongs to\n * @address:\tthe user virtual address mapped\n *\n * When a page belongs exclusively to one process after a COW event,\n * that page can be moved into the anon_vma that belongs to just that\n * process, so the rmap code will not search the parent or sibling\n * processes.\n */\nvoid page_move_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON(!anon_vma);\n\tVM_BUG_ON_PAGE(page->index != linear_page_index(vma, address), page);\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n}\n\n/**\n * __page_set_anon_rmap - set up new anonymous rmap\n * @page:\tPage to add to rmap\t\n * @vma:\tVM area to add page to.\n * @address:\tUser virtual address of the mapping\t\n * @exclusive:\tthe page is exclusively owned by the current process\n */\nstatic void __page_set_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tBUG_ON(!anon_vma);\n\n\tif (PageAnon(page))\n\t\treturn;\n\n\t/*\n\t * If the page isn't exclusively mapped into this vma,\n\t * we must use the _oldest_ possible anon_vma for the\n\t * page mapping!\n\t */\n\tif (!exclusive)\n\t\tanon_vma = anon_vma->root;\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n\tpage->index = linear_page_index(vma, address);\n}\n\n/**\n * __page_check_anon_rmap - sanity check anonymous rmap addition\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n */\nstatic void __page_check_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n#ifdef CONFIG_DEBUG_VM\n\t/*\n\t * The page's anon-rmap details (mapping and index) are guaranteed to\n\t * be set up correctly at this point.\n\t *\n\t * We have exclusion against page_add_anon_rmap because the caller\n\t * always holds the page locked, except if called from page_dup_rmap,\n\t * in which case the page is already known to be setup.\n\t *\n\t * We have exclusion against page_add_new_anon_rmap because those pages\n\t * are initially only visible via the pagetables, and the pte is locked\n\t * over the call to page_add_new_anon_rmap.\n\t */\n\tBUG_ON(page_anon_vma(page)->root != vma->anon_vma->root);\n\tBUG_ON(page->index != linear_page_index(vma, address));\n#endif\n}\n\n/**\n * page_add_anon_rmap - add pte mapping to an anonymous page\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n *\n * The caller needs to hold the pte lock, and the page must be locked in\n * the anon_vma case: to serialize mapping,index checking after setting,\n * and to ensure that PageAnon is not being upgraded racily to PageKsm\n * (but PageKsm is never downgraded to PageAnon).\n */\nvoid page_add_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tdo_page_add_anon_rmap(page, vma, address, 0);\n}\n\n/*\n * Special version of the above for do_swap_page, which often runs\n * into pages that are exclusively owned by the current process.\n * Everybody else should continue to use page_add_anon_rmap above.\n */\nvoid do_page_add_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tint first = atomic_inc_and_test(&page->_mapcount);\n\tif (first) {\n\t\tif (PageTransHuge(page))\n\t\t\t__inc_zone_page_state(page,\n\t\t\t\t\t      NR_ANON_TRANSPARENT_HUGEPAGES);\n\t\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\t\thpage_nr_pages(page));\n\t}\n\tif (unlikely(PageKsm(page)))\n\t\treturn;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\t/* address might be in next vma when migration races vma_adjust */\n\tif (first)\n\t\t__page_set_anon_rmap(page, vma, address, exclusive);\n\telse\n\t\t__page_check_anon_rmap(page, vma, address);\n}\n\n/**\n * page_add_new_anon_rmap - add pte mapping to a new anonymous page\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n *\n * Same as page_add_anon_rmap but must only be called on *new* pages.\n * This means the inc-and-test can be bypassed.\n * Page does not have to be locked.\n */\nvoid page_add_new_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tVM_BUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\tSetPageSwapBacked(page);\n\tatomic_set(&page->_mapcount, 0); /* increment count (starts at -1) */\n\tif (PageTransHuge(page))\n\t\t__inc_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);\n\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\thpage_nr_pages(page));\n\t__page_set_anon_rmap(page, vma, address, 1);\n\tif (!mlocked_vma_newpage(vma, page)) {\n\t\tSetPageActive(page);\n\t\tlru_cache_add(page);\n\t} else\n\t\tadd_page_to_unevictable_list(page);\n}\n\n/**\n * page_add_file_rmap - add pte mapping to a file page\n * @page: the page to add the mapping to\n *\n * The caller needs to hold the pte lock.\n */\nvoid page_add_file_rmap(struct page *page)\n{\n\tbool locked;\n\tunsigned long flags;\n\n\tmem_cgroup_begin_update_page_stat(page, &locked, &flags);\n\tif (atomic_inc_and_test(&page->_mapcount)) {\n\t\t__inc_zone_page_state(page, NR_FILE_MAPPED);\n\t\tmem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\n\t}\n\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n}\n\n/**\n * page_remove_rmap - take down pte mapping from a page\n * @page: page to remove mapping from\n *\n * The caller needs to hold the pte lock.\n */\nvoid page_remove_rmap(struct page *page)\n{\n\tbool anon = PageAnon(page);\n\tbool locked;\n\tunsigned long flags;\n\n\t/*\n\t * The anon case has no mem_cgroup page_stat to update; but may\n\t * uncharge_page() below, where the lock ordering can deadlock if\n\t * we hold the lock against page_stat move: so avoid it on anon.\n\t */\n\tif (!anon)\n\t\tmem_cgroup_begin_update_page_stat(page, &locked, &flags);\n\n\t/* page still mapped by someone else? */\n\tif (!atomic_add_negative(-1, &page->_mapcount))\n\t\tgoto out;\n\n\t/*\n\t * Hugepages are not counted in NR_ANON_PAGES nor NR_FILE_MAPPED\n\t * and not charged by memcg for now.\n\t */\n\tif (unlikely(PageHuge(page)))\n\t\tgoto out;\n\tif (anon) {\n\t\tmem_cgroup_uncharge_page(page);\n\t\tif (PageTransHuge(page))\n\t\t\t__dec_zone_page_state(page,\n\t\t\t\t\t      NR_ANON_TRANSPARENT_HUGEPAGES);\n\t\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\t\t-hpage_nr_pages(page));\n\t} else {\n\t\t__dec_zone_page_state(page, NR_FILE_MAPPED);\n\t\tmem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\n\t\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n\t}\n\tif (unlikely(PageMlocked(page)))\n\t\tclear_page_mlock(page);\n\t/*\n\t * It would be tidy to reset the PageAnon mapping here,\n\t * but that might overwrite a racing page_add_anon_rmap\n\t * which increments mapcount after us but sets mapping\n\t * before us: so leave the reset to free_hot_cold_page,\n\t * and remember that it's only reliable while mapped.\n\t * Leaving it set also helps swapoff to reinstate ptes\n\t * faster for those pages still in swapcache.\n\t */\n\treturn;\nout:\n\tif (!anon)\n\t\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n}\n\n/*\n * @arg: enum ttu_flags will be passed to this argument\n */\nint try_to_unmap_one(struct page *page, struct vm_area_struct *vma,\n\t\t     unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tint ret = SWAP_AGAIN;\n\tenum ttu_flags flags = (enum ttu_flags)arg;\n\n\tpte = page_check_address(page, mm, address, &ptl, 0);\n\tif (!pte)\n\t\tgoto out;\n\n\t/*\n\t * If the page is mlock()d, we cannot swap it out.\n\t * If it's recently referenced (perhaps page_referenced\n\t * skipped over this mm) then we should reactivate it.\n\t */\n\tif (!(flags & TTU_IGNORE_MLOCK)) {\n\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\tgoto out_mlock;\n\n\t\tif (TTU_ACTION(flags) == TTU_MUNLOCK)\n\t\t\tgoto out_unmap;\n\t}\n\tif (!(flags & TTU_IGNORE_ACCESS)) {\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte)) {\n\t\t\tret = SWAP_FAIL;\n\t\t\tgoto out_unmap;\n\t\t}\n  \t}\n\n\t/* Nuke the page table entry. */\n\tflush_cache_page(vma, address, page_to_pfn(page));\n\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t/* Move the dirty bit to the physical page now the pte is gone. */\n\tif (pte_dirty(pteval))\n\t\tset_page_dirty(page);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tif (PageHWPoison(page) && !(flags & TTU_IGNORE_HWPOISON)) {\n\t\tif (!PageHuge(page)) {\n\t\t\tif (PageAnon(page))\n\t\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\telse\n\t\t\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t}\n\t\tset_pte_at(mm, address, pte,\n\t\t\t   swp_entry_to_pte(make_hwpoison_entry(page)));\n\t} else if (pte_unused(pteval)) {\n\t\t/*\n\t\t * The guest indicated that the page content is of no\n\t\t * interest anymore. Simply discard the pte, vmscan\n\t\t * will take care of the rest.\n\t\t */\n\t\tif (PageAnon(page))\n\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\telse\n\t\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t} else if (PageAnon(page)) {\n\t\tswp_entry_t entry = { .val = page_private(page) };\n\t\tpte_t swp_pte;\n\n\t\tif (PageSwapCache(page)) {\n\t\t\t/*\n\t\t\t * Store the swap location in the pte.\n\t\t\t * See handle_pte_fault() ...\n\t\t\t */\n\t\t\tif (swap_duplicate(entry) < 0) {\n\t\t\t\tset_pte_at(mm, address, pte, pteval);\n\t\t\t\tret = SWAP_FAIL;\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\t\t\tif (list_empty(&mm->mmlist)) {\n\t\t\t\tspin_lock(&mmlist_lock);\n\t\t\t\tif (list_empty(&mm->mmlist))\n\t\t\t\t\tlist_add(&mm->mmlist, &init_mm.mmlist);\n\t\t\t\tspin_unlock(&mmlist_lock);\n\t\t\t}\n\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\tinc_mm_counter(mm, MM_SWAPENTS);\n\t\t} else if (IS_ENABLED(CONFIG_MIGRATION)) {\n\t\t\t/*\n\t\t\t * Store the pfn of the page in a special migration\n\t\t\t * pte. do_swap_page() will wait until the migration\n\t\t\t * pte is removed and then restart fault handling.\n\t\t\t */\n\t\t\tBUG_ON(TTU_ACTION(flags) != TTU_MIGRATION);\n\t\t\tentry = make_migration_entry(page, pte_write(pteval));\n\t\t}\n\t\tswp_pte = swp_entry_to_pte(entry);\n\t\tif (pte_soft_dirty(pteval))\n\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\tset_pte_at(mm, address, pte, swp_pte);\n\t\tBUG_ON(pte_file(*pte));\n\t} else if (IS_ENABLED(CONFIG_MIGRATION) &&\n\t\t   (TTU_ACTION(flags) == TTU_MIGRATION)) {\n\t\t/* Establish migration entry for a file page */\n\t\tswp_entry_t entry;\n\t\tentry = make_migration_entry(page, pte_write(pteval));\n\t\tset_pte_at(mm, address, pte, swp_entry_to_pte(entry));\n\t} else\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\n\tpage_remove_rmap(page);\n\tpage_cache_release(page);\n\nout_unmap:\n\tpte_unmap_unlock(pte, ptl);\n\tif (ret != SWAP_FAIL)\n\t\tmmu_notifier_invalidate_page(mm, address);\nout:\n\treturn ret;\n\nout_mlock:\n\tpte_unmap_unlock(pte, ptl);\n\n\n\t/*\n\t * We need mmap_sem locking, Otherwise VM_LOCKED check makes\n\t * unstable result and race. Plus, We can't wait here because\n\t * we now hold anon_vma->rwsem or mapping->i_mmap_mutex.\n\t * if trylock failed, the page remain in evictable lru and later\n\t * vmscan could retry to move the page to unevictable lru if the\n\t * page is actually mlocked.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tmlock_vma_page(page);\n\t\t\tret = SWAP_MLOCK;\n\t\t}\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\t}\n\treturn ret;\n}\n\n/*\n * objrmap doesn't work for nonlinear VMAs because the assumption that\n * offset-into-file correlates with offset-into-virtual-addresses does not hold.\n * Consequently, given a particular page and its ->index, we cannot locate the\n * ptes which are mapping that page without an exhaustive linear search.\n *\n * So what this code does is a mini \"virtual scan\" of each nonlinear VMA which\n * maps the file to which the target page belongs.  The ->vm_private_data field\n * holds the current cursor into that scan.  Successive searches will circulate\n * around the vma's virtual address space.\n *\n * So as more replacement pressure is applied to the pages in a nonlinear VMA,\n * more scanning pressure is placed against them as well.   Eventually pages\n * will become fully unmapped and are eligible for eviction.\n *\n * For very sparsely populated VMAs this is a little inefficient - chances are\n * there there won't be many ptes located within the scan cluster.  In this case\n * maybe we could scan further - to the end of the pte page, perhaps.\n *\n * Mlocked pages:  check VM_LOCKED under mmap_sem held for read, if we can\n * acquire it without blocking.  If vma locked, mlock the pages in the cluster,\n * rather than unmapping them.  If we encounter the \"check_page\" that vmscan is\n * trying to unmap, return SWAP_MLOCK, else default SWAP_AGAIN.\n */\n#define CLUSTER_SIZE\tmin(32*PAGE_SIZE, PMD_SIZE)\n#define CLUSTER_MASK\t(~(CLUSTER_SIZE - 1))\n\nstatic int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,\n\t\tstruct vm_area_struct *vma, struct page *check_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tunsigned long address;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tunsigned long end;\n\tint ret = SWAP_AGAIN;\n\tint locked_vma = 0;\n\n\taddress = (vma->vm_start + cursor) & CLUSTER_MASK;\n\tend = address + CLUSTER_SIZE;\n\tif (address < vma->vm_start)\n\t\taddress = vma->vm_start;\n\tif (end > vma->vm_end)\n\t\tend = vma->vm_end;\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn ret;\n\n\tmmun_start = address;\n\tmmun_end   = end;\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,\n\t * keep the sem while scanning the cluster for mlocking pages.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tlocked_vma = (vma->vm_flags & VM_LOCKED);\n\t\tif (!locked_vma)\n\t\t\tup_read(&vma->vm_mm->mmap_sem); /* don't need it */\n\t}\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tfor (; address < end; pte++, address += PAGE_SIZE) {\n\t\tif (!pte_present(*pte))\n\t\t\tcontinue;\n\t\tpage = vm_normal_page(vma, address, *pte);\n\t\tBUG_ON(!page || PageAnon(page));\n\n\t\tif (locked_vma) {\n\t\t\tmlock_vma_page(page);   /* no-op if already mlocked */\n\t\t\tif (page == check_page)\n\t\t\t\tret = SWAP_MLOCK;\n\t\t\tcontinue;\t/* don't unmap */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte))\n\t\t\tcontinue;\n\n\t\t/* Nuke the page table entry. */\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t\t/* If nonlinear, store the file page offset in the pte. */\n\t\tif (page->index != linear_page_index(vma, address)) {\n\t\t\tpte_t ptfile = pgoff_to_pte(page->index);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tpte_file_mksoft_dirty(ptfile);\n\t\t\tset_pte_at(mm, address, pte, ptfile);\n\t\t}\n\n\t\t/* Move the dirty bit to the physical page now the pte is gone. */\n\t\tif (pte_dirty(pteval))\n\t\t\tset_page_dirty(page);\n\n\t\tpage_remove_rmap(page);\n\t\tpage_cache_release(page);\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t(*mapcount)--;\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\tif (locked_vma)\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\treturn ret;\n}\n\nstatic int try_to_unmap_nonlinear(struct page *page,\n\t\tstruct address_space *mapping, void *arg)\n{\n\tstruct vm_area_struct *vma;\n\tint ret = SWAP_AGAIN;\n\tunsigned long cursor;\n\tunsigned long max_nl_cursor = 0;\n\tunsigned long max_nl_size = 0;\n\tunsigned int mapcount;\n\n\tlist_for_each_entry(vma,\n\t\t&mapping->i_mmap_nonlinear, shared.nonlinear) {\n\n\t\tcursor = (unsigned long) vma->vm_private_data;\n\t\tif (cursor > max_nl_cursor)\n\t\t\tmax_nl_cursor = cursor;\n\t\tcursor = vma->vm_end - vma->vm_start;\n\t\tif (cursor > max_nl_size)\n\t\t\tmax_nl_size = cursor;\n\t}\n\n\tif (max_nl_size == 0) {\t/* all nonlinears locked or reserved ? */\n\t\treturn SWAP_FAIL;\n\t}\n\n\t/*\n\t * We don't try to search for this page in the nonlinear vmas,\n\t * and page_referenced wouldn't have found it anyway.  Instead\n\t * just walk the nonlinear vmas trying to age and unmap some.\n\t * The mapcount of the page we came in with is irrelevant,\n\t * but even so use it as a guide to how hard we should try?\n\t */\n\tmapcount = page_mapcount(page);\n\tif (!mapcount)\n\t\treturn ret;\n\n\tcond_resched();\n\n\tmax_nl_size = (max_nl_size + CLUSTER_SIZE - 1) & CLUSTER_MASK;\n\tif (max_nl_cursor == 0)\n\t\tmax_nl_cursor = CLUSTER_SIZE;\n\n\tdo {\n\t\tlist_for_each_entry(vma,\n\t\t\t&mapping->i_mmap_nonlinear, shared.nonlinear) {\n\n\t\t\tcursor = (unsigned long) vma->vm_private_data;\n\t\t\twhile (cursor < max_nl_cursor &&\n\t\t\t\tcursor < vma->vm_end - vma->vm_start) {\n\t\t\t\tif (try_to_unmap_cluster(cursor, &mapcount,\n\t\t\t\t\t\tvma, page) == SWAP_MLOCK)\n\t\t\t\t\tret = SWAP_MLOCK;\n\t\t\t\tcursor += CLUSTER_SIZE;\n\t\t\t\tvma->vm_private_data = (void *) cursor;\n\t\t\t\tif ((int)mapcount <= 0)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tvma->vm_private_data = (void *) max_nl_cursor;\n\t\t}\n\t\tcond_resched();\n\t\tmax_nl_cursor += CLUSTER_SIZE;\n\t} while (max_nl_cursor <= max_nl_size);\n\n\t/*\n\t * Don't loop forever (perhaps all the remaining pages are\n\t * in locked vmas).  Reset cursor on all unreserved nonlinear\n\t * vmas, now forgetting on which ones it had fallen behind.\n\t */\n\tlist_for_each_entry(vma, &mapping->i_mmap_nonlinear, shared.nonlinear)\n\t\tvma->vm_private_data = NULL;\n\n\treturn ret;\n}\n\nbool is_vma_temporary_stack(struct vm_area_struct *vma)\n{\n\tint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\n\n\tif (!maybe_stack)\n\t\treturn false;\n\n\tif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\n\t\t\t\t\t\tVM_STACK_INCOMPLETE_SETUP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)\n{\n\treturn is_vma_temporary_stack(vma);\n}\n\nstatic int page_not_mapped(struct page *page)\n{\n\treturn !page_mapped(page);\n};\n\n/**\n * try_to_unmap - try to remove all page table mappings to a page\n * @page: the page to get unmapped\n * @flags: action and flags\n *\n * Tries to remove all the page table entries which are mapping this\n * page, used in the pageout path.  Caller must hold the page lock.\n * Return values are:\n *\n * SWAP_SUCCESS\t- we succeeded in removing all mappings\n * SWAP_AGAIN\t- we missed a mapping, try again later\n * SWAP_FAIL\t- the page is unswappable\n * SWAP_MLOCK\t- page is mlocked.\n */\nint try_to_unmap(struct page *page, enum ttu_flags flags)\n{\n\tint ret;\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_unmap_one,\n\t\t.arg = (void *)flags,\n\t\t.done = page_not_mapped,\n\t\t.file_nonlinear = try_to_unmap_nonlinear,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\t};\n\n\tVM_BUG_ON_PAGE(!PageHuge(page) && PageTransHuge(page), page);\n\n\t/*\n\t * During exec, a temporary VMA is setup and later moved.\n\t * The VMA is moved under the anon_vma lock but not the\n\t * page tables leading to a race where migration cannot\n\t * find the migration ptes. Rather than increasing the\n\t * locking requirements of exec(), migration skips\n\t * temporary VMAs until after exec() completes.\n\t */\n\tif (flags & TTU_MIGRATION && !PageKsm(page) && PageAnon(page))\n\t\trwc.invalid_vma = invalid_migration_vma;\n\n\tret = rmap_walk(page, &rwc);\n\n\tif (ret != SWAP_MLOCK && !page_mapped(page))\n\t\tret = SWAP_SUCCESS;\n\treturn ret;\n}\n\n/**\n * try_to_munlock - try to munlock a page\n * @page: the page to be munlocked\n *\n * Called from munlock code.  Checks all of the VMAs mapping the page\n * to make sure nobody else has this page mlocked. The page will be\n * returned with PG_mlocked cleared if no other vmas have it mlocked.\n *\n * Return values are:\n *\n * SWAP_AGAIN\t- no vma is holding page mlocked, or,\n * SWAP_AGAIN\t- page mapped in mlocked vma -- couldn't acquire mmap sem\n * SWAP_FAIL\t- page cannot be located at present\n * SWAP_MLOCK\t- page is now mlocked.\n */\nint try_to_munlock(struct page *page)\n{\n\tint ret;\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_unmap_one,\n\t\t.arg = (void *)TTU_MUNLOCK,\n\t\t.done = page_not_mapped,\n\t\t/*\n\t\t * We don't bother to try to find the munlocked page in\n\t\t * nonlinears. It's costly. Instead, later, page reclaim logic\n\t\t * may call try_to_unmap() and recover PG_mlocked lazily.\n\t\t */\n\t\t.file_nonlinear = NULL,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\n\t};\n\n\tVM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);\n\n\tret = rmap_walk(page, &rwc);\n\treturn ret;\n}\n\nvoid __put_anon_vma(struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *root = anon_vma->root;\n\n\tif (root != anon_vma && atomic_dec_and_test(&root->refcount))\n\t\tanon_vma_free(root);\n\n\tanon_vma_free(anon_vma);\n}\n\nstatic struct anon_vma *rmap_walk_anon_lock(struct page *page,\n\t\t\t\t\tstruct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma;\n\n\tif (rwc->anon_lock)\n\t\treturn rwc->anon_lock(page);\n\n\t/*\n\t * Note: remove_migration_ptes() cannot use page_lock_anon_vma_read()\n\t * because that depends on page_mapped(); but not all its usages\n\t * are holding mmap_sem. Users without mmap_sem are required to\n\t * take a reference count to prevent the anon_vma disappearing\n\t */\n\tanon_vma = page_anon_vma(page);\n\tif (!anon_vma)\n\t\treturn NULL;\n\n\tanon_vma_lock_read(anon_vma);\n\treturn anon_vma;\n}\n\n/*\n * rmap_walk_anon - do something to anonymous page using the object-based\n * rmap method\n * @page: the page to be handled\n * @rwc: control variable according to each walk type\n *\n * Find all the mappings of a page using the mapping pointer and the vma chains\n * contained in the anon_vma struct it points to.\n *\n * When called from try_to_munlock(), the mmap_sem of the mm containing the vma\n * where the page was found will be held for write.  So, we won't recheck\n * vm_flags for that VMA.  That should be OK, because that vma shouldn't be\n * LOCKED.\n */\nstatic int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma;\n\tpgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\n\tstruct anon_vma_chain *avc;\n\tint ret = SWAP_AGAIN;\n\n\tanon_vma = rmap_walk_anon_lock(page, rwc);\n\tif (!anon_vma)\n\t\treturn ret;\n\n\tanon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long address = vma_address(page, vma);\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tret = rwc->rmap_one(page, vma, address, rwc->arg);\n\t\tif (ret != SWAP_AGAIN)\n\t\t\tbreak;\n\t\tif (rwc->done && rwc->done(page))\n\t\t\tbreak;\n\t}\n\tanon_vma_unlock_read(anon_vma);\n\treturn ret;\n}\n\n/*\n * rmap_walk_file - do something to file page using the object-based rmap method\n * @page: the page to be handled\n * @rwc: control variable according to each walk type\n *\n * Find all the mappings of a page using the mapping pointer and the vma chains\n * contained in the address_space struct it points to.\n *\n * When called from try_to_munlock(), the mmap_sem of the mm containing the vma\n * where the page was found will be held for write.  So, we won't recheck\n * vm_flags for that VMA.  That should be OK, because that vma shouldn't be\n * LOCKED.\n */\nstatic int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)\n{\n\tstruct address_space *mapping = page->mapping;\n\tpgoff_t pgoff = page->index << compound_order(page);\n\tstruct vm_area_struct *vma;\n\tint ret = SWAP_AGAIN;\n\n\t/*\n\t * The page lock not only makes sure that page->mapping cannot\n\t * suddenly be NULLified by truncation, it makes sure that the\n\t * structure at mapping cannot be freed and reused yet,\n\t * so we can safely take mapping->i_mmap_mutex.\n\t */\n\tVM_BUG_ON(!PageLocked(page));\n\n\tif (!mapping)\n\t\treturn ret;\n\tmutex_lock(&mapping->i_mmap_mutex);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\tunsigned long address = vma_address(page, vma);\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tret = rwc->rmap_one(page, vma, address, rwc->arg);\n\t\tif (ret != SWAP_AGAIN)\n\t\t\tgoto done;\n\t\tif (rwc->done && rwc->done(page))\n\t\t\tgoto done;\n\t}\n\n\tif (!rwc->file_nonlinear)\n\t\tgoto done;\n\n\tif (list_empty(&mapping->i_mmap_nonlinear))\n\t\tgoto done;\n\n\tret = rwc->file_nonlinear(page, mapping, rwc->arg);\n\ndone:\n\tmutex_unlock(&mapping->i_mmap_mutex);\n\treturn ret;\n}\n\nint rmap_walk(struct page *page, struct rmap_walk_control *rwc)\n{\n\tif (unlikely(PageKsm(page)))\n\t\treturn rmap_walk_ksm(page, rwc);\n\telse if (PageAnon(page))\n\t\treturn rmap_walk_anon(page, rwc);\n\telse\n\t\treturn rmap_walk_file(page, rwc);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * The following three functions are for anonymous (private mapped) hugepages.\n * Unlike common anonymous pages, anonymous hugepages have no accounting code\n * and no lru code, because we handle hugepages differently from common pages.\n */\nstatic void __hugepage_set_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tBUG_ON(!anon_vma);\n\n\tif (PageAnon(page))\n\t\treturn;\n\tif (!exclusive)\n\t\tanon_vma = anon_vma->root;\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n\tpage->index = linear_page_index(vma, address);\n}\n\nvoid hugepage_add_anon_rmap(struct page *page,\n\t\t\t    struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\tint first;\n\n\tBUG_ON(!PageLocked(page));\n\tBUG_ON(!anon_vma);\n\t/* address might be in next vma when migration races vma_adjust */\n\tfirst = atomic_inc_and_test(&page->_mapcount);\n\tif (first)\n\t\t__hugepage_set_anon_rmap(page, vma, address, 0);\n}\n\nvoid hugepage_add_new_anon_rmap(struct page *page,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tBUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\tatomic_set(&page->_mapcount, 0);\n\t__hugepage_set_anon_rmap(page, vma, address, 1);\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n"], "fixing_code": ["/*\n *\tlinux/mm/mlock.c\n *\n *  (C) Copyright 1995 Linus Torvalds\n *  (C) Copyright 2002 Christoph Hellwig\n */\n\n#include <linux/capability.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/mempolicy.h>\n#include <linux/syscalls.h>\n#include <linux/sched.h>\n#include <linux/export.h>\n#include <linux/rmap.h>\n#include <linux/mmzone.h>\n#include <linux/hugetlb.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n\n#include \"internal.h\"\n\nint can_do_mlock(void)\n{\n\tif (capable(CAP_IPC_LOCK))\n\t\treturn 1;\n\tif (rlimit(RLIMIT_MEMLOCK) != 0)\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL(can_do_mlock);\n\n/*\n * Mlocked pages are marked with PageMlocked() flag for efficient testing\n * in vmscan and, possibly, the fault path; and to support semi-accurate\n * statistics.\n *\n * An mlocked page [PageMlocked(page)] is unevictable.  As such, it will\n * be placed on the LRU \"unevictable\" list, rather than the [in]active lists.\n * The unevictable list is an LRU sibling list to the [in]active lists.\n * PageUnevictable is set to indicate the unevictable state.\n *\n * When lazy mlocking via vmscan, it is important to ensure that the\n * vma's VM_LOCKED status is not concurrently being modified, otherwise we\n * may have mlocked a page that is being munlocked. So lazy mlock must take\n * the mmap_sem for read, and verify that the vma really is locked\n * (see mm/rmap.c).\n */\n\n/*\n *  LRU accounting for clear_page_mlock()\n */\nvoid clear_page_mlock(struct page *page)\n{\n\tif (!TestClearPageMlocked(page))\n\t\treturn;\n\n\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t    -hpage_nr_pages(page));\n\tcount_vm_event(UNEVICTABLE_PGCLEARED);\n\tif (!isolate_lru_page(page)) {\n\t\tputback_lru_page(page);\n\t} else {\n\t\t/*\n\t\t * We lost the race. the page already moved to evictable list.\n\t\t */\n\t\tif (PageUnevictable(page))\n\t\t\tcount_vm_event(UNEVICTABLE_PGSTRANDED);\n\t}\n}\n\n/*\n * Mark page as mlocked if not already.\n * If page on LRU, isolate and putback to move to unevictable list.\n */\nvoid mlock_vma_page(struct page *page)\n{\n\t/* Serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Isolate a page from LRU with optional get_page() pin.\n * Assumes lru_lock already held and page already pinned.\n */\nstatic bool __munlock_isolate_lru_page(struct page *page, bool getpage)\n{\n\tif (PageLRU(page)) {\n\t\tstruct lruvec *lruvec;\n\n\t\tlruvec = mem_cgroup_page_lruvec(page, page_zone(page));\n\t\tif (getpage)\n\t\t\tget_page(page);\n\t\tClearPageLRU(page);\n\t\tdel_page_from_lru_list(page, lruvec, page_lru(page));\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Finish munlock after successful page isolation\n *\n * Page must be locked. This is a wrapper for try_to_munlock()\n * and putback_lru_page() with munlock accounting.\n */\nstatic void __munlock_isolated_page(struct page *page)\n{\n\tint ret = SWAP_AGAIN;\n\n\t/*\n\t * Optimization: if the page was mapped just once, that's our mapping\n\t * and we don't need to check all the other vmas.\n\t */\n\tif (page_mapcount(page) > 1)\n\t\tret = try_to_munlock(page);\n\n\t/* Did try_to_unlock() succeed or punt? */\n\tif (ret != SWAP_MLOCK)\n\t\tcount_vm_event(UNEVICTABLE_PGMUNLOCKED);\n\n\tputback_lru_page(page);\n}\n\n/*\n * Accounting for page isolation fail during munlock\n *\n * Performs accounting when page isolation fails in munlock. There is nothing\n * else to do because it means some other task has already removed the page\n * from the LRU. putback_lru_page() will take care of removing the page from\n * the unevictable list, if necessary. vmscan [page_referenced()] will move\n * the page back to the unevictable list if some other vma has it mlocked.\n */\nstatic void __munlock_isolation_failed(struct page *page)\n{\n\tif (PageUnevictable(page))\n\t\t__count_vm_event(UNEVICTABLE_PGSTRANDED);\n\telse\n\t\t__count_vm_event(UNEVICTABLE_PGMUNLOCKED);\n}\n\n/**\n * munlock_vma_page - munlock a vma page\n * @page - page to be unlocked, either a normal page or THP page head\n *\n * returns the size of the page as a page mask (0 for normal page,\n *         HPAGE_PMD_NR - 1 for THP head page)\n *\n * called from munlock()/munmap() path with page supposedly on the LRU.\n * When we munlock a page, because the vma where we found the page is being\n * munlock()ed or munmap()ed, we want to check whether other vmas hold the\n * page locked so that we can leave it on the unevictable lru list and not\n * bother vmscan with it.  However, to walk the page's rmap list in\n * try_to_munlock() we must isolate the page from the LRU.  If some other\n * task has removed the page from the LRU, we won't be able to do that.\n * So we clear the PageMlocked as we might not get another chance.  If we\n * can't isolate the page, we leave it for putback_lru_page() and vmscan\n * [page_referenced()/try_to_unmap()] to deal with.\n */\nunsigned int munlock_vma_page(struct page *page)\n{\n\tunsigned int nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\t/* For try_to_munlock() and to serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(&zone->lru_lock);\n\n\tnr_pages = hpage_nr_pages(page);\n\tif (!TestClearPageMlocked(page))\n\t\tgoto unlock_out;\n\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(&zone->lru_lock);\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(&zone->lru_lock);\n\nout:\n\treturn nr_pages - 1;\n}\n\n/**\n * __mlock_vma_pages_range() -  mlock a range of pages in the vma.\n * @vma:   target vma\n * @start: start address\n * @end:   end address\n *\n * This takes care of making the pages present too.\n *\n * return 0 on success, negative error code on error.\n *\n * vma->vm_mm->mmap_sem must be held for at least read.\n */\nlong __mlock_vma_pages_range(struct vm_area_struct *vma,\n\t\tunsigned long start, unsigned long end, int *nonblocking)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long nr_pages = (end - start) / PAGE_SIZE;\n\tint gup_flags;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(end   & ~PAGE_MASK);\n\tVM_BUG_ON(start < vma->vm_start);\n\tVM_BUG_ON(end   > vma->vm_end);\n\tVM_BUG_ON(!rwsem_is_locked(&mm->mmap_sem));\n\n\tgup_flags = FOLL_TOUCH | FOLL_MLOCK;\n\t/*\n\t * We want to touch writable mappings with a write fault in order\n\t * to break COW, except for shared mappings because these don't COW\n\t * and we would not want to dirty them for nothing.\n\t */\n\tif ((vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE)\n\t\tgup_flags |= FOLL_WRITE;\n\n\t/*\n\t * We want mlock to succeed for regions that have any permissions\n\t * other than PROT_NONE.\n\t */\n\tif (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC))\n\t\tgup_flags |= FOLL_FORCE;\n\n\t/*\n\t * We made sure addr is within a VMA, so the following will\n\t * not result in a stack expansion that recurses back here.\n\t */\n\treturn __get_user_pages(current, mm, start, nr_pages, gup_flags,\n\t\t\t\tNULL, NULL, nonblocking);\n}\n\n/*\n * convert get_user_pages() return value to posix mlock() error\n */\nstatic int __mlock_posix_error_return(long retval)\n{\n\tif (retval == -EFAULT)\n\t\tretval = -ENOMEM;\n\telse if (retval == -ENOMEM)\n\t\tretval = -EAGAIN;\n\treturn retval;\n}\n\n/*\n * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()\n *\n * The fast path is available only for evictable pages with single mapping.\n * Then we can bypass the per-cpu pvec and get better performance.\n * when mapcount > 1 we need try_to_munlock() which can fail.\n * when !page_evictable(), we need the full redo logic of putback_lru_page to\n * avoid leaving evictable page in unevictable list.\n *\n * In case of success, @page is added to @pvec and @pgrescued is incremented\n * in case that the page was previously unevictable. @page is also unlocked.\n */\nstatic bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,\n\t\tint *pgrescued)\n{\n\tVM_BUG_ON_PAGE(PageLRU(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\n\tif (page_mapcount(page) <= 1 && page_evictable(page)) {\n\t\tpagevec_add(pvec, page);\n\t\tif (TestClearPageUnevictable(page))\n\t\t\t(*pgrescued)++;\n\t\tunlock_page(page);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Putback multiple evictable pages to the LRU\n *\n * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of\n * the pages might have meanwhile become unevictable but that is OK.\n */\nstatic void __putback_lru_fast(struct pagevec *pvec, int pgrescued)\n{\n\tcount_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));\n\t/*\n\t *__pagevec_lru_add() calls release_pages() so we don't call\n\t * put_page() explicitly\n\t */\n\t__pagevec_lru_add(pvec);\n\tcount_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\n}\n\n/*\n * Munlock a batch of pages from the same zone\n *\n * The work is split to two main phases. First phase clears the Mlocked flag\n * and attempts to isolate the pages, all under a single zone lru lock.\n * The second phase finishes the munlock only for pages where isolation\n * succeeded.\n *\n * Note that the pagevec may be modified during the process.\n */\nstatic void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(&zone->lru_lock);\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(&zone->lru_lock);\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}\n\n/*\n * Fill up pagevec for __munlock_pagevec using pte walk\n *\n * The function expects that the struct page corresponding to @start address is\n * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.\n *\n * The rest of @pvec is filled by subsequent pages within the same pmd and same\n * zone, as long as the pte's are present and vm_normal_page() succeeds. These\n * pages also get pinned.\n *\n * Returns the address of the next page that should be scanned. This equals\n * @start + PAGE_SIZE when no page could be added by the pte walk.\n */\nstatic unsigned long __munlock_pagevec_fill(struct pagevec *pvec,\n\t\tstruct vm_area_struct *vma, int zoneid,\tunsigned long start,\n\t\tunsigned long end)\n{\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Initialize pte walk starting at the already pinned page where we\n\t * are sure that there is a pte, as it was pinned under the same\n\t * mmap_sem write op.\n\t */\n\tpte = get_locked_pte(vma->vm_mm, start,\t&ptl);\n\t/* Make sure we do not cross the page table boundary */\n\tend = pgd_addr_end(start, end);\n\tend = pud_addr_end(start, end);\n\tend = pmd_addr_end(start, end);\n\n\t/* The page next to the pinned page is the first we will try to get */\n\tstart += PAGE_SIZE;\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tpte++;\n\t\tif (pte_present(*pte))\n\t\t\tpage = vm_normal_page(vma, start, *pte);\n\t\t/*\n\t\t * Break if page could not be obtained or the page's node+zone does not\n\t\t * match\n\t\t */\n\t\tif (!page || page_zone_id(page) != zoneid)\n\t\t\tbreak;\n\n\t\tget_page(page);\n\t\t/*\n\t\t * Increase the address that will be returned *before* the\n\t\t * eventual break due to pvec becoming full by adding the page\n\t\t */\n\t\tstart += PAGE_SIZE;\n\t\tif (pagevec_add(pvec, page) == 0)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn start;\n}\n\n/*\n * munlock_vma_pages_range() - munlock all pages in the vma range.'\n * @vma - vma containing range to be munlock()ed.\n * @start - start address in @vma of the range\n * @end - end of range in @vma.\n *\n *  For mremap(), munmap() and exit().\n *\n * Called with @vma VM_LOCKED.\n *\n * Returns with VM_LOCKED cleared.  Callers must be prepared to\n * deal with this.\n *\n * We don't save and restore VM_LOCKED here because pages are\n * still on lru.  In unmap path, pages might be scanned by reclaim\n * and re-mlocked by try_to_{munlock|unmap} before we unmap and\n * free them.  This will result in freeing mlocked pages.\n */\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tvma->vm_flags &= ~VM_LOCKED;\n\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tunsigned int page_mask;\n\t\tunsigned long page_increm;\n\t\tstruct pagevec pvec;\n\t\tstruct zone *zone;\n\t\tint zoneid;\n\n\t\tpagevec_init(&pvec, 0);\n\t\t/*\n\t\t * Although FOLL_DUMP is intended for get_dump_page(),\n\t\t * it just so happens that its special treatment of the\n\t\t * ZERO_PAGE (returning an error instead of doing get_page)\n\t\t * suits munlock very well (and if somehow an abnormal page\n\t\t * has sneaked into the range, we won't oops here: great).\n\t\t */\n\t\tpage = follow_page_mask(vma, start, FOLL_GET | FOLL_DUMP,\n\t\t\t\t&page_mask);\n\n\t\tif (page && !IS_ERR(page)) {\n\t\t\tif (PageTransHuge(page)) {\n\t\t\t\tlock_page(page);\n\t\t\t\t/*\n\t\t\t\t * Any THP page found by follow_page_mask() may\n\t\t\t\t * have gotten split before reaching\n\t\t\t\t * munlock_vma_page(), so we need to recompute\n\t\t\t\t * the page_mask here.\n\t\t\t\t */\n\t\t\t\tpage_mask = munlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Non-huge pages are handled in batches via\n\t\t\t\t * pagevec. The pin from follow_page_mask()\n\t\t\t\t * prevents them from collapsing by THP.\n\t\t\t\t */\n\t\t\t\tpagevec_add(&pvec, page);\n\t\t\t\tzone = page_zone(page);\n\t\t\t\tzoneid = page_zone_id(page);\n\n\t\t\t\t/*\n\t\t\t\t * Try to fill the rest of pagevec using fast\n\t\t\t\t * pte walk. This will also update start to\n\t\t\t\t * the next page to process. Then munlock the\n\t\t\t\t * pagevec.\n\t\t\t\t */\n\t\t\t\tstart = __munlock_pagevec_fill(&pvec, vma,\n\t\t\t\t\t\tzoneid, start, end);\n\t\t\t\t__munlock_pagevec(&pvec, zone);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\t\t/* It's a bug to munlock in the middle of a THP page */\n\t\tVM_BUG_ON((start >> PAGE_SHIFT) & page_mask);\n\t\tpage_increm = 1 + page_mask;\n\t\tstart += page_increm * PAGE_SIZE;\nnext:\n\t\tcond_resched();\n\t}\n}\n\n/*\n * mlock_fixup  - handle mlock[all]/munlock[all] requests.\n *\n * Filters out \"special\" vmas -- VM_LOCKED never gets set for these, and\n * munlock is a no-op.  However, for some special vmas, we go ahead and\n * populate the ptes.\n *\n * For vmas that pass the filters, merge/split as appropriate.\n */\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\n\tunsigned long start, unsigned long end, vm_flags_t newflags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgoff_t pgoff;\n\tint nr_pages;\n\tint ret = 0;\n\tint lock = !!(newflags & VM_LOCKED);\n\n\tif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\n\t    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\n\t\tgoto out;\t/* don't set VM_LOCKED,  don't count */\n\n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\n\t\t\t  vma->vm_file, pgoff, vma_policy(vma));\n\tif (*prev) {\n\t\tvma = *prev;\n\t\tgoto success;\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tret = split_vma(mm, vma, start, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\tret = split_vma(mm, vma, end, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nsuccess:\n\t/*\n\t * Keep track of amount of locked VM.\n\t */\n\tnr_pages = (end - start) >> PAGE_SHIFT;\n\tif (!lock)\n\t\tnr_pages = -nr_pages;\n\tmm->locked_vm += nr_pages;\n\n\t/*\n\t * vm_flags is protected by the mmap_sem held in write mode.\n\t * It's okay if try_to_unmap_one unmaps a page just after we\n\t * set VM_LOCKED, __mlock_vma_pages_range will bring it back.\n\t */\n\n\tif (lock)\n\t\tvma->vm_flags = newflags;\n\telse\n\t\tmunlock_vma_pages_range(vma, start, end);\n\nout:\n\t*prev = vma;\n\treturn ret;\n}\n\nstatic int do_mlock(unsigned long start, size_t len, int on)\n{\n\tunsigned long nstart, end, tmp;\n\tstruct vm_area_struct * vma, * prev;\n\tint error;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tvma = find_vma(current->mm, start);\n\tif (!vma || vma->vm_start > start)\n\t\treturn -ENOMEM;\n\n\tprev = vma->vm_prev;\n\tif (start > vma->vm_start)\n\t\tprev = vma;\n\n\tfor (nstart = start ; ; ) {\n\t\tvm_flags_t newflags;\n\n\t\t/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */\n\n\t\tnewflags = vma->vm_flags & ~VM_LOCKED;\n\t\tif (on)\n\t\t\tnewflags |= VM_LOCKED;\n\n\t\ttmp = vma->vm_end;\n\t\tif (tmp > end)\n\t\t\ttmp = end;\n\t\terror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\n\t\tif (error)\n\t\t\tbreak;\n\t\tnstart = tmp;\n\t\tif (nstart < prev->vm_end)\n\t\t\tnstart = prev->vm_end;\n\t\tif (nstart >= end)\n\t\t\tbreak;\n\n\t\tvma = prev->vm_next;\n\t\tif (!vma || vma->vm_start != nstart) {\n\t\t\terror = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * __mm_populate - populate and/or mlock pages within a range of address space.\n *\n * This is used to implement mlock() and the MAP_POPULATE / MAP_LOCKED mmap\n * flags. VMAs must be already marked with the desired vm_flags, and\n * mmap_sem must not be held.\n */\nint __mm_populate(unsigned long start, unsigned long len, int ignore_errors)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long end, nstart, nend;\n\tstruct vm_area_struct *vma = NULL;\n\tint locked = 0;\n\tlong ret = 0;\n\n\tVM_BUG_ON(start & ~PAGE_MASK);\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\n\tfor (nstart = start; nstart < end; nstart = nend) {\n\t\t/*\n\t\t * We want to fault in pages for [nstart; end) address range.\n\t\t * Find first corresponding VMA.\n\t\t */\n\t\tif (!locked) {\n\t\t\tlocked = 1;\n\t\t\tdown_read(&mm->mmap_sem);\n\t\t\tvma = find_vma(mm, nstart);\n\t\t} else if (nstart >= vma->vm_end)\n\t\t\tvma = vma->vm_next;\n\t\tif (!vma || vma->vm_start >= end)\n\t\t\tbreak;\n\t\t/*\n\t\t * Set [nstart; nend) to intersection of desired address\n\t\t * range with the first VMA. Also, skip undesirable VMA types.\n\t\t */\n\t\tnend = min(end, vma->vm_end);\n\t\tif (vma->vm_flags & (VM_IO | VM_PFNMAP))\n\t\t\tcontinue;\n\t\tif (nstart < vma->vm_start)\n\t\t\tnstart = vma->vm_start;\n\t\t/*\n\t\t * Now fault in a range of pages. __mlock_vma_pages_range()\n\t\t * double checks the vma flags, so that it won't mlock pages\n\t\t * if the vma was already munlocked.\n\t\t */\n\t\tret = __mlock_vma_pages_range(vma, nstart, nend, &locked);\n\t\tif (ret < 0) {\n\t\t\tif (ignore_errors) {\n\t\t\t\tret = 0;\n\t\t\t\tcontinue;\t/* continue at next VMA */\n\t\t\t}\n\t\t\tret = __mlock_posix_error_return(ret);\n\t\t\tbreak;\n\t\t}\n\t\tnend = nstart + ret * PAGE_SIZE;\n\t\tret = 0;\n\t}\n\tif (locked)\n\t\tup_read(&mm->mmap_sem);\n\treturn ret;\t/* 0 or negative error code */\n}\n\nSYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)\n{\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tint error = -ENOMEM;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlen = PAGE_ALIGN(len + (start & ~PAGE_MASK));\n\tstart &= PAGE_MASK;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = len >> PAGE_SHIFT;\n\n\tdown_write(&current->mm->mmap_sem);\n\n\tlocked += current->mm->locked_vm;\n\n\t/* check against resource limits */\n\tif ((locked <= lock_limit) || capable(CAP_IPC_LOCK))\n\t\terror = do_mlock(start, len, 1);\n\n\tup_write(&current->mm->mmap_sem);\n\tif (!error)\n\t\terror = __mm_populate(start, len, 0);\n\treturn error;\n}\n\nSYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)\n{\n\tint ret;\n\n\tlen = PAGE_ALIGN(len + (start & ~PAGE_MASK));\n\tstart &= PAGE_MASK;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mlock(start, len, 0);\n\tup_write(&current->mm->mmap_sem);\n\n\treturn ret;\n}\n\nstatic int do_mlockall(int flags)\n{\n\tstruct vm_area_struct * vma, * prev = NULL;\n\n\tif (flags & MCL_FUTURE)\n\t\tcurrent->mm->def_flags |= VM_LOCKED;\n\telse\n\t\tcurrent->mm->def_flags &= ~VM_LOCKED;\n\tif (flags == MCL_FUTURE)\n\t\tgoto out;\n\n\tfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\n\t\tvm_flags_t newflags;\n\n\t\tnewflags = vma->vm_flags & ~VM_LOCKED;\n\t\tif (flags & MCL_CURRENT)\n\t\t\tnewflags |= VM_LOCKED;\n\n\t\t/* Ignore errors */\n\t\tmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\n\t\tcond_resched();\n\t}\nout:\n\treturn 0;\n}\n\nSYSCALL_DEFINE1(mlockall, int, flags)\n{\n\tunsigned long lock_limit;\n\tint ret = -EINVAL;\n\n\tif (!flags || (flags & ~(MCL_CURRENT | MCL_FUTURE)))\n\t\tgoto out;\n\n\tret = -EPERM;\n\tif (!can_do_mlock())\n\t\tgoto out;\n\n\tif (flags & MCL_CURRENT)\n\t\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\n\tret = -ENOMEM;\n\tdown_write(&current->mm->mmap_sem);\n\n\tif (!(flags & MCL_CURRENT) || (current->mm->total_vm <= lock_limit) ||\n\t    capable(CAP_IPC_LOCK))\n\t\tret = do_mlockall(flags);\n\tup_write(&current->mm->mmap_sem);\n\tif (!ret && (flags & MCL_CURRENT))\n\t\tmm_populate(0, TASK_SIZE);\nout:\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(munlockall)\n{\n\tint ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mlockall(0);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n\n/*\n * Objects with different lifetime than processes (SHM_LOCK and SHM_HUGETLB\n * shm segments) get accounted against the user_struct instead.\n */\nstatic DEFINE_SPINLOCK(shmlock_user_lock);\n\nint user_shm_lock(size_t size, struct user_struct *user)\n{\n\tunsigned long lock_limit, locked;\n\tint allowed = 0;\n\n\tlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tif (lock_limit == RLIM_INFINITY)\n\t\tallowed = 1;\n\tlock_limit >>= PAGE_SHIFT;\n\tspin_lock(&shmlock_user_lock);\n\tif (!allowed &&\n\t    locked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\n\t\tgoto out;\n\tget_uid(user);\n\tuser->locked_shm += locked;\n\tallowed = 1;\nout:\n\tspin_unlock(&shmlock_user_lock);\n\treturn allowed;\n}\n\nvoid user_shm_unlock(size_t size, struct user_struct *user)\n{\n\tspin_lock(&shmlock_user_lock);\n\tuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tspin_unlock(&shmlock_user_lock);\n\tfree_uid(user);\n}\n", "/*\n * mm/rmap.c - physical to virtual reverse mappings\n *\n * Copyright 2001, Rik van Riel <riel@conectiva.com.br>\n * Released under the General Public License (GPL).\n *\n * Simple, low overhead reverse mapping scheme.\n * Please try to keep this thing as modular as possible.\n *\n * Provides methods for unmapping each kind of mapped page:\n * the anon methods track anonymous pages, and\n * the file methods track pages belonging to an inode.\n *\n * Original design by Rik van Riel <riel@conectiva.com.br> 2001\n * File methods by Dave McCracken <dmccr@us.ibm.com> 2003, 2004\n * Anonymous methods by Andrea Arcangeli <andrea@suse.de> 2004\n * Contributions by Hugh Dickins 2003, 2004\n */\n\n/*\n * Lock ordering in mm:\n *\n * inode->i_mutex\t(while writing or truncating, not reading or faulting)\n *   mm->mmap_sem\n *     page->flags PG_locked (lock_page)\n *       mapping->i_mmap_mutex\n *         anon_vma->rwsem\n *           mm->page_table_lock or pte_lock\n *             zone->lru_lock (in mark_page_accessed, isolate_lru_page)\n *             swap_lock (in swap_duplicate, swap_info_get)\n *               mmlist_lock (in mmput, drain_mmlist and others)\n *               mapping->private_lock (in __set_page_dirty_buffers)\n *               inode->i_lock (in set_page_dirty's __mark_inode_dirty)\n *               bdi.wb->list_lock (in set_page_dirty's __mark_inode_dirty)\n *                 sb_lock (within inode_lock in fs/fs-writeback.c)\n *                 mapping->tree_lock (widely used, in set_page_dirty,\n *                           in arch-dependent flush_dcache_mmap_lock,\n *                           within bdi.wb->list_lock in __sync_single_inode)\n *\n * anon_vma->rwsem,mapping->i_mutex      (memory_failure, collect_procs_anon)\n *   ->tasklist_lock\n *     pte map lock\n */\n\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/ksm.h>\n#include <linux/rmap.h>\n#include <linux/rcupdate.h>\n#include <linux/export.h>\n#include <linux/memcontrol.h>\n#include <linux/mmu_notifier.h>\n#include <linux/migrate.h>\n#include <linux/hugetlb.h>\n#include <linux/backing-dev.h>\n\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic struct kmem_cache *anon_vma_cachep;\nstatic struct kmem_cache *anon_vma_chain_cachep;\n\nstatic inline struct anon_vma *anon_vma_alloc(void)\n{\n\tstruct anon_vma *anon_vma;\n\n\tanon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);\n\tif (anon_vma) {\n\t\tatomic_set(&anon_vma->refcount, 1);\n\t\t/*\n\t\t * Initialise the anon_vma root to point to itself. If called\n\t\t * from fork, the root will be reset to the parents anon_vma.\n\t\t */\n\t\tanon_vma->root = anon_vma;\n\t}\n\n\treturn anon_vma;\n}\n\nstatic inline void anon_vma_free(struct anon_vma *anon_vma)\n{\n\tVM_BUG_ON(atomic_read(&anon_vma->refcount));\n\n\t/*\n\t * Synchronize against page_lock_anon_vma_read() such that\n\t * we can safely hold the lock without the anon_vma getting\n\t * freed.\n\t *\n\t * Relies on the full mb implied by the atomic_dec_and_test() from\n\t * put_anon_vma() against the acquire barrier implied by\n\t * down_read_trylock() from page_lock_anon_vma_read(). This orders:\n\t *\n\t * page_lock_anon_vma_read()\tVS\tput_anon_vma()\n\t *   down_read_trylock()\t\t  atomic_dec_and_test()\n\t *   LOCK\t\t\t\t  MB\n\t *   atomic_read()\t\t\t  rwsem_is_locked()\n\t *\n\t * LOCK should suffice since the actual taking of the lock must\n\t * happen _before_ what follows.\n\t */\n\tif (rwsem_is_locked(&anon_vma->root->rwsem)) {\n\t\tanon_vma_lock_write(anon_vma);\n\t\tanon_vma_unlock_write(anon_vma);\n\t}\n\n\tkmem_cache_free(anon_vma_cachep, anon_vma);\n}\n\nstatic inline struct anon_vma_chain *anon_vma_chain_alloc(gfp_t gfp)\n{\n\treturn kmem_cache_alloc(anon_vma_chain_cachep, gfp);\n}\n\nstatic void anon_vma_chain_free(struct anon_vma_chain *anon_vma_chain)\n{\n\tkmem_cache_free(anon_vma_chain_cachep, anon_vma_chain);\n}\n\nstatic void anon_vma_chain_link(struct vm_area_struct *vma,\n\t\t\t\tstruct anon_vma_chain *avc,\n\t\t\t\tstruct anon_vma *anon_vma)\n{\n\tavc->vma = vma;\n\tavc->anon_vma = anon_vma;\n\tlist_add(&avc->same_vma, &vma->anon_vma_chain);\n\tanon_vma_interval_tree_insert(avc, &anon_vma->rb_root);\n}\n\n/**\n * anon_vma_prepare - attach an anon_vma to a memory region\n * @vma: the memory region in question\n *\n * This makes sure the memory mapping described by 'vma' has\n * an 'anon_vma' attached to it, so that we can associate the\n * anonymous pages mapped into it with that anon_vma.\n *\n * The common case will be that we already have one, but if\n * not we either need to find an adjacent mapping that we\n * can re-use the anon_vma from (very common when the only\n * reason for splitting a vma has been mprotect()), or we\n * allocate a new one.\n *\n * Anon-vma allocations are very subtle, because we may have\n * optimistically looked up an anon_vma in page_lock_anon_vma_read()\n * and that may actually touch the spinlock even in the newly\n * allocated vma (it depends on RCU to make sure that the\n * anon_vma isn't actually destroyed).\n *\n * As a result, we need to do proper anon_vma locking even\n * for the new allocation. At the same time, we do not want\n * to do any locking for the common case of already having\n * an anon_vma.\n *\n * This must be called with the mmap_sem held for reading.\n */\nint anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\tstruct anon_vma_chain *avc;\n\n\tmight_sleep();\n\tif (unlikely(!anon_vma)) {\n\t\tstruct mm_struct *mm = vma->vm_mm;\n\t\tstruct anon_vma *allocated;\n\n\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\tif (!avc)\n\t\t\tgoto out_enomem;\n\n\t\tanon_vma = find_mergeable_anon_vma(vma);\n\t\tallocated = NULL;\n\t\tif (!anon_vma) {\n\t\t\tanon_vma = anon_vma_alloc();\n\t\t\tif (unlikely(!anon_vma))\n\t\t\t\tgoto out_enomem_free_avc;\n\t\t\tallocated = anon_vma;\n\t\t}\n\n\t\tanon_vma_lock_write(anon_vma);\n\t\t/* page_table_lock to protect against threads */\n\t\tspin_lock(&mm->page_table_lock);\n\t\tif (likely(!vma->anon_vma)) {\n\t\t\tvma->anon_vma = anon_vma;\n\t\t\tanon_vma_chain_link(vma, avc, anon_vma);\n\t\t\tallocated = NULL;\n\t\t\tavc = NULL;\n\t\t}\n\t\tspin_unlock(&mm->page_table_lock);\n\t\tanon_vma_unlock_write(anon_vma);\n\n\t\tif (unlikely(allocated))\n\t\t\tput_anon_vma(allocated);\n\t\tif (unlikely(avc))\n\t\t\tanon_vma_chain_free(avc);\n\t}\n\treturn 0;\n\n out_enomem_free_avc:\n\tanon_vma_chain_free(avc);\n out_enomem:\n\treturn -ENOMEM;\n}\n\n/*\n * This is a useful helper function for locking the anon_vma root as\n * we traverse the vma->anon_vma_chain, looping over anon_vma's that\n * have the same vma.\n *\n * Such anon_vma's should have the same root, so you'd expect to see\n * just a single mutex_lock for the whole traversal.\n */\nstatic inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *new_root = anon_vma->root;\n\tif (new_root != root) {\n\t\tif (WARN_ON_ONCE(root))\n\t\t\tup_write(&root->rwsem);\n\t\troot = new_root;\n\t\tdown_write(&root->rwsem);\n\t}\n\treturn root;\n}\n\nstatic inline void unlock_anon_vma_root(struct anon_vma *root)\n{\n\tif (root)\n\t\tup_write(&root->rwsem);\n}\n\n/*\n * Attach the anon_vmas from src to dst.\n * Returns 0 on success, -ENOMEM on failure.\n */\nint anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)\n{\n\tstruct anon_vma_chain *avc, *pavc;\n\tstruct anon_vma *root = NULL;\n\n\tlist_for_each_entry_reverse(pavc, &src->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma;\n\n\t\tavc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!avc)) {\n\t\t\tunlock_anon_vma_root(root);\n\t\t\troot = NULL;\n\t\t\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\t\t\tif (!avc)\n\t\t\t\tgoto enomem_failure;\n\t\t}\n\t\tanon_vma = pavc->anon_vma;\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_chain_link(dst, avc, anon_vma);\n\t}\n\tunlock_anon_vma_root(root);\n\treturn 0;\n\n enomem_failure:\n\tunlink_anon_vmas(dst);\n\treturn -ENOMEM;\n}\n\n/*\n * Attach vma to its own anon_vma, as well as to the anon_vmas that\n * the corresponding VMA in the parent process is attached to.\n * Returns 0 on success, non-zero on failure.\n */\nint anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)\n{\n\tstruct anon_vma_chain *avc;\n\tstruct anon_vma *anon_vma;\n\n\t/* Don't bother if the parent process has no anon_vma here. */\n\tif (!pvma->anon_vma)\n\t\treturn 0;\n\n\t/*\n\t * First, attach the new VMA to the parent VMA's anon_vmas,\n\t * so rmap can find non-COWed pages in child processes.\n\t */\n\tif (anon_vma_clone(vma, pvma))\n\t\treturn -ENOMEM;\n\n\t/* Then add our own anon_vma. */\n\tanon_vma = anon_vma_alloc();\n\tif (!anon_vma)\n\t\tgoto out_error;\n\tavc = anon_vma_chain_alloc(GFP_KERNEL);\n\tif (!avc)\n\t\tgoto out_error_free_anon_vma;\n\n\t/*\n\t * The root anon_vma's spinlock is the lock actually used when we\n\t * lock any of the anon_vmas in this anon_vma tree.\n\t */\n\tanon_vma->root = pvma->anon_vma->root;\n\t/*\n\t * With refcounts, an anon_vma can stay around longer than the\n\t * process it belongs to. The root anon_vma needs to be pinned until\n\t * this anon_vma is freed, because the lock lives in the root.\n\t */\n\tget_anon_vma(anon_vma->root);\n\t/* Mark this anon_vma as the one where our new (COWed) pages go. */\n\tvma->anon_vma = anon_vma;\n\tanon_vma_lock_write(anon_vma);\n\tanon_vma_chain_link(vma, avc, anon_vma);\n\tanon_vma_unlock_write(anon_vma);\n\n\treturn 0;\n\n out_error_free_anon_vma:\n\tput_anon_vma(anon_vma);\n out_error:\n\tunlink_anon_vmas(vma);\n\treturn -ENOMEM;\n}\n\nvoid unlink_anon_vmas(struct vm_area_struct *vma)\n{\n\tstruct anon_vma_chain *avc, *next;\n\tstruct anon_vma *root = NULL;\n\n\t/*\n\t * Unlink each anon_vma chained to the VMA.  This list is ordered\n\t * from newest to oldest, ensuring the root anon_vma gets freed last.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\troot = lock_anon_vma_root(root, anon_vma);\n\t\tanon_vma_interval_tree_remove(avc, &anon_vma->rb_root);\n\n\t\t/*\n\t\t * Leave empty anon_vmas on the list - we'll need\n\t\t * to free them outside the lock.\n\t\t */\n\t\tif (RB_EMPTY_ROOT(&anon_vma->rb_root))\n\t\t\tcontinue;\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n\tunlock_anon_vma_root(root);\n\n\t/*\n\t * Iterate the list once more, it now only contains empty and unlinked\n\t * anon_vmas, destroy them. Could not do before due to __put_anon_vma()\n\t * needing to write-acquire the anon_vma->root->rwsem.\n\t */\n\tlist_for_each_entry_safe(avc, next, &vma->anon_vma_chain, same_vma) {\n\t\tstruct anon_vma *anon_vma = avc->anon_vma;\n\n\t\tput_anon_vma(anon_vma);\n\n\t\tlist_del(&avc->same_vma);\n\t\tanon_vma_chain_free(avc);\n\t}\n}\n\nstatic void anon_vma_ctor(void *data)\n{\n\tstruct anon_vma *anon_vma = data;\n\n\tinit_rwsem(&anon_vma->rwsem);\n\tatomic_set(&anon_vma->refcount, 0);\n\tanon_vma->rb_root = RB_ROOT;\n}\n\nvoid __init anon_vma_init(void)\n{\n\tanon_vma_cachep = kmem_cache_create(\"anon_vma\", sizeof(struct anon_vma),\n\t\t\t0, SLAB_DESTROY_BY_RCU|SLAB_PANIC, anon_vma_ctor);\n\tanon_vma_chain_cachep = KMEM_CACHE(anon_vma_chain, SLAB_PANIC);\n}\n\n/*\n * Getting a lock on a stable anon_vma from a page off the LRU is tricky!\n *\n * Since there is no serialization what so ever against page_remove_rmap()\n * the best this function can do is return a locked anon_vma that might\n * have been relevant to this page.\n *\n * The page might have been remapped to a different anon_vma or the anon_vma\n * returned may already be freed (and even reused).\n *\n * In case it was remapped to a different anon_vma, the new anon_vma will be a\n * child of the old anon_vma, and the anon_vma lifetime rules will therefore\n * ensure that any anon_vma obtained from the page will still be valid for as\n * long as we observe page_mapped() [ hence all those page_mapped() tests ].\n *\n * All users of this function must be very careful when walking the anon_vma\n * chain and verify that the page in question is indeed mapped in it\n * [ something equivalent to page_mapped_in_vma() ].\n *\n * Since anon_vma's slab is DESTROY_BY_RCU and we know from page_remove_rmap()\n * that the anon_vma pointer from page->mapping is valid if there is a\n * mapcount, we can dereference the anon_vma after observing those.\n */\nstruct anon_vma *page_get_anon_vma(struct page *page)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!page_mapped(page))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this page is still mapped, then its anon_vma cannot have been\n\t * freed.  But if it has been unmapped, we have no security against the\n\t * anon_vma structure being freed and reused (for another anon_vma:\n\t * SLAB_DESTROY_BY_RCU guarantees that - so the atomic_inc_not_zero()\n\t * above cannot corrupt).\n\t */\n\tif (!page_mapped(page)) {\n\t\tput_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t}\nout:\n\trcu_read_unlock();\n\n\treturn anon_vma;\n}\n\n/*\n * Similar to page_get_anon_vma() except it locks the anon_vma.\n *\n * Its a little more complex as it tries to keep the fast path to a single\n * atomic op -- the trylock. If we fail the trylock, we fall back to getting a\n * reference like with page_get_anon_vma() and then block on the mutex.\n */\nstruct anon_vma *page_lock_anon_vma_read(struct page *page)\n{\n\tstruct anon_vma *anon_vma = NULL;\n\tstruct anon_vma *root_anon_vma;\n\tunsigned long anon_mapping;\n\n\trcu_read_lock();\n\tanon_mapping = (unsigned long) ACCESS_ONCE(page->mapping);\n\tif ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)\n\t\tgoto out;\n\tif (!page_mapped(page))\n\t\tgoto out;\n\n\tanon_vma = (struct anon_vma *) (anon_mapping - PAGE_MAPPING_ANON);\n\troot_anon_vma = ACCESS_ONCE(anon_vma->root);\n\tif (down_read_trylock(&root_anon_vma->rwsem)) {\n\t\t/*\n\t\t * If the page is still mapped, then this anon_vma is still\n\t\t * its anon_vma, and holding the mutex ensures that it will\n\t\t * not go away, see anon_vma_free().\n\t\t */\n\t\tif (!page_mapped(page)) {\n\t\t\tup_read(&root_anon_vma->rwsem);\n\t\t\tanon_vma = NULL;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* trylock failed, we got to sleep */\n\tif (!atomic_inc_not_zero(&anon_vma->refcount)) {\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\tif (!page_mapped(page)) {\n\t\tput_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t\tgoto out;\n\t}\n\n\t/* we pinned the anon_vma, its safe to sleep */\n\trcu_read_unlock();\n\tanon_vma_lock_read(anon_vma);\n\n\tif (atomic_dec_and_test(&anon_vma->refcount)) {\n\t\t/*\n\t\t * Oops, we held the last refcount, release the lock\n\t\t * and bail -- can't simply use put_anon_vma() because\n\t\t * we'll deadlock on the anon_vma_lock_write() recursion.\n\t\t */\n\t\tanon_vma_unlock_read(anon_vma);\n\t\t__put_anon_vma(anon_vma);\n\t\tanon_vma = NULL;\n\t}\n\n\treturn anon_vma;\n\nout:\n\trcu_read_unlock();\n\treturn anon_vma;\n}\n\nvoid page_unlock_anon_vma_read(struct anon_vma *anon_vma)\n{\n\tanon_vma_unlock_read(anon_vma);\n}\n\n/*\n * At what user virtual address is page expected in @vma?\n */\nstatic inline unsigned long\n__vma_address(struct page *page, struct vm_area_struct *vma)\n{\n\tpgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\n\n\tif (unlikely(is_vm_hugetlb_page(vma)))\n\t\tpgoff = page->index << huge_page_order(page_hstate(page));\n\n\treturn vma->vm_start + ((pgoff - vma->vm_pgoff) << PAGE_SHIFT);\n}\n\ninline unsigned long\nvma_address(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address = __vma_address(page, vma);\n\n\t/* page should be within @vma mapping range */\n\tVM_BUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\n\treturn address;\n}\n\n/*\n * At what user virtual address is page expected in vma?\n * Caller should check the page is actually part of the vma.\n */\nunsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address;\n\tif (PageAnon(page)) {\n\t\tstruct anon_vma *page__anon_vma = page_anon_vma(page);\n\t\t/*\n\t\t * Note: swapoff's unuse_vma() is more efficient with this\n\t\t * check, and needs it to match anon_vma when KSM is active.\n\t\t */\n\t\tif (!vma->anon_vma || !page__anon_vma ||\n\t\t    vma->anon_vma->root != page__anon_vma->root)\n\t\t\treturn -EFAULT;\n\t} else if (page->mapping && !(vma->vm_flags & VM_NONLINEAR)) {\n\t\tif (!vma->vm_file ||\n\t\t    vma->vm_file->f_mapping != page->mapping)\n\t\t\treturn -EFAULT;\n\t} else\n\t\treturn -EFAULT;\n\taddress = __vma_address(page, vma);\n\tif (unlikely(address < vma->vm_start || address >= vma->vm_end))\n\t\treturn -EFAULT;\n\treturn address;\n}\n\npmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd = NULL;\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tpud = pud_offset(pgd, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\tpmd = NULL;\nout:\n\treturn pmd;\n}\n\n/*\n * Check that @page is mapped at @address into @mm.\n *\n * If @sync is false, page_check_address may perform a racy check to avoid\n * the page table lock when the pte is not present (helpful when reclaiming\n * highly shared pages).\n *\n * On success returns with pte mapped and locked.\n */\npte_t *__page_check_address(struct page *page, struct mm_struct *mm,\n\t\t\t  unsigned long address, spinlock_t **ptlp, int sync)\n{\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tif (unlikely(PageHuge(page))) {\n\t\t/* when pud is not present, pte will be NULL */\n\t\tpte = huge_pte_offset(mm, address);\n\t\tif (!pte)\n\t\t\treturn NULL;\n\n\t\tptl = huge_pte_lockptr(page_hstate(page), mm, pte);\n\t\tgoto check;\n\t}\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tif (pmd_trans_huge(*pmd))\n\t\treturn NULL;\n\n\tpte = pte_offset_map(pmd, address);\n\t/* Make a quick check before getting the lock */\n\tif (!sync && !pte_present(*pte)) {\n\t\tpte_unmap(pte);\n\t\treturn NULL;\n\t}\n\n\tptl = pte_lockptr(mm, pmd);\ncheck:\n\tspin_lock(ptl);\n\tif (pte_present(*pte) && page_to_pfn(page) == pte_pfn(*pte)) {\n\t\t*ptlp = ptl;\n\t\treturn pte;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn NULL;\n}\n\n/**\n * page_mapped_in_vma - check whether a page is really mapped in a VMA\n * @page: the page to test\n * @vma: the VMA to test\n *\n * Returns 1 if the page is mapped into the page tables of the VMA, 0\n * if the page is not mapped into the page tables of this VMA.  Only\n * valid for normal file or anonymous VMAs.\n */\nint page_mapped_in_vma(struct page *page, struct vm_area_struct *vma)\n{\n\tunsigned long address;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\taddress = __vma_address(page, vma);\n\tif (unlikely(address < vma->vm_start || address >= vma->vm_end))\n\t\treturn 0;\n\tpte = page_check_address(page, vma->vm_mm, address, &ptl, 1);\n\tif (!pte)\t\t\t/* the page is not in this mm */\n\t\treturn 0;\n\tpte_unmap_unlock(pte, ptl);\n\n\treturn 1;\n}\n\nstruct page_referenced_arg {\n\tint mapcount;\n\tint referenced;\n\tunsigned long vm_flags;\n\tstruct mem_cgroup *memcg;\n};\n/*\n * arg: page_referenced_arg will be passed\n */\nint page_referenced_one(struct page *page, struct vm_area_struct *vma,\n\t\t\tunsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tspinlock_t *ptl;\n\tint referenced = 0;\n\tstruct page_referenced_arg *pra = arg;\n\n\tif (unlikely(PageTransHuge(page))) {\n\t\tpmd_t *pmd;\n\n\t\t/*\n\t\t * rmap might return false positives; we must filter\n\t\t * these out using page_check_address_pmd().\n\t\t */\n\t\tpmd = page_check_address_pmd(page, mm, address,\n\t\t\t\t\t     PAGE_CHECK_ADDRESS_PMD_FLAG, &ptl);\n\t\tif (!pmd)\n\t\t\treturn SWAP_AGAIN;\n\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tspin_unlock(ptl);\n\t\t\tpra->vm_flags |= VM_LOCKED;\n\t\t\treturn SWAP_FAIL; /* To break the loop */\n\t\t}\n\n\t\t/* go ahead even if the pmd is pmd_trans_splitting() */\n\t\tif (pmdp_clear_flush_young_notify(vma, address, pmd))\n\t\t\treferenced++;\n\t\tspin_unlock(ptl);\n\t} else {\n\t\tpte_t *pte;\n\n\t\t/*\n\t\t * rmap might return false positives; we must filter\n\t\t * these out using page_check_address().\n\t\t */\n\t\tpte = page_check_address(page, mm, address, &ptl, 0);\n\t\tif (!pte)\n\t\t\treturn SWAP_AGAIN;\n\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tpte_unmap_unlock(pte, ptl);\n\t\t\tpra->vm_flags |= VM_LOCKED;\n\t\t\treturn SWAP_FAIL; /* To break the loop */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte)) {\n\t\t\t/*\n\t\t\t * Don't treat a reference through a sequentially read\n\t\t\t * mapping as such.  If the page has been used in\n\t\t\t * another mapping, we will catch it; if this other\n\t\t\t * mapping is already gone, the unmap path will have\n\t\t\t * set PG_referenced or activated the page.\n\t\t\t */\n\t\t\tif (likely(!(vma->vm_flags & VM_SEQ_READ)))\n\t\t\t\treferenced++;\n\t\t}\n\t\tpte_unmap_unlock(pte, ptl);\n\t}\n\n\tif (referenced) {\n\t\tpra->referenced++;\n\t\tpra->vm_flags |= vma->vm_flags;\n\t}\n\n\tpra->mapcount--;\n\tif (!pra->mapcount)\n\t\treturn SWAP_SUCCESS; /* To break the loop */\n\n\treturn SWAP_AGAIN;\n}\n\nstatic bool invalid_page_referenced_vma(struct vm_area_struct *vma, void *arg)\n{\n\tstruct page_referenced_arg *pra = arg;\n\tstruct mem_cgroup *memcg = pra->memcg;\n\n\tif (!mm_match_cgroup(vma->vm_mm, memcg))\n\t\treturn true;\n\n\treturn false;\n}\n\n/**\n * page_referenced - test if the page was referenced\n * @page: the page to test\n * @is_locked: caller holds lock on the page\n * @memcg: target memory cgroup\n * @vm_flags: collect encountered vma->vm_flags who actually referenced the page\n *\n * Quick test_and_clear_referenced for all mappings to a page,\n * returns the number of ptes which referenced the page.\n */\nint page_referenced(struct page *page,\n\t\t    int is_locked,\n\t\t    struct mem_cgroup *memcg,\n\t\t    unsigned long *vm_flags)\n{\n\tint ret;\n\tint we_locked = 0;\n\tstruct page_referenced_arg pra = {\n\t\t.mapcount = page_mapcount(page),\n\t\t.memcg = memcg,\n\t};\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = page_referenced_one,\n\t\t.arg = (void *)&pra,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\t};\n\n\t*vm_flags = 0;\n\tif (!page_mapped(page))\n\t\treturn 0;\n\n\tif (!page_rmapping(page))\n\t\treturn 0;\n\n\tif (!is_locked && (!PageAnon(page) || PageKsm(page))) {\n\t\twe_locked = trylock_page(page);\n\t\tif (!we_locked)\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If we are reclaiming on behalf of a cgroup, skip\n\t * counting on behalf of references from different\n\t * cgroups\n\t */\n\tif (memcg) {\n\t\trwc.invalid_vma = invalid_page_referenced_vma;\n\t}\n\n\tret = rmap_walk(page, &rwc);\n\t*vm_flags = pra.vm_flags;\n\n\tif (we_locked)\n\t\tunlock_page(page);\n\n\treturn pra.referenced;\n}\n\nstatic int page_mkclean_one(struct page *page, struct vm_area_struct *vma,\n\t\t\t    unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tint ret = 0;\n\tint *cleaned = arg;\n\n\tpte = page_check_address(page, mm, address, &ptl, 1);\n\tif (!pte)\n\t\tgoto out;\n\n\tif (pte_dirty(*pte) || pte_write(*pte)) {\n\t\tpte_t entry;\n\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tentry = ptep_clear_flush(vma, address, pte);\n\t\tentry = pte_wrprotect(entry);\n\t\tentry = pte_mkclean(entry);\n\t\tset_pte_at(mm, address, pte, entry);\n\t\tret = 1;\n\t}\n\n\tpte_unmap_unlock(pte, ptl);\n\n\tif (ret) {\n\t\tmmu_notifier_invalidate_page(mm, address);\n\t\t(*cleaned)++;\n\t}\nout:\n\treturn SWAP_AGAIN;\n}\n\nstatic bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)\n{\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn false;\n\n\treturn true;\n}\n\nint page_mkclean(struct page *page)\n{\n\tint cleaned = 0;\n\tstruct address_space *mapping;\n\tstruct rmap_walk_control rwc = {\n\t\t.arg = (void *)&cleaned,\n\t\t.rmap_one = page_mkclean_one,\n\t\t.invalid_vma = invalid_mkclean_vma,\n\t};\n\n\tBUG_ON(!PageLocked(page));\n\n\tif (!page_mapped(page))\n\t\treturn 0;\n\n\tmapping = page_mapping(page);\n\tif (!mapping)\n\t\treturn 0;\n\n\trmap_walk(page, &rwc);\n\n\treturn cleaned;\n}\nEXPORT_SYMBOL_GPL(page_mkclean);\n\n/**\n * page_move_anon_rmap - move a page to our anon_vma\n * @page:\tthe page to move to our anon_vma\n * @vma:\tthe vma the page belongs to\n * @address:\tthe user virtual address mapped\n *\n * When a page belongs exclusively to one process after a COW event,\n * that page can be moved into the anon_vma that belongs to just that\n * process, so the rmap code will not search the parent or sibling\n * processes.\n */\nvoid page_move_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\tVM_BUG_ON(!anon_vma);\n\tVM_BUG_ON_PAGE(page->index != linear_page_index(vma, address), page);\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n}\n\n/**\n * __page_set_anon_rmap - set up new anonymous rmap\n * @page:\tPage to add to rmap\t\n * @vma:\tVM area to add page to.\n * @address:\tUser virtual address of the mapping\t\n * @exclusive:\tthe page is exclusively owned by the current process\n */\nstatic void __page_set_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tBUG_ON(!anon_vma);\n\n\tif (PageAnon(page))\n\t\treturn;\n\n\t/*\n\t * If the page isn't exclusively mapped into this vma,\n\t * we must use the _oldest_ possible anon_vma for the\n\t * page mapping!\n\t */\n\tif (!exclusive)\n\t\tanon_vma = anon_vma->root;\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n\tpage->index = linear_page_index(vma, address);\n}\n\n/**\n * __page_check_anon_rmap - sanity check anonymous rmap addition\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n */\nstatic void __page_check_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n#ifdef CONFIG_DEBUG_VM\n\t/*\n\t * The page's anon-rmap details (mapping and index) are guaranteed to\n\t * be set up correctly at this point.\n\t *\n\t * We have exclusion against page_add_anon_rmap because the caller\n\t * always holds the page locked, except if called from page_dup_rmap,\n\t * in which case the page is already known to be setup.\n\t *\n\t * We have exclusion against page_add_new_anon_rmap because those pages\n\t * are initially only visible via the pagetables, and the pte is locked\n\t * over the call to page_add_new_anon_rmap.\n\t */\n\tBUG_ON(page_anon_vma(page)->root != vma->anon_vma->root);\n\tBUG_ON(page->index != linear_page_index(vma, address));\n#endif\n}\n\n/**\n * page_add_anon_rmap - add pte mapping to an anonymous page\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n *\n * The caller needs to hold the pte lock, and the page must be locked in\n * the anon_vma case: to serialize mapping,index checking after setting,\n * and to ensure that PageAnon is not being upgraded racily to PageKsm\n * (but PageKsm is never downgraded to PageAnon).\n */\nvoid page_add_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tdo_page_add_anon_rmap(page, vma, address, 0);\n}\n\n/*\n * Special version of the above for do_swap_page, which often runs\n * into pages that are exclusively owned by the current process.\n * Everybody else should continue to use page_add_anon_rmap above.\n */\nvoid do_page_add_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tint first = atomic_inc_and_test(&page->_mapcount);\n\tif (first) {\n\t\tif (PageTransHuge(page))\n\t\t\t__inc_zone_page_state(page,\n\t\t\t\t\t      NR_ANON_TRANSPARENT_HUGEPAGES);\n\t\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\t\thpage_nr_pages(page));\n\t}\n\tif (unlikely(PageKsm(page)))\n\t\treturn;\n\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\t/* address might be in next vma when migration races vma_adjust */\n\tif (first)\n\t\t__page_set_anon_rmap(page, vma, address, exclusive);\n\telse\n\t\t__page_check_anon_rmap(page, vma, address);\n}\n\n/**\n * page_add_new_anon_rmap - add pte mapping to a new anonymous page\n * @page:\tthe page to add the mapping to\n * @vma:\tthe vm area in which the mapping is added\n * @address:\tthe user virtual address mapped\n *\n * Same as page_add_anon_rmap but must only be called on *new* pages.\n * This means the inc-and-test can be bypassed.\n * Page does not have to be locked.\n */\nvoid page_add_new_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tVM_BUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\tSetPageSwapBacked(page);\n\tatomic_set(&page->_mapcount, 0); /* increment count (starts at -1) */\n\tif (PageTransHuge(page))\n\t\t__inc_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);\n\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\thpage_nr_pages(page));\n\t__page_set_anon_rmap(page, vma, address, 1);\n\tif (!mlocked_vma_newpage(vma, page)) {\n\t\tSetPageActive(page);\n\t\tlru_cache_add(page);\n\t} else\n\t\tadd_page_to_unevictable_list(page);\n}\n\n/**\n * page_add_file_rmap - add pte mapping to a file page\n * @page: the page to add the mapping to\n *\n * The caller needs to hold the pte lock.\n */\nvoid page_add_file_rmap(struct page *page)\n{\n\tbool locked;\n\tunsigned long flags;\n\n\tmem_cgroup_begin_update_page_stat(page, &locked, &flags);\n\tif (atomic_inc_and_test(&page->_mapcount)) {\n\t\t__inc_zone_page_state(page, NR_FILE_MAPPED);\n\t\tmem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\n\t}\n\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n}\n\n/**\n * page_remove_rmap - take down pte mapping from a page\n * @page: page to remove mapping from\n *\n * The caller needs to hold the pte lock.\n */\nvoid page_remove_rmap(struct page *page)\n{\n\tbool anon = PageAnon(page);\n\tbool locked;\n\tunsigned long flags;\n\n\t/*\n\t * The anon case has no mem_cgroup page_stat to update; but may\n\t * uncharge_page() below, where the lock ordering can deadlock if\n\t * we hold the lock against page_stat move: so avoid it on anon.\n\t */\n\tif (!anon)\n\t\tmem_cgroup_begin_update_page_stat(page, &locked, &flags);\n\n\t/* page still mapped by someone else? */\n\tif (!atomic_add_negative(-1, &page->_mapcount))\n\t\tgoto out;\n\n\t/*\n\t * Hugepages are not counted in NR_ANON_PAGES nor NR_FILE_MAPPED\n\t * and not charged by memcg for now.\n\t */\n\tif (unlikely(PageHuge(page)))\n\t\tgoto out;\n\tif (anon) {\n\t\tmem_cgroup_uncharge_page(page);\n\t\tif (PageTransHuge(page))\n\t\t\t__dec_zone_page_state(page,\n\t\t\t\t\t      NR_ANON_TRANSPARENT_HUGEPAGES);\n\t\t__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,\n\t\t\t\t-hpage_nr_pages(page));\n\t} else {\n\t\t__dec_zone_page_state(page, NR_FILE_MAPPED);\n\t\tmem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);\n\t\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n\t}\n\tif (unlikely(PageMlocked(page)))\n\t\tclear_page_mlock(page);\n\t/*\n\t * It would be tidy to reset the PageAnon mapping here,\n\t * but that might overwrite a racing page_add_anon_rmap\n\t * which increments mapcount after us but sets mapping\n\t * before us: so leave the reset to free_hot_cold_page,\n\t * and remember that it's only reliable while mapped.\n\t * Leaving it set also helps swapoff to reinstate ptes\n\t * faster for those pages still in swapcache.\n\t */\n\treturn;\nout:\n\tif (!anon)\n\t\tmem_cgroup_end_update_page_stat(page, &locked, &flags);\n}\n\n/*\n * @arg: enum ttu_flags will be passed to this argument\n */\nint try_to_unmap_one(struct page *page, struct vm_area_struct *vma,\n\t\t     unsigned long address, void *arg)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tint ret = SWAP_AGAIN;\n\tenum ttu_flags flags = (enum ttu_flags)arg;\n\n\tpte = page_check_address(page, mm, address, &ptl, 0);\n\tif (!pte)\n\t\tgoto out;\n\n\t/*\n\t * If the page is mlock()d, we cannot swap it out.\n\t * If it's recently referenced (perhaps page_referenced\n\t * skipped over this mm) then we should reactivate it.\n\t */\n\tif (!(flags & TTU_IGNORE_MLOCK)) {\n\t\tif (vma->vm_flags & VM_LOCKED)\n\t\t\tgoto out_mlock;\n\n\t\tif (TTU_ACTION(flags) == TTU_MUNLOCK)\n\t\t\tgoto out_unmap;\n\t}\n\tif (!(flags & TTU_IGNORE_ACCESS)) {\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte)) {\n\t\t\tret = SWAP_FAIL;\n\t\t\tgoto out_unmap;\n\t\t}\n  \t}\n\n\t/* Nuke the page table entry. */\n\tflush_cache_page(vma, address, page_to_pfn(page));\n\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t/* Move the dirty bit to the physical page now the pte is gone. */\n\tif (pte_dirty(pteval))\n\t\tset_page_dirty(page);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tif (PageHWPoison(page) && !(flags & TTU_IGNORE_HWPOISON)) {\n\t\tif (!PageHuge(page)) {\n\t\t\tif (PageAnon(page))\n\t\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\telse\n\t\t\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t}\n\t\tset_pte_at(mm, address, pte,\n\t\t\t   swp_entry_to_pte(make_hwpoison_entry(page)));\n\t} else if (pte_unused(pteval)) {\n\t\t/*\n\t\t * The guest indicated that the page content is of no\n\t\t * interest anymore. Simply discard the pte, vmscan\n\t\t * will take care of the rest.\n\t\t */\n\t\tif (PageAnon(page))\n\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\telse\n\t\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t} else if (PageAnon(page)) {\n\t\tswp_entry_t entry = { .val = page_private(page) };\n\t\tpte_t swp_pte;\n\n\t\tif (PageSwapCache(page)) {\n\t\t\t/*\n\t\t\t * Store the swap location in the pte.\n\t\t\t * See handle_pte_fault() ...\n\t\t\t */\n\t\t\tif (swap_duplicate(entry) < 0) {\n\t\t\t\tset_pte_at(mm, address, pte, pteval);\n\t\t\t\tret = SWAP_FAIL;\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\t\t\tif (list_empty(&mm->mmlist)) {\n\t\t\t\tspin_lock(&mmlist_lock);\n\t\t\t\tif (list_empty(&mm->mmlist))\n\t\t\t\t\tlist_add(&mm->mmlist, &init_mm.mmlist);\n\t\t\t\tspin_unlock(&mmlist_lock);\n\t\t\t}\n\t\t\tdec_mm_counter(mm, MM_ANONPAGES);\n\t\t\tinc_mm_counter(mm, MM_SWAPENTS);\n\t\t} else if (IS_ENABLED(CONFIG_MIGRATION)) {\n\t\t\t/*\n\t\t\t * Store the pfn of the page in a special migration\n\t\t\t * pte. do_swap_page() will wait until the migration\n\t\t\t * pte is removed and then restart fault handling.\n\t\t\t */\n\t\t\tBUG_ON(TTU_ACTION(flags) != TTU_MIGRATION);\n\t\t\tentry = make_migration_entry(page, pte_write(pteval));\n\t\t}\n\t\tswp_pte = swp_entry_to_pte(entry);\n\t\tif (pte_soft_dirty(pteval))\n\t\t\tswp_pte = pte_swp_mksoft_dirty(swp_pte);\n\t\tset_pte_at(mm, address, pte, swp_pte);\n\t\tBUG_ON(pte_file(*pte));\n\t} else if (IS_ENABLED(CONFIG_MIGRATION) &&\n\t\t   (TTU_ACTION(flags) == TTU_MIGRATION)) {\n\t\t/* Establish migration entry for a file page */\n\t\tswp_entry_t entry;\n\t\tentry = make_migration_entry(page, pte_write(pteval));\n\t\tset_pte_at(mm, address, pte, swp_entry_to_pte(entry));\n\t} else\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\n\tpage_remove_rmap(page);\n\tpage_cache_release(page);\n\nout_unmap:\n\tpte_unmap_unlock(pte, ptl);\n\tif (ret != SWAP_FAIL)\n\t\tmmu_notifier_invalidate_page(mm, address);\nout:\n\treturn ret;\n\nout_mlock:\n\tpte_unmap_unlock(pte, ptl);\n\n\n\t/*\n\t * We need mmap_sem locking, Otherwise VM_LOCKED check makes\n\t * unstable result and race. Plus, We can't wait here because\n\t * we now hold anon_vma->rwsem or mapping->i_mmap_mutex.\n\t * if trylock failed, the page remain in evictable lru and later\n\t * vmscan could retry to move the page to unevictable lru if the\n\t * page is actually mlocked.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tmlock_vma_page(page);\n\t\t\tret = SWAP_MLOCK;\n\t\t}\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\t}\n\treturn ret;\n}\n\n/*\n * objrmap doesn't work for nonlinear VMAs because the assumption that\n * offset-into-file correlates with offset-into-virtual-addresses does not hold.\n * Consequently, given a particular page and its ->index, we cannot locate the\n * ptes which are mapping that page without an exhaustive linear search.\n *\n * So what this code does is a mini \"virtual scan\" of each nonlinear VMA which\n * maps the file to which the target page belongs.  The ->vm_private_data field\n * holds the current cursor into that scan.  Successive searches will circulate\n * around the vma's virtual address space.\n *\n * So as more replacement pressure is applied to the pages in a nonlinear VMA,\n * more scanning pressure is placed against them as well.   Eventually pages\n * will become fully unmapped and are eligible for eviction.\n *\n * For very sparsely populated VMAs this is a little inefficient - chances are\n * there there won't be many ptes located within the scan cluster.  In this case\n * maybe we could scan further - to the end of the pte page, perhaps.\n *\n * Mlocked pages:  check VM_LOCKED under mmap_sem held for read, if we can\n * acquire it without blocking.  If vma locked, mlock the pages in the cluster,\n * rather than unmapping them.  If we encounter the \"check_page\" that vmscan is\n * trying to unmap, return SWAP_MLOCK, else default SWAP_AGAIN.\n */\n#define CLUSTER_SIZE\tmin(32*PAGE_SIZE, PMD_SIZE)\n#define CLUSTER_MASK\t(~(CLUSTER_SIZE - 1))\n\nstatic int try_to_unmap_cluster(unsigned long cursor, unsigned int *mapcount,\n\t\tstruct vm_area_struct *vma, struct page *check_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tpte_t pteval;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\tunsigned long address;\n\tunsigned long mmun_start;\t/* For mmu_notifiers */\n\tunsigned long mmun_end;\t\t/* For mmu_notifiers */\n\tunsigned long end;\n\tint ret = SWAP_AGAIN;\n\tint locked_vma = 0;\n\n\taddress = (vma->vm_start + cursor) & CLUSTER_MASK;\n\tend = address + CLUSTER_SIZE;\n\tif (address < vma->vm_start)\n\t\taddress = vma->vm_start;\n\tif (end > vma->vm_end)\n\t\tend = vma->vm_end;\n\n\tpmd = mm_find_pmd(mm, address);\n\tif (!pmd)\n\t\treturn ret;\n\n\tmmun_start = address;\n\tmmun_end   = end;\n\tmmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);\n\n\t/*\n\t * If we can acquire the mmap_sem for read, and vma is VM_LOCKED,\n\t * keep the sem while scanning the cluster for mlocking pages.\n\t */\n\tif (down_read_trylock(&vma->vm_mm->mmap_sem)) {\n\t\tlocked_vma = (vma->vm_flags & VM_LOCKED);\n\t\tif (!locked_vma)\n\t\t\tup_read(&vma->vm_mm->mmap_sem); /* don't need it */\n\t}\n\n\tpte = pte_offset_map_lock(mm, pmd, address, &ptl);\n\n\t/* Update high watermark before we lower rss */\n\tupdate_hiwater_rss(mm);\n\n\tfor (; address < end; pte++, address += PAGE_SIZE) {\n\t\tif (!pte_present(*pte))\n\t\t\tcontinue;\n\t\tpage = vm_normal_page(vma, address, *pte);\n\t\tBUG_ON(!page || PageAnon(page));\n\n\t\tif (locked_vma) {\n\t\t\tif (page == check_page) {\n\t\t\t\t/* we know we have check_page locked */\n\t\t\t\tmlock_vma_page(page);\n\t\t\t\tret = SWAP_MLOCK;\n\t\t\t} else if (trylock_page(page)) {\n\t\t\t\t/*\n\t\t\t\t * If we can lock the page, perform mlock.\n\t\t\t\t * Otherwise leave the page alone, it will be\n\t\t\t\t * eventually encountered again later.\n\t\t\t\t */\n\t\t\t\tmlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t}\n\t\t\tcontinue;\t/* don't unmap */\n\t\t}\n\n\t\tif (ptep_clear_flush_young_notify(vma, address, pte))\n\t\t\tcontinue;\n\n\t\t/* Nuke the page table entry. */\n\t\tflush_cache_page(vma, address, pte_pfn(*pte));\n\t\tpteval = ptep_clear_flush(vma, address, pte);\n\n\t\t/* If nonlinear, store the file page offset in the pte. */\n\t\tif (page->index != linear_page_index(vma, address)) {\n\t\t\tpte_t ptfile = pgoff_to_pte(page->index);\n\t\t\tif (pte_soft_dirty(pteval))\n\t\t\t\tpte_file_mksoft_dirty(ptfile);\n\t\t\tset_pte_at(mm, address, pte, ptfile);\n\t\t}\n\n\t\t/* Move the dirty bit to the physical page now the pte is gone. */\n\t\tif (pte_dirty(pteval))\n\t\t\tset_page_dirty(page);\n\n\t\tpage_remove_rmap(page);\n\t\tpage_cache_release(page);\n\t\tdec_mm_counter(mm, MM_FILEPAGES);\n\t\t(*mapcount)--;\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tmmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);\n\tif (locked_vma)\n\t\tup_read(&vma->vm_mm->mmap_sem);\n\treturn ret;\n}\n\nstatic int try_to_unmap_nonlinear(struct page *page,\n\t\tstruct address_space *mapping, void *arg)\n{\n\tstruct vm_area_struct *vma;\n\tint ret = SWAP_AGAIN;\n\tunsigned long cursor;\n\tunsigned long max_nl_cursor = 0;\n\tunsigned long max_nl_size = 0;\n\tunsigned int mapcount;\n\n\tlist_for_each_entry(vma,\n\t\t&mapping->i_mmap_nonlinear, shared.nonlinear) {\n\n\t\tcursor = (unsigned long) vma->vm_private_data;\n\t\tif (cursor > max_nl_cursor)\n\t\t\tmax_nl_cursor = cursor;\n\t\tcursor = vma->vm_end - vma->vm_start;\n\t\tif (cursor > max_nl_size)\n\t\t\tmax_nl_size = cursor;\n\t}\n\n\tif (max_nl_size == 0) {\t/* all nonlinears locked or reserved ? */\n\t\treturn SWAP_FAIL;\n\t}\n\n\t/*\n\t * We don't try to search for this page in the nonlinear vmas,\n\t * and page_referenced wouldn't have found it anyway.  Instead\n\t * just walk the nonlinear vmas trying to age and unmap some.\n\t * The mapcount of the page we came in with is irrelevant,\n\t * but even so use it as a guide to how hard we should try?\n\t */\n\tmapcount = page_mapcount(page);\n\tif (!mapcount)\n\t\treturn ret;\n\n\tcond_resched();\n\n\tmax_nl_size = (max_nl_size + CLUSTER_SIZE - 1) & CLUSTER_MASK;\n\tif (max_nl_cursor == 0)\n\t\tmax_nl_cursor = CLUSTER_SIZE;\n\n\tdo {\n\t\tlist_for_each_entry(vma,\n\t\t\t&mapping->i_mmap_nonlinear, shared.nonlinear) {\n\n\t\t\tcursor = (unsigned long) vma->vm_private_data;\n\t\t\twhile (cursor < max_nl_cursor &&\n\t\t\t\tcursor < vma->vm_end - vma->vm_start) {\n\t\t\t\tif (try_to_unmap_cluster(cursor, &mapcount,\n\t\t\t\t\t\tvma, page) == SWAP_MLOCK)\n\t\t\t\t\tret = SWAP_MLOCK;\n\t\t\t\tcursor += CLUSTER_SIZE;\n\t\t\t\tvma->vm_private_data = (void *) cursor;\n\t\t\t\tif ((int)mapcount <= 0)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tvma->vm_private_data = (void *) max_nl_cursor;\n\t\t}\n\t\tcond_resched();\n\t\tmax_nl_cursor += CLUSTER_SIZE;\n\t} while (max_nl_cursor <= max_nl_size);\n\n\t/*\n\t * Don't loop forever (perhaps all the remaining pages are\n\t * in locked vmas).  Reset cursor on all unreserved nonlinear\n\t * vmas, now forgetting on which ones it had fallen behind.\n\t */\n\tlist_for_each_entry(vma, &mapping->i_mmap_nonlinear, shared.nonlinear)\n\t\tvma->vm_private_data = NULL;\n\n\treturn ret;\n}\n\nbool is_vma_temporary_stack(struct vm_area_struct *vma)\n{\n\tint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\n\n\tif (!maybe_stack)\n\t\treturn false;\n\n\tif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\n\t\t\t\t\t\tVM_STACK_INCOMPLETE_SETUP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)\n{\n\treturn is_vma_temporary_stack(vma);\n}\n\nstatic int page_not_mapped(struct page *page)\n{\n\treturn !page_mapped(page);\n};\n\n/**\n * try_to_unmap - try to remove all page table mappings to a page\n * @page: the page to get unmapped\n * @flags: action and flags\n *\n * Tries to remove all the page table entries which are mapping this\n * page, used in the pageout path.  Caller must hold the page lock.\n * Return values are:\n *\n * SWAP_SUCCESS\t- we succeeded in removing all mappings\n * SWAP_AGAIN\t- we missed a mapping, try again later\n * SWAP_FAIL\t- the page is unswappable\n * SWAP_MLOCK\t- page is mlocked.\n */\nint try_to_unmap(struct page *page, enum ttu_flags flags)\n{\n\tint ret;\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_unmap_one,\n\t\t.arg = (void *)flags,\n\t\t.done = page_not_mapped,\n\t\t.file_nonlinear = try_to_unmap_nonlinear,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\t};\n\n\tVM_BUG_ON_PAGE(!PageHuge(page) && PageTransHuge(page), page);\n\n\t/*\n\t * During exec, a temporary VMA is setup and later moved.\n\t * The VMA is moved under the anon_vma lock but not the\n\t * page tables leading to a race where migration cannot\n\t * find the migration ptes. Rather than increasing the\n\t * locking requirements of exec(), migration skips\n\t * temporary VMAs until after exec() completes.\n\t */\n\tif (flags & TTU_MIGRATION && !PageKsm(page) && PageAnon(page))\n\t\trwc.invalid_vma = invalid_migration_vma;\n\n\tret = rmap_walk(page, &rwc);\n\n\tif (ret != SWAP_MLOCK && !page_mapped(page))\n\t\tret = SWAP_SUCCESS;\n\treturn ret;\n}\n\n/**\n * try_to_munlock - try to munlock a page\n * @page: the page to be munlocked\n *\n * Called from munlock code.  Checks all of the VMAs mapping the page\n * to make sure nobody else has this page mlocked. The page will be\n * returned with PG_mlocked cleared if no other vmas have it mlocked.\n *\n * Return values are:\n *\n * SWAP_AGAIN\t- no vma is holding page mlocked, or,\n * SWAP_AGAIN\t- page mapped in mlocked vma -- couldn't acquire mmap sem\n * SWAP_FAIL\t- page cannot be located at present\n * SWAP_MLOCK\t- page is now mlocked.\n */\nint try_to_munlock(struct page *page)\n{\n\tint ret;\n\tstruct rmap_walk_control rwc = {\n\t\t.rmap_one = try_to_unmap_one,\n\t\t.arg = (void *)TTU_MUNLOCK,\n\t\t.done = page_not_mapped,\n\t\t/*\n\t\t * We don't bother to try to find the munlocked page in\n\t\t * nonlinears. It's costly. Instead, later, page reclaim logic\n\t\t * may call try_to_unmap() and recover PG_mlocked lazily.\n\t\t */\n\t\t.file_nonlinear = NULL,\n\t\t.anon_lock = page_lock_anon_vma_read,\n\n\t};\n\n\tVM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);\n\n\tret = rmap_walk(page, &rwc);\n\treturn ret;\n}\n\nvoid __put_anon_vma(struct anon_vma *anon_vma)\n{\n\tstruct anon_vma *root = anon_vma->root;\n\n\tif (root != anon_vma && atomic_dec_and_test(&root->refcount))\n\t\tanon_vma_free(root);\n\n\tanon_vma_free(anon_vma);\n}\n\nstatic struct anon_vma *rmap_walk_anon_lock(struct page *page,\n\t\t\t\t\tstruct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma;\n\n\tif (rwc->anon_lock)\n\t\treturn rwc->anon_lock(page);\n\n\t/*\n\t * Note: remove_migration_ptes() cannot use page_lock_anon_vma_read()\n\t * because that depends on page_mapped(); but not all its usages\n\t * are holding mmap_sem. Users without mmap_sem are required to\n\t * take a reference count to prevent the anon_vma disappearing\n\t */\n\tanon_vma = page_anon_vma(page);\n\tif (!anon_vma)\n\t\treturn NULL;\n\n\tanon_vma_lock_read(anon_vma);\n\treturn anon_vma;\n}\n\n/*\n * rmap_walk_anon - do something to anonymous page using the object-based\n * rmap method\n * @page: the page to be handled\n * @rwc: control variable according to each walk type\n *\n * Find all the mappings of a page using the mapping pointer and the vma chains\n * contained in the anon_vma struct it points to.\n *\n * When called from try_to_munlock(), the mmap_sem of the mm containing the vma\n * where the page was found will be held for write.  So, we won't recheck\n * vm_flags for that VMA.  That should be OK, because that vma shouldn't be\n * LOCKED.\n */\nstatic int rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc)\n{\n\tstruct anon_vma *anon_vma;\n\tpgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);\n\tstruct anon_vma_chain *avc;\n\tint ret = SWAP_AGAIN;\n\n\tanon_vma = rmap_walk_anon_lock(page, rwc);\n\tif (!anon_vma)\n\t\treturn ret;\n\n\tanon_vma_interval_tree_foreach(avc, &anon_vma->rb_root, pgoff, pgoff) {\n\t\tstruct vm_area_struct *vma = avc->vma;\n\t\tunsigned long address = vma_address(page, vma);\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tret = rwc->rmap_one(page, vma, address, rwc->arg);\n\t\tif (ret != SWAP_AGAIN)\n\t\t\tbreak;\n\t\tif (rwc->done && rwc->done(page))\n\t\t\tbreak;\n\t}\n\tanon_vma_unlock_read(anon_vma);\n\treturn ret;\n}\n\n/*\n * rmap_walk_file - do something to file page using the object-based rmap method\n * @page: the page to be handled\n * @rwc: control variable according to each walk type\n *\n * Find all the mappings of a page using the mapping pointer and the vma chains\n * contained in the address_space struct it points to.\n *\n * When called from try_to_munlock(), the mmap_sem of the mm containing the vma\n * where the page was found will be held for write.  So, we won't recheck\n * vm_flags for that VMA.  That should be OK, because that vma shouldn't be\n * LOCKED.\n */\nstatic int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)\n{\n\tstruct address_space *mapping = page->mapping;\n\tpgoff_t pgoff = page->index << compound_order(page);\n\tstruct vm_area_struct *vma;\n\tint ret = SWAP_AGAIN;\n\n\t/*\n\t * The page lock not only makes sure that page->mapping cannot\n\t * suddenly be NULLified by truncation, it makes sure that the\n\t * structure at mapping cannot be freed and reused yet,\n\t * so we can safely take mapping->i_mmap_mutex.\n\t */\n\tVM_BUG_ON(!PageLocked(page));\n\n\tif (!mapping)\n\t\treturn ret;\n\tmutex_lock(&mapping->i_mmap_mutex);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {\n\t\tunsigned long address = vma_address(page, vma);\n\n\t\tif (rwc->invalid_vma && rwc->invalid_vma(vma, rwc->arg))\n\t\t\tcontinue;\n\n\t\tret = rwc->rmap_one(page, vma, address, rwc->arg);\n\t\tif (ret != SWAP_AGAIN)\n\t\t\tgoto done;\n\t\tif (rwc->done && rwc->done(page))\n\t\t\tgoto done;\n\t}\n\n\tif (!rwc->file_nonlinear)\n\t\tgoto done;\n\n\tif (list_empty(&mapping->i_mmap_nonlinear))\n\t\tgoto done;\n\n\tret = rwc->file_nonlinear(page, mapping, rwc->arg);\n\ndone:\n\tmutex_unlock(&mapping->i_mmap_mutex);\n\treturn ret;\n}\n\nint rmap_walk(struct page *page, struct rmap_walk_control *rwc)\n{\n\tif (unlikely(PageKsm(page)))\n\t\treturn rmap_walk_ksm(page, rwc);\n\telse if (PageAnon(page))\n\t\treturn rmap_walk_anon(page, rwc);\n\telse\n\t\treturn rmap_walk_file(page, rwc);\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n/*\n * The following three functions are for anonymous (private mapped) hugepages.\n * Unlike common anonymous pages, anonymous hugepages have no accounting code\n * and no lru code, because we handle hugepages differently from common pages.\n */\nstatic void __hugepage_set_anon_rmap(struct page *page,\n\tstruct vm_area_struct *vma, unsigned long address, int exclusive)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\n\tBUG_ON(!anon_vma);\n\n\tif (PageAnon(page))\n\t\treturn;\n\tif (!exclusive)\n\t\tanon_vma = anon_vma->root;\n\n\tanon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;\n\tpage->mapping = (struct address_space *) anon_vma;\n\tpage->index = linear_page_index(vma, address);\n}\n\nvoid hugepage_add_anon_rmap(struct page *page,\n\t\t\t    struct vm_area_struct *vma, unsigned long address)\n{\n\tstruct anon_vma *anon_vma = vma->anon_vma;\n\tint first;\n\n\tBUG_ON(!PageLocked(page));\n\tBUG_ON(!anon_vma);\n\t/* address might be in next vma when migration races vma_adjust */\n\tfirst = atomic_inc_and_test(&page->_mapcount);\n\tif (first)\n\t\t__hugepage_set_anon_rmap(page, vma, address, 0);\n}\n\nvoid hugepage_add_new_anon_rmap(struct page *page,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tBUG_ON(address < vma->vm_start || address >= vma->vm_end);\n\tatomic_set(&page->_mapcount, 0);\n\t__hugepage_set_anon_rmap(page, vma, address, 1);\n}\n#endif /* CONFIG_HUGETLB_PAGE */\n"], "filenames": ["mm/mlock.c", "mm/rmap.c"], "buggy_code_start_loc": [81, 1335], "buggy_code_end_loc": [176, 1337], "fixing_code_start_loc": [82, 1335], "fixing_code_end_loc": [179, 1348], "type": "CWE-400", "message": "The try_to_unmap_cluster function in mm/rmap.c in the Linux kernel before 3.14.3 does not properly consider which pages must be locked, which allows local users to cause a denial of service (system crash) by triggering a memory-usage pattern that requires removal of page-table mappings.", "other": {"cve": {"id": "CVE-2014-3122", "sourceIdentifier": "secalert@redhat.com", "published": "2014-05-11T21:55:06.060", "lastModified": "2023-02-13T00:38:55.107", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The try_to_unmap_cluster function in mm/rmap.c in the Linux kernel before 3.14.3 does not properly consider which pages must be locked, which allows local users to cause a denial of service (system crash) by triggering a memory-usage pattern that requires removal of page-table mappings."}, {"lang": "es", "value": "La funci\u00f3n try_to_unmap_cluster en mm/rmap.c en el kernel de Linux anterior a 3.14.3 no considera debidamente que p\u00e1ginas deben cerrarse, lo que permite a usuarios locales causar una denegaci\u00f3n de servicio (ca\u00edda de sistema) mediante la provocaci\u00f3n de una pauta de uso de memoria que requiere la eliminaci\u00f3n de asignaciones de tablas de p\u00e1ginas."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.14.3", "matchCriteriaId": "32164898-C5E7-420F-9BC0-24D4AF176CF8"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "16F59A04-14CF-49E2-9973-645477EA09DA"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=57e68e9cd65b4b8eb4045a1e0d0746458502554c", "source": "secalert@redhat.com"}, {"url": "http://www.debian.org/security/2014/dsa-2926", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.14.3", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2014/05/01/7", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/67162", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.ubuntu.com/usn/USN-2240-1", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1093076", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/57e68e9cd65b4b8eb4045a1e0d0746458502554c", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/57e68e9cd65b4b8eb4045a1e0d0746458502554c"}}