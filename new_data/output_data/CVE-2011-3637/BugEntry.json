{"buggy_code": ["#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/highmem.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <asm/elf.h>\n#include <asm/uaccess.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\nvoid task_mem(struct seq_file *m, struct mm_struct *mm)\n{\n\tunsigned long data, text, lib, swap;\n\tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n\n\t/*\n\t * Note: to minimize their overhead, mm maintains hiwater_vm and\n\t * hiwater_rss only when about to *lower* total_vm or rss.  Any\n\t * collector of these hiwater stats must therefore get total_vm\n\t * and rss too, which will usually be the higher.  Barriers? not\n\t * worth the effort, such snapshots can always be inconsistent.\n\t */\n\thiwater_vm = total_vm = mm->total_vm;\n\tif (hiwater_vm < mm->hiwater_vm)\n\t\thiwater_vm = mm->hiwater_vm;\n\thiwater_rss = total_rss = get_mm_rss(mm);\n\tif (hiwater_rss < mm->hiwater_rss)\n\t\thiwater_rss = mm->hiwater_rss;\n\n\tdata = mm->total_vm - mm->shared_vm - mm->stack_vm;\n\ttext = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK)) >> 10;\n\tlib = (mm->exec_vm << (PAGE_SHIFT-10)) - text;\n\tswap = get_mm_counter(mm, MM_SWAPENTS);\n\tseq_printf(m,\n\t\t\"VmPeak:\\t%8lu kB\\n\"\n\t\t\"VmSize:\\t%8lu kB\\n\"\n\t\t\"VmLck:\\t%8lu kB\\n\"\n\t\t\"VmHWM:\\t%8lu kB\\n\"\n\t\t\"VmRSS:\\t%8lu kB\\n\"\n\t\t\"VmData:\\t%8lu kB\\n\"\n\t\t\"VmStk:\\t%8lu kB\\n\"\n\t\t\"VmExe:\\t%8lu kB\\n\"\n\t\t\"VmLib:\\t%8lu kB\\n\"\n\t\t\"VmPTE:\\t%8lu kB\\n\"\n\t\t\"VmSwap:\\t%8lu kB\\n\",\n\t\thiwater_vm << (PAGE_SHIFT-10),\n\t\t(total_vm - mm->reserved_vm) << (PAGE_SHIFT-10),\n\t\tmm->locked_vm << (PAGE_SHIFT-10),\n\t\thiwater_rss << (PAGE_SHIFT-10),\n\t\ttotal_rss << (PAGE_SHIFT-10),\n\t\tdata << (PAGE_SHIFT-10),\n\t\tmm->stack_vm << (PAGE_SHIFT-10), text, lib,\n\t\t(PTRS_PER_PTE*sizeof(pte_t)*mm->nr_ptes) >> 10,\n\t\tswap << (PAGE_SHIFT-10));\n}\n\nunsigned long task_vsize(struct mm_struct *mm)\n{\n\treturn PAGE_SIZE * mm->total_vm;\n}\n\nunsigned long task_statm(struct mm_struct *mm,\n\t\t\t unsigned long *shared, unsigned long *text,\n\t\t\t unsigned long *data, unsigned long *resident)\n{\n\t*shared = get_mm_counter(mm, MM_FILEPAGES);\n\t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n\t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\t*data = mm->total_vm - mm->shared_vm;\n\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n\treturn mm->total_vm;\n}\n\nstatic void pad_len_spaces(struct seq_file *m, int len)\n{\n\tlen = 25 + sizeof(void*) * 6 - len;\n\tif (len < 1)\n\t\tlen = 1;\n\tseq_printf(m, \"%*c\", len, ' ');\n}\n\nstatic void vma_stop(struct proc_maps_private *priv, struct vm_area_struct *vma)\n{\n\tif (vma && vma != priv->tail_vma) {\n\t\tstruct mm_struct *mm = vma->vm_mm;\n\t\tup_read(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}\n\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tunsigned long last_addr = m->version;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma, *tail_vma = NULL;\n\tloff_t l = *pos;\n\n\t/* Clear the per syscall fields in priv */\n\tpriv->task = NULL;\n\tpriv->tail_vma = NULL;\n\n\t/*\n\t * We remember last_addr rather than next_addr to hit with\n\t * mmap_cache most of the time. We have zero last_addr at\n\t * the beginning and also after lseek. We will have -1 last_addr\n\t * after the end of the vmas.\n\t */\n\n\tif (last_addr == -1UL)\n\t\treturn NULL;\n\n\tpriv->task = get_pid_task(priv->pid, PIDTYPE_PID);\n\tif (!priv->task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\tmm = mm_for_maps(priv->task);\n\tif (!mm || IS_ERR(mm))\n\t\treturn mm;\n\tdown_read(&mm->mmap_sem);\n\n\ttail_vma = get_gate_vma(priv->task->mm);\n\tpriv->tail_vma = tail_vma;\n\n\t/* Start with last addr hint */\n\tvma = find_vma(mm, last_addr);\n\tif (last_addr && vma) {\n\t\tvma = vma->vm_next;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check the vma index is within the range and do\n\t * sequential scan until m_index.\n\t */\n\tvma = NULL;\n\tif ((unsigned long)l < mm->map_count) {\n\t\tvma = mm->mmap;\n\t\twhile (l-- && vma)\n\t\t\tvma = vma->vm_next;\n\t\tgoto out;\n\t}\n\n\tif (l != mm->map_count)\n\t\ttail_vma = NULL; /* After gate vma */\n\nout:\n\tif (vma)\n\t\treturn vma;\n\n\t/* End of vmas has been reached */\n\tm->version = (tail_vma != NULL)? 0: -1UL;\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n\treturn tail_vma;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *vma = v;\n\tstruct vm_area_struct *tail_vma = priv->tail_vma;\n\n\t(*pos)++;\n\tif (vma && (vma != tail_vma) && vma->vm_next)\n\t\treturn vma->vm_next;\n\tvma_stop(priv, vma);\n\treturn (vma != tail_vma)? tail_vma: NULL;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *vma = v;\n\n\tvma_stop(priv, vma);\n\tif (priv->task)\n\t\tput_task_struct(priv->task);\n}\n\nstatic int do_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops)\n{\n\tstruct proc_maps_private *priv;\n\tint ret = -ENOMEM;\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (priv) {\n\t\tpriv->pid = proc_pid(inode);\n\t\tret = seq_open(file, ops);\n\t\tif (!ret) {\n\t\t\tstruct seq_file *m = file->private_data;\n\t\t\tm->private = priv;\n\t\t} else {\n\t\t\tkfree(priv);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void show_map_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tint flags = vma->vm_flags;\n\tunsigned long ino = 0;\n\tunsigned long long pgoff = 0;\n\tunsigned long start;\n\tdev_t dev = 0;\n\tint len;\n\n\tif (file) {\n\t\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tpgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\n\t}\n\n\t/* We don't show the stack guard page in /proc/maps */\n\tstart = vma->vm_start;\n\tif (vma->vm_flags & VM_GROWSDOWN)\n\t\tif (!vma_stack_continue(vma->vm_prev, vma->vm_start))\n\t\t\tstart += PAGE_SIZE;\n\n\tseq_printf(m, \"%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n\",\n\t\t\tstart,\n\t\t\tvma->vm_end,\n\t\t\tflags & VM_READ ? 'r' : '-',\n\t\t\tflags & VM_WRITE ? 'w' : '-',\n\t\t\tflags & VM_EXEC ? 'x' : '-',\n\t\t\tflags & VM_MAYSHARE ? 's' : 'p',\n\t\t\tpgoff,\n\t\t\tMAJOR(dev), MINOR(dev), ino, &len);\n\n\t/*\n\t * Print the dentry name for named mappings, and a\n\t * special [heap] marker for the heap:\n\t */\n\tif (file) {\n\t\tpad_len_spaces(m, len);\n\t\tseq_path(m, &file->f_path, \"\\n\");\n\t} else {\n\t\tconst char *name = arch_vma_name(vma);\n\t\tif (!name) {\n\t\t\tif (mm) {\n\t\t\t\tif (vma->vm_start <= mm->brk &&\n\t\t\t\t\t\tvma->vm_end >= mm->start_brk) {\n\t\t\t\t\tname = \"[heap]\";\n\t\t\t\t} else if (vma->vm_start <= mm->start_stack &&\n\t\t\t\t\t   vma->vm_end >= mm->start_stack) {\n\t\t\t\t\tname = \"[stack]\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tname = \"[vdso]\";\n\t\t\t}\n\t\t}\n\t\tif (name) {\n\t\t\tpad_len_spaces(m, len);\n\t\t\tseq_puts(m, name);\n\t\t}\n\t}\n\tseq_putc(m, '\\n');\n}\n\nstatic int show_map(struct seq_file *m, void *v)\n{\n\tstruct vm_area_struct *vma = v;\n\tstruct proc_maps_private *priv = m->private;\n\tstruct task_struct *task = priv->task;\n\n\tshow_map_vma(m, vma);\n\n\tif (m->count < m->size)  /* vma is copied successfully */\n\t\tm->version = (vma != get_gate_vma(task->mm))\n\t\t\t? vma->vm_start : 0;\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_maps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_map\n};\n\nstatic int maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_maps_op);\n}\n\nconst struct file_operations proc_maps_operations = {\n\t.open\t\t= maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n\n/*\n * Proportional Set Size(PSS): my share of RSS.\n *\n * PSS of a process is the count of pages it has in memory, where each\n * page is divided by the number of processes sharing it.  So if a\n * process has 1000 pages all to itself, and 1000 shared with one other\n * process, its PSS will be 1500.\n *\n * To keep (accumulated) division errors low, we adopt a 64bit\n * fixed-point pss counter to minimize division errors. So (pss >>\n * PSS_SHIFT) would be the real byte count.\n *\n * A shift of 12 before division means (assuming 4K page size):\n * \t- 1M 3-user-pages add up to 8KB errors;\n * \t- supports mapcount up to 2^24, or 16M;\n * \t- supports PSS up to 2^52 bytes, or 4PB.\n */\n#define PSS_SHIFT 12\n\n#ifdef CONFIG_PROC_PAGE_MONITOR\nstruct mem_size_stats {\n\tstruct vm_area_struct *vma;\n\tunsigned long resident;\n\tunsigned long shared_clean;\n\tunsigned long shared_dirty;\n\tunsigned long private_clean;\n\tunsigned long private_dirty;\n\tunsigned long referenced;\n\tunsigned long anonymous;\n\tunsigned long anonymous_thp;\n\tunsigned long swap;\n\tu64 pss;\n};\n\n\nstatic void smaps_pte_entry(pte_t ptent, unsigned long addr,\n\t\tunsigned long ptent_size, struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = mss->vma;\n\tstruct page *page;\n\tint mapcount;\n\n\tif (is_swap_pte(ptent)) {\n\t\tmss->swap += ptent_size;\n\t\treturn;\n\t}\n\n\tif (!pte_present(ptent))\n\t\treturn;\n\n\tpage = vm_normal_page(vma, addr, ptent);\n\tif (!page)\n\t\treturn;\n\n\tif (PageAnon(page))\n\t\tmss->anonymous += ptent_size;\n\n\tmss->resident += ptent_size;\n\t/* Accumulate the size in pages that have been accessed. */\n\tif (pte_young(ptent) || PageReferenced(page))\n\t\tmss->referenced += ptent_size;\n\tmapcount = page_mapcount(page);\n\tif (mapcount >= 2) {\n\t\tif (pte_dirty(ptent) || PageDirty(page))\n\t\t\tmss->shared_dirty += ptent_size;\n\t\telse\n\t\t\tmss->shared_clean += ptent_size;\n\t\tmss->pss += (ptent_size << PSS_SHIFT) / mapcount;\n\t} else {\n\t\tif (pte_dirty(ptent) || PageDirty(page))\n\t\t\tmss->private_dirty += ptent_size;\n\t\telse\n\t\t\tmss->private_clean += ptent_size;\n\t\tmss->pss += (ptent_size << PSS_SHIFT);\n\t}\n}\n\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t   struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = mss->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tspin_lock(&walk->mm->page_table_lock);\n\tif (pmd_trans_huge(*pmd)) {\n\t\tif (pmd_trans_splitting(*pmd)) {\n\t\t\tspin_unlock(&walk->mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\t} else {\n\t\t\tsmaps_pte_entry(*(pte_t *)pmd, addr,\n\t\t\t\t\tHPAGE_PMD_SIZE, walk);\n\t\t\tspin_unlock(&walk->mm->page_table_lock);\n\t\t\tmss->anonymous_thp += HPAGE_PMD_SIZE;\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tspin_unlock(&walk->mm->page_table_lock);\n\t}\n\t/*\n\t * The mmap_sem held all the way back in m_start() is what\n\t * keeps khugepaged out of here and from collapsing things\n\t * in here.\n\t */\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tsmaps_pte_entry(*pte, addr, PAGE_SIZE, walk);\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int show_smap(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct task_struct *task = priv->task;\n\tstruct vm_area_struct *vma = v;\n\tstruct mem_size_stats mss;\n\tstruct mm_walk smaps_walk = {\n\t\t.pmd_entry = smaps_pte_range,\n\t\t.mm = vma->vm_mm,\n\t\t.private = &mss,\n\t};\n\n\tmemset(&mss, 0, sizeof mss);\n\tmss.vma = vma;\n\t/* mmap_sem is held in m_start */\n\tif (vma->vm_mm && !is_vm_hugetlb_page(vma))\n\t\twalk_page_range(vma->vm_start, vma->vm_end, &smaps_walk);\n\n\tshow_map_vma(m, vma);\n\n\tseq_printf(m,\n\t\t   \"Size:           %8lu kB\\n\"\n\t\t   \"Rss:            %8lu kB\\n\"\n\t\t   \"Pss:            %8lu kB\\n\"\n\t\t   \"Shared_Clean:   %8lu kB\\n\"\n\t\t   \"Shared_Dirty:   %8lu kB\\n\"\n\t\t   \"Private_Clean:  %8lu kB\\n\"\n\t\t   \"Private_Dirty:  %8lu kB\\n\"\n\t\t   \"Referenced:     %8lu kB\\n\"\n\t\t   \"Anonymous:      %8lu kB\\n\"\n\t\t   \"AnonHugePages:  %8lu kB\\n\"\n\t\t   \"Swap:           %8lu kB\\n\"\n\t\t   \"KernelPageSize: %8lu kB\\n\"\n\t\t   \"MMUPageSize:    %8lu kB\\n\"\n\t\t   \"Locked:         %8lu kB\\n\",\n\t\t   (vma->vm_end - vma->vm_start) >> 10,\n\t\t   mss.resident >> 10,\n\t\t   (unsigned long)(mss.pss >> (10 + PSS_SHIFT)),\n\t\t   mss.shared_clean  >> 10,\n\t\t   mss.shared_dirty  >> 10,\n\t\t   mss.private_clean >> 10,\n\t\t   mss.private_dirty >> 10,\n\t\t   mss.referenced >> 10,\n\t\t   mss.anonymous >> 10,\n\t\t   mss.anonymous_thp >> 10,\n\t\t   mss.swap >> 10,\n\t\t   vma_kernel_pagesize(vma) >> 10,\n\t\t   vma_mmu_pagesize(vma) >> 10,\n\t\t   (vma->vm_flags & VM_LOCKED) ?\n\t\t\t(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);\n\n\tif (m->count < m->size)  /* vma is copied successfully */\n\t\tm->version = (vma != get_gate_vma(task->mm))\n\t\t\t? vma->vm_start : 0;\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_smaps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_smap\n};\n\nstatic int smaps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_smaps_op);\n}\n\nconst struct file_operations proc_smaps_operations = {\n\t.open\t\t= smaps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->private;\n\tpte_t *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\n\tsplit_huge_page_pmd(walk->mm, pmd);\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = *pte;\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tptep_test_and_clear_young(vma, addr, pte);\n\t\tClearPageReferenced(page);\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\n#define CLEAR_REFS_ALL 1\n#define CLEAR_REFS_ANON 2\n#define CLEAR_REFS_MAPPED 3\n\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tlong type;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\tif (strict_strtol(strstrip(buffer), 10, &type))\n\t\treturn -EINVAL;\n\tif (type < CLEAR_REFS_ALL || type > CLEAR_REFS_MAPPED)\n\t\treturn -EINVAL;\n\ttask = get_proc_task(file->f_path.dentry->d_inode);\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.mm = mm,\n\t\t};\n\t\tdown_read(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\tclear_refs_walk.private = vma;\n\t\t\tif (is_vm_hugetlb_page(vma))\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Writing 1 to /proc/pid/clear_refs affects all pages.\n\t\t\t *\n\t\t\t * Writing 2 to /proc/pid/clear_refs only affects\n\t\t\t * Anonymous pages.\n\t\t\t *\n\t\t\t * Writing 3 to /proc/pid/clear_refs only affects file\n\t\t\t * mapped pages.\n\t\t\t */\n\t\t\tif (type == CLEAR_REFS_ANON && vma->vm_file)\n\t\t\t\tcontinue;\n\t\t\tif (type == CLEAR_REFS_MAPPED && !vma->vm_file)\n\t\t\t\tcontinue;\n\t\t\twalk_page_range(vma->vm_start, vma->vm_end,\n\t\t\t\t\t&clear_refs_walk);\n\t\t}\n\t\tflush_tlb_mm(mm);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}\n\nconst struct file_operations proc_clear_refs_operations = {\n\t.write\t\t= clear_refs_write,\n\t.llseek\t\t= noop_llseek,\n};\n\nstruct pagemapread {\n\tint pos, len;\n\tu64 *buffer;\n};\n\n#define PM_ENTRY_BYTES      sizeof(u64)\n#define PM_STATUS_BITS      3\n#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)\n#define PM_STATUS_MASK      (((1LL << PM_STATUS_BITS) - 1) << PM_STATUS_OFFSET)\n#define PM_STATUS(nr)       (((nr) << PM_STATUS_OFFSET) & PM_STATUS_MASK)\n#define PM_PSHIFT_BITS      6\n#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)\n#define PM_PSHIFT_MASK      (((1LL << PM_PSHIFT_BITS) - 1) << PM_PSHIFT_OFFSET)\n#define PM_PSHIFT(x)        (((u64) (x) << PM_PSHIFT_OFFSET) & PM_PSHIFT_MASK)\n#define PM_PFRAME_MASK      ((1LL << PM_PSHIFT_OFFSET) - 1)\n#define PM_PFRAME(x)        ((x) & PM_PFRAME_MASK)\n\n#define PM_PRESENT          PM_STATUS(4LL)\n#define PM_SWAP             PM_STATUS(2LL)\n#define PM_NOT_PRESENT      PM_PSHIFT(PAGE_SHIFT)\n#define PM_END_OF_BUFFER    1\n\nstatic int add_to_pagemap(unsigned long addr, u64 pfn,\n\t\t\t  struct pagemapread *pm)\n{\n\tpm->buffer[pm->pos++] = pfn;\n\tif (pm->pos >= pm->len)\n\t\treturn PM_END_OF_BUFFER;\n\treturn 0;\n}\n\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tunsigned long addr;\n\tint err = 0;\n\tfor (addr = start; addr < end; addr += PAGE_SIZE) {\n\t\terr = add_to_pagemap(addr, PM_NOT_PRESENT, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic u64 swap_pte_to_pagemap_entry(pte_t pte)\n{\n\tswp_entry_t e = pte_to_swp_entry(pte);\n\treturn swp_type(e) | (swp_offset(e) << MAX_SWAPFILES_SHIFT);\n}\n\nstatic u64 pte_to_pagemap_entry(pte_t pte)\n{\n\tu64 pme = 0;\n\tif (is_swap_pte(pte))\n\t\tpme = PM_PFRAME(swap_pte_to_pagemap_entry(pte))\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_SWAP;\n\telse if (pte_present(pte))\n\t\tpme = PM_PFRAME(pte_pfn(pte))\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_PRESENT;\n\treturn pme;\n}\n\nstatic int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma;\n\tstruct pagemapread *pm = walk->private;\n\tpte_t *pte;\n\tint err = 0;\n\n\tsplit_huge_page_pmd(walk->mm, pmd);\n\n\t/* find the first VMA at or above 'addr' */\n\tvma = find_vma(walk->mm, addr);\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tu64 pfn = PM_NOT_PRESENT;\n\n\t\t/* check to see if we've left 'vma' behind\n\t\t * and need a new, higher one */\n\t\tif (vma && (addr >= vma->vm_end))\n\t\t\tvma = find_vma(walk->mm, addr);\n\n\t\t/* check that 'vma' actually covers this address,\n\t\t * and that it isn't a huge page vma */\n\t\tif (vma && (vma->vm_start <= addr) &&\n\t\t    !is_vm_hugetlb_page(vma)) {\n\t\t\tpte = pte_offset_map(pmd, addr);\n\t\t\tpfn = pte_to_pagemap_entry(*pte);\n\t\t\t/* unmap before userspace copy */\n\t\t\tpte_unmap(pte);\n\t\t}\n\t\terr = add_to_pagemap(addr, pfn, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic u64 huge_pte_to_pagemap_entry(pte_t pte, int offset)\n{\n\tu64 pme = 0;\n\tif (pte_present(pte))\n\t\tpme = PM_PFRAME(pte_pfn(pte) + offset)\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_PRESENT;\n\treturn pme;\n}\n\n/* This function walks within one hugetlb entry in the single call */\nstatic int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tint err = 0;\n\tu64 pfn;\n\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tint offset = (addr & ~hmask) >> PAGE_SHIFT;\n\t\tpfn = huge_pte_to_pagemap_entry(*pte, offset);\n\t\terr = add_to_pagemap(addr, pfn, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n#endif /* HUGETLB_PAGE */\n\n/*\n * /proc/pid/pagemap - an array mapping virtual pages to pfns\n *\n * For each page in the address space, this file contains one 64-bit entry\n * consisting of the following:\n *\n * Bits 0-55  page frame number (PFN) if present\n * Bits 0-4   swap type if swapped\n * Bits 5-55  swap offset if swapped\n * Bits 55-60 page shift (page size = 1<<page shift)\n * Bit  61    reserved for future use\n * Bit  62    page swapped\n * Bit  63    page present\n *\n * If the page is not present but in swap, then the PFN contains an\n * encoding of the swap file number and the page's offset into the\n * swap. Unmapped pages return a null PFN. This allows determining\n * precisely which pages are mapped (or in swap) and comparing mapped\n * pages between processes.\n *\n * Efficient users of this interface will use /proc/pid/maps to\n * determine which areas of memory are actually mapped and llseek to\n * skip over unmapped regions.\n */\n#define PAGEMAP_WALK_SIZE\t(PMD_SIZE)\n#define PAGEMAP_WALK_MASK\t(PMD_MASK)\nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct task_struct *task = get_proc_task(file->f_path.dentry->d_inode);\n\tstruct mm_struct *mm;\n\tstruct pagemapread pm;\n\tint ret = -ESRCH;\n\tstruct mm_walk pagemap_walk = {};\n\tunsigned long src;\n\tunsigned long svpfn;\n\tunsigned long start_vaddr;\n\tunsigned long end_vaddr;\n\tint copied = 0;\n\n\tif (!task)\n\t\tgoto out;\n\n\tmm = mm_for_maps(task);\n\tret = PTR_ERR(mm);\n\tif (!mm || IS_ERR(mm))\n\t\tgoto out_task;\n\n\tret = -EINVAL;\n\t/* file position must be aligned */\n\tif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\n\t\tgoto out_task;\n\n\tret = 0;\n\n\tif (!count)\n\t\tgoto out_task;\n\n\tpm.len = PM_ENTRY_BYTES * (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\n\tpm.buffer = kmalloc(pm.len, GFP_TEMPORARY);\n\tret = -ENOMEM;\n\tif (!pm.buffer)\n\t\tgoto out_mm;\n\n\tpagemap_walk.pmd_entry = pagemap_pte_range;\n\tpagemap_walk.pte_hole = pagemap_pte_hole;\n#ifdef CONFIG_HUGETLB_PAGE\n\tpagemap_walk.hugetlb_entry = pagemap_hugetlb_range;\n#endif\n\tpagemap_walk.mm = mm;\n\tpagemap_walk.private = &pm;\n\n\tsrc = *ppos;\n\tsvpfn = src / PM_ENTRY_BYTES;\n\tstart_vaddr = svpfn << PAGE_SHIFT;\n\tend_vaddr = TASK_SIZE_OF(task);\n\n\t/* watch out for wraparound */\n\tif (svpfn > TASK_SIZE_OF(task) >> PAGE_SHIFT)\n\t\tstart_vaddr = end_vaddr;\n\n\t/*\n\t * The odds are that this will stop walking way\n\t * before end_vaddr, because the length of the\n\t * user buffer is tracked in \"pm\", and the walk\n\t * will stop when we hit the end of the buffer.\n\t */\n\tret = 0;\n\twhile (count && (start_vaddr < end_vaddr)) {\n\t\tint len;\n\t\tunsigned long end;\n\n\t\tpm.pos = 0;\n\t\tend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\n\t\t/* overflow ? */\n\t\tif (end < start_vaddr || end > end_vaddr)\n\t\t\tend = end_vaddr;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = walk_page_range(start_vaddr, end, &pagemap_walk);\n\t\tup_read(&mm->mmap_sem);\n\t\tstart_vaddr = end;\n\n\t\tlen = min(count, PM_ENTRY_BYTES * pm.pos);\n\t\tif (copy_to_user(buf, pm.buffer, len)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcopied += len;\n\t\tbuf += len;\n\t\tcount -= len;\n\t}\n\t*ppos += copied;\n\tif (!ret || ret == PM_END_OF_BUFFER)\n\t\tret = copied;\n\nout_free:\n\tkfree(pm.buffer);\nout_mm:\n\tmmput(mm);\nout_task:\n\tput_task_struct(task);\nout:\n\treturn ret;\n}\n\nconst struct file_operations proc_pagemap_operations = {\n\t.llseek\t\t= mem_lseek, /* borrow this */\n\t.read\t\t= pagemap_read,\n};\n#endif /* CONFIG_PROC_PAGE_MONITOR */\n\n#ifdef CONFIG_NUMA\nextern int show_numa_map(struct seq_file *m, void *v);\n\nstatic const struct seq_operations proc_pid_numa_maps_op = {\n        .start  = m_start,\n        .next   = m_next,\n        .stop   = m_stop,\n        .show   = show_numa_map,\n};\n\nstatic int numa_maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_numa_maps_op);\n}\n\nconst struct file_operations proc_numa_maps_operations = {\n\t.open\t\t= numa_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n#endif\n"], "fixing_code": ["#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/mount.h>\n#include <linux/seq_file.h>\n#include <linux/highmem.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <asm/elf.h>\n#include <asm/uaccess.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\nvoid task_mem(struct seq_file *m, struct mm_struct *mm)\n{\n\tunsigned long data, text, lib, swap;\n\tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n\n\t/*\n\t * Note: to minimize their overhead, mm maintains hiwater_vm and\n\t * hiwater_rss only when about to *lower* total_vm or rss.  Any\n\t * collector of these hiwater stats must therefore get total_vm\n\t * and rss too, which will usually be the higher.  Barriers? not\n\t * worth the effort, such snapshots can always be inconsistent.\n\t */\n\thiwater_vm = total_vm = mm->total_vm;\n\tif (hiwater_vm < mm->hiwater_vm)\n\t\thiwater_vm = mm->hiwater_vm;\n\thiwater_rss = total_rss = get_mm_rss(mm);\n\tif (hiwater_rss < mm->hiwater_rss)\n\t\thiwater_rss = mm->hiwater_rss;\n\n\tdata = mm->total_vm - mm->shared_vm - mm->stack_vm;\n\ttext = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK)) >> 10;\n\tlib = (mm->exec_vm << (PAGE_SHIFT-10)) - text;\n\tswap = get_mm_counter(mm, MM_SWAPENTS);\n\tseq_printf(m,\n\t\t\"VmPeak:\\t%8lu kB\\n\"\n\t\t\"VmSize:\\t%8lu kB\\n\"\n\t\t\"VmLck:\\t%8lu kB\\n\"\n\t\t\"VmHWM:\\t%8lu kB\\n\"\n\t\t\"VmRSS:\\t%8lu kB\\n\"\n\t\t\"VmData:\\t%8lu kB\\n\"\n\t\t\"VmStk:\\t%8lu kB\\n\"\n\t\t\"VmExe:\\t%8lu kB\\n\"\n\t\t\"VmLib:\\t%8lu kB\\n\"\n\t\t\"VmPTE:\\t%8lu kB\\n\"\n\t\t\"VmSwap:\\t%8lu kB\\n\",\n\t\thiwater_vm << (PAGE_SHIFT-10),\n\t\t(total_vm - mm->reserved_vm) << (PAGE_SHIFT-10),\n\t\tmm->locked_vm << (PAGE_SHIFT-10),\n\t\thiwater_rss << (PAGE_SHIFT-10),\n\t\ttotal_rss << (PAGE_SHIFT-10),\n\t\tdata << (PAGE_SHIFT-10),\n\t\tmm->stack_vm << (PAGE_SHIFT-10), text, lib,\n\t\t(PTRS_PER_PTE*sizeof(pte_t)*mm->nr_ptes) >> 10,\n\t\tswap << (PAGE_SHIFT-10));\n}\n\nunsigned long task_vsize(struct mm_struct *mm)\n{\n\treturn PAGE_SIZE * mm->total_vm;\n}\n\nunsigned long task_statm(struct mm_struct *mm,\n\t\t\t unsigned long *shared, unsigned long *text,\n\t\t\t unsigned long *data, unsigned long *resident)\n{\n\t*shared = get_mm_counter(mm, MM_FILEPAGES);\n\t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n\t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\t*data = mm->total_vm - mm->shared_vm;\n\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n\treturn mm->total_vm;\n}\n\nstatic void pad_len_spaces(struct seq_file *m, int len)\n{\n\tlen = 25 + sizeof(void*) * 6 - len;\n\tif (len < 1)\n\t\tlen = 1;\n\tseq_printf(m, \"%*c\", len, ' ');\n}\n\nstatic void vma_stop(struct proc_maps_private *priv, struct vm_area_struct *vma)\n{\n\tif (vma && vma != priv->tail_vma) {\n\t\tstruct mm_struct *mm = vma->vm_mm;\n\t\tup_read(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n}\n\nstatic void *m_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tunsigned long last_addr = m->version;\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma, *tail_vma = NULL;\n\tloff_t l = *pos;\n\n\t/* Clear the per syscall fields in priv */\n\tpriv->task = NULL;\n\tpriv->tail_vma = NULL;\n\n\t/*\n\t * We remember last_addr rather than next_addr to hit with\n\t * mmap_cache most of the time. We have zero last_addr at\n\t * the beginning and also after lseek. We will have -1 last_addr\n\t * after the end of the vmas.\n\t */\n\n\tif (last_addr == -1UL)\n\t\treturn NULL;\n\n\tpriv->task = get_pid_task(priv->pid, PIDTYPE_PID);\n\tif (!priv->task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\tmm = mm_for_maps(priv->task);\n\tif (!mm || IS_ERR(mm))\n\t\treturn mm;\n\tdown_read(&mm->mmap_sem);\n\n\ttail_vma = get_gate_vma(priv->task->mm);\n\tpriv->tail_vma = tail_vma;\n\n\t/* Start with last addr hint */\n\tvma = find_vma(mm, last_addr);\n\tif (last_addr && vma) {\n\t\tvma = vma->vm_next;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Check the vma index is within the range and do\n\t * sequential scan until m_index.\n\t */\n\tvma = NULL;\n\tif ((unsigned long)l < mm->map_count) {\n\t\tvma = mm->mmap;\n\t\twhile (l-- && vma)\n\t\t\tvma = vma->vm_next;\n\t\tgoto out;\n\t}\n\n\tif (l != mm->map_count)\n\t\ttail_vma = NULL; /* After gate vma */\n\nout:\n\tif (vma)\n\t\treturn vma;\n\n\t/* End of vmas has been reached */\n\tm->version = (tail_vma != NULL)? 0: -1UL;\n\tup_read(&mm->mmap_sem);\n\tmmput(mm);\n\treturn tail_vma;\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *vma = v;\n\tstruct vm_area_struct *tail_vma = priv->tail_vma;\n\n\t(*pos)++;\n\tif (vma && (vma != tail_vma) && vma->vm_next)\n\t\treturn vma->vm_next;\n\tvma_stop(priv, vma);\n\treturn (vma != tail_vma)? tail_vma: NULL;\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct vm_area_struct *vma = v;\n\n\tif (!IS_ERR(vma))\n\t\tvma_stop(priv, vma);\n\tif (priv->task)\n\t\tput_task_struct(priv->task);\n}\n\nstatic int do_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops)\n{\n\tstruct proc_maps_private *priv;\n\tint ret = -ENOMEM;\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (priv) {\n\t\tpriv->pid = proc_pid(inode);\n\t\tret = seq_open(file, ops);\n\t\tif (!ret) {\n\t\t\tstruct seq_file *m = file->private_data;\n\t\t\tm->private = priv;\n\t\t} else {\n\t\t\tkfree(priv);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void show_map_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tint flags = vma->vm_flags;\n\tunsigned long ino = 0;\n\tunsigned long long pgoff = 0;\n\tunsigned long start;\n\tdev_t dev = 0;\n\tint len;\n\n\tif (file) {\n\t\tstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tpgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\n\t}\n\n\t/* We don't show the stack guard page in /proc/maps */\n\tstart = vma->vm_start;\n\tif (vma->vm_flags & VM_GROWSDOWN)\n\t\tif (!vma_stack_continue(vma->vm_prev, vma->vm_start))\n\t\t\tstart += PAGE_SIZE;\n\n\tseq_printf(m, \"%08lx-%08lx %c%c%c%c %08llx %02x:%02x %lu %n\",\n\t\t\tstart,\n\t\t\tvma->vm_end,\n\t\t\tflags & VM_READ ? 'r' : '-',\n\t\t\tflags & VM_WRITE ? 'w' : '-',\n\t\t\tflags & VM_EXEC ? 'x' : '-',\n\t\t\tflags & VM_MAYSHARE ? 's' : 'p',\n\t\t\tpgoff,\n\t\t\tMAJOR(dev), MINOR(dev), ino, &len);\n\n\t/*\n\t * Print the dentry name for named mappings, and a\n\t * special [heap] marker for the heap:\n\t */\n\tif (file) {\n\t\tpad_len_spaces(m, len);\n\t\tseq_path(m, &file->f_path, \"\\n\");\n\t} else {\n\t\tconst char *name = arch_vma_name(vma);\n\t\tif (!name) {\n\t\t\tif (mm) {\n\t\t\t\tif (vma->vm_start <= mm->brk &&\n\t\t\t\t\t\tvma->vm_end >= mm->start_brk) {\n\t\t\t\t\tname = \"[heap]\";\n\t\t\t\t} else if (vma->vm_start <= mm->start_stack &&\n\t\t\t\t\t   vma->vm_end >= mm->start_stack) {\n\t\t\t\t\tname = \"[stack]\";\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tname = \"[vdso]\";\n\t\t\t}\n\t\t}\n\t\tif (name) {\n\t\t\tpad_len_spaces(m, len);\n\t\t\tseq_puts(m, name);\n\t\t}\n\t}\n\tseq_putc(m, '\\n');\n}\n\nstatic int show_map(struct seq_file *m, void *v)\n{\n\tstruct vm_area_struct *vma = v;\n\tstruct proc_maps_private *priv = m->private;\n\tstruct task_struct *task = priv->task;\n\n\tshow_map_vma(m, vma);\n\n\tif (m->count < m->size)  /* vma is copied successfully */\n\t\tm->version = (vma != get_gate_vma(task->mm))\n\t\t\t? vma->vm_start : 0;\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_maps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_map\n};\n\nstatic int maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_maps_op);\n}\n\nconst struct file_operations proc_maps_operations = {\n\t.open\t\t= maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n\n/*\n * Proportional Set Size(PSS): my share of RSS.\n *\n * PSS of a process is the count of pages it has in memory, where each\n * page is divided by the number of processes sharing it.  So if a\n * process has 1000 pages all to itself, and 1000 shared with one other\n * process, its PSS will be 1500.\n *\n * To keep (accumulated) division errors low, we adopt a 64bit\n * fixed-point pss counter to minimize division errors. So (pss >>\n * PSS_SHIFT) would be the real byte count.\n *\n * A shift of 12 before division means (assuming 4K page size):\n * \t- 1M 3-user-pages add up to 8KB errors;\n * \t- supports mapcount up to 2^24, or 16M;\n * \t- supports PSS up to 2^52 bytes, or 4PB.\n */\n#define PSS_SHIFT 12\n\n#ifdef CONFIG_PROC_PAGE_MONITOR\nstruct mem_size_stats {\n\tstruct vm_area_struct *vma;\n\tunsigned long resident;\n\tunsigned long shared_clean;\n\tunsigned long shared_dirty;\n\tunsigned long private_clean;\n\tunsigned long private_dirty;\n\tunsigned long referenced;\n\tunsigned long anonymous;\n\tunsigned long anonymous_thp;\n\tunsigned long swap;\n\tu64 pss;\n};\n\n\nstatic void smaps_pte_entry(pte_t ptent, unsigned long addr,\n\t\tunsigned long ptent_size, struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = mss->vma;\n\tstruct page *page;\n\tint mapcount;\n\n\tif (is_swap_pte(ptent)) {\n\t\tmss->swap += ptent_size;\n\t\treturn;\n\t}\n\n\tif (!pte_present(ptent))\n\t\treturn;\n\n\tpage = vm_normal_page(vma, addr, ptent);\n\tif (!page)\n\t\treturn;\n\n\tif (PageAnon(page))\n\t\tmss->anonymous += ptent_size;\n\n\tmss->resident += ptent_size;\n\t/* Accumulate the size in pages that have been accessed. */\n\tif (pte_young(ptent) || PageReferenced(page))\n\t\tmss->referenced += ptent_size;\n\tmapcount = page_mapcount(page);\n\tif (mapcount >= 2) {\n\t\tif (pte_dirty(ptent) || PageDirty(page))\n\t\t\tmss->shared_dirty += ptent_size;\n\t\telse\n\t\t\tmss->shared_clean += ptent_size;\n\t\tmss->pss += (ptent_size << PSS_SHIFT) / mapcount;\n\t} else {\n\t\tif (pte_dirty(ptent) || PageDirty(page))\n\t\t\tmss->private_dirty += ptent_size;\n\t\telse\n\t\t\tmss->private_clean += ptent_size;\n\t\tmss->pss += (ptent_size << PSS_SHIFT);\n\t}\n}\n\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t   struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = mss->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tspin_lock(&walk->mm->page_table_lock);\n\tif (pmd_trans_huge(*pmd)) {\n\t\tif (pmd_trans_splitting(*pmd)) {\n\t\t\tspin_unlock(&walk->mm->page_table_lock);\n\t\t\twait_split_huge_page(vma->anon_vma, pmd);\n\t\t} else {\n\t\t\tsmaps_pte_entry(*(pte_t *)pmd, addr,\n\t\t\t\t\tHPAGE_PMD_SIZE, walk);\n\t\t\tspin_unlock(&walk->mm->page_table_lock);\n\t\t\tmss->anonymous_thp += HPAGE_PMD_SIZE;\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tspin_unlock(&walk->mm->page_table_lock);\n\t}\n\t/*\n\t * The mmap_sem held all the way back in m_start() is what\n\t * keeps khugepaged out of here and from collapsing things\n\t * in here.\n\t */\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tsmaps_pte_entry(*pte, addr, PAGE_SIZE, walk);\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int show_smap(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct task_struct *task = priv->task;\n\tstruct vm_area_struct *vma = v;\n\tstruct mem_size_stats mss;\n\tstruct mm_walk smaps_walk = {\n\t\t.pmd_entry = smaps_pte_range,\n\t\t.mm = vma->vm_mm,\n\t\t.private = &mss,\n\t};\n\n\tmemset(&mss, 0, sizeof mss);\n\tmss.vma = vma;\n\t/* mmap_sem is held in m_start */\n\tif (vma->vm_mm && !is_vm_hugetlb_page(vma))\n\t\twalk_page_range(vma->vm_start, vma->vm_end, &smaps_walk);\n\n\tshow_map_vma(m, vma);\n\n\tseq_printf(m,\n\t\t   \"Size:           %8lu kB\\n\"\n\t\t   \"Rss:            %8lu kB\\n\"\n\t\t   \"Pss:            %8lu kB\\n\"\n\t\t   \"Shared_Clean:   %8lu kB\\n\"\n\t\t   \"Shared_Dirty:   %8lu kB\\n\"\n\t\t   \"Private_Clean:  %8lu kB\\n\"\n\t\t   \"Private_Dirty:  %8lu kB\\n\"\n\t\t   \"Referenced:     %8lu kB\\n\"\n\t\t   \"Anonymous:      %8lu kB\\n\"\n\t\t   \"AnonHugePages:  %8lu kB\\n\"\n\t\t   \"Swap:           %8lu kB\\n\"\n\t\t   \"KernelPageSize: %8lu kB\\n\"\n\t\t   \"MMUPageSize:    %8lu kB\\n\"\n\t\t   \"Locked:         %8lu kB\\n\",\n\t\t   (vma->vm_end - vma->vm_start) >> 10,\n\t\t   mss.resident >> 10,\n\t\t   (unsigned long)(mss.pss >> (10 + PSS_SHIFT)),\n\t\t   mss.shared_clean  >> 10,\n\t\t   mss.shared_dirty  >> 10,\n\t\t   mss.private_clean >> 10,\n\t\t   mss.private_dirty >> 10,\n\t\t   mss.referenced >> 10,\n\t\t   mss.anonymous >> 10,\n\t\t   mss.anonymous_thp >> 10,\n\t\t   mss.swap >> 10,\n\t\t   vma_kernel_pagesize(vma) >> 10,\n\t\t   vma_mmu_pagesize(vma) >> 10,\n\t\t   (vma->vm_flags & VM_LOCKED) ?\n\t\t\t(unsigned long)(mss.pss >> (10 + PSS_SHIFT)) : 0);\n\n\tif (m->count < m->size)  /* vma is copied successfully */\n\t\tm->version = (vma != get_gate_vma(task->mm))\n\t\t\t? vma->vm_start : 0;\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_smaps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_smap\n};\n\nstatic int smaps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_smaps_op);\n}\n\nconst struct file_operations proc_smaps_operations = {\n\t.open\t\t= smaps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->private;\n\tpte_t *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\n\tsplit_huge_page_pmd(walk->mm, pmd);\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = *pte;\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t/* Clear accessed and referenced bits. */\n\t\tptep_test_and_clear_young(vma, addr, pte);\n\t\tClearPageReferenced(page);\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\n#define CLEAR_REFS_ALL 1\n#define CLEAR_REFS_ANON 2\n#define CLEAR_REFS_MAPPED 3\n\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tlong type;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\tif (strict_strtol(strstrip(buffer), 10, &type))\n\t\treturn -EINVAL;\n\tif (type < CLEAR_REFS_ALL || type > CLEAR_REFS_MAPPED)\n\t\treturn -EINVAL;\n\ttask = get_proc_task(file->f_path.dentry->d_inode);\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tstruct mm_walk clear_refs_walk = {\n\t\t\t.pmd_entry = clear_refs_pte_range,\n\t\t\t.mm = mm,\n\t\t};\n\t\tdown_read(&mm->mmap_sem);\n\t\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\n\t\t\tclear_refs_walk.private = vma;\n\t\t\tif (is_vm_hugetlb_page(vma))\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * Writing 1 to /proc/pid/clear_refs affects all pages.\n\t\t\t *\n\t\t\t * Writing 2 to /proc/pid/clear_refs only affects\n\t\t\t * Anonymous pages.\n\t\t\t *\n\t\t\t * Writing 3 to /proc/pid/clear_refs only affects file\n\t\t\t * mapped pages.\n\t\t\t */\n\t\t\tif (type == CLEAR_REFS_ANON && vma->vm_file)\n\t\t\t\tcontinue;\n\t\t\tif (type == CLEAR_REFS_MAPPED && !vma->vm_file)\n\t\t\t\tcontinue;\n\t\t\twalk_page_range(vma->vm_start, vma->vm_end,\n\t\t\t\t\t&clear_refs_walk);\n\t\t}\n\t\tflush_tlb_mm(mm);\n\t\tup_read(&mm->mmap_sem);\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}\n\nconst struct file_operations proc_clear_refs_operations = {\n\t.write\t\t= clear_refs_write,\n\t.llseek\t\t= noop_llseek,\n};\n\nstruct pagemapread {\n\tint pos, len;\n\tu64 *buffer;\n};\n\n#define PM_ENTRY_BYTES      sizeof(u64)\n#define PM_STATUS_BITS      3\n#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)\n#define PM_STATUS_MASK      (((1LL << PM_STATUS_BITS) - 1) << PM_STATUS_OFFSET)\n#define PM_STATUS(nr)       (((nr) << PM_STATUS_OFFSET) & PM_STATUS_MASK)\n#define PM_PSHIFT_BITS      6\n#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)\n#define PM_PSHIFT_MASK      (((1LL << PM_PSHIFT_BITS) - 1) << PM_PSHIFT_OFFSET)\n#define PM_PSHIFT(x)        (((u64) (x) << PM_PSHIFT_OFFSET) & PM_PSHIFT_MASK)\n#define PM_PFRAME_MASK      ((1LL << PM_PSHIFT_OFFSET) - 1)\n#define PM_PFRAME(x)        ((x) & PM_PFRAME_MASK)\n\n#define PM_PRESENT          PM_STATUS(4LL)\n#define PM_SWAP             PM_STATUS(2LL)\n#define PM_NOT_PRESENT      PM_PSHIFT(PAGE_SHIFT)\n#define PM_END_OF_BUFFER    1\n\nstatic int add_to_pagemap(unsigned long addr, u64 pfn,\n\t\t\t  struct pagemapread *pm)\n{\n\tpm->buffer[pm->pos++] = pfn;\n\tif (pm->pos >= pm->len)\n\t\treturn PM_END_OF_BUFFER;\n\treturn 0;\n}\n\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tunsigned long addr;\n\tint err = 0;\n\tfor (addr = start; addr < end; addr += PAGE_SIZE) {\n\t\terr = add_to_pagemap(addr, PM_NOT_PRESENT, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic u64 swap_pte_to_pagemap_entry(pte_t pte)\n{\n\tswp_entry_t e = pte_to_swp_entry(pte);\n\treturn swp_type(e) | (swp_offset(e) << MAX_SWAPFILES_SHIFT);\n}\n\nstatic u64 pte_to_pagemap_entry(pte_t pte)\n{\n\tu64 pme = 0;\n\tif (is_swap_pte(pte))\n\t\tpme = PM_PFRAME(swap_pte_to_pagemap_entry(pte))\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_SWAP;\n\telse if (pte_present(pte))\n\t\tpme = PM_PFRAME(pte_pfn(pte))\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_PRESENT;\n\treturn pme;\n}\n\nstatic int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma;\n\tstruct pagemapread *pm = walk->private;\n\tpte_t *pte;\n\tint err = 0;\n\n\tsplit_huge_page_pmd(walk->mm, pmd);\n\n\t/* find the first VMA at or above 'addr' */\n\tvma = find_vma(walk->mm, addr);\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tu64 pfn = PM_NOT_PRESENT;\n\n\t\t/* check to see if we've left 'vma' behind\n\t\t * and need a new, higher one */\n\t\tif (vma && (addr >= vma->vm_end))\n\t\t\tvma = find_vma(walk->mm, addr);\n\n\t\t/* check that 'vma' actually covers this address,\n\t\t * and that it isn't a huge page vma */\n\t\tif (vma && (vma->vm_start <= addr) &&\n\t\t    !is_vm_hugetlb_page(vma)) {\n\t\t\tpte = pte_offset_map(pmd, addr);\n\t\t\tpfn = pte_to_pagemap_entry(*pte);\n\t\t\t/* unmap before userspace copy */\n\t\t\tpte_unmap(pte);\n\t\t}\n\t\terr = add_to_pagemap(addr, pfn, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic u64 huge_pte_to_pagemap_entry(pte_t pte, int offset)\n{\n\tu64 pme = 0;\n\tif (pte_present(pte))\n\t\tpme = PM_PFRAME(pte_pfn(pte) + offset)\n\t\t\t| PM_PSHIFT(PAGE_SHIFT) | PM_PRESENT;\n\treturn pme;\n}\n\n/* This function walks within one hugetlb entry in the single call */\nstatic int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tint err = 0;\n\tu64 pfn;\n\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tint offset = (addr & ~hmask) >> PAGE_SHIFT;\n\t\tpfn = huge_pte_to_pagemap_entry(*pte, offset);\n\t\terr = add_to_pagemap(addr, pfn, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n#endif /* HUGETLB_PAGE */\n\n/*\n * /proc/pid/pagemap - an array mapping virtual pages to pfns\n *\n * For each page in the address space, this file contains one 64-bit entry\n * consisting of the following:\n *\n * Bits 0-55  page frame number (PFN) if present\n * Bits 0-4   swap type if swapped\n * Bits 5-55  swap offset if swapped\n * Bits 55-60 page shift (page size = 1<<page shift)\n * Bit  61    reserved for future use\n * Bit  62    page swapped\n * Bit  63    page present\n *\n * If the page is not present but in swap, then the PFN contains an\n * encoding of the swap file number and the page's offset into the\n * swap. Unmapped pages return a null PFN. This allows determining\n * precisely which pages are mapped (or in swap) and comparing mapped\n * pages between processes.\n *\n * Efficient users of this interface will use /proc/pid/maps to\n * determine which areas of memory are actually mapped and llseek to\n * skip over unmapped regions.\n */\n#define PAGEMAP_WALK_SIZE\t(PMD_SIZE)\n#define PAGEMAP_WALK_MASK\t(PMD_MASK)\nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct task_struct *task = get_proc_task(file->f_path.dentry->d_inode);\n\tstruct mm_struct *mm;\n\tstruct pagemapread pm;\n\tint ret = -ESRCH;\n\tstruct mm_walk pagemap_walk = {};\n\tunsigned long src;\n\tunsigned long svpfn;\n\tunsigned long start_vaddr;\n\tunsigned long end_vaddr;\n\tint copied = 0;\n\n\tif (!task)\n\t\tgoto out;\n\n\tmm = mm_for_maps(task);\n\tret = PTR_ERR(mm);\n\tif (!mm || IS_ERR(mm))\n\t\tgoto out_task;\n\n\tret = -EINVAL;\n\t/* file position must be aligned */\n\tif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\n\t\tgoto out_task;\n\n\tret = 0;\n\n\tif (!count)\n\t\tgoto out_task;\n\n\tpm.len = PM_ENTRY_BYTES * (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\n\tpm.buffer = kmalloc(pm.len, GFP_TEMPORARY);\n\tret = -ENOMEM;\n\tif (!pm.buffer)\n\t\tgoto out_mm;\n\n\tpagemap_walk.pmd_entry = pagemap_pte_range;\n\tpagemap_walk.pte_hole = pagemap_pte_hole;\n#ifdef CONFIG_HUGETLB_PAGE\n\tpagemap_walk.hugetlb_entry = pagemap_hugetlb_range;\n#endif\n\tpagemap_walk.mm = mm;\n\tpagemap_walk.private = &pm;\n\n\tsrc = *ppos;\n\tsvpfn = src / PM_ENTRY_BYTES;\n\tstart_vaddr = svpfn << PAGE_SHIFT;\n\tend_vaddr = TASK_SIZE_OF(task);\n\n\t/* watch out for wraparound */\n\tif (svpfn > TASK_SIZE_OF(task) >> PAGE_SHIFT)\n\t\tstart_vaddr = end_vaddr;\n\n\t/*\n\t * The odds are that this will stop walking way\n\t * before end_vaddr, because the length of the\n\t * user buffer is tracked in \"pm\", and the walk\n\t * will stop when we hit the end of the buffer.\n\t */\n\tret = 0;\n\twhile (count && (start_vaddr < end_vaddr)) {\n\t\tint len;\n\t\tunsigned long end;\n\n\t\tpm.pos = 0;\n\t\tend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\n\t\t/* overflow ? */\n\t\tif (end < start_vaddr || end > end_vaddr)\n\t\t\tend = end_vaddr;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = walk_page_range(start_vaddr, end, &pagemap_walk);\n\t\tup_read(&mm->mmap_sem);\n\t\tstart_vaddr = end;\n\n\t\tlen = min(count, PM_ENTRY_BYTES * pm.pos);\n\t\tif (copy_to_user(buf, pm.buffer, len)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcopied += len;\n\t\tbuf += len;\n\t\tcount -= len;\n\t}\n\t*ppos += copied;\n\tif (!ret || ret == PM_END_OF_BUFFER)\n\t\tret = copied;\n\nout_free:\n\tkfree(pm.buffer);\nout_mm:\n\tmmput(mm);\nout_task:\n\tput_task_struct(task);\nout:\n\treturn ret;\n}\n\nconst struct file_operations proc_pagemap_operations = {\n\t.llseek\t\t= mem_lseek, /* borrow this */\n\t.read\t\t= pagemap_read,\n};\n#endif /* CONFIG_PROC_PAGE_MONITOR */\n\n#ifdef CONFIG_NUMA\nextern int show_numa_map(struct seq_file *m, void *v);\n\nstatic const struct seq_operations proc_pid_numa_maps_op = {\n        .start  = m_start,\n        .next   = m_next,\n        .stop   = m_stop,\n        .show   = show_numa_map,\n};\n\nstatic int numa_maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_numa_maps_op);\n}\n\nconst struct file_operations proc_numa_maps_operations = {\n\t.open\t\t= numa_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_private,\n};\n#endif\n"], "filenames": ["fs/proc/task_mmu.c"], "buggy_code_start_loc": [185], "buggy_code_end_loc": [186], "fixing_code_start_loc": [185], "fixing_code_end_loc": [187], "type": "CWE-476", "message": "The m_stop function in fs/proc/task_mmu.c in the Linux kernel before 2.6.39 allows local users to cause a denial of service (OOPS) via vectors that trigger an m_start error.", "other": {"cve": {"id": "CVE-2011-3637", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-17T11:00:31.540", "lastModified": "2023-02-13T01:21:15.217", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The m_stop function in fs/proc/task_mmu.c in the Linux kernel before 2.6.39 allows local users to cause a denial of service (OOPS) via vectors that trigger an m_start error."}, {"lang": "es", "value": "La funci\u00f3n m_stop en fs/proc/task_mmu.c en el kernel de Linux antes de v2.6.39 permite a usuarios locales provocar una denegaci\u00f3n de servicio a trav\u00e9s de vectores que provocan un error m_start."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.39", "matchCriteriaId": "176353CE-F17E-4776-AD9F-19014DA75B76"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:6.0:*:*:*:*:*:*:*", "matchCriteriaId": "2F6AB192-9D7D-4A9A-8995-E53A9DE9EAFC"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=76597cd31470fa130784c78fadb4dab2e624a723", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/02/06/1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=747848", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/76597cd31470fa130784c78fadb4dab2e624a723", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/76597cd31470fa130784c78fadb4dab2e624a723"}}