{"buggy_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/pooling_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Device, typename T>\nclass QuantizedAvgPoolingOp : public OpKernel {\n public:\n  explicit QuantizedAvgPoolingOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 4,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 4,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n\n    OP_REQUIRES(context, params.depth_window == 1,\n                errors::Unimplemented(\"Non-spatial pooling is not \"\n                                      \"yet supported. Volunteers? :)\"));\n\n    OP_REQUIRES(context, tensor_in.dims() == 4,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0, params.forward_output_shape(), &output));\n    const int32_t highest = static_cast<int32>(Eigen::NumTraits<T>::highest());\n    const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T>::lowest());\n\n    // TODO(vrv): Switch this to the Eigen::Tensor version of\n    // SpatialAvgPooling once that version is running quickly.\n    Tensor int32_output(DT_INT32, params.forward_output_shape());\n    // Cast input to int32 tensor and call SpatialAvgPool.\n    Tensor int32_input(DT_INT32, tensor_in.shape());\n    int32_input.flat<int32>() = tensor_in.flat<T>().template cast<int32>();\n    SpatialAvgPool<Device, int32>(context, &int32_output, int32_input, params,\n                                  padding_);\n\n    // Clamp the int32 output back into quantized space.\n    output->flat<T>() = int32_output.flat<int32>()\n                            .cwiseMax(lowest)\n                            .cwiseMin(highest)\n                            .template cast<T>();\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n};\n\ntemplate <typename Device, typename T>\nclass QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n public:\n  explicit QuantizedMaxPoolingOp(OpKernelConstruction* context)\n      : MaxPoolingOp<Device, T>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    auto min_input_tensor = context->input(1);\n    auto max_input_tensor = context->input(2);\n    OP_REQUIRES(\n        context, min_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"min_input must be a scalar float value, got tensor with shape \",\n            min_input_tensor.shape()));\n    OP_REQUIRES(\n        context, max_input_tensor.NumElements() == 1,\n        errors::InvalidArgument(\n            \"max_input must be a scalar float value, got tensor with shape \",\n            max_input_tensor.shape()));\n    const float min_input = context->input(1).flat<float>()(0);\n    const float max_input = context->input(2).flat<float>()(0);\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedAvgPool\").Device(DEVICE_CPU).TypeConstraint<quint8>(\"T\"),\n    QuantizedAvgPoolingOp<CPUDevice, quint8>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedMaxPool\").Device(DEVICE_CPU).TypeConstraint<quint8>(\"T\"),\n    QuantizedMaxPoolingOp<CPUDevice, quint8>);\n\n#ifdef INTEL_MKL\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedAvgPool\").Device(DEVICE_CPU).TypeConstraint<qint8>(\"T\"),\n    QuantizedAvgPoolingOp<CPUDevice, qint8>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedMaxPool\").Device(DEVICE_CPU).TypeConstraint<qint8>(\"T\"),\n    QuantizedMaxPoolingOp<CPUDevice, qint8>);\n#endif\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/kernels/ops_testutil.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nclass QuantizedPoolingTest : public OpsTestBase {\n protected:\n};\n\nTEST_F(QuantizedPoolingTest, SmallAveragePooling) {\n  const int ksize = 2;\n  const int stride = 2;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_avg_pool_op\", \"QuantizedAvgPool\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n                   .Attr(\"ksize\", {1, ksize, ksize, 1})\n                   .Attr(\"strides\", {1, stride, stride, 1})\n                   .Attr(\"padding\", \"SAME\")\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  const float input_min = 0.0f;\n  const float input_max = 255.0f;\n  const int input_height = 4;\n  const int input_width = 4;\n  const int input_channels = 2;\n  Tensor input_float(DT_FLOAT, {1, input_height, input_width, input_channels});\n  test::FillValues<float>(\n      &input_float,\n      {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32});\n  Tensor input_quantized =\n      FloatTensorToQuantized<quint8>(input_float, input_min, input_max);\n\n  const int expected_width = input_width / stride;\n  const int expected_height = input_height / stride;\n  Tensor expected_float(DT_FLOAT,\n                        {1, expected_height, expected_width, input_channels});\n  test::FillValues<float>(&expected_float, {6, 7, 10, 11, 22, 23, 26, 27});\n\n  AddInputFromArray<quint8>(input_quantized.shape(),\n                            input_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({1}), {input_min});\n  AddInputFromArray<float>(TensorShape({1}), {input_max});\n  TF_ASSERT_OK(RunOpKernel());\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<quint8>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 0.2);\n}\n\nTEST_F(QuantizedPoolingTest, SmallMaxPooling) {\n  const int ksize = 2;\n  const int stride = 2;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_max_pool_op\", \"QuantizedMaxPool\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n                   .Attr(\"ksize\", {1, ksize, ksize, 1})\n                   .Attr(\"strides\", {1, stride, stride, 1})\n                   .Attr(\"padding\", \"SAME\")\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  const float input_min = 0.0f;\n  const float input_max = 255.0f;\n  const int input_height = 4;\n  const int input_width = 4;\n  const int input_channels = 2;\n  Tensor input_float(DT_FLOAT, {1, input_height, input_width, input_channels});\n  test::FillValues<float>(\n      &input_float,\n      {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32});\n  Tensor input_quantized =\n      FloatTensorToQuantized<quint8>(input_float, input_min, input_max);\n\n  const int expected_width = input_width / stride;\n  const int expected_height = input_height / stride;\n  Tensor expected_float(DT_FLOAT,\n                        {1, expected_height, expected_width, input_channels});\n  test::FillValues<float>(&expected_float, {11, 12, 15, 16, 27, 28, 31, 32});\n\n  AddInputFromArray<quint8>(input_quantized.shape(),\n                            input_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({1}), {input_min});\n  AddInputFromArray<float>(TensorShape({1}), {input_max});\n  TF_ASSERT_OK(RunOpKernel());\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<quint8>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 0.2);\n}\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tf.quantize ops.\"\"\"\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import googletest\n\n\nclass FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n\n\nclass FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[[0.0]], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[1.0], max=[[1.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n\n\nclass QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=[],\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=[],\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=[],\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=[],\n              out_type=dtypes.qint32))\n\n\nclass QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n\n\nclass RequantizeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=[],\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=[],\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=[],\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=[],\n              out_type=dtypes.qint8))\n\n\nclass QuantizedAddOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    x = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantized_add(\n              x=x,\n              y=y,\n              min_x=[],\n              max_x=1.0,\n              min_y=0.0,\n              max_y=1.0,\n              Toutput=dtypes.qint32))\n\n\nclass QuantizedReluOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu6(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(input=inputs,\n                                                  input_min=[],\n                                                  input_max=4.0,\n                                                  out_type=dtypes.quint8))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// See docs in ../ops/nn_ops.cc.\n\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/pooling_ops_common.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/util/padding.h\"\n#include \"tensorflow/core/util/tensor_format.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename Device, typename T>\nclass QuantizedAvgPoolingOp : public OpKernel {\n public:\n  explicit QuantizedAvgPoolingOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"ksize\", &ksize_));\n    OP_REQUIRES(context, ksize_.size() == 4,\n                errors::InvalidArgument(\"Sliding window ksize field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n    OP_REQUIRES(context, stride_.size() == 4,\n                errors::InvalidArgument(\"Sliding window strides field must \"\n                                        \"specify 4 dimensions\"));\n    OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n    OP_REQUIRES(context, ksize_[0] == 1 && stride_[0] == 1,\n                errors::Unimplemented(\n                    \"Pooling is not yet supported on the batch dimension.\"));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& tensor_in = context->input(0);\n    PoolParameters params{context,\n                          ksize_,\n                          stride_,\n                          padding_,\n                          /*explicit_paddings=*/{},\n                          FORMAT_NHWC,\n                          tensor_in.shape()};\n    if (!context->status().ok()) {\n      return;\n    }\n\n    const Tensor& min_input_tensor = context->input(1);\n    const Tensor& max_input_tensor = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n                errors::InvalidArgument(\n                    \"min_input shape must be rank 0 but is rank \",\n                    min_input_tensor.dims(),\n                    \", received shape: \", min_input_tensor.shape()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n                errors::InvalidArgument(\n                    \"max_input shape must be rank 0 but is rank \",\n                    max_input_tensor.dims(),\n                    \", received shape: \", max_input_tensor.shape()));\n    const float min_input = context->input(1).scalar<float>()();\n    const float max_input = context->input(2).scalar<float>()();\n\n    OP_REQUIRES(context, params.depth_window == 1,\n                errors::Unimplemented(\"Non-spatial pooling is not \"\n                                      \"yet supported. Volunteers? :)\"));\n\n    OP_REQUIRES(context, tensor_in.dims() == 4,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0, params.forward_output_shape(), &output));\n    const int32_t highest = static_cast<int32>(Eigen::NumTraits<T>::highest());\n    const int32_t lowest = static_cast<int32>(Eigen::NumTraits<T>::lowest());\n\n    // TODO(vrv): Switch this to the Eigen::Tensor version of\n    // SpatialAvgPooling once that version is running quickly.\n    Tensor int32_output(DT_INT32, params.forward_output_shape());\n    // Cast input to int32 tensor and call SpatialAvgPool.\n    Tensor int32_input(DT_INT32, tensor_in.shape());\n    int32_input.flat<int32>() = tensor_in.flat<T>().template cast<int32>();\n    SpatialAvgPool<Device, int32>(context, &int32_output, int32_input, params,\n                                  padding_);\n\n    // Clamp the int32 output back into quantized space.\n    output->flat<T>() = int32_output.flat<int32>()\n                            .cwiseMax(lowest)\n                            .cwiseMin(highest)\n                            .template cast<T>();\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n\n private:\n  std::vector<int32> ksize_;\n  std::vector<int32> stride_;\n  Padding padding_;\n};\n\ntemplate <typename Device, typename T>\nclass QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {\n public:\n  explicit QuantizedMaxPoolingOp(OpKernelConstruction* context)\n      : MaxPoolingOp<Device, T>(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& min_input_tensor = context->input(1);\n    const Tensor& max_input_tensor = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),\n                errors::InvalidArgument(\n                    \"min_input shape must be rank 0 but is rank \",\n                    min_input_tensor.dims(),\n                    \", received shape: \", min_input_tensor.shape()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),\n                errors::InvalidArgument(\n                    \"max_input shape must be rank 0 but is rank \",\n                    max_input_tensor.dims(),\n                    \", received shape: \", max_input_tensor.shape()));\n    const float min_input = context->input(1).scalar<float>()();\n    const float max_input = context->input(2).scalar<float>()();\n    MaxPoolingOp<Device, T>::Compute(context);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));\n    output_min->flat<float>()(0) = min_input;\n    Tensor* output_max = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(2, {}, &output_max));\n    output_max->flat<float>()(0) = max_input;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedAvgPool\").Device(DEVICE_CPU).TypeConstraint<quint8>(\"T\"),\n    QuantizedAvgPoolingOp<CPUDevice, quint8>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedMaxPool\").Device(DEVICE_CPU).TypeConstraint<quint8>(\"T\"),\n    QuantizedMaxPoolingOp<CPUDevice, quint8>);\n\n#ifdef INTEL_MKL\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedAvgPool\").Device(DEVICE_CPU).TypeConstraint<qint8>(\"T\"),\n    QuantizedAvgPoolingOp<CPUDevice, qint8>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"QuantizedMaxPool\").Device(DEVICE_CPU).TypeConstraint<qint8>(\"T\"),\n    QuantizedMaxPoolingOp<CPUDevice, qint8>);\n#endif\n\n}  // namespace tensorflow\n", "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/allocator.h\"\n#include \"tensorflow/core/framework/fake_input.h\"\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/kernels/ops_testutil.h\"\n#include \"tensorflow/core/kernels/ops_util.h\"\n#include \"tensorflow/core/kernels/quantization_utils.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nclass QuantizedPoolingTest : public OpsTestBase {\n protected:\n};\n\nTEST_F(QuantizedPoolingTest, SmallAveragePooling) {\n  const int ksize = 2;\n  const int stride = 2;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_avg_pool_op\", \"QuantizedAvgPool\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n                   .Attr(\"ksize\", {1, ksize, ksize, 1})\n                   .Attr(\"strides\", {1, stride, stride, 1})\n                   .Attr(\"padding\", \"SAME\")\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  const float input_min = 0.0f;\n  const float input_max = 255.0f;\n  const int input_height = 4;\n  const int input_width = 4;\n  const int input_channels = 2;\n  Tensor input_float(DT_FLOAT, {1, input_height, input_width, input_channels});\n  test::FillValues<float>(\n      &input_float,\n      {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32});\n  Tensor input_quantized =\n      FloatTensorToQuantized<quint8>(input_float, input_min, input_max);\n\n  const int expected_width = input_width / stride;\n  const int expected_height = input_height / stride;\n  Tensor expected_float(DT_FLOAT,\n                        {1, expected_height, expected_width, input_channels});\n  test::FillValues<float>(&expected_float, {6, 7, 10, 11, 22, 23, 26, 27});\n\n  AddInputFromArray<quint8>(input_quantized.shape(),\n                            input_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({}), {input_min});\n  AddInputFromArray<float>(TensorShape({}), {input_max});\n  TF_ASSERT_OK(RunOpKernel());\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<quint8>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 0.2);\n}\n\nTEST_F(QuantizedPoolingTest, SmallMaxPooling) {\n  const int ksize = 2;\n  const int stride = 2;\n  TF_ASSERT_OK(NodeDefBuilder(\"quantized_max_pool_op\", \"QuantizedMaxPool\")\n                   .Input(FakeInput(DT_QUINT8))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Input(FakeInput(DT_FLOAT))\n                   .Attr(\"T\", DataTypeToEnum<quint8>::v())\n                   .Attr(\"ksize\", {1, ksize, ksize, 1})\n                   .Attr(\"strides\", {1, stride, stride, 1})\n                   .Attr(\"padding\", \"SAME\")\n                   .Finalize(node_def()));\n  TF_ASSERT_OK(InitOp());\n  const float input_min = 0.0f;\n  const float input_max = 255.0f;\n  const int input_height = 4;\n  const int input_width = 4;\n  const int input_channels = 2;\n  Tensor input_float(DT_FLOAT, {1, input_height, input_width, input_channels});\n  test::FillValues<float>(\n      &input_float,\n      {1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32});\n  Tensor input_quantized =\n      FloatTensorToQuantized<quint8>(input_float, input_min, input_max);\n\n  const int expected_width = input_width / stride;\n  const int expected_height = input_height / stride;\n  Tensor expected_float(DT_FLOAT,\n                        {1, expected_height, expected_width, input_channels});\n  test::FillValues<float>(&expected_float, {11, 12, 15, 16, 27, 28, 31, 32});\n\n  AddInputFromArray<quint8>(input_quantized.shape(),\n                            input_quantized.flat<quint8>());\n  AddInputFromArray<float>(TensorShape({}), {input_min});\n  AddInputFromArray<float>(TensorShape({}), {input_max});\n  TF_ASSERT_OK(RunOpKernel());\n  const Tensor& output_quantized = *GetOutput(0);\n  const float output_min = GetOutput(1)->flat<float>()(0);\n  const float output_max = GetOutput(2)->flat<float>()(0);\n  Tensor output_float =\n      QuantizedTensorToFloat<quint8>(output_quantized, output_min, output_max);\n  test::ExpectTensorNear<float>(expected_float, output_float, 0.2);\n}\n\n}  // namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for tf.quantize ops.\"\"\"\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.platform import googletest\n\n\nclass FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars(\n              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))\n\n\nclass FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[[0.0]], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0, 0.1], max=[1.0]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 1\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[1.0], max=[[1.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"Dimensions must be equal|incorrect size\"):\n      self.evaluate(\n          array_ops.fake_quant_with_min_max_vars_per_channel(\n              inputs=inputs, min=[0.0], max=[1.0, 1.1]))\n\n\nclass QuantizedBiasedAddTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)\n    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=[],\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=[],\n              min_bias=0.0,\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=[],\n              max_bias=1.0,\n              out_type=dtypes.qint32))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_bias_add(\n              input=inputs,\n              bias=bias,\n              min_input=0.0,\n              max_input=1.0,\n              min_bias=0.0,\n              max_bias=[],\n              out_type=dtypes.qint32))\n\n\nclass QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          array_ops.quantized_instance_norm(\n              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))\n\n\nclass QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_avg_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    ksize = [1, 1, 1, 1]\n    strides = [1, 1, 1, 1]\n    padding = \"SAME\"\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=[],\n              max_input=1.0,\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),\n                                \"must be.* rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_max_pool(\n              input=inputs,\n              min_input=0.0,\n              max_input=[],\n              ksize=ksize,\n              strides=strides,\n              padding=padding))\n\n\nclass RequantizeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=[],\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=[],\n              requested_output_min=0.0,\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=[],\n              requested_output_max=1.0,\n              out_type=dtypes.qint8))\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.requantize(\n              input=inputs,\n              input_min=0.0,\n              input_max=1.0,\n              requested_output_min=0.0,\n              requested_output_max=[],\n              out_type=dtypes.qint8))\n\n\nclass QuantizedAddOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    x = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantized_add(\n              x=x,\n              y=y,\n              min_x=[],\n              max_x=1.0,\n              min_y=0.0,\n              max_y=1.0,\n              Toutput=dtypes.qint32))\n\n\nclass QuantizedReluOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizedRelu6OpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          nn_ops.quantized_relu6(\n              features=inputs,\n              min_features=[],\n              max_features=127.0,\n              out_type=dtypes.quint8))\n\n\nclass QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_in_graph_and_eager_modes\n  def test_invalid_inputs(self):\n    inputs = constant_op.constant(\n        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)\n\n    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),\n                                \"must be rank 0\"):\n      self.evaluate(\n          math_ops.quantize_down_and_shrink_range(input=inputs,\n                                                  input_min=[],\n                                                  input_max=4.0,\n                                                  out_type=dtypes.quint8))\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/quantized_pooling_ops.cc", "tensorflow/core/kernels/quantized_pooling_ops_test.cc", "tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py"], "buggy_code_start_loc": [18, 72, 156], "buggy_code_end_loc": [136, 119, 156], "fixing_code_start_loc": [17, 72, 157], "fixing_code_end_loc": [148, 119, 223], "type": "NVD-CWE-noinfo", "message": "TensorFlow is an open source platform for machine learning. If `QuantizedAvgPool` is given `min_input` or `max_input` tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 7cdf9d4d2083b739ec81cfdace546b0c99f50622. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35966", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T21:15:09.033", "lastModified": "2022-09-20T20:06:36.873", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. If `QuantizedAvgPool` is given `min_input` or `max_input` tensors of a nonzero rank, it results in a segfault that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 7cdf9d4d2083b739ec81cfdace546b0c99f50622. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Si a \"QuantizedAvgPool\" le son dados tensores \"min_input\" o \"max_input\" de un rango distinto de cero, es producido un segfault que puede usarse para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit 7cdf9d4d2083b739ec81cfdace546b0c99f50622 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/7cdf9d4d2083b739ec81cfdace546b0c99f50622", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-4w68-4x85-mjj9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/7cdf9d4d2083b739ec81cfdace546b0c99f50622"}}