{"buggy_code": ["#include <linux/gfp.h>\n#include <linux/initrd.h>\n#include <linux/ioport.h>\n#include <linux/swap.h>\n#include <linux/memblock.h>\n#include <linux/bootmem.h>\t/* for max_low_pfn */\n\n#include <asm/cacheflush.h>\n#include <asm/e820.h>\n#include <asm/init.h>\n#include <asm/page.h>\n#include <asm/page_types.h>\n#include <asm/sections.h>\n#include <asm/setup.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <asm/proto.h>\n#include <asm/dma.h>\t\t/* for MAX_DMA_PFN */\n#include <asm/microcode.h>\n#include <asm/kaslr.h>\n\n/*\n * We need to define the tracepoints somewhere, and tlb.c\n * is only compied when SMP=y.\n */\n#define CREATE_TRACE_POINTS\n#include <trace/events/tlb.h>\n\n#include \"mm_internal.h\"\n\n/*\n * Tables translating between page_cache_type_t and pte encoding.\n *\n * The default values are defined statically as minimal supported mode;\n * WC and WT fall back to UC-.  pat_init() updates these values to support\n * more cache modes, WC and WT, when it is safe to do so.  See pat_init()\n * for the details.  Note, __early_ioremap() used during early boot-time\n * takes pgprot_t (pte encoding) and does not use these tables.\n *\n *   Index into __cachemode2pte_tbl[] is the cachemode.\n *\n *   Index into __pte2cachemode_tbl[] are the caching attribute bits of the pte\n *   (_PAGE_PWT, _PAGE_PCD, _PAGE_PAT) at index bit positions 0, 1, 2.\n */\nuint16_t __cachemode2pte_tbl[_PAGE_CACHE_MODE_NUM] = {\n\t[_PAGE_CACHE_MODE_WB      ]\t= 0         | 0        ,\n\t[_PAGE_CACHE_MODE_WC      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC_MINUS]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC      ]\t= _PAGE_PWT | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WT      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WP      ]\t= 0         | _PAGE_PCD,\n};\nEXPORT_SYMBOL(__cachemode2pte_tbl);\n\nuint8_t __pte2cachemode_tbl[8] = {\n\t[__pte2cm_idx( 0        | 0         | 0        )] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx( 0        | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC,\n\t[__pte2cm_idx( 0        | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(0         | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC,\n};\nEXPORT_SYMBOL(__pte2cachemode_tbl);\n\nstatic unsigned long __initdata pgt_buf_start;\nstatic unsigned long __initdata pgt_buf_end;\nstatic unsigned long __initdata pgt_buf_top;\n\nstatic unsigned long min_pfn_mapped;\n\nstatic bool __initdata can_use_brk_pgt = true;\n\n/*\n * Pages returned are already directly mapped.\n *\n * Changing that is likely to break Xen, see commit:\n *\n *    279b706 x86,xen: introduce x86_init.mapping.pagetable_reserve\n *\n * for detailed information.\n */\n__ref void *alloc_low_pages(unsigned int num)\n{\n\tunsigned long pfn;\n\tint i;\n\n\tif (after_bootmem) {\n\t\tunsigned int order;\n\n\t\torder = get_order((unsigned long)num << PAGE_SHIFT);\n\t\treturn (void *)__get_free_pages(GFP_ATOMIC | __GFP_NOTRACK |\n\t\t\t\t\t\t__GFP_ZERO, order);\n\t}\n\n\tif ((pgt_buf_end + num) > pgt_buf_top || !can_use_brk_pgt) {\n\t\tunsigned long ret;\n\t\tif (min_pfn_mapped >= max_pfn_mapped)\n\t\t\tpanic(\"alloc_low_pages: ran out of memory\");\n\t\tret = memblock_find_in_range(min_pfn_mapped << PAGE_SHIFT,\n\t\t\t\t\tmax_pfn_mapped << PAGE_SHIFT,\n\t\t\t\t\tPAGE_SIZE * num , PAGE_SIZE);\n\t\tif (!ret)\n\t\t\tpanic(\"alloc_low_pages: can not alloc memory\");\n\t\tmemblock_reserve(ret, PAGE_SIZE * num);\n\t\tpfn = ret >> PAGE_SHIFT;\n\t} else {\n\t\tpfn = pgt_buf_end;\n\t\tpgt_buf_end += num;\n\t\tprintk(KERN_DEBUG \"BRK [%#010lx, %#010lx] PGTABLE\\n\",\n\t\t\tpfn << PAGE_SHIFT, (pgt_buf_end << PAGE_SHIFT) - 1);\n\t}\n\n\tfor (i = 0; i < num; i++) {\n\t\tvoid *adr;\n\n\t\tadr = __va((pfn + i) << PAGE_SHIFT);\n\t\tclear_page(adr);\n\t}\n\n\treturn __va(pfn << PAGE_SHIFT);\n}\n\n/*\n * By default need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS.\n * With KASLR memory randomization, depending on the machine e820 memory\n * and the PUD alignment. We may need twice more pages when KASLR memory\n * randomization is enabled.\n */\n#ifndef CONFIG_RANDOMIZE_MEMORY\n#define INIT_PGD_PAGE_COUNT      6\n#else\n#define INIT_PGD_PAGE_COUNT      12\n#endif\n#define INIT_PGT_BUF_SIZE\t(INIT_PGD_PAGE_COUNT * PAGE_SIZE)\nRESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);\nvoid  __init early_alloc_pgt_buf(void)\n{\n\tunsigned long tables = INIT_PGT_BUF_SIZE;\n\tphys_addr_t base;\n\n\tbase = __pa(extend_brk(tables, PAGE_SIZE));\n\n\tpgt_buf_start = base >> PAGE_SHIFT;\n\tpgt_buf_end = pgt_buf_start;\n\tpgt_buf_top = pgt_buf_start + (tables >> PAGE_SHIFT);\n}\n\nint after_bootmem;\n\nearly_param_on_off(\"gbpages\", \"nogbpages\", direct_gbpages, CONFIG_X86_DIRECT_GBPAGES);\n\nstruct map_range {\n\tunsigned long start;\n\tunsigned long end;\n\tunsigned page_size_mask;\n};\n\nstatic int page_size_mask;\n\nstatic void __init probe_page_size_mask(void)\n{\n#if !defined(CONFIG_KMEMCHECK)\n\t/*\n\t * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will\n\t * use small pages.\n\t * This will simplify cpa(), which otherwise needs to support splitting\n\t * large pages into small in interrupt context, etc.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_PSE) && !debug_pagealloc_enabled())\n\t\tpage_size_mask |= 1 << PG_LEVEL_2M;\n#endif\n\n\t/* Enable PSE if available */\n\tif (boot_cpu_has(X86_FEATURE_PSE))\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PSE);\n\n\t/* Enable PGE if available */\n\tif (boot_cpu_has(X86_FEATURE_PGE)) {\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PGE);\n\t\t__supported_pte_mask |= _PAGE_GLOBAL;\n\t} else\n\t\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/* Enable 1 GB linear kernel mappings if available: */\n\tif (direct_gbpages && boot_cpu_has(X86_FEATURE_GBPAGES)) {\n\t\tprintk(KERN_INFO \"Using GB pages for direct mapping\\n\");\n\t\tpage_size_mask |= 1 << PG_LEVEL_1G;\n\t} else {\n\t\tdirect_gbpages = 0;\n\t}\n}\n\n#ifdef CONFIG_X86_32\n#define NR_RANGE_MR 3\n#else /* CONFIG_X86_64 */\n#define NR_RANGE_MR 5\n#endif\n\nstatic int __meminit save_mr(struct map_range *mr, int nr_range,\n\t\t\t     unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t     unsigned long page_size_mask)\n{\n\tif (start_pfn < end_pfn) {\n\t\tif (nr_range >= NR_RANGE_MR)\n\t\t\tpanic(\"run out of range for init_memory_mapping\\n\");\n\t\tmr[nr_range].start = start_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].end   = end_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].page_size_mask = page_size_mask;\n\t\tnr_range++;\n\t}\n\n\treturn nr_range;\n}\n\n/*\n * adjust the page_size_mask for small range to go with\n *\tbig page size instead small one if nearby are ram too.\n */\nstatic void __ref adjust_range_page_size_mask(struct map_range *mr,\n\t\t\t\t\t\t\t int nr_range)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_range; i++) {\n\t\tif ((page_size_mask & (1<<PG_LEVEL_2M)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_2M))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PMD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PMD_SIZE);\n\n#ifdef CONFIG_X86_32\n\t\t\tif ((end >> PAGE_SHIFT) > max_low_pfn)\n\t\t\t\tcontinue;\n#endif\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_2M;\n\t\t}\n\t\tif ((page_size_mask & (1<<PG_LEVEL_1G)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_1G))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PUD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PUD_SIZE);\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_1G;\n\t\t}\n\t}\n}\n\nstatic const char *page_size_string(struct map_range *mr)\n{\n\tstatic const char str_1g[] = \"1G\";\n\tstatic const char str_2m[] = \"2M\";\n\tstatic const char str_4m[] = \"4M\";\n\tstatic const char str_4k[] = \"4k\";\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_1G))\n\t\treturn str_1g;\n\t/*\n\t * 32-bit without PAE has a 4M large page size.\n\t * PG_LEVEL_2M is misnamed, but we can at least\n\t * print out the right size in the string.\n\t */\n\tif (IS_ENABLED(CONFIG_X86_32) &&\n\t    !IS_ENABLED(CONFIG_X86_PAE) &&\n\t    mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_4m;\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_2m;\n\n\treturn str_4k;\n}\n\nstatic int __meminit split_mem_range(struct map_range *mr, int nr_range,\n\t\t\t\t     unsigned long start,\n\t\t\t\t     unsigned long end)\n{\n\tunsigned long start_pfn, end_pfn, limit_pfn;\n\tunsigned long pfn;\n\tint i;\n\n\tlimit_pfn = PFN_DOWN(end);\n\n\t/* head if not big page alignment ? */\n\tpfn = start_pfn = PFN_DOWN(start);\n#ifdef CONFIG_X86_32\n\t/*\n\t * Don't use a large page for the first 2/4MB of memory\n\t * because there are often fixed size MTRRs in there\n\t * and overlapping MTRRs into large pages can cause\n\t * slowdowns.\n\t */\n\tif (pfn == 0)\n\t\tend_pfn = PFN_DOWN(PMD_SIZE);\n\telse\n\t\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#else /* CONFIG_X86_64 */\n\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\tif (end_pfn > limit_pfn)\n\t\tend_pfn = limit_pfn;\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\t\tpfn = end_pfn;\n\t}\n\n\t/* big page (2M) range */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#ifdef CONFIG_X86_32\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#else /* CONFIG_X86_64 */\n\tend_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tif (end_pfn > round_down(limit_pfn, PFN_DOWN(PMD_SIZE)))\n\t\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n\n#ifdef CONFIG_X86_64\n\t/* big page (1G) range */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PUD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask &\n\t\t\t\t ((1<<PG_LEVEL_2M)|(1<<PG_LEVEL_1G)));\n\t\tpfn = end_pfn;\n\t}\n\n\t/* tail is not big page (1G) alignment */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n#endif\n\n\t/* tail is not big page (2M) alignment */\n\tstart_pfn = pfn;\n\tend_pfn = limit_pfn;\n\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\n\tif (!after_bootmem)\n\t\tadjust_range_page_size_mask(mr, nr_range);\n\n\t/* try to merge same page size and continuous */\n\tfor (i = 0; nr_range > 1 && i < nr_range - 1; i++) {\n\t\tunsigned long old_start;\n\t\tif (mr[i].end != mr[i+1].start ||\n\t\t    mr[i].page_size_mask != mr[i+1].page_size_mask)\n\t\t\tcontinue;\n\t\t/* move it */\n\t\told_start = mr[i].start;\n\t\tmemmove(&mr[i], &mr[i+1],\n\t\t\t(nr_range - 1 - i) * sizeof(struct map_range));\n\t\tmr[i--].start = old_start;\n\t\tnr_range--;\n\t}\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tpr_debug(\" [mem %#010lx-%#010lx] page %s\\n\",\n\t\t\t\tmr[i].start, mr[i].end - 1,\n\t\t\t\tpage_size_string(&mr[i]));\n\n\treturn nr_range;\n}\n\nstruct range pfn_mapped[E820_X_MAX];\nint nr_pfn_mapped;\n\nstatic void add_pfn_range_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tnr_pfn_mapped = add_range_with_merge(pfn_mapped, E820_X_MAX,\n\t\t\t\t\t     nr_pfn_mapped, start_pfn, end_pfn);\n\tnr_pfn_mapped = clean_sort_range(pfn_mapped, E820_X_MAX);\n\n\tmax_pfn_mapped = max(max_pfn_mapped, end_pfn);\n\n\tif (start_pfn < (1UL<<(32-PAGE_SHIFT)))\n\t\tmax_low_pfn_mapped = max(max_low_pfn_mapped,\n\t\t\t\t\t min(end_pfn, 1UL<<(32-PAGE_SHIFT)));\n}\n\nbool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_pfn_mapped; i++)\n\t\tif ((start_pfn >= pfn_mapped[i].start) &&\n\t\t    (end_pfn <= pfn_mapped[i].end))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Setup the direct mapping of the physical memory at PAGE_OFFSET.\n * This runs before bootmem is initialized and gets pages directly from\n * the physical memory. To access them they are temporarily mapped.\n */\nunsigned long __ref init_memory_mapping(unsigned long start,\n\t\t\t\t\t       unsigned long end)\n{\n\tstruct map_range mr[NR_RANGE_MR];\n\tunsigned long ret = 0;\n\tint nr_range, i;\n\n\tpr_debug(\"init_memory_mapping: [mem %#010lx-%#010lx]\\n\",\n\t       start, end - 1);\n\n\tmemset(mr, 0, sizeof(mr));\n\tnr_range = split_mem_range(mr, 0, start, end);\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tret = kernel_physical_mapping_init(mr[i].start, mr[i].end,\n\t\t\t\t\t\t   mr[i].page_size_mask);\n\n\tadd_pfn_range_mapped(start >> PAGE_SHIFT, ret >> PAGE_SHIFT);\n\n\treturn ret >> PAGE_SHIFT;\n}\n\n/*\n * We need to iterate through the E820 memory map and create direct mappings\n * for only E820_RAM and E820_KERN_RESERVED regions. We cannot simply\n * create direct mappings for all pfns from [0 to max_low_pfn) and\n * [4GB to max_pfn) because of possible memory holes in high addresses\n * that cannot be marked as UC by fixed/variable range MTRRs.\n * Depending on the alignment of E820 ranges, this may possibly result\n * in using smaller size (i.e. 4K instead of 2M or 1G) page tables.\n *\n * init_mem_mapping() calls init_range_memory_mapping() with big range.\n * That range would have hole in the middle or ends, and only ram parts\n * will be mapped in init_range_memory_mapping().\n */\nstatic unsigned long __init init_range_memory_mapping(\n\t\t\t\t\t   unsigned long r_start,\n\t\t\t\t\t   unsigned long r_end)\n{\n\tunsigned long start_pfn, end_pfn;\n\tunsigned long mapped_ram_size = 0;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, NULL) {\n\t\tu64 start = clamp_val(PFN_PHYS(start_pfn), r_start, r_end);\n\t\tu64 end = clamp_val(PFN_PHYS(end_pfn), r_start, r_end);\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * if it is overlapping with brk pgt, we need to\n\t\t * alloc pgt buf from memblock instead.\n\t\t */\n\t\tcan_use_brk_pgt = max(start, (u64)pgt_buf_end<<PAGE_SHIFT) >=\n\t\t\t\t    min(end, (u64)pgt_buf_top<<PAGE_SHIFT);\n\t\tinit_memory_mapping(start, end);\n\t\tmapped_ram_size += end - start;\n\t\tcan_use_brk_pgt = true;\n\t}\n\n\treturn mapped_ram_size;\n}\n\nstatic unsigned long __init get_new_step_size(unsigned long step_size)\n{\n\t/*\n\t * Initial mapped size is PMD_SIZE (2M).\n\t * We can not set step_size to be PUD_SIZE (1G) yet.\n\t * In worse case, when we cross the 1G boundary, and\n\t * PG_LEVEL_2M is not set, we will need 1+1+512 pages (2M + 8k)\n\t * to map 1G range with PTE. Hence we use one less than the\n\t * difference of page table level shifts.\n\t *\n\t * Don't need to worry about overflow in the top-down case, on 32bit,\n\t * when step_size is 0, round_down() returns 0 for start, and that\n\t * turns it into 0x100000000ULL.\n\t * In the bottom-up case, round_up(x, 0) returns 0 though too, which\n\t * needs to be taken into consideration by the code below.\n\t */\n\treturn step_size << (PMD_SHIFT - PAGE_SHIFT - 1);\n}\n\n/**\n * memory_map_top_down - Map [map_start, map_end) top down\n * @map_start: start address of the target memory range\n * @map_end: end address of the target memory range\n *\n * This function will setup direct mapping for memory range\n * [map_start, map_end) in top-down. That said, the page tables\n * will be allocated at the end of the memory, and we map the\n * memory in top-down.\n */\nstatic void __init memory_map_top_down(unsigned long map_start,\n\t\t\t\t       unsigned long map_end)\n{\n\tunsigned long real_end, start, last_start;\n\tunsigned long step_size;\n\tunsigned long addr;\n\tunsigned long mapped_ram_size = 0;\n\n\t/* xen has big range in reserved near end of ram, skip it at first.*/\n\taddr = memblock_find_in_range(map_start, map_end, PMD_SIZE, PMD_SIZE);\n\treal_end = addr + PMD_SIZE;\n\n\t/* step_size need to be small so pgt_buf from BRK could cover it */\n\tstep_size = PMD_SIZE;\n\tmax_pfn_mapped = 0; /* will get exact value next */\n\tmin_pfn_mapped = real_end >> PAGE_SHIFT;\n\tlast_start = start = real_end;\n\n\t/*\n\t * We start from the top (end of memory) and go to the bottom.\n\t * The memblock_find_in_range() gets us a block of RAM from the\n\t * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages\n\t * for page table.\n\t */\n\twhile (last_start > map_start) {\n\t\tif (last_start > step_size) {\n\t\t\tstart = round_down(last_start - 1, step_size);\n\t\t\tif (start < map_start)\n\t\t\t\tstart = map_start;\n\t\t} else\n\t\t\tstart = map_start;\n\t\tmapped_ram_size += init_range_memory_mapping(start,\n\t\t\t\t\t\t\tlast_start);\n\t\tlast_start = start;\n\t\tmin_pfn_mapped = last_start >> PAGE_SHIFT;\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n\n\tif (real_end < map_end)\n\t\tinit_range_memory_mapping(real_end, map_end);\n}\n\n/**\n * memory_map_bottom_up - Map [map_start, map_end) bottom up\n * @map_start: start address of the target memory range\n * @map_end: end address of the target memory range\n *\n * This function will setup direct mapping for memory range\n * [map_start, map_end) in bottom-up. Since we have limited the\n * bottom-up allocation above the kernel, the page tables will\n * be allocated just above the kernel and we map the memory\n * in [map_start, map_end) in bottom-up.\n */\nstatic void __init memory_map_bottom_up(unsigned long map_start,\n\t\t\t\t\tunsigned long map_end)\n{\n\tunsigned long next, start;\n\tunsigned long mapped_ram_size = 0;\n\t/* step_size need to be small so pgt_buf from BRK could cover it */\n\tunsigned long step_size = PMD_SIZE;\n\n\tstart = map_start;\n\tmin_pfn_mapped = start >> PAGE_SHIFT;\n\n\t/*\n\t * We start from the bottom (@map_start) and go to the top (@map_end).\n\t * The memblock_find_in_range() gets us a block of RAM from the\n\t * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages\n\t * for page table.\n\t */\n\twhile (start < map_end) {\n\t\tif (step_size && map_end - start > step_size) {\n\t\t\tnext = round_up(start + 1, step_size);\n\t\t\tif (next > map_end)\n\t\t\t\tnext = map_end;\n\t\t} else {\n\t\t\tnext = map_end;\n\t\t}\n\n\t\tmapped_ram_size += init_range_memory_mapping(start, next);\n\t\tstart = next;\n\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n}\n\nvoid __init init_mem_mapping(void)\n{\n\tunsigned long end;\n\n\tprobe_page_size_mask();\n\n#ifdef CONFIG_X86_64\n\tend = max_pfn << PAGE_SHIFT;\n#else\n\tend = max_low_pfn << PAGE_SHIFT;\n#endif\n\n\t/* the ISA range is always mapped regardless of memory holes */\n\tinit_memory_mapping(0, ISA_END_ADDRESS);\n\n\t/* Init the trampoline, possibly with KASLR memory offset */\n\tinit_trampoline();\n\n\t/*\n\t * If the allocation is in bottom-up direction, we setup direct mapping\n\t * in bottom-up, otherwise we setup direct mapping in top-down.\n\t */\n\tif (memblock_bottom_up()) {\n\t\tunsigned long kernel_end = __pa_symbol(_end);\n\n\t\t/*\n\t\t * we need two separate calls here. This is because we want to\n\t\t * allocate page tables above the kernel. So we first map\n\t\t * [kernel_end, end) to make memory above the kernel be mapped\n\t\t * as soon as possible. And then use page tables allocated above\n\t\t * the kernel to map [ISA_END_ADDRESS, kernel_end).\n\t\t */\n\t\tmemory_map_bottom_up(kernel_end, end);\n\t\tmemory_map_bottom_up(ISA_END_ADDRESS, kernel_end);\n\t} else {\n\t\tmemory_map_top_down(ISA_END_ADDRESS, end);\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (max_pfn > max_low_pfn) {\n\t\t/* can we preseve max_low_pfn ?*/\n\t\tmax_low_pfn = max_pfn;\n\t}\n#else\n\tearly_ioremap_page_table_range_init();\n#endif\n\n\tload_cr3(swapper_pg_dir);\n\t__flush_tlb_all();\n\n\tearly_memtest(0, max_pfn_mapped << PAGE_SHIFT);\n}\n\n/*\n * devmem_is_allowed() checks to see if /dev/mem access to a certain address\n * is valid. The argument is a physical page number.\n *\n *\n * On x86, access has to be given to the first megabyte of ram because that area\n * contains BIOS code and data regions used by X and dosemu and similar apps.\n * Access has to be given to non-kernel-ram areas as well, these contain the PCI\n * mmio resources as well as potential bios/acpi data regions.\n */\nint devmem_is_allowed(unsigned long pagenr)\n{\n\tif (pagenr < 256)\n\t\treturn 1;\n\tif (iomem_is_exclusive(pagenr << PAGE_SHIFT))\n\t\treturn 0;\n\tif (!page_is_ram(pagenr))\n\t\treturn 1;\n\treturn 0;\n}\n\nvoid free_init_pages(char *what, unsigned long begin, unsigned long end)\n{\n\tunsigned long begin_aligned, end_aligned;\n\n\t/* Make sure boundaries are page aligned */\n\tbegin_aligned = PAGE_ALIGN(begin);\n\tend_aligned   = end & PAGE_MASK;\n\n\tif (WARN_ON(begin_aligned != begin || end_aligned != end)) {\n\t\tbegin = begin_aligned;\n\t\tend   = end_aligned;\n\t}\n\n\tif (begin >= end)\n\t\treturn;\n\n\t/*\n\t * If debugging page accesses then do not free this memory but\n\t * mark them not present - any buggy init-section access will\n\t * create a kernel page fault:\n\t */\n\tif (debug_pagealloc_enabled()) {\n\t\tpr_info(\"debug: unmapping init [mem %#010lx-%#010lx]\\n\",\n\t\t\tbegin, end - 1);\n\t\tset_memory_np(begin, (end - begin) >> PAGE_SHIFT);\n\t} else {\n\t\t/*\n\t\t * We just marked the kernel text read only above, now that\n\t\t * we are going to free part of that, we need to make that\n\t\t * writeable and non-executable first.\n\t\t */\n\t\tset_memory_nx(begin, (end - begin) >> PAGE_SHIFT);\n\t\tset_memory_rw(begin, (end - begin) >> PAGE_SHIFT);\n\n\t\tfree_reserved_area((void *)begin, (void *)end,\n\t\t\t\t   POISON_FREE_INITMEM, what);\n\t}\n}\n\nvoid __ref free_initmem(void)\n{\n\te820_reallocate_tables();\n\n\tfree_init_pages(\"unused kernel\",\n\t\t\t(unsigned long)(&__init_begin),\n\t\t\t(unsigned long)(&__init_end));\n}\n\n#ifdef CONFIG_BLK_DEV_INITRD\nvoid __init free_initrd_mem(unsigned long start, unsigned long end)\n{\n\t/*\n\t * end could be not aligned, and We can not align that,\n\t * decompresser could be confused by aligned initrd_end\n\t * We already reserve the end partial page before in\n\t *   - i386_start_kernel()\n\t *   - x86_64_start_kernel()\n\t *   - relocate_initrd()\n\t * So here We can do PAGE_ALIGN() safely to get partial page to be freed\n\t */\n\tfree_init_pages(\"initrd\", start, PAGE_ALIGN(end));\n}\n#endif\n\nvoid __init zone_sizes_init(void)\n{\n\tunsigned long max_zone_pfns[MAX_NR_ZONES];\n\n\tmemset(max_zone_pfns, 0, sizeof(max_zone_pfns));\n\n#ifdef CONFIG_ZONE_DMA\n\tmax_zone_pfns[ZONE_DMA]\t\t= min(MAX_DMA_PFN, max_low_pfn);\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\tmax_zone_pfns[ZONE_DMA32]\t= min(MAX_DMA32_PFN, max_low_pfn);\n#endif\n\tmax_zone_pfns[ZONE_NORMAL]\t= max_low_pfn;\n#ifdef CONFIG_HIGHMEM\n\tmax_zone_pfns[ZONE_HIGHMEM]\t= max_pfn;\n#endif\n\n\tfree_area_init_nodes(max_zone_pfns);\n}\n\nDEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {\n#ifdef CONFIG_SMP\n\t.active_mm = &init_mm,\n\t.state = 0,\n#endif\n\t.cr4 = ~0UL,\t/* fail hard if we screw up cr4 shadow initialization */\n};\nEXPORT_SYMBOL_GPL(cpu_tlbstate);\n\nvoid update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)\n{\n\t/* entry 0 MUST be WB (hardwired to speed up translations) */\n\tBUG_ON(!entry && cache != _PAGE_CACHE_MODE_WB);\n\n\t__cachemode2pte_tbl[cache] = __cm_idx2pte(entry);\n\t__pte2cachemode_tbl[entry] = cache;\n}\n", "/*\n *  linux/drivers/char/mem.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Added devfs support.\n *    Jan-11-1998, C. Scott Ananian <cananian@alumni.princeton.edu>\n *  Shared /dev/zero mmapping support, Feb 2000, Kanoj Sarcar <kanoj@sgi.com>\n */\n\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/mman.h>\n#include <linux/random.h>\n#include <linux/init.h>\n#include <linux/raw.h>\n#include <linux/tty.h>\n#include <linux/capability.h>\n#include <linux/ptrace.h>\n#include <linux/device.h>\n#include <linux/highmem.h>\n#include <linux/backing-dev.h>\n#include <linux/shmem_fs.h>\n#include <linux/splice.h>\n#include <linux/pfn.h>\n#include <linux/export.h>\n#include <linux/io.h>\n#include <linux/uio.h>\n\n#include <linux/uaccess.h>\n\n#ifdef CONFIG_IA64\n# include <linux/efi.h>\n#endif\n\n#define DEVPORT_MINOR\t4\n\nstatic inline unsigned long size_inside_page(unsigned long start,\n\t\t\t\t\t     unsigned long size)\n{\n\tunsigned long sz;\n\n\tsz = PAGE_SIZE - (start & (PAGE_SIZE - 1));\n\n\treturn min(sz, size);\n}\n\n#ifndef ARCH_HAS_VALID_PHYS_ADDR_RANGE\nstatic inline int valid_phys_addr_range(phys_addr_t addr, size_t count)\n{\n\treturn addr + count <= __pa(high_memory);\n}\n\nstatic inline int valid_mmap_phys_addr_range(unsigned long pfn, size_t size)\n{\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_STRICT_DEVMEM\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\tu64 from = ((u64)pfn) << PAGE_SHIFT;\n\tu64 to = from + size;\n\tu64 cursor = from;\n\n\twhile (cursor < to) {\n\t\tif (!devmem_is_allowed(pfn))\n\t\t\treturn 0;\n\t\tcursor += PAGE_SIZE;\n\t\tpfn++;\n\t}\n\treturn 1;\n}\n#else\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\treturn 1;\n}\n#endif\n\n#ifndef unxlate_dev_mem_ptr\n#define unxlate_dev_mem_ptr unxlate_dev_mem_ptr\nvoid __weak unxlate_dev_mem_ptr(phys_addr_t phys, void *addr)\n{\n}\n#endif\n\n/*\n * This funcion reads the *physical* memory. The f_pos points directly to the\n * memory location.\n */\nstatic ssize_t read_mem(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t read, sz;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn 0;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\tread = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\tif (sz > 0) {\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tcount -= sz;\n\t\t\tread += sz;\n\t\t}\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tunsigned long remaining;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\tif (!range_is_allowed(p >> PAGE_SHIFT, count))\n\t\t\treturn -EPERM;\n\n\t\t/*\n\t\t * On ia64 if a page has been mapped somewhere as uncached, then\n\t\t * it must also be accessed uncached by the kernel or data\n\t\t * corruption may occur.\n\t\t */\n\t\tptr = xlate_dev_mem_ptr(p);\n\t\tif (!ptr)\n\t\t\treturn -EFAULT;\n\n\t\tremaining = copy_to_user(buf, ptr, sz);\n\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\tif (remaining)\n\t\t\treturn -EFAULT;\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\tread += sz;\n\t}\n\n\t*ppos += read;\n\treturn read;\n}\n\nstatic ssize_t write_mem(struct file *file, const char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t written, sz;\n\tunsigned long copied;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn -EFBIG;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\n\twritten = 0;\n\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\t/* Hmm. Do something? */\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tsz = size_inside_page(p, count);\n\n\t\tif (!range_is_allowed(p >> PAGE_SHIFT, sz))\n\t\t\treturn -EPERM;\n\n\t\t/*\n\t\t * On ia64 if a page has been mapped somewhere as uncached, then\n\t\t * it must also be accessed uncached by the kernel or data\n\t\t * corruption may occur.\n\t\t */\n\t\tptr = xlate_dev_mem_ptr(p);\n\t\tif (!ptr) {\n\t\t\tif (written)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tcopied = copy_from_user(ptr, buf, sz);\n\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\tif (copied) {\n\t\t\twritten += sz - copied;\n\t\t\tif (written)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n\n\t*ppos += written;\n\treturn written;\n}\n\nint __weak phys_mem_access_prot_allowed(struct file *file,\n\tunsigned long pfn, unsigned long size, pgprot_t *vma_prot)\n{\n\treturn 1;\n}\n\n#ifndef __HAVE_PHYS_MEM_ACCESS_PROT\n\n/*\n * Architectures vary in how they handle caching for addresses\n * outside of main memory.\n *\n */\n#ifdef pgprot_noncached\nstatic int uncached_access(struct file *file, phys_addr_t addr)\n{\n#if defined(CONFIG_IA64)\n\t/*\n\t * On ia64, we ignore O_DSYNC because we cannot tolerate memory\n\t * attribute aliases.\n\t */\n\treturn !(efi_mem_attributes(addr) & EFI_MEMORY_WB);\n#elif defined(CONFIG_MIPS)\n\t{\n\t\textern int __uncached_access(struct file *file,\n\t\t\t\t\t     unsigned long addr);\n\n\t\treturn __uncached_access(file, addr);\n\t}\n#else\n\t/*\n\t * Accessing memory above the top the kernel knows about or through a\n\t * file pointer\n\t * that was marked O_DSYNC will be done non-cached.\n\t */\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 1;\n\treturn addr >= __pa(high_memory);\n#endif\n}\n#endif\n\nstatic pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,\n\t\t\t\t     unsigned long size, pgprot_t vma_prot)\n{\n#ifdef pgprot_noncached\n\tphys_addr_t offset = pfn << PAGE_SHIFT;\n\n\tif (uncached_access(file, offset))\n\t\treturn pgprot_noncached(vma_prot);\n#endif\n\treturn vma_prot;\n}\n#endif\n\n#ifndef CONFIG_MMU\nstatic unsigned long get_unmapped_area_mem(struct file *file,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   unsigned long len,\n\t\t\t\t\t   unsigned long pgoff,\n\t\t\t\t\t   unsigned long flags)\n{\n\tif (!valid_mmap_phys_addr_range(pgoff, len))\n\t\treturn (unsigned long) -EINVAL;\n\treturn pgoff << PAGE_SHIFT;\n}\n\n/* permit direct mmap, for read, write or exec */\nstatic unsigned memory_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT |\n\t\tNOMMU_MAP_READ | NOMMU_MAP_WRITE | NOMMU_MAP_EXEC;\n}\n\nstatic unsigned zero_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_COPY;\n}\n\n/* can't do an in-place private mapping if there's no MMU */\nstatic inline int private_mapping_ok(struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & VM_MAYSHARE;\n}\n#else\n\nstatic inline int private_mapping_ok(struct vm_area_struct *vma)\n{\n\treturn 1;\n}\n#endif\n\nstatic const struct vm_operations_struct mmap_mem_ops = {\n#ifdef CONFIG_HAVE_IOREMAP_PROT\n\t.access = generic_access_phys\n#endif\n};\n\nstatic int mmap_mem(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t size = vma->vm_end - vma->vm_start;\n\n\tif (!valid_mmap_phys_addr_range(vma->vm_pgoff, size))\n\t\treturn -EINVAL;\n\n\tif (!private_mapping_ok(vma))\n\t\treturn -ENOSYS;\n\n\tif (!range_is_allowed(vma->vm_pgoff, size))\n\t\treturn -EPERM;\n\n\tif (!phys_mem_access_prot_allowed(file, vma->vm_pgoff, size,\n\t\t\t\t\t\t&vma->vm_page_prot))\n\t\treturn -EINVAL;\n\n\tvma->vm_page_prot = phys_mem_access_prot(file, vma->vm_pgoff,\n\t\t\t\t\t\t size,\n\t\t\t\t\t\t vma->vm_page_prot);\n\n\tvma->vm_ops = &mmap_mem_ops;\n\n\t/* Remap-pfn-range will mark the range VM_IO */\n\tif (remap_pfn_range(vma,\n\t\t\t    vma->vm_start,\n\t\t\t    vma->vm_pgoff,\n\t\t\t    size,\n\t\t\t    vma->vm_page_prot)) {\n\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic int mmap_kmem(struct file *file, struct vm_area_struct *vma)\n{\n\tunsigned long pfn;\n\n\t/* Turn a kernel-virtual address into a physical page frame */\n\tpfn = __pa((u64)vma->vm_pgoff << PAGE_SHIFT) >> PAGE_SHIFT;\n\n\t/*\n\t * RED-PEN: on some architectures there is more mapped memory than\n\t * available in mem_map which pfn_valid checks for. Perhaps should add a\n\t * new macro here.\n\t *\n\t * RED-PEN: vmalloc is not supported right now.\n\t */\n\tif (!pfn_valid(pfn))\n\t\treturn -EIO;\n\n\tvma->vm_pgoff = pfn;\n\treturn mmap_mem(file, vma);\n}\n\n/*\n * This function reads the *virtual* memory as seen by the kernel.\n */\nstatic ssize_t read_kmem(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tssize_t low_count, read, sz;\n\tchar *kbuf; /* k-addr because vread() takes vmlist_lock rwlock */\n\tint err = 0;\n\n\tread = 0;\n\tif (p < (unsigned long) high_memory) {\n\t\tlow_count = count;\n\t\tif (count > (unsigned long)high_memory - p)\n\t\t\tlow_count = (unsigned long)high_memory - p;\n\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t\t/* we don't have page 0 mapped on sparc and m68k.. */\n\t\tif (p < PAGE_SIZE && low_count > 0) {\n\t\t\tsz = size_inside_page(p, low_count);\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tread += sz;\n\t\t\tlow_count -= sz;\n\t\t\tcount -= sz;\n\t\t}\n#endif\n\t\twhile (low_count > 0) {\n\t\t\tsz = size_inside_page(p, low_count);\n\n\t\t\t/*\n\t\t\t * On ia64 if a page has been mapped somewhere as\n\t\t\t * uncached, then it must also be accessed uncached\n\t\t\t * by the kernel or data corruption may occur\n\t\t\t */\n\t\t\tkbuf = xlate_dev_kmem_ptr((void *)p);\n\t\t\tif (!virt_addr_valid(kbuf))\n\t\t\t\treturn -ENXIO;\n\n\t\t\tif (copy_to_user(buf, kbuf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tread += sz;\n\t\t\tlow_count -= sz;\n\t\t\tcount -= sz;\n\t\t}\n\t}\n\n\tif (count > 0) {\n\t\tkbuf = (char *)__get_free_page(GFP_KERNEL);\n\t\tif (!kbuf)\n\t\t\treturn -ENOMEM;\n\t\twhile (count > 0) {\n\t\t\tsz = size_inside_page(p, count);\n\t\t\tif (!is_vmalloc_or_module_addr((void *)p)) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsz = vread(kbuf, (char *)p, sz);\n\t\t\tif (!sz)\n\t\t\t\tbreak;\n\t\t\tif (copy_to_user(buf, kbuf, sz)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount -= sz;\n\t\t\tbuf += sz;\n\t\t\tread += sz;\n\t\t\tp += sz;\n\t\t}\n\t\tfree_page((unsigned long)kbuf);\n\t}\n\t*ppos = p;\n\treturn read ? read : err;\n}\n\n\nstatic ssize_t do_write_kmem(unsigned long p, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tssize_t written, sz;\n\tunsigned long copied;\n\n\twritten = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\t/* Hmm. Do something? */\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tvoid *ptr;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\t/*\n\t\t * On ia64 if a page has been mapped somewhere as uncached, then\n\t\t * it must also be accessed uncached by the kernel or data\n\t\t * corruption may occur.\n\t\t */\n\t\tptr = xlate_dev_kmem_ptr((void *)p);\n\t\tif (!virt_addr_valid(ptr))\n\t\t\treturn -ENXIO;\n\n\t\tcopied = copy_from_user(ptr, buf, sz);\n\t\tif (copied) {\n\t\t\twritten += sz - copied;\n\t\t\tif (written)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n\n\t*ppos += written;\n\treturn written;\n}\n\n/*\n * This function writes to the *virtual* memory as seen by the kernel.\n */\nstatic ssize_t write_kmem(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tssize_t wrote = 0;\n\tssize_t virtr = 0;\n\tchar *kbuf; /* k-addr because vwrite() takes vmlist_lock rwlock */\n\tint err = 0;\n\n\tif (p < (unsigned long) high_memory) {\n\t\tunsigned long to_write = min_t(unsigned long, count,\n\t\t\t\t\t       (unsigned long)high_memory - p);\n\t\twrote = do_write_kmem(p, buf, to_write, ppos);\n\t\tif (wrote != to_write)\n\t\t\treturn wrote;\n\t\tp += wrote;\n\t\tbuf += wrote;\n\t\tcount -= wrote;\n\t}\n\n\tif (count > 0) {\n\t\tkbuf = (char *)__get_free_page(GFP_KERNEL);\n\t\tif (!kbuf)\n\t\t\treturn wrote ? wrote : -ENOMEM;\n\t\twhile (count > 0) {\n\t\t\tunsigned long sz = size_inside_page(p, count);\n\t\t\tunsigned long n;\n\n\t\t\tif (!is_vmalloc_or_module_addr((void *)p)) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tn = copy_from_user(kbuf, buf, sz);\n\t\t\tif (n) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvwrite(kbuf, (char *)p, sz);\n\t\t\tcount -= sz;\n\t\t\tbuf += sz;\n\t\t\tvirtr += sz;\n\t\t\tp += sz;\n\t\t}\n\t\tfree_page((unsigned long)kbuf);\n\t}\n\n\t*ppos = p;\n\treturn virtr + wrote ? : err;\n}\n\nstatic ssize_t read_port(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tunsigned long i = *ppos;\n\tchar __user *tmp = buf;\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\twhile (count-- > 0 && i < 65536) {\n\t\tif (__put_user(inb(i), tmp) < 0)\n\t\t\treturn -EFAULT;\n\t\ti++;\n\t\ttmp++;\n\t}\n\t*ppos = i;\n\treturn tmp-buf;\n}\n\nstatic ssize_t write_port(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\tunsigned long i = *ppos;\n\tconst char __user *tmp = buf;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\n\twhile (count-- > 0 && i < 65536) {\n\t\tchar c;\n\n\t\tif (__get_user(c, tmp)) {\n\t\t\tif (tmp > buf)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\toutb(c, i);\n\t\ti++;\n\t\ttmp++;\n\t}\n\t*ppos = i;\n\treturn tmp-buf;\n}\n\nstatic ssize_t read_null(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\treturn 0;\n}\n\nstatic ssize_t write_null(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nstatic ssize_t read_iter_null(struct kiocb *iocb, struct iov_iter *to)\n{\n\treturn 0;\n}\n\nstatic ssize_t write_iter_null(struct kiocb *iocb, struct iov_iter *from)\n{\n\tsize_t count = iov_iter_count(from);\n\tiov_iter_advance(from, count);\n\treturn count;\n}\n\nstatic int pipe_to_null(struct pipe_inode_info *info, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\treturn sd->len;\n}\n\nstatic ssize_t splice_write_null(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t\t loff_t *ppos, size_t len, unsigned int flags)\n{\n\treturn splice_from_pipe(pipe, out, ppos, len, flags, pipe_to_null);\n}\n\nstatic ssize_t read_iter_zero(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tsize_t written = 0;\n\n\twhile (iov_iter_count(iter)) {\n\t\tsize_t chunk = iov_iter_count(iter), n;\n\n\t\tif (chunk > PAGE_SIZE)\n\t\t\tchunk = PAGE_SIZE;\t/* Just for latency reasons */\n\t\tn = iov_iter_zero(chunk, iter);\n\t\tif (!n && iov_iter_count(iter))\n\t\t\treturn written ? written : -EFAULT;\n\t\twritten += n;\n\t\tif (signal_pending(current))\n\t\t\treturn written ? written : -ERESTARTSYS;\n\t\tcond_resched();\n\t}\n\treturn written;\n}\n\nstatic int mmap_zero(struct file *file, struct vm_area_struct *vma)\n{\n#ifndef CONFIG_MMU\n\treturn -ENOSYS;\n#endif\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn shmem_zero_setup(vma);\n\treturn 0;\n}\n\nstatic unsigned long get_unmapped_area_zero(struct file *file,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags)\n{\n#ifdef CONFIG_MMU\n\tif (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_zero() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge;\n\t\t * and pass NULL for file as in mmap.c's get_unmapped_area(),\n\t\t * so as not to confuse shmem with our handle on \"/dev/zero\".\n\t\t */\n\t\treturn shmem_get_unmapped_area(NULL, addr, len, pgoff, flags);\n\t}\n\n\t/* Otherwise flags & MAP_PRIVATE: with no shmem object beneath it */\n\treturn current->mm->get_unmapped_area(file, addr, len, pgoff, flags);\n#else\n\treturn -ENOSYS;\n#endif\n}\n\nstatic ssize_t write_full(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\treturn -ENOSPC;\n}\n\n/*\n * Special lseek() function for /dev/null and /dev/zero.  Most notably, you\n * can fopen() both devices with \"a\" now.  This was previously impossible.\n * -- SRB.\n */\nstatic loff_t null_lseek(struct file *file, loff_t offset, int orig)\n{\n\treturn file->f_pos = 0;\n}\n\n/*\n * The memory devices use the full 32/64 bits of the offset, and so we cannot\n * check against negative addresses: they are ok. The return value is weird,\n * though, in that case (0).\n *\n * also note that seeking relative to the \"end of file\" isn't supported:\n * it has no meaning, so it returns -EINVAL.\n */\nstatic loff_t memory_lseek(struct file *file, loff_t offset, int orig)\n{\n\tloff_t ret;\n\n\tinode_lock(file_inode(file));\n\tswitch (orig) {\n\tcase SEEK_CUR:\n\t\toffset += file->f_pos;\n\tcase SEEK_SET:\n\t\t/* to avoid userland mistaking f_pos=-9 as -EBADF=-9 */\n\t\tif ((unsigned long long)offset >= -MAX_ERRNO) {\n\t\t\tret = -EOVERFLOW;\n\t\t\tbreak;\n\t\t}\n\t\tfile->f_pos = offset;\n\t\tret = file->f_pos;\n\t\tforce_successful_syscall_return();\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tinode_unlock(file_inode(file));\n\treturn ret;\n}\n\nstatic int open_port(struct inode *inode, struct file *filp)\n{\n\treturn capable(CAP_SYS_RAWIO) ? 0 : -EPERM;\n}\n\n#define zero_lseek\tnull_lseek\n#define full_lseek      null_lseek\n#define write_zero\twrite_null\n#define write_iter_zero\twrite_iter_null\n#define open_mem\topen_port\n#define open_kmem\topen_mem\n\nstatic const struct file_operations __maybe_unused mem_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_mem,\n\t.write\t\t= write_mem,\n\t.mmap\t\t= mmap_mem,\n\t.open\t\t= open_mem,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = get_unmapped_area_mem,\n\t.mmap_capabilities = memory_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations __maybe_unused kmem_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_kmem,\n\t.write\t\t= write_kmem,\n\t.mmap\t\t= mmap_kmem,\n\t.open\t\t= open_kmem,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = get_unmapped_area_mem,\n\t.mmap_capabilities = memory_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations null_fops = {\n\t.llseek\t\t= null_lseek,\n\t.read\t\t= read_null,\n\t.write\t\t= write_null,\n\t.read_iter\t= read_iter_null,\n\t.write_iter\t= write_iter_null,\n\t.splice_write\t= splice_write_null,\n};\n\nstatic const struct file_operations __maybe_unused port_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_port,\n\t.write\t\t= write_port,\n\t.open\t\t= open_port,\n};\n\nstatic const struct file_operations zero_fops = {\n\t.llseek\t\t= zero_lseek,\n\t.write\t\t= write_zero,\n\t.read_iter\t= read_iter_zero,\n\t.write_iter\t= write_iter_zero,\n\t.mmap\t\t= mmap_zero,\n\t.get_unmapped_area = get_unmapped_area_zero,\n#ifndef CONFIG_MMU\n\t.mmap_capabilities = zero_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations full_fops = {\n\t.llseek\t\t= full_lseek,\n\t.read_iter\t= read_iter_zero,\n\t.write\t\t= write_full,\n};\n\nstatic const struct memdev {\n\tconst char *name;\n\tumode_t mode;\n\tconst struct file_operations *fops;\n\tfmode_t fmode;\n} devlist[] = {\n#ifdef CONFIG_DEVMEM\n\t [1] = { \"mem\", 0, &mem_fops, FMODE_UNSIGNED_OFFSET },\n#endif\n#ifdef CONFIG_DEVKMEM\n\t [2] = { \"kmem\", 0, &kmem_fops, FMODE_UNSIGNED_OFFSET },\n#endif\n\t [3] = { \"null\", 0666, &null_fops, 0 },\n#ifdef CONFIG_DEVPORT\n\t [4] = { \"port\", 0, &port_fops, 0 },\n#endif\n\t [5] = { \"zero\", 0666, &zero_fops, 0 },\n\t [7] = { \"full\", 0666, &full_fops, 0 },\n\t [8] = { \"random\", 0666, &random_fops, 0 },\n\t [9] = { \"urandom\", 0666, &urandom_fops, 0 },\n#ifdef CONFIG_PRINTK\n\t[11] = { \"kmsg\", 0644, &kmsg_fops, 0 },\n#endif\n};\n\nstatic int memory_open(struct inode *inode, struct file *filp)\n{\n\tint minor;\n\tconst struct memdev *dev;\n\n\tminor = iminor(inode);\n\tif (minor >= ARRAY_SIZE(devlist))\n\t\treturn -ENXIO;\n\n\tdev = &devlist[minor];\n\tif (!dev->fops)\n\t\treturn -ENXIO;\n\n\tfilp->f_op = dev->fops;\n\tfilp->f_mode |= dev->fmode;\n\n\tif (dev->fops->open)\n\t\treturn dev->fops->open(inode, filp);\n\n\treturn 0;\n}\n\nstatic const struct file_operations memory_fops = {\n\t.open = memory_open,\n\t.llseek = noop_llseek,\n};\n\nstatic char *mem_devnode(struct device *dev, umode_t *mode)\n{\n\tif (mode && devlist[MINOR(dev->devt)].mode)\n\t\t*mode = devlist[MINOR(dev->devt)].mode;\n\treturn NULL;\n}\n\nstatic struct class *mem_class;\n\nstatic int __init chr_dev_init(void)\n{\n\tint minor;\n\n\tif (register_chrdev(MEM_MAJOR, \"mem\", &memory_fops))\n\t\tprintk(\"unable to get major %d for memory devs\\n\", MEM_MAJOR);\n\n\tmem_class = class_create(THIS_MODULE, \"mem\");\n\tif (IS_ERR(mem_class))\n\t\treturn PTR_ERR(mem_class);\n\n\tmem_class->devnode = mem_devnode;\n\tfor (minor = 1; minor < ARRAY_SIZE(devlist); minor++) {\n\t\tif (!devlist[minor].name)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Create /dev/port?\n\t\t */\n\t\tif ((minor == DEVPORT_MINOR) && !arch_has_dev_port())\n\t\t\tcontinue;\n\n\t\tdevice_create(mem_class, NULL, MKDEV(MEM_MAJOR, minor),\n\t\t\t      NULL, devlist[minor].name);\n\t}\n\n\treturn tty_init();\n}\n\nfs_initcall(chr_dev_init);\n"], "fixing_code": ["#include <linux/gfp.h>\n#include <linux/initrd.h>\n#include <linux/ioport.h>\n#include <linux/swap.h>\n#include <linux/memblock.h>\n#include <linux/bootmem.h>\t/* for max_low_pfn */\n\n#include <asm/cacheflush.h>\n#include <asm/e820.h>\n#include <asm/init.h>\n#include <asm/page.h>\n#include <asm/page_types.h>\n#include <asm/sections.h>\n#include <asm/setup.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <asm/proto.h>\n#include <asm/dma.h>\t\t/* for MAX_DMA_PFN */\n#include <asm/microcode.h>\n#include <asm/kaslr.h>\n\n/*\n * We need to define the tracepoints somewhere, and tlb.c\n * is only compied when SMP=y.\n */\n#define CREATE_TRACE_POINTS\n#include <trace/events/tlb.h>\n\n#include \"mm_internal.h\"\n\n/*\n * Tables translating between page_cache_type_t and pte encoding.\n *\n * The default values are defined statically as minimal supported mode;\n * WC and WT fall back to UC-.  pat_init() updates these values to support\n * more cache modes, WC and WT, when it is safe to do so.  See pat_init()\n * for the details.  Note, __early_ioremap() used during early boot-time\n * takes pgprot_t (pte encoding) and does not use these tables.\n *\n *   Index into __cachemode2pte_tbl[] is the cachemode.\n *\n *   Index into __pte2cachemode_tbl[] are the caching attribute bits of the pte\n *   (_PAGE_PWT, _PAGE_PCD, _PAGE_PAT) at index bit positions 0, 1, 2.\n */\nuint16_t __cachemode2pte_tbl[_PAGE_CACHE_MODE_NUM] = {\n\t[_PAGE_CACHE_MODE_WB      ]\t= 0         | 0        ,\n\t[_PAGE_CACHE_MODE_WC      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC_MINUS]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC      ]\t= _PAGE_PWT | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WT      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WP      ]\t= 0         | _PAGE_PCD,\n};\nEXPORT_SYMBOL(__cachemode2pte_tbl);\n\nuint8_t __pte2cachemode_tbl[8] = {\n\t[__pte2cm_idx( 0        | 0         | 0        )] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx( 0        | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC,\n\t[__pte2cm_idx( 0        | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(0         | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC,\n};\nEXPORT_SYMBOL(__pte2cachemode_tbl);\n\nstatic unsigned long __initdata pgt_buf_start;\nstatic unsigned long __initdata pgt_buf_end;\nstatic unsigned long __initdata pgt_buf_top;\n\nstatic unsigned long min_pfn_mapped;\n\nstatic bool __initdata can_use_brk_pgt = true;\n\n/*\n * Pages returned are already directly mapped.\n *\n * Changing that is likely to break Xen, see commit:\n *\n *    279b706 x86,xen: introduce x86_init.mapping.pagetable_reserve\n *\n * for detailed information.\n */\n__ref void *alloc_low_pages(unsigned int num)\n{\n\tunsigned long pfn;\n\tint i;\n\n\tif (after_bootmem) {\n\t\tunsigned int order;\n\n\t\torder = get_order((unsigned long)num << PAGE_SHIFT);\n\t\treturn (void *)__get_free_pages(GFP_ATOMIC | __GFP_NOTRACK |\n\t\t\t\t\t\t__GFP_ZERO, order);\n\t}\n\n\tif ((pgt_buf_end + num) > pgt_buf_top || !can_use_brk_pgt) {\n\t\tunsigned long ret;\n\t\tif (min_pfn_mapped >= max_pfn_mapped)\n\t\t\tpanic(\"alloc_low_pages: ran out of memory\");\n\t\tret = memblock_find_in_range(min_pfn_mapped << PAGE_SHIFT,\n\t\t\t\t\tmax_pfn_mapped << PAGE_SHIFT,\n\t\t\t\t\tPAGE_SIZE * num , PAGE_SIZE);\n\t\tif (!ret)\n\t\t\tpanic(\"alloc_low_pages: can not alloc memory\");\n\t\tmemblock_reserve(ret, PAGE_SIZE * num);\n\t\tpfn = ret >> PAGE_SHIFT;\n\t} else {\n\t\tpfn = pgt_buf_end;\n\t\tpgt_buf_end += num;\n\t\tprintk(KERN_DEBUG \"BRK [%#010lx, %#010lx] PGTABLE\\n\",\n\t\t\tpfn << PAGE_SHIFT, (pgt_buf_end << PAGE_SHIFT) - 1);\n\t}\n\n\tfor (i = 0; i < num; i++) {\n\t\tvoid *adr;\n\n\t\tadr = __va((pfn + i) << PAGE_SHIFT);\n\t\tclear_page(adr);\n\t}\n\n\treturn __va(pfn << PAGE_SHIFT);\n}\n\n/*\n * By default need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS.\n * With KASLR memory randomization, depending on the machine e820 memory\n * and the PUD alignment. We may need twice more pages when KASLR memory\n * randomization is enabled.\n */\n#ifndef CONFIG_RANDOMIZE_MEMORY\n#define INIT_PGD_PAGE_COUNT      6\n#else\n#define INIT_PGD_PAGE_COUNT      12\n#endif\n#define INIT_PGT_BUF_SIZE\t(INIT_PGD_PAGE_COUNT * PAGE_SIZE)\nRESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);\nvoid  __init early_alloc_pgt_buf(void)\n{\n\tunsigned long tables = INIT_PGT_BUF_SIZE;\n\tphys_addr_t base;\n\n\tbase = __pa(extend_brk(tables, PAGE_SIZE));\n\n\tpgt_buf_start = base >> PAGE_SHIFT;\n\tpgt_buf_end = pgt_buf_start;\n\tpgt_buf_top = pgt_buf_start + (tables >> PAGE_SHIFT);\n}\n\nint after_bootmem;\n\nearly_param_on_off(\"gbpages\", \"nogbpages\", direct_gbpages, CONFIG_X86_DIRECT_GBPAGES);\n\nstruct map_range {\n\tunsigned long start;\n\tunsigned long end;\n\tunsigned page_size_mask;\n};\n\nstatic int page_size_mask;\n\nstatic void __init probe_page_size_mask(void)\n{\n#if !defined(CONFIG_KMEMCHECK)\n\t/*\n\t * For CONFIG_KMEMCHECK or pagealloc debugging, identity mapping will\n\t * use small pages.\n\t * This will simplify cpa(), which otherwise needs to support splitting\n\t * large pages into small in interrupt context, etc.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_PSE) && !debug_pagealloc_enabled())\n\t\tpage_size_mask |= 1 << PG_LEVEL_2M;\n#endif\n\n\t/* Enable PSE if available */\n\tif (boot_cpu_has(X86_FEATURE_PSE))\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PSE);\n\n\t/* Enable PGE if available */\n\tif (boot_cpu_has(X86_FEATURE_PGE)) {\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PGE);\n\t\t__supported_pte_mask |= _PAGE_GLOBAL;\n\t} else\n\t\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\n\t/* Enable 1 GB linear kernel mappings if available: */\n\tif (direct_gbpages && boot_cpu_has(X86_FEATURE_GBPAGES)) {\n\t\tprintk(KERN_INFO \"Using GB pages for direct mapping\\n\");\n\t\tpage_size_mask |= 1 << PG_LEVEL_1G;\n\t} else {\n\t\tdirect_gbpages = 0;\n\t}\n}\n\n#ifdef CONFIG_X86_32\n#define NR_RANGE_MR 3\n#else /* CONFIG_X86_64 */\n#define NR_RANGE_MR 5\n#endif\n\nstatic int __meminit save_mr(struct map_range *mr, int nr_range,\n\t\t\t     unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t     unsigned long page_size_mask)\n{\n\tif (start_pfn < end_pfn) {\n\t\tif (nr_range >= NR_RANGE_MR)\n\t\t\tpanic(\"run out of range for init_memory_mapping\\n\");\n\t\tmr[nr_range].start = start_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].end   = end_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].page_size_mask = page_size_mask;\n\t\tnr_range++;\n\t}\n\n\treturn nr_range;\n}\n\n/*\n * adjust the page_size_mask for small range to go with\n *\tbig page size instead small one if nearby are ram too.\n */\nstatic void __ref adjust_range_page_size_mask(struct map_range *mr,\n\t\t\t\t\t\t\t int nr_range)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_range; i++) {\n\t\tif ((page_size_mask & (1<<PG_LEVEL_2M)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_2M))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PMD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PMD_SIZE);\n\n#ifdef CONFIG_X86_32\n\t\t\tif ((end >> PAGE_SHIFT) > max_low_pfn)\n\t\t\t\tcontinue;\n#endif\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_2M;\n\t\t}\n\t\tif ((page_size_mask & (1<<PG_LEVEL_1G)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_1G))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PUD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PUD_SIZE);\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_1G;\n\t\t}\n\t}\n}\n\nstatic const char *page_size_string(struct map_range *mr)\n{\n\tstatic const char str_1g[] = \"1G\";\n\tstatic const char str_2m[] = \"2M\";\n\tstatic const char str_4m[] = \"4M\";\n\tstatic const char str_4k[] = \"4k\";\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_1G))\n\t\treturn str_1g;\n\t/*\n\t * 32-bit without PAE has a 4M large page size.\n\t * PG_LEVEL_2M is misnamed, but we can at least\n\t * print out the right size in the string.\n\t */\n\tif (IS_ENABLED(CONFIG_X86_32) &&\n\t    !IS_ENABLED(CONFIG_X86_PAE) &&\n\t    mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_4m;\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_2m;\n\n\treturn str_4k;\n}\n\nstatic int __meminit split_mem_range(struct map_range *mr, int nr_range,\n\t\t\t\t     unsigned long start,\n\t\t\t\t     unsigned long end)\n{\n\tunsigned long start_pfn, end_pfn, limit_pfn;\n\tunsigned long pfn;\n\tint i;\n\n\tlimit_pfn = PFN_DOWN(end);\n\n\t/* head if not big page alignment ? */\n\tpfn = start_pfn = PFN_DOWN(start);\n#ifdef CONFIG_X86_32\n\t/*\n\t * Don't use a large page for the first 2/4MB of memory\n\t * because there are often fixed size MTRRs in there\n\t * and overlapping MTRRs into large pages can cause\n\t * slowdowns.\n\t */\n\tif (pfn == 0)\n\t\tend_pfn = PFN_DOWN(PMD_SIZE);\n\telse\n\t\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#else /* CONFIG_X86_64 */\n\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\tif (end_pfn > limit_pfn)\n\t\tend_pfn = limit_pfn;\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\t\tpfn = end_pfn;\n\t}\n\n\t/* big page (2M) range */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#ifdef CONFIG_X86_32\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#else /* CONFIG_X86_64 */\n\tend_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tif (end_pfn > round_down(limit_pfn, PFN_DOWN(PMD_SIZE)))\n\t\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n\n#ifdef CONFIG_X86_64\n\t/* big page (1G) range */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PUD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask &\n\t\t\t\t ((1<<PG_LEVEL_2M)|(1<<PG_LEVEL_1G)));\n\t\tpfn = end_pfn;\n\t}\n\n\t/* tail is not big page (1G) alignment */\n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n#endif\n\n\t/* tail is not big page (2M) alignment */\n\tstart_pfn = pfn;\n\tend_pfn = limit_pfn;\n\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\n\tif (!after_bootmem)\n\t\tadjust_range_page_size_mask(mr, nr_range);\n\n\t/* try to merge same page size and continuous */\n\tfor (i = 0; nr_range > 1 && i < nr_range - 1; i++) {\n\t\tunsigned long old_start;\n\t\tif (mr[i].end != mr[i+1].start ||\n\t\t    mr[i].page_size_mask != mr[i+1].page_size_mask)\n\t\t\tcontinue;\n\t\t/* move it */\n\t\told_start = mr[i].start;\n\t\tmemmove(&mr[i], &mr[i+1],\n\t\t\t(nr_range - 1 - i) * sizeof(struct map_range));\n\t\tmr[i--].start = old_start;\n\t\tnr_range--;\n\t}\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tpr_debug(\" [mem %#010lx-%#010lx] page %s\\n\",\n\t\t\t\tmr[i].start, mr[i].end - 1,\n\t\t\t\tpage_size_string(&mr[i]));\n\n\treturn nr_range;\n}\n\nstruct range pfn_mapped[E820_X_MAX];\nint nr_pfn_mapped;\n\nstatic void add_pfn_range_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tnr_pfn_mapped = add_range_with_merge(pfn_mapped, E820_X_MAX,\n\t\t\t\t\t     nr_pfn_mapped, start_pfn, end_pfn);\n\tnr_pfn_mapped = clean_sort_range(pfn_mapped, E820_X_MAX);\n\n\tmax_pfn_mapped = max(max_pfn_mapped, end_pfn);\n\n\tif (start_pfn < (1UL<<(32-PAGE_SHIFT)))\n\t\tmax_low_pfn_mapped = max(max_low_pfn_mapped,\n\t\t\t\t\t min(end_pfn, 1UL<<(32-PAGE_SHIFT)));\n}\n\nbool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_pfn_mapped; i++)\n\t\tif ((start_pfn >= pfn_mapped[i].start) &&\n\t\t    (end_pfn <= pfn_mapped[i].end))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Setup the direct mapping of the physical memory at PAGE_OFFSET.\n * This runs before bootmem is initialized and gets pages directly from\n * the physical memory. To access them they are temporarily mapped.\n */\nunsigned long __ref init_memory_mapping(unsigned long start,\n\t\t\t\t\t       unsigned long end)\n{\n\tstruct map_range mr[NR_RANGE_MR];\n\tunsigned long ret = 0;\n\tint nr_range, i;\n\n\tpr_debug(\"init_memory_mapping: [mem %#010lx-%#010lx]\\n\",\n\t       start, end - 1);\n\n\tmemset(mr, 0, sizeof(mr));\n\tnr_range = split_mem_range(mr, 0, start, end);\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tret = kernel_physical_mapping_init(mr[i].start, mr[i].end,\n\t\t\t\t\t\t   mr[i].page_size_mask);\n\n\tadd_pfn_range_mapped(start >> PAGE_SHIFT, ret >> PAGE_SHIFT);\n\n\treturn ret >> PAGE_SHIFT;\n}\n\n/*\n * We need to iterate through the E820 memory map and create direct mappings\n * for only E820_RAM and E820_KERN_RESERVED regions. We cannot simply\n * create direct mappings for all pfns from [0 to max_low_pfn) and\n * [4GB to max_pfn) because of possible memory holes in high addresses\n * that cannot be marked as UC by fixed/variable range MTRRs.\n * Depending on the alignment of E820 ranges, this may possibly result\n * in using smaller size (i.e. 4K instead of 2M or 1G) page tables.\n *\n * init_mem_mapping() calls init_range_memory_mapping() with big range.\n * That range would have hole in the middle or ends, and only ram parts\n * will be mapped in init_range_memory_mapping().\n */\nstatic unsigned long __init init_range_memory_mapping(\n\t\t\t\t\t   unsigned long r_start,\n\t\t\t\t\t   unsigned long r_end)\n{\n\tunsigned long start_pfn, end_pfn;\n\tunsigned long mapped_ram_size = 0;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, NULL) {\n\t\tu64 start = clamp_val(PFN_PHYS(start_pfn), r_start, r_end);\n\t\tu64 end = clamp_val(PFN_PHYS(end_pfn), r_start, r_end);\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * if it is overlapping with brk pgt, we need to\n\t\t * alloc pgt buf from memblock instead.\n\t\t */\n\t\tcan_use_brk_pgt = max(start, (u64)pgt_buf_end<<PAGE_SHIFT) >=\n\t\t\t\t    min(end, (u64)pgt_buf_top<<PAGE_SHIFT);\n\t\tinit_memory_mapping(start, end);\n\t\tmapped_ram_size += end - start;\n\t\tcan_use_brk_pgt = true;\n\t}\n\n\treturn mapped_ram_size;\n}\n\nstatic unsigned long __init get_new_step_size(unsigned long step_size)\n{\n\t/*\n\t * Initial mapped size is PMD_SIZE (2M).\n\t * We can not set step_size to be PUD_SIZE (1G) yet.\n\t * In worse case, when we cross the 1G boundary, and\n\t * PG_LEVEL_2M is not set, we will need 1+1+512 pages (2M + 8k)\n\t * to map 1G range with PTE. Hence we use one less than the\n\t * difference of page table level shifts.\n\t *\n\t * Don't need to worry about overflow in the top-down case, on 32bit,\n\t * when step_size is 0, round_down() returns 0 for start, and that\n\t * turns it into 0x100000000ULL.\n\t * In the bottom-up case, round_up(x, 0) returns 0 though too, which\n\t * needs to be taken into consideration by the code below.\n\t */\n\treturn step_size << (PMD_SHIFT - PAGE_SHIFT - 1);\n}\n\n/**\n * memory_map_top_down - Map [map_start, map_end) top down\n * @map_start: start address of the target memory range\n * @map_end: end address of the target memory range\n *\n * This function will setup direct mapping for memory range\n * [map_start, map_end) in top-down. That said, the page tables\n * will be allocated at the end of the memory, and we map the\n * memory in top-down.\n */\nstatic void __init memory_map_top_down(unsigned long map_start,\n\t\t\t\t       unsigned long map_end)\n{\n\tunsigned long real_end, start, last_start;\n\tunsigned long step_size;\n\tunsigned long addr;\n\tunsigned long mapped_ram_size = 0;\n\n\t/* xen has big range in reserved near end of ram, skip it at first.*/\n\taddr = memblock_find_in_range(map_start, map_end, PMD_SIZE, PMD_SIZE);\n\treal_end = addr + PMD_SIZE;\n\n\t/* step_size need to be small so pgt_buf from BRK could cover it */\n\tstep_size = PMD_SIZE;\n\tmax_pfn_mapped = 0; /* will get exact value next */\n\tmin_pfn_mapped = real_end >> PAGE_SHIFT;\n\tlast_start = start = real_end;\n\n\t/*\n\t * We start from the top (end of memory) and go to the bottom.\n\t * The memblock_find_in_range() gets us a block of RAM from the\n\t * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages\n\t * for page table.\n\t */\n\twhile (last_start > map_start) {\n\t\tif (last_start > step_size) {\n\t\t\tstart = round_down(last_start - 1, step_size);\n\t\t\tif (start < map_start)\n\t\t\t\tstart = map_start;\n\t\t} else\n\t\t\tstart = map_start;\n\t\tmapped_ram_size += init_range_memory_mapping(start,\n\t\t\t\t\t\t\tlast_start);\n\t\tlast_start = start;\n\t\tmin_pfn_mapped = last_start >> PAGE_SHIFT;\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n\n\tif (real_end < map_end)\n\t\tinit_range_memory_mapping(real_end, map_end);\n}\n\n/**\n * memory_map_bottom_up - Map [map_start, map_end) bottom up\n * @map_start: start address of the target memory range\n * @map_end: end address of the target memory range\n *\n * This function will setup direct mapping for memory range\n * [map_start, map_end) in bottom-up. Since we have limited the\n * bottom-up allocation above the kernel, the page tables will\n * be allocated just above the kernel and we map the memory\n * in [map_start, map_end) in bottom-up.\n */\nstatic void __init memory_map_bottom_up(unsigned long map_start,\n\t\t\t\t\tunsigned long map_end)\n{\n\tunsigned long next, start;\n\tunsigned long mapped_ram_size = 0;\n\t/* step_size need to be small so pgt_buf from BRK could cover it */\n\tunsigned long step_size = PMD_SIZE;\n\n\tstart = map_start;\n\tmin_pfn_mapped = start >> PAGE_SHIFT;\n\n\t/*\n\t * We start from the bottom (@map_start) and go to the top (@map_end).\n\t * The memblock_find_in_range() gets us a block of RAM from the\n\t * end of RAM in [min_pfn_mapped, max_pfn_mapped) used as new pages\n\t * for page table.\n\t */\n\twhile (start < map_end) {\n\t\tif (step_size && map_end - start > step_size) {\n\t\t\tnext = round_up(start + 1, step_size);\n\t\t\tif (next > map_end)\n\t\t\t\tnext = map_end;\n\t\t} else {\n\t\t\tnext = map_end;\n\t\t}\n\n\t\tmapped_ram_size += init_range_memory_mapping(start, next);\n\t\tstart = next;\n\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n}\n\nvoid __init init_mem_mapping(void)\n{\n\tunsigned long end;\n\n\tprobe_page_size_mask();\n\n#ifdef CONFIG_X86_64\n\tend = max_pfn << PAGE_SHIFT;\n#else\n\tend = max_low_pfn << PAGE_SHIFT;\n#endif\n\n\t/* the ISA range is always mapped regardless of memory holes */\n\tinit_memory_mapping(0, ISA_END_ADDRESS);\n\n\t/* Init the trampoline, possibly with KASLR memory offset */\n\tinit_trampoline();\n\n\t/*\n\t * If the allocation is in bottom-up direction, we setup direct mapping\n\t * in bottom-up, otherwise we setup direct mapping in top-down.\n\t */\n\tif (memblock_bottom_up()) {\n\t\tunsigned long kernel_end = __pa_symbol(_end);\n\n\t\t/*\n\t\t * we need two separate calls here. This is because we want to\n\t\t * allocate page tables above the kernel. So we first map\n\t\t * [kernel_end, end) to make memory above the kernel be mapped\n\t\t * as soon as possible. And then use page tables allocated above\n\t\t * the kernel to map [ISA_END_ADDRESS, kernel_end).\n\t\t */\n\t\tmemory_map_bottom_up(kernel_end, end);\n\t\tmemory_map_bottom_up(ISA_END_ADDRESS, kernel_end);\n\t} else {\n\t\tmemory_map_top_down(ISA_END_ADDRESS, end);\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (max_pfn > max_low_pfn) {\n\t\t/* can we preseve max_low_pfn ?*/\n\t\tmax_low_pfn = max_pfn;\n\t}\n#else\n\tearly_ioremap_page_table_range_init();\n#endif\n\n\tload_cr3(swapper_pg_dir);\n\t__flush_tlb_all();\n\n\tearly_memtest(0, max_pfn_mapped << PAGE_SHIFT);\n}\n\n/*\n * devmem_is_allowed() checks to see if /dev/mem access to a certain address\n * is valid. The argument is a physical page number.\n *\n * On x86, access has to be given to the first megabyte of RAM because that\n * area traditionally contains BIOS code and data regions used by X, dosemu,\n * and similar apps. Since they map the entire memory range, the whole range\n * must be allowed (for mapping), but any areas that would otherwise be\n * disallowed are flagged as being \"zero filled\" instead of rejected.\n * Access has to be given to non-kernel-ram areas as well, these contain the\n * PCI mmio resources as well as potential bios/acpi data regions.\n */\nint devmem_is_allowed(unsigned long pagenr)\n{\n\tif (page_is_ram(pagenr)) {\n\t\t/*\n\t\t * For disallowed memory regions in the low 1MB range,\n\t\t * request that the page be shown as all zeros.\n\t\t */\n\t\tif (pagenr < 256)\n\t\t\treturn 2;\n\n\t\treturn 0;\n\t}\n\n\t/*\n\t * This must follow RAM test, since System RAM is considered a\n\t * restricted resource under CONFIG_STRICT_IOMEM.\n\t */\n\tif (iomem_is_exclusive(pagenr << PAGE_SHIFT)) {\n\t\t/* Low 1MB bypasses iomem restrictions. */\n\t\tif (pagenr < 256)\n\t\t\treturn 1;\n\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nvoid free_init_pages(char *what, unsigned long begin, unsigned long end)\n{\n\tunsigned long begin_aligned, end_aligned;\n\n\t/* Make sure boundaries are page aligned */\n\tbegin_aligned = PAGE_ALIGN(begin);\n\tend_aligned   = end & PAGE_MASK;\n\n\tif (WARN_ON(begin_aligned != begin || end_aligned != end)) {\n\t\tbegin = begin_aligned;\n\t\tend   = end_aligned;\n\t}\n\n\tif (begin >= end)\n\t\treturn;\n\n\t/*\n\t * If debugging page accesses then do not free this memory but\n\t * mark them not present - any buggy init-section access will\n\t * create a kernel page fault:\n\t */\n\tif (debug_pagealloc_enabled()) {\n\t\tpr_info(\"debug: unmapping init [mem %#010lx-%#010lx]\\n\",\n\t\t\tbegin, end - 1);\n\t\tset_memory_np(begin, (end - begin) >> PAGE_SHIFT);\n\t} else {\n\t\t/*\n\t\t * We just marked the kernel text read only above, now that\n\t\t * we are going to free part of that, we need to make that\n\t\t * writeable and non-executable first.\n\t\t */\n\t\tset_memory_nx(begin, (end - begin) >> PAGE_SHIFT);\n\t\tset_memory_rw(begin, (end - begin) >> PAGE_SHIFT);\n\n\t\tfree_reserved_area((void *)begin, (void *)end,\n\t\t\t\t   POISON_FREE_INITMEM, what);\n\t}\n}\n\nvoid __ref free_initmem(void)\n{\n\te820_reallocate_tables();\n\n\tfree_init_pages(\"unused kernel\",\n\t\t\t(unsigned long)(&__init_begin),\n\t\t\t(unsigned long)(&__init_end));\n}\n\n#ifdef CONFIG_BLK_DEV_INITRD\nvoid __init free_initrd_mem(unsigned long start, unsigned long end)\n{\n\t/*\n\t * end could be not aligned, and We can not align that,\n\t * decompresser could be confused by aligned initrd_end\n\t * We already reserve the end partial page before in\n\t *   - i386_start_kernel()\n\t *   - x86_64_start_kernel()\n\t *   - relocate_initrd()\n\t * So here We can do PAGE_ALIGN() safely to get partial page to be freed\n\t */\n\tfree_init_pages(\"initrd\", start, PAGE_ALIGN(end));\n}\n#endif\n\nvoid __init zone_sizes_init(void)\n{\n\tunsigned long max_zone_pfns[MAX_NR_ZONES];\n\n\tmemset(max_zone_pfns, 0, sizeof(max_zone_pfns));\n\n#ifdef CONFIG_ZONE_DMA\n\tmax_zone_pfns[ZONE_DMA]\t\t= min(MAX_DMA_PFN, max_low_pfn);\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\tmax_zone_pfns[ZONE_DMA32]\t= min(MAX_DMA32_PFN, max_low_pfn);\n#endif\n\tmax_zone_pfns[ZONE_NORMAL]\t= max_low_pfn;\n#ifdef CONFIG_HIGHMEM\n\tmax_zone_pfns[ZONE_HIGHMEM]\t= max_pfn;\n#endif\n\n\tfree_area_init_nodes(max_zone_pfns);\n}\n\nDEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {\n#ifdef CONFIG_SMP\n\t.active_mm = &init_mm,\n\t.state = 0,\n#endif\n\t.cr4 = ~0UL,\t/* fail hard if we screw up cr4 shadow initialization */\n};\nEXPORT_SYMBOL_GPL(cpu_tlbstate);\n\nvoid update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)\n{\n\t/* entry 0 MUST be WB (hardwired to speed up translations) */\n\tBUG_ON(!entry && cache != _PAGE_CACHE_MODE_WB);\n\n\t__cachemode2pte_tbl[cache] = __cm_idx2pte(entry);\n\t__pte2cachemode_tbl[entry] = cache;\n}\n", "/*\n *  linux/drivers/char/mem.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n *\n *  Added devfs support.\n *    Jan-11-1998, C. Scott Ananian <cananian@alumni.princeton.edu>\n *  Shared /dev/zero mmapping support, Feb 2000, Kanoj Sarcar <kanoj@sgi.com>\n */\n\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/mman.h>\n#include <linux/random.h>\n#include <linux/init.h>\n#include <linux/raw.h>\n#include <linux/tty.h>\n#include <linux/capability.h>\n#include <linux/ptrace.h>\n#include <linux/device.h>\n#include <linux/highmem.h>\n#include <linux/backing-dev.h>\n#include <linux/shmem_fs.h>\n#include <linux/splice.h>\n#include <linux/pfn.h>\n#include <linux/export.h>\n#include <linux/io.h>\n#include <linux/uio.h>\n\n#include <linux/uaccess.h>\n\n#ifdef CONFIG_IA64\n# include <linux/efi.h>\n#endif\n\n#define DEVPORT_MINOR\t4\n\nstatic inline unsigned long size_inside_page(unsigned long start,\n\t\t\t\t\t     unsigned long size)\n{\n\tunsigned long sz;\n\n\tsz = PAGE_SIZE - (start & (PAGE_SIZE - 1));\n\n\treturn min(sz, size);\n}\n\n#ifndef ARCH_HAS_VALID_PHYS_ADDR_RANGE\nstatic inline int valid_phys_addr_range(phys_addr_t addr, size_t count)\n{\n\treturn addr + count <= __pa(high_memory);\n}\n\nstatic inline int valid_mmap_phys_addr_range(unsigned long pfn, size_t size)\n{\n\treturn 1;\n}\n#endif\n\n#ifdef CONFIG_STRICT_DEVMEM\nstatic inline int page_is_allowed(unsigned long pfn)\n{\n\treturn devmem_is_allowed(pfn);\n}\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\tu64 from = ((u64)pfn) << PAGE_SHIFT;\n\tu64 to = from + size;\n\tu64 cursor = from;\n\n\twhile (cursor < to) {\n\t\tif (!devmem_is_allowed(pfn))\n\t\t\treturn 0;\n\t\tcursor += PAGE_SIZE;\n\t\tpfn++;\n\t}\n\treturn 1;\n}\n#else\nstatic inline int page_is_allowed(unsigned long pfn)\n{\n\treturn 1;\n}\nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\treturn 1;\n}\n#endif\n\n#ifndef unxlate_dev_mem_ptr\n#define unxlate_dev_mem_ptr unxlate_dev_mem_ptr\nvoid __weak unxlate_dev_mem_ptr(phys_addr_t phys, void *addr)\n{\n}\n#endif\n\n/*\n * This funcion reads the *physical* memory. The f_pos points directly to the\n * memory location.\n */\nstatic ssize_t read_mem(struct file *file, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t read, sz;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn 0;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\tread = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\tif (sz > 0) {\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tcount -= sz;\n\t\t\tread += sz;\n\t\t}\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tunsigned long remaining;\n\t\tint allowed;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\tallowed = page_is_allowed(p >> PAGE_SHIFT);\n\t\tif (!allowed)\n\t\t\treturn -EPERM;\n\t\tif (allowed == 2) {\n\t\t\t/* Show zeros for restricted memory. */\n\t\t\tremaining = clear_user(buf, sz);\n\t\t} else {\n\t\t\t/*\n\t\t\t * On ia64 if a page has been mapped somewhere as\n\t\t\t * uncached, then it must also be accessed uncached\n\t\t\t * by the kernel or data corruption may occur.\n\t\t\t */\n\t\t\tptr = xlate_dev_mem_ptr(p);\n\t\t\tif (!ptr)\n\t\t\t\treturn -EFAULT;\n\n\t\t\tremaining = copy_to_user(buf, ptr, sz);\n\n\t\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\t}\n\n\t\tif (remaining)\n\t\t\treturn -EFAULT;\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\tread += sz;\n\t}\n\n\t*ppos += read;\n\treturn read;\n}\n\nstatic ssize_t write_mem(struct file *file, const char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tphys_addr_t p = *ppos;\n\tssize_t written, sz;\n\tunsigned long copied;\n\tvoid *ptr;\n\n\tif (p != *ppos)\n\t\treturn -EFBIG;\n\n\tif (!valid_phys_addr_range(p, count))\n\t\treturn -EFAULT;\n\n\twritten = 0;\n\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\t/* Hmm. Do something? */\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tint allowed;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\tallowed = page_is_allowed(p >> PAGE_SHIFT);\n\t\tif (!allowed)\n\t\t\treturn -EPERM;\n\n\t\t/* Skip actual writing when a page is marked as restricted. */\n\t\tif (allowed == 1) {\n\t\t\t/*\n\t\t\t * On ia64 if a page has been mapped somewhere as\n\t\t\t * uncached, then it must also be accessed uncached\n\t\t\t * by the kernel or data corruption may occur.\n\t\t\t */\n\t\t\tptr = xlate_dev_mem_ptr(p);\n\t\t\tif (!ptr) {\n\t\t\t\tif (written)\n\t\t\t\t\tbreak;\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\n\t\t\tcopied = copy_from_user(ptr, buf, sz);\n\t\t\tunxlate_dev_mem_ptr(p, ptr);\n\t\t\tif (copied) {\n\t\t\t\twritten += sz - copied;\n\t\t\t\tif (written)\n\t\t\t\t\tbreak;\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n\n\t*ppos += written;\n\treturn written;\n}\n\nint __weak phys_mem_access_prot_allowed(struct file *file,\n\tunsigned long pfn, unsigned long size, pgprot_t *vma_prot)\n{\n\treturn 1;\n}\n\n#ifndef __HAVE_PHYS_MEM_ACCESS_PROT\n\n/*\n * Architectures vary in how they handle caching for addresses\n * outside of main memory.\n *\n */\n#ifdef pgprot_noncached\nstatic int uncached_access(struct file *file, phys_addr_t addr)\n{\n#if defined(CONFIG_IA64)\n\t/*\n\t * On ia64, we ignore O_DSYNC because we cannot tolerate memory\n\t * attribute aliases.\n\t */\n\treturn !(efi_mem_attributes(addr) & EFI_MEMORY_WB);\n#elif defined(CONFIG_MIPS)\n\t{\n\t\textern int __uncached_access(struct file *file,\n\t\t\t\t\t     unsigned long addr);\n\n\t\treturn __uncached_access(file, addr);\n\t}\n#else\n\t/*\n\t * Accessing memory above the top the kernel knows about or through a\n\t * file pointer\n\t * that was marked O_DSYNC will be done non-cached.\n\t */\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 1;\n\treturn addr >= __pa(high_memory);\n#endif\n}\n#endif\n\nstatic pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,\n\t\t\t\t     unsigned long size, pgprot_t vma_prot)\n{\n#ifdef pgprot_noncached\n\tphys_addr_t offset = pfn << PAGE_SHIFT;\n\n\tif (uncached_access(file, offset))\n\t\treturn pgprot_noncached(vma_prot);\n#endif\n\treturn vma_prot;\n}\n#endif\n\n#ifndef CONFIG_MMU\nstatic unsigned long get_unmapped_area_mem(struct file *file,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   unsigned long len,\n\t\t\t\t\t   unsigned long pgoff,\n\t\t\t\t\t   unsigned long flags)\n{\n\tif (!valid_mmap_phys_addr_range(pgoff, len))\n\t\treturn (unsigned long) -EINVAL;\n\treturn pgoff << PAGE_SHIFT;\n}\n\n/* permit direct mmap, for read, write or exec */\nstatic unsigned memory_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT |\n\t\tNOMMU_MAP_READ | NOMMU_MAP_WRITE | NOMMU_MAP_EXEC;\n}\n\nstatic unsigned zero_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_COPY;\n}\n\n/* can't do an in-place private mapping if there's no MMU */\nstatic inline int private_mapping_ok(struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & VM_MAYSHARE;\n}\n#else\n\nstatic inline int private_mapping_ok(struct vm_area_struct *vma)\n{\n\treturn 1;\n}\n#endif\n\nstatic const struct vm_operations_struct mmap_mem_ops = {\n#ifdef CONFIG_HAVE_IOREMAP_PROT\n\t.access = generic_access_phys\n#endif\n};\n\nstatic int mmap_mem(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t size = vma->vm_end - vma->vm_start;\n\n\tif (!valid_mmap_phys_addr_range(vma->vm_pgoff, size))\n\t\treturn -EINVAL;\n\n\tif (!private_mapping_ok(vma))\n\t\treturn -ENOSYS;\n\n\tif (!range_is_allowed(vma->vm_pgoff, size))\n\t\treturn -EPERM;\n\n\tif (!phys_mem_access_prot_allowed(file, vma->vm_pgoff, size,\n\t\t\t\t\t\t&vma->vm_page_prot))\n\t\treturn -EINVAL;\n\n\tvma->vm_page_prot = phys_mem_access_prot(file, vma->vm_pgoff,\n\t\t\t\t\t\t size,\n\t\t\t\t\t\t vma->vm_page_prot);\n\n\tvma->vm_ops = &mmap_mem_ops;\n\n\t/* Remap-pfn-range will mark the range VM_IO */\n\tif (remap_pfn_range(vma,\n\t\t\t    vma->vm_start,\n\t\t\t    vma->vm_pgoff,\n\t\t\t    size,\n\t\t\t    vma->vm_page_prot)) {\n\t\treturn -EAGAIN;\n\t}\n\treturn 0;\n}\n\nstatic int mmap_kmem(struct file *file, struct vm_area_struct *vma)\n{\n\tunsigned long pfn;\n\n\t/* Turn a kernel-virtual address into a physical page frame */\n\tpfn = __pa((u64)vma->vm_pgoff << PAGE_SHIFT) >> PAGE_SHIFT;\n\n\t/*\n\t * RED-PEN: on some architectures there is more mapped memory than\n\t * available in mem_map which pfn_valid checks for. Perhaps should add a\n\t * new macro here.\n\t *\n\t * RED-PEN: vmalloc is not supported right now.\n\t */\n\tif (!pfn_valid(pfn))\n\t\treturn -EIO;\n\n\tvma->vm_pgoff = pfn;\n\treturn mmap_mem(file, vma);\n}\n\n/*\n * This function reads the *virtual* memory as seen by the kernel.\n */\nstatic ssize_t read_kmem(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tssize_t low_count, read, sz;\n\tchar *kbuf; /* k-addr because vread() takes vmlist_lock rwlock */\n\tint err = 0;\n\n\tread = 0;\n\tif (p < (unsigned long) high_memory) {\n\t\tlow_count = count;\n\t\tif (count > (unsigned long)high_memory - p)\n\t\t\tlow_count = (unsigned long)high_memory - p;\n\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t\t/* we don't have page 0 mapped on sparc and m68k.. */\n\t\tif (p < PAGE_SIZE && low_count > 0) {\n\t\t\tsz = size_inside_page(p, low_count);\n\t\t\tif (clear_user(buf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tread += sz;\n\t\t\tlow_count -= sz;\n\t\t\tcount -= sz;\n\t\t}\n#endif\n\t\twhile (low_count > 0) {\n\t\t\tsz = size_inside_page(p, low_count);\n\n\t\t\t/*\n\t\t\t * On ia64 if a page has been mapped somewhere as\n\t\t\t * uncached, then it must also be accessed uncached\n\t\t\t * by the kernel or data corruption may occur\n\t\t\t */\n\t\t\tkbuf = xlate_dev_kmem_ptr((void *)p);\n\t\t\tif (!virt_addr_valid(kbuf))\n\t\t\t\treturn -ENXIO;\n\n\t\t\tif (copy_to_user(buf, kbuf, sz))\n\t\t\t\treturn -EFAULT;\n\t\t\tbuf += sz;\n\t\t\tp += sz;\n\t\t\tread += sz;\n\t\t\tlow_count -= sz;\n\t\t\tcount -= sz;\n\t\t}\n\t}\n\n\tif (count > 0) {\n\t\tkbuf = (char *)__get_free_page(GFP_KERNEL);\n\t\tif (!kbuf)\n\t\t\treturn -ENOMEM;\n\t\twhile (count > 0) {\n\t\t\tsz = size_inside_page(p, count);\n\t\t\tif (!is_vmalloc_or_module_addr((void *)p)) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsz = vread(kbuf, (char *)p, sz);\n\t\t\tif (!sz)\n\t\t\t\tbreak;\n\t\t\tif (copy_to_user(buf, kbuf, sz)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount -= sz;\n\t\t\tbuf += sz;\n\t\t\tread += sz;\n\t\t\tp += sz;\n\t\t}\n\t\tfree_page((unsigned long)kbuf);\n\t}\n\t*ppos = p;\n\treturn read ? read : err;\n}\n\n\nstatic ssize_t do_write_kmem(unsigned long p, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tssize_t written, sz;\n\tunsigned long copied;\n\n\twritten = 0;\n#ifdef __ARCH_HAS_NO_PAGE_ZERO_MAPPED\n\t/* we don't have page 0 mapped on sparc and m68k.. */\n\tif (p < PAGE_SIZE) {\n\t\tsz = size_inside_page(p, count);\n\t\t/* Hmm. Do something? */\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n#endif\n\n\twhile (count > 0) {\n\t\tvoid *ptr;\n\n\t\tsz = size_inside_page(p, count);\n\n\t\t/*\n\t\t * On ia64 if a page has been mapped somewhere as uncached, then\n\t\t * it must also be accessed uncached by the kernel or data\n\t\t * corruption may occur.\n\t\t */\n\t\tptr = xlate_dev_kmem_ptr((void *)p);\n\t\tif (!virt_addr_valid(ptr))\n\t\t\treturn -ENXIO;\n\n\t\tcopied = copy_from_user(ptr, buf, sz);\n\t\tif (copied) {\n\t\t\twritten += sz - copied;\n\t\t\tif (written)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbuf += sz;\n\t\tp += sz;\n\t\tcount -= sz;\n\t\twritten += sz;\n\t}\n\n\t*ppos += written;\n\treturn written;\n}\n\n/*\n * This function writes to the *virtual* memory as seen by the kernel.\n */\nstatic ssize_t write_kmem(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\tssize_t wrote = 0;\n\tssize_t virtr = 0;\n\tchar *kbuf; /* k-addr because vwrite() takes vmlist_lock rwlock */\n\tint err = 0;\n\n\tif (p < (unsigned long) high_memory) {\n\t\tunsigned long to_write = min_t(unsigned long, count,\n\t\t\t\t\t       (unsigned long)high_memory - p);\n\t\twrote = do_write_kmem(p, buf, to_write, ppos);\n\t\tif (wrote != to_write)\n\t\t\treturn wrote;\n\t\tp += wrote;\n\t\tbuf += wrote;\n\t\tcount -= wrote;\n\t}\n\n\tif (count > 0) {\n\t\tkbuf = (char *)__get_free_page(GFP_KERNEL);\n\t\tif (!kbuf)\n\t\t\treturn wrote ? wrote : -ENOMEM;\n\t\twhile (count > 0) {\n\t\t\tunsigned long sz = size_inside_page(p, count);\n\t\t\tunsigned long n;\n\n\t\t\tif (!is_vmalloc_or_module_addr((void *)p)) {\n\t\t\t\terr = -ENXIO;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tn = copy_from_user(kbuf, buf, sz);\n\t\t\tif (n) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvwrite(kbuf, (char *)p, sz);\n\t\t\tcount -= sz;\n\t\t\tbuf += sz;\n\t\t\tvirtr += sz;\n\t\t\tp += sz;\n\t\t}\n\t\tfree_page((unsigned long)kbuf);\n\t}\n\n\t*ppos = p;\n\treturn virtr + wrote ? : err;\n}\n\nstatic ssize_t read_port(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tunsigned long i = *ppos;\n\tchar __user *tmp = buf;\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\twhile (count-- > 0 && i < 65536) {\n\t\tif (__put_user(inb(i), tmp) < 0)\n\t\t\treturn -EFAULT;\n\t\ti++;\n\t\ttmp++;\n\t}\n\t*ppos = i;\n\treturn tmp-buf;\n}\n\nstatic ssize_t write_port(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\tunsigned long i = *ppos;\n\tconst char __user *tmp = buf;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\n\twhile (count-- > 0 && i < 65536) {\n\t\tchar c;\n\n\t\tif (__get_user(c, tmp)) {\n\t\t\tif (tmp > buf)\n\t\t\t\tbreak;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\toutb(c, i);\n\t\ti++;\n\t\ttmp++;\n\t}\n\t*ppos = i;\n\treturn tmp-buf;\n}\n\nstatic ssize_t read_null(struct file *file, char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\treturn 0;\n}\n\nstatic ssize_t write_null(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nstatic ssize_t read_iter_null(struct kiocb *iocb, struct iov_iter *to)\n{\n\treturn 0;\n}\n\nstatic ssize_t write_iter_null(struct kiocb *iocb, struct iov_iter *from)\n{\n\tsize_t count = iov_iter_count(from);\n\tiov_iter_advance(from, count);\n\treturn count;\n}\n\nstatic int pipe_to_null(struct pipe_inode_info *info, struct pipe_buffer *buf,\n\t\t\tstruct splice_desc *sd)\n{\n\treturn sd->len;\n}\n\nstatic ssize_t splice_write_null(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t\t loff_t *ppos, size_t len, unsigned int flags)\n{\n\treturn splice_from_pipe(pipe, out, ppos, len, flags, pipe_to_null);\n}\n\nstatic ssize_t read_iter_zero(struct kiocb *iocb, struct iov_iter *iter)\n{\n\tsize_t written = 0;\n\n\twhile (iov_iter_count(iter)) {\n\t\tsize_t chunk = iov_iter_count(iter), n;\n\n\t\tif (chunk > PAGE_SIZE)\n\t\t\tchunk = PAGE_SIZE;\t/* Just for latency reasons */\n\t\tn = iov_iter_zero(chunk, iter);\n\t\tif (!n && iov_iter_count(iter))\n\t\t\treturn written ? written : -EFAULT;\n\t\twritten += n;\n\t\tif (signal_pending(current))\n\t\t\treturn written ? written : -ERESTARTSYS;\n\t\tcond_resched();\n\t}\n\treturn written;\n}\n\nstatic int mmap_zero(struct file *file, struct vm_area_struct *vma)\n{\n#ifndef CONFIG_MMU\n\treturn -ENOSYS;\n#endif\n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn shmem_zero_setup(vma);\n\treturn 0;\n}\n\nstatic unsigned long get_unmapped_area_zero(struct file *file,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags)\n{\n#ifdef CONFIG_MMU\n\tif (flags & MAP_SHARED) {\n\t\t/*\n\t\t * mmap_zero() will call shmem_zero_setup() to create a file,\n\t\t * so use shmem's get_unmapped_area in case it can be huge;\n\t\t * and pass NULL for file as in mmap.c's get_unmapped_area(),\n\t\t * so as not to confuse shmem with our handle on \"/dev/zero\".\n\t\t */\n\t\treturn shmem_get_unmapped_area(NULL, addr, len, pgoff, flags);\n\t}\n\n\t/* Otherwise flags & MAP_PRIVATE: with no shmem object beneath it */\n\treturn current->mm->get_unmapped_area(file, addr, len, pgoff, flags);\n#else\n\treturn -ENOSYS;\n#endif\n}\n\nstatic ssize_t write_full(struct file *file, const char __user *buf,\n\t\t\t  size_t count, loff_t *ppos)\n{\n\treturn -ENOSPC;\n}\n\n/*\n * Special lseek() function for /dev/null and /dev/zero.  Most notably, you\n * can fopen() both devices with \"a\" now.  This was previously impossible.\n * -- SRB.\n */\nstatic loff_t null_lseek(struct file *file, loff_t offset, int orig)\n{\n\treturn file->f_pos = 0;\n}\n\n/*\n * The memory devices use the full 32/64 bits of the offset, and so we cannot\n * check against negative addresses: they are ok. The return value is weird,\n * though, in that case (0).\n *\n * also note that seeking relative to the \"end of file\" isn't supported:\n * it has no meaning, so it returns -EINVAL.\n */\nstatic loff_t memory_lseek(struct file *file, loff_t offset, int orig)\n{\n\tloff_t ret;\n\n\tinode_lock(file_inode(file));\n\tswitch (orig) {\n\tcase SEEK_CUR:\n\t\toffset += file->f_pos;\n\tcase SEEK_SET:\n\t\t/* to avoid userland mistaking f_pos=-9 as -EBADF=-9 */\n\t\tif ((unsigned long long)offset >= -MAX_ERRNO) {\n\t\t\tret = -EOVERFLOW;\n\t\t\tbreak;\n\t\t}\n\t\tfile->f_pos = offset;\n\t\tret = file->f_pos;\n\t\tforce_successful_syscall_return();\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tinode_unlock(file_inode(file));\n\treturn ret;\n}\n\nstatic int open_port(struct inode *inode, struct file *filp)\n{\n\treturn capable(CAP_SYS_RAWIO) ? 0 : -EPERM;\n}\n\n#define zero_lseek\tnull_lseek\n#define full_lseek      null_lseek\n#define write_zero\twrite_null\n#define write_iter_zero\twrite_iter_null\n#define open_mem\topen_port\n#define open_kmem\topen_mem\n\nstatic const struct file_operations __maybe_unused mem_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_mem,\n\t.write\t\t= write_mem,\n\t.mmap\t\t= mmap_mem,\n\t.open\t\t= open_mem,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = get_unmapped_area_mem,\n\t.mmap_capabilities = memory_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations __maybe_unused kmem_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_kmem,\n\t.write\t\t= write_kmem,\n\t.mmap\t\t= mmap_kmem,\n\t.open\t\t= open_kmem,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = get_unmapped_area_mem,\n\t.mmap_capabilities = memory_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations null_fops = {\n\t.llseek\t\t= null_lseek,\n\t.read\t\t= read_null,\n\t.write\t\t= write_null,\n\t.read_iter\t= read_iter_null,\n\t.write_iter\t= write_iter_null,\n\t.splice_write\t= splice_write_null,\n};\n\nstatic const struct file_operations __maybe_unused port_fops = {\n\t.llseek\t\t= memory_lseek,\n\t.read\t\t= read_port,\n\t.write\t\t= write_port,\n\t.open\t\t= open_port,\n};\n\nstatic const struct file_operations zero_fops = {\n\t.llseek\t\t= zero_lseek,\n\t.write\t\t= write_zero,\n\t.read_iter\t= read_iter_zero,\n\t.write_iter\t= write_iter_zero,\n\t.mmap\t\t= mmap_zero,\n\t.get_unmapped_area = get_unmapped_area_zero,\n#ifndef CONFIG_MMU\n\t.mmap_capabilities = zero_mmap_capabilities,\n#endif\n};\n\nstatic const struct file_operations full_fops = {\n\t.llseek\t\t= full_lseek,\n\t.read_iter\t= read_iter_zero,\n\t.write\t\t= write_full,\n};\n\nstatic const struct memdev {\n\tconst char *name;\n\tumode_t mode;\n\tconst struct file_operations *fops;\n\tfmode_t fmode;\n} devlist[] = {\n#ifdef CONFIG_DEVMEM\n\t [1] = { \"mem\", 0, &mem_fops, FMODE_UNSIGNED_OFFSET },\n#endif\n#ifdef CONFIG_DEVKMEM\n\t [2] = { \"kmem\", 0, &kmem_fops, FMODE_UNSIGNED_OFFSET },\n#endif\n\t [3] = { \"null\", 0666, &null_fops, 0 },\n#ifdef CONFIG_DEVPORT\n\t [4] = { \"port\", 0, &port_fops, 0 },\n#endif\n\t [5] = { \"zero\", 0666, &zero_fops, 0 },\n\t [7] = { \"full\", 0666, &full_fops, 0 },\n\t [8] = { \"random\", 0666, &random_fops, 0 },\n\t [9] = { \"urandom\", 0666, &urandom_fops, 0 },\n#ifdef CONFIG_PRINTK\n\t[11] = { \"kmsg\", 0644, &kmsg_fops, 0 },\n#endif\n};\n\nstatic int memory_open(struct inode *inode, struct file *filp)\n{\n\tint minor;\n\tconst struct memdev *dev;\n\n\tminor = iminor(inode);\n\tif (minor >= ARRAY_SIZE(devlist))\n\t\treturn -ENXIO;\n\n\tdev = &devlist[minor];\n\tif (!dev->fops)\n\t\treturn -ENXIO;\n\n\tfilp->f_op = dev->fops;\n\tfilp->f_mode |= dev->fmode;\n\n\tif (dev->fops->open)\n\t\treturn dev->fops->open(inode, filp);\n\n\treturn 0;\n}\n\nstatic const struct file_operations memory_fops = {\n\t.open = memory_open,\n\t.llseek = noop_llseek,\n};\n\nstatic char *mem_devnode(struct device *dev, umode_t *mode)\n{\n\tif (mode && devlist[MINOR(dev->devt)].mode)\n\t\t*mode = devlist[MINOR(dev->devt)].mode;\n\treturn NULL;\n}\n\nstatic struct class *mem_class;\n\nstatic int __init chr_dev_init(void)\n{\n\tint minor;\n\n\tif (register_chrdev(MEM_MAJOR, \"mem\", &memory_fops))\n\t\tprintk(\"unable to get major %d for memory devs\\n\", MEM_MAJOR);\n\n\tmem_class = class_create(THIS_MODULE, \"mem\");\n\tif (IS_ERR(mem_class))\n\t\treturn PTR_ERR(mem_class);\n\n\tmem_class->devnode = mem_devnode;\n\tfor (minor = 1; minor < ARRAY_SIZE(devlist); minor++) {\n\t\tif (!devlist[minor].name)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Create /dev/port?\n\t\t */\n\t\tif ((minor == DEVPORT_MINOR) && !arch_has_dev_port())\n\t\t\tcontinue;\n\n\t\tdevice_create(mem_class, NULL, MKDEV(MEM_MAJOR, minor),\n\t\t\t      NULL, devlist[minor].name);\n\t}\n\n\treturn tty_init();\n}\n\nfs_initcall(chr_dev_init);\n"], "filenames": ["arch/x86/mm/init.c", "drivers/char/mem.c"], "buggy_code_start_loc": [646, 62], "buggy_code_end_loc": [661, 208], "fixing_code_start_loc": [646, 63], "fixing_code_end_loc": [680, 230], "type": "CWE-732", "message": "The mm subsystem in the Linux kernel through 3.2 does not properly enforce the CONFIG_STRICT_DEVMEM protection mechanism, which allows local users to read or write to kernel memory locations in the first megabyte (and bypass slab-allocation access restrictions) via an application that opens the /dev/mem file, related to arch/x86/mm/init.c and drivers/char/mem.c.", "other": {"cve": {"id": "CVE-2017-7889", "sourceIdentifier": "cve@mitre.org", "published": "2017-04-17T00:59:00.203", "lastModified": "2023-02-14T21:12:55.230", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The mm subsystem in the Linux kernel through 3.2 does not properly enforce the CONFIG_STRICT_DEVMEM protection mechanism, which allows local users to read or write to kernel memory locations in the first megabyte (and bypass slab-allocation access restrictions) via an application that opens the /dev/mem file, related to arch/x86/mm/init.c and drivers/char/mem.c."}, {"lang": "es", "value": "El subsistema mm en el kernel de Linux hasta la versi\u00f3n 3.2 no aplica adecuadamente el mecanismo de protecci\u00f3n CONFIG_STRICT_DEVMEM, lo que permite a usuarios locales leer o escribir en ubicaciones de la memoria del kernel en el primer megabyte (y eludir restricciones de acceso de asignaci\u00f3n de slab) a trav\u00e9s de una aplicaci\u00f3n que abre el archivo /dev/mem, relacionado con arch/x86/mm/init.c y drivers/char/mem.c"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-732"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.91", "matchCriteriaId": "67FEA8F8-276F-454D-AD5C-E76D8B3A95FC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.10.107", "matchCriteriaId": "314F9C88-C8E1-46EF-8119-538C824ED137"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.74", "matchCriteriaId": "75647580-464B-4AEF-8DE2-F17D1748F182"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.16.46", "matchCriteriaId": "C21D8DE7-A83C-4010-8DC6-FAAECB47D229"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.50", "matchCriteriaId": "853F13A0-776A-4078-B05B-7C1E20D4973D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.41", "matchCriteriaId": "9019BEC9-FE77-4506-A019-B8B4D8BCEBAE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.63", "matchCriteriaId": "16332D79-B98D-4D8D-BEA8-C1815D93CA8B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.9.24", "matchCriteriaId": "9035A20D-ECBA-4959-8F7F-EF1BFE41A7AF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.10", "versionEndExcluding": "4.10.12", "matchCriteriaId": "59CCD4BB-9F0D-405F-B1DD-ACEDF5BB2EDD"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:-:*:*:*", "matchCriteriaId": "CB66DB75-2B16-4EBF-9B93-CE49D8086E41"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:esm:*:*:*", "matchCriteriaId": "815D70A8-47D3-459C-A32C-9FEACA0659D1"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a4866aa812518ed1a37d8ea0c881dc946409de94", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.debian.org/security/2017/dsa-3945", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2017/04/16/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/97690", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2669", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:1854", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git/commit/?id=b8f254aa17f720053054c4ecff3920973a83b9d6", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a4866aa812518ed1a37d8ea0c881dc946409de94", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3583-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3583-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a4866aa812518ed1a37d8ea0c881dc946409de94"}}