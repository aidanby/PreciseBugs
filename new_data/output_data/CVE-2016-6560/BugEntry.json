{"buggy_code": ["/*\n * CDDL HEADER START\n *\n * The contents of this file are subject to the terms of the\n * Common Development and Distribution License (the \"License\").\n * You may not use this file except in compliance with the License.\n *\n * You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE\n * or http://www.opensolaris.org/os/licensing.\n * See the License for the specific language governing permissions\n * and limitations under the License.\n *\n * When distributing Covered Code, include this CDDL HEADER in each\n * file and include the License file at usr/src/OPENSOLARIS.LICENSE.\n * If applicable, add the following below this CDDL HEADER, with the\n * fields enclosed by brackets \"[]\" replaced with your own identifying\n * information: Portions Copyright [yyyy] [name of copyright owner]\n *\n * CDDL HEADER END\n */\n/*\n * Copyright 2009 Sun Microsystems, Inc.  All rights reserved.\n * Use is subject to license terms.\n */\n\n/*\n * Copyright (c) 2009, Intel Corporation\n * All rights reserved.\n */\n\n/*       Copyright (c) 1990, 1991 UNIX System Laboratories, Inc.\t*/\n/*       Copyright (c) 1984, 1986, 1987, 1988, 1989, 1990 AT&T\t\t*/\n/*         All Rights Reserved\t\t\t\t\t\t*/\n\n/*       Copyright (c) 1987, 1988 Microsoft Corporation\t\t\t*/\n/*         All Rights Reserved\t\t\t\t\t\t*/\n\n/*\n * Copyright 2015 Joyent, Inc.\n */\n\n#include <sys/errno.h>\n#include <sys/asm_linkage.h>\n\n#if defined(__lint)\n#include <sys/types.h>\n#include <sys/systm.h>\n#else\t/* __lint */\n#include \"assym.h\"\n#endif\t/* __lint */\n\n#define\tKCOPY_MIN_SIZE\t128\t/* Must be >= 16 bytes */\n#define\tXCOPY_MIN_SIZE\t128\t/* Must be >= 16 bytes */\n/*\n * Non-temopral access (NTA) alignment requirement\n */\n#define\tNTA_ALIGN_SIZE\t4\t/* Must be at least 4-byte aligned */\n#define\tNTA_ALIGN_MASK\t_CONST(NTA_ALIGN_SIZE-1)\n#define\tCOUNT_ALIGN_SIZE\t16\t/* Must be at least 16-byte aligned */\n#define\tCOUNT_ALIGN_MASK\t_CONST(COUNT_ALIGN_SIZE-1)\n\n/*\n * With the introduction of Broadwell, Intel has introduced supervisor mode\n * access protection -- SMAP. SMAP forces the kernel to set certain bits to\n * enable access of user pages (AC in rflags, defines as PS_ACHK in\n * <sys/psw.h>). One of the challenges is that the implementation of many of the\n * userland copy routines directly use the kernel ones. For example, copyin and\n * copyout simply go and jump to the do_copy_fault label and traditionally let\n * those deal with the return for them. In fact, changing that is a can of frame\n * pointers.\n *\n * Rules and Constraints:\n *\n * 1. For anything that's not in copy.s, we have it do explicit calls to the\n * smap related code. It usually is in a position where it is able to. This is\n * restricted to the following three places: DTrace, resume() in swtch.s and\n * on_fault/no_fault. If you want to add it somewhere else, we should be\n * thinking twice.\n *\n * 2. We try to toggle this at the smallest window possible. This means that if\n * we take a fault, need to try to use a copyop in copyin() or copyout(), or any\n * other function, we will always leave with SMAP enabled (the kernel cannot\n * access user pages).\n *\n * 3. None of the *_noerr() or ucopy/uzero routines should toggle SMAP. They are\n * explicitly only allowed to be called while in an on_fault()/no_fault() handler,\n * which already takes care of ensuring that SMAP is enabled and disabled. Note\n * this means that when under an on_fault()/no_fault() handler, one must not\n * call the non-*_noeer() routines.\n *\n * 4. The first thing we should do after coming out of an lofault handler is to\n * make sure that we call smap_enable again to ensure that we are safely\n * protected, as more often than not, we will have disabled smap to get there.\n *\n * 5. The SMAP functions, smap_enable and smap_disable may not touch any\n * registers beyond those done by the call and ret. These routines may be called\n * from arbitrary contexts in copy.s where we have slightly more special ABIs in\n * place.\n *\n * 6. For any inline user of SMAP, the appropriate SMAP_ENABLE_INSTR and\n * SMAP_DISABLE_INSTR macro should be used (except for smap_enable() and\n * smap_disable()). If the number of these is changed, you must update the\n * constants SMAP_ENABLE_COUNT and SMAP_DISABLE_COUNT below.\n *\n * 7. Note, at this time SMAP is not implemented for the 32-bit kernel. There is\n * no known technical reason preventing it from being enabled.\n *\n * 8. Generally this .s file is processed by a K&R style cpp. This means that it\n * really has a lot of feelings about whitespace. In particular, if you have a\n * macro FOO with the arguments FOO(1, 3), the second argument is in fact ' 3'.\n *\n * 9. The smap_enable and smap_disable functions should not generally be called.\n * They exist such that DTrace and on_trap() may use them, that's it.\n *\n * 10. In general, the kernel has its own value for rflags that gets used. This\n * is maintained in a few different places which vary based on how the thread\n * comes into existence and whether it's a user thread. In general, when the\n * kernel takes a trap, it always will set ourselves to a known set of flags,\n * mainly as part of ENABLE_INTR_FLAGS and F_OFF and F_ON. These ensure that\n * PS_ACHK is cleared for us. In addition, when using the sysenter instruction,\n * we mask off PS_ACHK off via the AMD_SFMASK MSR. See init_cpu_syscall() for\n * where that gets masked off.\n */\n\n/*\n * The optimal 64-bit bcopy and kcopy for modern x86 processors uses\n * \"rep smovq\" for large sizes. Performance data shows that many calls to\n * bcopy/kcopy/bzero/kzero operate on small buffers. For best performance for\n * these small sizes unrolled code is used. For medium sizes loops writing\n * 64-bytes per loop are used. Transition points were determined experimentally.\n */ \n#define BZERO_USE_REP\t(1024)\n#define BCOPY_DFLT_REP\t(128)\n#define\tBCOPY_NHM_REP\t(768)\n\n/*\n * Copy a block of storage, returning an error code if `from' or\n * `to' takes a kernel pagefault which cannot be resolved.\n * Returns errno value on pagefault error, 0 if all ok\n */\n\n/*\n * I'm sorry about these macros, but copy.s is unsurprisingly sensitive to\n * additional call instructions.\n */\n#if defined(__amd64)\n#define\tSMAP_DISABLE_COUNT\t16\n#define\tSMAP_ENABLE_COUNT\t26\n#elif defined(__i386)\n#define\tSMAP_DISABLE_COUNT\t0\n#define\tSMAP_ENABLE_COUNT\t0\n#endif\n\n#define\tSMAP_DISABLE_INSTR(ITER)\t\t\\\n\t.globl\t_smap_disable_patch_/**/ITER;\t\\\n\t_smap_disable_patch_/**/ITER/**/:;\t\\\n\tnop; nop; nop;\n\n#define\tSMAP_ENABLE_INSTR(ITER)\t\t\t\\\n\t.globl\t_smap_enable_patch_/**/ITER;\t\\\n\t_smap_enable_patch_/**/ITER/**/:;\t\\\n\tnop; nop; nop;\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nkcopy(const void *from, void *to, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n\t.globl\tkernelbase\n\t.globl\tpostbootkernelbase\n\n#if defined(__amd64)\n\n\tENTRY(kcopy)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi \t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.kcopy_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_kcopy_copyerr(%rip), %rcx\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\ndo_copy_fault:\n\tmovq\tT_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%rcx, T_LOFAULT(%r9)\t/* new lofault */\n\tcall\tbcopy_altentry\n\txorl\t%eax, %eax\t\t/* return 0 (success) */\n\tSMAP_ENABLE_INSTR(0)\n\n\t/*\n\t * A fault during do_copy_fault is indicated through an errno value\n\t * in %rax and we iretq from the trap handler to here.\n\t */\n_kcopy_copyerr:\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n\tleave\n\tret\n\tSET_SIZE(kcopy)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_COUNT\t16\n\n\tENTRY(kcopy)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%ebp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%ebp)\n\tjnb\t1f\n0:\tpushl\t$.kcopy_panic_msg\n\tcall\tpanic\n1:\tpopl\t%ebp\n#endif\n\tlea\t_kcopy_copyerr, %eax\t/* lofault value */\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\ndo_copy_fault:\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\t\t/* setup stack frame */\n\tpushl\t%esi\n\tpushl\t%edi\t\t\t/* save registers */\n\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tmovl\tARG_FROM(%ebp), %esi\n\tmovl\tARG_TO(%ebp), %edi\n\tshrl\t$2, %ecx\t\t/* word count */\n\trep\n\t  smovl\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tandl\t$3, %ecx\t\t/* bytes left over */\n\trep\n\t  smovb\n\txorl\t%eax, %eax\n\n\t/*\n\t * A fault during do_copy_fault is indicated through an errno value\n\t * in %eax and we iret from the trap handler to here.\n\t */\n_kcopy_copyerr:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n\tret\n\tSET_SIZE(kcopy)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/*\n * Copy a block of storage.  Similar to kcopy but uses non-temporal\n * instructions.\n */\n\n/* ARGSUSED */\nint\nkcopy_nta(const void *from, void *to, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n#define\tCOPY_LOOP_INIT(src, dst, cnt)\t\\\n\taddq\tcnt, src;\t\t\t\\\n\taddq\tcnt, dst;\t\t\t\\\n\tshrq\t$3, cnt;\t\t\t\\\n\tneg\tcnt\n\n\t/* Copy 16 bytes per loop.  Uses %rax and %r8 */\n#define\tCOPY_LOOP_BODY(src, dst, cnt)\t\\\n\tprefetchnta\t0x100(src, cnt, 8);\t\\\n\tmovq\t(src, cnt, 8), %rax;\t\t\\\n\tmovq\t0x8(src, cnt, 8), %r8;\t\t\\\n\tmovnti\t%rax, (dst, cnt, 8);\t\t\\\n\tmovnti\t%r8, 0x8(dst, cnt, 8);\t\t\\\n\taddq\t$2, cnt\n\n\tENTRY(kcopy_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi \t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.kcopy_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_kcopy_nta_copyerr(%rip), %rcx\t/* doesn't set rflags */\n\tjnz\tdo_copy_fault\t\t/* use regular access */\n\t/*\n\t * Make sure cnt is >= KCOPY_MIN_SIZE\n\t */\n\tcmpq\t$KCOPY_MIN_SIZE, %rdx\n\tjb\tdo_copy_fault\n\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\tdo_copy_fault\n\n\tALTENTRY(do_copy_fault_nta)\n\tmovq    %gs:CPU_THREAD, %r9     /* %r9 = thread addr */\n\tmovq    T_LOFAULT(%r9), %r11    /* save the current lofault */\n\tmovq    %rcx, T_LOFAULT(%r9)    /* new lofault */\n\n\t/*\n\t * COPY_LOOP_BODY uses %rax and %r8\n\t */\n\tCOPY_LOOP_INIT(%rdi, %rsi, %rdx)\n2:\tCOPY_LOOP_BODY(%rdi, %rsi, %rdx)\n\tjnz\t2b\n\n\tmfence\n\txorl\t%eax, %eax\t\t/* return 0 (success) */\n\tSMAP_ENABLE_INSTR(1)\n\n_kcopy_nta_copyerr:\n\tmovq\t%r11, T_LOFAULT(%r9)    /* restore original lofault */\n\tleave\n\tret\n\tSET_SIZE(do_copy_fault_nta)\n\tSET_SIZE(kcopy_nta)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_COUNT\t16\n\n#define\tCOPY_LOOP_INIT(src, dst, cnt)\t\\\n\taddl\tcnt, src;\t\t\t\\\n\taddl\tcnt, dst;\t\t\t\\\n\tshrl\t$3, cnt;\t\t\t\\\n\tneg\tcnt\n\n#define\tCOPY_LOOP_BODY(src, dst, cnt)\t\\\n\tprefetchnta\t0x100(src, cnt, 8);\t\\\n\tmovl\t(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, (dst, cnt, 8);\t\t\\\n\tmovl\t0x4(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0x4(dst, cnt, 8);\t\t\\\n\tmovl\t0x8(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0x8(dst, cnt, 8);\t\t\\\n\tmovl\t0xc(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0xc(dst, cnt, 8);\t\t\\\n\taddl\t$2, cnt\n\n\t/*\n\t * kcopy_nta is not implemented for 32-bit as no performance\n\t * improvement was shown.  We simply jump directly to kcopy\n\t * and discard the 4 arguments.\n\t */\n\tENTRY(kcopy_nta)\n\tjmp\tkcopy\n\n\tlea\t_kcopy_nta_copyerr, %eax\t/* lofault value */\n\tALTENTRY(do_copy_fault_nta)\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\t\t/* setup stack frame */\n\tpushl\t%esi\n\tpushl\t%edi\n\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\t/* COPY_LOOP_BODY needs to use %esi */\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tmovl\tARG_FROM(%ebp), %edi\n\tmovl\tARG_TO(%ebp), %eax\n\tCOPY_LOOP_INIT(%edi, %eax, %ecx)\n1:\tCOPY_LOOP_BODY(%edi, %eax, %ecx)\n\tjnz\t1b\n\tmfence\n\n\txorl\t%eax, %eax\n_kcopy_nta_copyerr:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%esi\n\tleave\n\tret\n\tSET_SIZE(do_copy_fault_nta)\n\tSET_SIZE(kcopy_nta)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nvoid\nbcopy(const void *from, void *to, size_t count)\n{}\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(bcopy)\n#ifdef DEBUG\n\torq\t%rdx, %rdx\t\t/* %rdx = count */\n\tjz\t1f\n\tcmpq\tpostbootkernelbase(%rip), %rdi\t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\t\t\n\tjnb\t1f\n0:\tleaq\t.bcopy_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\t/*\n\t * bcopy_altentry() is called from kcopy, i.e., do_copy_fault.\n\t * kcopy assumes that bcopy doesn't touch %r9 and %r11. If bcopy\n\t * uses these registers in future they must be saved and restored.\n\t */\n\tALTENTRY(bcopy_altentry)\ndo_copy:\n#define\tL(s) .bcopy/**/s\n\tcmpq\t$0x50, %rdx\t\t/* 80 */\n\tjge\tbcopy_ck_size\n\n\t/*\n\t * Performance data shows many caller's copy small buffers. So for\n\t * best perf for these sizes unrolled code is used. Store data without\n\t * worrying about alignment.\n\t */\n\tleaq\tL(fwdPxQx)(%rip), %r10\n\taddq\t%rdx, %rdi\n\taddq\t%rdx, %rsi\n\tmovslq\t(%r10,%rdx,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t.p2align 4\nL(fwdPxQx):\n\t.int       L(P0Q0)-L(fwdPxQx)\t/* 0 */\n\t.int       L(P1Q0)-L(fwdPxQx)\n\t.int       L(P2Q0)-L(fwdPxQx)\n\t.int       L(P3Q0)-L(fwdPxQx)\n\t.int       L(P4Q0)-L(fwdPxQx)\n\t.int       L(P5Q0)-L(fwdPxQx)\n\t.int       L(P6Q0)-L(fwdPxQx)\n\t.int       L(P7Q0)-L(fwdPxQx) \n\n\t.int       L(P0Q1)-L(fwdPxQx)\t/* 8 */\n\t.int       L(P1Q1)-L(fwdPxQx)\n\t.int       L(P2Q1)-L(fwdPxQx)\n\t.int       L(P3Q1)-L(fwdPxQx)\n\t.int       L(P4Q1)-L(fwdPxQx)\n\t.int       L(P5Q1)-L(fwdPxQx)\n\t.int       L(P6Q1)-L(fwdPxQx)\n\t.int       L(P7Q1)-L(fwdPxQx) \n\n\t.int       L(P0Q2)-L(fwdPxQx)\t/* 16 */\n\t.int       L(P1Q2)-L(fwdPxQx)\n\t.int       L(P2Q2)-L(fwdPxQx)\n\t.int       L(P3Q2)-L(fwdPxQx)\n\t.int       L(P4Q2)-L(fwdPxQx)\n\t.int       L(P5Q2)-L(fwdPxQx)\n\t.int       L(P6Q2)-L(fwdPxQx)\n\t.int       L(P7Q2)-L(fwdPxQx) \n\n\t.int       L(P0Q3)-L(fwdPxQx)\t/* 24 */\n\t.int       L(P1Q3)-L(fwdPxQx)\n\t.int       L(P2Q3)-L(fwdPxQx)\n\t.int       L(P3Q3)-L(fwdPxQx)\n\t.int       L(P4Q3)-L(fwdPxQx)\n\t.int       L(P5Q3)-L(fwdPxQx)\n\t.int       L(P6Q3)-L(fwdPxQx)\n\t.int       L(P7Q3)-L(fwdPxQx) \n\n\t.int       L(P0Q4)-L(fwdPxQx)\t/* 32 */\n\t.int       L(P1Q4)-L(fwdPxQx)\n\t.int       L(P2Q4)-L(fwdPxQx)\n\t.int       L(P3Q4)-L(fwdPxQx)\n\t.int       L(P4Q4)-L(fwdPxQx)\n\t.int       L(P5Q4)-L(fwdPxQx)\n\t.int       L(P6Q4)-L(fwdPxQx)\n\t.int       L(P7Q4)-L(fwdPxQx) \n\n\t.int       L(P0Q5)-L(fwdPxQx)\t/* 40 */\n\t.int       L(P1Q5)-L(fwdPxQx)\n\t.int       L(P2Q5)-L(fwdPxQx)\n\t.int       L(P3Q5)-L(fwdPxQx)\n\t.int       L(P4Q5)-L(fwdPxQx)\n\t.int       L(P5Q5)-L(fwdPxQx)\n\t.int       L(P6Q5)-L(fwdPxQx)\n\t.int       L(P7Q5)-L(fwdPxQx) \n\n\t.int       L(P0Q6)-L(fwdPxQx)\t/* 48 */\n\t.int       L(P1Q6)-L(fwdPxQx)\n\t.int       L(P2Q6)-L(fwdPxQx)\n\t.int       L(P3Q6)-L(fwdPxQx)\n\t.int       L(P4Q6)-L(fwdPxQx)\n\t.int       L(P5Q6)-L(fwdPxQx)\n\t.int       L(P6Q6)-L(fwdPxQx)\n\t.int       L(P7Q6)-L(fwdPxQx) \n\n\t.int       L(P0Q7)-L(fwdPxQx)\t/* 56 */\n\t.int       L(P1Q7)-L(fwdPxQx)\n\t.int       L(P2Q7)-L(fwdPxQx)\n\t.int       L(P3Q7)-L(fwdPxQx)\n\t.int       L(P4Q7)-L(fwdPxQx)\n\t.int       L(P5Q7)-L(fwdPxQx)\n\t.int       L(P6Q7)-L(fwdPxQx)\n\t.int       L(P7Q7)-L(fwdPxQx) \n\n\t.int       L(P0Q8)-L(fwdPxQx)\t/* 64 */\n\t.int       L(P1Q8)-L(fwdPxQx)\n\t.int       L(P2Q8)-L(fwdPxQx)\n\t.int       L(P3Q8)-L(fwdPxQx)\n\t.int       L(P4Q8)-L(fwdPxQx)\n\t.int       L(P5Q8)-L(fwdPxQx)\n\t.int       L(P6Q8)-L(fwdPxQx)\n\t.int       L(P7Q8)-L(fwdPxQx)\n\n\t.int       L(P0Q9)-L(fwdPxQx)\t/* 72 */\n\t.int       L(P1Q9)-L(fwdPxQx)\n\t.int       L(P2Q9)-L(fwdPxQx)\n\t.int       L(P3Q9)-L(fwdPxQx)\n\t.int       L(P4Q9)-L(fwdPxQx)\n\t.int       L(P5Q9)-L(fwdPxQx)\n\t.int       L(P6Q9)-L(fwdPxQx)\n\t.int       L(P7Q9)-L(fwdPxQx)\t/* 79 */\n\n\t.p2align 4\nL(P0Q9):\n\tmov    -0x48(%rdi), %rcx\n\tmov    %rcx, -0x48(%rsi)\nL(P0Q8):\n\tmov    -0x40(%rdi), %r10\n\tmov    %r10, -0x40(%rsi)\nL(P0Q7):\n\tmov    -0x38(%rdi), %r8\n\tmov    %r8, -0x38(%rsi)\nL(P0Q6):\n\tmov    -0x30(%rdi), %rcx\n\tmov    %rcx, -0x30(%rsi)\nL(P0Q5):\n\tmov    -0x28(%rdi), %r10\n\tmov    %r10, -0x28(%rsi)\nL(P0Q4):\n\tmov    -0x20(%rdi), %r8\n\tmov    %r8, -0x20(%rsi)\nL(P0Q3):\n\tmov    -0x18(%rdi), %rcx\n\tmov    %rcx, -0x18(%rsi)\nL(P0Q2):\n\tmov    -0x10(%rdi), %r10\n\tmov    %r10, -0x10(%rsi)\nL(P0Q1):\n\tmov    -0x8(%rdi), %r8\n\tmov    %r8, -0x8(%rsi)\nL(P0Q0):                                   \n\tret   \n\n\t.p2align 4\nL(P1Q9):\n\tmov    -0x49(%rdi), %r8\n\tmov    %r8, -0x49(%rsi)\nL(P1Q8):\n\tmov    -0x41(%rdi), %rcx\n\tmov    %rcx, -0x41(%rsi)\nL(P1Q7):\n\tmov    -0x39(%rdi), %r10\n\tmov    %r10, -0x39(%rsi)\nL(P1Q6):\n\tmov    -0x31(%rdi), %r8\n\tmov    %r8, -0x31(%rsi)\nL(P1Q5):\n\tmov    -0x29(%rdi), %rcx\n\tmov    %rcx, -0x29(%rsi)\nL(P1Q4):\n\tmov    -0x21(%rdi), %r10\n\tmov    %r10, -0x21(%rsi)\nL(P1Q3):\n\tmov    -0x19(%rdi), %r8\n\tmov    %r8, -0x19(%rsi)\nL(P1Q2):\n\tmov    -0x11(%rdi), %rcx\n\tmov    %rcx, -0x11(%rsi)\nL(P1Q1):\n\tmov    -0x9(%rdi), %r10\n\tmov    %r10, -0x9(%rsi)\nL(P1Q0):\n\tmovzbq -0x1(%rdi), %r8\n\tmov    %r8b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P2Q9):\n\tmov    -0x4a(%rdi), %r8\n\tmov    %r8, -0x4a(%rsi)\nL(P2Q8):\n\tmov    -0x42(%rdi), %rcx\n\tmov    %rcx, -0x42(%rsi)\nL(P2Q7):\n\tmov    -0x3a(%rdi), %r10\n\tmov    %r10, -0x3a(%rsi)\nL(P2Q6):\n\tmov    -0x32(%rdi), %r8\n\tmov    %r8, -0x32(%rsi)\nL(P2Q5):\n\tmov    -0x2a(%rdi), %rcx\n\tmov    %rcx, -0x2a(%rsi)\nL(P2Q4):\n\tmov    -0x22(%rdi), %r10\n\tmov    %r10, -0x22(%rsi)\nL(P2Q3):\n\tmov    -0x1a(%rdi), %r8\n\tmov    %r8, -0x1a(%rsi)\nL(P2Q2):\n\tmov    -0x12(%rdi), %rcx\n\tmov    %rcx, -0x12(%rsi)\nL(P2Q1):\n\tmov    -0xa(%rdi), %r10\n\tmov    %r10, -0xa(%rsi)\nL(P2Q0):\n\tmovzwq -0x2(%rdi), %r8\n\tmov    %r8w, -0x2(%rsi)\n\tret   \n\n\t.p2align 4\nL(P3Q9):\n\tmov    -0x4b(%rdi), %r8\n\tmov    %r8, -0x4b(%rsi)\nL(P3Q8):\n\tmov    -0x43(%rdi), %rcx\n\tmov    %rcx, -0x43(%rsi)\nL(P3Q7):\n\tmov    -0x3b(%rdi), %r10\n\tmov    %r10, -0x3b(%rsi)\nL(P3Q6):\n\tmov    -0x33(%rdi), %r8\n\tmov    %r8, -0x33(%rsi)\nL(P3Q5):\n\tmov    -0x2b(%rdi), %rcx\n\tmov    %rcx, -0x2b(%rsi)\nL(P3Q4):\n\tmov    -0x23(%rdi), %r10\n\tmov    %r10, -0x23(%rsi)\nL(P3Q3):\n\tmov    -0x1b(%rdi), %r8\n\tmov    %r8, -0x1b(%rsi)\nL(P3Q2):\n\tmov    -0x13(%rdi), %rcx\n\tmov    %rcx, -0x13(%rsi)\nL(P3Q1):\n\tmov    -0xb(%rdi), %r10\n\tmov    %r10, -0xb(%rsi)\n\t/*\n\t * These trailing loads/stores have to do all their loads 1st, \n\t * then do the stores.\n\t */\nL(P3Q0):\n\tmovzwq -0x3(%rdi), %r8\n\tmovzbq -0x1(%rdi), %r10\n\tmov    %r8w, -0x3(%rsi)\n\tmov    %r10b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P4Q9):\n\tmov    -0x4c(%rdi), %r8\n\tmov    %r8, -0x4c(%rsi)\nL(P4Q8):\n\tmov    -0x44(%rdi), %rcx\n\tmov    %rcx, -0x44(%rsi)\nL(P4Q7):\n\tmov    -0x3c(%rdi), %r10\n\tmov    %r10, -0x3c(%rsi)\nL(P4Q6):\n\tmov    -0x34(%rdi), %r8\n\tmov    %r8, -0x34(%rsi)\nL(P4Q5):\n\tmov    -0x2c(%rdi), %rcx\n\tmov    %rcx, -0x2c(%rsi)\nL(P4Q4):\n\tmov    -0x24(%rdi), %r10\n\tmov    %r10, -0x24(%rsi)\nL(P4Q3):\n\tmov    -0x1c(%rdi), %r8\n\tmov    %r8, -0x1c(%rsi)\nL(P4Q2):\n\tmov    -0x14(%rdi), %rcx\n\tmov    %rcx, -0x14(%rsi)\nL(P4Q1):\n\tmov    -0xc(%rdi), %r10\n\tmov    %r10, -0xc(%rsi)\nL(P4Q0):\n\tmov    -0x4(%rdi), %r8d\n\tmov    %r8d, -0x4(%rsi)\n\tret   \n\n\t.p2align 4\nL(P5Q9):\n\tmov    -0x4d(%rdi), %r8\n\tmov    %r8, -0x4d(%rsi)\nL(P5Q8):\n\tmov    -0x45(%rdi), %rcx\n\tmov    %rcx, -0x45(%rsi)\nL(P5Q7):\n\tmov    -0x3d(%rdi), %r10\n\tmov    %r10, -0x3d(%rsi)\nL(P5Q6):\n\tmov    -0x35(%rdi), %r8\n\tmov    %r8, -0x35(%rsi)\nL(P5Q5):\n\tmov    -0x2d(%rdi), %rcx\n\tmov    %rcx, -0x2d(%rsi)\nL(P5Q4):\n\tmov    -0x25(%rdi), %r10\n\tmov    %r10, -0x25(%rsi)\nL(P5Q3):\n\tmov    -0x1d(%rdi), %r8\n\tmov    %r8, -0x1d(%rsi)\nL(P5Q2):\n\tmov    -0x15(%rdi), %rcx\n\tmov    %rcx, -0x15(%rsi)\nL(P5Q1):\n\tmov    -0xd(%rdi), %r10\n\tmov    %r10, -0xd(%rsi)\nL(P5Q0):\n\tmov    -0x5(%rdi), %r8d\n\tmovzbq -0x1(%rdi), %r10\n\tmov    %r8d, -0x5(%rsi)\n\tmov    %r10b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P6Q9):\n\tmov    -0x4e(%rdi), %r8\n\tmov    %r8, -0x4e(%rsi)\nL(P6Q8):\n\tmov    -0x46(%rdi), %rcx\n\tmov    %rcx, -0x46(%rsi)\nL(P6Q7):\n\tmov    -0x3e(%rdi), %r10\n\tmov    %r10, -0x3e(%rsi)\nL(P6Q6):\n\tmov    -0x36(%rdi), %r8\n\tmov    %r8, -0x36(%rsi)\nL(P6Q5):\n\tmov    -0x2e(%rdi), %rcx\n\tmov    %rcx, -0x2e(%rsi)\nL(P6Q4):\n\tmov    -0x26(%rdi), %r10\n\tmov    %r10, -0x26(%rsi)\nL(P6Q3):\n\tmov    -0x1e(%rdi), %r8\n\tmov    %r8, -0x1e(%rsi)\nL(P6Q2):\n\tmov    -0x16(%rdi), %rcx\n\tmov    %rcx, -0x16(%rsi)\nL(P6Q1):\n\tmov    -0xe(%rdi), %r10\n\tmov    %r10, -0xe(%rsi)\nL(P6Q0):\n\tmov    -0x6(%rdi), %r8d\n\tmovzwq -0x2(%rdi), %r10\n\tmov    %r8d, -0x6(%rsi)\n\tmov    %r10w, -0x2(%rsi)\n\tret   \n\n\t.p2align 4\nL(P7Q9):\n\tmov    -0x4f(%rdi), %r8\n\tmov    %r8, -0x4f(%rsi)\nL(P7Q8):\n\tmov    -0x47(%rdi), %rcx\n\tmov    %rcx, -0x47(%rsi)\nL(P7Q7):\n\tmov    -0x3f(%rdi), %r10\n\tmov    %r10, -0x3f(%rsi)\nL(P7Q6):\n\tmov    -0x37(%rdi), %r8\n\tmov    %r8, -0x37(%rsi)\nL(P7Q5):\n\tmov    -0x2f(%rdi), %rcx\n\tmov    %rcx, -0x2f(%rsi)\nL(P7Q4):\n\tmov    -0x27(%rdi), %r10\n\tmov    %r10, -0x27(%rsi)\nL(P7Q3):\n\tmov    -0x1f(%rdi), %r8\n\tmov    %r8, -0x1f(%rsi)\nL(P7Q2):\n\tmov    -0x17(%rdi), %rcx\n\tmov    %rcx, -0x17(%rsi)\nL(P7Q1):\n\tmov    -0xf(%rdi), %r10\n\tmov    %r10, -0xf(%rsi)\nL(P7Q0):\n\tmov    -0x7(%rdi), %r8d\n\tmovzwq -0x3(%rdi), %r10\n\tmovzbq -0x1(%rdi), %rcx\n\tmov    %r8d, -0x7(%rsi)\n\tmov    %r10w, -0x3(%rsi)\n\tmov    %cl, -0x1(%rsi)\n\tret   \n\n\t/*\n\t * For large sizes rep smovq is fastest.\n\t * Transition point determined experimentally as measured on\n\t * Intel Xeon processors (incl. Nehalem and previous generations) and\n\t * AMD Opteron. The transition value is patched at boot time to avoid\n\t * memory reference hit.\n\t */\n\t.globl bcopy_patch_start\nbcopy_patch_start:\n\tcmpq\t$BCOPY_NHM_REP, %rdx\n\t.globl bcopy_patch_end\nbcopy_patch_end:\n\n\t.p2align 4\n\t.globl bcopy_ck_size\nbcopy_ck_size:\n\tcmpq\t$BCOPY_DFLT_REP, %rdx\n\tjge\tL(use_rep)\n\n\t/*\n\t * Align to a 8-byte boundary. Avoids penalties from unaligned stores\n\t * as well as from stores spanning cachelines.\n\t */\n\ttest\t$0x7, %rsi\n\tjz\tL(aligned_loop)\n\ttest\t$0x1, %rsi\n\tjz\t2f\n\tmovzbq\t(%rdi), %r8\n\tdec\t%rdx\n\tinc\t%rdi\n\tmov\t%r8b, (%rsi)\n\tinc\t%rsi\n2:\n\ttest\t$0x2, %rsi\n\tjz\t4f\n\tmovzwq\t(%rdi), %r8\n\tsub\t$0x2, %rdx\n\tadd\t$0x2, %rdi\n\tmov\t%r8w, (%rsi)\n\tadd\t$0x2, %rsi\n4:\n\ttest\t$0x4, %rsi\n\tjz\tL(aligned_loop)\n\tmov\t(%rdi), %r8d\n\tsub\t$0x4, %rdx\n\tadd\t$0x4, %rdi\n\tmov\t%r8d, (%rsi)\n\tadd\t$0x4, %rsi\n\n\t/*\n\t * Copy 64-bytes per loop\n\t */\n\t.p2align 4\nL(aligned_loop):\n\tmov\t(%rdi), %r8\n\tmov\t0x8(%rdi), %r10\n\tlea\t-0x40(%rdx), %rdx\n\tmov\t%r8, (%rsi)\n\tmov\t%r10, 0x8(%rsi)\n\tmov\t0x10(%rdi), %rcx\n\tmov\t0x18(%rdi), %r8\n\tmov\t%rcx, 0x10(%rsi)\n\tmov\t%r8, 0x18(%rsi)\n\n\tcmp\t$0x40, %rdx\n\tmov\t0x20(%rdi), %r10\n\tmov\t0x28(%rdi), %rcx\n\tmov\t%r10, 0x20(%rsi)\n\tmov\t%rcx, 0x28(%rsi)\n\tmov\t0x30(%rdi), %r8\n\tmov\t0x38(%rdi), %r10\n\tlea\t0x40(%rdi), %rdi\n\tmov\t%r8, 0x30(%rsi)\n\tmov\t%r10, 0x38(%rsi)\n\tlea\t0x40(%rsi), %rsi\n\tjge\tL(aligned_loop)\n\n\t/*\n\t * Copy remaining bytes (0-63)\n\t */\nL(do_remainder):\n\tleaq\tL(fwdPxQx)(%rip), %r10\n\taddq\t%rdx, %rdi\n\taddq\t%rdx, %rsi\n\tmovslq\t(%r10,%rdx,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t/*\n\t * Use rep smovq. Clear remainder via unrolled code\n\t */\n\t.p2align 4\nL(use_rep):\n\txchgq\t%rdi, %rsi\t\t/* %rsi = source, %rdi = destination */\n\tmovq\t%rdx, %rcx\t\t/* %rcx = count */\n\tshrq\t$3, %rcx\t\t/* 8-byte word count */\n\trep\n\t  smovq\n\n\txchgq\t%rsi, %rdi\t\t/* %rdi = src, %rsi = destination */\n\tandq\t$7, %rdx\t\t/* remainder */\n\tjnz\tL(do_remainder)\n\tret\n#undef\tL\n\n#ifdef DEBUG\n\t/*\n\t * Setup frame on the run-time stack. The end of the input argument\n\t * area must be aligned on a 16 byte boundary. The stack pointer %rsp,\n\t * always points to the end of the latest allocated stack frame.\n\t * panic(const char *format, ...) is a varargs function. When a\n\t * function taking variable arguments is called, %rax must be set\n\t * to eight times the number of floating point parameters passed\n\t * to the function in SSE registers.\n\t */\ncall_panic:\n\tpushq\t%rbp\t\t\t/* align stack properly */\n\tmovq\t%rsp, %rbp\n\txorl\t%eax, %eax\t\t/* no variable arguments */\n\tcall\tpanic\t\t\t/* %rdi = format string */\n#endif\n\tSET_SIZE(bcopy_altentry)\n\tSET_SIZE(bcopy)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t4\n#define\tARG_TO\t\t8\n#define\tARG_COUNT\t12\n\n\tENTRY(bcopy)\n#ifdef DEBUG\n\tmovl\tARG_COUNT(%esp), %eax\n\torl\t%eax, %eax\n\tjz\t1f\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%esp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%esp)\n\tjnb\t1f\n0:\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.bcopy_panic_msg\n\tcall\tpanic\n1:\n#endif\ndo_copy:\n\tmovl\t%esi, %eax\t\t/* save registers */\n\tmovl\t%edi, %edx\n\tmovl\tARG_COUNT(%esp), %ecx\n\tmovl\tARG_FROM(%esp), %esi\n\tmovl\tARG_TO(%esp), %edi\n\n\tshrl\t$2, %ecx\t\t/* word count */\n\trep\n\t  smovl\n\tmovl\tARG_COUNT(%esp), %ecx\n\tandl\t$3, %ecx\t\t/* bytes left over */\n\trep\n\t  smovb\n\tmovl\t%eax, %esi\t\t/* restore registers */\n\tmovl\t%edx, %edi\n\tret\n\tSET_SIZE(bcopy)\n\n#undef\tARG_COUNT\n#undef\tARG_FROM\n#undef\tARG_TO\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n\n/*\n * Zero a block of storage, returning an error code if we\n * take a kernel pagefault which cannot be resolved.\n * Returns errno value on pagefault error, 0 if all ok\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nkzero(void *addr, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(kzero)\n#ifdef DEBUG\n        cmpq\tpostbootkernelbase(%rip), %rdi\t/* %rdi = addr */\n        jnb\t0f\n        leaq\t.kzero_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n0:\n#endif\n\t/*\n\t * pass lofault value as 3rd argument for fault return \n\t */\n\tleaq\t_kzeroerr(%rip), %rdx\n\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\tmovq\tT_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%rdx, T_LOFAULT(%r9)\t/* new lofault */\n\tcall\tbzero_altentry\n\txorl\t%eax, %eax\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tret\n\t/*\n\t * A fault during bzero is indicated through an errno value\n\t * in %rax when we iretq to here.\n\t */\n_kzeroerr:\n\taddq\t$8, %rsp\t\t/* pop bzero_altentry call ret addr */\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tret\n\tSET_SIZE(kzero)\n\n#elif defined(__i386)\n\n#define\tARG_ADDR\t8\n#define\tARG_COUNT\t12\n\n\tENTRY(kzero)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tpostbootkernelbase, %eax\n        cmpl\t%eax, ARG_ADDR(%ebp)\n        jnb\t0f\n        pushl   $.kzero_panic_msg\n        call    panic\n0:\tpopl\t%ebp\n#endif\n\tlea\t_kzeroerr, %eax\t\t/* kzeroerr is lofault value */\n\n\tpushl\t%ebp\t\t\t/* save stack base */\n\tmovl\t%esp, %ebp\t\t/* set new stack base */\n\tpushl\t%edi\t\t\t/* save %edi */\n\n\tmov\t%gs:CPU_THREAD, %edx\t\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\t/* get size in bytes */\n\tmovl\tARG_ADDR(%ebp), %edi\t/* %edi <- address of bytes to clear */\n\tshrl\t$2, %ecx\t\t/* Count of double words to zero */\n\txorl\t%eax, %eax\t\t/* sstol val */\n\trep\n\t  sstol\t\t\t/* %ecx contains words to clear (%eax=0) */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\t/* get size in bytes */\n\tandl\t$3, %ecx\t\t/* do mod 4 */\n\trep\n\t  sstob\t\t\t/* %ecx contains residual bytes to clear */\n\n\t/*\n\t * A fault during kzero is indicated through an errno value\n\t * in %eax when we iret to here.\n\t */\n_kzeroerr:\n\tpopl\t%edi\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%edi\n\tpopl\t%ebp\n\tret\n\tSET_SIZE(kzero)\n\n#undef\tARG_ADDR\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Zero a block of storage.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nvoid\nbzero(void *addr, size_t count)\n{}\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(bzero)\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi\t/* %rdi = addr */\n\tjnb\t0f\n\tleaq\t.bzero_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n0:\n#endif\n\tALTENTRY(bzero_altentry)\ndo_zero:\n#define\tL(s) .bzero/**/s\n\txorl\t%eax, %eax\n\n\tcmpq\t$0x50, %rsi\t\t/* 80 */\n\tjge\tL(ck_align)\n\n\t/*\n\t * Performance data shows many caller's are zeroing small buffers. So\n\t * for best perf for these sizes unrolled code is used. Store zeros\n\t * without worrying about alignment.\n\t */\n\tleaq\tL(setPxQx)(%rip), %r10\n\taddq\t%rsi, %rdi\n\tmovslq\t(%r10,%rsi,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t.p2align 4\nL(setPxQx):\n\t.int       L(P0Q0)-L(setPxQx)\t/* 0 */\n\t.int       L(P1Q0)-L(setPxQx)\n\t.int       L(P2Q0)-L(setPxQx)\n\t.int       L(P3Q0)-L(setPxQx)\n\t.int       L(P4Q0)-L(setPxQx)\n\t.int       L(P5Q0)-L(setPxQx)\n\t.int       L(P6Q0)-L(setPxQx)\n\t.int       L(P7Q0)-L(setPxQx) \n\n\t.int       L(P0Q1)-L(setPxQx)\t/* 8 */\n\t.int       L(P1Q1)-L(setPxQx)\n\t.int       L(P2Q1)-L(setPxQx)\n\t.int       L(P3Q1)-L(setPxQx)\n\t.int       L(P4Q1)-L(setPxQx)\n\t.int       L(P5Q1)-L(setPxQx)\n\t.int       L(P6Q1)-L(setPxQx)\n\t.int       L(P7Q1)-L(setPxQx) \n\n\t.int       L(P0Q2)-L(setPxQx)\t/* 16 */\n\t.int       L(P1Q2)-L(setPxQx)\n\t.int       L(P2Q2)-L(setPxQx)\n\t.int       L(P3Q2)-L(setPxQx)\n\t.int       L(P4Q2)-L(setPxQx)\n\t.int       L(P5Q2)-L(setPxQx)\n\t.int       L(P6Q2)-L(setPxQx)\n\t.int       L(P7Q2)-L(setPxQx) \n\n\t.int       L(P0Q3)-L(setPxQx)\t/* 24 */\n\t.int       L(P1Q3)-L(setPxQx)\n\t.int       L(P2Q3)-L(setPxQx)\n\t.int       L(P3Q3)-L(setPxQx)\n\t.int       L(P4Q3)-L(setPxQx)\n\t.int       L(P5Q3)-L(setPxQx)\n\t.int       L(P6Q3)-L(setPxQx)\n\t.int       L(P7Q3)-L(setPxQx) \n\n\t.int       L(P0Q4)-L(setPxQx)\t/* 32 */\n\t.int       L(P1Q4)-L(setPxQx)\n\t.int       L(P2Q4)-L(setPxQx)\n\t.int       L(P3Q4)-L(setPxQx)\n\t.int       L(P4Q4)-L(setPxQx)\n\t.int       L(P5Q4)-L(setPxQx)\n\t.int       L(P6Q4)-L(setPxQx)\n\t.int       L(P7Q4)-L(setPxQx) \n\n\t.int       L(P0Q5)-L(setPxQx)\t/* 40 */\n\t.int       L(P1Q5)-L(setPxQx)\n\t.int       L(P2Q5)-L(setPxQx)\n\t.int       L(P3Q5)-L(setPxQx)\n\t.int       L(P4Q5)-L(setPxQx)\n\t.int       L(P5Q5)-L(setPxQx)\n\t.int       L(P6Q5)-L(setPxQx)\n\t.int       L(P7Q5)-L(setPxQx) \n\n\t.int       L(P0Q6)-L(setPxQx)\t/* 48 */\n\t.int       L(P1Q6)-L(setPxQx)\n\t.int       L(P2Q6)-L(setPxQx)\n\t.int       L(P3Q6)-L(setPxQx)\n\t.int       L(P4Q6)-L(setPxQx)\n\t.int       L(P5Q6)-L(setPxQx)\n\t.int       L(P6Q6)-L(setPxQx)\n\t.int       L(P7Q6)-L(setPxQx) \n\n\t.int       L(P0Q7)-L(setPxQx)\t/* 56 */\n\t.int       L(P1Q7)-L(setPxQx)\n\t.int       L(P2Q7)-L(setPxQx)\n\t.int       L(P3Q7)-L(setPxQx)\n\t.int       L(P4Q7)-L(setPxQx)\n\t.int       L(P5Q7)-L(setPxQx)\n\t.int       L(P6Q7)-L(setPxQx)\n\t.int       L(P7Q7)-L(setPxQx) \n\n\t.int       L(P0Q8)-L(setPxQx)\t/* 64 */\n\t.int       L(P1Q8)-L(setPxQx)\n\t.int       L(P2Q8)-L(setPxQx)\n\t.int       L(P3Q8)-L(setPxQx)\n\t.int       L(P4Q8)-L(setPxQx)\n\t.int       L(P5Q8)-L(setPxQx)\n\t.int       L(P6Q8)-L(setPxQx)\n\t.int       L(P7Q8)-L(setPxQx)\n\n\t.int       L(P0Q9)-L(setPxQx)\t/* 72 */\n\t.int       L(P1Q9)-L(setPxQx)\n\t.int       L(P2Q9)-L(setPxQx)\n\t.int       L(P3Q9)-L(setPxQx)\n\t.int       L(P4Q9)-L(setPxQx)\n\t.int       L(P5Q9)-L(setPxQx)\n\t.int       L(P6Q9)-L(setPxQx)\n\t.int       L(P7Q9)-L(setPxQx)\t/* 79 */\n\n\t.p2align 4\nL(P0Q9): mov    %rax, -0x48(%rdi)\nL(P0Q8): mov    %rax, -0x40(%rdi)\nL(P0Q7): mov    %rax, -0x38(%rdi)\nL(P0Q6): mov    %rax, -0x30(%rdi)\nL(P0Q5): mov    %rax, -0x28(%rdi)\nL(P0Q4): mov    %rax, -0x20(%rdi)\nL(P0Q3): mov    %rax, -0x18(%rdi)\nL(P0Q2): mov    %rax, -0x10(%rdi)\nL(P0Q1): mov    %rax, -0x8(%rdi)\nL(P0Q0): \n\t ret\n\n\t.p2align 4\nL(P1Q9): mov    %rax, -0x49(%rdi)\nL(P1Q8): mov    %rax, -0x41(%rdi)\nL(P1Q7): mov    %rax, -0x39(%rdi)\nL(P1Q6): mov    %rax, -0x31(%rdi)\nL(P1Q5): mov    %rax, -0x29(%rdi)\nL(P1Q4): mov    %rax, -0x21(%rdi)\nL(P1Q3): mov    %rax, -0x19(%rdi)\nL(P1Q2): mov    %rax, -0x11(%rdi)\nL(P1Q1): mov    %rax, -0x9(%rdi)\nL(P1Q0): mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P2Q9): mov    %rax, -0x4a(%rdi)\nL(P2Q8): mov    %rax, -0x42(%rdi)\nL(P2Q7): mov    %rax, -0x3a(%rdi)\nL(P2Q6): mov    %rax, -0x32(%rdi)\nL(P2Q5): mov    %rax, -0x2a(%rdi)\nL(P2Q4): mov    %rax, -0x22(%rdi)\nL(P2Q3): mov    %rax, -0x1a(%rdi)\nL(P2Q2): mov    %rax, -0x12(%rdi)\nL(P2Q1): mov    %rax, -0xa(%rdi)\nL(P2Q0): mov    %ax, -0x2(%rdi)\n\t ret\n\n\t.p2align 4\nL(P3Q9): mov    %rax, -0x4b(%rdi)\nL(P3Q8): mov    %rax, -0x43(%rdi)\nL(P3Q7): mov    %rax, -0x3b(%rdi)\nL(P3Q6): mov    %rax, -0x33(%rdi)\nL(P3Q5): mov    %rax, -0x2b(%rdi)\nL(P3Q4): mov    %rax, -0x23(%rdi)\nL(P3Q3): mov    %rax, -0x1b(%rdi)\nL(P3Q2): mov    %rax, -0x13(%rdi)\nL(P3Q1): mov    %rax, -0xb(%rdi)\nL(P3Q0): mov    %ax, -0x3(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P4Q9): mov    %rax, -0x4c(%rdi)\nL(P4Q8): mov    %rax, -0x44(%rdi)\nL(P4Q7): mov    %rax, -0x3c(%rdi)\nL(P4Q6): mov    %rax, -0x34(%rdi)\nL(P4Q5): mov    %rax, -0x2c(%rdi)\nL(P4Q4): mov    %rax, -0x24(%rdi)\nL(P4Q3): mov    %rax, -0x1c(%rdi)\nL(P4Q2): mov    %rax, -0x14(%rdi)\nL(P4Q1): mov    %rax, -0xc(%rdi)\nL(P4Q0): mov    %eax, -0x4(%rdi)\n\t ret\n\n\t.p2align 4\nL(P5Q9): mov    %rax, -0x4d(%rdi)\nL(P5Q8): mov    %rax, -0x45(%rdi)\nL(P5Q7): mov    %rax, -0x3d(%rdi)\nL(P5Q6): mov    %rax, -0x35(%rdi)\nL(P5Q5): mov    %rax, -0x2d(%rdi)\nL(P5Q4): mov    %rax, -0x25(%rdi)\nL(P5Q3): mov    %rax, -0x1d(%rdi)\nL(P5Q2): mov    %rax, -0x15(%rdi)\nL(P5Q1): mov    %rax, -0xd(%rdi)\nL(P5Q0): mov    %eax, -0x5(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P6Q9): mov    %rax, -0x4e(%rdi)\nL(P6Q8): mov    %rax, -0x46(%rdi)\nL(P6Q7): mov    %rax, -0x3e(%rdi)\nL(P6Q6): mov    %rax, -0x36(%rdi)\nL(P6Q5): mov    %rax, -0x2e(%rdi)\nL(P6Q4): mov    %rax, -0x26(%rdi)\nL(P6Q3): mov    %rax, -0x1e(%rdi)\nL(P6Q2): mov    %rax, -0x16(%rdi)\nL(P6Q1): mov    %rax, -0xe(%rdi)\nL(P6Q0): mov    %eax, -0x6(%rdi)\n\t mov    %ax, -0x2(%rdi)\n\t ret\n\n\t.p2align 4\nL(P7Q9): mov    %rax, -0x4f(%rdi)\nL(P7Q8): mov    %rax, -0x47(%rdi)\nL(P7Q7): mov    %rax, -0x3f(%rdi)\nL(P7Q6): mov    %rax, -0x37(%rdi)\nL(P7Q5): mov    %rax, -0x2f(%rdi)\nL(P7Q4): mov    %rax, -0x27(%rdi)\nL(P7Q3): mov    %rax, -0x1f(%rdi)\nL(P7Q2): mov    %rax, -0x17(%rdi)\nL(P7Q1): mov    %rax, -0xf(%rdi)\nL(P7Q0): mov    %eax, -0x7(%rdi)\n\t mov    %ax, -0x3(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t/*\n\t * Align to a 16-byte boundary. Avoids penalties from unaligned stores\n\t * as well as from stores spanning cachelines. Note 16-byte alignment\n\t * is better in case where rep sstosq is used.\n\t */\n\t.p2align 4\nL(ck_align):\n\ttest\t$0xf, %rdi\n\tjz\tL(aligned_now)\n\ttest\t$1, %rdi\n\tjz\t2f\n\tmov\t%al, (%rdi)\n\tdec\t%rsi\n\tlea\t1(%rdi),%rdi\n2:\n\ttest\t$2, %rdi\n\tjz\t4f\n\tmov\t%ax, (%rdi)\n\tsub\t$2, %rsi\n\tlea\t2(%rdi),%rdi\n4:\n\ttest\t$4, %rdi\n\tjz\t8f\n\tmov\t%eax, (%rdi)\n\tsub\t$4, %rsi\n\tlea\t4(%rdi),%rdi\n8:\n\ttest\t$8, %rdi\n\tjz\tL(aligned_now)\n\tmov\t%rax, (%rdi)\n\tsub\t$8, %rsi\n\tlea\t8(%rdi),%rdi\n\n\t/*\n\t * For large sizes rep sstoq is fastest.\n\t * Transition point determined experimentally as measured on\n\t * Intel Xeon processors (incl. Nehalem) and AMD Opteron.\n\t */\nL(aligned_now):\n\tcmp\t$BZERO_USE_REP, %rsi\n\tjg\tL(use_rep)\n\n\t/*\n\t * zero 64-bytes per loop\n\t */\n\t.p2align 4\nL(bzero_loop):\n\tleaq\t-0x40(%rsi), %rsi\n\tcmpq\t$0x40, %rsi\n\tmovq\t%rax, (%rdi) \n\tmovq\t%rax, 0x8(%rdi) \n\tmovq\t%rax, 0x10(%rdi) \n\tmovq\t%rax, 0x18(%rdi) \n\tmovq\t%rax, 0x20(%rdi) \n\tmovq\t%rax, 0x28(%rdi) \n\tmovq\t%rax, 0x30(%rdi) \n\tmovq\t%rax, 0x38(%rdi) \n\tleaq\t0x40(%rdi), %rdi\n\tjge\tL(bzero_loop)\n\n\t/*\n\t * Clear any remaining bytes..\n\t */\n9:\n\tleaq\tL(setPxQx)(%rip), %r10\n\taddq\t%rsi, %rdi\n\tmovslq\t(%r10,%rsi,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t/*\n\t * Use rep sstoq. Clear any remainder via unrolled code\n\t */\n\t.p2align 4\nL(use_rep):\n\tmovq\t%rsi, %rcx\t\t/* get size in bytes */\n\tshrq\t$3, %rcx\t\t/* count of 8-byte words to zero */\n\trep\n\t  sstoq\t\t\t\t/* %rcx = words to clear (%rax=0) */\n\tandq\t$7, %rsi\t\t/* remaining bytes */\n\tjnz\t9b\n\tret\n#undef\tL\n\tSET_SIZE(bzero_altentry)\n\tSET_SIZE(bzero)\n\n#elif defined(__i386)\n\n#define\tARG_ADDR\t4\n#define\tARG_COUNT\t8\n\n\tENTRY(bzero)\n#ifdef DEBUG\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_ADDR(%esp)\n\tjnb\t0f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.bzero_panic_msg\n\tcall\tpanic\n0:\n#endif\ndo_zero:\n\tmovl\t%edi, %edx\n\tmovl\tARG_COUNT(%esp), %ecx\n\tmovl\tARG_ADDR(%esp), %edi\n\tshrl\t$2, %ecx\n\txorl\t%eax, %eax\n\trep\n\t  sstol\n\tmovl\tARG_COUNT(%esp), %ecx\n\tandl\t$3, %ecx\n\trep\n\t  sstob\n\tmovl\t%edx, %edi\n\tret\n\tSET_SIZE(bzero)\n\n#undef\tARG_ADDR\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Transfer data to and from user space -\n * Note that these routines can cause faults\n * It is assumed that the kernel has nothing at\n * less than KERNELBASE in the virtual address space.\n *\n * Note that copyin(9F) and copyout(9F) are part of the\n * DDI/DKI which specifies that they return '-1' on \"errors.\"\n *\n * Sigh.\n *\n * So there's two extremely similar routines - xcopyin_nta() and\n * xcopyout_nta() which return the errno that we've faithfully computed.\n * This allows other callers (e.g. uiomove(9F)) to work correctly.\n * Given that these are used pretty heavily, we expand the calling\n * sequences inline for all flavours (rather than making wrappers).\n */\n\n/*\n * Copy user data to kernel space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyin(const void *uaddr, void *kaddr, size_t count)\n{ return (0); }\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyin)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyin_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_copyin_err(%rip), %rcx\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t3f\t\t\t/* take copyop if uaddr > kernelbase */\n\tSMAP_DISABLE_INSTR(0)\n\tjmp\tdo_copy_fault\t\t/* Takes care of leave for us */\n\n_copyin_err:\n\tSMAP_ENABLE_INSTR(2)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\t\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_COPYIN(%rax)\n\n2:\tmovl\t$-1, %eax\t\n\tleave\n\tret\n\tSET_SIZE(copyin)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n\n\tENTRY(copyin)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyin_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyin_err, %eax\n\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copy_fault\n\tjmp\t3f\n\n_copyin_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYIN(%eax)\n\n2:\tmovl\t$-1, %eax\n\tret\n\tSET_SIZE(copyin)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nxcopyin_nta(const void *uaddr, void *kaddr, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(xcopyin_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t * %rcx is consumed in this routine so we don't need to save\n\t * it.\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.xcopyin_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_xcopyin_err(%rip), %rcx\t/* doesn't set rflags */\n\tjnz\t6f\t\t\t/* use regular access */\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpq\t$XCOPY_MIN_SIZE, %rdx\n\tjae\t5f\n6:\n\tSMAP_DISABLE_INSTR(1)\n\tjmp\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n5:\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\t6b\t\n\tleaq\t_xcopyin_nta_err(%rip), %rcx\t/* doesn't set rflags */\n\tSMAP_DISABLE_INSTR(2)\n\tjmp\tdo_copy_fault_nta\t/* use non-temporal access */\n\t\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %rax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyin_err:\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n_xcopyin_nta_err:\n\tSMAP_ENABLE_INSTR(3)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %r8\n\tcmpq\t$0, %r8\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_XCOPYIN(%r8)\n\n2:\tleave\n\tret\n\tSET_SIZE(xcopyin_nta)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n#define\tARG_COUNT\t12\n#define\tARG_CACHED\t16\n\n\t.globl\tuse_sse_copy\n\n\tENTRY(xcopyin_nta)\n\tmovl\tkernelbase, %ecx\n\tlea\t_xcopyin_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpl\t$0, use_sse_copy\t/* no sse support */\n\tjz\tdo_copy_fault\n\n\tcmpl\t$0, ARG_CACHED(%esp)\t/* copy_cached hint set? */\n\tjnz\tdo_copy_fault\n\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpl\t$XCOPY_MIN_SIZE, ARG_COUNT(%esp)\n\tjb\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovl\tARG_UADDR(%esp), %ecx\n\torl\tARG_KADDR(%esp), %ecx\n\tandl\t$NTA_ALIGN_MASK, %ecx\n\torl\tARG_COUNT(%esp), %ecx\n\tandl\t$COUNT_ALIGN_MASK, %ecx\n\tjnz\tdo_copy_fault\n\n\tjmp\tdo_copy_fault_nta\t/* use regular access */\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %eax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyin_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tcmpl\t$0, T_COPYOPS(%edx)\n\tjz\t2f\n\tmovl\tT_COPYOPS(%edx), %eax\n\tjmp\t*CP_XCOPYIN(%eax)\n\n2:\trep; \tret\t/* use 2 byte return instruction when branch target */\n\t\t\t/* AMD Software Optimization Guide - Section 6.2 */\n\tSET_SIZE(xcopyin_nta)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n#undef\tARG_COUNT\n#undef\tARG_CACHED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy kernel data to user space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyout(const void *kaddr, void *uaddr, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyout)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyout_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_copyout_err(%rip), %rcx\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t3f\t\t\t/* take copyop if uaddr > kernelbase */\n\tSMAP_DISABLE_INSTR(3)\n\tjmp\tdo_copy_fault\t\t/* Calls leave for us */\n\n_copyout_err:\n\tSMAP_ENABLE_INSTR(4)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_COPYOUT(%rax)\n\n2:\tmovl\t$-1, %eax\n\tleave\n\tret\n\tSET_SIZE(copyout)\n\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n\n\tENTRY(copyout)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyout_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyout_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copy_fault\n\tjmp\t3f\n\t\n_copyout_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYOUT(%eax)\n\n2:\tmovl\t$-1, %eax\n\tret\n\tSET_SIZE(copyout)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nxcopyout_nta(const void *kaddr, void *uaddr, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(xcopyout_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.xcopyout_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_xcopyout_err(%rip), %rcx\n\tjnz\t6f\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpq\t$XCOPY_MIN_SIZE, %rdx\n\tjae\t5f\n6:\n\tSMAP_DISABLE_INSTR(4)\n\tjmp\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n5:\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\t6b\t\n\tleaq\t_xcopyout_nta_err(%rip), %rcx\n\tSMAP_DISABLE_INSTR(5)\n\tcall\tdo_copy_fault_nta\n\tSMAP_ENABLE_INSTR(5)\n\tret\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %rax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyout_err:\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n_xcopyout_nta_err:\n\tSMAP_ENABLE_INSTR(6)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %r8\n\tcmpq\t$0, %r8\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_XCOPYOUT(%r8)\n\n2:\tleave\n\tret\n\tSET_SIZE(xcopyout_nta)\n\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n#define\tARG_COUNT\t12\n#define\tARG_CACHED\t16\n\n\tENTRY(xcopyout_nta)\n\tmovl\tkernelbase, %ecx\n\tlea\t_xcopyout_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpl\t$0, use_sse_copy\t/* no sse support */\n\tjz\tdo_copy_fault\n\n\tcmpl\t$0, ARG_CACHED(%esp)\t/* copy_cached hint set? */\n\tjnz\tdo_copy_fault\n\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpl\t$XCOPY_MIN_SIZE, %edx\n\tjb\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovl\tARG_UADDR(%esp), %ecx\n\torl\tARG_KADDR(%esp), %ecx\n\tandl\t$NTA_ALIGN_MASK, %ecx\n\torl\tARG_COUNT(%esp), %ecx\n\tandl\t$COUNT_ALIGN_MASK, %ecx\n\tjnz\tdo_copy_fault\n\tjmp\tdo_copy_fault_nta\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %eax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyout_err:\n\t/ restore the original lofault\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/ original lofault\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tcmpl\t$0, T_COPYOPS(%edx)\n\tjz\t2f\n\tmovl\tT_COPYOPS(%edx), %eax\n\tjmp\t*CP_XCOPYOUT(%eax)\n\n2:\trep;\tret\t/* use 2 byte return instruction when branch target */\n\t\t\t/* AMD Software Optimization Guide - Section 6.2 */\n\tSET_SIZE(xcopyout_nta)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n#undef\tARG_COUNT\n#undef\tARG_CACHED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from one point to another in\n * the kernel address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopystr(const char *from, char *to, size_t maxlength, size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copystr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\t%rax, %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.copystr_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tmovq\tT_LOFAULT(%r9), %r8\t/* pass current lofault value as */\n\t\t\t\t\t/* 5th argument to do_copystr */\n\txorl\t%r10d,%r10d\t\t/* pass smap restore need in %r10d */\n\t\t\t\t\t/* as a non-ABI 6th arg */\ndo_copystr:\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\tmovq    T_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%r8, T_LOFAULT(%r9)\t/* new lofault */\n\n\tmovq\t%rdx, %r8\t\t/* save maxlength */\n\n\tcmpq\t$0, %rdx\t\t/* %rdx = maxlength */\n\tje\tcopystr_enametoolong\t/* maxlength == 0 */\n\ncopystr_loop:\n\tdecq\t%r8\n\tmovb\t(%rdi), %al\n\tincq\t%rdi\n\tmovb\t%al, (%rsi)\n\tincq\t%rsi\n\tcmpb\t$0, %al\n\tje\tcopystr_null\t\t/* null char */\n\tcmpq\t$0, %r8\n\tjne\tcopystr_loop\n\ncopystr_enametoolong:\n\tmovl\t$ENAMETOOLONG, %eax\n\tjmp\tcopystr_out\n\ncopystr_null:\n\txorl\t%eax, %eax\t\t/* no error */\n\ncopystr_out:\n\tcmpq\t$0, %rcx\t\t/* want length? */\n\tje\tcopystr_smap\t\t/* no */\n\tsubq\t%r8, %rdx\t\t/* compute length and store it */\n\tmovq\t%rdx, (%rcx)\n\ncopystr_smap:\n\tcmpl\t$0, %r10d\n\tjz\tcopystr_done\n\tSMAP_ENABLE_INSTR(7)\n\ncopystr_done:\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tleave\n\tret\n\tSET_SIZE(copystr)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_MAXLEN\t16\n#define\tARG_LENCOPIED\t20\n\n\tENTRY(copystr)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%esp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%esp)\n\tjnb\t1f\n0:\tpushl\t$.copystr_panic_msg\n\tcall\tpanic\n1:\tpopl\t%ebp\n#endif\n\t/* get the current lofault address */\n\tmovl\t%gs:CPU_THREAD, %eax\n\tmovl\tT_LOFAULT(%eax), %eax\ndo_copystr:\n\tpushl\t%ebp\t\t\t/* setup stack frame */\n\tmovl\t%esp, %ebp\n\tpushl\t%ebx\t\t\t/* save registers */\n\tpushl\t%edi\n\n\tmovl\t%gs:CPU_THREAD, %ebx\t\n\tmovl\tT_LOFAULT(%ebx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%ebx)\t/* new lofault */\n\n\tmovl\tARG_MAXLEN(%ebp), %ecx\n\tcmpl\t$0, %ecx\n\tje\tcopystr_enametoolong\t/* maxlength == 0 */\n\n\tmovl\tARG_FROM(%ebp), %ebx\t/* source address */\n\tmovl\tARG_TO(%ebp), %edx\t/* destination address */\n\ncopystr_loop:\n\tdecl\t%ecx\n\tmovb\t(%ebx), %al\n\tincl\t%ebx\t\n\tmovb\t%al, (%edx)\n\tincl\t%edx\n\tcmpb\t$0, %al\n\tje\tcopystr_null\t\t/* null char */\n\tcmpl\t$0, %ecx\n\tjne\tcopystr_loop\n\ncopystr_enametoolong:\n\tmovl\t$ENAMETOOLONG, %eax\n\tjmp\tcopystr_out\n\ncopystr_null:\n\txorl\t%eax, %eax\t\t/* no error */\n\ncopystr_out:\n\tcmpl\t$0, ARG_LENCOPIED(%ebp)\t/* want length? */\n\tje\tcopystr_done\t\t/* no */\n\tmovl\tARG_MAXLEN(%ebp), %edx\n\tsubl\t%ecx, %edx\t\t/* compute length and store it */\n\tmovl\tARG_LENCOPIED(%ebp), %ecx\n\tmovl\t%edx, (%ecx)\n\ncopystr_done:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %ebx\t\n\tmovl\t%edi, T_LOFAULT(%ebx)\t/* restore the original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n\tret\t\n\tSET_SIZE(copystr)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_MAXLEN\n#undef\tARG_LENCOPIED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from the user address space into\n * the kernel address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyinstr(const char *uaddr, char *kaddr, size_t maxlength,\n    size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyinstr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$32, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\tmovq\t%rcx, 0x18(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyinstr_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 5th argument to do_copystr\n\t * do_copystr expects whether or not we need smap in %r10d\n\t */\n\tleaq\t_copyinstr_error(%rip), %r8\n\tmovl\t$1, %r10d\n\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tSMAP_DISABLE_INSTR(6)\n\tjmp\tdo_copystr\n4:\n\tmovq\t%gs:CPU_THREAD, %r9\n\tjmp\t3f\n\n_copyinstr_error:\n\tSMAP_ENABLE_INSTR(8)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tmovq\t0x18(%rsp), %rcx\n\tleave\n\tjmp\t*CP_COPYINSTR(%rax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tleave\n\tret\n\tSET_SIZE(copyinstr)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n\n\tENTRY(copyinstr)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyinstr_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyinstr_error, %eax\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copystr\n\tmovl\t%gs:CPU_THREAD, %edx\n\tjmp\t3f\n\n_copyinstr_error:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYINSTR(%eax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tret\n\tSET_SIZE(copyinstr)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from the kernel\n * address space to the user address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyoutstr(const char *kaddr, char *uaddr, size_t maxlength,\n    size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyoutstr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$32, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\tmovq\t%rcx, 0x18(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyoutstr_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\t/*\n\t * pass lofault value as 5th argument to do_copystr\n\t * pass one as 6th argument to do_copystr in %r10d\n\t */\n\tleaq\t_copyoutstr_error(%rip), %r8\n\tmovl\t$1, %r10d\n\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tSMAP_DISABLE_INSTR(7)\n\tjmp\tdo_copystr\n4:\n\tmovq\t%gs:CPU_THREAD, %r9\n\tjmp\t3f\n\n_copyoutstr_error:\n\tSMAP_ENABLE_INSTR(9)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tmovq\t0x18(%rsp), %rcx\n\tleave\n\tjmp\t*CP_COPYOUTSTR(%rax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tleave\n\tret\n\tSET_SIZE(copyoutstr)\t\n\t\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n\n\tENTRY(copyoutstr)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyoutstr_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyoutstr_error, %eax\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copystr\n\tmovl\t%gs:CPU_THREAD, %edx\n\tjmp\t3f\n\n_copyoutstr_error:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* restore the original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYOUTSTR(%eax)\n\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tret\n\tSET_SIZE(copyoutstr)\n\t\n#undef\tARG_KADDR\n#undef\tARG_UADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Since all of the fuword() variants are so similar, we have a macro to spit\n * them out.  This allows us to create DTrace-unobservable functions easily.\n */\n\t\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/* ARGSUSED */\nint\nfuword64(const void *addr, uint64_t *dst)\n{ return (0); }\n\n#endif\n\n/* ARGSUSED */\nint\nfuword32(const void *addr, uint32_t *dst)\n{ return (0); }\n\n/* ARGSUSED */\nint\nfuword16(const void *addr, uint16_t *dst)\n{ return (0); }\n\n/* ARGSUSED */\nint\nfuword8(const void *addr, uint8_t *dst)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n/*\n * Note that we don't save and reload the arguments here\n * because their values are not altered in the copy path.\n * Additionally, when successful, the smap_enable jmp will\n * actually return us to our original caller.\n */\n\n#define\tFUWORD(NAME, INSTR, REG, COPYOP, DISNUM, EN1, EN2)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovq\t%gs:CPU_THREAD, %r9;\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tleaq\t_flt_/**/NAME, %rdx;\t\t\\\n\tmovq\t%rdx, T_LOFAULT(%r9);\t\t\\\n\tSMAP_DISABLE_INSTR(DISNUM)\t\t\\\n\tINSTR\t(%rdi), REG;\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n\tINSTR\tREG, (%rsi);\t\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN1)\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN2)\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovq\tT_COPYOPS(%r9), %rax;\t\t\\\n\tcmpq\t$0, %rax;\t\t\t\\\n\tjz\t2f;\t\t\t\t\\\n\tjmp\t*COPYOP(%rax);\t\t\t\\\n2:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\t\n\tFUWORD(fuword64, movq, %rax, CP_FUWORD64,8,10,11)\n\tFUWORD(fuword32, movl, %eax, CP_FUWORD32,9,12,13)\n\tFUWORD(fuword16, movw, %ax, CP_FUWORD16,10,14,15)\n\tFUWORD(fuword8, movb, %al, CP_FUWORD8,11,16,17)\n\n#elif defined(__i386)\n\n#define\tFUWORD(NAME, INSTR, REG, COPYOP)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t%gs:CPU_THREAD, %ecx;\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n\tcmpl\t%eax, 4(%esp);\t\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tlea\t_flt_/**/NAME, %edx;\t\t\\\n\tmovl\t%edx, T_LOFAULT(%ecx);\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\t(%eax), REG;\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n\tINSTR\tREG, (%edx);\t\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\tT_COPYOPS(%ecx), %eax;\t\t\\\n\tcmpl\t$0, %eax;\t\t\t\\\n\tjz\t2f;\t\t\t\t\\\n\tjmp\t*COPYOP(%eax);\t\t\t\\\n2:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD(fuword32, movl, %eax, CP_FUWORD32)\n\tFUWORD(fuword16, movw, %ax, CP_FUWORD16)\n\tFUWORD(fuword8, movb, %al, CP_FUWORD8)\n\n#endif\t/* __i386 */\n\n#undef\tFUWORD\n\n#endif\t/* __lint */\n\n/*\n * Set user word.\n */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/* ARGSUSED */\nint\nsuword64(void *addr, uint64_t value)\n{ return (0); }\n\n#endif\n\n/* ARGSUSED */\nint\nsuword32(void *addr, uint32_t value)\n{ return (0); }\n\n/* ARGSUSED */\nint\nsuword16(void *addr, uint16_t value)\n{ return (0); }\n\n/* ARGSUSED */\nint\nsuword8(void *addr, uint8_t value)\n{ return (0); }\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n/*\n * Note that we don't save and reload the arguments here\n * because their values are not altered in the copy path.\n */\n\n#define\tSUWORD(NAME, INSTR, REG, COPYOP, DISNUM, EN1, EN2)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovq\t%gs:CPU_THREAD, %r9;\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tleaq\t_flt_/**/NAME, %rdx;\t\t\\\n\tSMAP_DISABLE_INSTR(DISNUM)\t\t\\\n\tmovq\t%rdx, T_LOFAULT(%r9);\t\t\\\n\tINSTR\tREG, (%rdi);\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN1)\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN2)\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovq\tT_COPYOPS(%r9), %rax;\t\t\\\n\tcmpq\t$0, %rax;\t\t\t\\\n\tjz\t3f;\t\t\t\t\\\n\tjmp\t*COPYOP(%rax);\t\t\t\\\n3:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD(suword64, movq, %rsi, CP_SUWORD64,12,18,19)\n\tSUWORD(suword32, movl, %esi, CP_SUWORD32,13,20,21)\n\tSUWORD(suword16, movw, %si, CP_SUWORD16,14,22,23)\n\tSUWORD(suword8, movb, %sil, CP_SUWORD8,15,24,25)\n\n#elif defined(__i386)\n\n#define\tSUWORD(NAME, INSTR, REG, COPYOP)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t%gs:CPU_THREAD, %ecx;\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n\tcmpl\t%eax, 4(%esp);\t\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tlea\t_flt_/**/NAME, %edx;\t\t\\\n\tmovl\t%edx, T_LOFAULT(%ecx);\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\tREG, (%eax);\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\tT_COPYOPS(%ecx), %eax;\t\t\\\n\tcmpl\t$0, %eax;\t\t\t\\\n\tjz\t3f;\t\t\t\t\\\n\tmovl\tCOPYOP(%eax), %ecx;\t\t\\\n\tjmp\t*%ecx;\t\t\t\t\\\n3:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD(suword32, movl, %edx, CP_SUWORD32)\n\tSUWORD(suword16, movw, %dx, CP_SUWORD16)\n\tSUWORD(suword8, movb, %dl, CP_SUWORD8)\n\n#endif\t/* __i386 */\n\n#undef\tSUWORD\n\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/*ARGSUSED*/\nvoid\nfuword64_noerr(const void *addr, uint64_t *dst)\n{}\n\n#endif\n\n/*ARGSUSED*/\nvoid\nfuword32_noerr(const void *addr, uint32_t *dst)\n{}\n\n/*ARGSUSED*/\nvoid\nfuword8_noerr(const void *addr, uint8_t *dst)\n{}\n\n/*ARGSUSED*/\nvoid\nfuword16_noerr(const void *addr, uint16_t *dst)\n{}\n\n#else   /* __lint */\n\n#if defined(__amd64)\n\n#define\tFUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tcmovnbq\tkernelbase(%rip), %rdi;\t\t\\\n\tINSTR\t(%rdi), REG;\t\t\t\\\n\tINSTR\tREG, (%rsi);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD_NOERR(fuword64_noerr, movq, %rax)\n\tFUWORD_NOERR(fuword32_noerr, movl, %eax)\n\tFUWORD_NOERR(fuword16_noerr, movw, %ax)\n\tFUWORD_NOERR(fuword8_noerr, movb, %al)\n\n#elif defined(__i386)\n\n#define\tFUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tcmpl\tkernelbase, %eax;\t\t\\\n\tjb\t1f;\t\t\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n1:\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\t(%eax), REG;\t\t\t\\\n\tINSTR\tREG, (%edx);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD_NOERR(fuword32_noerr, movl, %ecx)\n\tFUWORD_NOERR(fuword16_noerr, movw, %cx)\n\tFUWORD_NOERR(fuword8_noerr, movb, %cl)\n\n#endif\t/* __i386 */\n\n#undef\tFUWORD_NOERR\n\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/*ARGSUSED*/\nvoid\nsuword64_noerr(void *addr, uint64_t value)\n{}\n\n#endif\n\n/*ARGSUSED*/\nvoid\nsuword32_noerr(void *addr, uint32_t value)\n{}\n\n/*ARGSUSED*/\nvoid\nsuword16_noerr(void *addr, uint16_t value)\n{}\n\n/*ARGSUSED*/\nvoid\nsuword8_noerr(void *addr, uint8_t value)\n{}\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n#define\tSUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tcmovnbq\tkernelbase(%rip), %rdi;\t\t\\\n\tINSTR\tREG, (%rdi);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD_NOERR(suword64_noerr, movq, %rsi)\n\tSUWORD_NOERR(suword32_noerr, movl, %esi)\n\tSUWORD_NOERR(suword16_noerr, movw, %si)\n\tSUWORD_NOERR(suword8_noerr, movb, %sil)\n\n#elif defined(__i386)\n\n#define\tSUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tcmpl\tkernelbase, %eax;\t\t\\\n\tjb\t1f;\t\t\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\tREG, (%eax);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD_NOERR(suword32_noerr, movl, %edx)\n\tSUWORD_NOERR(suword16_noerr, movw, %dx)\n\tSUWORD_NOERR(suword8_noerr, movb, %dl)\n\n#endif\t/* __i386 */\n\n#undef\tSUWORD_NOERR\n\n#endif\t/* lint */\n\n\n#if defined(__lint)\n\n/*ARGSUSED*/\nint\nsubyte(void *addr, uchar_t value)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nsubyte_noerr(void *addr, uchar_t value)\n{}\n\n/*ARGSUSED*/\nint\nfulword(const void *addr, ulong_t *valuep)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nfulword_noerr(const void *addr, ulong_t *valuep)\n{}\n\n/*ARGSUSED*/\nint\nsulword(void *addr, ulong_t valuep)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nsulword_noerr(void *addr, ulong_t valuep)\n{}\n\n#else\n\n\t.weak\tsubyte\n\tsubyte=suword8\n\t.weak\tsubyte_noerr\n\tsubyte_noerr=suword8_noerr\n\n#if defined(__amd64)\n\n\t.weak\tfulword\n\tfulword=fuword64\n\t.weak\tfulword_noerr\n\tfulword_noerr=fuword64_noerr\n\t.weak\tsulword\n\tsulword=suword64\n\t.weak\tsulword_noerr\n\tsulword_noerr=suword64_noerr\n\n#elif defined(__i386)\n\n\t.weak\tfulword\n\tfulword=fuword32\n\t.weak\tfulword_noerr\n\tfulword_noerr=fuword32_noerr\n\t.weak\tsulword\n\tsulword=suword32\n\t.weak\tsulword_noerr\n\tsulword_noerr=suword32_noerr\n\n#endif /* __i386 */\n\n#endif /* __lint */\n\n#if defined(__lint)\n\n/*\n * Copy a block of storage - must not overlap (from + len <= to).\n * No fault handler installed (to be called under on_fault())\n */\n\n/* ARGSUSED */\nvoid\ncopyout_noerr(const void *kfrom, void *uto, size_t count)\n{}\n\n/* ARGSUSED */\nvoid\ncopyin_noerr(const void *ufrom, void *kto, size_t count)\n{}\n\n/*\n * Zero a block of storage in user space\n */\n\n/* ARGSUSED */\nvoid\nuzero(void *addr, size_t count)\n{}\n\n/*\n * copy a block of storage in user space\n */\n\n/* ARGSUSED */\nvoid\nucopy(const void *ufrom, void *uto, size_t ulength)\n{}\n\n/*\n * copy a string in user space\n */\n\n/* ARGSUSED */\nvoid\nucopystr(const char *ufrom, char *uto, size_t umaxlength, size_t *lencopied)\n{}\n\n#else /* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyin_noerr)\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kto */\n\tjae\t1f\n\tleaq\t.cpyin_ne_pmsg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\tcmpq\t%rax, %rdi\t\t/* ufrom < kernelbase */\n\tjb\tdo_copy\n\tmovq\t%rax, %rdi\t\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyin_noerr)\n\n\tENTRY(copyout_noerr)\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kfrom */\n\tjae\t1f\n\tleaq\t.cpyout_ne_pmsg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\tcmpq\t%rax, %rsi\t\t/* uto < kernelbase */\n\tjb\tdo_copy\n\tmovq\t%rax, %rsi\t\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyout_noerr)\n\n\tENTRY(uzero)\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tjb\tdo_zero\n\tmovq\t%rax, %rdi\t/* force fault at kernelbase */\n\tjmp\tdo_zero\n\tSET_SIZE(uzero)\n\n\tENTRY(ucopy)\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tcmovaeq\t%rax, %rdi\t/* force fault at kernelbase */\n\tcmpq\t%rax, %rsi\n\tcmovaeq\t%rax, %rsi\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(ucopy)\n\n\t/*\n\t * Note, the frame pointer is required here becuase do_copystr expects\n\t * to be able to pop it off!\n\t */\n\tENTRY(ucopystr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tcmovaeq\t%rax, %rdi\t/* force fault at kernelbase */\n\tcmpq\t%rax, %rsi\n\tcmovaeq\t%rax, %rsi\t/* force fault at kernelbase */\n\t/* do_copystr expects lofault address in %r8 */\n\t/* do_copystr expects whether or not we need smap in %r10 */\n\txorl\t%r10d, %r10d\n\tmovq\t%gs:CPU_THREAD, %r8\n\tmovq\tT_LOFAULT(%r8), %r8\n\tjmp\tdo_copystr\n\tSET_SIZE(ucopystr)\n\n#elif defined(__i386)\n\n\tENTRY(copyin_noerr)\n\tmovl\tkernelbase, %eax\n#ifdef DEBUG\n\tcmpl\t%eax, 8(%esp)\n\tjae\t1f\n\tpushl\t$.cpyin_ne_pmsg\n\tcall\tpanic\n1:\n#endif\n\tcmpl\t%eax, 4(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyin_noerr)\n\n\tENTRY(copyout_noerr)\n\tmovl\tkernelbase, %eax\n#ifdef DEBUG\n\tcmpl\t%eax, 4(%esp)\n\tjae\t1f\n\tpushl\t$.cpyout_ne_pmsg\n\tcall\tpanic\n1:\n#endif\n\tcmpl\t%eax, 8(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyout_noerr)\n\n\tENTRY(uzero)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\tdo_zero\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_zero\n\tSET_SIZE(uzero)\n\n\tENTRY(ucopy)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\t1f\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n1:\n\tcmpl\t%eax, 8(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(ucopy)\n\n\tENTRY(ucopystr)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\t1f\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n1:\n\tcmpl\t%eax, 8(%esp)\n\tjb\t2f\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n2:\n\t/* do_copystr expects the lofault address in %eax */\n\tmovl\t%gs:CPU_THREAD, %eax\n\tmovl\tT_LOFAULT(%eax), %eax\n\tjmp\tdo_copystr\n\tSET_SIZE(ucopystr)\n\n#endif\t/* __i386 */\n\n#ifdef DEBUG\n\t.data\n.kcopy_panic_msg:\n\t.string \"kcopy: arguments below kernelbase\"\n.bcopy_panic_msg:\n\t.string \"bcopy: arguments below kernelbase\"\n.kzero_panic_msg:\n        .string \"kzero: arguments below kernelbase\"\n.bzero_panic_msg:\n\t.string\t\"bzero: arguments below kernelbase\"\n.copyin_panic_msg:\n\t.string \"copyin: kaddr argument below kernelbase\"\n.xcopyin_panic_msg:\n\t.string\t\"xcopyin: kaddr argument below kernelbase\"\n.copyout_panic_msg:\n\t.string \"copyout: kaddr argument below kernelbase\"\n.xcopyout_panic_msg:\n\t.string\t\"xcopyout: kaddr argument below kernelbase\"\n.copystr_panic_msg:\n\t.string\t\"copystr: arguments in user space\"\n.copyinstr_panic_msg:\n\t.string\t\"copyinstr: kaddr argument not in kernel address space\"\n.copyoutstr_panic_msg:\n\t.string\t\"copyoutstr: kaddr argument not in kernel address space\"\n.cpyin_ne_pmsg:\n\t.string \"copyin_noerr: argument not in kernel address space\"\n.cpyout_ne_pmsg:\n\t.string \"copyout_noerr: argument not in kernel address space\"\n#endif\n\n#endif\t/* __lint */\n\n/*\n * These functions are used for SMAP, supervisor mode access protection. They\n * are hotpatched to become real instructions when the system starts up which is\n * done in mlsetup() as a part of enabling the other CR4 related features.\n *\n * Generally speaking, smap_disable() is a stac instruction and smap_enable is a\n * clac instruction. It's safe to call these any number of times, and in fact,\n * out of paranoia, the kernel will likely call it at several points.\n */\n\n#if defined(__lint)\n\nvoid\nsmap_enable(void)\n{}\n\nvoid\nsmap_disable(void)\n{}\n\n#else\n\n#if defined (__amd64) || defined(__i386)\n\tENTRY(smap_disable)\n\tnop\n\tnop\n\tnop\n\tret\n\tSET_SIZE(smap_disable)\n\n\tENTRY(smap_enable)\n\tnop\n\tnop\n\tnop\n\tret\n\tSET_SIZE(smap_enable)\n\n#endif /* __amd64 || __i386 */\n\n#endif /* __lint */\n\n#ifndef __lint\n\n.data\n.align \t4\n.globl\t_smap_enable_patch_count\n.type\t_smap_enable_patch_count,@object\n.size\t_smap_enable_patch_count, 4\n_smap_enable_patch_count:\n\t.long\tSMAP_ENABLE_COUNT\n\n.globl\t_smap_disable_patch_count\n.type\t_smap_disable_patch_count,@object\n.size\t_smap_disable_patch_count, 4\n_smap_disable_patch_count:\n\t.long SMAP_DISABLE_COUNT\n\n#endif /* __lint */\n"], "fixing_code": ["/*\n * CDDL HEADER START\n *\n * The contents of this file are subject to the terms of the\n * Common Development and Distribution License (the \"License\").\n * You may not use this file except in compliance with the License.\n *\n * You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE\n * or http://www.opensolaris.org/os/licensing.\n * See the License for the specific language governing permissions\n * and limitations under the License.\n *\n * When distributing Covered Code, include this CDDL HEADER in each\n * file and include the License file at usr/src/OPENSOLARIS.LICENSE.\n * If applicable, add the following below this CDDL HEADER, with the\n * fields enclosed by brackets \"[]\" replaced with your own identifying\n * information: Portions Copyright [yyyy] [name of copyright owner]\n *\n * CDDL HEADER END\n */\n/*\n * Copyright 2009 Sun Microsystems, Inc.  All rights reserved.\n * Use is subject to license terms.\n */\n\n/*\n * Copyright (c) 2009, Intel Corporation\n * All rights reserved.\n */\n\n/*       Copyright (c) 1990, 1991 UNIX System Laboratories, Inc.\t*/\n/*       Copyright (c) 1984, 1986, 1987, 1988, 1989, 1990 AT&T\t\t*/\n/*         All Rights Reserved\t\t\t\t\t\t*/\n\n/*       Copyright (c) 1987, 1988 Microsoft Corporation\t\t\t*/\n/*         All Rights Reserved\t\t\t\t\t\t*/\n\n/*\n * Copyright 2016 Joyent, Inc.\n */\n\n#include <sys/errno.h>\n#include <sys/asm_linkage.h>\n\n#if defined(__lint)\n#include <sys/types.h>\n#include <sys/systm.h>\n#else\t/* __lint */\n#include \"assym.h\"\n#endif\t/* __lint */\n\n#define\tKCOPY_MIN_SIZE\t128\t/* Must be >= 16 bytes */\n#define\tXCOPY_MIN_SIZE\t128\t/* Must be >= 16 bytes */\n/*\n * Non-temopral access (NTA) alignment requirement\n */\n#define\tNTA_ALIGN_SIZE\t4\t/* Must be at least 4-byte aligned */\n#define\tNTA_ALIGN_MASK\t_CONST(NTA_ALIGN_SIZE-1)\n#define\tCOUNT_ALIGN_SIZE\t16\t/* Must be at least 16-byte aligned */\n#define\tCOUNT_ALIGN_MASK\t_CONST(COUNT_ALIGN_SIZE-1)\n\n/*\n * With the introduction of Broadwell, Intel has introduced supervisor mode\n * access protection -- SMAP. SMAP forces the kernel to set certain bits to\n * enable access of user pages (AC in rflags, defines as PS_ACHK in\n * <sys/psw.h>). One of the challenges is that the implementation of many of the\n * userland copy routines directly use the kernel ones. For example, copyin and\n * copyout simply go and jump to the do_copy_fault label and traditionally let\n * those deal with the return for them. In fact, changing that is a can of frame\n * pointers.\n *\n * Rules and Constraints:\n *\n * 1. For anything that's not in copy.s, we have it do explicit calls to the\n * smap related code. It usually is in a position where it is able to. This is\n * restricted to the following three places: DTrace, resume() in swtch.s and\n * on_fault/no_fault. If you want to add it somewhere else, we should be\n * thinking twice.\n *\n * 2. We try to toggle this at the smallest window possible. This means that if\n * we take a fault, need to try to use a copyop in copyin() or copyout(), or any\n * other function, we will always leave with SMAP enabled (the kernel cannot\n * access user pages).\n *\n * 3. None of the *_noerr() or ucopy/uzero routines should toggle SMAP. They are\n * explicitly only allowed to be called while in an on_fault()/no_fault() handler,\n * which already takes care of ensuring that SMAP is enabled and disabled. Note\n * this means that when under an on_fault()/no_fault() handler, one must not\n * call the non-*_noeer() routines.\n *\n * 4. The first thing we should do after coming out of an lofault handler is to\n * make sure that we call smap_enable again to ensure that we are safely\n * protected, as more often than not, we will have disabled smap to get there.\n *\n * 5. The SMAP functions, smap_enable and smap_disable may not touch any\n * registers beyond those done by the call and ret. These routines may be called\n * from arbitrary contexts in copy.s where we have slightly more special ABIs in\n * place.\n *\n * 6. For any inline user of SMAP, the appropriate SMAP_ENABLE_INSTR and\n * SMAP_DISABLE_INSTR macro should be used (except for smap_enable() and\n * smap_disable()). If the number of these is changed, you must update the\n * constants SMAP_ENABLE_COUNT and SMAP_DISABLE_COUNT below.\n *\n * 7. Note, at this time SMAP is not implemented for the 32-bit kernel. There is\n * no known technical reason preventing it from being enabled.\n *\n * 8. Generally this .s file is processed by a K&R style cpp. This means that it\n * really has a lot of feelings about whitespace. In particular, if you have a\n * macro FOO with the arguments FOO(1, 3), the second argument is in fact ' 3'.\n *\n * 9. The smap_enable and smap_disable functions should not generally be called.\n * They exist such that DTrace and on_trap() may use them, that's it.\n *\n * 10. In general, the kernel has its own value for rflags that gets used. This\n * is maintained in a few different places which vary based on how the thread\n * comes into existence and whether it's a user thread. In general, when the\n * kernel takes a trap, it always will set ourselves to a known set of flags,\n * mainly as part of ENABLE_INTR_FLAGS and F_OFF and F_ON. These ensure that\n * PS_ACHK is cleared for us. In addition, when using the sysenter instruction,\n * we mask off PS_ACHK off via the AMD_SFMASK MSR. See init_cpu_syscall() for\n * where that gets masked off.\n */\n\n/*\n * The optimal 64-bit bcopy and kcopy for modern x86 processors uses\n * \"rep smovq\" for large sizes. Performance data shows that many calls to\n * bcopy/kcopy/bzero/kzero operate on small buffers. For best performance for\n * these small sizes unrolled code is used. For medium sizes loops writing\n * 64-bytes per loop are used. Transition points were determined experimentally.\n */ \n#define BZERO_USE_REP\t(1024)\n#define BCOPY_DFLT_REP\t(128)\n#define\tBCOPY_NHM_REP\t(768)\n\n/*\n * Copy a block of storage, returning an error code if `from' or\n * `to' takes a kernel pagefault which cannot be resolved.\n * Returns errno value on pagefault error, 0 if all ok\n */\n\n/*\n * I'm sorry about these macros, but copy.s is unsurprisingly sensitive to\n * additional call instructions.\n */\n#if defined(__amd64)\n#define\tSMAP_DISABLE_COUNT\t16\n#define\tSMAP_ENABLE_COUNT\t26\n#elif defined(__i386)\n#define\tSMAP_DISABLE_COUNT\t0\n#define\tSMAP_ENABLE_COUNT\t0\n#endif\n\n#define\tSMAP_DISABLE_INSTR(ITER)\t\t\\\n\t.globl\t_smap_disable_patch_/**/ITER;\t\\\n\t_smap_disable_patch_/**/ITER/**/:;\t\\\n\tnop; nop; nop;\n\n#define\tSMAP_ENABLE_INSTR(ITER)\t\t\t\\\n\t.globl\t_smap_enable_patch_/**/ITER;\t\\\n\t_smap_enable_patch_/**/ITER/**/:;\t\\\n\tnop; nop; nop;\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nkcopy(const void *from, void *to, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n\t.globl\tkernelbase\n\t.globl\tpostbootkernelbase\n\n#if defined(__amd64)\n\n\tENTRY(kcopy)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi \t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.kcopy_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_kcopy_copyerr(%rip), %rcx\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\ndo_copy_fault:\n\tmovq\tT_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%rcx, T_LOFAULT(%r9)\t/* new lofault */\n\tcall\tbcopy_altentry\n\txorl\t%eax, %eax\t\t/* return 0 (success) */\n\tSMAP_ENABLE_INSTR(0)\n\n\t/*\n\t * A fault during do_copy_fault is indicated through an errno value\n\t * in %rax and we iretq from the trap handler to here.\n\t */\n_kcopy_copyerr:\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n\tleave\n\tret\n\tSET_SIZE(kcopy)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_COUNT\t16\n\n\tENTRY(kcopy)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%ebp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%ebp)\n\tjnb\t1f\n0:\tpushl\t$.kcopy_panic_msg\n\tcall\tpanic\n1:\tpopl\t%ebp\n#endif\n\tlea\t_kcopy_copyerr, %eax\t/* lofault value */\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\ndo_copy_fault:\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\t\t/* setup stack frame */\n\tpushl\t%esi\n\tpushl\t%edi\t\t\t/* save registers */\n\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tmovl\tARG_FROM(%ebp), %esi\n\tmovl\tARG_TO(%ebp), %edi\n\tshrl\t$2, %ecx\t\t/* word count */\n\trep\n\t  smovl\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tandl\t$3, %ecx\t\t/* bytes left over */\n\trep\n\t  smovb\n\txorl\t%eax, %eax\n\n\t/*\n\t * A fault during do_copy_fault is indicated through an errno value\n\t * in %eax and we iret from the trap handler to here.\n\t */\n_kcopy_copyerr:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n\tret\n\tSET_SIZE(kcopy)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/*\n * Copy a block of storage.  Similar to kcopy but uses non-temporal\n * instructions.\n */\n\n/* ARGSUSED */\nint\nkcopy_nta(const void *from, void *to, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n#define\tCOPY_LOOP_INIT(src, dst, cnt)\t\\\n\taddq\tcnt, src;\t\t\t\\\n\taddq\tcnt, dst;\t\t\t\\\n\tshrq\t$3, cnt;\t\t\t\\\n\tneg\tcnt\n\n\t/* Copy 16 bytes per loop.  Uses %rax and %r8 */\n#define\tCOPY_LOOP_BODY(src, dst, cnt)\t\\\n\tprefetchnta\t0x100(src, cnt, 8);\t\\\n\tmovq\t(src, cnt, 8), %rax;\t\t\\\n\tmovq\t0x8(src, cnt, 8), %r8;\t\t\\\n\tmovnti\t%rax, (dst, cnt, 8);\t\t\\\n\tmovnti\t%r8, 0x8(dst, cnt, 8);\t\t\\\n\taddq\t$2, cnt\n\n\tENTRY(kcopy_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi \t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.kcopy_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_kcopy_nta_copyerr(%rip), %rcx\t/* doesn't set rflags */\n\tjnz\tdo_copy_fault\t\t/* use regular access */\n\t/*\n\t * Make sure cnt is >= KCOPY_MIN_SIZE\n\t */\n\tcmpq\t$KCOPY_MIN_SIZE, %rdx\n\tjb\tdo_copy_fault\n\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\tdo_copy_fault\n\n\tALTENTRY(do_copy_fault_nta)\n\tmovq    %gs:CPU_THREAD, %r9     /* %r9 = thread addr */\n\tmovq    T_LOFAULT(%r9), %r11    /* save the current lofault */\n\tmovq    %rcx, T_LOFAULT(%r9)    /* new lofault */\n\n\t/*\n\t * COPY_LOOP_BODY uses %rax and %r8\n\t */\n\tCOPY_LOOP_INIT(%rdi, %rsi, %rdx)\n2:\tCOPY_LOOP_BODY(%rdi, %rsi, %rdx)\n\tjnz\t2b\n\n\tmfence\n\txorl\t%eax, %eax\t\t/* return 0 (success) */\n\tSMAP_ENABLE_INSTR(1)\n\n_kcopy_nta_copyerr:\n\tmovq\t%r11, T_LOFAULT(%r9)    /* restore original lofault */\n\tleave\n\tret\n\tSET_SIZE(do_copy_fault_nta)\n\tSET_SIZE(kcopy_nta)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_COUNT\t16\n\n#define\tCOPY_LOOP_INIT(src, dst, cnt)\t\\\n\taddl\tcnt, src;\t\t\t\\\n\taddl\tcnt, dst;\t\t\t\\\n\tshrl\t$3, cnt;\t\t\t\\\n\tneg\tcnt\n\n#define\tCOPY_LOOP_BODY(src, dst, cnt)\t\\\n\tprefetchnta\t0x100(src, cnt, 8);\t\\\n\tmovl\t(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, (dst, cnt, 8);\t\t\\\n\tmovl\t0x4(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0x4(dst, cnt, 8);\t\t\\\n\tmovl\t0x8(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0x8(dst, cnt, 8);\t\t\\\n\tmovl\t0xc(src, cnt, 8), %esi;\t\t\\\n\tmovnti\t%esi, 0xc(dst, cnt, 8);\t\t\\\n\taddl\t$2, cnt\n\n\t/*\n\t * kcopy_nta is not implemented for 32-bit as no performance\n\t * improvement was shown.  We simply jump directly to kcopy\n\t * and discard the 4 arguments.\n\t */\n\tENTRY(kcopy_nta)\n\tjmp\tkcopy\n\n\tlea\t_kcopy_nta_copyerr, %eax\t/* lofault value */\n\tALTENTRY(do_copy_fault_nta)\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\t\t/* setup stack frame */\n\tpushl\t%esi\n\tpushl\t%edi\n\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\t/* COPY_LOOP_BODY needs to use %esi */\n\tmovl\tARG_COUNT(%ebp), %ecx\n\tmovl\tARG_FROM(%ebp), %edi\n\tmovl\tARG_TO(%ebp), %eax\n\tCOPY_LOOP_INIT(%edi, %eax, %ecx)\n1:\tCOPY_LOOP_BODY(%edi, %eax, %ecx)\n\tjnz\t1b\n\tmfence\n\n\txorl\t%eax, %eax\n_kcopy_nta_copyerr:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%esi\n\tleave\n\tret\n\tSET_SIZE(do_copy_fault_nta)\n\tSET_SIZE(kcopy_nta)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nvoid\nbcopy(const void *from, void *to, size_t count)\n{}\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(bcopy)\n#ifdef DEBUG\n\torq\t%rdx, %rdx\t\t/* %rdx = count */\n\tjz\t1f\n\tcmpq\tpostbootkernelbase(%rip), %rdi\t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\tpostbootkernelbase(%rip), %rsi\t\t/* %rsi = to */\t\t\n\tjnb\t1f\n0:\tleaq\t.bcopy_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\t/*\n\t * bcopy_altentry() is called from kcopy, i.e., do_copy_fault.\n\t * kcopy assumes that bcopy doesn't touch %r9 and %r11. If bcopy\n\t * uses these registers in future they must be saved and restored.\n\t */\n\tALTENTRY(bcopy_altentry)\ndo_copy:\n#define\tL(s) .bcopy/**/s\n\tcmpq\t$0x50, %rdx\t\t/* 80 */\n\tjae\tbcopy_ck_size\n\n\t/*\n\t * Performance data shows many caller's copy small buffers. So for\n\t * best perf for these sizes unrolled code is used. Store data without\n\t * worrying about alignment.\n\t */\n\tleaq\tL(fwdPxQx)(%rip), %r10\n\taddq\t%rdx, %rdi\n\taddq\t%rdx, %rsi\n\tmovslq\t(%r10,%rdx,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t.p2align 4\nL(fwdPxQx):\n\t.int       L(P0Q0)-L(fwdPxQx)\t/* 0 */\n\t.int       L(P1Q0)-L(fwdPxQx)\n\t.int       L(P2Q0)-L(fwdPxQx)\n\t.int       L(P3Q0)-L(fwdPxQx)\n\t.int       L(P4Q0)-L(fwdPxQx)\n\t.int       L(P5Q0)-L(fwdPxQx)\n\t.int       L(P6Q0)-L(fwdPxQx)\n\t.int       L(P7Q0)-L(fwdPxQx) \n\n\t.int       L(P0Q1)-L(fwdPxQx)\t/* 8 */\n\t.int       L(P1Q1)-L(fwdPxQx)\n\t.int       L(P2Q1)-L(fwdPxQx)\n\t.int       L(P3Q1)-L(fwdPxQx)\n\t.int       L(P4Q1)-L(fwdPxQx)\n\t.int       L(P5Q1)-L(fwdPxQx)\n\t.int       L(P6Q1)-L(fwdPxQx)\n\t.int       L(P7Q1)-L(fwdPxQx) \n\n\t.int       L(P0Q2)-L(fwdPxQx)\t/* 16 */\n\t.int       L(P1Q2)-L(fwdPxQx)\n\t.int       L(P2Q2)-L(fwdPxQx)\n\t.int       L(P3Q2)-L(fwdPxQx)\n\t.int       L(P4Q2)-L(fwdPxQx)\n\t.int       L(P5Q2)-L(fwdPxQx)\n\t.int       L(P6Q2)-L(fwdPxQx)\n\t.int       L(P7Q2)-L(fwdPxQx) \n\n\t.int       L(P0Q3)-L(fwdPxQx)\t/* 24 */\n\t.int       L(P1Q3)-L(fwdPxQx)\n\t.int       L(P2Q3)-L(fwdPxQx)\n\t.int       L(P3Q3)-L(fwdPxQx)\n\t.int       L(P4Q3)-L(fwdPxQx)\n\t.int       L(P5Q3)-L(fwdPxQx)\n\t.int       L(P6Q3)-L(fwdPxQx)\n\t.int       L(P7Q3)-L(fwdPxQx) \n\n\t.int       L(P0Q4)-L(fwdPxQx)\t/* 32 */\n\t.int       L(P1Q4)-L(fwdPxQx)\n\t.int       L(P2Q4)-L(fwdPxQx)\n\t.int       L(P3Q4)-L(fwdPxQx)\n\t.int       L(P4Q4)-L(fwdPxQx)\n\t.int       L(P5Q4)-L(fwdPxQx)\n\t.int       L(P6Q4)-L(fwdPxQx)\n\t.int       L(P7Q4)-L(fwdPxQx) \n\n\t.int       L(P0Q5)-L(fwdPxQx)\t/* 40 */\n\t.int       L(P1Q5)-L(fwdPxQx)\n\t.int       L(P2Q5)-L(fwdPxQx)\n\t.int       L(P3Q5)-L(fwdPxQx)\n\t.int       L(P4Q5)-L(fwdPxQx)\n\t.int       L(P5Q5)-L(fwdPxQx)\n\t.int       L(P6Q5)-L(fwdPxQx)\n\t.int       L(P7Q5)-L(fwdPxQx) \n\n\t.int       L(P0Q6)-L(fwdPxQx)\t/* 48 */\n\t.int       L(P1Q6)-L(fwdPxQx)\n\t.int       L(P2Q6)-L(fwdPxQx)\n\t.int       L(P3Q6)-L(fwdPxQx)\n\t.int       L(P4Q6)-L(fwdPxQx)\n\t.int       L(P5Q6)-L(fwdPxQx)\n\t.int       L(P6Q6)-L(fwdPxQx)\n\t.int       L(P7Q6)-L(fwdPxQx) \n\n\t.int       L(P0Q7)-L(fwdPxQx)\t/* 56 */\n\t.int       L(P1Q7)-L(fwdPxQx)\n\t.int       L(P2Q7)-L(fwdPxQx)\n\t.int       L(P3Q7)-L(fwdPxQx)\n\t.int       L(P4Q7)-L(fwdPxQx)\n\t.int       L(P5Q7)-L(fwdPxQx)\n\t.int       L(P6Q7)-L(fwdPxQx)\n\t.int       L(P7Q7)-L(fwdPxQx) \n\n\t.int       L(P0Q8)-L(fwdPxQx)\t/* 64 */\n\t.int       L(P1Q8)-L(fwdPxQx)\n\t.int       L(P2Q8)-L(fwdPxQx)\n\t.int       L(P3Q8)-L(fwdPxQx)\n\t.int       L(P4Q8)-L(fwdPxQx)\n\t.int       L(P5Q8)-L(fwdPxQx)\n\t.int       L(P6Q8)-L(fwdPxQx)\n\t.int       L(P7Q8)-L(fwdPxQx)\n\n\t.int       L(P0Q9)-L(fwdPxQx)\t/* 72 */\n\t.int       L(P1Q9)-L(fwdPxQx)\n\t.int       L(P2Q9)-L(fwdPxQx)\n\t.int       L(P3Q9)-L(fwdPxQx)\n\t.int       L(P4Q9)-L(fwdPxQx)\n\t.int       L(P5Q9)-L(fwdPxQx)\n\t.int       L(P6Q9)-L(fwdPxQx)\n\t.int       L(P7Q9)-L(fwdPxQx)\t/* 79 */\n\n\t.p2align 4\nL(P0Q9):\n\tmov    -0x48(%rdi), %rcx\n\tmov    %rcx, -0x48(%rsi)\nL(P0Q8):\n\tmov    -0x40(%rdi), %r10\n\tmov    %r10, -0x40(%rsi)\nL(P0Q7):\n\tmov    -0x38(%rdi), %r8\n\tmov    %r8, -0x38(%rsi)\nL(P0Q6):\n\tmov    -0x30(%rdi), %rcx\n\tmov    %rcx, -0x30(%rsi)\nL(P0Q5):\n\tmov    -0x28(%rdi), %r10\n\tmov    %r10, -0x28(%rsi)\nL(P0Q4):\n\tmov    -0x20(%rdi), %r8\n\tmov    %r8, -0x20(%rsi)\nL(P0Q3):\n\tmov    -0x18(%rdi), %rcx\n\tmov    %rcx, -0x18(%rsi)\nL(P0Q2):\n\tmov    -0x10(%rdi), %r10\n\tmov    %r10, -0x10(%rsi)\nL(P0Q1):\n\tmov    -0x8(%rdi), %r8\n\tmov    %r8, -0x8(%rsi)\nL(P0Q0):                                   \n\tret   \n\n\t.p2align 4\nL(P1Q9):\n\tmov    -0x49(%rdi), %r8\n\tmov    %r8, -0x49(%rsi)\nL(P1Q8):\n\tmov    -0x41(%rdi), %rcx\n\tmov    %rcx, -0x41(%rsi)\nL(P1Q7):\n\tmov    -0x39(%rdi), %r10\n\tmov    %r10, -0x39(%rsi)\nL(P1Q6):\n\tmov    -0x31(%rdi), %r8\n\tmov    %r8, -0x31(%rsi)\nL(P1Q5):\n\tmov    -0x29(%rdi), %rcx\n\tmov    %rcx, -0x29(%rsi)\nL(P1Q4):\n\tmov    -0x21(%rdi), %r10\n\tmov    %r10, -0x21(%rsi)\nL(P1Q3):\n\tmov    -0x19(%rdi), %r8\n\tmov    %r8, -0x19(%rsi)\nL(P1Q2):\n\tmov    -0x11(%rdi), %rcx\n\tmov    %rcx, -0x11(%rsi)\nL(P1Q1):\n\tmov    -0x9(%rdi), %r10\n\tmov    %r10, -0x9(%rsi)\nL(P1Q0):\n\tmovzbq -0x1(%rdi), %r8\n\tmov    %r8b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P2Q9):\n\tmov    -0x4a(%rdi), %r8\n\tmov    %r8, -0x4a(%rsi)\nL(P2Q8):\n\tmov    -0x42(%rdi), %rcx\n\tmov    %rcx, -0x42(%rsi)\nL(P2Q7):\n\tmov    -0x3a(%rdi), %r10\n\tmov    %r10, -0x3a(%rsi)\nL(P2Q6):\n\tmov    -0x32(%rdi), %r8\n\tmov    %r8, -0x32(%rsi)\nL(P2Q5):\n\tmov    -0x2a(%rdi), %rcx\n\tmov    %rcx, -0x2a(%rsi)\nL(P2Q4):\n\tmov    -0x22(%rdi), %r10\n\tmov    %r10, -0x22(%rsi)\nL(P2Q3):\n\tmov    -0x1a(%rdi), %r8\n\tmov    %r8, -0x1a(%rsi)\nL(P2Q2):\n\tmov    -0x12(%rdi), %rcx\n\tmov    %rcx, -0x12(%rsi)\nL(P2Q1):\n\tmov    -0xa(%rdi), %r10\n\tmov    %r10, -0xa(%rsi)\nL(P2Q0):\n\tmovzwq -0x2(%rdi), %r8\n\tmov    %r8w, -0x2(%rsi)\n\tret   \n\n\t.p2align 4\nL(P3Q9):\n\tmov    -0x4b(%rdi), %r8\n\tmov    %r8, -0x4b(%rsi)\nL(P3Q8):\n\tmov    -0x43(%rdi), %rcx\n\tmov    %rcx, -0x43(%rsi)\nL(P3Q7):\n\tmov    -0x3b(%rdi), %r10\n\tmov    %r10, -0x3b(%rsi)\nL(P3Q6):\n\tmov    -0x33(%rdi), %r8\n\tmov    %r8, -0x33(%rsi)\nL(P3Q5):\n\tmov    -0x2b(%rdi), %rcx\n\tmov    %rcx, -0x2b(%rsi)\nL(P3Q4):\n\tmov    -0x23(%rdi), %r10\n\tmov    %r10, -0x23(%rsi)\nL(P3Q3):\n\tmov    -0x1b(%rdi), %r8\n\tmov    %r8, -0x1b(%rsi)\nL(P3Q2):\n\tmov    -0x13(%rdi), %rcx\n\tmov    %rcx, -0x13(%rsi)\nL(P3Q1):\n\tmov    -0xb(%rdi), %r10\n\tmov    %r10, -0xb(%rsi)\n\t/*\n\t * These trailing loads/stores have to do all their loads 1st, \n\t * then do the stores.\n\t */\nL(P3Q0):\n\tmovzwq -0x3(%rdi), %r8\n\tmovzbq -0x1(%rdi), %r10\n\tmov    %r8w, -0x3(%rsi)\n\tmov    %r10b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P4Q9):\n\tmov    -0x4c(%rdi), %r8\n\tmov    %r8, -0x4c(%rsi)\nL(P4Q8):\n\tmov    -0x44(%rdi), %rcx\n\tmov    %rcx, -0x44(%rsi)\nL(P4Q7):\n\tmov    -0x3c(%rdi), %r10\n\tmov    %r10, -0x3c(%rsi)\nL(P4Q6):\n\tmov    -0x34(%rdi), %r8\n\tmov    %r8, -0x34(%rsi)\nL(P4Q5):\n\tmov    -0x2c(%rdi), %rcx\n\tmov    %rcx, -0x2c(%rsi)\nL(P4Q4):\n\tmov    -0x24(%rdi), %r10\n\tmov    %r10, -0x24(%rsi)\nL(P4Q3):\n\tmov    -0x1c(%rdi), %r8\n\tmov    %r8, -0x1c(%rsi)\nL(P4Q2):\n\tmov    -0x14(%rdi), %rcx\n\tmov    %rcx, -0x14(%rsi)\nL(P4Q1):\n\tmov    -0xc(%rdi), %r10\n\tmov    %r10, -0xc(%rsi)\nL(P4Q0):\n\tmov    -0x4(%rdi), %r8d\n\tmov    %r8d, -0x4(%rsi)\n\tret   \n\n\t.p2align 4\nL(P5Q9):\n\tmov    -0x4d(%rdi), %r8\n\tmov    %r8, -0x4d(%rsi)\nL(P5Q8):\n\tmov    -0x45(%rdi), %rcx\n\tmov    %rcx, -0x45(%rsi)\nL(P5Q7):\n\tmov    -0x3d(%rdi), %r10\n\tmov    %r10, -0x3d(%rsi)\nL(P5Q6):\n\tmov    -0x35(%rdi), %r8\n\tmov    %r8, -0x35(%rsi)\nL(P5Q5):\n\tmov    -0x2d(%rdi), %rcx\n\tmov    %rcx, -0x2d(%rsi)\nL(P5Q4):\n\tmov    -0x25(%rdi), %r10\n\tmov    %r10, -0x25(%rsi)\nL(P5Q3):\n\tmov    -0x1d(%rdi), %r8\n\tmov    %r8, -0x1d(%rsi)\nL(P5Q2):\n\tmov    -0x15(%rdi), %rcx\n\tmov    %rcx, -0x15(%rsi)\nL(P5Q1):\n\tmov    -0xd(%rdi), %r10\n\tmov    %r10, -0xd(%rsi)\nL(P5Q0):\n\tmov    -0x5(%rdi), %r8d\n\tmovzbq -0x1(%rdi), %r10\n\tmov    %r8d, -0x5(%rsi)\n\tmov    %r10b, -0x1(%rsi)\n\tret   \n\n\t.p2align 4\nL(P6Q9):\n\tmov    -0x4e(%rdi), %r8\n\tmov    %r8, -0x4e(%rsi)\nL(P6Q8):\n\tmov    -0x46(%rdi), %rcx\n\tmov    %rcx, -0x46(%rsi)\nL(P6Q7):\n\tmov    -0x3e(%rdi), %r10\n\tmov    %r10, -0x3e(%rsi)\nL(P6Q6):\n\tmov    -0x36(%rdi), %r8\n\tmov    %r8, -0x36(%rsi)\nL(P6Q5):\n\tmov    -0x2e(%rdi), %rcx\n\tmov    %rcx, -0x2e(%rsi)\nL(P6Q4):\n\tmov    -0x26(%rdi), %r10\n\tmov    %r10, -0x26(%rsi)\nL(P6Q3):\n\tmov    -0x1e(%rdi), %r8\n\tmov    %r8, -0x1e(%rsi)\nL(P6Q2):\n\tmov    -0x16(%rdi), %rcx\n\tmov    %rcx, -0x16(%rsi)\nL(P6Q1):\n\tmov    -0xe(%rdi), %r10\n\tmov    %r10, -0xe(%rsi)\nL(P6Q0):\n\tmov    -0x6(%rdi), %r8d\n\tmovzwq -0x2(%rdi), %r10\n\tmov    %r8d, -0x6(%rsi)\n\tmov    %r10w, -0x2(%rsi)\n\tret   \n\n\t.p2align 4\nL(P7Q9):\n\tmov    -0x4f(%rdi), %r8\n\tmov    %r8, -0x4f(%rsi)\nL(P7Q8):\n\tmov    -0x47(%rdi), %rcx\n\tmov    %rcx, -0x47(%rsi)\nL(P7Q7):\n\tmov    -0x3f(%rdi), %r10\n\tmov    %r10, -0x3f(%rsi)\nL(P7Q6):\n\tmov    -0x37(%rdi), %r8\n\tmov    %r8, -0x37(%rsi)\nL(P7Q5):\n\tmov    -0x2f(%rdi), %rcx\n\tmov    %rcx, -0x2f(%rsi)\nL(P7Q4):\n\tmov    -0x27(%rdi), %r10\n\tmov    %r10, -0x27(%rsi)\nL(P7Q3):\n\tmov    -0x1f(%rdi), %r8\n\tmov    %r8, -0x1f(%rsi)\nL(P7Q2):\n\tmov    -0x17(%rdi), %rcx\n\tmov    %rcx, -0x17(%rsi)\nL(P7Q1):\n\tmov    -0xf(%rdi), %r10\n\tmov    %r10, -0xf(%rsi)\nL(P7Q0):\n\tmov    -0x7(%rdi), %r8d\n\tmovzwq -0x3(%rdi), %r10\n\tmovzbq -0x1(%rdi), %rcx\n\tmov    %r8d, -0x7(%rsi)\n\tmov    %r10w, -0x3(%rsi)\n\tmov    %cl, -0x1(%rsi)\n\tret   \n\n\t/*\n\t * For large sizes rep smovq is fastest.\n\t * Transition point determined experimentally as measured on\n\t * Intel Xeon processors (incl. Nehalem and previous generations) and\n\t * AMD Opteron. The transition value is patched at boot time to avoid\n\t * memory reference hit.\n\t */\n\t.globl bcopy_patch_start\nbcopy_patch_start:\n\tcmpq\t$BCOPY_NHM_REP, %rdx\n\t.globl bcopy_patch_end\nbcopy_patch_end:\n\n\t.p2align 4\n\t.globl bcopy_ck_size\nbcopy_ck_size:\n\tcmpq\t$BCOPY_DFLT_REP, %rdx\n\tjae\tL(use_rep)\n\n\t/*\n\t * Align to a 8-byte boundary. Avoids penalties from unaligned stores\n\t * as well as from stores spanning cachelines.\n\t */\n\ttest\t$0x7, %rsi\n\tjz\tL(aligned_loop)\n\ttest\t$0x1, %rsi\n\tjz\t2f\n\tmovzbq\t(%rdi), %r8\n\tdec\t%rdx\n\tinc\t%rdi\n\tmov\t%r8b, (%rsi)\n\tinc\t%rsi\n2:\n\ttest\t$0x2, %rsi\n\tjz\t4f\n\tmovzwq\t(%rdi), %r8\n\tsub\t$0x2, %rdx\n\tadd\t$0x2, %rdi\n\tmov\t%r8w, (%rsi)\n\tadd\t$0x2, %rsi\n4:\n\ttest\t$0x4, %rsi\n\tjz\tL(aligned_loop)\n\tmov\t(%rdi), %r8d\n\tsub\t$0x4, %rdx\n\tadd\t$0x4, %rdi\n\tmov\t%r8d, (%rsi)\n\tadd\t$0x4, %rsi\n\n\t/*\n\t * Copy 64-bytes per loop\n\t */\n\t.p2align 4\nL(aligned_loop):\n\tmov\t(%rdi), %r8\n\tmov\t0x8(%rdi), %r10\n\tlea\t-0x40(%rdx), %rdx\n\tmov\t%r8, (%rsi)\n\tmov\t%r10, 0x8(%rsi)\n\tmov\t0x10(%rdi), %rcx\n\tmov\t0x18(%rdi), %r8\n\tmov\t%rcx, 0x10(%rsi)\n\tmov\t%r8, 0x18(%rsi)\n\n\tcmp\t$0x40, %rdx\n\tmov\t0x20(%rdi), %r10\n\tmov\t0x28(%rdi), %rcx\n\tmov\t%r10, 0x20(%rsi)\n\tmov\t%rcx, 0x28(%rsi)\n\tmov\t0x30(%rdi), %r8\n\tmov\t0x38(%rdi), %r10\n\tlea\t0x40(%rdi), %rdi\n\tmov\t%r8, 0x30(%rsi)\n\tmov\t%r10, 0x38(%rsi)\n\tlea\t0x40(%rsi), %rsi\n\tjae\tL(aligned_loop)\n\n\t/*\n\t * Copy remaining bytes (0-63)\n\t */\nL(do_remainder):\n\tleaq\tL(fwdPxQx)(%rip), %r10\n\taddq\t%rdx, %rdi\n\taddq\t%rdx, %rsi\n\tmovslq\t(%r10,%rdx,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t/*\n\t * Use rep smovq. Clear remainder via unrolled code\n\t */\n\t.p2align 4\nL(use_rep):\n\txchgq\t%rdi, %rsi\t\t/* %rsi = source, %rdi = destination */\n\tmovq\t%rdx, %rcx\t\t/* %rcx = count */\n\tshrq\t$3, %rcx\t\t/* 8-byte word count */\n\trep\n\t  smovq\n\n\txchgq\t%rsi, %rdi\t\t/* %rdi = src, %rsi = destination */\n\tandq\t$7, %rdx\t\t/* remainder */\n\tjnz\tL(do_remainder)\n\tret\n#undef\tL\n\n#ifdef DEBUG\n\t/*\n\t * Setup frame on the run-time stack. The end of the input argument\n\t * area must be aligned on a 16 byte boundary. The stack pointer %rsp,\n\t * always points to the end of the latest allocated stack frame.\n\t * panic(const char *format, ...) is a varargs function. When a\n\t * function taking variable arguments is called, %rax must be set\n\t * to eight times the number of floating point parameters passed\n\t * to the function in SSE registers.\n\t */\ncall_panic:\n\tpushq\t%rbp\t\t\t/* align stack properly */\n\tmovq\t%rsp, %rbp\n\txorl\t%eax, %eax\t\t/* no variable arguments */\n\tcall\tpanic\t\t\t/* %rdi = format string */\n#endif\n\tSET_SIZE(bcopy_altentry)\n\tSET_SIZE(bcopy)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t4\n#define\tARG_TO\t\t8\n#define\tARG_COUNT\t12\n\n\tENTRY(bcopy)\n#ifdef DEBUG\n\tmovl\tARG_COUNT(%esp), %eax\n\torl\t%eax, %eax\n\tjz\t1f\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%esp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%esp)\n\tjnb\t1f\n0:\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.bcopy_panic_msg\n\tcall\tpanic\n1:\n#endif\ndo_copy:\n\tmovl\t%esi, %eax\t\t/* save registers */\n\tmovl\t%edi, %edx\n\tmovl\tARG_COUNT(%esp), %ecx\n\tmovl\tARG_FROM(%esp), %esi\n\tmovl\tARG_TO(%esp), %edi\n\n\tshrl\t$2, %ecx\t\t/* word count */\n\trep\n\t  smovl\n\tmovl\tARG_COUNT(%esp), %ecx\n\tandl\t$3, %ecx\t\t/* bytes left over */\n\trep\n\t  smovb\n\tmovl\t%eax, %esi\t\t/* restore registers */\n\tmovl\t%edx, %edi\n\tret\n\tSET_SIZE(bcopy)\n\n#undef\tARG_COUNT\n#undef\tARG_FROM\n#undef\tARG_TO\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n\n/*\n * Zero a block of storage, returning an error code if we\n * take a kernel pagefault which cannot be resolved.\n * Returns errno value on pagefault error, 0 if all ok\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nkzero(void *addr, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(kzero)\n#ifdef DEBUG\n        cmpq\tpostbootkernelbase(%rip), %rdi\t/* %rdi = addr */\n        jnb\t0f\n        leaq\t.kzero_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n0:\n#endif\n\t/*\n\t * pass lofault value as 3rd argument for fault return \n\t */\n\tleaq\t_kzeroerr(%rip), %rdx\n\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\tmovq\tT_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%rdx, T_LOFAULT(%r9)\t/* new lofault */\n\tcall\tbzero_altentry\n\txorl\t%eax, %eax\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tret\n\t/*\n\t * A fault during bzero is indicated through an errno value\n\t * in %rax when we iretq to here.\n\t */\n_kzeroerr:\n\taddq\t$8, %rsp\t\t/* pop bzero_altentry call ret addr */\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tret\n\tSET_SIZE(kzero)\n\n#elif defined(__i386)\n\n#define\tARG_ADDR\t8\n#define\tARG_COUNT\t12\n\n\tENTRY(kzero)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tpostbootkernelbase, %eax\n        cmpl\t%eax, ARG_ADDR(%ebp)\n        jnb\t0f\n        pushl   $.kzero_panic_msg\n        call    panic\n0:\tpopl\t%ebp\n#endif\n\tlea\t_kzeroerr, %eax\t\t/* kzeroerr is lofault value */\n\n\tpushl\t%ebp\t\t\t/* save stack base */\n\tmovl\t%esp, %ebp\t\t/* set new stack base */\n\tpushl\t%edi\t\t\t/* save %edi */\n\n\tmov\t%gs:CPU_THREAD, %edx\t\n\tmovl\tT_LOFAULT(%edx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%edx)\t/* new lofault */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\t/* get size in bytes */\n\tmovl\tARG_ADDR(%ebp), %edi\t/* %edi <- address of bytes to clear */\n\tshrl\t$2, %ecx\t\t/* Count of double words to zero */\n\txorl\t%eax, %eax\t\t/* sstol val */\n\trep\n\t  sstol\t\t\t/* %ecx contains words to clear (%eax=0) */\n\n\tmovl\tARG_COUNT(%ebp), %ecx\t/* get size in bytes */\n\tandl\t$3, %ecx\t\t/* do mod 4 */\n\trep\n\t  sstob\t\t\t/* %ecx contains residual bytes to clear */\n\n\t/*\n\t * A fault during kzero is indicated through an errno value\n\t * in %eax when we iret to here.\n\t */\n_kzeroerr:\n\tpopl\t%edi\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* restore the original lofault */\n\tpopl\t%edi\n\tpopl\t%ebp\n\tret\n\tSET_SIZE(kzero)\n\n#undef\tARG_ADDR\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Zero a block of storage.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nvoid\nbzero(void *addr, size_t count)\n{}\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(bzero)\n#ifdef DEBUG\n\tcmpq\tpostbootkernelbase(%rip), %rdi\t/* %rdi = addr */\n\tjnb\t0f\n\tleaq\t.bzero_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n0:\n#endif\n\tALTENTRY(bzero_altentry)\ndo_zero:\n#define\tL(s) .bzero/**/s\n\txorl\t%eax, %eax\n\n\tcmpq\t$0x50, %rsi\t\t/* 80 */\n\tjae\tL(ck_align)\n\n\t/*\n\t * Performance data shows many caller's are zeroing small buffers. So\n\t * for best perf for these sizes unrolled code is used. Store zeros\n\t * without worrying about alignment.\n\t */\n\tleaq\tL(setPxQx)(%rip), %r10\n\taddq\t%rsi, %rdi\n\tmovslq\t(%r10,%rsi,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t.p2align 4\nL(setPxQx):\n\t.int       L(P0Q0)-L(setPxQx)\t/* 0 */\n\t.int       L(P1Q0)-L(setPxQx)\n\t.int       L(P2Q0)-L(setPxQx)\n\t.int       L(P3Q0)-L(setPxQx)\n\t.int       L(P4Q0)-L(setPxQx)\n\t.int       L(P5Q0)-L(setPxQx)\n\t.int       L(P6Q0)-L(setPxQx)\n\t.int       L(P7Q0)-L(setPxQx) \n\n\t.int       L(P0Q1)-L(setPxQx)\t/* 8 */\n\t.int       L(P1Q1)-L(setPxQx)\n\t.int       L(P2Q1)-L(setPxQx)\n\t.int       L(P3Q1)-L(setPxQx)\n\t.int       L(P4Q1)-L(setPxQx)\n\t.int       L(P5Q1)-L(setPxQx)\n\t.int       L(P6Q1)-L(setPxQx)\n\t.int       L(P7Q1)-L(setPxQx) \n\n\t.int       L(P0Q2)-L(setPxQx)\t/* 16 */\n\t.int       L(P1Q2)-L(setPxQx)\n\t.int       L(P2Q2)-L(setPxQx)\n\t.int       L(P3Q2)-L(setPxQx)\n\t.int       L(P4Q2)-L(setPxQx)\n\t.int       L(P5Q2)-L(setPxQx)\n\t.int       L(P6Q2)-L(setPxQx)\n\t.int       L(P7Q2)-L(setPxQx) \n\n\t.int       L(P0Q3)-L(setPxQx)\t/* 24 */\n\t.int       L(P1Q3)-L(setPxQx)\n\t.int       L(P2Q3)-L(setPxQx)\n\t.int       L(P3Q3)-L(setPxQx)\n\t.int       L(P4Q3)-L(setPxQx)\n\t.int       L(P5Q3)-L(setPxQx)\n\t.int       L(P6Q3)-L(setPxQx)\n\t.int       L(P7Q3)-L(setPxQx) \n\n\t.int       L(P0Q4)-L(setPxQx)\t/* 32 */\n\t.int       L(P1Q4)-L(setPxQx)\n\t.int       L(P2Q4)-L(setPxQx)\n\t.int       L(P3Q4)-L(setPxQx)\n\t.int       L(P4Q4)-L(setPxQx)\n\t.int       L(P5Q4)-L(setPxQx)\n\t.int       L(P6Q4)-L(setPxQx)\n\t.int       L(P7Q4)-L(setPxQx) \n\n\t.int       L(P0Q5)-L(setPxQx)\t/* 40 */\n\t.int       L(P1Q5)-L(setPxQx)\n\t.int       L(P2Q5)-L(setPxQx)\n\t.int       L(P3Q5)-L(setPxQx)\n\t.int       L(P4Q5)-L(setPxQx)\n\t.int       L(P5Q5)-L(setPxQx)\n\t.int       L(P6Q5)-L(setPxQx)\n\t.int       L(P7Q5)-L(setPxQx) \n\n\t.int       L(P0Q6)-L(setPxQx)\t/* 48 */\n\t.int       L(P1Q6)-L(setPxQx)\n\t.int       L(P2Q6)-L(setPxQx)\n\t.int       L(P3Q6)-L(setPxQx)\n\t.int       L(P4Q6)-L(setPxQx)\n\t.int       L(P5Q6)-L(setPxQx)\n\t.int       L(P6Q6)-L(setPxQx)\n\t.int       L(P7Q6)-L(setPxQx) \n\n\t.int       L(P0Q7)-L(setPxQx)\t/* 56 */\n\t.int       L(P1Q7)-L(setPxQx)\n\t.int       L(P2Q7)-L(setPxQx)\n\t.int       L(P3Q7)-L(setPxQx)\n\t.int       L(P4Q7)-L(setPxQx)\n\t.int       L(P5Q7)-L(setPxQx)\n\t.int       L(P6Q7)-L(setPxQx)\n\t.int       L(P7Q7)-L(setPxQx) \n\n\t.int       L(P0Q8)-L(setPxQx)\t/* 64 */\n\t.int       L(P1Q8)-L(setPxQx)\n\t.int       L(P2Q8)-L(setPxQx)\n\t.int       L(P3Q8)-L(setPxQx)\n\t.int       L(P4Q8)-L(setPxQx)\n\t.int       L(P5Q8)-L(setPxQx)\n\t.int       L(P6Q8)-L(setPxQx)\n\t.int       L(P7Q8)-L(setPxQx)\n\n\t.int       L(P0Q9)-L(setPxQx)\t/* 72 */\n\t.int       L(P1Q9)-L(setPxQx)\n\t.int       L(P2Q9)-L(setPxQx)\n\t.int       L(P3Q9)-L(setPxQx)\n\t.int       L(P4Q9)-L(setPxQx)\n\t.int       L(P5Q9)-L(setPxQx)\n\t.int       L(P6Q9)-L(setPxQx)\n\t.int       L(P7Q9)-L(setPxQx)\t/* 79 */\n\n\t.p2align 4\nL(P0Q9): mov    %rax, -0x48(%rdi)\nL(P0Q8): mov    %rax, -0x40(%rdi)\nL(P0Q7): mov    %rax, -0x38(%rdi)\nL(P0Q6): mov    %rax, -0x30(%rdi)\nL(P0Q5): mov    %rax, -0x28(%rdi)\nL(P0Q4): mov    %rax, -0x20(%rdi)\nL(P0Q3): mov    %rax, -0x18(%rdi)\nL(P0Q2): mov    %rax, -0x10(%rdi)\nL(P0Q1): mov    %rax, -0x8(%rdi)\nL(P0Q0): \n\t ret\n\n\t.p2align 4\nL(P1Q9): mov    %rax, -0x49(%rdi)\nL(P1Q8): mov    %rax, -0x41(%rdi)\nL(P1Q7): mov    %rax, -0x39(%rdi)\nL(P1Q6): mov    %rax, -0x31(%rdi)\nL(P1Q5): mov    %rax, -0x29(%rdi)\nL(P1Q4): mov    %rax, -0x21(%rdi)\nL(P1Q3): mov    %rax, -0x19(%rdi)\nL(P1Q2): mov    %rax, -0x11(%rdi)\nL(P1Q1): mov    %rax, -0x9(%rdi)\nL(P1Q0): mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P2Q9): mov    %rax, -0x4a(%rdi)\nL(P2Q8): mov    %rax, -0x42(%rdi)\nL(P2Q7): mov    %rax, -0x3a(%rdi)\nL(P2Q6): mov    %rax, -0x32(%rdi)\nL(P2Q5): mov    %rax, -0x2a(%rdi)\nL(P2Q4): mov    %rax, -0x22(%rdi)\nL(P2Q3): mov    %rax, -0x1a(%rdi)\nL(P2Q2): mov    %rax, -0x12(%rdi)\nL(P2Q1): mov    %rax, -0xa(%rdi)\nL(P2Q0): mov    %ax, -0x2(%rdi)\n\t ret\n\n\t.p2align 4\nL(P3Q9): mov    %rax, -0x4b(%rdi)\nL(P3Q8): mov    %rax, -0x43(%rdi)\nL(P3Q7): mov    %rax, -0x3b(%rdi)\nL(P3Q6): mov    %rax, -0x33(%rdi)\nL(P3Q5): mov    %rax, -0x2b(%rdi)\nL(P3Q4): mov    %rax, -0x23(%rdi)\nL(P3Q3): mov    %rax, -0x1b(%rdi)\nL(P3Q2): mov    %rax, -0x13(%rdi)\nL(P3Q1): mov    %rax, -0xb(%rdi)\nL(P3Q0): mov    %ax, -0x3(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P4Q9): mov    %rax, -0x4c(%rdi)\nL(P4Q8): mov    %rax, -0x44(%rdi)\nL(P4Q7): mov    %rax, -0x3c(%rdi)\nL(P4Q6): mov    %rax, -0x34(%rdi)\nL(P4Q5): mov    %rax, -0x2c(%rdi)\nL(P4Q4): mov    %rax, -0x24(%rdi)\nL(P4Q3): mov    %rax, -0x1c(%rdi)\nL(P4Q2): mov    %rax, -0x14(%rdi)\nL(P4Q1): mov    %rax, -0xc(%rdi)\nL(P4Q0): mov    %eax, -0x4(%rdi)\n\t ret\n\n\t.p2align 4\nL(P5Q9): mov    %rax, -0x4d(%rdi)\nL(P5Q8): mov    %rax, -0x45(%rdi)\nL(P5Q7): mov    %rax, -0x3d(%rdi)\nL(P5Q6): mov    %rax, -0x35(%rdi)\nL(P5Q5): mov    %rax, -0x2d(%rdi)\nL(P5Q4): mov    %rax, -0x25(%rdi)\nL(P5Q3): mov    %rax, -0x1d(%rdi)\nL(P5Q2): mov    %rax, -0x15(%rdi)\nL(P5Q1): mov    %rax, -0xd(%rdi)\nL(P5Q0): mov    %eax, -0x5(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t.p2align 4\nL(P6Q9): mov    %rax, -0x4e(%rdi)\nL(P6Q8): mov    %rax, -0x46(%rdi)\nL(P6Q7): mov    %rax, -0x3e(%rdi)\nL(P6Q6): mov    %rax, -0x36(%rdi)\nL(P6Q5): mov    %rax, -0x2e(%rdi)\nL(P6Q4): mov    %rax, -0x26(%rdi)\nL(P6Q3): mov    %rax, -0x1e(%rdi)\nL(P6Q2): mov    %rax, -0x16(%rdi)\nL(P6Q1): mov    %rax, -0xe(%rdi)\nL(P6Q0): mov    %eax, -0x6(%rdi)\n\t mov    %ax, -0x2(%rdi)\n\t ret\n\n\t.p2align 4\nL(P7Q9): mov    %rax, -0x4f(%rdi)\nL(P7Q8): mov    %rax, -0x47(%rdi)\nL(P7Q7): mov    %rax, -0x3f(%rdi)\nL(P7Q6): mov    %rax, -0x37(%rdi)\nL(P7Q5): mov    %rax, -0x2f(%rdi)\nL(P7Q4): mov    %rax, -0x27(%rdi)\nL(P7Q3): mov    %rax, -0x1f(%rdi)\nL(P7Q2): mov    %rax, -0x17(%rdi)\nL(P7Q1): mov    %rax, -0xf(%rdi)\nL(P7Q0): mov    %eax, -0x7(%rdi)\n\t mov    %ax, -0x3(%rdi)\n\t mov    %al, -0x1(%rdi)\n\t ret\n\n\t/*\n\t * Align to a 16-byte boundary. Avoids penalties from unaligned stores\n\t * as well as from stores spanning cachelines. Note 16-byte alignment\n\t * is better in case where rep sstosq is used.\n\t */\n\t.p2align 4\nL(ck_align):\n\ttest\t$0xf, %rdi\n\tjz\tL(aligned_now)\n\ttest\t$1, %rdi\n\tjz\t2f\n\tmov\t%al, (%rdi)\n\tdec\t%rsi\n\tlea\t1(%rdi),%rdi\n2:\n\ttest\t$2, %rdi\n\tjz\t4f\n\tmov\t%ax, (%rdi)\n\tsub\t$2, %rsi\n\tlea\t2(%rdi),%rdi\n4:\n\ttest\t$4, %rdi\n\tjz\t8f\n\tmov\t%eax, (%rdi)\n\tsub\t$4, %rsi\n\tlea\t4(%rdi),%rdi\n8:\n\ttest\t$8, %rdi\n\tjz\tL(aligned_now)\n\tmov\t%rax, (%rdi)\n\tsub\t$8, %rsi\n\tlea\t8(%rdi),%rdi\n\n\t/*\n\t * For large sizes rep sstoq is fastest.\n\t * Transition point determined experimentally as measured on\n\t * Intel Xeon processors (incl. Nehalem) and AMD Opteron.\n\t */\nL(aligned_now):\n\tcmp\t$BZERO_USE_REP, %rsi\n\tja\tL(use_rep)\n\n\t/*\n\t * zero 64-bytes per loop\n\t */\n\t.p2align 4\nL(bzero_loop):\n\tleaq\t-0x40(%rsi), %rsi\n\tcmpq\t$0x40, %rsi\n\tmovq\t%rax, (%rdi) \n\tmovq\t%rax, 0x8(%rdi) \n\tmovq\t%rax, 0x10(%rdi) \n\tmovq\t%rax, 0x18(%rdi) \n\tmovq\t%rax, 0x20(%rdi) \n\tmovq\t%rax, 0x28(%rdi) \n\tmovq\t%rax, 0x30(%rdi) \n\tmovq\t%rax, 0x38(%rdi) \n\tleaq\t0x40(%rdi), %rdi\n\tjae\tL(bzero_loop)\n\n\t/*\n\t * Clear any remaining bytes..\n\t */\n9:\n\tleaq\tL(setPxQx)(%rip), %r10\n\taddq\t%rsi, %rdi\n\tmovslq\t(%r10,%rsi,4), %rcx\n\tleaq\t(%rcx,%r10,1), %r10\n\tjmpq\t*%r10\n\n\t/*\n\t * Use rep sstoq. Clear any remainder via unrolled code\n\t */\n\t.p2align 4\nL(use_rep):\n\tmovq\t%rsi, %rcx\t\t/* get size in bytes */\n\tshrq\t$3, %rcx\t\t/* count of 8-byte words to zero */\n\trep\n\t  sstoq\t\t\t\t/* %rcx = words to clear (%rax=0) */\n\tandq\t$7, %rsi\t\t/* remaining bytes */\n\tjnz\t9b\n\tret\n#undef\tL\n\tSET_SIZE(bzero_altentry)\n\tSET_SIZE(bzero)\n\n#elif defined(__i386)\n\n#define\tARG_ADDR\t4\n#define\tARG_COUNT\t8\n\n\tENTRY(bzero)\n#ifdef DEBUG\n\tmovl\tpostbootkernelbase, %eax\n\tcmpl\t%eax, ARG_ADDR(%esp)\n\tjnb\t0f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.bzero_panic_msg\n\tcall\tpanic\n0:\n#endif\ndo_zero:\n\tmovl\t%edi, %edx\n\tmovl\tARG_COUNT(%esp), %ecx\n\tmovl\tARG_ADDR(%esp), %edi\n\tshrl\t$2, %ecx\n\txorl\t%eax, %eax\n\trep\n\t  sstol\n\tmovl\tARG_COUNT(%esp), %ecx\n\tandl\t$3, %ecx\n\trep\n\t  sstob\n\tmovl\t%edx, %edi\n\tret\n\tSET_SIZE(bzero)\n\n#undef\tARG_ADDR\n#undef\tARG_COUNT\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Transfer data to and from user space -\n * Note that these routines can cause faults\n * It is assumed that the kernel has nothing at\n * less than KERNELBASE in the virtual address space.\n *\n * Note that copyin(9F) and copyout(9F) are part of the\n * DDI/DKI which specifies that they return '-1' on \"errors.\"\n *\n * Sigh.\n *\n * So there's two extremely similar routines - xcopyin_nta() and\n * xcopyout_nta() which return the errno that we've faithfully computed.\n * This allows other callers (e.g. uiomove(9F)) to work correctly.\n * Given that these are used pretty heavily, we expand the calling\n * sequences inline for all flavours (rather than making wrappers).\n */\n\n/*\n * Copy user data to kernel space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyin(const void *uaddr, void *kaddr, size_t count)\n{ return (0); }\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyin)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyin_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_copyin_err(%rip), %rcx\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t3f\t\t\t/* take copyop if uaddr > kernelbase */\n\tSMAP_DISABLE_INSTR(0)\n\tjmp\tdo_copy_fault\t\t/* Takes care of leave for us */\n\n_copyin_err:\n\tSMAP_ENABLE_INSTR(2)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\t\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_COPYIN(%rax)\n\n2:\tmovl\t$-1, %eax\t\n\tleave\n\tret\n\tSET_SIZE(copyin)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n\n\tENTRY(copyin)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyin_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyin_err, %eax\n\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copy_fault\n\tjmp\t3f\n\n_copyin_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYIN(%eax)\n\n2:\tmovl\t$-1, %eax\n\tret\n\tSET_SIZE(copyin)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nxcopyin_nta(const void *uaddr, void *kaddr, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(xcopyin_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t * %rcx is consumed in this routine so we don't need to save\n\t * it.\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.xcopyin_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_xcopyin_err(%rip), %rcx\t/* doesn't set rflags */\n\tjnz\t6f\t\t\t/* use regular access */\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpq\t$XCOPY_MIN_SIZE, %rdx\n\tjae\t5f\n6:\n\tSMAP_DISABLE_INSTR(1)\n\tjmp\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n5:\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\t6b\t\n\tleaq\t_xcopyin_nta_err(%rip), %rcx\t/* doesn't set rflags */\n\tSMAP_DISABLE_INSTR(2)\n\tjmp\tdo_copy_fault_nta\t/* use non-temporal access */\n\t\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %rax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyin_err:\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n_xcopyin_nta_err:\n\tSMAP_ENABLE_INSTR(3)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %r8\n\tcmpq\t$0, %r8\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_XCOPYIN(%r8)\n\n2:\tleave\n\tret\n\tSET_SIZE(xcopyin_nta)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n#define\tARG_COUNT\t12\n#define\tARG_CACHED\t16\n\n\t.globl\tuse_sse_copy\n\n\tENTRY(xcopyin_nta)\n\tmovl\tkernelbase, %ecx\n\tlea\t_xcopyin_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpl\t$0, use_sse_copy\t/* no sse support */\n\tjz\tdo_copy_fault\n\n\tcmpl\t$0, ARG_CACHED(%esp)\t/* copy_cached hint set? */\n\tjnz\tdo_copy_fault\n\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpl\t$XCOPY_MIN_SIZE, ARG_COUNT(%esp)\n\tjb\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovl\tARG_UADDR(%esp), %ecx\n\torl\tARG_KADDR(%esp), %ecx\n\tandl\t$NTA_ALIGN_MASK, %ecx\n\torl\tARG_COUNT(%esp), %ecx\n\tandl\t$COUNT_ALIGN_MASK, %ecx\n\tjnz\tdo_copy_fault\n\n\tjmp\tdo_copy_fault_nta\t/* use regular access */\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %eax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyin_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tcmpl\t$0, T_COPYOPS(%edx)\n\tjz\t2f\n\tmovl\tT_COPYOPS(%edx), %eax\n\tjmp\t*CP_XCOPYIN(%eax)\n\n2:\trep; \tret\t/* use 2 byte return instruction when branch target */\n\t\t\t/* AMD Software Optimization Guide - Section 6.2 */\n\tSET_SIZE(xcopyin_nta)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n#undef\tARG_COUNT\n#undef\tARG_CACHED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy kernel data to user space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyout(const void *kaddr, void *uaddr, size_t count)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyout)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyout_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_copyout_err(%rip), %rcx\n\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t3f\t\t\t/* take copyop if uaddr > kernelbase */\n\tSMAP_DISABLE_INSTR(3)\n\tjmp\tdo_copy_fault\t\t/* Calls leave for us */\n\n_copyout_err:\n\tSMAP_ENABLE_INSTR(4)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_COPYOUT(%rax)\n\n2:\tmovl\t$-1, %eax\n\tleave\n\tret\n\tSET_SIZE(copyout)\n\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n\n\tENTRY(copyout)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyout_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyout_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copy_fault\n\tjmp\t3f\n\t\n_copyout_err:\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/* restore original lofault */\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYOUT(%eax)\n\n2:\tmovl\t$-1, %eax\n\tret\n\tSET_SIZE(copyout)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\nxcopyout_nta(const void *kaddr, void *uaddr, size_t count, int copy_cached)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(xcopyout_nta)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$24, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.xcopyout_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpq\t$0, %rcx\t\t/* No non-temporal access? */\n\t/*\n\t * pass lofault value as 4th argument to do_copy_fault\n\t */\n\tleaq\t_xcopyout_err(%rip), %rcx\n\tjnz\t6f\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpq\t$XCOPY_MIN_SIZE, %rdx\n\tjae\t5f\n6:\n\tSMAP_DISABLE_INSTR(4)\n\tjmp\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n5:\n\tmovq\t%rdi, %r10\n\torq\t%rsi, %r10\n\tandq\t$NTA_ALIGN_MASK, %r10\n\torq\t%rdx, %r10\n\tandq\t$COUNT_ALIGN_MASK, %r10\n\tjnz\t6b\t\n\tleaq\t_xcopyout_nta_err(%rip), %rcx\n\tSMAP_DISABLE_INSTR(5)\n\tcall\tdo_copy_fault_nta\n\tSMAP_ENABLE_INSTR(5)\n\tret\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %rax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyout_err:\n\taddq\t$8, %rsp\t\t/* pop bcopy_altentry call ret addr */\n_xcopyout_nta_err:\n\tSMAP_ENABLE_INSTR(6)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %r8\n\tcmpq\t$0, %r8\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tleave\n\tjmp\t*CP_XCOPYOUT(%r8)\n\n2:\tleave\n\tret\n\tSET_SIZE(xcopyout_nta)\n\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n#define\tARG_COUNT\t12\n#define\tARG_CACHED\t16\n\n\tENTRY(xcopyout_nta)\n\tmovl\tkernelbase, %ecx\n\tlea\t_xcopyout_err, %eax\n\tmovl\t%gs:CPU_THREAD, %edx\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjae\t4f\n\n\tcmpl\t$0, use_sse_copy\t/* no sse support */\n\tjz\tdo_copy_fault\n\n\tcmpl\t$0, ARG_CACHED(%esp)\t/* copy_cached hint set? */\n\tjnz\tdo_copy_fault\n\n\t/*\n\t * Make sure cnt is >= XCOPY_MIN_SIZE bytes\n\t */\n\tcmpl\t$XCOPY_MIN_SIZE, %edx\n\tjb\tdo_copy_fault\n\t\n\t/*\n\t * Make sure src and dst are NTA_ALIGN_SIZE aligned,\n\t * count is COUNT_ALIGN_SIZE aligned.\n\t */\n\tmovl\tARG_UADDR(%esp), %ecx\n\torl\tARG_KADDR(%esp), %ecx\n\tandl\t$NTA_ALIGN_MASK, %ecx\n\torl\tARG_COUNT(%esp), %ecx\n\tandl\t$COUNT_ALIGN_MASK, %ecx\n\tjnz\tdo_copy_fault\n\tjmp\tdo_copy_fault_nta\n\n4:\n\tmovl\t$EFAULT, %eax\n\tjmp\t3f\n\n\t/*\n\t * A fault during do_copy_fault or do_copy_fault_nta is\n\t * indicated through an errno value in %eax and we iret from the\n\t * trap handler to here.\n\t */\n_xcopyout_err:\n\t/ restore the original lofault\n\tpopl\t%ecx\n\tpopl\t%edi\n\tmovl\t%ecx, T_LOFAULT(%edx)\t/ original lofault\n\tpopl\t%esi\n\tpopl\t%ebp\n3:\n\tcmpl\t$0, T_COPYOPS(%edx)\n\tjz\t2f\n\tmovl\tT_COPYOPS(%edx), %eax\n\tjmp\t*CP_XCOPYOUT(%eax)\n\n2:\trep;\tret\t/* use 2 byte return instruction when branch target */\n\t\t\t/* AMD Software Optimization Guide - Section 6.2 */\n\tSET_SIZE(xcopyout_nta)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n#undef\tARG_COUNT\n#undef\tARG_CACHED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from one point to another in\n * the kernel address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopystr(const char *from, char *to, size_t maxlength, size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copystr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n#ifdef DEBUG\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\t\t/* %rdi = from */\n\tjb\t0f\n\tcmpq\t%rax, %rsi\t\t/* %rsi = to */\n\tjnb\t1f\n0:\tleaq\t.copystr_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\tmovq\t%gs:CPU_THREAD, %r9\n\tmovq\tT_LOFAULT(%r9), %r8\t/* pass current lofault value as */\n\t\t\t\t\t/* 5th argument to do_copystr */\n\txorl\t%r10d,%r10d\t\t/* pass smap restore need in %r10d */\n\t\t\t\t\t/* as a non-ABI 6th arg */\ndo_copystr:\n\tmovq\t%gs:CPU_THREAD, %r9\t/* %r9 = thread addr */\n\tmovq    T_LOFAULT(%r9), %r11\t/* save the current lofault */\n\tmovq\t%r8, T_LOFAULT(%r9)\t/* new lofault */\n\n\tmovq\t%rdx, %r8\t\t/* save maxlength */\n\n\tcmpq\t$0, %rdx\t\t/* %rdx = maxlength */\n\tje\tcopystr_enametoolong\t/* maxlength == 0 */\n\ncopystr_loop:\n\tdecq\t%r8\n\tmovb\t(%rdi), %al\n\tincq\t%rdi\n\tmovb\t%al, (%rsi)\n\tincq\t%rsi\n\tcmpb\t$0, %al\n\tje\tcopystr_null\t\t/* null char */\n\tcmpq\t$0, %r8\n\tjne\tcopystr_loop\n\ncopystr_enametoolong:\n\tmovl\t$ENAMETOOLONG, %eax\n\tjmp\tcopystr_out\n\ncopystr_null:\n\txorl\t%eax, %eax\t\t/* no error */\n\ncopystr_out:\n\tcmpq\t$0, %rcx\t\t/* want length? */\n\tje\tcopystr_smap\t\t/* no */\n\tsubq\t%r8, %rdx\t\t/* compute length and store it */\n\tmovq\t%rdx, (%rcx)\n\ncopystr_smap:\n\tcmpl\t$0, %r10d\n\tjz\tcopystr_done\n\tSMAP_ENABLE_INSTR(7)\n\ncopystr_done:\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n\tleave\n\tret\n\tSET_SIZE(copystr)\n\n#elif defined(__i386)\n\n#define\tARG_FROM\t8\n#define\tARG_TO\t\t12\n#define\tARG_MAXLEN\t16\n#define\tARG_LENCOPIED\t20\n\n\tENTRY(copystr)\n#ifdef DEBUG\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, ARG_FROM(%esp)\n\tjb\t0f\n\tcmpl\t%eax, ARG_TO(%esp)\n\tjnb\t1f\n0:\tpushl\t$.copystr_panic_msg\n\tcall\tpanic\n1:\tpopl\t%ebp\n#endif\n\t/* get the current lofault address */\n\tmovl\t%gs:CPU_THREAD, %eax\n\tmovl\tT_LOFAULT(%eax), %eax\ndo_copystr:\n\tpushl\t%ebp\t\t\t/* setup stack frame */\n\tmovl\t%esp, %ebp\n\tpushl\t%ebx\t\t\t/* save registers */\n\tpushl\t%edi\n\n\tmovl\t%gs:CPU_THREAD, %ebx\t\n\tmovl\tT_LOFAULT(%ebx), %edi\n\tpushl\t%edi\t\t\t/* save the current lofault */\n\tmovl\t%eax, T_LOFAULT(%ebx)\t/* new lofault */\n\n\tmovl\tARG_MAXLEN(%ebp), %ecx\n\tcmpl\t$0, %ecx\n\tje\tcopystr_enametoolong\t/* maxlength == 0 */\n\n\tmovl\tARG_FROM(%ebp), %ebx\t/* source address */\n\tmovl\tARG_TO(%ebp), %edx\t/* destination address */\n\ncopystr_loop:\n\tdecl\t%ecx\n\tmovb\t(%ebx), %al\n\tincl\t%ebx\t\n\tmovb\t%al, (%edx)\n\tincl\t%edx\n\tcmpb\t$0, %al\n\tje\tcopystr_null\t\t/* null char */\n\tcmpl\t$0, %ecx\n\tjne\tcopystr_loop\n\ncopystr_enametoolong:\n\tmovl\t$ENAMETOOLONG, %eax\n\tjmp\tcopystr_out\n\ncopystr_null:\n\txorl\t%eax, %eax\t\t/* no error */\n\ncopystr_out:\n\tcmpl\t$0, ARG_LENCOPIED(%ebp)\t/* want length? */\n\tje\tcopystr_done\t\t/* no */\n\tmovl\tARG_MAXLEN(%ebp), %edx\n\tsubl\t%ecx, %edx\t\t/* compute length and store it */\n\tmovl\tARG_LENCOPIED(%ebp), %ecx\n\tmovl\t%edx, (%ecx)\n\ncopystr_done:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %ebx\t\n\tmovl\t%edi, T_LOFAULT(%ebx)\t/* restore the original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n\tret\t\n\tSET_SIZE(copystr)\n\n#undef\tARG_FROM\n#undef\tARG_TO\n#undef\tARG_MAXLEN\n#undef\tARG_LENCOPIED\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from the user address space into\n * the kernel address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyinstr(const char *uaddr, char *kaddr, size_t maxlength,\n    size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyinstr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$32, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\tmovq\t%rcx, 0x18(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyinstr_panic_msg(%rip), %rdi\n\txorl\t%eax, %eax\n\tcall\tpanic\n1:\n#endif\n\t/*\n\t * pass lofault value as 5th argument to do_copystr\n\t * do_copystr expects whether or not we need smap in %r10d\n\t */\n\tleaq\t_copyinstr_error(%rip), %r8\n\tmovl\t$1, %r10d\n\n\tcmpq\t%rax, %rdi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tSMAP_DISABLE_INSTR(6)\n\tjmp\tdo_copystr\n4:\n\tmovq\t%gs:CPU_THREAD, %r9\n\tjmp\t3f\n\n_copyinstr_error:\n\tSMAP_ENABLE_INSTR(8)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tmovq\t0x18(%rsp), %rcx\n\tleave\n\tjmp\t*CP_COPYINSTR(%rax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tleave\n\tret\n\tSET_SIZE(copyinstr)\n\n#elif defined(__i386)\n\n#define\tARG_UADDR\t4\n#define\tARG_KADDR\t8\n\n\tENTRY(copyinstr)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyinstr_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyinstr_error, %eax\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copystr\n\tmovl\t%gs:CPU_THREAD, %edx\n\tjmp\t3f\n\n_copyinstr_error:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYINSTR(%eax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tret\n\tSET_SIZE(copyinstr)\n\n#undef\tARG_UADDR\n#undef\tARG_KADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Copy a null terminated string from the kernel\n * address space to the user address space.\n */\n\n#if defined(__lint)\n\n/* ARGSUSED */\nint\ncopyoutstr(const char *kaddr, char *uaddr, size_t maxlength,\n    size_t *lencopied)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyoutstr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tsubq\t$32, %rsp\n\n\t/*\n\t * save args in case we trap and need to rerun as a copyop\n\t */\n\tmovq\t%rdi, (%rsp)\n\tmovq\t%rsi, 0x8(%rsp)\n\tmovq\t%rdx, 0x10(%rsp)\n\tmovq\t%rcx, 0x18(%rsp)\n\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kaddr */\n\tjnb\t1f\n\tleaq\t.copyoutstr_panic_msg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\t/*\n\t * pass lofault value as 5th argument to do_copystr\n\t * pass one as 6th argument to do_copystr in %r10d\n\t */\n\tleaq\t_copyoutstr_error(%rip), %r8\n\tmovl\t$1, %r10d\n\n\tcmpq\t%rax, %rsi\t\t/* test uaddr < kernelbase */\n\tjae\t4f\n\tSMAP_DISABLE_INSTR(7)\n\tjmp\tdo_copystr\n4:\n\tmovq\t%gs:CPU_THREAD, %r9\n\tjmp\t3f\n\n_copyoutstr_error:\n\tSMAP_ENABLE_INSTR(9)\n\tmovq\t%r11, T_LOFAULT(%r9)\t/* restore the original lofault */\n3:\n\tmovq\tT_COPYOPS(%r9), %rax\n\tcmpq\t$0, %rax\n\tjz\t2f\n\n\t/*\n\t * reload args for the copyop\n\t */\n\tmovq\t(%rsp), %rdi\n\tmovq\t0x8(%rsp), %rsi\n\tmovq\t0x10(%rsp), %rdx\n\tmovq\t0x18(%rsp), %rcx\n\tleave\n\tjmp\t*CP_COPYOUTSTR(%rax)\n\t\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tleave\n\tret\n\tSET_SIZE(copyoutstr)\t\n\t\n#elif defined(__i386)\n\n#define\tARG_KADDR\t4\n#define\tARG_UADDR\t8\n\n\tENTRY(copyoutstr)\n\tmovl\tkernelbase, %ecx\n#ifdef DEBUG\n\tcmpl\t%ecx, ARG_KADDR(%esp)\n\tjnb\t1f\n\tpushl\t%ebp\n\tmovl\t%esp, %ebp\n\tpushl\t$.copyoutstr_panic_msg\n\tcall\tpanic\n1:\n#endif\n\tlea\t_copyoutstr_error, %eax\n\tcmpl\t%ecx, ARG_UADDR(%esp)\t/* test uaddr < kernelbase */\n\tjb\tdo_copystr\n\tmovl\t%gs:CPU_THREAD, %edx\n\tjmp\t3f\n\n_copyoutstr_error:\n\tpopl\t%edi\n\tmovl\t%gs:CPU_THREAD, %edx\t\n\tmovl\t%edi, T_LOFAULT(%edx)\t/* restore the original lofault */\n\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n3:\n\tmovl\tT_COPYOPS(%edx), %eax\n\tcmpl\t$0, %eax\n\tjz\t2f\n\tjmp\t*CP_COPYOUTSTR(%eax)\n\n2:\tmovl\t$EFAULT, %eax\t\t/* return EFAULT */\n\tret\n\tSET_SIZE(copyoutstr)\n\t\n#undef\tARG_KADDR\n#undef\tARG_UADDR\n\n#endif\t/* __i386 */\n#endif\t/* __lint */\n\n/*\n * Since all of the fuword() variants are so similar, we have a macro to spit\n * them out.  This allows us to create DTrace-unobservable functions easily.\n */\n\t\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/* ARGSUSED */\nint\nfuword64(const void *addr, uint64_t *dst)\n{ return (0); }\n\n#endif\n\n/* ARGSUSED */\nint\nfuword32(const void *addr, uint32_t *dst)\n{ return (0); }\n\n/* ARGSUSED */\nint\nfuword16(const void *addr, uint16_t *dst)\n{ return (0); }\n\n/* ARGSUSED */\nint\nfuword8(const void *addr, uint8_t *dst)\n{ return (0); }\n\n#else\t/* __lint */\n\n#if defined(__amd64)\n\n/*\n * Note that we don't save and reload the arguments here\n * because their values are not altered in the copy path.\n * Additionally, when successful, the smap_enable jmp will\n * actually return us to our original caller.\n */\n\n#define\tFUWORD(NAME, INSTR, REG, COPYOP, DISNUM, EN1, EN2)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovq\t%gs:CPU_THREAD, %r9;\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tleaq\t_flt_/**/NAME, %rdx;\t\t\\\n\tmovq\t%rdx, T_LOFAULT(%r9);\t\t\\\n\tSMAP_DISABLE_INSTR(DISNUM)\t\t\\\n\tINSTR\t(%rdi), REG;\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n\tINSTR\tREG, (%rsi);\t\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN1)\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN2)\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovq\tT_COPYOPS(%r9), %rax;\t\t\\\n\tcmpq\t$0, %rax;\t\t\t\\\n\tjz\t2f;\t\t\t\t\\\n\tjmp\t*COPYOP(%rax);\t\t\t\\\n2:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\t\n\tFUWORD(fuword64, movq, %rax, CP_FUWORD64,8,10,11)\n\tFUWORD(fuword32, movl, %eax, CP_FUWORD32,9,12,13)\n\tFUWORD(fuword16, movw, %ax, CP_FUWORD16,10,14,15)\n\tFUWORD(fuword8, movb, %al, CP_FUWORD8,11,16,17)\n\n#elif defined(__i386)\n\n#define\tFUWORD(NAME, INSTR, REG, COPYOP)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t%gs:CPU_THREAD, %ecx;\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n\tcmpl\t%eax, 4(%esp);\t\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tlea\t_flt_/**/NAME, %edx;\t\t\\\n\tmovl\t%edx, T_LOFAULT(%ecx);\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\t(%eax), REG;\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n\tINSTR\tREG, (%edx);\t\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\tT_COPYOPS(%ecx), %eax;\t\t\\\n\tcmpl\t$0, %eax;\t\t\t\\\n\tjz\t2f;\t\t\t\t\\\n\tjmp\t*COPYOP(%eax);\t\t\t\\\n2:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD(fuword32, movl, %eax, CP_FUWORD32)\n\tFUWORD(fuword16, movw, %ax, CP_FUWORD16)\n\tFUWORD(fuword8, movb, %al, CP_FUWORD8)\n\n#endif\t/* __i386 */\n\n#undef\tFUWORD\n\n#endif\t/* __lint */\n\n/*\n * Set user word.\n */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/* ARGSUSED */\nint\nsuword64(void *addr, uint64_t value)\n{ return (0); }\n\n#endif\n\n/* ARGSUSED */\nint\nsuword32(void *addr, uint32_t value)\n{ return (0); }\n\n/* ARGSUSED */\nint\nsuword16(void *addr, uint16_t value)\n{ return (0); }\n\n/* ARGSUSED */\nint\nsuword8(void *addr, uint8_t value)\n{ return (0); }\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n/*\n * Note that we don't save and reload the arguments here\n * because their values are not altered in the copy path.\n */\n\n#define\tSUWORD(NAME, INSTR, REG, COPYOP, DISNUM, EN1, EN2)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovq\t%gs:CPU_THREAD, %r9;\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tleaq\t_flt_/**/NAME, %rdx;\t\t\\\n\tSMAP_DISABLE_INSTR(DISNUM)\t\t\\\n\tmovq\t%rdx, T_LOFAULT(%r9);\t\t\\\n\tINSTR\tREG, (%rdi);\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN1)\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tSMAP_ENABLE_INSTR(EN2)\t\t\t\\\n\tmovq\t$0, T_LOFAULT(%r9);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovq\tT_COPYOPS(%r9), %rax;\t\t\\\n\tcmpq\t$0, %rax;\t\t\t\\\n\tjz\t3f;\t\t\t\t\\\n\tjmp\t*COPYOP(%rax);\t\t\t\\\n3:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD(suword64, movq, %rsi, CP_SUWORD64,12,18,19)\n\tSUWORD(suword32, movl, %esi, CP_SUWORD32,13,20,21)\n\tSUWORD(suword16, movw, %si, CP_SUWORD16,14,22,23)\n\tSUWORD(suword8, movb, %sil, CP_SUWORD8,15,24,25)\n\n#elif defined(__i386)\n\n#define\tSUWORD(NAME, INSTR, REG, COPYOP)\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t%gs:CPU_THREAD, %ecx;\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n\tcmpl\t%eax, 4(%esp);\t\t\t\\\n\tjae\t1f;\t\t\t\t\\\n\tlea\t_flt_/**/NAME, %edx;\t\t\\\n\tmovl\t%edx, T_LOFAULT(%ecx);\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\tREG, (%eax);\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n\txorl\t%eax, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n_flt_/**/NAME:\t\t\t\t\t\\\n\tmovl\t$0, T_LOFAULT(%ecx);\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\tT_COPYOPS(%ecx), %eax;\t\t\\\n\tcmpl\t$0, %eax;\t\t\t\\\n\tjz\t3f;\t\t\t\t\\\n\tmovl\tCOPYOP(%eax), %ecx;\t\t\\\n\tjmp\t*%ecx;\t\t\t\t\\\n3:\t\t\t\t\t\t\\\n\tmovl\t$-1, %eax;\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD(suword32, movl, %edx, CP_SUWORD32)\n\tSUWORD(suword16, movw, %dx, CP_SUWORD16)\n\tSUWORD(suword8, movb, %dl, CP_SUWORD8)\n\n#endif\t/* __i386 */\n\n#undef\tSUWORD\n\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/*ARGSUSED*/\nvoid\nfuword64_noerr(const void *addr, uint64_t *dst)\n{}\n\n#endif\n\n/*ARGSUSED*/\nvoid\nfuword32_noerr(const void *addr, uint32_t *dst)\n{}\n\n/*ARGSUSED*/\nvoid\nfuword8_noerr(const void *addr, uint8_t *dst)\n{}\n\n/*ARGSUSED*/\nvoid\nfuword16_noerr(const void *addr, uint16_t *dst)\n{}\n\n#else   /* __lint */\n\n#if defined(__amd64)\n\n#define\tFUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tcmovnbq\tkernelbase(%rip), %rdi;\t\t\\\n\tINSTR\t(%rdi), REG;\t\t\t\\\n\tINSTR\tREG, (%rsi);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD_NOERR(fuword64_noerr, movq, %rax)\n\tFUWORD_NOERR(fuword32_noerr, movl, %eax)\n\tFUWORD_NOERR(fuword16_noerr, movw, %ax)\n\tFUWORD_NOERR(fuword8_noerr, movb, %al)\n\n#elif defined(__i386)\n\n#define\tFUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tcmpl\tkernelbase, %eax;\t\t\\\n\tjb\t1f;\t\t\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n1:\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\t(%eax), REG;\t\t\t\\\n\tINSTR\tREG, (%edx);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tFUWORD_NOERR(fuword32_noerr, movl, %ecx)\n\tFUWORD_NOERR(fuword16_noerr, movw, %cx)\n\tFUWORD_NOERR(fuword8_noerr, movb, %cl)\n\n#endif\t/* __i386 */\n\n#undef\tFUWORD_NOERR\n\n#endif\t/* __lint */\n\n#if defined(__lint)\n\n#if defined(__amd64)\n\n/*ARGSUSED*/\nvoid\nsuword64_noerr(void *addr, uint64_t value)\n{}\n\n#endif\n\n/*ARGSUSED*/\nvoid\nsuword32_noerr(void *addr, uint32_t value)\n{}\n\n/*ARGSUSED*/\nvoid\nsuword16_noerr(void *addr, uint16_t value)\n{}\n\n/*ARGSUSED*/\nvoid\nsuword8_noerr(void *addr, uint8_t value)\n{}\n\n#else\t/* lint */\n\n#if defined(__amd64)\n\n#define\tSUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tcmpq\tkernelbase(%rip), %rdi;\t\t\\\n\tcmovnbq\tkernelbase(%rip), %rdi;\t\t\\\n\tINSTR\tREG, (%rdi);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD_NOERR(suword64_noerr, movq, %rsi)\n\tSUWORD_NOERR(suword32_noerr, movl, %esi)\n\tSUWORD_NOERR(suword16_noerr, movw, %si)\n\tSUWORD_NOERR(suword8_noerr, movb, %sil)\n\n#elif defined(__i386)\n\n#define\tSUWORD_NOERR(NAME, INSTR, REG)\t\t\\\n\tENTRY(NAME)\t\t\t\t\\\n\tmovl\t4(%esp), %eax;\t\t\t\\\n\tcmpl\tkernelbase, %eax;\t\t\\\n\tjb\t1f;\t\t\t\t\\\n\tmovl\tkernelbase, %eax;\t\t\\\n1:\t\t\t\t\t\t\\\n\tmovl\t8(%esp), %edx;\t\t\t\\\n\tINSTR\tREG, (%eax);\t\t\t\\\n\tret;\t\t\t\t\t\\\n\tSET_SIZE(NAME)\n\n\tSUWORD_NOERR(suword32_noerr, movl, %edx)\n\tSUWORD_NOERR(suword16_noerr, movw, %dx)\n\tSUWORD_NOERR(suword8_noerr, movb, %dl)\n\n#endif\t/* __i386 */\n\n#undef\tSUWORD_NOERR\n\n#endif\t/* lint */\n\n\n#if defined(__lint)\n\n/*ARGSUSED*/\nint\nsubyte(void *addr, uchar_t value)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nsubyte_noerr(void *addr, uchar_t value)\n{}\n\n/*ARGSUSED*/\nint\nfulword(const void *addr, ulong_t *valuep)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nfulword_noerr(const void *addr, ulong_t *valuep)\n{}\n\n/*ARGSUSED*/\nint\nsulword(void *addr, ulong_t valuep)\n{ return (0); }\n\n/*ARGSUSED*/\nvoid\nsulword_noerr(void *addr, ulong_t valuep)\n{}\n\n#else\n\n\t.weak\tsubyte\n\tsubyte=suword8\n\t.weak\tsubyte_noerr\n\tsubyte_noerr=suword8_noerr\n\n#if defined(__amd64)\n\n\t.weak\tfulword\n\tfulword=fuword64\n\t.weak\tfulword_noerr\n\tfulword_noerr=fuword64_noerr\n\t.weak\tsulword\n\tsulword=suword64\n\t.weak\tsulword_noerr\n\tsulword_noerr=suword64_noerr\n\n#elif defined(__i386)\n\n\t.weak\tfulword\n\tfulword=fuword32\n\t.weak\tfulword_noerr\n\tfulword_noerr=fuword32_noerr\n\t.weak\tsulword\n\tsulword=suword32\n\t.weak\tsulword_noerr\n\tsulword_noerr=suword32_noerr\n\n#endif /* __i386 */\n\n#endif /* __lint */\n\n#if defined(__lint)\n\n/*\n * Copy a block of storage - must not overlap (from + len <= to).\n * No fault handler installed (to be called under on_fault())\n */\n\n/* ARGSUSED */\nvoid\ncopyout_noerr(const void *kfrom, void *uto, size_t count)\n{}\n\n/* ARGSUSED */\nvoid\ncopyin_noerr(const void *ufrom, void *kto, size_t count)\n{}\n\n/*\n * Zero a block of storage in user space\n */\n\n/* ARGSUSED */\nvoid\nuzero(void *addr, size_t count)\n{}\n\n/*\n * copy a block of storage in user space\n */\n\n/* ARGSUSED */\nvoid\nucopy(const void *ufrom, void *uto, size_t ulength)\n{}\n\n/*\n * copy a string in user space\n */\n\n/* ARGSUSED */\nvoid\nucopystr(const char *ufrom, char *uto, size_t umaxlength, size_t *lencopied)\n{}\n\n#else /* __lint */\n\n#if defined(__amd64)\n\n\tENTRY(copyin_noerr)\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rsi\t\t/* %rsi = kto */\n\tjae\t1f\n\tleaq\t.cpyin_ne_pmsg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\tcmpq\t%rax, %rdi\t\t/* ufrom < kernelbase */\n\tjb\tdo_copy\n\tmovq\t%rax, %rdi\t\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyin_noerr)\n\n\tENTRY(copyout_noerr)\n\tmovq\tkernelbase(%rip), %rax\n#ifdef DEBUG\n\tcmpq\t%rax, %rdi\t\t/* %rdi = kfrom */\n\tjae\t1f\n\tleaq\t.cpyout_ne_pmsg(%rip), %rdi\n\tjmp\tcall_panic\t\t/* setup stack and call panic */\n1:\n#endif\n\tcmpq\t%rax, %rsi\t\t/* uto < kernelbase */\n\tjb\tdo_copy\n\tmovq\t%rax, %rsi\t\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyout_noerr)\n\n\tENTRY(uzero)\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tjb\tdo_zero\n\tmovq\t%rax, %rdi\t/* force fault at kernelbase */\n\tjmp\tdo_zero\n\tSET_SIZE(uzero)\n\n\tENTRY(ucopy)\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tcmovaeq\t%rax, %rdi\t/* force fault at kernelbase */\n\tcmpq\t%rax, %rsi\n\tcmovaeq\t%rax, %rsi\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(ucopy)\n\n\t/*\n\t * Note, the frame pointer is required here becuase do_copystr expects\n\t * to be able to pop it off!\n\t */\n\tENTRY(ucopystr)\n\tpushq\t%rbp\n\tmovq\t%rsp, %rbp\n\tmovq\tkernelbase(%rip), %rax\n\tcmpq\t%rax, %rdi\n\tcmovaeq\t%rax, %rdi\t/* force fault at kernelbase */\n\tcmpq\t%rax, %rsi\n\tcmovaeq\t%rax, %rsi\t/* force fault at kernelbase */\n\t/* do_copystr expects lofault address in %r8 */\n\t/* do_copystr expects whether or not we need smap in %r10 */\n\txorl\t%r10d, %r10d\n\tmovq\t%gs:CPU_THREAD, %r8\n\tmovq\tT_LOFAULT(%r8), %r8\n\tjmp\tdo_copystr\n\tSET_SIZE(ucopystr)\n\n#elif defined(__i386)\n\n\tENTRY(copyin_noerr)\n\tmovl\tkernelbase, %eax\n#ifdef DEBUG\n\tcmpl\t%eax, 8(%esp)\n\tjae\t1f\n\tpushl\t$.cpyin_ne_pmsg\n\tcall\tpanic\n1:\n#endif\n\tcmpl\t%eax, 4(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyin_noerr)\n\n\tENTRY(copyout_noerr)\n\tmovl\tkernelbase, %eax\n#ifdef DEBUG\n\tcmpl\t%eax, 4(%esp)\n\tjae\t1f\n\tpushl\t$.cpyout_ne_pmsg\n\tcall\tpanic\n1:\n#endif\n\tcmpl\t%eax, 8(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(copyout_noerr)\n\n\tENTRY(uzero)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\tdo_zero\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_zero\n\tSET_SIZE(uzero)\n\n\tENTRY(ucopy)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\t1f\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n1:\n\tcmpl\t%eax, 8(%esp)\n\tjb\tdo_copy\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n\tjmp\tdo_copy\n\tSET_SIZE(ucopy)\n\n\tENTRY(ucopystr)\n\tmovl\tkernelbase, %eax\n\tcmpl\t%eax, 4(%esp)\n\tjb\t1f\n\tmovl\t%eax, 4(%esp)\t/* force fault at kernelbase */\n1:\n\tcmpl\t%eax, 8(%esp)\n\tjb\t2f\n\tmovl\t%eax, 8(%esp)\t/* force fault at kernelbase */\n2:\n\t/* do_copystr expects the lofault address in %eax */\n\tmovl\t%gs:CPU_THREAD, %eax\n\tmovl\tT_LOFAULT(%eax), %eax\n\tjmp\tdo_copystr\n\tSET_SIZE(ucopystr)\n\n#endif\t/* __i386 */\n\n#ifdef DEBUG\n\t.data\n.kcopy_panic_msg:\n\t.string \"kcopy: arguments below kernelbase\"\n.bcopy_panic_msg:\n\t.string \"bcopy: arguments below kernelbase\"\n.kzero_panic_msg:\n        .string \"kzero: arguments below kernelbase\"\n.bzero_panic_msg:\n\t.string\t\"bzero: arguments below kernelbase\"\n.copyin_panic_msg:\n\t.string \"copyin: kaddr argument below kernelbase\"\n.xcopyin_panic_msg:\n\t.string\t\"xcopyin: kaddr argument below kernelbase\"\n.copyout_panic_msg:\n\t.string \"copyout: kaddr argument below kernelbase\"\n.xcopyout_panic_msg:\n\t.string\t\"xcopyout: kaddr argument below kernelbase\"\n.copystr_panic_msg:\n\t.string\t\"copystr: arguments in user space\"\n.copyinstr_panic_msg:\n\t.string\t\"copyinstr: kaddr argument not in kernel address space\"\n.copyoutstr_panic_msg:\n\t.string\t\"copyoutstr: kaddr argument not in kernel address space\"\n.cpyin_ne_pmsg:\n\t.string \"copyin_noerr: argument not in kernel address space\"\n.cpyout_ne_pmsg:\n\t.string \"copyout_noerr: argument not in kernel address space\"\n#endif\n\n#endif\t/* __lint */\n\n/*\n * These functions are used for SMAP, supervisor mode access protection. They\n * are hotpatched to become real instructions when the system starts up which is\n * done in mlsetup() as a part of enabling the other CR4 related features.\n *\n * Generally speaking, smap_disable() is a stac instruction and smap_enable is a\n * clac instruction. It's safe to call these any number of times, and in fact,\n * out of paranoia, the kernel will likely call it at several points.\n */\n\n#if defined(__lint)\n\nvoid\nsmap_enable(void)\n{}\n\nvoid\nsmap_disable(void)\n{}\n\n#else\n\n#if defined (__amd64) || defined(__i386)\n\tENTRY(smap_disable)\n\tnop\n\tnop\n\tnop\n\tret\n\tSET_SIZE(smap_disable)\n\n\tENTRY(smap_enable)\n\tnop\n\tnop\n\tnop\n\tret\n\tSET_SIZE(smap_enable)\n\n#endif /* __amd64 || __i386 */\n\n#endif /* __lint */\n\n#ifndef __lint\n\n.data\n.align \t4\n.globl\t_smap_enable_patch_count\n.type\t_smap_enable_patch_count,@object\n.size\t_smap_enable_patch_count, 4\n_smap_enable_patch_count:\n\t.long\tSMAP_ENABLE_COUNT\n\n.globl\t_smap_disable_patch_count\n.type\t_smap_disable_patch_count,@object\n.size\t_smap_disable_patch_count, 4\n_smap_disable_patch_count:\n\t.long SMAP_DISABLE_COUNT\n\n#endif /* __lint */\n"], "filenames": ["usr/src/uts/intel/ia32/ml/copy.s"], "buggy_code_start_loc": [39], "buggy_code_end_loc": [1434], "fixing_code_start_loc": [39], "fixing_code_end_loc": [1434], "type": "CWE-20", "message": "illumos osnet-incorporation bcopy() and bzero() implementations make signed instead of unsigned comparisons allowing a system crash.", "other": {"cve": {"id": "CVE-2016-6560", "sourceIdentifier": "cret@cert.org", "published": "2017-03-31T19:59:00.160", "lastModified": "2019-10-09T23:19:15.863", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "illumos osnet-incorporation bcopy() and bzero() implementations make signed instead of unsigned comparisons allowing a system crash."}, {"lang": "es", "value": "Implementaciones illumos osnet-incorporation bcopy() y bzero() hacen firma en lugar de comparaciones no firmadas que permiten un bloqueo del sistema."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 8.6, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.8}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.9, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}, {"source": "cret@cert.org", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-195"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:illumos:illumos:-:*:*:*:*:*:*:*", "matchCriteriaId": "F696284A-497A-411E-994F-F4376162482F"}]}]}], "references": [{"url": "https://github.com/illumos/illumos-gate/commit/5aaab1a49679c26dbcb6fb6dc25799950d70cc71", "source": "cret@cert.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.illumos.org/issues/7488", "source": "cret@cert.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://www.openindiana.org/2016/11/01/cve-2016-6560-cve-2016-6561-security-issues-in-illumos/", "source": "cret@cert.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/illumos/illumos-gate/commit/5aaab1a49679c26dbcb6fb6dc25799950d70cc71"}}